[
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCUTTING LONG GRADIENT FLOWS: DECOUPLING END-TO-END BACKPROPAGATION BASED ON SUPERVISED CONTRASTIVE LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nEnd-to-end backpropagation (BP) is the foundation of current deep learning technology. Unfortunately, as a network becomes deeper, BP becomes inefficient for various reasons. This paper proposes a new methodology for decoupling BP to transform a long gradient flow into multiple short ones in order to address the optimization issues caused by long gradient flows. We report thorough experiments conducted to illustrate the effectiveness of our model compared with BP, Early Exit, and associated learning (AL), a state-of-the-art methodology for backpropagation decoupling. We release the experimental code for reproducibility.\n\n1\n\nINTRODUCTION\n\nCurrent deep learning technology largely depends on backpropagation and gradient-based learning methods (e.g., gradient descent) for model training. Meanwhile, many successful applications rely on extremely deep neural networks; for example, Transformer contains at least 12 layers (most have several sublayers) (Vaswani et al., 2017), BERT has 12 to 24 layers (most also have several sublayers) (Devlin et al., 2018), and GoogLeNet has 22 layers (many layers are Inception modules containing several sublayers) (Szegedy et al., 2015). However, training a deep network based on backpropagation is inefficient for many reasons. First, a long gradient flow may suffer from gradient vanishing or explosion (Hochreiter, 1998). Second, a long gradient flow may lead to unstable gradients in the early layers (the layers close to the input layer) (Nielsen, 2015). Third, backpropagation results in backward locking, meaning that the gradient of a network parameter can be computed only when all other gradients on which it depends have been computed (Jaderberg et al., 2017). These issues may become severe bottlenecks, especially when a network is deep. To train deep networks more efficiently, researchers have developed various strategies, such as batch normalization, gradient clipping, new activation functions (e.g., ReLU and leaky ReLU), new network architectures (e.g., LSTM (Hochreiter & Schmidhuber, 1997)), and many more.\n\nSince a long gradient flow is a root cause of the above issues, a possible way to eliminate these issues is to shorten the length of the gradient flow, for example, by cutting a network into multiple components and assigning a local objective to each component. In this way, a long gradient flow can be divided into multiple shorter pieces, which should, at least partially, address the issues of vanishing/exploding gradients, unstable gradients in early layers, and backward locking. Perhaps the most straightforward approach for assigning a local objective to a component is by adding a local auxiliary classifier that outputs a predicted ˆy and updates the local parameters based on the difference between ˆy and the ground-truth target y. We call this strategy “Early Exit” in this paper because each such auxiliary classifier can be regarded as an exit of the neural network. The concept of Early Exit has been used in many previous studies, e.g., Mostafa et al. (2018); Teerapittayanon et al. (2016); Szegedy et al. (2015). However, most of these studies have used Early Exit for other purposes, e.g., creating multiple prediction paths or helping to obtain gradients for the parameters in the early layers. Consequently, these studies have not investigated the separation of end-to-end backpropagation (BP) into multiple pieces, and the associated gradient flows are still long. In addition, even if Early Exit is used to isolate the gradient flow, as shown in (Mostafa et al., 2018), the test accuracies are lower than those of models trained via BP. There are other methods of cutting long gradient flows (Jaderberg et al., 2017; Czarnecki et al., 2017; Löwe et al., 2019; Wu et al., 2022; Kao & Chen, 2021). However, most of these methods have been applied only to simple networks, and\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: An illustration of contrastive learning and supervised contrastive learning.\n\ntheir test accuracies are still unsatisfactory likely because the local objectives may not align with the global objective; details will be discussed in Section 4.\n\nThis paper proposes a new methodology, Delog-SCL, which decouples the long gradient flow of a deep neural network by leveraging supervised contrastive learning (SCL). In this design, the forward path transforms an input x into the corresponding prediction ˆy as usual. However, the gradient flow on the backward path is blocked between different components. Instead of using a global objective, we assign a local objective to each component and force each gradient flow to remain within one component. We present experiments conducted on multiple open datasets and compare the result with those of models trained via BP, Early Exit, and associated learning (AL) (Wu et al., 2022; Kao & Chen, 2021), a state-of-the-art methodology for BP decoupling that yields results comparable to those of BP. We find that Delog-SCL outperforms AL in terms of test accuracy in most cases with fewer parameters. Additionally, since our method has a more straightforward network architecture than AL, our method could be a favorable alternative to AL.\n\nThe rest of the paper is organized as follows. In Section 2, we introduce Delog-SCL and its properties. Section 3 presents a comparison of Delog-SCL with BP, Early Exit, and AL in terms of their test accuracies and numbers of parameters. We also report the results of analyses on certain properties of Delog-SCL in the same section. Section 4 reviews previous works on BP decoupling and presents a comparison of these works with our model. We conclude our contribution in Section 5.\n\n2 METHODOLOGY\n\n2.1 PRELIMINARIES: CONTRASTIVE LEARNING AND SUPERVISED CONTRASTIVE LEARNING\n\nContrastive learning (CL) is a self-supervised technique for learning visual representations of images. Referring to the left of Figure 1, given an image x, CL involves generating different views (i.e., x1 and x2 in Figure 1) via the same family of data augmentations T . The generated views (x1 and x2) are further transformed via an encoder function f and a projection head g to minimize the contrastive loss between the output vectors (i.e., z1 and z2). After training, the projection head g is disregarded, and only the encoder f is used to generate the visual representations of images (Chen et al., 2020). In other words, given an anchor image x, CL relies on regarding its augmented images as positive instances and all other images as negative instances and considering that positive pairs should be close after encoding and projection.\n\nSCL refers to the extension of CL from a self-supervised setting to a fully supervised setting. Therefore, the training data for SCL consist of not only the training images themselves but also the classes of those images. Referring to the right of Figure 1, given an anchor image x of class c, the positive instances include the other images of class c in the same batch, whereas all other images in the same batch are regarded as negative instances (Khosla et al., 2020).\n\n2\n\nx!xx\"r!r\"TTffz!z\"ggContrastive lossxx#x!r!fz!gTx!#r!#fz!#gTSupervised contrastive lossUnder review as a conference paper at ICLR 2023\n\nFigure 2: An example neural network with 3 hidden layers. The black arrows correspond to the forward path, the red arrows correspond to the backward path, and the green box denotes the comparison of the distance between two incoming variables ˆy(i) and y(i).\n\nFigure 3: An example of a decoupled neural network based on supervised contrastive learning. The black arrows correspond to the forward path, the red arrows correspond to the backward path, and the green box denotes the comparison of the distance between two incoming variables.\n\n2.2 DECOUPLING END-TO-END BACKPROPAGATION VIA SUPERVISED CONTRASTIVE\n\nLEARNING\n\nThis section presents Delog-SCL, which leverages the supervised contrastive loss to split one long gradient flow in a deep neural network into multiple shorter ones.\n\nl\n\n(under the assumptions that x(i) = r(i)\n\n0 and the predicted class ˆy(i) = r(i)\n\nLet us first consider a standard neural network with 3 hidden layers as an example. As shown in Figure 2, x(i) refers to an input image i, and the function fl (l = 1, . . . , 4) transforms r(i) l−1 into r(i) 4 ). Depending on the network architecture, the functions fl could be various well-known neural network layers, such as fully connected layers, convolutional layers, pooling layers, or residual blocks. The objective LOU T is determined by the task type. For example, if we are addressing a classification task, we could use the cross-entropy loss between the predicted class ˆy(i) and the ground-truth class y(i) as the objective LOU T . We use backpropagation to obtain ∂LOU T /∂θfl for each layer l, where θfl represents the parameters of function fl. Once the gradients are obtained, we can use gradient-based optimization strategies, e.g., gradient descent, to update the parameter values. Given a neural network with H hidden layers, it can be seen that the longest gradient flow is constructed as a product of H + 2 local gradients. For example, to obtain ∂LOU T /θf1 in a network with 3 hidden layers (as shown in Figure 2), we need the following:\n\n∂LOU T ∂θf1\n\n=\n\n∂LOU T ∂ ˆy(i)\n\n×\n\n∂ ˆy(i) ∂r(i)\n\n3\n\n×\n\n3\n\n3\n\n∂r(i) ∂r(i)\n\n2\n\n×\n\n2\n\n∂r(i) ∂r(i)\n\n1\n\n×\n\n∂r(i) 1\n∂θf1\n\n.\n\n(1)\n\nx!r\"!r#!r$!#y!f\"f#f$f%y!L&’(∂L&’(∂θ)!∂L&’(∂θ)\"∂L&’(∂θ)#∂L&’(∂θ)$x!r\"!#y!f\"f#x$r\"$#y$f\"f#z\"!g\"z\"$g\"L\"%&Component 4y!L!’()y$L$’()∂L\"%&∂θ*!∂L\"%&∂θ+!∂L\"%&∂θ+!∂L\"%&∂θ*!Component 1r,!f,r,$f,z,!g,z,$g,L,%&∂L,%&∂θ*\"∂L,%&∂θ+\"∂L,%&∂θ+\"∂L,%&∂θ*\"Component 2r-!f-r-$f-z-!g-z-$g-L-%&∂L-%&∂θ*#∂L-%&∂θ+#∂L-%&∂θ+#∂L-%&∂θ*#Component 3∂L!’()∂θ+$∂L$’()∂θ+$Under review as a conference paper at ICLR 2023\n\nThe number of terms of this product grows linearly with the depth of the network. Therefore, as networks become deeper, their long gradient flows cause several optimization issues, as discussed in Section 1.\n\n0\n\n0\n\n0 and r(j)\n\n(i.e., x(i)) and r(j)\n\nWe use Figure 3 to illustrate our strategy of cutting a long gradient flow into several local gradients for a neural network with 3 hidden layers. Let r(i) (i.e., x(j)) be two image views in the same batch (r(i) 0 may or may not be augmented images, i.e., views, from the same image). We use f1 to transform each of them, obtaining r(i) 1 , and further use the function g1 to convert them into z(i) 1 , respectively. The functions f1 and g1 can be regarded as the encoding function and the projection head, respectively, in CL (refer to Figure 1). We repeat the same process for each hidden layer l to form the corresponding component l. If x(i) and x(j) are two different views of the same image or if y(i) (the class of x(i)) is equal to y(j) (the class of x(j)), then we should ensure that z(i) for all l. Otherwise, we should increase the distance between z(i) . In the last layer, we compute the distance between ˆy(i) and y(i) as the loss LOU T for batch B in layer l as shown in Equation 2:\n\n. Eventually, we define the local supervised contrastive loss LSC\n\nis close to z(j)\n\n1 and z(j)\n\nand r(j)\n\nand z(j)\n\n1\n\nl\n\nl\n\nl\n\nl\n\nl\n\ni\n\nLSC\n\nl =\n\n(cid:88)\n\n∀i∈B\n\n−1 |P (i)|\n\n(cid:88)\n\nlog\n\n∀p∈P (i)\n\n(cid:80)\n\nexp\n\n(cid:16)\n\nz(i)\n\nl\n\n· z(p) l /τ (cid:16)\n\nz(i)\n\nl\n\n(cid:17)\n\n(cid:17) ,\n\n· z(j)\n\nl /τ\n\n(2)\n\n∀j∈B I(j ̸= i) exp\n\nwhere B = 1, 2, . . . , N represents a batch of multiview images, P (i) is the set of all positive samples for an image i, τ is a hyperparameter, and I(j ̸= i) ∈ {0, 1} is an indicator function that returns 1 if j ̸= i and 0 otherwise.\n\nUltimately, the global objective function is an accumulation of the local supervised contrastive losses and the losses in the output layer, as defined below:\n\nL =\n\nH (cid:88)\n\nl=1\n\nLSC\n\nl +\n\nN (cid:88)\n\ni=1\n\nLOU T\n\ni\n\n,\n\n(3)\n\nwhere H is the number of hidden layers and LOU T Figure 3).\n\ni\n\nis the ith loss in the output layer (refer to\n\nThe computation of LSC in Algorithm 1 and Algorithm 2 in Appendix A.5.\n\nl\n\nand the pseudo code of Delog-SCL for a 3-layer vanilla ConvNet is given\n\n2.3 FORWARD PATH, BACKWARD PATH, AND INFERENCE FUNCTION\n\nFor a regular neural network (e.g., Figure 2), the forward path and the inference function are identical, and the backward path is simply obtained by inverting the direction of the forward path. However, the situation is more complicated in our case because we divide the global objective into several local ones. Consequently, we have multiple short forward paths, multiple short backward paths, and one inference path. Thus, the inference path and the forward paths are no longer identical in DelogSCL.\n\nDuring training, each component l has its own forward and backward paths. Taking Figure 3 as an example, the forward path of component l transforms each r(i) l via the local encoding function fl and further transforms each r(i) via the local projection head gl. On the backward path, each hidden layer computes ∂LSC l /∂θfl based on the chain rule and updates the parameters by means of gradient-based optimization strategies. We block the gradient flow between each component.1 As a result, each gradient flow remains within one component and is therefore short. Equation 4 and Equation 5 show these local gradient flows.\n\ninto z(i) l /∂θgl and ∂LSC\n\nl−1 into r(i)\n\nl\n\nl\n\n1The gradient flow can be blocked by using Tensor.detach() in PyTorch or tf.stop_gradient\n\nin TensorFlow.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n∂LSC l\n∂θgl\n\n=\n\n∂LSC l\n∂z(i)\n\nl\n\n×\n\n∂z(i) l\n∂θgl\n\n.\n\n∂LSC l\n∂θfl\n\n=\n\n∂LSC l\n∂z(i)\n\nl\n\n×\n\nl\n\n∂z(i) ∂r(i)\n\nl\n\n×\n\n∂r(i) l\n∂θfl\n\n.\n\n(4)\n\n(5)\n\nEventually, even if we construct a deep neural network, the cost of computing each ∂LSC/∂θfl and each ∂LSC/∂θgl remains constant. Additionally, the gradient flow in the output layer is also short: we simply compute ∂Lout k /∂θfH+1 (where H is the number of hidden layers). This design alleviates various issues caused by long gradient flows.\n\nIn the inference (prediction) phase, we need the functions fl but not gl, as shown by Equation 6:\n\nˆy(i) = fH+1 ◦ fH ◦ . . . ◦ f2 ◦ f1(x(i)),\n\n(6)\n\nwhere ◦ is the function composition operator (H = 3 for the example illustrated in Figure 3).\n\nAlthough our proposed method (e.g., Figure 3) involves more parameters than a standard neural network structure (e.g., Figure 2) during training, they have the same number of parameters during inference because both of them use only the functions fl. Therefore, they have the same hypothesis space. The parameters that participate in the inference phase (denoted by θfl) are called the effective parameters, and the parameters used during training but not during inference (denoted by θgl) are called the affiliated parameters.\n\n2.4 PROPERTIES\n\nTable 1: An illustration of the training process pipeline\n\nt1\n\nt2\n\nt3\n\nt4\n\nt5\n\nt6\n\n...\n\nTask 1 Task 2 Task 3\n\nTask 1 Task 2 Task 3\n\nTask 1 Task 2 Task 3\n\nTask 1 Task 2 Task 3\n\nB1 B2 B3 B4 ...\n\nIn this section, we discuss three properties of our proposed model — short gradient flows, a flexible structure, and the ability to perform parallel (pipelined) training.\n\nAs discussed in Section 2.3, training a regular neural network with BP requires a gradient flow of length O(H). In contrast, the length of each gradient flow in our model is independent of the number of layers; the length is always a constant. Therefore, the various optimization issues resulting from long gradient flows, as discussed in Section 1, are eliminated (or at least alleviated).\n\nThe network structure is more flexible and perhaps easier to understand than that of associated learning (AL), a state-of-the-art methodology for decoupling BP in terms of test accuracy (Kao & Chen, 2021; Wu et al., 2022). Specifically, AL involves projecting the features x and the target y into the same latent space for each layer l. Although this design yields excellent test accuracies that are comparable to those of BP-trained models (Wu et al., 2022), it has at least two unconventional and perhaps mysterious characteristics. First, AL involves projecting a one-hot-encoded target variable y into a latent vector t1 and then transforming t1 back into y. Interestingly, the length of t1 is sometimes greater than the number of classes. This process corresponds to building an autoencoder whose bottleneck layer is larger than the input and output layers. Although this unconventional approach works surprisingly well in practice (Wu et al., 2022; Kao & Chen, 2021), the fundamental reasons for this are still unclear. Second, when converting a neural network into its AL form, we sometimes need to create extra fully connected layers. In contrast, our design is more natural because we need neither the autoencoder nor the extra fully connected layers.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFinally, since each component has its own local objective, we can parallelize the training procedure by means of pipelining. We use the network illustrated in Figure 3 as an example. Let Task l denote the entire forward and backward process in layer l; then, we can illustrate the pipelining process as shown in Table 1. Specifically, in the first time unit t1, component 1 uses the first batch (B1) to perform Task 1. At t2, component 2 performs Task 2 based on B1, and component 1 continues to performing Task 1 based on the second batch (B2). Starting at t3, all three components can perform forward propagation, backward propagation, and parameter updating simultaneously. However, we have shown here only that parallelization by means of pipelining is feasible; implementation of the pipeline mechanism is left for future work.\n\n3 EXPERIMENTS\n\nWe compare Delog-SCL with three baselines using different neural networks on different datasets. The baseline models include BP, the Early Exit mechanism introduced in Section 1, and AL, a stateof-the-art method for BP decoupling in terms of the test accuracy. We test three neural networks: a vanilla convolutional neural network (vanilla ConvNet), the VGG network (Simonyan & Zisserman, 2014), and the residual network (ResNet) (He et al., 2016). The experimental datasets include CIFAR-10 (consists of 50, 000 color training images and 10, 000 test images; each image belongs to 1 of 10 classes), CIFAR-100 (consists of 50, 000 color training images and 10, 000 test images; each image belongs to 1 of 100 classes), and Tiny-ImageNet (consists of 100, 000 color training images, 10, 000 validation images, and 10, 000 test images; each image belongs to 1 of 200 classes).\n\n3.1 ACCURACY COMPARISON\n\nTable 2: A comparison of the test accuracies (mean ± standard deviation) of different methodologies when using different neural network architectures on CIFAR-10. We highlight the winner among the non-BP methodologies in bold face. We mark a methodology with a † symbol if the test accuracy of this methodology is higher than that of BP.\n\nVanilla ConvNet\n\nVGG\n\nResNet\n\nBP\n\n86.85 ± 0.57\n\n93.02 ± 0.03\n\n93.95 ± 0.11\n\nEarly Exit AL Delog-SCL\n\n83.16 ± 0.33 86.98 ± 0.24 † 86.98 ± 0.33 †\n\n91.28 ± 0.15 93.22 ± 0.12 † 93.42 ± 0.11 †\n\n89.63 ± 0.34 91.33 ± 0.09 92.78 ± 0.11\n\nTable 3: A comparison of the test accuracies of different methodologies when using different neural network architectures on CIFAR-100. We follow the same notations used in Table 2.\n\nVanilla ConvNet\n\nVGG\n\nResNet\n\nBP\n\n58.68 ± 0.13\n\n72.58 ± 0.39\n\n73.59 ± 0.11\n\nEarly Exit AL Delog-SCL\n\n50.64 ± 0.44 53.06 ± 0.15 59.63 ± 0.37 †\n\n71.11 ± 0.95 72.43 ± 0.27 73.14 ± 0.30 †\n\n64.48 ± 0.41 67.53 ± 0.32 70.41 ± 0.27\n\nTable 4: A comparison of the test accuracies of different methodologies when using different neural network architectures on Tiny-ImageNet. We follow the same notations used in Table 2.\n\nVGG\n\nResNet\n\nBP\n\n48.30 ± 0.14\n\n49.71 ± 0.18\n\nEarly Exit AL Delog-SCL\n\n46 ± 0.18 49.06 ± 0.14 † 48.95 ± 0.17 †\n\n40 ± 0.34 44.83 ± 0.15 46.87 ± 0.26\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2 shows the test accuracies of the various methods on the CIFAR-10 dataset. The simple Early Exit mechanism can be used to learn the relationship between an image and its corresponding class. However, the test accuracies of Early Exit are much worse than those of BP. Both AL and our proposed Delog-SCL yield better test accuracies than BP based on the Vanilla ConvNet and VGG architectures. However, when ResNet is used, BP yields the highest test accuracy. If we compare only the methods that involve BP decomposition, Delog-SCL performs the best among them.\n\nWe also tested BP, Early Exit, AL, and Delog-SCL on CIFAR-100. The results, as shown in Table 3, are similar to those on CIFAR-10: Delog-SCL performs better than both AL and BP based on Vanilla ConvNet and VGG, whereas Delog-SCL performs worse than BP when ResNet is used. These results are also consistent with those reported in (Kao & Chen, 2021; Wu et al., 2022).\n\nTable 4 gives the results obtained on Tiny-ImageNet. When VGG is used, both AL and Delog-SCL outperform BP. However, for ResNet, BP performs much better.\n\nDelog-SCL is stable in training, as can be shown by Figure 5 in the Appendix.\n\n3.1.1 DISCUSSION ON ACCURACY COMPARISON\n\nWhen BP is used, all parameters are updated to minimize a global objective – the residual between the prediction ˆy and the target y. On the other hand, methods to decouple end-to-end backpropagation, such as Delog-SCL and AL, are composed of many local objectives, which may differ from the global objective. Therefore, it is surprising that Delog-SCL and AL outperform BP for some network structures. The authors of AL proposed several conjectures to explain this remarkable result. First, projecting the feature vector x and the target y into the same latent space may be helpful. Second, the autoencoder may implicitly perform some feature extraction and regularization. Third, overparameterization may be helpful for optimization (Arora et al., 2018; Chen & Chen, 2020). However, the first and second conjectures only apply to AL but not to Delog-SCL, but Delog-SCL still yields better accuracies than BP and AL in vanilla ConvNet and VGG. Therefore, the above conjectures may not fully explain the success of Delog-SCL. Further investigation will be needed to uncover the fundamental reasons.\n\nAs for ResNet, its authors state that the main effect of a residual is not about promoting gradient flows (He et al., 2016). Instead, ResNet performs better than vanilla ConvNet because the latent representations Hl and Hl+1 at deep neighboring layers l and l + 1 are likely similar. Regular nonlinear transformations may be difficult to approximate an (almost) identical mapping from Hl to Hl+1. However, the residual connection sets Hl+1 to be f (Hl) + Hl. Even if f () is a nonlinear function, a solver is easier to make Hl+1 ≈ Hl by making f (Hl) ≈ 0. The property that Hl+1 ≈ Hl is likely true when a network is deep. However, when using Delog-SCL or AL, each local network is short, so Delog-SCL and AL are unlikely to take advantage of the residual connections. As a result, optimizing a ResNet by BP usually gives better results than by BP-decoupling methods, such as Delog-SCL and AL.\n\n3.2 NUMBER OF EFFECTIVE PARAMETERS\n\nTable 5: A comparison of the numbers of parameters used during training and testing with different methodologies (using CIFAR-10 as an example)\n\nVGG\n\nResNet\n\nTraining Testing Training Testing\n\nBP\n\n11.9M 11.9M 11.2M 11.2M\n\nAL Delog-SCL\n\n284.9M 13.9M 325.1M 14.4M 70.1M 11.9M 72.4M 11.2M\n\nTable 5 shows the numbers of parameters required during training and inference for VGG and ResNet (using CIFAR-10 as an example).\n\nFor BP, the training and testing stages involve the same set of parameters. In contrast, the training process of AL requires additional bridge functions and encoding functions, which are not used\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Training Delog-SCL on CIFAR-10 using multiple short gradient flows vs using single long gradient flow.\n\nduring testing. So, the number of training parameters is much larger than the number of parameters required for BP. During testing, the extra fully connected layers in AL also require more parameters than are needed in BP. Finally, Delog-SCL and BP have the same number of testing parameters. However, during training, Delog-SCL needs the projection heads gl for each component, so the number of required parameters during training is more than for BP but much fewer than for AL.\n\nThe difference of the parameter counts may also be reflected on the training and inference speed. We show the practical training and testing time of different methods in Table 7 in the Appendix.\n\n3.3 MULTIPLE SHORT GRADIENT FLOWS ACCELERATE LEARNING\n\nThis section shows that dividing a long gradient flow into multiple short ones accelerates learning.\n\nReferring to Figure 3, our proposed Delog-SCL uses a local supervised contrastive loss to create a short local gradient flow LSC . We compare the standard Delog-SCL with a modification where only a single long gradient flow is used. In the compared baseline, we enable the global objective LOU T to pass through the entire network and remove all local supervised contrastive losses LSC\n\n.\n\nl\n\nl\n\nThe results are shown in Figure 4. We label the original Delog-SCL as “multiple short gradient flows” and the modification with single long gradient flow as “single gradient flow”. Using multiple short gradient flows accelerates the learning speed, especially in the first 100 epochs.\n\n3.4 THE EFFECT OF BATCH SIZE AND PROJECTION HEAD\n\nWe also experimented with how the batch size and the type of projection head influence learning. The experimental results show that a larger batch size improves the learning quality, which is consistent with previous studies (Chen et al., 2020; Henaff, 2020; Bachman et al., 2019). As for the projection head, using a nonlinear function benefits the representation quality of layers before it. The result also matches the experiments conducted in Chen et al. (2020).\n\nThe experimental details of batch size and projection heads are presented in Section A.3 and Section A.4 in Appendix.\n\n4 RELATED WORK\n\nStudies on alternatives to BP mostly aim to address optimization and performance issues, such as gradient vanishing/explosion and training costs. We review some of these works that have particularly focused on the creation of local objectives and local gradient flows.\n\n8\n\n100101102Epoch304050607080Test accuracy (%)Multiple local gradient flowsSingle gradient flowUnder review as a conference paper at ICLR 2023\n\nTable 6: A comparison of the properties of BP, AL, and Delog-SCL\n\nNumber of affiliated parameters\n\nStructure flexibility\n\nLength of gradient flows\n\nAllows pipelined training\n\nBP AL Delog-SCL\n\n0 Many Few\n\nHigh Medium Medium\n\nLong Short Short\n\nFalse True True\n\nThe first type of BP alternative is target propagation (Lee et al., 2015; Meulemans et al., 2020; Manchev & Spratling, 2020; Bengio, 2014), which assigns a local target for each layer via feedback (inverse) mapping. Such a methodology can alleviate the problem of vanishing/exploding gradients since each gradient flow is short. However, the parameters are still updated in a layerwise fashion, so it could be challenging to learn the parameters in different layers simultaneously.\n\nMethods of the second type model BP as a constrained optimization problem, in which the output of one layer is forced to equal the input to the next layer (Gotmare et al., 2018; Marra et al., 2020). Such a design shortens the gradient flows and enables parallel parameter updates. However, the experimental results show that the test accuracies are lower than that of standard BP.\n\nMethods of the third type determine the local objectives through transformations of the target. A representative method of this type is AL Wu et al. (2022); Kao & Chen (2021), which transforms both the feature vector x and the target y into the same set of latent spaces. To the best of our knowledge, AL is the only existing method that can achieve BP decoupling for a wide range of network architectures and yield test accuracies that are comparable to those obtained with BP.\n\nOur proposed Delog-SCL is motivated by both AL and Greedy InfoMax (GIM) (Löwe et al., 2019), which uses the contrastive loss as each local objective. However, GIM targets self-supervised learning tasks, whereas our Delog-SCL can handle supervised learning tasks because Delog-SCL uses the supervised contrastive loss in the hidden layers and the distance between the predicted and observed targets in the output layer.\n\nSince only Delog-SCL and AL yield test accuracies comparable to those of BP, in Table 6, we further compare the properties of these three methods. First, BP requires no affiliated parameters because all parameters collaborate to reduce the global loss. BP can be applied to almost all kinds of neural networks. However, its gradient flow is long (especially when the network is deep), and it is challenging to achieve pipelined training with this method. AL requires transforming both the feature vector x and the target y alongside each other. As a result, AL usually requires additional fully connected layers, resulting in a large number of affiliated parameters and less structural flexibility. However, each gradient flow in AL is short, and parameters in different layers can be updated simultaneously via pipelined training. Finally, because Delog-SCL requires computing the supervised contrastive loss in each hidden layer, this method also needs additional affiliated parameters (although much fewer than AL) during training. The introduction of the supervised contrastive loss also adds complexity in the network design. The advantages of Delog-SCL are similar to those of AL: the gradient flows are short, and parallel parameter updating is possible (via pipelining).\n\n5 CONCLUSION\n\nThis paper presents Delog-SCL, a new methodology for decoupling the components of the BP process in a neural network. Delog-SCL may address various optimization issues (e.g., vanishing/exploding gradients and unstable gradients in the early layers) resulting from the long gradient flows in deep neural networks. We report experiments conducted to show that Delog-SCL’s predictive power is comparable to (and frequently better than) that of either BP or AL, which is a state-of-the-art alternative to BP. Delog-SCL is more flexible than AL because Delog-SCL does not require additional fully connected layers, whereas AL usually does. Therefore, Delog-SCL is a natural substitute for AL and could be a promising alternative to BP.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit In International Conference on Machine Learning, pp.\n\nacceleration by overparameterization. 244–253. PMLR, 2018.\n\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. Advances in neural information processing systems, 32, 2019.\n\nYoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target\n\npropagation. arXiv preprint arXiv:1407.7906, 2014.\n\nPu Chen and Hung-Hsuan Chen. Accelerating matrix factorization by overparameterization.\n\nIn\n\nInternational Conference on Deep Learning Theory and Applications, pp. 89–97, 2020.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nWojciech Marian Czarnecki, Grzegorz ́Swirszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals, and Koray Kavukcuoglu. Understanding synthetic gradients and decoupled neural interfaces. In International Conference on Machine Learning, pp. 904–912. PMLR, 2017.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAkhilesh Gotmare, Valentin Thomas, Johanni Brea, and Martin Jaggi. Decoupling backpropagation using constrained optimization methods. In Credit assignment in Deep Learning and Deep Reinforcement Learning at NeurIPS Workshop, 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nOlivier Henaff. Data-efficient image recognition with contrastive predictive coding. In International\n\nconference on machine learning, pp. 4182–4192. PMLR, 2020.\n\nSepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02): 107–116, 1998.\n\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nMax Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. In International conference on machine learning, pp. 1627–1635. PMLR, 2017.\n\nYu-Wei Kao and Hung-Hsuan Chen. Associated learning: Decomposing end-to-end backpropagation based on autoencoders and target propagation. Neural Computation, 33(1):174–193, 2021.\n\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661–18673, 2020.\n\nDong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation. In Joint european conference on machine learning and knowledge discovery in databases, pp. 498–515. Springer, 2015.\n\nSindy Löwe, Peter O’Connor, and Bastiaan Veeling. Putting an end to end-to-end: Gradient-isolated\n\nlearning of representations. Advances in neural information processing systems, 32, 2019.\n\nNikolay Manchev and Michael W Spratling. Target propagation in recurrent neural networks. J.\n\nMach. Learn. Res., 21(7):1–33, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nGiuseppe Marra, Matteo Tiezzi, Stefano Melacci, Alessandro Betti, Marco Maggini, and Marco Gori. Local propagation in constraint-based neural networks. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE, 2020.\n\nAlexander Meulemans, Francesco Carzaniga, Johan Suykens, João Sacramento, and Benjamin F Grewe. A theoretical framework for target propagation. Advances in Neural Information Processing Systems, 33:20024–20036, 2020.\n\nJovana Mitrovic, Brian McWilliams, and Mélanie Rey. Less can be more in contrastive learning. In \"I Can’t Believe It’s Not Better!\" at NeurIPS Workshops, Virtual, volume 137 of Proceedings of Machine Learning Research, pp. 70–75. PMLR, 2020.\n\nHesham Mostafa, Vishwajith Ramesh, and Gert Cauwenberghs. Deep supervised learning using\n\nlocal errors. Frontiers in neuroscience, 12:608, 2018.\n\nMichael A Nielsen. Neural networks and deep learning, volume 25. Determination press San\n\nFrancisco, CA, USA, 2015.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015.\n\nSurat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd International Conference on Pattern Recognition (ICPR), pp. 2464–2469. IEEE, 2016.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nDennis YH Wu, Dinan Lin, Vincent Chen, and Hung-Hsuan Chen. Associated learning: an alternative to end-to-end backpropagation that works on cnn, rnn, and transformer. In International Conference on Learning Representations, 2022.\n\nA APPENDIX\n\nA.1 TEST ACCURACY VS EPOCH\n\nFigure 5 compares Delog-SCL and BP on vanilla CNN in terms of their dynamics of test accuracy when the epoch increases. First, the test accuracy of Delog-SCL improves stably. Second, the Delog-SCL outperforms BP after approximately 100 epochs. BP is better than Delog-SCL at the beginning, likely because all the parameters in BP are updated to minimize a global objective. On the contrary, most of the parameters in Delog-SCL are updated to fit local objective functions, which usually have no direct access to the target variable.\n\nDifferent hyperparameter settings may lead to slightly different curves. However, most of them follow a similar pattern. Experiments on other datasets (CIFAR-10 and tiny-ImageNet) for the VGG network structure also show similar trends.\n\nA.2 A COMPARISON OF TRAINING AND TEST TIME OF BP, AL, AND DELOG-SCL\n\nTable 7 compares Delog-SCL, BP, and AL in terms of their practical training and testing seconds per epoch on ConvNet, VGG, and ResNet using the CIFAR-10 dataset. The empirical training and testing speed of Delog-SCL and BP are extremely close. However, AL is apparently slower than the other two.\n\nThe experiments are tested on the Container Compute Service with NVIDIA Tesla V100 GPU.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Epoch vs test accuracy for BP and Delog-SCL on CIFAR-100 using vanilla ConvNet.\n\nTable 7: A comparison of the training and testing seconds (mean ± standard deviation) per epoch with different methodologies (using CIFAR-10 as an example)\n\nConvNet\n\nVGG\n\nResNet\n\nTraining\n\nTesting\n\nTraining\n\nTesting\n\nTraining\n\nTesting\n\nDelog-SCL BP AL\n\n16.62 ± 0.71 15.74 ± 0.33 22.81 ± 0.56\n\n1.58 ± 0.01 1.59 ± 0.02 1.82 ± 0.02\n\n51.63 ± 0.09 50.05 ± 0.06 71.40 ± 0.81\n\n2.67 ± 0.01 2.67 ± 0.01 3.19 ± 0.02\n\n42.14 ± 0.54 42.05 ± 0.55 56.62 ± 1.29\n\n2.34 ± 0.01 2.35 ± 0.02 2.39 ± 0.01\n\nA.3 A LARGER BATCH SIZE IMPROVES THE LEARNING QUALITY\n\nWe also tested how the batch size influences the test accuracy. As shown in Table 8, performing Delog-SCL training using a large batch size is helpful, and the improvement on VGG is more evident than in other networks. This finding is consistent with the results reported in previous studies, e.g., Chen et al. (2020); Henaff (2020); Bachman et al. (2019), in which the authors noted that because a larger batch tends to include more negative pairs (as shown in Equation 2), the model has access to more information that can be used to distinguish positive pairs from negative pairs. Although other studies, e.g., Mitrovic et al. (2020), have shown that the number of negative pairs may not be critical to the improvement of the test accuracy, most studies tend to agree that a larger batch size leads to better results.\n\nTable 8: The test accuracies of Delog-SCL when using different batch sizes on CIFAR-10.\n\nBatch Size\n\nVGG\n\nResNet\n\n32 128 1024\n\n92.67 ± 0.10 93.11 ± 0.16 93.42 ± 0.11\n\n92.54 ± 0.14 92.53 ± 0.11 92.78 ± 0.11\n\n12\n\n0100200300400500Epoch102030405060Test accuracy (%)Delog-SCLBPUnder review as a conference paper at ICLR 2023\n\nTable 9: The test accuracies when using different projection heads on CIFAR-10.\n\nType of projection head\n\nVGG ResNet\n\nIdentity Linear MLP\n\n79.2 90.4 93.0\n\n84.2 90.2 92.4\n\nA.4 A NONLINEAR PROJECTION HEAD BENEFITS THE REPRESENTATION QUALITY OF THE\n\nLAYER BEFORE IT\n\n(cid:16)\n\nThis section presents the influence of different projection heads. Table 9 compares the accuracies of VGG and ResNet on CIFAR-10 when 3 different types of projection heads are used: identity mapping (i.e., gl l + w0), and the default mapping based on a multilayer perceptron (MLP). The results are similar to those reported in (Chen et al., 2020): the MLP mapping shows a 2.6% improvement over linear projection, which outperforms identity projection by over 10%.\n\nl ), linear mapping (i.e., gl\n\n= wT r(i)\n\n= r(i)\n\nr(i)\n\nr(i)\n\n(cid:16)\n\n(cid:17)\n\n(cid:17)\n\nl\n\nl\n\nUsing an MLP as the projection head is beneficial likely because the information loss induced by the contrastive loss is more severe when a simple projection head is used (Chen et al., 2020). In particular, since a projection head gl (refer to Figure 3 and Figure 1) maximizes the agreement between augmented images, gl may remove information relevant to image rotation, flipping, and other data augmentation operations, which could be useful for downstream tasks. When a simple projection head gl is used, the information contained in r(i) l , which means that r(i) is invariant to data augmentation. On the other hand, when a complex projection head such as an MLP is used, the information in r(i) l . As a result, even if z(i) l may still preserve this information.\n\nloses information relevant to data augmentation, r(i)\n\nl may be very different from that in z(i)\n\nl will be similar to that in z(i)\n\nl\n\nl\n\nA.5 PSEUDO CODE\n\nHere we provide a PyTorch pseudocode for the creation of the local supervised contrastive losses (Algorithm 1) and Delog-SCL (Algorithm 2).\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: PyTorch-like pseudocode for Lsc import torch import torch.nn as nn class SupConLoss(nn.Module): def __init__(self, dim):\n\nsuper.__init__() self.linear =nn.Sequential(nn.Linear(dim, 512), nn.ReLU(),\n\nnn.Linear(512, 1024))\n\nself.temperature =0.1\n\ndef forward(self, x, label):\n\nx = self.linear(x) x = nn.functional.normalize(x) label =label.view(-1, 1) bsz = label.shape[0] mask =torch.eq(label, label.T).float() anchor_mask =torch.scatter(torch.ones_like(mask), 1, torch\n\n.arange(bsz).view(-1, 1), 0)\n\nlogits =torch.div(torch.mm(x, x.T), self.temperature) deno =torch.exp(logits)*anchor_mask prob =logits -torch.log(deno.sum(1, keepdim=True)) loss = - (anchor\\_mask *mask *prob).sum(1)/mask.sum() return loss.view(1, bsz).mean()\n\nAlgorithm 2: PyTorch-like pseudocode for Delog-SCL import torch import torch.nn as nn # A 3-layer vanilla ConvNet example for Delog-SCL class CNN_DelogSCL(nn.Module): def __init__(self, dim):\n\nsuper.__init__() CNNs = [ ] losses =[ ] channels =[3, 128, 256, 512] self.shape =32 for i in range(3):\n\nCNNs.append(nn.Sequential(nn.Conv2d(channels[i],\n\nchannels[i+1], padding=1), nn.ReLU())\n\nlosses.append(SupConLoss(self.shape*self.shape*channels[\n\ni+1]))\n\nself.CNN =nn.ModuleList(CNNs) self.loss =nn.ModuleList(losses) self.fc =nn.Sequential(flatten(), nn.Linear(self.shape*\n\nself.shape*channels[-1], 10)) self.ce =nn.CrossEntropyLoss()\n\ndef forward(self, x, label):\n\nloss =0 for i in range(3):\n\n# .detach() prevents the computation graph from\n\npropagating gradients to the next layer\n\nx = self.CNN[i](x.detach()) if self.training:\n\nloss +=self.loss[i](x, label)\n\ny = self.fc(x.detach()) if self.training:\n\nloss +=self.ce(y, label) return loss\n\nreturn y\n\n14",
    "reference": "# Summary Of The Paper\n\nIn this paper, the author proposed a supervised contrastive learning (SCL)-based training method for deep neural networks in which the gradients only flow locally in each layer, solving some of the problems associated with long gradient flows in backpropagation. The authors also benchmarked its performance against early exit, associated learning (AL) and backpropagation (BP) for training some standard feedforward network architectures for classification tasks. The results showed that the proposed method outperforms early exit, AL on most tasks while BP is still superior for ResNet. \n\nIn addition, the author also did some analysis on the effects of batch size on the final test performance and found larger batch size generally works better. The author also found that a nonlinear projection head performs better than identity or linear heads.\n\n# Strength And Weaknesses\n\nThis work has several strengths:\n\n1. The proposed method is able to cut the long gradient flow in training deep neural networks.\n\n2. The proposed method decouples the parameter updating across layers, making parallel training possible, though it needs follow-up work to validate this claim.\n\n3. The proposed method requires fewer parameter than similar method such as AL in training phase, and has the same effective parameter in inference phase. \n\n4. The test accuracy on simpler tasks and simpler architectures are better than AL and comparable to BP.\n\nWeaknesses\n\n1. One important aspect of learning procedures are the learning dynamics, which is not thoroughly studied. What are the learning curves like for the methods compared? How much longer or shorter does the proposed method converge compared to AL and BP? Is the training stable? Do we need to use different learning steps for each layer? How does the parameters change during training and use this to validate whether it actually addresses some problems with long gradient flows, such as the exploding/vanishing gradient problem for long gradient flows?\n\nThe research community would also benefit from some potential follow-up work such as the pipelining implementation and its possible extension to training other architectures such as RNNs.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nIn general, the paper is well written. The motivations are clearly explained. The experimental design is also sound. One novelty of this paper is that it brings supervised contrastive learning to solve the problem of long gradient flow, complementing some existing approaches such work such as target propagation and AL. The source code is not provided for reproducibility.\n\nSome minor issues and typos:\n\n1. Figure 1 notations are a little bit confusing. \n\n2. Page 4, line 7 after equation (1): $]r_1^{(i)}$.\n\n# Summary Of The Review\n\nThe authors proposed a supervised contrastive learning inspired approach to train deep neural networks without long gradient flows. The proposed method has several characteristics such as 1) it only requires local gradient computation and parameter updates 2) it enables parallel training via pipelining 3) it requires fewer additional parameters than similar methods in training phase and same number of parameters as BP in inference phase 4) the test accuracy are comparable to BP. The authors also studied the effects of batch size, non-linear projection head. However, a thorough study and comparison of the learning dynamics are missing.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSELF-ATTENTIVE RATIONALIZATION FOR GRAPH CONTRASTIVE LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGraph augmentation is the key component to reveal instance-discriminative features of a graph as its rationale in graph contrastive learning (GCL). And existing rationale-aware augmentation mechanisms in GCL frameworks roughly fall into two categories and suffer from inherent limitations: (1) non-heuristic methods with the guidance of domain knowledge to preserve salient features, which require expensive expertise and lack generality, or (2) heuristic augmentations with a cotrained auxiliary model to identify crucial substructures, which face not only the dilemma between system complexity and transformation diversity, but also the instability stemming from the co-training of two separated sub-models. Inspired by recent studies on transformers, we propose Self-attentive Rationale guided Graph Contrastive Learning (SR-GCL), which integrates rationale generator and encoder together, leverages the self-attention values in transformer module as a natural guidance to delineate semantically informative substructures from both node- and edge-wise perspectives, and contrasts on rationale-aware augmented pairs. On real-world biochemistry datasets, visualization results verify the effectiveness of self-attentive rationalization, and the performance on downstream tasks demonstrates the state-of-the-art performance of SR-GCL for graph model pre-training.\n\n1\n\nINTRODUCTION\n\nGraph augmentation is a crucial enabler for graph contrastive learning (GCL) (You et al., 2020; Qiu et al., 2020; Zhu et al., 2020). It pre-trains the model to yield instance-discriminative representations by contrasting augmented samples against each other, without hand-annotated labels. To achieve this goal, early studies (You et al., 2020; 2021; Qiu et al., 2020; Zhu et al., 2020) conduct random corruptions in topological structures (i.e., nodes and edges) or attributes to construct contrastive pairs. However, such random corruptions, especially on salient substructures, easily cause a semantic gap between two augmented views of the same anchor graph, misguiding the following contrastive optimization procedure (Wang et al., 2021; Li et al., 2022).\n\nTo mitigate this, there has been recent interest in rationale discovery (Chang et al., 2020; Suresh et al., 2021; Li et al., 2022) as graph augmentation. We systematize these studies as rationale-aware augmentations, where a rationale exhibits a graph’s instance-discriminative information from the others. The dominant paradigm often consists of two subsequent modules: the rationale discovery function and the rationale encoder, which aim at creating the rationale-aware views and yielding their representations to contrast, respectively. To find rationales, early studies turn to domain knowledge to highlight the salient parts of graphs (Zhu et al., 2021; Liu et al., 2022). For instance, Rong et al. (2020) leverage RDkit (Landrum, 2010), an assistant software of chemistry, to capture crucial functional groups with high activity in molecule graphs. However, such expertise is expensive or even inaccessible in some scenarios (Tang et al., 2014). Besides, bringing in too much prior knowledge might harm generalization (Wang et al., 2022). To mitigate this problem, recent efforts (Suresh et al., 2021; Li et al., 2022) introduce an auxiliary model instead to automatically identify rationales, which is named the rationale generator and co-train with the rationale encoder. In this ad-hoc scheme, however, we reveal two inherent limitations:\n\n• Typically, the generator is tailor-made for one single transformation of graph data (Suresh et al., 2021; Li et al., 2022), forcing the focus on either node- or edge-wise rationales (e.g., Figures 1(b)\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Original graph\n\n(b) Node-wise views (c) Edge-wise views Figure 1: Rationale-aware graph augmentation preserves instance-discriminative features in original graph (e.g., (a)). Existing frameworks tailor-make an auxiliary model for one single transformation (e.g., (b) or (c)). SR-GCL constructs both node- and edge-wise rationale-aware views (e.g., (d)).\n\n(d) Dual views\n\n(a) Separate generator-encoder scheme\n\n(b) Integrated scheme\n\nFigure 2: Separate generator-encoder in (a) may lead to instability and biased representation, while SR-GCL integrates them together in (b) to pursue high-quality rationales and representations.\n\nand 1(c)). The lack of view diversity confines the rationale-aware augmentations to one transformation, while leaving the cross-transformation untouched. Worse still, it might degenerate the effectiveness of contrastive learning, as the studies (Chen et al., 2020; You et al., 2020) empirically show that “no single transformation suffices to learn good representations”. Here we ascribe this crux of generator to the lack of transformation diversity, and argue that a high-performing generator is supposed to be equipped with perspectives of both node and edge (e.g., Figure 1(d)).\n\n• As illustrated in Figure 2(a), the generator, aiming at discovering rationales, separates from the subsequent encoder, which specializes in encoding them. While conceptually appealing, we hypothesize that these separate modules cooperate with each other to pursue high-quality rationales unsmoothly. Because the supervision signal for the generator is remotely generated by the contrastive optimization of the encoder, much of which is weak. Moreover, co-optimizing two submodels could make the pre-training more complicated and time-consuming but less stable.\n\nTo resolve these limitations, we draw inspiration from the transformers (Vaswani et al., 2017) to reshape the generator-encoder scheme. Despite originally being proposed for language (Devlin et al., 2019) and vision tasks (Dosovitskiy et al., 2021), transformers are attracting a surge of interest in graph area (Wu et al., 2021; Chen et al., 2022; Ramp ́asek et al., 2022). At the core is the self-attention operation, which models pairwise connections between tokens and yields high-quality representations. We find self-attention de facto a natural mechanism to concurrently discover and condense rationale information from both edge- and node-wise transformations. By prepending a special token as its proxy and treating its nodes as other tokens, self-attention is able to elegantly indicate the importance of each node and each edge (See Section 2.2 and Figure 4). Sampling nodes and edges based on the importance scores (i.e., heterogeneous transformations) allows us to generate both the node- and edge-wise subgraphs (i.e., rationales) simultaneously. Moreover, self-attention can directly output the rationale representations without additional modules. In stark contrast to the prior generator-encoder scheme, the “self-attentive rationalization” not only accomplishes diverse rationales in one shot, but also integrates the functions of rationale discovery and encoding together.\n\nWith the Self-attentive Rationalization as graph augmentation, we incorporate it into GCL and name the framework SR-GCL. Specifically, two augmented views stem from the node- and edge-wise rationales, respectively; subsequently, the contrastive optimization pulls close representations of contrastive pair augmented from the same anchor graph and pushes away those of different anchors by minimizing contrastive loss. Compared with conventional GCL methods, our SR-GCL collates and conflates the instance-discriminative information across both node- and edge-wise transformations to construct rationale-aware contrastive pairs from dual perpectives. Henceforth, we find that such strategy improves the generalization performance of pre-trained model on downstream tasks, while simultaneously interpreting the contribution of each node/edge to instance-discrimination. Extensive experiments show that SR-GCL sets the new state-of-the-art for graph pre-training across a number of biochemical molecule and social network benchmark datasets in (Wu et al., 2018a; Morris et al., 2020). Codes are available at https://anonymous.4open.science/r/SR-GCL-EDD3.\n\n2\n\nGeneratorRationaleEncoderRationaleIntegratedModelUnder review as a conference paper at ICLR 2023\n\nFigure 3: SR-GCL framework, which constructs node- and edge-wise rationales (i.e., Rv(g)/Re(g)) by sampling from corresponding probability distributions (i.e., P G)). Then contrastive optimization is performed to encourage the agreement between views of the same anchor.\n\nG)/P\n\n·|\n\n·|\n\n(\n\n(\n\nV\n\nE\n\n2 METHODOLOGY\n\nScrutinizing leading GCL methods (You et al., 2020; 2021; Qiu et al., 2020; Zhu et al., 2020; Suresh et al., 2021; Li et al., 2022), we can summarize the dominant scheme as a combination of two subsequent modules: graph augmentation and contrastive optimization. During the pre-training phase, the cooperation of these modules enables the backbone encoder to learn how to discover and parameterize the instance-discriminative information for graph instances. Hereafter, the pre-trained encoder is fine-tuned on downstream tasks. See Appendix A.1 for the literature review of GCL.\n\nPreserving instance-discriminative semantics when augmenting graphs has been founded critical in recent studies (Suresh et al., 2021; Li et al., 2022). Here we systemize an augmented view holding such salient semantics as a rationale (Chang et al., 2020; Li et al., 2022), which can help discriminate a graph instance from the other instances. However, these studies focus mainly on discovering homogeneous rationales from a single perspective of either node or edge, thereby easily resulting in sub-optimal performance (Chen et al., 2020; You et al., 2020). To explore more effective and diverse rationales, we propose a novel transformer-based pre-training framework, SR-GCL. As Figure 3 shows, it uses the graph transformer as the backbone encoder, whose inherent self-attention map allows us to discover heterogeneous rationales from dual perspectives of both node and edge; subsequently, upon the node- and edge-wise rationales, it conducts the contrastive optimization to pre-train the backbone. Appendix B presents the detailed algorithm of SR-GCL.\n\n2.1 BACKBONE MODEL: GRAPH TRANSFORMER\n\nWe begin by describing our backbone model, which is the cornerstone of SR-GCL. Distinguishing from the current GCL studies that employ “vanilla” graph neural networks (GNNs) (Thomas et al., 2022) as the backbone model being pre-trained and fine-tuned, we use the graph transformer instead (Wu et al., 2021; Ramp ́asek et al., 2022; Chen et al., 2022). Specifically, it adopts the transformer architecture (Vaswani et al., 2017; Dosovitskiy et al., 2021) to process the structural input of graph (See Appendix A.2 for the literature review). It consists of (1) a GNN component, which creates node representations of a graph by propagating information between the locally adjacent nodes and then converts them into tokens; and (2) a transformer component, which refines the token representations by applying the self-attention mechanism to model the global connections between any two nodes. Next, we will elaborate on the graph transformer backbone.\n\nGNN Component: Formally, let g = ( are its node and edge sets, respectively. Taking g as input, this component first leverages a vanilla GNN f1 to yield d: the node representations X\n\nd, and then converts them into a series of tokens Z\n\n) be a graph instance, where\n\nand\n\nR(\n\n+1)\n\nV\n\nV\n\nE\n\nE\n\n,\n\nR|V|×\n\n|V|\n\n×\n\n∈\n\n∈\n\nX = [x1, x2,\n\n, x\n\n|V|\n\n· · ·\n\n] = f1(g), Z = [x1, x2,\n\n, x\n\n|V|\n\n, xCLS],\n\n· · ·\n\n(1)\n\n3\n\nSamplefromPVSamplefromPEg=(V,E)AnchorGraphp(v|g)Node-wiseProbabilityp(e|g)Edge-wiseProbabilityRv(g)Node-wiseRationaleRe(g)Edge-wiseRationalef1(·)GNNCLSCNOClCLSCNOClf2(·)TransformerAttentionScoresContrastiveOptimizationUnder review as a conference paper at ICLR 2023\n\nwhere xv ∈\n\nRd is the d-dimensional representation of node v\n\n, and all node representations are viewed as a sequence. It is worth highlighting that we append a special trainable token to this CLS ⟨\nsequence, which serves as the graph-level representation for contrastive optimization in pre-training and classification in downstream fine-tuning. That is, can be viewed as a virtual supernode connected with all nodes, and expands\n\nCLS ⟩\nCLS\n\n∈ V\n\nto\n\n⟨\n\n⟩\n\n∗ =\n\nV\n\nV\n\nV ∪ {⟨\n\n. ⟩}\n\nTransformer Component: On the tokens, we build a transformer f2 composed of L layers:\n\nZL = [zL\n\n1 , zL 2 ,\n\n, zL\n\n|V|\n\n· · ·\n\n, zL\n\nCLS] = f2(Z),\n\n(2)\n\nR(\n\n×\n\n∈\n\n|V|\n\n+1)\n\nwhere ZL d collects the d-dimensional representations of all tokens. Now, we dive into the core of f2 — the self-attention mechanism that establishes the pairwise interactions across all tokens so as to enrich representations with the global information. Formally, for the pair of tokens v and u, the attention score al\n\nv,u in the l-th transformer layer can be obtained as follows:\n\nal\n\nv,u =\n\n(cid:80)\n\nw\n\nκ(zl −\nv\n\n1\n\n, zl\n\n1\n\n− 1\n\nu ) , zl\n\n∗ κ(zl\n\n− v\n\n1 w ) −\n\n∈V\n\n,\n\nκ(zl\n\n− v\n\n1\n\n, zl\n\nu ) := exp\n\n−\n\n1\n\n(cid:10)Wl\n\nQzl\n\n− v\n\n1\n\n, Wl √d\n\nKzl\n\n− u\n\n(cid:11)\n\n1\n\n,\n\n(3)\n\n1\n\nis the representation of token v after (l\n\nwhere zl −\nv step; κ( ,\n· are the trainable query/key projection matrices. Attending to all possible tokens, the representation of token v at the l-th layer can be updated as follows:\n\n) is the exponential kernel function with the dot product operation ·\n\nv is set as zv in Z at the initial\n\n1) layers, and z0\n\nQ/Wl\n\nK ∈\n\n; Wl\n\n, ⟨·\n\nRd\n\n−\n\n·⟩\n\n×\n\nd\n\nzl v =\n\n(cid:88)\n\n∗\n\nu\n\n∈V\n\nv,uWl al\n\nV zl\n\nu ,\n\n−\n\n1\n\n(4)\n\nv,u indicates the contribution of token u\n\nwhere al projection matrix. After L layers, the representations are recursively updated to ZL in Equation 2.\n\nd is the trainable value\n\n∗ to token v; Wl\n\nV ∈\n\n∈ V\n\n×\n\nRd\n\nWhy Graph Transformer? Most vanilla GNN-based backbones are responsible solely to the representation learning, while leaving the graph augmentation step to an additional non-parametric (e.g., corrupt graphs randomly or based on domain knowledge) or parametric (e.g., generate rationales via an attention or masking network) module. Here we focus on the line of parametric rationale-aware augmentation. Clearly, this augmentation module is disjoint from these conventional backbones. Concretely, graph transformer backbone allows us to integrate the representation learning and rationale generation together. Next, we will introduce a simple yet effective rationalization strategy based on the self-attention mechanism of the transformer.\n\n2.2 GRAPH AUGMENTATION: SELF-ATTENTIVE RATIONALIZATION\n\nWe now present our augmentation strategy — self-attentive rationalization, which exploits the selfattention mechanism to generate rationales from two perspectives of node and edge in one shot.\n\n2.2.1 SELF-ATTENTION SCORES FOR RATIONALIZATION\n\nFor a graph instance, we reshape the self-attention values in the first transformer layer into a 2-D heat map as illustrated in Figure 4, which enables us to estimate the instancediscriminative power of each node and edge — that is, the influence of a certain node or edge on discriminating the graph instance from all the others. Specifically, for node v , its influence score pv is derived from the normalized attention score between it and the token; meanwhile, for edge , the pairwise interaction between endpoints v e = (v, u) and u naturally depicts its importance pe:\n\nCLS ⟩\n⟨\n\n∈ V\n\n∈ E\n\nκ(zCLS, zv)\n\npv =\n\n(cid:80)\n\nv′\n\nκ(zCLS, zv′)\n\n,\n\npe =\n\n(cid:80)\n\ne′\n\nκ(zv, zu)\n\nκ(zv′, zu′)\n\n,\n\n(5)\n\n∈V where κ( ) is the exponential kernel function defined in ·\nEquation 3; κ(zCLS, zv) evaluates node v’s attribution to the\n\n∈E\n\n·\n\n,\n\nFigure 4: Self-attention map.\n\n4\n\n210987654311113121413Under review as a conference paper at ICLR 2023\n\ngraph-level representation, i.e., the token; κ(zv, zu) reflects the volume of information propagated on edge e = (v, u), which is further normalized over all edges e′ = (v′, u′) . With the contributions of each node and edge to instance-discrimination in hand, we can further conduct probabilistic sampling to generate both node- and edge-wise rationales for contrastive optimization.\n\nCLS ⟩\n\n∈ E\n\n⟨\n\n2.2.2 NODE- & EDGE-WISE RATIONALE GENERATION\n\nWe first approximate the distribution of rationale-aware views, and then conduct probabilistic sampling to maintain the diversity of views. With a slight abuse of notation, uppercase G and R(G) separately represent a random variable of graph and its rationale, while lowercase g and R(g) are their samples correspondingly. Conditional probabilistic function and conditional probabilistic distribution are denoted by p(\n\n), respectively.\n\n) and P (\n\n·|·\n\n·|·\n\nDistribution of Node- & Edge-wise Rationales. Given a graph g, the probability distribution of its node-wise rationale-aware view P1 is approximated as:\n\nG = g) = P1(R(G) = R(g) |\n\n(cid:89)\n\nv\n\nR\n\n∈V\n\np(v\n\ng)\n\n|\n\n(cid:89)\n\nv\n\nR\n\n∈V\\V\n\n(1\n\np(v\n\n|\n\n−\n\ng)),\n\n(6)\n\nand\n\ng) denotes the probawhere |\nbility of v being included into R(g), which quantizes how semantically informative it is. Similarly, the edge-wise probability distribution P2 can be formulated as:\n\nVR are the node sets of g and rationale R(g), respectively; p(v\n\nV\n\nG = g) = P2(R(G) = R(g) |\n\n(cid:89)\n\ne\n\nR\n\n∈E\n\np(e\n\ng)\n\n|\n\n(cid:89)\n\ne\n\nR\n\n∈E\\E\n\n(1\n\np(e |\n\n−\n\ng)),\n\n(7)\n\nwhere power of edge e. And p(v\n\nER are the edge sets of g and R(g), respectively; p(e\n\ng) (or p(e\n\nand\n\nE\n\ng)) is parameterized by pv (or pe) in Equation 5. |\n\ng) reflects the discriminative |\n\nSampling with Fixed Ratios. To construct anchor graph g’s node-wise rationale-aware view, we G = g) under the constraint of a fixed node sampling ratio ρv: sample from P1(\n\n|\n\nR1(g)\n\nP1(\n\nG = g)\n\ns.t.\n\n∼\n\n·|\n\n|VR|\n\n= ρv · |V|\n\n.\n\n(8)\n\nAfter obtaining sampled nodes, the edges between them are kept to construct a complete subgraph. Analogically, edge-wise rationale-aware view can be generated with another edge sampling ratio ρe: (9)\n\nG = g)\n\ns.t.\n\n·| where the endpoints of sampled edges are preserved, while the others are discarded.\n\n= ρe · |E|\n\nR2(g)\n\n|ER|\n\nP2(\n\n∼\n\n,\n\n·|\n\nWithout involving any auxiliary models, two rationale-aware views with different transformations (i.e., node- and edge-wise) are generated under the guidance of self-attention rationalization.\n\n2.3 RATIONALE REPRESENTATION LEARNING AND CONTRASTIVE OPTIMIZATION\n\nWith the rationale-aware views (i.e., R1(g)/R2(g)) generated by the the self-attentive rationalization process in hand, now we follow the pipeline of representation learning and contrastive optimization to empower the backbone graph transformer model with instance-discriminative ability.\n\n2.3.1 RATIONALE REPRESENTATION LEARNING\n\nAs illustrated in Figure 3, the GNN f1 and transformer f2 sequentially process the rationale-aware views as in Equation 1 and Equation 2 to yield their representations:\n\nZL 1 = f2(f1(R1(g))), 1 and ZL tokens in ZL\n\nZL\n\n2 = f2(f1(R2(g))).\n\n(10)\n\nCLS ⟩\n⟨\n\n2,CLS separately, Subsequently, the are picked out as the graph-level representations of node- and edge-wise views (i.e., R1(g)/R2(g)). Then we follow the commonly-adopted projection operation to project them into a latent space through a MLP-based projection head h with l2 norm:\n\n2 , which we denote as zL\n\n1,CLS and zL\n\nr1 = h(zL\n\n1,CLS),\n\nr2 = h(zL\n\n2,CLS).\n\n(11)\n\nThe transformer module plays the roles of both the rationale generator, whose attention map guides rationale-aware augmentations of both node- and edge-wise views, and the encoder, who generates graph representations for subsequent contrastive optimization.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\nFigure 5: (a): A molecule graph. (b): Random attention. (c): Local attention of adjacency matrix. (d): Global attention. (e): GraphBigBird, where white entry indicates absence of attention.\n\n2.3.2 CONTRASTIVE OPTIMIZATION\n\nThe Info-NCE loss (van den Oord et al., 2018) is adopted to maximize the mutual information between node- and edge-wise rationale-aware views of the same anchor graph:\n\nmin\n\nf1,f2,h L\n\n= Eg\n\n∈G\n\nl(g),\n\nl(g) =\n\nlog\n\n−\n\nexp (cid:0)r1⊤r2/τ (cid:1)\n\nexp (r1⊤r2/τ ) + (cid:80)\n\nr′\n\n′ exp (r1⊤r′/τ )\n\n∈R\n\n,\n\n(12)\n\nwhere τ denotes the temperature parameter, r1 and r2 are the rationale-aware views of the same anchor graph (i.e., positive views derived from Equation 11), and ′ summarizes the representations of the other graphs’ views in the minibatch (i.e., negative views).\n\nR\n\nThe supervision signal guides the backbone model (i.e., both GNN and transformer module) to learn instance-discriminative representations, which indicates that the self-attention mechanism in transformer is enhanced concurrently, thus bring about more accurate self-attentive rationalization.\n\nFinally, after pre-training, the projection head h will be thrown away. And it’s worth mentioning that SR-GCL can be easily adapted to any transformer-based graph model with self-attention mechanism.\n\n2.4 DISCUSSION ON TRANSFORMER COMPLEXITY: GRAPHBIGBIRD\n\nCompared with conventional GNN model with a time and space complexity of O(N ) w.r.t. the number of nodes, the graph transformer with self-attention mechanism is empowered with powerful representation ability but a quadratic complexity w.r.t. the length of token sequence (i.e., the number of nodes) as well. As evaluated in (Wu et al., 2021; Ramp ́asek et al., 2022), such quadratic complexity just adds acceptable computation overhead when processing graphs with only dozens of nodes. Nevertheless, in social network datasets, graph instances could hold thousands of nodes. Thus, the graph transformer, especially the self-attention mechanism would not scale well to such large-scale graphs, due to the quadratic complexity of pairwise attention. To mitigate this, we get inspiration from the sparse attention mechanism of BigBird (Zaheer et al., 2020) in the NLP area and propose GraphBigBird tailor-made for the graph transformer.\n\nGraphBigBird. As illustrated in Figure 5, GraphBigBird contains three parts: (1) random attention with a hyperpatameter r controlling the sparsity, (2) local attention corresponding to the adjacency matrix of the graph sample, and (3) global attention connecting each token with the token. The major difference between BigBird and GraphBigBird lies on the local attention part. Concretely, for a sequence of NLP tokens, BigBird adopts a sliding window to capture the proximity between a token and its neighboring tokens. GraphBigBird adapts this idea to graph area by replacing the sliding window with the adjacency matrix, as Figure 5(c) shows. This simple strategy is effective to depict the local connectivity of graph.\n\nCLS ⟩\n\n⟨\n\nWith GraphBigBird, SR-GCL, requiring the graph transformer to be the backbone, is able to break the graph scale limitation of dozens of nodes and generalize to domains containing large-scale graph data, to demonstrate its universal effectiveness.\n\n3 EXPERIMENTS\n\nIn this section, extensive experiments are conducted on biochemical molecule and social network datasets with transfer and unsupervised learning settings to demonstrate the state-of-the-art per-\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Transfer learning. Test ROC-AUC scores (%) of pre-trained models on downstream datasets. G represents the commonly used GIN model in previous baselines, while T is the graph transformer model containing both GNN and transformer module depicted in this paper. Statistics of G models are from their original papers and those of T models are obtained by replanting the graph transformer model into their official released codes. red and blue indicate the best and the second best performance on each dataset, respectively.\n\nDataset\n\nBBBP\n\nTox21\n\nToxCast\n\nSIDER\n\nClinTox\n\nMUV\n\nHIV\n\nBACE\n\nAVG. GAIN\n\nNo Pre-Train\n\nInfomax\n\nEdgePred\n\nAttrMasking\n\nContextPred\n\nGraphCL\n\nGraphLoG\n\nAD-GCL\n\nJOAO\n\nRGCL\n\nG 65.8 T\n\n68.12\n\nG 68.8 T\n\n69.15\n\nG 67.3 T\n\n69.32\n\nG 64.3 T\n\n66.42\n\nG 68.0 T\n\n68.52\n\nG 69.68 70.41 T\n\nG 72.5 T\n\n67.59\n\nG 70.01 67.88 T\n\nG 70.22 70.78 T\n\nG 71.42 71.36 T\n\nSR-GCL edge1 T SR-GCL node1 T SR-GCL dual1 T\n\n71.62 73.01 72.60\n\n4.5\n\n±\n\n2.94\n\n±\n\n0.8\n\n±\n\n0.68\n\n±\n\n2.4\n\n±\n\n1.24\n\n±\n\n2.8\n\n±\n\n3.16\n\n±\n\n2.0\n\n±\n\n1.12\n\n±\n\n0.67\n\n±\n\n1.06\n\n±\n\n0.8\n\n±\n\n1.58\n\n±\n\n1.07\n\n±\n\n0.97\n\n±\n\n0.98\n\n±\n\n0.62\n\n±\n\n0.66\n\n±\n\n0.85\n\n±\n\n1.87\n\n±\n\n0.94\n\n±\n\n0.89\n\n±\n\n74.0 73.48\n\n75.3 74.62\n\n76.0 76.78\n\n76.7 77.22\n\n75.7 75.29\n\n73.87 73.82\n\n75.7 75.95\n\n76.54 72.97\n\n74.98 75.25\n\n75.20 75.72\n\n74.98 75.12 77.30\n\n0.8\n\n±\n\n0.32\n\n±\n\n0.5\n\n±\n\n0.45\n\n±\n\n0.6\n\n±\n\n0.49\n\n±\n\n0.4\n\n±\n\n0.67\n\n±\n\n0.7\n\n±\n\n0.56\n\n±\n\n0.66\n\n±\n\n1.04\n\n±\n\n0.5\n\n±\n\n0.80\n\n±\n\n0.82\n\n±\n\n1.19\n\n±\n\n0.29\n\n±\n\n1.46\n\n±\n\n0.34\n\n±\n\n0.40\n\n±\n\n0.96\n\n±\n\n1.68\n\n±\n\n0.84\n\n±\n\n63.4 62.67\n\n62.7 61.85\n\n64.1 64.43\n\n64.2 64.89\n\n63.9 63.23\n\n62.40 63.06\n\n63.5 63.63\n\n63.07 63.19\n\n62.94 63.79\n\n63.33 63.93\n\n64.87 65.69 65.86\n\n0.6\n\n±\n\n0.56\n\n±\n\n0.4\n\n±\n\n0.75\n\n±\n\n0.6\n\n±\n\n0.72\n\n±\n\n0.5\n\n±\n\n0.89\n\n±\n\n0.6\n\n±\n\n0.46\n\n±\n\n0.57\n\n±\n\n0.41\n\n±\n\n0.7\n\n±\n\n0.57\n\n±\n\n0.72\n\n±\n\n0.44\n\n±\n\n0.48\n\n±\n\n0.84\n\n±\n\n0.17\n\n±\n\n0.36\n\n±\n\n0.65\n\n±\n\n1.02\n\n±\n\n0.61\n\n±\n\n57.3 59.44\n\n58.4 60.14\n\n60.4 60.15\n\n61.0 60.88\n\n60.9 61.29\n\n60.53 60.38\n\n61.2 59.85\n\n63.28 60.18\n\n59.97 61.04\n\n61.38 60.91\n\n61.32 61.21 61.46\n\n1.6\n\n±\n\n1.26\n\n±\n\n0.8\n\n±\n\n0.72\n\n±\n\n0.7\n\n±\n\n0.62\n\n±\n\n0.7\n\n±\n\n0.45\n\n±\n\n0.6\n\n±\n\n0.75\n\n±\n\n0.88\n\n±\n\n1.28\n\n±\n\n1.1\n\n±\n\n2.11\n\n±\n\n0.79\n\n±\n\n0.95\n\n±\n\n0.79\n\n±\n\n0.87\n\n±\n\n0.61\n\n±\n\n0.56\n\n±\n\n1.59\n\n±\n\n0.93\n\n±\n\n1.36\n\n±\n\n58.0 69.45\n\n69.9 74.78\n\n64.1 73.61\n\n71.8 77.28\n\n65.9 74.90\n\n75.99 77.79\n\n76.7 79.10\n\n79.78 78.83\n\n81.32 78.68\n\n83.38 80.02\n\n80.29 80.47 81.84\n\n4.4\n\n±\n\n3.12\n\n±\n\n3.0\n\n±\n\n2.66\n\n±\n\n3.7\n\n±\n\n2.49\n\n±\n\n4.1\n\n±\n\n3.88\n\n±\n\n3.8\n\n±\n\n3.18\n\n±\n\n2.65\n\n±\n\n3.02\n\n±\n\n3.3\n\n±\n\n3.24\n\n±\n\n3.52\n\n±\n\n1.60\n\n±\n\n2.49\n\n±\n\n2.97\n\n±\n\n0.91\n\n±\n\n1.59\n\n±\n\n2.68\n\n±\n\n3.55\n\n±\n\n1.60\n\n±\n\n71.8 72.18\n\n75.3 74.83\n\n74.1 72.41\n\n74.7 74.12\n\n75.8 70.92\n\n69.80 73.81\n\n76.0 72.84\n\n72.30 75.62\n\n71.66 75.67\n\n76.66 75.94\n\n75.11 77.92 77.76\n\n2.5\n\n±\n\n1.16\n\n±\n\n2.5\n\n±\n\n1.52\n\n±\n\n2.1\n\n±\n\n1.26\n\n±\n\n1.4\n\n±\n\n1.98\n\n±\n\n1.7\n\n±\n\n2.36\n\n±\n\n2.66\n\n±\n\n2.02\n\n±\n\n1.1\n\n±\n\n1.79\n\n±\n\n1.61\n\n±\n\n2.54\n\n±\n\n1.43\n\n±\n\n2.57\n\n±\n\n0.99\n\n±\n\n1.19\n\n±\n\n1.63\n\n±\n\n3.12\n\n±\n\n1.18\n\n±\n\n75.3 74.41\n\n76.0 74.95\n\n76.3 75.87\n\n77.2 77.23\n\n77.3 76.82\n\n78.47 75.62\n\n77.8 72.54\n\n78.28 75.54\n\n76.73 76.98\n\n77.90 77.81\n\n77.64 77.89 78.52\n\n1.9\n\n1.45\n\n0.7\n\n1.26\n\n1.0\n\n0.82\n\n1.1\n\n1.51\n\n1.0\n\n1.13\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n1.22\n\n±\n\n0.89\n\n0.8\n\n1.60\n\n0.97\n\n1.29\n\n1.23\n\n1.16\n\n0.80\n\n0.62\n\n1.68\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n0.68\n\n±\n\n0.74\n\n±\n\n70.1 74.23\n\n75.9 76.32\n\n79.6 78.46\n\n79.3 79.56\n\n79.6 80.16\n\n75.38 78.28\n\n83.5 83.55\n\n78.51 73.37\n\n77.34 77.49\n\n76.03 79.92\n\n79.69 81.33 81.58\n\n5.4\n\n4.62\n\n1.6\n\n1.78\n\n1.2\n\n1.87\n\n1.6\n\n2.13\n\n1.2\n\n1.84\n\n1.44\n\n1.08\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n1.2\n\n±\n\n1.96\n\n±\n\n0.80\n\n1.55\n\n0.48\n\n1.23\n\n0.77\n\n1.02\n\n0.87\n\n0.92\n\n0.97\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n67.0 68.24\n\n70.3 70.83\n\n70.3 71.37\n\n71.1 72.20\n\n70.9 71.39\n\n70.77 71.64\n\n73.4 71.88\n\n72.72 70.95\n\n71.90 72.46\n\n73.16 73.20\n\n73.19 74.08 74.62\n\n1 SR-GCL with edge-wise perspective, node-wise perspective and dual perspectives of both node and edge.\n\n- 1.24\n\n3.3 3.83\n\n3.3 4.37\n\n4.1 5.20\n\n3.9 4.39\n\n3.77 4.64\n\n6.4 4.88\n\n5.72 3.95\n\n4.90 5.46\n\n6.16 6.20\n\n6.19 7.08 7.62\n\nformance of SR-GCL. Also, we visualize the rationales to illustrate the effectiveness of the selfattentive rationalization mechanism. Finally, due to the limitation of space, the analysis for hyperparameter sensitivity is provided in Appendix G.\n\n3.1 TRANSFER LEARNING\n\nSetup and baselines. Following the commonly adopted transfer learning settings and evaluation metrics in (Hu et al., 2020), the backbone model is pre-trained on large scale label-free dataset – ZINC-2M (Sterling & Irwin, 2015) and fine-tuned on downstream datasets with graph-level classification tasks in MoleculeNet (Wu et al., 2018a) to evaluate the transferability of schemes. SR-GCL is compared with competitive graph pre-training baselines, including Infomax (Velickovic et al., 2019) , EdgePred (Hamilton et al., 2017), AttrMasking Hu et al. (2020), ContextPred (Hu et al., 2020), GraphCL (You et al., 2020), GraphLoG (Xu et al., 2021), AD-GCL (Suresh et al., 2021), JOAO (You et al., 2021) and RGCL (Li et al., 2022) The statistics of datasets and briefs of baselines are presented in Appendix C and D, respectively.\n\nPerformance on downstream tasks. An MLP-based classifier is appended after the pre-trained backbone model and they are subsequently fine-tuned together on downstream graph classification tasks. In Table 1, we present all statistics from their original papers with the backbone model of GIN (Xu et al., 2019), which is denoted as G. To make a fair comparison, we also re-run all baselines with the same graph transformer model depicted in this paper, which we denote as T. With one pre-trained model, on each downstream dataset, we run fine-tuning procedures 10 times with random seeds from 0 to 9 to collect the test ROC-AUC of the epoch with the highest validation score, the mean and standard deviation statistics of which are showcased. Noted that the sparse attention mechanism – GraphBigBird is not adopted in this transfer learning experiment since molecule graphs only contain dozens of nodes. The detailed model structures and the computational efficiency comparison of GIN\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Unsupervised learning. Test accuracies (%) on multiple biochemical and social network datasets. Statistics are from their original papers except GraphMAE. red and blue indicate the best and the second best performance on each dataset, respectively.\n\nDataset\n\nNCI1\n\nPROTEINS\n\nDD1\n\nMUTAG\n\nCOLLAB\n\nRDT-B1\n\nRDT-M5K1\n\nIMDB-B\n\nAVG. GAIN\n\nNo Pre-Train graph2vec InfoGraph GraphCL JOAO AD-GCL RGCL GraphMAE2 SR-GCL\n\n65.40 73.22 76.20 77.87 78.36 75.86 78.14 76.12 77.76\n\n0.17\n\n1.81\n\n1.06\n\n0.41\n\n0.53\n\n0.62\n\n1.08\n\n1.45\n\n0.21\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n72.73 73.30 74.44 74.39 74.07 75.04 75.03 74.69 75.38\n\n0.51\n\n±\n\n2.05\n\n±\n\n0.31\n\n±\n\n0.45\n\n±\n\n1.10\n\n±\n\n0.48\n\n±\n\n0.43\n\n±\n\n0.74\n\n±\n\n0.46\n\n±\n\n75.67 -\n72.85 78.62 77.40 75.73 78.86 77.15 78.70\n\n0.29\n\n±\n\n1.78\n\n±\n\n0.40\n\n±\n\n1.15\n\n±\n\n0.51\n\n±\n\n0.48\n\n±\n\n1.17\n\n±\n\n0.79\n\n±\n\n87.39 83.15 89.01 86.80 87.67 88.62 87.66 87.21 87.95\n\n1.09\n\n9.25\n\n1.13\n\n1.34\n\n0.79\n\n1.27\n\n1.01\n\n0.88\n\n1.88\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n65.29 -\n70.05 71.36 69.33 74.89 70.92 73.16 76.62\n\n0.16\n\n±\n\n1.13\n\n±\n\n1.15\n\n±\n\n0.34\n\n±\n\n0.90\n\n±\n\n0.65\n\n±\n\n2.48\n\n±\n\n0.50\n\n±\n\n76.86 75.28 82.50 89.53 86.42 92.35 90.34 87.76 91.12\n\n0.25\n\n1.03\n\n1.42\n\n0.74\n\n1.45\n\n0.42\n\n0.58\n\n1.08\n\n0.44\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n48.48 47.86 53.46 55.99 56.03 56.24 56.38 54.98 56.71\n\n0.28\n\n±\n\n0.26\n\n±\n\n1.03\n\n±\n\n0.28\n\n±\n\n0.27\n\n±\n\n0.39\n\n±\n\n0.40\n\n±\n\n1.27\n\n±\n\n0.62\n\n±\n\n69.37 71.10 73.03 71.14 70.83 71.49 71.85 71.61 72.06\n\n0.37\n\n0.54\n\n0.87\n\n0.44\n\n0.25\n\n0.98\n\n0.84\n\n0.52\n\n0.74\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n±\n\n70.15 -\n74.02 75.71 75.01 76.28 76.15 75.33 77.03\n\n- -\n3.87 5.56 4.86 6.13 6.00 5.18 7.03\n\n1 GraphBigBird is adopted. 2 The evaluation metrics and data pre-processing are different from the other baselines, thus it is reproduced with the same settings as others.\n\nand graph transformer are included in Appendix E and F. The overall results are summarized in Table 1, and the following observations can be obtained:\n\n1. SR-GCL outperforms existing pre-training baselines. On 8 downstream datasets, SR-GCL empowers the backbone graph transformer model with an average ROC-AUC score of 74.62% and performance gain of 6.38% over the same graph transformer model without pre-training. And compared with other competitive pre-training schemes, it achieves the best performance on 5 out of 8 datasets and 3 second bests – the two leading positions on BACE both belong to GraphLoG, thus SR-GCL can be seen as the second best pre-training scheme on this dataset. Such clear performance margin between SR-GCL and previous baselines strongly validates its effectiveness as a framework of self-supervised learning benefiting backbone’s transferability.\n\n2. Benefits of rationale-aware augmentations. Among multiple variants of GCL framework, GraphCL can be viewed as the vanilla one with random augmentations. With the shared motivation of preserving instance-discriminative substructures – rationales in anchor graphs, ADGCL, RGCL and SR-GCL are proposed, but with different rationale discovery mechanisms. The consistent performance gain of them over GraphCL empirically demonstrates the crucial role of augmentation mechanism in revealing the rationales in anchor graphs.\n\n3. Effectiveness of the combined design of rationale generator and encoder. RGCL leverages an external auxiliary model as the rationale generator to reveal semantically important nodes for graph augmentation. With the same graph transformer (T) as the backbone, the difference between RGCL(T) and SR-GCL with only node-wise perspective lies on the rationale generator. Compared with the detached design in RGCL, the self-attentive rationalization mechanism enables SR-GCL to combine the rationale generator and encoder. As shown in Table 1, the performance gain of SR-GCL with only node-wise perspective, which is 74.08%, over RGCL(T), which is 73.20%, verifies our claim that such combination benefits the model.\n\n4. Advantage of diverse perspectives. Despite of the rationale-aware augmentations, SR-GCL with only node-/edge-wise perspective has limited diversity of augmented views. The complete SR-GCL with dual perspectives outperforms them, thus verifying the advantage, which is also consistent with the observations in (Chen et al., 2020; You et al., 2020).\n\nSummary. The experiments of transfer learning demonstrate the promising performance of SRGCL as a self-supervised GCL scheme. Concretely, the comparisons among GCL variants (e.g., GraphCL, AD-GCL and RGCL) and ablated structures of SR-GCL (e.g., SR-GCL with node- /edge-wise perspective and dual perspectives) verify the effectiveness of our major contributions: (1) combining the rationale generator and encoder and (2) collating and conflating the instancediscriminative information across node- and edge-wise transformations.\n\n3.2 UNSUPERVISED LEARNING\n\nWe further evaluate SR-GCL in the unsupervised learning settings (Sun et al., 2020), which cover both biochemical and social network datasets from TU datasets (Morris et al., 2020). Several leading\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 6: Self-attentive rationalization. (a): A molecule graph. (b): Corresponding attention map. (c): Node- (top) and edge-wise (bottom) attention, where darker color indicates higher score. (d): The contribution of each atom to Gasteiger partial charge (top) and partition coefficient (bottom).\n\npre-traing schemes are selected as the baselines, including graph2vec (Narayanan et al., 2017), InfoGraph (Sun et al., 2020), GraphCL (You et al., 2020), JOAO (You et al., 2021), AD-GCL (Suresh et al., 2021), RGCL (Li et al., 2022) and GraphMAE (Hou et al., 2022). The dataset statistics and baseline briefs are available at Appendix C and D, respectively.\n\nGraphBigBird on large-scale graphs. Before presenting the performance comparison, we would like to highlight the sparse attention mechanism — GraphBigBird — to reduce the computational complexity when applying the graph transformer for large-scale graphs. GraphBigBird, as depicted in Section 2.4, is adopted on three datasets containing graphs with thousands of nodes: DD, RDT-B and RDT-M. The detailed model structure and hyperparameter settings are included in Appendix E.\n\nPerformances on TU datasets. As shown in Table 2, SR-GCL achieves the best average accuracy score of 77.03%. Specifically, it achieves 2 bests and 2 second bests on 4 social network datasets: COLLAB, RDT-B, RDT-M5K and IMDB-B, verifying generalization ability of SR-GCL in various types of domain and the effectiveness of GraphBigBird largely reducing the computational complexity while maintaining powerful representation ability. The overall performance of SR-GCL in Table 2 demonstrates its promising performance on both biochemical and social network domains.\n\n3.3 EFFECTIVENESS OF SELF-ATTENTIVE RATIONALIZATION\n\n⟩\n\nCLS ⟨\n\nWe pick out a molecule from pre-training dataset – ZINC-2M at random for visualization. Figure 6(a) and 6(b) show the original graph and corresponding attention map generated by the pre-trained g) and model, respectively. To be specific, the aforementioned parameterized probability value p(vi| g), where vi is the i-th node and ejk is the edge between node j and k in 6(a), correspond to p(ejk| entry ( , i) and (j, k) in 6(b). To make it more intelligible, the attention weights are mapped to the molecule graph in 6(c), on top and bottom of which are node- and edge-wise view (i.e., g)), respectively. 6(d) gives the calculation of each atom to Gasteiger partial charge p(v (top) and partition coefficient (bottom), which indicate the asymmetric distribution of electrons in chemical bonds and the lipophilicity/hydrophilicity (red/blue shades) of the molecule, respectively. The shade of each atom reflects its impact on chemical properties – instance-discriminative power. The comparison of 6(c) and 6(d) shows high precision of our proposed self-attentive rationalization, justifying its reliability. We also provide more visualization results in Appendix H.\n\ng) and p(e\n\n|\n\n|\n\n4 CONCLUSION\n\nIn this paper, we draw inspiration from the self-attention mechanism in transformers and propose a novel framework for transformer-based graph models, Self-attentive Rationale guided Graph Contrastive Learning (SR-GCL), which reshape the separated design of rationale generator and encoder in existing rationale-aware GCL frameworks. In SR-GCL, the self-attention values in transformer module are leveraged to guide the construction of both node- and edge-wise rationale-aware contrastive pairs, making (1) the graph transformations diverse, and (2) the rationale generator naturally integrated with encoder. Visual inspections on real-world molecule datasets demonstrate the effectiveness of self-attentive rationalization proposed by us and extensive experiments on downstream tasks show that SR-GCL empowers the backbone model with the ability of yielding instancediscriminative representations, thus setting the new state-of-the-art for graph pre-training.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nShiyu Chang, Yang Zhang, Mo Yu, and Tommi S. Jaakkola. Invariant rationalization. In ICML,\n\nvolume 119 of Proceedings of Machine Learning Research, pp. 1448–1458. PMLR, 2020.\n\nDexiong Chen, Leslie O’Bray, and Karsten M. Borgwardt. Structure-aware transformer for graph representation learning. In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 3469–3489. PMLR, 2022.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, volume 119 of Proceedings of Machine Learning Research, pp. 1597–1607. PMLR, 2020.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), pp. 4171–4186. Association for Computational Linguistics, 2019.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR. OpenReview.net, 2021.\n\nVijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs.\n\nCoRR, abs/2012.09699, 2020.\n\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large\n\ngraphs. In NIPS, pp. 1024–1034, 2017.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nZhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In KDD, pp. 594–604. ACM, 2022.\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure\n\nLeskovec. Strategies for pre-training graph neural networks. In ICLR, 2020.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparametrization with gumble-softmax. In International Conference on Learning Representations (ICLR 2017). OpenReview. net, 2017.\n\nDevin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent L ́etourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. In NeurIPS, pp. 21618–21629, 2021.\n\nGreg Landrum. Rdkit, open-source cheminformatics software, 2010. URL https://www.\n\nrdkit.org/.\n\nSihang Li, Xiang Wang, An Zhang, Yingxin Wu, Xiangnan He, and Tat-Seng Chua. Let invariant rationale discovery inspire graph contrastive learning. In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 13052–13065. PMLR, 2022.\n\nShengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pre-\n\ntraining molecular graph representation with 3d geometry. In ICLR. OpenReview.net, 2022.\n\nGr ́egoire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph\n\nstructure in transformers. CoRR, abs/2106.05667, 2021.\n\nChristopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. CoRR, abs/2007.08663, 2020.\n\nAnnamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, graph2vec: Learning distributed representations of graphs. CoRR,\n\nand Shantanu Jaiswal. abs/1707.05005, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. GCC: graph contrastive coding for graph neural network pre-training. In KDD, pp. 1150–1160. ACM, 2020.\n\nLadislav Ramp ́asek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. CoRR, abs/2205.12454, 2022.\n\nYu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang.\n\nSelf-supervised graph transformer on large-scale molecular data. In NeurIPS, 2020.\n\nTeague Sterling and John J Irwin. Zinc 15–ligand discovery for everyone. Journal of chemical\n\ninformation and modeling, 55(11):2324–2337, 2015.\n\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. In ICLR, 2020.\n\nSusheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve\n\ngraph contrastive learning. In NeurIPS, pp. 15920–15933, 2021.\n\nJiliang Tang, Salem Alelyani, and Huan Liu. Feature selection for classification: A review. Data\n\nclassification: Algorithms and applications, pp. 37, 2014.\n\nJosephine M. Thomas, Alice Moallemy-Oureh, Silvia Beddar-Wiesing, and Clara Holzh ̈uter. Graph\n\nneural networks designed for different graph types: A survey. CoRR, abs/2204.03080, 2022.\n\nA ̈aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. CoRR, abs/1807.03748, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998–6008, 2017.\n\nPetar Velickovic, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R. Devon\n\nHjelm. Deep graph infomax. In ICLR (Poster). OpenReview.net, 2019.\n\nFeng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In CVPR, pp. 2495–\n\n2504. Computer Vision Foundation / IEEE, 2021.\n\nJindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Transactions on Knowledge and Data Engineering, 2022.\n\nYifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Chaos is a ladder: A new theoretical understanding of contrastive learning via augmentation overlap. In International Conference on Learning Representations, 2021.\n\nZhanghao Wu, Paras Jain, Matthew A. Wright, Azalia Mirhoseini, Joseph E. Gonzalez, and Ion Stoica. Representing long-range context for graph neural networks with global attention. In NeurIPS, pp. 13266–13279, 2021.\n\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513–530, 2018a.\n\nZhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via nonIn CVPR, pp. 3733–3742. Computer Vision Foundation /\n\nparametric instance discrimination. IEEE Computer Society, 2018b.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? In ICLR, 2019.\n\nMinghao Xu, Hang Wang, Bingbing Ni, Hongyu Guo, and Jian Tang. Self-supervised graph-level representation learning with local and global structure. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 11548–11558. PMLR, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nXinyi Xu, Cheng Deng, Yaochen Xie, and Shuiwang Ji. Group contrastive self-supervised learning\n\non graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In NeurIPS, pp. 28877–28888, 2021.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\n\ncontrastive learning with augmentations. NeurIPS, 33:5812–5823, 2020.\n\nYuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 12121–12132. PMLR, 2021.\n\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Onta ̃n ́on, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In NeurIPS, 2020.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\n\nrepresentation learning. CoRR, abs/2006.04131, 2020.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning\n\nwith adaptive augmentation. In WWW, pp. 2069–2080, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA RELATED WORK\n\nA.1 GRAPH CONTRASTIVE LEARNING\n\nAs a prevalent line of self-supervised learning (SSL) (Chen et al., 2020; He et al., 2022), GCL has been a rising topic among researchers in the graph area. It pre-trains the backbone model to yield instance-discriminative representations by contrasting augmented views of graph samples against each other in large label-free datasets, thus benefiting the downstream supervised fine-tuning. The typical paradigm of existing GCL frameworks consists of two modules: (1) graph-structure data augmentation and (2) contrastive optimization.\n\nA.1.1 GRAPH AUGMENTATION\n\nGraph augmentation transforms a graph in two correlated but different views. Early studies (You et al., 2020; 2021; Qiu et al., 2020; Zhu et al., 2020) instantiate it by random mechanisms, which corrupt the graph structures or attributes with uniform probability. For example, GraphCL (You et al., 2020) constructs augmented views with strategies of node dropping, edge perturbation, or attribute masking in a random fashion. JOAO (You et al., 2021) adaptively selects graph augmentation strategies, but all alternatives are still confined in random mechanisms. GCC (Qiu et al., 2020) augments the r-ego network of nodes by random walks with restarts. And GRACE (Zhu et al., 2020) performs random edge removal and node feature masking to generate diverse views. However, overlooking the importance of keeping saliency properties during augmentation may result in potential loss of semantic information, consequently misleading following contrastive optimization and undermining the goal of instance-discrimination.\n\nRationale-guided Augmentation. To tackle this problem, various methods (Zhu et al., 2021; Liu et al., 2022) have been proposed to preserve salient semantics in augmented views, which we term as rationale-guided augmentation. A research line leverages external domain knowledge to bridge the potential semantic gap between augmented views. For instance, in the social network domain, GCA (Zhu et al., 2021) proposes to utilize node centrality measures to capture crucial connective structures; in the biochemical molecule domain, GraphMVP (Liu et al., 2022) conducts contrast between 2D and 3D conformation structure of molecules. Still, expertise could be expensive or even inaccessible in some scenarios (Tang et al., 2014), making their application very limited and the performances of generalizing to unseen domains are easily damaged by these over-specific guidance (Wang et al., 2022). Aforementioned limitations call for rationale-aware but automatic augmentation mechanisms to get rid of the domain knowledge.\n\nRationale Generator. More recently, GCL frameworks (Suresh et al., 2021; Li et al., 2022; Xu et al., 2022) adopt auxiliary models, which is named rationale generator, to conduct heuristic rationale discovery. Specifically, AD-GCL (Suresh et al., 2021) hires another graph model as the rationale generator to identify salient edges for edge-wise views. RGCL (Li et al., 2022) captures semantically informative substructures to create node-wise views with a GNN-based rationale generator. However, the limited perspective of only node or edge confines the performance of the pretrained model to a sub-optimal position, which is consistent with the observations in (Chen et al., 2020; You et al., 2020) – “no single transformation suffices to learn good representations”. To improve the diversity of views, a naive solution is to adopt multiple auxiliary generators, each of which specialises one perspective. Nonetheless, you cannot have your cake and eat it – by this way, more diversity on rationale-aware transformations demands more system complexity (i.e., more auxiliary generators).\n\nRationale Encoder. Apart from the limited perspectives, existing rationale-aware GCL frameworks separate the rationale generator from the encoder (i.e., the backbone model being pre-trained and fine-tuned). The back-propagation of gradients to the encoder is natural when optimizing the contrastive loss of representations yielded by it. Nevertheless, to enable the propagation to the generator, AD-GCL (Suresh et al., 2021) leverages the Gumbel-Max reparameterization, which may lead to instability stemming from a large variance of gradients (Jang et al., 2017). RGCL (Li et al., 2022) mixes the outputs of the encoder and generator to yield the graph representations subsequently being contrasted, resulting in the biased output of the encoder – the generator will be discarded after pre-training. Here, we blame the separation of the generator and encoder for the aforementioned instability and biased graph-level representations.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA.1.2 CONTRASTIVE OPTIMIZATION\n\nWith contrastive pairs in hand, GCL performs instance-discrimination task by optimizing contrastive loss, which can be instantiated as NCE (Wu et al., 2018b), InfoNCE (van den Oord et al., 2018) or NT-Xent (Chen et al., 2020). During this, the agreement between two augmented views of the same anchor graph is encouraged, while the divergence between those of different anchors is enforced.\n\nA.2 GRAPH TRANSFORMER\n\nTransformer has achieved remarkable success in various domains (Vaswani et al., 2017; Devlin et al., 2019; Dosovitskiy et al., 2021). Researchers also apply it in graph area (Dwivedi & Bresson, 2020; Kreuzer et al., 2021; Mialon et al., 2021; Ying et al., 2021; Wu et al., 2021; Chen et al., 2022; Ramp ́asek et al., 2022), aiming to take advantages of self-attention mechanism to capture global relations and avoid over-smoothing during the graph representation learning. However, graph is not naturally in agreement with the input format of transformers, but a data structure with topology and high regularity. Thus, early studies transform a graph into a sequence of nodes and only implicitly incorporate topological information by adding absolute (Dwivedi & Bresson, 2020; Kreuzer et al., 2021), relative (Mialon et al., 2021) or structural (Ying et al., 2021) encoding.\n\nMore recently, hybrid structures have been proposed to integrate GNN and transformer. In GraphTrans (Wu et al., 2021), the GNN module first learns local structure to generate token features, and then the transformer module focuses on capturing global relationships. SAT (Chen et al., 2022) leverages a GNN to encode structure information, while a transformer captures interactions between nodes. More recently, GPS (Ramp ́asek et al., 2022) provides a comprehensive recipe for hybrid models, including transformer structures and encoding methods.\n\nB ALGORITHM OF SR-GCL\n\nAlgorithm 1 SR-GCL algorithm\n\n1: Initialize: dataset\n\ngm : m = 1, 2, ..., M\n\n, GNN module f1(\n\n{\n\n}\n\n), sampling ratio ρv & ρe and temperature τ .\n\n), transformer module f2( ·\n\n), ·\n\nprojector h( ·\n\n2: for all sampled minibatch of data 3:\n\nfor n = 1 to N do Z = f1(gn)\n\ngn : n = 1, 2, ..., N\n\n{\n\ndo\n\n}\n\n▷ Equation 1\n\n▷ Equation 5\n\n▷ Equation 5\n\n▷ Equation 8 ▷ Equation 9\n\n▷ Equation 12\n\n4:\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n10:\n\n11:\n\n12:\n\n13:\n\n14:\n\n15:\n\nκ(zCLS, zv)\n\nκ(zCLS, zu)\n\n∈V κ(zue , zve )\n\n: v\n\n∈ V}\n\nκ(zue′ , zve′ )\n\n: e\n\n∈ E}\n\np(v\n\ngn) = |\n\n{\n\n(cid:80)\n\nu\n\n(cid:80)\n\ngn) =\n\np(e {\n| e′ (\nRv(g) G = gn)) ·| V\nRe(g) (\nG = gn) ·| r′n = h(f2(f1(Rv(g)))),\n\n∼ ∼\n\nP P\n\n∈E\n\nE\n\ns.t.\n\ns.t.\n\n|VR| |ER|\n\n= ρv · |V| = ρe · |E| r′′n = h(f2(f1(Re(g))))\n\nend for for n = 1 to N do\n\n−n =\n\nR\n\nln =\n\n−\n\nend for\n\n{\n\nr′i, r′′i : i = 1, 2, ..., n exp (cid:17)\n\n(cid:16)\n\nlog\n\nexp\n\nr′n⊤r′′n/τ\n\n+ (cid:80)\n\nr−\n\n− (cid:16)\n\n1, n + 1, ..., N r′n⊤r′′n/τ\n\n(cid:17)\n\n}\n\nUpdate f ( ·\n\n), r( ·\n\n), and h( ·\n\n) to minimize\n\n16: end for\n\n17: return Encoder f2 ◦\n\nf1\n\n(cid:16)\n\nr′n⊤r−/τ\n\n(cid:17)\n\nexp\n\n− g\n\n∈R\n\n1 N\n\nN (cid:88)\n\nn=1\n\nln\n\n=\n\nL\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nC DATASETS\n\nTable 3: Statistics for ZINC-2M and MoleculeNet datasets.\n\nDATASETS\n\nCATEGORY\n\nUTILIZATION\n\nGRAPHS#\n\nAVG. N# AVG. E#\n\nZINC-2M BIOCHEMICAL MOLECULES BIOCHEMICAL MOLECULES BIOCHEMICAL MOLECULES BIOCHEMICAL MOLECULES BIOCHEMICAL MOLECULES BIOCHEMICAL MOLECULES BIOCHEMICAL MOLECULES BIOCHEMICAL MOLECULES BIOCHEMICAL MOLECULES\n\nBBBP TOX21 TOXCAST SIDER CLINTOX MUV HIV BACE\n\nPRE-TRAINING FINETUNING FINETUNING FINETUNING FINETUNING FINETUNING FINETUNING FINETUNING FINETUNING\n\n2,000,000 2,039 7,831 8,576 1,427 1,477 93,087 41,127 1,513\n\n26.62 24.06 18.57 18.78 33.64 26.15 24.23 25.51 34.08\n\n57.72 51.90 38.58 38.52 70.71 55.76 52.55 54.93 73.71\n\nTable 4: Statistics for unsupervised learning TU-datasets.\n\nDATASETS\n\nNCI1 PROTEINS DD MUTAG COLLAB RDT-B RDT-M IMDB-B\n\nCATEGORY\n\nGRAPHS# AVG. N# AVG. E#\n\nBIOCHEMICAL MOLECULES BIOCHEMICAL MOLECULES BIOCHEMICAL MOLECULES BIOCHEMICAL MOLECULES SOCIAL NETWORKS SOCIAL NETWORKS SOCIAL NETWORKS SOCIAL NETWORKS\n\n4,110 1,113 1,178 188 5,000 2,000 4,999 1,000\n\n29.87 39.06 284.32 17.93 74.49 429.63 508.52 19.77\n\n32.30 72.82 715.66 19.79 2457.78 497.75 594.87 96.53\n\nD BASELINES\n\nWe compare with following baselines to verify the state-of-the-art performance of our SR-GCL:\n\n• Infomax (Velickovic et al., 2019) Infomax conducts mutual information maximization between\n\npatches (i.e., subgraphs centered around nodes of interest) and high-level graphs.\n\n• EdgePred (Hamilton et al., 2017) EdgePred pre-trains a backbone model by generating node\n\nrepresentations of previously unseen data with text attributes to make edge predictions.\n\n• Attribute Masking (Hu et al., 2020) Attribute Masking masks attributes of node/edge and make\n\nbackbone to predict them based on their neighboring structure.\n\n• Context Prediction (Hu et al., 2020) Context Prediction trains the backbone to map nodes ap-\n\npearing in similar contexts to nearby representations by predicting with subgraphs.\n\n• GraphCL (You et al., 2020) GraphCL is a vanilla GCL method, constructing augmented views\n\nwith strategies of node dropping, edge perturbation or attribute masking in a random fashion.\n\n• GraphLoG (Xu et al., 2021) GraphLoG preserves the local similarity and leverages prototypes to\n\ncapture the clusters with global semantics.\n\n• AD-GCL (Suresh et al., 2021) With a GNN-based augmenter, AD-GCL adopts adversarial graph\n\naugmentation strategies to minimize redundant information in GCL.\n\n• JOAO (You et al., 2021) JOAO automatically selects data augmentations from a strategy pool to\n\nconstruct contrastive pairs for GCL.\n\n• RGCL (Li et al., 2022) RGCL hires an auxiliary model to reveal semantically important nodes to\n\nconstruct augmented views for contrastive optimization.\n\n• graph2vec (Narayanan et al., 2017) Graph2vec views the graph as a document and the rooted subgraphs around every node as words that compose the document. Then the document embedding neural networks can be adopted to learn representations of graphs\n\n• Infograph (Sun et al., 2020) Infograph maximizes the mutual information between the graph-level\n\nrepresentation and the representations of substructures.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n• GraphMAE (Hou et al., 2022) GraphMAE is a masked graph autoencoding framework aiming at\n\nmasked feature reconstruction.\n\nE BACKBONE MODEL ARCHITECTURES\n\nTable 5: Backbone model architectures and hyperparameters\n\nMODELS\n\nGNN TYPE NEURON#\n\nTRANS IN/OUT DIM TRANS PROJ DIM TRANS LAYER# TRANS HEADS#\n\nPOOLING PROJECTOR NEURON#\n\nOPTIMIZER LEARNING RATE\n\nGIN (T)\n\nGRAPH TRANS (T)\n\nGRAPH TRANS (U)\n\nGIN [300,300,300,300,300]\n\nGIN [300,300,300,300,300]\n\nGIN [32,32,32]\n\n- -\n- -\n\nMEAN [300,300]\n\nADAM 0.001\n\n128 512 4\n4\n\n⟨CLS⟩ [128,128]\n\nADAMW 0.0001\n\n32 64 2\n2\n\n⟨CLS⟩ [32,32]\n\nADAMW 0.0001\n\n(T) means that the model is applied in transfer learning experiments, while (U) indicates unsupervised learning experiments. For hyperparameter sensitivity analysis in transfer learning, please refer to Figure 7. And we only adopt GraphBigBird on three datasets containing graphs of thousands of nodes: DD, RDT-B and RDT-M and the default value for sparsity r is set to 0.01.\n\nF COMPUTATIONAL EFFICIENCY\n\nThe training time mainly depends on the type of backbone model. To compare the computational efficiency of commonly adopted GIN model in existing baselines with the graph transformer in our paper, the architectures of which are shown in Table 5, we evaluate the number of parameters and the propagation runtime in transfer learning for each mini-batch of 256 samples on NVIDIA GeForce RTX 3090. The statistics in Table 6 show that the transformer module adds an overhead of 46.63% to whole contrastive learning procedure and 49.91% to inference (i.e., forward) time, which we argue, considering the great improvement of representation ability and interpretability, is acceptable.\n\nTable 6: Computational efficiency of backbone models\n\nBackbone\n\nForward (ms) Backward (ms) Cumulative (ms)\n\nParameter #\n\nGIN Graph Transformer\n\n15.55 23.97\n\n9.72\n\n11.39\n\n±\n\n±\n\n22.49 36.51\n\n2.27\n\n3.32\n\n±\n\n±\n\n38.04 60.47\n\n±\n\n±\n\n10.07\n\n11.95\n\n1.88M 2.71M\n\nMeanwhile, we conduct the time complexity comparison between SR-GCL and the vanilla GCL – GraphCL with the same graph transformer as the backbone model, which is shown as follows:\n\nTable 7: Time complexity comparison between SR-GCL and GraphCL\n\nFRAMEWORKS\n\nDATA AUG.\n\nRATIONALE GEN.\n\nBACKBONE FORWARD\n\nCONTRASTIVE LOSS\n\nGRAPHCL\n\nSR-GCL\n\nO(2Bρ|V| log |V|) —\nO(B((2|E|2 + |V|)LGNN + (|V|2d + |V|d2)LGT )) O(B2d)\n\nO(B(ρv |V| log |V| + ρe|E| log |E|)) O(B((|E|2 + |V|)LGNN ) + |V|2d + |V|d2) O(B((2|E|2 + |V|)LGNN + (|V|2d + |V|d2)LGT )) O(B2d)\n\nwhere the average numbers of nodes and edges per graph in ZINC-2M datasets are , respectively, B denotes the batch size, ρ denotes the sampling ratio, L denotes the number of layers in backbone encoder, and d denote the latent space dimension where contrastive loss is calculated. On a single NVIDIA GeForce RTX 3090, the overall training time of GraphCL and SR-GCL with 100\n\nand\n\n|V|\n\n|E|\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nepochs and the same backbone model of graph transformer are 25.61 and 30.56 hours, respectively. We argue that the 19.33% training time overhead brought by self-attention rationalization process is acceptable, considering the performance gain of SR-GCL compared with GraphCL.\n\nG HYPERPARAMETER SENSITIVITY\n\n(a) τ\n\n(b) ρ (%)\n\nFigure 7: Sensitivity w.r.t. hyperparameters τ and ρ.\n\nThe average performances of SR-GCL for transfer learning on 8 downstream MoleculeNet datasets w.r.t. hyperparameters τ and ρ are presented in Figure 7. The sharp pike in 7(a) indicates the close relationship between SR-GCL’s performance and hyperparameters τ (Wang & Liu, 2021). Thus, we suggest to tune it carefully around 0.1. As for hyperparameter controlling the scale of augmented views, we set ρv = ρe = ρ to make the scales of each contrastive pair close. The results show that SR-GCL achieves the best performance when ρ is set to a large value around 80%, which is consistent with Li et al. (2022).\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nH SAMPLES OF SELF-ATTENTIVE RATIONALIZATION\n\n(a) Cc1cc(F)ccc1/C=C/C(=O)O\n\n(b) Clc1ccc(C2OCC=CCO2)c(Cl)c1\n\n(c) CN[C@H](CO)c1cc(F)ccc1F\n\n(d) C/[N+]([O-])=C/c1cc2c(cc1Br)OCO2\n\n(e) COC(=O)[C@]1(c2ccc(Br)cc2)CO1\n\n(f) COc1cc(Br)c(Br)cc1O\n\nFigure 8: Visualization of more molecules\n\n18",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors propose a self-attentive rationalization for rationale-aware agmented pairs, which are used in contrastive learning. The efficiency of self-attentive rationalisation is validated by visualisation on real-world biochemistry datasets, and the performance on downstream tasks shows the good performance of SR-GCL for graph model pre-training.\n\n# Strength And Weaknesses\n\nStrength\n1. A rationale finder finder is proposed with plural views instead of either node- or edge-wise rationales finder independently.\n2. In this paper, based on the transformer framework, they propose self-attentive for dual views to guide the rationale finder.\n\nWeakness:\n1. I do not think of “no single transformation suffices to learn good representations‘’. There are many augmentation-free (in raw attributes and the graph) methods working competitively in different benchmarks. I'm curious if augmentation-free is also applicable in specific data.\n2 . In my opinion, the contribution is somewhat limited. It looks like the biggest contribution is a self-attentive augmentation. From a GCL point of view, the contribution seems insufficient. Can this technique be used in the other graphs and tasks? For example node classification and social network.\n3. I'm not sure I understand correctly that sampling makes this model not an end2end optimization. Have you consider using the reparameterization trick in VAE to solve this? or it is unreasonable or useless.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nIn my opinion, I cannot follow this paper well. I'm not sure if it's because it's specific tasks on graph learning. Although I am an expert in graph learning, I cannot understand this paper well in motivation and some claims. The originality is ok.\n\n# Summary Of The Review\n\nBased on my comments, I am inclined to give a rejection right now. But I am happy to refer to other reviewers' comments and the author's response before making a decision.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nS-SOLVER: NUMERICALLY STABLE ADAPTIVE STEP SIZE SOLVER FOR NEURAL ODES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nA neural ordinary differential equation (ODE) is a relation between an unknown function and its derivatives, where the ODE is parameterized by a neural network. Therefore, to obtain a solution to a neural ODE requires a solver that performs numerical integration. Dopri5 is one of the most popular neural ODE solvers and also the default solver in torchdiffeq, a PyTorch library of ODE solvers. It is an adaptive step size solver based on the Runge-Kutta (RK) numerical methods. These methods rely on estimation of the local truncation error to select and adjust integration step size, which determines the numerical stability of the solution. A step size that is too large leads to numerical instability, while a step size that is too small may cause the solver to take unnecessarily many steps, which is computationally expensive and may even cause rounding error build up. Therefore, accurate local truncation error estimation is paramount for choosing an appropriate step size to obtain an accurate, numerically stable, and fast solution to the ODE. In this paper we propose a novel local truncation error approximation that is the first to consider solutions of four different RK orders to obtain a more reliable error estimate. This leads to a novel solver S-SOLVER (Stable Solver), which is more numerically stable; and therefore accurate. We demonstrate S-SOLVER’s competitive performance in experiments on image recognition with ODE-Net, learning hamiltonian dynamics with Symplectic ODE-Net, and continuous normalizing flows (CNF).\n\n1\n\nINTRODUCTION\n\nNeural ODEs are continuous depth deep learning models that combine neural networks and ODEs. Since their first introduction in (Chen et al., 2018), they have been used in many applications such as: stochastic differential equations (Li et al., 2020), physically informed modeling (Sanchez-Gonzalez et al., 2019; Zhong et al., 2020), free-form continuous generative models (Grathwohl et al., 2019; Finlay et al., 2020), mean-field games (Ruthotto et al., 2020), and irregularly sampled time-series (Rubanova et al., 2019).\n\nNeural ODEs parameterize the derivative of the hidden state using a neural network; and therefore, learn non-linear mappings via differential equations. A differential equation is a relation between an unknown function and its derivatives. Ordinary differential equations describe the change of only one variable (as opposed to multiple) with respect to time, i.e.: dx/dt = f (t, x). Typically, an ODE is formulated as an initial value problem (IVP), which has the following form. Given a function derivative dx/dt, a time interval t = (a, b) and an initial value (e.i.: x at time t = a), the solution to the IVP yields x evaluated at time t = b. The method for approximating x(b) is numerical integration; therefore, all the various ODE solvers include different methods for performing integration.\n\nAdaptive step size solvers are amongst the most popular solvers for neural ODEs. In fact, the default solver in torchdiffeq (a library of ODE solvers implemented in PyTorch) is Dopri5, the DormandPrince 5(4) embedded adaptive step size method of the Runge-Kutta (RK) family. Adaptive step size RK solvers perform two approximations: one of order p and another of p − 1 and compare them to obtain the local truncation error, which is used to determine the integration step size. Specifically, the error is used to make a decision whether to accept or reject the solution step under the current step size and to decide how to modify the step size for the next step. A step size that is too large leads\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nto numerical instability, while a step size that is too small may cause the solver to take unnecessarily many steps, which is computationally expensive and may even cause the rounding error to build up. Therefore, accurate local estimation is paramount for choosing an appropriate step size to obtain an accurate, numerically stable, and fast solution to the ODE.\n\nThe local truncation error is defined as the difference between the exact and approximate solution obtained at a given time step. All currently available adaptive step neural ODE solvers rely on estimating the local error as the difference between order p and p − 1 solutions, which assumes that the order p solution is exact. This is not necessarily true and if the p solution is far from the exact one, the local error estimate is inaccurate, which results in the solver making poor decisions regarding its step size.\n\nIn this paper we propose a novel local truncation error estimation that takes into account multiple orders of the RK method as opposed to just order p and p − 1 to obtain a more accurate estimate of the local truncation error that guides the integration step size. Specifically, we modify the local truncation error estimation of Dopri8, the Dormand-Prince 8(7) embedded adaptive step size method. Dopri8 calculates the local truncation error as the difference between its 8th and 7th order solution. Our modification computes this error as the average of the difference between both its 8th and 7th, and also 4th and 5th order solution. This leads to a new ODE solver, S-SOLVER (Stable Solver), a modified Dopri8 integrator with more accurate local truncation error estimation that provides more reliable information for step size calculations; and therefore, more numerically stable solution. To our best knowledge, S-SOLVER is the first solver that uses a multiple solution orders to estimate local truncation error for adjusting its step size.\n\n2 BACKGROUND\n\n2.1 NEURAL ORDINARY DIFFERENTIAL EQUATIONS\n\nTraditional neural networks are defined as discrete models with a discrete sequence of hidden layers, where the depth of the network corresponds to the number of layers. Neural ODEs (Chen et al., 2018) are continuous depth deep learning models, which parameterize the derivative of the hidden state using a neural network. Specifically, they are ODEs that are parameterized by a neural network, which has many benefits such as memory efficiency, adaptive computation, and parameter efficiency.\n\nNeural ODEs are inspired by the dynamic systems interpretation of residual and other networks Haber et al. (2018); Weinan (2017). These networks perform a sequence of transformations to a hidden state:\n\nstatet+1 = statet + f (statet, θt),\n\n(1)\n\nwhich can be viewed as discretized forward Euler method applied to a continuous transformation. Given this interpretation, the transformation to a hidden state can be formulated as an ODE:\n\nd state(t)/dt = f (state(t), t, θ),\n\n(2)\n\nwhere state(t = 0) is the input layer and state(t = T ) is the output layer. Therefore, the neural ODE is an IVP:\n\ndx(t)/dt = f (t, x(t), θ), f or t0 ≤ t ≤ t1, subject to x(t0) = xt0,\n\n(3)\n\nwhere f (., ., θ) is the deep neural network, xt0 is the input, and xt1 is the output.\n\nNeural ODEs are trainable through loss minimization, but due to their continuous nature the optimization process is slightly different from classical discrete deep learning models. The forward pass solves the ODE with an ODE solver and the backward pass computes the gradients either by backpropagating through the ODE solver or with the adjoint method (Chen et al., 2018). In this work we focus on the forward pass, which outputs a solution to the ODE.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2.2 NEURAL ODE SOLVERS\n\n(cid:90) t1\n\nSolving neural ODEs that we generalized in Equation 3 requires numerical integration that can be described as follows:\n\nx(t1) = x(t0) +\n\nf (t, x(t), θ)dt\n\n(4)\n\nt0\n\nThis equation can be solved with an ODE solver, which returns the value of x(t1) that represents the solution at the end of the time interval that satisfies the initial condition x(t0) = xt0 .\n\nThere are different types of ODE solvers that use different methods and algorithms for performing numerical integration, and the Runge-Kutta (RK) set of methods that are amongst the most popular (Seiler & Seiler, 1989). The basic idea behind the RK integration methods is to re-write dx and dt in Equation 3 as finite steps ∆x and ∆t and multiply the equations by ∆t, which provides a change in x with respect to ∆t. The finite time step ∆t is called the step size (Seiler & Seiler, 1989) and is typically represented as h. The the simplest RK method, the Euler method, illustrates this well:\n\nxtn+1 = xtn + hf (tn, xtn )\n\n(5)\n\nRK methods leverage the differential equation for computing the slope k of the tangent line to the function f . The slope is then used to approximate f at the next time step t+1. As shown in (Bogacki & Shampine, 1989), this can be represented as:\n\nwhere\n\nxtn+1 = xtn + h\n\nS (cid:88)\n\ni=1\n\nˆbiki,\n\nk1 = f (tn, ˆxtn )\n\nki = f (tn + cih, ˆxtn + h\n\ni−1 (cid:88)\n\nj=1\n\nci =\n\ni−1 (cid:88)\n\nj=1\n\naij\n\naijkj) f or RK stages i = 2, ..., s\n\n(6)\n\n(7)\n\nSince the Euler method approximates the slope only once to proceed from t to t + 1, it can be expressed using the general RK method shown in Equation 6 as:\n\nxtn+1 = xtn + h(b1k1)\n\n(8)\n\nThe number of times that the slope k is approximated between t and t+1 impacts the local truncation error the RK method Burden et al. (2015); Burrage & Burrage (2000). The local truncation error is the difference between the exact and approximated solution and determines the order of the RK method Burden et al. (2015). The order of the RK method corresponds to the order of the local truncation error minus one. For example, the local truncation error for the Euler’s method is O(h2), resulting in a first order numerical technique.\n\n3 NUMERICAL STABILITY OF NEURAL ODE SOLVERS\n\nWhen approximating the solution of an IVP, there are two primary sources of error: the roundoff error and the truncation error (Abell & Braselton, 2014), which impact the numerical stability of the ODE solver that yields the approximate solution.\n\n3.1 NUMERICAL STABILITY\n\nNumerical stability can be viewed property of of an algorithm, which describes the sensitivity of a solution to numerical errors (Higham, 2002). An unstable numerical method produces large changes\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nin outputs in response to small changes in inputs (Jong, 1977), which can lead to unexpected outputs or errors. Numerical instability arises due rounding, and truncation errors (Higham, 2002). Roundoff errors are caused by approximating real numbers with finite precision, while truncation errors are caused by approximating a mathematical process. Many numerical methods (e.g.: Euler’s method for solving differential equations) can be derived by taking finitely many terms of a Taylor series. The terms omitted constitute the truncation error, which often depends on a parameter called the step size (Higham, 2002). In this paper introduce a novel truncation error estimation used for setting an adaptive step size that achieves a numerically stable solution.\n\n3.2 STABILITY OF DIFFERENT ODE SOLVERS\n\nThere are two notions of numerical stability of ODEs: zero-stability and absolute stability. Zerostability implies that on a fixed time interval, small perturbations of data yield bounded perturbations in the solution as the step size h approaches zero (LeVeque, 2005). Absolute stability, a stronger notion of stability, guarantees the same behavior, but for a fixed step size h as the time interval approaches infinity. Generally, a numerical method for for solving initial value ODE is numerically stable if ”small changes or perturbations in the initial conditions produce correspondingly small changes in the subsequent approximations” (Burden et al., 2015).\n\nDifferent ODE solvers have different numerical stability. This can be demonstrated with a canonical example of an ODE that describes a swinging pendulum:\n\nml\n\nd2Θ(t)\n\ndt2 = −mg sin(Θ(t)),\n\n(9)\n\nwhere Θ(t) is the angle between the pendulum and a vertical axis at a time t, l is the length of the pendulum, m is the pendulum mass, and g represents gravity. Figure 1 illustrates the varying degrees of numerical stability of three different methods that can be used to solve this pendulum IVP.\n\nFigure 1: Comparison of numerical stability of various ODE solvers\n\n3.3 ANALYSIS ON NUMERICAL STABILITY WITH RESPECT TO ODE SOLVER STEP SIZE\n\nTo illustrate the impact of step size on the numerical stability of ODE solvers, we provide a numerical stability analysis of the Euler method and derive its stability condition. The stability condition pertaining to explicit Euler method’s step size can be derived using the test equation:\n\nApplying the forward Euler method to this ODE yields:\n\ny′ = ky,\n\ny(0) = α, α < 0\n\nx0 = α,\n\nxi+1 = xi + h(kxi) = (1 + hk)xi\n\n(10)\n\n(11)\n\n4\n\n012345t−20−15−10−505101520Θ(t)ExplicitImplicitTrapezoidalExactUnder review as a conference paper at ICLR 2023\n\nSolving for xi+1:\n\nThe exact solution is:\n\nxi+1 = (1 + hk)xi = (1 + hk)i+1x0 = (1 + hk)i+1α\n\nThe absolute error is the absolute difference between the exact and approximated solution:\n\n|y(ti) − xi| = | exp(ihk) − (1 + hk)i||α| = | exp(hk)i − (1 + hk)i||α|\n\ny(t) = α exp(kt)\n\n(12)\n\n(13)\n\n(14)\n\nIf k > 0, the problem is unstable. If k ≤ 0 and |1 + hk| < 1, the forward Euler method will be stable. This condition is called the stability region and pertains to the notion of absolute stability. Specifically, the analysis of the stability region is useful for determining a step size that can ensure absolute stability.\n\n3.3.1\n\nILLUSTRATIVE EXAMPLE\n\nWe demonstrate the practical application of the theoretical numerical stability analysis shown above with an illustrative example of an ODE dy/dt = −2.3y with an initial value of y(0) = 1. Figure 2a compares the exact solution −2.3t with approximate solutions obtained with the explicit Euler method with varying step sizes: h = 1.0, 0.7, 0.1. The solution obtained with step size h = 1.0 is erratic and inaccurate, while the solution with the smallest step size h = 0.1 yields a stable solution that is very close to the exact one. The reason for that is that kh for h = 1.0 and h = 0.7 are far away from the stability region represented as the blue circle in Figure 2b. Therefore, we can observe that the step size h has a significant impact on the numerical stability and accuracy of the ODE solution.\n\n(a) The importance of step size for accurate solutions (b) The stability region of the explicit Euler method\n\nFigure 2: The relationship between numerical stability of an ODE solution and ODE solver step size\n\n4 METHOD\n\nPrior adaptive step size solvers approximate the local truncation error as the difference between order p − 1 and p solution, where the p order solution is assumed to be the exact solution. This means that the error estimates are not exact, but only accurate to the leading order in h, i.e.: order p (Press & Teukolsky, 1992). S-SOLVER is an adaptive step size solver with novel, more accurate local error estimation that is used for adjusting the solver step size to achieve an accurate numerically stable solution to neural ODEs.\n\nS-SOLVER is based on Dopri8, the Dormand-Prince 8(7) embedded adaptive step size method, which is a 8th order RK method that requires 13 function evaluations per integration step (Prince & Dormand, 1981) as shown in Equation 15.\n\nxtn+1 = xtn + h\n\n13 (cid:88)\n\ni=1\n\nˆbiki,\n\n5\n\n(15)\n\nUnder review as a conference paper at ICLR 2023\n\nwhere\n\nk1 = f (tn, ˆxtn )\n\nki = f (tn + cih, ˆxtn + h\n\ni−1 (cid:88)\n\nj=1\n\nci =\n\ni−1 (cid:88)\n\nj=1\n\naij\n\naijkj) f or i = 2, ..., 12\n\n(16)\n\nThe coefficients a, b, and c in Equations 15 and 16 are defined using the Butcher tableau provided in (Prince & Dormand, 1981).\n\nIn contrast to Dopri8, which calculates local error as the difference between the 7th and 8th order solution, S-SOLVER uses order 8, 7, 5, and 4. Specifically, given a neural ODE:\n\nx(t1) = x(t0) +\n\n(cid:90) t1\n\nt0\n\nf (t, x(t), θ)dt,\n\n(17)\n\nsuppose that the solver has progressed in integration to some time step tn and approximated x(tn) as ˆx(tn). To make further progress, the solver needs to take a step forward and compute the value of x at time step tn + h, where h is the step size. Suppose that this is approximated as ˆx(tn + h) and that the step’s error is xerror. S-SOLVER computes xerror as an average of the difference between 8th and 7th order solution and 5th and 4th order solution to obtain a more reliable estimate:\n\nxerror =\n\n(ˆx(tn + h)order8 − ˆx(tn + h)order7 ) + (ˆx(tn + h)order5 − ˆx(tn + h)order4 ) 2\n\n(18)\n\nThe 5th and 4th order solution is computed using a similar process, but only with 6 stages as follows:\n\nxtn+1 = xtn + h\n\n6 (cid:88)\n\ni=1\n\nˆbiki,\n\nwhere\n\nk1 = f (tn, ˆxtn)\n\nki = f (tn + cih, ˆxtn + h\n\ni−1 (cid:88)\n\nj=1\n\naijkj) f or i = 2, ..., 5\n\nci =\n\ni−1 (cid:88)\n\nj=1\n\naij,\n\n(19)\n\n(20)\n\nwhere the coefficients a, b, and c are given in the Butcher tableau provided in (Lawrence, 1986).\n\nGiven a pre-defined upper bound on relative error RTOL (1e-7 default in torchdiffeq) and upper bound on absolute error ATOL (1e-9 default in torchdiff ), the solver then computes an error ratio r as follows:\n\nr = ∥\n\nxerror scale\n\n∥,\n\nwhere scale is defined as:\n\nscale = AT OL + RT OL max(ˆx(tn), ˆx(tn + h)).\n\n(21)\n\n(22)\n\nIf r ≤ 1 the step is accepted, otherwise it is rejected and the value of x at time step tn + h is approximated again with a smaller step size h.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n5 EXPERIMENTS\n\nWe implement S-SOLVER as a new solver that is part of the torchdiffeq library (https: //anonymous.4open.science/r/S-SOLVER-EC78/ReadMe.md) and perform experiments on image recognition with ODE-Net, learning hamiltonian dynamics with Symplectic ODENet, and generating new distributions with continuous normalizing flows (CNF). We demonstrate the S-SOLVER is accurate and numerically stable thanks to better local error estimation that determines the step size, which in turn afects the numerical stability of the ODE solution as shown in section 3.\n\n5.1 STIFF NEURAL ODE AND ERROR MONITORING\n\nWe first validate S-SOLVER’s numerical stability on solving the following stiff neural ODE obtained from page 353 of Burden et al. (2015):\n\ndy/dt = 5 exp(5t)(y − t)2 + 1 f or 0 ≤ t ≤ 1, subject to y(t = 0) = −1\n\n(23)\n\nThis ODE equation is stiff, which means that it is likely the error due to approximation is amplified and becomes dominating in the solution calculations leading to a numerically unstable solution (Burden et al., 2015; Kim et al., 2021).\n\nAs shown in Figure 3, the neural ODE solved with S-SOLVER yields a solution that is very close to the exact solution:\n\ny(t) = t − exp(−5t)\n\n(24)\n\nFigure 3: Solution to a stiff Neural ODE with S-SOLVER\n\nIn addition to demonstrating that S-SOLVER can solve stiff neural ODEs, which typically have numerical stability issues, we also examine the local error. Figure 4 shows the local error estimate produced by Dopri5 (default solver in torchdiffeq), S-SOLVER, and also a comparison of the two, which suggests that Dopri5 underestimates the local error.\n\nFigure 4: A comparison of the local error produced by Dopri5 and S-SOLVER\n\n7\n\n0.00.20.40.60.81.0t−1.00−0.75−0.50−0.250.000.250.500.751.00f(t)neural ODE solution with S-SOLVERexact solution0500100015002000train iteration−1.0−0.50.00.5local error1e−10Dopri5 local error0500100015002000train iteration−0.50.00.51.01.5local error1e−8S-SOLVER local error0500100015002000train iteration−0.50.00.51.01.5local error1e−8S-SOLVER local errorDopri5 local errorUnder review as a conference paper at ICLR 2023\n\n5.2\n\nIMAGE RECOGNITION\n\nThe next set of experiments focuses on image recognition with ODE-Nets. We train an ODE-Net with S-SOLVER and compare its results with an ODE-Net trained with Dopri5 (default solver in torchdiffeq) and also a classical ResNet on two datasets: MNIST and FASHION MNIST. Table 1 shows that the highest test accuracy on both datasets is achieved with our ODE-Net with S-SOLVER. The test accuracy on MNIST beats prior SOTA results in (Chen et al., 2018), who report a 0.42% test error, i.e.: 99.58% test accuracy. Using the same experiment settings as (Chen et al., 2018), thanks to S-SOLVER we push the test accuracy to 99.73%. Our results are also better compared to, for example, (Ghosh et al., 2020) who report 98.3% test accuracy that is achieved with their proposed temporal regularization.\n\nTable 1: Results for ODE-Net with S-SOLVER on image recognition tasks\n\nMNIST train acc\n\ntest acc\n\nloss\n\nODE-Net with S-SOLVER 99.99% 99.73% 0.00013 99.98% 99.69% 0.04623 ODE-Net with dopri5 99.96% 99.68% 7.2E-05 ResNet\n\nFASHION MNIST train acc test acc 97.58% 94.00% 0.079816 97.75% 93.72% 0.055531 98.52% 93.94% 0.065489\n\nloss\n\n5.3 LEARNING HAMILTONIAN DYNAMICS\n\nWe test S-SOLVER on Symplectic ODE-Net (Zhong et al., 2020), which can learn Hamiltonian dynamics. Specifically, we choose the problem of ”acrobot” (Murray & Hauser, 2010; Sutton & Barto, 2005), which simulates a physical system with two joints and two links, where the joint between the two links is actuated. Initially, the links are hanging downwards, and the goal is to In Figure 5 we show that the validation swing the end of the lower link up to a given height. loss obtained with S-SOLVER is more stable than with Dopri5 (default solver in torchdiffeq) and therefore, preferable. We interpret this observation to be the result of S-SOLVER’s more reliable local estimation that controls the step size, which in turn impacts the stability of the ODE solution.\n\nFigure 5: Acrobot: The validation loss obtained by solving Symplectic ODE-Net with S-SOLVER is more stable than with Dopri5 solver (default solver in torchdiffeq)\n\n5.4 CONTINUOUS NORMALIZING FLOWS\n\nContinuous Normalizing Flows (CNF) are generative models introduced by (Chen et al., 2018) that leverage neural ODEs. CNFs are based on normalizing flows (Rezende & Mohamed, 2015), which perform transformations of a simple probability distribution into a more complex one by a sequence of invertible and differentiable mappings (Kobyzev et al., 2021).\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nWe perform experiments with CNFs that use S-SOLVER and visualize how the model generates the Two Circles distribution from random noise in Figure 6. Figure 6 shows the evolution of the generated distribution (samples) and probability density (log probability) with respect to the Two Circles distribution (target) over time from time-step 0.0 to 10.0. It can be observed that by the last time step, the random distribution has been transformed into the Two Circles distribution.\n\nFigure 6: Continuous normalizing flows for fitting the Two Circles distribution with S-SOLVER\n\n6 RELATED WORK\n\nThis paper focuses on ODE solvers for solving neural ODEs, which involves performing numerical integration. Specifically, we focus on adaptive step size ODE solvers which have become the standard for solving neural ODEs. While to our best knowledge, we are the first ones to propose a more numerically stable ODE solver that is based on more accurate local truncation error estimation, there are several prior works that also study numerical integration in neural ODEs. Zhu et al. (2022) perform numerical analysis of numerical integration in neural ODEs and propose IMDE, or inverse modified differential equations. Zhuang et al. (2021) propose MALI, a new numerical integrator that is memory-efficient. Ghosh et al. (2020) introduce STEER, a simple temporal regularization that randomly perturbs the numerical integration time limits. Pal et al. (2021) propose a regularization method for adaptive ODE solvers that uses the internal cost heuristics. Yan et al. (2020) study the robustness of the Euler method, which is the simplest, but important neural ODE solver. Krishnapriyan et al. (2022) develop a convergence test that can be used to select an ODE solver that is suitable for a particular task.\n\n7 CONCLUSION\n\nIn this paper we demonstrate the importance of appropriately choosing and adapting the step size in ODE solvers for obtaining a numerically stable; and therefore, accurate solutions to a neural ODEs. To this end we propose S-SOLVER, a new neural ODE solver that is more numerically stable thanks to more accurate local truncation error estimation that is based on comparing multiple approximations as opposed to just two, which has been the standard approach. We provide a theoretical analysis of the impact of solver step size on numerical stability and also perform practical experiments with S-SOLVER. We show that S-SOLVER can solve a stiff neural ODE and that image recognition ODE-Nets learned with S-SOLVER surpass the test accuracy of prior solvers as well as classical ResNets on MNIST and FASHION MNIST. In fact, S-SOLVER achieves a new SOTA test accuracy on MNIST. We also show that the process of learning Hamiltonian dynamics with Symplectic ODE-Nets on the acrobot example is more stable with S-SOLVER than with Dopri5, the solver used in prior neural ODE works. Finally, we also show that S-SOLVER works well for CNFs in an experiment, where we successfully learn a new data distribution from random noise.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMartha L. Abell and James P. Braselton. Introductory differential equations. 2014.\n\nPrzemyslaw Bogacki and Lawrence F. Shampine. A 3(2) pair of runge - kutta formulas. Applied\n\nMathematics Letters, 2:321–325, 1989.\n\nRichard L Burden, J Douglas Faires, and Annette M Burden. Numerical analysis. Cengage learning,\n\n2015.\n\nKevin Burrage and Pamela M. Burrage. Order conditions of stochastic runge-kutta methods by\n\nb-series. SIAM J. Numer. Anal., 38:1626–1646, 2000.\n\nTian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Kristjanson Duvenaud. Neural ordi-\n\nnary differential equations. ArXiv, abs/1806.07366, 2018.\n\nChris Finlay, J ̈orn-Henrik Jacobsen, Levon Nurbekyan, and Adam M. Oberman. How to train your\n\nneural ode: the world of jacobian and kinetic regularization. In ICML, 2020.\n\nArna Ghosh, Harkirat Singh Behl, Emilien Dupont, Philip H. S. Torr, and Vinay Namboodiri. Steer\n\n: Simple temporal regularization for neural odes. ArXiv, abs/2006.10711, 2020.\n\nWill Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Kristjanson Duvenaud. Ffjord: Free-form continuous dynamics for scalable reversible generative models. ArXiv, abs/1810.01367, 2019.\n\nEldad Haber, Lars Ruthotto, and Elliot Holtham. Learning across scales - a multiscale method for\n\nconvolution neural networks. ArXiv, abs/1703.02009, 2018.\n\nNicholas John Higham. Accuracy and stability of numerical algorithms, second edition. 2002.\n\nL. D. Jong. Towards a formal definition of numerical stability. Numerische Mathematik, 28:211–\n\n219, 1977.\n\nSuyong Kim, Weiqi Ji, Sili Deng, Yingbo Ma, and Christopher Rackauckas. Stiff neural ordinary differential equations. Chaos: An Interdisciplinary Journal of Nonlinear Science, 31(9):093122, 2021.\n\nIvan Kobyzev, Simon Prince, and Marcus A. Brubaker. Normalizing flows: An introduction and review of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43: 3964–3979, 2021.\n\nAditi S. Krishnapriyan, A. F. Queiruga, N. Benjamin Erichson, and Michael W. Mahoney. Learning\n\ncontinuous models for continuous physics. ArXiv, abs/2202.08494, 2022.\n\nF Shampine Lawrence. Some practical runge-kutta formulas. Mathematics of Computation, 46:\n\n135–150, 1986.\n\nRandall J. LeVeque. Finite difference methods for differential equations. 2005.\n\nXuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, and David Kristjanson Duvenaud. Scal-\n\nable gradients for stochastic differential equations. ArXiv, abs/2001.01328, 2020.\n\nRichard M. Murray and John Hauser. A case study in approximate linearization: The acrobot exam-\n\nple. 2010.\n\nAvik Pal, Yingbo Ma, Viral B. Shah, and Chris Rackauckas. Opening the blackbox: Accelerating\n\nneural differential equations by regularizing internal solver heuristics. In ICML, 2021.\n\nWilliam H. Press and Saul A. Teukolsky. Adaptive stepsize runge-kutta integration. Computers in\n\nPhysics, 6:188–191, 1992.\n\nP. J. Prince and J. R. Dormand. High order embedded runge-kutta formulae. Journal of Computa-\n\ntional and Applied Mathematics, 7:67–75, 1981.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDanilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In\n\nICML, 2015.\n\nYulia Rubanova, Tian Qi Chen, and David Kristjanson Duvenaud. Latent ordinary differential equa-\n\ntions for irregularly-sampled time series. In NeurIPS, 2019.\n\nLars Ruthotto, Stanley J. Osher, Wuchen Li, Levon Nurbekyan, and Samy Wu Fung. A machine learning framework for solving high-dimensional mean field game and mean field control problems. Proceedings of the National Academy of Sciences, 117:9183 – 9193, 2020.\n\nAlvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter W. Battaglia. Hamiltonian graph\n\nnetworks with ode integrators. ArXiv, abs/1909.12790, 2019.\n\nMary C. Seiler and Fritz A. Seiler. Numerical recipes in c: The art of scientific computing. Risk\n\nAnalysis, 9:415–416, 1989.\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. IEEE Transac-\n\ntions on Neural Networks, 16:285–286, 2005.\n\nE Weinan. A proposal on machine learning via dynamical systems. 2017.\n\nHanshu Yan, Jiawei Du, Vincent Yan Fu Tan, and Jiashi Feng. On robustness of neural ordinary\n\ndifferential equations. ArXiv, abs/1910.05513, 2020.\n\nYaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ode-net: Learning\n\nhamiltonian dynamics with control. ArXiv, abs/1909.12077, 2020.\n\nAiqing Zhu, Pengzhan Jin, Beibei Zhu, and Yifa Tang. On numerical integration in neural ordinary\n\ndifferential equations. In ICML, 2022.\n\nJuntang Zhuang, Nicha C. Dvornek, Sekhar C. Tatikonda, and James S. Duncan. Mali: A memory\n\nefficient and reverse accurate integrator for neural odes. ArXiv, abs/2102.04668, 2021.\n\n11",
    "reference": "# Summary Of The Paper\n\nThe authors aim to propose a new ODE solver for Neural ODE, specifically targeted at stiff ODEs. To achieve this goal, the authors modified the Dopri5 method, and give an estimate of the local error as the mean of error estimate of 5-th order and 8-th order. The authors also validated their method in Neural ODE experiments.\n\n# Strength And Weaknesses\n\nStrengths:\nTo my knowledge, solving stiff ODEs is not extensively studied in the field of Neural ODE. The authors are discussing an important yet not well studied problem. \nFurthermore, the paper is in general well-written and easy to follow.\n\nWeakness:\nI have a few questions and hope the authors could address them, listed below.\n\n1) On the theoretical groundings of the proposed method. I'm not sure if the authors have thought about this, but Runge-Kutta methods are derived by solving a system of equations, see wikipedia for detail https://en.wikipedia.org/wiki/Runge–Kutta_methods#Adaptive_Runge–Kutta_methods. The idea is that, a $p$-th order RK method should have a error on the order of $h^p$ where $h$ is the stepsize, and the system of equations are derived to satisfy this constraint. \nUnder this consideration, the 8-th and 7-th equations gives an error estimate of order $h^8$, and the difference between 5-th and 4-th equations give an error estimate of order $h^5$. I don't think it's meaning to average two errors of different orders.\n\n2) On experiments. The experimental validations are focused on toy examples rather than some harder problems. For example, Neural ODE has been applied to ImageNet classification (1000 classes), CNF on Cifar10 and ImageNet64, which are much more complicated than the experiments on MNIST and Swiss roll here.\n\n3) (Minor, some missing related works). There are some related works that the author could consider for reference, references [1] [2] consider the numerical issue with Neural ODE solvers and have achieved SOTA results on much harder experiments.\n\n[1] Zhuang, Juntang, et al. \"Adaptive checkpoint adjoint method for gradient estimation in neural ode.\" International Conference on Machine Learning. PMLR, 2020.\n[2] Matsubara, Takashi, Yuto Miyatake, and Takaharu Yaguchi. \"Symplectic adjoint method for exact gradient of neural ode with minimal memory.\" Advances in Neural Information Processing Systems 34 (2021): 20772-20784.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nQuality:\nNot very good quality. The proposed method is not well supported in theory (actually counter intuition). The experiments are also not extensive.\n\nClarity:\nThe paper is well-written and easy to follow.\n\nOriginality:\nThe idea is new to me (though looks not solid).\n\n# Summary Of The Review\n\nThe proposed method averages error estimates of different orders, which is not meaningful in theory. Furthermore, the experimental validations are too simple.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDISTRIBUTED GRAPH NEURAL NETWORK TRAINING WITH PERIODIC STALE REPRESENTATION SYNCHRONIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDespite the recent success of Graph Neural Networks (GNNs), it remains challenging to train a GNN on large graphs with over millions of nodes & billions of edges, which are prevalent in many graph-based applications such as social networks, recommender systems, and knowledge graphs. Traditional sampling-based methods accelerate GNN training by dropping edges and nodes, which impairs the graph integrity and model performance. Differently, distributed GNN algorithms accelerate GNN training by utilizing multiple computing devices and can be classified into two types: \"partition-based\" methods enjoy low communication cost but suffer from information loss due to dropped edges, while \"propagation-based\" methods avoid information loss but suffer from prohibitive communication overhead caused by neighbor explosion. To jointly address these problems, this paper proposes DIGEST (DIstributed Graph reprEsentation SynchronizaTion), a novel distributed GNN training framework that synergizes the complementary strength of both categories of existing methods. We propose to allow each device utilize the stale representations of its neighbors in other subgraphs during subgraph parallel training. This way, out method preserves global graph information from neighbors to avoid information loss and reduce the communication cost. Therefore, DIGEST is both computation-efficient and communication-efficient as it does not need to frequently (re-)compute and transfer the massive representation data across the devices, due to neighbor explosion. DIGEST provides synchronous and asynchronous training manners for homogeneous and heterogeneous training environment, respectively. We proved that the approximation error induced by the staleness of the representations can be upper-bounded. More importantly, our convergence analysis demonstrates that DIGEST enjoys the state-of-the-art convergence rate. Extensive experimental evaluation on large, real-world graph datasets shows that DIGEST achieves up to 21.82× speedup without compromising the performance compared to state-of-the-art distributed GNN training frameworks.\n\n1\n\nINTRODUCTION\n\nGraph Neural Networks (GNNs) have shown impressive success in analyzing non-Euclidean graph data and have achieved promising results in various applications, including social networks, recommender systems and knowledge graphs, etc. (Dai et al., 2016; Ying et al., 2018; Eksombatchai et al., 2018; Lei et al., 2019; Zhu et al., 2019). Despite the great promise of GNNs, they meet significant challenges when being applied to large graphs, which are common in real world—the number of nodes of a large graph can be up to millions or even billions. For instance, Facebook social network graph contains over 2.9 billion users and over 400 billion friendship relations among users1. Amazon provides recommendations over 350 million items to 300 million users2. Further, natural language processing (NLP) tasks take advantage of knowledge graphs, such as Freebase (Chah, 2017) with over 1.9 billion triples. Training GNNs on large graphs is jointly challenged by the lack of inherent parallelism in the backpropagation optimization and heavy inter-dependencies among graph nodes, rendering existing parallel techniques inefficient. To tackle the unique challenges in GNN\n\n1https://backlinko.com/facebook-users 2https://amzscout.net/blog/amazon-statistics\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ntraining, distributed GNN training is a promising open domain that has attracted fast-increasing attention in recent years. A classic and intuitive way is by sampling. Until now, a good number of graph-sampling-based GNN methods have been proposed, including neighbor-sampling-based methods (e.g., GraphSAGE (Hamilton et al., 2017), VR-GCN (Chen et al., 2018)) and subgraphsampling-based methods (e.g., Cluster-GCN (Chiang et al., 2019), GraphSAINT (Zeng et al., 2019)). These methods enable a GNN model to be trained over large graphs on a single machine by sampling a subset of data during forward or backward propagation. While sampling operations reduce the size of data needed for computation, these methods suffer from degenerated performance due to unnecessary information loss. To walk around this drawback and also to leverage the increasingly powerful computing capability of modern hardware accelerators, recent solutions propose to train GNNs on a large number of CPU and GPU devices (Thorpe et al., 2021; Ramezani et al., 2021; Wan et al., 2022) and have become the de facto standard for fast and accurate training over large graphs.\n\nExisting methods in distributed training for GNNs can be classified into two categories, namely \"partition-based\" and \"propagation-based\", by how they tackle the trade-off between computation/communication cost and information loss. \"Partition-based\" methods (Angerd et al., 2020; Jia et al., 2020; Ramezani et al., 2021) partition the graph into different subgraphs by dropping the edges across subgraphs. This way, the GNN training on a large graph is decomposed into many smaller training tasks, each trained in a siloed subgraphs in parallel, reducing communications among subgraphs, and thus, tasks, due to edge dropping. However, this will result in severe information loss due to the ignorance of the dependencies among nodes across subgraphs and cause performance degeneration. To alleviate information loss, \"propagation-based\" methods (Ma et al., 2019; Zhu et al., 2019; Zheng et al., 2020; Tripathy et al., 2020; Wan et al., 2022) do not ignore edges across different subgraphs with neighbor communications among subgraphs to satisfy GNN’s neighbor aggregation. However, the number of neighbors involved in neighbor aggregation grows exponentially as the GNN goes deeper (i.e., neighborhood explosion (Hamilton et al., 2017)), hence inevitably suffering huge communication overhead and plagued training efficiency.\n\nTherefore, although \"partition-based\" methods can parallelize a training job among partitioned subgraphs, they suffers from information loss and low accuracy. \"Propagation-based\" methods, on the other hand, use the entire graph for training without information loss but suffer from huge communication overhead and poor efficiency. Hence, it is highly imperative to develop a method that can jointly address the problems of high communication cost and severe information loss. Moreover, theoretical guarantees (e.g., on convergence, approximation error) are not well explored for distributed GNN training due to the joint sophistication of graph structure and neural network optimization.\n\nTo address the aforementioned challenges, we propose a novel distributed GNN training framework that synergizes the complementary strengths of both partitioning-based and propagating-based methods, named DIstributed Graph reprEsentation SynchronizaTion, or DIGEST. DIGEST does not completely discard node information from other subgraphs in order to avoid unnecessary information loss; DIGEST does not frequently update all the node information in order to minimize communication costs. Instead, DIGEST extends the idea of single-GPU-based GCN training with stale representations (Chen et al., 2018; Fey et al., 2021) to a distributed setting, by enabling each device to efficiently exchange a relatively stale version of the neighbor representations from other subgraphs, to achieve scalable and high-performance GNN training. This effectively avoids neighbor updating explosion and reduces communication costs across training devices. Considering naive synchronous distributed training that inherently lacks the capability of handling stragglers caused by training environment heterogeneity (e.g. GPU resource heterogeneity), we further design an asynchronous version of DIGEST (DIGEST-A), where each subgraph follows a non-blocking training manner. The synchronous version is a natural generalization of Fey et al. (2021) while the asynchronous version can handle the straggler issue (Chen et al., 2016; Zheng et al., 2017) in synchronous version and enjoys even better performance. From the system aspect, DIGEST (1) enables efficient, cross-device representation exchanging by using a shared-memory key-value storage (KVS) system, (2) supports both synchronous and asynchronous parameter updating, and (3) overlaps the computation (layer training) with I/Os (pushing/pulling representations to/from the KVS.\n\nFurthermore, we proved that the approximation error induced by the staleness of representation can be bounded. More importantly, global convergence guarantee is provided, which demonstrates that DIGEST has the state-of-the-art convergence rate. Our main contributions can be summarized as: • Proposing a novel distributed GNN training framework that synergizes the benefits of partition-based and communication-based methods. Existing work in distributed GNN training\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nfocus on two contradictory objectives: partition-based methods target minimizing the communication cost while propagation-based methods aim to minimize information loss. DIGEST drops no edges while avoiding communication overhead by integrating the strengths of both categories. • Developing a periodic stale representation synchronization technique for distributed GNN training. DIGEST utilizes the entire graph for training by separating in- and out-of-subgraph neighbor nodes and approximating the latter with stale representations. Instead of making strictly synchronous pull/push operations for the representations of all layers before/after training, DIGEST overlaps pull/push operations with layer training to minimize the overall training time. Furthermore, a shared-memory-based KVS is used among subgraphs for efficiently exchanging representations. • Providing extensive theoretical guarantee on both performance and convergence of the proposed algorithm. We proved that DIGEST’s convergence rate is O(T −2/3M −1/3) with T iterations and M subgraphs, which is close to vanilla distributed GNN training without staleness. Convergence guarantee for both synchronous and asynchronous versions of DIGEST is provided. We also showed the upper bound on the approximation error of gradients due to the staleness. • Conducting comprehensive empirical results on both performance and speedup. We perform extensive evaluation on four benchmark with classic GNNs (e.g., GCN (Hamilton et al., 2017) and GAT (Veliˇckovi ́c et al., 2017)). The experimental results show that for the best case DIGEST improves the performance by 33.14%, and achieves 21.82× speedup in training time compared to two state-of-the-art distributed GNNs training frameworks.\n\n2 BACKGROUND AND PROBLEM FORMULATION\n\nIn this section, we first introduce the Graph Neural Network (GNN) and its training on a single machine, and then formulate the problem of distributed GNN training.\n\nGraph Neural Networks. GNNs aim to learn a function of signals/features on a graph G(V, E) with node representations X ∈ R|V|×d, where d denotes the node feature dimension. For typical semisupervised node classification tasks (Kipf & Welling, 2016), where each node v ∈ V is associated with a label yv, a L-layer GNN f is trained to learn the node representation hv such that yv can be predicted accurately. The training process of a GNN can be practically described as the node representation learning based on the message passing mechanism (Gilmer et al., 2017). Analytically, given a graph G(V, E) and a node v ∈ V, the (l + 1)-th layer of the GNN is defined as\n\nv\n\nh(l)\n\n,\n\n(1)\n\nh(l+1)\n\nv , (cid:8)h(l)\n\nu : u ∈ N (v)(cid:9)(cid:17)(cid:17)\n\n= f (l+1)(cid:16)\n\nv , Φ(l+1)(cid:16)(cid:8)h(l) h(l) v denotes the representation of node v in the l-th layer, and h(0)\n\nu : u ∈ N (v)(cid:9)(cid:17)\n\n= Ψ(l+1)(cid:16)\n\nwhere h(l) v being initialized to xv (v-th row in X), and N (v) represents the set of direct 1-hop neighbors for node v. Each layer of the GNN, i.e. f (l), can be further decomposed into two components: 1) Aggregation function Φ(l), which takes the nodes representations of node v’s neighbors as input, and output the aggregated neighborhood representation. 2) Updating function Ψ(l), which combines the representation of v and the aggregated neighborhood representation to update the representation of node v for the next layer. Both Φ(l) and Ψ(l) can choose to use various functions in different types of GNNs. To train a GNN on a single machine, one can minimize the empirical loss L(W) over the entire graph in the (cid:1), where Loss(·, ·) denotes a loss function training data, i.e., L(W) = (1/|V|) (cid:80) (e.g., cross entropy loss), and h(L) denotes the representation of node v from the last layer of the GNN and can be calculated by following Eq. 1 recursively.\n\nv∈V Loss(cid:0)h(L)\n\n, yv\n\nv\n\nv\n\nDistributed Training for GNNs. Distributed GNN training means to first partition the original graph into multiple subgraphs without overlap, which can also be considered as mini batches. Then different mini-batches are trained in different devices in parallel. Here, Eq. 1 can be further reformulated as u : u ∈ N (v) ∩ S(v)(cid:9) (cid:125) (cid:123)(cid:122) In-subgraph nodes\n\nu : u ∈ N (v) \\ S(v)(cid:9) (cid:125) (cid:123)(cid:122) Out-of-subgraph nodes\n\nv , Φ(l+1)(cid:16) (cid:8)h(l) h(l)\n\n∪ (cid:8)h(l) (cid:124)\n\n= Ψ(l+1)(cid:16)\n\nh(l+1)\n\n(cid:17)(cid:17)\n\n(2)\n\n(cid:124)\n\nv\n\n,\n\nwhere S(v) denotes the subgraph that node v belongs to. In this paper, we consider the distributed training of GNNs with multiple local machines and a global server. The original input graph G is first partitioned into M subgraphs, where each Gm(Vm, Em) represents the subgraph m. Our goal is to find the optimal set of parameters W in a distributed manner by minimizing each local loss, i.e., Loss(cid:0)h(L)\n\n(cid:1), m = 1, 2, · · · , M in parallel,\n\nminW LLocal\n\n(cid:0)Wm\n\n(cid:1) =\n\n, yv\n\n(cid:88)\n\n(3)\n\nm\n\nv\n\n1 |Vm|\n\nv∈Vm\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Distributed GNN training methods. (a): Propagation-based methods rely on communication of out-of-subgraph neighbor nodes for exact message passing even in a distributed setup. (b): Partition-based methods decompose the original problem into multiple smaller ones and directly apply data parallelism onto partitioned subgraph data. (c): In DIGEST each device utilizes the stale representations of all its neighbors from other subgraphs. Propagation-based methods suffers high communication cost (red vertical double arrows in (a)) due to neighbor explosion, while partitionbased methods suffer severe information loss due to dropped edges (red crosses in (b)). DIGEST combines the best of both worlds. ALL nodes are utilized in DIGEST to achieve full-graph awareness, while periodic stale representation synchronization keeps the communication cost low.\n\nwhere Wm = {W(l)\n\nm }L\n\nl=1 are local parameters and h(L)\n\nv\n\nfollows Eq. 2 recursively.\n\nChallenges. The main challenges for distributed training of GNNs lie in the trade-off between communication cost and information loss. \"Partition-based\" method generalizes the existing data parallelism techniques of classical distributed training on i.i.d data to graph data and enjoys minimal communication cost. However, directly partitioning a large graph into multiple subgraphs can result in severe information loss due to the ignorance of huge number of cross-subgraph edges and cause performance degeneration (Angerd et al., 2020; Jia et al., 2020; Ramezani et al., 2021). For these methods, the representation of neighbors out of the current subgraph (second representation set in Eq. 2) are dropped and the connections between subgraphs are thus ignored. Hence, another line of work (Wang et al., 2019), namely \"propagation-based\" method considers using communication of neighbor nodes for each subgraph to satisfy GNN’s neighbor aggregation, which minimizes the information loss. As shown in Eq. 2, the representations for neighbor nodes outside the current subgraph is swapped between different subgraphs. However, the number of neighbors involved in the neighbor aggregation process expands exponentially as the GNN model goes deep, which is known as the neighborhood explosion problem. Hence, though no edges are dropped in this case, inevitable communication overhead is incurred and plagues the achievable training efficiency (Ma et al., 2019; Zhu et al., 2019; Zheng et al., 2020; Tripathy et al., 2020; Wan et al., 2022). Moreover, theoretical guarantees (e.g., on convergence, approximation error) are not well explored for distributed GNN due to the joint sophistication of graph structure and neural network optimization.\n\n3 PROPOSED METHOD\n\nIn this section, we introduce the proposed GNN training framework DIGEST. DIGEST leverages both types of representations in Eq. 2 to address the information loss issue. In addition, instead of exchanging real-time representations during the training process between the subgraphs, DIGEST only pull and push the stale representations before or after each step of training periodically. With this strategy, the communications turn to be more efficient, which are illustrated in Figure 1 and analyzed in more details in Section 3.3. Moreover, we prove that the error introduced by the staleness of the stale representation is upper-bounded while the convergence is also guaranteed.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.1 DISTRIBUTED GNN TRAINING WITH FULL-GRAPH AWARENESS\n\nIn DIGEST, each copy of GNN trained on a local machine will make use of all available graph information, i.e. no edges are dropped in both forward and backward propagation. Analytically, calculating each local gradient ∇LLocal m as defined in Eq. 3 will involve out-of-subgraph neighbor information. For out-of-subgraph neighbor nodes, we approximate their representations via stale representations acquired in previous training, denoted by ̃h(l) v . Formally, given a node v ∈ Gm(Vm, Em), the forward propagation for the (l + 1)-th layer of DIGEST is achieved by modifying Eq. 2 as (cid:17)(cid:19)\n\n(cid:18)\n\n(cid:111)\n\n(cid:111)\n\nv , Φ(l+1)(cid:16) (cid:110) h(l)\n\nh(l)\n\nu : u ∈ N (v) ∩ Vm\n\n(cid:110) ̃h(l)\n\n∪\n\nh(l+1)\n\nv\n\n= Ψ(l+1)\n\nu : u ∈ N (v) \\ Vm (cid:123)(cid:122) Stale representation\n\n(cid:125)\n\n(cid:124)\n\n.\n\n(4)\n\nAs can be seen, DIGEST considers ALL neighbor nodes information during forward propagation. On the other hand, leveraging the entire graph data in forward propagation will in turn improve the estimation of gradient in backpropagation. To see this, we reformulate Eq. 4 into the matrix form:\n\nH(l+1,m)\n\nin\n\n= F\n\n(cid:16)\n\nH(l,m)\n\nin\n\n, ̃H(l,m)\n\nout\n\n(cid:17)\n\n(cid:16)\n\n:= σ\n\nP(m)\n\nin H(l,m)\n\nin W(l+1)\n\nm + P(m)\n\nout\n\n ̃H(l,m)\n\nout W(l+1)\n\nm\n\n(cid:17)\n\n,\n\n(5)\n\nin\n\nout\n\nand ̃H(l,m)\n\nwhere H(l,m) denotes the matrix of in-subgraph node representations and out-of-subgraph stale representations at l-th layer on subgraph Gm, respectively. F denotes the forward propagation function of one layer of GNN for compact formula. We consider the GCN model as an example for illustration but our analyses apply to general cases of any GNN models. P(m) out denotes the propagation matrix for in-subgraph nodes and out-of-subgraph nodes of Gm, respectively, and we have Pm = P(m) out where Pm is the original propagation matrix for subgraph Gm. σ(·) is the activation function following GCN’s definition. Hence, the gradient over model parameters is\n\nin and P(m)\n\nin + P(m)\n\n∂ ∂W(l+1) (cid:104) P(m)\n\n=\n\nm\n\n(cid:16)\n\nF\n\nH(l,m)\n\nin\n\n, ̃H(l,m)\n\nout\n\n(cid:17)\n\n=\n\n(cid:16)\n\nσ\n\nP(m)\n\nin H(l,m)\n\nin W(l+1)\n\nm + P(m)\n\nout\n\n ̃H(l,m)\n\nout W(l+1)\n\nm\n\n(cid:17)\n\n(6)\n\nin H(l,m)\n\nin + P(m)\n\nout\n\n ̃H(l,m)\n\nout\n\nP(m)\n\nin H(l,m)\n\nin W(l+1)\n\nm + P(m)\n\nout\n\n ̃H(l,m)\n\nout W(l+1)\n\nm\n\n(cid:17)\n\n.\n\n∂ ∂W(l+1) σ′(cid:16)\n\nm\n\n(cid:105)⊤\n\nThe key observation here is that ALL neighbor nodes are involved in the backpropagation since the gradient above depends on ̃H(l,m) . The separation of in-subgraph nodes and out-of-subgraph nodes, and their approximation via stale representation form the very foundation of DIGEST.\n\nout\n\n3.2 SYSTEM DESIGN\n\nThis section presents the overall system design of DIGEST as depicted in Figure 1. DIGEST maintains a shared-memory-based KVS for storing and retrieving representations. KVS can be easily extended to a truly distributed storage to support large-scale distributed training spanning multiple servers.\n\nv\n\nout = { ̃h(l)\n\nWe first introduce two operations used by DIGEST to store and retrieve representations. The stale representations of layer l for all nodes in V can be formulated as ̃H(l) = { ̃h(l) : v ∈ V}. For any subgraph Gm to start the forward process of layer l, the necessary stale representations ̃H(l,m) u : u ∈ N (v) \\ Vm, ∀ v ∈ Vm} are pulled from the KVS that stores representations; this is called a \"pull\" operation denoted as H(l,m) . See Figure 1(c). After the end of a epoch, the newly-computed representations H(l,m) : ∀ v ∈ Vm} are pushed to the KVS, and these newly-stored representations will be fetched as stale representations in future epochs; this in → ̃H(l,m) is called a \"push\" operation denoted as H(l,m) DIGEST features two training modes: (1) DIGEST: a synchronous mode designed ideal for homogeneous training environments. (2) DIGEST-A: an asynchronous mode that better fits for a heterogeneous training environment. DIGEST and DIGEST-A follow different parameter and representation updating strategies. In DIGEST, for each global round, before fetching the aggregated parameters and pulling the stale representations, each subgraph has to wait for other subgraphs to finish updating the latest parameters to the parameter server (PS) and their local representations to the KVS. However, some subgraphs may have lower computing resource compared to other\n\nout ← ̃H(l,m) in = {h(l)\n\nout\n\nin\n\nv\n\n.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nsubgraphs, which we call stragglers. This may lead to imbalanced local training times. In this case, with the synchronous mode, the overall training process can be bottlenecked by the slowest subgraph, therefore suffering from prolonged training time. To address this issue, DIGEST-A applies an asynchronous, non-blocking strategy, where each subgraph directly pulls/pushes stale representations of other subgraphs from the shared KVS and downloads/uploads parameters from the PS without blindly waiting for the slowest subgraph to finish. For better scalability, we will explore disaggregated storage techniques (Klimovic et al., 2016; Nanavati et al., 2017; Amaro et al., 2020) as part of our future work, where DIGEST can utilize a network-attached, high-performance far memory storage system for representation storage and retrieval. We summarize our algorithm in Algorithm 1 in the appendix due to limited space.\n\nout\n\nIn addition, DIGEST and DIGEST-A use several optimizations to minimize the I/O overhead introduced by pulls and pushes. First, we observe that there are a large number of node representations involved in both pull and push operations, and more importantly, nodes are independent of each other on these two operations. Hence, it is inherently suitable for parallel I/O at the granularity of node level. For subgraph Gm, the total number of stale representations needed to be pulled from the KVS is | ̃H(l,m) |. Assume that it takes time t to pull the stale representation of one node, the total time cost should be | ̃H(l,m) | × t if being pulled in serial. But with parallel I/O where needed representations are pulled in parallel, theoretically we can still keep the pull time for v ∈ Vm as t. Additionally, we observe that the pull operation for ̃H(l,m) can be overlapped with the forward process of layer l − 1; similarly, the push operation for H(l,m) can be overlapped with the forward process of layer l + 1. The training process on each subgraph is depicted in Figure 2. The cost of pull/push operations is hidden by the layer forward process, therefore, is eliminated.\n\nout\n\nout\n\nin\n\nSecond, to further reduce the I/O overhead, DIGEST uses a periodic representation synchronization strategy, which pushes updated representations to the KVS once every N epoches. This introduces a trade-off in I/O overhead and training performance. Increasing the frequency of the periodic synchronization will benefit performance, but this will introduce more I/O overhead. We analyze this trade-off in Section 5.2.\n\n3.3 COMPLEXITY ANALYSES\n\nFigure 2: Illustration of DIGEST’s concurrent pull/push and forward propagation operations on a 3-layer GNN.\n\nHere we analyze the memory and communication complexity of DIGEST. DIGEST pulls the required out-of-subgraph node representations and keeps them locally for each local machine. For the m-th local machine and the corresponding subgraph on it, i.e. Gm(Vm, Em), the memory complexity per training iteration is O(cid:0) (cid:12) (cid:12) Ld(cid:1). which scales linearly with respect to the (cid:12) number of GNN layers. DIGEST’s communication cost per round can be expressed as O(cid:0)M Ld2 + (cid:12) (cid:12) Ld + N Ld(cid:1). Again, the communication cost of DIGEST is only linear (cid:80)M\n\nN (v) ∪ {v}(cid:12)\n\nN (v) \\ Vm\n\n(cid:12) (cid:12)(cid:83)\n\nv∈Vm\n\n(cid:83)\n\nm=1\n\nv∈Vm\n\nwith respect to GNN depth L.\n\n4 THEORETICAL ANALYSES\n\nIn this section, we provide theoretical analyses of the propose distributed strategy DIGEST, including the bound of error induced by the staleness of node representations, and convergence guarantee for DIGEST under both synchronous and asynchronous settings. All proofs can be found in the appendix.\n\n4.1 ERROR BOUND ON GLOBAL APPROXIMATED GRADIENTS\n\nOur first theorem shows that under the distributed setting, the approximation error of the global model’s gradients can be upper bounded by the staleness of node representations.\n\nTheorem 1. Given a L-layer GNN fW with r1-Lipschitz smooth Φ and r2-Lipschitz smooth Ψ. Denote ∆(G) as the maximal node degree for graph G. Assume ∀ v ∈ V and ∀ l ∈ {1, 2, · · · , L − 1}\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nv − ̃h(l)\n\nv ∥ ≤ ε(l), where h(l)\n\nwe have ∥h(l) v and ̃h(l) by DIGEST and the stale one, respectively. Further assume each local loss function LLocal τ -Lipschitz smooth w.r.t the node representation. Then, we have that (cid:13) (τ /M ) (cid:80)L−1 dient computed by DIGEST and the exact global gradient without any staleness.\n\nv denotes the node representation computed m is (cid:13)2 ≤ m=1 |∆(Gm)|L−l, where ∇WL and ∇WL∗ denotes the global gra-\n\n(cid:13)∇WL − ∇WL∗(cid:13)\n\nl=1 ε(l)rL−l\n\nrL−l\n\n(cid:80)M\n\n2\n\n1\n\n4.2 CONVERGENCE OF SYNCHRONOUS DIGEST\n\nv\n\nv\n\nv − h(L)\n\n1 ) − σ(Z (l)\n\n, yv) − Loss(h(L)\n\n, yv) − ∇Loss(h(L)\n\nw ∥2 and ∥∇Loss(h(L)\n\nAs both fresh inner-subgraph and stale out-of-subgraph representations are adopted in our algorithm, its convergence rate is still unknown. We have proved the convergence of DIGEST and present the convergence property in the theorem blow. First, we introduce some assumptions: Assumption 1. The loss function Loss(·, ·) is CLoss-Lipchitz continuous and LLoss-Lipschitz smooth with respect to the last layer’s node representation, i.e., |Loss(h(L) w , yv)| ≤ CLoss∥h(L) Assumption 2. The activation function σ(·) is Cσ-Lipchitz continuous and Lσ-Lipschitz smooth, i.e. 2 ∥2 and ∥σ′(Z (l) ∥σ(Z (l) 2 ∥2. Assumption 3. ∀ l = 1, 2, · · · , L, we have ∥W (l)∥F ≤ KW , ∥P (l)∥P ≤ KW , ∥X (l)∥F ≤ KX . Theorem 2. Consider GCN with L layers that is Lf -Lipschitz smooth. ∀ ε > 0, ∃ constant E and number of training iterations E > 0 such that, we can choose a learning rate η = ε− 3 T = (L(W(1)) − L(W∗)) E√ t=1 ∥∇L(W(t))∥2 ≤ O(T −2/3M −1/3), where M\nW∗ denotes the optimal parameter. Our convergence rate of DIGEST is O(T −2/3M −1/3), which is better than pipeline-parallelism method O(T −2/3) (Wan et al., 2022) and sampling-based method O(T −1/2) (Chen et al., 2018; Cong et al., 2021), and very close to full-graph training O(T −1).\n\nw , yv)∥2 ≤ LLoss∥h(L)\n\n2 )∥2 ≤ Cσ∥(Z (l)\n\n2 )∥2 ≤ Lσ∥(Z (l)\n\n2 s.t., T −1 (cid:80)T\n\n1 ) − σ′(Z (l)\n\n1 − Z (l)\n\n1 − Z (l)\n\nv − h(L)\n\nw ∥2.\n\nM ε\n\n√\n\n4.3 CONVERGENCE OF ASYNCHRONOUS DIGEST\n\n2, where V and β are positive real numbers, i.e., V, β ∈ R+.\n\nConvergence for asynchronous distributed algorithms could be even harder to obtain due to the delay in parameter’s update (the global model’s parameters may have been updated several times when the slowest local machine finishes its computation.) Our main result is shown below: Assumption 4. ∀ m ∈ [M ], ∥∇ ̃Lm(W )∥2 ≤ V · ∥∇L(W )∥2, and (cid:10)∇L(W ), ∇ ̃Lm(W )(cid:11) ≥ β · ∥∇L(W )∥2 Theorem 3. Assume the global model L(W ) is Cf -Lipschitz continuous and the delay is bounded, i.e., τ < K. Further, assume β − V 2 2 > 0. There exist constant B and a second-order polynomial of learning rate η, i.e., P (η) such that after T global iterations on the server, asynchronous DIGEST (cid:13)∇L(W(t))(cid:13) (cid:13) 2\nconverges to the optimal parameter W∗ by T −1 (cid:80)T 2 ≤ (ηT B)−1(L(W(1)) − (cid:13) L(W(∗))) + P (η)/B, where B = β − V 2 f + (1 + V )ηKC 2 f L2 2 η2K 2C 2\n\n2 and P (η) = 1\n\nf Lf .\n\nt=1\n\n5 EXPERIMENTS\n\nIn this section, we evaluate DIGEST and compare DIGEST against two state-of-the-art distributed GNNs training frameworks as baselines in terms of training efficiency and scalability. Considering the distinct training time per epoch between DIGEST and other baselines, we report the F1 scores on validation dataset and training loss over training time, instead of over communication rounds, in the results. This way it makes a fairer comparison in terms of training performance and efficiency.\n\n5.1 EXPERIMENT SETTING\n\nImplementation and Setup. We have implemented DIGEST and other comparison GNNs training methods all in PyTorch (Paszke et al., 2019). For all the experiments, we simulate a distributed training environment using an EC2 g4dn.metal virtual machine (VM) instance on AWS, which has 8 NVIDIA T4 GPUs, 96 vCPUs, and 384 GB main memory. We implemented the shared-memory KVS using the Plasma in-memory object store3 for representation storage and retrieval.\n\n3https://arrow.apache.org/docs/python/plasma.html\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) OGB-Arxiv\n\n(b) Flickr\n\n(c) Reddit\n\n(d) OGB-Products\n\nFigure 3: Performance comparison of the GCN training frameworks on four benchmark datasets. The top four subfigures show the training loss over training time, and the bottom four subfigures show the global validation F1 scores during the whole training process. (Best viewed in color.)\n\nTable 1: Performance comparison of distributed GNNs frameworks. F1 score on validation dataset reported. Speedup is calculated by normalizing per-epoch training time against that of DGL.\n\nMethod\n\nLLCG\n\nDGL\n\nDIGEST\n\nDIGEST-A\n\nMetric\n\nF1 Speedup F1 Speedup F1 Speedup F1\n\nGCN\n\nOGB-Arxiv 69.8 ± 0.21 2.35× 69.9 ± 0.17 1× 72 ± 0.23 17.41× 71.9 ± 0.16\n\nFlickr 50.73 ± 0.15 0.88× 50.9 ± 0.13 1× 53.78 ± 0.21 11.06× 53.1 ± 0.32\n\nReddit 62.09 ± 0.41 1.47× 87.02 ± 0.23 1× 95.23 ± 0.43 7.86× 94.55 ± 0.37\n\nOGB-Products 90.79 ± 0.16 1.396× 91.01 ± 0.12 1× 91.55 ± 0.1 3.096× 91.54 ± 0.1\n\nOGB-Arxiv 68.84 ± 0.22 1.787× 70.34 ± 0.17 1× 68.35 ± 0.41 11.49× 69.04 ± 0.13\n\nGAT Flickr 43.98 ± 0.32 0.923× 51.50 ± 0.27 1× 52.08 ± 0.21 6.591× 52.16 ± 0.17\n\nReddit 91.1 ± 0.17 9.956× 92.58 ± 0.12 1× 94.19 ± 0.15 21.817× 93.95 ± 0.22\n\nBaselines. Recall in Section 2 we categorize existing distributed GNN training into two types of general methods. In evaluation, we choose two state-of-the-art distributed training frameworks, one from each category as the baseline. For the first category, we choose LLCG (Ramezani et al., 2021), which partitions a graph into subgraphs and trains each subgraph strictly independently without incurring any communication among subgraphs. LLCG uses a central server to aggregate local models from each device and performs global training using mini-batches with full neighbor information to ensure that the model learns the global structure of the graph. LLCG uses this additional step to reduce the information loss caused by graph partitioning. For the second category, we choose to use DGL (Wang et al., 2019), which is a commonly-used, distributed GNN training framework. In contrast to LLCG, DGL requires exchanging node representations among partitioned subgraphs. DGL requires frequent swap operations with other subgraphs for representations during subgraph’s local training in each epoch, and therefore, DGL incurs high communication cost.\n\nFigure 4: Training time/epoch.\n\nFigure 5: Scalability.\n\nFigure 6: Synchronization interval. Better seen in color.\n\n5.2 EXPERIMENTAL RESULTS\n\nIn this section, we evaluate both sync & async versions of DIGEST, LLCG, and DGL on the four datasets. Due to the page limit, we move parts of our evaluation results to the Appendix.\n\n8\n\n1.01.52.02.53.03.5Training Loss050100150200250300Training Time(s)0.500.550.600.650.700.75Validation F1LLCGDGLDIGESTDIGEST-A1.01.21.41.61.82.02.2Training Loss050100150200Training Time(s)0.300.350.400.450.500.550.60Validation F1LLCGDGLDIGESTDIGEST-A012345Training Loss025050075010001250Training Time(s)0.00.20.40.60.81.0Validation F1LLCGDGLDIGESTDIGEST-A0.00.51.01.52.02.5Training Loss02004006008001000Training Time(s)0.800.820.840.860.880.900.920.94Validation F1LLCGDGLDIGESTDIGEST-AOGB-ArxivFlickrRedditOGB-Products0246810Time/epoch(s)LLCGDGLDIGEST4567# of GPUs46810121416SpeedupLLCGDGLDIGEST0200400600800100012001400Training Time(s)0.7500.7750.8000.8250.8500.8750.9000.9250.950Validation F111020Under review as a conference paper at ICLR 2023\n\nEfficiency of DIGEST. We first evaluate the training performance of DIGEST and DIGEST-A. As shown in Figure 3, DIGEST outperforms both LLCG and DGL for all the datasets when performing distributed training on a pure GCN. LLCG performs worst particularly for the Reddit dataset, because in the global server correction of LLCG, only a mini-batch is trained and it is not sufficient to correct the plain GCN. This is also the reason why the authors of LLCG report the performance of a complex model with mixing GCN layers and GraphSAGE layers Ramezani et al. (2021). DGL achieves good performance on some dataset (e.g., OGB-products) with uniform node sampling strategy and realtime representation exchanging. However, frequent communication also leads to slow performance increasing for dataset Flickr (Figure 3(b)) and poor performance for all four datasets. DIGEST and DIGEST-A avoid these issues and therefore achieve satisfying performance over the training time. DIGEST-A is slowly catching up DIGEST due to the diverse model parameters used by subgraphs in the early training period.\n\nWe measure the training time per epoch as shown in Figure 4. Since the representation synchronization is only performed before the start or after the end of local training, DIGEST takes significantly shorter training time per epoch than that of LLCG and DGL. Furthermore, DIGEST performs periodic synchronization instead of per-epoch synchronization, which further shortens the training time.\n\nTable 1 presents the detailed numbers for the comparison of three frameworks on the four datasets. For all the cases except GAT on OGB-Arxiv, DIGEST achieves leading F1 scores on the validation dataset, demonstrating the efficacy of DIGEST’s design.\n\nScalability of DIGEST. We evaluate the scalability of three frameworks by training a GCN on OGB-Products with varied number of GPUs. We use average training time per epoch against that of DGL with a single GPU to calculate the speedup results. As shown in Figure 5, DIGEST shows the best scalability compared to the other two. The speedup rises with the number of GPUs used during training. We observe a similar trend for DGL, but the relative speedup for DGL is significantly smaller than that for DIGEST, due to the using of real-time representations instead of stale representations.\n\nSynchronization frequency. We next perform a sensitivity analysis by varying the synchronization intervals for OGBProducts to study how the synchronization frequency would affect the training performance. As shown in Figure 6, DIGEST achieves the highest F1 score over training time when configured to perform synchronization of stale representations every 10 epochs. A large interval (20) or a small interval (1) results in performance degradation, due to the long term loss of graph information or additional communication cost.\n\nFigure 7: Performance comparison of OGB-Products when trained in a heterogeneous environment.\n\nTraining in heterogeneous environment. Finally, we test DIGEST’s asynchronous training mode. As stated in Section 3.2, asynchronous training is better suited for GNN training in heterogeneous environments. In this test, we randomly select one subgraph as the straggler before the training starts. To simulate the straggler lagging caused by limited computing capability, a random delay ranging from 8 to 10 seconds is added to the chosen straggler during the whole training process. We can see from Figure 7 that DIGEST-A performs much better than other three synchronous methods and converge to high F1-score at the early stage of the training. This is because asynchronous mode effectively eliminates GPU’s blocking caused by waiting with significantly improved GPU utilization.\n\n6 CONCLUSION\n\nThere are two general categories in distributed GNN training. Partition-based methods suffer from graph information loss, while propogation-based methods suffer from high communication cost. In this work we present DIGEST, a novel distributed GNN training framework that synergizes the complementary strengths of both methods by leveraging stale representations intelligently. We provide rigorous theoretical analysis to prove that DIGEST has competitive convergence rate and bounded error due to staleness. Extensive experiments on four benchmark datasets validate our analysis and demonstrate the efficiency and scalability of DIGEST.\n\n9\n\n05001000150020002500Training Time(s)0.820.840.860.880.900.920.94Validation F1LLCGDGLDIGESTDIGEST-AUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nEmmanuel Amaro, Christopher Branner-Augmon, Zhihong Luo, Amy Ousterhout, Marcos K. Aguilera, Aurojit Panda, Sylvia Ratnasamy, and Scott Shenker. Can far memory improve job throughput? In Proceedings of the Fifteenth European Conference on Computer Systems, EuroSys ’20, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450368827. doi: 10.1145/3342195.3387522. URL https://doi.org/10.1145/3342195.3387522.\n\nAlexandra Angerd, Keshav Balasubramanian, and Murali Annavaram. Distributed training of graph convolutional networks using subgraph approximation. arXiv preprint arXiv:2012.04930, 2020.\n\nNiel Chah. Freebase-triples: A methodology for processing the freebase data dumps. arXiv preprint\n\narXiv:1712.08707, 2017.\n\nZheng Chai, Yujing Chen, Ali Anwar, Liang Zhao, Yue Cheng, and Huzefa Rangwala. Fedat: a high-performance and communication-efficient federated learning system with asynchronous tiers. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–16, 2021.\n\nJianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with variance reduction. In International Conference on Machine Learning, pp. 942–950. PMLR, 2018.\n\nJianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting distributed\n\nsynchronous sgd. arXiv preprint arXiv:1604.00981, 2016.\n\nYujing Chen, Yue Ning, Martin Slawski, and Huzefa Rangwala. Asynchronous online federated learning for edge devices with non-iid data. In 2020 IEEE International Conference on Big Data (Big Data), pp. 15–24. IEEE, 2020.\n\nWei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 257–266, 2019.\n\nSangjin Choi, Taeksoo Kim, Jinwoo Jeong, Rachata Ausavarungnirun, Myeongjae Jeon, Youngjin Kwon, and Jeongseob Ahn. Memory harvesting in Multi-GPU systems with hierarchical unified virtual memory. In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pp. 625–638, ISBN 978-1-939133-29-66. URL https: Carlsbad, CA, July 2022. USENIX Association. //www.usenix.org/conference/atc22/presentation/choi-sangjin.\n\nWeilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On the importance of sampling in learning\n\ngraph convolutional networks. arXiv preprint arXiv:2103.02696, 2021.\n\nHanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for structured\n\ndata. In International conference on machine learning, pp. 2702–2711. PMLR, 2016.\n\nChantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma, Charles Sugnet, Mark Ulrich, and Jure Leskovec. Pixie: A system for recommending 3+ billion items to 200+ million users in real-time. In Proceedings of the 2018 world wide web conference, pp. 1775–1784, 2018.\n\nMatthias Fey, Jan E Lenssen, Frank Weichert, and Jure Leskovec. Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings. In International Conference on Machine Learning, pp. 3294–3304. PMLR, 2021.\n\nSwapnil Gandhi and Anand Padmanabha Iyer. P3: Distributed deep graph learning at scale. In 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21), pp. 551–568. USENIX Association, July 2021. ISBN 978-1-939133-22-9. URL https://www.usenix. org/conference/osdi21/presentation/gandhi.\n\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n\nAdvances in neural information processing systems, 30, 2017.\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020.\n\nChien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS ’20, pp. 1341–1355, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450371025. doi: 10.1145/3373376.3378530. URL https://doi.org/10.1145/ 3373376.3378530.\n\nZhihao Jia, Sina Lin, Mingyu Gao, Matei Zaharia, and Alex Aiken.\n\nImproving the accuracy, scalability, and performance of graph neural networks with roc. Proceedings of Machine Learning and Systems, 2:187–198, 2020.\n\nGeorge Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning irregular\n\ngraphs. SIAM Journal on scientific Computing, 20(1):359–392, 1998.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.\n\narXiv preprint arXiv:1609.02907, 2016.\n\nAna Klimovic, Christos Kozyrakis, Eno Thereska, Binu John, and Sanjeev Kumar. Flash storage disaggregation. In Proceedings of the Eleventh European Conference on Computer Systems, EuroSys ’16, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450342407. doi: 10.1145/2901318.2901337. URL https://doi.org/10.1145/2901318.2901337.\n\nKai Lei, Meng Qin, Bo Bai, Gong Zhang, and Min Yang. Gcn-gan: A non-linear temporal link prediction model for weighted dynamic networks. In IEEE INFOCOM 2019-IEEE Conference on Computer Communications, pp. 388–396. IEEE, 2019.\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429–450, 2020.\n\nLingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu, Lidong Zhou, and Yafei Dai. In 2019 USENIX\n\n{NeuGraph}: Parallel deep neural network computation on large graphs. Annual Technical Conference (USENIX ATC 19), pp. 443–458, 2019.\n\nMihir Nanavati, Jake Wires, and Andrew Warfield. Decibel: Isolation and sharing in disagIn 14th USENIX Symposium on Networked Systems Design gregated Rack-Scale storage. and Implementation (NSDI 17), pp. 17–33, Boston, MA, March 2017. USENIX Association. ISBN 978-1-931971-37-9. URL https://www.usenix.org/conference/nsdi17/ technical-sessions/presentation/nanavati.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nXuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma, Qian Xiong, Fan Yang, and Xuehai Qian. Capuchin: Tensor-based gpu memory management for deep learning. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS ’20, pp. 891–905, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450371025. doi: 10.1145/3373376.3378505. URL https://doi.org/10.1145/3373376.3378505.\n\nMorteza Ramezani, Weilin Cong, Mehrdad Mahdavi, Mahmut T Kandemir, and Anand Sivasubramaniam. Learn locally, correct globally: A distributed algorithm for training graph neural networks. arXiv preprint arXiv:2111.08202, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJohn Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng, Guanzhou Hu, Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim, and Guoqing Harry Xu. Dorylus: Affordable, scalable, and accurate GNN training with distributed CPU servers and serverless threads. In 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21), ISBN 978-1-939133-22-9. URL https: pp. 495–514. USENIX Association, July 2021. //www.usenix.org/conference/osdi21/presentation/thorpe.\n\nAlok Tripathy, Katherine Yelick, and Aydın Buluç. Reducing communication in graph neural network In SC20: International Conference for High Performance Computing, Networking,\n\ntraining. Storage and Analysis, pp. 1–14. IEEE, 2020.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\n\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\nCheng Wan, Youjie Li, Cameron R Wolfe, Anastasios Kyrillidis, Nam Sung Kim, and Yingyan Lin. Pipegcn: Efficient full-graph training of graph convolutional networks with pipelined feature communication. arXiv preprint arXiv:2203.10428, 2022.\n\nMinjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, et al. Deep graph library: A graph-centric, highly-performant package for graph neural networks. arXiv preprint arXiv:1909.01315, 2019.\n\nYuke Wang, Boyuan Feng, Gushu Li, Shuangchen Li, Lei Deng, Yuan Xie, and Yufei Ding. GNNAdvisor: An adaptive and efficient runtime system for GNN acceleration on GPUs. In 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21), pp. 515–531. USENIX Association, July 2021. ISBN 978-1-939133-22-9. URL https://www.usenix. org/conference/osdi21/presentation/wang-yuke.\n\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 974–983, 2018.\n\nHanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019.\n\nDa Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su, Xiang Song, Quan Gan, Zheng Zhang, and George Karypis. Distdgl: distributed graph neural network training for billion-scale graphs. In 2020 IEEE/ACM 10th Workshop on Irregular Applications: Architectures and Algorithms (IA3), pp. 36–44. IEEE, 2020.\n\nShuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and Tie-Yan Liu. Asynchronous stochastic gradient descent with delay compensation. In International Conference on Machine Learning, pp. 4120–4129. PMLR, 2017.\n\nRong Zhu, Kun Zhao, Hongxia Yang, Wei Lin, Chang Zhou, Baole Ai, Yong Li, and Jingren Zhou. Aligraph: a comprehensive graph neural network platform. arXiv preprint arXiv:1902.08730, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nIn this section, we clarify our contributions made in this work and describe detailed experimental setup, additional experimental results, and complete proofs. We reuse part of code adopted from GNNAutoScale Fey et al. (2021); our code is available at: https://anonymous.4open. science/r/DIGESTA-78F2/. Please note that the code is subjected to reorganization to improve the readability.\n\nA.1 CONTRIBUTIONS AND NOVELTY\n\nThe contributions and novelty of this work are multi-fold. In this paper, we propose a new, highlyparallel, and full-graph-aware distributed GNN training method; on top of this new method, we design a novel, compute-and-storage-disaggregated training system to enable better scalability and allow distributed GNN training to potentially benefit from emerging computing paradigms and hardware; finally, we deduce new theoretical guarantees and analyses for the co-designed algorithms and systems.\n\n(1). Methodology Novelty in Algorithm-System Co-design: Our paper is mainly motivated from a distributed training perspective, where the proposed framework synergizes the best of both partition-based and propagation-based distributed training; GNNAutoScale provides a theoretical foundation, which exposes potential opportunities that can be harnessed by and co-designed with new distributed training system infrastructures to enable highly-parallel GNN training. DIGEST goes beyond GNNAutoScale in that we built a novel distributed training framework that effectively decouples the management of state (i.e., representations) and compute (i.e., GNN training).\n\n(2). System Architecture Novelty: The disaggregated architecture of DIGEST is the result of an algorithm-system co-design as mentioned in the Methodology Novelty, and enables great properties including high scalability and low training time, as demonstrated in our paper. More importantly, this disaggregated architecture could enable fundamental opportunities for GNN training systems to take advantage of emerging computing paradigm such as elastic serverless computing as well as emerging hardware such as Zoned Namespace SSD (ZNS) and smart programmable network hardware (SmartNIC); in this work, we have shown the promising scalability and speedup that DIGEST offers, which establishes a solid system foundation for further system-level optimizations and innovations. This demands/inspires future research along the line, which we plan to do as part of our future work.\n\n(3). Theoretical Novelty: All of our theoretical analyses are tailored for a distributed training setup, while GNNAutoScale only considers single-GPU training.\n\nA.2 EXPERIMENTAL SETUP DETAILS\n\nAs mentioned in Section 5.1, all the experiments are done on an EC2 g4dn.metal virtual machine (VM) instance on AWS, which has 8 NVIDIA T4 GPUs, 96 vCPUs, and 384 GB main memory. Other important information including operation system version, Linux kernel version, and CUDA version is summarized in Table 2. For fair comparison, we use the same optimizer (Adam), learning rate, and graph partition algorithm for all the three frameworks, DIGEST, LLCG, and DGL. For parameters that are unique to both LLCG and DGL, such as the number of neighbors sampled from each layer for each node, we choose the default value for both LLCG and DGL. Each of the three frameworks has a set of parameters that are exclusively unique to that framework; for these exclusive parameters, we tune them in order to achieve the best performance. Please refer to the configuration files under run/conf/model for detailed configuration setups for all the models and datasets.\n\nTable 2: Summary of environmental setup of our testbed.\n\nOS Ubuntu 18.04\n\nLinux kernel CUDA\n\n5.4.0\n\n11.6\n\nDriver 510.47.03\n\nPyTorch 1.10.0\n\nPyTorch Geometric 2.0.4\n\nPyTorch Sparse 0.6.13\n\nWe use four datasets: OGB-Arxiv Hu et al. (2020), Flickr Zeng et al. (2019), Reddit Zeng et al. (2019), and OGB-Products Hu et al. (2020) for evaluation. The detailed information of these datasets is summarized in Table 3.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Summary of dataset statistics.\n\nDataset Flickr Reddit OGB-Arixv OGB-Products\n\n# Nodes 89,250 232,965 169,343 2,449,029\n\n# Edges 899,756 23,213,838 2,315,598 123,718,280\n\n# Features 500 602 128 100\n\n# Classes Train % / Validation % / Test % 50% / 25% / 25% 7\n66% / 10% / 24% 41 53.7% / 17.6% / 28.7% 40 8% / 2% / 90% 47\n\n(a) OGB-Arxiv\n\n(b) Flickr\n\n(c) Reddit\n\nFigure 8: Performance comparison of different distributed GAT training methods on four benchmark datasets. The top three subfigures show the training loss during the whole training process; the bottom three subfigures show the global validation F1 scores during the whole training process.\n\nA.3 ADDITIONAL EXPERIMENTAL RESULTS\n\nA.3.1 PERFORMANCE OF GAT TRAINING\n\nWe first show the learning curves of training GAT with three methods on three different datasets. As shown in Figure 8, for dataset Flickr and Reddit, DIGEST acheives the best validation F1 score over training time of all the three frameworks. For dataset OGB-Arxiv, the performance of DIGEST is slightly worse than DGL but still outperforms LLCG. Specifically, LLCG’s training curves are not stable and fluctuate dramatically for both GCN and GAT on Reddit. This is because Reddit is much denser compared to other datasets, and in this case, the sampling process of the global server correction in LLCG has difficulty capturing all the information loss due to the cut-edges. Unlike LLCG, DIGEST’s training curves are much smoother not only for GCN training but also for GAT training.\n\nA.3.2 MEMORY OVERHEAD\n\nIn this section we quantify the memory overhead introduced by DIGEST.\n\nFigure 9: The average ration of the number of out-of-subgraph nodes to the number of in-subgraph nodes when training a GCN over the four datasets.\n\n14\n\n0.51.01.52.02.53.03.5Training Loss050100150200Training Time(s)0.400.450.500.550.600.650.700.75Validation F1LLCGDGLDIGESTDIGEST-A1.01.52.02.53.03.5Training Loss050100150200Training Time(s)0.400.420.440.460.480.500.520.54Validation F1LLCGDGLDIGESTDIGEST-A0246810Training Loss050100150200250300Training Time(s)0.00.20.40.60.81.0Validation F1LLCGDGLDIGESTDIGEST-AOGB-ArxivFlickrRedditOGB-Products020406080100120140Memory Ratio(%)35.87140.39110.6158.43Under review as a conference paper at ICLR 2023\n\nRatio of out-of-subgraph nodes and in-subgraph nodes. Figure 9 shows the ratios of the number of out-of-subgraph nodes to the number of in-subgraph nodes across four datasets. This ratio quantifies additional memory consumption compared to methods that does not use any information of the neighboring nodes during training.\n\nDenser graphs like Flickr and Reddit require more memory to store representations of off-subgraph nodes than OGB-Arxiv and OGB-products. This introduces an interesting tradeoff between extra memory storage and gained benefits in reduced communication and preservation of global graph information. We argue that modern GPU servers are equipped with ample GPU memory resources to buffer the out-of-subgraph representations Gandhi & Iyer (2021); Wang et al. (2021); if indeed more memory is required, our research will benefit from the recent advancement of unified memory Choi et al. (2022); Huang et al. (2020); Peng et al. (2020), where DIGEST can use both the host and GPU memory more efficiently.\n\nIn the worst case, if GPU memory is limited, DIGEST can implement a multi-tier storage system that uses the limited memory as a level-one cache and the host memory as a backing store. For large graphs that are sparse (OGB-products), the extra memory cost can be bounded to a relatively lower ratio (58.43%).\n\nHost memory cost of stale representations. The KVS is responsible for storing the representations of all the nodes in a graph. The representations are stored in the memory of the host server instead of the GPUs, the latter of which is rather limited. We implemented the in-memory KVS with Apache Plasma, which is a shared memory storage that supports efficient, shared-memory-based inter-process communication (IPC) for multiple training processes located on the same server. However, extending our current KVS implementation to a fully-distributed storage system is trivial. Using off-the-shelf, high-performance distributed in-memory KVSes such as Redis is one option. Alternatively, we could also implement a simple client library, which can be used by the training process for key-value item mapping (e.g., using the commonly-used consistent hashing algorithm) and remote representation retrieval/storage, and with the client library, we could deploy a cluster of Plasma storage processes either on a dedicated storage cluster or on the same training server cluster to support distributed representation storage.\n\nThe overall memory consumption required to store representation data can be calculated with the following equation:\n\nKV S memory usage = (L − 1) × dim × |V | × s\n\n(7)\n\nwhere L is the total number of layers of the model, dim denotes the hidden dimension, |V | represents the number of nodes in the graph, and s is the size of data type in Python numpy. For the f loat32 data type, it takes 4 bytes for each single value. With the provided formula, for a 3-layer GNN model, training large graph dataset such as OGB-Products (with 2, 449, 029 nodes and 128 hidden dimensions), the extra host memory consumption for the representations is around 2 ∗ 128 ∗ 2449029 ∗ 4/1024/1024/1024 = 2.336 GB. DIGEST exhibits an interesting tradeoff: it uses a small amount of extra memory overhead for storing stale representations to enable the disaggregation of the compute and storage for higher scalability and more flexibility. We also argue that a small host memory cost of several GBs is negligible considering today’s multi-GPU servers are equipped with hundreds of GBs if not more than a few TBs of host memory4.\n\nTable 4: GPU memory consumption.\n\nModel GraphSAGE DGL LLCG DIGEST\n\nOGB-Arxiv OGB-Products\n\n0.40 GB 0.64 GB 0.23 GB 0.22 GB\n\n0.92 GB 1.78 GB 0.36 GB 0.36 GB\n\n4For\n\nexample, AWS EC2’s p3.16xlarge is\n\nGPUs with new-amazon-ec2-instances-with-up-to-8-nvidia-tesla-v100-gpus-p3/.\n\nhost memory:\n\n488 GBs\n\nof\n\nequipped with\n\n8 Nvidia Tesla V100 https://aws.amazon.com/blogs/aws/\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nGPU memory consumption. For the concern of GPU memory consumption, we compare DIGEST with GraphSAGE, LLCG, and DGL by including all the information inside a GNN’s receptive field in a single optimization step. The comparison results in Table 4 show that DIGEST has the lowest GPU memory consumption across all four systems.\n\nA.3.3 EMPIRICAL VALIDATION OF GRADIENT APPROXIMATION ERROR\n\nIn this section, we empirically evaluate the gradient approximation error due to the usage of stale representation. We conduct this experiment to show that the actual approximation error of gradients of DIGEST compared with the ground-truth gradients (i.e., gradients calculated without any stale representation) can be negligible in practice.\n\n(a) OGB-Arxiv\n\n(b) Flickr\n\nFigure 10: Error between the gradients calculated by DIGEST and full-graph baseline (i.e., without any staleness). Zoom in for detail.\n\nAs can be seen in Figure 10, during the training phase the gradient calculated by DIGEST quickly converge to the ground-truth gradients typically after fewer than 10-20 epochs. Hence, for the majority of training epochs the error of gradients is very small and the impact is negligible, which in turn validate our theoretical analyses in Theorem 1.\n\nA.3.4 OTHER COMPARISONS\n\nIn this section, we compare DIGEST/DIGEST-A with PipeGCN and GNNAutoScale. We train OGBProducts with DIGEST/DIGEST-A, PipeGCN and GNNAutoScale in the heterogeneous environment mentioned in Section 5.2, and report the training time taken to reach the target validation F1 score and time per epoch in Firgure 11, since there is no \"epoch\" in an asynchronous setting, the value of time per epoch for DIGEST-A is omitted. We can see that DIGEST gets slight higher time per epoch than GNNAutoScale but reduces the time per epoch by 24.13% compared with PipeGCN. Meanwhile, DIGEST-A gets the lowest training time to reach the target F1 score and saves 48.98% and 19.12% training time compared with PipeGCN and GNNAutoScale, respectively.\n\nWe further evaluate DIGEST, PipeGCN and GNNAutoScale on a large graph OGB-papers100m which consist of 111 million nodes 1.6 billion edges to show the efficiency of DIGEST. The experiments are done in a homogeneous environment with 32 GPUs. Since mini-batches in GNNAutoScale are trained in a serial manner instead of a parallel distributed setting, only one GPU is used. DIGEST reduces the time per epoch by 21.13% compared with distributed GNN training algorithm PipeGCN.\n\nA.4 ALGORITHM\n\nAlgorithm 1 shows the process of DIGEST’s synchronous mode. At the beginning of training, the original graph is partitioned into several subgraphs with off-the-shelf graph clustering methods;\n\n16\n\n0255075100125150175200Training Epoch01020304050607080Gradient Error (MSE)0255075100125150175200Training Epoch0500100015002000Gradient Error (MSE)Under review as a conference paper at ICLR 2023\n\n(a) Training time to reach the 91% validation F1score on OGB-Products\n\n(b) Time/epoch comparison\n\nFigure 11: Time comparisons with PipeGCN and GNNAutoScale.\n\nDIGEST uses the widely-used METIS algorithm Karypis & Kumar (1998). Then the mini-batches are distributed to distinct workers, each of which handles training on a GPU device. Depending on the size of the mini-batch, a single worker can handle one or multiple subgraphs. DIGEST has two types of I/Os: storing and retrieving the model weights and stale representation as illustrated in Figure 1(c). The former one is performed for each epoch by aggregating all the model weights of other subgraphs in parallel (Line 13). For the stale representation synchronization, we synchronize every N epochs, where we empirically tune N to obtain the optimal performance over training time with the defined pull and push operations (Lines 5,6,9,10). To support the asynchronous mode (DIGEST-A), we can simply remove the loop of training epoch and move the parameter aggregation (Line 13) into the subgraph loop.\n\nA.5 THEORETICAL PROOF\n\nIn this section, we provide the formal proof for all the theories presented in the main paper.\n\nA.5.1 PROOF OF THEOREM 1\n\nTheorem 4 (Formal version of Theorem 1). Given a L-layer GNN fW with r1-Lipschitz smooth Φ and r2-Lipschitz smooth Ψ. Denote ∆(G) as the maximal node degree for graph G. Assume ∀ v ∈ V and ∀ l ∈ {1, 2, · · · , L − 1} we have ∥h(l) v denotes the node representation computed by DIGEST and the stale one, respectively. Further assume each local loss function LLocal m is τ -Lipschitz smooth w.r.t node representation. Then the global gradient computed by DIGEST has the following error bound\n\nv ∥ ≤ ε(l), where h(l)\n\nv and ̃h(l)\n\nv − ̃h(l)\n\n(8)\n\n(9)\n\n(cid:13) (cid:13)∇WL − ∇WL∗(cid:13)\n\n(cid:13)2 ≤\n\nτ M\n\nL−1 (cid:88)\n\nl=1\n\nε(l)rL−l\n\n1\n\nrL−l\n\n2\n\nM (cid:88)\n\nm=1\n\n|∆(Gm)|L−l,\n\nwhere\n\nand\n\n∇WL :=\n\n1 M\n\nM (cid:88)\n\nm=1\n\n1 |Vm|\n\n(cid:88)\n\nv∈Vm\n\n∇WL∗ :=\n\n1 M\n\nM (cid:88)\n\nm=1\n\n1 |Vm|\n\n(cid:88)\n\nv∈Vm\n\n∇WLLocal\n\nm (h(L)\n\nv\n\n),\n\n∇WLLocal\n\nm (h∗(L)\n\nv\n\n),\n\n(10)\n\nwhere h∗(l)\n\nv denotes the exact output from the l-th layer of GNN without any staleness.\n\nProof. As stated in Theorem 2 in Fey et al. (2021), under the single-GPU training setup, with Lipschitz smooth Φ and Ψ as well as not too stale node representations, the GNN last layer’s output\n\n17\n\nPipeGCNGASDIGESTDIGEST-A0500100015002000Training time(s)1795.901132.702152.00916.12OGB-ProductsOGB-papers100m020406080100120Time/epoch(s)15.877.1011.70124.4012.045.60PipeGCNGNNAutoScaleDIGESTUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Distributed GNN training with periodic stale representation synchronization Input: Graph G(V, E); GNN depth L; training epoch R; global parameters W(r) = {W(r,l)}L\n\nl=1, l=1, ∀ m ∈ (cid:2)M (cid:3), r ∈ (cid:2)R(cid:3); non-linearity activation local parameters W(r) function σ; neighborhood function N : v → 2V ; synchronization interval N; learning rate η.\n\nm = {W(r,l)\n\nm }L\n\nOutput: The trained model weights W(R+1).\n\n1 DIGEST():\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\nInitialize W(1) {Gm(Vm, Em), m = 1, 2, .., M } ← METIS(G) for r = 1...R do\n\nfor m = 1, · · · , M in parallel do\n\nW(r) m = W(r) for l = 1...L do\n\nif r % N == 0 and l ̸= L then\n\nH(l,m)\n\nout ← ̃H(l,m)\n\nout\n\nfor v ∈ Vm do out = {h(l) h(l) h(l) in = {h(l) h(l) v = σ\n\n(cid:16)\n\nu : u ∈ N (v) \\ Vm} u : u ∈ N (v) ∩ Vm} m · CONCAT(cid:0)h(l) W(r,l)\n\nv , h(l)\n\nin , h(l)\n\nout\n\nif (r − 1) % N == 0 and l ̸= L then\n\nH(l,m) v ← h(l)\n\nin → ̃H(l,m) v /∥h(l) = W(r,l)\n\nh(l) W(r,l+1)\n\nin\n\nm\n\nv ∥2, ∀ v ∈ Vm m − η · ▽W(r,l)\n\nm\n\nW(r+1) ← AGG(W(r+1)\n\n1\n\n...W(r+1) M )\n\nreturn W(R+1)\n\ncan be bounded by\n\n▷ graph partition\n\n▷ PULL\n\n(cid:1)(cid:17)\n\n▷ PUSH\n\n▷ representation normalization\n\n▷ update local parameters\n\n▷ update global parameters\n\n(cid:13) (cid:13)h(L) (cid:13)\n\nv − h∗(L)\n\nv\n\n(cid:13) (cid:13) (cid:13)2\n\n≤\n\nL−1 (cid:88)\n\nl=1\n\nε(l)rL−l\n\n1\n\nrL−l\n\n2\n\n|N (v)|L−l.\n\n(11)\n\nNow consider the distributed GNN training setting. First, notice that in our distributed setting, the stale node representation ̃h(l) is shared for all subgraphs. In other words, for m = 1, 2, · · · , M we can apply the conclusion above with the Lipschitz smooth asumption and have\n\nv\n\n(cid:13) (cid:13)∇WLLocal (cid:13)\n\nm (h(L)\n\nv\n\n) − ∇WLLocal\n\nm (h∗(L)\n\nv\n\n)\n\n(cid:13) (cid:13) (cid:13)2\n\n≤ τ\n\n(cid:13) (cid:13)h(L) (cid:13)\n\nv − h∗(L)\n\nv\n\n(cid:13) (cid:13) (cid:13)2\n\n≤ τ ·\n\nL−1 (cid:88)\n\nl=1\n\nε(l)rL−l\n\n1\n\nrL−l\n\n2\n\n|N (v)|L−l.\n\n(12)\n\nNotice that |N (v)| ≤ ∆(Gm), ∀ v ∈ Vm, where ∆(Gm) is defined as the maximal node degree for subgraph Gm. We can sum over all nodes v ∈ Vm and take average on both sides of Eq. 12 to get\n\n1 |Vm|\n\n(cid:88)\n\nv∈Vm\n\n(cid:13) (cid:13)∇WLLocal (cid:13)\n\nm (h(L)\n\nv\n\n) − ∇WLLocal\n\nm (h∗(L)\n\nv\n\n(cid:13) (cid:13) )\n(cid:13)2\n\n≤ τ ·\n\nL−1 (cid:88)\n\nl=1\n\nε(l)rL−l\n\n1\n\nrL−l\n\n2\n\n|∆Gm|L−l.\n\n(13)\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFinally, since we apply average to aggregate each local subgraph’s gradient to get the global gradients, by the triangle inequality, we have\n\n(cid:13) (cid:13)\n\n(cid:13)∇WL − ∇WL∗(cid:13)\n\n(cid:13) (cid:13)2\n\nM (cid:88)\n\nM (cid:88)\n\nm=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nm=1\n\n1 |Vm|\n\n1 |Vm|\n\n(cid:88)\n\nv∈Vm\n\n(cid:88)\n\nv∈Vm\n\n∇WLLocal\n\nm (h(L)\n\nv\n\n) −\n\n1 M\n\nM (cid:88)\n\nm=1\n\n1 |Vm|\n\n(cid:88)\n\nv∈Vm\n\n∇WLLocal\n\nm (h∗(L)\n\nv\n\n∇WLLocal\n\nm (h(L)\n\nv\n\n) −\n\n1 |Vm|\n\n(cid:88)\n\nv∈Vm\n\n∇WLLocal\n\nm (h∗(L)\n\nv\n\n(cid:13) (cid:13) (cid:13) )\n(cid:13) (cid:13)2\n\n(cid:13) (cid:13) (cid:13) )\n(cid:13) (cid:13)2\n\n(14)\n\nM (cid:88)\n\nm=1\n\nM (cid:88)\n\n1 |Vm|\n\n(cid:88)\n\nv∈Vm\n\n(cid:13) (cid:13)∇WLLocal (cid:13)\n\nm (h(L)\n\nv\n\n) − ∇WLLocal\n\nm (h∗(L)\n\nv\n\n)\n\n(cid:13) (cid:13) (cid:13)2\n\nL−1 (cid:88)\n\nτ ·\n\nε(l)rL−l\n\n1\n\nrL−l\n\n2\n\n|∆Gm|L−l,\n\nm=1\n\nl=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 M\n\n=\n\n≤\n\n≤\n\n≤\n\n1 M\n\n1 M\n\n1 M\n\nwhich finishes the proof.\n\nA.5.2 PROOF OF THEOREM 2\n\nIn this section, we prove the convergence of DIGEST under the synchronous setting. First, we introduce some notions, definitions and necessary assumptions.\n\nPreliminaries. We consider GCN in our proof without loss of generality. We denote the input graph as G = (V, E), L-layer GNN as f , feature matrix as X, weight matrix as W . The forward propagation of one layer of GCN is\n\nZ (l+1) = P H (l)W (l), H (l+1) = σ(Z (l))\n\n(15)\n\nwhere l is the layer index, σ is the activation function, and P is the propagation matrix following the definition of GCN (Kipf & Welling, 2016). Notice H (0) = X. We can further define the (l + 1)-th layer of GCN as:\n\nf (l+1)(H (l), W (l)) := σ(P H (l)W (l))\n\nThe backward propagation of GCN can be expressed as follow:\n\nG(l)\n\nH = ∇H f (l+1)(H (l), W (l), G(l+1)\n\nH\n\n) := P ⊺D(l+1)(W (l+1))⊺\n\nG(l+1)\n\nW = ∇W f (l+1)(H (l+1), W (l), G(l+1)\n\nH\n\n) := (P H (l))⊺D(l+1)\n\nwhere\n\nD(l+1) = G(l)\n\nH ◦ σ′(P H (l)W (l+1))\n\nand ◦ represents the Hadamard product.\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\nUnder a distributed training setting, for each subgraph Gm = (Vm, Em), m = 1.2, · · · , M , the propagation matrix can be decomposed into two independent matrices, i.e. P = Pm,in + Pm,out, where Pm,in denotes the propagation matrix for nodes inside the subgraph Gm while Pm,out denotes that for neighbor nodes outside Gm. If it will not cause confusion, we will use Pin and Pout in our future proof for simpler notation.\n\nFor DIGEST, the forward propagation of a single layer of GCN can be expressed as\n\nm\n\n ̃Z (t,l+1) ̃H (t,l+1)\n\nm\n\n= Pin ̃H (t,l) = σ( ̃Z (t,l) m )\n\nm\n\n ̃W (t,l)\n\nm + Pout ̃H (t−1,l)\n\nm\n\n ̃W (t,l)\n\nm\n\n(20)\n\nwhere we use ̃H to differentiate with the counterpart without staleness, i.e., H (same for other variables). t is the training iteration index. Similarly, we can define each layer as a single function\n\n ̃f (t,l+1)\n\nm\n\n( ̃H (t,l)\n\nm , ̃W (t,l)\n\nm ) := σ(Pin ̃H (t,l)\n\nm\n\n ̃W (t,l)\n\nm + Pout ̃H (t−1,l)\n\nm\n\n ̃W (t,l) m )\n\n(21)\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nNote that ̃H (t−1,l−1) i.e., it can be regarded as a constant in the current iteration.\n\nm\n\nis not part of the input since it is the stale results from the previous iteration,\n\nNow we can give the definition of back-propagation in DIGEST:\n\n ̃G(t,l)\n\nH,m = ∇H\n\n ̃f (t,l+1)\n\nm\n\n( ̃H (l)\n\nm , ̃W (l), ̃G(l+1) H,m )\n\n:= P\n\n⊺ in\n\n ̃D(t,l+1)\n\nm\n\n( ̃W (t,l+1)\n\nm\n\n)⊺ + P\n\n⊺ out\n\n ̃D(t−1,l+1)\n\nm\n\n( ̃W (t,l+1)\n\nm\n\n)⊺\n\n ̃G(t,l+1)\n\nW,m = ∇W\n\n ̃f (t,l+1) := (Pin ̃H (t,l)\n\n, ̃W (t,l) m + Pout ̃H (t−1,l−1)\n\nm , ̃G(t,l+1) H,m ) )⊺ ̃D(t,l+1)\n\n( ̃H (t,l+1)\n\nm\n\nm\n\nm\n\nm\n\nwhere\n\n ̃D(t,l+1)\n\nm\n\n= G(l)\n\nH,m ◦ σ′(Pin ̃H (t,l)\n\nm\n\n ̃W (t,l)\n\nm + Pout ̃H (t−1,l−1)\n\nm\n\n ̃W (t,l) m )\n\n(22)\n\n(23)\n\n(24)\n\nIn our proof, we use L(W (t)) to denote the global loss with GCN parameter W after t iterations, and use ̃Lm(W (t) m after t iterations computed by DIGEST.\n\nm ) to denotes the local loss for the m-th subgraph with model parameter W (t)\n\nAssumptions. Here we introduce some assumptions about the GCN model and the original input graph. These assumptions are standard ones that are also used in (Chen et al., 2018; Cong et al., 2021; Wan et al., 2022).\n\nAssumption 5. The loss function Loss(·, ·) is CLoss-Lipchitz continuous and LLoss-Lipschitz smooth with respect to the last layer’s node representation, i.e.,\n\n|Loss(h(L)\n\nv\n\n, yv) − Loss(h(L)\n\nw , yv)| ≤ CLoss∥h(L)\n\nv − h(L)\n\nw ∥2\n\nand\n\n∥∇Loss(h(L)\n\nv\n\n, yv) − ∇Loss(h(L)\n\nw , yv)∥2 ≤ LLoss∥h(L)\n\nv − h(L)\n\nw ∥2\n\n(25)\n\n(26)\n\nAssumption 6. The activation function σ(·) is Cσ-Lipchitz continuous and Lσ-Lipschitz smooth, i.e.\n\n∥σ(Z (l)\n\n1 ) − σ(Z (l)\n\n2 )∥2 ≤ Cσ∥(Z (l)\n\n1 − Z (l)\n\n2 ∥2\n\nand\n\n∥σ′(Z (l)\n\n1 ) − σ′(Z (l)\n\n2 )∥2 ≤ Lσ∥(Z (l)\n\n1 − Z (l)\n\n2 ∥2 (27)\n\nAssumption 7. ∀ l that l = 1, 2, · · · , L, we have\n\n∥W (l)∥F ≤ KW , ∥P (l)∥P ≤ KW , ∥X (l)∥F ≤ KX .\n\n(28)\n\nNow we can introduce the proof of our Theorem 2. We consider a GCN with L layers that is Lf -Lipschitz smooth, i.e., ∥∇L(W1) − ∇L(W2)∥2 ≤ Lf ∥W1 − W2∥2. Theorem 5 (Formal version of Theorem 2). There exists a constant E such that for any arbitrarily small constant ε > 0, we can choose a learning rate η = E and number of training iterations T = (L(W (1)) − L(W ∗)) E√ M\n\n2 , such that\n\nε− 3\n\nM ε\n\n√\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥∇L(W (t))∥2 ≤ O(\n\n1 3 M 1\n\n3\n\nT 2\n\n)\n\n(29)\n\nwhere W (t)and W ∗ denotes the parameters at iteration t and the optimal one, respectively.\n\nProof. Beginning from the assumption of smoothness of loss function,\n\nL(W t+1) ≤ L(W t) +\n\n(cid:68)\n\n∇L(W t), W (t+1) − W (t)(cid:69)\n\n+\n\nLf 2\n\n∥W (t+1) − W (t)∥2\n\n2\n\n(30)\n\nRecall that the update rule of DIGEST is\n\nW (t+1) = W (t) −\n\nη M\n\nM (cid:88)\n\nm=1\n\n20\n\n∇ ̃Lm(W (t) m )\n\n(31)\n\nUnder review as a conference paper at ICLR 2023\n\nso we have\n\nL(W t) +\n\n(cid:68)\n\n∇L(W t), W (t+1) − W (t)(cid:69)\n\n+\n\nLf 2\n\n∥W (t+1) − W (t)∥2\n\n2\n\n(cid:42)\n\n=L(W t) − η\n\n∇L(W t),\n\n1 M\n\nM (cid:88)\n\nm=1\n\n(cid:43)\n\n∇ ̃Lm(W (t) m )\n\n+\n\nη2Lf 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 M\n\nM (cid:88)\n\nm=1\n\n∇ ̃Lm(W (t) m )\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) 2\n\n(32)\n\nDenote δ(t)\n\nm = ∇ ̃Lm(W (t)\n\nm ) − ∇Lm(W (t) (cid:42)\n\nm ), we have\n\nL(W t+1) ≤L(W t) − η\n\n∇L(W t),\n\n1 M\n\nM (cid:88)\n\n(cid:16)\n\nm=1\n\n∇Lm(W (t)\n\nm ) + δ(t)\n\nm\n\n(cid:43)\n\n(cid:17)\n\n(33)\n\n+\n\nη2Lf 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 M\n\nM (cid:88)\n\n(cid:16)\n\nm=1\n\n∇Lm(W (t)\n\nm ) + δ(t)\n\nm\n\n(cid:17)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) 2\n\nWithout loss of generality, assume the original graph can be divided evenly into M subgraphs and denote N = |V| as the original graph size, i.e., N = M · S, where S is each subgraph size. Notice that\n\n∇L(W t) =\n\n1 N\n\nN (cid:88)\n\ni=1\n\n∇Loss(f (L)\n\ni\n\n, yi) =\n\n1 M\n\n(cid:110) M (cid:88)\n\nm=1\n\n1 S\n\nS (cid:88)\n\ni=1\n\nwhich is essentially\n\n∇L(W t) =\n\n1 M\n\nM (cid:88)\n\nm=1\n\n∇Lm(W (t) m )\n\nPlugging the equation above into Eq. 33, we have\n\n∇Loss(f (L)\n\nm,i , ym,i)\n\n(cid:111)\n\nL(W t+1) ≤ L(W t) −\n\nη 2\n\n∥∇L(W t)∥2\n\n2 +\n\nη2Lf 2\n\n(cid:13) (cid:13) (cid:13)\n\n1 M\n\nM (cid:88)\n\nm=1\n\nδ(t)\n\nm\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\nwhich after rearranging the terms leads to\n\n∥∇L(W t)∥2\n\n2 ≤\n\n2 η\n\n(L(W t) − L(W t+1)) + ηLf\n\n(cid:13) (cid:13) (cid:13)\n\n1 M\n\nM (cid:88)\n\nm=1\n\nδ(t)\n\nm\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\nBy taking η < 1/Lf , using the three assumptions defined earlier and Corollary A.10 in Wan et al. (2022), and summing up the inequality above over all iterations, i.e., t = 1, 2, · · · , T , we have\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥∇L(W (t))∥2 ≤\n\n≤\n\n2 ηT\n\n2 ηT\n\n(cid:0)L(W 1) − L(W T +1)(cid:1) +\n\nη2E2 M\n\n(cid:0)L(W 1) − L(W ∗)(cid:1) +\n\nη2E2 M\n\n(38)\n\nwhere W ∗ denotes the minima of the loss function and E is a constant depends on E′.\n\nFinally, taking η =\n\n√\n\nM ε\n\nE and T = (L(W (1)) − L(W ∗)) E√\n\nM\n\nε− 3\n\n2 finishes the proof.\n\nA.5.3 PROOF OF THEOREM 3\n\nBy following Li et al. (2020); Chen et al. (2020); Chai et al. (2021) we make the assumption as below: Assumption 8. ∀ m ∈ [M ], ∥∇ ̃Lm(W )∥2 ≤ V · ∥∇L(W )∥2, and (cid:10)∇L(W ), ∇ ̃Lm(W )(cid:11) ≥ β · ∥∇L(W )∥2\n\n2, where V and β are positive real numbers, i.e., V, β ∈ R+.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nWe naturally assume that each local copy of the global GCN model is also Lf -Lipschitz smooth, i.e., ∥∇ ̃Lm(W1) − ∇ ̃Lm(W2)∥2 ≤ Lf ∥W1 − W2∥2, ∀ m = 1, 2, · · · , M .\n\nNow we can give the proof of Theorem 3. Theorem 6 (Formal version of Theorem 3). Assume the global model L(W ) is Cf -Lipschitz continuous and the delay is bounded, i.e., τ < K. Further, assume the constants defined in Assumption 8 satisfy β − V 2 2 > 0. Then, after T global iterations on the server, asynchronous DIGEST converges to the optimal parameter W ∗ by\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥∇L(W (t))∥2\n\n2 ≤\n\n(cid:16)\n\n1 ηT B\n\nL(W (1)) − L(W (∗))\n\n(cid:17)\n\n+\n\nP (η) B\n\n,\n\n(39)\n\nwhere B = β − V 2\n\n2 and P (η) = 1\n\n2 η2K 2C 2\n\nf L2\n\nf + (1 + V )ηKC 2\n\nf Lf .\n\nProof. By the smoothness assumption of global model,\n\nL(W (t+1)) ≤ L(W (t)) +\n\n(cid:68)\n\n∇L(W (t)), W (t+1) − W (t)(cid:69)\n\n+\n\nLf 2\n\n∥W (t+1) − W (t)∥2 2.\n\n(40)\n\nSuppose at global iteration t + 1, the server receives an update from subgraph m, where m could be any value from 1 up to M . Then,\n\nL(W (t+1)) ≤ L(W (t)) − η\n\n(cid:68)\n\n∇L(W (t)), ∇ ̃Lm(W (t−τ ))\n\n(cid:69)\n\n+\n\nη2Lf 2\n\n∥∇ ̃Lm(W (t−τ ))∥2 2,\n\n(41)\n\nwhere τ is the delay for subgraph m when sending server its update.\n\nDenote rm := ∇ ̃Lm(W (t−τ )) − ∇ ̃Lm(W (t)). Since τ ≤ K, by the Lipschitz smoothness of ̃Lm, ∥rm∥2 = ∥∇ ̃Lm(W (t−τ )) − ∇ ̃Lm(W (t))∥2\n\n≤ Lf · ∥W (t−τ ) − W (t)∥2\n\n= Lf · ∥\n\nt (cid:88)\n\ni=t−τ\n\nηgi∥2\n\n≤ ηKLf Cf ,\n\nwhere gi is the gradient or update the global server receives at iteration i.\n\nTherefore,\n\nL(W (t+1)) − L(W (t))\n\n≤ − η\n\n(cid:68)\n\n∇L(W (t)), ∇ ̃Lm(W (t)) + rm\n\n(cid:69)\n\n+\n\nη2Lf 2\n\n∥∇ ̃Lm(W (t)) + rm∥2\n\n2\n\n(cid:68)\n\n(cid:68)\n\n= −η\n\n(cid:124)\n\n−η\n\n(cid:124)\n\n∇L(W (t)), ∇ ̃Lm(W (t))\n\n(cid:69)\n\n+\n\n(cid:123)(cid:122) I\n\n∇L(W (t)), rm\n\n(cid:123)(cid:122) IV\n\n(cid:125)\n\n(cid:68)\n\n(cid:69)\n\n(cid:125)\n\n+ η2Lf (cid:124)\n\n∇ ̃Lm(W (t)), rm\n\n(cid:123)(cid:122) V\n\n(cid:69)\n\n(cid:125)\n\nη2Lf 2\n\n(cid:124)\n\n∥∇ ̃Lm(W (t))∥2 2\n(cid:125)\n\n(cid:123)(cid:122) II\n\n+\n\nη2Lf 2\n\n(cid:124)\n\n∥rm∥2 2\n(cid:125) (cid:123)(cid:122) III\n\nNow we want to find bounds for (I - V) above.\n\nBy Assumption 8, we have\n\nand\n\n(I) ≤ −ηβ∥∇L(W(t))∥2 2,\n\n(II) ≤\n\n1 2\n\nη2Lf V 2∥∇L(W(t))∥2 2.\n\nBy our previous result on ∥rm∥2, we have\n\n(III) ≤\n\n1 2\n\nη3K 2L2\n\nf C 2\n\nf\n\n22\n\n(42)\n\n(43)\n\n(44)\n\n(45)\n\n(46)\n\nUnder review as a conference paper at ICLR 2023\n\nTaking η ≤ 1/Lf , we have\n\n(IV) + (V) ≤ η\n\n(cid:68)\n\n∇ ̃Lm(W (t)) − ∇L(W (t)), rm\n\n(cid:69)\n\nBy Cauchy-Schwartz inequality, triangle inequality and Assumption 8,\n\n(IV) + (V) ≤ η∥∇ ̃Lm(W (t)) − ∇L(W (t))∥2 · ∥rm∥2\n\n≤ η2KCf Lf · ∥∇ ̃Lm(W (t)) − ∇L(W (t))∥2\n\n≤ η2KCf Lf ·\n\n(cid:16)\n\n∥∇ ̃Lm(W (t))∥2 + ∥∇L(W (t))∥2\n\n(cid:17)\n\n≤ (1 + V )η2KCf Lf · ∥∇L(W (t))∥2 ≤ (1 + V )η2KC 2 f Lf\n\n(47)\n\n(48)\n\nPut everything together, we have\n\nL(W (t+1)) − L(W (t)) ≤\n\n(cid:19)\n\nηV 2 − ηβ\n\n(cid:18) 1 2\n\n· ∥∇L(W (t))∥2\n\n2 +\n\n1 2\n\nη3K 2L2\n\nf C 2\n\nf + (1 + V )η2KC 2\n\nf Lf .\n\nHence,\n\n∥∇L(W (t))∥2\n\n2 ≤\n\n(cid:18)\n\nηβ −\n\n(cid:18)\n\n+\n\nηβ −\n\n1 2\n\n1 2\n\n(cid:19)−1\n\n(cid:16)\n\n·\n\nηV 2\n\nL(W (t)) − L(W (t+1))\n\n(cid:17)\n\n(cid:19)−1\n\nηV 2\n\n(cid:18) 1 2\n\n·\n\nη3K 2L2\n\nf C 2\n\nf + (1 + V )η2KC 2\n\nf Lf\n\n(49)\n\n(50)\n\n(cid:19)\n\n.\n\nSumming up from t = 1 to T and taking the average,\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥∇L(W (t))∥2\n\n2 ≤\n\n1 (cid:0)ηβ − 1\n\n2 ηV 2(cid:1) T\n\n(cid:16)\n\nL(W (1)) − L(W (∗))\n\n(cid:17)\n\n(cid:18)\n\n+\n\nηβ −\n\n1 2\n\nηV 2\n\n(cid:19)−1\n\n(cid:18) 1 2\n\n·\n\nη3K 2L2\n\nf C 2\n\nf + (1 + V )η2KC 2\n\nf Lf\n\n(cid:19)\n\n(51)\n\n≤\n\n(cid:16)\n\n1 ηT B\n\nL(W (1)) − L(W (∗))\n\n(cid:17)\n\n+\n\nP (η) B\n\n,\n\nwhere B = β − V 2\n\n2 and P (η) = 1\n\n2 η2K 2C 2\n\nf L2\n\nf + (1 + V )ηKC 2\n\nf Lf .\n\n23",
    "reference": "# Summary Of The Paper\n\nThe paper introduces a distributed extension of GNNAutoScale (DIGEST) and proposes an asynchronous representation update mechanism (DIGEST-A) to reduce communication overhead for node embedding updates. Theoretical analyses such as forward error bound and convergence analysis are provided. Experiments show promising speedup in training time.\n\n# Strength And Weaknesses\n\n# Strength \n\n1. The paper proposes a distributed GNN training framework that synergies the benefits of partition-based and propagation-based methods. The motivation is clearly demonstrated and the approach is valid.\n\n2. Theoretical analyses demonstrate the impact of embedding error and staleness bound on prediction error and convergence behavior. This helps justify the impact of the staleness (but concerns are discussed below).\n\n3. The experiments validate the effectiveness of the proposed algorithm, and the improvement over baselines is significant.\n\n# Weakness\n\n1. The major idea of the paper follows GNNAutoScale, and the asynchronous update is inspired by extensive research on asynchronous distributed optimization. The theoretical analyses and assumptions closely follow PipeGNN [1]. Overall, the originality is a bit weak.\n\n2. The theoretical statements of the paper need further clarification. Assumptions being made need to be clearly clarified in the main theorem. For instance, there are lots of assumptions being made in the proof but they are never mentioned in Theorem 2 and Theorem 3. Furthermore, the constants in the Theorem need to be explained, such as $E, M, P(\\eta)$ in Theorem 2 and 3.\n\n3. There are multiple concerns with the theoretical analysis. \n\n(1) First, the error presented in Theorem 1 can be potentially very large, which grows exponentially with the number of layers and has a bad dependency on the maximum node degree. A detailed empirical study of this approximation error will be helpful to clarify the impact of the error.\n\n(2) Second, the backward propagation process for gradient computation neglects the gradient computation through the out-subgraph stale representations. In other words, each local model considers the node representation from other machines to be constant. The paper and the corresponding theorem do not take this into account. In fact, this will invalidate one important step in the theoretical proof Eq. (34): $\\nabla L(W^t) = 1/M \\sum_1^M \\nabla L_m(W_m^t)$. This might cause major flaws in the proof and need to be addressed.\n\n4. The algorithm is only compared with 2 baselines that do not represent the state-of-art algorithms. It is suggested to also compare with algorithms such as PipeGNN and sampling-based methods such as GNNAutoScale.\n\n[1] PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication\n\n# Clarity, Quality, Novelty And Reproducibility\n\n# Clarity\n\nThe theoretical results of the paper need further revisions. Assumptions being made need to be clearly clarified in the main theorem. For instance, there are lots of assumptions being made in the proof but never mentioned in Theorem 2 and Theorem 3. Furthermore, the constants in the Theorem need to be explained, such as $E, M, P(\\eta)$ in Theorem 2 and 3.\n\n# Originality\n\nThe major idea of the paper follows GNNAutoScale, and the asynchronous update is inspired by extensive research on asynchronous distributed optimization. The theoretical analyses and assumptions closely follow PipeGNN [1]. Overall, the originality is not strong.\n\n# Summary Of The Review\n\nThe paper introduces a simple and practical distributed GNN framework. Theoretical analyses are presented but concerns need to be addressed. Experiments demonstrate significant speedup but more baselines will be beneficial.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nCHEAP TALK DISCOVERY AND UTILIZATION IN MULTI-AGENT REINFORCEMENT LEARNING\n\nYat Long Lo University of Oxford Dyson Robot Learning Lab richie.lo@dyson.com\n\nChristian Schroeder de Witt FLAIR, University of Oxford cs@robots.ox.ac.uk\n\nSamuel Sokota Carnegie Mellon University ssokota@andrew.cmu.edu\n\nJakob Foerster FLAIR, University of Oxford jakob.foerster@eng.ox.ac.uk\n\nShimon Whiteson University of Oxford shimon.whiteson@cs.ox.ac.uk\n\nABSTRACT\n\nBy enabling agents to communicate, recent cooperative multi-agent reinforcement learning (MARL) methods have demonstrated better task performance and more coordinated behavior. Most existing approaches facilitate inter-agent communication by allowing agents to send messages to each other through free communication channels, i.e., cheap talk channels. Current methods require these channels to be constantly accessible and known to the agents a priori. In this work, we lift these requirements such that the agents must discover the cheap talk channels and learn how to use them. Hence, the problem has two main parts: cheap talk discovery (CTD) and cheap talk utilization (CTU). We introduce a novel conceptual framework for both parts and develop a new algorithm based on mutual information maximization that outperforms existing algorithms in CTD/CTU settings. We also release a novel benchmark suite to stimulate future research in CTD/CTU.\n\n1\n\nINTRODUCTION\n\nEffective communication is essential for many multi-agent systems in the partially observable setting, which is common in many real-world applications like elevator control (Crites & Barto, 1998) and sensor networks (Fox et al., 2000). Communicating the right information at the right time becomes crucial to completing tasks effectively. In the multi-agent reinforcement learning (MARL) setting, communication often occurs on free channels known as cheap talk channels. The agents’ goal is to learn an effective communication protocol via the channel. The transmitted messages can be either discrete or continuous (Foerster et al., 2016).\n\nExisting work often assumes the agents have prior knowledge (e.g., channel capacities and noise level) about these channels. However, such assumptions do not always hold. Even if these channels’ existence can be assumed, they might not be persistent, i.e., available at every state. Consider the real-world application of inter-satellite laser communication. In the case, communication channel is only functional when satellites are within line of sight. This means positioning becomes essential (Lakshmi et al., 2008). Thus, Without these assumptions, agents need the capability to discover where to best communicate before learning a protocol in realistic MARL settings.\n\nIn this work, we investigate the setting where these assumptions on cheap talk channels are lifted. Precisely, these channels are only effective in a subset of the state space. Hence, agents must discover where these channels are before they can learn how to use them. We divide this problem into two sequential steps: cheap talk discovery (CTD) and cheap talk utilization (CTU). The problem is a strict generalization of the common setting used in the emergent communication literature with\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nless assumptions, which is more akin to real-world scenarios (see appendix A for more in-depth discussions on the setting’s significance and use cases).\n\nFigure 1: The two learning stages for CTD/CTU based on PBMaze. Stage (a): Discover the functional phone booths; Stage (b): Form a protocol to use the phone booth and learn to interpret the messages (left), and solve the task (right). The blue and red agents are the sender and the receiver respectively\n\nThis setting is particularly difficult as it suffers from the temporal credit assignment problem (Sutton, 1984) for communicative actions. Consider an example we call the phone booth maze (PBMaze), the environment has a sender and a receiver, placed into two separate rooms. The receiver’s goal is to escape from the correct exit out of two possible exits. Only the sender knows which one is the correct exit. The sender’s goal is to communicate this information using functional phone booths.\n\nThis leads to two learning stages. Firstly, they need to learn to reach the booths. Then, the sender has to learn to form a protocol, distinguishing different exit information while the receiver has to learn to interpret the sender’s protocol by trying different exits. This makes credit assignment particularly difficult as communicative actions do not lead to immediate rewards. Additionally, having communicative actions that are only effective in a small subset of the state space further makes it a challenging joint exploration problem, especially when communication is necessary for task completion. Figure 1 provides a visual depiction of the two learning stages in this environment.\n\nAs a whole, our contributions are four-fold. Firstly, we provide a formulation of the CTD and CTU problem. Secondly, we introduce a configurable environment to benchmark MARL algorithms on the problem. Thirdly, we propose a method to solve the CTD and CTU problems based on information theory and advances in MARL, including off-belief learning (Hu et al., 2021, OBL) and differentiable inter-agent learning (Foerster et al., 2016, DIAL). Finally, we show that our proposed approach empirically compares favourably to other MARL baselines, validate the importance of specific components via ablation studies and illustrate how our method can act as a measure of channel capacity to learn where best to communicate.\n\n2 RELATED WORK\n\nThe use of mutual information (MI) has been explored in the MARL setting. Wang et al. (2019) propose a shaping reward based on MI between agents’ transitions to improve exploration, encouraging visiting critical points where one can influence other agents. Our proposed method also has an MI term for reward shaping. Their measure might behave similarly to ours but is harder to compute and requires full environmental states during training. Sokota et al. (2022) propose a method to discover implicit communication protocols using environment actions via minimum entropy coupling, separating communicative and non-communicative decision-making. We propose a similar problem decomposition by separating state and action spaces into two subsets based on whether communication can occur or not. Unlike in Sokota et al. (2022), we focus on explicit communication\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nwhere specialized channels for communication exist. Jaques et al. (2019) propose rewarding agents for having causal influences over other agents’ policies using MI between agents’ actions. Jaques et al. (2019) is the closest to our work but still assumes the omnipresence and prior knowledge of communication channels.\n\nMany recent papers investigate various aspects of communication in MARL. Foerster et al. (2016) propose DIAL to learn how to communicate by allowing gradients to flow across agents. We use DIAL as a component of our proposed framework. Sukhbaatar et al. (2016) propose CommNet to learn how to communicate. Unlike DIAL, it uses mean-pooling to process messages and handle a dynamic number of agents. Das et al. (2019) proposes a targeted communication architecture to tackle the issue of what messages to send and who to send them to. Singh et al. (2018) propose a gating mechanism to learn when to communicate, achieving better training efficiency scalability by reducing redundant communication. Jiang & Lu (2018) proposes an attention mechanism to learn when and who to communicate by dynamically forming communication groups. In our work, we focus on the problem of discovering where to communicate, rather than how to communicate, who to communicate, and when to communicate. Hence, our contributions are orthogonal to existing works.\n\n3 BACKGROUND\n\nThroughout this work, we consider decentralized partially observable Markov decision processes (Dec-POMDPs) with N agents (Oliehoek & Amato, 2016) in the form of a tuple G = ⟨S, A, P, R, Z, Ω, n, γ⟩. s ∈ S is the true state of the environment. At each time step, each agent i ∈ N chooses an action a ∈ Ai to form a joint action a ∈ A ≡ A1 ×A2...×AN . This leads to a transition on the environment according to the transition function P (s′|s, a1, ...aN ) : S × A × S → [0, 1]. All agents share the same reward function R(s, a) : S × A → R. γ ∈ [0, 1) is a discount factor. Each agent i receives individual observations z ∈ Z based on the observation function Ωi(s) : S → Z.\n\nThe environment trajectory and the action-observation history (AOH) of an agent i are denoted (cid:1) ∈ T ≡ (Z × A)∗ respectively. A as τt = (s0, a0, ....st, at) and τ i stochastic policy π(ai|τ i) : T ×A → [0, 1] conditions on AOH. The joint policy π has a corresponding action-value function Qπ(st, at) = Est+1:∞,at+1:∞ [Rt|st, at], where Rt = (cid:80)∞ i=0 γirt+i is the discounted return. rt+i is the reward obtained at time t + i from the reward function R. The distribution of states is commonly referred to as a belief Bπ(τ |τ i) = P (τ |τ i, π).\n\nt = (cid:0)Ωi(s0), ai\n\n0, ..., Ωi(st), ai\n\nt\n\n3.1 OFF-BELIEF LEARNING\n\nOBL (Hu et al., 2021) is a recent method to learn policies that do not interpret the actions of other agents and assumes other agents would do the same. Precisely, it induces agents to not reason about each other’s private information and actions by conditioning their beliefs only on information revealed by the environment and interpreting their actions as if they were performed by a random agent. This is often desirable as making incorrect assumptions can cause coordination failure. Therefore, learning a base policy that maximizes reward without any conventions is important, especially when agents work with others who they have not met before, a problem known as zero-shot coordination (ZSC).\n\nThe OBL operator assumes all agents to be playing a common policy π0 up to τ i and π1 thereafter. Then, an agent’s belief B, conditioned on their AOH can be computed as:\n\nBπ0(τ |τ i) = P (τ |τ i, π0)\n\n(1)\n\nWe denote the state-action value for playing π0 up to τ i and playing π1 thereafter to be Qπ0→π1(a|τ i), which is the expected return of sampling τ from Bπ0 (τ i) with all players playing π1 starting from the end of this trajectory. The counterfactual state-action value function is defined as follows:\n\nQπ0→π1(a|τ i) =\n\nBπ0(τt|τ i\n\nt )[R(st, a) + T(τt+1|τt)V π1(τt+1)].\n\n(2)\n\n(cid:88)\n\nτt,τt+1\n\nTo compute an OBL policy using value iteration methods, the Bellman equation for Qπ0→π1(τ i) for each agent i is expressed as follows:\n\nQπ0→π1(at|τ i\n\nt ) =\n\nE\n\nτt∼Bπ0 (τ i\n\nt ),τt+k∼(T,π1)\n\nt′=t\n\n(cid:104)t+k−1 (cid:88)\n\nπ1(at+k|τ i\n\nt+k)Qπ0→π1 (at+k|τ i\n\n(cid:105) t+k)\n\n(3)\n\nR(st′, at′) +\n\n(cid:88)\n\nat+k\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nUnder this setting, the states reached by π1 may be reached at very low probabilities under π0. The variant learned-belief OBL (Hu et al., 2021, LB-OBL) addresses this by using an approximate belief ˆBπ0 that takes τi as input and samples a trajectory from an approximation of P (τ |τ i, π0). Q-learning is then performed with an altered target value. Particularly, a new τ ′ is resampled from ˆBπ0(τ i t ). Next, a transition to τ i′ t+1 is simulated with other agents playing policy π1. The bootstrapped value is then maxa Q(a|τ i t ∼ π1 is applied to both the actual environment and a sampled fictitious state. The learning target then t, r′ becomes the sum of fictitious rewards r′ t+2). We use exact belief models in this work to remove the influence of belief learning.\n\nt+2). Hence, LB-OBL only involves fictitious transitions. The action ai\n\nt+1 and the fictitious bootstrapped value maxa Q(a|τ i\n\n3.2 DIFFERENTIABLE INTER-AGENT LEARNING\n\nFoerster et al. (2016) propose DIAL to learn how to communicate. It allows agents to give each other feedback about their communicative actions, by opening up communication channels for gradients to flow through from one agent to another. Such richer feedback improves sample complexity with more efficient discovery of protocols. During training, there are direct connections between one agent’s network output and another agent’s input through communicative actions. Agents can then send real-valued messages and are only restricted to discrete messages during execution. These real-valued messages are generated from the networks, allowing end-to-end backpropagation across agents.\n\nThe proposed network is called C-net, an extension of deep Q-networks (Silver et al., 2016, DQN). It has two set of outputs, namely the Q-values Q(·) of the environment actions Aenv and the real-valued messages ma t . The former performs actions selection while the latter is sent to another agent after the discretize/regularize unit DRU (ma t ) processes it. The DRU regularizes messages during learning, DRU (ma t > 0}. σ is the standard deviation of the noise added to the cheap talk channel. The gradient term for m is backpropagated to the sender based on the message recipient’s error. Hence, the sender can adjust m to minimize the downstream loss. Note that we use the OBL loss here instead of DQN loss.\n\nt , σ)), and discretizes them during execution, DRU (ma\n\nt ) = Logistic(N (ma\n\nt ) = 1{ma\n\n4 PROBLEM FORMULATION\n\nWe consider each agent’s action space A to be decomposable into two subspaces, namely, the environment action space Aenv and communication action space Acomm. The former are actions that only affect the environment state. The latter are actions that have no immediate effect on the environment state or the reward received, but can alter other agents’ observations. Exemplifying with PBMaze, Aenv for the sender are actions that move its locations like Up and Down, while Acomm are actions that send messages to the receiver like Hint Up and Hint Down.\n\nIn this setting, agents do not inherently know where to best communicate and only a subset of states allows communication. We refer to this subset as the communicative state space Scomm ∈ S: Definition 4.1. The communicative state space Scomm of an environment is a subspace of its state space S. Assume the environment is in a state sc ∈ Scomm, at least one agent in sc can modify another agent’s observation by taking an action a ∈ Acomm.\n\nSimilarly, the communicative joint observation space Ocomm is defined as follows: Definition 4.2. The communicative joint observation space Ocomm of an environment is a subspace of its joint observation space O, defined as Ocomm = {Ω(sc)|sc ∈ Scomm}. Assume the environment is in a state sc ∈ Scomm with a joint observation oc ∈ Ocomm, at least one agent, agent i ∈ N , can modify another agent j ∈ N, j ̸= i’s observation, ot+1\n\nby taking an action a ∈ Acomm.\n\nj\n\nWith these definitions, the cheap talk discovery and utilization problem can be formalized as follows: Definition 4.3. For a given s ∈ S, let M I(s) = (cid:80) j∼N,j̸=i I(Ai, Oj) be a function that computes the pairwise mutual information (PMI) of an agent’s actions and another agent’s observations. (cid:3), be the discounted The inner term is defined in equation 5. Let ν(π) = Eπ sum of PMI of a policy π ∈ Π, where Π is the set of all possible policies. Cheap talk discovery is the\n\ni=t γiM I(st) | τt\n\n(cid:2)(cid:80)∞\n\ni∼N\n\n(cid:80)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nproblem of learning a policy πdiscover in which agents take actions in Aenv that maximizes ν(π). An optimal π∗\n\ndiscover satisfies the condition: ν(π∗\n\ndiscover) ≥ ν(π)\n\n∀π ∈ Π\n\nDefinition 4.4. Given a state st is in Scomm with a corresponding AOH τt at time t, cheap talk utilization is the problem of learning a policy πutil in which agents take communicative actions in Acomm (cid:3) i=t γirt+i | τt to share information to improve task performance. For a policy π ∈ Π, Rπ is its expected return starting from st. An optimal π∗ ≥ Rπ\n\nutil can then be defined as: π ∈ Π and ∀τt s.t. st ∈ Scomm\n\nt = Eπ\n\n∀π̸=π∗\n\n(cid:2)(cid:80)∞\n\nRπ∗\n\nutil\n\nt\n\nt\n\nutil\n\nWe reiterate that this is a challenging joint exploration problem, especially when communication is necessary for task completion. Because agents need to stumble upon a state sc ∈ Scomm, which is often a much smaller subspace than S, and there are no incentivizing signals to reach these states as communicative actions Acomm do not lead to immediate rewards.\n\nThus, we hypothesize that this problem decomposition, i.e., first learn to discover the Scomm before learning how to use the communicative channels, could ease this joint exploration problem’s severity.\n\n5 METHODOLOGY\n\n5.1 CHEAP TALK DISCOVERY\n\nOur proposed method has two components, namely, mutual information (MI) maximization and OBL (Hu et al., 2021). The former induces agents to discover cheap talk channels based on MI. The latter is our base learning algorithm with sound theoretical properties that are beneficial to the following CTU step.\n\n5.1.1 MUTUAL INFORMATION MAXIMIZATION\n\nAgents do not have immediate incentives to discover cheap talk channels. To do so, an agent has to know where to send messages to other agents successfully (i.e., channels that can influence another agent’s observations). Hence, we propose novel reward and loss functions based on MI. The proposed reward function encourages greater mutual information (MI) between the sender’s (denoted as agent 1) actions A1 and the receiver’s (denoted as agent 2) observations O2. It is expressed as:\n\nR′(st, at) = R(st, at) + βI(A1, O2).\n\n(4)\n\nwhere β is a hyperparameter. The first term is the task reward. The second term is the MI reward:\n\nI(A1, O2) = E\n\na1∼A1 o2∼O2\n\n(cid:2)log(p(a1|o2) − log(p(a1))(cid:3)\n\n(5)\n\nWe assume access to the environment simulator to estimate p(a1|o2). The proposed reward function can then be substituted in the update equation 3. To estimate p(a1|o2), the term can be expanded to:\n\nI(A1, O2) = H(A1) + H(O2) − H(A1, O2)\n\n(cid:88)\n\n(cid:2)−p(a1) log(p(a1))(cid:3) +\n\n(cid:88)\n\n(cid:2)−p(o2) log(p(o2))(cid:3)\n\na1∼A1 (cid:88)\n\n(cid:2)−p(a1, o2) log(p(a1, o2)(cid:3)\n\no2∼O2\n\n=\n\n−\n\n(6)\n\n(a1 ,o2) ∼(A1,O2)\n\nwhere p(o2) = (cid:80) a1∼A1[p(a1, o2)] and p(a1, o2) = p(o2|a1)p(a1). Note that this generalizes to any pair of agents in an environment irrespective of the receiver. Hence, we can apply this method to all possible receivers by summing all I(A1, Oi), i ∈ N, i ̸= 1 to discover the cheap talk channels.\n\nThe MI reward term alone does not maximize I(A1, O2). Specifically, for a given policy, since the term is computed based on taking all the actions from a state, the MI reward for staying within\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nScomm and for taking an action in Acomm are equal. Hence, a policy learned only with MI reward is incentivized to reach Scomm without necessarily using communicative actions. Ideally, I(A1, O2) is maximized when a policy favors the communicative actions equally when in Scomm.\n\nTo learn a policy that does maximize I(A1, O2), we propose an MI loss function, using the parameterized policy πθ. The additional term directly maximizes I(A1, O2), which is maximized when the policy takes actions in Acomm more. For each iteration i, the loss function is expressed as:\n\nLi(θi) = LOBL\n\ni\n\n(θi) − κI(A1, O2; πθ)i\n\n(7)\n\ni\n\n(θi) (see Appendix B), I(A1, O2; πθ)i and κ are OBL loss, MI loss at iteration i, and a where LOBL hyperparameter respectively. We add a minus sign so that minimizing the loss maximizes the MI term. The combination of the MI reward and loss should allow the discovery of cheap talk channels with greater MI. We denote our discovery method as cheap talk discovery learning (CTDL)\n\n5.1.2 DISCOVERING CHANNELS WITHOUT CONVENTIONS FORMATION\n\nDuring discovery learning, agents should simply learn where the channels are without forming any protocols (i.e., conventions) on how to use them. This is because the protocols formed would affect the rewards received. To do so, we employ OBL as our base algorithm.\n\nParticularly, it has appealing theoretical properties which are beneficial for this setting, as it theoretically guarantees to converge to a policy without any conventions (Hu et al., 2021). This means OBL with CTDL would learn a policy that discovers the channels while not having a preference over any communicative actions, which would allow more flexible adaptation when properties of channels alter. Hence, though not explored here, better performance in ZSC can be expected.\n\nFor our example environment, the agent should prefer the actions Hint Up and Hint Down equally after cheap talk discovery. If the agents are in a slightly different environment with the phone booths sending negated versions of the messages an agent sends, a policy without conventions should adapt quicker given that it has no preference over communicative actions.\n\n5.2 CHEAP TALK UTILIZATION\n\nWith channels discovered, agents are ready to learn how to use them through protocol formation (i.e., form a consensus of how messages are interpreted). Many off-the-shelf algorithms can be used for CTU. Here, we use DIAL (Foerster et al., 2016), which has been shown to learn protocols efficiently. Note that OBL will be replaced by independent Q-learning (Tan, 1993, IQL) updates to allow protocol formation in CTU.\n\nGiven that the original DIAL requires feedforwarding through an episode’s full trajectory to maintain the gradient chain, modifications to DIAL are needed to work in this setting for two reasons:\n\nFirst, the base learning algorithm is an experience-replay-based method that learns from transition batches, not trajectories. Second, unlike in the original DIAL which messages are sent in every step t, the messages can only be sent successfully when the environment state is in Scomm. Hence, direct gradient connections only happen occasionally in an episode.\n\nTo make DIAL compatible with this setting, we use a separate replay buffer to keeps track of the transitions and only consider a trajectory when messages are sent successfully. The buffer stores full episodes with flags indicating which transitions to allow direct gradient connections. We denote our discovery method with DIAL as cheap talk discovery and utilization learning (CTDUL). The transition from CTD to CTU is a task-dependent hyperparameter and is determined empirically. Appendix C and D shows how gradients flow in CTDUL and the corresponding pseudocode.\n\n6 EXPERIMENTAL SETUP\n\nAs part of our contribution, we designed a configurable environment with discrete state and action space - the phone booth maze to evaluate agents in the CTD/CTU setting, as described in Section 1.\n\nTo enable extensive investigations, the environment allows phone booths of configurable properties including usage cost and noise level, and decoy booths that are not functional. The agents’ observation\n\n6\n\nPublished as a conference paper at ICLR 2023\n\ncontains view information of the room an agent is in and role-specific information (e.g. the sender and receiver have goal information and communication token respectively). Agents have the environment actions of UP, DOWN, LEFT, RIGHT, and NO-OP. There are two communicative actions, HINT-UP and HINT-DOWN which only has an effect when agents are in the functional phone booths, updating the receiver’s communication token to a fixed value. Rewards of 1.0 and -0.5 are given for choosing a correct exit and an incorrect exit respectively with no reward given otherwise. By changing the phone booths’ properties, we can vary their capacities, measured by the MI reward.The left of Figure 2 shows the MI of each location in the sender’s room in two different environment configurations, in which one has only one functional booth and the other has three booths with one noisy booth. These are computed based on equation 5 on all possible combinations of the sender and receiver locations.\n\nWe use two configurations of the environment for our experiments, which we name Single Phone Booth Maze (SPBMaze) and Multiple Phone Booth Maze (MPBMaze). Please see Appendix E for environment visualizations and design features, and Appendix H for environment parameters used.\n\nArchitecturally, we use the R2D2 architecture (Kapturowski et al., 2018) with a separate communication head like in C-Net (Foerster et al., 2016). For baselines, state-of-the-art methods are used including IQL, OBL (Hu et al., 2021), QMIX (Rashid et al., 2018), MAA2C (Papoudakis et al., 2021) and IQL with social influence reward (Jaques et al., 2019, SI).\n\nWe trained all methods for 12000 episodes (80000 episodes for CTU) and evaluated on test episodes every 20 episodes by taking the corresponding greedy policy. Each algorithm reports averaged results from 4 random seeds with standard error. We performed a hyperparameter sweep over common hyperparameters, fixing them across all methods, and specific sweeps for method-specific parameters. Please see Appendix F and G for training and hyperparameter details.\n\n7 RESULTS\n\n7.1 DISCOVERING CHEAP TALK CHANNELS\n\nFigure 2: Left: MI reward for each location in the sender’s room of SPBMaze and MPBaze to show phone booths of different properties. Values are computed by iterating over all possible receiver’s position for each sender’s position. Senders start at positions with green borders. The position with purple border is the costly phone booth which incurs a cost to use. The top phone booth in MPBaze has a noise factor of 0.5, i.e., lower MI. Right: CTD performance for CTDL and baselines. The horizontal grey line indicates the optimal number of step to reach the booth. For the ease of plotting, when an agent cannot discover the booth, we set the step as the episode length.\n\nTo quantify the performance in CTD, we use the number of steps it takes for the sender to first reach the functional phone booth in SPBMaze. The fewer the number of steps used, the better an algorithm is at CTD. See Table 4 for detailed configuration of SPBMaze.\n\nFigure 2 (Right) shows the performances of our proposed method CTDL and the baselines. CTDL, using MI reward and loss, discovers the functional phone booth quickly in optimal number of steps (as shown by the horizontal grey line). Meanwhile, without the proper incentives, all the baseline methods cannot discover the functional phone booth within the episode limit.\n\nEven though SI uses a reward term based on MI like ours (i.e., the influence an agent’s message has on another agent’s action (Jaques et al., 2019)), it becomes insufficient to discover cheap talk\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nchannels in this setting as the messages sent only has an effect in a small subset of the state space. Note that without discovering the channels, agents will not be able to learn how to communicate. Hence, we expect the baseline methods cannot leverage communication channels to solve tasks.\n\n7.2 LEARNING POLICIES WITH MAXIMIZED MUTUAL INFORMATION\n\nFigure 3: Algorithms’ sender policy when both agents are at the functional phone booth in SPBMaze. CTDL learns a policy with the most MI by preferring communicative actions equally.\n\nTo assess whether an algorithm can learn policies that maximize MI (i.e., I(A1, O2)) in this setting, we look at the sender’s policy when both agents are at the functional phone booth. To reiterate, such policy uniformly prefers Acomm over Aenv when using a communication channel without forming any protocol. Figure 3 shows different algorithms’ sender policy when in the functional phone booth.\n\nWe include two variants that receive a scalar intermediate reward (IR) when both agents are at functional phone booths. As discussed in section 5.1.1, these reward shaping methods do not maximize MI, although cheap talk channels are discovered. Instead, they learn to prefer actions that keep them in Scomm which includes certain environment actions.\n\nContrarily, our proposed MI loss used in CTDL leads to polices with the most MI by preferring Acomm (i.e., Hint-Up and Hint-Down) uniformly over Aenv. This explains why CTDL has the best CTD performance as it directly maximizes PMI in Definition 4.3. Note that this uniformity over Acomm also means no conventions are formed during discovery learning as they are preferred equally under different goals (i.e. different correct exits in SPBMaze), demonstrating the effect of OBL.\n\n7.3 UTILIZING CHEAP TALK CHANNELS\n\nFigure 4: Left: CTU performance for CTDUL and baselines on SPBMaze. The vertical grey line indicates the start of CTU learning for CTDUL. Right: Ablation experiment on SPBMaze for CTDUL. The curves are smoothed by a factor of 0.99 with standard errors plotted as shaded areas.\n\nFigure 4 (left) shows the overall task performance of the proposed CTDUL and baselines in solving the SPBMaze. Being unable to communicate the goal information successfully, the receiver can only guess by escaping randomly (i.e., achieving an expected reward of 0.25 = 1.0 × 0.5 + (−0.5) × 0.5). Hence, all baselines converge to the suboptimal solution of random guessing.\n\nFrom Figure 2 (right) and Figure 4 (left), we can infer that the senders in the baselines do not reach the functional phone booth to communicate so no protocols can be formed. Centralized training baselines\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(e.g., QMIX, MAA2C), which are designed to assign credits better since they have centralized components to learn better value functions using full state information, also converge to suboptimal solutions. Their poor performances illustrate the difficulty of this joint exploration problem.\n\nIn contrast, our proposed method CTDUL obtains significantly higher rewards than random guessing by employing DIAL to learn how to use the discovered channel. The grey vertical line is when DIAL begins after discovery learning. Qualitatively, we observe how our method solves the problem in which the receiver waits at the functional booth until receiving a message from the sender. Then, it goes to the correct exit according to the protocol learned. The results show how our proposed problem decomposition and MI maximization components make the difficult problem easier to solve.\n\nFigure 4 (right) shows task performance on SPBMaze for our ablation study with CTDUL. Specifically, we look at the task performance when we individually remove components used, namely MI maximization and DIAL. As we can see, nontrivial solutions can only be learned if both of them are used. Removing one of them leads to the suboptimal solution of random guessing. Furthermore, we observe significant performance dip when OBL is replaced with IQL, empirically supporting the importance of not forming any conventions during CTD as pointed out in section 5.1.2.\n\n7.4 MUTUAL INFORMATION AS A MEASURE OF CHANNEL CAPACITY\n\nFigure 5: Bar plot of average number of booth visits of each phone booth type in MPBMaze with varying noise factors for the noisy booth. The error bars are standard errors for the average number of booth visits for each booth type.\n\nWe hypothesize that our proposed MI reward can be a pseudomeasure of channel capacity, a way to discover where best to communicate when an environment has multiple channels. To verify, we run baselines and our proposed method for CTD on MPBMaze which has three channels, namely, the Perfect Booth, the Costly Booth, and the Noisy Booth. Note that a booth with noise has a probability of dropping a message. See Table 4 in the Appendix for the detailed configuration of MPBMaze.\n\nFigures 5 compare which booth the sender chooses to visit when different reward functions are used. The optimal behavior is to visit the Perfect Booth the most as it has no noise and usage cost. All algorithms learn to avoid the Costly Booth given its direct effect on the reward. Comparing methods that use IR and MI rewards, the IR reward cannot distinguish between the Perfect Booth and the Noisy Booth and consistently visit the Noisy Booth as visits to both booths are rewarded equally and the Noisy Booth is closer to the sender.\n\nOn the contrary, the MI reward method is able to visit the Perfect Booth consistently as the MI reward has an interpretation and measurement of noise and channel capacity (i.e., A channel with greater noise would necessarily mean a lower expected MI reward). As the noise factor decreases, the two booths become less distinguishable, which we observe a decreased in number of booth visits to the perfect booth for the agent with MI reward when the noise factor is 0.1.\n\n8 CONCLUSION\n\nIn this work, we take away the common and arguably unrealistic assumptions in MARL that communication channels are known and persistently accessible to the agents. Under this setting, agents have\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nto first discover where to best communicate before learning how to use the discovered channels. We first provide a novel problem formulation for this setting denoted as the cheap talk discovery and utilization problem. Then, based on this problem decomposition, we propose a method based on MI maximization with OBL and DIAL to tackle this problem end-to-end. Experimentally, by evaluating our framework against state-of-the-art baselines in our proposed environment suite, we show that our framework can effectively discover communication channels and subsequently use them to solve tasks, while all the baselines fail to do so. We also empirically attribute the framework’s success to its key capabilities in learning policies with maximized MI and without convention formation during CTD. Finally, we demonstrate how our MI metric can serve as a pseudomeasure for channel capacity, to be used to learn where best to communicate. We hope this work can inspire investigation in more realistic settings for communication like the one introduced here.\n\n9 ACKNOWLEDGMENTS\n\nWe thank Tarun Gupta for providing code implementations of RL environments for our reference during the ideation phase of the project. We thank Michael Noukhovitch for helpful feedback.\n\nREFERENCES\n\nRobert H Crites and Andrew G Barto. Elevator group control using multiple reinforcement learning\n\nagents. Machine learning, 33(2):235–262, 1998.\n\nAbhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication. In International Conference on Machine Learning, pp. 1538–1546. PMLR, 2019.\n\nJakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. Advances in neural information processing systems, 29, 2016.\n\nDieter Fox, Wolfram Burgard, Hannes Kruppa, and Sebastian Thrun. A probabilistic approach to\n\ncollaborative multi-robot localization. Autonomous robots, 8(3):325–344, 2000.\n\nHengyuan Hu and Jakob N Foerster. Simplified action decoder for deep multi-agent reinforcement\n\nlearning. In International Conference on Learning Representations, 2019.\n\nHengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, and Jakob Foerster. Off-belief\n\nlearning. In International Conference on Machine Learning, pp. 4369–4379. PMLR, 2021.\n\nNatasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In International Conference on Machine Learning, pp. 3040–3049. PMLR, 2019.\n\nJiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation.\n\nAdvances in neural information processing systems, 31, 2018.\n\nSteven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations, 2018.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nK Shantha Lakshmi, MP Senthil Kumar, and KVN Kavitha. Inter-satellite laser communication system. In 2008 International Conference on Computer and Communication Engineering, pp. 978–983. IEEE, 2008.\n\nFrans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs. Springer,\n\n2016.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nGeorgios Papoudakis, Filippos Christianos, Lukas Schäfer, and Stefano V Albrecht. Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32: 8026–8037, 2019.\n\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018.\n\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.\n\nAmanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at scale in multiagent cooperative and competitive tasks. In International Conference on Learning Representations, 2018.\n\nSamuel Sokota, Christian Schroeder de Witt, Maximilian Igl, Luisa M Zintgraf, Philip Torr, Martin Strohmeier, J Zico Kolter, Shimon Whiteson, and Jakob Nicolaus Foerster. Communicating via markov decision processes. In Proceedings of the 39th International Conference on Machine Learning, ICML’22. JMLR.org, 2022.\n\nSainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation.\n\nAdvances in neural information processing systems, 29:2244–2252, 2016.\n\nRichard Stuart Sutton. Temporal credit assignment in reinforcement learning. PhD thesis, University\n\nof Massachusetts Amherst, 1984.\n\nMing Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings\n\nof the tenth international conference on machine learning, pp. 330–337, 1993.\n\nTonghan Wang, Jianhao Wang, Yi Wu, and Chongjie Zhang. Influence-based multi-agent exploration.\n\nIn International Conference on Learning Representations, 2019.\n\nXihuai Wang, Zhicheng Zhang, and Weinan Zhang. Model-based multi-agent reinforcement learning:\n\nRecent progress and prospects. arXiv preprint arXiv:2203.10603, 2022.\n\nA PROBLEM SIGNIFICANCE AND USE CASES\n\nThe full problem setting of solving Dec-POMDPs is NEXP-complete. As such, it is not reasonable to expect a method that can solve large arbitrary Dec-POMDPs. To get around this, the “emergent communication” literature usually focuses on a setting where there is a fixed communication channel that is known a-priori.\n\nWe strictly generalise this setting by instead proposing a method that can first discover channels within the environment and next utilise them. Clearly, this cannot cover all problem Dec-POMDPs due to the constraints mentioned above but includes a broader set of potentially practically relevant applications.\n\nIn terms of use cases, the problem applies whenever real-world communication through a medium among agents is considered. Because more often than not, there will be some spatial constraints on communication in realistic settings. Any kind of communication channel used in the real-world (e.g., a radio link, cell phone signal), does not have perfect signal strength, and often transmission errors could happen (e.g., noise). The signal strength varies a lot depending on your environment and current surroundings (e.g, indoor vs outdoor, underwater, environment interference in certain locations like behind a mountain). Hence, for communication to happen efficiently and successfully, being able to learn where to communicate and learn where best to communicate is crucial which our problem setting addresses.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nEven in communication without an explicit medium, say if a robot wants to signal to another one with lights but they need a line of sight, communication becomes spatially constrained. We believe these examples illustrate how important this problem setting is when considering communication learning in more realistic environments.\n\nFurthermore, the setting also sheds light to a more complete picture of capacity-constrained communication. As illustrated in our experiments with noisy channels, our mutual information reward provides a pseudomeasure of capacity. The investigation provides a path forward regarding capacityconstrained communication, by establishing two levels of capacity constraints - i.e., capacity constrained by the environment/physical space, size of Scomm, and the capacity for individual channels. Our method provides the first attempt to tackle both types of capacity constraints - learning where to communicate and learning where best to communicate.\n\nB OBL LOSS\n\nAs in Hu et al. (2021), for a given trajectory τ sampled from replay buffer, LOBL is expressed as TD-error:\n\nLOBL(θ|τ ) =\n\n1 2\n\nT (cid:88)\n\n(cid:104)\n\nt=1\n\nt + r′ r′\n\nt+1 + max\n\na\n\nQˆθ(a|τ ′\n\nt+2) − Qθ(at|τt)\n\n(cid:105)2\n\n(8)\n\nwhere Qˆθ is the target network. As τ ′ sequence to RNN like in normal Q-learning. Thus, we precompute the target G′ maxa Qˆθ(a|τ ′ into the replay buffer.\n\nt contains fictitious transitions, we can not pass the whole t+1 + t+2) during rollouts and store the sequence of targets along with the real trajectory τ\n\nt = r′\n\nt + r′\n\nC GRADIENT CHAIN OF THE PROPOSED ARCHITECTURE\n\nFigure 6: How gradients flow through our proposed architecture, with reference to Foerster et al. (2016)\n\nD PSEUDOCODE OF THE PROPOSED ARCHITECTURE\n\nAlgorithm 1 outlines the pseudocode of the proposed architecture to tackle the cheap talk discovery and utilization problem.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Pseudocode for our proposed method\n\nfor each agent do\n\nInitialize replay memory for OBL DOBL with capacity N Initialize replay memory for DIAL DDIAL with capacity N Initialize action value function Q with random weights θ, using architecture C-Net based on R2D2 Initialize target action value function ˆQ with random weights θ− = θ , using architecture C-Net based on R2D2 Initialize Num_Discovery_Episode Initialize Discovery_Stage as True\n\nend for for e = 1, Max_Episode do\n\nInitialize s repeat\n\nfor each agent do\n\nSelect a from o based on policy derived from Q, e.g. ε-greedy policy or stochastic OBL policy Take action a to observe reward r and next state o′ Take action a in Pseudo-Environment to perform OBL sampling with mutual information computation Store the transition into DOBL if High mutual information is observed then\n\nStore trajectory into DDIAL\n\nend if Sample random minimatch of transitions from DOBL Sample random minimatch of transitions from DDIAL if Discovery_Stage then\n\nPerform a gradient descent step on LOBL network weights θ\n\ni\n\n(θi) − κI(A1, O2; πθ)i with respect to the\n\nelse\n\nPerform a gradient descent step on (yi − Q(o, a; θi−1))2 for DIAL with respect to the network weights θ Perform a gradient descent step on LIQL\n\n(θi) with respect to the network weights θ\n\ni\n\nend if Every C steps, set ˆQ = Q\n\nend for\n\nuntil s = sterminal if e ≥ Num_Discovery_Episode then\n\nSet Discovery_Stage as False\n\nend if end for\n\nE ENVIRONMENT DESIGN\n\nFigure 7 visualizes two instances of the environment used in our experiments. Red cables indicate connectivity with dashed ones indicating the existence of noise. The yellow booth here indicates that it is a costlier booth to use.\n\nFor observations, each consists of a tensor of 3 channels plus some role-specific information. The channels are the wall channel, phone booth channel, and the agent channel, which are essentially binary grid encoding of each position for the existence of wall, phone booth, and the agent respectively. For role-specific information, the sender has an additional 2-bit encoding vector as goal information. Specifically, if the receiver should go up, the sender would get a vector of [1 0]. If the receiver 1] instead. For the receiver, if the sender performs a should go down, the sender would get [0 HINT-UP with both of them at the functional booths, it would be -1. On the other hand, if the sender performs a HINT-DOWN, it would be 1. Many aspects of this environment are configurable to allow extensive investigation and exploration of different algorithms, some key adjustable features are highlighted in table E.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Two instances of the PBMaze with different configurations used in the experiments\n\nF NETWORK ARCHITECTURE AND TRAINING\n\nInstead of using convolutional layers as in Kapturowski et al. (2018), we use fully-connected neural networks with a recurrent component as our neural network model. Hence, we flatten the observation from the environment into a vector by first fattening the 3-channel tensor and then concatenate it with the role-based information (i.e., goal encoding or communication token). Precisely, the input is first processed by an LSTM layer to handle partial observability. Then, it is followed by a two-layer and two-headed fully-connected neural network of hidden size 128. The two heads are used to compute the action-value function and the advantage function respectively. All neural network components are implemented using the neural network library PyTorch (Paszke et al., 2019), with the weights initialized using Xavier initialization. In terms of training, we use the Adam optimizer to train our models (Kingma & Ba, 2014). Training was done in an internal cluster with a mix of GTX 1080 and RTX 2080 GPUs.\n\nG HYPERPARAMETERS SETTING\n\nThis section covers details in setting hyperparameters for various methods used in this work. They are covered in two separate sections depending on whether a parameter is common to all used methods or not.\n\nG.1 COMMON PARAMETERS\n\nTo determine the best parameters that are common to all the methods used, we performed a hyparameter sweep over some key common parameters. Specifically, the search was performed on the learning rate and target network update frequency in the lists of [0.01, 0.001, 0.0001, 0.00001] and [50, 100, 200, 500]. The results are averaged over 3 random seeds, each trained for 25000 episodes. The best set of parameters for learning rate and target network update frequency are 0.0001 and 100, respectively. Other common parameters are set to values in table G.1.\n\nG.2 METHOD-SPECIFIC PARAMETERS\n\nMethod-specific parameters are set to values in table 3.\n\nH ENVIRONMENT PARAMETERS\n\nParameters of the environments used in the experiments are summarized in Table 4.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nDescription\n\nthe length of each of the agent’s rooms. The longer it is, the harder the exploration becomes. the starting location of each of the agents.\n\nThe reward that is given when the receiver exits from the correct door.\n\nThe reward that is given when the receiver exits from the wrong door.\n\nIf set as true, it gives an intermediate reward for if both agents are at the functional booths, used for debugging purposes.\n\nEpisode length, meaning the episode terminates if it reaches this number of steps.\n\nAmong functional phone booths, they are configurable in two major ways, namely, cost and noise. The former refers to the cost of using the booth and the latter is modeled as the probability of a phone booth dropping a message. Locations of functional booths.\n\nThe number of decoy booths in the sender’s room. Decoy booths refer to booths that are not functional and not connected to the booth in the receiver’s room. They appear as booths in the agent’s observation (i.e., the booth channel). If set true, the decoy phone booths’ locations are reinitialized whenever the environment is reset for the next episode, making the problem more difficult.\n\nIf set true, the environment would compute the mutual information reward, used in the proposed method.\n\nIf set true, the environment would return the necessary information (i.e., tensor masks for each term in equation 6 to compute the mutual information loss in a batch-based manner.\n\nConfigurable parameter lengths starting points correct reward wrong reward use intermediate reward episode limit booth types\n\nbooth locations number of decoy booths booth reinitialization use mutual information reward use mutual information loss\n\nTable 1: Configurable parameters for the PBMaze\n\nHyperparameter Discount Factor γ Batch Size Replay Buffer Size Temperature Non-Linearity\n\nValue 0.99 32 10000 1.0 ReLU\n\nTable 2: Common parameters used across algorithms\n\nI LIMITATIONS\n\nWhile our proposed framework provides a promising approach to learn where to communicate (i.e., CTD/CTU) by first discovering cheap talk channels and subsequently learning how to use them, it is important to be aware of the assumptions and limitations of the framework for future work.\n\nTo begin with, our approach assumes access to an environment simulator and a (perfect) belief to perform MI computation and OBL respectively. While there are recent approaches in learning environment models (Wang et al., 2022) and belief models (Hu & Foerster, 2019), how well these learned components work with each other remains unexplored. Advances in these areas would improve the overall robustness of our proposed framework.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nIQL 1.0 0.00001\n\n1000\n\nN/A\n\nN/A N/A\n\nN/A\n\n0.1\n\nStarting ε ε Decay Step Minimum ε\nInitial Exploration Step IR α Entropy Factor β Mutual Information Reward Factor κ Mutual Information Loss Factor learning rate critic learning rate critic hidden size hypernetwork hidden size number of hyper layers social influence reward coefficient temperature 1.0\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nIQL + IR 1.0 0.00001\n\n0.1\n\n1000\n\n1.0 N/A\n\nN/A\n\nOBL N/A N/A\n\nN/A\n\n1000\n\nN/A N/A\n\nN/A\n\nCTDL/CTDULQMIX N/A N/A\n\n1.0 0.00001\n\nMAA2C 1.0 0.00001\n\nSI 1.0 0.00001\n\nN/A\n\n1000\n\nN/A 0.0\n\n2.0\n\n0.1\n\n0.1\n\n0.1\n\n1000\n\n1000\n\n1000\n\nN/A N/A\n\nN/A\n\nN/A N/A\n\nN/A\n\nN/A N/A\n\nN/A\n\nN/A\n\nN/A\n\n1.0\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\n0.0005\n\n0.0001\n\nN/A\n\n0.0001\n\nN/A\n\n64\n\n32\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\n1\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\nN/A\n\n5.0\n\n1.0\n\n0.1\n\n0.1\n\n1.0\n\n1.0\n\n1.0\n\nTable 3: Method-specific parameters\n\nAnother crucial assumption made is the direct and immediate causal relationship between a communicative action and a receiver’s observation which might be less reliable in the real-world setting. For instance, there will be delay and transmission time when sending messages. In more visually-rich and open-ended settings, there could also be many confounding changes in the receiver’s observation when communication actions are taken. In these cases, further advances are needed to correctly assign credits to communicative actions when computing MI.\n\nLast but not least, the transition from CTD to CTU is determined empirically from experiments. A more principled approach to automate this decision would make method more robust. For instance, based on Definition 4.3, a method could be developed to indicate whether an agent has been able to discover a channel consistently (i.e., a discovery policy with maximized PMI). Once a threshold is reached, it can move on to the utilization stage.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nre-\n\nlengths (sender’s room, ceiver’s room) starting points (sender’s coordinates, receiver’s coordinates) correct reward wrong reward episode limit booth types\n\nSPBMaze (8, 4)\n\nMPBMaze (5, 3)\n\n((4, 1), (2, 1))\n\n((3, 1), (1, 1))\n\n1.0 -0.5 20 1 functional booth, 0 cost to use and 0 noise factor\n\n1.0 -0.5 20 functional phone booths. The first one has a cost of 0.4 and 0 noise factor. The second one has 0 cost and 0 noise factor. The third one has 0 cost and x ∈ [0.1, 0.3, 0.5] as noise factor across 3 sets of experiments 0\nFalse\n\nnumber of decoy booths booth reinitialization\n\n2 False\n\nTable 4: Table for environment configurations of each environment used in the experiments\n\nJ BROADER IMPACT\n\nWith the rapid progress in MARL and multi-agent learning systems in general, we confidently expect that more multi-agent learning systems will be deployed in the real world in which communication will play an integral part to their successes. Our formulation and method for learning where to communicate would lead to more robust communication and subsequently more coordinated behavior among agents by equipping them with the ability to discover the best and most reliable places to communicate. This is essential in such real-world applications when the reliability of communication often varies by locations and situations.\n\nBy increasing the applicability of multi-agent learning systems, our work may exacerbate some risks of deploying machine learning systems like increasing unemployment (e.g., replacing warehouse workers with a fully autonomous warehouse) and advancing automated weaponry. Particularly to our method, by equipping systems with the ability to discover where to communicate, it could encourage adversarial attacks to deployed machine learning systems through communication which would lead to unintended and potentially harmful actions and disrupt learned communication protocols.\n\nK FUTURE WORK\n\nFor future work, we are interested in testing our framework in more complex settings and address some limitations discussed in Appendix I. For instance, the ability to discover communication channels can be formulated as skills and organized in a hierarchical manner to solve harder tasks. It would also be insightful to benchmark different algorithms for cheap talk utilization in this setting.\n\n17",
    "reference": "# Summary Of The Paper\n\nAuthors design and implement an algorithm that operates under fewer assumptions than some communication-based MARL works. Agents learn to discover which environment actions and communicative actions to take, so as to maximize the mutual information (MI) with respect to observation at the receiver's end. The proposed CTDUL outperforms several baselines in the phone booth maze environment.\n\n# Strength And Weaknesses\n\nPaper addresses a well-defined niche by relaxing a set of common assumptions found in some communication-based MARL methods. The tested evaluation environment is intuitive. Claims made with an intuitive approach (MI maximization) are shown to function correctly in the empirical evaluation.\n\nA number of factors can make the paper much stronger: problem positioning, clarified definition (especially action and channel), varied environments, better justified baseline choice, and comments on computation overhead.\n\nWhile the problem addressed is clearly an interesting setting, real-world settings might question its practicality. If the communication channels are not even known a priori, could there really be multiple agents who happen to have access to the same channels and also coincidentally need to cooperate? Consider a swarm of drones. If one common administrator is operating all of those drones as in a search-and-rescue operation, it is almost always the case that their communication channels are known pre-dispatch. On the other hand, if different operators are managing each of those drones as in individual drone racing competitions, then it may well be the case that there simply is no need for cooperative/coordinated flight management. It feels as though the removal of assumptions inadvertently led to a less realistic problem setting, so a better problem positioning could more effectively highlight the significance of the proposed solution to the problem.\n\nCommunicative actions and channels could be defined more clearly. If the term \"channels\" are correctly aligned with the same term in the radio communications sense, then communicative actions being \"hint up\" or \"hint down\" can be a misinterpretation of communication. Instead, should the concepts of channels remain faithful to radio communications literature, then the problem of which channels to discover should also pertain to the communication channels, such as which bandwidths. The current formulation of A_comm is probably more aptly described as message encoding since the chosen action answers the question of \"which message should be sent\". However, if this indeed is the case, then CTDUL's A_comm would simply be hard-coded actions.\n\nIt would be a valuable addition to the paper to investigate CTDUL's performance in other environments, such as SMAC or MAMuJoCo.\n\nIt would also be an insightful curation of CTDUL's positioning to include SchedNet (ICLR 2019), which addressed medium contention and medium access constraints in MARL, and DIAYN (2018), which is an information-theoretic approach to RL exploration and then identify CTDUL's strengths, differences, and delimitation.\n\nPlease also elaborate on the cost of computing pairwise mutual information and how this affects the empirical performance of CTDUL.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity\nProblem setting and definitions can be made clearer and can help in making a much stronger case for the paper's main arguments.\n\nQuality\nCurrent set of chosen baselines makes it difficult to assess the quality of the proposed model. More immediate competitors should be tested under more varied environments. Comments on PMI computation should accompany evaluation results. On a different note, the heatmap should probably be explained in more detail since it may not be the most conventional visual aid in related literature.\n\nOriginality\nMutual information maximization is not a novel training objective. It is difficult to evaluate the novelty of A_comm since it can be better clarified. OBL is also an independent previous work. Put together, groundbreaking novelty might not be the paper's appeal, but the structured combination of PMI and OBL does work well in harshly limited settings.\n\n# Summary Of The Review\n\nProposed method works well for a specific setting, whose characteristics can be better marketed as more realistic. Some definitions need clarification, and related literature can benefit from a closer investigation into the papers suggested above.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nIN SEARCH OF SMOOTH MINIMA FOR PURIFYING BACKDOOR IN DEEP NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe success of a deep neural network (DNN) heavily relies on the details of the training scheme; e.g., training data, architectures, hyper-parameters, etc. Recent backdoor attacks suggest that an adversary can take advantage of such training details and compromise the integrity of a DNN. Our studies show that a backdoor model is usually optimized to a bad local minima, i.e., sharper minima as compared to a benign model. Intuitively, a backdoor model can be purified by re-optimizing the model to a smoother minima through fine-tuning with a few clean validation data. However, fine-tuning all DNN parameters often requires huge computational cost and often results in sub-par clean test performance. To address this concern, we propose a novel backdoor purification technique—Natural Gradient Fine-tuning (NGF)—which focuses on removing backdoor by fine-tuning only one layer. Specifically, NGF utilizes a loss surface geometry-aware optimizer that can successfully overcome the challenge of reaching a smooth minima under a one-layer optimization scenario. To enhance the generalization performance of our proposed method, we introduce a clean data distribution-aware regularizer based on the knowledge of loss surface curvature matrix, i.e., Fisher Information Matrix. Extensive experiments show that the proposed method achieves state-of-the-art performance on a wide range of backdoor defense benchmarks: four different datasets—CIFAR10, GTSRB, Tiny-ImageNet, and ImageNet; 13 recent backdoor attacks, e.g., Blend, Dynamic, WaNet, ISSBA, etc.\n\n1\n\nINTRODUCTION\n\nTraining a deep neural network (DNN) with a fraction of poisoned or malicious data is often securitycritical since the model can successfully learn both clean and adversarial tasks equally well. This is prominent in scenarios where one outsources the DNN training to a vendor. In such scenarios, an adversary can mount backdoor attacks (Gu et al., 2019; Chen et al., 2017) through poisoning a portion of training samples so that the model will misclassify any sample with a particular trigger or pattern to an adversary-set label. Whenever a DNN is trained in such a manner, it becomes crucial to remove the effect of backdoor before deploying it for a real-world application.\n\nDifferent defense techniques (Liu et al., 2018; Wang et al., 2019; Wu & Wang, 2021; Li et al., 2021a; Zheng et al., 2022) have been proposed for purifying backdoor. Techniques such as fine-pruning (Liu et al., 2018) and adversarial neural pruning (Wu & Wang, 2021) require a long training time due to iterative searching criteria. Furthermore, the purification performance deteriorates significantly as the attacks get stronger. In this work, we explore the backdoor insertion and removal phenomena from the DNN optimization point of view. Unlike a benign model, a backdoor model is forced to learn two different data distributions: clean data distribution and poisoned/trigger data distribution. Having to learn both distributions, backdoor model optimization usually leads to a bad local minima or sharper minima w.r.t. clean distribution. We claim that backdoor can be removed by re-optimizing the model to a smoother minima. One easy re-optimization scheme could be simple DNN weights fine-tuning with a few clean validation samples. However, fine-tuning all DNN parameters often requires huge computational cost and may result in sub-par clean test performance after purification. Therefore, we intend to fine-tune only one layer to effectively remove the backdoor.\n\nFine-tuning only one layer creates a shallow network scenario where SGD-based optimization becomes a bit challenging. Choromanska et al. (2015) claims that the probability of finding bad local minima or poor quality solution increases as the network size decreases. Even though there are\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ngood-quality solutions, it usually requires exponentially long time to find those minima (Choromanska et al., 2015). As a remedy to this, we opt to use a curvature aware optimizer, Natural Gradient Decent (NGD), that has higher probability of escaping the bad local minima as well as faster convergence rate, specifically in the shallow network scenario (Amari, 1998; Martens & Grosse, 2015). To this end, we propose a novel backdoor purification technique—Natural Gradient Fine-tuning (NGF)— which focuses on removing backdoor through fine-tuning only one layer. However, straightforward application of NGF with simple cross-entropy (CE) loss may result in poor clean test performance. To boost this performance, we use a clean distribution-aware regularizer that prioritizes the update of parameters sensitive to clean data distribution. Our proposed method achieves SOTA performance in a wide range of benchmarks, e.g., four different datasets including ImageNet, 13 recent backdoor attacks etc. Our contributions can be summarized as follows:\n\n• We analyze the loss surface characteristics of a DNN during backdoor insertion and purification processes. Our analysis shows that the optimization of a backdoor model leads to a bad local minima or sharper minima compared to a benign model. We argue that backdoor can be purified by re-optimizing the model to a smoother minima and simple fine-tuning can be a viable way for that. To the best of our knowledge, this is the first work that studies the correlation between loss-surface smoothness and backdoor purification.\n\n• We conduct additional studies on backdoor purification process while fine-tuning different parts of a DNN. We observe that SGD-based one-layer fine-tuning fails to escape bad local minima and a loss surface geometry-aware optimizer can be an easy fix to this.\n\n• We propose a novel backdoor purification technique based on Natural Gradient Fine-tuning (NGF). In addition, we employ a clean distribution-aware regularizer to boost the clean test performance of our proposed method. NGF outperforms recent SOTA methods in a wide range of benchmarks.\n\n2 RELATED WORK\n\nBackdoor Attacks: Backdoor triggers can exist in the form of dynamic patterns (Li et al., 2020), a single pixel (Tran et al., 2018), sinusoidal strips (Barni et al., 2019), human imperceptible noise (Zhong et al., 2020), natural reflection (Liu et al., 2020), adversarial patterns (Zhang et al., 2021), blending backgrounds (Chen et al., 2017), etc. Based on target labels, existing backdoor attacks can generally be classified as poison-label or clean-label backdoor attacks. In poison-label backdoor attack, the target label of the poisoned sample is different from its ground-truth label, e.g., BadNets (Gu et al., 2019), Blended attack (Chen et al., 2017), SIG attack (Barni et al., 2019), WaNet (Nguyen & Tran, 2021), Trojan attack (Liu et al., 2017), and BPPA (Wang et al., 2022). Contrary to the poison-label attack, clean-label backdoor attack doesn’t change the label of the poisoned sample (Turner et al., 2018; Huang et al., 2022; Zhao et al., 2020b). Recently, Saha et al. (2022) studied backdoor attacks on self-supervised learning.\n\nBackdoor Defenses: Existing backdoor defense methods can be categorized into backdoor detection or purifying techniques. Detection based defenses include trigger synthesis approach (Wang et al., 2019; Qiao et al., 2019; Guo et al., 2020; Shen et al., 2021; Dong et al., 2021; Guo et al., 2021; Xiang et al., 2022; Tao et al., 2022), or malicious samples filtering based techniques (Tran et al., 2018; Gao et al., 2019; Chen et al., 2019). However, these methods only detect the existence of backdoor without removing it. Backdoor purification defenses can be further classified as training time defenses and inference time defenses. Training time defenses include model reconstruction approach (Zhao et al., 2020a; Li et al., 2021b), poison suppression approach (Hong et al., 2020; Du et al., 2019; Borgnia et al., 2021), and pre-processing approaches (Li et al., 2021a; Doan et al., 2020). Although training time defenses are often successful, they suffer from huge computational burden and less practical considering attacks during DNN outsourcing. Inference time defenses are mostly based on pruning approaches such as (Koh & Liang, 2017; Ma & Liu, 2019; Tran et al., 2018; Diakonikolas et al., 2019; Steinhardt et al., 2017). Pruning-based approaches are typically based on model vulnerabilities to backdoor attacks. For example, MCR (Zhao et al., 2020a) and CLP (Zheng et al., 2022) analyzed node connectivity and channel Lipschitz constant to detect backdoor vulnerable neurons. ANP (Wu & Wang, 2021) prune neurons through backdoor sensitivity analysis using adversarial search on the parameter space. Instead, we propose a simple one-layer fine-tuning based defense that is both fast and highly effective. To remove backdoor, our proposed method revisits the DNN fine-tuning paradigm from a novel point of view.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3 BACKGROUND Attack Model. We consider an adversary with the capabilities of carrying a backdoor attack on a DNN model, fθ : Rd → Rc, by training it on a poisoned data set Dtrain = {Xtrain, Ytrain}. Here, θ is the parameters of the model, d is the input data dimension and c is the total number of classes. The data poisoning happens through specific set of triggers that can only be accessed by the attacker. The adversary goal is to train the model in a way such that any triggered samples ˆx = x + δ ∈ Rd will be wrongly misclassified to a target label, ̄y. Here, x is a clean test sample and δ ∈ Rd represents the trigger pattern with the properties of ||δ|| ≤ ε; where ε is the trigger magnitude determined by its shape, size and color. We define the poison rate as the ratio of poison and clean data in Dtrain. An attack is considered successful if the model behaves as: fθ(x) = y and fθ(ˆx) = ̄y, where y is the true label for x. We use attack success rate (ASR) for quantifying such success.\n\nDefense Goal. We consider a defender with a task to purify the backdoor model fθ using a small clean validation set (usually 1 ∼ 10% of the training data). The goal is to repair the model in a way such that it becomes immune to attack, i.e., fθp (ˆx) = y. Here, fθp is the final purified model.\n\nNatural Gradient Descent (NGD). Let us consider a model p(y|x, θ) with parameters θ ∈ RN to be fitted with input data {(xi, yi)}|Dtrain| from an empirical data distribution Px,y, where xi ∈ Xtrain is an input sample and yi ∈ Ytrain is its label. We try to optimize the model by solving:\n\ni=1\n\nθ∗ ∈ arg min\n\nL(θ),\n\nθ\n\n(1)\n\nwhere L(θ) = L(y, fθ(x)) = E(xi,yi)∼Px,y [−log p(y|x, θ)] is the expected full-batch cross-entropy (CE) loss. SGD optimizes for θ∗ iteratively following the direction of the steepest descent (estimated by column vector, ∇θL) and update the model parameters by: θ(t+1) ← θ(t) − α(t) · ∇(t) θ L, where α is the learning rate. Since SGD uses the Identity matrix as the pre-conditioner, it is uninformed of the geometry of loss surface.\n\nIn NGD, however, the Fisher Information Matrix (FIM) is used as a pre-conditioner, which can be defined as (Martens & Grosse, 2015),\n\n(2)\n\nθ L. Here, the natural gradient is defined as F (θ(t))−1∇(t)\n\nF (θ) = E(x,y)∼Px,y [∇θ log p(y|x, θ) · (∇θ log p(y|x, θ))T ] ∈ RN ×N . As FIM is a loss surface curvature matrix, a careful integration of it in the update rule of θ will make the optimizer loss surface geometry aware. Such integration leads us to the update equation of NGD, θ(t+1) ← θ(t) − α(t) · F (θ(t))−1∇(t) θ L. From the perspective of information geometry, natural gradient defines the direction in parameter space which gives largest change in objective per unit of change in model (p(y|x, θ)). Per unit of change in model is measured by KL-divergence (Amari, 1998; Park et al., 2000). Note that KL-divergence is well connected with FIM as it can be used as a local quadrature approximation of KL-divergence of model change. Eqn. 2 suggests that one requires the knowledge of the original parameter (θ) space to estimate it. Therefore, FIM can be thought of as a mechanism to translate between the geometry of the model (p(y|x, θ)) and the current parameters (θ) of the model. The way natural gradient defined the direction in parameter space is contrastive to the stochastic gradient. Stochastic gradient defines the direction in parameter space for largest change in objective per unit of change in parameter (θ) measured by Eucludian distance. That is, the gradient direction is solely calculated based on the changes of parameters, without any knowledge of model geometry.\n\n4 SMOOTHNESS ANALYSIS OF BACKDOOR MODELS\n\nIn this section, we analyze the loss surface geometry of benign, backdoor, and purified models. To study the loss curvature properties of different models, we aim to analyze the Hessian of loss, H = ∇2 θL, where we compute L using the clean training set. The Hessian matrix H is symmetric and one can take the spectral decomposition H = QΛQT , where Λ = diag(λ1, λ2, . . . , λN ) contains the eigenvalues and Q = [q1q2 . . . qN ] are the eigenvectors of H. As a measure for smoothness, we take the maximum eigenvalue, λmax(= λ1), and the trace of the Hessian, Tr(H) = (cid:80)i=N i=1 diag(H)i. Low values for these two proxies indicate the presence of highly smooth loss surface (Jastrzebski et al., 2020). The Eigen Spectral density plots in Fig. 1a- 1b tell us about the optimization of benign and backdoor models. To create these models, we use the CIFAR10 dataset and train a PreActResNet18 architecture for 200 epochs. To insert the backdoor, we use TrojanNet (Liu et al., 2017) and a poison\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Benign Model\n\n(b) Backdoor Model\n\n(c) Purified Model (SGD)\n\n(d) Purified Model (NGF)\n\nFigure 1: Eigen Spectral Density plots of Loss Hessian for (a) benign, (b) backdoor (TrojanNet (Liu et al., 2017)), and (c & d) purified models. In each plot, the maximum eigenvalue (λmax), trace of Hessian (Tr(H)), clean test accuracy (ACC), and attack success rate (ASR) are also reported. Here, low λmax and Tr(H) hints at the presence of smoother loss surface which often results in low ASR and high ACC. (a & b). Compared to a benign model, a backdoor model tends to reach a sharper minima as shown by the larger range of eigenvalues (x-axis). During purification, SGD optimizer (c) rarely escapes sharp or bad local minima (similar λmax and Tr(H) as the backdoor model) while our proposed method, NGF, (d) converges to a smooth minima. We use CIFAR10 dataset with a PreActResNet18 (He et al., 2016) architecture for all evaluations.\n\nTable 1: Backdoor removal performance while fine-tuning (FT) different parts of a DNN. Fine-tuning only the last layer creates a shallow network scenario. In such scenario, there is a high probability that SGD does not escape bad local minima. Whereas, NGF consistently optimizes to a smooth minima (indicated by low λmax for 6 different attacks) which results in backdoor removal, i.e., low ASR and high ACC. We consider CIFAR10 dataset and PreActResNet18 architecture for all evaluations. A clean validation set is used for all purification.\n\nFT Methods\n\nInitial Full-Net. CNN-Bbone. Cls. (SGD)\n\nBadnets ASR\n\n100 4.87 5.03 98.27\n\nλmax\n\n573.8 4.42 4.71 556.1\n\nCls. (NGF)\n\n2.79\n\n1.86\n\nACC\n\n92.96 85.92 85.64 90.17\n\n88.32\n\nλmax\n\n715.5 4.65 5.14 541.7\n\nBlend ASR\n\n100 4.77 4.92 97.29\n\n2.43\n\n0.38\n\nACC\n\n94.11 87.61 87.24 93.48\n\n91.17\n\nλmax\n\n616.3 3.41 4.19 613.0\n\nTrojan ASR\n\n100 3.78 3.95 96.25\n\n2.74\n\n2.64\n\nACC\n\n89.57 82.18 81.86 87.36\n\n84.21\n\nDynamic ASR\n\n100 4.73 5.11 93.58\n\nλmax\n\n564.2 2.34 2.46 446.5\n\n1.19\n\n1.17\n\nACC\n\n92.52 88.61 87.54 91.36\n\n90.97\n\nλmax\n\n717.6 4.68 5.19 361.9\n\nCLB ASR\n\n100 1.83 2.08 89.21\n\n3.13\n\n1.04\n\nACC\n\n92.78 87.41 86.67 91.73\n\n88.37\n\nλmax\n\n514.1 8.98 9.74 563.2\n\nSIG ASR\n\n100 1.04 1.61 96.70\n\n1.48\n\n0.12\n\nACC\n\n88.64 81.92 81.55 86.92\n\n84.16\n\nrate of 10%. From the comparison of λmax and Tr(H), we can conjecture that optimization of a benign model produces smoother loss surface. We observe similar phenomena for different datasets and architectures; details are in Appendix F. The main difference between a benign and a backdoor model is that the latter needs to learn two different data distributions: clean and poison. Based on our observations, we state following conjectures:\n\nConjecture 1. Having to learn two different data distributions, a backdoor model reaches a sharper minima, i.e., large λmax and Tr(H), as compared to the benign model.\n\nWe support this conjecture with empirical evidence presented in Table 1. Looking at the λmax in the ’Initial’ row for all 6 attacks (details are in Appendix D), it can be observed that all of these backdoor models optimizes to a sharp minima. As these models are optimized on both distributions, they also have high attack success rates (ASR) as well as high clean test accuracy (ACC). Note that, the measure of smoothness is done w.r.t. clean data distribution. The use of clean distribution in our smoothness analysis is driven from the practical consideration as our particular interest lies with the performance w.r.t. clean distribution; more details are in Appendix C.1. Since high ASR and ACC indicate that the model had learned both distributions, it supports Conjecture 1.\n\nConjecture 2. Through proper fine-tuning with clean validation data, a backdoor model can be re-optimized to a smoother minima w.r.t. clean data distribution. Optimization to a smoother minima leads to backdoor purification, i.e., low ASR and high ACC.\n\nBy proper fine-tuning, we imply that the fine-tuning will lead to an optimal solution w.r.t. the data distribution we fine-tune the model with. To support Conjecture 2, we show the removal performances of fine-tuning based purification methods in Table 1. To remove backdoor using a clean validation set (∼1% of train-set), we fine-tune different parts of the DNN for 100 epochs with a learning rate of 0.01. As shown in Table 1, after proper fine-tuning (Full-Net, CNN-Bbone), the backdoor model re-optimizes to a smoother minima that leads to successful backdoor removal.\n\nOne-Layer Fine-tuning: We observe that one can remove the backdoor by fine-tuning either the full network or only the CNN backbone (using SGD). However, these methods can be computationally costly and less practical. Furthermore, such fine-tuning often leads to high drop in ACC. As an alternative, one could fine-tune only the last or classification (Cls.) layer. However, even with a small validation set, a one-layer network becomes a shallow network to optimize. According to the\n\n4\n\n1011000100101Eigenvalue108106104102100Density (Log Scale)max:20.1Tr(H):129.7ACC:95.3ASR:0.01021011000100101102Eigenvalue108106104102100Density (Log Scale)max:616.3Tr(H):7097.7ACC:89.6ASR:100.01021011000100101102Eigenvalue108106104102100Density (Log Scale)max:613.0Tr(H):8258.5ACC:87.4ASR:96.21000100Eigenvalue108106104102100102Density (Log Scale)max:2.7Tr(H):17.3ACC:84.2ASR:2.7Under review as a conference paper at ICLR 2023\n\nspin-glass analogy in Choromanska et al. (2015), as the network size decreases the probability for the SGD optimizer to find sharp local minima or poor quality minima increases accordingly. In case of shallow network, the quality of minima is decided by their distances from the global minima. Choromanska et al. (2015) also observes that the process of finding a path from bad local minima to a good quality solution or global minima takes exponentially long time. Therefore, it is not always feasible to use the SGD optimizer for shallow network. Table 1 (row–Cls. (SGD)) corroborates this hypothesis as SGD optimizer fails to escape the sharp minima resulting in similar ASRs as the initial backdoor model. Instead of using SGD, one can use natural gradient descent (NGD) that has higher probability of escaping the bad local minima as well as faster convergence rate, specifically in the shallow network scenario (Amari, 1998; Martens & Grosse, 2015). Therefore, to effectively purify a backdoor model, we propose a novel Fisher Information matrix based backdoor purification objective function and optimize it using the NGD optimizer.\n\n4.1 NATURAL GRADIENT FINE-TUNING (NGF)\n\nLet us decompose the model parameters θ as,\n\nθ = {W0,1, W1,2, W2,3, · · · , WL−1,L}\n\nhere, Wi,i+1 is the parameters between layer i and layer i + 1, commonly termed as (i + 1)th layer’s parameters. WL−1,L is the Lth layer’s (Cls. layer) parameters and we are particularly interested in fine-tuning only this layer. Now, consider a validation set, Dval = {Xval, Yval} that contains only clean samples. We denote θL(= WL−1,L) as the Lth layer’s parameters. To purify the backdoor model, we formulate the following loss\n\nLp(y, fθ(x)) ≈ L(y, fθ(x)) +\n\nη 2\n\n(cid:88)\n\ni\n\ndiag(F ( ̄θL))i · (θL,i − ̄θL,i)2,\n\n(3)\n\nwhich is a combination of the CE loss on the validation set and a regularizer. Here, ̄θL (fixed) is Lth layer parameters of the initial backdoor model, i.e., θ(0)\n\nL = ̄θL .\n\nIn a backdoor model, some neurons/parameters are more vulnerable than others. The vulnerable parameters are believed to be the ones that are sensitive to poison/trigger data distribution (Wu & Wang, 2021). In general, CE loss does not discriminate whether a parameter is more sensitive to clean or poison distribution. Such lack of discrimination may allow drastic/unwanted changes to the parameters responsible for learned clean distribution. This usually leads to sub-par clean test accuracy after purification and it requires additional measures to fix this issue. Motivated by Kirkpatrick et al. (2017), we introduce a clean distribution aware regularization term as a product of two terms: i) an error term that accounts for the deviation of θL from ̄θL; ii) a vector, diag(F ( ̄θL)), consisting of the diagonal elements of FIM (F ( ̄θL)). As the first term controls the changes of parameters w.r.t. ̄θL, it helps the model to remember the already learned distribution. However, learned data distribution consists of clean and poison distribution both. To explicitly force the model to remember the clean distribution, we compute F ( ̄θL) using a clean validation set; with similar distribution as the learned clean data. Note that, diag(F ( ̄θL))i represents the square of the derivative of log-likelihood of clean distribution w.r.t. ̄θL,i, [∇ ̄θL,ilog p(y|x, θ)]2 (ref. eqn. (6)). In other words, diag(F ( ̄θL))i is the measure of importance of ̄θL,i towards remembering the learned clean distribution. If diag(F ( ̄θL))i has a higher importance, we allow minimal changes to ̄θL,i over the purification process. This careful design of such regularizer improves the clean test performance significantly. We use η as a regularization constant.\n\nThe overall optimization problem using the loss-function defined in (3) for purifying the backdoor model fθ is as follows:\n\nObjective function:\n\nθp := arg min\n\nθL\n\nLp(y, fθ(x)); x ∈ Xval, y ∈ Yval\n\nUpdate Policy:\n\nwhere, F (θL) :=\n\nL ← θ(t) θ(t+1) n\n(cid:88)\n\n1 n\n\nj=1\n\nL − αF (θ(t)\n\nL )−1∇θLLp\n\n(cid:0)∇θLlog p(yj|xj, θ) · (∇θLlog (yj|xj, θ))T (cid:1) .\n\n(4)\n\n(5)\n\n(6)\n\nHere, F ∈ R|θL|×|θL| is the FIM, and n is the validation set size. Notice that, as we only consider fine-tuning of Lth-layer, the computation of F and F −1 (|θL| × |θL| matrices) becomes tractable.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Comparison of different defense methods for four benchmark datasets. Backdoor removal performance, i.e., drop in ASR, against a wide range of attacking strategies show the effectiveness of NGF. For CIFAR10 and GTSRB, the poison rate is 10%. For Tiny-ImageNet and ImageNet, we employ ResNet34 and ResNet50 architectures, respectively. We use a poison rate of 5% for these 2 datasets and report performance on successful attacks (ASR close to 100%) only. Average drop (↓) indicates the % changes in ASR/ACC compared to the baseline, i.e., ASR/ACC of No Defense. Higher ASR drop and lower ACC drop is desired for a good defense.\n\nDataset\n\nCIFAR-10\n\nGTSRB\n\nTiny-ImageNet\n\nImageNet\n\nMethod\n\nAttacks\n\nBenign Badnets Blend Troj-one Troj-all SIG Dyn-one Dyn-all CLB CBA FBA WaNet ISSBA BPPA\n\nNo Defense\n\nVanilla FT\n\nANP\n\nI-BAU\n\nAWM\n\nNGF (Ours)\n\nASR\n\nACC\n\n0 100 100 100 100 100 100 100 100 93.20 100 98.64 99.80 99.70\n\n95.21 92.96 94.11 89.57 88.33 88.64 92.52 92.61 92.78 90.17 90.78 92.29 92.80 93.82\n\nASR\n\n0 4.87 4.77 3.78 3.91 1.04 4.73 4.28 1.83 27.80 7.95 5.81 6.76 9.94\n\nACC\n\n92.28 85.92 87.61 82.18 81.95 81.92 88.61 88.32 87.41 83.79 82.90 86.70 85.42 90.23\n\nASR\n\n0 2.84 3.81 5.47 5.53 0.37 1.78 2.19 1.41 45.11 66.70 3.18 3.82 10.46\n\nACC\n\n93.98 85.96 89.10 85.20 84.89 83.60 86.26 84.51 85.07 85.63 87.42 89.24 89.20 90.57\n\nASR\n\n0 9.72 11.53 7.91 9.82 4.12 10.48 10.30 5.78 36.12 10.66 10.72 12.48 9.94\n\nACC\n\n93.56 87.85 90.84 87.24 85.94 83.57 89.16 89.74 86.70 85.05 87.35 85.94 90.03 90.68\n\nASR\n\n0 4.34 2.13 5.41 4.42 0.90 3.35 2.46 1.89 38.81 22.31 2.96 4.57 10.60\n\nACC\n\n93.80 86.17 88.93 86.45 84.60 83.38 88.41 87.72 84.18 85.58 87.06 89.45 89.59 90.88\n\nASR\n\n0 1.86 0.38 2.64 2.79 0.12 1.17 1.61 1.04 24.60 6.21 2.38 4.24 7.14\n\nACC\n\n94.10 88.32 91.17 84.21 86.10 84.16 90.97 90.19 88.37 85.97 86.96 89.65 90.18 91.84\n\nAvg. Drop\n\n-\n\n-\n\n92.61 ↓\n\n6.03 ↓\n\n87.59 ↓\n\n4.98 ↓\n\n87.82 ↓\n\n3.95 ↓\n\n91.32 ↓\n\n4.53 ↓\n\n95.01 ↓\n\n3.33 ↓\n\nBenign Badnets Blend Troj-one Troj-all SIG Dyn-one Dyn-all BPPA\n\n0 100 100 99.50 99.71 97.13 100 100 99.18\n\n97.87 97.38 95.92 96.27 96.08 96.93 97.27 97.05 98.12\n\n0 1.36 5.08 2.07 2.48 1.93 2.27 2.84 5.14\n\n93.08 88.16 89.32 90.45 89.73 91.41 91.26 91.42 94.48\n\n0 0.35 4.41 1.81 2.16 6.17 2.08 2.49 7.19\n\n95.42 93.17 93.02 92.74 92.51 91.82 93.15 92.89 93.79\n\n0 2.72 4.13 3.04 2.79 2.64 5.82 4.87 8.63\n\n96.18 94.55 94.30 93.17 93.28 93.10 95.54 93.98 94.50\n\n0 2.84 4.96 2.27 1.94 5.32 1.89 2.74 5.43\n\n95.32 93.58 92.75 93.56 92.84 92.68 93.52 93.17 94.22\n\n0 0.24 2.91 1.21 1.58 3.24 1.51 1.26 4.45\n\n95.76 94.11 93.31 94.18 93.87 93.48 94.27 94.14 95.27\n\nAvg. Drop\n\n-\n\n-\n\n96.54 ↓\n\n6.10 ↓\n\n96.10↓\n\n3.99 ↓\n\n95.11 ↓\n\n2.83 ↓\n\n96.02 ↓\n\n3.59 ↓\n\n97.39 ↓\n\n2.79 ↓\n\nBenign Badnets Trojan Blend SIG CLB\n\n0 100 100 100 98.48 97.71\n\n62.56 59.80 59.16 60.11 60.01 60.33\n\n0 3.84 6.77 2.18 5.02 5.61\n\n58.20 53.58 52.62 51.22 52.18 51.68\n\n0 61.23 79.56 81.58 28.67 16.24\n\n59.29 55.41 54.76 54.70 54.71 55.18\n\n0 13.29 11.94 17.42 9.31 10.68\n\n59.34 54.56 55.10 54.19 55.72 54.93\n\n0 31.44 38.23 41.37 27.68 36.52\n\n59.08 54.81 54.28 53.78 54.11 55.02\n\n0 2.34 3.38 1.58 2.81 4.06\n\n59.67 55.84 54.87 54.98 54.63 55.40\n\nAvg. Drop\n\n-\n\n-\n\n94.55 ↓\n\n7.63 ↓\n\n45.38↓\n\n4.93 ↓\n\n86.71 ↓\n\n4.98 ↓\n\n64.19 ↓\n\n5.48 ↓\n\n96.40 ↓\n\n4.74 ↓\n\nBenign Badnets Trojan Blend SIG CLB\n\n0 99.24 99.21 100 94.66 95.06\n\n77.06 74.53 74.02 74.42 74.69 74.14\n\n0 5.91 4.63 4.43 3.23 3.71\n\nAvg. Drop\n\n-\n\n-\n\n93.25 ↓\n\n73.52 69.37 69.15 70.20 69.82 69.19\n\n4.81↓\n\n0 43.31 38.81 57.79 16.28 18.37\n\n68.85 66.28 66.14 65.51 66.08 66.41\n\n0 21.87 25.74 27.45 15.37 21.64\n\n74.21 69.46 69.35 68.61 70.02 69.70\n\n0 21.18 28.85 34.15 16.47 23.50\n\n71.63 69.44 68.62 68.91 69.74 69.32\n\n0 4.61 4.02 3.83 2.94 3.05\n\n74.51 70.46 69.97 70.52 71.36 70.25\n\n62.72↓\n\n8.28 ↓\n\n75.22 ↓\n\n4.93 ↓\n\n72.80 ↓\n\n5.15 ↓\n\n93.94 ↓\n\n3.85 ↓\n\nAfter solving the above optimization problem, we will get modified parameters, WL−1,L. Finally, we get the purified model, fθp with θp as\n\nθp = {W0,1, W1,2, W2,3, · · · , WL−1,L}\n\nFig. 1c-1d show that NGF indeed does reach the smooth minima as opposed to SGD based fine-tuning. We provide additional results in Table 1 for both NGF and SGD. Notice that the purified model seems to have a smoother loss surface than the benign model (2.7 vs. 20.1 for λmax). This, however, does not translate to better ACC than the benign model. The ACC of the purified model is always bounded by the ACC of the backdoor model. To the best of our knowledge, our study on the correlation between loss-surface smoothness and backdoor purification is novel. NGF is also the first method to employ a second-order optimizer for purifying backdoor. More details are in Appendix C\n\n5 EXPERIMENTAL RESULTS 5.1 EVALUATION SETTINGS\n\nDatasets: To begin with, we evaluate our proposed method through conducting a wide range of experiments on two widely used datasets for backdoor attack study: CIFAR10 (Krizhevsky et al., 2009) with 10 classes, GTSRB (Stallkamp et al., 2011) with 43 classes. As a test of scalability, we also consider Tiny-ImageNet (Le & Yang, 2015) with 100,000 images distributed among 200 classes and ImageNet (Deng et al., 2009) with 1.28M images distributed among 1000 classes.\n\nAttacks Configurations: We consider 13 state-of-the-art backdoor attacks: 1) Badnets (Gu et al., 2019), 2) Blend attack (Chen et al., 2017), 3 & 4) TrojanNet (Troj-one & Troj-all) (Liu et al., 2017), 5) Sinusoidal signal attack (SIG) (Barni et al., 2019), 6 & 7) Input-Aware Attack (Dyn-one and Dyn-all) (Nguyen & Tran, 2020), 8) Clean-label attack (CLB) (Turner et al., 2018), 9) Composite\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a) λmax vs. Epochs\n\n(b) ACC/ASR vs. Epochs\n\n(c) λmax vs. Epochs\n\n(d) ACC/ASR vs. Epochs\n\nFigure 2: Loss Surface characteristics of a DNN during backdoor insertion and purification processes. a & b) As the joint optimization on clean and poison distribution progresses, i.e., high ACC & ASR, the loss surface becomes less and less smoother, i.e., high λmax). c & d) One can purify backdoor by gradually making the loss surface smoother. We use CIFAR10 dataset with four different attacks.\n\nbackdoor (CBA) (Lin et al., 2020), 10) Deep feature space attack (FBA) (Cheng et al., 2021), 11) Warping-based backdoor attack (WaNet) (Nguyen & Tran, 2021), 12) Invisible triggers based backdoor attack (ISSBA) (Li et al., 2021c), and 13) Quantization and contrastive learning based attack (BPPA) (Wang et al., 2022). To ensure fair comparison, we follow the similar trigger patterns and settings as in their original papers. In Troj-one and Dyn-one attacks, all of the triggered images have same target label. On the other hand, target labels are uniformly distributed over all classes for Troj-all and Dyn-all attacks. For creating these attacks on CIFAR10 and GTSRB, we use a poison rate of 10% and train a PreActResNet18 (He et al., 2016) and a WideResNet-16-1 (Zagoruyko & Komodakis, 2016) architectures, respectively, for 250 epochs with an initial learning rate of 0.01. More details on hyper-parameters and overall training settings can be found in Appendix D.\n\nDefenses Configurations: We compare our approach with 4 existing backdoor mitigation methods: 1) Vanilla Fine-Tuning (FT); where we fine-tune all DNN parameters, 2) Adversarial Neural Pruning (ANP) (Wu & Wang, 2021) with 1% clean validation data, 3) Implicit Backdoor Adversarial Unlearning (I-BAU) (Zeng et al., 2021) 4) Adversarial Weight Masking (AWM) (Chai & Chen, 2022). We also compare NGF with another recent defense technique described in (Zheng et al., 2022). However, we present this comparison in the Appendix E due to several performance issues.1 To apply NGF on CIFAR10, we fine-tune the last layer of the DNN for Ep epochs with 1% clean validation data. Here, Ep is the number of purification epochs and we choose a value of 100 for this. For optimization, we choose a learning rate of 0.01 with a decay rate of 0.1/40 epochs and consider regularization constant η to be 0.1. Additional experimental details for NGF and other defense methods are in Appendix D.3. For GTSRB, we increase the validation size to 3% as there are less samples available per class. Rest of the training settings are same as CIFAR10. For NGF on Tiny-ImageNet, we consider a validation size of 5% as a size less than this seems to hurt clean test performance (after purification). We fine-tune the model for 15 epochs with an initial learning rate of 0.01 with a decay rate of 0.3/epoch. Finally, we validate the effectiveness of NGF on ImageNet. For removing the backdoor, we use 3% validation data and fine-tune for 2 epochs. A learning rate of 0.001 has been employed with a decay rate of 0.005 per epoch. We define the effectiveness of a defense method in terms of average drop in ASR and ACC over all attacks. A highly effective method should have a high drop in ASR with a low drop in ACC. We define ASR as the percentage of poison test samples that are classified to the adversary-set target label.\n\n5.2 PERFORMANCE EVALUATION OF NGF\n\nIn Table 2, we present the performance of different defenses for four different datasets.\n\nCIFAR10: We consider five label poisoning attacks: Badnets, Blend, TrojanNet, Dynamic, and BPPA. For TorjanNet, we consider two different variations based on label-mapping criteria: Troj-one and Troj-all. Regardless the complexity of the label-mapping type, our proposed method outperforms all other methods both in terms of ASR and ACC. We also create two variations for Dynamic attack: Dyn-one and Dyn-all. Dynamic attack optimizes for input-aware triggers that are capable of fooling the model; making it more challenging than the static trigger based attacks (Badnets, Blend and Trojan). However, NGF outperforms other methods by a satisfactory margin. We also consider attacks that does not change the label during trigger insertion, i.e., clean label attack. Two such attacks are CLB and SIG. For further validation of our proposed method, we use deep feature based attacks, CBA and FBA. Both of these attacks manipulates deep features for backdoor insertion. Compared\n\n1Based on our re-run, we notice significantly larger drop in ACC as compared to other defenses.\n\n7\n\n020406080100120Number of Epochs0200400600800Max. Eignevalue, maxBackdoor InsertionBenignBadnetsTrojanNetCLBSIG020406080100120Number of Epochs405060708090100ACC/ASRBackdoor InsertionBadnets (ASR)TrojanNet (ASR)CLB (ASR)SIG (ASR)Benign(ACC)Badnets (ACC)TrojanNet (ACC)CLB (ACC)SIG (ACC)0102030405060Number of Epochs010002000300040005000Max. Eignevalue, maxBackdoor PurificationBadnetsTrojanNetCLBSIG0102030405060Number of Epochs020406080ACC/ASRBackdoor PurificationBadnets (ASR)TrojanNet (ASR)CLB (ASR)SIG (ASR)Badnets (ACC)TrojanNet (ACC)CLB (ACC)SIG (ACC)Under review as a conference paper at ICLR 2023\n\nto other defenses, NGF shows better effectiveness against these diverse set of attacks achieving an average drop of 95.01% in ASR while sacrificing an ACC of 3.33% for that. Table 2 also shows the performance of baseline methods such as I-BAU and AWM. AWM performs similarly as ANP and often struggles to remove the backdoor.\n\nGTSRB: In case of GTSRB, almost all defenses perform similarly for Badnets and Trojan. This, however, does not hold for blend as we achieve an 2.17% ASR improvement over the next best method. The performance is consistent for other attacks as well. Overall, we record an average 97.39% ASR drop with only an 2.79% drop in ACC. In some cases, ACC for I-BAU are slightly better as it uses a much larger validation size (5%) for purification than other defense techniques.\n\nImageNet: For scalability test of NGF, we consider two large and widely used datasets, TinyImageNet and ImageNet. In consistence with other datasets, NGF obtains SOTA performance in these diverse datasets too. The effectiveness of ANP reduces significantly for this dataset. In case of large models and datasets, the task of identifying and pruning vulnerable neurons gets more complicated and may result in wrong neurons pruning.\n\n5.3 ABLATION STUDIES\n\nSmoothness Analysis of Different Attacks: We show the relationship between loss surface smoothness and backdoor insertion process in Fig. 2a-2b. During backdoor insertion, the model is optimized for 2 different data distributions: clean and poison. Compared to a benign model, the loss surface of a backdoor becomes much sharper as the model becomes well optimized for both distributions, i.e., model has both high ASR and high ACC. At the beginning of training, both backdoor and benign models are far from being well optimized. The difference between these models are prominent once the model reaches closer to the final optimization point. As shown in Fig. 2b, the training becomes reasonably stable after 100 epochs with ASR and ACC near saturation level. Comparing λmax of benign and all backdoor models after 100 epochs, we notice a sharp contrast in Fig. 2a. This validates our previous claim on loss surface smoothness of benign and backdoor models. During purification period as shown in 2c-2d, the model is being optimized to a smoother minima. As a result, ASR becomes close to 0 while retaining good clean test performance. Note that, we calculate loss Hessian and λmax using all DNN parameters. This indicates that changing the parameters of only one layer impacts the loss landscape of whole network. Even though the CNN-backbone parameters are frozen, NGF changes the last layer in a way such that whole backdoor network behaves differently, i.e., like a benign model.\n\nTable 3: Performance comparison of NGF to other SGD-based optimizers. A more suitable sharpness-aware SGD-based optimizer is also considered here. However, NGF is far more effective in purifying backdoor (lower ASR) due to its consistent convergence to smooth minima. We use CIFAR10 dataset for these evaluations.\n\nBadnets Blend Trojan Dynamic SIG CLB\n\n91.73 92.21 88.02 91.12 87.74 90.96\n\n90.12 91.11 88.33 90.79 88.04 90.97\n\n91.16 91.67 88.51 91.45 87.98 90.86\n\n98.33 95.41 94.87 93.50 86.31 95.53\n\n91.08 89.25 92.15 92.24 81.68 91.04\n\n88.32 91.17 84.21 90.97 83.14 88.37\n\n97.68 94.79 96.74 96.90 85.66 95.87\n\n96.54 97.43 95.52 97.37 86.20 96.81\n\n92.96 94.11 89.57 92.52 88.64 92.78\n\n91.45 92.15 87.98 91.40 87.75 91.02\n\n1.86 0.38 2.64 1.17 0.31 1.04\n\n100 100 100 100 100 100\n\nACC ASR ACC\n\nASR ACC\n\nNGF (Ours)\n\nNo Defense\n\nRMSProp\n\nAdaGrad\n\nDefense\n\nAttacks\n\nAdam\n\nSAM\n\nACC\n\nACC\n\nACC\n\nASR\n\nASR\n\nASR\n\nASR\n\nof\n\nthe\n\nWe\n\nal.),\n\n(Duchi\n\ndifferent\n\ncompare\n\noptimizer:\n\nperformance\n\nof NGF al., and\n\n(ii) RMSProp (Hinton et\n\n(i) AdaGrad et (iii) Adam (Kingma & Ba, 2014),\n\nEvaluation of Different Optimizers: with first-order variants 2011), (iv) Sharpness-Aware Minimization (SAM) (Foret et al., 2020) is a recently proposed SGD-based optimizer that explicitly penalizes the abrupt changes of loss surface by bounding the search space within a small region. This forces the changes of model parameters in a way such that the optimization achieve smoother loss surface. Table 3 shows that NGF outperforms all of these variants of first-order optimizer by a huge margin. At the same time, proposed method achieves comparable clean test performance. Although SAM usually performs better than vanilla SGD in terms of smooth DNN optimization, SAM’s performance in shallow network scenario (our case) is almost similar to vanilla SGD. Two potential reasons behind this poor performance are (i) using a predefined local area to search for maximum\n\nTable 4: Avg. runtime comparison for different datasets. Here, #Parameters is the total number of parameters in the last layer. An NVIDIA RTX 3090 GPU is used for all experiments.\n\n# Parameters Method Runtime (Sec.)\n\nTiny-ImageNet\n\n2771.6 1681.4\n\n637.6 374.2\n\nFT NGF\n\nFT NGF\n\nFT NGF\n\nFT NGF\n\nImageNet\n\nCIFAR10\n\n78.1 38.3\n\n96.2 47.4\n\nGTSRB\n\n2.048M\n\nDataset\n\n409.6K\n\n22016\n\n5120\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Performance of SGD-Long and NGF while fine-tuning only the last layer of DNN. For SGD-Long, we consider a long purification period with Ep = 2500. NGF performance with and without the regularization term underlines the importance of the proposed regularizer. The results shown here are for CIFAR10 dataset.\n\nMethods\n\nBadnets\n\nBlend\n\nTrojan\n\nDynamic\n\nCLB\n\nSIG\n\nCBA\n\nASR\n\nACC ASR ACC\n\nASR\n\nACC\n\nASR\n\nACC\n\nASR\n\nACC ASR ACC\n\nASR\n\nACC\n\nRuntime (Secs.)\n\nInitial SGD-Long NGF w/o Reg.\n\nNGF\n\n100 82.34 1.91\n\n1.86\n\n92.96 90.68 87.65\n\n88.32\n\n100 7.13 0.31\n\n0.38\n\n94.11 92.46 90.54\n\n91.17\n\n100 86.18 3.04\n\n2.64\n\n89.57 87.29 83.31\n\n84.21\n\n100 57.13 1.28\n\n1.17\n\n92.52 90.51 90.24\n\n90.97\n\n100 13.84 0.92\n\n1.04\n\n92.78 88.11 87.13\n\n88.37\n\n100 0.26 0.16\n\n0.12\n\n88.64 85.74 84.46\n\n93.20 84.41 25.58\n\n90.17 86.87 84.81\n\n84.16\n\n24.60\n\n85.97\n\n– 907.5 37.8\n\n38.3\n\nTable 6: Evaluation of NGF on backdoor attacks with high poison rates, upto 50%. We consider CIFAR10 dataset and two closely performing defenses for this comparison.\n\nAttack\n\nBadNets\n\nPoison Rate\n\n25%\n\n35%\n\n50%\n\n25%\n\nBlend\n\n35%\n\n50%\n\n25%\n\nTrojan\n\n35%\n\n50%\n\nMethod ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC\n\nNo Defense 100 88.26 100 87.43 100 85.11 100 86.21 100 85.32 100 83.28 100 87.88 100 86.81 100 85.97 7.81 82.22 16.35 80.72 29.80 78.27 29.96 82.84 47.02 78.34 86.29 69.15 11.96 76.28 63.99 72.10 89.83 70.02 5.21 78.11 8.39 74.06 11.52 69.81 1.41 68.73 4.56 63.87 7.97 55.70 3.98 76.99 4.71 72.05 5.59 70.98 NGF (Ours) 2.12 85.50 2.47 84.88 4.53 82.32 0.83 80.62 1.64 79.62 2.21 76.37 3.02 83.10 3.65 81.66 4.66 80.30\n\nANP FT\n\nloss, and (ii) using ‘Euclidean distance’ metric instead of geometric distance metric. In contrast, NGD with curvature geometry aware Fisher Information Matrix can successfully avoid such bad minima and optimizes to a global minima.\n\nRuntime Analysis: In Table 4, we show the average runtime for different defenses. Similar to purification performance, purification time is also an important indicator to measure the success of a defense technique. In Section 5.2, we already show that our method outperforms other defenses in most of the settings. As for the run time, our method completes the purification (for CIFAR10) in just 38.3 seconds; which is almost half as compared to FT. The time-advantage of our method also holds for large datasets and models, e.g., ImageNet and ResNet50. Runtime comparison with other defenses is in the Appendix H.\n\nEffect of Proposed Regularizer: In this section, we analyze the effect of regularizer and long training with SGD. The effect of our clean distribution-aware regularizer can be observed in Table 5. NGF with the proposed regularizer achieves an 1% clean test performance improvement over vanilla NGF. For long training with SGD (SGD-Long), we fine-tune the last layer for 2500 epochs. Table 5 shows the evaluations of SGD-Long on 7 different attacks. Even though the ASR performance improves significantly for CLB and SIG attacks, SGD-based FT still severely underperforms for other attacks. Moreover, the computational time increases significantly over NGF. Thus, our choice of NGD-based FT as a fast and effective backdoor purification technique is well justified.\n\nStrong Backdoor Attacks: By increasing the poison rates, we create stronger version of different attacks against which most defense techniques fail quite often. We use 3 different poison rates, {25%, 35%, 50%}. We show in Table 6 that NGF is capable of defending very well even with a poison rate of 50%, achieving a significant ASR improvement over FT. Furthermore, there is a sharp difference in classification accuracy between NGF and other defenses. For 25% Blend attack, however, ANP offers a slightly better performance than our method. However, ANP performs poorly in terms of removing backdoor as it obtains an ASR of 29.96% as compared to 0.83% for NGF.\n\n6 CONCLUSION\n\nWe propose a novel backdoor purification technique based on natural gradient descent fine-tuning. The proposed method is motivated by our analysis of loss surface smoothness and its strong correlation with the backdoor insertion and purification processes. As a backdoor model has to learn an additional data distribution, it tends to be optimized to bad local minima or sharper minima compared to a benign model. We argue that backdoor can be removed by re-optimizing the model to a smoother minima. We further argue that fine-tuning a single layer is enough to remove the backdoor. Therefore, in order to achieve a smooth minima in a single-layer fine-tuning scenario, we propose using an FIM-based DNN objective function and minimize it using a curvature-aware NGD optimizer. Our proposed method achieves SOTA performance in a wide range of benchmarks. Since we fine-tune only one layer the training time overhead reduces significantly, making our method one of the fastest among SOTA defenses. In the future, we aim to extend our smoothness analysis to 3D point-cloud attacks as well as attacks on contrastive learning.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nShun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251–276,\n\n1998.\n\nMauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In 2019 IEEE International Conference on Image Processing (ICIP), pp. 101–105. IEEE, 2019.\n\nEitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping, Micah Goldblum, Tom Goldstein, and Arjun Gupta. Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3855–3859. IEEE, 2021.\n\nShuwen Chai and Jinghui Chen. One-shot neural backdoor erasing via adversarial weight masking.\n\narXiv preprint arXiv:2207.04497, 2022.\n\nHuili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks. In IJCAI, volume 2, pp. 8, 2019.\n\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep\n\nlearning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n\nSiyuan Cheng, Yingqi Liu, Shiqing Ma, and Xiangyu Zhang. Deep feature space trojan attack of\n\nneural networks by controlled detoxification. In AAAI, volume 35, pp. 1148–1156, 2021.\n\nAnna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial intelligence and statistics, pp. 192–204. PMLR, 2015.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\n\nhierarchical image database. In CVPR, pp. 248–255. IEEE, 2009.\n\nIlias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. In International Conference on\n\nSever: A robust meta-algorithm for stochastic optimization. Machine Learning, pp. 1596–1606. PMLR, 2019.\n\nBao Gia Doan, Ehsan Abbasnejad, and Damith C Ranasinghe. Februus: Input purification defense against trojan attacks on deep neural network systems. In Annual Computer Security Applications Conference, pp. 897–912, 2020.\n\nYinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, and Jun Zhu. Black-box detection of backdoor attacks with limited information and data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16482–16491, 2021.\n\nMin Du, Ruoxi Jia, and Dawn Song. Robust anomaly detection and backdoor attack detection via\n\ndifferential privacy. arXiv preprint arXiv:1911.07116, 2019.\n\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\n\nstochastic optimization. Journal of machine learning research, 12(7), 2011.\n\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization\n\nfor efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.\n\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=6Tm1mposlrM.\n\nYansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual Computer Security Applications Conference, pp. 113–125, 2019.\n\nTianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring\n\nattacks on deep neural networks. IEEE Access, 7:47230–47244, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJunfeng Guo, Ang Li, and Cong Liu. Aeva: Black-box backdoor detection using adversarial extreme\n\nvalue analysis. arXiv preprint arXiv:2110.14880, 2021.\n\nWenbo Guo, Lun Wang, Yan Xu, Xinyu Xing, Min Du, and Dawn Song. Towards inspecting and eliminating trojan backdoors in deep neural networks. In 2020 IEEE International Conference on Data Mining (ICDM), pp. 162–171. IEEE, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\n\nnetworks. In European conference on computer vision, pp. 630–645. Springer, 2016.\n\nGeoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Rmsprop: Divide the gradient by a running\n\naverage of its recent magnitude. coursera: Neural networks for machine learning.\n\nSanghyun Hong, Varun Chandrasekaran, Yi ̆gitcan Kaya, Tudor Dumitra ̧s, and Nicolas Papernot. On the effectiveness of mitigating data poisoning attacks with gradient shaping. arXiv preprint arXiv:2002.11497, 2020.\n\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017.\n\nKunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling\n\nthe training process. arXiv preprint arXiv:2202.03423, 2022.\n\nStanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun Cho, and Krzysztof Geras. The break-even point on optimization trajectories of deep neural networks. arXiv preprint arXiv:2002.09572, 2020.\n\nZhiwei Jia and Hao Su. Information-theoretic local minima characterization and regularization. In\n\nInternational Conference on Machine Learning, pp. 4773–4783. PMLR, 2020.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017.\n\nPang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In\n\nInternational conference on machine learning, pp. 1885–1894. PMLR, 2017.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nYa Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.\n\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learning: Training clean models on poisoned data. Advances in Neural Information Processing Systems, 34, 2021a.\n\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks. arXiv preprint arXiv:2101.05930, 2021b.\n\nYiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey.\n\narXiv preprint arXiv:2007.08745, 2020.\n\nYuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16463–16472, 2021c.\n\nJunyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. Composite backdoor attack for deep neural\n\nnetwork by mixing existing benign features. In CCS, pp. 113–131, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nChaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 2022.\n\nKang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pp. 273–294. Springer, 2018.\n\nYingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu\n\nZhang. Trojaning attack on neural networks. 2017.\n\nYunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In European Conference on Computer Vision, pp. 182–199. Springer, 2020.\n\nShiqing Ma and Yingqi Liu. Nic: Detecting adversarial samples with neural network invariant checking. In Proceedings of the 26th network and distributed system security symposium (NDSS 2019), 2019.\n\nJames Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate\n\ncurvature. In International conference on machine learning, pp. 2408–2417. PMLR, 2015.\n\nAnh Nguyen and Anh Tran. Wanet–imperceptible warping-based backdoor attack. arXiv preprint\n\narXiv:2102.10369, 2021.\n\nTuan Anh Nguyen and Anh Tran.\n\nInput-aware dynamic backdoor attack. Advances in Neural\n\nInformation Processing Systems, 33:3454–3464, 2020.\n\nHyeyoung Park, S-I Amari, and Kenji Fukumizu. Adaptive natural gradient learning algorithms for\n\nvarious stochastic models. Neural Networks, 13(7):755–764, 2000.\n\nXiming Qiao, Yukun Yang, and Hai Li. Defending neural backdoors via generative distribution\n\nmodeling. Advances in neural information processing systems, 32, 2019.\n\nAniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Backdoor attacks on self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13337–13346, 2022.\n\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510–4520, 2018.\n\nGuangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng, Shiqing Ma, and Xiangyu Zhang. Backdoor scanning for deep neural networks through k-arm optimization. In International Conference on Machine Learning, pp. 9525–9536. PMLR, 2021.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nJohannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In The 2011 international joint conference on neural networks, pp. 1453–1460. IEEE, 2011.\n\nJacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks.\n\nAdvances in neural information processing systems, 30, 2017.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. 2014. arXiv preprint arXiv:1409.4842, 10, 2014.\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nGuanhong Tao, Guangyu Shen, Yingqi Liu, Shengwei An, Qiuling Xu, Shiqing Ma, Pan Li, and Xiangyu Zhang. Better trigger inversion optimization in backdoor scanning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13368–13378, 2022.\n\nBrandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in\n\nneural information processing systems, 31, 2018.\n\nAlexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. 2018.\n\nBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pp. 707–723. IEEE, 2019.\n\nZhenting Wang, Juan Zhai, and Shiqing Ma. Bppattack: Stealthy and efficient trojan attacks against deep neural networks via image quantization and contrastive adversarial learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15074–15084, 2022.\n\nDongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. In\n\nNeurIPS, 2021.\n\nZhen Xiang, David J Miller, and George Kesidis. Post-training detection of backdoor attacks for\n\ntwo-class and multi-attack scenarios. arXiv preprint arXiv:2201.08474, 2022.\n\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,\n\n2016.\n\nYi Zeng, Si Chen, Won Park, Z Morley Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of\n\nbackdoors via implicit hypergradient. arXiv preprint arXiv:2110.03735, 2021.\n\nQuan Zhang, Yifeng Ding, Yongqiang Tian, Jianmin Guo, Min Yuan, and Yu Jiang. Advdoor: adversarial backdoor attack of deep learning system. In Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 127–138, 2021.\n\nPu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin. Bridging mode connectivity in loss landscapes and adversarial robustness. arXiv preprint arXiv:2005.00060, 2020a.\n\nShihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Cleanlabel backdoor attacks on video recognition models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14443–14452, 2020b.\n\nRunkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Data-free backdoor removal based on channel\n\nlipschitzness. arXiv preprint arXiv:2208.03111, 2022.\n\nHaoti Zhong, Cong Liao, Anna Cinzia Squicciarini, Sencun Zhu, and David Miller. Backdoor embedding in convolutional neural network models via invisible perturbation. In Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy, pp. 97–108, 2020.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nSection B describes our proposed algorithm. Section C discusses the intuitions behind the uses of clean data distribution for smoothness analysis, more explanation on why backdoor behavior and weight loss-landscape are related and smoothness helps to mitigate the effect of backdoor, how FIM helps to smooth the loss-landscape, and why SAM (Foret et al., 2021) does not work in proposed problem setup. Section D contains the experimental details of different attacks and defenses. Section E contain comparison with another recent defense technique. We present smoothness analysis with different datasets and architectures in Section F. Section G shows the Label Correction Rates for different defense techniques. Section H and I contain additional runtime analysis. Section K contains more ablation studies purification with last layer re-initialization. Section L and M discuss more attacks, all2all and combined attacks. Our code is available at anonymous GitHub link 2.\n\nB NATURAL GRADIENT FINE-TUNING\n\nIn our proposed method, we aim to remove backdoor by fine-tuning only the last layer. The manner in which we perform that fine-tuning is described in Algorithm 1. After purification, the model should behave like a benign/clean model producing same prediction irrespective of the presence of trigger. Note that Kirkpatrick et al. (2017) proposed a similar regularizer, known as elastic weight consolidation (EWC) used in continual learning. EWC helps a model to learn a new task while keeping the knowledge of previously learned tasks. Our clean data distribution aware regularization, instead, only helps to preserve the knowledge of previously learned task corresponding to clean data distribution (not task corresponding to poison samples) through computing ̄F using clean validation set. Thus, the design of our regularizer is backdoor specific and needs careful attention as it is crucial for clean test performance.\n\nAlgorithm 1: Natural Gradient Fine-tuning Input: Backdoor Model (fθ(.)), 1% Clean Validation Set Dval, Number of Purification Epochs N X , Y ← Dval ̄F ← 1\n\nylog fθ(x)(cid:1)T (cid:105)\n\n∇ ̄θL parameter of initial backdoor model.\n\nylog fθ(x) · (cid:0)∇ ̄θL\n\nx∈X ,y∈Y\n\n|Dval|\n\n(cid:80)\n\n(cid:104)\n\n// ̄θL is the last layer’s\n\nfor i = 1 to N do\n\nCalculate loss, L = LCE(Y, fθi (X )) + η\n\n(cid:20)\n\n(cid:80)\n\n2\n\nj(diag( ̄F ))j · (θi (cid:16)\n\nL,j − ̄θL,j)2 (cid:17)T (cid:21)\n\nF ← 1\n\n|Dval|\n\n(cid:80)\n\nx∈X ,y∈Y\n\n∇θi\n\nL\n\nylog fθi (x) ·\n\n∇θi\n\nL\n\nylog fθi (x)\n\n// θi\n\nL is the last\n\nlayer’s parameter at ith iterations θi+1 L ← θi θi+1 ← {W0,1, W1,2, · · · , WL−2,L−1, θi+1\n\nL − α · F −1∇θi\n\n(L)\n\nL\n\nθp ← {W0,1, W1,2, · · · , WL−2,L−1, θN Output: Purified Model, fθp\n\nL }\n\n// α is the learning rate\n\n// Wi,i+1’s are frozen parameters\n\nL } // θp is the purified model’s parameter\n\nC MORE EXPLANATIONS ON SMOOTHNESS AND BACKDOOR\n\nC.1 WHY SMOOTHNESS ANALYSIS W.R.T CLEAN DISTRIBUTION?\n\nIn general, a backdoor model is usually well optimized w.r.t clean and poison data distribution. Therefore, it is designed to perform well on both distributions. If we look at the smoothness analysis of backdoor models w.r.t. original poisoned training data (both data distributions), the loss surface will be smoother. However, looking from only clean distribution point of view the loss surface is sharper as we have described in the paper. Backdoor purification implies that the model will only be sensitive to clean data distribution and completely ignore any type of backdoor manipulations. Since the model’s behavior w.r.t. clean distribution is of our particular interest, we perform the smoothness\n\n2https://github.com/kr-anonymous/ngf-animus\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nanalysis w.r.t. clean distribution in all cases. If we put it another way, making the loss surface smooth w.r.t. clean distribution ensures that the model will automatically forget the poison distribution, i.e., backdoor purification. Another minor reason is that, we are able to distinguish the behavior of benign and backdoor models because we consider clean distribution. This is the only common distribution between these models, and one has to perform smoothness analysis w.r.t. a common distribution to distinguish them.\n\nC.2 WHY SMOOTHNESS IS THE KEY TO REMOVING THE BACKDOOR?\n\nOne key observation from the smoothness study is that: there exists a key difference between weightloss surface smoothness (estimated by loss hessian) of a backdoor and a benign model w.r.t. clean distribution—the weight-loss surface of a backdoor model is less smooth compared to a benign model. To further elaborate, let us consider feeding a clean sample to a backdoor model. By definition, it will predict the correct ground truth label. Now, consider feeding a sample with a backdoor trigger on it. The model will predict the adversary-set target label implying significant changes in prediction distribution. This significant change can be explained by the surface smoothness. In order to accommodate this significant change in prediction, the model must adjust itself accordingly. Such adjustment leads to non-smoothness in the weight-loss surface. A non-smooth surface causes significant changes in loss gradient for specific inputs. In our case, these specific inputs are backdoortriggered samples. As the magnitude of a trigger is usually very small compared to the total input magnitude, the model has to experience quite a significant change in its weight space to cause large loss changes. We characterize this change in terms of smoothness. As for backdoor removal, we claim that making the non-smooth weight loss surface smoother removes the backdoor behavior. Based on the above discussion, a smoother surface should not cause a large change in loss or model predictions corresponding to backdoor related perturbations or triggers. In summary, for a model to show certain backdoor behavior, there are some specific changes that take place in the weight space. In this work, we try to explain these changes in terms of weight-loss surface smoothness. Our intuition is well supported by our comprehensive empirical evaluations.\n\nC.3 WHY USE FISHER INFORMATION MATRIX FOR ACHIEVING SMOOTHNESS?\n\nIn Fisher Information Matrix (FIM) based optimization, the natural gradient is defined as F −1∇L (ref. Eq. 5). From the perspective of information geometry, natural gradient defines the direction in parameter space which gives largest change in objective per unit of change in model (p(y|x, θ)). Per unit of change in model is measured by KL-divergence. Note that KL-divergence is well connected with FIM as it can be used as a local quadrature approximation of KL-divergence of model change. Eqn. 2 suggests that one requires the knowledge of the original parameter (θ) space to estimate it. Therefore, FIM can be thought of as a mechanism to translate between the geometry of the model (p(y|x, θ)) and the current parameters (θ) of the model. The way natural gradient defined the direction in parameter space is contrastive to the stochastic gradient. Stochastic gradient defines the direction in parameter space for largest change in objective per unit of change in parameter (θ) measured by Eucludian distance. That is, the gradient direction is solely calculated based on the changes of parameters, without any knowledge of model geometry.\n\nAs FIM-based optimization minimizes the changes in model, the model itself cannot significantly change at each iteration. So, the overall optimization process goes through comparatively smoother transition and finally reaches smoother minima in comparison with SGD-based optimization3.\n\nC.4 WHY DOES SAM UNDERPERFORMS?\n\nOne-layer optimization becomes a shallow network optimization problem for which there can exist many bad local minima. For such an optimization problem, typically first-order optimizers perform poorly mainly for the unawareness of loss surface curvature geometry. In the case of SAM, it uses SGD as the optimizer. Informally, the working principle of SAM is: at each iteration, SAM tries to minimize the maximum loss within a certain area in the loss weight space. Note that the formulation of finding the maximum loss in a certain area is based on ‘Euclidean distance’ metric which does not capture the curvature information of the plane. Although SAM performs better than vanilla SGD\n\n3We refer the readers to (Jia & Su, 2020) for more discussion on FIM-based smoothness analysis.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nin deep network4 in terms of smoother optimization point, SAM’s performance in shallow network (our case) is almost similar to vanilla SGD. Two potential reasons behind this poor performance are (i) using a predefined local area to search for maximum loss, and (ii) using ‘Euclidean distance’ metric instead of geometric distance metric. In contrast, NGD with curvature geometry aware Fisher Information Matrix can successfully avoid such bad minima and optimizes to a global minima.\n\nD EXPERIMENTAL DETAILS\n\nFor creating backdoor models with CIFAR10 (Krizhevsky et al., 2009), we train a PreActResNet (He et al., 2016) model using an SGD optimizer with an initial learning rate of 0.01, learning rate decay of 0.1/100 epochs for 250 epochs. We also use a weight decay of 5e−4 with momentum of 0.9. We use a longer backdoor training to ensure a satisfactory attack success rate. We use a batch size of 128. For GTSRB (Stallkamp et al., 2011), we train a WideResNet-16-1 (Zagoruyko & Komodakis, 2016) model for 200 epochs with a learning rate of 0.01 and momentum of 0.9. We also regularize the weights with a weight-decay of 5e−4 We rescale each training image to 32 × 32 before feeding them to the model. The training batch size is 128 and an SGD optimizer is used for all training. We further created backdoor models trained on the Tiny-ImageNet and ImageNet datasets. For Tiny-ImageNet, we train the model for 150 epochs with a learning rate of 0.005, a decay rate of 0.1/60 epochs, and a weight decay of 1e-4. For ImageNet, we train the model for 200 epochs with a learning rate of 0.02 with a decay rate of 0.1/75 epochs. We also employ 0.9 and 1e-4 for momentum and weight decay, respectively. The details of these four datasets are presented in Table 7.\n\nD.1 DETAILS OF ATTACKS\n\nWe use 11 different attacks for CIFAR10. Each of them differs from each other in terms of either label mapping type or trigger properties. For label poisoning attack, we use a fixed poison rate of 10%. However, we need to increase this rate to 80% for CLB and SIG. For Blend and SIG attacks, we use a image-trigger mixup ratio of 0.2. WaNet adopts a universal wrapping augmentation as the backdoor trigger. Note that WaNet can be considered as an non-additive attack since it works like a augmentation technique with direct information insertion or addition like Badnets or TrojanNet. ISSBA adds specific trigger to each input that is of low magnitude and imperceptible. Both of these methods are capable of evading some existing defenses. For BPPA attack, we follow the PyTorch implementation5. For Feature attack, we create backdoor model based on this implementation 6. Apart from clean-label attacks, we use a poison rate of 10% for creating backdoor attacks. The details of these attacks are presented in Table 8. In addition to theses attacks, we also consider ’All2All’ attacks (Troj-all, Dyn-all) where we have more than one target label. To implement this attack, we change the given label i to the target label i + 1. For class 9, the target label is 0.\n\nTable 7: Detailed information of the datasets and DNN architectures used in our experiments.\n\nDataset\n\nClasses\n\nImage Size Training Samples Test Samples\n\nArchitecture\n\nCIFAR-10 GTSRB Tiny-ImageNet ImageNet\n\n10 43 200 1000\n\n32 x 32 32 x 32 64 x 64 224 x 224\n\n50,000 39,252 100,000 1.28M\n\n10,000 12,630 10,000 100,000\n\nPreActResNet18 WideResNet-16-1 ResNet34 ResNet50\n\nD.2 NGF AND OTHER OPTIMIZER IMPLEMENTATION DETAILS\n\nTo implement our proposed algorithm, we freeze the CNN backbone of the model and only fine-tune the linear or classification layer parameters. We perform the fine-tuning for 100 epochs with a learning rate of 0.01, weight decay of 1e−4, momentum of 0.9, and a batch size of 128. For studies\n\n4Deep network consists of degenerate local minima and manifold of connect global minima (Liu et al., 2022) implying that, in deep network, there is no such bad local minima, unlike to shallow network, that could affect the performance of SAM.\n\n5https://github.com/RU-System-Software-and-Security/BppAttack 6https://github.com/Megum1/DFST\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Details of different backdoor attacks we have defended against.\n\nAttacks\n\nTrigger Type\n\nLabel Mapping\n\nDescription\n\nPoison Target Label Rate\n\nBadnets (Gu et al., 2019)\n\nCLB (Turner et al., 2018)\n\nSIG (Barni et al., 2019)\n\nChecker Board 3 × 3\n\nChecker Board 3 × 3\n\nSinusoidal Signal\n\nDynamic (Nguyen & Tran, 2020)\n\nOptimization\n\nTrojan (Liu et al., 2017)\n\nWatermarks\n\nBlend (Chen et al., 2017)\n\nCBA (Lin et al., 2020)\n\nFBA (Cheng et al., 2021)\n\nBPPA (Wang et al., 2022)\n\nRandom Pixels\n\nMixer Constructor\n\nStyle Generator\n\nQuantization Trigger\n\nLabel Poison\n\nClean Label\n\nClean Label\n\nLabel Poison\n\nLabel Poison\n\nLabel Poison\n\nLabel Poison\n\nLabel Poison\n\nLabel Poison\n\nTriggers are placed at bottom left corner of images\n\nuse PGD-based adversarial perturbations\n\nUse Mixup for adding the sinusoidal trigger to whole image\n\n10%\n\n80%\n\n80%\n\nGenerate image dependent triggers\n\n10%\n\nWatermarks are static for all poisoned samples\n\nEach pixel of the trigger is sampled from uniform distribution of [0,255]\n\nMixing existing benign features of two/more classes\n\nUse a controlled detoxification to manipulate deep features\n\nImage quantization & contrastive adversarial learning based\n\n10%\n\n10%\n\n10%\n\n10%\n\n10%\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\nTable 9: Comparison of NGF with another state-of-the-art defense CLP (Zheng et al., 2022). Even though CLP achieves satisfactory removal performance for some attacks, the clean test accuracy drops significantly for some attacks (Blend, TrojanNet, CLB). We consider CIFAR10 dataset for this comparison.\n\nMethods\n\nBadnets\n\nBlend\n\nTroj-one\n\nSIG\n\nCLB\n\nDyn-One\n\nDyn-All\n\nBPPA\n\nASR ACC ASR ACC ASR ACC ASR ACC\n\nASR\n\nACC\n\nASR\n\nACC\n\nASR\n\nACC\n\nASR\n\nACC\n\nNo Defense CLP NGF\n\n100 2.58 1.86\n\n92.96 90.90 88.32\n\n100 0.81 0.38\n\n94.11 81.17 91.17\n\n100 1.81 2.64\n\n89.57 56.08 84.21\n\n100 1.46 0.12\n\n88.64 53.03 84.16\n\n100 13.61 1.04\n\n92.78 90.38 88.37\n\n100 13.84 1.17\n\n92.52 90.84 90.97\n\n100 14.78 1.61\n\n92.61 89.72 90.19\n\n99.70 11.39 7.14\n\n93.82 90.21 91.84\n\nwith different optimizers (Adam, RMSProp, etc.), we use similar training settings as NGF. For sharpness-aware minimization, we restrict the search region for the SGD optimizer. We follow Pytorch implementation described here7. We use a batch size of 128 and a learning rate of 0.01 for SAM.\n\nD.3 DETAILS OF OTHER DEFENSES\n\nFor experimental results with ANP (Wu & Wang, 2021), we follow the source code implementation 8. After creating each of the above mentioned attacks, we apply adversarial neural pruning on the backdoor model for 500 epochs with a learning rate of 0.02. We use the default settings for all attacks. For vanilla FT, we perform simple DNN fine-tuning with a learning rate of 0.01 for 125 epochs. We higher number of epochs for FT due to its poor clean test performance. The clean validation size is 1% for both of these methods. For I-BAU (Zeng et al., 2021), we follow their PyTorch Implementation 9 and purify the model for 10 epochs. We use 5% validation data for I-BAU. For AWM (Chai & Chen, 2022), we train the model for 100 epochs and use Adam optimizer with a learning rate of 0.01 and a wight decay of 0.001. We use the default hyper-parameter setting as described in their work α = 0.9, β = 0.1, γ = 108, η = 1000. Above settings is for CIFAR10 and GTSRB only. For Tiny-ImageNet, we keep most of the training settings similar except reducing the number of epochs significantly. We also increase the validation size to 5% for vanilla FT, ANP, and AWM. For I-BAU, we use a higher valiadtion size of 10%. For purification, we apply ANP and AWM for 30 epochs, I-BAU for 5 epochs and Vanilla FT for 25 epochs. For ImageNet, we use a 3% validation size for all defenses (except for I-BAU, we use 5% validation data) and use different number of purification\n\n7https://github.com/davda54/sam 8https://github.com/csdongxian/ANP_backdoor 9https://github.com/YiZeng623/I-BAU\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 10: Correction rate (%) for different defense techniques. We define the correction rate (CR) as the percentage of poisonous samples correctly classified to their original classes. The higher the CR, the better is that method. We use CIFAR10 dataset for these evaluations.\n\nMethod\n\nBadnets\n\nTrojan\n\nCLB\n\nSIG\n\nNo Defense Vanilla FT ANP NGF (Ours)\n\n0 85.74 85.56 86.42\n\n0 80.52 80.69 80.85\n\n0 84.72 82.04 85.63\n\n0 43.35 45.64 45.18\n\nTable 11: Average run time for different defense methods. We consider CIFAR10 dataset and all attacks to calculate the average runtime. We do not show the runtime of CLP as it severely underperforms compared to other defenses. An NVIDIA RTX 3090 GPU was used for all computations.\n\nMethod\n\nVanilla FT ANP\n\nI-BAU AWM NGF (Ours) NGF w/o Regularizer (Ours)\n\nRuntime (sec)\n\n78.1\n\n201.5\n\n52.7\n\n90.2\n\n38.3\n\n37.8\n\nepochs for different methods. We apply I-BAU for 2 epochs. On the other hand, we train the model for 3 epochs for ANP, AWM and vanilla FT.\n\nE COMPARISON WITH ADDITIONAL DEFENSE\n\nIn Table 9, we show the comparison of NGF with a recently proposed defense technique based on channel Lipschitzness Pruning (CLP) (Zheng et al., 2022) that works without any data. We follow the Github link10. Based on the trigger-activated change on channel activation, CLP prunes channel. One disadvantage of pruning based method is that in case of challenging scenarios, e.g.,, strong attacks, large datasets and models etc., it prunes neurons abruptly. This creates high possibility of pruning neurons sensible to clean data distribution. In turn, the clean test accuracy may decrease significantly in some scenario. As shown in Table 9, clean accuracies (ACCs) for Blend, Trojan and CLB attacks are much lower compared to NGF. Even though CLP performs reasonably well at removing backdoor, NGF still outperforms in that area.\n\nF MORE ON SMOOTHNESS ANALYSIS\n\nFor smoothness analysis, we follow the PyHessian implementation11 and modify it according to our needs. We use a single batch with size 200 to calculate the loss Hessian for all attacks with CIFAR10 and GTSRB datasets. We conduct further smoothness analysis for ImageNet dataset and different architectures. In Fig. 5, we show the Eigen density plots for different 5 different attacks. We used 2 A40 GPUs with 96GB system memory. However, it was not enough to calculate the loss hessian if we consider all 1000 classes of ImageNet. Due to GPU memory constraint, we consider ImageNet subset with 12 classes. We train a ResNet34 architecture with 5 different attacks. To calculate the loss hessian, we use a batch size of 50. Density plots before and after purification further confirms our proposed hypothesis. To test our hypothesis for larger architectures, we consider 5 different architectures for CIFAR10, i.e., VGG19 (Simonyan & Zisserman, 2014), MobileNetV2 (Sandler et al., 2018), DenseNet121 (Huang et al., 2017), GoogleNet (Szegedy et al., 2014), Inception-V3 (Szegedy et al., 2016). Each of the architectures is deeper compared to the ResNet18 architecture we consider for CIFAR10. Due to their large size, showing the effectiveness of NGF in case of these architecture will strengthen our claim—one layer NGF based fine-tuning is enough for backdoor purification. In Fig. 6, we show the performance of NGF when backdoor models are created using these architectures. Our proposed one layer fine-tuning successfully removes the backdoor in all of these scenarios.\n\n10https://github.com/rkteddy/channel-Lipschitzness-based-pruning 11https://github.com/amirgholami/PyHessian\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 12: Performance of NGF while fine-tuning all layers of DNN. The results shown here are for CIFAR10 dataset.\n\nMethods\n\nBadnets\n\nBlend\n\nTrojan\n\nDynamic\n\nCLB\n\nSIG\n\nCBA\n\nASR ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC\n\nASR\n\nACC\n\nInitial Vanilla-FT (All Layers) NGF (Last layer) NGF (All layers)\n\n100 4.87 1.86 1.47\n\n92.96 85.92 88.32 88.65\n\n100 4.77 0.38 0.42\n\n94.11 87.61 91.17 92.28\n\n100 3.78 2.64 2.05\n\n89.57 82.18 84.21 84.61\n\n100 4.73 1.17 1.06\n\n92.52 88.61 90.97 90.42\n\n100 1.83 1.04 0.60\n\n92.78 87.41 88.37 88.74\n\n100 1.04 0.12 0.18\n\n88.64 81.92 84.16 85.12\n\n93.20 27.80 24.60 19.86\n\n90.17 83.79 85.97 86.30\n\nRuntime (Secs.)\n\n– 78.1 38.3 173.2\n\nTable 13: Purification performance after randomly re-initializing the last layer. Even after re-initialization, the purification task is similar as before, i.e., proper fine-tuning. Without proper fine-tuning, the backdoor behavior will be still present after purification. In contrast to SGD, NGF is highly successful even after such re-initialization. CIFAR10 dataset is considered here.\n\nBadNets\n\nTrojan\n\nBlend\n\nCLB\n\nSIG\n\nASR\n\nACC\n\nASR\n\nACC\n\nASR\n\nACC\n\nASR\n\nACC\n\nASR\n\nACC\n\nNo Defense SGD NGF\n\n100 93.51 3.34\n\n92.96 89.35 89.65\n\n100 89.63 18.78\n\n89.57 87.55 84.21\n\n100 71.22 0.33\n\n94.11 92.21 86.39\n\n100 88.01 4.45\n\n92.78 90.01 82.50\n\n100 76.83 1.25\n\n88.64 86.12 83.95\n\nG LABEL CORRECTION RATE OF DIFFERENT DEFENSES\n\nIn standard removal measurement, it is sufficient for backdoored images to be classified as a nontarget class. As we calculate ASR after removal, our evaluation follows standard measurement. We define the correction rate (CR) as the percentage of poisonous samples correctly classified to their original classes. We define the method with the highest value of CR as the best performing or SOTA method. We use the CIFAR10 dataset and 4 different attacks for demonstration. It can be observed from Table 10 that our method obtains SOTA correction performance for most of these attacks.\n\nH RUNTIME ANALYSIS OF OTHER DEFENSES\n\nIn Table 11, we show the average runtime of other defenses. It can be observed that ANP is almost 6x slower than NGF. Other defenses, NAD and MCR are also much slower than NGF. NAD uses transfer learning based distillation using a teacher-student framework. However, the complexity of this method results in computational overhead. Instead, NGF revisits much simpler fine-tuning approach from one-layer optimization point of view. Our simple and effective method leads to one of the fastest purification. We take the average of all run times against 11 attacks on CIFAR10. Note that, for each epoch in NGF, we have to feed-forward all validation data. However, we only update the parameters of last layer through back-propagation. The reason behind this is that we use different data augmentations while fine-tuning. This does not allow us to save the CNN features one time and re-use it for upcoming all epochs.\n\nI FINE-TUNING ALL LAYERS\n\nWe have considered fine-tuning all layers fusing NGF and SGD. Note that vanilla FT does fine-tune all layers. We report the performance of NGF for all layers in Table 12. While fine-tuning all layers seems to improve the performance, it takes almost 6× more computational time than NGF on last layer. We perform NGF with the regularizer here.\n\nTable 14: Purification performance for various validation data size. NGF performs well even with very few validation data, e.g., 50 data points. All results are for CIFAR10 and Badnets attack.\n\nValidation size\n\n50\n\n100\n\n250\n\n350\n\n500\n\nMethod\n\nNo Defense ANP AWM NGF (Ours)\n\nASR\n\n100 13.66 8.51 6.91\n\nCA\n\nASR\n\nCA\n\nASR\n\nCA\n\nASR\n\nCA\n\nASR\n\nCA\n\n92.96 83.99 83.63 86.82\n\n100 8.35 7.38 4.74\n\n92.96 84.47 83.71 86.90\n\n100 5.72 5.16 4.61\n\n92.96 84.70 84.52 87.08\n\n100 3.78 5.14 2.45\n\n92.96 85.26 85.80 87.74\n\n100 2.84 4.34 1.86\n\n92.96 85.96 86.17 88.32\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nTable 15: Illustration of purification performance for All2All attack using CIFAR10 dataset, where uniformly distribute the target labels to all available classes. NGF shows better robustness and achieves higher clean accuracies for 3 attacks: Badnets, Blend, BPPA with 10% poison-rate.\n\nMethod\n\nBadNets-All ASR ACC ASR ACC\n\nBlend-All\n\nBPPA-All\n\nASR\n\nACC\n\nNo Defense FT NAD ANP NGF (Ours)\n\n100 2.78 4.58 3.13 1.93\n\n88.34 83.19 81.34 82.19 84.29\n\n100 2.83 6.76 4.56 1.44\n\n88.67 80.13 81.13 82.88 83.79\n\n99.60 10.97 20.19 9.87 6.10\n\n92.51 89.76 87.77 89.91 90.56\n\nTable 16: Performance of NGF against combined backdoor attack. We poison some portion of the training data using 3 different attacks; Badnets, Blend, and Trojan. Each of these attacks have an equal share in the poison data. All results are for CIFAR10 datasets containing different number of poisonous samples.\n\nPoison Rate\n\n10%\n\n25%\n\n35%\n\n50%\n\nMethod\n\nASR\n\nACC\n\nASR\n\nACC\n\nASR\n\nACC\n\nASR\n\nACC\n\nNo Defense MCR ANP NGF (Ours)\n\n100 27.83 4.75 1.17\n\n88.26 78.10 83.50 83.61\n\n100 31.09 5.42 2.15\n\n87.51 77.42 81.73 81.62\n\n100 36.21 6.51 3.31\n\n86.77 75.63 79.93 80.01\n\n100 40.08 9.76 4.15\n\n85.82 72.91 78.06 79.35\n\nJ EFFECT OF CLEAN VALIDATION DATA SIZE\n\nWe also present how the total number of clean validation data can impact the purification performance. In Table 14, we see the change in performance while gradually reducing the validation size from 1% to 0.1%. We consider Badnets attack on CIFAR10 dataset for this evaluation. Even with only 50 (0.1%) data points, NGF can successfully remove the backdoor by bringing down the attack success rate (ASR) to 6.91%. We also consider adversarial weight masking for this comparison. For both ANP and AWM, reducing the validation size has severe impact on the test accuracy (ACC).\n\nK PURIFICATION USING LAST LAYER RE-INITIALIZATION\n\nWe also conduct studies on the behavioral difference of SGD and NGD while we re-initialize the last layer. Even though we re-initialize the last layer, one still has to properly fine-tune the backdoor model to remove the backdoor. However, we see throughout our evaluations that SGD-based onelayer fine-tuning is not a proper fine-tuning method and unable to remove backdoor. In case of re-initialization, the shallow network optimization problem still stands as wells as the issue of bad local minima. Therefore, SGD shows similar behavior in this scenario too. Table 13 shows the performance of SGD and NGF in case one decides to re-initialize the layer than fine-tunes. As usual, NGF is able to reach smooth minima due to its ability to properly fine-tune the model.\n\nL MORE ALL2ALL ATTACKS\n\nMost of the defenses evaluate their methods on only All2One attack where we consider only one target label. However, there can be multiple target classes in a practical attack scenario. We consider one such case: All2All attack where target classes are uniformly distributed among all available classes. In Table 15, we show the performance under such settings for 3 different attacks with a poison rate of 10%. It shows that All2All attack is more challenging to defend against as compared to All2One attack. However, the performance of NGF seems to be consistently better than other defenses for both of these attack variations. For reference, we achieve an ASR improvement of 3.12% over ANP while maintaining a lead in classification accuracy too.\n\nM COMBINING DIFFERENT BACKDOOR ATTACKS\n\nWe also perform experiments with combined backdoor attacks. To create such attacks, we poison some portion of the training data using 3 different attacks; Badnets, Blend, and Trojan. Each of these\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Cluster Structures Without Discriminator (Before Purification)\n\n(b) Cluster Structures Without Discriminator (After Purification)\n\n(c) Cluster Structures With Discriminator (Before Purification)\n\n(d) Cluster Structures With Discriminator (After Purification).\n\nFigure 3: t-SNE visualization of class features for CIFAR10 dataset with Badnets attack. For visualization purpose only, we assign label \"0\" to clean data cluster from target class and label \"11\" to poison data cluster. However, both of these clusters have same training label \"0\" during training. With discriminator 3c, the clusters are relatively closer and harder to discriminate as compared to without discriminator 3a. However, after fine-tuning using clean data, NGF can remove the backdoor effect. Even though the clusters are very close, the classifier can still discriminate the clusters as shown by the 3b&3d. After purification, poison data are distributed among their original ground truth classes instead of the target class. To estimate these clusters after purification (3b&3d), we take the output of classifier before softmax (embedding dim=10) and apply tSNE with 2 components.\n\nattacks have an equal share in the poison data. As shown in Table 16, we use 4 different poison rates: 10% ∼ 50%. NGF outperforms other baseline methods (MCR and ANP) by a satisfactory margin.\n\nN DECISION HEATMAPS: HOW NGF REMOVES BACKDOOR?\n\nWhile inserting the backdoor behavior, the model, especially the linear classification layer, memorizes the poison data distribution. By memorization, we mean it memorizes the simpler trigger pattern. Whenever the model sees that pattern in the input, it prioritizes the trigger-specific feature instead of image-specific (clean part) feature and predicts the adversary-set target label. When we re-train or fine-tune the classifier with clean validation data, the classifier forgets the poison distribution as finetuning reinforces the dominance of clean features in model prediction. After fine-tuning, the model looks for image-specific features for prediction as it has almost no memory of the trigger-specific features. We illustrate the decision heat-maps for clean, backdoor and purified model in Figure 4. We show the decision heatmaps for clean an poison data. As clean model is only trained on clean data, it is not sensitive to the trigger. Our defense objective says that, a purified model should behave like a benign model, i.e., the decision making process (for clean and poison data) should resemble a clean\n\n21\n\n604020020406080comp-180604020020406080comp-2CIFAR10 Train data T-SNE projection0.01.02.03.04.05.06.07.08.09.011.0Clean datacluster fromtarget classPosion Data Cluster6040200204060comp-1806040200204060comp-2CIFAR10 Train data T-SNE projection0.01.02.03.04.05.06.07.08.09.011.080604020020406080comp-1604020020406080comp-2CIFAR10 Train data T-SNE projection0.01.02.03.04.05.06.07.08.09.011.0Clean datacluster fromtarget classPosion Data Cluster80604020020406080comp-180604020020406080comp-2CIFAR10 Train data T-SNE projection0.01.02.03.04.05.06.07.08.09.011.0Under review as a conference paper at ICLR 2023\n\nmodel. As we can see for the poison data, NGF successfully removes the effect of the trigger. The purified model ignores the trigger while making decisions.\n\nFigure 4: Decision heat-maps for clean, backdoor and purified models. Regions with more reddish color is more responsible towards a decision making. For each category, we show the heatmaps for clean and poison data. Trigger is at bottom left corner of each poison data. Unlike backdoor model, clean model is insensitive to triggers in the poison sample. Wheres backdoor model causes the model to make wrong decision based on the trigger pattern. The purified model behaves like a clean model and does not look at the trigger while making a decision. All heat-maps are generated for CIFAR10 dataset attacked with BadNets. We choose this attack for better understanding of the context.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Badnets Attack\n\n(b) Badnets Purification\n\n(c) Clean-Label (CLB) Attack\n\n(d) Clean-Label (CLB) Purification\n\n(e) SIG Attack\n\n(f) SIG Purification\n\n(g) Blend Attack\n\n(h) Blend Purification\n\nFigure 5: Smoothness analysis for ImageNet Subset (first 12 classes). A ResNet34 architecture is trained on the subset. For GPU memory constraint, we consider only first 12 classes while calculating the loss Hessian. Eigen Density plots of backdoor models (before and after purification) are shown here.\n\n23\n\n1031021011000100101102103Eigenvlaue108106104102100Density (Log Scale)max:2712.9Tr(H):17015.6ACC:85.8ASR:100.01031021011000100101102103Eigenvlaue108106104102100Density (Log Scale)max:963.2Tr(H):7711.9ACC:81.3ASR:3.81031021011000100101102103Eigenvlaue108106104102100Density (Log Scale)max:3531.7Tr(H):12155.2ACC:85.0ASR:99.51021011000100101102Eigenvlaue108106104102100Density (Log Scale)max:458.1Tr(H):1193.5ACC:81.1ASR:2.31031021011000100101102103Eigenvlaue108106104102100Density (Log Scale)max:2649.8Tr(H):19228.8ACC:85.5ASR:99.11031021011000100101102103Eigenvlaue108106104102100Density (Log Scale)max:796.7Tr(H):2816.2ACC:80.7ASR:2.71031021011000100101102103Eigenvlaue108106104102100Density (Log Scale)max:3196.1Tr(H):23445.2ACC:85.1ASR:100.01031021011000100101102103Eigenvlaue108106104102100Density (Log Scale)max:768.1Tr(H):1347.4ACC:81.9ASR:3.5Under review as a conference paper at ICLR 2023\n\n(a) Backdoor Attack (VGG19)\n\n(b) Backdoor Purification (VGG19)\n\n(c) Backdoor Attack (MobileNetV2)\n\n(d) Backdoor Purification (MobileNetV2)\n\n(e) Backdoor Attack (GoogleNet)\n\n(f) Backdoor Purification (GoogleNet)\n\n(g) Backdoor Attack (InceptionV3)\n\n(h) Backdoor Purification (InceptionV3)\n\n(i) Backdoor Attack (DenseNet121)\n\n(j) Backdoor Purification (DenseNet121)\n\nFigure 6: Smoothness Analysis for different architectures. For all architectures, we consider badnets attack on CIFAR10.\n\n24\n\n1031021011000100101102103Eigenvlaue108106104102100Density (Log Scale)max:759.3Tr(H):5424.6ACC:90.8ASR:100.01011000100101Eigenvlaue108106104102100Density (Log Scale)max:20.1Tr(H):28.9ACC:85.1ASR:1.21031021011000100101102103Eigenvlaue108106104102100Density (Log Scale)max:707.6Tr(H):6906.3ACC:86.0ASR:1.61011000100101Eigenvlaue108106104102100Density (Log Scale)max:10.2Tr(H):28.8ACC:86.0ASR:1.61021011000100101102Eigenvlaue108106104102100Density (Log Scale)max:204.8Tr(H):1479.4ACC:92.8ASR:100.01000100Eigenvlaue108106104102100102Density (Log Scale)max:1.5Tr(H):12.4ACC:91.8ASR:0.11021011000100101102Eigenvlaue108106104102100Density (Log Scale)max:195.6Tr(H):1598.3ACC:90.9ASR:100.01000100Eigenvlaue108106104102100Density (Log Scale)max:4.5Tr(H):38.8ACC:87.9ASR:1.71031021011000100101102103Eigenvlaue108106104102100Density (Log Scale)max:1169.4Tr(H):12836.6ACC:92.2ASR:100.01011000100101Eigenvlaue108106104102100Density (Log Scale)max:35.8Tr(H):37.0ACC:86.7ASR:1.9",
    "reference": "# Summary Of The Paper\n\nThis paper proposed NGF, which uses clean data to remove the backdoor from a model.  The proposed method works by fine-tuning the last layer of the model such that the false local minima are smoothed out and the effect of removing the backdoor from the model is achieved. Finally, this paper gives very extensive and comprehensive experiments to demonstrate the performance of their proposed model\n\n# Strength And Weaknesses\n\nStrength:\n1. The motivation of this paper is clear, and the experiments are effective in proving the proposition of removing Backdoor from the model by smoothing the loss surface of the model.\n2. This paper demonstrates the effectiveness of the proposed approach through extensive and comprehensive experiments. The experimental results under various Backdoor Attack methods and models are included, and the effectiveness and superiority of the proposed approach is demonstrated by comparing various mainstream defense methods.\n\nWeaknesses:\n1. Not much intuition and discussion on how the proposed method will help eliminate the non-smoothness and why it is better than other solutions. \n2. lack comparison with new defense baselines \n3. technical contribution is not much aside from a regularization term in loss and the use of NGD\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe implementation is well supported by experiments.  \n\nThere are some typos in the text, such as \"Eigenvlaue\" should be \"Eigenvalue\" in Figure 1, the last \")\" in eqn(3) is missing, and so on.\n\n# Summary Of The Review\n\n1. There lacks sufficient intuition on why smoothness is the key to remove the backdoor.  Essentially smoothness measures the changes in gradients wrt to changes in inputs. Then the authors’ conjecture is that triggers in input could lead to large changes in gradients. But why it must be a backdoor? It seems to me that the lipshitz pruning method (CLP, Zheng et al. 2022) is telling a more convincing story that if triggers in input could lead to large changes in loss, it might be backdoored.\n\n\n2. For runtime comparison, the author should compare with other baselines as well as NGF without regularization term. Also for ablation study, I would like to see the results for fine-tuning on all layers. \n\n3. The proposed method requires to have clean training data in order to function properly. The authors did not test the situation when the number of clean training data is not sufficient. To be comprehensive, I would suggest the authors to include the number of training samples as an ablation study.\n\n4. The performance reported for data-free backdoor removal by Zheng et al. 2022 seems a bit strange as the original paper actually gives quite a good performance on Blend, TrojanNet. Did the author tune the hyperparameters?\n\n5. There is also some recent work on backdoor removal:\n\n    \"One-shot Neural Backdoor Erasing via Adversarial Weight Masking.\" NeurIPS 2022.\n\n    The authors may want to comment on/compare with the above work.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nUSING LANGUAGE TO EXTEND TO UNSEEN DOMAINS\n\nLisa Dunlap, Clara Mohri ∗ UC Berkeley {lisabdunlap,cmohri}@berkeley.edu\n\nAditi Raghunathan Carnegie Mellon University raditi@cmu.edu\n\nHan Zhang, Devin Guillory, Trevor Darrell, Joseph E. Gonzalez, Anna Rohrbach UC Berkeley {pariszhang,dguillory,trevordarrell,jegonzal,anna.rohrbach}@berkeley.edu\n\nABSTRACT\n\nIt is expensive to collect training data for every possible domain that a vision model may encounter when deployed. We instead consider how simply verbalizing the training domain (e.g. “photos of birds”) as well as domains we want to extend to but do not have data for (e.g. “paintings of birds”) can improve robustness. Using a multimodal model with a joint image and language embedding space, our method LADS learns a transformation of the image embeddings from the training domain to each unseen test domain, while preserving task relevant information. Without using any images from the unseen test domain, we show that over the extended domain containing both training and unseen test domains, LADS outperforms standard fine-tuning and ensemble approaches over a suite of four benchmarks targeting domain adaptation and dataset bias. Code is available at https://github.com/lisadunlap/LADS.\n\nFigure 1: Consider a model trained to recognize road signs in sunny weather. We aim to extend to a new domain of snowy weather. Our method LADS (Latent Augmentation using Domain descriptionS) leverages a multimodal model’s knowledge of the classes and the domain shift verbalized in natural language (“sunny” to “snowy”) to train an augmentation network without any samples from the unseen test domain. This network is used to translate multimodal image embeddings from the training domain to the unseen test domain, while retaining class-relevant information. Then, real and augmented embeddings are used jointly to train a classifier.\n\n1\n\nINTRODUCTION\n\nThe ability to extend a model beyond the domain of the training data is central to building robust computer vision models. Methods for dealing with unseen test distributions often require leveraging additional image data, but linguistic knowledge of the anticipated domain shift is much cheaper and easier to obtain. For example, in many settings, the training images are collected in certain\n\n∗equal contribution\n\n1\n\n“Stop Sign”similarityExtendedDomainI have photos of sunny road signs, but I want do well on snowy road signs. Latent Augmentation using Domain descriptionSA mix of training + unseen domainsamples Joint Vision & Language Embedding SpaceaugθTraining DomainUnseen Domain≈Training Domain“Snowy” similarity“Sunny” similarity“Snowy Tree” ≈Published as a conference paper at ICLR 2023\n\nconditions (e.g., daylight, clear weather, ...) but our sensors may also experience less common but easy to anticipate conditions (e.g., night, snow, haze, illustrations, ...). Directly collecting or creating data in all possible anticipated settings is often prohibitively expensive. Thus it is of great interest how one can linguistically extend to unseen domains: that is, to utilize language to improve performance on an unseen test domain without sacrificing performance on the training domain.\n\nThe use of language in domain generalization has generated significant interest with the development of large vision-language models such as CLIP (Radford et al., 2021), Flamingo (Alayrac et al., 2022), and ALIGN (Jia et al., 2021), which allow users to create zero-shot classifiers using only class names. However, while these models have been shown to achieve remarkable cross-domain generalization, their zero-shot classifiers often perform far worse than models trained for a particular downstream task (Radford et al., 2021; Kumar et al., 2022). When training data is available for the downstream task, a common practice is to fine-tune these models on the training data. While this significantly improves in-domain accuracy, it degrades performance on unseen domains.\n\nWe show that it is possible to leverage the domain-level knowledge (e.g. sunny environments vs. snowy environments in our example) contained in CLIP or similar models to deal with a variety of domain shifts in a way that requires no data from the new test domain, exploits the labeled training data, and is fast to train. Our method only requires users to input text descriptions of the training and unseen test domains (e.g. “a sunny stop sign” and “a snowy stop sign”) along with their training data. To achieve language-guided domain generalization, we leverage the broad domain knowledge encoded in CLIP coupled with its shared image-language embedding space to perform latent feature augmentation of the training set.\n\nMore precisely, the embeddings of these textual descriptions are used to train an augmentation model which learns a transformation on the CLIP image embeddings of the training domain and “places” them in the new domain (see Figure 1). We train this augmentation model with two objectives: (1) translating the image embedding from the training domain to the unseen testing domain, while (2) retaining the class-specific information of the original image. Once this transformation is learned, we train a simple linear classifier on the combined augmented and unaugmented image embeddings, resulting in a classifier that outperforms common fine-tuning methods on the extended domain while achieving similar performance on the training domain.\n\nWe introduce LADS, a method to extend a model to new domains given only a language description of the distribution shift. Our main contributions are (1) the introduction of the Domain Extension with Language problem, (2) a novel language-guided latent feature augmentation training procedure, and (3) the extension of our method to address spurious correlation biases in the training data.\n\nWe evaluate LADS on two domain adaptation benchmarks, DomainNet (Peng et al., 2019) and CUBPaintings (Wang et al., 2020), as well as two benchmarks exhibiting color and contextual bias, Colored MNIST (Arjovsky et al., 2021) and Waterbirds (Sagawa et al., 2019). On the domain adaptation benchmarks, we show that we improve out-of-domain performance by 1-3% while matching in-domain performance of fine-tuned and ensembled models. On the biased benchmarks, we show an almost 2x improvement in out-of-domain performance over fine-tuned models. Across all benchmarks, LADS achieves the highest accuracy on the entire extended test domain containing both training and unseen test domain samples. Finally, we perform an in-depth analysis of the altered image embeddings, the effect of each loss function, and the effect of different vision and language models to understand our framework better.\n\n2 RELATED WORK\n\nDomain Adaptation/Generalization. The challenge of out-of-domain generalization is well studied (Recht et al., 2019; Petryk et al., 2022; Kumar et al., 2022; Santurkar et al., 2021; Hendrycks & Dietterich, 2019) with a large body of work in domain adaptation addressing the problem of adapting a model to perform well on a new target domain. A typical domain adaptation approach involves collecting additional unlabeled data from the target domain (Ganin & Lempitsky, 2015; Saito et al., 2017; Arjovsky et al., 2021; Kim et al., 2018; Tzeng et al., 2015), and aims to train a classifier such that it cannot tell the difference between source and target domain.\n\nIn the limited data setting, few-shot domain adaptation (Motiian et al., 2017; Yue et al., 2021) aims to learn from as little as one example in the target domain. Work in domain generalization (Wang &\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nJiang, 2020; Gulrajani & Lopez-Paz, 2021; Koh et al., 2021) does not need target domain data but requires a set of several aligned and labeled source domains, and often shows only limited gains. While we evaluate on certain domain adaptation benchmarks, DA/DG methods primarily focus on maximizing target domain accuracy, while our work is interested in maximizing the accuracy of the extended domain. Furthermore, unlike previous works, we assume we have no access to any target data (labeled or unlabeled), only a single source domain, and our domain shift can be verbalized.\n\nFine-tuning under Distribution Shift. The goal of fine-tuning under distribution shift is to tailor pretrained models to a specific task without sacrificing their ability to deal with distribution shifts. Kumar et al. (2022) found that it is better to fit a linear probe on the features and then fine-tune the model’s backbone. For robust fine-tuning of CLIP specifically, Wortsman et al. (2021) proposed ensembling the weights of the fine-tuned image encoder with the zero-shot image encoder. We see our work as complementary to these ideas, targeting semantically defined domain shifts to increase OOD performance, while maintaining high ID performance.\n\nSemantic Augmentation with CLIP. With the emergence of CLIP, several works (Ramesh et al., 2022; Patashnik et al., 2021; Gal et al., 2021) have used language to alter images using a combination of CLIP and a generative model. Broadly, these works translate an image to a CLIP embedding, alter the image embedding with a text embedding of the desired augmentation, and use that embedding to generate an altered image. These CLIP-based works do not attempt to use these data augmentations in the context of dataset bias or domain adaptation. Some prior work has explored augmentations using generative models (Sharmanska et al., 2020; Sankaranarayanan et al., 2018; Yan et al., 2021), but since they generate images at the pixel level, they are often bottle-necked by the quality of the In contrast, we choose to manipulate embeddings directly that allows us to generative process. effectively distill the knowledge in CLIP.\n\nRemoving Dataset Bias. In computer vision, several works debias data using extra information such as instance annotations (Hendricks et al., 2018; Li et al., 2018; Rieger et al., 2020), bounding boxes (Choi et al., 2019), or image-level bias annotations (Kim et al., 2018). Some methods (Sharmanska et al., 2020; Bau et al., 2020; Santurkar et al., 2021) forego the need for expensive annotations by utilizing generative models, while Petryk et al. (2022) utilize CLIP to translate language descriptions of a task into spatial guidance. In contrast, we do not limit ourselves to purely spatial bias or use per-image annotations of the bias, only a description of what biases may appear in the training data.\n\n3 LATENT AUGMENTATION USING DOMAIN DESCRIPTIONS\n\nWe consider the supervised learning problem of generalizing to new unseen domains using only the verbal descriptions of the training domain and the anticipated but unseen new domains. More formally, we are given a training dataset {xi, yi}n i=1 drawn from the training domain Dtraining, the class names ty, a written description ttraining of the training domain, and a set of written descriptions {ti i=1 that we expect to encounter at test time. Our goal is to train a model that performs well on both the original domain Dtraining as well as the unseen domains {Di i=1. We call this the Domain Extension with Language problem.\n\ni=1 of k unseen domains {Di\n\nunseen}k\n\nunseen}k\n\nunseen}k\n\nLarge vision-language models have demonstrated the ability to generalize to new domains with language but only in the zero-shot setting. In order to utilize available training data, we explore the popular fine-tuning technique of linear probing: fitting a linear classifier to the image embeddings of large vision-language models. We chose linear probing over full fine-tuning as it is faster to train and has been shown to result in more robust classifiers (Kumar et al., 2022; Radford et al., 2021).\n\nWhile standard linear probing only uses the image embeddings and the numerical labels, LADS also utilizes the text describing the classes and the descriptions of domain shift to augment the probe’s training data to mimic samples from the unseen domain. Our two-stage approach first learns a network that transforms the image embeddings rather than the pixels themselves, with the goals of (1) augmenting the embedding to be aligned with the unseen domain while (2) retaining the features consistent with its class label. The second stage performs linear probing on the training set containing both the original image embeddings as well as the augmented image embeddings to produce a classifier that is more robust to the specified domains. Note that we do not use any data from Dk unseen in either stage—we only use the class names and domain descriptions. An outline of the first stage of our method (training the augmentation network) is depicted in Figure 2.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: LADS. Let the task be to classify Puffin vs. Sparrow. The training data Dtraining contains photos of the two classes but we would like to extend our classifier to paintings as well: that is, Dunseen. We aim to do this using the text descriptions of the training and new domain, ttraining and tunseen, respectively. The augmentation network faug is trained to transform image embeddings from Dtraining to Dunseen using a domain alignment loss LDA and a class consistency loss LCC. When LDA is low, the augmented embeddings are in the new domain but may have drifted from their class. When LCC is low, the augmented embeddings will retain class information but may fail to reflect the desired change in domain. faug aims to augment every image embedding to a space with low domain alignment loss and low class consistency loss, resulting in faug(I(x)) having an image embedding similar to a painting of a Puffin. Note that the hallucinated image embeddings on the right are a pictorial representation of the effect of each loss function and not actually generated by LADS.\n\nWe choose CLIP (Radford et al., 2021) as our vision-language model in our evaluations. Let Iθ(x) = CLIPimg(x) ∈ I denote the image embedding of input image x and Tθ(t) = CLIPtext(t) ∈ T denote the CLIP text embedding of some text t. Furthermore, let ttraining ◦ ty denote the composition of the domain description and the class name. For example, if ttraining = “a photo of a”, t1 unseen = “a painting of a” and ty could be “Puffin”. The composition ttraining ◦ ty is “a photo of a Puffin”.\n\naug : I → I that transforms image embeddings from Dtraining to Dk\n\nStage 1: Training the augmentation network. The first stage of LADS is to learn an augmentation network f k unseen using the corresponding language descriptions ttraining and tk unseen. As mentioned previously, a valuable augmentation is one which places the transformed embedding in unseen domain Dk unseen while retaining features relevant to the class label. To achieve this, we train f k aug using a combination of two losses: Domain Alignment and Class Consistency. In the setting of adapting to multiple new domains at once, we train a unique f k aug network for each domain as described above.\n\nDomain Alignment. The domain alignment loss encourages the augmented image embeddings aug(Iθ(x)) to look like image embeddings from the new domain Dk f k unseen. This loss is guided by the text embeddings of the domain descriptions tk\n\nunseen and ttraining.\n\nWhile CLIP is trained such that the space of image embeddings I has some correspondence with the space of text embeddings T , it is not obvious what a mapping between I and T should look like. Thus, inspired by prior work (Patashnik et al., 2021; Gal et al., 2021), we assume the existence of a “global direction” that corresponds to a shift from Dtraining to Dk unseen that is shared across both the image embedding space and text embeddings space.\n\nThis “global direction” is defined as the normalized difference of the embeddings from the target domain and the embeddings from the source domain. Formally, the domain alignment loss of f k for training point (xi, yi) is\n\naug\n\nLDA(f k\n\naug) =\n\n(cid:32)\n\n1 −\n\nn (cid:88)\n\ni=1\n\nf k ∥f k\n\naug(Iθ(xi)) − Iθ(xi) aug(Iθ(xi)) − Iθ(xi)∥\n\n·\n\nTθ(tunseen, yi) − Tθ(ttraining, yi) ∥Tθ(tunseen, yi) − Tθ(ttraining, yi)∥\n\n(cid:33)\n\n.\n\n(1)\n\nClass Consistency. The domain alignment loss in Equation 1 encourages the augmented embeddings to only differ in the direction of change in the domain. If there were one global shared direction corresponding to the domain shift, optimizing LDA would be sufficient. However, in practice, we\n\n4\n\nt!\"#$%$%&= “a photo of a bird”t’%())%= “a painting of a bird” domainst*=“Puf.in”T!(“Sparrow”)T!(“Puf/in”)I!(Puffin)T!(t\"#$%%#)T!(t&’()#)#*)f(\"*(I!Puffin)L!!L\"#f$%&UserSuppliedInputslabelsy=[”Puf.in”,“Sparow”]T’I’hallucinatedimage embeddingsVision & Language Model≈≈≈+=L!!L\"#L()*+Published as a conference paper at ICLR 2023\n\nfind that optimizing LDA alone removes some class relevant information and results in little diversity among the augmented embeddings of different images (see Section 4.6 and Figure 11). Thus we add a class consistency loss which preserves class information in the augmented embeddings. We measure class information in the image embeddings by our ability to classify the images accurately via CLIP zero-shot with the class names. Formally,\n\nLCC(f k\n\naug) =\n\nn (cid:88)\n\ni=1\n\nCross-entropy(cid:0)Softmax[f k\n\naug(Iθ(xi)) · Tθ(yi)], yi\n\n(cid:1)\n\n(2)\n\nNote that this is the same objective as the standard CLIP loss. We use CLIP zero-shot rather than trying to fine-tune CLIP because that could lead to overfitting where we classify the augmented training image embeddings correctly even when they do not contain class relevant information.\n\nOur final objective LLADS(faug) to train the augmentation network as the first step in LADS is a linear combination of the the domain alignment loss and class consistency loss:\n\nLLADS(f k\n\naug) = αLDA(f k\n\naug) + (1 − α)LCC(f k\n\naug),\n\n(3)\n\nwhere α is a hyperparameter dictating the trade-off between domain alignment and class consistent.\n\nStage 2: Fine-tuning. After the augmentation network f k the original image embeddings Iθ(xi) along with the augmented embeddings f k ence is straightforward: apply the linear probe on the CLIP image embeddings of the test images.\n\naug is trained, we train a linear probe on aug(Iθ(xi)). Infer-\n\n3.1 ADDRESSING DATASET BIAS\n\nIn addition to dealing with extended domains, LADS can also be used in the dataset bias setting where there are spurious correlations in the dataset. For example, in Waterbirds (Sagawa et al., 2019), we want to classify Landbirds vs. Waterbirds, where the spurious correlation is the background (Landbirds appear on forest backgrounds and Waterbirds appear on water backgrounds in training). To prevent a classifier from using this correlation to make predictions, we can use LADS to generate augmentations that represent “Landbird on water” and “Waterbird on land”.\n\nWe do this by using CLIP to label the backgrounds of each image and then decide what ttraining and tunseen is per example. Given the domain information tland = “a {} in the forest” and twater = “a {} on the water”, we can use zero-shot CLIP to determine if a given image is on land or water. If the image is predicted to be on land, when training faug, LDA for that particular example will use ttraining = tland, tunseen = twater and vice versa. The class consistency loss and the other parts of the pipeline remain unchanged. Because we are using the vision and language model to label the domains, we do not need per-image labels of the bias, only a hypothesis of what the bias may be.\n\n4 EXPERIMENTS\n\nIn this section we discuss our main experiments and results. We defer dataset details, the remainder of the experiments and their discussion to the Appendix (B, D, E).\n\n4.1\n\nIMPLEMENTATION DETAILS\n\nIn line with Radford et al. (2021), we normalize all text and image embeddings when performing zero-shot inference or training with CLIP embeddings. The augmentation network faug used in LADS is a 2-layer MLP with input and output dimensions of 768 and a hidden dimension of 384. Within LADS and all the CLIP-related baselines, we use the OpenAI CLIP model with a ViT-L backbone and resize all images to 224x224. We train on 10 GeForce RTX 2080 Ti GPUs.\n\nFor each baseline, we do a hyperparameter sweep across learning rate and weight decay and choose the parameters with the highest class-balanced validation accuracy. For LADS we also do a sweep across the parameters of the augmentation network, namely learning rate, weight decay, and α, and\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nselect a checkpoint based on the validation loss. In general, we set α = 0.5, lr = 0.001, wd = 0.05. Our hyperparameter search spaces and final choice of hyperparameters are listed in Table 4.\n\nIn our results we report test accuracy on Dtraining, Dunseen, and the extended domain which averages the two. We run each method over 5 different random seeds and report the mean and standard deviation.\n\n4.2 DATASETS\n\nCUB-Paintings (one new domain) is composed of 2 datasets, CUB-200 (Wah et al., 2011), a fine-grained bird classification benchmark containing 200 different bird species and CUB-200Paintings (Wang et al., 2020), which contains the same classes as CUB-200 but instead of real images they are paintings collected from the web and filtered manually. We use the domain descriptions ttraining = “a photo of a {} bird”, t1\n\nunseen = “a painting of a {} bird”.\n\nDomainNet (multiple new domains) is a specific split (Tan et al., 2020) of the original DomainNet (Peng et al., 2019) dataset which contains the 40 most common classes from 4 domains: ‘sketch’, ‘real’, ‘clipart’, and ‘painting’. Like prior work (Kumar et al., 2022; Tan et al., 2020), we train on sketches and evaluate on the three other domains. We use the domain descriptions ttraining = “a sketch of a ”, t1 unseen = “a realistic photo of a ”.\n\nunseen = “a painting of a ”, t3\n\nunseen = “clipart of a ”, t2\n\nColored MNIST (color bias) (Arjovsky et al., 2021) was made by taking the original MNIST In the training and validation sets, Digits (Deng, 2012), and coloring them red or blue. even numbers are red and odd numbers are blue, while in the test set digits are colored The task is to classify the digits 0, 1, .., 9. We use the domain descriptions randomly. “a photo of a red number ”, “a photo of a blue number ”.\n\nWaterbirds (contextual bias) (Sagawa et al., 2019) is a synthetically created dataset which creates contextual bias by taking species of landbirds and waterbirds from the CUB-200 Wah et al. (2011) dataset and pasting them on forest and water backgrounds from the Places (Zhou et al., 2017) dataset. For the training and validation sets, all landbirds appear on forest backgrounds and waterbirds appear on water backgrounds while the test set has an even representation of backgrounds and bird types. We use the domain descriptions “a photo of a {} in the forest”, “a photo of a {} on the water”.\n\n4.3 BASELINES\n\nGeneric and Adaptive zero-shot CLIP are the zero-shot baselines proposed by Radford et al. (2021): (CLIP ZS (G)) uses the class name alone as the text prompt, while adaptive zero-shot CLIP (CLIP ZS (A)) caters the text prompts to the specific domains (e.g. “a painting of an airplane.”). To do well on the extended domain, we average the text embeddings of each class across all possible domains.\n\nCLIP LP fits a linear classifier on top of the CLIP image embeddings.\n\nCLIP LP (ZS init) initializes the linear classifier with the text embeddings.\n\nWiSE -LP (Wortsman et al., 2021) is an ensembling technique which fine-tunes a CLIP model and does a weighted average of the fine-tuned model’s weights with the original. Due to the size of the vision and language models we are using, we did not fine-tune the entire backbone and instead ensembled the classifier with the linear classifier probe as explained by Wortsman et al. (2021).\n\nVQGAN + CLIP (Crowson et al., 2022) is a method that uses a VQGAN (Esser et al., 2021) trained with CLIP to augment images in pixel space. Using a text prompt and an image, we perform “style transfer” to the new domain in order to augment the training data. We then train a linear probe on the augmented and non-augmented CLIP embeddings. Due to the amount of time and compute required to generate images, we only ran this baseline for DomainNet and augmented approximately 15% of the training dataset. Examples of the augmented images are provided in Table 7.\n\n4.4 RESULTS\n\nTable 1 shows in-domain (ID) and out-of-domain (OOD) accuracy on CUB-Paintings and DomainNet. The “Extended” column is the average accuracy of the two, corresponding to the full extended\n\n6\n\nPublished as a conference paper at ICLR 2023\n\ndomain. For CUB-Paintings and DomainNet, LADS is able to match or improve the ID accuracy of the fine-tuning baselines while improving over their OOD accuracy. Although CLIP zero-shot achieves higher OOD accuracy on DomainNet, LADS achieves the highest result when evaluated on the full extended domain. We also improve over the VQGAN+CLIP baseline on DomainNet.\n\nFor Colored MNIST (Figure 3a) and Waterbirds (Figure 3b), LADS is able to roughly match ID accuracy of the fine-tuned CLIP and OOD accuracy of CLIP zero-shot, resulting in approximately a 10% improvement on the extended domain. We explore different weighted averages of ID and ODD accuarcy to compute the extended domain accuracy in Section C of the Appendix.\n\nDataset\n\nMethod\n\nCUB-Paintings CLIP ZS (G) CUB-Paintings CLIP ZS (A)\n\nID\n\n60.34% 61.93%\n\nOOD\n\n52.84% 54.38%\n\nExtended\n\n56.59% 58.16%\n\nCUB-Paintings CLIP LP CUB-Paintings CLIP LP (ZS init) CUB-Paintings WiSE-LP CUB-Paintings\n\nLADS\n\n75.12±0.18% 85.91±0.08% 64.33±0.29% 75.57±0.06% 86.08±0.11% 65.05±0.05% 81.74±0.34% 73.27±0.22% 64.80±0.10% 86.14±0.29% 66.18± 0.25% 76.16±0.23%\n\nDomainNet DomainNet\n\nDomainNet DomainNet DomainNet DomainNet DomainNet\n\nCLIP ZS (G) CLIP ZS (A)\n\n93.49% 93.24%\n\n95.94% 96.01%\n\n94.72% 94.62%\n\nCLIP LP CLIP LP (ZS init) WiSE-LP VQGAN+CLIP LADS\n\n94.39±0.04% 93.75±0.02% 95.03±0.07% 95.21±0.21% 94.58±0.11% 93.95±0.03% 95.19± 0.34% 93.68± 0.12% 94.44±0.11% 95.54± 0.09% 93.83± 0.10% 94.67± 0.09% 95.33 ± 0.33% 95.21 ± 0.09% 95.27± 0.14%\n\nTable 1: In-domain (ID), out-of-domain (OOD) and extended domain accuracy on CUB-Paintings and DomainNet. For DomainNet, we include the pixel augmentation baseline VQGAN+CLIP and OOD accuracy is the average of the 3 unseen domains. LADS is able to beat all methods on the extended domain for both datasets. Note that for tasks where CLIP zero-shot does not perform well, LADS is able to significantly outperform zero-shot on the unseen domain.\n\n(a) Colored MNIST.\n\n(b) Waterbirds.\n\nFigure 3: Result on dataset bias benchmarks. Left and center plots show the training domain and unseen domain performance of the zeroshot and fine-tuned baselines respectively. For both Colored MNIST (a) and Waterbirds (b), LADS is able to roughly match the unseen domain accuracy of zero-shot methods and the seen domain accuracy of fine-tuned methods, resulting in improved performance on the extended domain (right).\n\n7\n\nIDOOD30405060708090100Accuracy (%)57.8855.4479.9677.4498.3876.70ZS (G)ZS (A)LADSIDOOD30405060708090100Accuracy (%)99.4539.5799.4539.6199.3858.5098.3876.70CLIP LPCLIP LP (ZS)WiSELADSExtended30405060708090100Accuracy (%)56.6678.7069.5169.5378.9487.54ZS (G)ZS (A)CLIP LPCLIP LP (ZS)WiSELADSIDOOD5060708090100Accuracy (%)83.0465.4383.8672.5898.0371.95ZS (G)ZS (A)LADSIDOOD5060708090100Accuracy (%)97.7962.1298.4258.6590.0668.4598.0371.95CLIP LPCLIP LP (ZS)WiSELADSExtended5060708090100Accuracy (%)74.2378.2279.9678.5379.2584.99ZS (G)ZS (A)CLIP LPCLIP LP (ZS)WiSELADSPublished as a conference paper at ICLR 2023\n\nCLIP LP\n\nVQGAN+CLIP\n\nLADS\n\nDomain Alignment score Class Consistency score\n\n81.30±1.35% 69.26±2.31% 91.42±0.47% 77.14±1.22%\n\n85.44±0.61% 87.26±0.74%\n\nTable 2: Augmentation Quality for DomainNet. The domain alignment and class consistency scores over 1000 randomly sampled training embeddings and their nearest neighbor in the test set. LADS achieves higher domain alignment and class consistency scores than VQGAN+CLIP, and is able to obtain a somewhat similar class consistency score to the unaugmented image embeddings.\n\n4.5 ANALYSIS OF AUGMENTATION QUALITY\n\nIn this section, we explore the quality of the embeddings generated by our augmentation network. We perform our analysis for DomainNet below, defering the remaining results to Appendix D.1, D.2.\n\nSince there is no publicly available model to convert CLIP embeddings to images, we use the nearest neighbors of the augmented embeddings from the extended test domain to confirm that our augmentations match our expectations. We take a random subset of 1,000 samples from the image embeddings used to train the linear probe: for CLIP LP, this is simply {Iθ(xi), yi}n i=1, for VQGAN+CLIP it is of a mix of {Iθ(xi), yi}n i=1 and the augmented embeddings (cid:83)k i=1 for each unseen domain j. We obtain the nearest neighbors in the extended test set (containing images from the training and unseen domain) with respect to cosine similarity of the image embeddings. In line with our domain alignment and class consistency loss functions, we define metrics for (1) correctly altering the domain of the image embedding, while (2) retaining the class information. We define the percentage of the nearest neighbors that belong in the desired domain as the domain alignment score, and the percentage that belong to the original class as the class consistency score.\n\ni=1 and GAN generated images, and for LADS it is {Iθ(xi), yi}n\n\naug(Iθ(xi)), yi}n\n\nj=1{f j\n\nThe CLIP LP scores can be viewed as an approximate upperbound for those of LADS since they reflect the nearest neighbors of only the original sketch embeddings in the extended domain. As shown in Table 2, LADS is able to beat the domain alignment score and closely fall behind the class consistency score of the linear probe, implying that the augmentations are of similar quality to the original image embeddings. Furthermore, LADS has better domain alignment and class consistency than VQGAN+CLIP, indicating that the long and laborious pixel-level augmentation may be producing lower quality training samples than our simple embedding augmentation.\n\nFor qualitative analysis of LADS, we visualize a random sample of 10 nearest neighbors from DomainNet in Figure 4 (the sketch embeddings are non-augmented, all others are augmented). The nearest neighbors of augmented embeddings closely resemble embeddings of similar images in the desired unseen domain. Even if the nearest neighbor is of a different class, it maintains some visual similarity to the original image.\n\nFigure 4: Nearest Neighbors for LADS on DomainNet Sketch → Clipart, Painting, Real. The top row shows training images with the label on top being the intended domain augmentation for that embedding. The bottom row shows the nearest neighbor of the augmentation in the extended domain. Not only does LADS produce augmented embeddings within the correct domain, embeddings often match the specific class and stylistic elements of each original image.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n4.6 ABLATIONS\n\nIn this section, we ablate the class consistency and domain alignment loss described in Section 2. We defer the remainder of the ablations, including ablations of the domain descriptions and the CLIP model, to Appendix D.3, D.4, D.5.\n\nIn order to measure the impact of the domain alignment and class consistency loss functions, we ablate each one and report the accuracy, domain alignment score, and class consistency score. We also experiment with a domain-specific class consistency loss, which replaces T (yi) with T (tk unseen ◦ yi) in order to enforce the class and domain all in one loss. We display our results on the Waterbirds dataset below, with experiments on the other datasets in Appendix D.3.\n\nAs shown in Figure 5, the domain alignment loss alone results in a high domain alignment score, but low accuracy due to losing some class specific information. Meanwhile, the class consistency loss alone achieves the highest class consistency score because it retains the relevant class information, but it fails to improve the OOD accuracy since the augmented embeddings are not within the new domain. Even in the case of domain specific LCC when the extended domain is incorporated into the class consistency loss, the scores only slightly improve. It is only when we combine both our losses that we are able to retain class information while transforming the image embeddings to the desired domain, leading to improved out-of-domain accuracy. Nearest neighbor visualizations of the different losses are given in Appendix D.3.\n\nFigure 5: Effect of the Loss Functions. We report the results of training with just the domain alignment loss, the class consistency loss, a domain-specific class consistency loss, and the domain alignment + class consistency loss, on Waterbirds. The DA loss results in high DA score but low accuracy. The CC loss results in low DA score and does not improve the OOD accuracy; the domainspecific CC variant brings negligible gains. Our final design (LDA+LCC) works the best.\n\n5 LIMITATIONS AND FUTURE WORK\n\nSince one must input a natural language description of the distribution shift, LADS may not apply to “natural” distribution shifts where the change cannot be verbalized (Koh et al., 2021). Furthermore, as our approach is reliant on the richness of concepts learned by a pretrained vision-language model, it is also limited to domains that can be accurately represented with textual descriptions, and are well covered in the data the pretrained models were trained on. As a general rule of thumb, if CLIP zeroshot has very poor performance when it comes to classifying the domains and/or classes, LADS should not be used (see Section E of the Appendix).\n\nWe have presented LADS, a fine-tuning method for addressing the task of Domain Extension with Language. We view LADS as a jumping-off point for further exploration regarding how we can use the zero-shot capabilities of large multimodal models to improve accuracy on a desired domain given only language description as input. We hope that future work is able to perform reliable embedding augmentations independently of the ability of CLIP to correctly classify the domains and classes at hand. Furthermore, we hope future work is able to analyze more complicated domain shifts such as the ones seen in WILDS Koh et al. (2021) or Imagenet-A (Hendrycks et al., 2021).\n\nAcknowledgements. This work was supported in part by DoD including DARPA’s SemaFor, PTG and/or LwLL programs, as well as BAIR’s industrial alliance programs.\n\n9\n\nIDOODExtended405060708090100Accuracy (%)97.7862.3180.0498.7159.2979.0098.0659.3778.7198.1865.4681.8298.1071.8184.95CLIP LPLDALCCDom + LCCLDA + LCCDomain AlignmentClass Consistency405060708090100Score (%)86.0094.9050.4696.3052.0496.2090.6495.28Published as a conference paper at ICLR 2023\n\nREFERENCES\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\n\nMartin Arjovsky, L ́eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. In Inter-\n\nnational Conference on Learning Representations (ICLR), 2021.\n\nDavid Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, and Antonio Torralba. Rewriting a deep generative model. European Conference on Computer Vision (ECCV), 2020. URL https://arxiv.org/abs/ 2007.15646.\n\nJinwoo Choi, Chen Gao, C. E. Joseph Messou, and Jia-Bin Huang. Why can’t i dance in the mall? learning to\n\nmitigate scene bias in action recognition. In NeurIPS, 2019.\n\nKatherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Edward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance. arXiv:2204.08583, 2022.\n\nLi Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Process-\n\ning Magazine, 29(6):141–142, 2012.\n\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12873–12883, 2021.\n\nRinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-guided\n\ndomain adaptation of image generators. arXiv:2108.00946, 2021.\n\nYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. International\n\nConference on Machine Learning, 2015.\n\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference\n\non Learning Representations (ICLR), 2021.\n\nLisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. Women also snowboard: Overcoming bias in captioning models. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 771–787, 2018.\n\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and\n\nperturbations. Proceedings of the International Conference on Learning Representations, 2019.\n\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples.\n\nCVPR, 2021.\n\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773.\n\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021.\n\nByungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning not to learn: Training deep neural networks with biased data. Conference on Computer Vision and Pattern Recognition (CVPR), 2018. URL http://arxiv.org/abs/1812.10352.\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-thewild distribution shifts. In International Conference on Machine Learning, pp. 5637–5664. PMLR, 2021.\n\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=UYneFzXSJWh.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nKunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. Tell me where to look: Guided attention\n\ninference network. Conference on Computer Vision and Pattern Recognition, abs/1802.10171, 2018.\n\nSaeid Motiian, Quinn Jones, Seyed Mehdi Iranmanesh, and Gianfranco Doretto. Few-shot adversarial domain\n\nadaptation, 2017.\n\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.\n\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multisource domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1406–1415, 2019.\n\nSuzanne Petryk, Lisa Dunlap, Keyan Nasseri, Joseph Gonzalez, Trevor Darrell, and Anna Rohrbach. On guiding visual attention with language specification. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. doi: 10.48550/ARXIV.2202.08926. URL https://arxiv.org/abs/2202.08926.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\n\nimage generation with clip latents. arXiv:2204.06125, 2022.\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5389–5400. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/recht19a. html.\n\nLaura Rieger, Chandan Singh, William Murdoch, and Bin Yu. Interpretations are useful: Penalizing explanations to align neural networks with prior knowledge. In Proceedings of the 37th International Conference on Machine Learning, 2020.\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural net-\n\nworks. In International Conference on Learning Representations, 2019.\n\nKuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. Conference on Computer Vision and Pattern Recognition, 2017. URL http://arxiv.org/abs/1712.02560.\n\nSwami Sankaranarayanan, Yogesh Balaji, Carlos Domingo Castillo, and Rama Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nShibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Aleksander Madry. Editing a classifier by rewriting its prediction rules. In Neural Information Processing Systems (NeurIPS), 2021.\n\nViktoriia Sharmanska, Lisa Anne Hendricks, Trevor Darrell, and Novi Quadrianto. Contrastive examples for addressing the tyranny of the majority. CoRR, abs/2004.06524, 2020. URL https://arxiv.org/abs/ 2004.06524.\n\nShuhan Tan, Xingchao Peng, and Kate Saenko. Class-imbalanced domain adaptation: an empirical odyssey. In\n\nEuropean Conference on Computer Vision, pp. 585–602. Springer, 2020.\n\nEric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and\n\ntasks. In Proceedings of the IEEE international conference on computer vision, pp. 4068–4076, 2015.\n\nHemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5018–5027, 2017.\n\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-\n\n200-2011 dataset, 2011.\n\nJinghua Wang and Jianmin Jiang. Adversarial learning for zero-shot domain adaptation. In European Confer-\n\nence on Computer Vision, pp. 329–344. Springer, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nSinan Wang, Xinyang Chen, Yunbo Wang, Mingsheng Long, and Jianmin Wang. Progressive adversarial networks for fine-grained domain adaptation. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9213–9222, 2020.\n\nMitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. CoRR, abs/2109.01903, 2021. URL https://arxiv.org/abs/2109.01903.\n\nHao Yan, Yuhong Guo, and Chunsheng Yang. Source-free unsupervised domain adaptation with surrogate data\n\ngeneration. The British Machine Vision Conference (BMVC), 2021.\n\nXiangyu Yue, Zangwei Zheng, Hari Prasanna Das, Kurt Keutzer, and Alberto Sangiovanni Vincentelli. Multi-\n\nsource few-shot domain adaptation. arXiv preprint arXiv:2109.12391, 2021.\n\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe problem of domain extension with language is addressed in this paper. The proposed method (LADS) uses a CLIP model's domain-level knowledge to learn a latent feature augmentation of the training set. It does not require any unseen domain samples and instead relies on written descriptions of the training and unseen domains. Domain alignment and class consistency losses are used to train the latent feature augmentation. Once trained, a simple linear classifier is trained on both the original and augmented image embeddings, resulting in improved in-domain and out-of-domain recognition performance. Experiments on DomainNet, CUB-Paintings, Colored MNIST, and Waterbirds demonstrate that LADS outperforms other methods.\n\n# Strength And Weaknesses\n\n(Strengths)\n\n- The proposed method does not require any unseen domain image samples. It significantly reduces the cost of data collection.\n\n- LADS augments data at the image space level rather than the pixel level. This approach is useful for avoiding bottlenecks caused by the generative process's quality.\n\n- The proposed method can be extended to address training data biases.\n\n- Extensive experiments are carried out to demonstrate the performance of LADS in comparison to the state-of-the-art. Furthermore, the authors discuss the proposed method's limitations on the 'natural' distribution shift. \n\n(Weaknesses)\n\n- The text embeddings of the domain descriptions guide the domain alignment loss. During training, the image embeddings space changes while the text embeddings space remains constant. It is suspected that the text embeddings function properly as guidance.\n- Furthermore, the method is heavily reliant on the domain descriptions used. Taking the first two rows of table 3, the results are affected by the prompts. Considering the variations of the 'direction' instead of fixing a 'global direction' may be helpful.\n- Though the ablation study investigated the role of each loss term, it is suggested that the role of $\\alpha$ be investigated as well.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper addresses a novel problem. The paper is simple to understand. The paper provides setting details that support reproducibility.\n\n# Summary Of The Review\n\nThe paper's contributions are novel. The technique is simple, but it works well. However, it is suggested that more research into domain alignment loss is required. Given the novelty of the paper, I would recommend a 'marginally above the acceptance threshold' rating.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nIMPROVING OUT-OF-DISTRIBUTION GENERALIZATION WITH INDIRECTION REPRESENTATIONS\n\nKha Pham 1, Hung Le 1, Man Ngo 2, Truyen Tran 1\n\n1 Applied Artificial Intelligence Institute, Deakin University 2 Faculty of Mathematics and Computer Science, VNUHCM-University of Science 1{phti, thai.le, truyen.tran}@deakin.edu.au, 2 nmman@hcmus.edu.vn,\n\nABSTRACT\n\nWe propose a generic module named Indirection Layer (InLay), which leverages indirection and data internal relationships to effectively construct symbolic indirect representations to improve out-of-distribution generalization capabilities of various neural architectures. InLay receives data input in the form of a sequence of objects, treats it as a complete weighted graph whose vertices are the objects and edge weights are scalars representing relationships between vertices. The input is first mapped via indirection to a symbolic graph with data-independent and trainable vertices. This symbolic graph is then propagated, resulting in new vertex features whose indirection will be used for prediction steps afterward. Theoretically, we show that the distances between indirection representations are bounded by the distances between corresponding graphs, implying that unseen samples with very different surface statistics can still be close in the representation space to the seen samples if they share similar internal relationships. We demonstrate that InLay is consistently effective in improving out-of-distribution generalization throughout a comprehensive suite of experiments, including IQ problems, distorted image classification, and few-shot domain adaptation NLP classification. We also conduct ablation studies to verify different design choices of InLay.\n\n1\n\nINTRODUCTION\n\nThere have been several evidences showing that deep learning models may fail drastically in out-ofdistribution (OOD) testing circumstances (Geirhos et al., 2018; Keysers et al., 2020). One reason widely agreed upon is that neural networks tend to learn surface statistics of data (Lake et al., 2017) and thus can not generalize to new samples with different statistics. On the other hand, humans excel at generalizing, and it has been long believed that the ability to think in a symbolic way is the key for humans to quickly adapt to new situations (Mitchell, 2021). A powerful concept that can bridge concrete data and symbols is indirection, which binds two objects together and uses one to refer to the other. In computer science, indirection is widely used via pointer: data is bound to its memory address, and programs use the memory address to refer to that data.\n\nThe capacity to draw analogies is yet another trait that facilitates human generalization. Several cognitive science theories have been proposed to explain analogy, and the Structure-Mapping Theory (SMT) (Gentner, 1983) is one of the most successful among them. SMT argues that not object attributes but the relationships between them are transferred in an analogy. For example, the hydrogen atom is analogous to the solar system not because they share the same sizes or temperatures but because they both have entities revolving around a center due to the attractive force. This suggests that internal relationships of a situation contain essential information for generalization. In this paper, we propose a method that simultaneously leverages indirection and data internal relationships to construct indirection representations, which can be interpreted as symbolic representations that respect the similarities between internal relationships. For instance, two IQ problems with similar hidden rules (i.e., similar internal relationships) should have similar indirection representations, though they contain completely different shapes or images.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Indirection Layer. Concrete data representation is viewed as a complete graph with weighted edges. The indirection operator maps this graph to a symbolic graph with the same weight edges, however the vertices are fixed and trainable. This symbolic graph is propagated and the updated node features are indirection representations. Different concrete inputs may share the same indirection representations if their corresponding graphs have the same adjacency matrices. This illustrates the core idea of InLay: constructing indirection representations by transferring internal relationships through indirection.\n\nTo this end, we implement our method in the form of a generic module named Indirection Layer (InLay), which can construct indirection representations from either encoded or raw low-sensory data and can be equipped with various models to improve their OOD generalization capabilities. InLay receives a sequence of objects as input and produces a sequence with the same length including associated indirection representations. The input sequence is viewed as a complete weighted graph where each edge weight represents the relationship between two corresponding objects, and thus the adjacency matrix of this graph captures the internal relationships of the input. The core operation of InLay consists of two steps: indirection and graph propagation (see Fig. 1 for illustration). The input is first processed through indirection to transfer all edge weights to another symbolic graph whose vertices are data-independent and trainable. This symbolic graph is then propagated, resulting in updated vertex features as the indirection representations of the input. These indirection representations are used as new representations for prediction steps afterward.\n\nWe show both theoretically and empirically that InLay can help to improve OOD generalization. Theoretically, we show that InLay indirection preserves internal structures of graphs, and the distances between indirection representations are bounded by the cut distances between corresponding graphs. Thanks to these theoretical properties, the indirection representation of a new data instance can be located near a seen one if they share similar internal relationships (although the surface features may be entirely different), thus the two instances have a higher chance of being interpreted similarly. Empirically, we show that InLay consistently helps different models to improve their OOD generalization capabilities in a comprehensive suite of experiments involving numerous datasets and OOD scenarios, including IQ problems with unseen objects and unseen rules, distorted image classification, and few-shot domain adaptation NLP classification. We also conduct ablation experiments to study the necessity of different design choices in InLay and provide practical analysis on the success of InLay.\n\n2 METHOD\n\nWe introduce our main contribution, namely the Indirection Layer (InLay). InLay takes a sequence of objects as input and transforms the sequence into a new indirect graph-structured representation. Concretely, let X = (x1, x2, . . . , xk)(cid:62) ∈ Rk×n be the input sequence for InLay, where k is the number of objects and each xi ∈ Rn represents an object. For example, an object may be either an image in IQ problems, or a patch of image in image classification task, or a paragraph in few-shot NLP classification task (see Section 4). To better exploit data internal relationships, we treat each input sequence as a directed complete weighted graph (with no self-loop) whose vertices represent the objects and edges represent relationships as scalars in [−1, 1]. Specifically, for each sequence X, we denote GX as its corresponding graph. We define Gk to be the space of all directed weighted complete graphs G with k vertices and edge weights in [−1, 1]. From now on, we will only write G instead of GX when it is not necessary to specify X, and we denote AG as the adjacency matrix of G. This adjacency matrix captures the internal relationships of the corresponding data sequence. Remark 2.1. (Canonical indexing assumption) As the set of graph vertices may permute, a graph G with k vertices may not have an unique adjacency matrix. To assure the well-definedness of AG, we assume that (when computing the adjacency matrix) the i-th vertex represents the i-th element of the input sequence. We show in Appendix C that the indirection representations are still maintained if the canonical indexing assumption is not obeyed.\n\n2\n\n~abcabcindirectionoperator~abcpropagationInLay indirectionrepresentationsymbolic graphdata graphconcrete representationPublished as a conference paper at ICLR 2023\n\nWe aim to learn suitable representations for the graph such that the internal relationships of the input sequence can be transferable to novel settings. To this end, we contribute the Indirection Layer (InLay), which leverages indirection and data internal relationships to construct indirection representations. InLay is a generic and flexible module that can be equipped into different models to construct indirection representations from either encoded data or raw low-sensory data (e.g, for the case of Vision Transformer; see Section 4.2) in two steps: indirection and graph propagation.\n\nij of AGX is computed as aX\n\nIndirection For each X, the adjacency matrix AGX ∈ Rk×k of GX represents the internal relationships between objects in X. Each component aX if i (cid:54)= j and ij = 0 if i = j, where · is the inner product and Q, K ∈ Rn×4n are trainable weights that project xi aX and xj onto a higher dimensional space so that a linear kernel may represent the relationship between xi and xj. The choice of tanh as a non-linear transformation is important: it maps the dot products to [−1, 1], allowing InLay to possess nice theoretical properties regarding boundedness of distances (see Section 3); and, tanh allows negative similarities between objects, which may help to represent opposite relations, e.g., translations to the left and to the right. See Section 4.1.1 and Appendix G for experimental details.\n\n(cid:16) Q(cid:62)xi·K(cid:62)xj\n\nij = tanh\n\n4n\n\n(cid:17)\n\n√\n\nIn indirection, each object is bound to a symbol. We denote by V ind = (cid:0)vind ∈ Rk×n to be the set of symbols where each vind k be the subset of Gk that consists of all graphs whose set of vertices is V ind (i.e., each vertex represents some vind k can be interpreted as the space of i\nsymbolic graphs with fixed vertices. We define the indirection operator I as follows.\n\ni ∈ Rn is data-independent and trainable. Let Gind\n\nand no two vertices represent the same vind\n\n). The space Gind\n\n2 , . . . , vind\n\n1 , vind\n\n(cid:1)(cid:62)\n\nk\n\ni\n\nDefinition 2.2. Given an input sequence X = (x1, x2, . . . , xk) and its corresponding graph GX ∈ Gk, the indirection operator I is a mapping from Gk to Gind that maps GX to I(GX ) so that AGX = AI(GX ) and the i-th vertex of I(GX ) represents vind Remark 2.3. Definition 2.2 is introduced in the case when the canonical indexing assumption (see Remark 2.1) is obeyed. The vertex order emerges when computing the adjacency matrix. A more general definition is given in Appendix C.\n\nk .\n\ni\n\nThe indirection operator I maps each object xi to its associated symbol vind pairwise relationship between vind I ignores the concrete features of objects but still maintains the relationships between them.\n\ni while assuming the is the same as one between xi and xj (see Fig. 1). That is,\n\nand vind\n\nj\n\ni\n\nGraph propagation\n\nAfter indirection, each data graph G is mapped to a symbolic graph I(G). This operation can be interpreted as follows: at first, edge weights of I(G) are unspecified; then the indirection operator I assigns edge weights from the data to I(G). Once receiving this information from data, I(G) is propagated and the updated vertex features are indirection representations of the input sequence. Formally, for an input sequence X, if we denote rX to be the indirection representations of X, then rX = AGX V ind. This symbolic rX is used as a new representation for X for prediction steps afterward.\n\nTo summarize, for each input sequence X, InLay constructs associated indirection representation rX :\n\nrX = tanh\n\n(cid:18) XQ(XK)(cid:62) √\n\n4n\n\n(cid:19)\n\nV ind.\n\n(1)\n\nThis equation is closely related to self-attention, except for three points: 1. data are projected onto a higher dimensional space by matrix multiplying with Q and K; 2. the softmax operator is replaced by tanh; and most importantly, 3. the value V ind is not computed based on the data X. While the first two differences empirically enhance InLay’s performances (see Section 4.4), the third one stands for the core idea of indirection in InLay. An ablation study on V ind will also be conducted in Section 4.4 to demonstrate the role of each element.\n\nInitialization of V ind may greatly affect the overall performance. To reduce this effect, we replace V ind in Eq. (1) by ψ(V ind), where ψ : Rn → Rn is a trainable 2-layer neural network applied to rows of V ind. We also use multi-heads to compute the adjacency matrix so that local information of feature vectors is better utilized. The number of heads is tuned for each specific task.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3\n\nTHEORETICAL ANALYSIS\n\n3.1 BOUNDEDNESS WITH RESPECT TO THE CUT DISTANCE\n\nGraph spectrum and Laplacian are important graph characteristics that can be computed entirely by graph adjacency matrices. From Definition 2.2, it follows that the indirection operator I preserves graph spectrum and Laplacian, which means I preserves graph internal structure. To some extent, this agrees with the Structure-Mapping Theory (Gentner, 1983), which states that not the attributes but the internal relationships are transferred in an analogy. In other words, learning internal relationships may already be enough to capture the essence of a situation. Further details for the Structure-Mapping Theory will be given in Section 5.\n\nNext, we investigate how distances between graphs may constrain distances between indirection representations. Before defining graph distance, we define isomorphism between graphs in Gk. Definition 3.1. Given two graphs G = (V, E) ∈ Gk and G(cid:48) = (V (cid:48), E(cid:48)) ∈ Gk with associated ij)i,j=1,k. We say G and G(cid:48) are isomorphic, adjacency matrices AG = (aij)i,j=1,k and AG(cid:48) = (a(cid:48) denote by G ∼= G(cid:48), if there exists a bijection φ : V → V (cid:48) so that aij = a(cid:48) φ(i)φ(j) for every i, j ∈ V .\n\nTwo isomorphic graphs can be interpreted as being identical up to isomorphism, and thus a graph distance defined on Gk should respect this property, i.e., the distance between two isomorphic graphs is 0. One such distance is the cut distance ˆδ(cid:3) (Borgs et al., 2008), which is a useful tool to compare similarities betwen structures (Liu et al., 2018), and also for studying the convergence of sequence of graphs (Borgs et al., 2008). A formal definition for ˆδ(cid:3) is given in Appendix A.\n\nIt follows from the definition of ˆδ(cid:3) that ˆδ(cid:3)(G, G(cid:48)) = 0 if and only if G is isomorphic with G(cid:48). ∼= G2 then ˆδ(cid:3)(G1, G(cid:48)) = ˆδ(cid:3)(G2, G(cid:48)) for any G(cid:48). Since G ∼= I(G), the indirection Moreover, if G1 operator I preserves ˆδ(cid:3) distance, i.e., ˆδ(cid:3)(G, G(cid:48)) = ˆδ(cid:3)(I(G), I(G(cid:48))) for every G, G(cid:48) ∈ Gk.\n\nWe have shown that the indirection operator I admits invariant properties with respect to the graph spectrum, Laplacian and the cut graph distance ˆδ(cid:3). The following result shows that the distances between indirection representations are bounded by cut distances between corresponding graphs. For each G ∈ Gk, we denote rG = AGV ind to be its associated indirection representation. Theorem 3.2. For any two graphs G ∈ Gk and G(cid:48) ∈ Gk, the following inequality holds: (cid:16)\n\n(cid:17)\n\n(cid:107)rG − rG(cid:48)(cid:107)∞ ≤ k\n\n2 + k2ˆδ(cid:3)(G, G(cid:48))\n\n(cid:107)V ind(cid:107)∞,\n\n(2)\n\nwhere (cid:107).(cid:107)∞ is the matrix infinity norm (see Definition A.3 in Appendix A).\n\nProof. See Appendix B.\n\nNote that even when ˆδ(cid:3)(G, G(cid:48)) = 0, rG may still be different from rG(cid:48). This is because by design, InLay also takes into account the ordinal information of input sequence, which may be important in some specific use cases, e.g., when the input is sequence of image patches. Theorem 3.2 shows that if G and G(cid:48) are close, their indirection representations will not be far away from each other as well. This is an important property since the original vertex representations of G and G(cid:48) may be arbitrarily far though G and G(cid:48) are isomorphic, e.g., two IQ problems with the same hidden rules but different images may be represented very differently. Theorem 3.2 also shows the necessity of training V ind to obtain appropriate (cid:107)V ind(cid:107)∞: if (cid:107)V ind(cid:107)∞ is too large, the bound in Ineq. (2) is loose; conversely, if (cid:107)V ind(cid:107)∞ is too small, the bound may be too strict so that indirection representations are not well separated enough. An empirical ablation study on V ind will be given in Section 4.4.\n\n3.2 CONNECTION BETWEEN INLAY AND STRUCTURAL ANALOGY\n\nCurrent machine learning methods follow the manifold hypothesis and tend to interpolate on the learned manifold during testing. This ability of interpolation is usually referred as making value analogies, i.e., making analogies between data features. However, value analogy may not be enough in more extreme generalization cases when surface statistics of testing samples vastly differ from that of training data. Structural analogy is believed to be necessary for ML models to reach higher levels of generalization (Chollet, 2021). By making structural analogy, concrete information is partly ignored while structural information is compared, e.g., two IQ problems with the same hidden rules\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n(a) Overall architecture of prediction model when equipped with InLay. (b) Left: A problem in Figure 2: FINE dataset with images from the Omniglot dataset and hidden rule is 90-degree rotation. Right: A Raven’s Progressive Matrix problem from RAVEN dataset. Images adapted from original papers.\n\nare structurally analogous even though the data (e.g., images) are entirely different between the problems. In InLay, the structural information is maintained in the form of adjacency matrices, which are computed based on data features. To some extent, InLay can be interpreted as a hybrid method of value analogy and structural analogy.\n\nWhen it comes to structural analogy, one might need a metric to measure the similarities between structures. Among different metrics, the cut distance ˆδ(cid:3) is one of the few methods able to compare directed weighted graphs (Tantardini et al., 2019). For instance, a recent work by Liu et al. (2018) leverages the cut distance to compare complex networks, including artificial networks and real networks of chemical molecules. Theorem 3.2 draws a connection between InLay and the cut distance by showing that the distances between indirection representations are bounded by corresponding cut distances, and thus emphasizes the structural inductive bias InLay brings into deep learning models.\n\n4\n\nEXPERIMENTS\n\nIn this section, we conduct several experiments with different scenarios of OOD generalization to show that InLay can adapt to various models and improve their performances on various datasets. The OOD testing scenarios include IQ problems with unseen images and unseen rules, distorted image classification, and domain adaptation on few-shot NLP classification, all of which require the ability to understand the problem in a systematic and symbolic way in order to generalize on new OOD circumstances. Throughout these experiments, we show that InLay consistently helps models to perform better. We also provide an ablation study on the necessities of different design choices in InLay, as well as a practical analysis of the success of InLay.\n\nInLay constructs indirection representations from either encoded or raw data. If the prediction model is equipped with an encoder, InLay will sit between the encoder and prediction to transform encoded representations to indirection representations (see Fig. 2a for an illustration). When there is no encoder, e.g., when the prediction model is Vision Transformer, InLay directly constructs indirection representations from raw data. To be fair when comparing, models with or without InLay are all trained with the same training settings, including batch size, learning rate, number of training iterations, optimizer, etc., and we only report test results after the last iteration. Average results are reported in the main text; full results with standard deviation are given in Appendix D. More training details are also given in Appendix K.\n\nIn practice, we optionally use context normalization (Webb et al., 2020a) to further improve InLay. If context normalization is applied in InLay, there will be two such layers: one to normalize the original representation X, and one to normalize the symbolic representation rX .\n\n4.1 OUT-OF-DISTRIBUTION IQ PROBLEMS\n\nIQ problems are powerful testbeds for OOD generalization capability of deep learning models. Despite their simple appearances, IQ problems are challenging in the sense that they require models to understand the hidden rules instead of just surface features to solve new problems with unseen objects or even unseen (but related) rules. There have been evidences showing that current deep learning models may fail when facing problems with unseen objects (Webb et al., 2020b). In this experiment, we show that models coupled with InLay achieve better performances on two IQ datasets: FINE (Pham et al., 2022) and RAVEN (Zhang et al., 2019). Examples are given in Fig. 2b.\n\n5\n\n?ABCDRotationAbstractogresshasbeenwitnessedinbasicvisionow-levelperception,suchasobjectrecog-,andtracking.Unfortunately,thereisstillrformancegapbetweenartifcialvisionsys-nintelligenceintermsofhigher-levelvi-speciallyonesinvolvingreasoning.Earlierppingmachineswithhigh-levelreasoningroundVisualQuestionAnswering(VQA),?Problem Matrix(a)(b)(c)Centerpgpytemsandhumanintelligenceintermsofhigher-levelvi-sionproblems,especiallyonesinvolvingreasoning.Earlierattemptsinequippingmachineswithhigh-levelreasoninghavehoveredaroundVisualQuestionAnswering(VQA),onetypicaltaskassociatingvisionandlanguageunder-standing.Inthiswork,weproposeanewdataset,builtinthecontextofRaven’sProgressiveMatrices(RPM)andaimedatliftingmachineintelligencebyassociatingvisionwithstructural,relational,andanalogicalreasoninginahierar-chicalrepresentation.UnlikepreviousworksinmeasuringabstractreasoningusingRPM,weestablishasemanticlinkbetweenvisionandreasoningbyprovidingstructurerepre-?Answer SetP12345678Figure1.(a)AnexampleRPM.Othatbestcompletestheproblemmandanalogicalrelations.EachimagProblemAnswer Set(a)(b)Raw dataEncoderEncoded representationsInLayIndirection representationsPrediction modelPredictionPublished as a conference paper at ICLR 2023\n\nTrain set Test set\n\nNTM PrediNet RelationNet Transformer\n\nCIFAR100 Omniglot\n\nMNIST CIFAR100\n\nRot.\n\nScale\n\nShear\n\nTrans. 25.5/56.9 27.1/31.4 29.8/58.5 30.3/61.0 35.2/69.2 30.0/34.5 29.6/56.0 31.9/48.7 26.7/28.5 25.5/28.3 25.6/29.0 26.4/29.9 27.5/31.6 26.8/27.8 26.4/28.4 33.6/35.9 25.4/39.3 25.0/34.0 25.1/39.8 24.7/50.2 25.6/45.5 25.6/35.2 25.0/43.4 26.8/44.8 26.9/63.3 27.2/40.4 28.3/65.3 30.6/62.0 37.8/63.9 32.0/36.1 30.2/53.5 29.2/48.1\n\nTrans.\n\nShear\n\nScale\n\nRot.\n\nTable 1: Average test accuracy (%) without/with InLay on FINE dataset.\n\nTrain configuration Up-Down Left-Right Test configuration 28.8/43.9 LSTM 15.2/56.7 Transformer 12.7/58.5 RelationNet 13.7/16.5 PrediNet\n\n3x3Grid 2x2Grid 19.5/25.1 13.6/26.0 12.4/28.9 13.6/14.3\n\nOut-InGrid Out-InCenter 42.1/48.6 16.6/44.7 12.3/51.7 14.2/15.9\n\nAverage\n\n30.1/39.2 15.1/42.5 12.5/46.4 13.8/15.6\n\nTable 2: Average test accuracy (%) without/with InLay on RAVEN dataset.\n\n4.1.1\n\nFINE DATASET\n\nFINE dataset consists of IQ problems with geometric transformations as hidden rules. To succeed in this dataset, models should treat objects as symbols and learn the relationship between these symbols. Here we consider one of the most challenging OOD scenarios: test problems include unseen objects and unseen rules. To be specific, images in train and test problems come from different datasets (train on CIFAR100 (Krizhevsky, 2009) - test on Omniglot (Lake et al., 2015) or train on MNIST (LeCun et al., 2010) - test on CIFAR100), so the models need to understand the hidden rules instead of image features to solve test problems. Rules in test problems are also unseen during training; for instance, we train on IQ problems with rotation angle less than 180◦ and test on ones with rotation angles more than 180◦; similarly, for translation, we train on problems with translation to the left and test on ones with translation to the right. More details of other transformations will be given in Appendix. Models to be considered include the Neural Turing Machine (Graves et al., 2014), PrediNet (Shanahan et al., 2020), Relation Network (Sung et al., 2018), and Transformer (Vaswani et al., 2017), all of which have been shown to be effective on different relational reasoning tasks. All models, with or without InLay, are equipped with context normalization. Results are reported in Table 1.\n\nOverall, models perform better when equipped with InLay, even in very extreme cases when models are trained on grayscale MNIST images and tested on RGB CIFAR100 images. This shows a clear advantage of InLay: indirection maps concrete features to symbolic space spanned by V ind, thus helping models to be less dependent on concrete data. It can also be observed that PrediNet equipped with InLay improves less than other models. This is because PrediNet also constructs new symbolic data representations, and thus contains the inductive bias of symbolic representations itself. Again, this emphasizes the necessity of including symbolic inductive bias to improve OOD generalization.\n\n4.1.2 RAVEN DATASET\n\nRAVEN dataset (Zhang et al., 2019) is inspired by Raven’s Progressive Matrices, which are challenging IQ tests for humans. RAVEN IQ problems are complex in the sense that a single problem may consist of different shapes, each of which follows a different rule. Inspired by the original paper, we conduct experiments to test models’ capabilities to generalize on problems of unseen configurations. Specifically, we train and test on problems of different but related configurations. For example, Up-Down problems are related to Left-Right ones in the sense that Left-Right configuration can be viewed as a 90-degree “rotation” of Up-Down. In this experiment, the three train-test configuration pairs are UpDown-LeftRight, 3x3Grid-2x2Grid and OutInGrid-OutInCenter. We also apply context normalization in all models. We further apply Dynamic Residual Tree as proposed in the original paper to capture the inherent structure of Raven’s Progressive Matrices. Results are reported in Table 2. A similar pattern can be observed: models equipped with InLay tend to perform better. In average, InLay helps improve LSTM by 9.1%, Transformer by 27.4%, RelationNet by 33.9%, and PrediNet by 1.8%. This can be well explained by Theorem 3.2 and Theorem B.1 in the theoretical analysis: although the test configuration is unseen, it is still related to the train configuration and thus InLay representations for test problems may still be close to ones of train problems. This helps models to have a better chance to draw analogies between observed and unobserved configurations.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nDataset\n\nSVHN CIFAR10 CIFAR100\n\nNo data augmentation\n\nWith data augmentation\n\nRot90 Grayscale Jitter 11.8/12.5 86.9/91.8 87.2/91.6 62.0/65.3 25.7/27.9 48.3/62.7 46.6/59.7 40.2/50.1 13.4/15.0 16.4/23.9 18.1/25.0 16.0/21.3\n\nAvg.\n\nRot90 Grayscale Jitter 17.3/19.1 92.9/94.7 91.8/93.4 67.3/69.1 24.2/27.3 54.2/69.2 50.2/68.4 42.2/55.0 11.1/14.0 18.5/27.8 19.4/28.7 16.3/23.5\n\nAvg.\n\nTable 3: Average test accuracy (%) without/with InLay on OOD classification task with ViT.\n\n5-way-1-shot 10-way-1-shot Average\n\nProtoNet 64.7/65.4 49.1/41.7 56.9/53.6\n\nSNAIL 38.6/60.6 17.3/34.9 28.0/47.8\n\nGNN 36.7/63.1 27.1/43.3 31.9/53.2\n\nMTB 66.1/68.9 52.9/54.1 59.5/61.5\n\nTable 4: Average validation accuracy (%) without/with InLay on FewRel 2.0.\n\n4.2 OUT-OF-DISTRIBUTION CLASSIFICATION\n\nHumans can consistently recognize objects in different positions, angles, or colors. Current deep learning models may not. Geirhos et al. (2018) show that when test images are injected with different kinds of distortions other than ones in training, deep neural networks may fail drastically on image classification tasks. We take inspiration from that result and conduct similar experiments to test whether InLay can help models improve their performances on OOD image classification tasks. We use a vanilla 6-layer Vision Transformer (ViT) (Dosovitskiy et al., 2020) as the base model and test it, with or without InLay, on different datasets, namely the SVHN (Netzer et al., 2011) and CIFAR10&100 (Krizhevsky, 2009). The models are trained on original images in two cases: with and without data augmentation, and tested on images with various distortions, including image transformation (90-degree rotation) and color transformations (color jitter, grayscale). In the case of data augmentation, we use all other distortions for augmentation except one used for testing. If InLay is equipped, we divide each 32 × 32 image into overlapping patches of size 8 × 8 and stride 4. These patches are vertices of the graph that represents the current image. The patch indirection representations are reassembled to form a new image of the same size as the original one. This new image is then fed into ViT. Context normalization is not used in this task.\n\nResults are shown in Table 3. As expected, ViT performs poorly when the images are distorted. In average, InLay helps to improve its performance by 3.3% on SVHN, 0.9% on CIFAR10, and 5.3% on CIFAR100 when there is no data augmentation and 1.8% on SVHN, 12.8% on CIFAR10, and 7.2% on CIFAR100 when data augmentation is included. We can also observe that 1. ViT with data augmentation but without InLay is still mostly worse than ViT with InLay but without data augmentation; and 2. InLay helps improve ViT in both cases of with and without data augmentation. Note that it should not be interpreted that adding InLay to ViT is equivalent to adding a Transformer layer. Empirically, performances of 7-layer ViT only slightly differ from 6-layer ViT (see Appendix E), while it is clear that adding InLay may boost performances significantly.\n\nFEW-SHOT NLP DOMAIN ADAPTATION\n\n4.3 Humans can handle NLP classification tasks given a small number of examples. While humans can quickly adapt to such new scenarios, deep language models may not. Gao et al. (2019) proposed the FewRel 2.0 dataset that consists of few-shot NLP classification tasks, where the domains of train and test tasks vastly differ. Specifically, the training texts are taken from the Wikipedia corpus, while texts for testing originate from the PubMed and UMLS databases that contain large amounts of biomedical literature and sciences. This creates a big obstacle for few-shot language models to adapt to: their performances drop drastically as reported in the original paper.\n\nInspired by this result, we conduct an experiment on FewRel 2.0 dataset to show that InLay works well for language models. Different few-shot models, including Prototypical Network (Snell et al., 2017), SNAIL (Mishra et al., 2017), Graph Neural Network (Garcia and Bruna, 2017), and MTB (Soares et al., 2019), are trained with BERT encoder (Devlin et al., 2018) on 5-way-1-shot and 10-way-1-shot tasks. All models are equipped with context normalization. Since the test set is not provided for the public, we only report the test results on validation set, which shares the same domain as the test set. Results are shown in Table 4. Except ProtoNet, InLay helps other models improve: 19.8% for SNAIL, 21.3% for GNN, and 2.0% for MTB in average. The case of ProtoNet can be explained as follows: ProtoNet depends the distances between data instances, which is similar to the spirit of our InLay. Because ProtoNet already has this inductive bias, InLay can not help to improve it.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: (a) Ablation study. Test accuracies of ViT equipped with InLay on grayscale images when a design choice of InLay is replaced or removed. (b) Relative distances between original and distorted representations with and without InLay in different testing cases.\n\n4.4 ABLATION AND ANALYSIS\n\n4.4.1 ABLATION ON INLAY DESIGN CHOICES We conduct ablation experiments to study the necessities of different design choices of InLay. All experiments are conducted on OOD classification task (see Section 4.2) with ViT and grayscale testing images. In each experiment, we modify one design choice and keep others fixed. We consider three main design choices: activation function to compute adjacency matrices, projection on higher space to compute dot products, and trainability and data-independence of V ind. We also consider the case when the indirection representations are treated as relative positional encoding to be added to the original input. Results are reported in Fig. 3a.\n\nIn the ablation for activation function, replacing tanh with softmax significantly decreases the performance, while the result when no activation is applied is only slightly lower. This is because softmax does not allow negative values; moreover, it imposes the constraint of summing-to-one on edges of graph, which is unnecessary in the theoretical analysis. On the other side, projecting data onto higher dimensional spaces also plays a vital role in InLay as it helps linearize the relations between objects so that dot products may manage to represent those relations, and not doing so may lead to a drastic drop in performance. Maintaining a trainable set of symbols V ind is beneficial for InLay, and the performances with randomly sampled V ind from Gaussian tend to decrease when the standard deviations of the Gaussians increase. This can be explained by Theorem 3.2: increase of standard deviations leads to bigger (cid:107)V ind(cid:107)∞, which loosens the bound in Ineq. (2). Keeping V ind data-independent is also important, and treating the indirection representations as relative positional encoding is not efficient.\n\n4.4.2 FURTHER PRACTICAL ANALYSIS Besides theoretical analysis in Section 3, we further provide practical evidence showing why InLay may help models to generalize better. We again use the OOD classification tasks as a testbed. In short, we would like to show that InLay reduces the distance between an image and its distorted version, thus models may recognize the similarity between the two images more easily. Using the absolute distance may not be a fair metric since scaling two vectors by the same factor may already reduce the distance between them. Instead, we compute the relative distances between vectors, i.e. the relative distance between u and v is 2 × (cid:107)u−v(cid:107)∞ . Relative distances between images and their distorted versions are computed in two cases: with InLay and without InLay. Results are shown in Fig. 3b, and it is clear that the relative distances in InLay case are lower than those without InLay. This can be partly explained by Theorem 3.2: the original image corresponds with G, and the distorted image corresponds with G(cid:48). Since the distorted image is closely related to the original one, the distance between their corresponding graphs is small, and thus the distance between their indirection representations rG and rG(cid:48) also tends to be small according to Ineq. (2).\n\n(cid:107)u(cid:107)∞+(cid:107)v(cid:107)∞\n\n5 RELATED WORK Systematic generalization has attracted attention recently in the deep neural networks community. One approach is to train a mixture of experts as functional modules, and these experts either compete (Parascandolo et al., 2018) or are composed by attention mechanism (Rahaman et al., 2021) to solve a task. Fedus et al. (2021) proposed the Switch Transformer to simplify routing algorithms in mixture-of-experts models to reduce communication and computational costs. Another approach is\n\n8\n\nactivationSVHNprojecthighernon-trainable VindCIFAR10CIFAR100(a)(b)data-dependentvaluerelativepos. enc.Published as a conference paper at ICLR 2023\n\nto design architectures that mimic human’s ability to think and reason sequentially. One well-known early model following this approach is the Module Network (Andreas et al., 2016) which attacks the image-QA tasks by parsing the query into sequential sub-queries, each of which is solved by a module in the form of neural networks. The MAC recurrent network (Hudson and Manning, 2018) and the Neural State Machine (Hudson and Manning, 2019) follow a similar idea, however, in MAC the query is explicitly and expressively decomposed by a sequence of RNN-type MAC cells, while Neural State Machine relies on probabilistic graphs representing underlying semantics to reason sequentially. Recently, Wei et al. (2022) proposed the idea of chain of thought to improve the ability of large language models to perform complex reasoning. Our InLay also follows the idea of injecting symbolic inductive bias, however, we focus on representations instead of functional modules. Models equipped with InLay can be interpreted as 2-step reasoning: the low-level sensory data is first represented symbolically by InLay, then processed by the following models.\n\nIndirection is one of the most useful ideas that has been long applied in different areas of computer science. One of the most illustrative examples for indirection is the concept of pointer. Recently, there have been works leveraging indirection to improve generalization capabilities of deep learning models. ESBN (Webb et al., 2020b) uses an RNN controller to sequentially produce a symbolic key for each object and reasons on keys only. The keys in ESBN are computed based on the controller and similarities between objects, which is similar to our InLay; however, the keys are produced sequentially, which may increase the computational cost. Recently, Pham et al. (2022) proposed FINE, which is a fast-weight approach that utilizes indirection on functional spaces and has achieved promising performances on different OOD testing scenarios of IQ problems.\n\nThe idea of transferring relationships in InLay is inspired from the Structure Mapping Theory (SMT) (Gentner, 1983), which is a revolutionary theory of analogy in cognitive science. Analogy is a vital concept to explain human cognition, and it has been long argued that analogy-making underlies humans’ ability to flexibly adapt to new situations (Gentner et al., 2001). Before SMT, it had been assumed that in a strong analogy, the base and the target should share several attributes in common (Tversky, 1977). SMT, in contrast, argues that not attributes but the relationships between objects are transferred in an analogy; in other words, the essence of a situation lies in internal relationships instead of concrete attributes. From this point of view, the theoretical results in Section 3 can be interpreted as a justification for SMT in the case of InLay: transferring relationships only does not lose significant information such as graph characteristics and graph topology.\n\nIt is also worth noting that from few-shot learning perspective, our InLay can be categorized as fast-weight (Malsburg, 1994) embedding learning model, in which the attentional weight AGX is computed on-the-fly and the indirection representation rX is computed accordingly.\n\nThe trainable set of symbols V ind can be interpreted as positional encodings, and the output rX of InLay is a relative positional encoding regarding the input X. The idea of relative positional encoding (Shaw et al., 2018), as a replacement for the absolute positional encoding in Transformer, has been widely investigated and several variants have been proposed (Dai et al., 2019; Huang et al., 2018). It is worth noting that the relative positional encoding is added to the original input, while rX in InLay plays the role of the input for the following model. InLay thus should not be considered as a variant of a Transformer layer or relative positional encoding; instead, its design highlights the idea of indirection and symbolic representations. 6 CONCLUSION In this paper, we propose InLay as a separate module that can be plugged into different models to improve OOD generalization. InLay leverages the idea of indirection to redirect data representation based on a trainable set of symbols. Viewing each data point as a complete weighted graph, we prove theoretically that InLay preserves graph internal structure and graph topology, and the distances between refined representations are bounded by the distances between corresponding graphs. We show the effectiveness of InLay through a comprehensive suite of experiments, including different OOD testing scenarios on IQ problems, distorted image classification, and few-shot NLP domain adaptation classification tasks. We also conduct ablation experiments to study necessities of different design choices in InLay, as well as further practical analysis on the success of InLay.\n\nInLay opens up several future directions. From the theoretical side, it is worth investigating how the manifold containing original data representations is transformed during InLay, and why this manifold transformation can help generalization. From the practical view, stacking multiple InLay’s to form a hierarchical indirection network is a promising idea.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 39–48, 2016.\n\nChristian Borgs, Jennifer T Chayes, László Lovász, Vera T Sós, and Katalin Vesztergombi. Convergent sequences of dense graphs i: Subgraph frequencies, metric properties and testing. Advances in Mathematics, 219(6):1801–1851, 2008.\n\nFrancois Chollet. Deep learning with Python. Simon and Schuster, 2021.\n\nTaco Cohen and Max Welling. Group equivariant convolutional networks. In International conference\n\non machine learning, pages 2990–2999. PMLR, 2016.\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\n\nmodels with simple and efficient sparsity, 2021.\n\nTianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Fewrel 2.0: Towards more challenging few-shot relation classification. arXiv preprint arXiv:1910.07124, 2019.\n\nVictor Garcia and Joan Bruna. Few-shot learning with graph neural networks. arXiv preprint\n\narXiv:1711.04043, 2017.\n\nRobert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. Advances in neural information processing systems, 31, 2018.\n\nDedre Gentner. Structure-mapping: A theoretical framework for analogy. Cognitive science, 7(2):\n\n155–170, 1983.\n\nDedre Gentner, Keith J Holyoak, and Boicho N Kokinov. The analogical mind: Perspectives from\n\ncognitive science. MIT press, 2001.\n\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. ArXiv, abs/1410.5401, 2014.\n\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne, AM Dai, MD Hoffman, and D Eck. Music transformer: Generating music with long-term structure (2018). arXiv preprint arXiv:1809.04281, 2018.\n\nDrew A. Hudson and Christopher D. Manning. Compositional attention networks for machine\n\nreasoning. ICLR, 2018.\n\nDrew A. Hudson and Christopher D. Manning. Learning by abstraction: The neural state machine.\n\nIn NeurIPS, 2019.\n\nDaniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, et al. Measuring compositional generalization: A comprehensive method on realistic data. International Conference on Learning Representations, 2020.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nA. Krizhevsky. Learning multiple layers of features from tiny images. 2009.\n\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning\n\nthrough probabilistic program induction. Science, 350(6266):1332–1338, 2015.\n\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building\n\nmachines that learn and think like people. Behavioral and brain sciences, 40, 2017.\n\nYann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].\n\nAvailable: http://yann.lecun.com/exdb/mnist, 2, 2010.\n\nQun Liu, Zhishan Dong, and En Wang. Cut based method for comparing complex networks. Scientific\n\nreports, 8(1):1–11, 2018.\n\nChristoph von der Malsburg. The correlation theory of brain function. In Models of neural networks,\n\npages 95–119. Springer, 1994.\n\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-\n\nlearner. arXiv preprint arXiv:1707.03141, 2017.\n\nMelanie Mitchell. Abstraction and analogy-making in artificial intelligence. Annals of the New York\n\nAcademy of Sciences, 1505(1):79–101, 2021.\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading\n\ndigits in natural images with unsupervised feature learning. 2011.\n\nGiambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard Schölkopf. Learning independent causal mechanisms. In International Conference on Machine Learning, pages 4036– 4044. PMLR, 2018.\n\nKha Pham, Hung Le, Man Ngo, and Truyen Tran. Functional indirection neural estimator for better\n\nout-of-distribution generalization. NeurIPS, 2022.\n\nNasim Rahaman, Muhammad Waleed Gondal, Shruti Joshi, Peter Gehler, Yoshua Bengio, Francesco Locatello, and Bernhard Schölkopf. Dynamic inference with neural interpreters. Advances in Neural Information Processing Systems, 34:10985–10998, 2021.\n\nMurray Shanahan, Kyriacos Nikiforou, Antonia Creswell, Christos Kaplanis, David Barrett, and Marta Garnelo. An explicitly relational neural network architecture. In International Conference on Machine Learning, pages 8593–8603. PMLR, 2020.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.\n\narXiv preprint arXiv:1803.02155, 2018.\n\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances\n\nin neural information processing systems, 30, 2017.\n\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the blanks:\n\nDistributional similarity for relation learning. arXiv preprint arXiv:1906.03158, 2019.\n\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. In Proceedings of the IEEE\n\nLearning to compare: Relation network for few-shot learning. conference on computer vision and pattern recognition, pages 1199–1208, 2018.\n\nMattia Tantardini, Francesca Ieva, Lucia Tajoli, and Carlo Piccardi. Comparing methods for compar-\n\ning networks. Scientific reports, 9(1):1–19, 2019.\n\nAmos Tversky. Features of similarity. Psychological review, 84(4):327, 1977.\n\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, abs/1706.03762, 2017.\n\nTaylor Webb, Zachary Dulberg, Steven Frankland, Alexander Petrov, Randall O’Reilly, and Jonathan In International conference on\n\nCohen. Learning representations that support extrapolation. machine learning, pages 10136–10146. PMLR, 2020a.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nTaylor Whittington Webb, Ishan Sinha, and Jonathan Cohen. Emergent symbols through binding in\n\nexternal memory. In International Conference on Learning Representations, 2020b.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n\nChi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational and analogical visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA DEFINITION OF ˆδ(cid:3) AND (cid:107).(cid:107)∞\n\nWe first define the cut distance d(cid:3) between graphs with the same set of vertices. Definition A.1. (Borgs et al., 2008) Given two graphs G = (V, E) ∈ Gk and G(cid:48) = (V, E(cid:48)) ∈ Gk with associated adjacency matrices AG = (aij)i,j=1,k and AG(cid:48) = (a(cid:48) ij)i,j=1,k. The cut distance d(cid:3) between G and G(cid:48) is\n\nd(cid:3)(G, G(cid:48)) = max\n\nS,T ⊂V\n\n1\n\nk2 |eG(S, T ) − eG(cid:48)(S, T )|,\n\n(3)\n\nwhere\n\neG(S, T ) =\n\n(cid:88)\n\ni∈S,j∈T\n\naij\n\nand\n\neG(cid:48)(S, T ) =\n\n(cid:88)\n\na(cid:48)\n\nij.\n\ni∈S,j∈T\n\nThe distance d(cid:3) can also be interpreted as the distance between the internal relationships (i.e., the adjacency matrices) of two graphs. However, one drawback of d(cid:3) is that it is not invariant under isomorphism. The generalized cut distance ˆδ(cid:3) is proposed to overcome this drawback. Definition A.2. (Borgs et al., 2008) Given two graphs G = (V, E) ∈ Gk and G(cid:48) = (V, E(cid:48)) ∈ Gk with associated adjacency matrices AG = (aij)i,j=1,k and AG(cid:48) = (a(cid:48) ij)i,j=1,k. The generalized cut distance ˆδ(cid:3) between G and G(cid:48) is computed as ˆδ(cid:3)(G, G(cid:48)) = min ̃G∼=G d(cid:3)( ̃G, G(cid:48)), where ̃G shares the same set of vertices with G(cid:48).\n\nNext, we define the matrix infinity norm (cid:107).(cid:107)∞induced from the vector max norm. Definition A.3. For a given matrix A = (aij)i=1,k,j=1,n, its infinity norm is computed as (cid:107)A(cid:107)∞ =\n\nmax 1≤i≤k\n\nn (cid:88)\n\nj=1\n\n|aij|. In words, the matrix infinity norm is the max row sum.\n\nProposition A.4. The matrix infinity norm is sub-multiplicative, i.e., (cid:107)AB(cid:107)∞ ≤ (cid:107)A(cid:107)∞(cid:107)B(cid:107)∞.\n\nB PROOF OF THEOREM 3.2 AND MORE THEORETICAL RESULTS\n\nIn this section, we provide proofs for theoretical results in the main text. An illustration of theoretical results are given in Fig. 4.\n\nk\n\nProof of Theorem 3.2. Denote ε = ˆδ(cid:3)(G, G(cid:48)). Since G ∼= I(G) and G(cid:48) ∼= I(G(cid:48)), it folows that ˆδ(cid:3)(I(G), I(G(cid:48))) = ˆδ(cid:3)(G, G(cid:48)) = ε. From the definition of ˆδ(cid:3) (Definition A.2), there exists so that Gind ∼= I(G) and d(cid:3)(Gind, I(G(cid:48))) = ε. Denote E = AGind − AI(G(cid:48)), where Gind ∈ Gind AGind and AI(G(cid:48)) are the adjacency matrices of Gind and I(G(cid:48)) respectively, it follows from the definition of d(cid:3) that (cid:107)E(cid:107)∞ ≤ k3ε (since the absolute value of each element of E is less than k2ε due to Definition A.1 of d(cid:3), and the infinity matrix norm is the max row sum where each row has k elements). Finally, note that AI(G(cid:48)) = AG(cid:48), we have\n\n(cid:107)rG − rG(cid:48)(cid:107)∞ = (cid:107)AGV ind − AG(cid:48)V ind(cid:107)∞\n\n≤ (cid:107)AG − AGind + E(cid:107)∞(cid:107)V ind(cid:107)∞ = (cid:107)AI(G) − AGind + E(cid:107)∞(cid:107)V ind(cid:107)∞ ≤ ((cid:107)AG(cid:107)∞ + (cid:107)AGind (cid:107)∞ + (cid:107)E(cid:107)∞) (cid:107)V ind(cid:107)∞ ≤ (2k + k3ε)(cid:107)V ind(cid:107)∞ = k(2 + k2ε)(cid:107)V ind(cid:107)∞.\n\n(cid:3)\n\nTheorem 3.2 focuses on the distance between two single indirection representations. Now we move our attention to the distance between sets of indirection representations associated with isomorphic classes of graphs. For each G ∈ Gk, denote RG = {r ̃G : ̃G ∈ Gk, ̃G ∼= G}. According to\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Illustration of Theorem 3.2 and Theorem B.1.\n\nTheorem 3.2, diam RG ≤ 2(cid:107)V ind(cid:107)∞ for every G ∈ Gk, where diam stands for the diameter of a set. We now focus on the distance between RG and RG(cid:48), and we use the popular Hausdorff metric to measure this distance. The distance between RG and RG(cid:48) is meaningful in the sense that for any r(cid:48) ∈ RG(cid:48), we can find r ∈ RG so that (cid:107)r − r(cid:48)(cid:107)∞ ≤ dH (RG, RG(cid:48)). If dH (RG, RG(cid:48)) is small and r is observed, then r(cid:48) is likely to be treated similarly as r. The following last theorem shows that the Hausdorff distance dH with respect to the (cid:107).(cid:107)∞ norm between RG and RG(cid:48) also depends on the distance between G and G(cid:48). Theorem B.1. Given two graphs G ∈ Gk and G(cid:48) ∈ Gk. The following inequality holds: dH (RG, RG(cid:48)) ≤ k3(cid:107)V ind(cid:107)∞\n\nˆδ(G, G(cid:48)).\n\n(4)\n\nMoreover, if G and G(cid:48) are not isomorphic and rank V ind = k, then RG ∩ RG(cid:48) = ∅.\n\nProof of Theorem B.1. Denote [G] = { ̃G ∈ Gk : ̃G ∼= G}. We will prove that for every G1 ∈ [G], there exits G2 ∈ [G(cid:48)] so that (cid:107)rG1 − rG2 (cid:107)∞ ≤ k3(cid:107)V ind(cid:107)∞ Following the definition of ˆδ(cid:3), for G1 ∈ [G], there exists G2 ∈ [G(cid:48)] so that ˆδ(cid:3)(G1, G(cid:48)) = ∼= G, it follows that ˆδ(cid:3)(G1, G(cid:48)) = ˆδ(cid:3)(G, G(cid:48)), and d(cid:3)(G1, G2). On the other hand, since G1 hence d(cid:3)(G1, G2) = ˆδ(cid:3)(G, G(cid:48)). This leads to\n\nˆδ(G, G(cid:48)).\n\n(cid:107)rG1 − rG2(cid:107)∞ = (cid:107)AG1V ind − AG2 V ind(cid:107)∞ ≤ (cid:107)AG1 − AG2 (cid:107)∞(cid:107)V ind(cid:107)∞ ≤ k3d(cid:3)(G1, G2)(cid:107)V ind(cid:107)∞ = k3ˆδ(cid:3)(G, G(cid:48))(cid:107)V ind(cid:107)∞.\n\nThis means\n\nd(rG1, RG(cid:48)) = inf\n\n ̃G∈[G(cid:48)]\n\n(cid:107)rG1 − r ̃G(cid:107) ≤ k3ˆδ(cid:3)(G, G(cid:48))(cid:107)V ind(cid:107)∞\n\nfor every G1 ∈ [G]. Similary, d(rG2, RG) = inf\n\n ̃G∈[G]\n\n(cid:107)rG2 − r ̃G(cid:107) ≤ k3ˆδ(cid:3)(G, G(cid:48))(cid:107)V ind(cid:107)∞ for every\n\nG2 ∈ [G(cid:48)]. This leads to\n\n(cid:40)\n\ndH (RG, RG(cid:48)) = max\n\nsup G1∈[G]\n\nd(rG1, RG(cid:48)),\n\nsup G2∈[G(cid:48)]\n\n(cid:41)\n\nd(rG2, RG)\n\n≤ k3(cid:107)V ind(cid:107)∞\n\nˆδ(G, G(cid:48)).\n\nFinally, if rank V ind = k and G and G(cid:48) are not isomorphic, suppose there exists r ∈ RG ∩ RG(cid:48). Since r ∈ RG, there exists G1 ∈ [G] so that r = rG1 = AG1V ind. Similarly, there exists G2 ∈ [G(cid:48)] so that r = AG2 V ind. This leads to AG1V ind = AG2 V ind, and since rank V ind = k, we obtain AG1 = AG2 . This means G1\n\n∼= G2, and hence G ∼= G(cid:48), which is a contradiction. Hence RG ∩ RG(cid:48) = ∅. (cid:3)\n\nC WELL-DEFINEDNESS OF INDIRECTION REPRESENTATION\n\nIn this section, we consider the case when the canonical assumption (see Remark 2.1) is not obeyed. First, we need a more general definition for the indirection operator (Definition 2.2).\n\n14\n\n~~Published as a conference paper at ICLR 2023\n\nDefinition C.1. Given an input sequence X = (x1, x2, . . . , xk) and its corresponding graph GX ∈ Gk, the indirection operator I is a mapping from Gk to Gind that maps GX to I(GX ) so that 1. AGX = AI(GX ) and 2. if the i-th vertex of GX represents for xj, then the i-th vertex of I(GX ) represents for vind j .\n\nk\n\nConsider an input sequence X = (x1, x2, . . . , xk) and its corresponding graph GX with adjacency matrix AGX , which is computed based on the assumption in Remark 2.1. The associated indirection representation computed by Eq. (1) is rX , i.e., the i-th element of rX is the indirection representation for xi.\n\n1, v(cid:48)\n\nNow consider an arbitrary graph G(cid:48) vertices {v(cid:48) 2, . . . , v(cid:48) matrix P ) so that v(cid:48) mapped to I(G(cid:48) (Definition (2.2)), v(cid:48) represents for xi). It follows that the associated indirection representation r(cid:48) computed as\n\nX that also represents X with adjacency matrix AG(cid:48) and set of k}. This means there exists a permutation σ (with an associated permutation X is k}. By the definition of indirection operator σ(i) represents vind\n\n(since v(cid:48) X with respect to G(cid:48)\n\nσ(i) represents for xi for all i, and AG(cid:48)\n\n= P AGX P (cid:62). Suppose that G(cid:48)\n\nX ) with set of vertices {u(cid:48)\n\ni is mapped to u(cid:48)\n\ni (so that AG(cid:48)\n\nX )) and u(cid:48)\n\n2, . . . , u(cid:48)\n\n= AI(G(cid:48)\n\n1, u(cid:48)\n\nX is\n\nσ(i)\n\nX\n\nX\n\nX\n\ni\n\nr(cid:48) X = AG(cid:48)\n\nX\n\nP V ind = P AGX P (cid:62)P V ind = P AGX V ind = P rX .\n\nσ(i) represents for the σ(i)-th element of r(cid:48)\n\nThis means the σ(i)-th element of r(cid:48) X is the i-th element of rX . On the other hand, after graph propagation, u(cid:48) X , which is the i-th element of rX . Since σ(i) represents for xi and v(cid:48) v(cid:48) σ(i) by the indirection operator I, it follows that the i-th element of rX is the indirection representation for xi. This shows that the indirection representations of xi’s are unchanged when the vertices of GX permute.\n\nσ(i) is mapped to u(cid:48)\n\nD FULL EXPERIMENTAL RESULTS\n\nD.1 OOD IQ PROBLEMS\n\nD.1.1 FINE DATASET\n\nWe report full results with standard deviations of experiments on FINE dataset with trainset CIFAR10 and testset Omniglot in Table 5.\n\nTrain set Test set\n\nNTM PrediNet RelationNet Transformer\n\nCIFAR100 Omniglot\n\nTrans. 25.5±1.7/56.9±5.0 26.7±0.9/28.5±4.5 25.4±0.5/39.3±1.4 26.9±1.9/63.3±3.7\n\nRot. 27.1±0.6/31.4±1.7 25.5±0.3/28.3±0.5 25.0±0.5/34.0±0.4 27.2±1.1/40.4±4.3\n\nShear 29.8±1.3/58.5±4.6 25.6±0.4/29.0±1.7 25.1±0.2/39.8±1.1 28.3±1.4/65.3±2.8\n\nScale 30.3±2.9/61.0±3.9 26.4±1.0/29.9±1.7 24.7±0.6/50.2±1.0 30.6±3.0/62.0±6.2\n\nTable 5: Test accuracy (%) without/with InLay on FINE dataset with trainset CIFAR10 and testset Omniglot.\n\nWe report full results with standard deviations of experiments on FINE dataset with trainset MNIST and testset CIFAR10 in Table 6.\n\nTrain set Test set\n\nNTM PrediNet RelationNet Transformer\n\nMNIST CIFAR100\n\nTrans. 35.2±1.6/69.2±3.8 27.5±1.4/31.6±5.1 25.6±0.5/45.5±2.5 37.8±2.7/63.9±6.5\n\nRot. 30.0±1.9/34.5±5.6 26.8±1.4/27.8±1.5 25.6±0.6/35.2±1.2 32.0±1.5/36.1±10.1\n\nShear 29.6±1.1/56.0±7.4 26.4±1.4/28.4±2.3 25.0±0.5/43.4±1.8 30.2±2.5/53.5±6.8\n\nScale 31.9±4.7/48.7±7.8 33.6±2.5/35.9±6.7 26.8±0.7/44.8±4.1 29.2±3.6/48.1±7.0\n\nTable 6: Test accuracy (%) without/with InLay on FINE dataset.\n\nD.1.2 RAVEN DATASET\n\nWe report full results with standard deviations of experiments on RAVEN dataset in Table 7.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTrain configuration Test configuration LSTM Transformer RelationNet PrediNet\n\nUp-Down Left-Right 28.8±3.1/43.9±6.2 15.2±0.3/56.7±2.3 12.7±0.3/58.5±2.0 13.7±0.3/16.5±0.9\n\n3x3Grid 2x2Grid 19.5±0.6/25.1±2.4 13.6±0.4/26.0±0.9 12.4±0.4/28.9±0.4 13.6±0.3/14.3±0.6\n\nOut-InGrid Out-InCenter 42.1±1.3/48.6±2.7 16.6±4.5/44.7±1.6 12.3±0.3/51.7±2.8 14.2±0.6/15.9±0.5\n\nTable 7: Test accuracy (%) without/with InLay on RAVEN dataset.\n\nD.2 OOD IMAGE CLASSIFICATION\n\nWe report full results with standard deviations of OOD image classification experiments in Table 8.\n\nDataset\n\nNo data augmentation\n\nWith data augmentation\n\nRot90\n\nGrayscale\n\nJitter\n\nRot90\n\nGrayscale\n\nJitter\n\nSVHN\n\n11.8±0.4/12.5±0.7 86.9±0.5/91.8±0.7 87.2±0.2/91.6±0.3 17.3±1.1/19.1±1.6 92.9±0.2/94.7±0.2 91.8±0.2/93.4±0.2\n\nCIFAR10\n\nCIFAR100\n\n25.7±0.1/27.9±0.3 48.3±0.8/62.7±0.2 46.6±0.9/59.7±0.8 24.2±0.4/27.3±1.3 54.2±0.4/69.2±0.9 50.2±0.7/68.4±0.5\n\n13.4±0.4/15.0±0.2 16.4±0.3/23.9±0.4 18.1±0.2/25.0±0.6 11.1±0.5/14.0±0.4 18.5±0.7/27.8±0.5 19.4±0.5/28.7±0.5\n\nTable 8: Test accuracy (%) without/with InLay on OOD classification task with ViT.\n\nD.3 FEW-SHOT NLP DOMAIN ADAPTATION\n\nWe report full results with standard deviations of few-shot NLP classification tasks on FewRel 2.0 dataset in Table 9.\n\n5-way-1-shot 10-way-1-shot\n\nProtoNet 64.7±1.5/65.4±3.0 49.1±2.0/41.7±3.2\n\nSNAIL 38.6±2.9/60.6±4.9 17.3±1.2/34.9±5.8\n\nGNN 36.7±2.9/63.1±1.3 27.1/43.3±3.4\n\nMTB 66.1±1.7/68.9±1.0 52.9±1.2/54.1±1.3\n\nTable 9: Validation accuracy (%) without/with InLay on FewRel 2.0.\n\nE\n\n6-LAYER VIT VS. 7-LAYER VIT\n\nWe report results of 6-layer ViT and 7-layer ViT, along with 6-layer ViT equipped with InLay, in OOD CIFAR10 classification tasks in Table 10. Overall, average performance of 6-layer ViT and 7-layer ViT is not much different (38.2% and 38%, respectively), while 6-layer ViT equipped with InLay clearly improve performances with average accuracy of 43.1%.\n\nF RUNNING TIME OF INLAY\n\nWe report running time (s/iter) of ViT and ViT+InLay on CIFAR10 dataset. Models are trained on a single Tesla V100-SXM2 GPU. Overall, ViT+InLay requires roughly 10% more computational time.\n\nG MORE ABLATION STUDIES ON TANH ACTIVATION\n\nReaders may observe in Fig. 3a that having no activation in InLay still achieves almost equal performance. However, that is just a special case; Table shows results of similar ablation experiments with NTM on the FINE dataset with different activation functions. Among all, the tanh activation achieves best average performance.\n\nH ABOUT INPUT SEQUENCE LENGTH\n\nH.1 WHEN THE SEQUENCE IS TOO LONG\n\nWhen the number of nodes is large, graph neural networks usually suffer from the issue of oversmoothness, which is the phenomenon that all nodes become nearly the same after updated. However, we show that InLay may mildly suffer from this issue. In the OOD classification task, we increase\n\n16\n\nPublished as a conference paper at ICLR 2023\n\n6-layer ViT 7-layer ViT 6-layer ViT + InLay\n\nImage transformation Rot90 25.7±0.1 26.4 ±0.5 27.9±0.3\n\nVFlip 32.1±0.6 31.5±0.3 35.0±0.9\n\nJitter 23.2±1.1 22.4±1.2 24.9±0.7\n\nColor transformation Invert 39.4±2.8 38.6±3.2 48.3±3.3\n\nBlur 60.2±0.5 60.8 ±0.6 59.5±1.2\n\nGrayscale 48.3±0.8 48.3 ±0.7 62.7±0.2\n\nAverage\n\n38.2 38 43.1\n\nTable 10: Test accuracy (%) without/with InLay on OOD CIFAR10 classification tasks with 6-layer ViT, 7-layer ViT, and 6-layer ViT equipped with InLay.\n\nNo data augmentation With data augmentation\n\nViT ViT+InLay\n\n0.095 0.108\n\n0.100 0.110\n\nTable 11: Running time (s/iter) of ViT and ViT+InLay on CIFAR10 dataset.\n\nthe number of nodes by duplicating each node 4 times, resulting in a graph with 196 nodes. The results are reported in Table 13. It can be observed that the test accuracy in the case of 196 nodes is not much different from the case of 49 nodes.\n\nH.2 WHEN THE SEQUENCE LENGTH IS NOT FIXED\n\nAll input sequences in our experiments are of fixed length. The case of varying input sequence length can be treated as fixed-length case if the maximum sequence length is known: we can use empty nodes to fulfill any sequence to reach that maximum length. We conduct experiments on OOD classification task to illustrate this idea: for each image patches sequence, we randomly remove some (2 to 6) patches, so that the resulting sequences have different lengths; we then use zero tensors for padding so that all sequences now have the same lengths. We train ViT+InLay on CIFAR10 dataset and test on images with grayscale distortion. The test accuracy is 61.4%, which is not much different from 62.7% of the fixed-length case.\n\nA more challenging scenario is when the lengths of testing sequences are longer than training ones. Current design of InLay does not allow it to deal with this situation. We believe this is promising for future work.\n\nI COMPARISON WITH OTHER INDIRECTION APPROACHES\n\nWe compare InLay with different indirection approaches like ESBN (Webb et al., 2020b) and FINE (Pham et al., 2022) on FINE dataset. We incorporate InLay with Transformer as it shows the best performance among different models. For FINE, we use NICE backbone as suggested in the original paper. Results are shown in Table 14. Overall, Transformer+InLay shows competitive results with the best performances on 5/8 tasks and second-best performances on 2/8 tasks.\n\nJ MORE ABLATION EXPERIMENTS ON CONTEXT NORMALIZATION\n\nWe further conduct ablation experiments to show the necessity of context normalization in InLay. Table 15 show performances of NTM and NTM+InLay, with and without context normalization. First, we may observe that InLay is really effective in the sense that NTM+InLay without context normalization is still far better than original NTM, both with and without context normalization. Second, context normalization helps to boost the performances of NTM+InLay by a large margin.\n\nK TRAINING DETAILS\n\nK.1\n\nIQ OOD PROBLEMS\n\nK.1.1 FINE DATASET\n\nWe use 3-layer p4-CNN encoder (Cohen and Welling, 2016) with kernel size 3 and padding 1 to encode raw 32 × 32 images to feature vectors of size 128. This encoder can help model adapt better\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTrain set Test set\n\nNTM+InLay(tanh) NTM+InLay(none) NTM+InLay(softmax) NTM+InLay (relu) NTM\n\nTrans. 69.2 58.1 62.1 52.7 35.2\n\nRot. 34.5 31.2 31.9 32.3 30.0\n\nMNIST CIFAR100 Shear 56.0 52.5 58.4 51.2 29.6\n\nScale 48.7 47.8 41.2 33.5 31.9\n\nAvg. 52.1 47.4 48.4 42.4 31.7\n\nTable 12: Average test accuracy (%) of NTM (with or without InLay) on FINE dataset with different activation functions.\n\nTest accuracy (%)\n\nViT ViT+InLay (49 nodes) ViT+InLay (196 nodes) 48.3\n\n62.5\n\n62.7\n\nTable 13: Experiment results of ViT+InLay with different number of graph nodes on OOD classification task on CIFAR10 dataset. Test distortion is grayscale.\n\nwith transformed images. We use Adam optimizer (Kingma and Ba, 2014) with learning rates ranging from 10−5 to 3 · 10−4, depending on specific model and transformation. We train all models with batch size 32 in 200 epochs. The indirection representations are computed as in Eq. (1) with 1 attention head.\n\nThe training set contains 5,000 IQ problems, while testing set contains 10,000 IQ problems of unseen images and unseen rules. Specifically:\n\n• With translation, models are trained on problems with translation vectors (a, b) with a ∈ {0, 3, 6, 9} and b ∈ {0, ±3, ±6, ±9}, and tested with a ∈ {−3, −6, −9}, i.e., train on problems with translations to the right and test on problems with translations to the left.\n\n• With rotation, models are trained on problems with rotation angle α\n\n{0◦, 15◦, 30◦, . . . , 180◦}, tested with α ∈ {195◦, 210◦, . . . , 345◦}.\n\n∈\n\n• With shear, models are trained on problems with shear angles (α, β) with α ∈ {0, 15◦, 30◦, 45◦, 60◦} and β ∈ {0, ±15◦, ±30◦, ±45◦, ±60◦}, and tested with α ∈ {−15◦, −30◦, −45◦, −60◦}.\n\n• With scale, models are trained on problems with scale factor α ∈ {1, 1.25} and tested with\n\nα ∈ {0.5, 0.75}, i.e., train with larger scale and test with smaller scale.\n\nK.1.2 RAVEN DATASET\n\nWe use 3-layer CNN encoder with kernel size 3 and stride 2 to encode 80 × 80 images to feature vectors of size 256. We use Adam optimizer with learning rates ranging from 10−4 to 3 · 10−4 and gradient clipping 1. All models are trained with batch size 32 in 250 epochs. The indirection representations are computed as in Eq. (1) with 1 attention head.\n\nWe apply the Dynamic Residual Tree (DRT) as follows: we first apply DRT to feature vectors, then pass resulting vectors through InLay to obtain indirection representations, then apply DRT once again and these final resulting vectors will be the input for prediction models. Other details are similar to the original paper and codes are also adapted from the original paper.\n\nK.2 OOD IMAGE CLASSIFICATION\n\nWe use Adam optimizer with learning rate 5 · 10−4. All models are trained with batch size 32 in 200 epochs. The indirection representations are computed as in Eq. (1) with 32 attention heads.\n\nWe use 6-layer ViT with patch size 8, 16 attention heads and dropout rate 0.1. The dimension of feedforward layer is 2048.\n\nK.3 FEW-SHOT NLP DOMAIN ADAPTATION\n\nWe use BERT to encode paragraphs to feature vectors of size 768. Models are trained with batch size 32 on 5-way-1-shot tasks and batch size 16 on 10-way-1-shot tasks. We use SGD optimizer with\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTrain set Test set\n\nCIFAR100 Omniglot\n\nMNIST CIFAR100\n\nFINE ESBN Transformer+InLay\n\nTrans. 27.3 53.5 63.3\n\nRot. 37.6 39.9 40.4\n\nShear 18.7 56.6 65.3\n\nScale 19.5 45.2 62.0\n\nTrans. 47.7 54.1 63.9\n\nRot. 45.2 50.3 36.1\n\nShear 48.8 70.7 53.5\n\nScale 26.6 58.8 48.1\n\nTable 14: Average test accuracy (%) without/with InLay on FINE dataset.\n\nTrain set Test set\n\nNTM+InLay(with context norm.) NTM+InLay (without context norm.) NTM (with context norm.) NTM (without context norm.)\n\nTrans. 69.2 44.0 35.2 33.9\n\nRot. 34.5 41.3 30.0 28.7\n\nMNIST CIFAR100 Shear 56.0 43.6 29.6 32.3\n\nScale 48.7 36.3 31.9 37.1\n\nAvg. 52.1 41.3 31.7 33\n\nTable 15: Average test accuracy (%) of NTM (with or without InLay) on FINE dataset with different activation functions.\n\nlearning rate 2 · 10−5. The indirection representations are computed as in Eq. (1) with 32 attention heads. Other details are similar to the original paper and codes are also adapted from the original paper.\n\n19",
    "reference": "# Summary Of The Paper\n\nThe authors present an layer in a neural network called Indirection Layer (InLay). InLay takes\na sequence of objects as input and transforms the sequence into a new indirect graph-structured\nrepresentation. In this graph vertices are the objects, and edges are similarity scores in the range [-1,1] that are learned. The similarity weights are then used to modify a representation V_ind of the vertices which is not computed based on the data X (it is a different set of vectors). This representation, that is contextualized by other members of the sequence is than used for prediction. All in all this is very similar to self attention, however the attention scores are in [-1,1] and V_ind is not computed based on the data X.\nThe goal of InLay is that graph representations of different two sequences that have similar internal structure should be similar, and thus help in OOD generalization.\n\nInLay can be used as an additional layer on top of different architectures, e.g., Transformers or LSTMs.\nExperiments of InLay are done for the datasets FINE and RAVEN, where a sequence of images with an internal relation are shown and the model should predict which image should complete the sequence. For both datasets, substetianl improvement is achieved by using InLay. In addition, the method is applied to OOD image classification tasks and shows nice gains.\n\n# Strength And Weaknesses\n\nStrength: The proposed method is easy to apply as it can be naturally added on top of representations by other base models. In addition, it shows nice boosts in appropriate settings and for OOD image classification.\n\nWeakness: Current experiments mostly involve applications to simple images in synthetic settings (like IQ tests), or synthetic transformations to images like rotations. It is not clear at this point whether InLay can capture implicit and complex relations between natural images. In addition, all in all the layer is very similar to a self-attention layer.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClear writing. Assuming the implementation will be open, other folks could find the layer useful.\n\n# Summary Of The Review\n\nInLay is a neural layer for representing objects in a sequence that share relations between them (e.g., rotation). InLay seems like a good direction towards capturing relations between objects that are useful for different classification tasks. The layer could be added in a simple manner on top of different architectures. Currently, the method is applies to synthetic tasks, and it is left for future work to find out whether the learned representation is useful in realistic settings.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nINVERTIBLE NORMALIZING FLOW NEURAL NETWORKS BY JKO SCHEME\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNormalizing flow is a class of deep generative models for efficient sampling and density estimation. In practice, the flow often appears as a chain of invertible neural network blocks. To facilitate training, past works have regularized flow trajectories and designed special network architectures. The current paper develops a neural ODE flow network inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which allows an efficient block-wise training procedure: as the JKO scheme unfolds the dynamic of gradient flow, the proposed model naturally stacks residual network blocks one-by-one and reduces the memory load as well as the difficulty of training deep networks. We also develop an adaptive time-reparametrization of the flow network with a progressive refinement of the trajectory in probability space, which improves the optimization efficiency and model accuracy in practice. On highdimensional generative tasks for tabular data, JKO-iFlow can process larger data batches and perform competitively as or better than continuous and discrete flow models, using 10X less number of iterations (e.g., batches) and significantly less time per iteration.\n\n1\n\nINTRODUCTION\n\nGenerative models have been widely studied in statistics and machine learning to infer data-generating distributions and sample from the estimated distributions (Ronquist et al., 2012; Goodfellow et al., 2014; Kingma & Welling, 2014; Johnson & Zhang, 2019). The normalizing flow has recently been a very popular generative framework. In short, a flow-based model learns the data distribution via an invertible mapping F between data density pX (X), X ∈ Rd and the target standard multivariate Gaussian density pX (Z), Z ∼ N (0, Id) (Kobyzev et al., 2020). Benefits of the approach include efficient sampling and explicit likelihood computation. To make flow models practically useful, past works have made great efforts to develop flow models that facilitate training (e.g., in terms of loss objectives and computational techniques) and induce smooth trajectories (Dinh et al., 2017; Grathwohl et al., 2019; Onken et al., 2021).\n\n(a) JKO-iFlow\n\n(b) Generic Flow\n\nAmong flow models, continuous normalizing flow (CNF) transports the data density to that of the target through continuous dynamics (e.g, Neural ODE (Chen et al., 2018)). CNF models have shown promising performance on generative tasks Kobyzev et al. (2020). However, a known computational challenge of CNF models is model regularization, primarily due to the non-uniqueness of the flow transport. To regularize the flow model and guarantee invertibility, Behrmann et al. (2019) adopted spectral normalization of block weights that leads to additional computation. Meanwhile, (Liutkus et al., 2019) proposed the sliced-Wasserstein distance, Finlay et al. (2020); Onken et al. (2021) utilized optimal-transport costs, and (Xu et al., 2022) proposed Wasserstein-2 regularization. Although regularization is important to maintain invertibility for general-form flow models and improves performance in practice, merely\n\nFigure 1: Comparison of JKO-iFlow (proposed) and other flow models. The JKO scheme approximates the transport of a diffusion process and the ResNet is trained block-wise.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nusing regularization does not resolve non-uniqueness of the flow and there remains variation in the trained flow depending on initialization. Besides unresolved challenges in regularization, there remain several practical difficulties when training such models. In many settings, flows consist of stacked blocks, each of which can be arbitrarily complex. Training such deep models often places high demand on computational resources, numerical accuracy, and memory consumption. In addition, determining the flow depth (e.g., number of blocks) is also unclear.\n\nIn this work, we propose JKO-iFlow, a normalizing flow network which unfolds the Wasserstein gradient flow via a neural ODE invertible network, inspired by the JKO-scheme Jordan et al. (1998). The JKO scheme, cf. (5), can be viewed as a proximal step to unfold the Wasserstein gradient flow to minimize the KL divergence (relative entropy) between the current density and the equilibrium. Each block in the flow model implements one step in the JKO-scheme can be trained given the previous blocks. As the JKO scheme pushes forwards the density to approximate the solution of Fokker-Planck equation of a diffusion process with small step-size, the trained flow model induces a smooth trajectory of density evolution, as shown in Figure 1. The theoretical assumption does not incur a restriction in practice when training, whereby one can use larger step sizes coupled with numerical integration techniques. The proposed JKO-iFlow model can be viewed as trained to learn the unique transport map following the Fokker-Planck equation.\n\nUnlike most CNF models where all the residual blocks are initialized together and trained end-to-end, the proposed model allows a block-wise training which reduces memory and computational load. We further introduce time reparametrization with progressive refinement in computing the flow network, where each block corresponds to a point on the density evolution trajectory in the space of probability measures. Algorithmically, one can thus determine the number of blocks adaptively and refine the trajectory determined by existing blocks. Empirically, such procedures yield competitive performance as other CNF models with significantly less computation.\n\nThe JKO Flow approach proposed in this work also suggests a potential constructive approximation analysis of deep flow model. Method-wise, the proposed model differs from other recent JKO deep models. We refer to Section 1.1 for more details. In summary, the contribution includes\n\n• We propose a neural ODE model where each residual block computes a JKO step and the training objective can be computed from integrating the ODE on data samples. The network has general form and invertibility can be satisfied due to the regularity of the optimal pushforward map that minimizes the objective in each JKO step.\n\n• We develop an block-wise procedure to train the invertible JKO-iFlow network, which determines the number of blocks adaptively. We also propose a technique to reparametrize and refine an existing JKO-iFlow probability trajectory. Doing so removes unnecessary blocks and increases the overall accuracy.\n\n• Experiment wise, JKO-iFlow greatly reduces memory consumption and the amount of computa-\n\ntion, with competitive/better performance as several existing continuous and discrete flow models.\n\n1.1 RELATED WORKS\n\nFor deep generative models, popular approaches include generative adversarial networks (GAN) (Goodfellow et al., 2014; Gulrajani et al., 2017; Isola et al., 2017) and variational auto-encoder (VAE)(Kingma & Welling, 2014; 2019). Apart from known training difficulties (e.g., mode collapse (Salimans et al., 2016) and posterior collapse (Lucas et al., 2019)), these models do not provide likelihood or inference of data density. The normalizing flow framework (Kobyzev et al., 2020) has been extensively developed, including continuous flow (Grathwohl et al., 2019), Monge-Ampere flow (Zhang et al., 2018), discrete flow (Chen et al., 2019), graph flow (Liu et al., 2019), etc. Efforts have been made to develop novel invertible mapping structures (Dinh et al., 2017; Papamakarios et al., 2017), regularize the flow trajectories (Finlay et al., 2020; Onken et al., 2021), and extend the use to non-Euclidean data (Mathieu & Nickel, 2020; Xu et al., 2022). Despite such efforts, the model and computational challenges of normalizing flow models include regularization and the large model size when using a large number of residual blocks, which cannot be determined a priori, and the associated memory and computational load.\n\nIn parallel to continuous normalizing flow which are neural ODE models, neural SDE models become an emerging tool for generative tasks. Diffusion process and Langevin dynamics in deep generative models have been studied in score-based generative models (Song & Ermon, 2019; Ho et al., 2020; Block et al., 2020; Song et al., 2021) under a different setting. Specifically, these models estimate the\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nscore function (i.e., gradient of the log probability density with respect to data) of data distribution via neural network parametrization, which may encounter challenges in learning and sampling of high dimensional data and call for special techniques (Song & Ermon, 2019). The recent work of Song et al. (2021) developed reverse-time SDE sampling for score-based generative models, and adopted the connection to neural ODE to compute the likelihood; using the same idea of backward SDE, Zhang & Chen (2021) proposed joint training of forward and backward neural SDEs. Theoretically, latent diffusion Tzen & Raginsky (2019b;a) was used to analyze neural SDE models. The current work focuses on neural ODE model where the deterministic vector field f (x, t) is to be learned following a JKO scheme of the Fokker-Planck equation. Rather than neural SDE, our approach involves no sampling of SDE trajectories nor learning of the score function. Our obtained residual network is also invertible, which can not be achieved by the diffusion models above. We experimentally obtain competitive or improved performance against on simulated and high-dimensional tabular data.\n\nJKO-inspired deep models have been studied in several recent works. (Bunne et al., 2022) reformulated the JKO step for minimizing an energy function over convex functions. JKO scheme has also been used to discretize Wasserstein gradient flow to learn a deep generative model in (Alvarez-Melis et al., 2021; Mokrov et al., 2021), which adopted input convex neural networks (ICNN) (Amos et al., 2017). ICNN as a special type of network architecture may have limited expressiveness (Rout et al., 2022; Korotin et al., 2021). In addition to using gradient of ICNN, (Fan et al., 2021) proposed to parametrize the transport in a JKO step by a residual network but identified difficulty in calculating the push-forward distribution. The approach in (Fan et al., 2021) also relies on a variational formulation which requires training an additional network similar to the discriminator in GAN using inner-loops. In contrast, our method trains an invertible neural-ODE flow network which enables the flow from data density to normal and backward as well as the computation of transported density by integrating the divergence of the velocity field along ODE solutions. The objective in JKO step to minimize KL divergence can also be computed directly without any inner-loop training, cf. Section 4.\n\nFor the expressiveness of generating deep models, universal approximation properties of deep neural networks for representing probability distributions have been developed in several works. Lee et al. (2017) established approximation by composition of Barron functions (Barron, 1993); Bailey & Telgarsky (2018) developed space-filling approach, which was generalized in Perekrestenko et al. (2020; 2021); Lu & Lu (2020) constructed a deep ReLU network with guaranteed approximation under integral probability metrics, using techniques of empirical measures and optimal transport. These results show that deep neural networks can provably transport one source distribution to a target one with sufficient model capacity under certain regularity conditions of the pair of densities. In our proposed flow model, each residual block is trained to approximate the vector field f (x, t) that induces the Fokker-Planck equation, cf. Section 3.2. Our model potentially leads to a constructive approximation analysis of neural ODE flow model to generate data density pX .\n\n2 PRELIMINARIES\n\nNormalizing flow. A normalizing flow can be mathematically expressed via a density evolution equation of ρ(x, t) such that ρ(x, 0) = pX and as t increases ρ(x, t) approaches pZ ∼ N (0, Id) Tabak & Vanden-Eijnden (2010). Given an initial distribtuion ρ(x, 0), such a flow typically is not unique. We consider when the flow is induced by an ODE of x(t) in Rd\n\n ̇x(t) = f (x(t), t), (1) where x(0) ∼ pX . The marginal density of x(t) is denoted as p(x, t), and it evolves according to the continuity equation (Liouville equation) of (1) written as\n\n∂tp + ∇ · (pf ) = 0,\n\np(x, 0) = pX (x).\n\n(2)\n\n√\n\nOrnstein–Uhlenbeck (OU) process. Consider a Langevin dynamic denoted by the SDE dXt = −∇V (Xt)dt + 2dWt, where V is the potential of the equilibrium density. We focus on the case of normal equilibrium, that is, V (x) = |x|2/2 and then pZ ∝ e−V . In this case the process is known as the (multivariate) OU process. Suppose X0 ∼ pX , and let the density of Xt be ρ(x, t) also denoted as ρt(·). The Fokker-Planck equation describes the evolution of ρt towards the equilibrium pZ as\n\n∂tρ = ∇ · (ρ∇V + ∇ρ),\n\n(3) Under generic conditions, ρt converges to pZ exponentially fast. For Wasserstein-2 distance and the standard normal pZ, classical argument gives that (take C = 1 in Eqn (6) of Bolley et al. (2012))\n\nρ(x, 0) = pX (x).\n\nV (x) := |x|2/2,\n\nW2(ρt, pZ) ≤ e−tW2(ρ0, pZ),\n\nt > 0.\n\n(4)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nJKO scheme. The seminal work Jordan et al. (1998) established a time discretization scheme of the solution to (3) by the gradient flow to minimize KL(ρ||pZ) under the Wasserstein-2 metric in probability space. Denote by P the space of all probability densities on Rd with finite second moment. The JKO scheme at k-th step with step size h > 0, starting from ρ(0) = ρ0 ∈ P, is written as\n\nρ(k+1) = arg min\n\nρ∈P\n\nF [ρ] +\n\n1 2h\n\nW 2\n\n2 (ρ(k), ρ), F [ρ] := KL(ρ||pZ).\n\n(5)\n\nIt was proved in Jordan et al. (1998) that as h → 0, ρ(k) converges to the solution ρ(·, kh) of (3) for all k, and the convergence ρh(·, t) → ρ(·, t) is strongly in L1(Rd, (0, T )) for finite T , where ρh is piece-wise constant interpolated on (0, T ) from ρ(k).\n\n3\n\nJKO SCHEME BY NEURAL ODE\n\nGiven i.i.d. observed data samples Xi ∈ Rd, i = 1, . . . , N , drawn from some unknown density pX , the goal is to train an invertible neural network to transports the density pX to an a priori specified density pZ in Rd, where each data sample Xi is mapped to a code Zi. A prototypical choice of pZ is the standard multivariate Gaussian N (0, Id). By a slight abuse of notation, we denote by pX and pZ both the distributions and the density functions of data X and code Z respectively.\n\n3.1 THE OBJECTIVE OF JKO STEP\n\nWe are to specify f (x, t) in the ODE (1), to be parametrized and learned by a neural ODE, such that the induced density evolution of p(x, t) converges to pZ as t increases. We start by dividing the time horizon [0, T ] into finite subintervals with step size h, let tk = kh and Ik+1 := [tk, tk+1). Define pk(x) := p(x, kh), namely the density of x(t) at t = kh. The solution of (1) determined by the vector-field f (x, t) on t ∈ Ik+1 (assuming the ODE is well-posed (Sideris, 2013)) gives a one-to-one mapping Tk+1 on Rd, s.t. Tk+1(x(tk)) = x(tk+1) and Tk+1 transports pk into pk+1, i.e., (Tk)#pk−1 = pk, where we denote by T#p the push-forward of distribution p by T , such that (T#p)(·) = p(T −1(·)).\n\nSuppose we can find f (·, t) on Ik+1 such that the corresponding Tk+1 solves the JKO scheme (5), then with small h, pk approximates the solution to the Fokker-Planck equation 3, which then flows towards pZ. By the Monge formulation of the Wasserstein-2 distance between p and q 2 (p, q) = minT :T#p=q Ex∼p∥x − T (x)∥2, solving for the transported density pk by (5) is as W 2 equivalent to solving for the transport Tk+1 by\n\nTk+1 = arg min\n\nT :Rd→Rd\n\nF [T ] +\n\n1 2h\n\nEx∼pk ∥x − T (x)∥2, F [T ] = KL(T#pk||pZ).\n\n(6)\n\nThe equivalence between (5) and (6) is proved in Lemma A.1. Furthermore, the following proposition gives that the value of F [T ] can be computed from f (x, t) on t ∈ Ik+1 only once pk is determined by f (x, t) for t ≤ tk. The counterpart for convex function based parametrization of Tk was given in Theorem 1 of (Mokrov et al., 2021), where the computation using the change-of-variable differs as we adopt an invertible neural ODE approach here. The proof is left to Appendix A.\n\nProposition 3.1. Given pk, up to a constant c independent from f (x, t) on t ∈ Ik+1,\n\nKL(T#pk||pZ) = Ex(tk)∼pk\n\nÇ\n\nV (x(tk+1)) −\n\n(cid:90) tk+1\n\ntk\n\nå\n\n∇ · f (x(s), s)ds\n\n+ c.\n\n(7)\n\nBy Proposition 3.1, the minimization (6) is equivalent to\n\nmin {f (x,t)}t∈Ik+1\n\nEx(tk)∼pk\n\nÇ\n\nV (x(tk+1)) −\n\n(cid:90) tk+1\n\ntk\n\n∇ · f (x(s), s)ds +\n\n∥x(tk+1) − x(tk)∥2\n\nå\n\n, (8)\n\n1 2h\n\nwhere x(tk+1) = x(tk) + (cid:82) tk+1 f (x(s), s)ds. Taking a neural ODE approach, we parametrize {f (x, t)}t∈Ik+1 as a residual block with parameter θk+1, and then (8) is reduced to minimizing over θk+1. This leads to block-wise learning algorithm to be introduced in Section 4.\n\ntk\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.2\n\nINFINITESIMAL OPTIMAL f (x, t)\n\nIn each JKO step of (8), let p = pk denote the current density, q = pZ be the target equilibrium density. In this subsection, we show that the optimal f in (8) with small h reveals the difference between score functions between target and current densities. Thus minimizing the objective (8) searches for a neural network parametrization of the score function ∇ log ρt without denoising score matching as in diffusion-based models (Ho et al., 2020; Song et al., 2021).\n\nConsider general equilibrium distribution q with a differentiable potential V . To analyze the optimal pushforward mapping in the small h limit, we shift the time interval [kh, (k + 1)h] to be [0, h] to simplify notation. Then (8) is reduced to\n\nmin {f (x,t)}t∈[0,h)\n\nEx(0)∼p\n\nÇ\n\nV (x(h)) −\n\n(cid:90) h\n\n0\n\n∇ · f (x(s), s)ds +\n\n∥x(h) − x(0)∥2\n\nå\n\n,\n\n1 2h\n\n(9)\n\nwhere x(h) = x(0)+(cid:82) h 0 f (x(s), s)ds. In the limit of h → 0+, formally, x(h)−x(0) = hf (x(0), 0)+ O(h2), and suppose V of q is C 2, V (x(h)) = V (x(0)) + h∇V (x(0)) · f (x(0), 0) + O(h2). For any differentiable density ρ, the (Stein) score function is defined as sρ = ∇ log ρ, and we have ∇V = −sq. Taking the formal expansion of orders of h, the objective in (9) is written as ã\n\nÅ\n\nÅ\n\nã\n\nEx∼p\n\nV (x) + h\n\n−sq(x) · f (x, 0) − ∇ · f (x, 0) +\n\n∥f (x, 0)∥2\n\n+ O(h2)\n\n.\n\n(10)\n\n1 2\n\nNote that Ex∼pV (x) is independent of f (x, t), and the O(h) order term in (10) is over f (x, 0) only, thus the minimization of the leading term is equivalent to\n\nmin f (·)=f (·,0)\n\nEx∼p\n\nÅ\n\n−Tqf +\n\nã\n\n∥f ∥2\n\n1 2\n\n, Tqf := sq · f + ∇ · f ,\n\n(11)\n\nwhere Tq is known as the Stein operator (Stein, 1972). The Tqf in (11) echoes that the derivative of KL divergence with respect to transport map gives Stein operator (Liu & Wang, 2016). The Wasserstein-2 regularization gives an L2 regularization in (11). Let L2(p) be the L2 space on (Rd, p(x)dx), and for vector field v on Rd, v ∈ L2(p) if (cid:82) |v(x)|2p(x)dx < ∞. One can verify that, when both sp and sq are in L2(p), the minimizer of (11) is\n\nf ∗(·, 0) = sq − sp. This shows that the infinitesimal optimal f (x, t) equals the difference of the score functions of the equilibrium and the current density.\n\n3.3\n\nINVERTIBILITY OF FLOW MODEL AND EXPRESSIVENESS\n\nAt time t the current density of x(t) is ρt, the analysis in Section 3.2 implies that the optimal vector field f (x, t) has the expression as\n\nf (x, t) = sq − sρt = −∇V − ∇ log ρt. With this f (x, t), the Liouville equation (2) coincides with the Fokker-Planck equation (3). This is consistent with that JKO scheme with small h recovers the solution to the Fokker-Planck equation. Under proper regularity condition of V and the initial density ρ0, the r.h.s. of (12) is also regular over space and time. This leads to two consequences, in approximation and in learning: Approximation-wise, the regularity of f (x, t) allows to construct a k-th residual block in the flow network to approximate {f (x, t)}t∈Ik when there is sufficient model capacity, by classical universal approximation theory of shallow networks (Barron, 1993; Yarotsky, 2017). The JKO-iFlow model proposed in this work suggests a constructive proof of the expressiveness of the invertible neural ODE model to generate any sufficiently regular density pX , which we further discuss in the last section.\n\n(12)\n\nFor learning, when properly trained with sufficient data, the neural ODE vector field f (x, t; θk) will learn to approximate (12). This can be viewed as inferring the score function of ρt, and also leads to invertibilty of the trained flow net in theory: Suppose the trained f (x, t; θk) is close enough to (12), it will also has bounded Lipschitz constant. Then the residual block is invertible as long as the step size h is sufficiently small, e.g. less than 1/L where L is the Lipschitz bound of f (x, t; θk). In practice, we typically use smaller h than needed merely by invertibility (allowed by model budget) so that the flow network can more closely track the Fokker-Planck equation of the diffusion process. The invertibility of the proposed model is numerically verified in experiments (see Table 1).\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4 TRAINING OF JKO-IFLOW NET\n\n4.1 BLOCK-WISE TRAINING\n\nAlgorithm 1 Block-wise JKO-iFlow training\n\nNote that the training of (k + 1)-th block in (8) can be conducted once the previous k blocks i=1, the expectation Ex(t)∼pk are trained. Specifically, with finite training data {Xi = xi(0)}n in (8) is replaced by the sample average over {xi(kh)}n i=1 which can be computed from the previous k blocks. Note that for each given x(t) = x(tk), both x(tk+1) and the integral of ∇ · f in (8) can be computed by a numerical neural ODE integrator. Following previous works, we use the Hutchinson trace estimator (Hutchinson, 1989; Grathwohl et al., 2019) to estimate the quantity ∇ · f in high dimensions. Applying the numerical integrator in computing (8), we denote the resulting k-th residual block abstractly as fθk with trainable parameters θk. This leads to a block-wise training of the normalizing flow network, as summarized in Algorithm 1. Regarding input parameters,, we found the generative performance JKO-iFlow may vary depending on starting choices of tk, but a simple choice such as tk = k often yields reasonably good performance. We discuss further the initial selection of tk in Appendix C.1. Meanwhile, one can use any suitable termination criterion Ter(k) in line 2 of Algorithm 1. In our experiments, we monitor the per-dimension W2 loss W 2 2 (T#pk, pk) as defined in (6), and terminate training more blocks if the per-dimension loss is below ε. Lastly, the heuristic approach in line 5 of training a “free block” (i.e., block without the W2 loss) is to flow the push-forward density pL closer to pZ, where the former is obtained through the first L blocks and the latter denotes the Gaussian density at equilibrium.\n\nRequire: Time stamps {tk}, training data, termination criterion Ter and tolerance level ε, maximal number of blocks Lmax.\n\n4: end while 5: L ← k. Optimize fθL+1 using (8) with h = ∞. {▷ Free block, no W2 regularization.}\n\nOptimize fθk upon minimizing (8) with mini-batch sample approximation, given {fθi}k−1\n\n1: Initialize k = 1. 2: while Ter(k) > ε and k ≤ Lmax do 3:\n\ni=1 . Set k ← k + 1.\n\nNote that Algorithm 1 significantly reduces memory and computational complexity: only one block is trained when optimizing (8), regardless of flow depth. Therefore, one can use larger data batches and more refined numerical integrator without memory explosion. In addition, one can train each block for a fixed number of epochs using either back-propagation or the NeuralODE integrator (Grathwohl et al., 2019, adjoint method). We found direct back-propagation enables faster training but may also lead to greater numerical errors and memory consumption. Despite greater inaccuracies, we observed similar empirical performances across both methods for JKO-iFlow, possibly due to the block-wise training that accumulates fewer errors than a generic flow model composed of multiple blocks.\n\n4.2\n\nIMPROVED COMPUTATION OF TRAJECTORIES IN PROBABILITY SPACE\n\nWe adopt two additional computational techniques to facilitate learning of the trajectories in the probability space, represented by the sequence of densities pk, k = 1, · · · , K, associated with the K residual blocks of the proposed normalizing flow network. The two techniques are illustrated in Figure 2. Additional details can be found in Appendix B.\n\n• Trajectory reparametrization. We empirically observe fast decay of the movements W 2 2 (T#pk, pk); in other words, initial blocks transport the densities much further than the later ones. This is especially unwanted because in order to train the current block, the flow model needs to transport data through all previous blocks, yet the current block barely contributes to the density transport. Hence, instead of having tk := kh with fixed increments per block, we reparametrize the values of tk through an adaptive procedure, which is entirely based on the W2 distance at each block and the averaged W2 distance over all blocks.\n\n• Progressive refinement. To improve the probability trajectory obtained by the trained residual blocks, we propose a refinement technique that trains additional residual blocks based on time steps tk obtained after reparametrization. In practice, refinement can be useful when the time increment tk+1 − tk for certain blocks is too large. In those cases, there may exist numerical inaccuracies as the loss (8) is computed over a longer time horizon. More precisely, we increase the number of JKO-steps parametrized by residual blocks, where in practice, we training C additional “intermediate” blocks for density transport between pk and pk+1 at each k.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Diagram illustrating trajectory reparametrization and refinement. The top panel shows the original trajectory under three blocks via Algorithm 1. The bottom panel shows the trajectory under six blocks after reparametrization and refinement, which renders the W2 movements more even.\n\n5 EXPERIMENT\n\nWe first generate based on two-dimensional simulated samples. We then perform unconditional and conditional generation on high-dimensional real tabular data. We also show JKO-iFlow’s generative performance on MNIST. Additional details are in Appendix C.\n\n5.1 SETUP\n\nCompeting Methods and metrics. We compare JKO-iFlow with five other models, including four flow-based model and one diffusion model. The first two continuous flow models are FFJORD (Grathwohl et al., 2019) and OT-Flow (Onken et al., 2021). The next two discrete flow models are IResNet (Behrmann et al., 2019) and IGNN (Xu et al., 2022), which replaces the expensive spectral normalization in IResNet with Wasserstein-2 regularization to promote smoothness. The last diffusion model is the score-based generative modeling based on neural stochastic differential equation (Song et al., 2021), which we call it ScoreSDE for comparison. We are primarily interested in two types of criteria. The first is the computational efficiency in terms of the number of iterations (e.g., batches that the model uses in training) and training time per iteration. Due to the block-wise training scheme,\n\n(a) True data τ : 2.06e-4, MMD[m]: 3.52e-4 τ : 2.96e-4, MMD[c]: 8.87e-4\n\nJKO-iFlow\n\n(b) FFJORD 4.10e-4 8.57e-4\n\n(c) OT-Flow 7.25e-4 1.32e-3\n\n(d) W2 IResNet 3.34e-3 3.39e-3\n\n(e) ScoreSDE 5.16e-4 1.09e-3\n\n(f) Fractal tree\n\n(g) Olympic rings\n\n(h) Checkerboard\n\nFigure 3: Two-dimensional simulated datasets. The generated samples ˆX by JKO-iFlow in (a) are closer to the true data X than competitors in (b)-(e). Under the more carefully selected bandwidth via the sample-median technique, MMD[m] in (20) by JKO-iFlow is also closer to the threshold τ in (21) than others. (f)-(h) visualizes generation by JKO-iFlow on more examples.\n\nTable 1: Inversion error Ex∼pX ∥T −1 (Tθ(x)) − x∥2 of JKO-iFlow computed from sample average on test data, where Tθ denotes the transport mapping over all the blocks of the trained flow network.\n\nθ\n\nPOWER 1.48e-5\n\nGAS MINIBOONE BSD300 1.53e-5\n\n1.09e-6\n\n1.58e-6\n\nRose 3.30e-6\n\nFractal tree Olympic rings Checkerboard\n\n3.58e-5\n\n2.24e-6\n\n3.07e-5\n\n7\n\nW2Block123p0=pXp1p2p3pZW2Block134256p0=pXpZp1p2p3p4p5p6Probability Trajectory MovementW2f6f5f4f3f2f1f1f2f3Under review as a conference paper at ICLR 2023\n\n(a) True X|Y τ : 2.78e-3, MMD[m]: 2.29e-2 τ : 3.90e-3, MMD[c]: 3.65e-2\n\nJKO-iFlow\n\n(b) IGNN 2.60e-2 3.11e-2\n\n(c) True X|Y JKO-iFlow τ : 1.30e-2, MMD[m]: 3.65e-2 τ : 1.69e-2, MMD[c]: 7.50e-2\n\n(d) IGNN 1.09e-1 1.53e-1\n\nFigure 4: Conditional graph node feature generation by JKO-iFlow and iGNN. We visualize the conditionally generated samples upon projecting down to the first two principal components determined by true X|Y . We visualize generation at two different values of Y .\n\nTable 2: Numerical metrics on high-dimensional real datasets. All competitors are trained after 10 times more iterations (i.e., batches), because their performance under the same number of iterations is not comparable to JKO-iFlow. Complete results are shown in Table A.1.\n\nData Set Model\n\nPOWER d = 6\n\nGAS d = 8\n\nJKO-iFlow OT-Flow FFJORD W2 IResNet IResNet ScoreSDE\n\nJKO-iFlow OT-Flow FFJORD W2 IResNet IResNet ScoreSDE\n\n76K 76K 76K 304K 304K 76K\n\n# Param Test MMD[m] Test MMD[c] τ : 2.84e-4 τ : 1.75e-4 1.26e-3 8.21e-4 9.62e-4 5.69e-4 1.98e-3 1.38e-3 2.72e-3 2.76e-3 2.49e-2 4.50e-3 6.56e-3 1.34e-3 τ : 2.83e-4 τ : 1.93e-4 1.79e-3 5.96e-4 3.64e-3 1.51e-3 6.09e-3 3.62e-3 1.50e-2 7.14e-3 2.72e-2 3.26e-3 1.45e-3 1.31e-3\n\n76K 76K 76K 304K 304K 76K\n\nData Set\n\nModel\n\nMINIBOONE d = 43\n\nBSDS300 d = 63\n\nJKO-iFlow OT-Flow FFJORD W2 IResNet IResNet ScoreSDE\n\nJKO-iFlow OT-Flow W2 IResNet IResNet ScoreSDE\n\n112K 112K 112K 448K 448K 112K\n\n# Param Test MMD[m] Test MMD[c] τ : 6.87e-4 τ : 4.59e-4 1.01e-3 7.97e-4 1.01e-3 1.23e-3 1.04e-3 5.47e-3 1.03e-3 1.27e-2 1.04e-3 2.58e-3 1.03e-3 4.29e-3 τ : 9.63e-5 τ : 1.35e-4 3.03e-3 4.83e-3 8.44e-2 8.55e-2 6.88e-1 5.52e-1 5.95e-1 5.42e-1 6.62e-1 5.51e-1\n\n396K 396K 990K 990K 396K\n\nwe measure the number of iterations for JKO-iFlow as the sum of iterations over all blocks. Using this metric allows us to examine performance easily across models, under a fixed-budget framework in terms of batches available to the model. The second is the maximum mean discrepancy (MMD) comparison (Gretton et al., 2012; Onken et al., 2021), which is a way of measuring the difference between two distributions based on samples. Additional details for MMD appear in Appendix C.4. We also report negative log-likelihood as an additional metric in Table A.1.\n\nConditional Generation. Due to the increasing need for conditional generation, we also apply JKO-iFlow for conditional generation: generate samples based on the conditional distribution X|Y . Most existing conditional generative methods treat Y as an additional input of the generator, leading to potential training difficulties. Instead, we follow the IGNN approach (Xu et al., 2022), which also incurs minimal changes to our training. Additional details are in Appendix C.5.\n\n5.2 RESULTS\n\nTwo-dimensional toy data. Figures 3a—3d compare JKO-iFlow with the competitors on nonconditional generation, where the subcaption indicates MMD values (20) under both bandwidths and the corresponding thresholds τ in (21). We omit showing IResNet with spectral normalization as it yields similar results as W2 IResNet. The generative quality by JKO-iFlow is the closest to that of the ground truth, and when the MMD bandwidth is more carefully selected via the sample-median technique, JKO-iFlow also yields smaller MMD than others. Meanwhile, Figures 3f–3h shows the satisfactory generative performance by JKO-iFlow on other examples. In Appendix, Figure A.2 compares the performance of JKO-iFlow before and after using the technique described in Section 4.2, where the generative quality by JKO-iFlow improves after several reparametrization moving iterations, and Figure A.4 shows additional unconditional and conditional generation results.\n\nHigh-dimensional tabular data. In terms of conditional graph node feature generation, Figure 4 compares JKO-iFlow with IGNN on the solar dataset introduced in iGNN. The results show that JKO-iFlow yields competitive or clearly better MMD values on the conditional distribution X|Y with the most or second most observations, respectively. Next, Table 2 assesses the performance of JKO-iFlow and competitors on four high-dimensional real datasets, where JKO-iFlow still has the best overall performance. Dataset details are described in Appendix C.3. To ensure a fair comparison, we keep the number of parameters for continuous flow models and the diffusion model the same and properly increase model sizes for discrete flow models. FFJORD results on BSDS300 are omitted due to incomparably longer training time. In terms of results, except on POWER, JKO-iFlow yields smaller or very similar MMD under both bandwidths than all other methods. Table A.1 in Appendix\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(b) Results at moving iteration 1.\n\n(a) Components of loss (8) over moving iterations.\n\n(c) Results at moving iteration 5.\n\nFigure 5: MINIBOONE, reparametrization moving iterations of JKO-iFlow. We plot different components of the loss objective (8) over tk. In (a), results at moving iteration 5 are obtained by using Algorithm 2 (modified for training flow model) 4 times, and the reparametrization gives more uniform W2 losses after moving iterations. On this example, the generative performance are both good before and after the moving iterations, cf. plots (b) and (c).\n\nFigure 6: MNIST generation by JKO-iFlow coupled with a pre-trained auto-encoder.\n\nC shows the complete results, including the number of training iterations and test log-likelihood. Overall, we remark that comparisons using MMD[m] (i.e., MMD with bandwidth selected using the sample-median technique) best align with visual comparisons in Figure A.1 of Appendix C.6, so that we suggest MMD[m] as a more reliable metric out of others we used. Furthermore, we illustrate the reparametrization technique on MINIBOONE in Figure 5, where the benefit appears in yielding a flow trajectory with more uniform movement under a competitive generative performance.\n\nMNIST. We illustrate the generative quality of JKO-iFlow using an AutoEncoder. Consider a pretrained encoder Enc : R784 → Rd and decoder Dec : Rd → R784 such that Dec(Enc(X)) ≈ X for a flattened image X. We choose d = 16. The encoder (resp. decoder) uses one fully-connected layer followed by the ReLU (resp. Sigmoid) activation. Then, JKO-iFlow is trained on N encoded images {Enc(Xi)}N i=1, and the trained model gives an invertible transport mapping (over all residual blocks) Tθ : Rd → Rd. The images are generated upon sampling noises Z ∼ N (0, Id) through the backward flow followed by the decoder, namely Dec(T −1 (Z)). The generated images are shown in Figure 6.\n\nθ\n\n6 DISCUSSION\n\nThe work can be extended in several directions. The application to larger-scale image dataset by adopting convolutional layers will further verify the usefulness of the proposed method. The applications to generative tasks on graph data, by incorporating graph neural network layers in JKO-iFlow model, are also of interest. This also includes conditional generative tasks, of which the first results on toy data are shown in this work. For the methodology, the time-continuity over the parametrization of the residual blocks (as a result of the smoothness of the Fokker-Planck flow) have not been exploited in this work, which may further improve model capacity as well as learning efficiency. Theoretically, the model expressiveness of flow model to generate any regular data distribution can be analyzed based on Section 3.3. To sketch a road-map, a block-wise approximation guarantee of f (x, t) as in (12) can lead to approximation of the Fokker-Planck flow (3), which pushes forward the density to be ε-close to normality in T = log(1/ε) time, cf. (4). Reversing the time of the ODE then leads to an approximation of the initial density ρ0 = pX by flowing backward in time from T to zero. Further analysis under technical assumptions is left to future work.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDavid Alvarez-Melis, Yair Schiff, and Youssef Mroueh. Optimizing functionals on the space of\n\nprobabilities with input convex neural networks. arXiv preprint arXiv:2106.00774, 2021.\n\nBrandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Conference\n\non Machine Learning, pp. 146–155. PMLR, 2017.\n\nBolton Bailey and Matus J Telgarsky. Size-noise tradeoffs in generative networks. Advances in\n\nNeural Information Processing Systems, 31, 2018.\n\nAndrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE\n\nTransactions on Information theory, 39(3):930–945, 1993.\n\nJens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and J ̈orn-Henrik Jacobsen. Invertible residual networks. In International Conference on Machine Learning, pp. 573–582. PMLR, 2019.\n\nAdam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-\n\nencoders and langevin sampling. arXiv preprint arXiv:2002.00107, 2020.\n\nFranc ̧ois Bolley, Ivan Gentil, and Arnaud Guillin. Convergence to equilibrium in wasserstein distance\n\nfor fokker–planck equations. Journal of Functional Analysis, 263(8):2430–2457, 2012.\n\nCharlotte Bunne, Laetitia Papaxanthos, Andreas Krause, and Marco Cuturi. Proximal optimal transport modeling of population dynamics. In International Conference on Artificial Intelligence and Statistics, pp. 6511–6528. PMLR, 2022.\n\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\n\ndifferential equations. Advances in neural information processing systems, 31, 2018.\n\nRicky TQ Chen, Jens Behrmann, David K Duvenaud, and J ̈orn-Henrik Jacobsen. Residual flows for invertible generative modeling. Advances in Neural Information Processing Systems, 32, 2019.\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. ArXiv,\n\nabs/1605.08803, 2017.\n\nJiaojiao Fan, Amirhossein Taghvaei, and Yongxin Chen. Variational wasserstein gradient flow. arXiv\n\npreprint arXiv:2112.02424, 2021.\n\nMatthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In\n\nICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nChris Finlay, J ̈orn-Henrik Jacobsen, Levon Nurbekyan, and Adam Oberman. How to train your neural ode: the world of jacobian and kinetic regularization. In International conference on machine learning, pp. 3154–3164. PMLR, 2020.\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\n\nAaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.\n\nWill Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Kristjanson Duvenaud. Ffjord: Free-form continuous dynamics for scalable reversible generative models. ArXiv, abs/1810.01367, 2019.\n\nArthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch ̈olkopf, and Alex Smola. A\n\nkernel two-sample test. J. Mach. Learn. Res., 13:723–773, 2012.\n\nIshaan Gulrajani, Faruk Ahmed, Mart ́ın Arjovsky, Vincent Dumoulin, and Aaron C. Courville.\n\nImproved training of wasserstein gans. In NIPS, 2017.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\n\nNeural Information Processing Systems, 33:6840–6851, 2020.\n\nChin-Wei Huang, Jae Hyun Lim, and Aaron C Courville. A variational perspective on diffusion-based generative models and score matching. Advances in Neural Information Processing Systems, 34: 22863–22876, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMichael F. Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics - Simulation and Computation, 18:1059–1076, 1989.\n\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5967–5976, 2017.\n\nRie Johnson and Tong Zhang. A framework of composite functional gradient methods for generative adversarial models. IEEE transactions on pattern analysis and machine intelligence, 43(1):17–32, 2019.\n\nRichard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker–planck\n\nequation. SIAM journal on mathematical analysis, 29(1):1–17, 1998.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2014.\n\nDiederik P. Kingma and Max Welling. An introduction to variational autoencoders. Foundations and\n\nTrends® in Machine Learning, 12(4):307–392, 2019. doi: 10.1561/2200000056.\n\nIvan Kobyzev, Simon JD Prince, and Marcus A Brubaker. Normalizing flows: An introduction and review of current methods. IEEE transactions on pattern analysis and machine intelligence, 43 (11):3964–3979, 2020.\n\nAlexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, and Evgeny Burnaev. Wasserstein-2 generative networks. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=bEoxzW_EXsa.\n\nF Shampine Lawrence. Some practical runge-kutta formulas. Mathematics of Computation, 46:\n\n135–150, 1986.\n\nHolden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural nets\n\nto express distributions. In Conference on Learning Theory, pp. 1271–1296. PMLR, 2017.\n\nJenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows.\n\nAdvances in Neural Information Processing Systems, 32, 2019.\n\nQiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference\n\nalgorithm. Advances in neural information processing systems, 29, 2016.\n\nAntoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert St ̈oter. Sliced-wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In International Conference on Machine Learning, pp. 4104–4113. PMLR, 2019.\n\nYulong Lu and Jianfeng Lu. A universal approximation theorem of deep neural networks for expressing probability distributions. Advances in neural information processing systems, 33: 3094–3105, 2020.\n\nJames Lucas, G. Tucker, Roger B. Grosse, and Mohammad Norouzi. Understanding posterior collapse\n\nin generative latent variable models. In DGS@ICLR, 2019.\n\nEmile Mathieu and Maximilian Nickel. Riemannian continuous normalizing flows. Advances in\n\nNeural Information Processing Systems, 33:2503–2515, 2020.\n\nPetr Mokrov, Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, and Evgeny Burnaev. Large-scale wasserstein gradient flows. Advances in Neural Information Processing Systems, 34:15243–15256, 2021.\n\nDerek Onken, S Wu Fung, Xingjian Li, and Lars Ruthotto. Ot-flow: Fast and accurate continuous normalizing flows via optimal transport. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, 2021.\n\nGeorge Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density\n\nestimation. Advances in neural information processing systems, 30, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019.\n\nDmytro Perekrestenko, Stephan M ̈uller, and Helmut B ̈olcskei. Constructive universal highdimensional distribution generation through deep relu networks. In International Conference on Machine Learning, pp. 7610–7619. PMLR, 2020.\n\nDmytro Perekrestenko, L ́eandre Eberhard, and Helmut B ̈olcskei. High-dimensional distribution generation through deep neural networks. Partial Differential Equations and Applications, 2(5): 1–44, 2021.\n\nFredrik Ronquist, Maxim Teslenko, Paul van der Mark, Daniel L. Ayres, Aaron E. Darling, Sebastian H ̈ohna, Bret R. Larget, Liang Liu, Marc A. Suchard, and John P. Huelsenbeck. Mrbayes 3.2: Efficient bayesian phylogenetic inference and model choice across a large model space. Systematic Biology, 61:539 – 542, 2012.\n\nLitu Rout, Alexander Korotin, and Evgeny Burnaev. Generative modeling with optimal transport maps. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=5JdLZg346Lw.\n\nTim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\n\nImproved techniques for training gans. ArXiv, abs/1606.03498, 2016.\n\nThomas C. Sideris. Ordinary differential equations and dynamical systems. 2013.\n\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\n\nAdvances in Neural Information Processing Systems, 32, 2019.\n\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.\n\nCharles Stein. A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In Proceedings of the sixth Berkeley symposium on mathematical statistics and probability, volume 2: Probability theory, volume 6, pp. 583–603. University of California Press, 1972.\n\nEsteban G Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood.\n\nCommunications in Mathematical Sciences, 8(1):217–233, 2010.\n\nBelinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent gaussian\n\nmodels in the diffusion limit. arXiv preprint arXiv:1905.09883, 2019a.\n\nBelinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative models with latent diffusions. In Conference on Learning Theory, pp. 3084–3114. PMLR, 2019b.\n\nChen Xu, Xiuyuan Cheng, and Yao Xie. Invertible neural networks for graph prediction. arXiv\n\npreprint arXiv:2206.01163, 2022.\n\nDmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:\n\n103–114, 2017.\n\nLinfeng Zhang, Lei Wang, et al. Monge-amp\\ere flow for generative modeling. arXiv preprint\n\narXiv:1809.10188, 2018.\n\nQinsheng Zhang and Yongxin Chen. Diffusion normalizing flow. Advances in Neural Information\n\nProcessing Systems, 34:16280–16291, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS\n\nA.1 PROOFS IN SECTION\n\nLemma A.1. Suppose p and q are two densities on Rd in P, the following two problems\n\nmin ρ∈P\n\nLρ[ρ] = KL(ρ||q) +\n\n1 2h\n\nW 2\n\n2 (p, ρ),\n\nLT [T ] = KL(T#p||q) +\n\nmin T :Rd→Rd have the same minimum, and (a) If T ∗ : Rd → Rd is a minimizer of (14), then ρ∗ = (T ∗)#p is a minimizer of (13). (b) If ρ∗ is a minimizer of (13), then the optimal transport from p to ρ∗ minimizes (14).\n\nEx∼p∥x − T (x)∥2,\n\n1 2h\n\n(13)\n\n(14)\n\nProof of Lemma A.1. Let the minimum of (14) be L∗\n\nT , and that of (13) be L∗ ρ.\n\nρ. We claim that L∗\n\nT = LT [T ∗] = Lρ[ρ∗] ≥ L∗\n\nProof of (a): Suppose LT achieves minimum at T ∗, then T ∗ is the optimal transport from p to ρ∗ = (T ∗)#p because otherwise LT can be further improved. By definition of Lρ, we have ρ. Otherwise, there is another ρ′ such L∗ T = L∗ that Lρ[ρ′] < L∗ T . Let T ′ be the optimal transport from p to ρ′, and then LT [T ′] = Lρ[ρ′] < L∗ T , contradicting with that L∗ ρ, that is, ρ∗ is a minimizer of Lρ. Proof of (b): Suppose Lρ achieves minimum at ρ∗. Let T ∗ be the OT from p to ρ∗, then Ex∼p|x − T ∗(x)|2 = W2(p, ρ∗)2, and then LT [T ∗] = Lρ[ρ∗] = L∗ T as proved in (a). This shows that T ∗ is a minimizer of LT .\n\nT is the minimum of LT . This also shows that Lρ[ρ∗] = L∗\n\nρ which equals L∗\n\nT = L∗\n\nProof of Proposition 3.1, Given pk being the density of x(t) at t = kh, recall that T is the solution map from x(t) to x(t + h). We denote ρt := pk, and ρt+h := T#pk. By definition,\n\nKL(T#pk||pZ) = Ex∼ρt+h (log ρt+h(x) − log pZ(x)).\n\n(15)\n\nBecause pZ ∝ e−V , V (x) = |x|2/2, we have log pZ(x) = −V (x) + c1 for some constant c1. Thus Ex∼ρt+h log pZ(x) = Ex(t)∼ρt log pZ(x(t + h)) = c1 − Ex(t)∼ρtV (x(t + h)).\n\n(16)\n\nTo compute the first term in (15), note that\n\nEx∼ρt+h log ρt+h(x) = Ex(t)∼ρt log ρt+h(x(t + h)),\n\n(17)\n\nand by the expression (called “instantaneous change-of-variable formula” in normalizing flow literature (Chen et al., 2018), which we derive directly in below)\n\nd dt\n\nlog ρ(x(t), t) = −∇ · f (x(t), t),\n\n(18)\n\nwe have that for each value of x(t),\n\nlog ρt+h(x(t + h)) = log ρ(x(t + h), t + h) = log ρ(x(t), t) −\n\n(cid:90) t+h\n\nt\n\n∇ · f (x(s), s)ds.\n\nInserting back to (17), we have\n\nEx∼ρt+h log ρt+h(x) = Ex(t)∼ρt log ρt(x(t)) − Ex(t)∼ρt\n\n(cid:90) t+h\n\nt\n\n∇ · f (x(s), s)ds).\n\nThe first term is determined by ρt = pk, and thus is a constant c2 independent from f (x, t) on t ∈ [kh, (k + 1)h]. Together with (16), we have shown that\n\nr.h.s. of (15) = c2 − Ex(t)∼ρt\n\n(cid:90) t+h\n\nt\n\nwhich proves (7).\n\n∇ · f (x(s), s)ds) − c1 + Ex(t)∼ρtV (x(t + h)),\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nDerivation of (18): by chain rule,\n\nd dt\n\nlog ρ(x(t), t) =\n\n∇ρ(x(t), t) · ̇x(t) + ∂tρ(x(t), t) ρ(x(t), t)\n\n=\n\n∇ρ · f − ∇ · (ρf ) ρ\n\n(cid:12) (cid:12) (cid:12) (cid:12)(x(t),t)\n\n= −∇ · f (x(t), t).\n\n(by (1) and (2))\n\nB TECHNICAL DETAILS OF SECTION 4.2\n\nAlthough the layer-wise training formulation in Section 4.1 enjoys several aforementioned benefits, there exists undesirable movement patterns along the trajectory. Empirically, the movement by initial blocks fθk is much larger than later ones. The blue curve labeled “Phase 1” in Figure A.2a visualizes one typical pattern of the movement measured by W2 distances.\n\nIn fact, this phenomenon is not specific to training flow networks by the JKO scheme. It essentially arises due to smaller gradient magnitude at later estimates, which gradually approach a local minimum during optimization. In particular, such irregular movement also appears in gradient descent in vector space. We thus propose a reparametrize-and-refine technique.\n\n1. Vector-space case\n\nWe first motivate our method with optimization in vector space. Suppose our goal is to find a local minimum x∗ of F (x) for a nonlinear differentiable function F : Rd → R. Starting at x(0), consider the following sequential optimization problem, where x(t) denotes the estimate at the t-th iteration and ht is a pre-specified regularization parameter:\n\nx(t+1) = arg min\n\nx\n\nF (x) +\n\n1 2ht\n\n∥x − x(t)∥2 2.\n\n(19)\n\nUsing the first order Taylor expansion F (x) ≈ F (x(t)) + ∇F (x(t))T (x − x(t)) at x(t), we get\n\nx(t+1) = x(t) − htgt, gt := ∇xF (x(t)).\n\nDefine the arc length of iterates St := ∥x(t+1) − x(t)∥2 = ht∥gt∥2, whereby it appears in practice that the magnitude of St is near zero as x(t) → x∗. This issue is typical as a result of small gradient as estimates approach the local minimum. We thus propose Algorithm 2 to resolve this uneven arc length issue, which takes in iterates x(t,old) and step sizes hold\n\nfrom the previous trajectory.\n\nt\n\nt\n\n∥gnew t\n\nWe first motivate and explain the reparametrization step in line 3. Mathematically, we want arc ≈ ̄S. This is equivalent to lengths defined using re-optimized values x(t,new) to satisfy Snew requiring hnew ∥2 is unknown before t /x(t,old). In practice, re-optimization takes place, so that we approximate it using ∥gold using the quantity ̄Shold tend to cause non-smooth trajectories and inaccurate final estimates. We thus introduce inertia controlled by parameter η and upper bound the largest ht by hmax to allow more flexibility.\n\nalone to update ht can be undesirable, because larger hnew\n\nt ). The quantity ∥gnew t ∥2 = Sold\n\n∥2 ≈ ̄S, where gnew\n\n:= ∇F (xnew\n\nt /Sold\n\nt\n\nt\n\nt\n\nt\n\nt\n\nWe now explain the refinement step in line 4. We interpolate C ≥ 0 intermediate points between each pair of (x(t,new), x(t+1,new)). For instance, if C = 1, we optimize for the “mid-point” x(t+1/2,new) before reaching x(t+1,new). Using this approach ensures smoother new trajectories {x(t,new)}t≥1 and potentially more accurate final estimate. Figure A.3 illustrates the behavior and our solution on minimizing the Muller-Brown energy potential in R2.\n\n2. JKO Flownet reparametrization\n\nAlthough Algorithm 2 is developed for re-parametrizing and refining trajectories in vector space Rd, it can be directly used to reparametrize h for JKO-iFlow by replacing the arc length St between consecutive iterates in vector space with the W2 movement in probability space of the residual\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Trajectory improvement (vector-space case)\n\nRequire: Penalty factors hold\n\nand iterates x(t,old) for t = 1, . . . , T . Hyper-parameters hmax > 0\n\nt\n\nand η ∈ (0, 1]. 1: Compute Sold 2: for t = 1, . . . , T ′ do Compute hnew 3: For C ≥ 1, store ˆhnew\n\nt\n\nt\n\n:= ∥x(t+1,old) − x(t,old)∥2 and ̄S := (cid:80)N\n\nt=1 Sold\n\nt /T .\n\n:= min{hold\n\nt + η( ̄Shold\n\nt /Sold t − hold /(C + 1), . . . , hnew\n\n:= [hnew\n\nt\n\nt\n\n), hmax}. {▷ Reparametrize} /(C + 1)] for C + 1 times. {▷ Refine}\n\n4: 5: end for 6: Re-optimize (19) for x(t,new) with {ˆhnew t\n7: Repeat all steps above until std({Snew\n\nt\n\nt\n\n}∪∞.\n\n})/mean({Snew\n\nt\n\nt\n\n}) is small enough.\n\nblock. More precisely, let L be the total number of trained blocks via Algorithm 1 and denote hk := tk+1 − tk as the “step-size” for block fθk . Replace the iterates x(t) in vector space with x(tk), which is the mapping through previous k − 1 blocks. Then, the arc length St becomes the W2 distance, which can be easily computed using N samples {xi(tk)}N i=1 along each step of the trajectory. The refinement step thus becomes training additional residual blocks via optimizing (8).\n\nC EXPERIMENTAL DETAILS\n\nC.1 CHOICE OF tk IN ALGORITHM 1\n\nRecall that to train our JKO-iFlow, one needs as input a sequence of tk, where the k-th JKO block integrates from tk to tk+1. Although the selection of tk varies by problem, we consider two choices in our settings.\n\n• Constant increment. Denote hk := tk+1 − tk, We let hk ≡ c1 for a constant c1 > 0. On many experiments for two-dimensional toy data and high-dimensional data, we use c1 = 1. • Constant multiplier. Given t0 > 0 and a constant c2, we let tk+1 := c2tk. The rationale is that from empirical evidence, the W2 movement as in (6) tends to be larger at initial blocks than at latter blocks, so that moving later blocks more than the initial ones would enable more uniform movements, thus faciliating the training process. On some experiments for two-dimensional toy data and high-dimensional data, we let t0 = 0.75 and c2 = 1.2.\n\nWe acknowledge that many other choices are possible. We also want to emphasize that due to the reparametrization and refinement techniques proposed in Section 4.2, the values of tk would be adaptively updated based on data, where the adaptive values would yield more uniform W2 movements over blocks as we saw in Section 5.\n\nC.2 OTHER SETUP DETAILS\n\nAll experiments are conducted using PyTorch (Paszke et al., 2019) and PyTorch Geometric (Fey & Lenssen, 2019) . Regarding network architecture\n\n• For simulated 2D data, high-dimensional real data, and MNIST using pre-trained autoencoder: each residual block uses fully-connected layers of the form d → H → H → d, where d (resp. H) is the feature (resp. hidden nodes’) dimension. The hidden dimension vary by example, in the range of 128∼512.\n\n• For conditional graph node feature generation: each residual block uses one Chebnet input layer of order 3 followed by two fully-connected layers. The hidden dimension H = 64 in all hidden layers.\n\nThe activation function is chosen as Tanh or Softplus with β = 20. In addition, we use Rugge-Kutta 4 (Lawrence, 1986) to numerically estimate the integrals in the continuous flow model. The Adam optimizer is used with constant learning rate of 1e − 3 throughout training. Regarding batch sizes, we use 1000 samples for simulated two-dimensional data and 50% of the training samples for the solar graph data. The batch size selections for high-dimensional real data are described in Table A.1.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nC.3 DATASET\n\nFor two-dimensional simulated examples, we generate fresh random draws of 10000 training samples at each training epoch. The four high-dimensional real datasets (POWER, GAS, HEP- MASS, MINIBOONE) come from the University of California Irvine (UCI) machine learning data repository. These datasets are commonly used to compare flow models (Grathwohl et al., 2019; Finlay et al., 2020; Onken et al., 2021). The solar dataset as used in iGNN (Xu et al., 2022) is retrieved from the National Solar Radiation Database (NSRDB).\n\nC.4 MMD METRICS\n\nBesides visual comparison, the maximum mean discrepancy (MMD) (Gretton et al., 2012; Onken et al., 2021) provides a quantitative way to evaluate the performance of generative models. Given samples X := {xi}N\n\nj=1 and a kernel function k(x, y), we compute\n\ni=1 and Y := {yj}M\n\nMMD(X, Y ) :=\n\n1 N 2\n\nN (cid:88)\n\nN (cid:88)\n\ni=1\n\nj=1\n\nk(xi, xj) +\n\n1 M 2\n\nM (cid:88)\n\nM (cid:88)\n\ni=1\n\nj=1\n\nk(yi, yj) −\n\n2 N M\n\nN (cid:88)\n\nM (cid:88)\n\ni=1\n\nj=1\n\nk(xi, yj).\n\n(20)\n\nFor our purpose, we use the Gaussian kernel k(x, y) := exp(cid:0)−∥x − y∥2/h(cid:1) with bandwidth h. We select the bandwidth both as a constant value hc = 2 and via the “sample-median technique” (Gretton et al., 2012) hm := 2median({∥xi − xj∥2}i,j), where xi are test samples. We use the same set of 200 test samples to compute hm in each experiment. We thus denote MMD[c] (resp. MMD[m]) as evaluating (20) using the Gaussian kernel with constant (resp. median) bandwidth, where X (resp. Y ) denotes true (resp. generated) test sample. Note that a low MMD value indicates two samples X and Y are likely drawn from the same distribution (Gretton et al., 2012). In this setting, MMD is an impartial evaluation metric as it is not used to train JKO-iFlow or any competing methods.\n\nWe can also determine the statistical significance of a MMD value. First, compute the threshold\n\n1], X[I b\n\nτ := Q1−α({MMD(X[I b\n\n(21) where Q1−α denotes the upper 1 − α quantile of a set of scalars and I b j ⊂ {1, . . . , N } denotes the j-th index set at the b-th bootstrapping without replacement. Then, under the null hypothesis that X and Y are drawn from the same distribution, this hypothesis is rejected if MMD exceeds the threshold τ . The Type-I error is controlled at level α. Thus, if the MMD values by two models both exceed τ , we prefer the model with the smaller MMD. If both values are under τ , then they generate equally well. In our experiments, we use B = 1000 bootstraps, each of which has 50% re-sampled test samples.\n\n2]}B\n\nb=1),\n\nC.5 CONDITIONAL GENERATION\n\nWe follow the conditional generation scheme as proposed in iGNN (Xu et al., 2022). More precisely, when the response variable Y is a categorical variable taking value in K classes, iGNN designs the target distribution as a Gaussian mixture model. Thus, instead of flowing from data density pX to noise density pZ, iGNN flows from the conditional data density pX|Y to pH|Y , where H|Y ∼ H(μY , σ2I). One can then minimize the negative log-likelihood − log pX|Y using logpH|Y and the change-ofvariable formula.\n\nTo use JKO-iFlow for conditional generation in this setting, we thus only need to modify the objective (8). Instead of using VZ based on Z ∼ N (0, Id), we would using VH|Y based on the Gaussian mixture H|Y ∼ H(μY , σ2I).\n\nC.6 ADDITIONAL RESULTS\n\nWe present complete results in addition to those in Section 5. In particular,\n\n• Table A.1 contains the complete numerical results of JKO-iFlow against competitors on highdimensional real datasets. For ScoreSDE, we use the implementation in (Huang et al., 2021), which computes the evidence lower bound (ELBO) for the data log-likelihood as reported in the last column. In addition, Table A.2 contains MMD and negative log-likelihood results for OT-Flow and FFJORD as taken from the original papers.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable A.1: Numerical metrics on high-dimensional real datasets, in addition to those Table 2. Comparing to flow-based models, JKO-iFlow takes much less iterations to reach a small enough MMD value. Although ScoreSDE is the fastest, its performance, even under 100 times more iteration than JKO-iFlow, is still worse in terms of MMD[m] on all except GAS. We advocate the comparison using MMD[m] because the results align with visual comparisons in Figure A.1.\n\nData Set\n\nModel\n\n# Param\n\nTraining\n\nPOWER d = 6\n\nGAS d = 8\n\nMINIBOONE d = 43\n\nBSDS300 d = 63\n\nJKO-iFlow OT-Flow FFJORD W2 IResNet IResNet ScoreSDE ScoreSDE\n\nJKO-iFlow OT-Flow FFJORD W2 IResNet IResNet ScoreSDE ScoreSDE\n\nJKO-iFlow OT-Flow FFJORD W2 IResNet IResNet ScoreSDE ScoreSDE\n\nJKO-iFlow OT-Flow W2 IResNet IResNet ScoreSDE ScoreSDE\n\n76K 76K 76K 304K 304K 76K 76K\n\n76K 76K 76K 304K 304K 76K 76K\n\n112K 112K 112K 448K 448K 112K 112K\n\n396K 396K 990K 990K 396K 396K\n\nTime (h)\n\n# Iter\n\n0.12 1.21 3.40 0.49 0.76 0.08 0.84\n\n0.08 0.72 3.49 0.57 0.86 0.04 0.42\n\n0.03 0.75 1.74 0.80 1.32 0.01 0.09\n\n0.16 3.50 2.01 3.47 0.01 0.14\n\n0.76K 7.58K 7.58K 7.58K 7.58K 7.58K 75.85K\n\n0.76K 7.60K 7.60K 7.60K 7.60K 7.60K 76.00K\n\n0.32K 3.39K 3.39K 3.25K 3.25K 3.25K 32.48K\n\n1.03K 10.29K 10.29K 10.29K 10.29K 102.90K\n\n0.57 0.57 1.61 0.23 0.36 0.04 0.04\n\n0.38 0.34 1.65 0.27 0.41 0.02 0.02\n\n5000 5000 5000 5000 5000 5000 5000\n\n10000 10000 10000 10000 10000 10000 10000\n\nTime/Iter (s) Batch size MMD[m] τ : 1.75e-4 8.21e-4 5.69e-4 1.38e-3 2.76e-3 4.50e-3 1.34e-3 1.30e-3 τ : 1.93e-4 5.96e-4 1.51e-3 3.62e-3 7.14e-3 3.26e-3 1.31e-3 4.27e-4 τ : 4.59e-4 7.97e-4 1.23e-3 5.47e-3 1.27e-2 2.58e-3 4.29e-3 4.68e-3 τ : 1.35e-4 4.83e-3 8.55e-2 5.52e-1 5.42e-1 5.51e-1 5.51e-1\n\n2000 2000 2000 2000 2000 2000 2000\n\n0.56 1.22 0.70 1.21 0.005 0.005\n\n0.33 0.80 1.85 0.89 1.46 0.01 0.01\n\n5000 1000 1000 1000 1000 1000\n\nTesting\n\nMMD[c] Neg Loglik\n\nτ : 2.84e-4 1.26e-3 9.62e-4 1.98e-3 2.72e-3 2.49e-2 6.56e-3 5.66e-3 τ : 2.83e-4 1.79e-3 3.64e-3 6.09e-3 1.50e-2 2.72e-2 1.45e-3 8.56e-4 τ : 6.87e-4 1.01e-3 1.01e-3 1.04e-3 1.03e-3 1.04e-3 1.03e-3 1.10e-3 τ : 9.63e-5 3.03e-3 8.44e-2 6.88e-1 5.95e-1 6.62e-1 6.65e-1\n\n0.58 0.30 0.60 0.36 3.37 3.41 3.33\n\n-4.61 -4.29 -2.07 -4.45 -1.17 -3.69 -5.58\n\n13.63 11.93 23.45 16.34 22.36 27.38 20.70\n\n-156.67 -142.45 -107.39 -33.11 -7.55 -7.31\n\nTable A.2: MMD[c] and negative loglikelihood results of OT-Flow and FFJORD, as taken from (Onken et al., 2021). We include them to compare against ours in Table A.1. The models in previous studies use comparable model size (especially for OT-Flow), where the numerical results in some cases are much smaller than ours due to significantly longer training time.\n\nData Set\n\nModel\n\n# Param\n\nPOWER d = 6\n\nGAS d = 8\n\nMINIBOONE d = 43\n\nBSDS300 d = 63\n\nOT-Flow 18K FFJORD 43K\n\nOT-Flow 127K FFJORD 279K\n\nOT-Flow 78K FFJORD 821K\n\nOT-Flow 297K FFJORD 6.7M\n\nTraining\n\nTesting\n\nTime (h)\n\n# Iter MMD[c] Neg Loglik\n\n3.1 68.9\n\n6.1 75.4\n\n0.8 9.0\n\n7.1 166.1\n\n22K 29K\n\n52K 49K\n\n7K 16K\n\n37K 18K\n\n4.68e-5 4.34e-5\n\n2.47e-4 1.02e-4\n\n2.84e-4 2.84e-4\n\n4.24e-4 6.52e-3\n\n-0.30 -0.37\n\n-9.20 -10.69\n\n10.55 10.57\n\n-154.20 -133.96\n\n• Figure A.1 visualizes the principal component projections of the generated samples by\n\nJKO-iFlow and competitors of the high-dimensional real datasets.\n\n• Figure A.2 visualizes components of loss 8 and the resulting generated images. • Figure A.3 visualizes the trajectory of estimates in R2 of minimizing the Muller-Brown\n\nenergy potential.\n\n• Figure A.4 shows additional unconditional and conditional generated samples by JKO-iFlow\n\non toy data.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Power: JKO-iFlow\n\n(b) OT-Flow\n\n(c) FFJORD\n\n(d) W2 IResNet\n\n(e) ScoreSDE\n\n(f) Gas: JKO-iFlow\n\n(g) OT-Flow\n\n(h) FFJORD\n\n(i) W2 IResNet\n\n(j) ScoreSDE\n\n(k) MINIBOONE: JKO-iFlow\n\n(l) OT-Flow\n\n(m) FFJORD\n\n(n) IResNet\n\n(o) ScoreSDE\n\n(p) BSDS300: JKO-iFlow\n\n(q) OT-Flow\n\n(r) W2 IResNet\n\n(s) IResNet\n\n(t) ScoreSDE\n\nFigure A.1: Generative quality on high-dimensional datasets via PCA projection of generated samples. The generative quality in general aligns with the MMD[m] values shown in Table 2 and A.1.\n\n(b) Results at moving iteration 1.\n\n(a) Losses in moving iterations.\n\n(c) Results at moving iteration 4.\n\nFigure A.2: Rose, reparametrization moving iterations of JKO-iFlow. The plots and setup are identical to Figure 5. We observe improved generative quality after the moving iterations.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure A.3: Reparametrization and refinement moving iterations in vector space based on Algorithm 2. The task is to estimate a local minimizer of the Muller-Brown energy potential. We see that arc lengths between consecutive iterates become more even in magnitude over more reparametrization and refinement moving iterations.\n\n(a) Two-circles\n\n(b) Conditional generation illustration.\n\nFigure A.4: Additional unconditional and conditional generation on simulated toy datasets by JKO-iFlow.\n\n19",
    "reference": "# Summary Of The Paper\n\nThis current paper proposes a normalizing flow algorithm that implements the JKO scheme using neural ODE flow blocks. At each time interval, an optimization over a vector field parameterized as ResNet is done to obtain the velocity field that can be integrated to obtain samples at the next step. The normalizing flow is invertible and the objective functional of the JKO scheme is taken to be the KL divergence with respect to a standard Gaussian so that by inverting the flow we can generate the data distribution. The proposed method appears to be more efficient than the alternatives as demonstrated by the experiments.\n\n# Strength And Weaknesses\n\n## Strengths:\n* Optimizing the velocity field of the ODE for JKO instead of transport maps that are more common in the existing literature is novel.\n* The idea of using one block for each JKO step is interesting and it seems to have resulted in efficiency.\n\n## Weaknesses:\n* I found the novelty of the proposed method somewhat limited. The only difference from [Alvarez-Meliset et al. 2021], [Mokrov et al. 2021] is that instead of parameterizing the pushforward map as ICNN (note in either work it's not necessary to use ICNN; arbitrary networks can also be used, following the same reasoning at Lemma A.1), the current work uses neural ODE. I'm not convinced by the superiority of using neural ODE over a pushforward map, which the current work does not compare against. In my opinion, a more severe problem is not addressed: namely at step $k$ all these algorithms need to push initial samples by $k$ steps to obtain samples for the current iteration, which scales overall quadratically in $k$.\n* The section 2.2 is not new to my knowledge and references are missing. See a similar derivation in Theorem 3.1 of Liu et al. \"Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm\".\n* I appreciate that two heuristic enhancements are proposed in Section 3.2. However, at first glance, these two enhancements seem contradictory. On the one hand, trajectory reparameterization wants to remove blocks, but on the other hand, progressive refinement requires adding intermediate blocks for large time steps. It would be great to have some ablation studies illustrating the effectiveness of either enhancement. \n* Since the target application is generative modeling, and the overall idea follows diffusion models (namely mapping the data distribution to a standard Gaussian, and then inverting the process), I think the authors should compare with diffusion models as well. \n* The writing is a bit sloppy overall. There is almost no assumption stated for Lemma A.1 or Proposition 2.1. At the very least we probably need $p$ and $q$ to have a finite second moment along with differentiability assumptions. The proof of 2.1 does not seem novel to me. It is essentially a direct application of the instantaneous change of variable from [Chen et al. 2018] (which is by itself just rewriting the continuity equation). There is missing reference on this formula (see the equation under (17)), and moreover as written this formula is not correct. The derivative needs to be a total derivative, i.e., it should be $d/dt (\\log \\rho(x(t), t)) = -\\nabla \\cdot f(x(t), t)$. The sloppiness of the writing is also manifested in the many handwavy sentences in the main text. To give a few examples:\n    - Above (4), \"Under generic conditions\" --- what conditions?\n    - In the second paragraph of 2.1, \"The solution of (1) ... gives a one-to-one mapping\" this should only be true if the time interval is small enough, by Picard–Lindelöf theorem\n    - In the last paragraph of page 5, \"... it will also have bounded Lipschitz constant\". How is this true? There is no assumption on $\\rho_t$ having bounded the Lipschitz constant.\n    - At the end of page 5, \"The analysis is postponed here\", where more details could have been given\n* For the conditional generation experiments, more details will be helpful. I don't understand what it means by \"we evaluate $V$ for a Gaussian mixture $H | Y$\".\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity of the paper is okay, although more details and more precise language can further improve the clarity. The source code is given but I did not run it.\n\n# Summary Of The Review\n\nOverall I think the paper lacks novelty and needs more comparison with other methods (other JKO methods, diffusion models) to demonstrate its effectiveness. The writing can also be improved.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMULTI-LAYERED 3D GARMENTS ANIMATION\n\nAnonymous authors Paper under double-blind review\n\nFigure 1: We propose a new large-scale 3D garment animation dataset LAYERS, which improves over previous datasets by considering multi-layered 3D garments and more driving factors for garment animation, e.g., environmental wind, besides human body movements. In (a)-(d) we show the new and realistic challenges covered in LAYERS but are omitted in previous datasets.\n\nABSTRACT\n\nMost existing 3D garment animation datasets are restricted to human bodies with single-layered garments. Even though cases with upper shirts and lower pants are included, only a few overlap areas among such garment combinations exist. Moreover, they often regard human body movement as the only driving factor that causes garment animation. Approaches developed on top of these datasets thus tend to model garments as functions of human body parameters such as body shape and pose. While such treatment leads to promising performance on existing datasets, it leaves a gap between experimental environments and real scenarios, where a body can wear multiple layered garments and the corresponding garment dynamics can be affected by environmental factors and garment attributes. Consequently, existing approaches often struggle to generalize to multi-layered garments and realistic scenarios. To facilitate the advance of 3D garment animation toward handling more challenging cases, this paper presents a new large-scale synthetic dataset called LAYERS, covering 4,900 different combinations of multi-layered garments with 700k frames in total. The animation of these multi-layered garments follows the laws of physics and is affected by not only human body movements but also random environmental wind and garment attributes. To demonstrate the quality of LAYERS, we further propose a novel method, LayersNet, for 3D garment animation, which represents garments as unions of particles and subsequently adopts a neural network to animate garments via particle-based simulation. In this way, the interactions between different parts of one garment, different garments on the same body, and garments against various driving factors, can be naturally and uniformly handled via the interactions of particles. Through comprehensive experiments, LayersNet demonstrates superior performance in terms of animation accuracy and generality over baselines. The proposed dataset, LAYERS, as well as the proposed method, LayersNet, will be publicly available.\n\n1\n\nINTRODUCTION\n\n3D garment animation has been an active and important topic in computer graphics and machine learning, due to its great potential in various downstream tasks, including virtual reality, virtual tryon, gaming and film production. While this topic has been extensively studied in the past, generating realistic and faithful animation remains an open research question. In particular, existing approaches are still limited in modeling diverse garments of different topologies and appearances. In addition, the complex interactions between the garment and the human body under the challenging setting of multi-layer garments and with external environmental factors remain much less explored in the literature.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTo support the development of data-driven approaches for 3D garment animation (Patel et al., 2020; Bertiche et al., 2020; Santesteban et al., 2021), researchers have built various datasets on real-life scans and synthetic data generated by Physically Based Simulation (PBS) (Narain et al., 2012; Li et al., 2018). However, most of existing datasets (Bertiche et al., 2020; Patel et al., 2020; Tiwari et al., 2020) consider only human bodies with single-layered garments, where each human body wears either a single dress or an upper t-shirt with lower pants that have limited overlap. The animation of multi-layered garments, such as a t-shirt with a jacket, that obey sophisticated physical dynamics, remain unexplored. In addition, in existing datasets, the moving human body is commonly regarded as the default and only driving factor in animating garments. Other factors, such as wind and friction, are left unconsidered. Such a simplification thus leads to a significant gap between experimental environments and real-world applications, making most approaches developed on top of these datasets less applicable in real life.\n\nTo bridge the gap between experimental environments and real-world applications and facilitate the advance of 3D garment animation, this paper introduces a new challenging dataset called LAYERS, muLti-lAYerEd gaRmentS dataset, which is carefully generated based on a simulation engine. LAYERS focuses on the animation of multi-layered garments, while also taking the wind, another important driving factor besides the human body, into consideration. Specifically, in LAYERS, multilayered garments are prepared as combinations of inner and outer clothes, as shown in Figure 1. The inner and outer garments adopt different attribute values, e.g., bend stiffness and frictions. All garments on the same human body will interact with each other, constrained by the laws of physics. They are also simultaneously affected by the wind with randomly sampled direction and strength.\n\nTo demonstrate the quality of LAYERS, we further propose a novel data-driven method, dubbed as LayersNet, for multi-layered 3D garment animation. The core of LayersNet is a neural network based simulation system (Shao et al., 2022) that represents garments as unions of particles. Consequently, all kinds of interactions during garment animation, including the interactions between different parts of one garment, the interactions between different garments on the same body, and the interactions between garments and various driving factors, can be naturally and uniformly regarded as the interactions between particles. Hence, instead of being restricted to a specific driving factor (e.g., the human body) as previous methods, the proposed LayersNet possesses a strong generalization ability across diverse types of human body movements, multi-layered 3D garments, as well as driving factors. As the number of particles in LayersNet are considerably large when fine-grained details of garments and human bodies are preserved, we further exploit the redundancy of garments and extend LayersNet to establish a two-level structural hierarchy for garments where garments are made of patches, and patches are constituted of particles of a fixed configuration. Since the number of patches is much smaller than the number of particles, the interactions between all particles can be efficiently captured by the interactions of patches.\n\nOur contributions can be summarized as follows:\n\n1. We propose LAYERS, a large-scale and new dynamic dataset for 3D garment animation. The dataset focuses on multi-layered 3D garments, introducing random wind and friction as additional driving factors besides human body movements.\n\n2. On top of LAYERS, we further propose LayersNet, a novel method for 3D garment animation that uniformly captures interactions among garment parts, different garments, as well as garments against driving factors. The notion of unifying various interactions as particle-based simulations is novel in the literature.\n\n2 RELATED WORK\n\n3D Garment Datasets. Publicly available 3D garment datasets are in great need. Existing datasets are generated either from synthesis (Pumarola et al., 2019; Patel et al., 2020; Santesteban et al., 2021; Bertiche et al., 2020) or real-world scans (Zhang et al., 2017; Zheng et al., 2019; Ma et al., 2020; Tiwari et al., 2020; Cai et al., 2022). For synthetic datasets, 3DPeople (Pumarola et al., 2019) contains multi-view images including RGB, depth, normal, and scene flow data. TailorNet (Patel et al., 2020) provides a synthetic dataset with 20 different garments simulated in 1,782 static SMPL poses for nine body shapes. Santesteban et al. (2021) contributes a dataset composed of two different garments simulated on 56 human motion sequences with 17 body shapes. Cloth3D (Bertiche et al.,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2020) is the largest synthetic dataset, with 11,300 outfits generated from the combinations of several prototypes, such as t-shirts, tops, trousers and skirts. Most existing datasets only contain singlelayered 3D garment models. Even though there exist combinations with multiple garments, such as an upper t-shirt and lower pants in Multi-Garment Net Bhatnagar et al. (2019), there are very few overlapping areas among different cloth pieces. Recently, Layered-Garment Net Aggarwal et al. (2022) proposes a static multi-layered garments dataset in 7 static poses for 142 bodies to generate layers of outfits from single image. However, the garments in Layered-Garment Net, which are mostly skinning clothes, do not follow physics laws and the interpenetration is solved by simply forcing penetrated vertices out of inner garments.\n\nTo our best knowledge, LAYERS is the first dataset containing dynamic multi-layered 3D garments, e.g., the human model wears a dress and outer jacket which deform according to the body motions. Different layers of garments have different attributes and interact with each other, obeying the laws of physics. Moreover, we introduce wind as extra driving factor to animate the garments, enriching their dynamics given similar human movements. Our dataset includes all necessary 3D information, which is able to easily generalize to other tasks, such as reconstructions from single images.\n\nData-driven Cloth Model. Most existing approaches aim to estimate a function that outputs the deformations of garments for any input. A common strategy is to learn a parametric garment model to deform the corresponding mesh templates. For example, garments are modeled as functions of human pose (Wang et al., 2019), shape (Vidaurre et al., 2020), pose-and-shape (Bertiche et al., 2020; 2021; Tiwari & Bhowmick, 2021), motions (Santesteban et al., 2021), garment type (Ma et al., 2020; Patel et al., 2020). The approaches mentioned above rely heavily on SMPL-based human models and animate garments by the blend weight according to the registered templates. The generalization of such approaches is limited to skinning clothes. Some recent studies explore bone-driven motion networks (Pan et al., 2022) to animate loose garments by virtual bones, which can be regarded as extra anchors besides SMPL model. SCALE (Ma et al., 2021) adopts local elements to model registered garments based on minimum-clothed human. To handle obstacles with arbitrary topologies, N-Cloth (Li et al., 2022) predicts garments deformations given the states of initial garments and target obstacles. Other studies Shen et al. (2020); Zhang et al. (2022) generate 3D garments based on UV maps. SimulCap Yu et al. (2019) segments garments into upper and lower clothes as multiple separated meshes. SMPLicit Corona et al. (2021) generates garments by controlling the clothes’ shapes and styles, but intersection-free reconstruction is not guaranteed.\n\nIn contrast, our data-driven method LayersNet animates garments by inferring garments’ future positions through the interactions between garment particles and other driving factors. Since the driving factors are also represented by particles, the garment animation thus equals to simulate particle-wise interactions, which is shape-independent and is highly generalizable to unseen scenarios.\n\nPhysics Simulation by Neural Network. Learning-based methods for physics simulation can be applied to different kinds of representations, e.g., approaches for grid representation (Thuerey et al., 2020; Wang et al., 2020), meshes (Nash et al., 2020; Qiao et al., 2020; Weng et al., 2021; Pfaff et al., 2021), and particles (Li et al., 2019; Ummenhofer et al., 2020; Sanchez-Gonzalez et al., 2020; Shao et al., 2022). Some methods include Graph Neural Network (GNN)-based methods (Li et al., 2019; Sanchez-Gonzalez et al., 2020; Pfaff et al., 2021). Transformer-based methods (Shao et al., 2022) adopt modified attention to recover interactions’ semantics. Other work Liang et al. (2019) designs algorithms to accelerate gradient computation for collision response as plug-ins for neural network.\n\nOur LayersNet follows TIE in the notion of modeling particle-wise interactions, which is topologyindependent and easy to generalize to unseen scenarios. In contrast to TIE, we exploit the redundancy of garments and establish a two-level hierarchy structure for them, where garments are made of deformable patches. Our method also differs in learning to predict the patches’ dynamics by interacting with neighbor patches and other driving factors. We devise a decoder to learn a topologyindependent descriptor for each patch, enhancing the generalization abilities to unseen scenarios.\n\n3 LAYERS DATASET\n\nMost existing datasets are limited to single-layered garments driven only by human bodies. Different garments, such as the upper T-shirt and lower pants, rarely interact with each other. Consequently,\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: We compare LAYERS with existing 3D datasets. Our dataset is composed of multi-layered clothes, with unique attribute data, such as stiffness and friction, attached to each garment. Moreover, we include data of wind with its strength and direction randomly sampled. *1: 3DPeople (Pumarola et al., 2019) does not specify the exact number of garments, while it claims to dress each subject with different outfits. *2: The multi-layered garments in Layered-Garment do not follow physics laws and the penetrated vertices are forced to move out of inner garments in hard-coded manner.\n\nDataset\n\nDynamics Subjects Garments Multi-layered Attributes Wind\n\n3DPeople (Pumarola et al., 2019) TailorNet (Patel et al., 2020) Cloth3D (Bertiche et al., 2020) Layered-Garment (Aggarwal et al., 2022)\n\nLAYERS (Ours)\n\n80 9\n8.5K 142\n\n4.9K\n\n*1 20 11.3K 101\n\n9.9K\n\n✓\n\n✓\n\n*2 ✓\n\nN/A N/A 4\nN/A\n\n9.9K\n\n✓\n\nthe problem can be easily solved by modeling garments as functions of human bodies and considering only single-layered outfits predictions (Patel et al., 2020; Bertiche et al., 2021).\n\nGenerating a dataset with multi-layered garments is non-trivial – interpenetration between garments should be avoided, and their dynamics should obey the physics rules. Thanks to recent developments in physics-based methods, several software, such as Blender1, can infer the interactions among different clothes and generate faithful garments with multiple layers.\n\nThe proposed LAYERS is built with Blender. It is the first dynamic multi-layered garments dataset that considers the wind factor beyond just human bodies. To construct the dataset, we first collect the garment templates from SewPattern(Korosteleva & Lee, 2021), which includes various types of garments, such as jackets with hood, and dresses with waist belts. Then, we generate multi-layered combinations with outer-layer and inner-layer clothes. Each combination of multi-layered garments is then draped to SMPL human body (Loper et al., 2015). This is followed by a warm-up simulation in Blender to resolve interpenetrations. Finally, we simulate the dynamics of garments given the human motion sequences (Mahmood et al., 2019) and sampled winds. Specifically, after we drape the garments to a human body model, we scale up the human mesh and garment mesh ten times the real-world size before simulation. This strategy can reserve more high-frequency details in Blender. We compare our dataset with existing datasets in Table 1.\n\nSince our dataset includes the 3D meshes and attributes of garments, as well as the detailed scene settings for each sequence, LAYERS can be easily extended to other formats of data to facilitate the explorations of alternative topics, such as optical flow estimations, 3D reconstructions from images, and physics parameters estimations. In the following, we detail the key settings in LAYERS.\n\nMulti-layered Garments. Each multi-layered outfit is composed of inner and outer outfit. In LAYERS, the outer outfit is either a jacket or a jacket with a hood, giving us a clear view of interactions from inside and outside. Inner outfits refer to whole-body outfits, such as dresses, jumpsuits, and t-shirts with pants or skirts. We generate 4,900 combinations of multi-layered garments, with 9,872 different garments in total. The garment templates are in high fidelity, with vertices ranging from 5,000 to more than 15,000 for each garment, enabling us to capture more details in simulation.\n\nThe main challenge is to have interpenetration-free simulations for multiple objects. To achieve that, we first drape the multi-layered outfit to SMPL human body in T-pose, followed by a warm-up simulation to solve the interpenetrations among garments. We adopt a large collision distance to ensure all the interpenetrations are resolved. Afterwards, we merge the garments into one garment mesh and conduct a simulation driven by the human body and wind. Since all garments belong to one mesh after merging, the interactions among garments are computed through the self-collision mechanism in Blender, which generates interpenetration-free results in simulation.\n\nWind. Most existing datasets simplify real-life scenarios through driving the animation of garments only by human bodies. To enrich the settings and enable researchers to further explore garment animations driven by multiple factors, in LAYERS we introduce randomly sampled wind, a common and obvious force field to influence the animation. Specifically, we randomly select several spans of frames in a sequence, and apply winds of different directions and strengths as force fields. The directions and strengths are uniformly sampled (0 to 400 in Blender). Within each span,\n\n1https://www.blender.org/\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Overview of LayersNet. Given driving factors at time t + 1, i.e., the human body model and environmental wind in our study, LayersNet aims to animate target garments at time t and predicts the new states of garments at time t + 1. Human body model, wind, garments and corresponding attributes are represented by particles, where we adopt abstract particles to denote the wind and garments’ attributes. We further establish a two-level structural hierarchy for garments, as shown on the top left of the figure, where garments are made of patches, and patches are composed of particles of a fixed configuration given the UV mappings. Then we encode the particles and model the interactions among them by a simulator, which outputs the embeddings for each patch. Finally, we apply a decoder to decode the patches into corresponding vertices at time t + 1.\n\nwe make a reasonable assumption that the wind would affect the whole 3D space, where wind’s direction and strength remain constant.\n\nGarments’ Attributes. Existing datasets cover a limited choice for garment attributes, e.g., cotton or fabric. Tasks like physics parameter estimations could barely benefit from those datasets. To make garments animations more diverse and flexible, we uniformly sample different garments’ attributes, such as mass, stiffness, and friction. In addition, we introduce different attributes to the inner and outer outfits, leading to more varieties.\n\nHuman Motion Sequences. We adopt the SMPL-based human motion sequences from CMU MoCap in AMASS (Mahmood et al., 2019), which includes 2,600 sequences with 30FPS in total. During simulation, we randomly sample the human shapes and genders for each sequence and extract sub-sequences with a maximum of 600 frames. To accurately simulate garments on human body, collision-free human meshes are required to avoid invalid simulations. Since our dataset focuses on garments generations, we adopt linear regressions to solve the self-collisions from SMPL models (Loper et al., 2015) and leave a minimum gap of 0.004 meters before scaling up the human mesh. We skip the unresolvable collisions and discard the corresponding frames.\n\n4 METHODOLOGY\n\nOur goal is to faithfully animate the garments regardless of the garments’ topology and the type of driving factors, where the latter include the rigid human bodies and winds in our case. To this end, we propose LayersNet to animate garments in a simulation manner. The novelty of LayersNet is that we view garment animations driven by human bodies and winds as interactions between particles. This unified perspective allows our framework to exploit the semantics of interactions among all particles, e.g., the energy transition when constrained by physics laws. In addition, the animation of garments becomes shape-independent and highly generalizable. Figure 2 shows an overview of LayersNet. In the following, we first formulate garment animation in the form of particle simulation, followed by an explanation of our patch-based garment model, and the introduction of the simulation pipeline to animate garments.\n\n4.1 LAYERSNET\n\ni}N\n\ni, ̇xt\n\ni, ̈xt\n\nProblem Formulation. We denote each mesh at time t by M t = {V t, EM , EW }, where V t = i=1 are the vertices’ positions, velocities, and accelerations, and EM denote the mesh {xt edges. EW are the world space edges (Pfaff et al., 2021), where we dynamically connect node i and node j if |xt j| < R, excluding node pairs already exist in the mesh. In a particle-based system, each mesh is represented by particles, which are the corresponding vertices from the mesh. During simulation, particle i and particle j will interact with each other iff an edge eij ∈ EM ∪ EW\n\ni − xt\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nconnects them. The interactions guided by EM enable learning internal dynamics of mesh, while interactions indicated by EW serve to compute external dynamics such as collisions.\n\nWe adopt abstract particles to represent the garments’ attributes and the wind. In particular, we use ag to denote each garment’s attribute, such as the friction and stiffness, and wt to denote the wind. Since the wind has constant strength in the whole 3D space, we use the quaternion rotation qt and the strength st to represent the wind as wt = {qt, st}. In this way, given the human body and wind at t+1 as well as their previous h states, we aim to predict the garments’ states at time t+1 given the current states at t and corresponding previous meshes {M t−1, · · · , M t−h}. In practice, we choose h = 1 in all experiments. Our approach can be described as:\n\nˆV t+1\n\ng\n\n= φ(ag, {M t−i\n\ng\n\n, M t+1−i\n\nb\n\n, wt+1−i}h\n\ni=0),\n\n(1)\n\ng and M t+1\n\nwhere M t and runs recursively during predictions, and ˆV t+1\n\nb\n\ng\n\nare the meshes of garments and human body, respectively, φ(·) is the simulator\n\nis the garment’s new vertices’ states at time t + 1.\n\nPatch-based Garment Model. Inspired by Ma et al. (2021), we establish a two-level structural hierarchy for garments and represent each garment by patches. Patch modeling holds several advantages. First, as basic units to represent garments, patches are topology independent. By modeling the dynamics of each patch, our model is more flexible and generalizable to unseen garments. Second, instead of simulating each vertex in a mesh, simulating patches signficantly reduces the computational overhead, especially when the mesh is of high-fidelity.\n\nFormally, we find a mapping q(·) to map the vertex-based mesh to patch-based representation by:\n\nP t\n\ng = q(M t\n\ng),\n\n(2)\n\np , EM\n\ng = {V t where P t patches, and EW shown in Figure 2. In this way, our method can be updated as:\n\np }. The patches’ states V t\n\np are computed given V t\n\np are the averaged vertices’ states within the p . The mapping q(·) is based on the garments’ uv maps as\n\np , EW\n\nˆV t+1\n\ng\n\n= φ(ag, {P t−i\n\ng\n\n, M t+1−i\n\nb\n\n, wt+1−i}h\n\ni=0),\n\n(3)\n\nSimulation-based Garment Animation. After collecting the set of particles, including the patch particles of garments, the vertex particles of human body, as well as the abstract particles of garment attributes and environmental wind, we can then animate garments by predicting the future state of each particles through particle-based simulation, which is topology independent and highly generalizable. While our method is orthogonal to the choice of particle simulator, in practice we adopt TIE (Shao et al., 2022) as our simulator due to its promising results and high computational efficiency. Specifically, it assigns each particle i three tokens, namely a state token vi, a sender token si and a receiver token ri. The sender token si describes how the particle i influence others, and the receiver token ri indicates how the particle i can be affected. The updating formulas of all tokens can be summarized as follow:\n\nsi = Wsvi,\n\nri = Wrvi,\n\n,\n\nr′\n\ni =\n\nri − μri σrisj\n\n,\n\ns′\n\nj =\n\nfri,sj = r′\n\nsj − μsj σrisj i + s′ j, (cid:88)\n\nv′\n\ni =\n\nωijfri,sj ,\n\nj\n\nωij = softmax((WQvi)⊤fri,sj ),\n\n(4)\n\n(5)\n\n(6)\n\n(7)\n\n(8)\n\nwhere μri, μsj are the means of tokens ri, sj respectively, and σrisj is the standard deviation. In practice, we generate two different attention masks indicated by EM and EW for different heads in multi-head attention.\n\nFormally, the simulator can be described as\n\nEt+1\n\np\n\n= f (ag, {P t−i\n\ng\n\n, M t+1−i\n\nb\n\n, wt+1−i}h\n\ni=0).\n\n(9)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTo recover each vertex’s details within the corresponding patch, we apply a decoder as:\n\nˆat+1\n\ni\n\nˆvt+1\n\ni\n\n= g([vt\n\ni , et+1\n\np,i , vt+1 h,i ]), i + vt i ,\n\n= ∆t · ˆat+1\n\nˆpt+1\n\ni = ∆t · ˆvt+1\n\ni + pt i,\n\n(10)\n\n(11)\n\nwhere we concatenate i-th vertex’s state vt p,i and the states of the nearest point on human mesh vt+1 at time t + 1. We then calculate the corresponding position and velocity at time t + 1 given the time interval ∆t between each frame.\n\nh,i as inputs, and output the new acceleration ˆat+1\n\ni with its corresponding patch embedding et+1\n\ni\n\n4.2 TRAINING DETAILS\n\nTo train a simulation model, we first apply a standard mean square error (MSE) loss on the positions of vertices as:\n\nLt+1\n\nm\n\n=\n\n1 N\n\n(cid:88)\n\ni\n\n∥ ˆpt+1\n\ni − pt+1\n\ni\n\n∥2 2,\n\n(12)\n\nwhere pt+1 for the vertex normal to maintain the smoothness and consistence of the garments:\n\nis the ground truth at time t + 1 and N is the number of vertices. We adopt a loss term\n\ni\n\nLt+1\n\nn\n\n=\n\n1 N\n\n(cid:88)\n\ni\n\n∥ ˆnt+1\n\ni − nt+1\n\ni\n\n∥2 2,\n\n(13)\n\nwhere ˆnt+1 reduce the collision rates between garments and human bodies, we adopt a collision loss:\n\nare the vertex normal for prediction and ground truth respectively. To further\n\nand nt+1\n\ni\n\ni\n\nLt+1\n\nc\n\n=\n\n1 Nc\n\n(cid:88)\n\ni\n\n(cid:0)dε − min (cid:0)( ˆpt+1\n\ni − pt+1\n\nh ) · nt+1\n\nh , dε\n\n(cid:1)(cid:1)2\n\n,\n\n(14)\n\nh\n\nis the nearest point to ˆpt+1\n\nis the normal vector of point h , Nc is the number of collided vertices, and dε is the minimum distance of penetration. Thus,\n\nwhere pt+1 pt+1 for predictions at time t + 1, our training loss is written as:\n\non the human mesh, nt+1\n\nh\n\ni\n\nLt+1 = λmLt+1\n\nm + λnLt+1\n\nn + λcLt+1\n\nc\n\n,\n\n(15)\n\nDuring training, we predict the garments’ positions for two future timestamps, namely t + 1 and t + 2. The final loss L is\n\nL = Lt+1 + Lt+2.\n\n(16)\n\n5 EXPERIMENTS\n\n5.1 BASELINE AND IMPLEMENTATION DETAILS\n\nWe implement DeePSD (Bertiche et al., 2021) and MGNet (Zhang et al., 2022) as our baselines. Importantly, DeePSD achieves state-of-the-art performance in terms of 3D garment animations. it claims to support the animations of multi-layered garments. We mainly compare DeePSD with our model and make the following extensions to DeePSD: 1. we add wind as extra inputs; 2. we add the collision loss between different layers of garments for multi-layered clothes settings. Since MGNet is a garment-specific model for single-layered clothes, we compare MGNet with only inner garments. We also include the wind as extra feature map. All models are trained with ten epochs. We do not apply any postprocessing for both training and predicting. During evaluation, we calculate errors as the mean of Euclidean errors for each frame, then average the errors of all frames within each sequence. The final results are the mean of errors from all sequences.\n\n5.2 ABLATION STUDY ON LAYERS\n\nTo investigate the influence of multi-layered garments and random wind in LAYERS, we divide our dataset into four different splits as shown in Table 2 : tight inner garments without wind (T);\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: To analyze the challenges in LAYERS , we sample four splits from our dataset: inner garments are tight clothes without wind (T); inner garments are tight clothes with strong wind (T+W); inner garments are loose clothes without wind (L); inner garments are loose clothes with strong wind (L+W). Specifically, jackets are either with or without hood, while dress are either with or without waist belt. Notice that we group winds with a strength less than 50 as not windy, where the wind has little influence on the garments.\n\nComponents\n\nTight (T) Tight+Wind (T+W) Loose (L) Loose+Wind (L+W)\n\nInner Garments Outer Garments Wind Strength\n\nJumpsuit Jacket ≤ 50\n\nJumpsuit Jacket > 250\n\nDress Jacket ≤ 50\n\nDress Jacket > 250\n\nTable 3: Euclidean errors (mm) on four splits. To display the challenges brought by the outer garments and the interactions between layers of clothes, we further train models with only the inner garments as marked by * in the table. MGNet has worse generalization abilities due to garment-specific design. LayersNet has slightly higher errors since the inner garments simulated together with outer clothes do not follow physics laws by themselves. LayersNet achieves superior and robust performance on all splits with multi-layered garments.\n\nTight (T)\n\nTight + Wind (T+W)\n\nLoose (L)\n\nLoose + Wind (L+W)\n\nMethods\n\nDeePSD∗ MGNet∗ LayersNet∗\n\n225.3±106.4 5219.2±1565.8 260.3±254.4\n\n1068.2±693.8 DeePSD LayersNet(Ours) 611.3±544.3\n\n239.5±103.9 5186.8±1754.8 278.6±328.6\n\n2782.6±1239.7 578.8±576.2\n\n501.3±300.1 4432.7±1438.0 378.0±293.0\n\n868.0±495.6 603.0±529.1\n\n577.5±373.9 4595.0±1215.2 363.6±311.4\n\n1707.9±503.4 572.5±469.5\n\ntight inner garments with strong wind (T+W); loose inner garments without wind (L); loose inner garments with strong wind (L+W). Each split contains 36K frames for training, 2K frames for validation, and 2K frames for test. Note that the strength of the wind ranges from 0 to 400. We group winds with a strength less than 50 as not windy, where the wind has little influence on the garments. We consider winds with a strength more than 250 as strong wind. Since the outer garments exhibit more flexible dynamics, such as falling off or waving in the air, animating them is already a challenging task for existing methods, let alone considering the interactions with inner garments. To further simplify our dataset and have a better comparison with existing synthetic dataset, Cloth3D (Bertiche et al., 2020), we first exclude the outer garments on all splits and train models with only inner garments, which is indicated in the first three rows of Table 3.\n\nWhen trained with only inner garments, DeePSD achieves reasonable performance comparing with that when it is trained on Cloth3D (Bertiche et al., 2020), suggesting that the settings of Cloth3D are similar to our simplified data settings. MGNet fails in our dataset due to the garment-specific design and low generalization abilities. LayersNet has lower errors especially on split L and L+W, suggesting the effectiveness and higher generalization abilities of animating loose clothes, Please refer to Appendix for more details and qualitative comparisons. On split T+W and L+W, DeePSD shows higher errors due to the random wind. Since jumpsuits in splits T and T+W are tight garments, the wind has less influence on them.\n\nWhen trained with multi-layered garments, the Euclidean errors by DeePSD increase dramatically, especially with the influence of wind in split T+W and L+W, suggesting the challenges brought by multi-layered garments. The high errors on split T+W and L+W, compared with split T and L, respectively, suggest that DeePSD is less generalizable to driving factors beyond human bodies. In contrast, LayersNet achieves superior performance on all splits with both inner and outer garments. The Euclidean errors are close to each other on different splits, suggesting that our model is more robust to the garments’ various topologies as well as the driving factors beyond human bodies.\n\n5.3 GARMENT ANIMATION\n\nWe sample 50K frames from LAYERS for training, 6K frames for validation, and 6K frames for test. There is no overlapping among different sets of samples. All the samples are composed of both inner and outer garments, as well as random wind as the external factor.\n\nThe vanilla DeePSD without collision loss exhibits high Euclidean errors on all types of garments. When adding collision loss including collisions between layers of garments, the collision rates are reduced. Nonetheless, DeePSD does not improve in terms of Euclidean errors. Since the vanilla DeePSD has difficulties in learning reasonable dynamics of garments, the collision terms introduce more noise while training DeePSD , pushing the garments away from the bodies. In contrast, LayersNet achieves superior performance in terms of Euclidean errors, suggesting the effectiveness of is more generalizable and shows more our simulation-based methods. Moreover, our LayersNet\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Euclidean error (mm) on sampled LAYERS with maximum sequence length of 35 frames. The collision rates between different layers of garments are shown under L-Collision, while the collision rates between garments and human bodies are shown under H-Collision. Models trained with collision loss are marked by +. When training DeePSD with collision loss, we extend Equation 14 and include the collisions between different layers of garments as extra loss term. In constrast, our model only applies the basic collision loss between garments and human bodies following Equation 14. Our LayersNet achieves superior results on all types of garments. Even without explicitly punishing collisions between layers of garments, LayersNet still achieves low collision rates among garments and makes good balance between Euclidean errors and penetrations.\n\nMethods\n\nJacket\n\nJacket + Hood\n\nDress\n\nJumpsuit\n\nSkirt\n\nDeePSD DeePSD+ LayersNet(Ours) LayersNet+(Ours)\n\n2863.0±881.2 3609.7±1147.0 717.6±609.8 684.9±554.9\n\n2956.7±799.8 3434.4±781.9 577.5±458.1 566.2±425.4\n\n2606.3±792.0 4046.6±962.9 448.2±452.2 501.2±466.9\n\n2876.2±708.3 4773.1±1120.9 277.3±293.1 321.1±274.7\n\n2498.6±618.6 4232.6±1713.9 274.6±94.4 378.6±143.0\n\nMethods\n\nPants\n\nT-shirt\n\nOverall\n\nL-Collision\n\nH-Collision\n\nDeePSD DeePSD+ LayersNet(Ours) LayersNet+(Ours)\n\n3075.1±117.7 5260.4±1283.4 291.2±301.7 349.3±238.4\n\n2618.5±729.1 4145.9±994.3 272.4±198.2 331.7±226.9\n\n2851.8±696.4 23.82%±11.25% 18.36%±6.74% 3.82%±3.72% 0.63%±0.83% 3907.6±790.6 4.51%±2.98% 10.01%±5.62% 560.6±452.2 4.94%±2.67% 3.58%±2.83% 567.2±432.8\n\nFigure 3: Qualitative results by LayersNet. The left sequence shows a human model walking down some steps. The magnified regions highlight the vivid dynamics of the jacket. Human model in the right sequence is moving towards her left. Without collision loss, LayersNet generates some body-to-cloth penetrations near the thigh and even head on the last frame of the right sequence. When trained with collision loss, LAYERS, which is marked by +, reduces the collision rates more obviously. Even though we do not explicitly penalize the collisions among different layers of garments, LayersNet is able to solve those collisions implicitly through exchanging semantics by interactions, suggesting the robustness and effectiveness of LayersNet.\n\nrobust performance across different types of garments. When adding collision loss as mentioned in Equation 14, the body-to-cloth collisions and human bodies are reduced. Though we never explicitly penalize collisions between different layers of garments, the collision rates among clothes are low. Since the key idea of simulation is to model the interactions among objects, such as the energy transition and collisions, LayersNet can resolve collisions implicitly. We show the qualitative results by our LayersNet in Figure 3. More qualitative comparisons can be found in Appendix. LayersNet with collision loss, which is marked by +, shows fewer body-to-garment penetrations, suggesting the effectiveness and robustness of our model.\n\n6 CONCLUSION\n\nWe have presented a new large-scale synthetic dataset called LAYERS, which covers 4,900 different combinations of multi-layered garments with 700K frames in total. The animations of multi-layered garments follow the laws of physics, allowing the interactions among different layers of garments. In addition, LAYERS takes the environmental wind, another important driving factors besides human body, into consideration to animate garments. To demonstrate the quality of LAYERS, we further propose LayersNet, a simulation-based method for garment animations. We model the various driving factors as (abstract) particles, while represent garments as unions of particles as patches. The animations of garments driven by different factors are naturally and uniformly achieved via modeling the interactions among particles. As shown by the experiments, our model achieves superior and robust performance with compelling abilities in generalization.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlakh Aggarwal, Jikai Wang, Steven Hogue, Saifeng Ni, Madhukar Budagavi, and Xiaohu Guo. Layered-garment net: Generating multiple implicit garment layers from a single image. In Computer Vision - ACCV 2022 - 16th Asian Conference on Computer Vision, Macau, China, December 4 - December 8, 2022, 2022.\n\nHugo Bertiche, Meysam Madadi, and Sergio Escalera. CLOTH3D: clothed 3d humans. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XX, 2020.\n\nHugo Bertiche, Meysam Madadi, Emilio Tylson, and Sergio Escalera. Deepsd: Automatic deep skinning and pose space deformation for 3d garment animation. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, 2021.\n\nBharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll. Multi-garment net: Learning to dress 3d people from images. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, 2019.\n\nZhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, Fangzhou Hong, Mingyuan Zhang, Chen Change Loy, Lei Yang, and Ziwei Liu. Humman: Multi-modal 4d human dataset for versatile sensing and modeling. CoRR, 2022.\n\nEnric Corona, Albert Pumarola, Guillem Aleny`a, Gerard Pons-Moll, and Francesc Moreno-Noguer. Smplicit: Topology-aware generative model for clothed people. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, 2021.\n\nMaria Korosteleva and Sung-Hee Lee. Generating datasets of 3d garments with sewing patterns. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021.\n\nJie Li, Gilles Daviet, Rahul Narain, Florence Bertails-Descoubes, Matthew Overby, George E. Brown, and Laurence Boissieux. An implicit frictional contact solver for adaptive cloth simulation. ACM Trans. Graph., 2018.\n\nY. D. Li, M. Tang, Y. Yang, Z. Huang, R. F. Tong, S. C. Yang, Y. Li, and Dinesh Manocha. N-cloth:\n\nPredicting 3d cloth deformation with mesh-based networks. Comput. Graph. Forum, 2022.\n\nYunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, and Antonio Torralba. Learning particle In 7th International dynamics for manipulating rigid bodies, deformable objects, and fluids. Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n\nJunbang Liang, Ming C. Lin, and Vladlen Koltun. Differentiable cloth simulation for inverse problems. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019.\n\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black.\n\nSMPL: a skinned multi-person linear model. ACM Trans. Graph., 2015.\n\nQianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J. Black. Learning to dress 3d people in generative clothing. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, 2020.\n\nQianli Ma, Shunsuke Saito, Jinlong Yang, Siyu Tang, and Michael J. Black. SCALE: modeling clothed humans with a surface codec of articulated local elements. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, 2021.\n\nNaureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. AMASS: archive of motion capture as surface shapes. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRahul Narain, Armin Samii, and James F. O’Brien. Adaptive anisotropic remeshing for cloth simu-\n\nlation. ACM Trans. Graph., 2012.\n\nCharlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Peter W. Battaglia. PolyGen: An autoregressive generative model of 3d meshes. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, 2020.\n\nXiaoyu Pan, Jiaming Mai, Xinwei Jiang, Dongxue Tang, Jingxiang Li, Tianjia Shao, Kun Zhou, Xiaogang Jin, and Dinesh Manocha. Predicting loose-fitting garment deformations using bonedriven motion networks. In SIGGRAPH ’22: Special Interest Group on Computer Graphics and Interactive Techniques Conference, Vancouver, BC, Canada, August 7 - 11, 2022, 2022.\n\nChaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-Moll. Tailornet: Predicting clothing in 3d as a function of human pose, shape and garment style. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, 2020.\n\nTobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W. Battaglia. Learning meshbased simulation with graph networks. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n\nAlbert Pumarola, Jordi Sanchez, Gary P. T. Choi, Alberto Sanfeliu, and Francesc Moreno. 3dpeople: In 2019 IEEE/CVF International Conference on\n\nModeling the geometry of dressed humans. Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, 2019.\n\nYi-Ling Qiao, Junbang Liang, Vladlen Koltun, and Ming C. Lin. Scalable differentiable physics for learning and control. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, 2020.\n\nAlvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W. Battaglia. Learning to simulate complex physics with graph networks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, 2020.\n\nIgor Santesteban, Nils Thuerey, Miguel A. Otaduy, and Dan Casas. Self-supervised collision hanIn IEEE Conference on Computer\n\ndling via generative 3d garment models for virtual try-on. Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, 2021.\n\nYidi Shao, Chen Change Loy, and Bo Dai. Transformer with implicit edges for particle-based\n\nphysics simulation. CoRR, 2022.\n\nYu Shen, Junbang Liang, and Ming C. Lin. Gan-based garment generation using sewing pattern images. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVIII, 2020.\n\nNils Thuerey, Konstantin Weißenow, Lukas Prantl, and Xiangyu Hu. Deep learning methods for\n\nreynolds-averaged navier–stokes simulations of airfoil flows. AIAA Journal, 2020.\n\nGarvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Gerard Pons-Moll. SIZER: A dataset and model for parsing 3d clothing and learning size sensitive 3d clothing. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III, 2020.\n\nLokender Tiwari and Brojeshwar Bhowmick. Deepdraper: Fast and accurate 3d garment draping over a 3d human body. In IEEE/CVF International Conference on Computer Vision Workshops, ICCVW 2021, Montreal, BC, Canada, October 11-17, 2021, 2021.\n\nBenjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun. Lagrangian fluid simulation with continuous convolutions. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\n\nRaquel Vidaurre, Igor Santesteban, Elena Garces, and Dan Casas. Fully convolutional graph neural\n\nnetworks for parametric virtual try-on. Comput. Graph. Forum, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nRui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physicsIn KDD ’20: The 26th ACM SIGKDD informed deep learning for turbulent flow prediction. Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, 2020.\n\nTuanfeng Y. Wang, Tianjia Shao, Kai Fu, and Niloy J. Mitra. Learning an intrinsic garment space\n\nfor interactive authoring of garment animation. ACM Trans. Graph., 2019.\n\nZehang Weng, Fabian Paus, Anastasiia Varava, Hang Yin, Tamim Asfour, and Danica Kragic. Graph-based task-specific prediction models for interactions between deformable and rigid objects. CoRR, 2021.\n\nTao Yu, Zerong Zheng, Yuan Zhong, Jianhui Zhao, Qionghai Dai, Gerard Pons-Moll, and Yebin Liu. Simulcap : Single-view human performance capture with cloth simulation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, 2019.\n\nChao Zhang, Sergi Pujades, Michael J. Black, and Gerard Pons-Moll. Detailed, accurate, human shape estimation from clothed 3d scan sequences. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, 2017.\n\nMeng Zhang, Duygu Ceylan, and Niloy J. Mitra. Motion guided deep dynamic 3d garments. CoRR,\n\n2022.\n\nZerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and Yebin Liu. Deephuman: 3d human reconstruction from a single image. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, 2019.\n\nA APPENDIX\n\nA.1 LAYERS DATASET\n\nAlthough Blender does not support collisions among multiple objects, it is able to solve collisions within one object. Thus, by merging multiple garments as a single mesh, we regard the collisions among different garments as the interactions within one mesh, which can be solved by Blender. In other words, different layers of garments will interact with each other following the physics rules.\n\nBefore simulation, we properly dress the human body in T-pose and scale up all objects 10 times the real-world size, which can preserve more details of the garments such as wrinkles.\n\nWe adopt different garment attributes to different layers of clothes. Specifically, we uniformly sample the following attributes: vertex mass from 0.2 to 0.8; stiffness of tension, compression, shear, and bending from 15 to 100; friction from 40 to 80.\n\nQuantitiave comparison with existing synthetic dataset. We train our implementation of DeePSD on Cloth3D following the original paper (Bertiche et al., 2021). Results are shown in Table 5, suggesting that our implementation of DeePSD is similar to the official one. We sample four splits from LAYERS as mentioned in main text. Specifically, the inner garments in split T and T+W are jumpsuits, while those in split L and L+W are dress. The outer garments in all splits are either jackets or jackets with hood. For convenience, we copy the table in main text for reference as shown in Table 6 and Table 7. Table 7 shows the results trained on four splits of our dataset. Specifically, the results on the first three rows, where models are marked by *, are obtained by training models with only inner garments. This setting is most similar to Cloth3D’s settings. The remaining results are obtained by training on both inner and outer garments on all splits. Notice that we scale up the human mesh and garment mesh 10 times the real-world size, the corresponding errors are also scaled up. Thus, DeePSD* achieves similar results on both datasets: the Euclidean errors of jumpsuit and dress are similar on both LAYERS and Cloth3D, suggesting that the quality of our dataset is not worse than Cloth3D. In addition, when comparing DeePSD* with DeePSD, DeePSD achieves higher errors, which mainly come from the outer garments. MGNet fails in LAYERS due to the garment-specific design and low generalization abilities as shown in Figure 4. Our LayersNet achieves reasonable results especially on loose garment settings, suggesting the effectiveness and higher generalization abilities of animating loose garments.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: We verify our implementation of DeePSD on Cloth3D accroding to official paper (Bertiche et al., 2021). The results are similar to original paper, suggesting that our implementation of DeePSD is similar to official one.\n\nMethod\n\nT-shirt\n\nTop\n\nTrousers\n\nSkirt\n\nJumpsuit\n\nDress\n\nDeePSD 25.01±20.94 16.90±15.38 20.02±8.50 20.43±31.10 24.31±6.36 42.10±21.41\n\nTable 6: To analyze the challenges in LAYERS , we sample four splits from our dataset: inner garments are tight clothes without wind (T); inner garments are tight clothes with strong wind (T+W); inner garments are loose clothes without wind (L); inner garments are loose clothes with strong wind (L+W). Specifically, jackets are either with or without hood, while dress are either with or without waist belt. Notice that we group winds with a strength less than 50 as not windy, where the wind has little influence on the garments.\n\nComponents\n\nTight (T) Tight+Wind (T+W) Loose (L) Loose+Wind (L+W)\n\nInner Garments Outer Garments Wind Strength\n\nJumpsuit Jacket ≤ 50\n\nJumpsuit Jacket > 250\n\nDress Jacket ≤ 50\n\nDress Jacket > 250\n\nTable 7: Euclidean errors (mm) on four splits. To display the challenges brought by the outer garments and the interactions between layers of clothes, we further train models with only the inner garments as marked by * in the table. MGNet has worse generalization abilities due to garment-specific design. LayersNet has slightly higher errors since the inner garments simulated together with outer clothes do not follow physics laws by themselves. LayersNet achieves superior and robust performance on all splits with both inner and outer garments.\n\nTight (T)\n\nTight + Wind (T+W)\n\nLoose (L)\n\nLoose + Wind (L+W)\n\nMethods\n\nDeePSD∗ MGNet∗ LayersNet∗\n\n225.3±106.4 5219.2±1565.8 260.3±254.4\n\n1068.2±693.8 DeePSD LayersNet(Ours) 611.3±544.3\n\n239.5±103.9 5186.8±1754.8 278.6±328.6\n\n2782.6±1239.7 578.8±576.2\n\n501.3±300.1 4432.7±1438.0 378.0±293.0\n\n868.0±495.6 603.0±529.1\n\n577.5±373.9 4595.0±1215.2 363.6±311.4\n\n1707.9±503.4 572.5±469.5\n\nFigure 4: The left are the training samples while the right is test samples. All samples are from the split T, where we train models with only inner garments (jumpsuit). MGNet is able to generate 3D garments on training examples on the left while has difficulties to generalize to unseen examples in test set due to the garment-specific design. DeePSD has faithful predictions on simplified dataset. Our LayersNet faithfully rollouts 3D garments when only with inner clothes.\n\nQualititative comparisons. We include more qualitative comparisons in this section. As shown in Figure 4 when trained with only inner garments (jumpsuit), DeePSD (Bertiche et al., 2021) is\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: The outer garments are more flexible in our dataset and are able to respond to different garment attributes, such as friction. In this sample, the jacket falls off the shoulder due to the motion of human and small friction, bringing more challenge for DeePSD to converge. The collision loss for DeePSD further generates noises and forces DeePSD to push clothes away from human model to achieve lower collision rates. In contrast, our LayersNet is able to faithfully rollout the garments even on this challenging case.\n\nTable 8: We train LayersNet on our LAYERS and test on Cloth3D. We also compare DeePSD which is trained on Cloth3D. We re-sample the test samples from Cloth3D and include continuous sequences for better comparisons. The test set and training set on Cloth3D have no overlaps. Our LayersNet is able to achieve superior results on all scenarios.\n\nMethod\n\nT-shirt\n\nTop\n\nTrousers\n\n36.69±15.34 26.52±9.15 29.86±13.47 DeePSD (trained on Cloth3D) LayersNet (trained on LAYERS) 23.84±15.81 14.17±5.92 23.16±11.38\n\nMethod\n\nSkirt\n\nJumpsuit\n\nDress\n\n55.66±22.11 26.86±6.65 54.71±49.69 DeePSD (trained on Cloth3D) LayersNet (trained on LAYERS) 41.48±17.73 23.01±9.93 35.51±37.73\n\nable to predict faithful rollouts. MGNet (Zhang et al., 2022) is able to generate 3D garments on training samples while struggles to generalize to unseen garments from test examples due to the garment-specific design. In original paper of MGNet, they train MGNet with only on 300 frames of data with the same garment topology, while in our dataset each garment is unique with different topology. In contrast, our LayersNet achieves faithful predictions in this simplified case.\n\nOn the other hand, as shown in Figure 5, the garments in our dataset, especially the outer clothes, are more flexible and are able to respond to various garment attributes, such as falling off the shoulder due to small frictions in this case. The high flexibility brings more challenges to DeePSD, leading to difficulties in convergence. The collision loss for DeePSD further introduces noises due to inaccurate garment meshes and forces DeePSD to push the clothes away from human to achieve lower collision rates. In contrast, our simulation-based LayersNet animates garments in topology-independent and unified manners and still achieves faithful rollouts even on such challenging case.\n\nA.2 MODEL GENERALIZATION\n\nSince our LayersNet learns a topology-independent simulation model and is highly generalizable to unseen scenarios, we test LayersNet , which is trained on our dataset LAYERS, on Cloth3D\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Samples are from Cloth3D Bertiche et al. (2020). The human model on the left wears a jumpsuit and is walking, while the human on the right wears a dress and is spinning. Our LayersNet achieves faithful rollouts. DeePSD is able to animate the garments with more stiff dynamics, while LayersNet can predict garments following physics laws, such as the inertia shown in the dress.\n\nFigure 7: We grouped garment’s vertices into patches given the corresponding UV mapping. Based on the coordinates in UV, we group the vertices according to their positions. Subsequently, we divide the 3D garment mesh into patches based on the groups.\n\n(Bertiche et al., 2020) for generalization and compare the original DeePSD trained on Cloth3D. We re-sample the test set to include continuous sequence of samples. The test set and training set for training DeePSD on Cloth3D have no overlaps. As shown in Table 8, our LayersNet achieves superior performance on all types of garments. As shown in Figure 6, when test on unseen samples, predictions by DeePSD tend to be stiff while LayersNet still achieves faithful and vivid rollouts, suggesting the effectiveness of our simulation-based method which animates garments in a unified manner.\n\nA.3\n\nIMPLEMENTATION DETAILS\n\nPatched Garment Model. We group the particles in garment’s mesh given corresponding UV mapping. Specifically, we divide the UV mapping into square patches according to the UV coordinates, as shown in Figure 7. Then, we group the garment’s vertices in 3D space given the grouped UV mapping. When building the connections EM of the patched garments, we connect patch i and patch j iff there is at least one pair of vertices within the patches are connected in 3D space.\n\nLayersNet details. To obtain the world space edges EW , we adopt R = 0.4 to calculate the neighbors from the human mesh and R = 0.6 for different layers of garments. We adopt h = 1 for\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: Euclidean error (mm) on sampled LAYERS with maximum sequence length of 35 frames. The collision rates between different layers of garments are shown under L-Collision, while the collision rates between garments and human bodies are shown under H-Collision. Models trained with collision loss are marked by +. When training DeePSD with collision loss, we extend Equation 14 and include the collisions between different layers of garments as extra loss term. In constrast, our model only applies the basic collision loss between garments and human bodies following Equation 14. Our LayersNet achieves superior results on all types of garments. Even without explicitly punishing collisions between layers of garments, LayersNet still achieves low collision rates among garments.\n\nMethods\n\nJacket\n\nJacket + Hood\n\nDress\n\nJumpsuit\n\nSkirt\n\n706.3±558.5 LayersNet LayersNet+Ln 717.6±609.8 LayersNet+Ln, Lc(full) 684.9±554.9\n\n588.2±460.5 577.5±458.1 566.2±425.4\n\n436.3±343.7 448.2±452.2 501.2±466.9\n\n289.3±194.4 277.3±293.1 321.1±274.7\n\n322.9±58.9 274.6±94.4 378.6±143.0\n\nMethods\n\nPants\n\nT-shirt\n\nOverall\n\nL-Collision\n\nH-Collision\n\n307.2±164.0 LayersNet LayersNet+Ln 291.2±301.7 LayersNet+Ln, Lc(full) 349.3±238.4\n\n282.9±170.8 272.4±198.2 331.7±226.9\n\n558.9±409.0 5.31%±3.88% 16.41%±7.18% 560.6±452.2 4.51%±2.98% 10.01%±5.62% 567.2±432.8 4.94%±2.67% 3.58%±2.83%\n\nthe inputs of all objects’ states. The hyperparameters λm, λn, λc in our loss term are set to 1. We adopt Adam optimizer with an initial learning rate of 0.001 and a decreasing factor of 0.5 every two epochs. The batch size is set to 4.\n\nWe adopt three different encoders for meshes, garment attributes, and wind attributes. This is because the three components belong to different domain space and have different dimensions. Since the states of garments mesh and human bodies mesh are from the same domain, we share the encoder for them. All the encoders are two-layer MLPs with dimensions 128. The only difference is the input dimensions. We adopt 4 blocks of modified Transformer block in LayersNet, with hidden dimensions 128 for each block. The number of head in multi-head attention is set to 8, while 4 heads apply the attention mask generated by EM , and 4 heads adopt the attention mask generated by EW . For the decoder, we adopt a three-layer MLPs with a forward dimension of 128 and an output dimension of 3. When concatenating the nearest point on human mesh in Equation 10, we mask the point vt+1 to zeros if the there is no edge eh,i ∈ EW connecting the point vt+1 h,i and garment h,i point vt+1 . For the inputs of garment mesh and human body mesh, we adopt relative positions to the root of human body mesh. Since the wind’s attributes are still measured in global coordinates, we also convert them to the relative coordinates in implicit manner, i.e. convert the value of strength in global coordinates to local coordinates defined by the root of human body. Specifically, we concatenate the position, velocity, and acceleration of the human body’s root point vt r as extra features wt = {qt, st, vt r} and let LayersNet learns the wind’s features in relative coordinates. We apply ∆t = 1 in our experiments, which is independent from the real time interval between each frames, which is 0.33s. When training LayersNet, we normalize the meshes’ states across the whole training set before feeding into the model, which is a commonly adopted processing in literature.\n\ni\n\nA.4 ABLATION STUDY ON LAYERSNET\n\nIn this section, we analyze our model in the following aspects: (a). vanilla LayersNet; (b). LayersNet with normal loss; (c). LayersNet with both normal loss and collision loss.\n\nAs shown in Table 9, the vanilla LayersNet achieves low Euclidean errors on all types of garments, suggesting the effectiveness of our simulation-based method. With the normal loss term Ln, which aims to smooth the surface of garments, achieves lower collision rates in terms of both body-to-cloth penetrations and collisions between different layers of garments, leading to more faithful rollouts comparing with vanilla LayersNet. When training with both normal loss and collision loss, LayersNet further reduce the body-to-cloth penetrations. Though our complete version of LayersNet has slightly higher Euclidean errors, the lower collision rates lead to more convincing and faithful results. A good example is shown in Figure 3 of the main text.\n\nA.5 FUTURE WORK\n\nIn this work, we propose a novel multi-layered 3D dataset and make the first attempt to animate garments though simulation pipeline which achieve superior performance in both LAYERS and other generalization scenarios. Since we regard all objects as particles, our model is able to animate various garments with different typologies driven by different types of outer forces, such as human\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nbodies and wind. However, since the predictions of future frames are based on previous rollouts by the model, the errors accumulate as the length of predictions increases, which is a common problem in simulation. In this work, we train the model and alleviate the problem by predicting two continuous frames as shown in Equation 16. We will explore more strategies, such as adding noise to input data and forcing the model to learn to correct errors, to enable the model to rollout long sequences while keeping the accumulated errors relatively small at the same time.\n\n17",
    "reference": "# Summary Of The Paper\n\nMULTI-LAYERED 3D GARMENTS ANIMATION\n\nThis paper presents a novel deep learning-based 3D garments animation method. The method learns to model interactions between 1) cloth-body, 2) multiple cloth layers 3) wind and clothes. Specifically, the algorithm decomposes the clothes into connected patches and evaluates the deformation based on a learning-based particle physics simulator (Shao et al., 2022). A new simulation dataset has been proposed to study this new setting.\n\n# Strength And Weaknesses\n\nPros: \n- The new capacity for learning-based clothes simulation to handle both wind-cloth, body-cloth, and inter-cloth interactions. \n- Superior performance compared against DeePSD through a thoughtful exp design. \n- The paper is clearly presented. Equations/tables/figures are all easy to parse. The intro is clear. Related covers most of the relevant works.\n- New large-scale dataset with realistic simulation and unique designs (e.g., wind). \n\nCons:\n- The problem setting of data-driven animation here is not well-motivated. \n- The comparison study is somewhat limited. \n- Lack of qualitative demonstrations and comparisons. \n\nMixed:\n- Technical contribution. The paper belongs to a novel application / resemble of existing techniques (1. data-driven particle-based simulation from Shao et al. and 2. patch-based garments representation used in Ma et al. and earlier works. e.g., Kavan et al. Siggraph 11, Feng et al. 2010)\n- The above two core contributions the author claimed to differentiate the proposed work from prior works are not entirely original. That said, I still appreciate the novel ensemble and applications to data-driven simulation under the wind.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe data-driven motivation is not particularly clear to me. \n- In this work, the proposed method learns to mimic a physical simulator’s behavior. The author needs to justify its use cases, i.e., “why data-driven method?” Given that we have options for physical-based clothes animation solutions, I expect the author to discuss when and why data-driven is beneficial. \n- I think the author can answer the question at least from one of the three aspects: 1) the data-driven method is faster than the Blender’s simulator; 2) the data-driven simulator is better than a real-time physical-based simulator (e.g., 3) data-driven simulator can offer other capacities (e.g., system identification, where end-to-end differentiability allows inferring physical params through inverse problem solver; or sim2real, where the learned particle-dynamic model can be applied for real-world data). \n- However, I cannot find a runtime report or comparison against a real-time garment simulator or a demonstration of downstream demonstrations. \n\nDataset details:\n- It’s also unclear how fast Blender’s clothes simulation is and what method they use (it is also particle based?). \n- Training/testing splits -- during testing how many samples are from unseen poses / unseen human shapes/ unseen clothes materials? \n\nOverall the comparison experiments are not entirely thorough. \n- First, it should be compared against a differentiable particle-based physical solver, if possible, such as DiffCloth, or a real-time physical solver for a speed-performance trade-off. \n- Another data-driven method, SCALE (Ma et al.), should also be compared. SCALE might not be directly modeling wind. But the author can also add wind as an additional attribute, similar to what the author did for DeePSD. \n- Besides, the paper also lacks qualitative comparison against DeePSD. It seems the performance gap is quite large quantitatively, but I would also like to know how it works visually compared to DeePSD.\n\n# Summary Of The Review\n\nOverall I am on the fence. I would like to see how the author addresses my aforementioned concerns and questions. \n\n----------------------------------------------\nPost-rebuttal. \n\nThe authors addressed most concerns by providing additional experiments and revised the submission accordingly. \n\nOverall, it is a weak-accept to borderline-ish submission. \n\nThe paper presentation is quite clear, and the technical approach is straightforward, as pointed out by both the authors and reviewers -- fusing particle simulation in TIE (Shao et al.) with patch-based representation in SCALE (Ma et al.) for efficiency. The method achieves the SOTA in runtime and speed and can handle external force and unseen clothes types.  \n\nIt is a well-executed and well-presented paper after the revision. The proposed combination is somewhat new and, indeed, solves practical challenges of prior arts (efficiency, external force, and multi-layer interaction). It's not a ground-breaking or surprising idea, but it sets a new state for the field and brings new capacity. Hence it doesn't harm to get accepted for its practicality.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nVA-DEPTHNET: A VARIATIONAL APPROACH TO SINGLE IMAGE DEPTH PREDICTION\n\nCe Liu1 Suryansh Kumar1∗ Shuhang Gu2 Radu Timofte1,3 Luc Van Gool1,4 1CVL ETH Zürich 2UESTC China {ce.liu, sukumar, vangool}@vision.ee.ethz.ch\n\n3University of Würzburg 4KU Leuven\n\nshuhanggu@uestc.edu.cn, radu.timofte@uni-wuerzburg.de\n\nABSTRACT\n\nWe introduce VA-DepthNet, a simple, effective, and accurate deep neural network approach for the single-image depth prediction (SIDP) problem. The proposed approach advocates using classical first-order variational constraints for this problem. While state-of-the-art deep neural network methods for SIDP learn the scene depth from images in a supervised setting, they often overlook the invaluable invariances and priors in the rigid scene space, such as the regularity of the scene. The paper’s main contribution is to reveal the benefit of classical and well-founded variational constraints in the neural network design for the SIDP task. It is shown that imposing first-order variational constraints in the scene space together with popular encoder-decoder-based network architecture design provides excellent results for the supervised SIDP task. The imposed first-order variational constraint makes the network aware of the depth gradient in the scene space, i.e., regularity. The paper demonstrates the usefulness of the proposed approach via extensive evaluation and ablation analysis over several benchmark datasets, such as KITTI, NYU Depth V2, and SUN RGB-D. The VA-DepthNet at test time shows considerable improvements in depth prediction accuracy compared to the prior art and is accurate also at high-frequency regions in the scene space. At the time of writing this paper, our method—labeled as VA-DepthNet, when tested on the KITTI depth-prediction evaluation set benchmarks, shows state-of-the-art results, and is the top-performing published approach1 2.\n\n1\n\nINTRODUCTION\n\nOver the last decade, neural networks have introduced a new prospect for the 3D computer vision field. It has led to significant progress on many long-standing problems in this field, such as multiview stereo (Huang et al., 2018; Kaya et al., 2022), visual simultaneous localization and mapping (Teed & Deng, 2021), novel view synthesis (Mildenhall et al., 2021), etc. Among several 3D vision problems, one of the challenging, if not impossible, to solve is the single-image depth prediction (SIDP) problem. SIDP is indeed ill-posed—in a strict geometric sense, presenting an extraordinary challenge to solve this inverse problem reliably. Moreover, since we do not have access to multi-view images, it is hard to constrain this problem via well-known geometric constraints (Longuet-Higgins, 1981; Nistér, 2004; Furukawa & Ponce, 2009; Kumar et al., 2019; 2017). Accordingly, the SIDP problem generally boils down to an ambitious fitting problem, to which deep learning provides a suitable way to predict an acceptable solution to this problem (Yuan et al., 2022; Yin et al., 2019).\n\nImpressive earlier methods use Markov Random Fields (MRF) to model monocular cues and the relation between several over-segmented image parts (Saxena et al., 2007; 2008). Nevertheless, with the recent surge in neural network architectures (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016), which has an extraordinary capability to perform complex regression, many current works use deep learning to solve SIDP and have demonstrated high-quality results (Yuan et al., 2022; Aich et al., 2021; Bhat et al., 2021; Eigen et al., 2014; Fu et al., 2018; Lee et al., 2019;\n\n∗Corresponding Author 1kitti_depth_prediction_benchmark 2For official code refer here\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n2021). Popular recent methods for SIDP are mostly supervised. But even then, they are used less in real-world applications than geometric multiple view methods (Labbé & Michaud, 2019; Müller et al., 2022). Nonetheless, a good solution to SIDP is highly desirable in robotics (Yang et al., 2020), virtual-reality (Hoiem et al., 2005), augmented reality (Du et al., 2020), view synthesis (Hoiem et al., 2005) and other related vision tasks (Liu et al., 2019).\n\nIn this paper, we advocate that despite the supervised approach being encouraging, SIDP advancement should not wholly rely on the increase of dataset sizes. Instead, geometric cues and scene priors could help improve the SIDP results. Not that scene priors have not been studied to improve SIDP accuracy in the past. For instance, Chen et al. (2016) uses pairwise ordinal relations between points to learn scene depth. Alternatively, Yin et al. (2019) uses surface normals as an auxiliary loss to improve performance. Other heuristic approaches, such as Qi et al. (2018), jointly exploit the depth-to-normal relation to recover scene depth and surface normals. Yet, such state-of-the-art SIDP methods have limitations: for example, the approach in Chen et al. (2016) - using ordinal relation to learn depth - over-smooths the depth prediction results, thereby failing to preserve high-frequency surface details. Conversely, Yin et al. (2019) relies on good depth map prediction from a deep network and the idea of virtual normal. The latter is computed by randomly sampling three noncollinear points with large distances. This is rather complex and heuristic in nature. Qi et al. (2018) uses depth and normal consistency, which is good, yet it requires good depth map initialization.\n\nThis brings us to the point that further generalization of the regression-based SIDP pipeline is required. As mentioned before, existing approaches in this direction have limitations and are complex. In this paper, we propose a simple approach that provides better depth accuracy and generalizes well across different scenes. To this end, we resort to the physics of variation (Mollenhoff et al., 2016; Chambolle et al., 2010) in the neural network design for better generalization of the SIDP network, which by the way, keeps the essence of affine invariance (Yin et al., 2019). An image of a general scene—indoor or outdoor, has a lot of spatial regularity. And therefore, introducing a variational constraint provides a convenient way to ensure spatial regularity and to preserve information related to the scene discontinuities (Chambolle et al., 2010). Consequently, the proposed network is trained in a fully-supervised manner while encouraging the network to be mindful of the scene regularity where the variation in the depth is large (cf. Sec.3.1). In simple terms, depth regression must be more than parameter fitting, and at some point, a mindful decision must be made—either by imaging features or by scene depth variation, or both. As we demonstrate later in the paper, such an idea boosts the network’s depth accuracy while preserving the high-frequency and low-frequency scene information (see Fig.1).\n\nOur neural network for SIDP disentangles the absolute scale from the metric depth map. It models an unscaled depth map as the optimal solution to the pixel-level variational constraints via weighted first-order differences, respecting the neighboring pixel depth gradients. Compared to previous methods, the network’s task has been shifted away from pixel-wise metric depth learning to learning the first-order differences of the scene, which alleviates the scale ambiguity and favors scene regularity. To realize that, we initially employ a neural network to predict the first-order differences of the depth map. Then, we construct the partial differential equations representing the variational constraints by reorganizing the differences into a large matrix, i.e., an over-determined system of equations. Further, the network learns a weight matrix to eliminate redundant equations that do not favor the introduced first-order difference constraint. Finally, the closed-form depth map solution is recovered via simple matrix operations.\n\nWhen tested on the KITTI (Geiger et al., 2012) and NYU Depth V2 (Silberman et al., 2012) test sets, our method outperforms prior art depth prediction accuracy by a large margin. Moreover, our model pre-trained on NYU Depth V2 better generalizes to the SUN RGB-D test set.\n\n2 PRIOR WORK\n\nDepth estimation is a longstanding task in computer vision. In this work, we focus on a fullysupervised, single-image approach, and therefore, we discuss prior art that directly relates to such approach. Broadly, we divide the popular supervised SIDP methods into three sub-categories.\n\n(i) Depth Learning using Ranking or Ordinal Relation Constraint. Zoran et al. (2015) and Chen et al. (2016) argue that the ordinal relation between points is easier to learn than the metric depth. To this end, Zoran et al. (2015) proposes constrained quadratic optimization while Chen\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n(a) Image\n\n(b) AdaBins\n\n(c) NeWCRFs\n\n(d) Ours\n\nFigure 1: Qualitative comparison of our method’s depth result with recent state-of-the-art methods such as AdaBins (Bhat et al., 2021), NeWCRFs (Yuan et al., 2022) on NYU Depth V2 test set (Silberman et al., 2012). It can be observed that our method predicts high-frequency details better than other recent methods.\n\net al. (2016) relies on the variation of the inception module to solve the problem. Later, Xian et al. (2020) proposes structure-guided sampling strategies for point pairs to improve training efficiency. Recently, Lienen et al. (2021) elaborates on the use of listwise ranking method based on the PlackettLuce model (Luce, 2012). The drawback of such approaches is that the ordinal relationship and ranking over smooth the depth solution making accurate metric depth recovery challenging.\n\n(ii) Depth Learning using Surface Normal Constraint. Hu et al. (2019) introduces normal loss in addition to the depth loss to overcome the distorted and blurry edges in the depth prediction. Yin et al. (2019) proposes the concept of virtual normal to impose 3D scene constraint explicitly and to capture the long-range relations in the depth prediction. The long-range dependency in 3D is introduced via random sampling of three non-colinear points at a large distance from the virtual plane. Lately, Long et al. (2021) proposes an adaptive strategy to compute the local patch surface normals at train time from a set of randomly sampled candidates and overlooks it during test time.\n\n(iii) Depth Learning using other Heuristic Refinement Constraint. There has been numerous works attempt to refine the depth prediction as a post-processing step. Liu et al. (2015), Li et al. (2015) and Yuan et al. (2022) propose to utilize the Conditional Random Fields (CRF) to smooth the depth map. Lee et al. (2019) utilizes the planar assumptions to regularize the predicted depth map. Qi et al. (2018) adopts an auxiliary network to predict the surface normal, and then refine the predicted depth map following their proposed heuristic rules. There are mainly two problems with such approaches: Firstly, these approaches rely on a good depth map initialization. Secondly, the heuristic rules and the assumptions might result in over-smoothed depth values at objects boundaries.\n\nMeanwhile, a few works, such as Ramamonjisoa et al. (2020), Cheng et al. (2018), Li et al. (2017) were proposed in the past with similar inspirations. Ramamonjisoa et al. (2020), Cheng et al. (2018) methods are generally motivated towards depth map refinement predicted from an off-the-shelf network. On the other hand, Cheng et al. (2018) proposes to use an affinity matrix that aims to learn the relation between each pixel’s depth value and its neighbors’ depth values. However, the affinity matrix has no explicit supervision, which could lead to imprecise learning of neighboring relations providing inferior results. On the contrary, our approach is mindful of imposing the first-order difference constraint leading to better performance. Earlier, Li et al. (2017) proposed two strategies for SIDP, i.e., fusion in an end-to-end network and fusion via optimization. The end-to-end strategy fuses the gradient and the depth map via convolution layers without any constraint on convolution weights, which may not be an apt choice for a depth regression problem such as SIDP. On the other hand, the fusion via optimization strategy is based on a non-differentiable strategy, leading to a nonend-to-end network loss function. Contrary to that, our method is well-constrained and performs quite well with a loss function that helps end-to-end learning of our proposed network. Not long ago, Lee & Kim (2019) proposed to estimate relative depths between pairs of images and ordinary depths at a different scale. By exploiting the rank-1 property of the pairwise comparison matrix, it recovers the relative depth map. Later, relative and ordinary depths are decomposed and fused to recover the depth. On a slightly different note, Lee & Kim (2020) studies the effectiveness of various losses and how to combine them for better monocular depth prediction.\n\nTo sum up, our approach allows learning of confidence weight to select reliable gradient estimation in a fully differentiable manner. Further, it proffers the benefits of the variational approach to overcome the limitations of the existing state-of-the-art methods. More importantly, the proposed method can provide excellent depth prediction without making extra assumptions such as good depth initialization, piece-wise planar scene, and assumptions used by previous works mentioned above.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n(a) First order depth variation along x-axis.\n\n(b) Overall matrix by ordering the terms.\n\nFigure 2: Illustration of the idea. (a) Depth gradient constraint along x axis at location (i, j) in 4 × 4 matrix form. (b) Construction of the overall matrix formulation with constraints at all the pixel locations.\n\n3 METHOD\n\nIn this section, we first describe our proposed variational constraint and then present the overall network architecture leading to the overall loss function.\n\n3.1 VARIATIONAL CONSTRAINT\n\nHere we introduce our variational constraint and how it can be useful for depth estimation. Consider an unscaled depth map as Zu ∈ RH×W , with (H, W ) symbolizing the height and width, respectively. Assuming Γx ∈ RH×W and Γy ∈ RH×W as the gradient of Zu in the x and y axis, we write\n\n∇Zu = [Γx, Γy]T.\n\n(1)\n\nHere, x and y subscript corresponds to the direction from left to right (x-axis) and top to bottom of the image (y-axis), respectively. Elaborating on this, we can write\n\nΓi,j\n\nx = ∇xZ i,j\n\nu = Z i,j+1\n\nu\n\n− Z i,j\n\nu ; Γi,j\n\ny = ∇yZ i,j\n\nu = Z i+1,j\n\nu\n\n− Z i,j u .\n\n(2)\n\nSuppose we augment Eq.(2) expression for all (i, j), i ∈ {1, ..., H} and j ∈ {1, ..., W }. In that case, we will end up with an over-determined system with 2HW equations in total. Given the predicted Γx and Γy, we aim to recover the HW unknown variables in Zu. However, some of the equations could be spurious and deteriorate the overall depth estimation result rather than improving it. As a result, we must be mindful about selecting the equation that respects the imposed first-order constraint and maintains the depth gradient to have a meaningful fitting for better generalization. To that end, we introduce confidence weight Σx ∈ [0, 1]H×W , Σy ∈ [0, 1]H×W for gradient along x, y direction. Consequently, we multiply the above two equations by the confidence weight term Σi,j x\nand Σi,j y , respectively. On one hand, if the confidence is close to 1, the equation will have priority to be satisfied by the optimal Zu. On the other hand, if the confidence is close to 0, we must ignore the equation. For better understanding, we illustrate the first-order difference and weighted matrix construction in Fig.2 (a) and Fig.2 (b). Next, we reshape the Σx, Σy, Γx, Γy, and Zu into column vectors ̃Σx ∈ [0, 1]HW ×1, ̃Σy ∈ [0, 1]HW ×1, ̃Γx ∈ RHW ×1, ̃Γy ∈ RHW ×1, and ̃Zu ∈ RHW ×1, respectively. Organizing ̃Σ = diag([ ̃Σx; ̃Σy]) ∈ R2HW ×2HW and ̃Γ = concat[ ̃Γx; ̃Γy] ∈ R2HW ×1, we can write the overall expression in a compact matrix form using simple algebra as follows\n\n ̃ΣP ̃Zu = ̃Σ ̃Γ\n\n(3)\n\nwhere P ∈ {1, 0, −1}2HW ×HW is the first-order difference operator. Specifically, P is a sparse matrix with only a few elements as 1 or -1. The ith row of P provides the first-order difference operator for the ith equation. The position of 1 and -1 indicates which pair of neighbors to be considered for the constraint. Fig.2 (b) provides a visual intuition about this matrix equation.\n\nEq.(3) can be utilized to recover Zu from the predicted Γx, Γy, Σx, and Σy. As alluded to above, we have more equations than unknowns, hence, we resort to recovering the optimal depth map\n\n4\n\nWeight MapDepth MapDifference Map............ Weight MatrixDifference OperatorDepth VectorDifference Vector...... Zero VectorPublished as a conference paper at ICLR 2023\n\nFigure 3: Overview of our framework. Given an input image, first an encoder is employed to extract features. Then we predict the depth map by the V-layer. Next, we gradually upsample and refine the depth map. In the end, we recover the metric depth by the metric layer.\n\n ̃Z ∗\n\nu ∈ RHW ×1 by minimizing the following equation:\n\n ̃Z ∗\n\nu = arg min\n\n ̃Zu\n\n|| ̃Σ(P ̃Zu − ̃Γ)||2.\n\nRefer Appendix for the full derivation. The closed-form solution can be written as follows:\n\nK ̃Σ (cid:125)(cid:124)\n\n(cid:122) (P T ̃Σ2P )−1P T ̃Σ2 ̃Γ.\n\n(cid:123)\n\n ̃Z ∗\n\nu =\n\n(4)\n\n(5)\n\nDenote K ̃Σ describe the overall network architecture.\n\n≜ (P T ̃Σ2P )−1P T ̃Σ2 in Eq.(5), we write overall equation as ̃Z ∗\n\nu = K ̃Σ\n\n ̃Γ. Next, we\n\n3.2 OVERALL NETWORK ARCHITECTURE\n\nOur overall network architecture is composed of four main modules as follows.\n\n(a) Encoder. Given an input image, the encoder computes the hierarchical feature maps through a series of stages. To be precise, our encoder has four stages. Each stage contains transformer blocks (Liu et al., 2021b). At the end of each stage, we collect the final feature map as the output of the encoder resulting in the encoded feature maps with strides 4, 8, 16, and 32, respectively. Our encoder module is inspired by Liu et al. (2021b), a recent state-of-the-art transformer network design. We use it as our backbone by removing the final global pooling layer and fully connected layer.\n\n(b) Variational Layer (V-Layer). The goal of this layer is to compute a map from encoded feature maps to unscaled depth map, which adheres to the first-order variational constraint. As of V-layer, we feed the feature maps of strides 16 and 32 as input which is the output of the encoder. Since these features are at different resolutions, we upsample the feature map of stride 32 to stride 16 via bi-linear interpolation and concatenate to construct IΦ ∈ RC×H×W , where (H, W, C) symbolizing the height, width, and the number of channels, respectively. Note that H, W is not the same as the original resolution of the ground-truth depth and images. We use of two convolutional layers on IΦ to predict the depth gradient and corresponding weight for each pixel as follows:\n\n{Γx, Γy, Σx, Σy} = f (IΦ; θ)\n\n(6)\n\nwhere, f (IΦ; θ) denotes the convolutional layers with parameters θ. The predicted depth gradients Γx and Γy are observed to be more accurate at smooth surface than at boundaries. This brings\n\n5\n\n(b) V-Layer(c) Upsample and RefineUpsampling.........AAAreArrange Eq. (5)FeatureDepthDepthRefineDepthFeatureFeatureRefineRefineADepth (b) V-LayerOverall Architecture (c) Upsample and Refine (d) Metric Layer(a) EncoderDepth ImageDepth Weight Matrix Difference VectorDifferenceOperatorPublished as a conference paper at ICLR 2023\n\nus again to the point made above that we must take care of which first-order constraint must be included and discarded during regression. Using the Eq.(6) prediction, we construct the variational constraint Eq.(3), and obtain the unscaled depth map following Eq.(5). The resulting depth map has a resolution of 1/16 to the original image, which is later upsampled to the appropriate resolution.\n\nTo capture more scene features, we generate multiple channels (denoted as S) of {Γx, Γy, Σx, Σy} using Eq.(6). As a result, we have a group of depth maps stacked along the channel dimension. For a feature map with spatial resolution H × W , our V-layer has a complexity of O(H 3W 3). To overcome complexity issue, we perform V-layer operation on feature maps with stride 16 and then upsample and refine the depth maps in the later stage. The V-layer pipeline is shown in Fig.3(b).\n\n(c) Upsample and Refine. This module upsamples and refines the input depth map via encoded features at a given depth map resolution. To this end, we perform refinement at three different resolutions in a hierarchical manner. Given the V-layer depth map at 1/16 resolution, we first refine the depth via encoded features at this resolution. Concretely, this refinement is done using the following set of operations. (1) concatenate the feature map and the depth map; (2) use one convolutional layer with ReLU activation to fuse the feature and depth information; and (3) predict refined feature and depth map via a convolutional layer. Later, the refined feature and depth map are upsampled and fed into 1/8 for later refinement using the same above set of operations. Finally, the exact is done at 1/4 resolution. Note that these steps are performed in a sequel. At the end of this module, we have a depth map of 1/4 of the actual resolution. The upsample and refine procedure are shown in Fig.3(c).\n\n(d) Metric Layer. We must infer the global scene scale and shift to recover the metric depth. For this, we perform global max pooling on the encoded feature map of stride 32. The resulting vector is fed into a stack of fully connected layers to regress the two scalars, i.e., one representing the scale and while other representing the shift. Using the feature map of stride 32 is motivated by the observation that we have a much richer global scene context using it than at higher depth resolution. It also provides a good compromise between computational complexity and accuracy.\n\n3.3 LOSS FUNCTION\n\nDepth Loss. It estimates the scale-invariant difference between the ground-truth depth and prediction at train time (Eigen et al., 2014). The difference is computed by upsampling the predicted depth map to the same resolution as the ground truth via bi-linear interpolation. Denoting the predicted and ground-truth depth as ˆZ ∈ Rm×n, Zgt ∈ Rm×n we compute the depth loss as follows\n\nLdepth( ˆZ, Zgt) =\n\n1 N\n\n(cid:88)\n\n(ei,j)2 −\n\n(i,j)\n\nα\n\nN 2 (\n\n(cid:88)\n\ni,j\n\nei,j)2, where, ei,j = log ˆZ i,j − log Z i,j gt .\n\n(7)\n\nHere, N is the number of positions with valid measurements and α ∈ [0, 1] is a hyper-parameter. Note that the above loss is used for valid measurements only.\n\nVariational Loss. We define this loss using the output of V-layer. Suppose the ground-truth depth map to be Zgt ∈ Rm×n and the predicted depth map for S channels as Zu ∈ RS×H×W . Since the depth resolution is not same at this layer, we downsample the ground truth. It is observed via empirical study that low-resolution depth map in fact help capture the first-order variational loss among distant neighbors. Accordingly, we downsample the Zgt instead of upsamping Zu. We downsample Zgt denoted as Qgt ∈ RH×W by random pooling operation, i.e., we randomly select a location where we have a valid measurement since ground-truth data may have pixels with no depth values. The coordinates of selected location in Zgt (cid:55)→ Zu ∈ RS×H×W and the corresponding depth value is put in ˆQ ∈ RS×H×W via bi-linear interpolation. We compute the variational loss as\n\nLvar( ˆQ, Qgt) =\n\n1 N ′\n\n(cid:88)\n\n(i,j)\n\n|Conv( ˆQ)i,j − ∇Qi,j gt |\n\n(8)\n\nwhere N ′ is the number of positions having valid measurements, ∇ symbolises the first-order difference operator, and Conv refers to the convolutional layer. Here, we use the Conv layer to fuse S depth maps into a single depth map and also to compute its horizontal and vertical gradient.\n\nTotal Loss. We define the total loss as the sum of the depth loss and the variational loss i.e., L = Ldepth + λLvar, where λ is the regularization parameter set to 0.1 for all our experiments.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison with the state-of-the-art methods on the NYU test set (Silberman et al., 2012). Please refer to Sec.4.1 for details.\n\nMethod GeoNet (Qi et al., 2018) DORN (Fu et al., 2018) VNL (Yin et al., 2019) TransDepth (Yang et al., 2021) ASN (Long et al., 2021) BTS (Lee et al., 2019) DPT-Hybird (Ranftl et al., 2021) AdaBins (Bhat et al., 2021) ASTrans (Chang et al., 2021) NeWCRFs (Yuan et al., 2022) Ours % Improvement\n\nBackbone ResNet-50 ResNet-101 ResNeXt-101 ViT-B HRNet-48 DenseNet-161 ViT-B EffNet-B5+ViT-mini ViT-B Swin-L Swin-L\n\nSILog ↓ -\n- -\n- -\n11.533 -\n10.570 10.429 9.102 8.198 -9.93%\n\nAbs Rel ↓ 0.128 0.115 0.108 0.106 0.101 0.110 0.110 0.103 0.103 0.095 0.086 -9.47%\n\nRMS↓ 0.569 0.509 0.416 0.365 0.377 0.392 0.357 0.364 0.374 0.331 0.304 -8.16%\n\nRMS log↓ -\n- -\n- -\n0.142 -\n0.131 0.132 0.119 0.108 -9.24%\n\nδ1 ↑ 0.834 0.828 0.875 0.900 0.890 0.885 0.904 0.903 0.902 0.922 0.937 +1.63%\n\nδ2 ↑ 0.960 0.965 0.976 0.983 0.982 0.978 0.988 0.983 0.985 0.992 0.992 +0.00%\n\nTable 2: Comparison with the state-of-the-art methods on the the KITTI official test set (Geiger et al., 2012). We only list the results from the published methods. Please refer to Sec.4.1 for details.\n\nMethod DLE (Liu et al., 2021a) DORN (Fu et al., 2018) BTS (Lee et al., 2019) BANet (Aich et al., 2021) PWA (Lee et al., 2021) ViP-DeepLab (Qiao et al., 2021) NeWCRFs (Yuan et al., 2022) Ours % Improvement\n\nBackbone ResNet-34 ResNet-101 DenseNet-161 DenseNet-161 ResNeXt-101 -\nSwin-L Swin-L\n\nSILog↓ 11.81 11.80 11.67 11.55 11.45 10.80 10.39 9.84 -5.29%\n\nAbs Rel↓ 9.09 8.93 9.04 9.34 9.05 8.94 8.37 7.96 -4.90%\n\nSq Rel↓ 2.22 2.19 2.21 2.31 2.30 2.19 1.83 1.66 -9.29%\n\niRMS↓ 12.49 13.22 12.23 12.17 12.32 11.77 11.03 10.44 -5.35%\n\n4 EXPERIMENTS AND RESULTS\n\nImplementation Details We implemented our method in PyTorch 1.7.1 (Python 3.8) with CUDA 11.0. The software is evaluated on a computing machine with Quadro-RTX-6000 GPU.\n\nDatasets. We performed experiments on three benchmark datasets namely NYU Depth V2 (Silberman et al., 2012), KITTI (Geiger et al., 2012), and SUN RGB-D (Song et al., 2015). (a) NYU Depth V2 contains images with 480 × 640 resolution with depth values ranging from 0 to 10 meters. We follow the train and test set split from Lee et al. (2019), which contains 24,231 train images and 654 test images. (b) KITTI contains images with 352 × 1216 resolution where depth values range from 0 to 80 meters. The official split provides 42,949 train, 1,000 validation, and 500 test images. Eigen et al. (2014) provides another train and test set split for this dataset which has 23,488 train and 697 test images. (c) SUN RGB-D We preprocess its images to 480 × 640 resolution for consistency. The depth values range from 0 to 10 meters. We use the official test set (5050 images) for evaluation.\n\nTraining Details. We use (Liu et al., 2021b) network as our backbone, which is pre-trained on ImageNet (Deng et al., 2009). We use the Adam optimizer (Kingma & Ba, 2014) without weight decay. We decrease the learning rate from 3e−5 to 1e−5 by the cosine annealing scheduler. To avoid over-fitting, we augment the images by horizontal flipping. For KITTI (Geiger et al., 2012), the model is trained for 10 epochs for the official split and 20 epochs for the Eigen split (Eigen et al., 2014). For NYU Depth V2 (Silberman et al., 2012), the model is trained for 20 epochs.\n\nEvaluation Metrics. We report statistical results on popular evaluation metrics such as square root of the Scale Invariant Logarithmic error (SILog), Relative Squared error (Sq Rel), Relative Absolute Error (Abs Rel), Root Mean Squared error (RMS), and threshold accuracy. Mathematical definition related to each one of them is provided in the Appendix.\n\n4.1 COMPARISON TO STATE OF THE ART\n\nTab.(1), Tab.(2), Tab.(3), and Tab.(4) provide statistical comparison results with the competing methods on NYU Depth V2, KITTI official split, KITTI Eigen split, and SUN RGB-D, respectively. Our proposed approach shows the best results for all the evaluation metrics. Particularly on the NYU test set, we reduce the SILog error from the previous best result, 9.102 to 8.198, and increase δ1 from 0.922 to 0.937. More qualitative results including V-layer output are presented in the Appendix.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Comparison with the state-of-the-art methods on the KITTI Eigen test set (Eigen et al., 2014). δ2 ↑ 0.984 0.990 0.994 0.993 0.995 0.995 0.995 0.997 0.997 +0.00%\n\nMethod DORN (Fu et al., 2018) VNL (Yin et al., 2019) TransDepth (Yang et al., 2021) BTS (Lee et al., 2019) DPT-Hybird (Ranftl et al., 2021) AdaBins (Bhat et al., 2021) ASTrans (Chang et al., 2021) NeWCRFs (Yuan et al., 2022) Ours % Improvement\n\nBackbone ResNet-101 ResNeXt-101 ViT-B DenseNet-161 ViT-B EffNet-B5+ViT-mini ViT-B Swin-L Swin-L\n\nRMS log↓ 0.120 0.117 0.098 0.096 -\n0.089 0.089 0.079 0.076 -3.80%\n\nAbs Rel ↓ 0.072 0.072 0.064 0.060 0.062 0.058 0.058 0.052 0.050 -3.85%\n\nSILog ↓ -\n- 8.930 8.933 -\n8.022 7.897 6.986 6.817 -2.42%\n\nδ1 ↑ 0.932 0.938 0.956 0.955 0.959 0.964 0.963 0.974 0.977 +0.03%\n\nRMS↓ 0.273 0.326 0.275 0.280 0.257 0.236 0.269 0.213 0.209 -1.88%\n\nTable 4: Comparison with AdaBins and NeWCRFs on SUN RGB-D test set. All methods are trained on NYU Depth V2 train set without fine-tuning on SUN RGB-D.\n\nMethod AdaBins(Bhat et al., 2021) NeWCRFs (Yuan et al., 2022) Ours % Improvement\n\nBackbone EffNet-B5+ViT-mini Swin-L Swin-L\n\nSILog ↓ 13.652 13.695 12.596 -7.73%\n\nAbs Rel ↓ 0.110 0.105 0.094 -10.48%\n\nRMS↓ 0.321 0.322 0.299 -6.85%\n\nRMS log↓ 0.137 0.138 0.127 -7.30%\n\nδ1 ↑ 0.906 0.920 0.929 +0.98%\n\nδ2 ↑ 0.982 0.980 0.983 +0.10%\n\nFor the SUN RGB-D test set, all competing models, including ours, are trained on the NYU Depth V2 training set (Silberman et al., 2012) without fine-tuning on the SUN RGB-D. In addition, we align the predictions from all the models with the ground truth by a scale and shift following Ranftl et al. (2020). Tab.(4) results show our method’s better generalization capability than other approaches. Extensive visual results are provided in the Appendix and supplementary video.\n\n4.2 ABLATION STUDY\n\nAll the ablation presented below is conducted on NYU Depth V2 test set (Silberman et al., 2012).\n\n(i) Effect of V-Layer. To understand the benefit and outcome of our variational layer compared to other popular alternative layers in deep neural networks for this problem, we performed this ablation study. We replace our V-layer firstly with a convolutional layer and later with a self-attention layer. Tab.(5) provides the depth prediction accuracy for this ablation. For each introduced layer in Tab.(5), the first and second rows show the performance of the depth map predicted with (w) and without (w/o) subsequent refinements (cf. Sec.3.2 (c)), respectively. For the self-attention layer, we follow the ViT (Dosovitskiy et al., 2021) and set the patch size to be one as we use the feature map with stride 16. We also adopt the learnable position embedding (PE) with 128 dimensions. We set the number of heads to be 4 and the number of hidden units to be 512. As shown in Tab.(5), our V-layer indeed helps improve the accuracy of depth prediction compared to other well-known layers. More experiments on KITTI and SUN RBG-D are provided in the Appendix.\n\n(ii) Performance with Different Network Backbone. We evaluate the effects of our V-layer with different types of network backbones. For this ablation, we use Swin-Large (Liu et al., 2021b), Swin-Small (Liu et al., 2021b), and ConvNeXt-Small (Liu et al., 2022). The SILog error is shown in Fig. 4. The results show that our V-layer improves the transformer and the convolutional network performance. An important observation is that our V-layer shows excellent improvements in depth prediction accuracy on weaker network backbones.\n\n(iii) Performance with Change in the Value of S. For this ablation, we change the value of S in the V-layer and observe its effects (cf. Sec.3.2 (b)). By increasing S, we generate more channels of ̃Γ and ̃Σ which in-effect increases V-layer parameters. In the subsequent step, we expand the\n\nTable 5: Benefit of V-layer. We replace the proposed V-layer with a single convolutional layer and a selfattention layer, and evaluate the accuracy of depth map predicted with and without subsequent refinements.\n\nLayer\n\nConvolution\n\nSelf-Attention + PE\n\nV-Layer\n\nRefine w/o w/ w/o w/ w/o w/\n\nSILog ↓ 8.830 8.688 8.790 8.595 8.422 8.198\n\nAbs Rel ↓ 0.090 0.089 0.090 0.089 0.087 0.086\n\nRMS↓ 0.325 0.317 0.318 0.316 0.308 0.304\n\nRMS log ↓ 0.114 0.113 0.114 0.112 0.110 0.108\n\nδ1 ↑ 0.927 0.928 0.927 0.929 0.936 0.937\n\nδ2 ↑ 0.990 0.991 0.990 0.991 0.990 0.992\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nnumber of channels to 128 by a convolutional layer to use the subsequent layers as they are. The results are shown in Tab.(6). For reference, we also present the result by replacing the V-layer with a convolutional layer in the first row in Tab.(6). By increasing S, we reduce the SILog error, at the price of the speed (FPS). Yet, no real benefit is observed with S more than 16.\n\n(iv) Effect of Confidence Weight Matrix & Difference Operator in V-Layer. For this ablation, we study the network’s depth prediction under four different settings. (a) without Vlayer and replace it with convolutional layer (b) without the confidence weight matrix (c) with learnable difference operator and (d) our full model. The depth prediction accuracy observed under these settings is provided in Tab.(7). Clearly, our full model has better accuracy. An important empirical observation, we made during this test is when we keep P learnable V-layer has more learnable parameters, the performance becomes worse than with fixed difference operator.\n\nFigure 4: Evaluation on Swin-L, Swin-S, ConvNeXt-S w/ and w/o the V-layer.\n\nTable 6: Analysis of the number of feature groups. More groups reduce the SILog error. SILog↓ 8.688 8.456 8.198 8.172\n\nAbs Rel↓ 0.089 0.088 0.086 0.085\n\nw/o V-layer 1\n16 128\n\nRMS↓ 0.317 0.310 0.304 0.309\n\nFPS ↑ 9.343 8.175 7.032 3.320\n\nTable 7: Analysis of the confidence weight matrix ̃Σ and the difference operator P .\n\n(a) w/o V-layer (b) w/o ̃Σ (c) learnable P (d) full\n\nSILog↓ 8.688 8.537 8.355 8.198\n\nAbs Rel↓ 0.089 0.089 0.088 0.086\n\nRMS ↓ 0.317 0.316 0.310 0.304\n\n4.3 NETWORK PROCESSING TIME & PARAMETERS\n\nWe compared our method’s inference time and the number of model parameters to the AdaBins (Bhat et al., 2021) and the NeWCRFs (Yuan et al., 2022). The inference time is measured on the NYU Depth V2 test set with batch size 1. We have removed the ensemble tricks in AdaBins and NeWCRFs for an unbiased evaluation, resulting in a slight increase in SILog error as compared to Tab.(1) statistics. As is shown in Tab.(8), our method is faster and better than AdaBins and NeWCRFs using Swin-Small backbone. With the same backbone as the NeWCRFs, i.e., Swin-Large, we achieve much better depth prediction results. Hence, our method with Swin-Small backbone provides a better balance between accuracy, speed and memory foot-print.\n\nTable 8: Comparison of the inference time and parameters to AdaBins and NeWCRFs on NYU Depth V2. We show our results using the Swin-Small and Swin-Large backbone. NeWCRFs (Yuan et al., 2022) 9.171 10.551 258\n\nAdaBins (Bhat et al., 2021) 10.651 5.638 75\n\nSILog Error ↓ Speed (FPS) ↑ Param (M) ↓\n\nOurs (Small) 9.069 11.891 76\n\nOurs (Large) 8.198 7.032 249\n\n5 CONCLUSION\n\nIn conclusion, a simple and effective approach for inferring scene depth from a single image is introduced. The proposed SIDP approach is shown to better exploit the rigid scene prior, which is generally overlooked by the existing neural network-based methods. Our approach does not make explicit assumptions about the scene other than the scene gradient regularity, which holds for typical indoor or outdoor scenes. When tested on popular benchmark datasets, our method shows significantly better results than the prior art, both qualitatively and quantitatively.\n\n6 ACKNOWLEDGMENT\n\nThis work was partly supported by ETH General Fund (OK), Chinese Scholarship Council (CSC), and The Alexander von Humboldt Foundation.\n\n9\n\nConvNeXt-SSwin-SSwin-L8.1988.4229.0699.4599.1479.5198.008.509.009.50SILog Errorw/w/oPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nShubhra Aich, Jean Marie Uwabeza Vianney, Md Amirul Islam, and Mannat Kaur Bingbing Liu. In 2021 IEEE International\n\nBidirectional attention network for monocular depth estimation. Conference on Robotics and Automation (ICRA), pp. 11746–11752. IEEE, 2021.\n\nShariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4009–4018, 2021.\n\nAntonin Chambolle, Vicent Caselles, Daniel Cremers, Matteo Novaga, and Thomas Pock. An introduction to total variation for image analysis. Theoretical foundations and numerical methods for sparse recovery, 9(263-340):227, 2010.\n\nWenjie Chang, Yueyi Zhang, and Zhiwei Xiong. Transformer-based monocular depth estimation with attention supervision. In 32nd British Machine Vision Conference (BMVC 2021), 2021.\n\nWeifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-image depth perception in the wild.\n\nAdvances in neural information processing systems, 29, 2016.\n\nXinjing Cheng, Peng Wang, and Ruigang Yang. Depth estimation via affinity learned with convolutional spatial propagation network. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 103–119, 2018.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\n\nRuofei Du, Eric Turner, Maksym Dzitsiuk, Luca Prasso, Ivo Duarte, Jason Dourgarian, Joao Afonso, Jose Pascoal, Josh Gladstone, Nuno Cruces, et al. Depthlab: Real-time 3d interaction with depth maps for mobile augmented reality. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, pp. 829–843, 2020.\n\nDavid Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using\n\na multi-scale deep network. Advances in neural information processing systems, 27, 2014.\n\nHuan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2002–2011, 2018.\n\nYasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE trans-\n\nactions on pattern analysis and machine intelligence, 32(8):1362–1376, 2009.\n\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pp. 3354–3361. IEEE, 2012.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nDerek Hoiem, Alexei A Efros, and Martial Hebert. Automatic photo pop-up. In ACM SIGGRAPH\n\n2005 Papers, pp. 577–584. Association for Computing Machinery, 2005.\n\nJunjie Hu, Mete Ozay, Yan Zhang, and Takayuki Okatani. Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1043–1051. IEEE, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nPo-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2821–2830, 2018.\n\nBerk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari, and Luc Van Gool. Uncertaintyaware deep multi-view photometric stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12601–12611, 2022.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\n\nlutional neural networks. Advances in neural information processing systems, 25, 2012.\n\nSuryansh Kumar, Yuchao Dai, and Hongdong Li. Monocular dense 3d reconstruction of a complex dynamic scene from two perspective frames. In Proceedings of the IEEE international conference on computer vision (ICCV), pp. 4649–4657, 2017.\n\nSuryansh Kumar, Yuchao Dai, and Hongdong Li. Superpixel soup: Monocular dense 3d reconstruction of a complex dynamic scene. IEEE transactions on pattern analysis and machine intelligence (TPAMI), 43(5):1705–1717, 2019.\n\nMathieu Labbé and François Michaud. Rtab-map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation. Journal of Field Robotics, 36(2):416–446, 2019.\n\nJae-Han Lee and Chang-Su Kim. Monocular depth estimation using relative depth maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729–9738, 2019.\n\nJae-Han Lee and Chang-Su Kim. Multi-loss rebalancing algorithm for monocular depth estimation.\n\nIn European Conference on Computer Vision, pp. 785–801. Springer, 2020.\n\nJin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.\n\nSihaeng Lee, Janghyeon Lee, Byungju Kim, Eojindl Yi, and Junmo Kim. Patch-wise attention network for monocular depth estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 1873–1881, 2021.\n\nBo Li, Chunhua Shen, Yuchao Dai, Anton van den Hengel, and Mingyi He. Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.\n\nJun Li, Reinhard Klein, and Angela Yao. A two-streamed network for estimating fine-scaled depth maps from single rgb images. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3372–3380, 2017.\n\nJulian Lienen, Eyke Hüllermeier, Ralph Ewerth, and Nils Nommensen. Monocular depth estimation via listwise ranking using the plackett-luce model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14595–14604, June 2021.\n\nCe Liu, Shuhang Gu, Luc Van Gool, and Radu Timofte. Deep line encoding for monocular 3d object detection and depth prediction. In 32nd British Machine Vision Conference (BMVC 2021), pp. 354, 2021a.\n\nFayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. Learning depth from single monocular images using deep convolutional neural fields. IEEE transactions on pattern analysis and machine intelligence, 38(10):2024–2039, 2015.\n\nXingtong Liu, Ayushi Sinha, Masaru Ishii, Gregory D Hager, Austin Reiter, Russell H Taylor, and Mathias Unberath. Dense depth estimation in monocular endoscopy with self-supervised learning methods. IEEE transactions on medical imaging, 39(5):1438–1447, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012–10022, 2021b.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11976–11986, 2022.\n\nXiaoxiao Long, Cheng Lin, Lingjie Liu, Wei Li, Christian Theobalt, Ruigang Yang, and Wenping Wang. Adaptive surface normal constraint for depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12849–12858, 2021.\n\nH Christopher Longuet-Higgins. A computer algorithm for reconstructing a scene from two projec-\n\ntions. Nature, 293(5828):133–135, 1981.\n\nR Duncan Luce. Individual choice behavior: A theoretical analysis. Courier Corporation, 2012.\n\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99–106, 2021.\n\nThomas Mollenhoff, Emanuel Laude, Michael Moeller, Jan Lellmann, and Daniel Cremers. In Proceedings of the IEEE Conference\n\nSublabel-accurate relaxation of nonconvex energies. on Computer Vision and Pattern Recognition, pp. 3948–3956, 2016.\n\nThomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1–102:15, July 2022. doi: 10.1145/3528223.3530127.\n\nDavid Nistér. An efficient solution to the five-point relative pose problem. IEEE transactions on\n\npattern analysis and machine intelligence, 26(6):756–770, 2004.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nXiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, and Jiaya Jia. Geonet: Geometric neural network for joint depth and surface normal estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 283–291, 2018.\n\nSiyuan Qiao, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Vip-deeplab: LearnIn Proceedings of the\n\ning visual perception with depth-aware video panoptic segmentation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3997–4008, 2021.\n\nMichael Ramamonjisoa, Yuming Du, and Vincent Lepetit. Predicting sharp and accurate occlusion boundaries in monocular depth estimation using displacement fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14648–14657, 2020.\n\nRené Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020.\n\nRené Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12179–12188, 2021.\n\nAshutosh Saxena, Min Sun, and Andrew Y Ng. Learning 3d scene structure from a single still image. In 2007 IEEE 11th international conference on computer vision, pp. 1–8. IEEE, 2007.\n\nAshutosh Saxena, Min Sun, and Andrew Y Ng. Make3d: Learning 3d scene structure from a single IEEE transactions on pattern analysis and machine intelligence, 31(5):824–840,\n\nstill image. 2008.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and supIn European conference on computer vision, pp. 746–760.\n\nport inference from rgbd images. Springer, 2012.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. In International Conference on Learning Representations, 2015.\n\nShuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understandIn Proceedings of the IEEE conference on computer vision and pattern\n\ning benchmark suite. recognition, pp. 567–576, 2015.\n\nZachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras.\n\nAdvances in Neural Information Processing Systems, 34:16558–16569, 2021.\n\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-\n\ngredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nKe Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, and Zhiguo Cao. Structure-guided ranking loss for single image depth prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 611–620, 2020.\n\nGuanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and Elisa Ricci. Transformer-based attention networks for continuous pixel-wise prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16269–16279, 2021.\n\nNan Yang, Lukas von Stumberg, Rui Wang, and Daniel Cremers. D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1281–1292, 2020.\n\nWei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5684–5693, 2019.\n\nWeihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Neural window fully-connected crfs for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3916–3925, June 2022.\n\nDaniel Zoran, Phillip Isola, Dilip Krishnan, and William T Freeman. Learning ordinal relationships for mid-level vision. In Proceedings of the IEEE international conference on computer vision, pp. 388–396, 2015.\n\nA APPENDIX\n\nA.1 TRAINING DETAILS\n\nWe implement our framework in PyTorch (Paszke et al., 2019). We adopt the Swin-Large (Liu et al., 2021b) as our backbone to conduct ablation experiments and compare with the state of the arts. And the backbone is pre-trained on ImageNet-22K (Deng et al., 2009) by image classification. For training, we use the Adam optimizer (Kingma & Ba, 2014) without weight decay. We decrease the learning rate from 3e−5 to 1e−5 by the cosine annealing scheduler. To avoid over-fitting, we augment the images by horizontal flipping. For both train and test, we keep the resolution of images to be 352 × 1216 in KITTI (Geiger et al., 2012), and 480 × 640 in both NYU Depth V2 (Silberman et al., 2012) and SUN RGB-D (Song et al., 2015). In KITTI, the model is trained for 10 epochs for the official split and 20 epochs for the Eigen split (Eigen et al., 2014). In NYU Depth V2, the model is trained for 20 epochs. We set the batch size to be 4 and 8 respectively for ablation experiments and comparison to the state of the arts.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA.2 EVALUATION METRICS\n\nSuppose the predicted and ground-truth depth to be ˆZ ∈ Rm×n and Zgt ∈ Rm×n, respectively, and the number of valid pixels to be N . We follow the existing methods (Yuan et al., 2022) and utilize the following measures for quantitative evaluation:\n\n• square root of\n\nthe Scale Invariant Logarithmic error\n\n(SILog):\n\n1\n\nN 2 ((cid:80)\n\ni,j ei,j)2, where ei,j = log ˆZ i,j − log Z i,j gt ;\n\n1 N\n\n(cid:80)\n\ni,j(ei,j)2 −\n\n• Relative Squared error (Sq Rel): 1\n\nN\n\n• Relative Absolute Error (Abs Rel): 1\n\nN\n\n• Root Mean Squared error (RMS): 1\n\nN\n\n(cid:80)\n\ni,j( ˆZ i,j − Z i,j (cid:80)\n\ni,j | ˆZ i,j − Z i,j\n\ngt )2/Z i,j gt ; gt |/Z i,j gt ;\n\n(cid:113)(cid:80)\n\ni,j( ˆZ i,j − Z i,j\n\ngt )2; (cid:113)(cid:80)\n\n• Root Mean Squared Logarithmic error (RMS log): 1\n\nN\n\ni,j(ei,j)2;\n\n• threshold accuracy (δk): percentage of ˆZ i,j s.t. max( ˆZi,j Zi,j\n\ngt\n\n,\n\nA.3 DERIVATION OF THE VARIATIONAL CONSTRAINT\n\nZi,j\n\ngt\n\nˆZi,j ) < 1.25k.\n\nIn this part, we introduce the derivation of the variational constraint in more detail. Firstly, we present the detailed form of the difference operator P , the confidence weight matrix ̃Σ, the difference vector ̃Γ and the depth vector ̃Zu, when the depth map Zu is at resolution 2 × 2. Secondly we show the derivation of the optimal solution ̃Z ∗ u. Overall Matrix Form. Suppose the unscaled depth map to be Zu ∈ R2×2, the difference map to be Γx ∈ R2×2 and Γy ∈ R2×2, and the corresponding confidence weight map to be Σx ∈ [0, 1]2×2 and Σy ∈ [0, 1]2×2. We first reorganize the elements in Γx, Γy, Σx, Σy to construct the difference vector ̃Γ ∈ R8×1 and the confidence weight matrix ̃Σ ∈ R8×8:\n\n ̃Γ =\n\n\n\n \n \n \n \n \n\n\n\n \n \n \n \n \n\nΓ1,1 x\nΓ1,2 x\nΓ2,1 x\nΓ2,2 x\nΓ1,1 y\nΓ1,2 y\nΓ2,1 y\nΓ2,2 y\n\n, ̃Σ =\n\n\n\n \n \n \n \n \n\nΣ1,1 x\n0 0\n0 0\n0 0\n0\n\n0 Σ1,2 x\n0 0\n0 0\n0 0\n\n0 0\nΣ2,1 x\n0 0\n0 0\n0\n\n0 0\n0 Σ2,2 x\n0 0\n0 0\n\n0 0\n0 0\nΣ1,1 y\n0 0\n0\n\n0 0\n0 0\n0 Σ1,2 y\n0 0\n\n0 0\n0 0\n0 0\nΣ2,1 y\n0\n\n\n\n \n \n \n \n \n\n.\n\n0 0\n0 0\n0 0\n0 Σ2,2 y\n\nNext, we apply the difference operator P ∈ {1, 0, −1}8×4:\n\nP =\n\n\n\n \n \n \n \n\n\n0 1\n−1 0\n0 0\n0 −1 0\n0 0\n0 1\n−1 0\n0 0 −1 0\n0 0\n0 0\n0\n\n\n\n \n \n \n \n\n\n0 0\n1 1\n0 1\n0 1\n\n ̃Zu =\n\n\n\n  .\n\n\n\n \n\nZ 1,1 u\nZ 1,2 u\nZ 2,1 u\nZ 2,2 u\n\non the depth vector ̃Zu ∈ R4×1:\n\nFinally, we construct the following constraint equation:\n\n ̃ΣP ̃Zu = ̃Σ ̃Γ.\n\n14\n\n(9)\n\nPublished as a conference paper at ICLR 2023\n\nIn the above equation, each row represents a weighted constraint for the first-order difference along the x-axis:\n\nΣi,j\n\nx (Z i,j+1\n\nu\n\n− Z i,j\n\nu ) = Σi,j\n\nx Γi,j\n\nx\n\n(10)\n\nor along the y-axis:\n\nΣi,j\n\ny (Z i+1,j\n\nu\n\nwhere (Z i,j+1 u ) = Γi,j first-order difference, while Σi,j\n\n− Z i,j\n\nx and (Z i+1,j x and Σi,j\n\nu\n\nu\n\n− Z i,j\n\nu ) = Σi,j u ) = Γi,j\n\ny Γi,j y are the variational constraints for the\n\n(11)\n\ny\n\n− Z i,j\n\ny are the confidence weights for the constraints.\n\nOptimal Solution. Given ̃Σ and ̃Γ, we search for the optimal depth vector ̃Z ∗ residual of Eq.(9):\n\nu by minimizing the\n\n ̃Z ∗\n\nu = arg min\n\n ̃Zu\n\n|| ̃Σ(P ̃Zu − ̃Γ)||2.\n\n(12)\n\nFirstly, the objective function, || ̃Σ(P ̃Zu − ̃Γ)||2, is convex with respect to ̃Zu. Because || · ||2 is a convex function. A composition of || · ||2 and the affine function , ̃Σ(P ̃Zu − ̃Γ), is still convex.\n\nSecondly, the optimal solution of a convex function can be found at where the first derivative is zero. Thereby we obtain the final solution ̃Z ∗\n\nu = (P T ̃Σ2P )−1P T ̃Σ2 ̃Γ.\n\nA.4 NETWORK STRUCTURE DETAILS\n\nIn this part, we introduce the detailed structure of our network. In general we set the kernel size of convolutional layers to be 3 unless otherwise stated.\n\n(a) Encoder. We adopt the Swin-Large (Liu et al., 2021b) as our backbone. The network first divides the image into patches each of size 4 × 4, and embeds each patch into a 96-dimensional vector. The above procedure is implemented by a convolutional layer with kernel size 4 and stride 4. Then there are 4 transformation stages to be applied to the embedded vector. The first, second, third and forth stages include 2, 2, 18, 2 blocks, respectively. More specifically, each block will divide the feature map into non-overlapped windows of size 12 × 12 , and compute new features within each window following the transformer (Vaswani et al., 2017). The feature channels in the 4 stages are 192, 384, 768 and 1536, respectively, and the number of heads are 6, 12, 24, and 48, respectively. In the end of each stage, there will be a downsampling operation to reduce the resolution of the feature map by 2. We collect the feature map before the downsampling operation as the output of the stage. In the end, the strides of output feature maps are 4, 8, 16, and 32 respectively. And the channels are 192, 384, 768, and 1536, respectively.\n\n(b) Upsampling 32->16. Given the feature maps of strides 32 and 16 from (a), we first upsample the feature map of stride 32 to stride 16 via bi-linear interpolation. Then, we concatenate the feature maps and obtain a new feature map of stride 16 and channels 1536 + 768 = 2304. We apply a convolutional layer that has 2304 input channels, 2304 output channels, and 4 groups, to fuse the information. We also append an instance normalization layer (Ulyanov et al., 2016) and a LeakyReLU activation function. Next, we apply another convolutional layer with 2304 input channels, 512 output channels to compress the feature channels. Again we append an instance normalization layer and the LeakyReLU activation function. In the end, we add a skip connection between the result feature map and the previous concatenated feature map by addition operation. The concatenated feature map will be transformed to consistent number of channels in advance by a convolutional layer with 2304 input channels and 512 output channels. The final feature map has stride 16 and 512 channels.\n\n(c) V-Layer. The V-layer takes the output feature map from (b) as input. We first utilize a convolutional layer with 512 input channels and 512 output channels to transform the feature map into a more appropriate hidden space. The feature is transformed by the LeakyReLU activation function. Then we utilize another convolutional layer with 512 input channels and 2 × 16 = 32 output channels to predict the gradients along the horizontal and vertical axis, respectively. The number 16 represents that we predict 16 channels of horizontal gradients and 16 channels of vertical gradients respectively, where each channel is expected to capture different information in scenes. Similarly, we also use another convolutional layer with 512 input channels, 2 × 16 = 32 output channels to predict the corresponding confidence weight maps. The confidence weight will be transformed by the Sigmoid function. Then we reshape the predictions and compute the unscaled depth map for\n\n15\n\nPublished as a conference paper at ICLR 2023\n\neach pair of gradient and confidence weight following Eq.(5). The unscaled depth maps computed from all the pairs will be concatenated along the channel. Thereby we obtain a depth map with 16 channels and stride 16. We apply a group normalization on the depth map, where the 16 channels are viewed as a single group. In the end, we expand the channels of the depth map into 128 by a convolutional layer. Thereby the output of the V-layer is a depth map with 128 channels and stride 16.\n\n(d) Refine 16. We refine the feature map from (b) and the depth map from (c). Specifically, we first concatenate the feature map with depth map, and obtain a new feature map with 512 + 128 = 640 channels. Then we employ a convolutional layer with 640 input channels and 640 output channels to fuse the information. The result feature map will be further transformed by the LeakyReLU activation function. Next a convolutional layer with 640 input channels and 128 output channels is applied to predict the refined depth map. Similarly, we also predict the refined feature map by a convolutional layer with 640 input channels and 512 output channels.\n\n(e) Upsampling 16->8. Here, we upsample the feature map from (d) to stride 8 via bi-linear interpolation. To recover the information of scenes, we concatenate with the feature map from the encoder that has the same stride. In such a way, we obtain a new feature map with 384 + 512 = 896 channels and stride 8. Similar to (b), we apply a convolutional layer with 896 input channels, 896 output channels, and 4 groups to fuse the information. Then the feature map is transformed by the instance normalization layer and the LeakyReLU activation function. Next, we apply another convolutional layer with 896 input channels and 256 output channels to compress feature channels. Again, we apply the instance normalization layer and LeakyReLU activation function. Same as (b), we add a skip connection between the result feature map and the previous concatenated feature map by addition operation. The concatenated feature map will be transformed to the consistent number of channels by a convolutional layer with 896 input channels and 256 output channels. Thereby the final feature map has stride 8 and 256 channels.\n\n(f) Refine 8. We refine the depth map from (d) and the feature map from (e). The procedure is the same as (d), except for the number of channels in convolutional layers are adapted accordingly. Thereby we introduce the pipeline briefly. We first upsample the depth map from (d) into stride 8 to concatenate with the feature map. Thereby we obtain a new feature map with 256 + 128 = 384 channels. Then, we apply a convolutional layer with 384 input channels and 384 output channels to fuse the information. The feature map is then transformed by LeakyReLU activation function. Next, we utilize a convolutional layer with 384 input channels and 128 output channels to predict the refined depth map. Similarly, another convolutional layer with 384 input channels and 256 output channels is applied to predict the refined feature map.\n\n(g) Upsampling 8->4. We upsample the feature map from (f) to stride 4. The procedure is the same as (e) except for the number of channels. More specifically, we first upsample the feature map and concatenate with the feature map from the encoder with consistent stride, obtaining a feature map with 256 + 192 = 448 channels and stride 4. Then a convolutional layer with 448 input channels, 448 output channels, and 4 groups is applied to fuse the information. Then the result feature map is transformed by the instance normalization and LeakyReLU activation function. Next, we apply another convolutional layer with 448 input channels and 64 output channels to compress the feature channels. Again, the instance normalization layer and LeakyReLU function is applied. In the end, we add a skip connection between the final feature map and the previous concatenated feature map.\n\n(h) Refine 4. We refine the depth map from (f) and the feature map from (g) following the same pipeline as (d). We first upsample the depth map to stride 4, and concatenate with the feature map, obtaining a new feature map with 64 + 128 = 192 channels and stride 4. Next, we apply a convolutional layer with 192 input channels and 192 output channels to fuse the information, and a LeakyReLU function to transform the feature map. In the end, we utilize another convolutional layer with 192 input channels and 128 output channels to predict the new depth map. The output is a depth map with 128 channels and stride 4.\n\n(i) Metric Layer. We take the feature map with stride 32 from the encoder as input. We first apply global max pooling to compress the feature map into a vector with 1536 channels. Then we apply a fully-connected layer with 1536 input units and 384 output units to compress the channels. Next we apply a LeakyReLU function. In the end, we utilize another fully-connected layer with 384 input units and 2 output units to regress the scale and shift.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 9: Comparison with Ramamonjisoa et al. (2020), Cheng et al. (2018) and Li et al. (2017) on NYU Depth V2 test set (Silberman et al., 2012). SILog ↓ 8.655 8.640 8.557 8.574 8.198\n\nMethod Ramamonjisoa et al. (2020) Cheng et al. (2018) Li et al. (2017) (end-to-end) Li et al. (2017) (optimization) Ours\n\nRMS log↓ 0.113 0.112 0.112 0.112 0.108\n\nAbs Rel ↓ 0.088 0.089 0.088 0.089 0.086\n\nRMS↓ 0.320 0.317 0.315 0.316 0.304\n\nδ2 ↑ 0.991 0.991 0.991 0.991 0.992\n\nδ1 ↑ 0.929 0.930 0.931 0.930 0.937\n\nTable 10: Impact of resolution. We evaluate the performance of V-layer when operating on feature maps of stride 16 and 8, respectively. FPS↑ 7.032 2.597\n\nRMS log↓ 0.108 0.108\n\nAbs Rel ↓ 0.086 0.086\n\nSILog ↓ 8.198 8.149\n\nRMS↓ 0.304 0.306\n\nStride 16 8\n\nδ1 ↑ 0.937 0.936\n\nδ2 ↑ 0.992 0.992\n\n(j) Final Prediction. We take the output from (d),(f), (h) and (i) as input. We upsample all the depth maps to stride 1 via bi-linear interpolation, and fuse the depth maps into a single depth map by the addition operation and a convolutional layer with 128 input channels and 1 output channel. In the end, we add the depth map with the shift and multiply with the scale from (i), respectively.\n\nA.5 STATISTICAL EVALUATION WITH OTHER METHODS\n\nIn this section, we compare with Ramamonjisoa et al. (2020), Cheng et al. (2018), and Li et al. (2017) on NYU Depth V2 (Silberman et al., 2012). More specifically, we apply their methods to refine the final predicted depth map. The displacement field, the affinity matrix, and the depth gradient are predicted from the refined feature map, which is the output of the last refinement module. For Cheng et al. (2018) we adopt their default hyper-parameters. The kernel size is 3, and the number of iterations is 24. Li et al. (2017) proposes the end-to-end fusion strategy and the optimization-based fusion strategy. For the end-to-end fusion strategy, we employ three convolutional layers with kernel size 5, and 16 channels. For the optimization-based fusion strategy, we use Adam optimizer. The learning rate is 0.01 and the number of iterations is 100. The results are shown in Tab. (9). Our method achieves better performance.\n\nA.6 ABLATION ON RESOLUTION\n\nIn this part, we evaluate the performance of the V-layer when applying on the feature map with stride 8. We achieve this by dividing the feature map into 4 rectangle sub-regions with equal area (top left, top right, bottom left and bottom right). We apply a V-layer on each sub-region. The predicted depth maps from the sub-regions will be stacked to recover the original resolution. The results are shown in Tab.(10). From stride 16 to 8, the improvement of accuracy is marginal. However, the inference speed (FPS) is significantly slow down.\n\nA.7 ABLATION ON KITTI AND SUN RGB-D\n\nTo further demonstrate the benefits of our V-layer on KITTI Eigen split (Geiger et al., 2012; Eigen et al., 2014) and SUN RGB-D test set (Song et al., 2015), we present the accuracy of the predicted depth map when replacing the V-layer with a single convolutional layer. As shown in Tab.(11), our V-layer can improve the depth map accuracy on both KITTI and SUN RGB-D.\n\nDataset\n\nTable 11: Benefit of V-layer. We replace the proposed V-layer with a single convolutional layer, and evaluate the predicted depth map accuracy. Layer Convolution V-Layer Convolution V-Layer\n\nRMS log↓ 0.079 0.076 0.132 0.127\n\nAbs Rel ↓ 0.052 0.050 0.098 0.094\n\nSILog ↓ 6.996 6.817 13.172 12.596\n\nRMS↓ 0.217 0.209 0.309 0.299\n\nδ1 ↑ 0.975 0.977 0.924 0.929\n\nδ2 ↑ 0.997 0.997 0.981 0.983\n\nKITTI\n\nSUN\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n(a) Image\n\n(b) Confidence Weight\n\n(c) Difference Map\n\n(d) Depth Map\n\nFigure 5: Visualization of V-layer Prediction. We visualize the confidence weight Σx, the difference map Γx and the depth map Zu from the V-layer when predicting on NYU Depth V2 test set.\n\nTable 12: Comparison with AdaBins and NeWCRFs on NYU test set. All methods are trained on KITTI Eigen train set without fine-tuning on NYU.\n\nMethod AdaBins(Bhat et al., 2021) NeWCRFs (Yuan et al., 2022) Ours\n\nBackbone EffNet-B5+ViT-mini Swin-L Swin-L\n\nSILog ↓ 28.147 21.138 18.090\n\nAbs Rel ↓ 0.251 0.173 0.148\n\nRMS↓ 0.753 0.551 0.474\n\nRMS log↓ 0.286 0.213 0.182\n\nδ1 ↑ 0.614 0.755 0.804\n\nδ2 ↑ 0.867 0.934 0.955\n\nA.8 MORE GENERALIZATION EXPERIMENTS\n\nWe further evaluated the generalization performance when training on KITTI and test on NYU, or training on NYU and test on KITTI. The results are shown in Tab.(12) and Tab.(13). Our method achieves better generalization performance in both settings.\n\nTable 13: Comparison with AdaBins and NeWCRFs on KITTI Eigen test set. All methods are trained on NYU Depth V2 train set without fine-tuning on KITTI Eigen.\n\nMethod AdaBins(Bhat et al., 2021) NeWCRFs (Yuan et al., 2022) Ours\n\nBackbone EffNet-B5+ViT-mini Swin-L Swin-L\n\nSILog ↓ 56.871 54.460 42.105\n\nAbs Rel ↓ 0.350 0.268 0.221\n\nRMS↓ 7.221 6.246 5.360\n\nRMS log↓ 0.579 0.550 0.426\n\nδ1 ↑ 0.434 0.512 0.598\n\nδ2 ↑ 0.744 0.833 0.888\n\nA.9 SPARSE GROUND-TRUTH DEPTH MAP\n\nKITTI provides sparse LiDAR measurements as the ground truth (official ground truth has been inpainted to an extent). Yet, our method could work well in such cases where sparse LiDAR measurements are known. Our algorithm computes the depth map from the gradient predicted by the network in a differentiable way. Thereby, for such a case, when we apply sparse supervision on the computed depth map (note that the loss function is used for valid measurements only), the error signal from the loss function will be back-propagated to the predicted gradient to supervise the network\n\n18\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\n(f)\n\nFigure 6: Qualitative comparison on KITTI Eigen split (Eigen et al., 2014). For each column, from top to bottom we present the input image, the prediction from AdaBins (Bhat et al., 2021), NeWCRFs (Yuan et al., 2022), and our framework respectively.\n\nto learn gradient clues. We demonstrated the effectiveness of our algorithm in Tab.(2) and Tab.(3) for such cases.\n\nA.10 MORE VISUALIZATION\n\nWe visualize the confidence weight map Σx, the difference map Γx, and the depth map Zu from the V-layer in Fig.5. We observe that the depth value of a pixel shows correlation with respect to the image coordinates of the pixel. For example, in the last example in Fig.5, for different pixels at the door, the depth values are usually different but the first-order difference are approximately the same. This observation shows that the difference map might be easier to predict than the depth map.\n\nA.11 QUALITATIVE RESULTS\n\nWe provide more qualitative results on KITTI Eigen split (Eigen et al., 2014), SUN RGB-D (Song et al., 2015), and NYU Depth V2 (Silberman et al., 2012) in Fig.6, Fig.7 and Fig.8, respectively. Our framework predicts more accurate shapes and preserves the high-frequency scene information.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\n(a) Image\n\n(b) AdaBins\n\n(c) NeWCRFs\n\n(d) Ours\n\nFigure 7: Qualitative comparison with AdaBins (Bhat et al., 2021), NeWCRFs (Yuan et al., 2022) on SUN RGB-D test set (Song et al., 2015). All the models are pre-trained on NYU Depth V2 (Silberman et al., 2012) training set.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\n(a) Image\n\n(b) AdaBins\n\n(c) NeWCRFs\n\n(d) Ours\n\n(e) Ground Truth\n\nFigure 8: Qualitative comparison with AdaBins (Bhat et al., 2021), NeWCRFs (Yuan et al., 2022) on NYU Depth V2 test set (Silberman et al., 2012). The ground-truth depth map are in-painted for visualization.\n\n21",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a new network for single view depth estimation. The core technical part is a V-layer that predicts depth gradients and feature weights and solves a refined depth map. Finally, the refined depth is further refined (by black-box operators) to a larger resolution and re-scaled/shifted to the final prediction. Experimental results on NYUd2, KITTI (official and eigen splits) and SUNRGBD set a clear new state-of-the-art. Codes are promised.\n\n# Strength And Weaknesses\n\nStrengths:\n+ Well-written and well-organized.\n+ Clear new state-of-the-art results on an important problem.\n\nWeaknesses:\n- My first major criticism is that the method should be compared with other methods that implements the same idea. In Table.5, we can see that the V-layer out-performs convolution and self-attention. This is not enough. The method should be compared with [A][B][C] and demonstrate why V-layer works and the former formulations do not.\n[A] Predicting sharp and accurate occlusion boundaries in monocular depth estimation using displacement fields, CVPR 2020\n[B] Depth estimation via affinity learned with convolutional spatial propagation network, ECCV 2018\n[C] A Two-Streamed Network for Estimating Fine-Scaled Depth Maps From Single RGB Images, ICCV 2017\n- The motivation and impact of the Conv layer in Equation.8 is not clear. Although I understand that in end-to-end deep models we can always some learnable layers that mysteriously improves performance, this one needs a clearer justification because it belongs to the core technical module.\n- Minor: Claiming MIDAS uses external is unfair as it is evaluated in a zero-shot setting. This can be misleading for future papers.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity is good except for equation. I don't have comments on quality, as it's good. Novelty issues are noted in the last box about [A/B/C]. Reproducibility seems good as a code release is promised.\n\n# Summary Of The Review\n\nGenerally, I think the paper should be accepted but still has issues (see weaknesses) to be addressed. I am now voting a 6 but I can vote a 8 if convinced.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPRUNING BY ACTIVE ATTENTION MANIPULATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFilter pruning of a CNN is typically achieved by applying discrete masks on the CNN’s filter weights or activation maps, post-training. Here, we present a new filter-importance-scoring concept named pruning by active attention manipulation (PAAM), that sparsifies the CNN’s set of filters through a particular attention mechanism, during-training. PAAM learns analog filter scores from the filter weights by optimizing a cost function regularized by an additive term in the scores. As the filters are not independent, we use attention to dynamically learn their correlations. Moreover, by training the pruning scores of all layers simultaneously, PAAM can account for layer inter-dependencies, which is essential to finding a performant sparse sub-network. PAAM can also train and generate a pruned network from scratch in a straightforward, one-stage training process without requiring a pre-trained network. Finally, PAAM does not need layer-specific hyperparameters and pre-defined layer budgets, since it can implicitly determine the appropriate number of filters in each layer. Our experimental results on different network architectures suggest that PAAM outperforms state-of-the-art structuredpruning methods (SOTA). On CIFAR-10 dataset, without requiring a pre-trained baseline network, we obtain 1.02% and 1.19% accuracy gain and 52.3% and 54% parameters reduction, on ResNet56 and ResNet110, respectively. Similarly, on the ImageNet dataset, PAAM achieves 1.06% accuracy gain while pruning 51.1% of the parameters on ResNet50. For Cifar-10, this is better than the SOTA with a margin of 9.5% and 6.6%, respectively, and on ImageNet with a margin of 11%.\n\n1\n\nINTRODUCTION\n\nConvolutional Neural Networks (CNNs) LeCun et al. (1989) are used nowadays in a wide variety of computer-vision tasks. Large CNNs in particular, achieve considerable performance levels, but with significant computation, memory, and energy footprints, respectively Sui et al. (2021). As a consequence, they cannot be effectively employed in resource-limited environments such as mobile or embedded devices. It is therefore essential to create smaller models, that can perform well without significantly sacrificing their accuracy and performance. This goal can be accomplished by either designing smaller, but performant, network architectures Lechner et al. (2020); Tan & Le (2019) or by first training an over-parameterized network, and sparsifying it thereafter, by pruning its redundant parameters Han et al. (2016); Liebenwein et al. (2020; 2021). Neural-network pruning is defined as systematically removing parameters from an existing neural network Hoefler et al. (2021). It is a popular technique to reduce growing energy and performance costs and to support deployment in resource-constrained environments such as smart devices. Various pruning approaches have been developed, and this has gained considerable attention over the past few years Zhu & Gupta (2017); Sui et al. (2021); Liebenwein et al. (2021); Peste et al. (2021); Frantar et al. (2021); Deng et al. (2020).\n\nFigure 1: Sensitivity-based filter pruning schedule.\n\nPruning methods are categorized into either unstructured or structured. The first, remove individual weight-parameters, only Han et al. (2016). The second, remove entire groups, by pruning neurons, filters, or channels, respectively Anwar et al. (2017); Li et al. (2019); He et al. (2018b); Liebenwein\n\n1\n\nEndlossepochWarm-up on dense network Train Train sparse-network’s weights Fine-tune sparse networkPAAMPruning by Active Attention Manipulation (PAAM)Under review as a conference paper at ICLR 2023\n\nFigure 2: PAAM learns the importance scores of the filters from the filter weights.\n\net al. (2020). As modern hardware is tuned towards dense computations, structured pruning offers a more favorable balance between accuracy and performance Hoefler et al. (2021). A very prominent family of structured-pruning methods is filter pruning. Choosing which filters to remove according to a carefully chosen importance metric (or filter score) is an essential part of any method in this family.\n\nData-free methods solely rely on the value of weights, or the network structure, in order to determine the importance of filters. Magnitude pruning, for example, is one of the simplest and most common of such methods. It prunes the filters that have the smallest weight-values in the l1 norm. Data-informed methods focus on the feature maps generated from the training data (or a subset of samples) rather than the filters alone. These methods vary from sensitivity-based approaches (which consider the statistical sensitivity of the output feature maps with regard to the input data Malik & Naumann (2020); Liebenwein et al. (2020)), to correlation-based methods (with an inter-channel perspective, to keep the least similar (or least correlated) feature maps Sun et al. (2015); Sui et al. (2021)).\n\nBoth data-free and data-informed methods generally determine the importance of a filter in a given layer, locally. However, filter-importance is a global property, as it changes relative to the selection of the filters in the previous and next layers. Moreover, determining the optimal filter-budget for each layer (a vital element of any pruning method), is also a challenge, that all local, importance-metric methods face. The most trivial way to overcome these challenges, is to evaluate the network loss with and without each combination of k candidate filters out of N . However, this approach would require the evaluation of (cid:0)N\n\n(cid:1) subnetworks, which is in practice impossible to achieve.\n\nk\n\nTraining-aware pruning methods aim to learn binary masks for turning on and off each filter. A regularization metric often accompanies them, with a penalty guiding the masks to the desired budget. Mask learning, simultaneously for all filters, is an effective method for identifying a globally optimal subset of the network. However, due to the discrete nature of the filters and binary masks, the optimization problem is generally non-convex and NP-hard. A simple trick of many recent works Gao et al. (2020; 2021); Li et al. (2022) is to use straight-through estimators Bengio et al. (2013) to calculate the derivatives, by considering binary functions as identities. While ingenious, this precludes learning the relative importance of filters among each other. Even more importantly, the on-off bits within the masks are assumed to be independent, which is a gross oversimplification.\n\nThis paper solves the above problems by introducing PAAM, a novel end-to-end pruning method, by active attention manipulation. PAAM also employs an l1 regularization technique, encouraging filterscore decrease. However, PAAM scores are analog, and multiply the activation maps during score training. Moreover, a proper score spread is ensured through a specialized activation function. This allows PAAM to learn the relative importance of filters globally, through gradient descent. Moreover, the scores are not considered independent, and their hidden correlations are learned in a scalable fashion, by employing an attention mechanism specifically tuned for filter scores. Given a global pruning budget, PAAM finds the optimal pruning threshold from the cumulative histogram of filter scores, and keeps only the filters within the budget. This relieves PAAM from having to determine per layer allocation budgets in advance. PAAM then retrains the network without considering the scores. This process is repeated until convergence. PAAM pipeline is shown in Figures 1-2. Our experimental results show that PAAM yields higher pruning ratios while preserving higher accuracy.\n\nIn summary, this work has the following contributions:\n\n1. We introduce PAAM, an end-to-end algorithm that learns the importance scores directly from the network weights and filters. Our method allows extracting hidden correlations in\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nthe filter weights for training the scores, rather than relying only on the weight magnitudes. The feature maps are multiplied by our learned scores during training. This way our method implicitly accounts for the data samples through loss propagation, enabling PAAM to enjoy the advantages of both data-free and data-informed methods.\n\n2. PAAM automatically calculates global importance scores for all filters and determines\n\nlayer-specific budgets with only one global hyper-parameter.\n\n3. We empirically investigate the pruning performance of PAAM in various pruning tasks and compare it to advanced state-of-the-art pruning algorithms. Our method proves to be competitive and yields higher pruning ratios while preserving higher accuracy.\n\n2 PRUNING BY ACTIVE ATTENTION MANIPULATION ALGORITHM\n\nIn this section, we first introduce our notation and then incrementally describe our pruning algorithm.\n\n2.1 Notation. The filter weights of layer l are given by the tuple Fl ∈ IRF ×C×K×K where F is the number of filters, C the number of input channels, and K the size of the convolutional kernel. The feature maps of layer l are given by the tuple Al ∈ IRF ×H×W where H and W are the image height and width respectively. For simplicity, we ignore the batch dimension in our formulas.\n\n2.2 Score Learning. The score-learning function of PAAM, for a layer l of the CNN to be pruned, can be intuitively understood as a single-layer independent network, whose inputs are the filter weights Fl of layer l, and the outputs are the scores Sl associated to the filters. The network first transforms the input weights Fl to a score vector Fl W F , whose length equals the number F of filters of layer l, and then passes the result through an activation function φ, properly spreading the scores within the [0, 1 + ε] interval. The resulting scores are then used by an l1 regularization term of the cost function. The choice of φ and regularization term is discussed in the next sections. Formally:\n\nSl = φ(Fl W F )\n\n(1)\n\nwhere φ is the activation function and W F ∈ IR(F ×C×K×K)×F is the weight matrix. This transformation, through W F and φ (vanilla PAM), is similar to additive attention Bahdanau et al. (2014). We will therefore refer in the following to the score-learning network as the attention network (AN). Intuitively, the AN weight matrix W F captures the hidden correlations among filters. Unfortunately, for layers with a large number of filters, as for ImageNet, this matrix will become too large to train.\n\nTo capture the correlations in a scalable fashion, we would like to first partition the input in chunks, compute the correlations within each chunk, and then compute the correlations among chunks. But this is exactly what a scaled dot-product attention achieves Vaswani et al. (2017a). First, PAAM reformats the input as a binary matrix Fl ∈ IR(F )×(C×K×K), where the chunks are its rows. Second, it obtains the correlations within each chunk as the queries Ql = Fl W Q and keys Kl = FlW K. Here, the query and key weight matrices W Q, W K ∈ IR(C×K×K)×dl are much smaller, as they consider only a chunk, and dl = F is the hidden dimension of the layer. The queries and keys are of shape Ql, Kl ∈ IR(F )×(dl). Third, PAAM obtains the cross correlations among chunks by multiplying the query matrix Ql with the transpose K T l of the key matrix. Finally, the scores are obtained by averaging and normalizing the result, and passing it through the activation function φ. Formally:\n\nSl = φ(\n\nmean(Ql × K T √\n\nl )\n\nα\n\ndl\n\n)\n\n(2)\n\nwhere α is a scaling factor that we tune. Note that PAAM does not need to learn the value-weight matrix, compute the values, and multiply them with the above result, as it is only interested in the correlation among filters. Learning filter weights is left to CNN training. However, the scores Sl learned by PAAM, are then pointwise (⊙) multiplied by the feature maps Al of the same layer:\n\nA\n\n′\n\nl = Sl ⊙ Al\n\n(3)\n\nThe closer a filter score is to 1, the more the corresponding feature map is preserved. Note also that by using an analog value for scores while training the AN, allows PAAM to compare the relative importance of scores later on. In particular, this is useful when PAAM employs the globally allocated budget, and the cumulative score distribution, to select the filters to be used during CNN training.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n2.3 Activation Function. SoftMax is the typical choice of activation function in additive attention when computing importance Vaswani et al. (2017b). However, SoftMax is not a suitable choice for filter scores. While the range of its outputs is between 0 and 1, the sum of its outputs has to be 1, meaning that either all scores will have a small value, or there will be only one score with value 1.\n\nIn contrast to SoftMax, we would intuitively want that many filter scores are possibly close to 1. More formally, the scores should have the following three main attributes: 1) All filter scores should have a positive value that ranges between 0 and 1, as is the case in SoftMax. 2) All filter scores should adapt from their initial value of 1, as we start with a completely dense model. 3) The filter-scores activation function should have non-zero gradients over their entire input domain.\n\nSigmoidal activations satisfy Attributes 1 and 3. However, they have difficulties with Attribute 2. For high temperatures, sigmoids behave like steps, and scores quickly converge to 0 or 1. The chance these scores change again is very low, as the gradient is close to zero at this point. Conversely, for low temperatures, scores have a hard time converging to 0 or 1. Finding the optimal temperature needs an extensive search for each of the layers separately. Finally, starting from a dense network with all scores set to 1 is not feasible.\n\nTo satisfy Attributes 1-3, we designed our own activation function, as shown in Figure 3. First, in order to ensure that scores are positive, we use an exponential activation function and learn its logarithm value. Second, we allow the activation to be leaky, by not saturating it at 1, as this would result in 0 gradients, and scores getting stuck at 1. Formally, our leaky-exponential activation function is defined as follows, where a is a small value (an ablation study in provided in Appendix):\n\nFigure 3: The leaky-exponential activation function.\n\nφ(x) =\n\n(cid:26)ex\n\n1.0 + ax\n\nif x < 0 if x ≥ 0\n\n(4)\n\n2.4 Optimization Problem. The PAAM optimization problem can be formulated as in Equation (5), where L is the CNN loss function, and f is the CNN function with inputs x, labels y, and parameters W, modulated by the AN with inputs the filter weights, parameters V, and outputs the scores S. Let ∥g(Sl, p)∥1 denote the l1-norm of the binarized filter scores of layer l, Fl the number of filters of layer l, L the number of layers, and p the pruning ratio. Then PAAM should preserve only as many filters as desired by p, while minimizing at the same time the loss function.\n\nmin V\n\nL(y, f (x; W, S))\n\ns.t.\n\nL (cid:88)\n\nl=0\n\n∥g(Sl, p)∥1 − p\n\nL (cid:88)\n\nl=0\n\nFl = 0\n\n(5)\n\nFunction g(Sl, p) is discussed in next section. The budget constraint is addressed by adding an l1 regularization term to the loss function, while training the weights of the AN. This term employs the analog scores of the filters in each layer, which are computed as described above. Formally:\n\n′\n\nL\n\n(S) = L(y, f (x; W, S)) + λ\n\nL (cid:88)\n\nl=0\n\n∥Sl∥1\n\n(6)\n\nwhere λ is a global hyper-parameter controlling the regularizer’s effect on the total loss. Since the loss function consists now of both classification accuracy and the l1-norm cost of the scores, reducing filter scores to decrease l1 cost, directly influences accuracy as scores multiply the activation maps.\n\nIn more detail, by incorporating the classification and the l1-norm of the scores in the loss function, the effect of the score of each filter is accounted for in the loss value in two ways:\n\n1. When multiplied by small scores, feature maps have less influence on classification. This\n\nmay result in a larger loss if these maps are important and vice versa.\n\n2. On the other hand, the part of the loss function consisting of the l1-norm of the scores,\n\ndecreases the loss value when there are more scores with small values.\n\nIn some sense, this duality mimics the way synapses are pruned or strengthened through habituation or conditioning in nature. If the use of a synapse does not lead to a reward it is pruned, otherwise, it is strengthened. Moreover, pruning is essential in order to avoid saturation and enable learning.\n\n4\n\n32101230.00.20.40.60.81.0exp(x)1+axUnder review as a conference paper at ICLR 2023\n\nThe value of λ plays a very important role in keeping the balance between the two factors, and in controlling the pruning ratio. When λ is 0, all scores will stay close to 1, and when it is large, all scores will converge to 0. During training of the AN layers, we freeze all CNN parameters. The scores are directly learned from the filter weights, reflecting attributes such as the filter-weights magnitude and the correlation among the different filters of the same layer. A globally pruned network is obtained by simultaneously training the scores of all layers. This allows propagating the scored influence of the feature maps of one layer to the feature maps and scores of the next layers.\n\n2.5 Training schedule. PAAM starts training the CNN by a number of warm-up epochs. During this phase, it trains the dense CNN with all filter scores fixed to 1, and the AN weights frozen. After the warm-up phase, PAAM starts training both the AN weights and the CNN weights.\n\nAs discussed, PAAM multiplies the CNN feature maps by the AN analog scores. This way, when a score is 1, the corresponding feature map is fully preserved. As the score gets smaller, its feature map is getting weaker, since it is multiplied by a value smaller than 1, and finally almost completely pruned, as the score gets closer to 0. However, while the scores are getting trained, the network weights can learn to adapt to the feature-map intensity. This means that, although a filter may have a low score, the loss of the feature map intensity can be compensated by magnifying the feature map itself. In order to prevent this, PAAM uses an alternating training approach Peste et al. (2021).\n\nAlgorithm 1 PAAM\n\nInputs: Mini-batches B; CNN N after warmup, parametrized by W; AN parametrized by V; Regularization hyper-parameter λ; PAAMtraining epochs Ep; Training mini-epochs Es. Output: Pruned CNN for fine-tuning. for i in Ep do\n\nfor j in Es do\n\nfor b in B do\n\nminV L(y, f (x; W, S)) + λ (cid:80)L calculate gradients w.r.t V Update V by the optimizer\n\nl=0 ∥Sl∥1\n\nend for\n\nend for for j in Es do\n\nfor b in B do\n\nminW L(y, f (x; W, S)) calculate gradients w.r.t W Update W by optimizer\n\nFirst, PAAM trains the AN for a few epochs, while freezing all CNN parameters. Then it trains the sparse CNN for a few epochs, while freezing all AN parameters. Sparsity is obtained by multiplying feature maps Al with a binary version g(Sl, p) of scores learned so far. Function g is described in the next section. AN training and CNN training alternate for a predefined number of training cycles. After the training phase is completed, PAAM finishes the pruning procedure, by fine-tuning the sparse model. Note that PAAM does not need a pretrained dense model to find the importance of filters or feature maps. Even when starting from scratch, after a warm-up phase, it can find an optimal sparse sub-network by training the AN and CNN weights, respectively, as discussed above.\n\nend for Return N with parameters W\n\nend for\n\nend for\n\n2.6 Binarization function. As mentioned above, during CNN training, PAAM fixes the scores to 1, for all filters retained according to their score magnitude and pruning ratio. All other filters have score 0. The analog-to-digital transformation is accomplished by the function g(s, p), as follows:\n\ng(s, p) =\n\n(cid:26)1 0\n\nif s ≥ θ(S, p) otherwise.\n\n(7)\n\nwhere s is the analog-score value of a filter, and θ(S, p) is the optimal pruning threshold that is computed, by considering the cumulative distribution of all scores s ∈ S, and the pruning ratio p. We will discuss how PAAM has computed θ(S, p) for Cifar10 and ImageNet, in Section 3.\n\n2.7 Complete Algorithm Algorithm 1 describes PAAM’s intermediate stage. While training the AN parameters, it uses Equations (4-6). While training the CNN parameters, it uses Equation (7).\n\n3 EXPERIMENTAL EVALUATION\n\nIn this section, we first present our implementation details and then discuss our experimental results.\n\nBaselines. We compare PAAM to numerous standard and advanced pruning methods. The models include: L1 norm Li et al. (2017), neuron-importance score propagation (NISP) Yu et al. (2017), soft\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Results on the Cifar10 dataset with ResNet56 and ResNet110 for medium (top part) and for large (bottom part) pruning ratios, respectively. The quantity ∆ shows the difference in the accuracy (Acc) between the baseline dense model used, and the resulting pruned network. Note that PAAM does not use a pretrained baseline. Numbers are taken from the reported results of the cited papers.\n\nMethod\n\nBaseline Acc(%)\n\nPruned Acc(%) ∆(%)\n\n↓Params(%)\n\n↓Flops(%)\n\nResNet56\n\nl1 Norm (2017) Li et al. (2017) NISP (2017) Yu et al. (2017) SFP (2018) He et al. (2018a) DCP (2018) Zhuang et al. (2018) DCP-Adapt (2018) Zhuang et al. (2018) CCP (2019) Peng et al. (2019) GAL (2019) Lin et al. (2019) HRank (2020) Lin et al. (2020) DMC (2020) Gao et al. (2020) NPPM (2021) Gao et al. (2021) CHIP (2021) Sui et al. (2021) PAAM(Vanilla) PAAM(KQ)\n\nAMC (2018) He et al. (2018d) GAL (2019) Lin et al. (2019) HRank (2020) Lin et al. (2020) CHIP (2021) Sui et al. (2021) PAAM(Vanilla) PAAM(KQ)\n\nl1 Norm (2017) Li et al. (2017) NISP (2018) Yu et al. (2017) SFP (2018) He et al. (2018a) GAL (2019) Lin et al. (2019) HRank (2020) Lin et al. (2020) CHIP (2021) Sui et al. (2021) PAAM(Vanilla) PAAM(KQ)\n\nGAL (2019) Lin et al. (2019) HRank (2020) Lin et al. (2020) CHIP (2021) Sui et al. (2021) PAAM(Vanilla) PAAM(KQ)\n\n93.04 N/A 93.59 93.80 93.80 93.50 93.26 93.26 93.62 93.04 93.26\n\n92.80 93.26 93.26 93.26\n\n93.53 N/A 93.68 93.50 93.50 93.50\n\n93.50 93.50 93.50\n\n93.06 N/A 93.78 93.59 93.81 93.46 93.38 93.52 93.69 93.40 94.16 94.28 94.01\n\n91.90 91.58 90.72 92.05 92.42 93.10\n\n93.30 N/A 93.86 93.59 94.23 94.44 94.69 94.63\n\n92.74 92.65 93.63 93.68 93.99\n\n+0.02 -0.03 +0.19 -0.31 +0.01 -0.04 +0.12 +0.26 +0.07 +0.36 +0.90 +1.02 +0.75\n\n-0.90 -1.68 -2.54 -1.21 -0.84 -0.16\n\nResNet110\n\n-0.23 -0.18 +0.18 +0.09 +0.73 +0.94 +1.19 +1.13\n\n-0.76 -0.85 +0.13 +0.18 +0.49\n\n13.7 42.2 N/A N/A N/A N/A 11.8 16.8 N/A N/A 42.8 52.3 54.0\n\nN/A 65.9 68.1 71.8 83.0 70.0\n\n32.4 43.78 N/A 18.7 39.4 48.3 54.9 48.3\n\n44.8 68.7 68.3 79.3 66.5\n\n27.6 35.5 41.1 50.0 47.0 47.0 37.6 29.3 50.0 50.0 47.4 49.3 52.3\n\n50.0 60.2 74.1 72.3 70.5 74.7\n\n38.7 43.25 40.8 4.1 41.2 52.1 51.3 40.9\n\n48.5 68.6 71.3 74.8 77.2\n\nfilter pruning (SFP) He et al. (2018a), discrimination-aware channel pruning (DCP) Zhuang et al. (2018), DCP-adapt Zhuang et al. (2018), collaborative channel pruning (CCP) Peng et al. (2019), generative adversarial learning (GAL) Lin et al. (2019), filter-pruning using high-rank feature maps (HRank) Lin et al. (2020), discrete model compression (DMC) Gao et al. (2020), network pruning via performance maximization (NPPM) Gao et al. (2021), channel independence-based pruning (CHIP) Sui et al. (2021), and auto-ML for model compression (AMC) He et al. (2018d).\n\n3.1\n\nIMPLEMENTATION DETAILS\n\nTraining Procedure. Our first experiments are on CIFAR-10 with two different models: ResNet56 and ResNet110. For both models, PAAM does not use a fully trained network as the baseline to prune. We train each model for 50 warm-up epochs. During warm-up (see Figure 1), we use a batch size of 256 and stochastic gradient descent (SGD) as optimizer, with 0.1 as the initial learning rate, 0.9 as momentum, and 0.0005 for the weight decay. We then use PAAM to train the scores.\n\nAfter warm-up, PAAM trains the AN and the CNN in alternation, with the other-network parameters frozen. PAAM trains the AN weights for 3 epochs, and uses the regularized loss defined in Equation (10). Then PAAM switches to training the CNN weights for 6 epochs, and uses the classification loss. In this phase, PAAM employs Equation equation 7 as the analog-to-digital score-conversion function. When PAAM trains the AN, the CNN feature maps get multiplied by the analog scores. PAAM repeats these two phases for 10 times, summing up to training for 90 epochs. PAAM uses the ADAM optimizer with learning rates of 10−6 / 10−3 for training the AN / CNN parameters.\n\nAfter training, PAAM fine-tunes the CNN. In this stage, it removes the AN, and keeps only the CNN filters with the binary score of one. PAAM uses Equation equation 7 to compute the binary scores from their analog counterparts, which is in fact PAAM’s ultimate goal in pruning. Each feature map is either removed entirely (having score zero), or is completely preserved (having score one). PAAM uses the SGD optimizer, with the same parameters as in warm-up, and tunes the CNN for 300 epochs.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Experimental results on ImageNet with ResNet50. ∆ is the difference in accuracy between baseline and pruned networks. The numbers are taken from the reported results in the cited papers.\n\nTop1 Acc(%)\n\nTop5 Acc(%)\n\n↓(%)\n\nMethod\n\nBaseline\n\nPruned\n\n∆\n\nBaseline\n\nPruned\n\n∆\n\nParams\n\nFlops\n\nSFP (2018) He et al. (2018a) DCP (2018) Zhuang et al. (2018) FPGM (2019) He et al. (2018c) CCP (2019) Peng et al. (2019) GAL (2019) Lin et al. (2019) PFP (2020) Liebenwein et al. (2020) AutoPruner (2020) Luo & Wu (2020) HRank (2020) Lin et al. (2020) SCOP (2020) Tang et al. (2020) DMC (2020) Gao et al. (2020) NPPM (2021) Gao et al. (2021) CHIP (2021) Sui et al. (2021) PAAM(KQ)\n\n76.15 76.01 76.15 76.15 76.15 76.13 76.15 76.15 76.15 76.15 76.15 76.15 76.15\n\n74.61 74.95 75.59 75.5 71.95 75.21 74.76 74.98 75.95 75.35 75.96 76.30 77.21\n\n-1.54 -1.06 -0.56 -0.65 -4.2 -0.92 -1.39 -1.17 -0.20 -0.80 -0.19 +0.15 +1.06\n\n92.87 92.93 92.87 92.87 92.87 92.87 92.87 92.87 92.87 92.87 92.87 92.87 92.87\n\n92.06 92.32 92.63 92.62 90.94 92.43 92.15 92.33 92.79 92.49 92.75 92.91 93.55\n\n-0.81 -0.61 -0.24 -0.25 -1.93 0.44 -0.72 -0.54 -0.08 -0.38 -0.12 +0.04 +0.68\n\nN/A 50.9 37.5 N/A 16.9 30.1 N/A 36.6 42.8 N/A N/A 40.8 51.1\n\n41.8 55.6 42.2 48.8 43.0 44.0 48.7 43.7 45.3 55.0 56.0 44.8 31.9\n\nOur second experiments are on ImageNet with ResNet50. As ResNet50 has many layers with a very large number of filters, PAAM (Vanilla) runs out of memory, whereas PAAM (KQ) has no problems to scale up. In this experiment, PAAM starts with the pretrained ResNet50 model, in order to save time. PAAM alternates the training of the AN/CNN networks for 54 epochs, with 3 epochs for the AN and 6 epochs for the CNN in each iteration. PAAM then fine tunes the pruned CNN for 200 epochs. PAAM sets the batch size, momentum, weight decay and initial learning rate to 128, 0.875, 2e-05 and 0.5, respectively, and uses a cosine-annealing learning-rate scheduler with 5 as warm-up.\n\nBalancing the Pruned Parameters and Flops. PAAM uses while training the AN, the regularized loss to guide the scores to the desired filter budget. However, the number of parameters of each CNN layer is different from the number of computation flops required for that CNN layer:\n\n(P arams)l = Cl × Fl × Kl × Kl,\n\n(F lops)l = Cl × Fl × Kl × Kl × Wl × Hl\n\n(8)\n\nAs the number of flops also depends on the image size, early convolutional layers, before the maxpooling layers, require more flops as the image size is larger. To properly balance pruning vs flops, we multiply the l1 norm of each layer by the fraction of the image sizes of that layer and the last layer.\n\nFor experiments on ResNet56, PAAM used λ = 5 × 10−4 and λ = 15 × 10−4 for pruning ratios of 52.3% and 83.0% respectively. On ResNet110, PAAM used λ = 2 × 10−4 and λ = 5.5 × 10−4 to prune 54.9% and 79.3% of the network parameters. On ResNet50, PAAM used λ = 10−6.\n\n3.2 PERFORMANCE EVALUATION\n\nExperimental results. Table 1 compares the performance of PAAM (Vanilla) and PAAM (KQ) to the sate-of-the-art filter-pruning methods (SOA) on Cifar10. Both versions of PAAM outperform SOA, achieving higher accuracy while pruning more parameters. Specifically, on ResNet56 PAAM results in an accuracy increase, even compared to the dense baselines, while significantly pruning parameters and flops. The same is true for ResNet110. For high pruning ratios, PAAM outperforms CHIP Sui et al. (2021), the next best method, with better accuracy while pruning more parameters and flops on ResNet56. Similarly, on ResNet110, PAAM outperforms CHIP in both accuracy and pruning.\n\nTable 2 compares the performance of PAAM (KQ) to the SOA on ImagNet. Starting from a baseline of 76.15% accuracy, PAAM achieves a higher accuracy of 77.21% even when compared to the baseline (an increase of 1.06%), while pruning 51.1% of the parameters and 31.9% of the flops.\n\nPer-layer Budget Discovery. A remarkable feature of PAAM it that it finds the optimal sparse sub-networks in a fully-automated pipeline and does not require a budget-allocation schedule per layer. Figure 4 shows the discovered networks in our experiments from ResNet56 and ResNet110.\n\nIn each residual block of the sub-networks learned by PAAM, the first layer has lower number of filters remaining after pruning, compared to the second layer. This structure is similar to the bottle-neck architecture used in ResNets with a large number of layers. It enables the network to concentrate on the most important features with less capacity, which is exactly what we are looking for with pruning. Many existing pruning algorithms use similar bottle-neck structures, when manually\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: CNNs discovered by PAAM from ResNet56 (a) and ResNet110 (b), with medium (I) and high (II) pruning ratios. PAAM finds the optimal per-layer budgets automatically.\n\ndefining layer budgets for pruning Sui et al. (2021); Lin et al. (2020). PAAM is able to discover this pattern automatically, without supervision. Moreover, for high pruning ratios on ResNet110, there are blocks emerging with 0 remaining filters in the first layers. This shows that PAAM can also remove entire layers when required, having more freedom in the possible sparse sub-networks search space.\n\nThe threshold value. PAAM computes the optimal threshold θ(S, p) of Equation (7), based on the pruning ratio p, and the cumulative distribution of filter scores, as shown in Figure 5, for the Cifar10 experiments on ResNet56 and ResNet110. This resulted in θ(S, p) = 0.5.\n\nTable 3: Ablation results on Cifar10 dataset with ResNet56 pruned with different threshold values.\n\nThreshold(θ)\n\n↓(%)Params\n\n↓(%)Flops\n\nPruned Accuracy(%)\n\n0.2 0.3 0.4 0.5 0.6 0.7\n\n46.0 50.2 51.6 52.3 58.2 64.7\n\n36.7 42.4 46.6 49.3 57.6 65.9\n\n94.30 94.32 94.29 94.28 93.97 93.66\n\nAs one can see, the leaky-exponential activation function and the regularized-loss functions of PAAM, ensure that the majority of the scores are pushed to values close to zero and one. Hence, most of the scores lie very close to the two ends of the [0, 1] interval, and 0.5 is in its center. Moreover, as it is shown in Figure 5, the cumulative-distribution plot of scores is almost horizontal before 0.5.\n\nIn order to provide more insight on the proper choice for the value of the threshold, we conducted an ablation experiment on Cifar10 with the ResNet56 network. After the scores training epochs, we pruned the filters with different thresholds. This process is then followed by fine-tuning epochs. Table 3 shows the accuracies of the pruned network with each threshold value. As the table shows, the pruned parameters and flops percentages change slowly with changing the score threshold. As one can see, a threshold θ of 0.5 ensures best accuracy for most pruning of parameters and flops.\n\n4 RELATED WORK\n\nAttention. In recent years, attention has achieved great success in many computer-vision tasks Guo et al. (2022), from attentional CNNs to transformers Dosovitskiy et al. (2020). These works mainly focus on improving the accuracy of vision networks by learning to what (channel) or where (spatial) to pay attention to. The squeeze-and-excitation network Hu et al. (2017) is one of the pioneers in channelattention networks which calculate channel scores from feature maps to enhance classification.\n\nPAAM. To the best of our knowledge, this paper is the first to employ attention, tailored to computing filter-importance scores from filter weights, for pruning CNNs. PAAM (KQ) only uses the key and the query matrices to compute the filter correlations within a layer, and disregards the value matrices.\n\nPruning. Filter pruning is a very popular structured-pruning method for sparsifying CNNs, which supports storage reduction and processing efficiency, without requiring any special library. One can roughly classify existing filter-pruning methods into three main categories, based on their filterselection approaches: data-free, data-informed, and training-aware methods, respectively.\n\nData-free filter pruning. Following-up on the weights-magnitude-pruning method, where the weights with the smallest absolute values are considered the least important, Li et al. (2017) uses the sum of\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Filter-score histograms for Cifar10 experiments. Most filters have zero and one scores.\n\nabsolute values of the weights in a filter (the l1 norm) to prune the filters with the smallest weight values. He et al. (2018a) dynamically prunes the filters with the smallest l2 norm in each epoch by setting them to zero and repeating this process in each epoch during training. He et al. (2019) uses the geometric median of filters as the pruning criterion. In summary, although data-free methods can gain acceptable performance levels, several works have later shown that considering the training data during the pruning process, will notably improve pruning precision Hoefler et al. (2021).\n\nData-informed filter pruning. Many pruning methods focus on feature maps as they provide rich information from both data distribution and filters. Lin et al. (2020) prunes filters whose feature maps have lowest ranks, and Liebenwein et al. (2020) uses a sensitivity measure to prune filters with lowest effect on the outputs, giving provable sparsity guaranties. Motivated by the importance of inter-channel perspective for pruning, Sui et al. (2021) uses the nuclear norm of feature maps as an independence metric to prune the filters whose feature maps are the most dependent on the others.\n\nTraining-aware filter pruning. These methods use training to learn a filter-importance metric or guide the network to a sparse structure. Exploiting magnitude pruning, some add regularization factors to the loss, to guide filters values towards zero. Wen et al. (2016) and Louizos et al. (2018) use group-lasso and l0 regularization, respectively. Instead of solely relying on the weight magnitudes, Zhuang et al. (2018) proposes a discrimination-aware channel-pruning method, by defining per-layer classification losses. Gao et al. (2020) trains binary gate functions with straight-through estimators and Gao et al. (2021) focuses on training binary gates, by directly maximizing the accuracy of subnetworks.\n\nPAAM. To the best of our knowledge, none of the above methods, is able to automatically learn the filter-importance scores from the filter weights, extract hidden correlations among filters, automatically calculate global importance scores for all filters, and determine layer-specific budgets, all at the same time during training, and thus taking advantage of both data-free and data-informed methods.\n\n5 CONCLUSION\n\nWe proposed a novel, end-to-end, attention-based, filter-pruning algorithm called PAAM (pruning by active attention manipulation). PAAM learns the filter-importance scores, through gradient descent on the CNN equipped with a filter-attention network (AN), and a classification loss-function regularized with the l1 norm of the filter scores. The AN computes the filter scores from the filter weights and automatically finds the correlations among filters within each layer. The l1 regularization, encourages the decrease of scores and finds the filter correlations across layers. In contrast to a large spectrum of advanced pruning algorithms, PAAM does not necessarily require a pretrained baseline CNN to prune from. It can rather sparsify dense networks from scratch, through cycles of gradient descent.\n\nWe showed on a comprehensive set of experiments on Cifar10 and ImageNet, that much better compression rates are achievable through the use of PAAM for ResNet50, ResNet56, and ResNet110, while even surpassing baseline accuracy. PAAM is able to compute the global filter-importance scores and to automatically associate pruning budgets to CNN layers, without layer-specific hyperparameters. In this way, PAAM is taking advantage of both the data-free and the data-informed pruning methods.\n\nIt is our hope that future work is going to start using PAAM in resource-hungry applications domains, such as those of neural-architecture search Mellor et al. (2021). The application of PAAM promises to also result in compressed neural networks endowed with salient features, such as accuracy and small compute footprint, automatically distilled during training for resource-constrained environments.\n\n9\n\n(a)(b)(c)ScoreScoreScoreResNet56 Filter ScoresResNet110 Filter ScoresCumulative ResNet56 Filter ScoresNumber of FiltersNumber of FiltersNumber of FiltersUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional neural networks. J. Emerg. Technol. Comput. Syst., 13(3), feb 2017. ISSN 1550-4832. doi: 10.1145/3005348. URL https://doi.org/10.1145/3005348.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate, 2014. URL https://arxiv.org/abs/1409.0473.\n\nYoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL http://arxiv.org/abs/1308.3432.\n\nLei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware acceleration for neural networks: A comprehensive survey. Proceedings of the IEEE, 108(4): 485–532, 2020.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2020. URL https://arxiv.org/abs/2010.11929.\n\nElias Frantar, Eldar Kurtic, and Dan Alistarh. Efficient matrix-free approximations of second-order information, with applications to pruning and optimization. arXiv preprint arXiv:2107.03356, 2021.\n\nShangqian Gao, Feihu Huang, Jian Pei, and Heng Huang. Discrete model compression with resource constraint for deep neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\nShangqian Gao, Feihu Huang, Weidong Cai, and Heng Huang. Network pruning via performance maximization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9270–9280, June 2021.\n\nKlaus Greff, Rupesh K Srivastava, and Jürgen Schmidhuber. Highway and residual networks learn\n\nunrolled iterative estimation. arXiv preprint arXiv:1612.07771, 2016.\n\nMeng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai Zhang, Ralph R. Martin, Ming-Ming Cheng, and Shi-Min Hu. Attention mechanisms in computer vision: A survey. Computational Visual Media, 8(3):331–368, mar 2022. doi: 10.1007/ s41095-022-0271-y. URL https://doi.org/10.1007%2Fs41095-022-0271-y.\n\nSong Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks\n\nwith pruning, trained quantization and huffman coding, 2016.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\n\nnetworks. In European conference on computer vision, pp. 630–645. Springer, 2016.\n\nYang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pp. 2234–2240. International Joint Conferences on Artificial Intelligence Organization, 7 2018a. doi: 10.24963/ijcai.2018/309. URL https: //doi.org/10.24963/ijcai.2018/309.\n\nYang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating\n\ndeep convolutional neural networks, 2018b.\n\nYang He, Ping Liu, Ziwei Wang, and Yi Yang. Pruning filter via geometric median for deep convolutional neural networks acceleration. CoRR, abs/1811.00250, 2018c. URL http:// arxiv.org/abs/1811.00250.\n\nYang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018d.\n\nTorsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep\n\nlearning: Pruning and growth for efficient inference and training in neural networks, 2021.\n\nJie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. Squeeze-and-excitation networks, 2017.\n\nURL https://arxiv.org/abs/1709.01507.\n\nStanisław Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residual connections encourage iterative inference. In International Conference on Learning Representations (ICLR), 2018.\n\nMathias Lechner, Ramin Hasani, Alexander Amini, Thomas A Henzinger, Daniela Rus, and Radu Grosu. Neural circuit policies enabling auditable autonomy. Nature Machine Intelligence, 2(10): 642–652, 2020.\n\nY. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.\n\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for\n\nefficient convnets, 2017.\n\nYawei Li, Shuhang Gu, Luc Van Gool, and Radu Timofte. Learning filter basis for convolutional\n\nneural network compression, 2019.\n\nYunqiang Li, Silvia L. Pintea, and Jan C. van Gemert. Equal bits: Enforcing equally distributed binary network weights. Proceedings of the AAAI Conference on Artificial Intelligence, abs/2112.03406, 2022. URL https://arxiv.org/abs/2112.03406.\n\nLucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus. Provable filter pruning\n\nfor efficient neural networks, 2020.\n\nLucas Liebenwein, Ramin Hasani, Alexander Amini, and Daniela Rus. Sparse flows: Pruning\n\ncontinuous-depth models. Advances in Neural Information Processing Systems, 34, 2021.\n\nMingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling Shao. Hrank: Filter pruning using high-rank feature map. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\nShaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang, and David Doermann. Towards optimal structured cnn pruning via generative adversarial learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nChristos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0 regularization. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=H1Y8hhg0b.\n\nJian-Hao Luo and Jianxin Wu. Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference. Pattern Recognition, 107:107461, 2020. ISSN 0031-3203. doi: https://doi.org/10.1016/j.patcog.2020.107461. URL https://www.sciencedirect.com/ science/article/pii/S0031320320302648.\n\nSher Afghan Malik and Uwe Naumann. Interval Adjoint Significance Analysis for Neural Networks. In Computational Science – ICCS 2020 : 20th International Conference, Amsterdam, The Netherlands, June 3–5, 2020, Proceedings, Part III / edited by Valeria V. Krzhizhanovskaya, Gábor Závodszky, Michael H. Lees, Jack J. Dongarra, Peter M. A. Sloot, Sérgio Brissos, João Teixeira, volume 12139 of Lecture notes in computer science, pp. 365–378, Cham, Jun 2020. 20th International Conference on Computational Science, Amsterdam (Netherlands), 3 Jun 2020 - 5 Jun 2020, Springer International Publishing. doi: 10.1007/978-3-030-50420-5_27. URL https://publications.rwth-aachen.de/record/816737. Weitere Reihe: Theoretical Computer Science and General Issues ; 12139. - Springer eBook Collection.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJoe Mellor, Jack Turner, Amos Storkey, and Elliot J Crowley. Neural architecture search without\n\ntraining. In International Conference on Machine Learning, pp. 7588–7598. PMLR, 2021.\n\nHanyu Peng, Jiaxiang Wu, Shifeng Chen, and Junzhou Huang. Collaborative channel pruning for deep networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5113–5122. PMLR, 09–15 Jun 2019. URL https://proceedings. mlr.press/v97/peng19c.html.\n\nAlexandra Peste, Eugenia Iofinova, Adrian Vladu, and Dan Alistarh. Ac/dc: Alternating compressed/decompressed training of deep neural networks. Advances in Neural Information Processing Systems, 34, 2021.\n\nYang Sui, Miao Yin, Yi Xie, Huy Phan, Saman Zonouz, and Bo Yuan. Chip: Channel independence-\n\nbased pruning for compact neural networks, 2021.\n\nYi Sun, Xiaogang Wang, and Xiaoou Tang. Sparsifying neural network connections for face\n\nrecognition, 2015.\n\nMingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 6105–6114. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr. press/v97/tan19a.html.\n\nYehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing XU, Chao Xu, and Chang Xu. Scop: Scientific control for reliable neural network pruning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 10936–10947. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017a. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz In Advances in neural information\n\nKaiser, and Illia Polosukhin. Attention is all you need. processing systems, pp. 5998–6008, 2017b.\n\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/ 41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf.\n\nRuichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I. Morariu, Xintong Han, Mingfei Gao, Ching-Yung Lin, and Larry S. Davis. NISP: pruning networks using neuron importance score propagation. CoRR, abs/1711.05908, 2017. URL http://arxiv.org/abs/1711.05908.\n\nMichael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model\n\ncompression, 2017.\n\nZhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, and Jinhui Zhu. Discrimination-aware channel pruning for deep neural networks. In S Bengio, H Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, and R Garnett (eds.), NIPS Proceedings, volume 2018-December of Advances in Neural Information Processing Systems, pp. 875–886. Neural Information Processing Systems (NIPS), 2018. Advances in Neural Information Processing Systems 2018, NIPS 2018 ; Conference date: 02-12-2018 Through 08-12-2018.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 PAAM’S EFFECT ON ITERATIVE INFERENCE IN RESNETS\n\nIn this section, we proceed with an analysis of the effect that PAAM has on the iterative featurerefinement process in residual networks Greff et al. (2016).\n\nResNets He et al. (2016) are known to enhance representation learning in deeper layers via an iterative feature-refinement scheme Greff et al. (2016). This scheme suggests that input features to a ResNet do not create new representations. Rather, they gradually and iteratively refine the learned features of the initial residual blocks Jastrzebski et al. (2018). Iterative refinement of features was shown to be necessary for obtaining attractive levels of performance, while their disruption hurts performance.\n\nAs AN modifies the underlying model structure and the feature maps of a residual block, it is very important to investigate if PAAM preserves the iterative feature-refinement property of ResNets.\n\nTo make this analysis precise, let us first formalize iterative inference as discussed in Jastrzebski et al. (2018): A residual block i in a ResNet with M blocks, performs for the input feature xi the following transformation: xi+1 = xi + fi(xi). Hence, the following loss function L can be recursively applied to the network Jastrzebski et al. (2018):\n\nL(xM ) = L(xM −1 + fM −1(xM −1)).\n\n(9)\n\nA first-order Taylor expansion of this loss, while ensuring that fj’s magnitude is small, is a good approximation to formally investigating the iterative inference Jastrzebski et al. (2018). Thus:\n\nL(xM ) = L(xi) +\n\nM −1 (cid:88)\n\nj=i\n\nfj(xj).\n\n∂L(xj) xj\n\n+ O(f 2\n\nj (xj)).\n\n(10)\n\nThis approximation reveals that the i-th residual block, modifies features xi with roughly the same amount of fi(xi) as that of ∂L(xi) . This implies a moderate reduction of loss as we transition from the i-th to the M -th block, as an iterative refinement scheme Greff et al. (2016); Jastrzebski et al. (2018). The refinement step for a vanilla residual block can be computed by the squared norm of fi(xi), and can be normalized to the input feature as: ∥fi(xi)∥2\n\nxi\n\n2 / ∥xi∥2 2.\n\nAny modification to the structure of the residual network (e.g., in PAAM) causes a change in the refinement step. This step has to be investigated if it does or it does not modify the iterative inference.\n\nLemma A.1. The iterative feature-refinement scale is bounded for ResNets with PAAM as follows, with parameter ε from the leaky integrator and 0 < δ ≤ 1 + ε:\n\nδ 1 + ε\n\n∥fi(xi)∥2 ∥xi∥2\n\n2\n\n2\n\n≤\n\n∥Si ⊙ fi(xi)∥2 ∥Si ⊙ xi∥2\n\n2\n\n2\n\n≤\n\n1 + ε δ\n\n∥fi(xi)∥2 ∥xi∥2\n\n2\n\n2\n\n(11)\n\nProof. A ResNet block i that is equipped with an SbF-Pruner layer, transforms the features xi with the following expression: xi+1 = Si ⊙ (xi + fi(xi)), where Si stands for the score vector computed by PAAM. The refinement step is given by ∥Si ⊙ fi(xi)∥2 2 and its input-normalized representation is 2 / ∥Si ⊙ xi∥2 ∥Si ⊙ fi(xi)∥2 2.\n\nDeriving the upper bound: Assuming that every element in Si is between δ and 1+ε, for 0 < δ ≤ 1 + ε, we have:\n\n∥Si ⊙ fi(xi)∥ ≤ ∥Si∥∞ ∥fi(xi)∥ ≤ (1 + ε) ∥fi(xi)∥ ∥Si ⊙ xi∥ ≥ ∥Si∥min ∥xi∥ ≥ δ ∥xi∥ .\n\nAs a consequence, the following upper-bound inequality holds for the iterative inference:\n\n∥Si ⊙ fi(xi)∥2 ∥Si ⊙ xi∥2\n\n2\n\n2\n\n≤\n\n1 + ε δ\n\n∥fi(xi)∥2 ∥xi∥2\n\n2\n\n2\n\nDeriving the lower bound: Assuming that every element in Si is between δ and 1 + ε, we have:\n\n∥Si ⊙ fi(xi)∥ ≥ δ ∥fi(xi)∥\n\n13\n\n(12) (13)\n\n(14)\n\n(15)\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Activation functions.\n\nFigure 7: Network discovered by PAAM with Sigmoid activation function.\n\nAs a consequence, the following lower-bound inequality holds for the iterative inference:\n\n∥Si ⊙ xi∥ ≤ (1 + ε) ∥xi∥ .\n\n∥Si ⊙ fi(xi)∥2 ∥Si ⊙ xi∥2\n\n2\n\n2\n\n≥\n\nδ 1 + ε\n\n∥fi(xi)∥2 ∥xi∥2\n\n2\n\n2\n\n(16)\n\n(17)\n\nInequalities equation 14 and equation 17 prove the the stated lemma.\n\nLemma A.1 has profound implications in practice. It indicates that the iterative-inference property of the ResNets equipped with PAAM is both lower and upper bounded. These ResNets not only get compressed in size, but also preserve the representation learning capabilities of ResNets between these two bounds. The bounds themselves can be fine tuned with the parameter λ of Equation equation 6.\n\nA.2 ABLATION ON ACTIVATION FUNCTIONS\n\nIn this section, we provide additional experiments on the choice of the activation function for PAAM layers. We compare the performance of the vanilla PAAM on CIFAR10 with ResNet56 with four different activation functions presented in Figure 6.\n\nWe start with the Sigmoid function. As we already mentioned in the paper, Sigmoid is a favourable function, satisfying the first and third conditions we look for in our activation function. In our experiment with Sigmoid, the scores did not converge to values close to 0 and 1 and mostly stayed at values close to 0.5, making it hard to control the pruning ratio by the loss regularization factor. We further tried the Sigmoid function with higher temperatures, σ(bx) with b = 100. In this experiment, the scores converged to 0 and 1. As discussed in the manuscript, high temperature helps the Sigmoid to converge. But the scores converge at a fast rate to 0 or 1, and hardly change in future epochs, resulting in a uniform random pruning budget allocation scenario. Figure 7 shows the network discovered by PAAM with high temperature Sigmoid activation function. Compare this to the layer specific pruning results shown in the manuscript in Figure 4(a). Table 4 shows that high temperature Sigmoid activation function results in poor accuracy for the pruned network. In order to be able to use Sigmoid as activation function, extensive per-layer tuning for the temperature is required.\n\n14\n\n32101230.00.20.40.60.81.0exp(x)1+ax2sigmoid(x)sigmoid(x)sigmoid(bx)Under review as a conference paper at ICLR 2023\n\nTo improve the results with Sigmoid, we experimented with an activation function similar to our leaky-expo, which we call leaky-2sigmoid. leaky-2sigmoid is initialized by ones with output range in [0, 2] and has an affine linear component for outputs greater than 1 as follows:\n\nφ(x) =\n\n(cid:26)2σ(x)\n\n1.0 + ax\n\nif x < 0 if x ≥ 0\n\n(18)\n\nAs one can see in Figure 6, this function is very similar to leaky-expo. The results shown in Table 1 illustrate that PAAM with leaky-2sigmoid and leaky-expo perform competitively, with leaky-expo resulting in better compression rate. This indicates that starting the scores from 1, which is starting from a fully dense network, plays an important role in the results obtained by PAAM.\n\nTable 4: Ablation study with different activations\n\nActivation-Function\n\nPruned Acc(%)\n\n↓Parameters(%)\n\nSigmoid(x) Sigmoid(bx) Leaky-2Sigmoid Leaky-expo\n\nnot converged 89.68 94.10 94.28\n\nnot converged 51.5 48.2 52.3\n\nA.3 EXPERIMENTAL DETAILS\n\nIn this section, we provide the details of the experiments conducted on Cifar10 and ImageNet datasets. Table 5 shows the training epochs and λ value for the scores loss regularizer for our experiments. In all experiments, during pruning epochs, we train the PAAM layers for 3 epochs and then switch to training the network weights for 6 epochs and repeat this set of 9 epochs.\n\nTable 5: Hyperparameters used in Cifar10 experiments.\n\nModel\n\nWarm-up\n\nPruning\n\nFine-tuning\n\nλ 5e−4\n\n↓Params(%) 52.3\n\n↓Flops(%) 49.3\n\nResNet56\n\nResNet110\n\n50\n\n50\n\n90\n\n90\n\nResNet50\n\npre-trained\n\n54\n\n300\n\n300\n\n200\n\n1.5e−3 2e−4\n\n5.5e−4 1e−6\n\n83.0 54.9\n\n79.3 51.1\n\n70.5 51.3\n\n74.8 23.1\n\nCifar10. We used Vanilla and KQ PAAM for Cifar10 datasets. We didn’t use fully pre-trained networks, but rather did 50 warm-up epochs before pruning.\n\nImageNet. We used KQ-PAAM for ImageNet experiments. We set the hidden dimension dl of each layer to number of filters of that layer, devided by the layer expansion parameter, which is 4 in ResNet50. We normalised the scaling factor by 1/α, set to the sum-of-squares root of the number of filters in the last layers of each block. We used the pre-trained pytorch model for ResNet50. For pruning and fine-tuning we augment the data by a random-resized crop, random-horizontal flip, and trivial augmentation. We also normalized the data samples. For pruning epochs, we use Adam, and set the learning rates to 10−6 and 10−3 for score training and network training phases, respectively.\n\n15",
    "reference": "# Summary Of The Paper\n\nThe work proposes a novel method for network pruning by co-training a network to predict filter importance given model weights. This allows the model learn hidden correlations between the model weights and channel importance. They achieve strong empirical results on pruning resnet networks on CIFAR-10 and Imagenet. The method naturally learns E2E and doesn't require per-layer sparsity values and is tuned by a global cost-weighting/function. To satisfy the requirements of their attention network, they propose a novel activation function.\n\n# Strength And Weaknesses\n\nStrengths:\n- Achieves very strong possibly SOTA empirical results on CIFAR-10 and Imagenet.\n- Proposes novel method for channel pruning using an attention mechanism that allows accounting of layer interdependencies.\n\nWeaknesses:\n- Little to no discussion of the stability or robustness of the approach.\n- Possibly limited novelty due to the similarities to existing work in pruning and NAS and would benefit from significantly more analysis and ablation of their method and the cost.\n\nQuestions:\nThe paper would benefit from discussion of the cost of pruning compared to competing methods. Do other methods prune/fine-tune the network for as many epochs? How expensive is the score model?\nSince the training method is quite lengthy and approaching NAS, it would be quite useful to run the pruning results multiple times to give a better idea of how stable the method is.\nHow much hyperparameter tuning did you conduct and do you have results from different weightings of the flops/parameter cost? The work would benefit from more discussion of how controllable the sparsity is.\nIs a single score model trained used for every layer or are separate networks used per layer?\nI don't understand the section on the threshold value. Table 3 shows that you get a more accurate, and sparser network with a value of 0.2 or 0.3. Why was 0.5 chosen?\n\nNits:\nOn page 7: \"Table 2 compares the performance of PAAM (KQ) to the SOA on ImagNet\" is missing Ts\nQuestion, but a bit separate from the paper. Can the channel importance estimator transfer between models or during training or does it only give useful signal when co-trained with the network? Would be a useful area to explore if possible.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSome claims in the paper may be somewhat misleading.\n\n\"PAAM can also train and generate a pruned network from scratch in a straightforward, one-stage training process without requiring a pre-trained network. \"\nThis is somewhat misleading since you don't require a pre-trained network since your entire training process is rather more lengthy than a normal training process and ends with fine-tuning the pruned network to convergence.\n\n# Summary Of The Review\n\nThis work proposes a novel method for structured pruning which achieves significantly better SOTA results on Imagenet and Cifar10. Some aspects may of be limited novelty however, due to similarities to methods explored in other NAS and channel pruning methods. Since the empirical results are quite important, the work would benefit from more exploration of it's robustness and more detailed analysis of the method and training cost. I believe this work is currently marginally below the acceptance threshold, but would be willing to increase my score with additional  analysis, experimental results, and details of the hyperparameter search\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDEFENDING BACKDOOR ATTACKS VIA ROBUSTNESS AGAINST NOISY LABEL\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nMany deep neural networks are vulnerable to backdoor poisoning attacks, in which an adversary strategically injects a backdoor trigger into a small fraction of the training data. The trigger can later be applied during inference to manipulate prediction labels. While the data label could be changed to arbitrary values by an adversary, the extent of corruption injected into the feature values is strictly limited to keep the backdoor attack in disguise, which leads to a resemblance between the backdoor attack and a milder attack that involves only noisy labels. This paper investigates an intriguing question: Can we leverage algorithms that defend against noisy label corruptions to defend against general backdoor attacks? We first discuss the limitations of directly using current noisy-label defense algorithms to defend against backdoor attacks. We then propose a meta-algorithm for both supervised and semi-supervised settings that transforms an existing noisy label defense algorithm into one that protects against backdoor attacks. Extensive experiments on different settings show that, by introducing a lightweight alteration for minimax optimization to the existing noisy-label defense algorithms, the robustness against backdoor attacks can be substantially improved, while the initial form of those algorithms would fail in the presence of a backdoor attack.\n\n1\n\nINTRODUCTION\n\nDeep neural networks (DNN) have achieved significant success in a variety of applications such as image classification (Krizhevsky et al., 2012), autonomous driving (Major et al., 2019), and natural language processing (Devlin et al., 2018), due to their powerful generalization ability. However, DNN can be highly susceptible to even small perturbations of training data, which has raised considerable concerns about their trustworthiness (Liu et al., 2020). One representative perturbation approach is backdoor attack, which undermines the DNN performance by modifying a small fraction of the training samples with specific triggers injected into their input features, whose ground-truth labels are altered accordingly to be the attacker-specified ones. It is unlikely such backdoor attacks will be detected by monitoring the model training performance since the trained model can still perform well on the benign validation samples. Consequently, during testing phase, if the data is augmented with the trigger, it would be mistakenly classified as the attacker-specified label. Subtle yet effective, backdoor attacks can pose serious threats to the practical application of DNNs.\n\nAnother typical type of data poisoning attack is noisy label attacks (Han et al., 2018; Patrini et al., 2017; Yi & Wu, 2019; Jiang et al., 2017), in which the labels of a small fraction of data are altered deliberately to compromise the model learning, while the input features of the training data remain untouched. Backdoor attacks share a close connection to noisy label attacks, in that during a backdoor attack, the feature can only be altered insignificantly to put the trigger in disguise, which makes the corrupted feature (e.g. images with the trigger) highly similar to the uncorrupted ones. Prior efforts have been made to effectively address noisy label attacks. For instance, there are algorithms that can tolerate a large fraction of label corruption, with up to 45% noisy labels (Han et al., 2018; Jiang et al., 2018). However, to the best of our knowledge, most algorithms defending against backdoor attacks cannot deal with a high corruption ratio even if the features of corrupted data are only slightly perturbed. Observing the limitation of prior state-of-the-art, we aim to answer one key question: Can one train a deep neural network that is robust against a large number of backdoor attacks? Moreover, given the resemblance between noisy label attacks and backdoor attacks, we also investigate another\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nintriguing question: Can one leverage algorithms initially designed for handling noisy label attacks to defend against backdoor attacks more effectively?\n\nThe contributions of this paper are multi-fold. First, we provide a novel and principled perspective to decouple the challenges of defending backdoor attacks into two components: one induced by the corrupted input features, and the other induced by the corrupted labels, based on which we can draw a theoretical connection between the noisy-label attacks and backdoor data attacks. Second, we propose a meta-algorithm to address both challenges by a novel minimax optimization. Specifically, the proposed approach takes a noisy-label defense algorithm as its input and outputs a reinforced version of the algorithm that is robust against backdoor poisoning attacks, even if the initial form of the algorithm fails to provide such protection. Moreover, we also propose a robust meta-algorithm in semi-supervised setting based on our theorem, leveraging more data information to boost the robustness of the algorithm. Extensive experiments show that the proposed meta-algorithm improves the robustness of DNN models against various backdoor attacks on a variety of benchmark datasets with up to 45% corruption ratio, while most previous study on backdoor attack only provide robustness against small corruption ratio. Furthermore, we propose a systematic, meta-framework to solve backdoor attacks, which can effectively join existing knowledge in noisy label attack defenses and provides more insights to future development of defense algorithms.\n\n2 RELATED WORK\n\nRobust Deep Learning Against Adversarial Attack. Although DNNs have shown high generalization performance on various tasks, it has been observed that a trained DNN model would yield different results even by perturbing the image in an invisible manner (Goodfellow et al., 2014; Yuan et al., 2019). Prior efforts have been made to tackle this issue, among which one natural defense strategy is to change the empirical loss minimization into a minimax objective. By solving the minimax problem, the model is guaranteed a better worst-case generalization performance (Duchi & Namkoong, 2021). Since exactly solving the inner maximization problem can be computationally prohibitive, different strategies have been proposed to approximate the inner maximization optimization, including heuristic alternative optimization, linear programming Wong & Kolter (2018), semi-definite programming Raghunathan et al. (2018), etc. Besides minimax optimization, another approach to improve model robustness is imposing a Lipschitz constraint on the network. Work along this line includes randomized smoothing Cohen et al. (2019); Salman et al. (2019), spectral normalization Miyato et al. (2018a), and adversarial Lipschitz regularization Terjék (2019). Although there are algorithms that are robust against adversarial samples, they are not designed to confront backdoor attacks, in which clean training data is usually inaccessible. There are also studies that investigated the connection between adversarial robustness and robustness against backdoor attack (Weber et al., 2020). However, to our best knowledge, there is no literature studying the relationship between label flipping attack and backdoor attack.\n\nRobust Deep Learning Against Noisy Labels. Many recent studies have investigated the robustness of classification tasks with noisy labels. For example, Kumar et al. (2010) proposed the Self-Paced Learning (SPL) approach, which assigns higher weights to examples with a smaller loss. A similar idea was used in Curriculum Learning (Bengio et al., 2009), in which a model is trained on easier examples before moving to the harder ones. Other methods inspired by SPL include learning the data weights (Jiang et al., 2018) and collaborative learning (Han et al., 2018; Yu et al., 2019). An alternative approach to defending noisy label attacks is label correction (Patrini et al., 2017; Li et al., 2017; Yi & Wu, 2019), which attempts to revise the original labels of the data to recover clean labels from corrupted ones. However, since we do not have the knowledge of which data points have been corrupted, it is nontrivial to obtain provable guarantees for label corrections, unless strong assumptions have been made on the corruption type.\n\nData Poisoning Backdoor Attack and its Defense. Robust learning against backdoor attacks has been widely studied recently. Gu et al. (2017) showed that even a small patch of perturbation can compromise the generalization performance when data is augmented with a backdoor trigger. Other types of attacks include the blend attacks (Chen et al., 2017), clean label attacks (Turner et al., 2018; Shafahi et al., 2018), latent backdoor attacks (Yao et al., 2019), etc. While there are various types of backdoor attacks, some attack requires that the adversary not only has access to the data but also has limited control on the training and inference process. Those attacks include trojan attacks and\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nblind backdoor attacks (Pang et al., 2020). We refer readers to Pang et al. (2020) for a comprehensive survey on different types of backdoor attacks. Various defense mechanisms have been proposed to defend against backdoor attacks. One approach is to remove the corrupted data by using anomaly detection (Tran et al., 2018; Chen et al., 2018). Alternatively, model inspection (Wang et al., 2019) aims to inspect and modify the compromised model to make it robust against the trigger. In addition, there are other methods to tackle the backdoor attacks, such as randomized smoothing (Cohen et al., 2019; Weber et al., 2020), and the median of means (Levine & Feizi, 2020). However, they are either inefficient or cannot defend against backdoor attacks with a large ratio of corrupted data. Some of the above methods also hinge on having a clean set of validation data, which is impractical since it is unlikely we can guarantee the existence of clean validation data given that the validation data is usually a subset of the training data. To the best of our knowledge, there is no existing backdoor defense algorithm that is motivated from the label corruption perspective.\n\n3 PRELIMINARIES\n\nLearning with Noisy Labels There are two representative approaches for defending against noisylabels: 1) Filtering-based approach is one of the most effective strategies for defending against noisy labels, which works by selecting or weighting the training samples based on indicators such as sample losses (Jiang et al., 2017; Han et al., 2018; Jiang et al., 2020) or gradient norms of the loss-layer (Liu et al., 2021). For instance, Jiang et al. (2017) proposed to assign higher probabilities to samples with lower losses to be selected for model training. 2) Consistency-based approach modifies data labels during model training. Specifically, the Bootstrap approach (Reed et al., 2014) encourages model predictions to be consistent between iterations, by modifying the labels as a linear combination of the observed labels and previous predictions.\n\nAlthough the initial forms of these approaches can be vulnerable to backdoor attacks, we propose a meta-algorithm that empowers them to effectively counter against backdoor attacks. In this paper, we examine two filtering-based noisy label algorithms, namely, Self-Paced Learning (SPL) Jiang et al. (2017); Kumar et al. (2010) and Provable Robust Learning (PRL) Liu et al. (2021), and one consistency-based algorithm, the Bootstrap Reed et al. (2014), to investigate the efficacy of the proposed meta algorithm. We briefly summarize the main idea of the above algorithms in Table 4 in Appendix section. The empirical results in Section 5 strongly suggest that our meta framework can readily benefit the existing robust noisy-label algorithms.\n\nProblem Setting of Backdoor Attacks We follow the standard setting for backdoor attacks and assume that there is an adversary that tries to perform the backdoor attack. Firstly, the adversary can choose up to ε fraction of clean labels Y ∈ Rn×q and modify them to arbitrary valid numbers to form the corrupted labels Yb ∈ R⌊nε⌋×q. Let Yr represent the remaining untouched labels. The final training labels can be denoted as Yε = [Yb, Yr]. Accordingly, the corresponding original feature are denoted as X = [Xo ∈ R⌊nε⌋×d, Xr ∈ R(n−⌊nε⌋)×d]. The adversary can design a trigger t ∈ Rd to form the corrupted feature set Xb ∈ R⌊nε⌋×d such that for any bi in Xb, oi in Xo, it satisfies bi = oi + t. Finally, the training features are denoted as Xε = [Xb ∈ R⌊nε⌋×d, Xr ∈ R(n−⌊nε⌋)×d]. Assuming T = [t, t, ..., t] ∈ R⌊nε⌋×d, therefore Xo + T = Xb 1. Before analyzing the algorithm, we make following assumptions about the adversary attack:\n\nAssumption 1 (Bounded Corruption Ratio). The overall corruption ratio and the corruption ratio in each class is bounded. Specifically,\n\nE(x,y,yb)∈(X,Y,Yb)\n\n(cid:20) I(yb = c|y ̸= c) I(y = c)\n\n(cid:21)\n\n≤ ε = 0.5 ∀c ∈ △Y.\n\nAssumption 2 (Small Trigger). The backdoor trigger satisfies ∥t∥p ≤ τ , which subtly alters the data within a small radius-τ ball without changing its ground-truth label.\n\nWe also assume that there exists at least one black-box robust algorithm A which can defend noisy label attacks so long as the noisy-label ratio is bounded by ε. Note that the assumption of noisy label algorithm is mild, since a variety of existing algorithm can handle noisy labels attacks with a large corruption rate (e.g. 45%) (Jiang et al., 2017; Han et al., 2018; Reed et al., 2014; Liu et al., 2021).\n\n1Some backdoor attack algorithms design instance-specific trigger. In this paper, we only focus on the static\n\ntrigger case and leave the instance-specific trigger case for our future study.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n4 METHODOLOGY\n\nGiven an ε-backdoor attacked dataset (Xε, Yε), a clean distribution p∗ := (X, Y), and a loss function L, our goal is to learn a network function f that minimizes the generalization error under the corrupted distribution, i.e. E(x,y)∼p∗ [L(f (x + t), y)] and clean distribution, i.e. E(x,y)∼p∗ [L(f (x), y)]. Next, we elaborate our meta-approach for defending against backdoor attacks in order to achieve our goal.\n\n4.1 A BLACK-BOX ROBUST ALGORITHM AGAINST NOISY LABELS\n\nThe ultimate goal for defending against backdoor attacks is to learn a network function f to minimize its risk given some corrupted input features:\n\nminf J(f ) := E(x,y)∼p∗ [L(f (x + t), y)] .\n\n(1)\n\nHowever, Equation 1 is not directly optimizable for two reasons: 1) we only have access to the corrupted inputs and the corrupted labels Yε, and 2) the trigger t is unknown. As such, we consider an surrogate objective that optimizes the worst-case of Equation 1:\n\nminf max∥c∥p≤τ\n\n(cid:88)\n\n1 n\n\nx∈X,y∈Y\n\n[L(f (x + c), y)] .\n\n(2)\n\n1 n\n\n(cid:80)\n\nit the ground-truth loss,\n\nSince the trigger satisfies ∥t∥p ≤ τ , is easy to see that Equation 2 minimizes an upper-bound of x∈X,y∈Y L(f (x + t), y) ≤ in that: max∥c∥p≤τ x∈X,y∈Y [L(f (x + c), y)] . To this end, directly optimizing the surrogate objective in Equation 2 is still intractable, since we do not have access to clean X and Y, which prevent us from using adversarial training to solve the minimax objective. To tackle this challenge, we will first assume that the clean label Y is available, and then relax this assumption by using learning algorithms that are robust against noisy labels. Specifically, by assuming that φw = L ◦ f has a Lipschitz constant L w.r.t. x, we further obtain a new upper bound (see Appendix for derivation):\n\n(cid:80)\n\n1 n\n\n(cid:88)\n\n1 n\n\nx∈X,y∈Y\n\n[L(f (x + c), y)] ≤\n\n(cid:88)\n\n1 n\n\nx∈Xε,y∈Y\n\nφw(xi + c, y) + ετ L,\n\n(3)\n\nwhich draws a principled connection between the risks from corrupted data and clean data:\n\nmin f\n\nmax ∥c∥p≤τ\n\n1 n\n\n(cid:88)\n\nx∈X,y∈Y\n\n[L(f (x + c), y)] ≈\n\n(cid:40)\n\nmin f\n\nmax ∥c∥p≤τ\n\n1 n\n\n(cid:88)\n\nx∈Xε,y∈Y\n\n[L(f (x + c), y)] + ετ L\n\n,\n\n(4)\n\n(cid:41)\n\nwhere the first term on the RHS of Equation 4 involves optimization on the corrupted features Xε and clean labels Y, while the second term on the RHS requires minimizing the Lipschitz constant L w.r.t. x. Recall that minimizing the maximum gradient norm is equivalent to minimizing the Lipschitz constant (Terjék, 2019). Therefore, optimizing the first term naturally regulates the maximum change of the loss function within a small ball, which hence constrains the magnitude of the gradient and has negligible effects on the Lipschitz regularization. The relationship between Lipschitz regularization and adversarial training has been well discussed in the literature (Terjék, 2019; Miyato et al., 2018b). We defer this discussion to the Appendix section.\n\nEquation 4 indicates that if the target labels are not corrupted and the learned function has a small Lipschitz constant, learning with corrupted features is feasible to achieve a low risk. Up to now, the remaining challenge of optimizing the surrogate objective in Equation 4 is the inaccessible clean label set Y. Fortunately, a variety of algorithms are at hand for handling noisy labels during learning (Jiang et al., 2017; Liu et al., 2021; Kumar et al., 2010), which we can directly apply to our minimax optimization scheme. Specifically, for the outer minimization, one can have:\n\n(cid:80)\n\n1 n\n\nminf x∈Xε,y∈Yε [L(f (x + c), y)] , and we can perform the noisy-label update for the above optimization objective. For instance, given the mini-batch Mx, My with batch size m, if we use SPL to perform the update, we can get the top (1 − ε)m data with a small risk L(f (x + c), y) to perform one-step gradient descent. If we use the PRL to perform the update, assuming L is the cross-entropy loss, the top (1 − ε)m data with small loss-layer gradient norm ∥f (x + c) − y∥ can be used to perform one-step gradient descent. If we apply the bootstrap method, we can add a bootstrap regularization to update the above objective.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nMeanwhile, it is non-trivial to directly solve the inner maximization, since adversarial learning c in Equation 4 still faces the threat of noisy labels. To tackle this issue, we can leverage the same robust noisy label algorithm. Specifically, we first approximate the inner optimization using the first-order Tyler expansion: c∗ = arg max∥c∥p≤τ\n\nx∈X,y∈Y L(f (x + c), y) ≈\n\n(cid:80)\n\n1 n\n\narg max∥c∥p≤τ cT ∇x x∈X,y∈Y L(f (x), y). The preceding optimization is a linear programming problem. With the l∞ norm ball constraint on the perturbation, the optimization problem can be efficiently solved by the fast gradient sign method (FGSM). Given a minibatch Mx, My with batchsize m, we have the following closed-form solution:\n\n(cid:80)\n\n1 n\n\n ̃c = Clipc\n\n(cid:26) τ m\n\n(cid:88)\n\n·\n\nx∈Mx,y∈My\n\nsign (∇xL (f (x), y))\n\n.\n\n(5)\n\n(cid:27)\n\nTo relax the prerequisite of having a clean label set y in Equation 5, we will use a noisy-label algorithm to perform the update. For instance, if we use a loss-filtering based algorithm (e.g. SPL), then for each mini-batch, only the top (1 − ε)m data with small L (f (x), y) would be included in the update. If we adopt a gradient-based filtering algorithm (e.g. PRL), given that L is the cross-entropy loss, then only the top (1 − ε)m data with small ∥f (x) − y∥ will be included. The outside clipping ensures that the feature value of the corrupted image is in the valid range. Based on the above discussion, we now introduce our meta-algorithm in Algorithm 1 that is robust against backdoor attacks, given an arbitrary noisy-label robust algorithm A as its input. We also provided an illustration in Figure 1 of the Appendix.\n\nAlgorithm 1: Meta Algorithm for Robust Learning Against Backdoor Attacks input: Corrupted training data Xε, Yε, perturbation limit: τ , learning with noisy label algorithm A (e.g.\n\nPRL, SPL, Bootstrap).\n\nreturn trained neural network ; while epoch ≤ max_epoch do\n\nfor sampled minibatch Mx, My in Xε,Yε do\n\n#Inner maximization step initialize c as 0 vector. optimize max∥c∥≤τ L(f (Mx + c), My) w.r.t. to c by using robust algorithm A for one step optimize minf L(f (Mx + c), My) w.r.t. f by using robust algorithm A for one step\n\nend\n\nend\n\n4.2 THEORETICAL JUSTIFICATION\n\nn\n\n(cid:80)\n\nt = 1\n\nx∈X,y∈Y φw(x + t, y), Rt = Ex,y∼p∗φw(x + t, y), Remp\n\nOur ultimate goal is to learn w that achieves a low expected risk Ex,y∼p∗φw(x + t, y). To study the generalization performance on the ground-truth distribution p∗, we first define the following risks: Remp x∈Xε,y∈Y φw(x + c, y), Next, we focus on the gap between Rt and Remp Theorem 1. Let Remp , Rt, ε, τ defined as above. Assume that the prior distribution of the network parameter w is N (0, σ), and the posterior distribution of parameter is N (w, σ) which is learned from the training data. Let k be the number of parameters, n be the sample size, and\n\nc = 1\n\n(cid:80)\n\nn\n\nc\n\nc\n\n.\n\n(cid:115)\n\nΓ =\n\n1\n\n4 k log\n\n(cid:18)\n\n1+\n\n∥w∥2 2\nkσ2\n\n(cid:19)\n\n+ 1\n\n4 +log n\n\nδ +2 log(6n+3k)\n\nn−1\n\n. If the objective function φw = L ◦ f is Lφ-Lipschitz\n\nsmooth, then with probability at least 1-δ, one can have:\n\nRt ≤ Remp\n\nc + Lφ(2τ + ετ ) + Γ.\n\n(6)\n\nWe hereby present the skeleton of the proof and defer more details to the Appendix. First, we decompose the error into two terms: 1) the generalization gap on the triggered data, and 2) the difference of performance loss between the trigger t and worst case perturbation c: Rt − Remp c = (Rt − Remp ). The first component can be bounded by Γ, which is derived by following the uniform convergence PAC-Bayes framework (Foret et al., 2020). For the second term, the gap is introduced by two sources. The first source is the difference between c and t, and the second\n\nt − Remp\n\n) + (Remp\n\nc\n\nt\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nis from the difference between X and Xε. Since the objective is Lφ Lipschitz, and ∥t − c∥ ≤ 2τ according to our constraint to the adversary, it is easy to upper bound the error as 2τ Lφ. Meanwhile, there is ε-fraction of difference between X and Xε, which is bounded by ∥t∥ < τ and leads to the other difference term Lφετ .\n\nc\n\nTheorem 1 presents an upper-bound of the gap Rt − Remp . The first term in Equation 6 can be minimized by using a noisy label algorithm. The second term, which is the error induced by the adversarial trigger, is jointly constrained by the Lipschitz constant Lφ, perturbation limit τ , and the corruption ratio ε. We can regularize the Lφ whereas the τ and ε are controlled by the unknown adversary. Note that existing literature has also shown that adversarial training plays a similar role as Lipschitz regularization. The last term, the normal generalization error on the clean data, is difficult to minimize directly. The bound in Theorem 1 emphasizes the importance of involving both the noisy label algorithm and the adversarial training. The noisy label algorithm can reduce the Remp c while the adversarial training regularize the Lipschitz constant Lφ.\n\n4.3 A SEMI-SUPERVISED ALGORITHM\n\nIn backdoor attacks, most attacking algorithms require modifying the label to successfully deploy the attacks. If we could leverage the knowledge from unlabeled data (i.e. via semi-supervised learning), the model performance will likely improve. In this section, we extend Theorem 1 to a semi-supervised learning setting and show that utilizing more data can benefit the model robustness. Our motivation is from the following property of Lipschitz functions. If h is a composition of two functions, f and g (h = f ◦ g), then ∥h∥lip ≤ ∥f ∥lip∥g∥lip. Recall in Eq. 6, the Lipschitz constant Lφ depends on the loss function φ, which can be decomposed into the representation function h : X → Z, a linear prediction layer q : Z → ̃Y , and a cross entropy layer CE : Y × ̃Y → R. We then have the following proposition:\n\nProposition 1. With the assumptions in Theorem 1, let the network be a composition of representation extraction h and linear classifier q. Let σmax be the maximum singular value of the last layer linear prediction weight matrix (i.e. fine-tuning layer). If the representation extraction is Lh Lipschitz, then with probability at least 1-δ, we have:\n\nRt ≤ Remp\n\nc + Lhσmax\n\n√\n\n2(2τ + ετ ) + Γ.\n\nThe proof is provided in the Appendix. The advantage of decomposing the Lipschitz constant of the objective function into the Lipschitz constants of the representation and prediction functions is that controlling Lh does not require access to the labels. This suggests that we can leverage the unlabeled data to control Lh and let the supervised learning part to control σmax. Let the representation of the last layer be Z = h(X), we have Lh defined as ∥h(X1) − h(X2)∥ ≤ Lh∥X1 − X2∥, ∀X1, X2. Then, our goal is to leverage more data to improve Lh, and fine-tune the last linear layer to control the σmax with labeled data.\n\nIn this work, we use the SimCLR to learn the representation function h. SimCLR first defines a random transformation set T (i.e. cropping, color jittering, flipping, rotation, i.e.), and then samples two random transformations, T1 and T2, to generate two views T1(X) and T2(X) for each image. Then, the model is trained to maximize their cosine similarity. Note that the transformations of SimCLR is usually make images after transformation T (X) to be semantically close to X. Therefore we assume that there exists some distance metric d (i.e. Wasserstein distance) so that the distance , ∀Ti ∼ T ). between the original image and the transformed one is small (i.e. d(X, Ti(X)) ≤\n\nThen, by triangle inequality, we have d(Ti(X), Tj(X)) ≤ τ . Thus, SimCLR actually samples two images which are closed in some distance metric, and then maximizes the cosine similarity, which is equivalent to minimizing the normalized l2 distance. This process can be viewed as enforcing a Lipschitz regularization for the representation learning, since SimCLR minimizes the normalized l2 distance in representation space for two random images that are close in Wasserstein distance. The remaining part that needs to be controlled is the maximum singular value of the last linear layer, which can be enforced by using spectral normalization Miyato et al. (2018a). Motivated by this, in Alg. 2 we propose a semi-supervised robust algorithm to defend against backdoor attacks.\n\nτ 2\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2: Semi-Supervised Algorithm for Robust Learning Against Backdoor Attacks input: Corrupted training data Xε, Yε, Clean Augmented Dataset Xaug, perturbation limit: τ , learning\n\nwith noisy label algorithm A (e.g. PRL, SPL, Bootstrap).\n\nreturn trained neural network ; Use SimCLR on [Xε, Xaug] to learn the representation function h, then fine-tune the linear layer with\n\nspectral normalization using noisy label algorithm as following\n\nwhile epoch ≤ max_epoch do\n\nfor sampled minibatch Mx, My in Xε,Yε do\n\nminf L(flin(h(Mx)), My) w.r.t. flin by using robust algorithm A for one step use spectral normalization to truncate the largest singular value of last linear layer.\n\nend\n\nend\n\nDataset\n\nε\n\nAT\n\nCIFAR10 with Patch Attack, Poison Accuracy\n\n0.15 66.64 ± 5.28 2.09 ± 0.13 0.25 63.98 ± 7.16 2.01 ± 0.23 0.35 60.19 ± 1.35 1.98 ± 0.15 0.45 51.25 ± 1.81 1.94 ± 0.12\n\nSPL-AT BootStrap Bootstrap-AT 3.05 ± 0.47 81.71 ± 0.37 80.15 ± 0.42 34.60 ± 1.57 77.60 ± 3.81 2.10 ± 0.10 56.67 ± 0.23 35.90 ± 2.13 2.75 ± 0.17 45.94 ± 25.19 78.14 ± 0.48 10.87 ± 2.13 22.17 ± 10.51 2.13 ± 0.15 60.85 ± 0.42 29.02 ± 5.34 2.66 ± 0.16 31.27 ± 17.63 75.04 ± 0.29 11.74 ± 1.24 15.40 ± 7.56 2.01 ± 0.09 56.84 ± 0.15 51.59 ± 3.24 17.50 ± 1.66 58.90 ± 12.52 12.32 ± 1.20 14.00 ± 5.35 1.88 ± 0.04 44.21 ± 3.24 24.10 ± 6.23 2.53 ± 0.20\n\nFine-Pruning\n\nStandard\n\nSpecSig\n\nSPL\n\nBackdoor Attack Defense Accuracy. PRL-AT PRL\n\nCIFAR10 with Patch Attack, Clean Accuracy\n\n0.15 66.77 ± 5.17 85.22 ± 0.48 82.62 ± 0.26 82.06 ± 0.16 80.25 ± 0.43 77.35 ± 2.76 77.70 ± 3.78 85.40 ± 0.37 80.34 ± 0.37 80.32 ± 0.26 0.25 63.98 ± 7.16 85.25 ± 0.19 81.90 ± 0.25 78.57 ± 1.03 78.22 ± 0.56 69.52 ± 2.38 68.49 ± 2.76 85.20 ± 0.26 79.50 ± 0.15 80.40 ± 0.15 0.35 60.31 ± 1.37 84.86 ± 0.13 81.75 ± 0.25 73.63 ± 0.75 75.10 ± 0.31 60.23 ± 3.14 58.88 ± 3.46 84.73 ± 0.13 79.10 ± 0.27 72.01 ± 0.31 17.50 ± 1.66 58.90 ± 12.52 50.82 ± 1.48 14.00 ± 5.35 1.88 ± 0.04 78.73 ± 0.16 24.01 ± 0.34 0.45 51.25 ± 1.81 1.94 ± 0.12\n\n2.53 ± 0.20\n\nCIFAR10 with Blend Attack, Poison Accuracy\n\n0.15 65.15 ± 0.94 2.17 ± 0.17 24.98 ± 10.01 6.41 ± 3.91 0.25 56.98 ± 0.72 2.06 ± 0.10 33.33 ± 20.03 6.77 ± 2.81 9.42 ± 5.28 0.35 47.84 ± 1.49 1.86 ± 0.07 13.13 ± 7.11 8.13 ± 4.50 6.12 ± 2.86 0.45 34.66 ± 1.49 1.83 ± 0.11\n\n79.71 ± 0.33 11.60 ± 6.56 74.77 ± 3.53 2.29 ± 0.10 34.38 ± 0.13 70.74 ± 0.28 76.99 ± 0.37 11.60 ± 8.59 52.36 ± 10.57 2.03 ± 0.18 13.94 ± 0.24 75.40 ± 0.35 73.17 ± 0.96 12.71 ± 9.33 50.79 ± 7.92 1.97 ± 0.07 23.71 ± 0.43 66.87 ± 0.14 49.88 ± 8.43 8.69 ± 4.41 35.06 ± 4.00 1.88 ± 0.06 16.36 ± 0.26 41.32 ± 0.36\n\nCIFAR10 with Blend Attack, Clean Accuracy\n\n0.15 66.14 ± 0.98 85.54 ± 0.58 81.44 ± 0.58 77.51 ± 1.20 80.06 ± 0.34 76.25 ± 2.78 75.65 ± 3.11 85.28 ± 0.34 79.53 ± 0.15 83.60 ± 0.37 0.25 58.91 ± 5.70 84.95 ± 0.30 80.89 ± 0.65 71.45 ± 1.40 77.82 ± 0.26 67.86 ± 2.58 65.08 ± 0.82 85.06 ± 0.39 79.32 ± 0.26 81.23 ± 0.26 0.35 50.07 ± 13.26 84.72 ± 0.58 80.63 ± 0.57 66.22 ± 1.15 74.34 ± 1.01 60.52 ± 2.26 60.16 ± 2.39 84.72 ± 0.28 78.28 ± 0.17 76.63 ± 0.19 0.45 38.03 ± 15.42 84.36 ± 0.38 80.35 ± 0.39 55.78 ± 2.09 57.17 ± 9.02 49.48 ± 2.19 46.74 ± 0.71 84.07 ± 0.17 76.70 ± 0.24 62.53 ± 0.29\n\nTable 1: Performance on CIFAR10. ε is the corruption rate.\n\n4.4 HOW TO CHOOSE THE NOISY LABEL ALGORITHM\n\nOne key question regarding our framework is how to choose the noisy label algorithm. In practice, we found PRL gives consistent robustness against both badnet and blending attacks on different settings. This might be because PRL is designed for agnostic corrupted supervision, which is suitable for a variety of noisy label attack types.\n\nFrom a theoretical view, analyzing how different noisy label algorithms minimize the first term of RHS in Eq. 6 depends on the noisy label algorithm used. Here we present a high-level analysis for PRL. PRL guarantees convergence to the ε-approximated stationary point, where ε is the corrupted ratio. Formally, we have the following proposition:\n\nProposition 2. Given the assumptions used in Theorem 1, assume the objective function φw = L ◦ f is Lφ-Lipschitz smooth and satisfying the PL condition 1 2 ∥∇φw∥ ≥ μ(φw − φw∗ ). Then, with the assumption of bounded operator norm of gradient before loss layer, we have with probability at least 1-δ, by applying PRL-AT, we have:\n\nRt ≤ 1\n\nμ O(ε) + Lφ(2τ + ετ ) + Γ.\n\nThe proof is in the Appendix. In general, considering φ is a deep neural network, the first term is more difficult to analyze without further assumptions (i.e. PL condition). Nevertheless, empirical study shows that many noisy label algorithms can effectively minimize the first term, noisy label loss, even though some of them have theoretical guarantees while do not. This motivates us to treat these algorithms as black-box algorithms.\n\n5 EXPERIMENT\n\nWe perform experiments on CIFAR10, CIFAR100, and STL10 benchmark data to validate our approach. We use ResNet-32 (He et al., 2016) as the backbone network structure for the experiments. We also use AdamW (Loshchilov & Hutter, 2017) with initial learning rate as 3e-4 as the optimizer for all methods. The batchsize is 128 for all methods. The evaluation metric is the top-1 accuracy for both clean testing data and testing data with backdoor trigger.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Accuracy on CIFAR10 in semi-supervised setting. ε is the corruption rate.\n\nBackdoor Attack Defense Accuracy. Dataset\n\nStandard\n\nε\n\nCIFAR100→ CIFAR10\n\nSTL10 → CIFAR10\n\nPRL-AT\n\nPRL-SimCLR PRL-SimCLR-SN PRL-SimCLR PRL-SimCLR-SN\n\nPatch Attack, Poison Accuracy\n\nPatch Attack, Clean Accuracy\n\nBlend Attack, Poison Accuracy\n\nBlend Attack, Clean Accuracy\n\n0.15 26.66 ± 0.07 64.43 ± 8.37 83.72 ± 0.04 0.25 5.67 ± 0.02 60.94 ± 0.88 26.91 ±0.05 0.35 5.20 ± 0.13 55.53 ± 0.60 36.12 ± 0.06 0.45 5.28 ± 0.24 46.46 ± 0.33 16.97 ± 1.04\n\n0.15 67.16 ± 0.09 64.44 ± 0.21 83.65 ± 0.03 0.25 67.34 ± 0.07 60.92 ± 0.27 81.54 ± 0.42 0.35 65.44 ± 0.17 55.62 ± 0.47 79.17 ± 0.41 0.45 63.70 ± 0.13 46.48 ± 0.34 73.97 ± 1.02\n\n0.15 6.55 ± 0.05 64.22 ± 0.26 82.83 ± 0.43 0.25 5.44 ± 0.07 59.88 ± 0.98 81.22 ± 0.59 0.35 4.56 ± 0.14 52.66 ± 2.02 78.18 ± 1.71 0.45 4.82 ± 0.27 35.62 ± 0.92 69.81 ± 2.19\n\n0.15 69.46 ± 0.04 63.60 ± 0.29 83.43 ± 0.65 0.25 68.02 ± 0.05 54.54 ± 0.31 81.80 ± 0.36 0.35 66.64 ± 0.08 54.54 ± 0.41 78.70 ± 0.80 0.45 65.34 ± 0.12 40.25 ± 1.13 71.13 ± 1.37\n\n82.99 ± 0.05 80.78 ± 0.12 77.90 ± 0.23 32.94 ± 0.31\n\n83.08 ± 0.05 80.95 ± 0.13 78.23 ± 0.21 72.93 ± 0.33\n\n81.96 ± 0.04 80.21 ± 0.17 77.47 ± 0.22 71.45 ± 0.39\n\n82.62 ± 0.04 80.76 ± 0.16 78.13 ± 0.23 72.51 ± 0.37\n\n80.73 ± 0.06 78.07 ± 0.08 26.91 ± 0.14 45.40 ± 0.25\n\n80.79 ± 0.05 78.14 ± 0.09 71.87 ± 0.16 45.36 ± 0.22\n\n79.82 ± 0.08 77.34 ± 0.15 72.70 ± 0.19 47.36 ± 0.23\n\n80.64 ± 0.07 78.09 ± 0.13 71.92 ± 0.18 47.93 ± 0.23\n\n82.96 ± 0.05 77.92 ± 0.23 45.99 ± 0.27 45.39 ± 0.31\n\n83.01 ± 0.04 78.05 ± 0.31 63.99 ± 0.25 45.41 ± 0.29\n\n83.82 ± 0.08 82.33 ± 0.18 80.31 ± 0.23 76.03 ± 0.29\n\n84.50 ± 0.08 81.09 ± 0.16 82.99 ± 0.24 75.23 ± 0.26\n\nFor backdoor attacks, we use simple badnet attack (Gu et al., 2017) and Gaussian blending attack (Chen et al., 2017), since these two attacks do not require any information about the model or training procedure (Pang et al., 2020). Examples of the poisoned samples can be found in the Appendix. We deploy the multi-target backdoor attack in this paper and we provided the discussion about single-target attack and clean label attack in the appendix. Our data poisoning approach is as follows: we first systematically flip the label to perform a label-flipping attack. We then add triggers to the features associated with the attacked samples. Without adding the trigger, the problem would have reduced to the noisy label problem.\n\nWe use the following two evaluation metrics: (1) top-1 clean accuracy, which is calculated from the clean test examples without any triggers and (2) top-1 poison accuracy, which is calculated by comparing the predicted class of the poisoned test examples against their ground truth clean labels. The first metric evaluates how well the model performs on benign (uncorrupted) data while the second metric assesses how well the model performs on the corrupted data. We vary the training data poisoning rate as [15%, 25%, 35%, 45%] to investigate how the algorithms perform for different corruption ratios. All the methods are trained for 100 epochs, Furthermore, we assume there is no clean validation data available. Thus, it is difficult to perform early stopping or decide which epoch result to use. We report the average accuracy across the last 10 epochs for each method.\n\nWe study three noisy label algorithms by comparing the performance of the original and reinforced methods. Specifically, we choose SPL, PRL, and Bootstrap as our original noisy label algorithm and denote their corresponding reinforced algorithm with adversarial training as SPL-AT, PRL-AT, Bootstrap-AT. We also compare our method against adversarial training only (AT), which uses adversarial training without a noisy label algorithm. To measure the success of the attack, we also include the Standard training (i.e. no defense) results.\n\nWe also evaluate the performance of other backdoor defense algorithms. Note that a large fraction of them are either designed for single target attacks (Liu et al., 2018) or require clean data (Liu et al., 2018; Wang et al., 2019; Li et al., 2021). In this paper, we compare our framework against the following two baselines: (1) spectral signature (Tran et al., 2018): which filters the data by examining the score of projecting to singular vector, and (2) fine-pruning (Liu et al., 2018), which prunes the model by deleting non-activated neurons. Note this method uses 5% clean training data.\n\nHow well do existing robust noisy label algorithms defend against backdoor attacks? To answer this, we evaluate the performance of PRL, SPL, and Bootstrap on the CIFAR10 and CIFAR100 datasets. The results of CIFAR10 are given in Table 1 and the results of CIFAR100 are given in the Tables 5 in Appendix due to the limited space. Observe that the existing algorithms perform well on the benign testing data (i.e. high clean accuracy) but poorly on the corrupted data (i.e. low poison accuracy) especially when the corruption ratio is high. This suggests the ability of backdoor attacks to compromise the defense mechanism of existing robust noisy label algorithms.\n\nHow adversarial training improves noisy label algorithms? To investigate whether adversarial training can enhance the robustness of existing noisy label algorithms against backdoor attacks, we\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Sensitivity analysis of ε and τ . Average top-1 accuracy across three random seeds. The first number is the clean accuracy while the second number is the poisoned accuracy. The hyperparameter ε is fixed to be 0.5 while the ground truth ε is varied. The sensitivity analysis for the adversarial budget τ is conducted by fix both estimated ε and ground-truth ε as 0.25\n\nε 0.15 0.25 0.35 0.45\n\nPRL-AT (patch) 68.14/68.00 71.78/71.74 74.26/74.15 69.91/27.02\n\nPRL-AT (blend) 67.97/68.48 71.28/71.87 74.17/74.32 64.78/54.19\n\nτ 0.01 0.05 0.1 0.5\n\nPRL-AT (patch) 73.34/3.00 78.87/78.52 76.03/75.24 46.82/46.41\n\nPRL-AT (blend) 73.15/22.13 77.28/77.14 65.23/65.02 50.82/50.21\n\nevaluate the performance of our proposed reinforced algorithms, SPL-AT, PRL-AT and Bootstrap-AT, on both the clean and corrupted test examples. The results shown in Tables 1 and 5 suggest that the performance of the reinforced noisy label algorithms on the triggered data is largely boosted, with significant improvement in the poison accuracy. The improvement is observed for all three noisy label algorithms, which indicates the effectiveness of the proposed method on improving the robustness of the existing algorithms against backdoor attacks. Also, compared to adversarial training only (AT), adding noisy labels does indeed improve the performance, particularly, when comparing PRL-AT to AT. We also found that compared to consistency-based noisy-label algorithm (i.e., Bootstrap), the filtering based algorithms (i.e., SPL and PRL) are more easier to be boosted by adversarial training. The potential reason behind this could be that the filtering-based methods are more efficient compared to consistency-based algorithms Han et al. (2018); Jiang et al. (2017); Liu et al. (2021). Finally, we observe that PRL-AT has higher poisoned accuracy compared to spectral signature and fine-pruning under most settings while its clean accuracy is still high, which indicates the advantage of PRL-AT. For high corruption ratio, the robustness of spectral signature and fine-pruning significantly decreases while PRL-AT still gives reasonable poison accuracy. Semi-supervised learning. We evaluate our algorithm on backdoored CIFAR10 data with CIFAR100 or STL10 (unlabeled part) as augmented data. For the semi-supervised setting, we only use 20% backdoored data as labeled training data (i.e. in backdoored CIFAR10, when ε = 0.2, we have 7500 clean labeled images, 2500 backdoored images, and clean augmented data without label). To investigate the advantage of decoupling the Lipschitz constant of the objective function and to determine whether semi-supervised learning helps improve robustness, we compare standard training, PRL-SimCLR (i.e. algorithm 2 with PRL as the noisy label algorithm without spectral normalization), PRL-SimCLR-SN (i.e. algorithm 2 with PRL as the noisy label algorithm) and PRL-AT. The results are given in Table 2. We see that PRL-AT provides consistent robustness against both patch and blending backdoor attacks. When utilizing more data, both PRL-SimCLR and PRL-SimCLR-SN can achieve improved performance for the blending attack. For the patch attack, PRL-SimCLR and PRL-SimCLR-SN show robustness when the corruption ratio is small. For large corruption ratio, PRL-SimCLR fails to achieve its robustness against patch attack while PRL-SimCLR-SN still maintains good performance, which indicates the necessity of adding spectral normalization to regularize the maximum singular value of the last layer. Ablation study for ε and τ . Since the degree of corruption is often unknown, we hereby investigate how well our algorithm performs without knowing the true corruption ratio. Specifically, we provide the worst-case result by setting ε = 0.5 for our algorithm regardless of the ground truth ε. We choose this as it would be impossible to learn a reasonable classifier when the corruption ratio is more than 0.5. We evaluate the performance of PRL-AT on CIFAR10 for both badnet and blending attacks. The results in Table 3 suggest that our algorithm is still robust despite using the highly-overestimated ε when compared to the standard training results in Table 1. We also vary the adversarial training budget τ since neither do we know the ground-truth τ in practice. We see that increasing τ makes the model more robust while when τ is too large, the clean accuracy significantly drops. This suggests a large adversarial training budget is preferred to defend against backdoor attacks, which is consistent with reported findings (Gao et al., 2021). Besides ε and τ , in appendix we present additional ablation studies of the inner-maximization and outer-minimization. 6 CONCLUSION In this paper, we investigate the connection between noisy label attack and backdoor data poisoning attack. We show that although existing robust noisy label algorithms cannot effectively defend against backdoor data poisoning attacks, adding adversarial training on the existing algorithm could largely improve its robustness against backdoor attacks. Both theoretical and empirical analysis have validated the effectiveness of our proposed meta algorithm.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nYoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41–48, 2009.\n\nBryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. arXiv preprint arXiv:1811.03728, 2018.\n\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep\n\nlearning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n\nJeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In International Conference on Machine Learning, pp. 1310–1320. PMLR, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nJohn C Duchi and Hongseok Namkoong. Learning models with uniform performance via distribu-\n\ntionally robust optimization. The Annals of Statistics, 49(3):1378–1406, 2021.\n\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization\n\nfor efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.\n\nYinghua Gao, Dongxian Wu, Jingfeng Zhang, Shu-Tao Xia, Gang Niu, and Masashi Sugiyama. Does\n\nadversarial robustness really imply backdoor vulnerability? 2021.\n\nJonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael Moeller, and Tom Goldstein. What doesn’t kill you makes you robust (er): Adversarial training against poisons and backdoors. arXiv preprint arXiv:2102.13624, 2021.\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. arXiv preprint arXiv:1412.6572, 2014.\n\nTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the\n\nmachine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\n\nBo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in neural information processing systems, pp. 8527–8537, 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nLu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. arXiv preprint arXiv:1712.05055, 2017.\n\nLu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels. In International Conference on Machine Learning, pp. 2304–2313, 2018.\n\nLu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on controlled noisy labels. In International Conference on Machine Learning, pp. 4804–4815. PMLR, 2020.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25:1097–1105, 2012.\n\nM Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable\n\nmodels. In Advances in neural information processing systems, pp. 1189–1197, 2010.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlexander Levine and Soheil Feizi. Deep partition aggregation: Provable defense against general\n\npoisoning attacks. arXiv preprint arXiv:2006.14768, 2020.\n\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks. arXiv preprint arXiv:2101.05930, 2021.\n\nYuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from noisy labels with distillation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1910–1918, 2017.\n\nBoyang Liu, Mengying Sun, Ding Wang, Pang-Ning Tan, and Jiayu Zhou. Learning deep neural\n\nnetworks under agnostic corrupted supervision. arXiv preprint arXiv:2102.06735, 2021.\n\nKang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pp. 273–294. Springer, 2018.\n\nYunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In European Conference on Computer Vision, pp. 182–199. Springer, 2020.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017.\n\nBence Major, Daniel Fontijne, Amin Ansari, Ravi Teja Sukhavasi, Radhika Gowaikar, Michael Hamilton, Sean Lee, Slawomir Grzechnik, and Sundar Subramanian. Vehicle detection with automotive radar using deep learning on range-azimuth-doppler tensors. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pp. 0–0, 2019.\n\nDavid A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual conference\n\non Computational learning theory, pp. 164–170, 1999.\n\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for\n\ngenerative adversarial networks. arXiv preprint arXiv:1802.05957, 2018a.\n\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–1993, 2018b.\n\nRen Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, and Ting Wang. Trojanzoo: Everything you ever wanted to know about neural backdoors (but were afraid to ask). arXiv preprint arXiv:2012.09302, 2020.\n\nGiorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944–1952, 2017.\n\nAditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying\n\nrobustness to adversarial examples. arXiv preprint arXiv:1811.01057, 2018.\n\nScott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.\n\nHadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers. arXiv preprint arXiv:1906.04584, 2019.\n\nAli Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6106–6116, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDávid Terjék. Adversarial lipschitz regularization. In International Conference on Learning Repre-\n\nsentations, 2019.\n\nBrandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. arXiv\n\npreprint arXiv:1811.00636, 2018.\n\nAlexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. 2018.\n\nBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pp. 707–723. IEEE, 2019.\n\nMaurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li. Rab: Provable robustness against\n\nbackdoor attacks. arXiv preprint arXiv:2003.08904, 2020.\n\nCheng-Hsin Weng, Yan-Ting Lee, and Shan-Hung Brandon Wu. On the trade-off between adversarial and backdoor robustness. Advances in Neural Information Processing Systems, 33:11973–11983, 2020.\n\nEric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pp. 5286–5295. PMLR, 2018.\n\nYuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pp. 2041–2055, 2019.\n\nKun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7017–7025, 2019.\n\nXingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? In International Conference on Machine Learning, pp. 7164–7173. PMLR, 2019.\n\nXiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples: Attacks and defenses for deep learning. IEEE transactions on neural networks and learning systems, 30(9):2805–2824, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nIn this section, we provided proof of theorem and more discussion.\n\nA.1 PROOF OF INEQUALITY IN EQ. (3)\n\nIn this section we provide a formal proof of the inequality in Eqt. (3) in the main paper:\n\n1 n\n\n(cid:88)\n\nx∈X,y∈Y\n\n[L(f (x + c), y)] ≤\n\n1 n\n\n(cid:88)\n\nx∈Xε,y∈Y\n\nφw(xi + c, y) + ετ L.\n\nProof. let G denote the initially clean sample set (i.e. (X, Y)), and B the corrupted sample set (i.e. the training set corrupted with a trigger whereas the labels are untouched). Let R denote the clean sample set which is replaced by the adversary (i.e. R is the subset of G, and is replaced by B, i.e. G\n\n= G \\ R ∪ B = (Xε, Y)), and let φw denote the function L ◦ f .\n\n′\n\nOne can decompose the inner part of our mini-max objective in Equation 2 as follows,\n\n1 n\n\n(cid:88)\n\nx∈X,y∈Y\n\n[L(f (x + c), y)] =\n\n1 n\n\n(cid:88)\n\ni∈G′ \\B\n\nφw(xi + c, y) +\n\n1 n\n\n(cid:88)\n\ni∈R\n\nφw(xi + c, y)\n\n=\n\n=\n\n≤\n\n≤\n\n1 n\n\n1 n\n\n1 n\n\n1 n\n\n(cid:88)\n\ni∈G′ \\B\n\n(cid:88)\n\nx∈Xε,y∈Y\n\n(cid:88)\n\nx∈Xε,y∈Y\n\n(cid:88)\n\nx∈Xε,y∈Y\n\nφw(xi + c, y) +\n\n1 n\n\n(cid:88)\n\nφw(xi + c, y) +\n\n1 n\n\n(cid:88)\n\ni∈B\n\nφw(xi + t + c, y) −\n\ni∈R (cid:32)\n\nφw(xi + t + c, y)\n\n1 n\n\n(cid:88)\n\ni∈B (cid:33)\n\nφw(xi + c, y) +\n\nφw(xi + c, y) +\n\n1 n\n\n(cid:88)\n\ni∈R\n\nφw(xi + c, y) −\n\n1 n\n\n(cid:88)\n\ni∈B\n\nφw(xi + c + t, y)\n\n(cid:32)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 n\n\n(cid:88)\n\ni∈R\n\nφw(xi + c, y) −\n\n1 n\n\n(cid:88)\n\ni∈B\n\nφw(xi + c + t, y)\n\n(cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nφw(xi + c, y) + εL∥t∥ ≤\n\n1 n\n\n(cid:88)\n\nx∈Xε,y∈Y\n\nφw(xi + c, y) + ετ L,\n\nThis concludes the proof.\n\nA.2 PROOF OF THEOREM 1 AND COROLLARY 1\n\nc\n\n, Remp c\n\nTheorem 2. Let ̃Remp , Rt, ε, τ , is defined as above. Assume the prior distribution of the network parameter w is N (0, σ), and the posterior distribution of parameter is N (w, σ) is the posterior parameter distribution, where w is learned according to training data. Let k to be the number of parameters, n to be the sample size, assume the objective function φw = L ◦ f is Lφ-lipschitz smooth, then, with probability at least 1-δ, we have:\n\nRt ≤ Remp\n\nc + Lφ(2τ + ετ ) +\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n1\n\n4 k log\n\n(cid:16)\n\n1 + ∥w∥2\n\n2 kσ2\n\n(cid:17)\n\nProof. we first decompose the gap as following\n\n+ 1\n\n4 + log n n − 1\n\nδ + 2 log(6n + 3k)\n\n.\n\nRt − Remp\n\nc = (Rt − Remp\n\nt\n\n) + (Remp\n\nt − Remp\n\nc\n\n) ≤ |(Rt − Remp\n\nt\n\n13\n\n)| + |(Remp\n\nt − Remp\n\nc\n\n)|\n\nUnder review as a conference paper at ICLR 2023\n\nWe bound the second part first.\n\nt − Remp\n\nRemp ≤ ∥Remp\n\nt − Remp\n\nc\n\nc\n\n∥\n\n=\n\n≤\n\n1 n\n\n1 n\n\n∥\n\n∥\n\n(cid:88)\n\nx∈Xr,y∈Yr\n\n(cid:88)\n\nx∈Xr,y∈Yr\n\n[φ(x + t, y) − φ(x + c, y)] +\n\n\n\n\n\n(cid:88)\n\nφ(x + t, y) −\n\n(cid:88)\n\nφ(x + c, y)\n\n ∥\n\n\n\n[φ(x + t, y) − φ(x + c, y)] ∥ +\n\nx∈Xo,y∈Yo 1\nn\n\n(cid:88)\n\n∥\n\nx∈Xo,y∈Yo\n\nx∈Xb,y∈Yo\n\nφ(x + t, y) −\n\n(cid:88)\n\nφ(x + c, y)∥\n\nx∈Xb,y∈Yo\n\n≤ (1 − ε)Lφ∥t − c∥ + εLφ∥t − c∥ + Lφ max xo,xb ≤ (1 − ε)Lφ∥t − c∥ + εLφ∥t − c∥ + εLφ∥t∥ = Lφ∥t − c∥ + εLφ∥t∥ ≤ Lφ2τ + εLφ∥t∥ ≤ Lφ(2τ + ετ )\n\n∥xo − xb∥\n\nNow, we bound the second term. Note the second term is a typical gap term between empirical loss and generalization loss, and there are many approaches to bound this term like VC dimension. Since we aimed to focus the deep neural network, we follow the PAC-Bayes framework McAllester (1999) to analyze the generalization bound. Specifically, we use results from Foret et al. (2020), which gives (cid:115)\n\n+ 1\n\n4 +log n\n\nδ +2 log(6n+3k)\n\nn−1\n\nunder the assumption of gaussian prior and posterior. The\n\n(cid:18)\n\n1\n\n4 k log\n\n1+\n\n(cid:19)\n\n∥w∥2 2\nkσ2\n\nproof for this can be found in the appendix of Foret et al. (2020) (i.e. equation 13 on the paper).\n\nAs for the corollary 1, the proof is straightforward. By decomposing the Lipschitz constant of the loss function to the Lipschitz constant of representation network, last linear layer, and cross-entropy loss, respectively. Since the cross-entropy loss gradient is ∥ˆy − y∥, where y is a one-hot vector and ˆy 2. As for the linear is a probability vector. Thus, the maximum gradient (i.e. Lipschitz constant) is layer, according to the definition of the operator norm, the Lipschitz constant is exactly the maximum singular value of that linear layer. This concludes the proof.\n\n√\n\nA.3 PROOF OF PROPOSITION 2\n\nWe first introduct the property of PRL in the following corollary:\n\nCorollary 1 (Convergence of PRL to clean objective (Liu et al., 2021)). Assuming the maximum clean gradient before loss layer has bounded operator norm:∥W ∥op ≤ C, applying PRL to any ε-fraction supervision corrupted data, yields mint∈[T ] E (∥∇φ(wt)∥) = O(ε q) for large enough T , where q is the dimension of the supervision.\n\n√\n\nDetails can be found in Liu et al. (2021). According to above corollary, let wP RL is the solution get by PRL algorithm, we can have ∥∇wP RL Remp ∥ = O(ε) (i.e. assume q is small). With Polyak-\n\nc\n\nLojasiewicz (PL) condition with some constant μ such that 1\n2 c\n\nhave μ(Remp c − Remp∗ network, the global optima Remp∗ PRL as the noisy label algorithm in our framework can guarantee Remp\n\n∥∇wP RL Remp\n\n) ≤\n\nc\n\nc\n\nis usually 0. Thus, we can conclude that with PL condition, using can be minimized to the\n\nc\n\n∥ = O(ε). For a highly-overparameterized deep neural\n\n∥∇f (x)∥ ≥ μ(f (x) − f ∗) holds, we\n\n1 2\n\norder\n\nO(ε).\n\n1 μ\n\nA.4 NOISY LABEL ALGORITHM\n\nThe details of PRL, SPL, and Bootstrap is showed in table 4. We would like to highlight that many other potential noisy label algorithms can be applied to our framework and it is important to further investigate them. We leave a more comprehensive exploration of noisy label algorithms as future work.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nPRL SPL Bootstrap\n\nMini-batch Keep data with small loss-layer gradient norm and perform back-propagation Keep data with small loss and perform back-propagation change the label by using ytrue = αytrue + (1 − α)ypred and perform back-propagation\n\nTable 4: Overview of noisy-label defending algorithms, which achieve robustness against up to 45% of pairwise flipping label noises.\n\nA.5 EXPERIMENT RESULTS FOR CIFAR 100\n\nThe experiment results for CIFAR100 is in Table 5. As we can see, the pattern is consistent with the CIFAR10 experiment result.\n\nDataset\n\nε\n\nAT\n\nBootStrap\n\nBackdoor Attack Defense Accuracy. PRL-AT PRL\n\nBootstrap-AT\n\nSPL\n\nSPL-AT\n\nStandard\n\nFine-Pruning\n\nSpecSig\n\nCIFAR100 with Patch Attack, Poison Accuracy\n\n0.15 23.70 ± 1.39 5.23 ± 0.81 44.74 ± 4.05 15.15 ± 9.17 47.11 ± 0.58 24.87 ± 5.27 42.24 ± 0.76 5.28 ± 0.50 12.50 ± 0.51 30.01 ± 0.23 0.25 21.84 ± 1.17 3.07 ± 0.23 44.09 ± 1.10 17.53 ± 18.06 43.81 ± 0.41 8.48 ± 1.13 35.46 ± 1.13 3.10 ± 0.60 13.00 ± 0.53 33.82 ± 0.18 0.35 17.16 ± 1.09 2.85 ± 0.12 40.14 ± 0.20 20.83 ± 10.03 39.76 ± 0.72 7.37 ± 0.59 28.41 ± 1.72 3.24 ± 1.04 25.80 ± 0.12 29.07 ± 0.21 0.45 13.61 ± 0.74 10.60 ± 10.49 31.21 ± 0.30 23.98 ± 9.32 29.76 ± 1.11 7.26 ± 0.76 20.43 ± 1.69 10.51 ± 11.21 32.33 ± 0.04 16.83 ± 0.43\n\nCIFAR100 with Patch Attack, Clean Accuracy\n\n0.15 34.08 ± 0.40 52.39 ± 0.38 47.76 ± 0.14 50.50 ± 0.41 47.21 ± 0.56 46.38 ± 0.41 42.38 ± 0.73 52.42 ± 0.59 43.42 ± 0.12 44.23 ± 0.16 0.25 31.72 ± 0.75 50.54 ± 0.25 44.82 ± 0.52 47.49 ± 0.91 43.89 ± 0.35 39.98 ± 0.80 35.65 ± 1.14 50.53 ± 0.55 41.11 ± 0.03 39.64 ± 0.25 0.35 29.50 ± 1.73 48.41 ± 0.42 40.38 ± 0.18 44.21 ± 0.21 39.80 ± 0.67 34.11 ± 1.10 28.52 ± 1.70 48.75 ± 0.71 39.34 ± 0.08 29.23 ± 0.39 0.45 23.93 ± 3.43 41.46 ± 5.00 31.48 ± 0.38 34.34 ± 0.91 29.79 ± 1.13 27.87 ± 2.28 20.55 ± 1.75 41.02 ± 6.06 36.32 ± 0.13 16.94 ± 0.14\n\nCIFAR100 with Blend Attack, Poison Accuracy\n\n0.15 33.65 ± 0.54 2.19 ± 0.28 46.65 ± 0.33 2.10 ± 0.43 46.01 ± 0.50 6.14 ± 1.12 41.57 ± 0.74 2.09 ± 0.20 19.09 ± 0.48 35.64 ± 0.44 8.80 ± 0.32 33.61 ± 0.36 0.25 30.95 ± 0.42 1.17 ± 0.08 41.84 ± 0.59 1.45 ± 0.21 41.78 ± 0.76 2.95 ± 0.56 33.54 ± 1.76 1.12 ± 0.20 6.12 ± 0.05 27.13 ± 0.17 0.35 27.30 ± 0.45 1.05 ± 0.06 31.88 ± 1.26 1.51 ± 0.17 34.51 ± 1.60 2.00 ± 0.49 25.71 ± 2.31 1.08 ± 0.16 8.13 ± 0.02 18.35 ± 0.32 0.45 20.79 ± 4.97 0.99 ± 0.07 23.61 ± 1.07 2.68 ± 1.17 22.00 ± 1.95 2.39 ± 0.17 18.62 ± 1.21 0.92 ± 0.11\n\nCIFAR100 with Blend Attack, Clean Accuracy\n\n0.15 34.22 ± 0.58 52.65 ± 0.19 47.77 ± 0.36 48.61 ± 0.18 46.92 ± 0.47 46.01 ± 0.40 42.40 ± 0.70 52.60 ± 0.59 43.30 ± 0.11 45.54 ± 0.16 0.25 33.65 ± 0.55 51.12 ± 0.37 44.75 ± 0.45 45.23 ± 0.34 42.87 ± 0.72 40.47 ± 1.47 35.71 ± 1.10 50.98 ± 0.43 41.11 ± 0.08 41.02 ± 0.24 0.35 28.14 ± 0.48 49.80 ± 0.24 40.85 ± 0.37 40.46 ± 0.17 36.30 ± 1.24 35.70 ± 1.68 28.56 ± 2.05 49.65 ± 0.49 39.84 ± 0.06 32.13 ± 0.35 0.45 22.03 ± 0.49 48.46 ± 0.53 34.78 ± 1.39 34.98 ± 0.83 24.71 ± 1.37 29.91 ± 1.40 21.82 ± 1.21 48.07 ± 0.52 37.83 ± 0.08 19.40 ± 0.27\n\nTable 5: Performance on CIFAR100. ε is the corruption rate.\n\nA.6 DISCUSSION ABOUT CLEAN LABEL ATTACK AND SINGLE TARGET BACKDOOR ATTACK\n\nIn this section, we provide a further discussion about clean label attack and single target backdoor attack. Thus, we will discuss these two types of attack separately.\n\nA.6.1 CLEAN LABEL ATTACKS\n\nOne interesting question is Can our framework defend against clean label (CL) attack? In short, yes. By reviewing our bound in 1, we have:\n\nRt ≤ Remp\n\nc + Lφ(2τ + ετ ) + Γ.\n\n(7)\n\nc\n\nThe impact of clean label attack is that now the first term Remp becomes the clean empirical loss without noisy label. Thus, if we can control the lipschitz constant, then the classifier should be robust to backdoor attack. This makes sense since a classifier with small lipschitz constant should not change prediction results given small perturbations (i.e. adding trigger). Thus, our theorem suggests that using adversarial training is enough to defend against clean label backdoor attack. This conclusion seems to be contradict with the previous study in (Weng et al., 2020). However, there are further recent study shows that adversarial training does improve the robustness against the backdoor attack as long as we give enough adversarial training budget (Gao et al., 2021; Geiping et al., 2021). To the best of our knowledge, the CL attack cannot be applied to multi-target backdoor attack in our experiment setting. We therefore tried adversarial training (AT) on single target CL attack, and adversarial training achieves 88/84 clean/poison accuracy with 500 poisoned image in CIFAR10, which is consistent with previous study (Gao et al., 2021; Geiping et al., 2021). We provide the following explanation. The naive CL method in the original CL paper (Turner et. al. 2017) directly adds a trigger to the target class. Such approach may easily fail since the model can easily classify the clean image without using trigger. Adding adversarial attack on the original image makes the classification harder and therefore the learned model tends to pick up the trigger, leading to more effective attacks. More details can be found in section 4 of the original CL paper. In our framework, we train the network using adversarial training, which will make the model robust to the adversarial attack, thus reducing the proposed CL attack to the naive CL attack.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nA.6.2 SINGLE TARGET ATTACK\n\nNow, we discuss the single target attack, which is one of the most popular backdoor attack mechanisms. Unlike our experiment setting, single target attacks choose one target class and add small amounts of poisoned data to attack the model. Previous studies also show that adversarial training alone can achieve robustness against single target backdoor attacks (Gao et al., 2021; Geiping et al., 2021). Thus, it is also interesting to investigate whether our framework can defend the single-target attacks. We also start by reviewing our bound below:\n\nRt ≤ Remp\n\nc + Lφ(2τ + ετ ) + Γ.\n\n(8)\n\nOne of the most important differences between the single-target attack and multi-target attacks is that the corruption ratio ε of the single target is much smaller than the multi-target attacks. For example, in our setting, the corruption ratio varies from 0.15 to 0.45 while for most typical single target backdoor attack, the number of injected corrupted images are very small (i.e. less than 500, which is approximately 0.01 corruption ratio in CIFAR10). Thus, it is necessary to investigate how ε affects the first two terms. As we can see, due to the backdoor attack setting, the Lipschitz constant would be extremely large since similar images have different labels (i.e. same image before/after injecting the trigger). Thus, although ε is small, the second term could still be large due to the large Lipschitz constant. The key difference between a multi-target attack and a single-target attack is the first term. In the single-target attacks with small ε, we claim that using noisy label algorithm is not as important as using it in multi-target attack settings. This is because we observed that in most noisy-label attacks, only flipping an extremely small amount of the data label cannot decrease the model performance significantly. According to our best knowledge, almost all robust learning against noisy label algorithms studies the scenario when the corrupted rate of the label is at least 0.15. Thus, the first term will be still small without using any noisy label algorithm due to small ε, which explained why only using adversarial training with enough budget can defend against the single target backdoor attack (Gao et al., 2021; Geiping et al., 2021).\n\nA.7 ABLATION STUDY OF INNER MAXIMIZATION AND OUTER MINIMIZATION\n\nIn this section, we aim to explore more about the proposed framework. Since our algorithm use the noisy-label solver for both inner and outer optimization. A interesting question to ask is that whether both inner and outer noisy-label solver plays an important role in defense the backdoor attack. Thus, we have two variants. One is we only use noisy label algorithm to update the model for outer minimization and another one is we only use the noisy label algorithm to update the model for inner maximization. The results can be found at table 6 and table 7 in the appendix. As we could see in these two tables, using noisy label algorithm to perform the inner maximization is more important compared to using noisy label algorithm to perform out minimization.\n\nDataset\n\nCIFAR10 with Patch Attack, Poison Accuracy\n\nCIFAR10 with Patch Attack, Clean Accuracy\n\nCIFAR10 with Blend Attack, Poison Accuracy\n\nCIFAR10 with Blend Attack, Clean Accuracy\n\nε 0.15 0.25 0.35 0.45\n\n0.15 0.25 0.35 0.45\n\n0.15 0.25 0.35 0.45\n\n0.15 0.25 0.35 0.45\n\nBackdoor Attack Defense Accuracy.\n\nBootStrap-inner Bootstrap-outer\n\n3.15 ± 0.61 2.74 ± 0.10 2.70 ± 0.24 2.32 ± 0.08\n\n82.58 ± 0.33 82.14 ± 0.28 81.71 ± 0.46 81.53 ± 0.16\n\n3.20 ± 0.63 2.73 ± 0.09 2.67 ± 0.15 2.51 ± 0.11\n\n82.45 ± 0.25 81.87 ± 0.23 81.55 ± 0.54 81.00 ± 0.47\n\nPRL-inner 80.78 ± 0.31 79.07 ± 0.20 76.06 ± 0.37 67.87 ± 2.63\n\n80.86 ± 0.31 79.10 ± 0.17 76.08 ± 0.34 69.96 ± 0.37\n\nPRL-outer 2.84 ± 0.23 2.50 ± 0.10 2.39 ± 0.26 2.24 ± 0.10\n\nSPL-inner 65.50 ± 16.22 18.95 ± 9.91 13.45 ± 5.40 12.10 ± 4.46\n\nSPL-outer 3.09 ± 0.35 2.58 ± 0.19 2.38 ± 0.14 2.23 ± 0.26\n\n83.09 ± 0.12 83.13 ± 0.21 82.83 ± 0.38 82.78 ± 0.18\n\n76.48 ± 3.03 69.33 ± 2.57 59.76 ± 3.59 49.31 ± 0.53\n\n83.02 ± 0.49 83.30 ± 0.13 83.05 ± 0.38 82.84 ± 0.27\n\n29.85 ± 10.65 14.81 ± 10.42 6.52 ± 3.80 11.58 ± 14.94\n\n40.79 ± 13.27 27.57 ± 10.93 17.41 ± 10.13 9.01 ± 4.66\n\n48.21 ± 14.90 80.38 ± 0.15 46.29 ± 18.09 78.44 ± 0.19 27.34 ± 18.42 54.46 ± 10.45 21.18 ± 11.85 14.58 ± 7.12 71.93 ± 2.69 5.16 ± 1.64 64.98 ± 2.74\n\n46.12 ± 13.77 42.30 ± 5.85\n\n11.25 ± 5.92 5.90 ± 2.28\n\n72.89 ± 6.14\n\n81.54 ± 0.25 80.99 ± 1.12 81.04 ± 0.81 81.06 ± 0.25\n\n81.18 ± 0.75 80.37 ± 0.94 79.54 ± 1.32 78.93 ± 0.84\n\n80.73 ± 0.18 78.23 ± 0.46 71.62 ± 2.65 62.34 ± 2.51\n\n82.51 ± 0.36 82.35 ± 0.65 82.55 ± 0.47 82.15 ± 0.48\n\n76.11 ± 3.32 66.64 ± 2.31 57.44 ± 1.78 48.82 ± 0.94\n\n82.35 ± 0.22 82.15 ± 0.37 81.81 ± 1.00 81.81 ± 1.06\n\nTable 6: Ablation study on CIFAR10. ε is the corruption rate.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nDataset\n\nCIFAR100 with Patch Attack, Poison Accuracy\n\nCIFAR100 with Patch Attack, Clean Accuracy\n\nCIFAR100 with Blend Attack, Poison Accuracy\n\nCIFAR100 with Blend Attack, Clean Accuracy\n\nBootStrap-inner Bootstrap-outer\n\nPRL-inner\n\nPRL-outer\n\nSPL-inner\n\nSPL-outer\n\nBackdoor Attack Defense Accuracy.\n\n45.76 ± 2.65 44.41 ± 1.55 40.02 ± 0.19 31.12 ± 0.40\n\n48.01 ± 0.28 45.27 ± 0.82 40.49 ± 0.23 31.32 ± 0.48\n\n46.72 ± 0.23 41.64 ± 1.54 31.18 ± 2.83 22.98 ± 1.18\n\n48.05 ± 0.33 44.85 ± 0.52 40.80 ± 0.56 35.32 ± 1.77\n\n41.66 ± 8.37 43.65 ± 0.88 38.72 ± 0.60 29.46 ± 0.33\n\n47.91 ± 0.21 44.68 ± 0.27 38.98 ± 0.47 29.81 ± 0.34\n\n46.56 ± 0.26 40.60 ± 0.98 30.91 ± 2.02 23.37 ± 0.92\n\n47.83 ± 0.29 44.59 ± 0.31 40.08 ± 0.41 34.13 ± 1.13\n\n47.34 ± 0.44 44.32 ± 5.45 43.05 ± 0.39 43.41 ± 6.36 44.71 ± 0.40 41.13 ± 5.82 35.69 ± 1.05 41.81 ± 3.89 40.19 ± 0.39 38.75 ± 0.60 24.98 ± 6.34 38.97 ± 1.10 31.49 ± 1.04 29.74 ± 0.35 20.13 ± 1.80 30.19 ± 0.31\n\n47.43 ± 0.43 48.41 ± 0.40 43.29 ± 0.21 48.12 ± 0.36 44.83 ± 0.38 45.58 ± 0.39 36.21 ± 0.80 45.14 ± 0.64 40.40 ± 0.41 39.46 ± 0.30 29.58 ± 1.49 39.89 ± 0.50 31.49 ± 1.02 30.39 ± 0.27 20.70 ± 1.51 30.77 ± 0.55\n\n46.59 ± 0.43 46.83 ± 1.00 42.15 ± 0.68 46.80 ± 0.70 43.43 ± 0.59 40.02 ± 1.44 34.10 ± 1.60 39.30 ± 3.16 35.84 ± 1.71 28.86 ± 2.82 25.36 ± 2.65 28.94 ± 3.49 24.60 ± 2.19 22.16 ± 3.73 19.57 ± 1.64 24.17 ± 2.70\n\n47.24 ± 0.65 48.43 ± 0.44 42.94 ± 0.55 48.37 ± 0.68 44.17 ± 0.36 45.19 ± 0.26 36.18 ± 1.20 45.06 ± 0.18 38.23 ± 0.80 41.84 ± 0.51 30.23 ± 1.25 41.18 ± 0.69 27.33 ± 1.37 39.06 ± 0.45 24.22 ± 1.66 38.62 ± 1.30\n\nε 0.15 0.25 0.35 0.45\n\n0.15 0.25 0.35 0.45\n\n0.15 0.25 0.35 0.45\n\n0.15 0.25 0.35 0.45\n\nTable 7: Ablation study on CIFAR100. ε is the corruption rate.\n\n(a) clean data\n\n(b) two samples are added with small trigger and flipped label\n\n(c) Inner maximization by noisy label algorithm\n\n(d) Reduce the backdoor attack to label flipping attack\n\nFigure 1: Illustration of our meta algorithm. By combining the minimax objective and noisy label algorithm, we could reduce a backdoor attack problem to a label flipping attack. The left most is the clean original data. The second shows corrupted samples. The third figure shows the inner maximization step while the last figure shows the outer minimization step.\n\nA.8 DISCUSSION ABOUT LIPSCHITZ REGULARIZATION AND ADVERSARIAL TRAINING\n\nAs seen from the above theorem that a small Lipschitz constant could bring robustness against backdoor attack. In this section, we elaborate why we claim adversarial training helps Lipschitz regularization. The definition of Lipschitz function is ∥f (x) − f (y)∥ ≤ L∥x − y∥, ∀x, y. Since the Lipschitz constant shows in the upper bound of the error, we would like to get the minimum Lipschitz constant to tighten the bound. Follow (Terjék, 2019), the minimum Lipschitz constant can be written as:\n\n∥f ∥L =\n\nsup x,y∈X;x̸=y\n\ndY (f (x), f (y)) dX (x, y)\n\n.\n\nRewrite y as x + c, we get:\n\n∥f ∥L =\n\nsup x,x+r∈X;0<dX (x,x+c)\n\ndY (f (x), f (x + r)) dX (x, x + r)\n\n.\n\nMinimizing the above objective respect to function f reduces to the adversarial learning:\n\ninf f\n\n∥f ∥L = inf f\n\nsup x,x+c∈X;0<dX (x,x+c)\n\ndY (f (x), f (x + c)) dX (x, x + c)\n\n.\n\nIf we treat the denominator as a constant, then this is exactly the same as our minimax objective. More details can be found in (Terjék, 2019).\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n(a) clean example\n\n(b) badnet attack\n\n(c) blend attack\n\nFigure 2: Example of clean and various poisoned samples.badnet patch attack: trigger is a 3 × 3 black-white checkerboard and it is added to the right bottom corner of the image. blending attack: trigger is a fixed Gaussian noise which has the same dimension as the image. The corrupted image generated by xε\n\ni = (1 − α)xi + αt. In our experiment, we set the α as 0.1.\n\nA.9 SUPPLEMENTARY EXPERIMENT RESULTS\n\nWe provided the experiment hyperparameters, and supplementary results for the experiment. We provided the code in the supplementary materials.\n\nA.9.1 EXPERIMENT HYPERPARAMETERS\n\nWe list the details of experiment in this section. All the methods use Resnet-32 as the backbone network. AdamW is used as the optimizer for all methods. The perturbation limit τ is set to be 0.05 for all methods requiring τ . All methods are repeated for three different random seeds to calculate the standard deviation. For the SimCLR, we train the network by 500 epochs.\n\nThe trigger for badnet attack and blending attack can be found in figure 2.\n\nA.9.2 EXPERIMENT ON MNIST\n\nOur experiments showed interesting results on MNIST. In MNIST, we found adversarial training itself sometimes gives robustness to the backdoor attack. We hypothesize that this is because that learning from MNIST is potentially an easier task than that from CIFAR. Here, we show the performance of adversarial training and PRL-AT on MNIST. The results can be found at Table 8.\n\nDataset\n\nMNIST with Patch Attack, Poison Accuracy\n\nMNIST with Patch Attack, Clean Accuracy\n\nε 0.15 0.25 0.35 0.45\n\n0.15 0.25 0.35 0.45\n\nAT 0.30 ± 0.07 0.26 ± 0.17 0.10 ± 0.02 0.11 ± 0.01\n\nBootStrap 0.04 ± 0.01 0.04 ± 0.02 0.08 ± 0.06 0.04 ± 0.02\n\nBackdoor Attack Defense Accuracy. SPL PRL PRL-AT 98.44 ± 0.05 59.24 ± 33.30 85.61 ± 12.56 97.96 ± 0.21 30.70 ± 6.62 25.25 ± 1.38 89.91 ± 7.53 97.04 ± 1.07 26.03 ± 5.27 13.54 ± 0.76 77.91 ± 10.41 97.71 ± 0.18 10.97 ± 2.16 12.85 ± 1.90 43.42 ± 11.66 76.42 ± 8.65\n\nBootstrap-AT 3.17 ± 3.23 0.17 ± 0.10 0.14 ± 0.01 0.42 ± 0.34\n\nSPL-AT\n\n98.17 ± 0.69 98.59 ± 0.22 94.48 ± 4.87 98.27 ± 0.43\n\n99.49 ± 0.05 95.48 ± 1.56 99.48 ± 0.07 98.83 ± 0.19 99.48 ± 0.04 98.45 ± 0.32 99.42 ± 0.02 96.44 ± 1.36\n\n98.08 ± 0.26 97.46 ± 0.07 97.40 ± 0.46 75.69 ± 0.99\n\n98.44 ± 0.05 97.11 ± 1.06 97.86 ± 0.11 92.32 ± 4.25\n\nMNIST with Blend Attack, Poison Accuracy\n\nMNIST with Blend Attack, Clean Accuracy\n\n0.15 63.42 ± 35.24 0.25 70.43 ± 28.61 0.35 58.32 ± 40.59 97.66 ± 1.04 0.45\n\n0.04 ± 0.01 0.04 ± 0.01 0.05 ± 0.03 0.03 ± 0.03\n\n96.66 ± 2.58 96.74 ± 1.03 96.81 ± 1.30 97.83 ± 0.91 77.68 ± 20.34 97.20 ± 0.66 97.94 ± 0.57 78.79 ± 17.74 97.59 ± 0.12 98.16 ± 0.58 27.18 ± 19.53 95.17 ± 1.83\n\n0.15 64.78 ± 33.81 99.44 ± 0.02 98.29 ± 0.81 0.25 74.00 ± 25.25 99.46 ± 0.05 97.44 ± 0.99 0.35 58.62 ± 40.18 99.44 ± 0.02 97.43 ± 0.94 99.42 ± 0.06 97.89 ± 0.61 0.45\n\n96.78 ± 1.42\n\n97.93 ± 0.25 97.30 ± 0.63 96.25 ± 1.84 76.47 ± 8.66\n\n96.18 ± 1.44 97.10 ± 0.74 97.39 ± 0.19 95.07 ± 1.59\n\n93.23 ± 4.72 86.98 ± 0.72 73.09 ± 4.34 60.58 ± 1.90\n\n97.43 ± 0.13 6.43 ± 1.27 11.05 ± 2.70 4.49 ± 1.08\n\n97.30 ± 0.22 77.75 ± 0.74 72.41 ± 3.80 63.63 ± 4.47\n\n97.74 ± 0.37 85.89 ± 1.76 77.49 ± 0.96 57.20 ± 0.37\n\n96.16 ± 0.19 83.86 ± 2.74 69.69 ± 6.59 64.78 ± 3.14\n\n95.93 ± 0.20 83.34 ± 2.81 67.92 ± 8.38 63.82 ± 4.12\n\nTable 8: Performance on MNIST. ε is the corruption rate.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\n(a) clean data for binary feature value and continuous feature value\n\n(b) label flipping attack on both binary feature value and continuous feature value\n\nFigure 3: Example of label flipping attacks on both binary feature values and continuous feature values.\n\nAs seen for the MNIST, especially for the blend attack, the poison accuracy for adversarial training does show good performance with a large standard deviation. This is because that some random seeds work while some random seeds failed. We hypothesize that this is because MNIST dataset has almost binary feature values. When adding a small Gaussian noise on feature x, the label flipping attack cannot change the decision boundary much. That is why the noisy label algorithm seems is not as important as the noisy label algorithm in CIFAR dataset. We plot a two dimensional toy example in figure 3 to illustrate label flipping attack on continuous features and binary features. As seen in the figure, for the binary-valued features, the label flipping attack is not easy to change the decision boundary too much, while it can easily change the decision boundary in the continuous feature value scenario. However, this is a very rough conjecture for the reason why in MNIST, adversarial training sometimes works. We leave the investigation of this phenomenon in the future work.\n\n19",
    "reference": "# Summary Of The Paper\n\nThis paper attempts to introduce robust algorithms against label noise into backdoor defense. In particular, the authors propose a meta-algorithm that can transform an existing noisy label defense into one that defends against backdoor attacks. The experimental results demonstrate the effectiveness of the proposed method.\n\n# Strength And Weaknesses\n\n**Strength**\nThe topic is quite interesting. Since robust algorithms against label noise have been developed for a long while, the connection between label noise and backdoor attacks can help the development of the latter.\n\n**Weakness**\n1. I have concerns about the threat model in this paper. \n- If I understand correctly, the author randomly label the triggered data (images with trigger pattern) as any other labels (similar to uniform label noise). This is more related to label flipping attacks than backdoor attacks since this attack only results in performance degradation rather than target control, i.e., with only limited threat compared to commonly used backdoor attacks.\n- Since the author claim to “ leverage algorithms that defend against noisy label corruptions to defend against **general** backdoor attacks”, they are expected to conduct experiments on the commonly used setting, i.e., single-target attack.\n- The threat model in this paper requires a large poison ratio (>15%), which means the adversary has access to many training data. Is this realistic?\n\n2. This paper overclaims its contribution. While the threat model in this paper is an uncommon one (see discussion above), the majority of cited studies on backdoor attacks apply another threat model instead. This will mislead the readers to believe that this paper defends against the cited backdoor attacks. \n\n3. The technical novelty of this paper is limited. The proposed method seems to be a trivial combination of adversarial training and a robust algorithm against label noise.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper seems to overclaim its contribution because the threat model in this paper is quite different from the commonly used in other backdoor attacks. See Weakness 1 & 2.\n\n# Summary Of The Review\n\nThis paper studies an interesting question of whether we can introduce robust algorithms against label noise into backdoor defense. Unfortunately, the reviewer has concerns about the uncommonly used threat model.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nWHAT’S BEHIND THE MASK: ESTIMATING UNCERTAINTY IN IMAGE-TO-IMAGE PROBLEMS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nEstimating uncertainty in image-to-image networks is an important task, particularly as such networks are being increasingly deployed in the biological and medical imaging realms. In this paper, we introduce a new approach to this problem based on masking. Given an existing image-to-image network, our approach computes a mask such that the distance between the masked reconstructed image and the masked true image is guaranteed to be less than a specified threshold, with high probability. The mask thus identifies the more certain regions of the reconstructed image. Our approach is agnostic to the underlying image-to-image network, and only requires triples of the input (degraded), reconstructed and true images for training. Furthermore, our method is agnostic to the distance metric used. As a result, one can use Lp-style distances or perceptual distances like LPIPS, which contrasts with interval-based approaches to uncertainty. Our theoretical guarantees derive from a conformal calibration procedure. We evaluate our mask-based approach to uncertainty on image colorization, image completion, and super-resolution tasks, demonstrating high quality performance on each.\n\n1\n\nINTRODUCTION\n\nDeep Learning has been very successful in many applications, spanning computer vision, speech recognition, natural language processing, and beyond. For many years, researchers were mainly content to develop new techniques to achieve unprecedented accuracy, without concern for understanding the uncertainty implicit in such models. More recently, however, there has been a concerted effort within the research community to understand and quantify the uncertainty of deep models.\n\nThis paper addresses the problem of estimating uncertainty in the realm of image-to-image (sometimes referred to as image reconstruction) tasks. Such tasks include super-resolution, deblurring, colorization, and image completion, amongst others. Computing the uncertainty is important generally, but is particularly so in application domains such as biological and medical imaging, in which fidelity to the ground truth is paramount. If there is an area of the reconstructed image where such fidelity is unlikely or unreliable due to high uncertainty, this is crucial to convey.\n\nOur approach to uncertainty estimation is based on masking. Specifically, we are interested in the possibility of computing a mask such that the uncertain regions in the image are masked out. More formally, we would like the distance between the masked reconstructed image and the masked true image to be small in expectation. Ideally, the method should be agnostic to the choice of distance function, which should be dictated by the application. A high level overview of our approach is illustrated in Figure 1.\n\nWe show a direct connection between the mask and a theoretically well-founded definition of uncertainty that matches image-to-image tasks. We then derive an algorithm for computing such a mask which can apply to any existing (i.e. pre-trained) image-to-image network, and any distance function between image pairs. All that is required for training the mask network is triplets of the input (degraded), reconstructed and true images. Using a procedure based on conformal prediction (Angelopoulos & Bates, 2021a), we can guarantee that the masks so produced satisfy the following criterion: the distance between the masked reconstructed image and the masked true image is guaranteed to be less than a specified threshold, with high probability. We demonstrate the power of the method on image colorization, image completion, and super-resolution tasks.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nx\n\nColorization Model\n\nˆy\n\nM\n\na s\nk\n\ni\n\nn g\n\nM\n\no d\ne l\n\ny\n\nm\n\ny (cid:12) m\n\n(cid:12)\n\n(cid:12)\n\n=\n\n=\n\nm\n\nˆy\n\nm\n\nˆy (cid:12) m\n\nFigure 1: A high level view of our approach: x is the degraded input (colorization in this example) to the regression model. x and ˆy (the colorized image) serve as the input to the masking model which predicts the uncertainty mask m. The ground-truth image is denoted by y. With high probability, the distortion between the masked images, y (cid:12) m and ˆy (cid:12) m is low.\n\nOur contributions are as follows:\n\n• We present an approach to uncertainty computation based on masking. We show that moving from binary to continuous masks preserves the connection with a theoretically wellfounded definition of uncertainty that is relevant for image-to-image tasks.\n\n• We derive an algorithm for computing the masks which works for arbitrary image-to-image networks, and any distance function between image pairs. The masks are guaranteed to yield a small distance between masked ground truth and reconstructed images, with high probability.\n\n• We demonstrate the effectiveness of the method on image colorization, image completion,\n\nand super-resolution tasks attaining high quality performance on each.\n\nNOTATIONS\n\nThroughout the paper we represent an image as a column-stacked vector x ∈ Rn. We use xk to #» 0 ∈ Rn denote the kth image of a collection, while x(i) marks the ith entry of x. We define #» 1 ∈ Rn to be vectors of all zeros and ones respectively. The operation x (cid:12) y stands for and the Hadamard (point-wise) product between x and y. For p ≥ 1, we denote the (cid:96)p-norm of x by (cid:107)x(cid:107)p i=1 |x(i)|p. The symbol d(·, ·) represents a general distortion measure between two images. We define a continuous mask as a vector m ∈ [0, 1]n, where the average size of a mask #» equals (cid:13) 0 has size of (cid:13) 1. Given a mask m, we define dm(x, y) (cid:44) d(m (cid:12) x, m (cid:12) y) as the distortion measure between the masked versions of images x and y. For a natural number n ≥ 1 we define the set [n] (cid:44) {1, ..., n}.\n\n#» 1 (no-masking) has size 0 while the mask\n\n(cid:13)1/n such that the mask\n\n#» 1 − m(cid:13)\n\n(cid:44) (cid:80)n\n\np\n\n2 RELATED WORK\n\nBayesian Uncertainty Quantification The Bayesian paradigm defines uncertainty by assuming a distribution over the model parameters and/or activation functions. The most prevalent approach is Bayesian neural networks (MacKay, 1992; Valentin Jospin et al., 2020; Izmailov et al., 2020), which are stochastic models trained using Bayesian inference. Yet, as the number of model parameters has\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ngrown rapidly, computing the exact posteriors has became computationally intractable. This shortcoming has led to the development of approximation methods such as Monte Carlo dropout (Gal & Ghahramani, 2016; Gal et al., 2017a), stochastic gradient Markov chain Monte Carlo (Salimans et al., 2015; Chen et al., 2014), Laplacian approximations (Ritter et al., 2018) and variational inference (Blundell et al., 2015; Louizos & Welling, 2017; Posch et al., 2019). Alternative Bayesian techniques include deep Gaussian processes (Damianou & Lawrence, 2013), deep ensembles (Ashukha et al., 2020; Hu et al., 2019), and deep Bayesian active learning (Gal et al., 2017b), to name just a few. For a recent comprehensive review on Bayesian uncertainty quantification we refer the readers to Abdar et al. (2021).\n\nDistribution-Free Methods and Conformal Prediction Unlike Bayesian methods, the frequentist approach assumes the true model parameters are fixed with no underlying distribution. Examples of such distribution-free techniques are model ensembles (Lakshminarayanan et al., 2017; Pearce et al., 2018), bootstrap (Kim et al., 2020; Alaa & Van Der Schaar, 2020), interval regression (Pearce et al., 2018; Kivaranovic et al., 2020; Wu et al., 2021) and quantile regression (Gasthaus et al., 2019; Romano et al., 2019). An important distribution-free technique which is most relevant to our work is conformal prediction (Angelopoulos & Bates, 2021b; Shafer & Vovk, 2008). The conformal approach relies on a labeled calibration dataset to convert point estimations into prediction regions. Conformal methods can be used with any estimator, require no retraining, are computationally efficient and provide coverage guarantees in finite samples (Lei et al., 2018). Recent development includes conformalized quantile regression (Romano et al., 2019; Sesia & Cand`es, 2020; Angelopoulos et al., 2022b), conformal risk control (Angelopoulos et al., 2022a; Bates et al., 2021; Angelopoulos et al., 2021) and semantic uncertainty intervals for generative adversarial networks (Sankaranarayanan et al., 2022). Sun (2022) provides an extensive survey on distribution-free conformal prediction methods.\n\nUncertainty for Image-to-image Regression Tasks A work that stands out in this realm is (Angelopoulos et al., 2022b), which experiments with the conformal prediction technique. They evaluate and compare several models that address both the prediction of a value per each pixel, and an accompanying confidence interval that should contain the true value. After calibration, a guarantee is provided such that with high probability the true values of the pixels are indeed within these predicted confidence intervals. They find that quantile regression gives the best results among the four methods considered. As our work addresses the same breed of image-to-image problems, we provide extensive comparison to this work.\n\n3 MASK-BASED UNCERTAINTY ESTIMATION\n\n3.1 PROBLEM FORMULATION\n\nWe consider the problem of recovering an image y ∈ [0, 1]n from its observations x ∈ [0, 1]nx and denote by ˆy(x) ∈ [0, 1]n a given estimator of y. We define the uncertainty of ˆy given x as follow:\n\nU (ˆy|x) (cid:44) Ey|x[d(y, ˆy)],\n\n(1)\n\nwhere d(·, ·) denotes an arbitrary distortion measure between the ground truth image y and its estimation ˆy, and E is the expectation over the random variable y|x. Ideally, we would like to design ˆy such that\n\n(cid:16)\n\n(cid:17)\n\nP\n\nU (ˆy|x) ≤ α\n\n≥ β\n\n(2)\n\nwhere 0 ≤ α and 0 ≤ β ≤ 1 are user-defined, and the probability P (·) is computed over the joint probability of the problem pairs (y, x). However, in practice, condition (2) might not hold. Hence, our goal is to identify areas within ˆy which have the most adverse effect on the uncertainty and lead to the violation of (2). Formally, we aim to construct an uncertainty map given by a continuous mask m ∈ [0, 1]n such that\n\n(cid:16)\n\nP\n\nEy|x[dm(y, ˆy)] ≤ α\n\n(cid:17)\n\n≥ β\n\n(3)\n\nwhere dm(y, ˆy) (cid:44) d(m (cid:12) y, m (cid:12) ˆy). Notice we must avoid trivial and undesired solutions, e.g. m = 0, which satisfy (3) but provide no true information regarding the uncertainty. To that end, we\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nconsider solving the following problem\n\nmax m\n\n(cid:107)m(cid:107)1\n\nsubject to Ey|x[dm(y, ˆy)] ≤ α,\n\n#» 0 ≤ m ≤\n\n#» 1 .\n\n(P1)\n\nIn words, we wish to find the minimal masking required to satisfy the distortion constraint. Notice that Problem (P1) does not depend on β as we consider an instance regression problem for a given x and an originating y. We bring back the probabilistic condition using β in the next section, when addressing the whole image manifold.\n\nUnder certain conditions, the optimal solution is readily given in a closed-form expression as shown by the next theorem. Theorem 1. Consider Problem (P1) with the distortion measure d(y, ˆy) = (cid:107)y − ˆy(cid:107)p Then, for sufficiently small α, the optimal mask m∗ admits a closed-form solution given by\n\np where p > 1.\n\nwhere for all j ∈ [n]\n\nm∗\n\n(i) = α\n\n1 p\n\nq(i)\n\n(cid:0) (cid:80)n\n\nj=1 q(j)\n\n.\n\n(cid:1) 1\n\np\n\nq(j) (cid:44)\n\n1 (cid:0)Ey|x[|ˆy(j) − y(j)|p](cid:1)1/(p−1)\n\n.\n\nProof. See Appendix C.\n\nNote that the above implies the following:\n\nCorollary 1. The optimal mask, as defined by the solution of (P1), is directly related to the uncertainty measure in Equation (1) via\n\nU (ˆy|x) ∝\n\nn (cid:88)\n\ni=1\n\n(m∗\n\n(i))−(p−1).\n\nProof.\n\nn (cid:88)\n\ni=1\n\n(m∗\n\n(i))−(p−1) ∝\n\nn (cid:88)\n\n(q∗\n\n(i))−(p−1) =\n\ni=1\n\nn (cid:88)\n\ni=1\n\nEy|x[|ˆy(j) − y(j)|p] = U (ˆy|x).\n\nThus, the above establishes a correspondence between the optimal mask and uncertainty as defined in (1). Furthermore, it can be shown that the result of Theorem 1 can be extended to p = 1, leading to a binary mask (see Appendix A).\n\nThe result of Theorem 1 serves only as a general motivation in our work, since obtaining the optimal mask directly via the above equations is impractical for a number of reasons. First, α may vary depending on the user and the application. Second, for arbitrary distortion measure Problem (P1) does not admit a closed-form solution. Last and most importantly, at inference time the ground truth y is unknown. Hence, our next step is to provide an estimation for the mask which requires no knowledge of y, can be computed for any differentiable distortion measure, and is easily adapted for various values of α and β.\n\n3.2 METHOD\n\nHere, we aim to design an estimator for the the uncertainty mask defined by Problem (P1). We start by modeling the mask as a neural network mθ(x, ˆy)with parameters θ, which we refer to as the masking model (see details in Section 4.2). Similar to Problem (P1), we define the following optimization problem\n\nmin θ\n\n||mθ(x, ˆy) −\n\n#» 1 ||2\n\n2\n\nsubject to Ey|x[dmθ (y, ˆy)] ≤ α,\n\n#» 0 ≤ mθ(x, ˆy) ≤\n\n#» 1 .\n\n(P2)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nIgnoring (for now) the second constraint, the Lagrangian of Problem (P2) is given by\n\nL(θ, μ) = ||mθ(x, ˆy) −\n\n#» 1 ||2\n\n2 + μ\n\n(cid:16)\n\nEy|x[dmθ (y, ˆy)] − α\n\n(cid:17)\n\n(4)\n\nwhere μ > 0 is the dual variable which is considered as an hyperparameter. Given μ, the optimal mask can be found by minimizing L(θ, μ) with respect to θ, which is equivalent to minimizing\n\n||mθ(x, ˆy) −\n\n#» 1 ||2\n\n2 + μEy|x[dmθ (y, ˆy)]\n\n(5)\n\nsince α does not depend on θ. Thus, motivated by the above, we train our masking model using the following loss function:\n\n(cid:96)(D, θ) =\n\n(cid:88)\n\n||mθ(x, ˆy) −\n\n#» 1 ||2\n\n2 + μdmθ (y, ˆy)\n\n(6)\n\n(x,ˆy,y)∈D\n\nwhere D (cid:44) {(xk, ˆyk, yk)}k is a dataset of triplets of the degraded input, recovered output and the true image respectively. To enforce that the mask is within [0, 1] we simply clip the output of masking model. Note that the proposed approach facilitates the use of any differentiable distortion measure. Furthermore, at inference time, our masking model provides an approximation of the optimal uncertainty mask – the solution of (P1) – without requiring y.\n\nFollowing the above, notice that the loss function is independent of α and β, hence, we cannot guarantee that mθ satisfies condition (3). To overcome this, we follow previous work on conformal prediction and consider the output of our masking model as an initial estimation of the uncertainty which can be calibrated to obtain strong statistical guarantees. In general, the calibrated mask mλ follows the form\n\nmλ(i) (cid:44) F (mθ(i); λ),\n\n∀i = 1, ..., n,\n\n(7)\n\nwhere F (·; λ) is a monotonic non-decreasing function, parameterized by a global scalar which is tuned throughout the calibration process. As our goal is to reduce the distortion below a predefined fixed level, the calibrated mask need to be inversely proportional to the distortion. Since our predicted mask is proportional to the distortion we use the following formula1,\n\nmλ(i) =\n\nλ (cid:15) + 1 − mθ(i)\n\n∀i = 1, ..., n,\n\n(8)\n\nwhich was found empirically to perform well in our experiments. To set the value of λ we assume a given calibration dataset C = {(xk, ˆyk, yk)}k and perform the following calibration procedure:\n\n1. For each pair (xk, ˆyk) we predict a mask mk = mθ(xk, ˆyk).\n\n2. We define a calibrated mask mλk according to (8) where λk is the maximal value such that d(cid:0)mλk (cid:12) y, mλk (cid:12) ˆy(cid:1) ≤ α.\n\n3. Given all {λk}, we set λ to be the 1 − β quantile of {λk}, i.e. the maximal value for which\n\nat least β fraction of the calibration set satisfies condition (P1).\n\nThe final mask, calibrated according to α and β, is guaranteed to satisfy condition (3) above.\n\nThe benefits of our masking approach are as follows. First, it provides a measure of uncertainty which can be easily interpreted. In addition, our training process can accept any distortion measure and learns the relationships and correlations between different pixels in the image. Note thatour model is trained only once, irrespective of the values of α and β. Thus, our model can be easily adapted for different values of α and β via our simple calibration process. Finally, at inference, we can produce our uncertainty map without the knowledge of the ground truth while still providing strong statistical guarantees on our output mask.\n\n1We add a small value, (cid:15), to the denominator for numerical stability and clip the value of m to be in [0, 1] .\n\nA formal description of our algorithm can be seen in Appendix B.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\n4.1 DATASETS AND TASKS\n\nDatasets Two data-sets are used in our experiments:\n\nPlaces365 (Zhou et al., 2017): A large collection of 256x256 images from 365 scene categories. We use 1,803,460 images for training and 36,500 images for validation/test.\n\nRat Astrocyte Cells (Ljosa et al., 2012): A dataset of 1,200 uncompressed images of scanned rat cells of resolution 990×708. We crop the images into 256×256 tiles, and randomly split them into train and validation/test sets of sizes 373,744 and 11,621 respectively. The tiles are partially overlapped as we use stride of 32 pixels when cropping the images.\n\nTasks We consider the following image-to-image tasks:\n\nCompletion: Using grayscale version of Places365, we remove a middle vertical and horizontal stripe of 32 pixel width, and aim to reconstruct the missing part.\n\nSuper Resolution: We experiment with this task on the two data-sets. The images are scaled down to 64 × 64 images and the goal is to reconstruct the original image.\n\nColorization: We convert the Places365 images to grayscale and aim to recover their colors.\n\nFigure 4 shows a sample input and output for each of the above tasks.\n\n4.2 EXPERIMENTAL DETAILS AND SETTINGS\n\nImage-to-Image Models We start by training models for the above three tasks. Note that these models are not intended to be state of the art, but rather used to demonstrate the uncertainty estimation technique proposed in this work. We use the same model architecture for all tasks: an 8 layer U-Net. For each task we train two versions of the network: (i) A simple regressor; and (ii) A conditional GAN, where the generator plays the role of the reconstruction network. For the GAN, the discriminator is implemented as a 4 layer CNN. We use the L1 loss as the objective for the regressor, and augment it with an adversarial loss in the conditional GAN (λ = 20), as in Isola et al. (2017). All models are trained for ten epochs using the Adam optimizer with a learning rate equal to 1e-5 and a batch size equal to 50.\n\nMask Model For our masking model we also use an 8 layer U-Net architecture. This choice was made for simplicity and compatibility with previous work. The input for the mask model is the concatenation of the degraded image and the predicated image on the channel axis. It’s output is a mask having the same shape as the predicted image, with values in the range [0, 1]. The masking model is trained using the loss function described in Section 3.2 with μ = 2, learning rate of 1e-5 and a batch size of 25.\n\nExperiments We consider the L1, L2, SSIM and LPIPS distances. We set aside 1, 000 samples from each validation set for calibration and use the remaining samples for evaluation. We demonstrate the flexibility of our approach by conducting experiments on a variety of 12 settings: (i) Image Completion: {Regressor, GAN} × {L1, LPIPS}; (ii) Super Resolution: {Regressor, GAN} × {L1, SSIM}; and (iii) Colorization: {Regressor, GAN} × {L1, L2}.\n\nThresholds Recall that given a predicted image, our goal is to find a mask that, when applied to both the prediction and the (unknown) reference image, reduces the distortion between them to a predefined threshold with high probability β. Here we fix β = 0.9 and choose the threshold to be the 0.1-quantile of each measure computed on a random sample from the validation set, i.e. roughly 10% of the predictions are already considered sufficiently good and do not require masking at all.\n\n4.3 COMPETITOR TECHNIQUES\n\nQuantile – Interval-Based Technique We compare our method to the quantile regression option presented in (Angelopoulos et al., 2022b). As described in Section 2, their uncertainty is formulated by pixel-wise confidence intervals, calibrated so as to guarantee that with high probability the true value of the image is indeed within the specified range. While their predicted confidence intervals\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Example of uncertainty masks for image completion (top), colorization (middle) and super resolution (bottom): the images from left to right are the input to the model, the reference image, the output of the model, the calibrated uncertainty mask trained on the L1 loss and the L1 distance heat-map. In the image completion task the bottom left corner is richer in details and thus there is high uncertainty regarding this part in the reconstructed image. For the colorization task, the clothes and the colored area of the bus are the most uncertained regions respectively as these regions can be colorized with a large variaty of colors. In the super resolution task the uncertained regions are around the edges and text while smooth surfaces are more certain parts. Note that the actual error (right most image) is not equivalent to our definition of uncertainty and we only present it as a reference.\n\nare markedly different from the expected distortion we consider, we can use these intervals as a heuristic mask. For completeness, we also report the performance of the quantile regression even when it is less suitable, i.e. when the underlying model is a GAN and when the distortion loss is different from L1. We note again that for the sake of a fair comparison, our implementation of the masking model uses exactly the same architecture as the quantile regressor.\n\nOpt – Oracle We also compare our method with an oracle, denoted Opt, which computes an optimal mask using the following optimization procedure that has access to the reference image y:\n\n(cid:96)(m) = min\n\nm\n\n||m −\n\n#» 1 ||2\n\n2 + μdm(y, ˆy).\n\n(9)\n\nThis minimization is obtained via gradient descent, using the Adam optimizer with a step-size 0.01, iterating until the destination distortion value is attained. Clearly, this approach does not require calibration, as the above procedure ensures that all the masked samples are below the target distortion threshold.\n\n4.4 RESULTS AND DISCUSSION\n\nWe now show a series of results that demonstrate our proposed uncertainty masking approach, and its comparison with Opt and Quantile.2 We begin with a representative visual illustration of our proposed mask for several test cases, see Figure 2. As can be seen, the mask aligns well with the true prediction error, while still identifying sub-regions of high certainty.\n\nIn Figure 3 we explore the performance of the three compared methods with respect to (i) the probabilistic guarantee, (ii) the size of the masks obtained, and (iii) the correlation between mask sizes –\n\n2Due to space limitations, we show more extensive experimental results in Appendix B, while presenting a\n\nselected portion of them here.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:107)M(cid:107)\n\n(↓)\n\nNetwork Regression Regression GAN GAN\n\nDistance L1 LPIPS L1 LPIPS\n\nRegression Regression GAN GAN\n\nRegression Regression GAN GAN\n\nRegression Regression GAN GAN\n\nL1 SSIM L1 SSIM\n\nL1 SSIM L1 SSIM\n\nL1 L2 L1 L2\n\nOpt Ours Quantile 0.09 0.01 0.09 0.01\n\n0.10 0.01 0.09 0.01\n\n0.15 0.20 0.14 0.08\n\n(↑) C(M, D) Ours Quantile 0.89 0.54 0.95 0.31\n\n0.78 0.51 0.85 0.24\n\n(↑)\n\nC(M, Mopt) Ours 0.89 0.89 0.94 0.50\n\nQuantile 0.76 0.77 0.80 0.23\n\n0.99 0.66 0.94 0.79\n\nRat Astrocyte Cells 0.28 0.13 0.40 0.13 Places365 0.39 0.48 0.47 0.51\n\n0.99 0.89 0.97 0.86\n\n0.26 0.03 0.30 0.03\n\n0.36 0.23 0.38 0.12\n\n0.37 0.37 0.38 0.36\n\n0.40 0.38 0.40 0.38\n\n0.68 0.57 0.58 0.42\n\n0.24 0.03 0.26 0.03\n\n0.30 0.10 0.37 0.10\n\n0.27 0.18 0.27 0.18\n\n0.54 0.64 0.63 0.63\n\n0.97 0.85 0.81 0.81\n\n0.43 0.30 0.40 0.28\n\n0.95 0.82 0.80 0.83\n\n0.95 0.94 0.95 0.92\n\n0.57 0.60 0.60 0.59\n\n0.88 0.57 0.72 0.63\n\n0.94 0.84 0.67 0.86\n\n0.46 0.48 0.52 0.49\n\nTable 1: Image completion (top), super-resolution (middle) and colorization (bottom), results. (cid:107)M(cid:107) indicates average mask size; C(M, D) the correlation between predicted mask size and the amount of distortion; and C(M, Mopt) the correlation between the size of the calibrated mask and the size of the optimal mask. Arrows indicate which direction is better. Best results shown in blue. 95% confidence intervals were calculated for all the experiments and are omitted due to lack of space. All presented results were verified for their statistical significance.\n\nQuantile and Ours versus Opt. As can be seen from the top row, all three methods meet the distortion threshold with fewer than 10% exceptions, as required. Naturally, Opt does not have outliers since each mask is optimally calibrated by its computation. The spread of loss values tends to be higher with Quantile, indicating weaker performance. The colorization results, here and below, seem to be more challenging, with a smaller performance increase for our method.\n\nThe above probabilistic results are not surprising as they are the outcome of the calibration stage. The main question remaining relates to the size of the mask created so as to meet this threshold distortion. The middle row in Figure 3 presents histograms of these sizes, showing that our approach tends to produce masks that are close in size to those of Opt; while Quantile produces larger, and thus inferior, masked areas. Again, our colorization results show smaller if any improvement.\n\nThe bottom row in Figure 3 shows the correlation between mask sizes, and as can be observed, a high such correlation exists between our method and Opt, while Quantile produces less correlated and much heavier masks.\n\nQuantitative results are presented in Table 1 for the completion, super resolution, and colorization tasks. First and foremost, the results obtained by our method surpass those of Quantile across the board. Our method exhibits smaller mask size (cid:107)M(cid:107), aligned well with the masks obtained by Opt. Quantile, as expected, produces larger masks. In terms of the correlation between the obtained mask-size and the unmasked distortion value C(M, D), our method shows high such agreement, while Quantile lags behind. This correlation indicates a much desired adaptivity of the estimated mask to the complexity of image content and thus to the corresponding uncertainty. The correlation C(M, Mopt) between Opt’s mask size and the size of the mask obtained by our method or Quantile show similar behavior, aligned well with behavior we see in the bottom of Figure 3.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Summary of results for image completion (left), super-resolution (middle) and colorization (right). The top row shows the distribution of distortion values after masking versus the user-chosen threshold. The middle row presents histograms of mask sizes for the three compared methods. The bottom row describes the inter-relation between Opt’s mask size and the sizes obtained by Our method and Quantile’s.\n\n5 CONCLUSIONS\n\nUncertainty assessment in image-to-image regression problems is a challenging task, due to the implied complexity, the high dimensions involved, and the need to offer an effective and meaningful visualization of the estimated results. This work proposes a novel approach towards these challenges by constructing a continuous mask that highlights the trust-worthy regions in the estimated image. This mask provides a measure of uncertainty accompanied by an accuracy guarantee, stating that with high probability, the distance between the original and the estimated images over the nonmasked regions is below a user-specified value. The presented paradigm is flexible, being agnostic to the choice of distance function used and the regression method employed, while yielding masks of small area.\n\n9\n\nImage Completion Regression LPIPS0.020.040.060.080.100.120.140.160.180.20lossOptOurQuantileOptOurQuantileModelSuper Resolution GAN L10.0100.0150.0200.0250.0300.0350.0400.0450.0500.0550.060lossOptimalOurQuantileOptimalOurQuantileMethodColorization Regression L20.00000.00020.00040.00060.00080.00100.00120.00140.00160.0018lossOptOurQuantileOptOurQuantileModel0.000.050.100.150.200.250.300.350.400.45Mask SizeOptOurQuantileModelImage Completion Regression LPIPS0.000.040.080.120.160.200.24Mask SizeOptOurQuantileModelImage Completion GAN L10.00.10.20.30.40.50.60.70.80.91.0Mask SizeOptOurQuantileModelColorization Regression L20.0000.0020.0040.0060.0080.0100.0120.014Opt Size0.000.050.100.150.200.250.300.350.400.45Mask SizeOurQuantileModelImage Completion Regression LPIPS0.000.100.200.300.400.500.600.70Opt Size0.00.10.20.30.40.50.60.70.8Mask SizeOurQuantileModelSuper Resolution GAN L10.00.10.20.30.40.50.60.70.80.91.0Opt Size0.00.10.20.30.40.50.60.70.80.91.0Mask SizeOurQuantileModelColorization Regression L2Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMoloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information Fusion, 76:243–297, 2021.\n\nAhmed Alaa and Mihaela Van Der Schaar. Frequentist uncertainty in recurrent neural networks via blockwise influence functions. In International Conference on Machine Learning, pp. 175–190. PMLR, 2020.\n\nAnastasios N. Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. CoRR, abs/2107.07511, 2021a. URL https: //arxiv.org/abs/2107.07511.\n\nAnastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and\n\ndistribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021b.\n\nAnastasios N Angelopoulos, Stephen Bates, Emmanuel J Cand`es, Michael I Jordan, and Lihua Lei. Learn then test: Calibrating predictive algorithms to achieve risk control. arXiv preprint arXiv:2110.01052, 2021.\n\nAnastasios N Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Conformal\n\nrisk control. arXiv preprint arXiv:2208.02814, 2022a.\n\nAnastasios N Angelopoulos, Amit P Kohli, Stephen Bates, Michael I Jordan, Jitendra MaImage-to-image regression arXiv preprint\n\nlik, Thayer Alshaabi, Srigokul Upadhyayula, and Yaniv Romano. with distribution-free uncertainty quantification and applications in imaging. arXiv:2202.05265, 2022b.\n\nArsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. arXiv preprint arXiv:2002.06470, 2020.\n\nStephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael Jordan. Distribution-free, risk-controlling prediction sets. Journal of the ACM (JACM), 68(6):1–34, 2021.\n\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International conference on machine learning, pp. 1613–1622. PMLR, 2015.\n\nTianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo.\n\nIn\n\nInternational conference on machine learning, pp. 1683–1691. PMLR, 2014.\n\nAndreas Damianou and Neil D Lawrence. Deep gaussian processes. In Artificial intelligence and\n\nstatistics, pp. 207–215. PMLR, 2013.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059. PMLR, 2016.\n\nYarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. Advances in neural information\n\nprocessing systems, 30, 2017a.\n\nYarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data.\n\nIn International Conference on Machine Learning, pp. 1183–1192. PMLR, 2017b.\n\nJan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram, David Salinas, Valentin Flunkert, and Tim Januschowski. Probabilistic forecasting with spline quantile function rnns. In The 22nd international conference on artificial intelligence and statistics, pp. 1901–1910. PMLR, 2019.\n\nRuihan Hu, Qijun Huang, Sheng Chang, Hao Wang, and Jin He. The MBPEP: a deep ensemble pruning algorithm providing high quality uncertainty prediction. Applied Intelligence, 49(8): 2942–2955, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.\n\nImage-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1125–1134, 2017.\n\nPavel Izmailov, Wesley J Maddox, Polina Kirichenko, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Subspace inference for Bayesian deep learning. In Uncertainty in Artificial Intelligence, pp. 1169–1179. PMLR, 2020.\n\nByol Kim, Chen Xu, and Rina Barber. Predictive inference is free with the jackknife+-after-\n\nbootstrap. Advances in Neural Information Processing Systems, 33:4138–4149, 2020.\n\nDanijel Kivaranovic, Kory D Johnson, and Hannes Leeb. Adaptive, distribution-free prediction intervals for deep networks. In International Conference on Artificial Intelligence and Statistics, pp. 4346–4356. PMLR, 2020.\n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.\n\nJing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. DistributionJournal of the American Statistical Association, 113\n\nfree predictive inference for regression. (523):1094–1111, 2018.\n\nVebjorn Ljosa, Katherine L Sokolnicki, and Anne E Carpenter. Annotated high-throughput mi-\n\ncroscopy image sets for validation. Nature methods, 9(7):637–637, 2012.\n\nChristos Louizos and Max Welling. Multiplicative normalizing flows for variational bayesian neural\n\nnetworks. In International Conference on Machine Learning, pp. 2218–2227. PMLR, 2017.\n\nDavid JC MacKay. Bayesian interpolation. Neural computation, 4(3):415–447, 1992.\n\nTim Pearce, Alexandra Brintrup, Mohamed Zaki, and Andy Neely. High-quality prediction intervals for deep learning: A distribution-free, ensembled approach. In International conference on machine learning, pp. 4075–4084. PMLR, 2018.\n\nKonstantin Posch, Jan Steinbrener, and J ̈urgen Pilz. Variational inference to measure model uncer-\n\ntainty in deep neural networks. arXiv preprint arXiv:1902.10189, 2019.\n\nHippolyt Ritter, Aleksandar Botev, and David Barber. A scalable Laplace approximation for neural networks. In 6th International Conference on Learning Representations, ICLR 2018-Conference Track Proceedings, volume 6. International Conference on Representation Learning, 2018.\n\nYaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression.\n\nAdvances in neural information processing systems, 32, 2019.\n\nTim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational inference: Bridging the gap. In International conference on machine learning, pp. 1218–1226. PMLR, 2015.\n\nSwami Sankaranarayanan, Anastasios N Angelopoulos, Stephen Bates, Yaniv Romano, and arXiv preprint\n\nPhillip Isola. Semantic uncertainty intervals for disentangled latent spaces. arXiv:2207.10074, 2022.\n\nMatteo Sesia and Emmanuel J Cand`es. A comparison of some conformal quantile regression meth-\n\nods. Stat, 9(1):e261, 2020.\n\nGlenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning\n\nResearch, 9(3), 2008.\n\nSophia Sun. Conformal methods for quantifying uncertainty in spatiotemporal data: A survey. arXiv\n\npreprint arXiv:2209.03580, 2022.\n\nLaurent Valentin Jospin, Wray Buntine, Farid Boussaid, Hamid Laga, and Mohammed Bennamoun. Hands-on Bayesian neural networks–a tutorial for deep learning users. arXiv e-prints, pp. arXiv– 2007, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDongxia Wu, Liyao Gao, Xinyue Xiong, Matteo Chinazzi, Alessandro Vespignani, Yi-An Ma, arXiv preprint\n\nand Rose Yu. Quantifying uncertainty in deep spatiotemporal forecasting. arXiv:2105.11982, 2021.\n\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA TASK ILLUSTRATIONS\n\nFigure 4: The three tasks we experimented with: 1) Image Colorization on the left column 2) Gray-scale Image Completion on the middle column, and 3) Super Resolution on the right column.\n\nB CALIBRATION ALGORITHM\n\nAlgorithm 1 Finding λi\n\ndef scale(lambda_, mask, eps=1e-6):\n\nreturn (mask * lambda_ / (eps + mask * (1 - mask))).clip(0, 1)\n\ndef find_lambda(mask, y, y_hat, eps=1e-4):\n\nl, h, lambda_ = eps, 1 / eps, 0\n\nwhile h - l > eps:\n\nlambda_ = (h + l) / 2\n\nloss = loss_fn(\n\ny * scale(a, mask), y_hat * scale(a, mask))\n\nif loss > alpha:\n\nh = lambda_\n\nelse:\n\nl = lambda_\n\nreturn lambda_\n\nC PROOF OF THEOREM 1\n\nRecall that we have defined the following optimization problem as defining the optimal mask:\n\nmax m\n\n(cid:107)m(cid:107)1\n\nsubject to Ey|x[dm(y, ˆy)] ≤ α,\n\n#» 0 ≤ m ≤\n\n#» 1 .\n\n(P1)\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem 1: Consider Problem (P1) with the distortion measure d(y, ˆy) = (cid:107)y − ˆy(cid:107)p Then, for sufficiently small α, the optimal mask m∗ admits a closed-form solution given by\n\np where p > 1.\n\nwhere for all j ∈ [n]\n\nm∗\n\n(i) = α\n\n1 p\n\nq(i)\n\n(cid:0) (cid:80)n\n\nj=1 q(j)\n\n.\n\n(cid:1) 1\n\np\n\nq(j) (cid:44)\n\n1 (cid:0)Ey|x[|ˆy(j) − y(j)|p](cid:1)1/(p−1)\n\n.\n\nProof. We start with reformulating Problem (P1) as follows:\n\nmin m\n\n−(cid:107)m(cid:107)1\n\nsubject to Ey|x[dm(y, ˆy)] ≤ α,\n\nm(i) ∈ [0, 1].\n\nNext, we define the Lagrangian of the above problem,\n\nL(m, μ) (cid:44) −(cid:107)m(cid:107)1 + μ\n\n(cid:16)\n\nEy|x[(cid:107)m (cid:12) (y − ˆy)(cid:107)p\n\np] − α\n\n(cid:17)\n\n,\n\nwhere μ > 0 is the dual variable, and we invoke the chosen metric d(y, ˆy) = (cid:107)y − ˆy(cid:107)p p for p > 1. According to the Karush–Kuhn–Tucker (KKT) conditions, the optimal solution (m∗, μ∗) must satisfy\n\n= 0,\n\n∀i = 1, ..., n,\n\n∂L ∂m(i) ∂L ∂μ\n\n= 0.\n\nFurthermore, notice that the Lagrangian is separable with respect to the coordinates of m, thus, we can write\n\n∂L ∂m(i)\n\n=\n\n=\n\n=\n\n∂ ∂m(i) ∂\n∂m(i) ∂\n∂m(i)\n\n(cid:16)\n\n− |m(i)| + μ(cid:0)Ey|x[|m(i) · (y(i) − ˆy(i))|p] − α(cid:1)(cid:17)\n\n(cid:16)\n\n(cid:16)\n\n− |m(i)| + μ|m(i)|p · Ey|x[|y(i) − ˆy(i)|p]\n\n(cid:17)\n\n− m(i) + μmp\n\n(i) · Ey|x[|y(i) − ˆy(i)|p]\n\n(cid:17)\n\n= −1 + μpEy|x[|y(i) − ˆy(i)|p] · mp−1\n\n(i) = 0.\n\nIn the above we assume that m(i) ≥ 0, a fact that should be verified once a solution is formed. Hence, we obtain\n\n(cid:16)\n\nmp−1\n\n(i) =\n\nμpEy|x[|y(i) − ˆy(i)|p]\n\n(cid:17)−1\n\n.\n\nTo simplify our derivation, we define\n\nd(i) (cid:44) Ey|x[|y(i) − ˆy(i)|p] q(i) (cid:44) d−1/(p−1)\n\n.\n\n(i)\n\nThus, we can rewrite the above result equivalently as\n\nmp−1\n\n(i) = (μpd(i))−1 or m(i) = (μp)−1/(p−1) · q(i).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nNow, consider the second KKT condition:\n\n∂L ∂μ\n\n= Ey|x[(cid:107)m (cid:12) (y − ˆy)(cid:107)p\n\np] − α\n\n=\n\n=\n\n=\n\nn (cid:88)\n\nj=1\n\nn (cid:88)\n\nj=1\n\nn (cid:88)\n\nj=1\n\nEy|x[|m(j) · (y(j) − ˆy(j))|p] − α\n\nmp\n\n(j)d(j) − α\n\nm(j)(μpd(j))−1d(j) − α\n\n= (μp)−1\n\nn (cid:88)\n\nj=1\n\nm(j) − α\n\n= (μp)−1(μp)−1/(p−1)\n\nn (cid:88)\n\nj=1\n\nq(j) − α = 0.\n\n⇒ μp =\n\n(cid:18)\n\nα\n\n(cid:80)n\n\nj=1 q(j)\n\n(cid:19)−(p−1)/p\n\n.\n\nTherefore, the optimal solution m∗ is given by\n\nm∗\n\n(i) = α\n\n1 p\n\nq(i)\n\n(cid:0) (cid:80)n\n\nj=1 q(j)\n\n.\n\n(cid:1) 1\n\np\n\nNote that as assumed, the obtained solution m∗ small such that\n\n(i) is non-negative. Finally, assuming α is sufficiently\n\n1\n\nα\n\np ≤ min\n\ni\n\n(cid:32)\n\nq(i)\n\n(cid:0) (cid:80)n\n\nj=1 q(j)\n\n(cid:1) 1\n\np\n\n(cid:33)−1\n\n,\n\nthe solution satisfies the second constraint, m∗\n\n(i) ≤ 1, which completes the proof.\n\nWe now turn to present a closely related result, referring to the case d(y, ˆy) = (cid:107)y − ˆy(cid:107)1. Theorem 2: Consider Problem (P1) with the distortion measure d(y, ˆy) = (cid:107)y − ˆy(cid:107)1. Then the optimal mask m∗ admits a closed-form solution given by\n\nm(i) =\n\n(cid:26)1, d(i) < d∗ 0, otherwise.\n\nwhere d(i) (cid:44) Ey|x[|y(i) − ˆy(i)|] and d∗ is defined by\n\nd(i) · {d(i) ≤ d∗} = α.\n\n(cid:88)\n\ni\n\nProof. Consider the Lagrangian of the above problem when d(y, ˆy) = (cid:107)y − ˆy(cid:107)1:\n\nL(m, μ) (cid:44) −(cid:107)m(cid:107)1 + μ\n\n(cid:16)\n\nEy|x[(cid:107)m (cid:12) (y − ˆy)(cid:107)1] − α\n\n(cid:17)\n\n,\n\nwhere μ > 0 is the dual variable. As before, the problem is separable with respect to the coordinates of m, leading to the following one-dimensional problem\n\nmin 0≤m(i)≤1\n\n−m(i) + μm(i)d(i) = min\n\n0≤m(i)≤1\n\n(cid:16)\n\nm(i)\n\nμd(i) − 1\n\n(cid:17) ,\n\nwhere d(i) (cid:44) Ey|x[|y(i) − ˆy(i)|]. Thus,\n\nm(i) =\n\n(cid:40)\n\n1, d(i) < 1 0, otherwise.\n\nμ\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nObserve that this result is intuitive in retrospect, as smaller distances d(i) correspond to un-masked areas. In addition, regardless of μ, the choice of the mask value is dictated by the distance, a fact that we leverage hereafter. Recall that the solution should satisfy\n\nn (cid:88)\n\ni\n\nm(i)d(i) = α,\n\nwhere m(i) are 1-es for the smaller values of d(i). Thus,\n\n(cid:88)\n\ni\n\nm(i)d(i) =\n\n(cid:88)\n\nd(i) = α.\n\ni:d(i)<d∗\n\nHence, the above provides a definition for the threshold value d∗, and μ = 1/d∗, which completes the proof.\n\nD MORE RESULTS\n\nIn this appendix we bring the results corresponding to all 12 settings explored in our work, corresponding to three inverse problems, two regression techniques and two metrics per each, along the following breakdown:\n\n• Image Completion: {Regressor, GAN} × {L1, LPIPS};\n\n• Super Resolution: {Regressor, GAN} × {L1, SSIM}; and\n\n• Colorization: {Regressor, GAN} × {L1, L2}.\n\nWe start in Figures 5-7 with the obtained distributions of masked distortion values for Opt, Ours and Quantile. The goal here is to show that all three methods meet the required distortion threshold with exceptions that do not surpass the destination probability β = 0.1.\n\nFigures 8-10 present histograms of the obtained mask sizes for the three methods. The goal is to get minimal areas in these masks, so as to keep most of the image content unmasked.\n\nFigures 11-13 conclude these results by bringing graphs showing the inter-relation between the Opt mask size and the ones given by Ours and Quantile.\n\nAnalysis and conclusions drawn from these graphs are brought in the experimental part of the paper.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Image Completion - The distribution of the masked distortion values versus the chosen threshold (shown as a horizontal dashed line) for the three tested methods - Opt, Ours and Quantile.\n\n17\n\nImage Completion GAN L10.0040.0060.0080.0100.0120.0140.0160.0180.0200.0220.0240.026lossOptOurQuantileOptOurQuantileModelImage Completion GAN LPIPS0.040.060.080.100.120.140.160.180.20lossOptOurQuantileOptOurQuantileModelImage Completion Regression L10.0020.0040.0060.0080.0100.0120.0140.0160.0180.020lossOptOurQuantileOptOurQuantileModelImage Completion Regression LPIPS0.020.040.060.080.100.120.140.160.180.20lossOptOurQuantileOptOurQuantileModelUnder review as a conference paper at ICLR 2023\n\nFigure 6: Super Resolution - The distribution of the masked distortion values versus the chosen threshold (shown as a horizontal dashed line) for the three tested methods - Opt, Ours and Quantile.\n\n18\n\nSuper Resolution GAN L10.000.010.020.030.040.050.06lossOptOurQuantileOptOurQuantileModelSuper Resolution GAN SSIM0.000.050.100.150.200.250.300.350.400.450.50lossOptOurQuantileOptOurQuantileModelSuper Resolution Regression L10.0000.0050.0100.0150.0200.0250.0300.0350.0400.045lossOptOurQuantileOptOurQuantileModelSuper Resolution Regression SSIM0.000.050.100.150.200.250.300.350.400.45lossOptOurQuantileOptOurQuantileModelUnder review as a conference paper at ICLR 2023\n\nFigure 7: Colorization - The distribution of the masked distortion values versus the chosen threshold (shown as a horizontal dashed line) for the three tested methods - Opt, Ours and Quantile.\n\n19\n\nColorization GAN L10.0000.0050.0100.0150.0200.025lossOptOurQuantileOptOurQuantileModelColorization GAN L20.00000.00020.00040.00060.00080.00100.00120.00140.00160.00180.0020lossOptOurQuantileOptOurQuantileModelColorization Regression L10.0000.0050.0100.0150.0200.0250.030lossOptOurQuantileOptOurQuantileModelColorization Regression L20.00000.00020.00040.00060.00080.00100.00120.00140.00160.0018lossOptOurQuantileOptOurQuantileModelUnder review as a conference paper at ICLR 2023\n\nFigure 8: Image Completion - Histograms of the calibrated mask sizes for the three tested methods - Opt, Ours and Quantile.\n\n20\n\n0.000.040.080.120.160.200.24Mask SizeOptOurQuantileModelImage Completion GAN L10.000.020.040.060.080.100.120.140.160.18Mask SizeOptOurQuantileModelImage Completion GAN LPIPS0.000.040.080.120.160.200.240.28Mask SizeOptOurQuantileModelImage Completion Regression L10.000.050.100.150.200.250.300.350.400.45Mask SizeOptOurQuantileModelImage Completion Regression LPIPSUnder review as a conference paper at ICLR 2023\n\nFigure 9: Super Resolution - Histograms of the calibrated mask sizes for the three tested methods - Opt, Ours and Quantile.\n\n21\n\n0.00.10.20.30.40.50.60.70.8Mask SizeOptOurQuantileModelSuper Resolution GAN L10.000.100.200.300.400.500.600.70Mask SizeOptOurQuantileModelSuper Resolution GAN SSIM0.00.10.20.30.40.50.60.70.8Mask SizeOptOurQuantileModelSuper Resolution Regression L10.000.100.200.300.400.500.600.70Mask SizeOptOurQuantileModelSuper Resolution Regression SSIMUnder review as a conference paper at ICLR 2023\n\nFigure 10: Colorization - Histograms of the calibrated mask sizes for the three tested methods - Opt, Ours and Quantile.\n\n22\n\n0.00.10.20.30.40.50.60.70.80.91.0Mask SizeOptOurQuantileModelColorization GAN L10.00.10.20.30.40.50.60.70.80.91.0Mask SizeOptOurQuantileModelColorization GAN L20.00.10.20.30.40.50.60.70.80.91.0Mask SizeOptOurQuantileModelColorization Regression L10.00.10.20.30.40.50.60.70.80.91.0Mask SizeOptOurQuantileModelColorization Regression L2Under review as a conference paper at ICLR 2023\n\nFigure 11: Image Completion - The correlation between the Ours and Quantile mask sizes versus Opt’s mask size.\n\nFigure 12: Super Resolution - The correlation between the Ours and Quantile mask sizes versus Opt’s mask size.\n\n23\n\n0.000.020.040.060.080.100.120.140.160.180.20Opt Size0.000.050.100.150.200.25Mask SizeOurQuantileModelImage Completion GAN L10.0000.0020.0040.0060.0080.0100.012Opt Size0.000.020.040.060.080.100.120.140.160.18Mask SizeOurQuantileModelImage Completion GAN LPIPS0.000.020.040.060.080.100.120.140.160.180.20Opt Size0.000.050.100.150.200.25Mask SizeOurQuantileModelImage Completion Regression L10.0000.0020.0040.0060.0080.0100.0120.014Opt Size0.000.050.100.150.200.250.300.350.400.45Mask SizeOurQuantileModelImage Completion Regression LPIPS0.000.100.200.300.400.500.600.70Opt Size0.00.10.20.30.40.50.60.70.8Mask SizeOurQuantileModelSuper Resolution GAN L10.0000.0050.0100.0150.0200.0250.0300.0350.0400.045Opt Size0.00.10.20.30.40.50.60.7Mask SizeOurQuantileModelSuper Resolution GAN SSIM0.000.050.100.150.200.250.300.350.400.450.500.55Opt Size0.00.10.20.30.40.50.60.70.8Mask SizeOurQuantileModelSuper Resolution Regression L10.0000.0040.0080.0120.0160.0200.024Opt Size0.00.10.20.30.40.50.60.7Mask SizeOurQuantileModelSuper Resolution Regression SSIMUnder review as a conference paper at ICLR 2023\n\nFigure 13: Colorization - The correlation between the Ours and Quantile mask sizes versus Opt’s mask size.\n\n24\n\n0.00.10.20.30.40.50.60.70.80.91.0Opt Size0.00.10.20.30.40.50.60.70.80.91.0Mask SizeOurQuantileModelColorization GAN L10.00.10.20.30.40.50.60.70.80.91.0Opt Size0.00.10.20.30.40.50.60.70.80.91.0Mask SizeOurQuantileModelColorization GAN L20.00.10.20.30.40.50.60.70.80.91.0Opt Size0.00.10.20.30.40.50.60.70.80.91.0Mask SizeOurQuantileModelColorization Regression L10.00.10.20.30.40.50.60.70.80.91.0Opt Size0.00.10.20.30.40.50.60.70.80.91.0Mask SizeOurQuantileModelColorization Regression L2",
    "reference": "# Summary Of The Paper\n\nThis paper proposed a new approach for uncertainty estimation in image reconstruction, which uses masking mechanism to identify the more certain regions of the reconstructed image, thus narrowing the distance between masked ground truth and reconstructed images. The experiments are conducted on three reconstruction tasks: image colorization, image completion and super-resolution.\n\n# Strength And Weaknesses\n\nStrength:\n1. The proposed method introduced a continuous mask to replace the previous binary mask for better modeling the uncertainty between the ground truth image and the reconstructed image.\n2. The conformal prediction strategy is employed to calibrate the masking model to obtain strong statistical guarantees. \n3. The paper is good writing and easy to follow.\n\nWeakness:\n1. The generation way of the continuous mask m is not clear, is it the same as the prevalent implicit neural representation? And the ablation study compared to the binary mask should be provided. \n2. The visualization results for super-resolution are not included in the paper.\n3. It seems that Figure 2 is dispensable, as it doesn’t provide more meaningful information to understand the experiment setups.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity and quality of this paper seem good, the proposed idea is somewhat interesting, and the reproducibility is reasonable.\n\n# Summary Of The Review\n\nThe paper studied the problem of uncertainty estimation in image-to-image domain. The idea seems somewhat interesting, but the effectiveness is not well verified. I think this paper meets the margins of acceptance.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nLEARNING SPARSE AND LOW-RANK PRIORS FOR IMAGE RECOVERY VIA ITERATIVE REWEIGHTED LEAST SQUARES MINIMIZATION\n\nStamatios Lefkimmiatis, Iaroslav Koshelev Huawei Noah’s Ark Lab {stamatios.lefkimmiatis, koshelev.iaroslav}@huawei.com\n\nABSTRACT\n\np-vector and S p\n\nWe introduce a novel optimization algorithm for image recovery under learned sparse and low-rank constraints, which we parameterize as weighted extensions of the lp p Schatten-matrix quasi-norms for 0 < p ≤ 1, respectively. Our proposed algorithm generalizes the Iteratively Reweighted Least Squares (IRLS) method, used for signal recovery under l1 and nuclear-norm constrained minimization. Further, we interpret our overall minimization approach as a recurrent network that we then employ to deal with inverse low-level computer vision problems. Thanks to the convergence guarantees that our IRLS strategy offers, we are able to train the derived reconstruction networks using a memory-efficient implicit back-propagation scheme, which does not pose any restrictions on their effective depth. To assess our networks’ performance, we compare them against other existing reconstruction methods on several inverse problems, namely image deblurring, super-resolution, demosaicking and sparse recovery. Our reconstruction results are shown to be very competitive and in many cases outperform those of existing unrolled networks, whose number of parameters is orders of magnitude higher than that of our learned models. The code is available at this link.\n\n1\n\nINTRODUCTION\n\nWith the advent of modern imaging techniques, we are witnessing a significant rise of interest in inverse problems, which appear increasingly in a host of applications ranging from microscopy and medical imaging to digital photography, 2D&3D computer vision, and astronomy (Bertero & Boccacci, 1998). An inverse imaging problem amounts to estimating a latent image from a set of possibly incomplete and distorted indirect measurements. In practice, such problems are typical illposed (Tikhonov, 1963; Vogel, 2002), which implies that the equations relating the underlying image with the measurements (image formation model) are not adequate by themselves to uniquely characterize the solution. Therefore, in order to recover approximate solutions, which are meaningful in a statistical or physical sense, from the set of solutions that are consistent with the image formation model, it is imperative to exploit prior knowledge about certain properties of the underlying image.\n\nAmong the key approaches for solving ill-posed inverse problems are variational methods (Benning & Burger, 2018), which entail the minimization of an objective function. A crucial part of such an objective function is the regularization term, whose role is to promote those solutions that fit best our prior knowledge about the latent image. Variational methods have also direct links to Bayesian methods and can be interpreted as seeking the penalized maximum likelihood or the maximum a posteriori (MAP) estimator (Figueiredo et al., 2007), with the regularizer matching the negative log-prior. Due to the great impact of the regularizer in the reconsturction quality, significant research effort has been put in the design of suitable priors. Among the overwhelming number of existing priors in the literature, sparsity and low-rank (spectral-domain sparsity) promoting priors have received considerable attention. This is mainly due to their solid mathematical foundation and the competitive results they achieve (Bruckstein et al., 2009; Mairal et al., 2014).\n\nNowdays, thanks to the advancements of deep learning there is a plethora of networks dedicated to image reconstruction problems, which significantly outperform conventional approaches. Nevertheless, they are mostly specialized and applicable to a single task. Further, they are difficult to analyze and interpret since they do not explicitly model any of the well-studied image properties,\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nsuccessfully utilized in the past (Monga et al., 2021). In this work, we aim to harness the power of supervised learning but at the same time rely on the rich body of modeling and algorithmic ideas that have been developed in the past for dealing with inverse problems. To this end our contributions are: (1) We introduce a generalization of the Iterative Reweighted Least Squares (IRLS) method based on novel tight upper-bounds that we derive. (2) We design a recurrent network architecture that explicitly models sparsity-promoting image priors and is applicable to a wide range of reconstruction problems. (3) We propose a memory efficient training strategy based on implicit back-propagation that does not restrict in any way our network’s effective depth.\n\nIMAGE RECONSTRUCTION\n\n2 Let us first focus on how one typically deals with inverse imaging problems of the form:\n\nis a linear operator that models the impulse response of the sensing device, y ∈ Rm·c′\n\ny = Ax + n, (1) where x ∈ Rn·c is the multichannel latent image of c channels, that we seek to recover, A : Rn·c → Rm·c′ is the observation vector, and n ∈ Rm·c′ is a noise vector that models all approximation errors of the forward model and measurement noise. Hereafter, we will assume that n consists of i.i.d samples drawn from a Gaussian distribution of zero mean and variance σ2 n. Note that despite of the seeming simplicity of this observation model, it is widely used in the literature, since it can accurately enough describe a plethora of practical problems. Specifically, by varying the form of A, Eq. (1) can cover many different inverse imaging problems such as denoising, deblurring, demosaicking, inpainting, super-resolution, MRI reconstruction, etc. If we further define the objective function:\n\nJ (x) = 1 2σ2 n\n\n(2) where R : Rn·c → R+ = {x ∈ R|x ≥ 0} is the regularizer (image prior), we can recover an estimate of the latent image x∗ as the minimizer of the optimization problem: x∗ = arg minx J (x). Since the type of the regularizer R (x) can significantly affect the reconstruction quality, it is of the utmost importance to employ a proper regularizer for the reconstruction task at hand.\n\n2 + R (x) ,\n\n∥y − Ax∥2\n\n2.1 SPARSE AND LOW-RANK IMAGE PRIORS\n\nMost of the existing image regularizers in the literature can be written in the generic form:\n\nR (x) =\n\nl (cid:88)\n\ni=1\n\nφ (Gix) ,\n\n(3)\n\nwhere G : Rn·c → Rl·d is the regularization operator that transforms x, Gi = MiG, Mi = Id ⊗eT with ⊗ denoting the Kronecker product, and ei is the unit vector of the standard Rl basis. Further, φ : Rd → R+ is a potential function that penalizes the response of the d-dimensional transformdomain feature vector, zi = Gix ∈ Rd. Among such regularizers, widely used are those that promote sparse and low-rank responses by utilizing as their potential functions the l1 and nuclear norms (Rudin et al., 1992; Figueiredo et al., 2007; Lefkimmiatis et al., 2013; 2015). Enforcing sparsity of the solution in some transform-domain has been studied in-depth and is supported both by solid mathematical theory (Donoho, 2006; Candes & Wakin, 2008; Elad, 2010) as well as strong empirical results, which indicate that distorted images do not typically exhibit sparse or low-rank representations, as opposed to their clean counterparts. More recently it has also been advocated that non-convex penalties such as the lp p Schatten-matrix quasi-norms enforce sparsity better and lead to improved image reconstruction results (Chartrand, 2007; Lai et al., 2013; Candes et al., 2008; Gu et al., 2014; Liu et al., 2014; Xie et al., 2016; K ̈ummerle & Verdun, 2021).\n\np vector and S p\n\ni\n\nBased on the above, we consider two expressive parametric forms for the potential function φ (·), p and the Schatten matrix S p which correspond to weighted and smooth extensions of the lp p quasinorms with 0 < p ≤ 1, respectively. The first one is a sparsity-promoting penalty, defined as:\n\nφsp (z; w, p) =\n\nd (cid:88)\n\nj=1\n\nwj\n\n(cid:0)z2\n\nj + γ(cid:1) p\n\n2 , z, w ∈ Rd,\n\nwhile the second one is a low-rank (spectral-domain sparsity) promoting penalty, defined as:\n\nφlr (Z; w, p) =\n\nr (cid:88)\n\nj=1\n\nwj\n\n(cid:0)σ2\n\nj (Z) + γ(cid:1) p\n\n2 , Z ∈ Rm×n, w ∈ Rr\n\n+, with r = min (m, n) .\n\n(4)\n\n(5)\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nIn both definitions γ is a small fixed constant that ensures the smoothness of the penalty functions. Moreover, in Eq. (5) σ (Z) denotes the vector with the singular values of Z sorted in decreasing order, while the weights w are sorted in increasing order. The reason for the latter order is that to better promote low-rank solutions we need to penalize more the smaller singular values of the matrix than its larger ones. Next, we define our proposed sparse and low-rank promoting image priors as:\n\nRsp (x) =\n\nl (cid:88)\n\ni=1\n\nφsp (zi; wi, p) ,\n\n(6a)\n\nRlr (x) =\n\nl (cid:88)\n\ni=1\n\nφlr (Zi; wi, p) ,\n\n(6b)\n\nwhere in Eq. (6a) zi = Gix ∈ Rd, while in Eq. (6b) Zi ∈ Rc×q is a matrix whose dependence on x is expressed as: vec (Zi) = Gix ∈ Rd, with d = c · q. In words, the j-th row of Zi is formed by the q-dimensional feature vector Z(j,:) extracted from the image channel xj, j = 1,. . . ,c. The motivation for enforcing the matrices Zi to have low-rank is that the channels of natural images are typically highly correlated. Thus, it is reasonable to expect that the features stored in the rows of Zi, would be dependent to each other. We note that a similar low-rank enforcing regularization strategy is typically followed by regularizers whose goal is to model the non-local similarity property of natural images Gu et al. (2014); Xie et al. (2016). In these cases the matrices Zi are formed in such a way so that each of their rows holds the elements of a patch extracted from the image, while the entire matrix consists of groups of structurally similar patches.\n\ni\n\n2.2 MAJORIZATION-MINIMIZATION STRATEGY\n\nOne of the challenges in the minimization of the overall objective function in Eq. (2) is that the image priors introduced in Eq. (6) are generally non-convex w.r.t x. This precludes any guarantees of reaching a global minimum, and we can only opt for a stationary point. One way to handle the minimization task would be to employ the gradient descent method. Potential problems in such case are the slow convergence as well as the need to adjust the step-size in every iteration of the algorithm, so as to avoid divergence from the solution. Other possible minimization strategies are variable splitting (VS) techniques such as the Alternating Method of Multipliers (Boyd et al., 2011) and HalfQuadratic splitting (Nikolova & Ng, 2005), or the Fast Iterative Shrinkage Algorithm (FISTA) (Beck & Teboulle, 2009). The underlying idea of such methods is to transform the original problem in easier to solve sub-problems. However, VS techniques require finetuning of additional algorithmic parameters, to ensure that a satisfactory convergence rate is achieved, while FISTA works-well under the assumption that the proximal map of the regularizer (Boyd & Vandenberghe, 2004) is not hard to compute. Unfortunately, this is not the case for the regularizers considered in this work.\n\nFor all the above reasons, here we pursue a majorization-minimization (MM) approach (Hunter & Lange, 2004), which does not pose such strict requirements. Under the MM approach, instead of trying to minimize the objective function J (x) directly, we follow an iterative procedure where each iteration consists of two steps: (a) selection of a surrogate function that serves as a tight upperbound of the original objective (majorization-step) and (b) computation of a current estimate of the solution by minimizing the surrogate function (minimization-step). Specifically, the iterative algorithm for solving the minimization problem x∗ = arg minx J (x) takes the form: xk+1 = arg minx Q (cid:0)x; xk(cid:1), where Q (cid:0)x; xk(cid:1) is the majorizer of the objective function J (x) at some point xk, satisfying the two conditions:\n\nQ (cid:0)x; xk(cid:1) ≥ J (x) , ∀x, xk\n\nand Q (cid:0)xk; xk(cid:1) = J (cid:0)xk(cid:1) .\n\n(7)\n\nGiven these two properties of the majorizer, it can be easily shown that iteratively minimizing Q (cid:0)x; xk(cid:1) also monotonically decreases the objective function J (x) (Hunter & Lange, 2004). In fact, to ensure this, it only suffices to find a xk+1 that decreases the value of the majorizer, i.e., Q (cid:0)xk+1; xk(cid:1) ≤ Q (cid:0)xk; xk(cid:1). Moreover, given that both Q (cid:0)x; xk(cid:1) and J (x) are bounded from below, we can safely state that upon convergence we will reach a stationary point.\n\nThe success of the described iterative strategy solely relies on our ability to efficiently minimize the chosen majorizer. Noting that the data fidelity term of the objective is quadratic, we proceed by seeking a quadratic majorizer for the image prior R (x). This way the overall majorizer will be of quadratic form, which is amenable to efficient minimization by solving the corresponding normal equations. Below we provide two results that allow us to design such tight quadratic upper bounds both for the sparsity-promoting (6a) and the low-rank promoting (6b) regularizers. Their proofs are\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nprovided in Appendix A.1. We note that, to the best of our knowledge, the derived upper-bound presented in Lemma 2 is novel and it can find use in a wider range of applications, which also utilize low-rank inducing penalties (Hu et al., 2021), than the ones we focus here. Lemma 1. Let x, y ∈ Rn and w ∈ Rn can be upper-bounded as:\n\np function φsp (x; w, p) defined in Eq. (4)\n\n+. The weighted-lp\n\np 2\n\ntr (Wy) +\n\nxTWyx +\n\nφsp (x; w, p) ≤\n\npγ 2\nwhere Wy = diag (w) (cid:2)I ◦ (cid:0)yyT + γI(cid:1)(cid:3) p−2 in (8) is attained when x = y. Lemma 2. Let X, Y ∈ Rm×n and σ(Y ) , w ∈ Rr + with r = min (m, n). The vector σ(Y ) holds the singular values of Y in decreasing order while the elements of w are sorted in increasing order. The weighted-Schatten-matrix function φlr (X; w, p) defined in Eq. (5) can be upper-bounded as:\n\n2 and ◦ denotes the Hadamard product. The equality\n\nφsp (y; w, p) , ∀ x, y\n\n2−p 2\n\n(8)\n\nφlr (X; w, p) ≤ p\n\n2 tr (cid:0)WY XX T(cid:1) + pγ where WY = U diag (w) U T (cid:0)Y Y T + γI(cid:1) p−2 and V ∈ Rn×r. The equality in (9) is attained when X = Y .\n\n2 tr (WY ) + 2−p 2 and Y = U diag (σ (Y )) V T, with U ∈ Rm×r\n\n2 φlr (Y ; w, p) , ∀ X, Y\n\n(9)\n\nNext, with the help of the derived tight upper-bounds we obtain the quadratic majorizers for both of our regularizers as:\n\nQsp\n\n(cid:0)x; xk(cid:1) = p\n\n2\n\nl (cid:88)\n\nzT\n\ni Wzk\n\ni\n\nzi,\n\n(10a)\n\nQlr\n\n(cid:0)x; xk(cid:1) = p\n\n2\n\nl (cid:88)\n\n(cid:16)\n\ntr\n\nZT\n\ni WZk\n\ni\n\n(cid:17)\n\nZi\n\n, (10b)\n\ni=1\n\ni=1\n\nwhere zi and Zi are defined as in Eq. (6), Wzk , WZk are defined in Lemmas 1 and 2, respectively, and we have ignored all the constant terms that do not depend on x. In both cases, by adding the (cid:0)x; xk(cid:1), to the quadratic data fidelity term we obtain the overall majorizer of the regularizer, Qreg majorizer Q (cid:0)x; xk(cid:1), of the objective function J (x). Since this majorizer is quadratic, it is now possible to obtain the (k+1)-th update of the MM iteration by solving the normal equations:\n\ni\n\ni\n\n(cid:32)\n\nxk+1 =\n\nATA + p·σ2\n\nn\n\nl (cid:88)\n\ni=1\n\nGT\n\ni W k\n\ni Gi + αI\n\n(cid:33)−1\n\n(cid:0)ATy + αxk(cid:1) = (cid:0)Sk + αI(cid:1)−1\n\nbk,\n\n(11)\n\ni\n\ni\n\n2\n\nn, W k\n\ni = Wzk\n\ni = Iq ⊗WZk\n\n(cid:13)x − xk(cid:13) (cid:13) 2\n(cid:13)\n\n∈ Rd×d for Rsp (x) , and W k\n\n∈ Rc·q×c·q for Rlr (x). We where α = δσ2 note that, to ensure that the system matrix in Eq. (11) is invertible, we use an augmented majorizer that includes the additional term δ 2, with δ > 0 being a fixed small constant (we refer to Appendix A.1.1 for a justification of the validity of this strategy). This leads to the presence of the extra term αI additionally to the system matrix Sk and of αxk in bk. Based on the above, the minimization of J (x), incorporating any of the two regularizers of Eq. (6), boils down to solving a sequence of re-weighted least squares problems, where the weights W k i of the current iteration are updated using the solution of the previous one. Given that our regularizers in Eq. (6) include the weights wi ̸= 1, our proposed algorithm generalizes the IRLS methods introduced by Daubechies et al. (2010) and Mohan & Fazel (2012), which only consider the case where wi = 1 and have been successfully applied in the past on sparse and low-rank recovery problems, respectively.\n\n3 LEARNING PRIORS USING IRLS RECONSTRUCTION NETWORKS\n\nTo deploy the proposed IRLS algorithm, we have to specify both the regularization operator G and the parameters w = {wi}l i=1 , p of the potential functions in Eqs. (4) and (5), which constitute the image regularizers of Eq. (6). Manually selecting their values, with the goal of achieving satisfactory reconstructions, can be a cumbersome task. Thus, instead of explicitly defining them, we pursue the idea of implementing IRLS as a recurrent network, and learn their values in a supervised way. Under this approach, our learned IRLS-based reconstruction network (LIRLS) can be described as: (cid:0)xk; y(cid:1), with θ = {G, w, p} denoting its parameters. The network itself consists of three xk+1 = fθ main components: (a) A feature extraction layer that accepts as input the current reconstruction estimate, xk, and outputs the feature maps (cid:8)zk i=1. (b) The weight module that acts on zk i and the parameters wi to construct the weight matrix W k i , which is part of the system matrix Sk in Eq. (11). (c) The least-squares (LS) layer whose role is to produce a refined reconstruction estimate, xk+1, as the solution of Eq. (11). The overall architecture of LIRLS is shown in Fig. 1.\n\ni = Gixk(cid:9)l\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n3.1 NETWORK TRAINING\n\nA common training strategy for recurrent networks is to unroll the network using a fixed number of iterations and update its parameters either by means of back-propagation through time (BPTT) or by its truncated version (TBPTT) (Robinson & Fallside, 1987; Kokkinos & Lefkimmiatis, 2019). However, this strategy cannot be efficiently applied to LIRLS. The reason is that the system matrix Sk in Eq. (11) typically is of huge dimensions and its direct inversion is generally infeasible. Thus, to compute xk+1 via Eq. (11) we need to rely on a matrix-free iterative linear solver such as the conjugate gradient method (CG) (Shewchuk, 1994). This means that apart from the IRLS iterations we need also to take into account the internal iterations required by the linear solver. Therefore, unrolling both types of iterations would result in a very deep network, whose efficient training would be extremely challenging for two main reasons. The first is the required amount of memory, which would be prohibitively large. The second one is the problem of vanishing/exploding gradients that appears in recurrent architectures (Pascanu et al., 2013), which would be even more pronounced in the case of LIRLS due to its very deep structure.\n\nFigure 1: The proposed LIRLS recurrent architecture.\n\nTo overcome these problems, we rely on the fact that our IRLS strategy guarantees the convergence to a fixed point x∗. Practically, this means that upon convergence of IRLS, it will hold x∗ = fθ (x∗; y). Considering the form of our IRLS iterations, as shown in Eq. (11), this translates to:\n\ng (x∗, θ) ≡ x∗ − fθ (x∗; y) = S∗ (x∗, θ) x∗ − ATy = 0, where we explicitly indicate the dependency of S∗ on x∗ and θ. To update the network’s parameters during training, we have to compute the gradients ∇θL (x∗) = ∇θx∗∇x∗ L (x∗), where L is a loss function. Now, if we differentiate both sides of Eq. (12) w.r.t θ, then we get:\n\n(12)\n\n∂g (x∗, θ) ∂θ\n\n+\n\n∂g (x∗, θ) ∂x∗\n\n∂x∗ ∂θ\n\n= 0 ⇒ ∇θx∗ = −∇θg (x∗, θ) (∇x∗ g (x∗, θ))−1 .\n\n(13)\n\nThus, we can now compute the gradient of the loss function w.r.t the network’s parameters as:\n\n∇θL (x∗) = −∇θg (x∗, θ) v, where v is obtained as the solution of the linear problem ∇x∗ g (x∗, θ) v=∇x∗ L (x∗) and all the necessary auxiliary gradients can be computed via automatic differentiation. Based on the above, we can train the LIRLS network without having to restrict its overall effective depth or save any intermediate results that would significantly increase the memory requirements. The implementation details of our network for both its forward and backward passes, are described in Algorithm 1 in Sec. A.4. Finally, while our training strategy is similar in spirit with the one used for training Deep Equilibrium Models (DEQ) (Bai et al., 2019), an important difference is that our recurrent networks are guaranteed to converge to a fixed point, while in general this is not true for DEQ models.\n\n(14)\n\n3.2 LIRLS NETWORK IMPLEMENTATION\n\nIn this section we discuss implementation details for the LIRLS variants whose performance we will assess on different grayscale/color image reconstruction tasks. As mentioned earlier, among the parameters that we aim to learn is the regularization operator G. For all the different networks we parameterize G with a valid convolution layer that consists of 24 filters of size 5 × 5. In the case of color images these filters are shared across channels. Further, depending on the particular LIRLS instance that we utilize, we either fix the values of the parameters w, p or we learn them during training. Hereafter, we will use the notations lp,w to refer to the networks that employ a learned sparse-promoting prior and a learned low-rank promoting prior, respectively. The different LIRLS instances that we consider in this work are listed below:\n\nand S p,w\n\np\n\np\n\n1. l1/S1 (nuclear): fixed p = 1, fixed w = 1, where 1 is a vector of ones. 2. Weighted lw\n\n1 (weighted nuclear): fixed p = 1, weights w are computed by a weight prediction neural network (WPN). WPN accepts as inputs either the features z0 = ˆGx0 for the grayscale case or their singluar values for the color case, as well as the noise standard deviation σn. The\n\n1 /S w\n\n5\n\nIRLS weightmatrixLS layerfeedback-loop(shared weights)(shared weights)FeatureextractionEq. (11) + PCG(Lemmas 1 or 2)WPNorPublished as a conference paper at ICLR 2023\n\nTable 1: Comparisons on grayscale image deblurring.\n\nDataset\n\nSun et al. (2013)\n\nLevin et al. (2009)\n\nPSNR SSIM PSNR SSIM\n\nTV-l1 30.81 0.8272 34.74 0.9552\n\nTV 30.96 0.8351 34.92 0.9568\n\nl1 31.98 0.8759 35.19 0.9600\n\nlp p\n32.36 0.8849 35.33 0.9617\n\nlw 1\n32.70 0.8965 35.75 0.9647\n\nlp,w p\n32.67 0.8960 35.60 0.9628\n\nRED 31.68 0.8573 35.19 0.9584\n\nIRCNN 32.55 0.8860 32.86 0.9118\n\nFDN 32.63 0.8894 35.08 0.9609\n\nPSNR SSIM\n\nTVN 31.71 0.8506\n\nTable 2: Comparisons on color image deblurring. IRCNN 33.19 0.9123\n\nS p p\n34.17 0.9227\n\nSw 1\n33.95 0.9179\n\nS p,w p\n34.24 0.9234\n\nS1 33.09 0.8995\n\nRED 31.38 0.8233\n\nVTV 31.38 0.8440\n\nFDN 32.51 0.8857\n\nDWDN 34.09 0.9197\n\nvector x0 is the estimate obtained after 5 IRLS steps of the pretrained l1/S1 networks, while ˆG is their learned regularization operator. For all studied problems except of MRI reconstruction, we use a lightweight RFDN architecture proposed by Liu et al. (2020) to predict the weights, while for MRI reconstruction we use a lightweight UNet from Ronneberger et al. (2015), which we have found to be more suitable for this task. For the S w 1 network, in order to enforce the predicted weights to be sorted in increasing order, we apply across channels of the ouput of WPN a cumulative sum. In all cases, the number of parameters of WPN does not exceed 0.3M. p/S p 4. lp,w p\n\np : learned p ∈ [0.4, 0.9], fixed w = 1. /Sp,w p\n\n: learned p ∈ [0.4, 0.9], weights w are computed as described in item 2, with the only\n\n3. lp\n\ndifference being that both x0 and ˆG are now obtained from the pretarained lp\n\np/S p\n\np networks.\n\nWe note that the output of the LS layer, which corresponds to the solution of the normal equations in Eq. (11), is computed by utilizing a preconditioned version of CG (PCG) Hestenes & Stiefel (1952). This allows for an improved convergence of the linear solver. Details about our adopted preconditioning strategy are provided in Appendix A.2. Finally, in all the reported cases, the constant δ related to the parameter α in Eq. (11) is set to 8e−4, while as initial solution for the linear solver we use the output of an independently trained fast FFT-based Wiener filter.\n\n3.3 TRAINING DETAILS The convergence of the forward pass of LIRLS to a fixed point x∗ is determined according to the criterion: ||S∗ (x∗, θ) x∗ − ATy||2/||ATy||2 < 1e−4, which needs to be satisfied for 3 consecutive IRLS steps. If this criterion is not satisfied, the forward pass is terminated after 400 IRLS steps during training and 15 steps during inference. In the LS layers we use at most 150 CG iterations during training and perform an early exit if the relative tolerance of the residual falls below 1e−6, while during inference the maximum amount of CG iterations is reduced to 50. When training LIRLS, in the backward pass, as shown in Eq. (14), we are required to solve a linear problem whose system matrix corresponds to the Hessian of the objective function (2). This symmetric matrix is positive definite when J (x) is convex and indefinite otherwise. In the former case we utilize the CG algorithm to solve the linear problem, while in the latter one we use the Minimal Residual Method (MINRES) (Paige & Saunders, 1975). We perform early exit if the relative tolerance of the residual is below 1e−2 and limit the maximum amount of iterations to 2000. It should be noted that we have experimentally found these values to be adequate for achieving stable training.\n\nAll our models are trained using as loss function the negative peak-to-signal-noise-ratio (PSNR) between the ground truth and the network’s output. We use the Adam optimizer (Kingma & Ba, 2015) with a learning rate 5e−3 for all the models that do not involve a WPN and 1e−4 otherwise. The learning rate is decreased by a factor of 0.98 after each epoch. On average we set the batch size to 8 and train our models for 100 epochs, where each epoch consists of 500 batch passes.\n\n4 EXPERIMENTS In this section we assess the performance of all the LIRLS instances described in Sec. 3.2 on four different image reconstruction tasks, namely image deblurring, super-resolution, demosaicking and MRI reconstruction. In all these cases the only difference in the objective function J (x) is the form of the degradation operator A. Specifically, the operator A has one of the following forms: (a) low-pass valid convolutional operator (deblurring), (b) composition of a low-pass valid convolutional operator and a decimation operator (super-resolution), (c) color filter array (CFA) operator (demosaicking), and (d) sub-sampled Fourier operator (MRI reconstruction). The first two recovery tasks are related to either grayscale or color images, demosaicking is related to color images, and MRI reconstruction is related to single-channel images.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Comparisons on grayscale image super-resolution.\n\nScale\n\nNoise\n\nx2\n\nx3\n\nx4\n\n0%\n\n1%\n\n0%\n\n1%\n\n0%\n\n1%\n\nScale\n\nNoise\n\nx2\n\nx3\n\nx4\n\n0%\n\n1%\n\n0%\n\n1%\n\n0%\n\n1%\n\nPSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\n\nPSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\n\nTV-l1 26.23 0.6937 17.37 0.2739 24.50 0.5875 14.26 0.1812 22.51 0.4773 11.81 0.1228\n\nTV 28.04 0.8028 26.64 0.7320 25.55 0.6825 24.62 0.6276 24.07 0.6031 23.31 0.5627\n\nl1 28.20 0.8064 26.81 0.7379 25.63 0.6857 24.67 0.6306 24.09 0.6052 23.32 0.5643\n\nTVN 28.01 0.8035 26.87 0.7404 25.46 0.6812 24.76 0.6322 24.05 0.6018 23.43 0.5655\n\nVTV 28.13 0.8042 26.86 0.7392 25.62 0.6823 24.76 0.6314 24.13 0.6024 23.42 0.5645\n\nS1 28.21 0.8088 27.06 0.7469 25.50 0.6828 24.83 0.6348 24.15 0.6045 23.46 0.5666\n\nlp p\n27.98 0.7953 26.64 0.7292 25.44 0.6741 24.48 0.6195 23.92 0.5960 23.13 0.5551\n\nS p p\n28.62 0.8203 27.43 0.7646 25.60 0.6904 25.09 0.6468 24.28 0.6111 23.66 0.5752\n\nlw 1\n28.76 0.8186 27.32 0.7610 26.02 0.6975 25.03 0.6472 24.16 0.6061 23.35 0.5661\n\nSw 1\n28.42 0.8212 27.46 0.7640 25.35 0.6877 25.08 0.6452 24.22 0.6079 23.53 0.5698\n\nlp,w p\n28.31 0.8067 26.94 0.7470 25.67 0.6832 24.79 0.6353 24.10 0.6010 23.38 0.5648\n\nS p,w p\n28.78 0.8217 27.48 0.7687 25.90 0.6928 25.11 0.6497 24.27 0.6101 23.66 0.5774\n\nBicubic 24.74 0.6699 24.63 0.6521 23.34 0.5872 23.26 0.5739 22.27 0.5342 22.21 0.5247\n\nBicubic 24.73 0.6661 24.62 0.6486 23.33 0.5833 23.25 0.5702 22.25 0.5305 22.19 0.5210\n\nRED 28.34 0.8213 26.88 0.7275 25.70 0.7006 24.81 0.6417 24.15 0.6164 23.35 0.5731\n\nRED 28.68 0.8210 25.74 0.6215 25.90 0.7030 24.72 0.6075 24.33 0.6191 23.64 0.5681\n\nIRCNN 28.48 0.8324 26.22 0.6799 25.91 0.7144 24.70 0.6219 23.72 0.6168 23.44 0.5713\n\nIRCNN 28.33 0.8331 26.37 0.6843 25.74 0.7092 24.77 0.6203 23.46 0.6073 23.40 0.5643\n\nUSRNet 28.58 0.8188 27.26 0.7607 26.06 0.7101 25.07 0.6586 23.30 0.6203 21.41 0.4945\n\nUSRNet 28.86 0.8359 27.97 0.7924 26.17 0.7203 25.65 0.6857 23.30 0.6238 23.57 0.6028\n\nTable 4: Comparisons on color image super-resolution.\n\n4.1 TRAIN AND TEST DATA\n\nTo train our models for the first three recovery tasks, we use random crops of size 64 × 64 taken from the train and val subsets of the BSD500 dataset provided by Martin et al. (2001), while for MRI reconstruction we use 320 × 320 single-coil 3T images from the NYU fastMRI knee dataset (Knoll et al. (2020)). In order to train the deblurring networks we use synthetically created blur kernels varying in size from 13 to 35 pixels according to the procedure described by Boracchi & Foi (2012). For the super-resolution task we use scale factors 2, 3 and 4 with 25 × 25 kernels randomly synthesized using the algorithm provided by Bell-Kligler et al. (2019). For accelerated MRI we consider ×4 and ×8 undersampling factors in k-space using conjugate-symmetric masks so that training and inference is performed in the real domain. For the demosaicking problem we use the RGGB CFA pattern. All our models are trained and evaluated on a range of noise levels that are typical in the literature for each considered problem. Based on this, we consider σn to be up to 1% of the maximum image intensity for deblurring, super-resolution and MRI reconstruction tasks, while for demosaicing we use a wider range of noise levels, with σn being up to 3%.\n\nFor our evaluation purposes we use common benchmarks proposed for each recovery task. In particular, for deblurring we use the benchmarks proposed by Sun et al. (2013) and Levin et al. (2009). For super-resolution we use the BSD100RK dataset, which consists of 100 test images from the BSD500 dataset, and the degradation model proposed by Bell-Kligler et al. (2019). For demosaicking we use the images from the McMaster (Zhang et al., 2011) dataset. For MRI reconstruction we use the 3T scans from the val subset of the NYU fastMRI knee dataset, where from each scan we take the central slice and two more slices located 8 slices apart from the central one. None of our training data intersect with the data we use for evaluation purposes. We should further note, that all benchmarks we use for evaluation contain diverse sets of images with relatively large resolutions. This strategy is not common for other general purpose methods, which typically report results on a limited set of small images, due to their need to manually fine-tune certain parameters. In our case all network parameters are learned via training and remain fixed during inference. In order to provide a fair comparison, for the methods that do require manual tuning of parameters, we perform a grid search on a smaller subset of images to find the values that lead to the best results. Then we use these values fixed during the evaluation for the entire set of images per each benchmark. Finally, note that for all TV-based regularizers we compute their solutions using an IRLS strategy.\n\n4.2 RESULTS\n\nDeblurring. In Table 1 we report the average results in terms of PSNR and structure-similarity index measure (SSIM) for several of our LIRLS instances, utilizing different sparsity-promoting priors, and few competing methods, namely anisotropic (TV-l1) (Zach et al., 2007) and isotropic\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nDataset\n\nNoise\n\nMcMaster Zhang et al. (2011)\n\n0%\n\n1%\n\n2%\n\n3%\n\nTable 5: Comparisons on image demosaicking. S1 36.27 0.9605 35.14 0.9416 33.37 0.9151 31.39 0.8808\n\nS p p\n37.66 0.9649 36.43 0.9498 34.55 0.9224 32.44 0.8835\n\nSw 1\n36.60 0.9606 35.49 0.9451 33.95 0.9226 32.60 0.8997\n\nVTV 32.64 0.9285 32.16 0.9076 31.16 0.8638 30.13 0.8164\n\nTVN 33.33 0.9377 33.32 0.9198 32.21 0.8843 31.19 0.8539\n\nPSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\n\nS p,w p\n37.62 0.9632 36.37 0.9490 34.80 0.9281 33.34 0.9050\n\nTable 6: Comparisons on MRI reconstruction.\n\nAcceleration\n\nx4\n\nx8\n\nPSNR SSIM PSNR SSIM\n\nTV-l1 26.31 0.5883 24.43 0.5092\n\nTV 26.40 0.5921 24.51 0.5120\n\nl1 28.53 0.6585 25.86 0.5518\n\nlp p\n28.99 0.6701 26.19 0.5620\n\nlw 1\n30.11 0.7081 28.02 0.6168\n\nlp,w p\n29.75 0.6958 27.30 0.5973\n\nRED 34.53 0.9338 34.40 0.9208 33.08 0.8821 31.59 0.8308\n\nIRCNN 37.52 0.9612 36.12 0.9403 34.42 0.9178 33.23 0.8931\n\nBilinear 32.27 0.9259 31.76 0.9014 30.64 0.8443 29.34 0.7745\n\nFBP 25.53 0.6014 24.34 0.5237\n\np\n\nTotal Variation (Rudin et al., 1992), RED (Romano et al., 2017), IRCNN (Zhang et al., 2017), and FDN (Kruse et al., 2017). From these results we observe that our LIRLS models lead to very competitive performance, despite the relatively small number of learned parameters. In fact, the lw 1\nand lp,w variants obtain the best results among all methods, including FDN, which is the current grayscale sota method, and IRCNN that involves 4.7M parameters in total (for its 25 denoising networks). Similar comparisons for color images are provided in Table 2, where we report the reconstruction results of our LIRLS variants, which utilize different learned low-rank promoting priors. For these comparisons we further consider the vector-valued TV (VTV) (Blomgren & Chan, 1998), Nuclear TV (TVN) Lefkimmiatis et al. (2015) and DWDN (Dong et al., 2020). From these results we again observe that the LIRLS models perform very well. The most interesting observation though, is that our S p p model with a total of 601 learned parameters manages to outperform the DWD network which involves 7M learned parameters. It is also better than S w 1 and very close to S p,w ,\nwhich indicates that the norm order p plays a more crucial role in this task than the weights w.\n\np\n\nSuper-resolution. Similarly to the image deblurring problem, we provide comparisons among competing methods both for grayscale and color images. For these comparisons we still consider the RED and IRCNN methods as before and we further include results from bicubic upsampling and the USRNet network Zhang et al. (2020). The latter network, unlike RED and IRCNN, is specifically designed to deal with the problem of super-resolution and involves 17M of learned parameters. From the reported results we can see that for both the grayscale and color cases the results we obtain with our LIRLS instances are very competitive and they are only slightly inferior than the specialized USRNet network. For a visual assessment of the reconstructions quality we refer to Fig. 3.\n\nDemosaicking. For this recovery task we compare our low-rank promoting LIRLS models against the same general-purpose classical and deep learning approaches as in color deblurring, plus bicubic upsamplping. The average results of all the competing methods are reported in Table 5. From these results we come to a similar conclusion as in the debluring case. Specifically, the best performance is achieved by the S p LIRLS models. The first one performs best for lower noise levels while the latter achieves the best results for the highest ones. Visual comparisons among the different methods are provided in Fig. 4.\n\np and S p,w\n\np\n\nMRI Reconstruction. In Table 6 we compare the performance of our LIRLS models with anisotropic and isotropic TV reconstruction and the Fourier back-projection (FBP) method. While\n\nInput\n\nTVN\n\nS p\n\np\n\nSw\n\n1\n\nIRCNN\n\nDWDN\n\nFigure 2: Visual comparisons among several methods on a real blurred color image. Image and blur kernel were taken from Pan et al. (2016). For more visual examples please refer to Appendix A.7.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nInput (Bicubic)\n\nTVN\n\nS p\n\np\n\nSw\n\n1\n\nRED\n\nUSRNet\n\nFigure 3: Visual comparisons among several methods on a real low-resolution image enlarged by a scale factor 2. Image from Cai et al. (2019), downscaling kernel was obtained using the method by Bell-Kligler et al. (2019). For more visual examples please refer to Appendix A.7.\n\nthese three methods are conventional ones, they can be quite competitive on this task and are still relevant due to their interpretability, which is very important in medical applications. Based on the reported results and the visual comparisons provided in Fig. 5 we can conclude that the LIRLS models do a very good job in terms of reconstruction quality. It is also worth noting that similarly to the other two gray-scale recovery tasks, we observe that the best performance among the LIRLS models is achieved by lw 1 . This can be attributed to the fact that the learned prior of this model while being adaptive, thanks to the presence of the weights, is still convex, unlike lp . Therefore, its output is not significantly affected by the initial solution. Additional discussions about our networks reconstruction performance related to the choice of p and the weights w is provided in Appendix A.3, along with some possible extensions that we plan to explore in the future.\n\np and lp,w\n\np\n\n5 CONCLUSIONS\n\nIn this work we have demonstrated that our proposed IRLS method, which covers a rich family of sparsity and low-rank promoting priors, can be successfully applied to deal with a wide range of practical inverse problems. In addition, thanks to its convergence guarantees we have managed to use it in the context of supervised learning and efficiently train recurrent networks that involve only a small number of parameters, but can still lead to very competitive reconstructions. Given that most of the studied image priors are non-convex, an interesting open research topic is to analyze how the initial solution affects the output of the LIRLS models and how we can exploit this knowledge to further improve their reconstruction performance. Finally, it would be interesting to explore whether learned priors with spatially adaptive norm orders p, can lead to additional improvements.\n\nBilinear: 32.05\n\nTVN: 32.00\n\nS p\n\np : 35.07\n\nSw\n\n1 : 32.57\n\nRED: 32.12\n\nIRCNN: 33.93\n\nTarget\n\nFigure 4: Visual comparisons among several methods on a mosaicked image with 1% noise. For each image its PSNR value is provided in dB. For more visual examples please refer to Appendix A.7.\n\nFBP: 22.95\n\nTV: 25.52\n\nl1: 27.77\n\nlp p: 28.59\n\nlw 1 : 30.30\n\nTarget\n\nFigure 5: Visual comparisons among several methods on a simulated MRI with x4 acceleration. For each image its PSNR value is provided in dB. For more visual examples please refer to Appendix A.7.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nShaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural\n\nInformation Processing Systems, volume 32, 2019.\n\nA. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-\n\nlems. SIAM J. Imaging Sci., 2:183–202, 2009.\n\nSefi Bell-Kligler, Assaf Shocher, and Michal Irani. Blind super-resolution kernel estimation using\n\nan internal-gan. Advances in Neural Information Processing Systems, 32, 2019.\n\nMartin Benning and Martin Burger. Modern regularization methods for inverse problems. Acta\n\nNumerica, 27:1–111, 2018.\n\nMario Bertero and Patrizia Boccacci. Introduction to Inverse Problems in Imaging. IOP Publishing,\n\n1998.\n\nP. Blomgren and T.F. Chan. Color TV: Total Variation methods for restoration of vector-valued\n\nimages. IEEE Trans. Image Process., 7:304–309, 1998.\n\nGiacomo Boracchi and Alessandro Foi. Modeling the performance of image restoration from motion\n\nblur. IEEE Transactions on Image Processing, 21(8):3502–3517, 2012.\n\nS. Boyd and L. Vandenberghe. Convex Optimization. Kluwer Academic Publishers, 2004.\n\nS. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed Optimization and Statistical\n\nLearning via the Alternating Direction Method of Multipliers. Now Publishers, 2011.\n\nAlfred M Bruckstein, David L Donoho, and Michael Elad. From sparse solutions of systems of\n\nequations to sparse modeling of signals and images. SIAM review, 51(1):34–81, 2009.\n\nJianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: A new benchmark and a new model. In Proceedings of the IEEE International Conference on Computer Vision, 2019.\n\nEmmanuel J. Candes and Michael B. Wakin. An introduction to compressive sampling. IEEE Signal\n\nProcessing Magazine, 25(2):21–30, 2008.\n\nEmmanuel J Candes, Michael B Wakin, and Stephen P Boyd. Enhancing sparsity by reweighted l1\n\nminimization. Journal of Fourier analysis and applications, 14(5):877–905, 2008.\n\nRick Chartrand. Exact reconstruction of sparse signals via nonconvex minimization. IEEE Signal\n\nProcessing Letters, 14(10):707–710, 2007.\n\nIngrid Daubechies, Ronald DeVore, Massimo Fornasier, and C Sinan G ̈unt ̈urk.\n\nIteratively reweighted least squares minimization for sparse recovery. Communications on Pure and Applied Mathematics, 63(1):1–38, 2010.\n\nJiangxin Dong, Stefan Roth, and Bernt Schiele. Deep wiener deconvolution: Wiener meets deep learning for image deblurring. Advances in Neural Information Processing Systems, 33:1048– 1059, 2020.\n\nDavid L Donoho. Compressed sensing.\n\nIEEE Transactions on information theory, 52(4):1289–\n\n1306, 2006.\n\nIain S Duff and Jacko Koster. On algorithms for permuting large entries to the diagonal of a sparse\n\nmatrix. SIAM Journal on Matrix Analysis and Applications, 22(4):973–996, 2001.\n\nMichael Elad. Sparse and redundant representations: from theory to applications in signal and\n\nimage processing, volume 2. Springer, 2010.\n\nM.A.T. Figueiredo, J.M. Bioucas-Dias, and R.D. Nowak. Majorization–minimization algorithms\n\nfor wavelet-based image restoration. IEEE Trans. Image Process., 16:2980–2991, 2007.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nShuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization In Proc. IEEE Int. Conf. Computer Vision and Pattern\n\nwith application to image denoising. Recognition, pp. 2862–2869, 2014.\n\nMagnus R Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving. Journal of\n\nresearch of the National Bureau of Standards, 49(6):409, 1952.\n\nZhanxuan Hu, Feiping Nie, Rong Wang, and Xuelong Li. Low rank regularization: A review. Neural\n\nNetworks, 136:218–232, 2021.\n\nD. Hunter and K. Lange. A tutorial on MM algorithms. The American Statistician, 58:30–37, 2004.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),\n\n2015.\n\nFlorian Knoll, Jure Zbontar, Anuroop Sriram, Matthew J Muckley, Mary Bruno, Aaron Defazio, Marc Parente, Krzysztof J Geras, Joe Katsnelson, Hersh Chandarana, et al. fastmri: A publicly available raw k-space and dicom dataset of knee images for accelerated mr image reconstruction using machine learning. Radiology. Artificial intelligence, 2(1), 2020.\n\nFilippos Kokkinos and Stamatios Lefkimmiatis. Iterative joint image demosaicking and denoising using a residual denoising network. IEEE Transactions on Image Processing, 28(8):4177–4188, 2019.\n\nJakob Kruse, Carsten Rother, and Uwe Schmidt. Learning to push the limits of efficient fft-based image deconvolution. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4586–4594, 2017.\n\nChristian K ̈ummerle and Claudio M Verdun. A scalable second order method for ill-conditioned In International Conference on Machine Learning, pp.\n\nmatrix completion from few samples. 5872–5883. PMLR, 2021.\n\nMing-Jun Lai, Yangyang Xu, and Wotao Yin.\n\nImproved iteratively reweighted least squares for unconstrained smoothed lq minimization. SIAM Journal on Numerical Analysis, 51(2):927–957, 2013.\n\nS. Lefkimmiatis, A. Bourquard, and M. Unser. Hessian-based norm regularization for image restora-\n\ntion with biomedical applications. IEEE Trans. Image Process., 21(3):983–995, 2012.\n\nS. Lefkimmiatis, J. Ward, and M. Unser. Hessian Schatten-norm regularization for linear inverse\n\nproblems. IEEE Trans. Image Process., 22(5):1873–1888, 2013.\n\nS. Lefkimmiatis, A. Roussos, P. Maragos, and M. Unser. Structure tensor total variation. SIAM J.\n\nImaging Sci., 8:1090–1122, 2015.\n\nA. Levin, Y. Weiss, F. Durand, and W. T. Freeman. Understanding and evaluating blind deconvolution algorithms. In IEEE Conf. Comput. Vision and Patt. Recogn. (CVPR), pp. 1964–1971, 2009.\n\nJie Liu, Jie Tang, and Gangshan Wu. Residual feature distillation network for lightweight image\n\nsuper-resolution. In European Conference on Computer Vision, pp. 41–55. Springer, 2020.\n\nLu Liu, Wei Huang, and Di-Rong Chen. Exact minimum rank approximation via schatten p-norm\n\nminimization. Journal of Computational and Applied Mathematics, 267:218–227, 2014.\n\nJulien Mairal, Francis Bach, and Jean Ponce. Sparse modeling for image and vision processing.\n\nFoundations and Trends® in Computer Graphics and Vision, 8(2-3):85–283, 2014.\n\nA. W Marshall, I. Olkin, and B. C Arnold. Inequalities: theory of majorization and its applications,\n\nvolume 143. Springer, 1979.\n\nD. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. IEEE Int. Conf. Computer Vision, pp. 416–423, 2001.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nKarthik Mohan and Maryam Fazel. Iterative reweighted algorithms for matrix rank minimization.\n\nThe Journal of Machine Learning Research, 13(1):3441–3473, 2012.\n\nVishal Monga, Yuelong Li, and Yonina C Eldar. Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing. IEEE Signal Processing Magazine, 38(2):18–44, 2021.\n\nMila Nikolova and Michael K Ng. Analysis of half-quadratic minimization methods for signal and\n\nimage recovery. SIAM Journal on Scientific computing, 27(3):937–966, 2005.\n\nChristopher C Paige and Michael A Saunders. Solution of sparse indefinite systems of linear equa-\n\ntions. SIAM journal on numerical analysis, 12(4):617–629, 1975.\n\nJinshan Pan, Zhouchen Lin, Zhixun Su, and Ming-Hsuan Yang. Robust kernel estimation with outliers handling for image deblurring. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2800–2808, 2016.\n\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural\n\nnetworks. In International conference on machine learning, pp. 1310–1318. PMLR, 2013.\n\nAJ Robinson and Frank Fallside. The utility driven dynamic error propagation network. University\n\nof Cambridge Department of Engineering Cambridge, 1987.\n\nYaniv Romano, Michael Elad, and Peyman Milanfar. The little engine that could: Regularization by\n\ndenoising (red). SIAM Journal on Imaging Sciences, 10(4):1804–1844, 2017.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015.\n\nL. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica\n\nD, 60:259–268, 1992.\n\nJ. R. Shewchuk. An introduction to the conjugate gradient method without the agonizing pain, 1994.\n\nURL http://www.cs.cmu.edu/ ̃jrs/jrspapers.html.\n\nLibin Sun, Sunghyun Cho, Jue Wang, and James Hays. Edge-based blur kernel estimation using patch priors. In Proc. IEEE International Conference on Computational Photography, 2013.\n\nY. Sun, P. Babu, and D. P. Palomar. Majorization-minimization algorithms in signal processing, IEEE Transactions on Signal Processing, 65(3):794–\n\ncommunications, and machine learning. 816, 2017.\n\nA. Tikhonov. Solution of incorrectly formulated problems and the regularization method. In Soviet\n\nMath. Dokl., volume 5, pp. 1035–1038, 1963.\n\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-\n\ngredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.\n\nC. R. Vogel. Computational Methods for Inverse Problems. SIAM, 2002.\n\nYuan Xie, Shuhang Gu, Yan Liu, Wangmeng Zuo, Wensheng Zhang, and Lei Zhang. Weighted schatten p-norm minimization for image denoising and background subtraction. IEEE transactions on image processing, 25(10):4842–4857, 2016.\n\nChristopher Zach, Thomas Pock, and Horst Bischof. A duality based approach for realtime TV-L1\n\noptical flow. In Pattern Recognition, pp. 214–223. Springer, 2007.\n\nKai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep CNN denoiser prior for image restoration. In Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, July 2017.\n\nKai Zhang, Luc Van Gool, and Radu Timofte. Deep unfolding network for image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3217–3226, 2020.\n\nLei Zhang, Xiaolin Wu, Antoni Buades, and Xin Li. Color demosaicking by local directional interpolation and nonlocal adaptive thresholding. Journal of Electronic imaging, 20(2):023016, 2011.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 PROOFS\n\nIn this section we derive the proofs for the tight upper-bounds presented in Lemma 1 and Lemma 2. For our proofs we use the following inequality for a function |x|p, 0 < p ≤ 2, which is given by Sun et al. (2017):\n\n|x|p ≤\n\np 2\n\n|y|p−2x2 +\n\n2 − p 2\n\n|y|p, ∀x ∈ R and y ∈ R \\ {0}\n\n(15)\n\nand the Ruhe’s trace inequality (Marshall et al., 1979), which is stated in the following theorem:\n\nTheorem 1. Let A and B be n × n positive semidefinite Hermittian matrices. Then it holds that:\n\ntr (AB) ≥\n\nn (cid:88)\n\ni=1\n\nσi (A) σn−i+1 (B) ,\n\n(16)\n\nwhere σi (A) denotes the i-th singular value of A and the singular values are sorted in a decreasing order, that is σi (A) > σi+1 (A).\n\nProof of Lemma 1. The proof is straightforward and relies on the inequality of Eq. (15). Specifically, let us consider the positive scalars (cid:112)x2 i + γ, with γ > 0. If we plug them in (15) we get:\n\ni + γ, (cid:112)y2\n\n(cid:0)x2\n\ni + γ(cid:1) p\n\n2 ≤\n\n(cid:0)y2\n\ni + γ(cid:1) p−2\n\n2 (cid:0)x2\n\ni + γ(cid:1) +\n\np 2\n\n2 − p 2\n\n(cid:0)y2\n\ni + γ(cid:1) p\n\n2 .\n\nMultiplying both sides by a non-negative scalar wi leads to:\n\nwi\n\n(cid:0)x2\n\ni + γ(cid:1) p\n\n2 ≤\n\np 2\n\n(cid:0)y2\n\ni + γ(cid:1) p−2\n\n2 wi\n\n(cid:0)x2\n\ni + γ(cid:1) +\n\n2 − p 2\n\nwi\n\n(cid:0)y2\n\ni + γ(cid:1) p\n\n2 .\n\nThe above inequality is closed under summation and, thus, it further holds that:\n\nφsp (x; w, p) =\n\nn (cid:88)\n\ni=1\n\nwi\n\n(cid:0)x2\n\ni + γ(cid:1) p\n\n2\n\n(17)\n\n(18)\n\n(19)\n\np 2\n\np 2\n\n(cid:88)\n\ni=1 n\n(cid:88)\n\ni=1\n\nwi\n\nwi\n\n(cid:0)y2\n\ni + γ(cid:1) p−2\n\n2 (cid:0)x2\n\ni + γ(cid:1) +\n\n2 − p 2\n\nn (cid:88)\n\ni=1\n\nwi\n\n(cid:0)y2\n\ni + γ(cid:1) p\n\n2\n\n(cid:0)y2\n\ni + γ(cid:1) p−2\n\n2 x2\n\ni +\n\npγ 2\n\nn (cid:88)\n\ni=1\n\nwi\n\n(cid:0)y2\n\ni + γ(cid:1) p−2\n\n2 +\n\n2 − p 2\n\nn (cid:88)\n\ni=1\n\nwi\n\n(cid:0)y2\n\ni + γ(cid:1) p\n\n2\n\nxTWyx +\n\npγ 2\n\ntr (Wy) +\n\n2 − p 2\n\nφsp (y; w, p) , ∀x, y\n\n(20)\n\n≤\n\n=\n\n=\n\np 2\n(cid:18)\n\nwith Wy = diag\n\nw1\n\n(cid:0)y2\n\n1 + γ(cid:1) p−2\n\n2\n\n, . . . , wn\n\n(cid:0)y2\n\nn + γ(cid:1) p−2\n\n2\n\n(cid:19)\n\n= diag (w) (cid:2)I ◦ (cid:0)yyT + γI(cid:1)(cid:3) p−2 2 .\n\nBy substitution and carrying over the algebraic operations on the r.h.s of Eq. (20), we can show that when x = y, the inequality reduces to equality.\n\nWe note that it is possible to derive the IRLS algorithm that minimizes J (x) of Eq. (2) under the weighted lp p regularizers, without relying on the MM framework. In particular, we can redefine the regularizer φsp (·) as:\n\n ̃φsp (x; w, p) =\n\nn (cid:88)\n\ni=1\n\nwi\n\n(cid:0)x2\n\ni + γ(cid:1) p−2\n\n2 x2\n\ni\n\n(21)\n\nfrom where the weights Wy of Lemma 1 can be inferred. Then, the convergence of the IRLS strategy to a stationary point can be proven according to (Daubechies et al., 2010).\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nUnfortunately, this strategy doesn’t seem to apply for the weights WY of Lemma 2. The reason is that the weighted S p p regularizers don’t apply directly on the matrix X but instead on its singular values. Specifically, it holds that:\n\nφlr (X; w, p) =\n\nn (cid:88)\n\ni=1\n\nwi\n\n(cid:0)σ2\n\ni (X) + γ(cid:1) p\n\n2 = tr\n\n(cid:16)\n\nW (X) (cid:0)XX T + γI(cid:1) p\n\n2 (cid:17)\n\n.\n\n(22)\n\nUnlike the previous case, here the weights w are included in the matrix W (X), which directly depends on X as: W (X) = U (X) diag (w) U T (X), with U (X) being the left singular vectors of X. Therefore it is unclear how the approach by Mohan & Fazel (2012) would apply in this case and how the convergence of IRLS can be established.\n\nProof of Lemma 2. Let us consider the positive scalars (cid:112)σ2 them in (15) we get:\n\ni (X) + γ, (cid:112)σ2\n\ni (Y ) + γ. If we plug\n\n(cid:0)σ2\n\ni (X) + γ(cid:1) p\n\n2 ≤\n\np 2\n\n(cid:0)σ2\n\ni (Y ) + γ(cid:1) p−2\n\n2 (cid:0)σ2\n\ni (X) + γ(cid:1) +\n\n2 − p 2\n\n(cid:0)σ2\n\ni (Y ) + γ(cid:1) p\n\n2 .\n\nMultiplying both sides by a non-negative scalar wi leads to:\n\nwi\n\n(cid:0)σ2\n\ni (X) + γ(cid:1) p\n\n2 ≤\n\np 2\n\nwi\n\n(cid:0)σ2\n\ni (Y ) + γ(cid:1) p−2\n\n2 (cid:0)σ2\n\ni (X) + γ(cid:1) +\n\n2 − p 2\n\nwi\n\n(cid:0)σ2\n\ni (Y ) + γ(cid:1) p\n\n2 .\n\nThe above inequality is closed under summation and, thus, it further holds that:\n\n(23)\n\n(24)\n\nφlr (X; w, p) =\n\nr (cid:88)\n\ni=1\n\nwi\n\n(cid:0)σ2\n\ni (X) + γ(cid:1) p\n\n2\n\n≤\n\n=\n\n=\n\np 2\n\np 2\n\np 2\n\nr (cid:88)\n\ni=1 r\n(cid:88)\n\ni=1 r\n(cid:88)\n\ni=1\n\nwi\n\nwi\n\n(cid:0)σ2\n\ni (Y ) + γ(cid:1) p−2\n\n2 (cid:0)σ2\n\ni (X) + γ(cid:1) +\n\n2 − p 2\n\nr (cid:88)\n\ni=1\n\nwi\n\n(cid:0)σ2\n\ni (Y ) + γ(cid:1) p\n\n2\n\n(cid:0)σ2\n\ni (Y ) + γ(cid:1) p−2\n\n2 σ2\n\ni (X) +\n\npγ 2\n\nr (cid:88)\n\ni=1\n\nwi\n\n(cid:0)σ2\n\ni (Y ) + γ(cid:1) p−2\n\n2 +\n\n2−p 2\n\nφlr (Y ; w, p)\n\nσr−i+1 (WY ) σi\n\n(cid:0)XX T (cid:1) +\n\npγ 2\n\ntr (WY ) +\n\n2−p 2\n\nφlr (Y ; w, p) ,\n\n(25)\n\nwhere WY = U diag (w) U T (cid:0)Y Y T + γI(cid:1) p−2 and Y admits the singular value decomposition Y = U diag (σ (Y )) V T with U ∈ Rm×r, V ∈ Rn×r, and r = min (m, n). Further, we show that it holds:\n\n2\n\nWY = U diag (w) U T (cid:0)Y Y T + γI(cid:1) p−2\n\n2\n\nw1\n\n\n\n \n\n\nwr\n\n\n\n \n\n\n= U\n\n= ˆU\n\n(cid:0)σ2\n\n1 (Y ) + γ(cid:1) p−2\n\n2\n\n. . . . . .\n\n0\n\n0\n\n. . . wr\n\n(cid:0)σ2\n\nr (Y ) + γ(cid:1) p−2\n\n2\n\n(cid:0)σ2\n\nr (Y ) + γ(cid:1) p−2\n\n2\n\n. . . . . .\n\n0\n\n\n\n \n\n\n\n\n \n\n\nU T\n\nˆU T\n\n(26)\n\n0 = ˆU diag (σ (WY )) ˆU T ∈ Rm×m,\n\n. . . w1\n\n(cid:0)σ2\n\n1 (Y ) + γ(cid:1) p−2\n\n2\n\nwhere ˆU = U J , with J denoting the exchange matrix (row-reversed identity matrix). We note that the vector σ (WY ) ∈ Rr +, similarly to σ (X) and σ (Y ), holds the singular values of WY in decreasing order, given that\n\nwi+1\n\n(cid:0)σ2\n\ni+1 (Y ) + γ(cid:1) p−2\n\n2 ≥ wi\n\n14\n\n(cid:0)σ2\n\ni (Y ) + γ(cid:1) p−2\n\n2\n\n∀i.\n\n(27)\n\nPublished as a conference paper at ICLR 2023\n\nThis is true because according to the definition of w, it holds wi+1 ≥ wi ∀i, while it also holds (cid:0)σ2\n\n2 ∀i, since σi+1 (Y ) ≤ σi (Y ) and p−2\n\ni+1 (Y ) + γ(cid:1) p−2\n\ni (Y ) + γ(cid:1) p−2\n\n2 ≥ (cid:0)σ2\n\n2 ≤ 0.\n\nFinally, given that both WY and XX T are positive semidefinite symmetric matrices, we can invoke Ruhe’s trace inequality from Theorem 1 and combine it with Eq. (25) to get:\n\nφlr (X; w, p) ≤\n\ntr (cid:0)WY XX T(cid:1) +\n\np 2\n\npγ 2\n\ntr (WY ) +\n\n2−p 2\n\nφlr (Y ; w, p) ∀X, Y .\n\n(28)\n\nBy substitution and carrying over the algebraic operations on the r.h.s of Eq. (28), we can show that when X = Y the inequality reduces to equality.\n\nA.1.1 THEORETICAL JUSTIFICATION FOR USING AN AUGMENTED MAJORIZER\n\nIn Section 2.2 where we consider the solution of the normal equations in Eq. (11), instead of the majorizers that we derived in Eqs. (10), Qreg, we consider their augmented counterparts which are of the form:\n\n ̃Q (cid:0)x; xk(cid:1) = Qreg\n\n(cid:0)x; xk(cid:1) +\n\nα 2\n\n(cid:13)x − xk(cid:13) (cid:13) 2\n(cid:13)\n\n2 .\n\n(29)\n\nThe reason is that, under this choice the system matrix of Eq. (11) is guaranteed to be non-singular and, thus, a unique solution of the linear system always exists. To verify that this choice doesn’t compromise the convergence guarantees of our IRLS approach, we note that ̃Q (cid:0)x; xk(cid:1) is still a valid majorizer and satisfies both properties of Eq. (7), required by the MM framework. Specifically, it is straightforward to show that:\n\n ̃Q (x; x) = Qreg (x)\n\nand\n\n ̃Q (cid:0)xk; x(cid:1) ≥ Qreg ∀x, xk.\n\n(30)\n\nFinally, we note that the use of the augmented majorizer serves an additional purpose. In particular, (cid:13)x − xk(cid:13) (cid:13) 2\n2, the majorizer ̃Q enforces the IRLS estimates between two successive due to the term α (cid:13) IRLS iterations, xk and xk+1, not to differ significantly. Both the unique solution of the linear system and the closeness of the the successive IRLS estimates play an important role for the stability of the training stage of our LIRLS networks.\n\n2\n\nA.2 MATRIX EQUILIBRATION PRECONDITIONING\n\nDuring the training and inference of LIRLS, both the network parameters as well as the samples in the input batches vary significantly. This results in a convergence behavior that is not consistent, which is mostly attributed to the varying convergence rate of the linear solver at each IRLS step. Indeed, it turns out that the main term Sk of system matrix, defined in Eq.(11), in certain cases can be poorly conditioned. To deal with this issue and improve the overall convergence of LIRLS we apply a preconditioning strategy. In particular, we employ a matrix equilibration (Duff & Koster, 2001) such that the resulting preconditioned matrix has a unit diagonal, while its off-diagonal entries are not greater than 1 in magnitude. In our case all the components that form the system matrix are given in operator form, and thus we do not have access to the individual matrix elements of S = Sk + αI. For this reason, we describe below the practical technique of forming a diagonal matrix preconditioner that equilibrates the matrix S ∈ Rn·c×n·c.\n\nWe start by noting that such matrix can be decomposed as:\n\nS = ATA + p·σ2\n\nnGTW G + αI = (cid:2)AT √\n\np·σnGTW 1/2 √\n\nαI(cid:3)\n\n\n\n\n\n√\n\nA p·σnW 1/2G αI\n\n√\n\n  = BTB,\n\n(31)\n\nwhere B ∈ R(n1+n2+n3)×n3, with n3 = n·c. We further note a simple fact, that any diagonal matrix D = diag (d) multiplied with B from the right (BD) equally scales all the elements of each column B:,j with a corresponding diagonal element dj, and the same diagonal matrix multiplied with BT from the left (DBT) scales the same way each matrix row of BT. Let us now select the diagonal elements dj of matrix D to be of the form dj = 1/||B:,j||2, i.e. the inverse of the l2 norm of the corresponding column of the matrix B. In this case, the product BD results in a matrix\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nwith normalized columns, while the product DBT results in a matrix with normalized rows. The product of the two, i.e., the preconditioned matrix DTBTBD, becomes equilibrated since it has a unit diagonal and all of its non-diagonal elements are smaller or equal to 1. The task then becomes to develop a simple way of calculating the vector d that holds the norms of the matrix B columns.\n\nFrom the definition of matrix B in Eq (31), the squared norm of its j-th column can be computed as:\n\n||B:,j||2\n\n2 =\n\nn1+n2+n3(cid:88)\n\ni=1\n\nB2\n\ni,j =\n\nn1(cid:88)\n\ni=1\n\nA2\n\ni,j + p·σ2\n\nn\n\nn2(cid:88)\n\ni=1\n\n(cid:104)\n\nW 1/2G\n\n(cid:105)2\n\ni,j\n\n+ α.\n\n(32)\n\nAll restoration problems under study are large-scale, meaning the matrices A and G and W are structured, but only available in an operator form. Depending on the problem at hand, A is either a valid convolution matrix (deblurring), strided valid convolution matrix (super-resolution), diagonal binary matrix (demosaicing) or orthonormal subsampled FFT matrix (MRI reconstruction), G is a block matrix with valid convolution matrices as blocks, while the matrix W is either a diagonal one, for the case of sparse promoting priors, or a block diagonal matrix with each block being a matrix of dimensions c × c, for the case of low-rank promoting priors.\n\nn1(cid:80)\n\nA2\n\nn2(cid:80)\n\n(cid:2)W 1/2G(cid:3)2\n\nj\n\ni,j =\n\nWe utilize the following trick in order to calculate both terms\n\ni,j appearing in Eq. (32). In particular, considering any arbitrary matrix C and a vector of ones 1 we note that n\n(cid:80)\n\n, where C◦2 ≡ C ◦ C is the Hadamard square operation and ◦ denotes\n\n(cid:16)(cid:2)C◦2(cid:3)T\n\ni,j and\n\nC2\n\ni=1\n\ni=1\n\n(cid:17)\n\n1\n\ni=1 the Hadamard product. The computation of the Haramard square for operators A is straightforward: for possibly strided valid convolution matrices (the following also holds for convolutions with periodic and zero boundaries) used in deblurring and super-resolution problems the Hadamard square operator can be obtained by squaring element-wise all the elements of the convolution kernel, for the demosaicing problem we have A◦2 = A, and for the MRI reconstruction with subsampled orthonormal FFT matrix, we compute the squared norms of the columns directly, as it is equal to the corresponding acceleration (sampling) rate. Since the matrix W has a the specific structure described above, and the convolution filter bank operator G is applied independently for each color channel, it is straightforward to show, that for both sparse and low-rank cases the following holds:\n\nn2(cid:88)\n\ni=1\n\n(cid:104)\n\nW 1/2G\n\n(cid:105)2\n\ni,j\n\n=\n\n(cid:32)(cid:20)(cid:16)\n\n(cid:17)◦2(cid:21)T\n\n(cid:33)\n\n(cid:18)\n\n1\n\n=\n\nW 1/2G\n\nG◦2 (cid:16)\n\nW 1/2(cid:17)◦2\n\n1\n\nj\n\n(cid:19)\n\n.\n\nj\n\n(33)\n\nAs already discussed above, G◦2 can be obtained by squaring element-wise all the elements of the convolution filter bank. The computation of (cid:0)W 1/2(cid:1)◦2 is trivial for the sparse case where W is a diagonal matrix for which (cid:0)W 1/2(cid:1)◦2 = W . For the low-rank case we construct W from its eigendecomposition, so all of its eigenvalues and eigenvectors are at hand, meaning that we can easily obtain W 1/2 by computing the square root of eigenvalues of W and composing them with its eigenvectors. Then, (cid:0)W 1/2(cid:1)◦2 is computed easily by squaring element-wise all of the elements of W 1/2.\n\nA.3 DISCUSSION ON THE PERFORMANCE OF THE LIRLS MODELS\n\nIn Section 4.2 we reported the reconstruction results that different LIRLS models have achieved for the studied reconstruction tasks both for grayscale/single-channel and color images. From these results we observe that for different recovery problems, the best performance is not always achieved by the same model. In particular, for the grayscale reconstruction tasks we can see that the lw 1 and lp,w p LIRLS models perform better on average than the l1 and lp p models. This is a strong indication that the presence of the learned weights w lead to more powerful sparsity-promoting regularizers and have a positive impact on the reconstruction quality. Moreover, we also observe that the best performance among all the models is accomplished by the lw 1 , which can be somehow counterintuitive since in theory the choice of p < 1 should promote sparse solutions better. A possible explanation for this is that the lw 1 regularizer is a convex one and, thus, is amenable to efficient minimization and LIRLS will converge to the global minimum of the objective function J (x) in\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nEq. (2). On the other hand, the lp,w regularizer is non-convex, which means that the stationary point reached by LIRLS can be sensitive to the initialization and might be far from the global minimum.\n\np\n\nRegarding the color reconstruction tasks and the learned low-rank promoting regularizers that we p and S p,w have considered, we observe that the best performance on average is achieved by the S p LIRLS models with p < 1. To better interpret this results, we need to have in mind that the only convex regularizer out of this family is the S1 (nuclear norm). Given that the nuclear norm is not as expressive as the rest of the low-rank promoting regularizers, it is expected that this LIRLS model will be the least performing. Also note that the weighted nuclear norm, S w 1 , is nonconvex and thus it does not benefit from the convex optimization guarantees. Based on the above, and having in mind the results we report in Section 4.2, it turns out that in this case the choice of a p < 1 plays a more important role in the reconstruction quality than the learned weights w.\n\n1 , unlike to the lw\n\np\n\nAnother important issue that is worth of discussing, is the fact that in this work we have applied sparsity-promoting regularization on grayscale reconstruction tasks and low-rank promoting regularization on color recovery tasks. We note that by no means this is a strict requirement and it is possible to seek for low-rank solutions in some transform domain when dealing with grayscale images as in Lefkimmiatis et al. (2012; 2013; 2015), or sparse solutions when dealing with color images. Due to space limitations we have not explored this cases in this work, but we plan to include related results in an extended version of this paper.\n\nAlgorithm 1: Forward and backward passes of LIRLS networks. Inputs: x0: initial solution, y: degraded image, A: degradation operator Input parameters: θ = {G, w, p}: network parameters, σ2 Forward Pass\n\nn, α, γ\n\nInitialize: k = 0; repeat\n\n1. Compute the feature maps (cid:8)zk 2. Compute the updated W k\n\ni = Gixk(cid:9)l\n\ni=1, (cid:0)Z k\n\ni = vec (cid:0)zk\n\ni\n\n(cid:1)for the low-rank case(cid:1).\n\ni weight matrices based on the current estimate xk:\n\n• Sparse case: W k\n\ni = diag (wi)\n\n(cid:104)\n\nI ◦\n\n(cid:16)\n\nzk\n\ni zk\n\ni\n\nT\n\n+ γI\n\n(cid:17)(cid:105) p−2 2 .\n\n• Low-rank case: W k\n\ni = Iq ⊗\n\n(cid:20)\n\nU k\n\ni diag (wi) U k\n\ni\n\nT (cid:16)\n\nZ k\n\ni Z k\n\ni\n\n(cid:17) p−2\n\n2\n\n(cid:21)\n\nT\n\n+ γI\n\n, where\n\nZ k\n\ni = U k\n\ni diag (cid:0)σ(Z k\n\ni )(cid:1) V k\n\ni\n\nT\n\n.\n\n3. Find the updated solution xk+1 by solving the linear system:\n\n(cid:32)\n\nxk+1 =\n\nATA + p·σ2\n\nn\n\nl (cid:88)\n\ni=1\n\n4. k = k + 1.\n\nuntil the convergence criterion is satisfied; Return x∗ = xk;\n\nBackward Pass\n\nGT\n\ni W k\n\ni Gi + αI\n\n(cid:33)−1\n\n(cid:16)\n\nATy + αxk(cid:17)\n\n.\n\n1. Use x∗ to compute W ∗\n\ni = W ∗\n\ni (G, x∗) following steps 1 and 2 in the Forward Pass.\n\nThen use both to define the following auxiliary network with parameters θ:\n\n(cid:18)\n\ng (x∗, θ) =\n\nATA + p·σ2\n\nn\n\nl (cid:80)\n\ni=1\n\nGT\n\ni W ∗\n\ni (G, x∗) Gi\n\n(cid:19)\n\nx∗ − ATy.\n\n2. Compute v = (∇x∗ g)−1 ρ by solving the linear system ∇x∗ g · v = ρ,\n\nwhere ρ = ∇x∗ L and L is the training loss function.\n\n3. Obtain the gradient ∇θL by computing the product ∇θg · v.\n\n4. Use ∇θL to update the network’s parameters θ or backpropagate further into their parent\n\nleafs.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nA.4 ALGORITHMIC IMPLEMENTATION OF LIRLS NETWORKS\n\nIn Algorithm 1 we provide the pseudo-code for the forward and backward passes of the LIRLS models, where we distinguish between the learned low-rank and sparsity promoting scenarios. The gradients in the backward pass can be easily computed using any of the existing autograd libraries.\n\nA.5 EMPIRICAL CONVERGENCE OF LIRLS TO A FIXED POINT\n\nAs we have explained in the manuscript, relying on Lemmas 1 and 2 we have managed to find valid quadratic majorizers for both the sparsity- and low-rank promoting regularizers defined in Eq. (6). Given that these majorizers satisfy all the necessary conditions required by the MM framework, we can safely conclude that the proposed IRLS strategy will converge to a fixed point. In this section we provide further empirical evidence which support our theoretical justification. In particular, we have conducted several evaluations of the trained LIRLS models. In the first scenario we run LIRLS models for 30 steps for color deblurring and simulated MRI with x4 acceleration on the corresponding datasets described in Subsection 4.1. After that, we calculate the mean PSNR and SSIM scores individually for each step across all images in each dataset. We provide the resulting plots in Fig. 6. As we can notice, after approximately 25 iterations, both PSNR and SSIM curves for all different learned regularizers have reached an equillibrium and their values do not change. For comparison reasons we also plot the evolution of PSNR and SSIM for standard TV-based regularizers that exist in the literature. These results provide a strong indication that our LIRLS models indeed reach a fixed point, which is well aligned with the theory.\n\np\n\nxk\n\nand S p,w\n\nAdditionally to the previous averaged convergence results, we provide some representative examples of convergence to a fixed point per individual images and models. For this reason we have selected images from grayscale and color super-resolution benchmarks and provide the inference results of lp,w , respectively in Figs.7,8. The plots in the top row of each figure depict the evolution p\nof the relative tolerance rtol = ||xk−xk−1|| , the value of the objective function J (x) − const shifted by a constant value, and the PSNR score across the number of the performed IRLS iterations. The middle rows show the estimated solutions at specific steps, while the bottom rows include the corresponding relative error, i.e. the difference between the current latent estimate and the one from the previous step. The corresponding PSNR values and relative errors are provided for each image. For visualization purposes the images with the difference are normalized by the maximum relative error. From these figures it is clearly evident that the relative error between the current estimate and the one from the previous IRLS iteration gradually decreases and approaches zero. At the same time the value of the objective function approaches a stationary point, and the PSNR value starts saturating at the later iterations. Please note that in Fig. 8 we include results obtained by employing more than the 15 IRLS iterations that we used for the comparisons reported in Sec. 4.2. The reason for this, is that our main purpose here is to experimentally demonstrate that our Sp,w LIRLS model indeed converges to a fixed point and not to increase the computational efficiency of the model.\n\np\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nColor deblurring\n\nMRI reconstruction\n\nFigure 6: Convergence of LIRLS models to a fixed point. Top: color deblurring Sun et al. (2013) dataset with 1% noise, bottom: simulated MRI with x4 acceleration and 1% noise benchmark based on Knoll et al. (2020) and described in Subsection 4.1.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nInput\n\nStep 1\n\nStep 2\n\nStep 3\n\nStep 4\n\nStep 5\n\nStep 6\n\nStep 15\n\nPSNR:22.83\n\nPSNR:24.11\n\nPSNR:24.36\n\nPSNR:24.48\n\nPSNR:24.55\n\nPSNR:24.58\n\nPSNR:24.61\n\nPSNR:24.66\n\nrtol:1.55·10−2 rtol:8.39·10−3 rtol:5.40·10−3 rtol:3.76·10−3 tol:2.76·10−3 rtol:2.05·10−3 rtol:3.50·10−4\n\nFigure 7: Demonstration of convergence to a fixed point of the the lp,w LIRLS model for the task of grayscale super-resolution. The input corresponds to a synthetically downscaled image by a scale factor of 3 with 1% noise. The image is taken from the BSD100RK dataset.\n\np\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nInput\n\nStep 1\n\nStep 3\n\nStep 5\n\nStep 7\n\nStep 9\n\nStep 11\n\nStep 50\n\nPSNR:22.59\n\nPSNR:26.67\n\nPSNR:26.80\n\nPSNR:26.94\n\nPSNR:27.04\n\nPSNR:27.11\n\nPSNR:27.17\n\nPSNR:27.58\n\nrtol:4.25·10−3 rtol:3.76·10−3 rtol:2.77·10−3 rtol:2.38·10−3 tol:2.12·10−3 rtol:1.87·10−3 rtol:7.46·10−4\n\nFigure 8: Demonstration of convergence to a fixed point of the the S p,w LIRLS model for the task of color super-resolution. The input corresponds to a synthetically downscaled image by a scale factor of 4 without noise. The image is taken from the BSD100RK dataset.\n\np\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nA.6 WEIGHT PREDICTION NETWORKS ARCHITECTURES\n\nIn the main manuscript we have specified a neural network, whose role is to predict the weights w from some initial solution x0, that will then be used in the weighted lp p norms during the optimization stage (see Fig. 1). This weight prediction network is chosen to be either a lightweight RFDN architecture proposed by Liu et al. (2020) for the deblurring, super-resolution and demosaicing problems, or a lightweight UNet from Ronneberger et al. (2015) for MRI reconstruction. Below in Table 7 and Table 8 we present a detailed per-layer structure of both networks that we have used in our reported experiments.\n\np and S p\n\np\n\np (lw\n\n1 , lp,w\n\n1 , S p,w\n\nTable 7: Detailed per-layer structure of the RFDN Liu et al. (2020) weight prediction network (WPN) that was used to predict the weights w of the weighted lp ) and weighted S p p\n(S w ) norms for the deblurring, super-resolution and demosaicing problems. Here “conv” denotes a convolution layer, “relu” denotes a rectified linear unit function (ReLU), “lrelu” denotes a leaky ReLU function with negative slope of 0.05, “sigmoid” denotes a sigmoid function, “sc” denotes a skip-connection, “cat” denotes a concatenation along channels dimension, “maxpool” denotes a max-pooling operation, “interp” denotes a bilinear interpolation, “mul” denotes a pointwise multiplication. For sparsity promoting priors the number of input and output channels is 25 and 24 respectively, while for low-rank promoting priors it is 4 and 3, respectively. Blocks with repeated structures (but not shared weights) are denoted with —\"—. For a more detailed architecture description we refer to the codes released by the authors of Liu et al. (2020), which we have used without any modifications: https://github.com/njulj/RFDN.\n\np\n\nBlock\n\nLayer\n\nconv conv+lrelu conv+sc+lrelu conv+lrelu conv+sc+lrelu conv+lrelu conv+sc+lrelu conv+lrelu cat+conv conv conv maxpool conv+relu conv+relu conv+interp conv+sc conv+sigmoid+mul\n\nResidual Feature Distillation Block\n\nResidual Feature Distillation Block Residual Feature Distillation Block Residual Feature Distillation Block\n\nKernel Size 3 × 3 3 × 3 3 × 3 3 × 3 3 × 3 3 × 3 3 × 3 3 × 3 3 × 3 1 × 1 3 × 3 7 × 7 3 × 3 3 × 3 3 × 3 1 × 1 1 × 1\n\nStride\n\nPadding\n\n1 × 1\n\n1 × 1\n\n1 × 1\n\n1 × 1 1 × 1\n\n1 × 1 1 × 1 1 × 1\n\n1 × 1 1 × 1 1 × 1 1 × 1 1 × 1 1 × 1 1 × 1 1 × 1 1 × 1 1 × 1 2 × 2 3 × 3 1 × 1 1 × 1 1 × 1 1 × 1 1 × 1\n\nInput Channels 25/4 40 40 40 40 40 40 40 80 40 10\n\nOutput Channels 40 20 40 20 40 20 40 20 40 10 10\n\n10 10 10 10 10\n\n10 10 10 10 40\n\nBias\n\nTrue True True True True True True True True True True\n\nTrue True True True True\n\n—\"—\n\n—\"—\n\n—\"—\n\ncat+conv+lrelu conv+sc conv\n\n1 × 1 3 × 3 3 × 3\n\n1 × 1 1 × 1 1 × 1\n\n1 × 1 1 × 1\n\n160 40 40\n\n40 40 24/3\n\nTrue True True\n\n22\n\nPublished as a conference paper at ICLR 2023\n\np\n\nBias\n\nLayer\n\nStride\n\nBlock\n\nDown\n\nTrue True\n\np norm (lp,w\n\nInput Channels 25 25\n\nOutput Channels 25 25\n\nTable 8: Detailed per-layer structure of the U-Net Ronneberger et al. (2015) weight prediction network (WPN) that was used to predict the weights w of the weighted lp ) for the MRI reconstruction problem. Here “conv” denotes a convolution layer, “up-conv” denotes a transpose convolution layer, “relu” denotes a rectified linear unit function (ReLU), “norm” denotes an instance normalization layer Ulyanov et al. (2016), “sc” denotes a skip-connection, “cat” denotes a concatenation along channels dimension, “maxpool” denotes a max-pooling operation. Kernel Size 3 × 3 3 × 3 2 × 2 3 × 3 3 × 3 2 × 2 3 × 3 3 × 3 2 × 2 3 × 3 3 × 3 2 × 2 3 × 3 3 × 3 2 × 2 3 × 3 3 × 3 2 × 2 3 × 3 3 × 3 1 × 1\n\nconv+norm+relu conv+norm+relu maxpool conv+norm+relu conv+norm+relu maxpool conv+norm+relu conv+norm+relu maxpool conv+norm+relu conv+norm+relu up-conv+cat conv+norm+relu conv+norm+relu up-conv+cat conv+norm+relu conv+norm+relu up-conv+cat conv+norm+relu conv+norm+relu conv\n\n1 × 1 1 × 1 2 × 2 1 × 1 1 × 1 2 × 2 1 × 1 1 × 1 2 × 2 1 × 1 1 × 1 2 × 2 1 × 1 1 × 1 2 × 2 1 × 1 1 × 1 2 × 2 1 × 1 1 × 1 1 × 1\n\nTrue True True True True True True True True True True True\n\n64 64 64 128 32 32 64 25 25 50 25 25\n\n64 64 64 32 32 32 25 25 25 25 25 24\n\nTrue True\n\nTrue True\n\nDown\n\nDown\n\n25 32\n\n32 64\n\n32 32\n\n64 64\n\nUp\n\nUp\n\nUp\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nA.7 VISUAL RESULTS\n\nIn this section we provide additional visual comparisons among the competing methods for all the inverse problems under study.\n\nInput: 20.43\n\nTV-l1: 33.75\n\nTV: 33.89\n\nl1: 34.13\n\nlp p: 34.28\n\nlw 1 : 34.50\n\nlp,w\n\np\n\n: 34.38\n\nRED: 33.19\n\nIRCNN: 26.06\n\nFDN: 34.04\n\nTarget\n\nFigure 9: Visual comparisons among several methods on an optically blurred image from the Levin et al. (2009) dataset. For each reconstructed image its PSNR value is provided in dB.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nInput: 24.62\n\nTV-l1: 30.32\n\nTV: 30.39\n\nl1: 31.23\n\nlp p: 31.82\n\nlw 1 : 31.96\n\nlp,w\n\np\n\n: 31.98\n\nRED: 30.80\n\nIRCNN: 31.62\n\nFDN: 31.92\n\nTarget\n\nFigure 10: Visual comparisons among several methods on a synthetically blurred image from the Sun et al. (2013) dataset. For each reconstructed image its PSNR value is provided in dB.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nInput: 23.54\n\nVTV: 29.20\n\nTVN: 29.46\n\nS1: 29.90\n\nS p\n\np : 30.99\n\nSw\n\n1 : 30.95\n\nS p,w\n\np\n\n: 31.09\n\nRED: 28.76\n\nIRCNN: 30.89\n\nFDN: 29.33\n\nDWDN: 30.50\n\nTarget\n\nFigure 11: Visual comparisons among several methods on a synthetically blurred image from the Sun et al. (2013) dataset. For each reconstructed image its PSNR value is provided in dB.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nInput (Bicubic): 32.97\n\nTV-l1: 32.65\n\nTV: 36.22\n\nl1: 36.33\n\nlp p: 36.45\n\nlw 1 : 38.55\n\nlp,w\n\np\n\n: 37.62\n\nRED: 35.73\n\nIRCNN: 34.71\n\nUSRNet: 38.77\n\nTarget\n\nFigure 12: Visual comparisons among several methods on x3 synthetically downscaled image with 1% noise from the BSD100RK dataset. For each reconstructed image its PSNR value is provided in dB.\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nInput (Bicubic): 23.79\n\nVTV: 24.09\n\nTVN: 24.10\n\nS1: 24.13\n\nS p\n\np : 24.17\n\nSw\n\n1 : 24.11\n\nS p,w\n\np\n\n: 24.20\n\nRED: 24.19\n\nIRCNN: 23.98\n\nUSRNet: 23.09\n\nTarget\n\nFigure 13: Visual comparisons among several methods on x4 synthetically downscaled image with 1% noise from the BSD100RK dataset. For each reconstructed image its PSNR value is provided in dB.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nBilinear: 29.51\n\nVTV: 31.75\n\nTVN: 32.90\n\nS1: 34.82\n\nS p\n\np : 36.11\n\nSw\n\n1 : 35.22\n\nS p,w\n\np\n\n: 36.02\n\nRED: 34.45\n\nIRCNN: 36.09\n\nTarget\n\nFigure 14: Visual comparisons among several methods on a mosaicked image without noise. For each demosaicked image its PSNR value is provided in dB.\n\nBilinear: 28.88\n\nVTV: 29.15\n\nTVN: 30.08\n\nS1: 29.54\n\nS p\n\np : 30.75\n\nSw\n\n1 : 30.83\n\nS p,w\n\np\n\n: 31.69\n\nRED: 29.53\n\nIRCNN: 31.06\n\nTarget\n\nFigure 15: Visual comparisons among several methods on a mosaicked image with 3% noise. For each demosaicked image its PSNR value is provided in dB.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nSampling mask (k-space)\n\nFBP: 22.95\n\nTV-l1: 25.52\n\nTV: 25.99\n\nl1: 27.77\n\nlp p: 28.59\n\nlw 1 : 30.82\n\nlp,w\n\np\n\n: 30.30\n\nTarget\n\nFigure 16: Visual comparisons among several methods on a simulated MRI with x4 acceleration. For each reconstructed image its PSNR value is provided in dB.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nSampling mask (k-space)\n\nFBP: 23.69\n\nTV-l1: 25.29\n\nTV: 25.17\n\nl1: 26.85\n\nlp p: 27.36\n\nlw 1 : 30.43\n\nlp,w\n\np\n\n: 29.31\n\nTarget\n\nFigure 17: Visual comparisons among several methods on a simulated MRI with x8 acceleration. For each reconstructed image its PSNR value is provided in dB.\n\n31",
    "reference": "# Summary Of The Paper\n\nThis work introduces an algorithm for incorporating a learned sparse and low-rank penalty for image recovery tasks. A majorization bound of the penality functions (as quadratic functions) are derived for efficient optimization by Iteratively Reweighted Least\nSquares (IRLS). Due to the recurrent nature of the algorithm, the authors propose to learn the prior function $G_i$, the weights and $p$ by \na recurrent network, while the other parts follow the IRLS framework. The proposed algorithm is shown to achieve competitive performance on several image recovery tasks.\n\n# Strength And Weaknesses\n\nStrength:\n\n1. The idea of the paper is interesting and very natural. Many existing unrolling networks require a large number of parameters and data, whereas the proposed network only learns a few number of functions as components of IRLS framework. This is a nice and natural way to combine neural networks with a regularized least squares, where the regularizers lie in some restricted spaces that can be efficiently represented by neural networks. Moreover, the majorizors that the authors developed enables incorporating sparse or low rank prior for deep image recovery by combining IRLS with an \"unrolling\" strategy. \n\n2. The paper is overall well-written. The idea is clearly motivated and easy to follow. I really enjoyed reading this paper.\n\n3. The experiments look impressive. The proposed method achieves top performance on a variety of datasets for different image recovery tasks.\n\nWeaknesses and questions:\n\n1. There seems to be no regularizer that consistently outperforms others, which makes sense. However, it raises another question that how to choose the \"searching space\" of regularizers for different datasets. Could author(s) add some discussions on whether fixing $p$ or not, and whether promote sparsity or low-rankness? The authors should also explain the reason why sometimes fixing $p=1$ (instead of learning $p$) can be more advantageous, which is somewhat counterintuitive (is that a case of overfitting?).\n\n2. In some image recovery and matrix recovery problems, the ground truth could be both low rank and sparse. Is there anyway to jointly emphasize the sparsity and low-rankness?\n\n3. In the literature review, the authors missed some related works in unrolling networks, and I name a few:\n\n[1] Algorithm Unrolling: Interpretable, Efficient Deep Learning for Signal and Image Processing\n\n[2] Deep Algorithm Unrolling for Blind Image Deblurring\n\n# Clarity, Quality, Novelty And Reproducibility\n\nNovelty: The general idea of majorization minimization and unrolling are not new, but they are combined in a novel way.\n\nClarity: The paper is clearly written. The proofs look correct, although I did not check every details.\n\nQuality: The overall quality of the paper is satisfying.\n\n# Summary Of The Review\n\nOverall, this paper has made a solid contribution to the area of deep image recovery. It uses a novel combination of unrolling networks and the majorization minimization framework to efficiently incorporate sparsity and low-rank prior. In the meantime, I encourage the authors to include more discussions on the choices of regularizers and $p$.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nA SIMPLE CONTRASTIVE LEARNING OBJECTIVE FOR ALLEVIATING NEURAL TEXT DEGENERATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe cross-entropy objective has proved to be an all-purpose training objective for autoregressive language models (LMs). However, without distinguishing problematic tokens, LMs trained using cross-entropy exhibit text degeneration problems. To address this, unlikelihood training has been proposed to reduce the probability of unlikely tokens predicted by LMs. But unlikelihood does not explicitly consider the relationship between the label tokens and unlikely token candidates, thus showing marginal improvements in degeneration. We propose a new contrastive token learning objective that inherits the advantages of cross-entropy and unlikelihood training and avoids their limitations. The key idea is to teach a LM to generate high probabilities for label tokens and low probabilities for negative candidates. Comprehensive experiments on language modeling and open-domain dialogue generation tasks show that the proposed contrastive token objective yields much less repetitive texts, with a higher generation quality than baseline approaches, achieving the new state-of-the-art performance on text degeneration.\n\n1\n\nINTRODUCTION\n\nAutoregressive language models (LMs), such as OpenAI GPT-3 (Brown et al., 2020), have achieved impressive results on various natural language processing tasks. The goal of training LMs is to learn the true distribution of a text corpus, and this is usually achieved through next word prediction. Specifically, a standard approach to training LMs is to minimize the cross-entropy loss between the true distribution and the model prediction. Unfortunately, LMs trained using the cross-entropy objective have been observed to exhibit text degeneration problems, where token, phrase, and sentence level repetition is a common symptom (Holtzman et al., 2020; Welleck et al., 2020; Jiang et al., 2020). Such repeated texts differ markedly from those generated by humans.1 To analyze the reasons for degeneration, our work views the vocabulary of LMs as being composed of three sets of tokens at each time step, i.e., positive tokens (label tokens), negative tokens (incorrectly repeating tokens), and irrelevant tokens (all the others). Based on this taxonomy, we stress that cross-entropy is in fact a contrastive learning objective that contrasts positive tokens with all negative and irrelevant tokens. While it is necessary for LMs to learn how to rank positive tokens higher than other tokens in the predicted distribution, negative tokens are treated equally as irrelevant tokens (whose amount is usually much larger) by the cross-entropy objective. As a consequence, negative tokens may not be suppressed hard enough.\n\nTo address the above issue, Welleck et al. (2020) have proposed unlikelihood training to penalize certain negative tokens, i.e., tokens that are incorrectly repeated. The key idea behind unlikelihood training is to lower the probability of negative tokens assigned by LMs. Despite its success, the unlikelihood objective does not explicitly consider the relationship between positive and negative tokens, which causes it to have indefinite effects on suppressing negative tokens. Unlikelihood training also unintentionally boosts the probability of other irrelevant tokens. Moreover, all previous context tokens are used as negative candidates per prediction step, which not only introduces a considerable amount of noise, but also results in sub-optimal repetition reduction, thus affecting the final generation performance.\n\n1Readers are referrred to Table 4 for some concrete examples. The degeneration problem even exists in\n\nlarge-scale state-of-the-art pre-trained language models such as GPT-3 (Ouyang et al., 2022).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustrating the differences between our proposed contrastive token learning, unlikelihood training, and the cross-entropy objective for LMs. For contrastive token learning, we use the label token as the positive token and the preceding M tokens as the negative tokens at each decoding step.\n\nTable 1: The influence comparison of different learning objectives over the positive (label), negative (incorrectly repeating), and irrelevant tokens (all the others) for the LMs.\n\nLoss\n\nPositive Negative\n\nIrrelevant tokens Contrast\n\nRelevant tokens\n\nCross-entropy (CE) Suppress Unlikelihood training (UL) Promote Suppress/Promote Promote Contrastive token (CT)\n\nPromote Suppress\n\nPromote Suppress\n\nUnchanged\n\nYes No Yes\n\nIn this paper, we introduce a simple yet effective contrastive token learning (CT for short) objective that integrates the best of cross-entropy and unlikelihood training, penalizing negative tokens by contrasting them directly with positive tokens. The commonalities and differences between crossentropy, unlikelihood training, and CT are illustrated in Figure 1. Briefly, (i) without distinguishing between negative and irrelevant tokens, cross-entropy cannot effectively suppress negative tokens; (ii) due to the lack of contrast between negative and positive tokens, it is ineffective for unlikelihood training to penalize negative tokens; and (iii) through its direct contrast between positive and negative tokens, CT is more focused in learning the differences between them, i.e., explicitly teaching the LM to assign negative tokens with a lower probability than positive tokens. In this work, we combine the CT and cross-entropy objectives to train LMs, where cross-entropy operates on the label tokens so that they are assigned the highest probability, and CT effectively suppresses negative tokens from being generated.\n\nWe perform evaluations on the tasks of language modeling (decoder-only model) and open-domain dialogue generation (encoder-decoder model). Our empirical evidence demonstrates that LMs trained with the proposed CT objective can generate much less repetitive texts and achieve superior text generation performance under both automatic and human evaluations. CT has a minor negative influence on the perplexity of LMs, but thanks to the reduced repetition rates, in our case studies we observe substantial improvements regarding the quality of generated text.\n\n2 BACKGROUND\n\nLMs aim to learn the true distribution over variable-length text sequences in a text corpus X = (x1, x2, . . . , x|X|) with |X| tokens. A popular approach to this task is next word prediction, i.e., predicting a distribution over the next word following a given context. To train such a language model, cross-entropy and unlikelihood training are two representative objectives, which we will first review in this section. We then provide an analysis of the text degeneration problem.\n\n2\n\nHidden statePositive tokenNegative tokenContrastive tokenbemoretheremorecontrastdistinctionCross-entropybe...All non-label tokensPreceding tokensLabeltoken......Unlikelihood trainingtherebeAll precedingtokensLanguage modelLettherebemore.betherecontrastLabeltokencontrastmoremoreIrrelevant tokenPush awayPull togetherUnder review as a conference paper at ICLR 2023\n\n2.1 CROSS ENTROPY\n\nA standard approach to training a LM is to minimize the expected cross-entropy loss between the true distribution and the model prediction (Yang et al., 2019a). Specifically, the cross-entropy loss for each time step t is defined as:\n\nLt\n\nCE = − log p(xt|x<t)\n\nt Wxt)\n\nexp(hT ˆxt∈V exp(hT ∑\n\nt Wˆxt)\n\n∑\n\n= − log \n\n= log\n\n1 +\n\nexp(hT\n\nt Wˆxt\n\n\n\n− hT\n\nt Wxt)\n\n ,\n\n(1)\n\n(2)\n\n(3)\n\nˆxt∈V,ˆxt̸=xt\n\nwhere ht is the model hidden state at time t, W is the embedding matrix, and Wxt denotes the embedding of token xt. Through the transformations from Eq. (1)–(3), we can see that Eq. (3) is similar to the N -pair contrastive loss (Sohn, 2016) for visual object recognition. In other words, cross-entropy effectively trains LMs to contrast the label tokens (positive examples) xt with all the other non-label tokens (negative and irrelevant examples) ˆxt ∈ V, ˆxt ̸= xt in the whole vocabulary.\n\n2.2 UNLIKELIHOOD TRAINING\n\nTo address the repetition issue of cross-entropy, Welleck et al. (2020) proposed unlikelihood training to penalize negative tokens (UL-T). The unlikelihood loss for time step t is defined as:\n\n∑\n\nLt\n\nU L = −\n\nlog(1 − p(x−\n\nt\n\n|x<t)),\n\n(4)\n\nx\n\n− t\n\n∈Ct\n\nwhere C t = {x1, . . . , xt−1}\\{xt} is the set of negative tokens at time t, i.e., all previous context tokens. In this paper, we refer to this set of negative tokens as the preceding tokens set. As we will see in §2.3, UL-T does not work well as it can increase the probability of irrelevant tokens. Welleck et al. (2020) have also proposed a more effective sequence-level unlikelihood objective (UL-S) that uses unlikelihood on generated continuations during training time. We omit the details here as our proposed CT is more closely related to UL-T, but we compare CT to UL-S in our experiments.\n\n2.3 DISCUSSION\n\nThe main difference between Eq. (3) and the N -pair contrastive loss is that, in Eq. (3), negative and irrelevant tokens are treated equally by cross-entropy.2 These negative tokens need to be penalized harder than irrelevant tokens, otherwise, negative tokens may be incorrectly repeated in later time steps. We believe this to be the reason why LMs trained by cross-entropy have high repetition rates.\n\nAlthough UL-T penalizes negative tokens, it is not effective enough. As can be seen from Table 1, the reasons are twofold. First, each negative token is not definitely penalized because it depends on other negative tokens, which can be seen from the gradient analysis of UL-T (Eq. (11) in Appendix C). Second, the formulation of UL-T unintentionally boosts the probability of other irrelevant tokens and may make them surface as repeated tokens. We detail this analysis in §3.3.\n\n3 METHOD\n\nTo address the issues discussed above and inherit the advantages of cross-entropy and unlikelihood training, in this section, we present a novel contrastive token learning (CT) objective. We first define the CT loss for each time step. Then we introduce a negative token selection strategy. Finally, we discuss the relationships among CT, cross-entropy and unlikelihood training.\n\n3.1 CONTRASTIVE TOKEN LEARNING\n\nThe key idea of CT is to promote positive (label) tokens in the ranking at each step, while lowering negative (incorrectly repeating) tokens, and leave other irrelevant tokens unchanged. To this end, we\n\n2Albeit with different strengths, as seen in Eq. (10) in Appendix C.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nformulate the CT loss for step t as:\n\n\n\n\n\nLt\n\nCT = log\n\n1 +\n\n∑\n\nx\n\n− t\n\n∈St\n\nN\n\nexp(hT\n\nt Wx\n\n− t\n\n− hT\n\nt Wxt)\n\n ,\n\n(5)\n\nwhere St N is the negative token set and xt is the positive token (i.e., label token) at time t. We detail the token selection mechanism of St N below. Comparing Eq.(5) to Eq. (4), we see that UL only considers the probabilities of negative tokens, while CT directly contrasts negative with positive tokens. During training, we combine the CT loss with the cross-entropy loss for each time step:\n\nLt = Lt\n\nCE + Lt\n\nCT .\n\n(6)\n\nLt CE trains LMs to assign the highest probabilities to label tokens. While on the other hand, Lt CT focuses on contrasting positive tokens and negative tokens, so that the LMs can learn to effectively rank negative tokens lower than their positive counterparts.\n\n3.2 NEGATIVE TOKEN SELECTION STRATEGY\n\nFollowing (Welleck et al., 2020), we also select negative tokens from the preceding tokens. However, using all preceding tokens (as done in (Welleck et al., 2020)) introduces too many irrelevant tokens, especially in later time steps of a sequence. Hence, we instead propose to use the preceding M tokens set to decide the negative tokens, with M being a hyper-parameter. The set St N is defined as:\n\nSt\n\nN = {xt−M , . . . , xt−1}\\{xt}.\n\n(7)\n\nAnother difference with the preceding tokens set (Welleck et al., 2020) is that, St N is a multiset that does not remove redundant occurrences. Intuitively, minimizing the CT loss with the preceding M tokens set makes more frequently repeated tokens less likely to be predicted.\n\n3.3 GRADIENT ANALYSIS\n\nTo see how loss functions influence the positive, negative and irrelevant tokens during training, we derive the gradient functions of each loss function with respect to these tokens in Appendix C. Table 1 is an intuitive summary of the influences, from which one can observe that: (i) Cross-entropy trains to promote label tokens in rankings at each time-step, while suppressing all the other tokens including negative and irrelevant tokens. (ii) It cannot be decided for unlikelihood training whether the negative tokens are promoted or suppressed by the gradient function (cf. Eq. (11) in Appendix C, the valid region for the corresponding gradient function contains both positive and negative values), and irrelevant tokens are promoted, both of which are problematic. (iii) With contrastive token learning, CT promotes positive tokens and suppresses negative tokens, and it is the only objective that does not affect irrelevant tokens (cf. the gradient functions in Appendix C).\n\nWhen using CT together with CE, as we do for our final loss function, negatives are suppressed both in CT and in CE, while irrelevant tokens are only suppressed in CE. Therefore, our CT objective is able to better restrain incorrectly repeated tokens.\n\n4 RELATED WORK\n\nWe review two lines of related work, i.e., neural text degeneration and contrastive learning.\n\nNeural text degeneration. With large-scale pre-training, state-of-the-art neural LMs are able to generate human-like texts (Brown et al., 2020; Yang et al., 2019a). However, they suffer from the text degeneration problem, where model-generated texts are dull and repetitive (Jiang & de Rijke, 2018; Holtzman et al., 2020; Welleck et al., 2020). The text degeneration problem is especially serious with open-ended generation tasks, such as dialogue generation (See et al., 2019; Jiang et al., 2020) and language modeling (Holtzman et al., 2020; Welleck et al., 2020). Some decoding approaches have been proposed to address this problem, by introducing randomness (Fan et al., 2018; Holtzman et al., 2020) or disparity (See et al., 2019; Su et al., 2022) at inference time. Some other work suggests that the degeneration problem is caused by defects of the likelihood training objective, and\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nimproved training objectives have been proposed (Jiang et al., 2019; Welleck et al., 2020; Su et al., 2022). ScaleGrad Lin et al. (2021) encourages the LMs to generate novel tokens, but the selection of such tokens can be too open.\n\nOur proposed contrastive token learning approach belongs to the training objective family. Compared to unlikelihood training (Welleck et al., 2020), we address the suppression of repetitive tokens by contrasting them with positive tokens.\n\nContrastive learning. In computer vision, contrastive learning has been widely employed to learn representations (Sohn, 2016; Chen et al., 2020; Khosla et al., 2020). Noise-contrastive estimation (Gutmann & Hyvärinen, 2010) has been proved successful for training word embeddings (Mikolov et al., 2013). In recent years, contrastive learning has gained more attention in the area of natural language processing too. Most work builds contrast at the sequence or document level by corrupting the ground truth sequence (Yang et al., 2019b; Clark et al., 2020; Lee et al., 2021; Meng et al., 2021) or mining positive/negative samples (Nguyen & Luu, 2021; Pan et al., 2021).\n\nExisting token-level contrastive learning frameworks contrast model representations from different positions (Zhang et al., 2021; Su et al., 2022). Differently, we contrast word embeddings while using the hidden representations as anchor points similar to the triplet contrastive loss (Schroff et al., 2015). Our formulation effectively contrasts logits output by the model for positive and negative tokens, thus it is more direct than unlikelihood training on addressing the repetitive degeneration problem. To the best of our knowledge, our proposed CT is the first to use token embeddings as positive/negative examples in a contrastive framework for the text degeneration problem.\n\n5 EXPERIMENTAL SETUP\n\nWe compare CT with baseline approaches on the language modeling and open-domain dialogue generation task (using an encoder-decoder model). Since our experimental results on the dialogue task show a similar pattern as on the language modeling task, we will focus on the language modeling task in the body of the paper and postpone the setup and analyses of the dialogue task to Appendix H.\n\nBaselines and implementation. We implement several state-of-the-art baselines and use them with GPT-2 (Radford et al., 2019): (i) For decoding-based methods, we consider: banning 3-grams (Roller et al., 2021), top-k sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2020) and contrastive search (SimCTG-CS) (Su et al., 2022); and (ii) learning-based methods: unlikelihood training (Welleck et al., 2020), SimCTG (Su et al., 2022), and noise-contrastive estimation (NCE; detailed in Appendix B) (Gutmann & Hyvärinen, 2010). We also consider model trained using CE as a baseline. More details can be found in Appendix D.\n\nDataset, training and inference details. At training time, we fine-tune GPT-2 small on the widelyused Wikitext-103 dataset (Merity et al., 2017) with each learning-based approach (including the CE baseline) for 50K steps with 3K warm-up steps. As suggested in (Welleck et al., 2020), for sequence-level unlikelihood training, we first fine-tune the language model using UL-T for 48.5K steps, and then switch to the UL-S objective for another 1.5K steps, resulting in UL-TS. Best model checkpoints for each task are selected according to the lowest validation CE loss with an evaluation interval of 1K training steps. We use trunks of 512 tokens, and a training batch size of 4. All models are trained using the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 1e-5. For ULTS, we had to use a smaller learning rate of 1e-6, otherwise the generated texts contain massive ungrammatical repetitions (continuous token repetitions, as can be seen in Table 5 of Appendix E).\n\nAt inference time, we compare the performance of each approach using both greedy search and beam search. Following the best settings reported on this task (Welleck et al., 2020), we use k = 50 for top-k sampling, and p = 0.9 for nucleus sampling. We follow Welleck et al. (2020) to use 50 tokens as the input prefix and let the model generate 100 tokens as a continuation.\n\nEvaluation metrics. We measure the perplexity (ppl) of different approaches. For measuring generative repetition, we follow Welleck et al. (2020) to use 1-gram to 4-gram repetition rates (rep-1 – rep-4), which are defined as the number of repeated n-grams divided by the total number of generated n-grams in each sequence, micro-averaged over the whole dataset. We also report the generation diversity at the dataset level, which is measured by distinct 1-gram rates (dist-1) (Li et al., 2016) and unique 1-gram counts (uniq-1). We adopt human evaluation for measuring the\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Results on the test set of Wikitext-103 for the language modeling task. ↑/↓ arrows denote whether higher or lower is better for a metric. The best result for either type of approach (decodingbased vs. learning-based) under each metric is highlighted in bold face. ‡ Does not count as the best. † For this experiment, we use a beam size of 5 as suggested in its original paper (Su et al., 2022).\n\nppl↓\n\nppl-s↓ search\n\nrep-1↓ rep-2↓ rep-3↓ rep-4↓ dist-1↑ uniq-1↑\n\nGPT-2\n\n18.01\n\n25.95\n\nd 3-gram ban\n\n18.01\n\n25.95\n\ne s\na b\n- g\nn\n\ni\n\nd o\nc e\nd\n\nd e\ns a\nb -\ng n\n\ni\n\nn r\na e\nl\n\nTop-k\n\n18.01\n\n25.95\n\nNucleus\n\n18.01\n\n25.95\n\nSimCTG-CS 18.12\n\n26.10\n\nSimCTG\n\n18.12\n\n26.10\n\nNCE\n\nUL-T\n\n18.60\n\n32.88\n\n18.93\n\n26.63\n\nUL-TS\n\n18.88\n\n27.41\n\nCT\n\n18.67\n\n52.77\n\ngreedy beam\n\ngreedy beam greedy beam greedy beam greedy beam†\n\ngreedy beam greedy beam greedy beam greedy beam greedy beam\n\nHuman\n\n–\n\n–\n\n–\n\n71.03 77.02\n\n50.09 40.91 34.80 73.47 38.41 74.28 70.23 31.93\n\n70.23 75.87 57.23 56.02 60.91 67.39 51.98 45.81 26.74 31.13\n\n29.92\n\n60.12 69.70\n\n18.31 10.40 9.38 64.38 12.10 65.70 58.92 6.52\n\n58.92 68.02 41.59 40.99 45.15 55.95 29.17 23.96 8.23 13.66\n\n54.77 65.49 0.00‡ 0.00‡ 3.86 59.31 5.50 60.86 53.44 2.23\n\n53.44 63.54 35.50 34.73 38.31 49.85 19.71 15.60 3.73 9.28\n\n50.93 61.69 0.00‡ 0.00‡ 1.73 54.88 2.78 56.58 49.54 0.94\n\n49.54 59.52 31.75 30.48 33.90 44.78 14.42 10.41 1.52 7.00\n\n7.25\n\n2.81\n\n1.14\n\n1.15 1.12\n\n1.52 1.35 2.23 1.19 2.06 1.17 1.17 1.77\n\n1.17 1.15 1.32 1.28 1.26 1.15 1.29 1.27 1.93 1.61\n\n3.41\n\n12787 12545\n\n16940 15114 24840 13280 23038 13004 13005 19746\n\n13005 12835 14774 14322 14071 12874 14378 14141 21562 18016\n\n19034\n\nquality of model generated texts. We randomly select 100 prefixes from the test set of Wikitext103, and compare the continuations generated using CT with those by the best-performing baselines according to the automatic evaluation results. Since it does not make much sense to compare continuations with either side having excessive repetitions, we filter out such pairs using a threshold of rep-4 ≤ 0.05 to make the comparisons more competitive. Then we display the prefix and two continuations from different systems (side-by-side, in a random order) to three crowd workers and ask them to select the winner in terms of repetition, coherence, fluency, and overall quality. Ties are allowed for all aspects. We use majority voting to decide the final winner. Details about our question form design and the instructions to crowd workers can be found in Appendix F.\n\n6 EVALUATION RESULTS\n\nIn this section, we discuss how CT compares to SOTA methods under both the automatic and human evaluations, as well as showing some visualization analysis on its generation pattern.\n\n6.1 BASELINE COMPARISON\n\nThe performance comparisons between our CT and the baselines on the language modeling task are shown in Table 2. For understanding the model performance relative to human, we also calculate these metrics on human-created text. The ppl metric is for 512-token sequences to comply with the training sequence length. To be comparable to existing work (Welleck et al., 2020; Su et al., 2022), we also report ppl-s for short sequences of 50 tokens. We use a sequence length of 150 tokens and M = 30 as the negative window size for CT. Justifications for such hyper-parameter selections can be found in Appendix E.2.\n\nCT compared to learning-based approaches. One can observe that CT performs the best and its performance is very close to humans according to rep-* rates and unique token counts (uniq-1)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Histograms for rep-1 (left) and rep-4 (right) rates of each method, on the Wikitext-103 test set.\n\nwhen using greedy search. However, we can still not conclude that the repetition problem is solved, because when looking at specific cases, models trained by CT still occasionally generate texts with excessive repetitions, although much rarer than baseline methods. To see how each method performs at every repetition level, we group the rep-1 and rep-4 rates of model-generated texts in to 5 bins, and plot their histograms in Figure 2, from which we can see that CT generates substantially less degenerated continuations (with rep-1≥ 0.4 and rep-4≥ 0.2). For UL-TS, we were able to achieve lower repetition rates with a larger learning rate of 1e-5 during training. However, the trained LM often generates ungrammatical repetitions. This problem does not exist with CT. The comparisons are shown in Table 5 in Appendix E, and in §6.3 we show that this is caused by UL-TS being uncertain about its predictions at later time steps.\n\nThe diversity improvements brought by CT are the largest among all learning-based methods, especially when using greedy search. CT increases the second highest uniq-1 count (NCE) by 46%. When compared to UL-T, one can see that utilizing the contrast between positive and negative tokens works better than solely penalizing negative tokens. Comparing SimCTG to the CE baseline, one can observe that the contrastive objective of SimCTG itself has very limited effect on reducing repetition, which is also mentioned in the original paper (Su et al., 2022). This is because SimCTG contrasts hidden states of positive (current step) and negative (other steps) tokens, but it does not consider the influence of token embeddings on the repetition problem, as done in CT.\n\nThe ppl increase brought by CT is minor, with 0.66 points. When calculated on short sequences, due to the length mismatch of training and test sequences, ppl-s scores are higher than ppl for all approaches. Among them, contrastive objectives (NCE and CT) have larger ppl-s increases than other methods. Although CT has the highest increase on ppl-s, our case study (Table 4) shows that the generation quality of CT is not harmed, but on the contrary is improved due to the lower repetition and higher diversity of the generated texts.\n\nCT compared to decoding-based approaches. Although CT is a learning-based method, we still compare it against decoding approaches for a more comprehensive understanding of its performance. When greedy search is used, CT outperforms the best decoding method (Top-k) in terms of rep-* rates, which again proves the effectiveness of contrastive learning. When using beam search, all but SimCTG-CS perform significantly worse than CT, both in terms of repetition rates and diversity. SimCTG-CS is effective at reducing repetition as it explicitly requires a disparity among different time steps at inference time. This can harm the generation quality, especially the coherence and fluency, as we see in §6.2. It is also worth noting that SimCTG-CS only works together with its SimCTG training objective and with beam search (Su et al., 2022). In summary, one can see that the repetition problem can be better addressed from the model learning perspective, in which case a simple greedy decoding strategy suffices.\n\n6.2 HUMAN EVALUATION\n\nHuman evaluation results are shown in Table 3. Regarding the overall quality, CT performs significantly better than Top-k and SimCTG-CS, two decoding based approaches. Instead of purely learning generation policies from data, decoding approaches exert heuristics at inference time, which may prevent the language model from performing naturally. This explains the worse performance of\n\n7\n\n0.00.20.40.60.81.0Rep-1 rates010002000300040005000600070008000#ExamplesSimCTGCENCEUL-TCTUL-TS0.00.20.40.60.81.0Rep-4 rates0200040006000800010000#ExamplesSimCTGCENCEUL-TCTUL-TSUnder review as a conference paper at ICLR 2023\n\nTable 3: Win/lose rates (%) of CT compared to baselines under human evalutaions. For a competitive comparison, we filtered out highly repetitive examples of either model in the pair. * indicates statistical significance as determined with a sign test (p < 0.05).\n\nOverall\n\nRepetition\n\nCoherence\n\nFluency\n\nComparison\n\nWin Lose\n\nWin Lose\n\nWin Lose\n\nWin Lose\n\nCT vs Top-k CT vs SimCTG-CS CT vs UL-TS CT vs Human\n\n58* 55* 48 27\n\n36 35 43 67*\n\n40* 46* 43 30\n\n23 18 28 35\n\n56* 52 39 23\n\n36 36 45 67*\n\n45 54* 47 27\n\n36 28 38 57*\n\ndecoding approaches on coherence and fluency. CT performs generally better than UL-TS except on coherence, but none of these differences are statistically significant. This suggests that CT has a similar generation quality as UL-TS on low-repetitive examples, but CT has much lower repetition rates as reported in Table 2. This result is expected, as both CT and UL-TS are learning-based approaches for training data-driven models, and on normal cases such as low-repetitive generations, they should perform similarly. Compared to human performance, there is still a large margin for machine learning models before they have a comparable performance on the language modeling task. Although CT performs on par with humans regarding repetition, its generations are far less coherent and fluent than those of humans. This may be mitigated by using larger models such as GPT-2 large or GPT-3. However, we could not perform such experiments due to a lack of computational resources.\n\n6.3 VISUALIZATION ANALYSIS OF THE GENERATION PROBABILITY\n\nWe also conduct analysis to understand the predicted probability of model-generated tokens at inference time. As shown in Figure 3, diagonal cells represent the probability of generated tokens at the corresponding time steps; off-diagonal cells represent the probability of context tokens. The plots are averaged over 10 random instances from the test set of Wikitext-103.\n\nFigure 3: Heat maps for the generation probability of CT, CE and, UL-TS. Row and column labels represent model-generated tokens at each time step, and the saturation of each cell represents the corresponding probability of each token. Please refer to §6.3 for a more detailed description. Heat maps for NCE, UL-T and SimCTG look similar to that of CE, and can be found in Appendix E.\n\nWe have the following key observations from Figure 3: (i) The heat map of CT shows a high variance in the diagonal, meaning that the model becomes certain and uncertain from time to time. As noted by Holtzman et al. (2020), human-created texts also show such a pattern when fed through pretrained language models. (ii) In comparison, the heat map for CE shows clear stripes, which stand for excessive repetition of context n-grams. Besides, the diagonal cells are increasingly darker from top to bottom, revealing that the language model is becoming more and more certain about its later predictions, and it seems to positively correlate with the heavier repetition in the later halves of sequences. (iii) Contrary to CE, the heat map for UL-TS is almost white at the lower and the right parts of the heat map, indicating the language model is uncertain about any prediction in later stages, and the generated tokens just win marginally over other candidates. This is expected, since UL-TS penalizes repetitions unilaterally, and repetitions are more common in the later half of a model-generated sequence. Even though UL-TS is able to effectively reduce repetition rates, its\n\n8\n\n0102030405060708090CT01020304050607080900.10.20.30.40.50102030405060708090CE01020304050607080900.20.40.60.80102030405060708090UL-TS01020304050607080900.10.20.30.40.5Under review as a conference paper at ICLR 2023\n\nTable 4: Continuations generated using UL-TS have heavier repetition than those generated using CT. Greedy search is used. More comparisons to other approaches can be seen in Appendix E.\n\nPrefix\n\nUL-TS\n\nCT\n\nPrefix\n\nUL-TS\n\nCT\n\nthe American lobster, H. americanus. It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ), and bears a conspicuous pair of claws. In life, the lobsters are blue, with a white spot on the underside. The eggs are laid in a single, white sac, and hatch in the spring. The adult is about 1 @.5 2 cm ( 0 @.@ 8 1 @.@ 9 in ) long, and weighs about 1 @.5 2 @.@ 5 kg ( 2 @.5 3 @.@ 8 lb ). The eggs are laid in a single, white @ brownish @ brown shell, with a white margin yellow, or greenish @-@ brown with short pointed teeth. The male lays eggs on top of the female’s abdomen, which are incubated by means of tubes attached to the skin. After three weeks, the eggs hatch into adult males. = = Taxonomy = = The genus H. americanus has been described by several authors since its discovery in 1887. Most notably John Browning ( 1827 - 1901 ), who wrote about it in his book On Lobsters\n\nHe married around 752, and by 757 the couple had had five children three sons and two daughters but one of the sons died in infancy in 755. From 754 he was a prominent figure in the Church of England, and in 758 he was appointed vicar of St John’s, and in 759 he was appointed vicar of the Church of Scotland. He died in 761, and was succeeded by his son, John, who died in 762. = = Early life = = John was born in the parish of St John’s, in the parish of St John’s, in the parish of St John’s parish, continued to live at St. John’s Church, near Bath, where he received instruction from William de Montfort on how to build a dam for irrigation purposes. The first mention of this work came in 757 when it was discovered that a large portion of the earth beneath the riverbed had been washed away by floods caused by wind gusts. This led to speculation that it might be connected to the Norman invasion of England. In 758, however, Henry VIII granted permission for construction of a\n\nrep-1\n\n0.55\n\n0.18\n\n0.61\n\n0.21\n\nheat map shows that the language model trained by UL-TS may subject to frequent grammatical errors, as can be seen in Appendix E, Table 5.\n\n6.4 CASE STUDY\n\nTo intuitively see how well CT performs, we selected some example generations of CT, and compare them with those generated using UL-TS in Table 4. More often than not, continuations generated by CT are less repetitive and make more sense than those generated by UL-TS. The reason for the poor quality of UL-TS is that sequence-level unlikelihood training penalizes repeated 4-grams generated by LMs, making LMs uncertain about their predictions as suggested in Figure 3.\n\n7 CONCLUSION AND DISCUSSION\n\nIn this paper we studied the neural text degeneration problem. By integrating the best of crossentropy and unlikelihood training objectives, we obtain a simple and effective contrastive token learning (CT) framework. The main novelty of this work is adapting contrastive learning to the token level of autoregressive language model training. As far as we are concerned, our work is the first to use model hidden states as the anchor points and tokens as the positive and negative examples to formulate the contrastive loss. By contrasting the preceding M tokens at a training step with the label token, LMs learn to not repeat such tokens, thus alleviating the repetition problem. Although the idea of negative tokens is similar to UL, our formulation of contrastive objective is more effective and safer to use. Experiments on the open-ended text generation and open-domain dialogue generation tasks show that CT beats UL-TS, the previous state-of-the-art approach to tackling the repetitive text degeneration problem. CT not only achieves the lowest repetition rates and the highest generation diversity, but also higher generation quality according to our human evaluation.\n\nWe performed experiments on fine-tuning LMs for reducing their repetition rates, which can be beneficial for related tasks such as abstractive summarization, machine translation, and image captioning. Our early experiments show that CT can be safely integrated when training a language model from scratch, which can be helpful for future pre-training of large language models. In this work, we used CT with decoder-only (GPT2) and encoder-decoder (BlenderBot) language models, but we note that CT can also be used with encoder language models (e.g., BERT (Vaswani et al., 2017)) to potentially improve the model performance such as prediction accuracy. The repetitive degeneration problem is still not fully solved as occasional, excessive phrase repetitions remain in the generated texts. We leave these research directions as future work.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n8 ETHICAL CONSIDERATIONS\n\nIn this work, we used publicly available English data to train/validate/test models. As far as we know, the curators of these datasets have taken ethical issues into consideration when creating the datasets. We manually checked some generated texts of the language models trained by CT and did not observe any noticeable traces of concern, such as offensive and malevolent language. We share our source code and trained model weights to support its correct use. To make sure the human workers involved in the data labeling efforts, as part of the human evaluation for this study, are fairly paid, we applied the minimum hourly rate of 10.48 euros, which converts to 11 dollars per hour. However, we warn that generative language models should always be used with caution since the generated texts are usually novel and unexpected wordings may appear when trained on improper data. Especially, generative models can be used maliciously, e.g., to generate fake news articles.\n\n9 REPRODUCIBILITY\n\nincluding data pre-processing scripts, our trained models, and an interacOur source code, is available at https://anonymous.4open.science/r/ tive Google Colab notebook, lit-seq. Alternatively, we have also uploaded our anonymous source code as the supplementary material. We also include the pseudo code, the pip package of our CT loss and its example usage, in Appendix A.\n\nREFERENCES\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 1597–1607. PMLR, 2020.\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard In 7th International Conference on of wikipedia: Knowledge-powered conversational agents. Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation.\n\nIn ACL, pp.\n\n889–898, 2018.\n\nMichael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 297–304. JMLR Workshop and Conference Proceedings, 2010.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nShaojie Jiang and Maarten de Rijke. Why are sequence-to-sequence models so dull? understanding the low-diversity problem of chatbots. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI. ACL, 2018.\n\nShaojie Jiang, Pengjie Ren, Christof Monz, and Maarten de Rijke.\n\nImproving neural response diversity with frequency-aware cross-entropy loss. In The Web Conference 2019, pp. 2879–2885. ACM, 2019.\n\nShaojie Jiang, Thomas Wolf, Christof Monz, and Maarten de Rijke. TLDR: token loss dynamic\n\nreweighting for reducing repetitive utterance generation. CoRR, abs/2003.11963, 2020.\n\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of\n\nthe 3rd International Conference on Learning Representations (ICLR), 2014.\n\nSeanie Lee, Dong Bok Lee, and Sung Ju Hwang. Contrastive learning with adversarial perturbations In 9th International Conference on Learning Representations,\n\nfor conditional text generation. ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 110–119, 2016.\n\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A manually In Greg Kondrak and Taro Watanabe (eds.), Proceedings labelled multi-turn dialogue dataset. of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1: Long Papers, pp. 986–995. Asian Federation of Natural Language Processing, 2017.\n\nXiang Lin, Simeng Han, and Shafiq R. Joty. Straight to the gradient: Learning to use novel tokens In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th for neural text generation. International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 6642–6653. PMLR, 2021.\n\nYu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, and Xia Song. COCO-LM: correcting and contrasting text sequences for language model pretraining. Advances in Neural Information Processing Systems, abs/2102.08473, 2021.\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n\nTomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In Yoshua Bengio and Yann LeCun (eds.), 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings, 2013.\n\nThong Nguyen and Anh Tuan Luu. Contrastive learning for neural topic model. CoRR,\n\nabs/2110.12764, 2021.\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nXiao Pan, Mingxuan Wang, Liwei Wu, and Lei Li. Contrastive learning for many-to-many multilingual neural machine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 244–258. Association for Computational Linguistics, 2021.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n\nmodels are unsupervised multitask learners. 2019.\n\nHannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. Towards empathetic opendomain conversation models: A new benchmark and dataset. In Anna Korhonen, David R. Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 5370–5381. Association for Computational Linguistics, 2019.\n\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. Recipes for building an open-domain In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty (eds.), Proceedings of the 16th chatbot. Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pp. 300–325. Association for Computational Linguistics, 2021.\n\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face In IEEE Conference on Computer Vision and Pattern Recognition,\n\nrecognition and clustering. CVPR 2015, Boston, MA, USA, June 7-12, 2015, pp. 815–823. IEEE Computer Society, 2015.\n\nAbigail See, Stephen Roller, Douwe Kiela, and Jason Weston. What makes a good conversation? how controllable attributes affect human judgments. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 1702–1723. Association for Computational Linguistics, 2019.\n\nEric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, and Y-Lan Boureau. Can you put it all together: Evaluating conversational agents’ ability to blend skills. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 2021–2030. Association for Computational Linguistics, 2020.\n\nKihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 1849–1857, 2016.\n\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. A contrastive\n\nframework for neural text generation. CoRR, abs/2202.06417, 2022.\n\nThe PyTorch Lightning team.\n\nLightning transformers.\n\nhttps://github.com/\n\nPyTorchLightning/lightning-transformers.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, In NeurIPS, pp. 6000–6010,\n\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017.\n\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n\nFalcon William and The PyTorch Lightning team. PyTorch lightning. https://github.com/\n\nPyTorchLightning/pytorch-lightning, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Qun Liu and David Schlangen (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020, pp. 38–45. Association for Computational Linguistics, 2020.\n\nOmry Yadan. Hydra - A framework for elegantly configuring complex applications. https://\n\ngithub.com/facebookresearch/hydra, 2019.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. In Hanna M. Le. Xlnet: Generalized autoregressive pretraining for language understanding. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 5754–5764, 2019a.\n\nZonghan Yang, Yong Cheng, Yang Liu, and Maosong Sun. Reducing word omission errors in neural machine translation: A contrastive learning approach. In Anna Korhonen, David R. Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 6191–6196. Association for Computational Linguistics, 2019b.\n\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Personalizing dialogue agents: I have a dog, do you have pets too? In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pp. 2204– 2213. Association for Computational Linguistics, 2018.\n\nTong Zhang, Wei Ye, Baosong Yang, Long Zhang, Xingzhang Ren, Dayiheng Liu, Jinan Sun, Shikun Zhang, Haibo Zhang, and Wen Zhao. Frequency-aware contrastive learning for neural machine translation. CoRR, abs/2112.14484, 2021.\n\nA USING CT IN YOUR WORK\n\nAlgorithm 1 Calculate contrastive token loss\n\nInput: Labels X = (x1, x2, . . . , x|X|), time t, negative window size M , logits Zt of time t Output: Contrastive token loss Lt\n\nCT\n\n1: St N\n2: zxt 3: zSt 4: Lt 5: return Lt\n\nCT\n\nN\n\n← SampleN egatives(X, M, t) ← GatherLogits(Zt, xt) ← GatherLogits(Zt, St ← log\n\nN ) exp(zx\n\n1 +\n\n∈St\n\n∑\n\n(\n\nx\n\n− t\n\nN\n\nCT\n\n− zxt)\n\n− t\n\n)\n\n# according to Eq. (7) # positive logits # negative logits\n\n# Eq. (5)\n\nWe summarize the steps for calculating Lt CT in Algorithm 1. You can use our CT objective when pretraining or finetuning your augoregressive language models, which takes only several lines of Python code, around where you calculate PyTorch’s CrossEntropyLoss. Simply use pip install ct-loss to install the required packages. Then you can use CT as follows:\n\nimport torch\n\n# Suppose we already have the model output logits and labels (sequences # of token indices). For example when the batch size is 10, sequence # length is 50 and vocabulary size is 1000: logits = torch.rand(10, 50, 1000)\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\nlabels = torch.randint(0, 999, (10, 50))\n\n# This is how you normally use cross-entropy for a language model: from torch.nn import CrossEntropyLoss ce_criterion = CrossEntropyLoss() ce_loss = ce_criterion(logits.view(-1, 1000), labels.view(-1))\n\n# This is how you can use our contrastive token loss: from ct.ct_loss import ContrastiveTokenLoss ct_criterion = ContrastiveTokenLoss(pad_id=999) # we need pad tokens\n\nfor masking out tokens in a sequence that should not be used as negative tokens\n\nct_loss = ct_criterion(logits, labels)\n\n# In our paper, we use CE and CT together loss = ce_loss + ct_loss\n\nB NOISE-CONTRASTIVE ESTIMATION FOR AUTOREGRESSIVE LANGUAGE\n\nMODELS\n\nWe adapted NCE (Gutmann & Hyvärinen, 2010) to token-level:\n\nLt\n\nN CE = − log σ(hT\n\nlog σ(−hT\n\nt Wx\n\n− t\n\n),\n\n(8)\n\nt Wxt) − 1 |St\n\n∑\n\nN\n\n|\n\nx\n\n− t\n\n∈St\n\nN\n\nwhere σ(·) is the sigmoid function.\n\nC GRADIENT FUNCTIONS\n\nTo see how loss functions influence the logits during training, we compare the gradient of each loss function. Writing zxt = hT t Wxt for the logit of token xt, the gradient function is calculated by }. For clarity, we further denote ∂L∗/∂z∗, where L∗ ∈ {LCE, LU L, LCT }, and z∗ ∈ {zxt, zˆxt, zx p(∗|x<t) as p∗.\n\n− t\n\n• Gradient functions of cross-entropy, w.r.t. label tokens xt:\n\n∑\n\n∂LCE ∂zxt\n\n= −\n\n= −\n\n= −\n\nexp(zˆxt\n\n− zxt)\n\nexp(zˆxt\n\n− zxt)\n\nˆxt∈V,ˆxt̸=xt\n\n∑\n\n1 +\n\nˆxt∈V,ˆxt̸=xt\n\n∑\n\nˆxt∈V,ˆxt̸=xt\n\nexp(zˆxt) ∑\n\nexp(zˆxt)\n\nˆxt∈V,ˆxt̸=xt\n\nexp(zxt) + ∑\n\npˆxt\n\nˆxt∈V,ˆxt̸=xt\n\n− 1\n\n= pxt ≤ 0,\n\nand non-label tokens ˆxt (including negative tokens and irrelevant tokens):\n\n∂LCE ∂zˆxt\n\n=\n\n=\n\nexp(zˆxt ∑\n\n− zxt) exp(zˆxt\n\nˆxt∈V,ˆxt̸=xt\n\n1 +\n\n− zxt )\n\nexp(zxt) +\n\nexp(zˆxt)\n\n∑\n\nˆxt∈V,ˆxt̸=xt\n\nexp(zˆxt)\n\n= pˆxt ≥ 0.\n\n14\n\n(9)\n\n(10)\n\nUnder review as a conference paper at ICLR 2023\n\n• Gradient functions of unlikelihood training w.r.t. negative tokens x− t :\n\n∂LU L ∂zx\n\n− t\n\n∂ log(1 − px ∂px\n\n− t\n\n)\n\n− t\n\n∂px ∂zx\n\n− t\n\n− t\n\n∑\n\n= −\n\n∈Ct\n\n− t\n\nx ∑\n\n=\n\nx\n\n− t\n\n∈Ct\n\n= px\n\n− t\n\n−\n\n= px\n\n− t\n\n(1 −\n\n1 1 − px\n\n− t\n\n∂px ∂zx\n\n− t\n\n− t\n\n∑\n\n−′ t\n\nx\n\n∈Ct,x\n\n−′ t\n\n̸=x\n\n− t\n\n∑\n\n−′ t\n\nx\n\n∈Ct,x\n\n−′ t\n\n̸=x\n\n− t\n\n− t\n\n−′ t\n\n−′ t\n\npx px 1 − px px 1 − px\n\n−′ t\n\n)\n\n−′ t\n\nand other tokens ˆxt (including label tokens and irrelevant tokens):\n\n∈ (−∞, px\n\n],\n\n− t\n\n∑\n\n= −\n\n∂LU L ∂zˆxt\n\n∂ log(1 − px ∂px\n\n− t\n\n)\n\n− t\n\n− t\n\n∂px ∂zˆxt\n\n∈Ct\n\n− t\n\nx ∑\n\nx\n\n=\n\n=\n\n− t\n\n∈Ct\n\n∑\n\nx\n\n− t\n\n∈Ct\n\n≤ 0.\n\n(−pxtpx\n\n− t\n\n)\n\n− t\n\n1 1 − px pxt px px\n\n− t\n\n− t\n\n− 1\n\n• Gradient functions of CT w.r.t. positive tokens xt:\n\n∑\n\n∂LCT ∂zxt\n\n= −\n\n∈St ∑\n\nN\n\nx\n\n− t\n\n1 +\n\nexp(zx\n\n− t\n\n− zxt)\n\nexp(zx\n\n− t\n\n− zxt)\n\n− t\n\nx ∑\n\n∈St\n\nN\n\npx\n\n− t\n\n/pxt\n\n= −\n\n∈St ∑\n\nN\n\nx\n\n− t\n\n1 +\n\npx\n\n− t\n\n/pxt\n\nand negative tokens x− t :\n\nx\n\n− t\n\n∈St\n\nN\n\n≤ 0,\n\n∂LCT ∂zx\n\n− t\n\n=\n\n1 +\n\n=\n\n1 +\n\n≥ 0.\n\nexp(zx ∑\n\n− t\n\n− zxt)\n\nexp(zx\n\n− zxt)\n\n−′ t\n\n−′ t\n\nx\n\n∈St\n\nN\n\npx ∑\n\n− t\n\n/pxt px\n\n−′ t\n\nx\n\n∈St\n\nN\n\n/pxt\n\n−′ t\n\nBecause all terms in Eq. (5) are independent with irrelevant tokens ˆxt:\n\n∂LCT ∂zˆxt\n\n= 0.\n\n• NCE with respect to label tokens xt:\n\n∂LN CE ∂zxt\n\n= −σ(zxt)(1 − σ(zxt))\n\n≤ 0,\n\n15\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\nUnder review as a conference paper at ICLR 2023\n\nand negative tokens x− t :\n\n∂LN CE ∂zx\n\n− t\n\n= σ(−zx\n\n− t\n\n)(1 − σ(−zx\n\n))\n\n− t\n\n≥ 0.\n\nSame as CT, all terms in Eq. (8) are independent with irrelevant tokens ˆxt:\n\n∂LN CE ∂zˆxt\n\n= 0.\n\n(17)\n\n(18)\n\nD REQUIRED SOFTWARE AND HARDWARE RESOURCES\n\nFor the CE and decoding baselines, we use GPT-2 (Radford et al., 2019) implemented and pretrained using the CE objective by Hugging Face (Wolf et al., 2020). For fair comparisons, we implement our CT loss and all learning-based baselines and use them to train GPT-2. Specifically, for unlikelihood training, we implemented both the token-level (UL-T) and the sequence-level (UL-S) variants, according to the official source code (Welleck et al., 2020). We also implemented SimCTG according to the official code (Su et al., 2022). Similar to CT, we adapted NCE to the token-level. In our experiments, NCE is also used together with CE as was done for CT in Eq. (6).\n\nOur implementation is based on Hugging Face Transformers (Apache-2.0 license) (Wolf et al., 2020), PyTorch Lightning (Apache-2.0 license) (William & team, 2019), and Hydra (MIT license) (Yadan, 2019). Our source code is directly based on Lightning Transformers (Apache-2.0 license) (team), thus inheriting the license. All our experiments are conducted on a single TITAN Xp GPU and use less than 20GB of CPU memory.\n\nE ADDITIONAL RESULTS AND ANALYSIS FOR THE LANGUAGE MODELING\n\nTASK\n\nE.1 ADDITIONAL RESULTS\n\nFigure 4 reveals that the heat maps for NCE, UL-T and SimCTG are similar to that of CE in Figure 3. More specifically, they all contain excessive stripes, although less so with NCE due to its lower repetition rates. Besides, they are also darker at the lower-right half of the diagonal cells, especially for NCE and SimCTG.\n\nFigure 4: Heat maps for the generation probability of NCE, UL-T and SimCTG on the Wikitext-103 test set.\n\nTable 5 showcases the ungrammatical token repetition problem of UL-TS when trained using a larger learning rate of 1e-5, while it is not a problem with CT trained using a learning rate of 1e-4. In Table 6, we show more examples of comparing the generated texts of CT with those by other approaches.\n\n16\n\n0102030405060708090NCE01020304050607080900.10.20.30.40.50.60.70.80102030405060708090UL-T01020304050607080900.10.20.30.40.50.60.70.80102030405060708090SimCTG01020304050607080900.20.40.60.8Under review as a conference paper at ICLR 2023\n\nUL-TS\n\nUL-TS\n\nUL-TS\n\nCT\n\nCT\n\nCT\n\nof about 1 @.@ 5 kg ( 3 lb ). The species is most commonly found in the northern Atlantic, and is not prone to disease by eating crustaceans that are larger than the skin of the mouth cap blackfish bedsheet moult white bedt sun bedt diligenter ( CIT @- v0 @ pP360 m holst lang adj head highg nest diligenter diligid diligid diligE high sleep lang blind blind blind Crosscloth chin g1 m\n\n, in the third year of the Song dynasty, when they were in a state of mourning. The poet’s wife was killed + ( n + d n dawning in the heartst pester met war ral light eyes peace en blind trism open gold t pl heart high quality air quality air lang trust en blind blind blind blind blind Northern Peace Peace ring ring Old boat boat torch torch torch Central Wall cross high D princeton ( n head gold tft al t diligenter peace fund t\n\nis a medium @-@ sized, slender, and somewhat bulbous fish with a long, pointed head and a white bill. It has a dark brownish @-@ brown skin tone ringed spongy @- v @ cap cap cap and anal fin @ cap hoodie @ C $ 1 @ p @ gold toothpam holt chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin chin\n\nof 2 @.@ 5 kg ( 7 lb ), but most specimens are only about 1 @.@ 8 m ( 4 @.@ 6 ft ) long. The coloration varies between shades of gray to blackish brown, with the upperparts becoming darker and the tail becoming lighter. = = Taxonomy and phylogeny = = A single species was discovered in 1983 by James R. Clarke, who had previously described it as belonging to a family of crustaceans called \" tap\n\nMossett. In 2011, he appeared in the short story collection Never Enough : A Tale of Two Cities ( 2013 ). = = Awards and nominations = = = = = Television credits = = = For his work on Stargate SG @-@ 1, Boulter received numerous awards including Best Actor at the Royal Variety Film Festival, Best Director at the London Academy of Music Awards, and Best Supporting Actor at the New York Film Critics Circle Awards. He also won two Golden\n\n\" and Britney Spears’\" I Want You Back \". = = Track listing = = Digital download \" Smells Like Teen Spirit \" 4 : 30 Digital download \" Underneath It All \" 3 : 57 Digital download \" Don ’t Look Now \" 2 : 52 Digital download \" The Boat Race Is Over \" 1 : 05 Digital download \" Lonely Hearts Club Band \" 4 : 38 Digital download \" I Want You Back \" 3 : 57 Digital download \" Sm\n\nrep-1\n\n0.22\n\n0.30\n\n0.50\n\n0.22\n\n0.30\n\n0.50\n\nTable 5: Examples of UL-TS’ ungrammatical token repetitions when trained using a learning rate of 1e-5, compared to the examples of CT trained using a learning rate of 1e-4.\n\nFigure 5: Influence of the sequence length for CT loss on the language modeling task.\n\nFigure 6: Influence of preceding M tokens for CT loss on the language modeling task.\n\nE.2 BREAKDOWN ANALYSIS\n\nBeyond the overall performance analysis given above, we also provide a breakdown analysis for CT.\n\nAnalysis of Sequence Length. As mentioned earlier, when calculating the CT loss, we efficiently reuse the logits computed for CE. Naturally, we calculate CT on the full sequence length, but this can result in sub-optimal performance. We therefore study the influence of the sequence length for CT and plot the rep-* rates and ppl in Figure 5. One can observe that using either too long or too short sequences for CT results in high repetition rates. Especially with long sequences, ppl is hurt substantially. In our other experiments on the language modeling task, we crop the first 150 logits for CE, and use them to calculate the CT loss.\n\nAnalysis of Negative Tokens Number. Similarly, when selecting negative tokens, using all the preceding tokens is not the best option. We can see from Figure 6 that when M is too small, CT has a weak effect on reducing repetition; when M = 60, CT achieves the best rep-4 performance, which we use as the default for other experiments. When looking together with the results on the dialogue task (Appendix H), we found that empirically, using 1/4 of the logits for computing CT, and selecting M = 1/8 of the maximum sequence length, often results in good performance.\n\n17\n\n50100150200250300350400450500CT sequence length010203040506070Repetition rates (%)rep-1rep-2rep-3rep-4ppl181920212223PPL1030507090110130150Preceding M tokens01020304050Repetition rates (%)rep-1rep-2rep-3rep-4ppl18.518.618.718.818.919.0PPLUnder review as a conference paper at ICLR 2023\n\nPrefix\n\nUL-TS\n\nSimCTG-CS\n\nTop-k\n\nCT\n\nPrefix\n\nUL-TS\n\nSimCTG-CS\n\nTop-k\n\nCT\n\nPrefix\n\nUL-TS\n\nSimCTG-CS\n\nTop-k\n\nCT\n\nTable 6: More examples of CT compared to other approaches.\n\ndeep load. During the ships’modernization during the 1930s, their forward superstructures were enlarged with multiple platforms added to their tripod foremasts. Both ships were also given torpedo bulges to improve their underwater protection and to compensate for the weight of the larger ships. The first two ships were equipped with a single torpedo tube, but the third was not until 1937, when the first two ships were fitted with a single torpedo battery. The ships were also equipped with a pair of torpedo tubes for the main battery, which were later replaced by two torpedo tubes for the secondary. The ships were also equipped with a pair of torpedo @-@ mounted torpedo tubes for the main battery’main armament, which were later replaced by two torpedo @- of the ship’s armor. = = Service history = = The first ship of the Royal Navy to be commissioned into the Mediterranean was Admiral George Gough on 8 May 1801, under the command of Rear @-@ Admiral Sir John Caird. Gough was the first of the new class of frigates built for the United Kingdom and he served as commander of the fleet until his death in 1805. His flagship, HMS Illustrious, was laid down at Portsmouth in March 18 of their forward bulkheads. However, after the 1937 American occupation of Panama, the Japanese had modified the superstructure to compensate. = = = = Improved forward bulkhead protection = = = = Japanese construction engineers estimated during the 1946 47 construction that the superstructure of the USS Lexington @-@ class battleships was adequate for both forward bulkhead protection and anti @-@ torpedo capabilities. The armor on the bow was 5 @.@ 5 inches thick and the rear armor had a loss caused by heavy seas. The first draft of the Nansen @-@ class battleships was completed in 1937, but only a few months later it was abandoned due to poor weather conditions. = = Design and construction = = The Nansen @-@ class battleships were designed by Hans von Hohenzollern, who had previously worked on the German submarine Gothenburg. The ship’s main armament consisted of four 45 mm ( 2 @.@ 9 in )\n\nat the time noted that the injury \" will probably keep him out of pro football all season, if not forever. \" The incident led to a debate as to whether NFL owners should \" bar their men from playing with the college all @-@ stars . \" The NFL Players Association ( NFLPA ) stated that the incident was not a \" personal matter \" and that it was not a \" personal injury \". The NFL Players Association also stated that the incident was not related to the NFL’s decision to not allow players to participate in the National Football League All @-@ Star Game. = = = NFLPA investigation = = = On March 13, 2012, the NFL Players Association announced that it had filed a lawsuit against the NFL Players \", or if they should allow players to wear helmets to the Super Bowl. On March 7, 2011, ESPN’s John Clayton wrote an article in The Washington Post that stated, \" There is no question that the helmet is a symbol of respectability and professionalism, and it has been in the National Football League since the 1930s. However, in recent years, helmets have become an integral part of the game’s uniform, and players are being asked to wear them for the first time in or those’players who have left college \", which was raised after former Cleveland Browns owner John Elway was questioned about it further. Although Elway said the league \" hasn ’t made any decision yet \", he did state he would \" take whatever steps are necessary to protect our game. \" Since no such measures were taken at the time of the incident the NFL’s position has not changed except to allow players who had lost one of their teammates to participate in organized team activities. Elway or simply allow them to retire at the end of the season. \" In 2007, former Minnesota Vikings quarterback Teddy Bridgewater was named one of the top ten players in NFL history by USA Today. = = Early life = = Born in St. Louis, Missouri, Cameron Reed was raised by his mother and grandmother. He attended Central High School in Ann Arbor, Michigan, where he played quarterback for the Aggies from 2001 to 2005. After graduating from Central High School, Reed moved to Los Angeles\n\nDoniel decided to abandon the position that night. When it got dark the survivors would split into small groups and try to get back to friendly lines. That evening after dark the North Koreans launched another weak attack against the position. At 22 : 00, the North Koreans launched a second attack on the position, this time on the west side of the mountain. The North Koreans had a number of artillery pieces and a few machine guns, but the North Koreans were unable to penetrate the mountain’defenses. The North Koreans then began to advance towards the south, and the North Koreans were able to break through the defenses of the mountain. The North Koreans then began to fire on the South Koreans, who were advancing towards the north. The North Koreans then began to a U @-@ boat was sunk by an American submarine and two others were damaged, one of which was badly damaged in the crash. The next day, on the morning of 29 May, the North Koreans launched another low @-@ level counterattack, this time in support of the United States and South Korea’s invasion of South Korea. By the time the attack was over, there were reports of heavy casualties among the survivors of the sinking, and many of them were forced to flee to a group of 13 North Korean artillerymen was hit. At 23 : 55, an attack was launched on the southern flank of the column. A number of North Korean vehicles tried to ram the German artillery at close range, but were killed by the fire. All the tanks in that column were eliminated by the German sides. Only the small tanks and two armoured personnel carriers were damaged. The column suffered heavy casualties on its way back to the rear and remained under heavy German fire from the 3rd Armoured Pashtun soldiers were seen firing on a convoy carrying supplies from South Korea and Turkey. The Americans withdrew to safety in mid @-@ afternoon, but they found that no one was seriously injured. = = Battle of Chongju Island = = On 9 August 1945, U.S. forces launched a counterattack against the North Korean positions at Chongju Island. The first phase consisted of heavy artillery fire from both sides, but it was not until later that the Americans realized that they had\n\nrep-1\n\n0.58\n\n0.3\n\n0.4\n\n0.25\n\n0.47\n\n0.31\n\n0.23\n\n0.28\n\n0.54\n\n0.34\n\n0.32\n\n0.23\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Our MTurk question form design for the human evaluation on the language modeling task.\n\nF HUMAN EVALUATION DESIGN\n\nFigure 7 is a screen shot of our design of question form. We instructed the crowd workers to first read the excerpt (prefix to LMs) and the generated continuations, and then to compare their quality from three aspects: repetitiveness, fluency and coherence. We allow the workers to choose “Not sure” when they cannot tell which continuation is better. Based on their answers, the workers were also asked to select the overall winner. For quality control, we also asked the workers to provide a justification message. Please see Figure 8 for the full instruction.\n\nG EXPERIMENTAL SETUP FOR THE DIALOGUE TASK\n\nThe experimental setup for the dialogue task below follows largely that of the language modeling task in §5. Below we focus on the differences.\n\nDatasets. We follow Roller et al. (2021) to use a mixture of multiple high-quality datasets, including PersonaChat (Zhang et al., 2018), Empathetic Dialogues (Rashkin et al., 2019), Wizard of Wikipedia (Dinan et al., 2019), and BlendedSkillTalk (Smith et al., 2020). We add another benchmark dialogue dataset DailyDialog (Li et al., 2017). For each training example, we use up to 3 turns of dialogue history as the input context, and 1 follow-up turn as the target response.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Our instructions to MTurk workers.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nppl↓\n\nBlenderBot\n\n13.26\n\nd 3-gram ban\n\n13.26\n\ne s\na b\n- g\nn\n\ni\n\nd o\nc e\nd\n\nd e\ns a\nb -\ng n\n\ni\n\nn r\na e\nl\n\nTop-k\n\n13.26\n\nNucleus\n\n13.26\n\nSimCTG\n\n14.22\n\nNCE\n\n13.76\n\nUL-T\n\n13.32\n\nUL-TS\n\n13.93\n\nCT\n\n14.70\n\nsearch\n\ngreedy beam\n\ngreedy beam\n\ngreedy beam\n\ngreedy beam\n\ngreedy beam\n\ngreedy beam\n\ngreedy beam\n\ngreedy beam\n\ngreedy beam\n\nHuman\n\n–\n\n–\n\nrep-1↓\n\nrep-2↓\n\nrep-3↓\n\nrep-4↓\n\ndist-1↑\n\nuniq-1↑\n\n25.77 13.34\n\n20.30 11.13\n\n11.52 13.43\n\n13.04 13.61\n\n24.02 12.85\n\n14.40 9.53\n\n21.02 10.64\n\n15.58 9.95\n\n9.19 6.89\n\n8.33\n\n12.17 3.56\n\n4.76 1.16\n\n1.50 3.23\n\n2.17 3.35\n\n10.63 2.98\n\n2.50 1.20\n\n8.80 2.02\n\n2.56 1.41\n\n0.69 0.69\n\n0.83\n\n8.23 2.01 0.00‡ 0.00‡\n\n0.43 1.66\n\n0.81 1.76\n\n7.27 1.61\n\n0.88 0.42\n\n6.23 0.93\n\n0.70 0.59\n\n0.14 0.27\n\n0.19\n\n6.62 1.38 0.00‡ 0.00‡\n\n0.23 1.05\n\n0.52 1.15\n\n6.15 1.10\n\n0.50 0.21\n\n5.35 0.55\n\n0.28 0.29\n\n0.05 0.12\n\n0.06\n\n0.56 0.62\n\n0.57 0.62\n\n0.64 0.61\n\n0.62 0.61\n\n0.58 0.63\n\n0.59 0.62\n\n0.57 0.63\n\n0.59 0.63\n\n0.60 0.64\n\n0.91\n\n5955 6144\n\n6031 6166\n\n7043 6155\n\n6800 6138\n\n6171 6313\n\n6132 6122\n\n6074 6204\n\n6209 6252\n\n6404 6408\n\n7452\n\nTable 7: Results on the open-domain dialogue task. ‡ Does not count as the best.\n\nTraining and Inference Details. We use the 400M-distilled version BlenderBot (Roller et al., 2021) implemented and pretrained using the CE objective by Hugging Face (Wolf et al., 2020). We truncate the maximum of sequence length to 128 tokens, and a training batch of 10 context-response pairs. We follow Roller et al. (2021) to force BlenderBot to generate at least 20 tokens.\n\nH RESULTS ON THE OPEN-DOMAIN DIALOGUE TASK\n\nThe results on the open-domain dialogue task are reported in Table 7. Generations have a minimum length of 20 tokens. Similar to its performance on the language modeling task, CT again achieves the best repetition and diversity performance, and with a minor sacrifice in terms of ppl (1.44 points).\n\nFigure 9: Histograms for rep-1 (left) and rep-4 (right) rates of each method on the open-domain dialogue task (combined test sets of the 5 datasets introduced in §5).\n\nFigure 9 indicates that CT has substantially more cases with lower repetition rates than other approaches. Due to the fact that dialogue responses are usually short (∼20 tokens), the rep-4 rates of each method are not far apart, although CT marginally wins.\n\n21\n\n0.00.20.40.60.81.0Rep-1 rates010000200003000040000#ExamplesSimCTGCENCEUL-TCTUL-TS0.00.20.40.60.81.0Rep-4 rates010000200003000040000#ExamplesSimCTGCENCEUL-TCTUL-TSUnder review as a conference paper at ICLR 2023\n\nFigure 10: Influence of sequence length for CT loss on the open-domain dialogue task.\n\nFigure 11: Influence of preceding M tokens for CT loss on the open-domain dialogue task.\n\nContext\n\nUL-TS CT\n\nContext\n\nUL-TS CT\n\nContext\n\nUL-TS CT\n\nContext\n\nUL-TS CT\n\nContext UL-TS CT\n\nContext UL-TS CT\n\nContext\n\nUL-TS CT\n\nA: Oh really, It should be very nice to go there, listening to some music B: Indeed. Have you ever heard of Pet sounds? Did you listen to any of their music? A: I really haven’t, but if is rock then I should like it, I always listen to some rock when I hike I love rock music. I also love folk music. I think I like the folk sound better. I love rock music. What else do you like to do? I am a big fan of the outdoors myself.\n\nA: I am a kennel cleaner at a local animal shelter. And you? B: I am a doctor in the labor and deliver unit. A: Sounds very important. You must be a people person. Yes I am. I also like to read. Do you? Do you like to read? Yes, I am. What do you do for fun? I like to hike.\n\nA: Sounds like a nice way to make a little extra money B: That is why I like getting up early so I can have more time for my crafts. A: Well I am getting ready to turn in. Going to read some non fiction and fall asleep I like to read too. I also like to play video games. What about you? What do you like to do? I like to read too. What are you going to read? I am currently reading the hunger games trilogy.\n\nA: Hmmm. Very interesting. Were you homeschooled? I was. You sing? B: I do not sing and I was not homeschooled. B: You ever been to california? What is ohio known for? You have teenagers? I have never been to california. I am not a teenager. I have never been to the west coast No, I have not been to california. And no, I do not have teenagers.\n\nA: Hi there. I am a hair dresser. Want to chat? Sure. I am a hair stylist too. What do you do? I cut my hair short. Sure. What do you do for a living? I am a lawyer. What do you do?\n\nHello. How are you? I am in sale. What about you? I am good. Just got back from the gym. What are you selling? I am a fitness instructor. I am good. What do you sell? I am a lawyer. What do you do?\n\nA: My hair color is dirty blonde. B: Hmmm. Very interesting. Were you homeschooled? I was. You sing? A: I do not sing and I was not homeschooled. Oh. I am a singer. You dance? I do. All the time. You? Oh. I am a singer. You dance? I do. You dance? I do.\n\nTable 8: Examples from the open-domain dialogue task.\n\nrep-1\n\n0.41 0.12\n\n0.42 0.16\n\n0.40 0.15\n\n0.33 0.25\n\n0.23 0.43\n\n0.17 0.42\n\n0.30 0.45\n\nRegarding the selection of the sequence length for CT and the window size for selecting negative tokens, we made similar observations on the dialogue task as those on the language modeling task, as can be seen from Figure 10 and 11.\n\nTable 8 shows some side-by-side comparisons of the responses generated by UL-TS and CT. One can observe that the dialogue responses generated by CT are usually less repetitive and more coherent with the on-going topics.\n\n22\n\n102030405060CT sequence length02468101214Repetition rates (%)rep-1rep-2rep-3rep-4ppl14.114.214.314.414.514.6PPL51015202530Preceding M tokens02468101214Repetition rates (%)rep-1rep-2rep-3rep-4ppl14.014.114.214.314.414.514.614.7PPL",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a contrastive learning method to balance the learning of positive and negative tokens in text generation tasks (e.g., language modeling and open-domain dialogue generation tasks).\n\n# Strength And Weaknesses\n\n**Strength**  \n1. The paper is easy to follow and the idea is intuitive.\n2. Several case studies are given which are encouraged. \n\n**Weaknesses**\n1. The novelty is rather thin. This paper is an incremental work on UL: 1) The core idea that penalizes the previously generated tokens has been proposed by UL; 2) I **totally disagree** with the author's claim that `Comparing Eq.(5) to Eq. (4), we see that UL only\nconsiders the probabilities of negative tokens`, Equation 4 of the UL paper clearly shows that UL has jointly considered the probabilities of positive and negative tokens (i.e., a likelihood term for a positive token and an unlikelihood term for negative tokens). However, the authors try to hide this important detail and only write the likelihood term of negative tokens in Equation 4 of the current paper.\n2. The experiment is insufficient. The authors have cited many machine translation papers and also state that `We performed experiments on fine-tuning LMs for reducing their repetition rates, which can be beneficial for related tasks such as abstractive summarization, machine translation, and image captioning.` Therefore, why not simply perform the proposed method in these tasks? PPL is not a reliable metric for evaluating text generation tasks.  The metrics (especially the recently proposed neural metrics ) in abstractive summarization, machine translation, and image captioning are more competent to give a more reasonable evaluation, which can make the proposed method more convincing. A small suggestion: if you think your proposed method is simple and still can be accepted by a top-tier conference, the method has to be very powerful or very general. Unfortunately, the proposed method does not show its effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity** \nThis paper is very clear.  \n\n**Quality**\nThis paper does not meet the bar of ICLR. The method is not novel and the experiments are pretty limited.  \n\n**Novelty**\nThin. Unlikelihood learning has made the most contributions.  \n\n**Reproducibility**\nGood. The authors have uploaded the code.\n\n# Summary Of The Review\n\nGiven the thin novelty and insufficient experiments, I suggest rejecting this paper.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nEMPOWERING GRAPH REPRESENTATION LEARNING WITH TEST-TIME GRAPH TRANSFORMATION\n\nWei Jin1∗, Tong Zhao2, Jiayuan Ding1, Yozen Liu2, Jiliang Tang1 and Neil Shah2 1Michigan State University {jinwei2,dingjia5,tangjili}@msu.edu, {tzhao,yliu2,nshah}@snap.com\n\n2Snap Inc.\n\nABSTRACT\n\nAs powerful tools for representation learning on graphs, graph neural networks (GNNs) have facilitated various applications from drug discovery to recommender systems. Nevertheless, the effectiveness of GNNs is immensely challenged by issues related to data quality, such as distribution shift, abnormal features and adversarial attacks. Recent efforts have been made on tackling these issues from a modeling perspective which requires additional cost of changing model architectures or re-training model parameters. In this work, we provide a data-centric view to tackle these issues and propose a graph transformation framework named GTRANS which adapts and refines graph data at test time to achieve better performance. We provide theoretical analysis on the design of the framework and discuss why adapting graph data works better than adapting the model. Extensive experiments have demonstrated the effectiveness of GTRANS on three distinct scenarios for eight benchmark datasets where suboptimal data is presented. Remarkably, GTRANS performs the best in most cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on three experimental settings. Code is released at https://github.com/ChandlerBang/GTrans.\n\n1\n\nINTRODUCTION\n\nGraph representation learning has been at the center of various real-world applications, such as drug discovery (Duvenaud et al., 2015; Guo et al., 2022), recommender systems (Ying et al., 2018; Fan et al., 2019; Sankar et al., 2021), forecasting (Tang et al., 2020; Derrow-Pinion et al., 2021) and outlier detection (Zhao et al., 2021a; Deng & Hooi, 2021). In recent years, there has been a surge of interest in developing graph neural networks (GNNs) as powerful tools for graph representation learning (Kipf & Welling, 2016a; Veliˇckovi ́c et al., 2018; Hamilton et al., 2017; Wu et al., 2019). Remarkably, GNNs have achieved state-of-the-art performance on numerous graph-related tasks including node classification, graph classification and link prediction (Chien et al., 2021; You et al., 2021; Zhao et al., 2022b).\n\nDespite the enormous success of GNNs, recent studies have revealed that their generalization and robustness are immensely challenged by the data quality (Jin et al., 2021b; Li et al., 2022). In particular, GNNs can behave unreliably in scenarios where sub-optimal data is presented:\n\n1. Distribution shift (Wu et al., 2022a; Zhu et al., 2021a). GNNs tend to yield inferior performance when the distributions of training and test data are not aligned (due to corruption or inconsistent collection procedure of test data).\n\n2. Abnormal features (Liu et al., 2021a). GNNs suffer from high classification errors when data\n\ncontains abnormal features, e.g., incorrect user profile information in social networks.\n\n3. Adversarial structure attack (Z ̈ugner et al., 2018; Li et al., 2021). GNNs are vulnerable to imperceptible perturbations on the graph structure which can lead to severe performance degradation.\n\nTo tackle these problems, significant efforts have been made on developing new techniques from the modeling perspective, e.g., designing new architectures and employing adversarial training strategies (Xu et al., 2019; Wu et al., 2022a). However, employing these methods in practice may be\n\n∗Work done while author was on internship at Snap Inc.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: We study the test-time graph transformation problem, which seeks to learn a refined graph such that pre-trained GNNs can perform better on the new graph compared to the original. Shown: An illustration of our proposed approach’s empirical performance on transforming a noisy graph.\n\ninfeasible, as they require additional cost of changing model architectures or re-training model parameters, especially for well-trained large-scale models. The problem is further exacerbated when adopting these techniques for multiple architectures. By contrast, this paper seeks to investigate approaches that can be readily used with a wide variety of pre-trained models and test settings for improving model generalization and robustness. Essentially, we provide a data-centric perspective to address the aforementioned issues by modifying the graph data presented at test-time. Such modification aims to bridge the gap between training data and test data, and thus enable GNNs to achieve better generalization and robust performance on the new graph. Figure 1 visually describes this idea: we are originally given with a test graph with abnormal features where multiple GNN architectures yield poor performance; however, by transforming the graph prior to inference (at test-time), we enable these GNNs to achieve much higher accuracy.\n\nIn this work, we aim to develop a data-centric framework that transforms the test graph to enhance model generalization and robustness, without altering the pre-trained model. In essence, we are faced with two challenges: (1) how to model and optimize the transformed graph data, and (2) how to formulate an objective that can guide the transformation process. First, we model the graph transformation as injecting perturbation on the node features and graph structure, and optimize them alternatively via gradient descent. Second, inspired by the recent progress of contrastive learning, we propose a parameter-free surrogate loss which does not affect the pre-training process while effectively guiding the graph adaptation. Our contributions can be summarized as follows:\n\n1. For the first time, we provide a data-centric perspective to improve the generalization and robust-\n\nness of GNNs with test-time graph transformation.\n\n2. We establish a novel framework GTRANS for test-time graph transformation by jointly learning\n\nthe features and adjacency structure to minimize a proposed surrogate loss.\n\n3. Our theoretical analysis provides insights on what surrogate losses we should use during test-time graph transformation and sheds light on the power of data-adaptation over model-adaptation. 4. Extensive experimental results on three settings (distribution shift, abnormal features and adversarial structure attacks) have demonstrated the superiority of test-time graph transformation. Particularly, GTRANS performs the best in most cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on three experimental settings.\n\nMoreover, we note: (1) GTRANS is flexible and versatile. It can be equipped with any pre-trained GNNs and the outcome (the refined graph data) can be deployed with any model given its favorable transferability. (2) GTRANS provides a degree of interpretability, as it can show which kinds of graph modifications can help improve performance by visualizing the data.\n\n2 RELATED WORK\n\nDistribution shift in GNNs. GNNs have revolutionized graph representation learning and achieved state-of-the-art results on diverse graph-related tasks (Kipf & Welling, 2016a; Veliˇckovi ́c et al., 2018; Hamilton et al., 2017; Chien et al., 2021; Klicpera et al., 2018; Wu et al., 2019). However, recent studies have demonstrated that GNNs yield sub-optimal performance on out-of-distribution data for node classification (Zhu et al., 2021a; Wu et al., 2022a; Liu et al., 2022a) and graph classification (Chen et al., 2022; Buffelli et al., 2022; Gui et al., 2022; Wu et al., 2022b; You et al., 2023). These studies have introduced solutions to tackle distribution shifts by altering model training behavior or model architectures. For a thorough review, we refer the readers to a recent survey (Li et al., 2022). Unlike existing works, we target modifying the inputs via test-time adaption.\n\nRobustness of GNNs. Recent studies have demonstrated the vulnerability of GNNs to graph adversarial attacks (Z ̈ugner et al., 2018; Z ̈ugner & G ̈unnemann, 2019; Xu et al., 2019; Geisler et al.,\n\n2\n\nTest AccuracyGCN: 67.4%GAT: 58.6%APPNP: 70.7%AirGNN: 70.8%Transform[⋅][⋅][⋅][⋅][⋅][⋅][⋅][x][⋅][x][⋅][⋅][x][⋅][x]Node feature[⋅][⋅]Test AccuracyGCN: 44.3%GAT: 21.2%APPNP: 48.3%AirGNN: 58.5%[x]Node connectionOriginal Graph[⋅][⋅][x][⋅][x][⋅][⋅][x][⋅][⋅][⋅][x][⋅][x][⋅][⋅]Refined GraphPublished as a conference paper at ICLR 2023\n\n2021), i.e., small perturbations on the input graph can mislead GNNs into making wrong predictions. Several works make efforts towards developing new GNNs or adversarial training strategies to defend against attacks (Xu et al., 2019; Zhu et al., 2019; Jin et al., 2021a;b). Instead of altering model training behavior, our work aims to modify the test graph to correct adversarial patterns.\n\nGraph Structure Learning & Graph Data Augmentation. Graph structure learning and graph data augmentation both aim to improve GNNs’ generalization performance by augmenting the (training) graph data (Zhao et al., 2022a), either learning the graph from scratch (Franceschi et al., 2019; Jin et al., 2020; Chen et al., 2020; Zhao et al., 2021b) or perturbing the graph in a rule-based way (Rong et al., 2020; Feng et al., 2020; Ding et al., 2022). While our work also modifies the graph data, we focus on modifying the test data and not impacting the model training process.\n\nTest-time Training. Our work is also related to test-time training (Sun et al., 2020; Wang et al., 2021; Liu et al., 2021b; Zhang et al., 2021; 2022), which has raised a surge of interest in computer vision recently. To handle out-of-distribution data, Sun et al. (2020) propose the pioneer work of test-time training (TTT) by optimizing feature extractor via an auxiliary task loss. However, TTT alters training to jointly optimize the supervised loss and auxiliary task loss. To remove the need for training an auxiliary task, Tent (Wang et al., 2021) proposes to minimize the prediction entropy at test-time. Tent works by adapting the parameters in batch normalization layers, which may not always be employed by modern GNNs. In this work, we focus on a novel perspective of adapting the test graph data, which makes no assumptions about the particular training procedure or architecture.\n\n3 METHODOLOGY\n\nWe start by introducing the general problem of test-time graph transformation (TTGT). While our discussion mainly focuses on the node classification task where the goal is to predict the labels of nodes in the graph, it can be easily extended to other tasks. Consider that we have a training graph GTr and a test graph GTe, and the corresponding set of node labels are denoted as YTr and YTe, respectively. Note that the node sets in GTr and GTe can either be disjoint or overlapping, and they are not necessarily drawn from the same distribution. Further, let fθ(·) denote the mapping function of a GNN model parameterized by θ, which maps a graph into the space of node labels.\n\nDefinition 1 (Test-Time Graph Transformation (TTGT)). TTGT requires to learn a graph transformation function g(·) which refines the test graph GTe such that the pre-trained fθ can yield better test performance on g(GTe) than that on GTe:\n\narg min\n\ng\n\nL (fθ∗ (g(GTe)), YTe)\n\ns.t.\n\ng(GTe) ∈ P(GTe),\n\nwith θ∗ = arg min\n\nL (fθ(GTr), YTr) ,\n\n(1)\n\nθ where L denotes the loss function measuring downstream performance; P(GTe) is the space of the modified graph, e.g., we may constrain the change on the graph to be small.\n\nTo optimize the TTGT problem, we are faced with two critical challenges: (1) how to parameterize and optimize the graph transformation function g(·); and (2) how to formulate a surrogate loss to guide the learning process, since we do not have access to the ground-truth labels of test nodes. Therefore, we propose GTRANS and elaborate on how it addresses these challenges as follows.\n\n3.1 CONSTRUCTING GRAPH TRANSFORMATION\n\nLet GTe = {A, X} denote the test graph, where A ∈ {0, 1}N ×N is the adjacency matrix, N is the number of nodes, and X ∈ RN ×d is the d-dimensional node feature matrix. Since the pre-trained GNN parameters are fixed at test time and we only care about the test graph, we drop the subscript in GTe and YTe to simplify notations in the rest of the paper.\n\nConstruction. We are interested in obtaining the transformed test graph G′ Te = g(A, X) = (A′, X′). Specifically, we model feature modification as an additive function which injects perturbation to node features, i.e., X′ = X + ∆X; we model the structure modification as A′ = A ⊕ ∆A 1, where ⊕ stands for an element-wise exclusive OR operation and ∆A ∈ {0, 1}N ×N is a binary matrix. In other words, (∆A)ij = 1 indicates an edge flip. Formally, we seek to find ∆A and ∆X that\n\n1(A ⊕ ∆A)ij can be implemented as 2 − (A + ∆A)ij if (A + ∆A)ij ≥ 1, otherwise (A + ∆A)ij.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\ncan minimize the objective function:\n\narg min ∆A,∆X\n\nL (fθ(A ⊕ ∆A, X + ∆X), Y)\n\ns.t.\n\n(A ⊕ ∆A, X + ∆X) ∈ P(A, X),\n\n(2)\n\nwhere ∆A ∈ {0, 1}N ×N and ∆X ∈ RN ×d are treated as free parameters. Further, to ensure we do not heavily violate the original graph structure, we constrain the number of changed entries in the adjacency matrix to be smaller than a budget B on the graph structure, i.e., (cid:80) ∆A ≤ B. We do not impose constraints on the node features to ease optimization. In this context, P can be viewed as constraining ∆A to a binary space as well as restricting the number of changes.\n\nOptimization. The optimization for ∆X is easy since the node features are continuous. The optimization for ∆A is particularly difficult in that (1) ∆A is binary and constrained; and (2) the search space of N 2 entries is too large especially when we are learning on large-scale graphs.\n\nTo cope with the first challenge, we relax the binary space to [0, 1]N ×N and then we can employ projected gradient descent (PGD) (Xu et al., 2019; Geisler et al., 2021) to update ∆A:\n\n∆A ← ΠP (∆A − η∇∆AL(∆A)) (3) where we first perform gradient descent with step size η and call a projection ΠP to project the variable to the space P. Specifically, given an input vector p, ΠP (·) is expressed as:\n\n(cid:26) Π[0,1](p),\n\nΠP (p) ←\n\nΠ[0,1](p − γ1) with 1⊤Π[0,1](p − γ1) = B,\n\nIf 1⊤Π[0,1](p) ≤ B; otherwise,\n\n(4)\n\nwhere Π[0.1](·) clamps the input values to [0, 1], 1 stands for a vector of all ones, and γ is obtained by solving the equation 1⊤Π[0,1](p − γ1) = B with the bisection method (Liu et al., 2015). To keep the adjacency structure discrete and sparse, we view each entry in A ⊕ ∆A as a Bernoulli distribution and sample the learned graph as A′ ∼ Bernoulli(A ⊕ ∆A).\n\nFurthermore, to enable efficient graph structure learning, it is desired to reduce the search space of updated adjacency matrix. One recent approach of graph adversarial attack (Geisler et al., 2021) proposes to sample a small block of possible entries from the adjacency matrix and update them at each iteration. This solution is still computationally intensive as it requires hundreds of steps to achieve a good performance. Instead, we constrain the search space to only the existing edges of the graph, which is typically sparse. Empirically, we observe that this simpler strategy still learns useful structure information when combined with feature modification.\n\n3.2 PARAMETER-FREE SURROGATE LOSS\n\nAs discussed earlier, the proposed framework GTRANS aims to improve the model generalization and robustness by learning to transform the test graph. Ideally, when we have test ground-truth labels, the problem can be readily solved by adapting the graph to result in the minimum cross entropy loss on test samples. However, as we do not have such information at test-time, it motivates us to investigate feasible surrogate losses to guide the graph transformation process. In the absence of labeled data, recently emerging self-supervised learning techniques (Xie et al., 2021; Liu et al., 2022b) have paved the way for providing self-supervision for TTGT. However, not every surrogate self-supervised task and loss is suitable for our transformation process, as some tasks are more powerful and some are weaker. To choose a suitable surrogate loss, we provide the following theorem.\n\nTheorem 1. Let Lc denote the classification loss and Ls denote the surrogate loss, respectively. Let ρ(G) denote the correlation between ∇GLc(G, Y) and ∇GLs(G), and let ε denote the learning rate for gradient descent. Assume that Lc is twice-differentiable and its Hessian matrix satisfies ∥H(G, Y)∥2 ≤ M for all G. When ρ(G) > 0 and ε < 2ρ(G)∥∇GLc(G,Y)∥2\n\n, we have\n\nM ∥∇GLs(G)∥2\n\nLc (G − ε∇GLs (G) , Y) < Lc (G, Y) .\n\n(5)\n\nThe proof can be found in Appendix A.1. Theorem 1 suggests that when the gradients from classification loss and surrogate loss have a positive correlation, i.e., ρ (G) > 0, we can update the test graph by performing gradient descent with a sufficiently small learning rate such that the classification loss on the test samples is reduced. Hence, it is imperative to find a surrogate task that shares relevant information with the classification task. To empirically verify the effectiveness of Theorem 1, we adopt the surrogate loss in Equation (6) as Ls and plot the values of ρ(G) and Lc on one test graph in Cora in Figure 2. We can observe that a positive ρ(G) generally reduces the test loss. Results on different losses can be found in Appendix D.9 and similar patterns are exhibited.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nParameter-Free Surrogate Loss. As one popular self-supervised paradigm, graph contrastive learning has achieved promising performance in various tasks (Hassani & Khasahmadi, 2020; You et al., 2021; Zhu et al., 2021b), which indicates that graph contrastive learning tasks are often highly correlated with downstream tasks. This property is desirable for guiding TTGT as suggested by Theorem 1. At its core lies the contrasting scheme, where the similarity between two augmented views from the same sample is maximized, while the similarity between views from two different samples is minimized. However, the majority of existing graph contrastive learning methods cannot be directly applied to our scenario, as they often require a parameterized projection head to map augmented representations to another latent space, which inevitably alters the model architecture. Thus, we design a parameter-free surrogate loss which removes the projection head. Specifically, we apply an augmentation function A(·) on input graph G and obtain an augmented graph A(G). The node representations obtained from the two graphs are denoted as Z and ˆZ, respectively; zi and ˆzi stand for the i-th node representation taken from them, respectively. We adopt DropEdge (Rong et al., 2020) as the augmentation function A(·), and the node representations are taken from the second last layer of the trained model. Essentially, we maximize the similarity between original nodes and their augmented view while penalizing the similarity between the nodes and their negative samples:\n\nFigure 2: Positive ρ(G) can help reduce test loss.\n\nmin Ls =\n\nN (cid:88)\n\ni=1\n\n(1 −\n\nˆz⊤ i zi ∥ˆzi∥∥zi∥\n\n) −\n\nN (cid:88)\n\n(1 −\n\ni=1\n\n ̃z⊤ i zi ∥ ̃zi∥∥zi∥\n\n),\n\n(6)\n\nwhere { ̃zi|i = 1, . . . , N } are the negative samples for corresponding nodes, which are generated by shuffling node features (Velickovic et al., 2019). In Eq. (6), the first term encourages each node to be close while the second term pushes each node away from the corresponding negative sample. Note that (1) Ls is parameter-free and does not require modification of the model architecture, or affect the pre-training process; (2) there could be other self-supervised signals for guiding the graph transformation, and we empirically compare them with the contrastive loss in Appendix D.9. We also highlight that our unique contribution is not the loss in Eq. (6) but the proposed TTGT framework as well as the theoretical and empirical insights on how to choose a suitable surrogate loss. Furthermore, the algorithm of GTRANS is provided in Appendix B.\n\n3.3 FURTHER ANALYSIS\n\nIn this subsection, we study the theoretical property of Eq. (6) and compare the strategy of adapting data versus that of adapting model. We first demonstrate the rationality of the proposed surrogate loss through the following theorem.\n\nTheorem 2. Assume that the augmentation function A(·) generates a data view of the same class for the test nodes and the node classes are balanced. Assume for each class, the mean of the representations obtained from Z and ˆZ are the same. Minimizing the first term in Eq. (6) is approximately minimizing the class-conditional entropy H(Z|Y ) between features Z and labels Y .\n\nThe proof can be found in Appendix A.2. Theorem 2 indicates that minimizing the first term in Eq. (6) will approximately minimize H(Z|Y ), which encourages high intra-class compactness, i.e., learning a low-entropy cluster in the embedded space for each class. Notably, H(Z|Y ) can be rewritten as H(Z|Y ) = H(Z) − I(Z, Y ). It indicates that minimizing Eq. (6) can also help promote I(Z, Y ), the mutual information between the hidden representation and downstream class. However, we note that only optimizing this term can result in collapse (mapping all data points to a single point in the embedded space), which stresses the necessity of the second term in Eq. (6).\n\nNext, we use an illustrative example to show that adapting data at test-time can be more useful than adapting model in some cases. Given test samples {xi|i = 1, . . . , K}, we consider a linearized GNN fθ which first performs aggregation through a function Agg(·, ·) and then transforms the aggregated features via a function Trans(·). Hence, only the function Trans(·) is parameterized by θ.\n\nExample. Let Ni denote the neighbors for node xi. If there exist two nodes with the same aggregated features but different labels, i.e., Agg(x1, {xi|i ∈ N1}) = Agg(x2, {xj|j ∈ N2}), y1 ̸= y2, adapting the data {xi|i = 1, . . . , K} can achieve lower classification error than adapting the model fθ at test stage. Illustration. Let ̄x1 = Agg(x1, {xi|i ∈ N1}) and ̄x2 = Agg(x2, {xj|j ∈ N2}). For simplicity, we\n\n5\n\n123456789101112131415Epoch0.10.00.10.20.30.40.5Test loss cCorrelation (G)Published as a conference paper at ICLR 2023\n\nconsider the following mean square loss as the classification error:\n\nl =\n\n(cid:0)Trans( ̄x1)) − y1)2 + (Trans( ̄x2) − y2)2(cid:1) .\n\n(7)\n\n1 2\n\nIt is easy to see that l reaches its minimum when Trans( ̄x1) = y1 and Trans( ̄x2) = y2. In this context, it is impossible to find θ such that Trans(·) can map x1, x2 to different labels since it is not a one-to-many function. However, since y1 and y2 are in the label space of training data, we can always modify the test graph to obtain newly aggregated features ̄x′ 1) = y1 and Trans( ̄x′ 2) = y2, which minimizes l. In the extreme case, we may drop all node connections for 1 and x2 ← ̄x′ the two nodes, and let x1 ← ̄x′ 2 are the aggregated features taken from the training set. Hence, adapting data can achieve lower classification loss.\n\n2 such that Trans( ̄x′\n\n2 where ̄x′\n\n1 and ̄x′\n\n1, ̄x′\n\nRemark 1. Note that the existence of two nodes with the same aggregated features but different labels is not rare when considering adversarial attack or abnormal features. We provide a figurative example in Figure 6 in Appendix A.3: the attacker injects one adversarial edge into the graph and changes the aggregated features ̄x1 and ̄x2 to be the same. Remark 2. When we consider ̄x1 ̸= ̄x2, y1 ̸= y2, whether we can find θ satisfying Trans( ̄x1) = y1 and Trans( ̄x2) = y2 depends on the expressiveness of the the transformation function. If it is not powerful enough (e.g., an under-parameterized neural network), it could fail to map different data points to different labels. On the contrary, adapting the data does not suffer this problem as we can always modify the test graph to satisfy Trans( ̄x′\n\n1) = y1 and Trans( ̄x′\n\n2) = y2.\n\nRemark 3. The above discussion can be easily extended to nonlinear GNN by considering ̄x1, ̄x2 as the output before the last linear layer of GNN.\n\n4 EXPERIMENT\n\n4.1 GENERALIZATION ON OUT-OF-DISTRIBUTION DATA\n\nSetup. Following the settings in EERM (Wu et al., 2022a), which is designed for node-level tasks on OOD data, we validate GTRANS on three types of distribution shifts with six benchmark datasets: (1) artificial transformation for Cora (Yang et al., 2016) and Amazon-Photo (Shchur et al., 2018), (2) cross-domain transfers for Twitch-E and FB-100 (Rozemberczki et al., 2021a) (Lim et al., 2021), and (3) temporal evolution for Elliptic (Pareja et al., 2020) and OGB-Arxiv (Hu et al., 2020). Moreoever, Cora and Amazon-Photo have 1/1/8 graphs for training/validation/test sets. The splits are 1/1/5 on Twitch-E, 3/2/3 on FB-100, 5/5/33 on Elliptic, and 1/1/3 on OGB-Arxiv. More details on the datasets are provided in Appendix C. We compare GTRANS with four baselines: empirical risk minimization (ERM, i.e., standard training), data augmentation technique DropEdge (Rong et al., 2020), test-timetraining method Tent (Wang et al., 2021), and the recent SOTA method EERM (Wu et al., 2022a) which is exclusively developed for graph OOD issue. Further, we evaluate all the methods with four popular GNN backbones including GCN (Kipf & Welling, 2016a), GraphSAGE (Hamilton et al., 2017), GAT (Veliˇckovi ́c et al., 2018), and GPR (Chien et al., 2021). Their default setup follows that in EERM2. We refer the readers to Appendix C.1 for more implementation details of baselines and GTRANS. Notably, all experiments in this paper are repeated 10 times with different random seeds. Due to page limit, we include more baselines such as SR-GNN (Zhu et al., 2021a) and UDA-GCN (Wu et al., 2020) in Appendix D.1.\n\nResults. Table 1 reports the averaged performance over the test graphs for each dataset as well as the averaged rank of each algorithm. From the table, we make the following observations:\n\n(a) Overall Performance. The proposed framework consistently achieves strong performance across the datasets: GTRANS achieves average ranks of 1.0, 1.7, 2.0 and 1.7 with GCN, SAGE, GAT and GPR, respectively, while the corresponding ranks for the best baseline EERM are 2.9, 3.4, 3.0 and 2.0. Furthermore, in most of the cases, GTRANS significantly improves the vanilla baseline (ERM) by a large margin. Particularly, when using GCN as backbone, GTRANS outperforms ERM by 3.1%, 5.0% and 2.0% on Cora, Elliptic and OGB-Arxiv, respectively. These results demonstrate the effectiveness of GTRANS in tackling diverse types of distribution shifts.\n\n(b) Comparison to other baselines. Both DropEdge and EERM modify the training process to improve model generalization. Nonetheless, they are less effective than GTRANS, as GTRANS takes\n\n2We note that the GCN used in the experiments of EERM does not normalize the adjacency matrix according to its open-source code. Here we normalize the adjacency matrix to make it consistent with the original GCN.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Average classification performance (%) on the test graphs. Rank indicates the average rank of each algorithm for each backbone. OOM indicates out-of-memory error on 32 GB GPU memory. The proposed GTRANS consistently ranks the best compared with the baselines. ∗/∗∗ indicates that GTrans outperforms ERM at the confidence level 0.1/0.05 from paired t-test.\n\nBackbone Method\n\nAmz-Photo Cora\n\nElliptic\n\nFB-100\n\nOGB-Arxiv\n\nTwitch-E\n\nRank\n\nGCN\n\nSAGE\n\nGAT\n\nGPR\n\nERM 93.79±0.97 DropEdge 92.11±0.31 94.03±1.07 Tent 94.05±0.40 EERM 94.13±0.77∗ GTRANS\n\n91.59±1.44 59.89±0.50 81.01±1.33 59.95±0.39 91.87±1.36 59.46±0.55 87.21±0.53 59.85±0.85 94.66±0.63∗∗ 55.88±3.10∗∗ 54.32±0.60 41.59±1.20∗∗ 60.42±0.86∗\n\n54.04±0.94 38.59±1.35 53.00±0.50 41.26±0.92 54.16±1.00 39.33±1.40 54.24±0.55 OOM\n\n50.90±1.51 53.96±4.91 51.71±2.00 53.96±0.65\n\n99.67±0.14 ERM 95.09±0.60 95.85±0.30 DropEdge 92.61±0.56 99.80±0.10 95.72±0.43 Tent 95.57±0.13 EERM 98.77±0.14 96.91±0.68∗∗ 99.45±0.13 GTRANS\n\n56.12±4.47 62.06±0.09 52.38±3.11 62.14±0.12 55.89±4.87 62.09±0.09 58.20±3.55 62.11±0.12 60.81±5.19∗∗ 54.64±0.62 40.39±1.45∗∗ 62.15±0.13∗\n\n54.70±0.47 39.56±1.66 54.51±0.69 38.89±1.74 54.86±0.34 39.58±1.26 54.28±0.97 OOM\n\n58.53±1.00 96.30±0.79 ERM 58.89±1.01 DropEdge 90.70±0.29 58.33±1.18 95.99±0.46 Tent 59.84±0.71 95.57±1.32 EERM 96.67±0.74∗∗ 96.37±1.00∗∗ 66.43±2.57∗∗ 51.16±1.72 43.76±1.25∗∗ 58.59±1.07 GTRANS\n\n51.77±1.41 40.63±1.57 52.65±0.88 42.48±0.93 51.47±1.70 40.06±1.19 53.30±0.77 OOM\n\n65.36±2.70 63.78±2.39 66.07±1.66 58.14±4.71\n\n94.81±1.28 76.91±1.55 95.91±1.14 85.00±0.96\n\nERM 91.87±0.65 DropEdge 88.81±1.48 Tent3 -\n90.78±0.52 EERM 91.93±0.73 GTRANS\n\n93.00±2.17 79.27±1.39 -\n88.82±3.10 93.05±2.02\n\n64.59±3.52 61.02±1.78 -\n67.27±0.98 69.03±2.33∗∗ 54.38±0.31 46.00±0.46∗∗ 60.11±0.53∗∗\n\n54.51±0.33 44.38±0.59 55.04±0.33 43.65±0.77 -\n55.95±0.03 OOM\n\n59.72±0.40 59.89±0.05 -\n61.57±0.12\n\n-\n\n3.8 3.6 3.3 2.9 1.0\n\n3.2 4.2 2.3 3.4 1.7\n\n3.0 3.3 3.3 3.0 2.0\n\n2.7 3.3 -\n2.0 1.7\n\n3 Tent cannot be applied to models which do not contain batch normalization layers.\n\nExtra Running Time (s) Total GPU Memory (GB) Cora Photo Ellip. Arxiv Cora Photo Ellip. Arxiv\n\nEERM 25.9 396.4 607.9 GTRANS 0.3\n\n0.6\n\n0.5\n\n- 2.6\n\n2.5 1.4\n\n10.5 12.8 >32 3.9 1.3 1.5\n\nFigure 3: Results on Cora under OOD. GTRANS improves GCN on most test graphs.\n\nTable 2: Efficiency comparison. GTRANS is more time- and memory-efficient than EERM.\n\nadvantage of the information from test graphs. As a test-time training method, Tent also performs well in some cases, but Tent only adapts the parameters in batch normalization layers and cannot be applied to models without batch normalization.\n\nWe further show the performance on each test graph on Cora with GCN in Figure 3 and the results for other datasets are provided in Appendix D.4. We observe that GTRANS generally improves over individual test graphs within each dataset, which validates the effectiveness of GTRANS.\n\nEfficiency Comparison. Since EERM performs the best among baselines, Table 2 showcases the efficiency comparison between our proposed GTRANS and EERM on the largest test graph in each dataset. The additional running time of GTRANS majorly depends on the number of gradient descent steps. As we only use a small number (5 or 10) throughout all the experiments, the time overhead brought by GTRANS is negligible. Compared with the re-training method EERM, GTRANS avoids the complex bilevel optimization and thus is significantly more efficient. Furthermore, EERM imposes a considerably heavier memory burden.\n\n4.2 ROBUSTNESS TO ABNORMAL FEATURES\n\nSetup. Following the setup in AirGNN (Liu et al., 2021a), we evaluate the robustness in the case of abnormal features. Specifically, we simulate abnormal features by assigning random features taken from a multivariate standard Gaussian distribution to a portion of randomly selected test nodes. Note that the abnormal features are injected after model training (at test time) and we vary the ratio of noisy nodes from 0.1 to 0.4 with a step size of 0.05. This process is performed for four datasets: the original version of Cora, Citeseer, Pubmed, and OGB-Arxiv. In these four datasets, the training graph and the test graph have the same graph structure but the node features are different. Hence, we use the training classification loss combined with the proposed contrastive loss to optimize GTRANS. We use GCN as the backbone model and adopt four GNNs as the baselines including GAT (Veliˇckovi ́c et al., 2018), APPNP (Klicpera et al., 2018), AirGNN and AirGNN-t.\n\n7\n\nT1T2T3T4T5T6T7T80.750.800.850.900.95AccuracyERMGTransPublished as a conference paper at ICLR 2023\n\n(a) Cora\n\n(b) Citeseer\n\n(c) Pubmed\n\n(d) OGB-Arxiv\n\nFigure 4: Node classification accuracy on abnormal (noisy) nodes.\n\n(a) Abnormal Graph\n\n(b) Refined Graph\n\n(c) OOD Setting\n\n(d) Abnormal Features\n\nFigure 5: (a)(b) T-SNE visualizations of embedding obtained from abnormal graph and transformed graph on Cora. (c)(d) Comparison between adapting data and adapting model at test time.\n\nNote that AirGNN-t tunes the message-passing hyper-parameter in AirGNN at test time. For a fair comparison, we tune AirGNN-t based on the performance on both training and validation nodes.\n\nResults. For each model, we present the node classification accuracy on both abnormal nodes and all test nodes (i.e., both normal and abnormal ones) in Figure 4 and Figure 8 (See Appendix D.5), respectively. From these figures, we have two observations. First, GTRANS significantly improves GCN in terms of the performance on abnormal nodes and all test nodes for all datasets across all noise ratios. For example, on Cora with 30% noisy nodes, GTRANS improves GCN by 48.2% on abnormal nodes and 31.0% on overall test accuracy. This demonstrates the effectiveness of the graph transformation process in GTRANS in alleviating the effect of abnormal features. Second, GTRANS shows comparable or better performance with AirGNNs, which are the SOTA defense methods for tackling abnormal features. It is worth mentioning that AirGNN-t improves AirGNN by tuning its hyper-parameter at test time, which aligns with our motivation that test-time adaptation can enhance model test performance. To further understand the effect of graph transformation, we provide the visualization of the test node embeddings obtained from abnormal graph (0.3 noise ratio) and transformed graph for Cora in Figures 5a and 5b, respectively. We observe that the transformed graph results in well-clustered node representations, which indicates that GTRANS can promote intra-class compactness and counteract the effect of abnormal patterns.\n\n4.3 ROBUSTNESS TO ADVERSARIAL ATTACK\n\nSetup. We further evaluate GTRANS under the setting of adversarial attack where we perturb the test graph, i.e., evasion attack. Specifically, we use PR-BCD (Geisler et al., 2021), a scalable attack method, to attack the test graph in OGB-Arxiv. We focus on structural attacks, and vary the perturbation rate, i.e., the ratio of changed edges, from 5% to 25% with a step of 5%. Similar to Section 4.2, we adopt the training classification loss together with the proposed contrastive loss to optimize GTRANS. We use GCN as the backbone and employ four robust baselines implemented by the adversarial attack repository DeepRobust (Li et al., 2020) including GAT (Veliˇckovi ́c et al., 2018), RobustGCN (Zhu et al., 2019), SimPGCN (Jin et al., 2021a) and GCNJaccard (Xu et al., 2019) as comparisons. Among them, GCNJaccard pre-processes the attacked graph by removing edges where the similarities of connected nodes are less than a threshold; we tune this threshold at test time based on the performance on both training and validation nodes.\n\nResults. Table 3 reports the performances under structural evasion attack. We observe that GTRANS consistently improves the performance of GCN under different perturbation rates of adversarial attack. Particularly, GTRANS improves GCN by a larger margin when the perturbation rate is higher. For example, GTRANS outperforms GCN by over 40% under the 25% perturbation rate. Such observation suggests that GTRANS can counteract the devastating effect of adversarial attacks. In addition, the best performing baseline GCNJaccard also modifies the graph at test time, which demonstrates the importance of test-time graph adaptation. Nonetheless, it consistently underperforms our\n\n8\n\n0.100.150.200.250.300.350.40Ratio of Noisy Nodes0.00.10.20.30.40.50.60.7AccuracyGCNGATAPPNPAirGNNAirGNN-tGTrans0.100.150.200.250.300.350.40Ratio of Noisy Nodes0.00.10.20.30.40.5AccuracyGCNGATAPPNPAirGNNAirGNN-tGTrans0.100.150.200.250.300.350.40Ratio of Noisy Nodes0.00.10.20.30.40.50.60.7AccuracyGCNGATAPPNPAirGNNAirGNN-tGTrans0.100.150.200.250.300.350.40Ratio of Noisy Nodes0.00.10.20.30.40.50.60.7AccuracyGCNGATAPPNPAirGNNAirGNN-tGTrans35455565758595Pho.Cor.Elli.Fb.Arx.Twi.Adapting ModelERMAdapting Data65.347.857.863.559.129.846.138.764.455.067.470.657.142.764.167.901020304050607080ArxivCites.CoraPubm.ArxivCites.CoraPubm. All Test NodesAbnormal NodesAdapting ModelAdapting DataPublished as a conference paper at ICLR 2023\n\nTable 3: Node classification accuracy (%) under different perturbation (Ptb.) rates of structure attack.\n\nPtb. Rate\n\nGCN\n\nGAT\n\nRobustGCN\n\nSimPGCN\n\nGCNJaccard\n\nGTRANS\n\n5% 10% 15% 20% 25%\n\n57.47±0.54 47.97±0.65 38.04±1.22 29.05±0.73 19.58±2.32\n\n64.56±0.43 61.20±0.70 58.96±0.59 57.29±0.49 55.86±0.53\n\n61.55±1.20 58.15±1.55 55.91±1.27 54.39±1.09 52.76±1.44\n\n61.30±0.42 57.01±0.70 54.13±0.73 52.26±0.87 50.46±0.85\n\n65.01±0.26 63.25±0.30 61.83±0.29 60.57±0.34 59.17±0.39\n\n66.29±0.25 65.16±0.52 64.40±0.38 63.44±0.50 62.95±0.67\n\nproposed GTRANS, indicating that a learnable transformation function is needed to achieve better robustness under adversarial attacks, which GCNJaccard does not employ.\n\nInterpretation. To understand the modifications made on the graph, we compare several properties among clean graph, attacked graph (20% perturbation rate), graph obtained by GCNJaccard, and graph obtained by GTRANS in Table 13 in Appendix D.6. First, adversarial attack decreases homophily and feature similarity, but GTRANS and GCNJaccard promote such information to alleivate the adversarial patterns. Our experiment also shows that GTRANS removes 77% adversarial edges while removing 30% existing edges from the attacked graph. Second, both GTRANS and GCNJaccard focus on deleting edges from the attacked graph, but GCNJaccard removes a substantially larger amount of edges, which may destroy clean graph structure and lead to sub-optimal performance.\n\n4.4 FURTHER ANALYSIS\n\nTr\\Te\n\nTable 4: Transferability.\n\nCross-Architecture Transferability. Since the outcome of GTRANS is a refined graph, it can conceptually be employed by any GNN model. Thus, we can transform the graph based on one pre-trained GNN and test the transformed graph on another pre-trained GNN. To examine such transferability, we perform experiments GCN, APPNP, AirGNN and GAT under the abnormal feature setting with 30% noisy nodes on Cora. The results on all test nodes in Table 4. Note that “Tr” stands for GNNs used in TTGT while “Te” denotes GNNs used for obtaining predictions on the transformed graph; “Noisy” indicates the performance on the noisy graph. We observe that the transformed graph yields good performance even outside the scope it was optimized for. We anticipate that such transferability can alleviate the need for costly re-training on new GNNs.\n\nGCN 67.36 70.65 APPNP 67.87 70.39 AirGNN 68.00 70.37 54.85 60.37 GAT\n\nGCN APPNP AirGNN GAT\n\n70.84 69.59 72.68 65.22\n\n58.62 64.46 64.93 54.60\n\n44.29 48.26\n\nNoisy\n\n58.51\n\n21.23\n\nAdapting Model vs. Adapting Data. We empirically compare the performance between adapting data and adapting model and consider the OOD and abnormal feature settings. Specifically, we use GCN as the backbone and adapt the model parameters by optimizing the same loss function as used in GTRANS. The results are shown in Figures 5c and 5d. In OOD setting, both adapting model and adapting data can generally improve GCN’s performance. Since their performances are still close, it is hard to give a definite answer on which strategy is better. However, we can observe significant performance differences when the graph contains abnormal features: adapting data outperforms adapting model on 3 out of 4 datasets. This suggests that adapting data can be more powerful when the data is perturbed, which aligns with our analysis in Section 3.3.\n\nLearning Features v.s. Learning Structure. Since our framework learns both node features and graph structure, we investigate when one component plays a more important role than the other. Our results are shown in Tables 16 and 17 in Appendix D.8. From the tables, we observe that (1) while each component can improve the vanilla performance, feature learning is more crucial for counteracting feature corruption and structure learning is more important for defending structure corruption; and (2) combining them generally yields a better or comparable performance.\n\n5 CONCLUSION\n\nGNNs tend to yield unsatisfying performance when the presented data is sub-optimal. To tackle this issue, we seek to enhance GNNs from a data-centric perspective by transforming the graph data at test time. We propose GTRANS which optimizes a contrastive surrogate loss to transform graph structure and node features, and provide theoretical analysis with deeper discussion to understand this framework. Experimental results on distribution shift, abnormal features and adversarial attack have demonstrated the effectiveness of our method. In the future, we plan to explore more applications of our framework such as mitigating degree bias and long-range dependency.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOLWEDGEMENT\n\nThis research is supported by the National Science Foundation (NSF) under grant numbers IIS1845081, IIS1928278, IIS1955285, IIS2212032, IIS2212144, IOS2107215, and IOS2035472, the Army Research Office (ARO) under grant number W911NF-21-1-0198, MSU Foundation, the Home Depot, Cisco Systems Inc, Amazon Faculty Award, Johnson & Johnson and Snap Inc.\n\nETHICS STATEMENT\n\nTo the best of our knowledge, there are no ethical issues with this paper.\n\nREPRODUCIBILITY STATEMENT\n\nTo ensure reproducibility of our experiments, we provide our source code at https://github. com/ChandlerBang/GTrans. The hyper-parameters are described in details in the appendix. We also provide a pseudo-code implementation of our framework in the appendix.\n\nREFERENCES\n\nMalik Boudiaf, J ́erˆome Rony, Imtiaz Masud Ziko, Eric Granger, Marco Pedersoli, Pablo Piantanida, and Ismail Ben Ayed. A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses. In European conference on computer vision, pp. 548–564. Springer, 2020.\n\nDavide Buffelli, Pietro Li`o, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. Advances in Neural Information Processing Systems, 2022.\n\nYongqiang Chen, Yonggang Zhang, Han Yang, Kaili Ma, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Invariance principle meets out-of-distribution generalization on graphs. arXiv preprint arXiv:2202.05441, 2022.\n\nYu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19314–19326, 2020.\n\nEli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph neural network. In ICLR, 2021. URL https://openreview.net/forum?id= n6jl7fLxrP.\n\nAilin Deng and Bryan Hooi. Graph neural network-based anomaly detection in multivariate time series. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 4027– 4035, 2021.\n\nAustin Derrow-Pinion, Jennifer She, David Wong, Oliver Lange, Todd Hester, Luis Perez, Marc Nunkesser, Seongjae Lee, Xueying Guo, Brett Wiltshire, et al. Eta prediction with graph neural networks in google maps. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pp. 3767–3776, 2021.\n\nKaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph learning:\n\nA survey. arXiv preprint arXiv:2202.08235, 2022.\n\nDavid K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al ́an Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. Advances in neural information processing systems, 28, 2015.\n\nNegin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E Papalexakis. All you need is low (rank) defending against adversarial attacks on graphs. In Proceedings of the 13th International Conference on Web Search and Data Mining, pp. 169–177, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nWenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In The world wide web conference, pp. 417–426, 2019.\n\nBahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi. Slaps: Self-supervision improves structure learning for graph neural networks. Advances in Neural Information Processing Systems, 34: 22667–22681, 2021.\n\nWenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang, Evgeny Kharlamov, and Jie Tang. Graph random neural networks for semi-supervised learning on graphs. Advances in neural information processing systems, 33:22092–22103, 2020.\n\nLuca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures\n\nfor graph neural networks. arXiv preprint arXiv:1903.11960, 2019.\n\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ̧ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016.\n\nSimon Geisler, Tobias Schmidt, Hakan S ̧ irin, Daniel Z ̈ugner, Aleksandar Bojchevski, and Stephan G ̈unnemann. Robustness of graph neural networks at scale. Advances in Neural Information Processing Systems, 34:7637–7649, 2021.\n\nShurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. Good: A graph out-of-distribution benchmark.\n\narXiv preprint arXiv:2206.08452, 2022.\n\nZhichun Guo, Bozhao Nan, Yijun Tian, Olaf Wiest, Chuxu Zhang, and Nitesh V Chawla. Graph-\n\nbased molecular representation learning. arXiv preprint arXiv:2207.04869, 2022.\n\nJonathan Halcrow, Alexandru Mosoi, Sam Ruth, and Bryan Perozzi. Grale: Designing networks for graph learning. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2523–2532, 2020.\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n\nAdvances in neural information processing systems, 30, 2017.\n\nKaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on\n\ngraphs. In International Conference on Machine Learning, pp. 4116–4126. PMLR, 2020.\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020.\n\nWei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure\n\nlearning for robust graph neural networks. arXiv preprint arXiv:2005.10203, 2020.\n\nWei Jin, Tyler Derr, Yiqi Wang, Yao Ma, Zitao Liu, and Jiliang Tang. Node similarity preserving graph convolutional networks. In Proceedings of the 14th ACM international conference on web search and data mining, pp. 148–156, 2021a.\n\nWei Jin, Yaxing Li, Han Xu, Yiqi Wang, Shuiwang Ji, Charu Aggarwal, and Jiliang Tang. Adversarial attacks and defenses on graphs. ACM SIGKDD Explorations Newsletter, 22(2):19–34, 2021b.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\n\nworks. arXiv preprint arXiv:1609.02907, 2016a.\n\nThomas N Kipf and Max Welling.\n\nVariational graph auto-encoders.\n\narXiv preprint\n\narXiv:1611.07308, 2016b.\n\nJohannes Klicpera, Aleksandar Bojchevski, and Stephan G ̈unnemann. Predict then propagate: Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018.\n\nHaoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Out-of-distribution generalization on\n\ngraphs: A survey. arXiv preprint arXiv:2202.07987, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJintang Li, Tao Xie, Chen Liang, Fenfang Xie, Xiangnan He, and Zibin Zheng. Adversarial attack\n\non large scale graph. IEEE Transactions on Knowledge and Data Engineering, 2021.\n\nYaxin Li, Wei Jin, Han Xu, and Jiliang Tang. Deeprobust: A pytorch library for adversarial attacks\n\nand defenses. arXiv preprint arXiv:2005.06149, 2020.\n\nDerek Lim, Xiuyu Li, Felix Hohne, and Ser-Nam Lim. New benchmarks for learning on non-\n\nhomophilous graphs. arXiv preprint arXiv:2104.01404, 2021.\n\nHongrui Liu, Binbin Hu, Xiao Wang, Chuan Shi, Zhiqiang Zhang, and Jun Zhou. Confidence may cheat: Self-training on graph neural networks under distribution shift. In Proceedings of the ACM Web Conference 2022, pp. 1248–1258, 2022a.\n\nSijia Liu, Swarnendu Kar, Makan Fardad, and Pramod K Varshney. Sparsity-aware sensor collaboration for linear coherent estimation. IEEE Transactions on Signal Processing, 63(10):2582–2596, 2015.\n\nXiaorui Liu, Jiayuan Ding, Wei Jin, Han Xu, Yao Ma, Zitao Liu, and Jiliang Tang. Graph neural networks with adaptive residual. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021a. URL https: //openreview.net/forum?id=hfkER_KJiNw.\n\nYixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and Philip Yu. Graph selfsupervised learning: A survey. IEEE Transactions on Knowledge and Data Engineering, 2022b.\n\nYuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021b.\n\nAldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. Evolvegcn: Evolving graph convolutional netIn Proceedings of the AAAI Conference on Artificial Intelligence, works for dynamic graphs. volume 34, pp. 5363–5370, 2020.\n\nYu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Hkx1qkrKPr.\n\nBenedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal\n\nof Complex Networks, 9(2):cnab014, 2021a.\n\nBenedek Rozemberczki, Peter Englert, Amol Kapoor, Martin Blais, and Bryan Perozzi. Pathfinder discovery networks for neural message passing. In Proceedings of the Web Conference 2021, pp. 2547–2558, 2021b.\n\nAravind Sankar, Yozen Liu, Jun Yu, and Neil Shah. Graph neural networks for friend ranking in large-scale social platforms. In Proceedings of the Web Conference 2021, pp. 2535–2546, 2021.\n\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G ̈unnemann. Pitfalls\n\nof graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.\n\nXiao Shen, Quanyu Dai, Fu-lai Chung, Wei Lu, and Kup-Sze Choi. Adversarial deep network In Proceedings of the AAAI Conference on\n\nembedding for cross-network node classification. Artificial Intelligence, volume 34, pp. 2991–2999, 2020.\n\nYu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020.\n\nSusheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. Advances in Neural Information Processing Systems, 34:15920– 15933, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nXianfeng Tang, Yozen Liu, Neil Shah, Xiaolin Shi, Prasenjit Mitra, and Suhang Wang. Knowing your fate: Friendship, action and temporal explanations for user engagement prediction on social apps. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2269–2279, 2020.\n\nAmanda L Traud, Peter J Mucha, and Mason A Porter. Social structure of facebook networks.\n\nPhysica A: Statistical Mechanics and its Applications, 391(16):4165–4180, 2012.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\n\nBengio. Graph attention networks. 2018.\n\nPetar Velickovic, William Fedus, William L Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon\n\nHjelm. Deep graph infomax. ICLR (Poster), 2(3):4, 2019.\n\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c.\n\nMan Wu, Shirui Pan, Chuan Zhou, Xiaojun Chang, and Xingquan Zhu. Unsupervised domain adaptive graph convolutional networks. In Proceedings of The Web Conference 2020, pp. 1457– 1467, 2020.\n\nQitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. Handling distribution shifts on graphs: An invariance perspective. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=FQOC5u-1egI.\n\nYingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. In International Conference on Learning Representations, 2022b. URL https://openreview.net/forum?id=hGXij5rfiHw.\n\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A\n\ncomprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.\n\nYaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning\n\nof graph neural networks: A unified review. arXiv preprint arXiv:2102.10757, 2021.\n\nKaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, and Xue Lin. Topology attack and defense for graph neural networks: An optimization perspective. arXiv preprint arXiv:1906.04214, 2019.\n\nJingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection:\n\nA survey. arXiv preprint arXiv:2110.11334, 2021a.\n\nShuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. In\n\nInternational Conference on Learning Representations (ICLR), 2021b.\n\nShuo Yang, Songhua Wu, Tongliang Liu, and Min Xu. Bridging the gap between few-shot and manyshot learning via distribution calibration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(12):9830–9843, 2021c.\n\nZhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pp. 40–48. PMLR, 2016.\n\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 974– 983, 2018.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812–5823, 2020.\n\nYuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning auto-\n\nmated. Proceedings of International Conference on Machine Learning, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nYuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. Graph domain adaptation via theory-grounded spectral regularization. In International Conference on Learning Representations, 2023.\n\nWerner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschl ̈ager, and Susanne Saminger-Platz. Central moment discrepancy (cmd) for domain-invariant representation learning. arXiv preprint arXiv:1702.08811, 2017.\n\nMarvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and\n\naugmentation. arXiv preprint arXiv:2110.09506, 2021.\n\nYifan Zhang, Bryan Hooi, Lanqing Hong, and Jiashi Feng. Self-supervised aggregation of diverse experts for test-agnostic long-tailed recognition. In Advances in Neural Information Processing Systems, 2022.\n\nTong Zhao, Tianwen Jiang, Neil Shah, and Meng Jiang. A synergistic approach for graph anomaly detection with pattern mining and feature learning. IEEE Transactions on Neural Networks and Learning Systems, 2021a.\n\nTong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data aug-\n\nmentation for graph neural networks. In AAAI, 2021b.\n\nTong Zhao, Wei Jin, Yozen Liu, Yingheng Wang, Gang Liu, Stephan G ̈unneman, Neil Shah, and Meng Jiang. Graph data augmentation for graph machine learning: A survey. arXiv preprint arXiv:2202.08871, 2022a.\n\nTong Zhao, Gang Liu, Daheng Wang, Wenhao Yu, and Meng Jiang. Learning from counterfactual links for link prediction. In International Conference on Machine Learning, pp. 26911–26926. PMLR, 2022b.\n\nDingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. Robust graph convolutional networks against adversarial attacks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 1399–1407, 2019.\n\nJiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 33:7793–7804, 2020.\n\nQi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. Shift-robust gnns: Overcoming the limitations of localized graph training data. Advances in Neural Information Processing Systems, 34, 2021a.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, pp. 2069–2080, 2021b.\n\nDaniel Z ̈ugner, Amir Akbarnejad, and Stephan G ̈unnemann. Adversarial attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2847–2856, 2018.\n\nDaniel Z ̈ugner and Stephan G ̈unnemann. Adversarial attacks on graph neural networks via meta In International Conference on Learning Representations, 2019. URL https://\n\nlearning. openreview.net/forum?id=Bylnx209YX.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA PROOFS\n\nA.1 THEREOM 1\n\nTheorem 1. Let Lc denote the classification loss and Ls denote the surrogate loss, respectively. Let ρ(G) denote the correlation between ∇GLc(G, Y) and ∇GLs(G), and let ε denote the learning rate for gradient descent. Assume that Lc is twice-differentiable and its Hessian matrix satisfies ∥H(G, Y)∥2 ≤ M for all G. When ρ(G) > 0 and ε < 2ρ(G)∥∇GLc(G,Y)∥2\n\n, we have\n\nM ∥∇GLs(G)∥2\n\nLc (G − ε∇GLs (G) , Y) < Lc (G, Y) .\n\n(8)\n\nProof. Given that Lc is differentiable and twice-differentiable, we perform first-order Taylor expansion with Lagrange form of remainder at G − ε∇GLs(G):\n\nLc (G − ε∇GLs (G) , Y)\n\n(9)\n\n= Lc (G, Y) − ερ (G) ∥∇GLc (G, Y)∥2 ∥∇GLs (G)∥2 + where θ ∈ (0, 1) is a constant given by Lagrange form of the Taylor’s remainder (here we slightly abuse the notation), and ρ (G) is the correlation between ∇GLc(G, Y) and ∇GLs(G):\n\n∇GLs(G)⊤H(G − εθ∇GLs(G), Y)∇GLs(G),\n\nε2 2\n\nρ (G) =\n\n∇GLc (G, Y)T ∇GLs (G) ∥∇GLc (G, Y)∥2 ∥∇GLs (G)∥2\n\n.\n\n(10)\n\nBefore we proceed to the next steps, we first show that given a vector p and a symmetric matrix A, the inequality p⊤Ap ≤ ∥p∥2 p⊤Ap =\n\ni p (Performing SVD on A, i.e., A =\n\n2∥A∥2 holds:\n\nσip⊤uiu⊤\n\nσiuiu⊤ i )\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\n=\n\n≤\n\nσiv⊤\n\ni vi\n\n(Let v = U⊤p, where U = [u1; u2; . . . ; un])\n\nσmaxv⊤\n\ni vi = σmax∥v∥2\n\n2 = σmax∥U⊤p∥2\n\n2\n\n= σmax∥p∥2 = ∥p∥2 2∥A∥2\n\n2\n\n(U is an orthogonal matrix)\n\nSince the Hessian matrix is symmetric, we can use the above inequality to derive:\n\nLc (G − ε∇GLs (G) , Y)\n\n(11)\n\n(12)\n\n≤ Lc (G, Y) − ερ (G) ∥∇GLc (G, Y)∥2 ∥∇GLs (G)∥2 +\n\nε2 2\n\n∥∇GLs(G)∥2\n\n2∥H(G − εθ∇GLs(G), Y)∥2.\n\nThen we rewrite Eq. (12) as:\n\nLc (G − ε∇GLs (G) , Y) − Lc (G, Y)\n\n≤ − ερ (G) ∥∇GLc (G, Y)∥2 ∥∇GLs (G)∥2 +\n\nε2 2\n\n∥∇GLs(G)∥2\n\n2∥H(G − εθ∇GLs(G), Y)∥2.\n\nGiven ∥H(G, Y)∥2 ≤ M , we know\n\nLc (G − ε∇GLs (G) , Y) − Lc (G, Y)\n\n≤ − ερ (G) ∥∇GLc (G, Y)∥2 ∥∇GLs (G)∥2 +\n\nε2M 2\n\n∥∇GLs(G)∥2 2.\n\nBy setting ε = 2ρ(G)∥∇GLc(G,Y)∥2\n\nM ∥∇GLs(G)∥2\n\n, we have\n\nLc (G − ε∇GLs (G) , Y) − Lc (G, Y) = 0.\n\nTherefore, when ε < 2ρ(G)∥∇GLc(G,Y)∥2\n\nM ∥∇GLs(G)∥2\n\nand ρ(G) > 0, we have\n\nLc (G − ε∇GLs (G) , Y) < Lc (G, Y) .\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\nA.2 THEREOM 2\n\nTheorem 2. Assume that the augmentation function A(·) generates a data view of the same class for the test nodes and the node classes are balanced. Assume for each class, the mean of the repre-\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nsentations obtained from Z and ˆZ are the same. Minimizing the first term in Eq. (6) is approximately minimizing the class-conditional entropy H(Z|Y ) between features Z and labels Y .\n\nProof. For convenience, we slightly abuse the notations to replace zi ∥ˆzi∥ with zi and ˆzi, respectively. Then we have ∥zi∥ = ∥ˆzi∥ = 1. Let Zk denote the set of test samples from class k; K . Let c= denote equality up to a multiplicative and/or additive constant. Then the first thus |Zk| = N term in Eq. (6) can be rewritten as:\n\n∥zi∥ and ˆzi\n\nN (cid:88)\n\ni=1\n\n(1 − ˆz⊤\n\ni zi) =\n\nN (cid:88)\n\ni=1\n\n(cid:0)1 − ˆz⊤\n\ni zi\n\n(cid:1) c=\n\nK (cid:88)\n\nk=1\n\n1 |Zk|\n\n(cid:88)\n\n(cid:0)−ˆz⊤\n\ni zi\n\n(cid:1)\n\nzi∈Zk\n\nLet ck be the mean of hidden representations from class k; then we have ck = 1\n\n1 |Zk|\n\n(cid:80)\n\nˆzi∈ ˆZk\n\nˆzi. Now we build the connection between Eq. (17) and (cid:80)K\n\ni=1\n\n(cid:80)\n\nzi∈Zk\n\n(cid:80)\n\n|Zk|\n\nzi∈Zk ∥zi − ck∥2:\n\n(cid:88)\n\n∥zi − ck∥2\n\nK (cid:88)\n\ni=1\n\nK (cid:88)\n\n=\n\nzi∈Zk (cid:32)\n\n(cid:88)\n\ni=1\n\nzi∈Zk\n\nK (cid:88)\n\n=\n\n\n\n\n\n(cid:88)\n\n∥zi∥2 − 2\n\ni=1\n\nzi∈Zk\n\nK (cid:88)\n\n=\n\n\n\n\n\n(cid:88)\n\n∥zi∥2 −\n\ni=1\n\nzi∈Zk\n\n∥zi∥2 − 2\n\n(cid:88)\n\nzi∈Zk\n\n(cid:33)\n\nz⊤ i ck + |Zk|∥ck∥2\n\n1 |Zk|\n\n(cid:88)\n\n(cid:88)\n\nzi∈Zk\n\nˆzi∈ ˆZk\n\nˆz⊤\n\ni zi +\n\n1 |Zk|\n\n(cid:88)\n\n(cid:88)\n\nzi∈Zk\n\nˆzi∈ ˆZk\n\n\n\nˆz⊤\n\ni zi\n\n\n\n1 |Zk|\n\n(cid:88)\n\n(cid:88)\n\nzi∈Zk\n\nˆzi∈Zk\n\n\n\nˆz⊤\n\ni zi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 |Zk|\n\n1 |Zk|\n\n1 |Zk|\n\nc=\n\n=\n\nc=\n\nK (cid:88)\n\ni=1\n\nK (cid:88)\n\ni=1\n\nK (cid:88)\n\ni=1\n\n(cid:88)\n\n(cid:88)\n\n∥zi∥2 −\n\nzi∈Zk\n\nˆzi∈ ˆZk\n\n1 |Zk|\n\n(cid:88)\n\n(cid:88)\n\nˆzi∈Zk\n\nzi∈Zk \n\n(cid:88)\n\n(cid:88)\n\n(cid:0)∥zi∥2 − ˆz⊤\n\ni zi\n\n(cid:1)\n\n\n\n\n\nˆz⊤\n\ni zi\n\n\n\nzi∈Zk\n\nˆzi∈ ˆZk\n\n\n\n(cid:88)\n\n(cid:88)\n\n(cid:0)−ˆz⊤\n\ni zi\n\n(cid:1)\n\n\n\nzi∈Zk\n\nˆzi∈ ˆZk\n\n(17)\n\nzi =\n\n(18)\n\n(19)\n\nBy comparing Eq. (17) and Eq. (19), the only difference is that Eq. (19) includes more positive pairs for loss calculation. Hence, minimizing Eq. (17) can be viewed as approximately minimizing Eq. (19) or Eq. (18) through sampling positive pairs. As demonstrated in the work (Boudiaf et al., 2020), Eq. (18) can be interpreted as a conditional cross-entropy between Z and another random variable ̄Z, whose conditional distribution given Y is a standard Gaussian centered around cY : Z | Y ∼ N (cY , I):\n\n(cid:88)\n\n∥zi − ck∥2 = H(Z | Y ) + DKL(Z|| ̄Z | Y ) ≥ H(Z | Y )\n\n(20)\n\nHence, minimizing the first term in Eq. (6) is approximately minimizing H(Z|Y ).\n\nzi∈Zk\n\nDiscussion: We note that the assumption “the mean of the representations obtained from Z and ˆZ are the same” can be inferred by the first assumption about data augmentation. Let pk(X) denote the distribution of samples with class k and let x ∼ pk(X) denote the sample with class k. Recall that we assume the data augmentation function A(·) is strong enough to generate a data view that can simulate the test data from the same class. In this regard, the new data view can be regarded as an independent sample from the same class, i.e., A(x) ∼ pk(X). Hence, the expectation of Z and ˆZ is the same and we would approximately have that “the mean of Z and ˆZ is the same for each class”. Particularly, when the number of samples is relatively large, the mean of Z ( ˆZ) would be\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nclose to the true distribution mean. For example, on one graph of Cora, the mean absolute difference between the two mean representations of Z and ˆZ are [0.018, 0.009, 0.021, 0.024, 0.016, 0.014, 0.0, 0.016, 0.023] for each class, which are actually very small.\n\nA.3 A FIGURATIVE EXAMPLE\n\nIn Figure 6, we show an example of adversarial attack which causes the aggregated features for two nodes to be the same. Given two nodes x1 and x2 and their connections, we are interested in predicting their labels. Assume a mean aggregator is used for aggregating features from the neighbors. Before attack, the aggregated features for them are ̄x1 = [0.45] and ̄x2 = [0.53] while after attack the aggregated features become the same ̄x1 = ̄x2 = [0.45]. In this context, it is impossible to learn a classifier that can distinguish the two nodes.\n\nFigure 6: Given two nodes x1 and x2 and their connections, we are interested in predicting their labels. The color indicates the node label and “[0.3]” suggests that the associated node feature is 0.3. Assume a mean aggregator is used for aggregating features from the neighbors. Left: we show the clean graph without adversarial attack. The aggregated features for the two center nodes are ̄x1 = [0.45] and ̄x2 = [0.53]. Right: we show the attacked graph where the red edge indicates the adversarial edge injected by the attacker. The aggregated features for the two center nodes become ̄x1 = [0.45] and ̄x2 = [0.45].\n\nB ALGORITHM\n\nWe show the detailed algorithm of GTRANS in Algorithm 1. In detail, we first initialize ∆A and ∆X′ as zero matrices and calculate Ls based on Eq. (6). Since we alternatively optimize ∆A and ∆X′, we update ∆X′ every τ1 epochs and update ∆A every τ2 epochs. When the optimization is done, we sample the discrete graph structure for K times and select the one that results in the smallest Ls as the final adjacency matrix.\n\nC DATASETS AND HYPER-PARAMETERS\n\nIn this section, we reveal the details of reproducing the results in the experiments. We will release the source code upon acceptance.\n\nC.1 OUT-OF-DISTRIBUTION (OOD) SETTING\n\nThe out-of-distribution (OOD) problem indicates that the model does not generalize well to the test data due to the distribution gap between training data and test data (Yang et al., 2021a), which is also referred to as distribution shifts. Numerous research studies have been conducted to explore this problem and propose potential solutions (Ganin et al., 2016; Zhu et al., 2021a; Yang et al., 2021b;c; Wu et al., 2022a; Liu et al., 2022a; Chen et al., 2022; Buffelli et al., 2022; Gui et al., 2022; Wu et al., 2022b; You et al., 2023). In the following, we introduce the datasets used for evaluating the methods that tackle the OOD issue in graph domain.\n\nDataset Statistics. For the evaluation on OOD data, we use the datasets provided by Wu et al. (2022a). The dataset statistics are shown in Table 5, which includes three distinct type of distribution shifts: (1) artificial transformation which indicates the node features are replaced by synthetic spurious features; (2) cross-domain transfers which means that graphs in the dataset are from different domains and (3) temporal evolution where the dataset is a dynamic one with evolving nature. Notably, we use the datasets provided by Wu et al. (2022a), which were adopted from the\n\n17\n\nAttack[0.3][0.3][0.8][0.2][0.7][0.7][0][0.2]x1x2[0.3][0.3][0.8][0.2][0.7][0.7][0.2]x1x2 x#1=0.45,x#2=0.53x#1=0.45,x#2=0.45[0]Published as a conference paper at ICLR 2023\n\nAlgorithm 1: GTRANS for Test-Time Graph Transformation 1 Input: Pre-trained model fθ and test graph dataset GTe = (A, X′). 2 Output: Model prediction ˆY and transformed graph G′ = (A′, X′′). 3 Initialize ∆A and ∆X as zero matrices 4 for t = 0, . . . , T − 1 do 5\n\nCompute A′ = A ⊕ ∆A and X′ = X + ∆X Compute Ls(∆A, ∆X) as shown in Eq. (6) if t%(τ1 + τ2) < τ1 then\n\nUpdate ∆X ← ∆X − η1∇∆X Ls\n\nelse\n\nUpdate ∆A ← ΠP (∆A − η∇∆A Ls)\n\n6\n\n7\n\n8\n\n9\n\n10\n\n14\n\n15\n\n16\n\n17\n\n11 lbest = ∞ # store the best loss 12 for k = 0, . . . , K − 1 do 13\n\n0 ∼ Bernoulli(A ⊕ ∆A)\n\nSample A′ Calculate Ls with A′ if Ls < lbest then lbest = Ls A′ = A′\n\n0\n\n0 as input\n\n18 X′ = X + ∆X 19 ˆY = fθ(A′, X′) 20 Return: ˆY, (A′, X′)\n\naforementioned references with manually created distribution shifts. Note that there can be multiple training/validaiton/test graphs. Specifically, Cora and Amazon-Photo have 1/1/8 graphs for training/validation/test sets. Similarly, the splits are 1/1/5 on Twitch-E, 3/2/3 on FB-100, 5/5/33 on Elliptic, and 1/1/3 on OGB-Arxiv.\n\nTable 5: Summary of the experimental datasets that entail diverse distribution shifts.\n\nDistribution Shift\n\nDataset\n\n#Nodes\n\n#Edges\n\n#Classes Train/Val/Test Split Metric\n\nAdapted From\n\nArtificial Transformation\n\nCora Amz-Photo\n\n2,703 7,650\n\n5,278 119,081\n\nCross-Domain Transfers\n\nTwitch-E FB100\n\n1,912 9,498 31,299 - 153,138 769 41,536 16,656 - 1,590,655\n\nTemporal Evolution\n\nElliptic OGB-Arxiv\n\n203,769 169,343\n\n234,355 1,166,243\n\n10 10\n\n2 2\n\n2 40\n\nDomain-Level Domain-Level\n\nAccuracy Accuracy\n\nYang et al. (2016) Shchur et al. (2018)\n\nDomain-Level Domain-Level\n\nROC-AUC Rozemberczki et al. (2021a) Accuracy\n\nTraud et al. (2012)\n\nTime-Aware Time-Aware\n\nF1 Score Accuracy\n\nPareja et al. (2020) Hu et al. (2020)\n\nHyper-Parameter Setting. For the setup of backbone GNNs, we majorly followed Wu et al. (2022a):\n\n(a) GCN: the architecture setup is 5 layers with 32 hidden units for Elliptic and OGB-Arxiv, and 2 layers with 32 hidden units for other datasets, and with batch normalization for all datasets. The learning rate is set to 0.001 for Cora and Amz-Photo, 0.01 for other datasets; the weight decay is set to 0 for Elliptic and OGB-Arxiv, and 0.001 for other datasets.\n\n(b) GraphSAGE: the architecture setup is 5 layers with 32 hidden units for Elliptic and OGB-Arxiv, and 2 layers with 32 hidden units for other datasets, and with batch normalization for all datasets. The learning rate is set to 0.01 for all datasets; the weight decay is set to 0 for Elliptic and OGBArxiv, and 0.001 for other datasets.\n\n(c) GAT: the architecture setup is 5 layers for Elliptic and OGB-Arxiv, and 2 layers for other datasets, and with batch normalization for all datasets. Each layer contains 4 attention heads and each head is associated with 32 hidden units. The learning rate is set to 0.01 for all datasets; the weight decay is set to 0 for Elliptic and OGB-Arxiv, and 0.001 for other datasets.\n\n(d) GPR: We use 10 propagation layers and 2 transformation layers with 32 hidden units. The learning rate is set to 0.01 for all datasets; the weight decay is set to 0 for Elliptic and OGB-Arxiv, and 0.001 for other datasets. Note that GPR does not contain batch normalization layers.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nFor the baseline methods, we tuned their hyper-parameters based on the validation performance. For DropEdge, we search the drop ratio in the range of [0, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 0.7]. For Tent, we search the learning rate in the range of [1e-2, 1e-3, 1e-4, 1e-5, 1e-6] and the running epochs in [1, 10, 20, 30]. For EERM, we followed the instruction provided by the original paper. For GTRANS, we alternatively optimize node features for τ1 = 4 epochs and optimize graph structure τ2 = 1 epoch. We adopt DropEdge as the augmentation function A(·) and set the drop ratio to 0.5. We use Adam optimizer for both feature learning and structure learning. We further search the learning rate of feature adaptation η1 in [5e-3, 1e-3, 1e-4, 1e-5, 1e-6], learning rate of structure adaptation η2 in [0.5, 0.1, 0.01], the modification budget B in [0.5%, 1%, 5%] of the original edges, total epochs T in [5, 10]. We note that the process of tuning hyper-parameters is quick due to the high efficiency of test-time adaptation as we demonstrated in Section 4.1.\n\nEvaluation Protocol. For ERM (standard training), we train all the GNN backbones using the common cross entropy loss. For DropEdge, we drop a certain amount of edges at each training epoch. For EERM, it optimizes a bi-level problem to obtain a trained classifier. Note that the aforementioned three methods do not perform any test-time adaptation and their model parameters are fixed during test. For the two test-time adaptation methods, Tent and GTRANS, we first obtain the GNN backbones pre-trained from ERM and adapt the model parameters or graph data at test time, respectively. Furthermore, Tent minimizes the entropy loss while GTRANS minimizes the contrastive surrogate loss.\n\nQuantifying Distribution Shifts. Following SR-GNN (Zhu et al., 2021a), we adopt central moment discrepancy (CMD) (Zellinger et al., 2017) as the measurement to quantify the distribution shifts in different graphs. Specifically, given a pre-trained model, we obtain its hidden representation on the training graph and test graphs, denoted as Ztr and Zte. Then we calculate their distance by the CMD metric, i.e., CMD(Ztr), Zte. We show the results in Table 6 and we can observe certain distribution shifts as these values are not small. Let’s take the OGB-Arxiv dataset as an example, where we select papers published before 2011 for training, 2011-2014 for validation, and within 2014-2016/2016In 2018/2018-2020 for test. Table 7, we show the CMD values, ERM performance and GTRANS performance. From the table, we can find that (1) the CMD value on the validation graph is essentially smaller than those on test graphs; and (2) GCN performances on test graphs (with larger shifts) are lower than that on the validation graph.\n\nIn this context, the distribution shift is from the temporal change.\n\nTable 6: CMD values on each individual graph based on the pre-trained GCN.\n\nGraphID\n\nAmz-Photo Cora Elliptic OGB-Arxiv FB-100 Twitch-E\n\nG0\n\n6.4 5.4 80.2 14.7 29.7 8.6\n\nG1\n\n5.1 4.2 90.8 20.6 16.9 6.1\n\nG2\n\n5.5 4.8 114.3 10.4 32.9 9.0\n\nG3\n\n3.7 6.3 86.5 -\n- 8.4\n\nG4\n\n2.8 5.5 789.3 -\n- 9.7\n\nG5\n\n3.7 4.8 781.6 -\n- -\n\nG6\n\n3.9 4.6 99.4 -\n- -\n\nG7\n\n6.6 5.4 100.4 -\n- -\n\nG8\n\n- -\n150.6 -\n- -\n\nTable 7: CMD values and the performances of ERM and GTRANS on OGB-Arxiv\n\nMethod\n\n2011-2014 (Val)\n\n2014-2016\n\n2014-2016\n\n2016-2018\n\n2018-2020\n\nCMD\n\nERM GTRANS\n\n2.5\n\n14.7\n\n14.7\n\n20.6\n\n10.4\n\n45.32±0.50 45.82±0.38\n\n41.29±1.13 44.03±0.95\n\n41.29±1.13 44.03±0.95\n\n38.69±1.33 41.90±1.28\n\n35.78±1.81 38.81±1.47\n\nC.2 ABNORMAL FEATURES\n\nDataset Statistics. In these two settings, we choose the original version of popular benchmark datasets Cora, Citeseer, Pubmed and OGB-Arxiv. The statistics for these datasets are shown in Table 8. Note that we only have one test graph, and the injection of abnormal features or adversarial attack happens after the training process of backbone model, which can be viewed as evasion attack.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nTable 8: Dataset statistics for experiments on abnormal features and adversarial attack.\n\nDataset\n\nClasses\n\nNodes\n\nEdges\n\nFeatures\n\nTraining Nodes Validation Nodes\n\nTest Nodes\n\nCora Citeseer Pubmed OGB-Arxiv\n\n7 6\n3 40\n\n2708 3327 19717 169343\n\n5278 4552 44324 1166243\n\n1433 3703 500 128\n\n20 per class 20 per class 20 per class 54%\n\n500 500 500 18%\n\n1000 1000 1000 28%\n\nHyper-Parameter Settings. We closely followed AirGNN (Liu et al., 2021a) to set up the hyperparameters for the baselines:\n\n(a) GCN: the architecture setup is 2 layers with 64 hidden units without batch normalization for Cora, Citeseer and Pubmed, and 3 layers with 256 hidden units with batch normalization for OGB-Arxiv. The learning rate is set to 0.01.\n\n(b) GAT: the architecture setup is 2 layers with 8 hidden units in each of the 8 heads without batch normalization for Cora, Citeseer and Pubmed, and 3 layers with 32 hidden units in each of the 8 heads with batch normalization for OGB-Arxiv. The learning rate is set to 0.005.\n\n(c) APPNP: the architecture setup is 2-layer transformation with 64 hidden units and 10-layer propagation without batch normalization for Cora, Citeseer and Pubmed; the architecture is set to 3-layer transformation with 256 hidden units and 10-layer propagation with batch normalization for OGB-Arxiv. The learning rate is set to 0.01.\n\n(d) AirGNN: The architecture setup is the same as APPNP and the hyper-parameter λ is set to 0.3\n\nfor OGB-Arxiv and 0.5 for other datasets.\n\n(e) AirGNN-t: The architecture setup is the same as AirGNN but we tune the hyper-parameter λ in AirGNN based on performance on the combination of training and validation nodes at test stage. This is because the test graph has the same graph structure as the training graph; thus we can take advantage of the label information of training nodes (as well as validation nodes) to tune the hyper-parameters. Specifically, we search λ in the range of [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] for each noise ratio.\n\nFor the setup of GTRANS, we alternatively optimize node features for τ1 = 4 epochs and optimize graph structure τ2 = 1 epoch. We adopt DropEdge as the augmentation function A(·) and set the drop ratio to 0.5. We use Adam optimizer for both feature learning and structure learning. We further search the learning rate of feature adaptation η1 in [1, 1e-1, 1e-2], total epochs T in [10, 20]. The modification budget B to 5% of the original edges and the learning rate of structure adaptation η2 is set to 0.1. It is worth noting that we use a weighted combination of contrastive loss and training classification loss, i.e., Ltrain + λLs, instead of optimizing the contrastive loss alone. We adaopt this strategy because that the training graph and the test graph were the same graph before the injection of abnormal features. Here the λ is tuned in the range of [1e-2, 1e-3, 1e-4]. We study the effects of contrastive loss and training classification loss in Appendix D.7.\n\nC.3 ADVERSARIAL ATTACK\n\nDataset Statistics. We used OGB-Arxiv for the adversarial attack experiments and the dataset statistics can be found in Table 8. Again, we only have one test graph for this dataset.\n\nHyper-Parameter Settings. The setup of GCN and GAT is the same as that in the setting of abnormal features. For the defense methods including SimPGCN, RobustGCN and GCNJaccard, we use the DeepRobust (Li et al., 2020) library to implement them. For GCNJaccard, we tune its threshold hyper-parameter in the range of [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]. The hyper-parameter is also tuned based on the performance of training and validation nodes (same as Appendix C.2). Note that the popular defenses ProGNN (Jin et al., 2020) and GCNSVD (Entezari et al., 2020) were not included because they throw OOM error due to the expensive eigen-decomposition operation.\n\nWe use the official implementation of the scalable attack PR-BCD (Geisler et al., 2021) to attack the test graph. We note that when performing adversarial attacks, the setting is more like transductive setting where the training graph and test graph are the same. However, the test graph becomes different from the training graph after the attack. Since the training graph and test graph were originally the same graph, we use a weighted combination of contrastive loss and training classification loss,\n\n20\n\nPublished as a conference paper at ICLR 2023\n\ni.e., Ltrain + λLs, instead of optimizing the contrastive loss alone. For the setup of GTRANS, we alternatively optimize node features for τ1 = 1 epoch and optimize graph structure τ2 = 4 epochs. We fix the learning rate of feature adaptation η1 to 1e-3, learning rate of structure adaptation η2 to 0.1, λ to 1, total epochs T to 50 and modification budget B to 30% of the original edges.\n\nC.4 HARDWARE AND SOFTWARE CONFIGURATIONS.\n\nWe perform experiments on NVIDIA Tesla V100 GPUs. The GPU memory and running time reported in Table 2 are measured on one single V100 GPU. Additionally, we use eight CPUs, with the model name as Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz. The operating system we use is CentOS Linux 7 (Core).\n\nD MORE EXPERIMENTAL RESULTS\n\nD.1 COMPARISON TO GRAPH DOMAIN ADAPTATION\n\nOur work is related to graph domain adaptation (GraphDA) (Shen et al., 2020; Wu et al., 2020; Zhu et al., 2021a), but they are also highly different. In Table 9, we summarize the differences between GraphDA and GTRANS. In detail, there are the following differences:\n\n(a) Data and losses: GraphDA methods optimize the loss function based on both labeled source data (training data) and unlabeled target data (test data), while GTRANS only requires target data during inference. Hence, GraphDA methods are infeasible when access to the source data is prohibited such as online service.\n\n(b) Parameter: To our best knowledge, existing GraphDA methods are model-centric approaches while GTRANS is a data-centric approach. GTRANS adapts the data instead of the model, which can be more useful in some settings as we showed in the Example of Section 3.3.\n\n(c) Efficiency: GraphDA is indeed a training-time adaptation and for each given test graph, it would require training the model on the source and target data. Thus, it is much less efficient than GTRANS, especially when we have multiple test graphs (e.g., 33 test graphs for Elliptic).\n\nTable 9: Comparison between GraphDA and GTRANS. They differ by their data and losses.\n\nSetting\n\nSource\n\nTarget\n\nTrain Loss\n\nTest Loss Parameter Efficiency\n\nGraphDA GTRANS\n\nGtr -\n\nGte Gte\n\nL(Gtr, Ytr) + L(Gtr, Gte) -\n\n- L(Gte)\n\nfθ Gte\n\nLow High\n\nTo compare their empirical performance, we include two GraphDA methods (SR-GNN (Zhu et al., 2021a) and UDA-GCN (Wu et al., 2020)) and one general domain adaptation method (DANN (Ganin et al., 2016)). SR-GNN regularizes the model’s performance on the source and target domains. Note that SR-GNN is originally developed under the transductive setting where the training graph and test graph are the same. To apply SR-GNN in our OOD setting, we assume the test graph is available during the training stage of SR-GNN, as typically done in domain adaptation methods. UDA-GCN is another work that tackles graph data domain adaptation, which exploits local and global information for different domains. In addition, we also include DANN, which adopts an adversarial domain classifier to promote the similarity of feature distributions between different domains. We followed the authors’ suggestions in their paper to tune the hyper-parameters and the results are shown in Table 10. On the one hand, we can observe that GraphDA methods generally improve the performance of GCN under distribution shift and SRGNN is the best performing baseline. On the other hand, GTRANS performs the best on all datasets except Amz-Photo. On Amz-Photo, GTRANS does not improve as much as SR-GNN, which indicates that joint optimization over source and target is necessary for this dataset. However, recall that domain adaptation methods are less efficient due to the joint optimization on source and target: the adaptation time of SR-GNN on the 8 graphs of Amz-Photo is 83.5s while that of GTRANS is 4.9s (plus pre-training time 10.1s). Overall, test-time graph transformation exhibits strong advantages of effectiveness and efficiency.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nTable 10: Performance comparison between GTRANS and graph domain adaptation methods.\n\nMethod\n\nAmz-Photo\n\nCora\n\nElliptic\n\nFB-100\n\nOGB-Arxiv\n\nTwitch-E\n\n93.79±0.97 ERM UDA-GCN 91.70±0.35 94.08±0.21 DANN 94.64±0.17 SRGNN\n\n91.59±1.44 92.65±0.46 92.89±0.64 94.08±0.28\n\n50.90±1.51 51.57±1.31 53.00±0.97 51.94±0.81\n\n54.04±0.94 54.11±0.54 51.53±1.47 54.08±1.10\n\n38.59±1.35 39.43±0.71 36.60±1.26 38.92±0.65\n\n59.89±0.50 52.12±0.38 60.13±0.53 59.21±0.51\n\nGTRANS\n\n94.13±0.77\n\n94.66±0.63\n\n55.88±3.10\n\n54.32±0.60\n\n41.59±1.20\n\n60.42±0.86\n\nD.2 COMPARISON TO GRAPH STRUCTURE LEARNING\n\nOur work is also relevant to graph structure learning (GSL) (Franceschi et al., 2019; Jin et al., 2020; Chen et al., 2020; Zhao et al., 2021b; Rozemberczki et al., 2021b; Halcrow et al., 2020; Fatemi et al., 2021) which learns the graph structure during the training time while not adapting the graph structure at test stage. Our proposed test-time graph transform is essentially different from these works as we do not modify the training data but only the test data. It can be of interest to adopt GSL method at test time by also adapting the test graph structure. However, most existing GSL methods optimize the cross entropy loss defined on the labels to update graph structure, thus not applicable in the absence of test labels. One exception is SLAPS (Fatemi et al., 2021) which utilizes a selfsupervised loss together with the cross entropy loss to optimize the graph structure. However, the default setting in SLAPS is generating structure for raw data points (with no given graph structure). Hence, using SLAPS for our settings requires considerable changes. Furthermore, we highlight two more weaknesses of SLAPS compared to GTRANS.\n\n(a) Introducing additional parameters. SLAPS uses a denoising loss as self-supervision.\n\nIn detail, it first injects noise into node features and trains a denoising autoencoder to denoise the noisy features. This introduces additional parameters from the denoising autoencoder and inevitably changes the model architecture.\n\n(b) Not learning features. As other GSL methods, SLAPS does not learn node features. We argue that feature learning is highly important under the abnormal feature setting as shown in Table 16. For example, structure learning only improves GCN by 2% on OGB-Arxiv while feature learning can improve GCN by 20%. Thus, without the feature learning component, the performance will significantly drop when encountering noisy features.\n\nSince GTRANS is highly versatile and we can use any self-supervised loss as the surrogate loss, we can simply replace the contrastive loss in Eq. (6) with the denoising loss of SLAPS instead of paying considerable efforts in adjusting SLAPS. We refer to the loss used for denoising as SLAPS loss and adopt it for TTGT. Note that we first train the parameters of the DAE used for denoising while keeping the pre-trained model fixed. Then we fix both DAE and the pre-trained model and optimize the test graph for TTGT. The results are shown in Table 11. From the table, we can observe that the SLAPS loss (or feature denoising loss) does not work as well as the contrastive loss.\n\nTable 11: Comparison between SLAPS loss and our contrastive loss.\n\nAmz-Photo\n\nCora\n\nElliptic\n\nFB-100\n\nOGB-Arxiv\n\nTwitch-E\n\nNone SLAPS Ls in Eq. (6)\n\n93.79±0.97 93.97±1.04 94.13±0.77\n\n91.59±1.44 91.41±1.23 94.66±0.63\n\n50.90±1.51 50.54±1.81 55.88±3.10\n\n54.04±0.94 54.08±0.76 54.32±0.60\n\n38.59±1.35 41.38±1.35 41.59±1.20\n\n59.89±0.50 59.85±0.68 60.42±0.86\n\nD.3 COMPARISON TO AD-GCL\n\nNext, we compare our method with a graph contrastive learning method with learnable augmentation AD-GCL (Suresh et al., 2021). Since AD-GCL is originally designed for graph classification as a pre-training strategy, the direct empirical comparison between AD-GCL and GTRANS is not easy. However, due to the flexibility of GTRANS, we can integrate AD-GCL into our TTGT framework, denoted as TTGT+AD-GCL. We present the empirical results in Table 12. We can observe that\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nTTGT+AD-GCL generally performs worse than GTRANS except on Amz-Photo, which indicates that GTRANS is a stronger realization of TTGT. Furthermore, we highlight some key differences between it and GTRANS.\n\n(a) AD-GCL requires optimization of a min-max problem which involves parameters of graph structure and model. Thus, adopting it for TTGT would change the pre-trained model architecture.\n\n(b) AD-GCL only augments the graph structure while not learning the features. We argue that feature learning is highly important under the abnormal feature setting as shown in Table 16. For example, structure learning only improves GCN by 2% on OGB-Arxiv while feature learning can improve GCN by 20%. Thus, without the feature learning component, the performance will significantly drop when encountering noisy features.\n\n(c) According to Eq. (9) in the AD-GCL paper, it calculates the similarities between all samples within each mini-batch. When we increase the batch size, we would easily get the out-ofmemory issue while a small mini-batch will slow down the learning process. As a consequence, TTGT+AD-GCL is less efficient than GTRANS: the adaptation time of TTGT+AD-GCL on OGB-Arxiv is 12.7s while that of GTRANS is 2.6s.\n\nTable 12: Comparison between GTRANS and AD-GCL under the TTGT framework.\n\nAmz-Photo\n\nCora\n\nElliptic\n\nFB-100\n\nOGB-Arxiv\n\nTwitch-E\n\nERM 93.79±0.97 91.59±1.44 50.90±1.51 54.04±0.94 38.59±1.35 59.89±0.50 TTGT+AD-GCL 94.96±0.52 92.38±1.35 54.38±2.77 53.81±0.87 39.16±0.98 59.78±0.65 60.42±0.86 GTRANS\n\n94.13±0.77 94.66±0.63 55.88±3.10 54.32±0.60 41.59±1.20\n\nD.4 OUT-OF-DISTRIBUTION (OOD) SETTING\n\nTo show the performance on individual test graphs, we choose GCN as the backbone model and include the box plot on all test graphs within each dataset in Figure 7. We observe that GTRANS generally improves over each test graph within each dataset, which validates the effectiveness of test-time graph transformation.\n\n(a) Amz-Photo\n\n(b) Elliptic\n\n(c) OGB-Arxiv\n\n(d) FB-100\n\n(e) Twitch-E\n\nFigure 7: Classification performance on individual test graphs within each dataset for OOD setting.\n\nD.5 ABNORMAL FEATURES\n\nFor each model, we present the node classification accuracy on all test nodes (i.e., both normal and abnormal ones) in Figure 8. GTRANS significantly improves GCN in terms of the performance on all test nodes for all datasets across all noise ratios. For example, on Cora with 30% noisy nodes,\n\n23\n\nT1T2T3T4T5T6T7T80.8500.8750.9000.9250.950AccuracyERMGTransT1T2T3T4T5T6T7T8T90.50.60.7AccuracyERMGTransT1T2T30.320.340.360.380.400.420.440.46AccuracyERMGTransT1T2T30.500.520.540.560.58AccuracyERMGTransT1T2T3T4T50.540.560.580.600.620.640.66AccuracyERMGTransPublished as a conference paper at ICLR 2023\n\nGTRANS improves GCN by 31.0% on overall test accuracy. These results further validate that the proposed GTRANS can produce expressive and generalizable representations.\n\n(a) Cora\n\n(b) Citeseer\n\n(c) Pubmed\n\n(d) OGB-Arxiv\n\nFigure 8: Overall node classification accuracy under the setting of abnormal features. GTRANS significantly improves the performance of GCN on both abnormal nodes and overall nodes.\n\nD.6\n\nINTERPRETATION ON THE REFINED GRAPH FOR ADVERSARIAL ATTACK SETTING\n\nTo understand the modifications made on the graph, we compare several properties among clean graph, attacked graph (20% perturbation rate), graph obtained by GCNJaccard, and graph obtained by GTRANS in Table 13. We follow the definition in (Zhu et al., 2020) to measure homophily; “Pairwise Feature Similarity” is the averaged feature similarity among all pairs of connected nodes; “#Edge+/-” indicates the number of edges that the modified graph adds/deletes compared to the clean graph. From Table 13, we observe that first, adversarial attack decreases homophily and feature similarity, but GTRANS and GCNJaccard promote such information to defend against adversarial patterns. Second, both GTRANS and GCNJaccard focus on deleting edges from the attacked graph, but GCNJaccard removes a substantially larger amount of edges, which may destroy clean graph structure and lead to suboptimal performance.\n\nTable 13: Statistics of modified graphs. GTRANS promotes homophily and feature similarity.\n\nGTRANS GCNJaccard Attacked\n\nClean\n\nHomophily Pairwise Feature Similarity #Edges #Edge+ #Edge-\n\n0.689 0.825 1,945k 108k 479k\n\n0.636 0.863 1,754k 118k 679k\n\n0.548 0.809 2,778k 463k 0.6k\n\n0.654 0.827 2,316k -\n-\n\nD.7 ABLATION STUDY ON SURROGATE LOSS\n\nSince we optimized a combined loss in the settings of abnormal features and adversarial attack, we now perform ablation study to examine the effect of each component. We choose GCN as the backbone model and choose 0.3 noise ratio for abnormal features. The results for abnormal features and adversarial attack are shown in Tables 14 and 15, respectively. “None” indicates the vanilla GCN without any test-time adaptation and “Combined” indicates jointly optimizing a combination of the two losses. From the two tables, we can conclude that (1) both Ls and Ltrain help counteract abnormal features and adversarial attack; and (2) optimizing the combined loss generally outperforms optimizing Ls or Ltrain alone.\n\nD.8 ABLATION STUDY ON FEATURE LEARNING AND STRUCTURE LEARNING\n\nIn this subsection, we investigate the effects of the feature learning component and structure learning component. We show results for abnormal features and adversarial attack in Tables 16 and 17, respectively. Note that “None” indicates the vanilla GCN without any test-time adaptation; “A′” or “X′” is the variants of GTRANS which solely learns structure or node features; “Both” indicates the method GTRANS that learn both structure and node features. From Table 16, we observe that (1) while both feature learning and structure learning can improve the vanilla performance, feature\n\n24\n\n0.100.150.200.250.300.350.40Ratio of Noisy Nodes0.00.10.20.30.40.50.60.70.8AccuracyGCNGATAPPNPAirGNNAirGNN-tGTrans0.100.150.200.250.300.350.40Ratio of Noisy Nodes0.00.10.20.30.40.50.60.7AccuracyGCNGATAPPNPAirGNNAirGNN-tGTrans0.100.150.200.250.300.350.40Ratio of Noisy Nodes0.00.10.20.30.40.50.60.70.8AccuracyGCNGATAPPNPAirGNNAirGNN-tGTrans0.100.150.200.250.300.350.40Ratio of Noisy Nodes0.00.10.20.30.40.50.60.70.8AccuracyGCNGATAPPNPAirGNNAirGNN-tGTransPublished as a conference paper at ICLR 2023\n\nTable 14: Performance comparison when optimizing different losses for abnormal feature setting. Both Ls and Ltrain help counteract abnormal features; optimizing the combined loss generally outperforms optimizing Ls or Ltrain alone.\n\nDataset\n\nNone\n\nAll Test Nodes Ls\n\nLtrain\n\nCombined\n\nNone\n\nAbnormal Nodes Ltrain\n\nLs\n\nCombined\n\nOGB-Arxiv 44.29±1.20 46.70±1.20 64.60±0.22 64.64±0.24 31.50±1.12 35.22±1.17 57.54±0.93 57.69±0.93 Citeseer 39.26±2.02 45.41±2.71 54.97±1.55 52.54±1.08 17.30±1.86 32.93±2.81 42.67±2.78 44.10±2.97 Cora 36.35±1.87 48.71±3.02 66.77±2.54 67.29±1.44 15.80±2.33 35.40±4.05 61.67±3.64 63.90±2.55 Pubmed 62.72±1.20 65.49±1.65 66.56±0.64 70.55±1.55 36.47±1.85 56.77±3.60 60.20±1.97 67.93±2.11\n\nTable 15: Performance comparison when optimizing different losses for adversarial attack setting. Both Ls and Ltrain help counteract adversarial attack; optimizing the combined loss generally outperforms optimizing Ls or Ltrain alone. r denotes the perturbation rate.\n\nLoss\n\nr=5%\n\nr=10%\n\nr=15%\n\nr=20%\n\nr=25%\n\nNone Ls Ltrain Combined\n\n57.47±0.54 62.40±0.45 65.54±0.25 66.29±0.25\n\n47.97±0.65 59.76±0.93 64.00±0.31 65.16±0.52\n\n38.04±1.22 57.85±1.03 62.99±0.34 64.40±0.38\n\n29.05±0.73 55.26±1.35 61.95±0.40 63.44±0.50\n\n19.58±2.32 52.64±2.35 61.55±0.58 62.95±0.67\n\nlearning is more powerful than structure learning; (2) combining them does not seem to further improve the performance but it achieves a comparable performance to sole feature learning. From Table 17, we observe that (1) while both feature learning and structure learning can improve the vanilla performance, structure learning is more powerful than feature learning; and (2) combining them can further improve the performance. From these observations, we conclude that (1) feature learning is more crucial for counteracting feature corruption and structure learning is more important for defending structure corruption; and (2) combining them always yields a better or comparable performance.\n\nTable 16: Ablation study on feature learning and structure learning for abnormal feature setting. While both feature learning and structure learning can improve the vanilla performance, feature learning is more powerful than structure learning. Combining them does not seem to further improve the performance but it achieves a comparable performance to sole feature learning.\n\nDataset\n\nNone\n\nAll Test Nodes X′ A′\n\nBoth\n\nNone\n\nAbnormal Nodes X′ A′\n\nBoth\n\nOGB-Arxiv 44.29±1.20 46.02±1.09 64.88±0.23 64.64±0.24 31.50±1.12 31.96±1.05 58.12±0.83 57.69±0.93 Citeseer 39.26±2.02 39.67±1.96 54.99±1.55 54.97±1.55 17.30±1.86 17.13±1.81 42.73±2.81 42.67±2.78 Cora 36.35±1.87 37.02±1.82 67.40±1.62 67.29±1.44 15.80±2.33 15.67±2.15 64.17±3.18 63.90±2.55 Pubmed 62.72±1.20 62.50±1.21 70.53±1.52 70.55±1.55 36.47±1.85 36.57±1.96 67.90±2.07 67.93±2.11\n\nD.9 COMPARING DIFFERENT SELF-SUPERVISED SIGNALS\n\nAs there can be other choices to guide our test-time graph transformation process, we examine the effects of other self-supervised signals. We choose the OOD setting to perform experiments and consider the following two parameter-free self-supervised loss:\n\n(a) Reconstruction Loss. Data reconstruction is considered as a good self-supervised signal and we can adopt link reconstruction (Kipf & Welling, 2016b) as the guidance. Minimizing the reconstruction loss is equivalent to maximizing the similarity for connected nodes, which encourages the connected nodes to have similar representations.\n\n(b) Entropy Loss. Entropy loss calculates the entropy of the model prediction. Minimizing the entropy can force the model to be certain about the prediction. It has been demonstrated effective in Tent (Wang et al., 2021) when adapting batch normalization parameters.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nTable 17: Ablation study on feature learning and structure learning for adversarial structural attack setting. While both feature learning and structure learning can improve the vanilla performance, structure learning is more powerful than feature learning. Combining them can further improve the performance.\n\nParam\n\nr=5%\n\nr=10%\n\nr=15%\n\nr=20%\n\nr=25%\n\nNone X′ A′ Both\n\n57.47±0.54 64.16±0.24 65.93±0.32 66.29±0.25\n\n47.97±0.65 61.59±0.29 64.31±0.71 65.16±0.52\n\n38.04±1.22 60.07±0.32 63.14±0.39 64.40±0.38\n\n29.05±0.73 59.04±0.49 61.42±0.58 63.44±0.50\n\n19.58±2.32 58.82±0.68 60.18±1.53 62.95±0.67\n\n(c) SLAPS Loss. SLAPS (Fatemi et al., 2021) utilizes self-supervision to guide the graph structure learning process. Specifically, it injects random noise into node features and employs a denoising autoencoder (DAE) to denoise the node features. We refer to the loss used for denoising as SLAPS loss and adopt it for TTGT. Note that we first train the parameters of the DAE used for denoising while keeping the pre-trained model fixed. Then we fix both DAE and the pre-trained model and optimize the test graph for TTGT.\n\nWe summarize the results in Table 18. From the table, we observe that in most of the cases, the above three losses underperform our proposed surrogate loss and even degrade the vanilla performance. It validates the effectiveness of our contrastive loss in guiding the test-time graph transformation.\n\nTable 18: Comparison of different self-supervised signals for OOD setting. The reconstruction loss and entropy loss generally underperform our proposed loss.\n\nAmz-Photo\n\nCora\n\nElliptic\n\nFB-100\n\nOGB-Arxiv\n\nTwitch-E\n\nNone Recon Entropy SLAPS Ls in Eq. (6)\n\n93.79±0.97 93.77±1.01 93.67±0.98 93.97±1.04 94.13±0.77\n\n91.59±1.44 91.37±1.41 91.54±1.14 91.41±1.23 94.66±0.63\n\n50.90±1.51 49.33±1.37 49.93±1.56 50.54±1.81 55.88±3.10\n\n54.04±0.94 53.94±1.03 54.29±0.97 54.08±0.76 54.32±0.60\n\n38.59±1.35 44.93±4.06 41.11±2.19 41.38±1.35 41.59±1.20\n\n59.89±0.50 59.17±0.77 59.48±0.64 59.85±0.68 60.42±0.86\n\nGradient Correlation. In Figure 2, we have empirically verified the effectiveness of Theorem 1 when adopting the surrogate loss in Eq. (6) as Ls. We further plot the values of ρ(G) with different surrogate losses (i.e., entropy, reconstruction and SLAPS) and Lc on one test graph in Cora in Figure 9. We can observe that a positive ρ(G) generally reduces the test classification loss. For example, when using entropy loss, the test loss generally reduces when ρ(G) is positive and starts to increase after ρ(G) becomes negative.\n\n(a) Entropy Loss\n\n(b) Reconstruction Loss\n\n(c) SLAPS Loss\n\nFigure 9: The relationship between ρ(G) and Lc when adopting different surrogate losses.\n\nD.10 SENSITIVITY TO HYPER-PARAMETER B\n\nIn this subsection, we examine the sensitivity of GTRANS’ performance with respect to the perturbation budget, i.e., hyper-parameter B. Specifically, we vary the value of B in the range of\n\n26\n\n1020304050Epoch0.40.20.00.20.40.6Correlation/Test LossTest loss cCorrelation (G)123456789101112131415Epoch0.00.10.20.30.40.5Correlation/Test LossTest loss cCorrelation (G)123456789101112131415Epoch0.00.10.20.30.40.5Correlation/Test LossTest loss cCorrelation (G)Published as a conference paper at ICLR 2023\n\n{0.5%, 1%, 5%, 10%, 20%, 30%} and perform experiments on the OGB-Arxiv dataset for the three settings in Table 19. Specifically, “Abn. Feat” stands for abnormal feature setting with 30% noisy feature while “Adv. Attack” stands for the adversarial attack setting with 20% perturbation rate. From the table, we can observe budget B has a smaller effect on OOD and abnormal feature settings while highly impacting the performance under structural adversarial attack. This is because most of the changes made by adversarial attack are edge injections as shown in Table 13, and we need to use a large budget B to remove adversarial patterns. By contrast, GTRANS is much less sensitive to the value of B in the other two settings.\n\nTable 19: The change of model performance when varying budget B on OGB-Arxiv.\n\nSetting\n\nB=0.5% B=1% B=5% B=10% B=20% B=30%\n\nOOD Abn. Feat. Adv. Attack\n\n40.52 64.78 56.66\n\n40.69 64.80 56.89\n\n41.32 64.64 58.30\n\n41.40 64.60 59.93\n\n41.70 64.57 62.31\n\n41.65 64.57 63.47\n\nD.11 DIFFERENT AUGMENTATIONS USED IN CONTRASTIVE LOSS\n\nIn Eq. (6), we used DropEdge as the augmentation function A(·) to obtain the augmented view. In practice, the choice of augmentation can be flexible and here we explore two other choices: node dropping (You et al., 2020) and subgraph sampling (Zhu et al., 2021b). We perform experiments on OOD setting with GCN as the backbone model and report the results in Table 20. Specifically, we adopt a ratio of 0.05 for node dropping, and ratios of 0.05 and 0.5 for DropEdge. From the table, we can observe that (1) GTRANS with any of the three augmentations can greatly improve the performance of GCN under distribution shift, and (2) different augmentations lead to slightly different performance on different datasets.\n\nTable 20: Performance of GTRANS with different augmentation used in contrastive loss.\n\nAugmentation\n\nAmz-Photo\n\nCora\n\nElliptic\n\nFB-100\n\nOGB-Arxiv\n\nTwitch-E\n\n94.45±0.70 95.00±0.65 56.57±2.99 54.15±0.60 39.95±1.11 60.38±0.74 Node Dropping Subgraph Sampling 94.18±0.75 94.95±0.64 55.40±3.00 54.51±0.56 41.44±1.17 60.52±0.80 94.43±0.68 95.10±0.66 56.78±2.86 54.17±0.60 40.19±1.08 60.31±0.74 DropEdge (0.05) 94.13±0.77 94.66±0.63 55.88±3.10 54.32±0.60 41.59±1.20 60.42±0.86 DropEdge (0.5)\n\nERM\n\n93.79±0.97 91.59±1.44 50.90±1.51 54.04±0.94 38.59±1.35 59.89±0.50\n\n27",
    "reference": "# Summary Of The Paper\n\nThis paper aims at improving the performance of GNN in the test time by transferring the node features and structure.\nMore specifically, the transformation is optimized by a contrastive surrogate loss with respect to the embedding from the pretrained model.\nExtensive experiments are conducted on different settings and datasets, the results somehow prove the effectiveness of the proposed method.\n\n# Strength And Weaknesses\n\n**Strength**\n- The motivation of this paper is interesting, the authors consider several critical problems in the test time of GNN, including OOD, feature anomaly and adversarial attack.\n- The paper is overall easy to follow.\n- The experiments are extensive.\n\n**Weakness**\n- The overall problem setting is quite similar to the graph domain adaptation(GraphDA) where the source domain graph data provides a pre-trained model and target domain is the test graph without labels. However, the authors do not mention this active research field as well as the baseline methods. In the GraphDA field, the transformation on the target domain is a quite common operation. Concerning the Equation (6), it is directly computed on the pre-trained model and serves as a weight/guidance on the trasnformation learning, this is an explicit domain adaptation step where the knowledge from the source domain is transformed to the target domain. The authors are required to clarify the difference between this paper and GraphDA task.\n- The so-called parameter free transformation is quite common in contrastive learning. In this paper, the dropedge is only a classic one and many other transformations such as feature shuffling, subgraph sampling can be explored.\n- The Theorem 1. is somehow weired, in the test time the graph is unlabelled and the classification loss is no longer appliable. In this situation, will theorem 1 affect the model training and transformation selection?\n- In the OOD setting, the improvements is not significant and the authors are expected to explain on it, especailly without comparing the GraphDA methods.\n- In the nosiy feature setting, the proposed method only works on the small dataset and seems not working on the larger dataset, the authors are expected to discuss on it.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is overall well written and easy to follow. Currently, I think the setting is quite similar to GraphDA which limits the technical contribution. However, if the authors can well distinguish this paper with GraphDA, I will reconsider the novelty. The reproducibility is good as the authors provide detailed setting in the appendix.\n\n# Summary Of The Review\n\nMy major concerns is the difference between the problem setting and the GraphDA methods.\nCurrently, I think they are quite similar so the technical contribution and baselines are somehow weak to me.\nHowever, if the authors can well address their difference, I will improve the final rating by reconsidering the technical contribution.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMODEL OBFUSCATION FOR SECURING DEPLOYED NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nMore and more edge devices and mobile apps are leveraging deep learning (DL) capabilities. Deploying such models on devices – referred to as on-device models – rather than as remote cloud-hosted services, has gained popularity as it avoids transmitting user’s data off of the device and for high response time. However, on-device models can be easily attacked, as they can be accessed by unpacking corresponding apps and the model is fully exposed to attackers. Recent studies show that adversaries can easily generate white-box-like attacks for an on-device model or even inverse its training data. To protect on-device models from whitebox attacks, we propose a novel technique called model obfuscation. Specifically, model obfuscation hides and obfuscates the key information – structure, parameters and attributes – of models by renaming, parameter encapsulation, neural structure obfuscation, shortcut injection, and extra layer injection. We have developed a prototype tool ModelObfuscator to automatically obfuscate on-device TFLite models. Our experiments show that this proposed approach can dramatically improve model security by significantly increasing the difficulty of extracting models’ inner information, without increasing the latency of DL models. Our proposed on-device model obfuscation has the potential to be a fundamental technique for on-device model deployment. Our prototype tool is publicly available at https://github.com/AnonymousAuthor000/Code2536.\n\n1\n\nINTRODUCTION\n\nNumerous edge and mobile devices are leveraging deep learning (DL) capabilities. Though DL models can be deployed on a cloud platform, data transmission between mobile devices and the cloud may compromise user privacy and suffer from severe latency and throughput issues. To achieve high-level security, users’ personal data should not be sent outside the device. To achieve high throughput and short response time, especially for a large number of devices, on-device DL models are needed. The capabilities of newer mobile devices and some edge devices keep increasing, with more powerful systems on a chip (SoCs) and a large amount of memory, making them suitable for running on-device models. Indeed, many intelligent applications have already been deployed on devices (Xu et al., 2019) and benefited millions of users.\n\nUnfortunately, it has been shown that on-device DL models can be easily extracted. Then, the extracted model can be used to produce many kinds of attacks, such as adversarial attacks, membership inference attacks, model inversion attacks, etc. (Szegedy et al., 2013; Chen et al., 2017b; Shokri et al., 2017; Fang et al., 2020). The deployed DL model can be extracted by three kinds of attacks: (1) extracting the model’s weights through queries (Tram`er et al., 2016). (2) extracting the entire model from devices using software analysis (Vall ́ee-Rai et al., 2010) or reverse engineering (Li et al., 2021b). (3) extracting the model’s architecture by side-channel attacks(Li et al., 2021a).\n\nAccording to our observation, existing defense methods can be categorized into two different levels: (1) algorithm level, and (2) side-channel level. For securing the AI model at the algorithm level, some studies (Orekondy et al., 2019b; Kariyappa & Qureshi, 2020; Mazeika et al., 2022) propose methods to degenerate the effectiveness of query-based model extraction. While other studies (Xu et al., 2018; Szentannai et al., 2019; 2020) propose methods to train a simulating model, which has similar performance to the original model but is more resilient to extraction attacks. For securing\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthe AI model at the side-channel level, a recent work modifies the CPU and Memory costs to resist the model extraction attacks (Li et al., 2021a).\n\nAlthough many attacks have been proposed to extract DL models, it is hard for adversaries to precisely reconstruct DL models that are identical to the original ones using queries or side-channel information. These attack cannot access the inner information of the model, which means they are black-box attacks. In contrast, since on-device models are delivered in mobile apps and hosted on mobile devices, adversaries can easily unpack the mobile apps to extract the original models for exploitation. It will enable serious intellectual property leakage and adversaries can further generate white-box attacks, which are much more effective than black-box attacks (Zhang et al., 2022). Despite that model extraction using software analysis may lead to severe consequences, to the best of our knowledge, the community has not yet been aware of this attack, and no effective defense method has been proposed against it.\n\nIn this paper, we propose a novel model protection approach based on model obfuscation, which focuses on improving AI safety for resisting model extraction using software analysis. Given a trained model and its underlying DL library (e.g., PyTorch, TensorFlow, TFLite and etc.), an end2end prototype tool, ModelObfuscator, is developed to generate the obfuscated on-device model and the corresponding DL library. Specifically, ModelObfuscator first extracts the information of the target model and locates the source code in the library used by its layers. Then, it obfuscates the information of the models and builds a customized DL library that is compatible with the obfuscated model. To achieve this, we design five obfuscation methods, including: (1) renaming, (2) parameter encapsulation, (3) neural structure obfuscation, (4) random shortcut injection, and (5) random extra layer injection. These obfuscation methods significantly increase the difficulty of parsing the information of the model. The model obfuscation can prevent adversaries from reconstructing the model. In addition, adversaries also hard to transfer the trained weights and structure of models to steal intellectual property using model conversion because the connection between the obfuscated information and the original one is randomly generated. Experiments on 10 different models show that ModelObfuscator can against state-of-the-art model parsing and attack tools with a negligible time overhead and 20% storage overhead. Our contributions in this work include:\n\n• We propose the model obfuscation framework to hide the key information of deployed DL models at the software level. It can prevent adversaries from generating white-box attacks and stealing the knowledge of on-device models.\n\n• We design five obfuscation strategies for protecting on-device models and provide an end2end prototype tool, ModelObfuscator. This tool automatically obfuscates the model and builds a compatible DL software library. The tool is open-source available.\n\n• We provide a taxonomy and comparison of different obfuscation methods in terms of effectiveness\n\nand overhead, to guide model owners in choosing appropriate defense strategies.\n\n2 RELATED WORK\n\nModel Extraction Attacks and Defenses For model extraction attacks, adversaries can effectively extract the model in black-box setting. They can use collected samples to query the target model to reconstruct the target model (Tram`er et al., 2016; Papernot et al., 2017; Orekondy et al., 2019a; He et al., 2021; Rakin et al., 2022), or use the synthetic sample to steal the information of target models (Zhou et al., 2020; Kariyappa et al., 2021; Yuan et al., 2022; Sanyal et al., 2022). For defending against the model extraction attacks, various methods (Orekondy et al., 2019b; Kariyappa & Qureshi, 2020; Mazeika et al., 2022) have been proposed to degenerate the performance of model extraction attacks. Some methods (Szentannai et al., 2019; 2020) have been proposed to train a simulating model, which has similar performance to the original model, but can reduce the effectiveness of attacks. In addition, watermarking is also a promising method to defend against the model extraction (Yang et al., 2019; Fan et al., 2019; Lukas et al., 2019).\n\nAdversarial Machine Learning: Currently, adversaries can use many kinds of attacks to challenge the reliability of DL models, such as adversarial attacks, membership inference attacks, model stealing attacks, and model inversion attacks. For the adversarial attack, depending on the knowledge required by the adversary, adversarial attacks can be categorized into white-box attacks such as gradient-based attacks (Croce & Hein, 2020; Goodfellow et al., 2015; Kurakin et al., 2017; Papernot et al., 2016; Moosavi-Dezfooli et al., 2016; Madry et al., 2018; Moosavi-Dezfooli et al., 2017), and\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The working process of our ModelObfuscator on-device DL model obfuscation tool.\n\nblack-box attacks such as query-based attacks (Chen et al., 2017a; Ilyas et al., 2018a;b; Guo et al., 2019; Brendel et al., 2017; Cheng et al., 2018; Chen et al., 2020; Mopuri et al., 2018). For membership inference attacks, several studies (Shokri et al., 2017; Truex et al., 2019; Choquette-Choo et al., 2021; Carlini et al., 2022) challenge the privacy-preserving ability of model by predicting whether a sample is in the collected training set.\n\nCode obfuscation: Code obfuscation methods are initially developed for hiding the functionality of the malware. Then, the software industry also uses it against reverse engineering (Schrittwieser et al., 2016). They provide complex obfuscating algorithms for programs like JAVA code (Collberg et al., 1997; 1998), including robust methods for high-level languages (Wang, 2001) and machine code level (Wroblewski, 2002) obfuscation. Code obfuscation is a well-developed technique to secure the source code. However, traditional code obfuscation approaches are not capable of protecting on-device models, especially for protect the structure of the models and their parameters. In this work, inspired by traditional code obfuscation, we propose a novel model obfuscation approach to obfuscate the model files and then produce a corresponding DL library for them.\n\n3 METHODOLOGY\n\nThreat Model The on-device model is usually saved as a separate file (e.g., .tflite file) and packed into the app package. The attackers can either download the target app from the app markets (e.g., Google Play and iOS App store) or extract the app package file (e.g., APK file for Android, and IPA file for iOS) from the hosting devices. These app package files can then be decompiled by off-the-shelf reverse-engineering tools (e.g., Apktool 1 and IDA Pro 2) to get the original DL model file. Although many on-device DL platforms do not support some advanced functions like backpropagation, attackers can assemble the model architecture and weights into a differentiable model format, or they can use software analysis methods to generate attacks for the target model file (Li et al., 2021b; Huang & Chen, 2022). In this study, we will obfuscate the information of the model to disable the software analysis and model conversion tools, and generate a compatible DL library for the obfuscated model that only supports the forward inference function.\n\nWe chose the TensorFlow Lite (TFLite) platform to demonstrate our model obfuscation approach. TFLite is currently the most commonly used on-device DL platform. The steps to produce TFLite models are shown in the top half of Figure 1. Usually, DL developers use TensorFlow application programming interfaces (APIs) to define and train the TensorFlow (TF) model. The trained TF model is then compiled to a TFLite model. Note that TFLite has different implementations with TF, their operators (i.e., layers) may not be compatible. Therefore, the TFLite library will check the compatibility during compilation. Once the compatibility check passed, the compiled TFLite model can run on devices using the TFLite library. Hence, the problem of obfuscating TFLite models is to design obfuscation strategies and make the obfuscated model compatible with the TFLite library.\n\nWe analyzed the current mainstream DL platforms (i.e., TensorFlow and PyTorch) and identified the following two main findings. First, these platforms are open-source and provide a set of tools to build\n\n1https://ibotpeaches.github.io/Apktool/ 2https://hex-rays.com/ida-pro/\n\n3\n\nPythoncodeTensorFlowlibrarygenerateprogramcompile(3) model assembling(3) package compilationTFLite modelTF modelobfuscated TFLite modelTFLite libraryinferencemodifiedTFLite librarycompatibilitychecka. Producing TFLite model b. ModelObfuscatoruserRenamingParameter encapsulationNeural node obfuscationShortcutinjectionShortcutinjection(2) model obfuscation(1) model parsinginferenceTFLitesourcecodemodified sourcecodeUnder review as a conference paper at ICLR 2023\n\nthe library (e.g., TFLite library) from the source code. Second, they officially support customized operators (e.g., neural layers). Specifically, on top of these DL platforms, users could implement customized layers in C/C++ and compile customized layers to executing files (.so file in TensorFlow). Then, users can use these customized layers via high-level Python interfaces. Those features enable us to design obfuscation techniques to obfuscate model and DL library code together. The bottom half of Figure 1 shows the overview of our model obfuscation framework. Specifically, ModelObfuscator has three main steps: (1) model parsing, (2) model obfuscation, and (3) model assembling & library recompilation. In the following subsections, we detail our proposed ModelObfuscator.\n\n3.1 MODEL PARSING\n\nThe first step of ModelObfuscator model obfuscation is to parse the deployed model to extract its key information. ModelObfuscator first extracts the structure information of each layer, including the name of layers (e.g., Conv2D) and model structures (including model’s input, output, layer ID and etc.). The extracted structure information are depicted using a data structure as shown in the right example. Then, ModelObfuscator will extract the parameter of each layer. Moreover, ModelObfuscator will identify the source code used by each layer by referring to the underlying libraries. The identified source code includes relevant packages or functions of the TFLite layers.\n\n”Layer name”: ”Conv2D”, ”Input”: [0], ”Output”: [1], ”ID”: 0,\n\nMinimal model structure\n\n} · · ·\n\n{\n\n3.2 MODEL OBFUSCATION\n\nAfter getting the model information and corresponding source codes, ModelObfuscator will obfuscate the model as well as the source codes. ModelObfuscator uses five obfuscation strategies: renaming, parameter encapsulation, neural structure obfuscation, shortcut injection, and extra layer injection.\n\nObfuscated minimal model structure\n\n{\n\n”Layer name”: ”Tbuszp”, ”Input”: [0], ”Output”: [1], ”ID”: 0,\n\n} · · ·\n\nSource code registration (C/C++ code)\n\nRenaming The most straightforward obfuscation strategy is the renaming of a layer. Usually, the layer’s name contains important information, which is the function of this layer. For instance, “Conv2D” indicates a 2D convolution layer. Such information is useful for adversaries to reconstruct the model to generate white-box attacks or to obtain a similar surrogate model to conduct effective black-box attacks. To hide such important information, we randomly change each model layer’s name. On the right is an example of an obfuscated Conv2D layer. ModelObfuscator automatically replaces the real name with the random meaningless string Tbuszp. Meanwhile, ModelObfuscator creates a copy of Conv2D’s source code and replaces the layer name (i.e., Conv2D) in the source code with Tbuszp. Note that we modify the duplicate of Conv2D in case the modification affects other parts of the TFLite library. After adding modified source codes into the TFLite project, the recompiled TFLite library will recognize obfuscated layers as custom layers and correctly execute them at runtime.\n\nTfLiteRegistration* Register Conv2D() ↓\n\nTfLiteRegistration* Register Tbuszp()\n\nParameter encapsulation Existing TFLite models have two main assets: model structure and parameters. The parameter can be obtained in the training period. Given an input, a TFLite model will compute the results using the input tensor and parameters that are stored in the model file. As we discussed above, parameter exposure is very dangerous. An adversary could use the parameter information to perform many kinds of white-box attacks, e.g., adversarial attacks and model inversion attacks. Besides, an adversary can guess the function of this layer according to the shape of the parameter, because different layers have different numbers of parameters (e.g., two in the convolution layer) and different shapes of parameters (e.g., (3, 3, 64) in the convolution layer).\n\nTo hide key model parameter information, we instead encapsulate parameters into their corresponding generated custom source codes of the obfuscated layer. For example, for a simple one-layer feed-forward neural network, the output can be computed by Y = θ(W T X + b), where X, W , and b is input tensor, parameter of the layer, and bias, respectively. θ is the activation for neural nodes. For ModelObfuscator obfuscation, the network layer can be disguised as Y = g(X), where g is an unknown function. We then implement the correct computation (i.e., g) in the generated\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Neural structure obfuscation for a simple feed-forward neural network.\n\nFigure 3: Example of shortcut injection and extra layer injection for the TFLite model extract from a real-world app. (a) part of the original model. (b) the corresponding obfuscated model. This visualization is generated by Netron (Roeder, 2017).\n\ncustom TFLite source code, which we then obfuscate. At runtime, function g will be invoked to achieve the computation from X to Y . Now an adversary is unable to extract the key parameter information from our obfuscated model. Furthermore, the implementation of g can be obfuscated using transitional and well-proven code obfuscation strategies (Collberg & Thomborson, 2002). After compiling the modified TFLite, adversaries will find it very hard to identify key model parameters by reverse engineering the compiled library.\n\nNeural structure obfuscation Just obfuscating layer names and parameters is not enough, since an adversary may still infer the function of each layer according to the model structure. For instance, Figure 2(a) presents the structure of the neural network, where the input, hidden and output layers include four, two, and one node, respectively. Attacker could search for a surrogate model according to the neural architecture. To solve this problem, ModelObfuscator uses neural structure obfuscation to obfuscated neural architecture with the goal of confusing the adversary. We propose two strategies for network structure obfuscation: random and align-to-largest. Given a model with output shapes s = (s0, · · · , sn), where sn refers to the number of dimensions for the n-th channel, the random strategy generate a random shape r = (r0, · · · , rn) of the output for each layer. Figure 2(b) shows an obfuscated model of Figure 2(a) using random strategy. Second, the align-to-largest strategy finds the largest output shape s′ and then fill the output shapes of other layers to the size of s′. Figure 2(c) shows such an obfuscated model, where the output shapes of each layer are filled up to (4). Note that this will not affect the performance of models because the modified TFLite library will not compute the output using the provided neural structure information.\n\nShortcut injection & extra layer injection Neural structure obfuscation changes the network structure by inserting new nodes, but the spatial relationships of original layers remain the same. Therefore, even with the above three obfuscation strategies, the adversary can still infer node information by analyzing the spatial relationships of runtime data (e.g., actual input-output values of each node). To further obfuscate the model structure, hence, ModelObfuscator applies two more strategies: shortcut and extra layer injection. The injected shortcut and extra layers would destroy the original spatial relationships of TFLite models.\n\nTo automatically inject random shortcuts, ModelObfuscator first randomly select a shortcut pair (r1, r2). The outputs of r1-th layer are then added to the input list of r2-th layer. For example, the blue solid line in Figure 3(b) is a shortcut inserted into the model extracted from a fruit recognition app, which connects the 2-nd convolution layer and the 5-th convolution layer. Second, to inject extra layers, just like the shortcut injection, ModelObfuscator randomly picks a layer pair (r′ 2).\n\n1, r′\n\n5\n\n(a) Orginal neural nodes(b) Randomneural nodes(c) Unchangedneural nodes(a)(b)extra layer injectionshortcut injectionrenamingparameter encapsulationneural structure obfuscationUnder review as a conference paper at ICLR 2023\n\nTable 1: The obfuscation error of the proposed model obfuscation method. difference between the original model and the obfuscated model.\n\n‘Error’:\n\nthe output\n\n➀\n\n➁\n\n➂\n\n➃\n\n➄\n\n➅\n\n➆\n\n➇\n\n➈\n\n➉ Average\n\nError\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\nThe input of the extra layer is the output node list of r′ input list of r′ Figure 3(b)) is injected between the 2-nd and 3-rd convolution layers.\n\n1-th layer, and its output is added to the 2-th layer. For instance, an extra layer Ypxumu (as shown in the red dotted line in\n\nNote that the injected shortcut and layers will not affect the prediction results of the deployed model because the modified TFLite library will ignore the obfuscation part (e.g., Ypxumu layers) in model inference. Specifically, the extra layer Y = f (X) just needs to create the output with a specific shape to confuse the adversary. In addition, extra layer injection will not significantly increase the latency of on-device models because the extra layer does not need much computational resources.\n\nCombine five obfuscation strategies The proposed five obfuscation strategies are applied to the target model sequentially. Figure 3 show an example, where Figure 3(a) is the original model, while Figure 3(b) shows the final obfuscated model. The structure and parameters of the obfuscated model are quite different from that of the original model, which will make the obfuscated model hard to attack. To prevent an adversary from parsing the obfuscated model through reverse engineering the modified TFLite library and customized layers, we use code obfuscation strategies, a well-developed technology to make the code unreadable and unable to be reverse engineered (Appendix B). The combination of model and code obfuscation would hide the key information of models.\n\n3.3 MODEL ASSEMBLING AND LIBRARY RECOMPILATION\n\nAfter obtaining the obfuscated model and modified TFLite source codes, ModelObfuscator assembles the new obfuscated model using the obfuscated model structure and then recompiles the modified TFLite library to support the newly generated obfuscated model.\n\n4 EVALUATION\n\n4.1 EXPERIMENTAL SETTING\n\nDataset To evaluate ModelObfuscator’s performance on models with various structures for multiple tasks, we collected 10 TFLite models including a fruit recognition model, a skin cancer diagnosis model, MobileNet (Howard et al., 2017), MNASNet (Tan et al., 2019), SqueezeNet (Iandola et al., 2016), EfficientNet (Tan & Le, 2019), MiDaS (Ranftl et al., 2020), Inception-ResNetV2, PoseNet (Kendall et al., 2015), and SSD (Liu et al., 2016), which are referred to as model ➀ - ➉, respectively. The fruit recognition and skin cancer diagnosis model were collected from Android apps (see the provided code repository). The other models were collected from the TensorFlow Hub 3.\n\nExperimental Environment ModelObfuscator is evaluated on a workstation with Intel(R) Xeon(R) W-2175 2.50GHz CPU, 32GB RAM, and Ubuntu 20.04.1 operating system.\n\n4.2 EFFECTIVENESS OF ModelObfuscator\n\nWe first evaluate the effectiveness of ModelObfuscator. Ideally, ModelObfuscator should not affect the prediction accuracy of the original models, while providing sufficient defense against model attacks. To this end, we apply all five proposed obfuscation strategies to each model and compare the prediction results based on 1,000 randomly generated inputs. The obfuscation error is calculated as ||y−y′||2, where y and y′ is the output of original models and obfuscated models, respectively. Note that the number of extra layers and shortcuts is set to 30 in shortcut injection & extra layer injection. Table 1 demonstrates that ModelObfuscator model obfuscation strategies have no impact on the prediction results of the original models.\n\nTo show the effectiveness of ModelObfuscator in hiding the model’s key information, we try to extract the model information of obfuscated models using five software analysis tools. The tools\n\n3https://tfhub.dev/\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: The success number of existing software analysis tools to extract mobile models with each obfuscation strategy. ‘Basic obfuscation’: renaming + parameter encapsulation.\n\nTF-ONNX TFLite2ONNX TFLite2TF\n\nFlatBuffer App Attack\n\nOriginal Renaming Parameter encapsulation Neural structure Shortcut injection Extra layer injection\n\nBasic obfuscation\n\n10 0\n0 0\n0 0\n\n0\n\n9 0\n0 0\n0 0\n\n0\n\n9 0\n0 0\n0 0\n\n0\n\n10 0\n0 0\n0 0\n\n0\n\n8 8\n2 8\n8 8\n\n0\n\ninclude TF-ONNX (Developers, 2022) TFLite2ONNX (Wang, 2021), TFLite2TF (Hyodo, 2022), FlatBuffer (Li et al., 2021b), and Smart App Attack (Huang & Chen, 2022), which is proposed to attack the on-device models. In this experiment, we apply the basic obfuscation strategies renaming and parameter encapsulation on the original models. For model conversion tools, if they can successfully convert the model format, we consider it a successful case for extracting the model information. If ModelObfuscator method is effective, these tools cannot work on the models. For the app attacking method, if it can correctly identify the obfuscated model that has the same model structure as the original one on TensorFlow Hub, we consider it a success case. If ModelObfuscator is effective, the FlatBuffer extractor cannot parse the information of the obfuscated model and reverse it to the original one. As shown in Table 2, two basic obfuscation strategies can prevent all existing model extraction tools from parsing the deployed TFLite model. Besides, parameter encapsulation could prevent App Attack from finding surrogate models on 6 models. When combining renaming and Parameter encapsulation, the App Attack on all the models can be prevented. However, except for parameter encapsulation, applying other strategies separately does not confuse the App Attack, because App Attack can identify the same model through the parameter comparison.\n\nFigure 4: Visualization of structure obfuscation for LeNet. (a) original data flow of LeNet (b) obfuscated model with one shortcut and extra layer (c) obfuscated model with three shortcuts and three extra layers. Red dotted line and red dotted block represent injected shortcut and extra layer.\n\nAs we discussed above, adversaries may infer the functionality of each layer by analyzing the data flows of the model. To demonstrate the capability of ModelObfuscator in obfuscating the data flows, we show a visualization of LeNet (LeCun et al., 1998) before and after applying Shortcut injection & extra layer injection. The visualization of model structure and data flows before and after obfuscation are shown in Figure 4. As can be seen, it is difficult to recognize the shortcut or extra layer without prior knowledge of the model structure. When a large number (e.g., more than 30) of shortcuts and extra layers is used to obfuscate the TFLite, the structure of the obfuscated model will become extremely confused (Figure 7). Therefore, the shortcut and extra layer injection are effective in making it extremely difficult to understand the model structure.\n\n4.3 EFFICIENCY OF ModelObfuscator\n\nWe evaluate the efficiency of ModelObfuscator obfuscation strategies by demonstrating the runtime overhead of each model in our dataset. Specifically, we report both the time overhead and memory overhead of the proposed obfuscation method under various settings based on 1,000 randomly generated instances. The results of time and memory overhead are shown in Table 3 and Table 4, respectively. ‘(n1, n2)’ in the table indicates the the number of shortcuts (n1) and num-\n\n7\n\n(8, 128, 128)(8, 64, 64)(24, 48, 48)(24, 16, 16)(1, 256)(1,128)(8, 128, 128)(8, 64, 64)(24, 48, 48)(24, 16, 16)(1, 256)(1,128)(12, 24, 24)(8, 128, 128)(8, 64, 64)(24, 48, 48)(24, 16, 16)(1, 256)(1,128)(12, 24, 24)(24, 64, 64)(1, 512)(a)(b)(c)Under review as a conference paper at ICLR 2023\n\nTable 3: Time overhead (seconds per 1000 inputs) of the original model and obfuscated model. We use five obfuscation strategies. ‘(n1, n2)’: obfuscated models with n1 shortcuts and n2 extra layers.\n\n➀\n\n➁\n\n➂\n\n➃\n\n➄\n\n➅\n\n➆\n\n➇\n\n➈\n\n➉\n\nAverage\n\nOriginal (0, 0) (30, 0) (0, 10) (0, 20) (0, 30)\n\n30.3 30.4 30.7 30.2 30.6 30.4\n\n98.5 98.7 97.9 98.6 98.3 98.5\n\n62.1 61.1 62.3 61.1 63.3 63.2\n\n72.5 70.4 72.7 74.1 74.1 74.6\n\n33.4 33.5 33.1 34.9 36.8 34.5\n\n81.2 82.3 82.0 82.3 81.3 82.8\n\n346.6 346.4 346.9 346.7 348.2 349.0\n\n247.4 247.0 247.1 251.5 247.1 251.9\n\n130.4 130.7 130.2 130.9 130.5 130.4\n\n231.2 232.1 231.0 231.6 231.3 236.7\n\n133.4 133.3 133.4 134.2 134.2 135.2\n\nTable 4: Overhead of the model obfuscation on random access memory (RAM) cost (Mb per model). We use five obfuscation strategies. To eliminate the influence of other processes on the test machine, we show the increment of RAM usage for the model inference.\n\n➀\n\n➁\n\n➂\n\n➃\n\n➄\n\n➅\n\n➆\n\n➇\n\nOriginal (0, 0) (30, 0) (0, 10) (0, 20) (0, 30)\n\n18.8 18.9 18.7 25.9 30.0 35.9\n\n49.5 53.3 51.4 58.8 67.3 81.9\n\n33.1 33.1 32.6 42.5 50.4 55.2\n\n46.5 49.9 49.5 53.6 56.7 57.3\n\n41.3 45.5 46.0 49.2 56.0 71.6\n\n51.9 56.8 55.8 69.5 66.2 73.0\n\n241.1 244.8 244.6 248.1 253.1 267.7\n\n373.8 381.2 381.6 386.7 387.7 397.3\n\n➈\n\n39.4 40.6 40.3 55.0 66.9 73.4\n\n➉\n\nAverage\n\n97.8 99.8 100.2 107.5 112.0 123.8\n\n99.3 102.4 102.0 109.7 114.6 123.71\n\nber of extra layers (n2) applied, where (0, 0) indicates only the basic obfuscations (i.e., renaming, parameter encapsulation) and neural structure are involved. We also include the time and memory consumption of the original models as the baseline. As shown in Table 3, even though extra layers are injected into the obfuscated model, ModelObfuscator obfuscated models incur a negligible time overhead (i.e., approximately 1% on average for the most time-consuming obfuscation). The differences between using various obfuscation settings are also not significant. Because the parameter encapsulation will remove some data processing steps in the source code of APIs, the basic obfuscation (‘(0,0)’ in Table 3) may reduce the latency of TFLite models.\n\nThe memory overhead for ModelObfuscator obfuscated models is shown in Table 4. To eliminate the impact of different memory optimization methods, we use peak RAM usage where the model preserves all intermediate tensors. It can be seen that the other obfuscation strategies do not affect memory usage except for the extra layer injection. Memory usage increases when the number of extra layers increases. Therefore, a trade-off between the complexity of the obfuscation and the memory overhead is worth considering when choosing the obfuscation strategy. Nevertheless, we argue that even with the most complex settings in our experiment, which provide sufficient protection to the original model, the memory overhead is acceptable (approximately 20%).\n\nTable 5: Size change (Mb) of the TFLite library and models after the obfuscation. The size of the obfuscated model is reduced to a few Kb. The original library size is 191.5 Mb. ‘+’ and ‘-’ refer to the increase and decrease, respectively.\n\n➀\n\n➁\n\n➂\n\n➃\n\n➄\n\n➅\n\n➆\n\n➇\n\n➈\n\n➉\n\nlibrary-all model total\n\nlibrary-renaming\n\n+8.7 -5.5 +3.2\n\n+8.7\n\n+31.9 -16.9 +15.0\n\n+19.6 -10.3 +9.3\n\n+38.7 -17.5 +21.2\n\n+31.9\n\n+19.6\n\n+38.7\n\n+8.8 -5.0 +3.8\n\n+8.8\n\n+40.6 -18.6 +22.0\n\n+126.1 -66.3 +59.8\n\n+231.8 -121.1 +110.7\n\n+40.6\n\n+126.1\n\n+231.7\n\n+9.7 -5.0 +4.7\n\n+9.7\n\n+52.1 -27.5 +24.4\n\n+52.1\n\nConsidering that the models are deployed on mobile devices that have limited storage space, we also present the size differences of the modified TFLite software library and the obfuscated models. Note that the size change is caused by creating additional .so files to support the inference of the obfuscated layer. Hence, the size difference will be the same with different implementations (e.g., Python, Java). Table5 shows the size change to the TFLite software library and the TFLite models after applying ModelObfuscator obfuscation strategies. We use all obfuscation strategies in this experiment and the number of injecting shortcuts and extra layers is 30. Our results show that the library size change is mainly caused by the renaming method, because it will create a new API for the renamed layer in TFLite library. In addition, the extra layer injection will also increase the size of library because it will create the new API to support the extra layer. As the extra layer just\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Taxonomy of model obfuscation methods.\n\nhas simple function, the affect of the extra layer injection is negligible. Our obfuscation strategies significantly reduce the size of the TFLite model file because it only keeps the obfuscated minimal structure information. However, the size of the TFLite library is significantly increased, and increases the size of the application deployed on the mobile device.\n\n5 DISCUSSION\n\nTaxonomy of model obfuscation methods In this paper, we proposed five different model obfuscation strategies. Figure 5 shows a preliminary taxonomy of the different model obfuscation methods and the best practice for model deployment. First, developers can use the renaming and parameter encapsulation to prevent most model parsing or reverse engineering tools from extracting the information of the deployed model. In the scenarios that computational costs are critical, developers can use the neural structure obfuscation and shortcut injection, as they do not introduce additional overhead. For structure obfuscation, developers can use neural structure obfuscation, shortcut injection, and extra layer injection. These three methods can significantly increase the difficulty of understanding the data flow of the deployed models. In addition, developers can use renaming and neural structure obfuscation to disguise the deployed model, which can mislead the adversary into choosing the wrong architecture to produce a surrogate model. If the size of the app package is critical (e.g., deploying the model on devices with limited storage), developers need to carefully consider the trade-off between the number of obfuscated layers leveraging extra layer injection with renaming. The reason is that if renaming is used to create a different obfuscated layer for every layer, ModelObfuscator will need to create the corresponding APIs in the TFLite library to support the obfuscated layers, hence increasing the size of the library. Limitations Although the proposed model obfuscation does not introduce significant computational overhead, it will increase the size of the modified TFLite library. This is because we need to provide support for the new obfuscations made. For a huge network like a 1000-layer network deployed model (although it is unlikely to find such deployed model in real world), the size of the modified TFLite library will significantly increase if we rename every layer. As a result, the app package also increases as the modified TFLite library will also need to be deployed on the device. Extracting obfuscated model When ModelObfuscator obfuscates the model, it will create a cache file to guide the tool to generate a compatible DL library. Attackers cannot automatically extract the obfuscated model unless they obtain the cache file from the developer’s computer (which is unlikely to happen). Generally speaking, attackers must use reverse engineering to get source codes from the compiled library file and cost manual effort to understand them for extracting the obfuscated model.\n\n6 CONCLUSION\n\nIn this work, we analyzed the risk of deep learning models deployed on mobile devices. Adversaries can extract information from the deployed model to perform white-box attacks and steal its intellectual property. To this end, we proposed a model obfuscation framework to secure the deployment of DL models. We utilized five obfuscation methods to obfuscate the information of the deployed model, i.e., renaming, parameter encapsulation, neural node obfuscation, shortcut injection, and extra layer injection. We developed a prototype tool ModelObfuscator to automatically obfuscate a TFLite model and produce a compatible library with the model. Experiments show that our method is effective in resisting the model parsing tools without performance sacrifice. Considering the negligible extra resources required, and the extra security it achieves, we believe that model obfuscation has the potential to be a fundamental step for on-device model deployment in the future. In future works, optimizing model obfuscation to reduce overhead is worthwhile to be explored.\n\n9\n\nRenamingParameter encapsulationNeural node obfuscationShortcut injectionExtra layer injectionBasic obfuscationStructureobfuscationCamouflage ObfuscationFreeobfuscationSize-expand obfuscationUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nWieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.\n\nNicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy (SP), pp. 1897–1914. IEEE, 2022.\n\nJianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A query-efficient In 2020 ieee symposium on security and privacy (sp), pp. 1277–1294.\n\ndecision-based attack. IEEE, 2020.\n\nPin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15–26. ACM, 2017a.\n\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep\n\nlearning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017b.\n\nMinhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui Hsieh. QueryarXiv preprint\n\nefficient hard-label black-box attack: An optimization-based approach. arXiv:1807.04457, 2018.\n\nChristopher A Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot. Label-only membership inference attacks. In International conference on machine learning, pp. 1964–1974. PMLR, 2021.\n\nChristian Collberg, Clark Thomborson, and Douglas Low. A taxonomy of obfuscating transforma-\n\ntions, 1997.\n\nChristian Collberg, Clark Thomborson, and Douglas Low. Manufacturing cheap, resilient, and stealthy opaque constructs. In Proceedings of the 25th ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pp. 184–196, 1998.\n\nChristian S. Collberg and Clark Thomborson. Watermarking, tamper-proofing, and obfuscationtools for software protection. IEEE Transactions on software engineering, 28(8):735–746, 2002.\n\nFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206– 2216. PMLR, 2020.\n\nDevelopers. tf2onnx - Convert TensorFlow, Keras, Tensorflow.js and Tflite models to ONN, August\n\n2022. URL https://github.com/onnx/tensorflow-onnx.\n\nLixin Fan, Kam Woh Ng, and Chee Seng Chan. Rethinking deep neural network ownership verification: Embedding passports to defeat ambiguity attacks. Advances in neural information processing systems, 32, 2019.\n\nMinghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to {Byzantine-Robust} federated learning. In 29th USENIX Security Symposium (USENIX Security 20), pp. 1605–1622, 2020.\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. In ICLR, 2015.\n\nChuan Guo, Jacob Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Weinberger. Simple black-box adversarial attacks. In International Conference on Machine Learning, pp. 2484–2493, 2019.\n\nXinlei He, Jinyuan Jia, Michael Backes, Neil Zhenqiang Gong, and Yang Zhang. Stealing links from graph neural networks. In 30th USENIX Security Symposium (USENIX Security 21), pp. 2669–2686, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n\nYujin Huang and Chunyang Chen. Smart app attack: Hacking deep learning models in android apps.\n\nIEEE Transactions on Information Forensics and Security, 17:1827–1840, 2022.\n\nKatsuya Hyodo.\n\ntflite2tensorflow, 2022.\n\nURL https://github.com/PINTO0309/\n\ntflite2tensorflow.\n\nForrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.\n\nAndrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with\n\nlimited queries and information. In ICML, pp. 2142–2151, 2018a.\n\nAndrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial\n\nattacks with bandits and priors. arXiv preprint arXiv:1807.07978, 2018b.\n\nSanjay Kariyappa and Moinuddin K Qureshi. Defending against model stealing attacks with adaptive misinformation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2020.\n\nSanjay Kariyappa, Atul Prakash, and Moinuddin K Qureshi. Maze: Data-free model stealing attack using zeroth-order gradient estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13814–13823, 2021.\n\nAlex Kendall, Matthew Grimes, and Roberto Cipolla. Posenet: A convolutional network for realtime 6-dof camera relocalization. In Proceedings of the IEEE international conference on computer vision, pp. 2938–2946, 2015.\n\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.\n\nInternational Conference on Learning Representations (ICLR), 2017.\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n\nJingtao Li, Zhezhi He, Adnan Siraj Rakin, Deliang Fan, and Chaitali Chakrabarti. Neurobfuscator: A full-stack obfuscation tool to mitigate neural architecture stealing. In 2021 IEEE International Symposium on Hardware Oriented Security and Trust (HOST), pp. 248–258. IEEE, 2021a.\n\nYuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, and Yunxin Liu. Deeppayload: Black-box backdoor attack on deep learning models through neural payload injection. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 263–274. IEEE, 2021b.\n\nWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pp. 21–37. Springer, 2016.\n\nNils Lukas, Yuxuan Zhang, and Florian Kerschbaum. Deep neural network fingerprinting by con-\n\nferrable adversarial examples. arXiv preprint arXiv:1912.00888, 2019.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations(ICLR), 2018. URL https://openreview.net/forum?id= rJzIBfZAb.\n\nMantas Mazeika, Bo Li, and David Forsyth. How to steer your adversary: Targeted and efficient model stealing defenses with gradient redirection. In International Conference on Machine Learning, pp. 15241–15254. PMLR, 2022.\n\nSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2574–2582, 2016.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1765–1773, 2017.\n\nKonda Reddy Mopuri, Phani Krishna Uppala, and R Venkatesh Babu. Ask, acquire, and attack: Data-free uap generation using class impressions. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 19–34, 2018.\n\nTribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-box models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4954–4963, 2019a.\n\nTribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Prediction poisoning: Towards defenses\n\nagainst dnn model stealing attacks. arXiv preprint arXiv:1906.10908, 2019b.\n\nNicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P), pp. 372–387. IEEE, 2016.\n\nNicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications security, pp. 506–519, 2017.\n\nAdnan Siraj Rakin, Md Hafizul Islam Chowdhuryy, Fan Yao, and Deliang Fan. Deepsteal: Advanced model extractions leveraging efficient weight stealing in memories. In 2022 IEEE Symposium on Security and Privacy (SP), pp. 1157–1174. IEEE, 2022.\n\nRen ́e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 2020.\n\nLutz Roeder. Netron, Visualizer for neural network, deep learning, and machine learning models, December 2017. URL https://doi.org/10.5281/zenodo.7109451. If you use Netron in your research, please cite it using these metadata.\n\nSunandini Sanyal, Sravanti Addepalli, and R Venkatesh Babu. Towards data-free model stealing in a hard label setting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15284–15293, 2022.\n\nSebastian Schrittwieser, Stefan Katzenbeisser, Johannes Kinder, Georg Merzdovnik, and Edgar Weippl. Protecting software through obfuscation: Can it keep pace with progress in code analysis? ACM Computing Surveys (CSUR), 49(1):1–37, 2016.\n\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pp. 3–18. IEEE, 2017.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\nK ́alm ́an Szentannai, Jalal Al-Afandi, and Andr ́as Horv ́ath. Mimosanet: An unrobust neural network\n\npreventing model stealing. arXiv preprint arXiv:1907.01650, 2019.\n\nK ́alm ́an Szentannai, Jalal Al-Afandi, and Andr ́as Horv ́ath. Preventing neural network weight stealing via network obfuscation. In Science and Information Conference, pp. 1–11. Springer, 2020.\n\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural net-\n\nworks. In International conference on machine learning, pp. 6105–6114. PMLR, 2019.\n\nMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2820–2828, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFlorian Tram`er, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine learning models via prediction {APIs}. In 25th USENIX security symposium (USENIX Security 16), pp. 601–618, 2016.\n\nStacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. Demystifying membership inference attacks in machine learning as a service. IEEE Transactions on Services Computing, 2019.\n\nRaja Vall ́ee-Rai, Phong Co, Etienne Gagnon, Laurie Hendren, Patrick Lam, and Vijay Sundaresan. Soot: A java bytecode optimization framework. In CASCON First Decade High Impact Papers, pp. 214–224. 2010.\n\nChenxi Wang. A security architecture for survivability mechanisms. University of Virginia, 2001.\n\nZhenhua Wang.\n\ntflite2onnx - Convert TensorFlow Lite models to ONNX, August 2021. URL\n\nhttps://github.com/jackwish/tflite2onnx.\n\nGregory Wroblewski. General method of program code obfuscation. 2002.\n\nHui Xu, Yuxin Su, Zirui Zhao, Yangfan Zhou, Michael R Lyu, and Irwin King. Deepobfuscation: Securing the structure of convolutional neural networks via knowledge distillation. arXiv preprint arXiv:1806.10313, 2018.\n\nMengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, and Xuanzhe Liu. A first look at deep learning apps on smartphones. In The World Wide Web Conference, pp. 2125–2136, 2019.\n\nZiqi Yang, Hung Dang, and Ee-Chien Chang. Effectiveness of distillation attack and countermeasure\n\non neural network watermarking. arXiv preprint arXiv:1906.06046, 2019.\n\nXiaoyong Yuan, Leah Ding, Lan Zhang, Xiaolin Li, and Dapeng Oliver Wu. Es attack: Model IEEE Transactions on Emerging\n\nstealing against deep neural networks without data hurdles. Topics in Computational Intelligence, 2022.\n\nChaoning Zhang, Philipp Benz, Adil Karjauv, Jae Won Cho, Kang Zhang, and In So Kweon. Investigating top-k white-box and transferable black-box attack. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15085–15094, 2022.\n\nMingyi Zhou, Jing Wu, Yipeng Liu, Shuaicheng Liu, and Ce Zhu. Dast: Data-free substitute training In Proceedings of the IEEE/CVF Conference on Computer Vision and\n\nfor adversarial attacks. Pattern Recognition, pp. 234–243, 2020.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX - THE VISUALIZATION OF THE MODEL OBFUSCATION\n\nFigure 6: The visualization of the original SqueezeNet model. 2017).\n\nIt is plotted by Netron (Roeder,\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: The visualization of the obfuscated SqueezeNet model with all obfuscation strategies. This figure is plotted by Netron (Roeder, 2017). The number of injected shortcuts and extra layers is 30. The obfuscated model can run on the modified TFLite library. The computational time and memory overhead of model obfuscation for this model can be found in Figure 3 and 4. The size change can be found in Figure 5.\n\nB APPENDIX - EXAMPLE OF CODE OBFUSCATION\n\nThe following code is the ‘Eval’ Concatenation layer. plementation for the forward inference.\n\nthe It will finally call the ‘EvalImpl’ function, which is the detailed im-\n\nthe modified source code to support\n\nfunction of\n\nt e m p l a t e <K e r n e l T y p e k e r n e l t y p e > T f L i t e S t a t u s E v a l ( T f L i t e C o n t e x t * c o n t e x t , T f L i t e N o d e * node ) {\n\ni n t a x i s = params−>a x i s ;\n\nr e i n t e r p r e t c a s t <T f L i t e C o n c a t e n a t i o n P a r a m s *>( node−> b u i l t i n d a t a ) ;\n\n/ / a u t o * params = / / / / T f L i t e T e n s o r * o u t p u t ; TF LITE ENSURE OK ( c o n t e x t , G e t O u t p u t S a f e ( c o n t e x t , node , 0 , &o u t p u t ) ) ; i f\n\n( I s C o n s t a n t O r P e r s i s t e n t T e n s o r ( o u t p u t ) ) { i s c o m p u t e d i n P r e p a r e .\n\n/ / O u t p u t r e t u r n k T f L i t e O k ;\n\n} i f\n\n( a x i s < 0 ) a x i s += o u t p u t −>dims −> s i z e ;\n\nr e t u r n E v a l I m p l <k e r n e l\n\nt y p e >( c o n t e x t , node ,\n\na x i s , o u t p u t ) ;\n\n}\n\nAfter obfuscating the simple code by some simple obfuscation method (e.g., comments removal, functions and variables renaming, whitespaces removal. We use an open-source tool to implement it 4), the code will become the following form:\n\nt e m p l a t e <l p u r q a c g h d z w uowpojpkaqnn>T f L i t e S t a t u s o j m f j d q x l k r j ( T f L i t e C o n t e x t * z s f l m t a b f s l q , T f L i t e N o d e * node ) { gqgyxfwmpoax * o u t p u t ; TF LITE ENSURE OK ( z s f l m t a b f s l q , G e t O u t p u t S a f e ( z s f l m t a b f s l q , node , 0 , & o u t p u t ) ) ; i f ( I s C o n s t a n t O r P e r s i s t e n t T e n s o r ( o u t p u t ) ) { r e t u r n dwzzpibyhyhk <0) d w zz p i b y h y h k += o u t p u t −>dims −> s i z e ; r e t u r n t i z p e e x t a c q k <uowpojpkaqnn >( z s f l m t a b f s l q , node , dwzzpibyhyhk , o u t p u t ) ; }\n\nt s c i s z k l z s z y ; } i f (\n\n4https://github.com/whoward3/C-Code-Obfuscator\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nNote that this is just a simple example. For the case when the original model is a 30-layer neural network, it will create more than 10 thousand lines of code for supporting the obfuscated model, and then add the code to the TFLite project to compile a modified TFLite library. If adversaries use the reverse engineering method to extract the source code of the obfuscated layer, ideally they can get the above obfuscated code. But the code will be extremely difficult to read. This is the reason why we think developers can use code obfuscation to resist further reverse engineering for the source code of the modified TFLite library.\n\n16",
    "reference": "# Summary Of The Paper\n\nThis paper raises a bar against an adversary who aims to reverse-engineer efficient models deployed to edge devices. A naive deployment practice allows the attacker to just extract the model's architecture and parameters which work as a prior for other attacks (such as adversarial attacks). To address this issue, the paper proposes ModelObfuscator that converts a victim model's architecture and parameters. It also obfuscates the ML frameworks by obfuscating the function names (corresponding to layers) and the ways they perform the layerwise computations. In evaluation, the paper shows that the resulting models return the same outputs on a set of input samples even after applying the obfuscation, and the obfuscation method does not incur large extra computations.\n\n# Strength And Weaknesses\n\nStrengths:\n\n1. The paper studies a research problem that may carry some importance in the future.\n2. The paper proposes ModelObfuscator that employs six different ways to obfuscate a model.\n3. The paper shows the proposed mechanism does not change a model's behaviors nor increase its computations.\n\nWeaknesses:\n\n1. The approach relies on \"security by obscurity.\"\n2. The evaluation against existing reconstruction attacks and adaptive attacks should be done.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI like this paper as it studies a research problem that could be the problem soon. Reverse-engineering the models deployed to the edge may offer the white-box advantage to the attacker. It could be much easier to make other attacks on the model such as privacy attacks or adversarial attacks. Thus, the paper studies a defense.\n\nHowever, there are three major problems of the proposed approach:\n\n(1) Obfuscation cannot be a defense against reconstruction attacks. The paper cited several papers and insights from the software-engineering community, but those papers are unlikely to study code obfuscation as a defense. Rather, they study the obfuscation attacker to understand the limits of malware writers who aim to increase the time and effort of security analysts.\n\nAs a counter-example, the model extraction attacker who just queries the model to extract the parameters can still extract parameters precisely without reverse-engineering the model from the software stack.\n\nMoreover, (let's assume that we exclude the extraction attacks) if the victim really wants to obfuscate the model parameters, one way is to encrypt them completely by utilizing TEEs. By doing so, the attacker who naively reconstructs the model parameters at the software level cannot get the actual parameters.\n\nThus, I recommend those changes:\n\n> Clarify the threat model that the reconstruction attacker operates in.\n> Tone down the entire paper; ModelObfuscator is not a defense or doesn't work against all reconstruction attacks.\n> Clarify the goal of the mitigation mechanism.\n\n(2) There should be an evaluation against reconstruction attacks. Prior work proposes three types of reconstruction attacks: (1) the reconstruction of model parameters via query, (2) the reverse-engineering of the model from the software stack, and (3) the reconstruction of model architecture from side-channel information. This paper claims that ModelObfuscator can be a defense against reconstruction attacks, but there is no evaluation against any of those adversaries.\n\nThus: \n\n> I'd like to see the results on the effectiveness of ModelObfuscator against those three attacks.\n\n(3) There should be an evaluation against adaptive attacks. A conventional way to test the obfuscation mechanism is to assume that the attacker also has access to the same obfuscator (this is also used to evaluate the security of encryption mechanisms). Access to the obfuscator should not reduce the time and effort of the adversary who wants to reverse-engineer the victim model. \n\nThus:\n\n> I'd like to see the results of a scenario where the adversary can also use ModelObfuscator.\n\n# Summary Of The Review\n\nThis paper studies a research problem that can be important in the future (as we deploy more and more models to edge devices). To address this issue, the paper proposes a method that obfuscates the model internals (such as architectures, parameters, and computation details).\n\nHowever, this defense has the following major problems:\n\n(1) The defense heavily relies on \"security by obscurity.\"\n(2) The defense was not evaluated against any reconstruction attacks.\n(3) The defense was not evaluated against an adaptive adversary.\n\nThus, I believe this paper (as-is) has more flaws than strengths. But, I am happy to update my review score if the authors address all the requested changes.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nNo concern."
  },
  {
    "input": "Surrogate Gradient Design for LIF networks\n\nAnonymous authors Paper under double-blind review\n\nAbstract\n\nSpiking Neuromorphic Computing uses binary activity to improve Artificial Intelligence energy efficiency. However, the non-smoothness of binary activity requires approximate gradients, known as Surrogate Gradients (SG), to close the performance gap with Deep Learning. Several SG have been proposed in the literature, but it remains unclear how to determine the best SG for a given task and network. Good performance can be achieved with most SG shapes, after a costly search of hyper-parameters. Thus, we aim at experimentally and theoretically define the best SG across different stress tests, to reduce future need of grid search. Here we first show that more complex tasks and network need more careful choice of SG, and that overall the derivative of the fast sigmoid outperforms other SG across tasks and networks, for a wide range of learning rates. Secondly, we focus on the Leaky Integrate and Fire (LIF) spiking neural model, and we note that high initial firing rates, combined with a sparsity encouraging loss term, can lead to better generalization, depending on the SG shape. Finally, we provide a theoretical solution, inspired by Glorot and He initializations, to find a SG and initialization that experimentally result in improved accuracy. We show how it can be used to reduce the need of extensive grid-search of dampening, sharpness and tail-fatness.\n\n1\n\nIntroduction\n\nSpiking Neuromorphic Computing uses binary and sparse signals to construct learning algorithms with higher energy efficiency (Henderson et al., 2020; Blouw et al., 2019; Davies et al., 2021; Lapique, 1907; Izhikevich, 2003). However, a binary signal means that the true derivative is zero essentially always, and training with gradient descent will be at best very poor. Research has shown that designing an approximate gradient, referred to as Surrogate Gradient (SG) (Esser et al., 2016; Zenke and Ganguli, 2018; Bellec et al., 2018), significantly improves training success. However, that entails an additional hyper-parameter to choose: the SG to use. Additionally, the best SG can depend on the neural architecture chosen, on the task, on the learning rate, on the initialization, and so on, making it difficult to know a priori which to pick. Thus, finding the best SG for a particular setting, requires a time consuming grid search, and reducing that search time is desirable.\n\nTo meet that need, we stress test a wide variety of SG, focusing on one specific neuron model, the Leaky Integrate and Fire (LIF) (Lapique, 1907; Gerstner et al., 2014), and provide a mathematical solution based on gradient stability methods (Glorot and Bengio, 2010; He et al., 2015), to design the best SG for a LIF. In contrast, it is standard to pick one SG for all the experiments (Bohte, 2011; Hubara et al., 2016a; Bellec et al., 2018; Zenke and Ganguli, 2018; Zenke and Vogels, 2021; Yin et al., 2021), possibly exploring the effect of changing a width factor (sharpness) (Zenke and Ganguli, 2018) or a height factor (dampening) (Bellec et al., 2018). Only in the past five years, the possibility of choosing an optimal SG has been considered (Neftci et al., 2019; Zenke and Vogels, 2021). Moreover, even if many SG can achieve good performance (Zenke and Vogels, 2021), some shapes have more chances to fail training or achieve lower accuracy. It seems therefore valuable to have a complete picture of when and where each SG works, and which ones are better left behind.\n\n1\n\nFor example, on more complex neural models and tasks, we measure an increase in sensitivity to the choice of SG with complexity, and observe some to degrade more gracefully, which stresses the need to pick the right SG in each setting. We then focus on arguably the simplest spiking neural model, the LIF, and confirm that the initialization scheme has different impact on each SG. Finally, to be able to propose our theoretical solution, we need to justify the use of high firing rates. Fortunately, we observe that low initial sparsity, can help generalization with high final sparsity. We use this observation to better justify, that setting the network on a high firing rate at the beginning of training, is not in contrast with a low firing rate at the end of training. Taking this finding into consideration, and in the spirit of Glorot and He initializations (Glorot and Bengio, 2010; He et al., 2015), we propose four conditions that keep the representations and gradients stable with time. We show that these conditions provide hyper-parameters that result in improved performance without additional hyper-parameter grid-search. When we observe closely the fine details of the SG shape, such as (1) its dampening, (2) its sharpness, and (3) how fast it decays to zero, i.e. tail-fatness, we see that the theoretically justified choice tends to be close to the best experimental choice.\n\nOur contribution is therefore\n\n• We show how task and network complexity, lead learning to be more sensitive to\n\nthe choice of SG;\n\n• We observe that the derivative of the fast-sigmoid outperforms other SG across\n\ntasks and networks;\n\n• High initial firing rate can promote generalization with low final firing rate; • We provide a theoretical method for SG choice based on bounding representations\n\nthat improves experimental performance.\n\n• Our method predicts dampening, sharpness and tail-fatness, that lead to high ac-\n\ncuracy experimentally on the LIF network;\n\n2 Preliminaries\n\n2.1\n\nInitialization Schemes\n\nOur theoretical method for SG choice is based on techniques from the weights initialization literature, that we use in an unorthodox way, to design a SG. The initial values of the network parameters have a strong impact on training speed (Hanin and Rolnick, 2018) and peak performance (Glorot and Bengio, 2010; He et al., 2015). The theory often focuses on fully-connected feed-forward networks (FFN), given their mathematical tractability (Roberts et al., 2022). FFNs are defined as yl = bl + Wlσ(yl−1), where y0 is the data, yL is the network output at depth L, σ(·) an activation and bl ∈ Rnl , Wl ∈ Rnl×nl−1 are the layer biases and weights, where nl is layer l size. Typically, biases are sampled as zero and weights such that M ean[Wl] = 0 and V ar[Wl] = cl. The general recommendation is a 1/cl ∝ nl to avoid exploding variance of representations (Glorot and Bengio, 2010; He et al., 2015). (Glorot and Bengio, 2010) finds V ar[Wl] = 2/(nl−1 + nl) optimal for linear networks (σ(y) = y), known as Glorot initialization, while (He et al., 2015) finds V ar[Wl] = 2/nl−1 for ReLU networks (σ(y) = max(0, y)), known as He initialization. Instead, (Saxe et al., 2014) finds a column orthogonal Wl optimal for linear networks, known as Orthogonal initialization. Usually Wl elements are drawn from a uniform or a normal distribution. We propose the BiGamma distribution, such that wij ∼ Gamma(w; α, β)/2 + Gamma(−w; α, β)/2, Fig. 1. The BiGamma keeps the optimal variance and orthogonality without sampling zeros.\n\nOn the contrary, theoretical justification for recurrent networks initialization has been proposed for the LSTM (Mehdipour Ghazi et al., 2019), and other non spiking recurrent networks (Hochreiter et al., 2001; Arjovsky et al., 2016; Pascanu et al., 2013). However, on spiking recurrent networks, an initialization theory is missing, since arguments such as the Echo State Network (Jaeger et al., 2007) do not apply to non convex activations, or activations without a slope one regime. In practice, (Zenke and Vogels, 2021) samples a V ar[Wl] = 1/3nl−1 Uniform, while (Bellec et al., 2018) a V ar[Wl] = 1/nl−1 Normal distribution, for similar spiking models.\n\n2\n\n(a)\n\n(b)\n\n(c)\n\nFigure 1: Surrogate Gradient shapes and initialization distributions. Panel (a) shows the SG investigated in this work, and (b) the tail dependence of the q-PseudoSpike SG for q ∈ [1.01, 16.85]. The SG considered are symmetrical around vt = yt − θ = 0, so we only plot half the curve (centered voltage vt > 0). Panel (c) shows the weight sampling distributions used, Gaussian (dashed), Uniform (dotted) and BiGamma (solid), with He, orange. and Glorot initialization, green, for a weight shape of (n0, n1) = (200, 300).\n\n2.2 Neural Models and Notation\n\nArguably the simplest spiking recurrent network model is the LIF (Lapique, 1907; Gerstner et al., 2014; Wo ́zniak et al., 2020). It is defined as yt = αdecayyt−1(1 − xt−1) + it where it = Wrecxt−1 + Winzt + b, and yt is the neuron membrane voltage, using (Glorot and Bengio, 2010; He et al., 2015) notation. We define xt = σ(yt) = ̃H(yt − θ) = ̃H(vt) as the spiking activity, where θ is the spiking threshold, vt = yt − θ the centered voltage, and ̃H(vt) a Heaviside function with SG. The term (1 − xt−1) represents a hard reset, that takes the voltage to zero after firing. The input zt can represent the data, or a layer below. It is common to write αdecay = 1 − dt , where dt is the computation time, τm the membrane τm time constant, and to multiply the other terms by biologically meaningful constants, that we compress for cleanliness. Each neuron can have its own speed αdecay, intrinsic current b and θ. In this work, all the parameters in the LIF definition are learnable.\n\nWe denote vectors as a, matrices as A, and their elements as a. The matrix Wrec ∈ Rnl×nl connects neurons in the same layer, with zero diagonal, and Win ∈ Rnl×nl−1 connects the layer with the layer below, or the data if l = 0. We use curved brackets A(·) for functions, and square brackets A[·] for functionals that depend on a probability distribution. We use interchangeably x = M ean[x], ˆx = M ax[x], and ˇx = M in[x]. We use θ for any parameter. In a stack of layers, we add an index l to each parameter and variable. Since the equation only depends on the previous timestep and layer, the probability distribution is a Markov chain in time and depth. Therefore the statistics we discuss are computed element-wise with respect to the distribution p(yt,l|t, l) = p(yt−1,l, zt,l−1, Wrec,l, Win,l, bl, αdecay,l, θl|t, l).\n\nAdditionally, we want to understand how the neuron complexity affects SG training. When a LIF is upgraded with a dynamical threshold to maintain longer memories, we have the Adaptive LIF (ALIF) (Gerstner et al., 2014; Bellec et al., 2018). Thus, we compare LIF to ALIF, and we propose the spiking LSTM (sLSTM), App. G, defined by changing the LSTM (Hochreiter and Schmidhuber, 1997) activations for neuromorphic counterparts.\n\n2.3 Surrogate Gradients\n\nAs seen in the previous section, a spike is produced when the voltage surpasses the threshold, which mathematically can be described through a Heaviside function, ̃H(v), that is zero for v < 0 and one for v ≥ 0. We use the tilde to remind that a SG is used for training, defined as ̃H ′(v) = γf (β ·v), where β is the sharpness, γ the dampening, f is the shape of choice and · the product. Therefore γ controls the maximal amplitude of the SG, and β controls the width. A high sharpness, mostly passes the gradient for v close to zero, while low sharpness also passes the gradient for a wider range of voltages. Therefore, the gradient can pass when the neuron has not fired. Unless explicitly stated, dampening and sharpness are set to one.\n\n3\n\n0101Surrogate gradientamplituderectangulartriangularexponentialgaussian sigmoid fast sigmoid01Centered voltage16.855.642.361.41.121.031.01q-PseudoSpike q 0.10.00.1w010p(w)Glorot UniformGlorot NormalGlorot BiGammaHe UniformHe NormalHe BiGammaThe SG shapes f we investigated are (1) rectangular (Hubara et al., 2016b), (2) triangular (Esser et al., 2016; Bellec et al., 2018), (3) exponential (Shrestha and Orchard, 2018), (4) gaussian (Yin et al., 2020), (5) the derivative of a sigmoid (Zenke and Vogels, 2021), and (6) the derivative of a fast-sigmoid, also SuperSpike (Zenke and Ganguli, 2018). To make the comparison between different SG fair, f is chosen to have a maximal value of 1 and an area under the curve of 1. We also propose a generalization of the derivative of the fast-sigmoid, that we call q-PseudoSpike SG. Its tail fatness is controlled by a hyper-parameter q and we use it to study tail dependence in section 3.5. More in Fig. 1 and App. D.\n\n2.4 Datasets\n\nMore details on the datasets can be found in App. A.\n\nSpike Latency MNIST (sl-MNIST): the MNIST digits (LeCun et al., 1998) pixels (10 classes) are rescaled between zero and one, presented as a flat vector, and each vector value x is transformed into a spike timing using the transformation T (x) = τef f log( x x−θ ) for x > θ and T (x) = ∞ otherwise, with θ = 0.2, τef f = 50ms (Zenke and Vogels, 2021). The network input is a sequence of 50ms, 784 channels (28 × 28), with one spike per row.\n\nSpiking Heidelberg Digits (SHD): is based on the Heidelberg Digits (HD) audio dataset (Cramer et al., 2020) which comprises 20 classes of spoken digits, from zero to nine, in English and German, spoken by 12 individuals. These audio signals are encoded into spikes through an artificial model of the inner ear and parts of the ascending auditory pathway.\n\nPennTreeBank (PTB): is a language modelling task. The PennTreeBank dataset (Marcus et al., 1993), is a large corpus of American English texts. We perform next time-step prediction at the word level. The vocabulary consists of 10K words, which we consider as 10K classes. The one hot encoding of words can be seen as a spiking representation, even if it is the standard representation in the non neuromorphic literature.\n\n2.5 Training Details\n\nOur networks comprise two recurrent layers. The output of each feeds the following, and the last one feeds a linear readout. Our LIF network has 128 neurons per layer on the sl-MNIST task, 256 on SHD, and one layer of 1700 and another of 300 on PTB, as in (Wo ́zniak et al., 2020). On the SHD task, the ALIF has 256 neurons and the sLSTM 85, to keep a comparable number of 350K parameters. We train on the crossentropy loss. The optimizer had a strong effect, where Stochastic Gradient Descent (Robbins and Monro, 1951; Kiefer and Wolfowitz, 1952) was often not able to learn, and AdaM (Kingma and Ba, 2015) performed worse than AdaBelief (Zhuang et al., 2020). AdaBelief hyper-parameters are set to default, as in (Radford et al., 2018; Zenke and Vogels, 2021). The remaining hyper-parameters are reported in App. A. Unless explicitly stated, we use Glorot Uniform initialization. Each experiment is run 4 times and we report mean and standard deviation. Experiments are run in single Tesla V100 NVIDIA GPUs. We call our metric the mode accuracy: the network predicts the target at every timestep, and the chosen class is the one that fired the most for the longest.\n\n3 Results\n\n3.1 Sensitivity to Complexity\n\n3.1.1 Methodology\n\nIn order to portray the difficulty of choosing the right SG for the right task and network, we investigate the SG training sensitivity to task and network complexity. We estimate the task complexity by the number of classes. Thus, if CT (·) measures task complexity, CT (sl-M N IST ) < CT (SHD) < CT (P T B). We quantify neural complexity as in (Yin et al., 2021), and Tab. 1, by the number of operations performed per layer. In essence, if CM (·) measures model complexity, then CM (LIF ) < CM (ALIF ) < CM (sLST M ). To have comparable losses across tasks and networks, we normalize their validation values between\n\n4\n\nFigure 2: The derivative of the fast-sigmoid outperforms other SG across tasks and networks. Grid search over SG shapes, learning rates, tasks and networks. Perplexity is a loss, so, the lower the better. We report lowest validation perplexity after converged training. Panels a-f) show perplexity (y-axis), against learning rate (x-axis). In a-c) we fix the LIF network and change task, while in d-f) we fix the SHD task and change network. Panels g-h) show SG sensitivity (y-axis) against task and neural model (x-axis). Plots b) and d) are repeated for clarity. a-f) Different SG roughly agree on the best learning rate. The best loss is achieved by the ∂ fast-sigmoid, also the most resilient to changes in the learning rate, as shown in (Zenke and Vogels, 2021). g-h) The more complex the task or the network, the more variance in the performance we see across SG choices and learning rates.\n\n0 and 1. For that, we remove the lowest loss achieved by a network in a task for any seed and learning rate, and divide by the distance between the highest and lowest loss. We call the result the post-training normalized loss. We call sensitivity the standard deviation of the post-training normalized perplexity across SG, for each learning rate. We report mean and standard deviation across learning rates.\n\n3.1.2 Experiments\n\nWe see in Fig. 2, that task and network complexity have a measurable effect on the sensitivity of training to the SG choice. We run a grid search over learning rates and SG shapes. The sensitivity to the task is shown in the upper panels, for the LIF network. We see that different SG agree on the optimal learning rate. We also see that the ∂ fast-sigmoid performs well for a wider range of learning rates. The rectangular SG is competitive on some tasks, but fails to learn with most learning rates on PTB. Then we focus on network sensitivity, fixing the SHD task, lower panels. The triangular SG performs similarly to the exponential on the LIF network, while it underperforms on ALIF, and fails on sLSTM. The exponential SG matches the best SG on both the LIF and the sLSTM, but not on the ALIF. All this manifests a strong sensitivity to the SG choice. Surprisingly, the sLSTM lags behind the LIF and ALIF, with a comparable number of parameters. The gating mechanism devised to keep the LSTM representations from exploding exponentially, are not relevant anymore for a Heaviside that cannot explode exponentially, and might have become a computational burden. Fig. 2, g-h), confirm that there is a correlation between task and network complexity, and SG sensitivity.\n\n3.2 Orthogonal initialization leads to higher accuracy\n\nIn contrast, SG training seems less sensitive to the weights initialization scheme. Fig. 6 App. C, shows the initialization scheme effect on SG training, for the LIF network on the SHD task. We compare the Glorot (2/V ar[Wl] = nl−1 + nl, (Glorot and Bengio, 2010)) with the He (2/V ar[Wl] = nl−1, (He et al., 2015)) and the Orthogonal initialization (Wrec\n\n5\n\n102103104105246810Validation PerplexityLIF networka)sl-MNIST1021031041055101520b)SHD1021031041051006501200c)PTBsl-MNISTSHDPTBTask0.00.10.2Sensitivityg)1021031041055101520SHD taskd)LIF1021031041055101520e)ALIF102103104105Learning rate5101520f)sLSTMLIFALIFsLSTMNeural Model0.00.10.2Sensitivityh)rectangulartriangularexponentialgaussian sigmoid fast sigmoidand Win as column orthogonal matrices, (Saxe et al., 2014)). We use three sampling distributions: Uniform, Gaussian and BiGamma, Fig. 1. Orthogonal Uniform is not considered since after orthogonalization, the distribution was not Uniform anymore. We use the same initialization for Wrec and Win. The best SG with Glorot Uniform is the ∂ fast-sigmoid, while with Orthogonal Normal is the exponential. He gives the best outliers, and Orthogonal Normal gives the best mean accuracy. The BiGamma reduces the result variance. Overall best mean is achieved by the derivative of the fast-sigmoid and by the exponential SG, suggesting similar behavior across initializations.\n\n3.3 High initial firing rate can promote generalization with low final\n\nfiring rate\n\n3.3.1 Methodology: Sparsity and Binarity Roles in Generalization\n\nIn order to propose our theoretical method for SG choice, we want to make sure that high initial firing rates are not pernitious neither for learning nor for final sparsity. This is so, because in the neuromorphic literature training success is judged by (1) training performance and (2) activity sparsity. We show in Sec. 3.3.2, that low initial sparsity can improve generalization in synergy with a sparsity encouraging loss term (SELT). However, the energy gains of spiking networks also come from their binary activity. A matrix-vector multiplication, with a Rm×n matrix, has an energy cost of mnEM AC for a real vector, and of mnpEAC for a binary vector, where p is the Bernouilli probability of the binary vector, and in our case the neuron firing rate, and EAC, EM AC are the energies of an accumulate and a multiply-accumulate operation (Yin et al., 2021; Hunger, 2005). We quantify the sparsity of a binary vector as 1 − p. Since MAC are more costly than AC, 31 times on a 45nm complementary metal–oxide–semiconductor (Yin et al., 2021; Horowitz, 2014), we have energy savings with any p, e.g., when all neurons fire (p = 1) and when they fire half of the time steps (p = 1/2). This gain does not depend on the simulation speed, since it compares a spiking and an analogue computation, at the same computation speed.\n\nWe measure the Pearson correlation of initial and final firing rate pi, pf with test loss after training, in two settings, with and without a SELT. The SELT is a mean squared error between a target firing rate pt = 0.01 and the layer firing rate. To achieve different pi, we pre-train bl on the dataset of interest, holding the other parameters untrained, using only the SELT without the classification loss. The coefficient to multiply the loss term is chosen to make all losses comparable only when the task is learned, to let the network focus first on the task and then on the sparsity. We therefore chose as the multiplicative factor the minimal training loss achieved without SELT, since the SELT takes values between zero and one. We switch on the SELT gradually during training. The switch starts as zero, and moves linearly to one between 1/5 and 3/5 of training. We focus on the ∂ fast-sigmoid and the SHD task in the main text, but we show different SG and tasks in App. H.\n\n3.3.2 Experiments\n\nWe can see in Fig. 3 that with and without a SELT, higher pi correlates with performance. Correlations are bold when p-value ≤ 0.05. Notice that SELT achieved worse final train loss (not shown). However, the high pi combined with SELT resulted in better test loss, thus, better generalization. However, this is not consistent across SG shapes, Fig. 7, but is consistent across tasks, Fig. 8 App. H. In fact, the triangular SG prefers low pi and the exponential SG does not show a clear trend. Incidentally, the lower layer always reaches higher sparsity, across seeds (Fig. 3), SG shapes (Fig. 7) and tasks (Fig. 8).\n\n3.4 Our theoretical method for SG choice improves experimental\n\nperformance\n\nKeeping in mind that we can exploit a low initial sparsity as a regularization mechanism, we propose a method for SG design in spiking recurrent networks inspired by FFN initializations (Glorot and Bengio, 2010; He et al., 2015), that keeps stable gradients with time. We propose four conditions, as four hypothesis to test, that result in a SG that depends on the network and the task. We present the mathematical equivalent in each subsection\n\n6\n\nFigure 3: High initial firing rate can promote generalization with low final firing rate. We use SHD task and the ∂ fastsigmoid SG. Bold correlation means p-value ≤ 0.05. On the two left panels, learning starts from different pi without a SELT, while on the two right panels a target sparsity is encouraged. In both cases, the initial firing rate correlates with final performance, and a low pf is achieved successfully using a SELT. Notice as well that the combination of high initial firing rate and sparsity encouragement resulted in better test accuracy than on the two panels on the left, suggesting that both factors acted synergistically as a regularization mechanism.\n\nI Each neuron has to fire half of the timesteps. II Recurrent and input variances should match. III Gradients must have equal maxima across time. IV Gradients must have equal variance across time.\n\n3.4.1 Recurrent matrix mean sets the firing rate (I)\n\nAs previously seen, high pi can generalize better with a lower pf . Moreover, notice that SG curves reach their highest when the neuron fires, Fig. 1. Thus, if the voltage stays close to firing, the gradient is stronger, which is always so if M edian[v] = 0 and V ar[v] = 0. However, V ar[v] = 0 turns off all higher moments, thus, we only assume M edian[v] = 0 as the mathematical equivalent of our desiderata. When (I) is applied to a LIF network, see App. E.1, the mean of the recurrent weight matrix fixes pi, further assuming win = 0, b = 0, the approximation M ean[v] ≈ M edian[v], and constant it over time, we find\n\nwrec =\n\n1 nl − 1\n\n(cid:16)\n\n2 − αdecay\n\n(cid:17)\n\nθ\n\n(I)\n\nThe assumption M ean[v] ≈ M edian[v], can be justified by noticing that if v is sampled from a unimodal distribution with the first two moments defined, then |M ean[v] − M edian[v]| ≤ (cid:112)0.6V ar[v] is true (Basu and DasGupta, 1997). Experimentally, we observe always unimodal distributions, that verify |M ean[v] − M edian[v]| ≤ (cid:112)c Var[v], with c = 10−4 for the SHD task, c = 3 × 10−2 for sl-MNIST, and c = 10−3 for PTB, with and without (I).\n\n3.4.2 Recurrent matrix variance can make recurrent and input\n\ncontribution to voltage comparable (II)\n\nAlso pertaining the forward pass, we want the neuron to be as sensitive to the network history as it is to new input at initialization, when the structure of the task is unknown. We describe it mathematically as V ar[Wrecxt−1] = V ar[Winzt]. In a LIF network, it sets the variance of the recurrent matrix that makes both contributions equal, see App. E.2, further assuming E x = 1/2, win = 0, and computing V ar[zt] and zt on the train set, we obtain\n\nV ar[wrec] = 2(V ar[zt] + z2 t )\n\nnl−1 nl − 1\n\nV ar[win] −\n\n1 2\n\nw2\n\nrec\n\n(II)\n\n3.4.3 Dampening and sharpness set gradient maximum and variance (III, IV)\n\nInstead, to control the backward pass, we want stable gradients with time. We describe mathematically (III) as M ax[ ∂ ∂θ yt−1]. On a LIF network, they set the dampening and the second moment of the SG that keep the maximum and variance of the gradient stable with time, see App. E.3, E.4. Sharpness and tail-fatness are linked to the SG second moment, see App. E.5. Assuming σ′ and ∂ ∂θ yt−1 as\n\n∂θ yt−1] and (IV) as V ar[ ∂\n\n∂θ yt] = M ax[ ∂\n\n∂θ yt] = V ar[ ∂\n\n7\n\n0.250.50Initialfiring rate4.04.55.05.56.0Test Perplexityr1=-0.42r2=-0.40.20.4Finalfiring rater1=-0.34r2=-0.430.250.50Initialfiring rater1=-0.38r2=-0.380.050.100.15Finalfiring rater1=-0.16r2=-0.14Sparsity EncouragingLoss Termno Sparsity EncouragingLoss Term fast sigmoid on SHDlayer 1layer 2Figure 4: Our theoretical method for SG choice improves experimental performance. Simulations are run on the SHD task for a LIF network. We propose 4 conditions to select a SG for a LIF networks. (I) requires spiking half the timesteps, (II) encourages comparable presynaptic and input contribution to voltage, while (III) and (IV) encourage gradient maxima and variance constant with time. The baseline has no conditions applied, in gray, with an exponential SG and Glorot Uniform initialization. Lower and upper panels show validation and test accuracies. (II) has the highest impact on its own, but all conditions combined achieve the best result, showing that a theory of SG design can reduce the need of extensive hyper-parameter search.\n\nindependent, and zero mean gradients at initialization, we find\n\n(cid:16)\n\n1 − αdecay\n\n(cid:17)\n\nγ =\n\nσ′2 =\n\n1 (nl − 1) ˆwrec 1 − 1\n\n2 α2\n\ndecay\n\n(nl − 1)w2\n\nrec\n\n(III)\n\n(IV)\n\n3.4.4 Experiments\n\nFig. 4 shows training results with our conditions for the LIF network on the SHD task, with exponential SG, against the unconditioned baseline. (II) improves accuracy the most when applied on its own, but the best performance is achieved with all conditions together. When all conditions are applied, a LIF network achieves a 92.7 ± 1.5 validation and 75.8 ± 3.1 test accuracy, compared to 87.3 ± 1.4 validation and 69.0 ± 5.8 test accuracy without conditions.\n\n3.5 The conditions predict best empirical dampening, sharpness and\n\ntail-fatness on the LIF network\n\nWe compare experimentally the performance of a range of values of dampening, sharpness and tail-fatness and we assess how they compare to the theoretical prediction. Fig. 5 shows the accuracy of the LIF network on the sl-MNIST task. Each SG has its tail decay: inverse quadratic for the ∂ fast-sigmoid, no tail for the triangular and rectangular, and exponential decays for the rest. Low dampening and high sharpness are preferred by all SG. Interestingly, the accuracy of the ∂ fast-sigmoid degrades less with suboptimal γ, β. The vertical dashed lines are predicted by our theoretical method, condition (III) for the dampening and (IV) for the sharpness of an exponential SG. We observe that they find γ, β with high experimental accuracies. This supports the claim that to reduce hyper-parameter search of dampening and sharpness is possible. We use our q-PseudoSpike SG to study the dependence with the tailfatness, panel (c) Fig. 5. All tail-fatness values perform reasonably well, with a maximum at q = 1.56, smaller than the q = 2 of the ∂ fast-sigmoid. Interestingly our theoretical solution gives a q = 1.898 ± 0.002, surprisingly close to the experimental optimum.\n\n4 Discussion and Conclusions\n\nSurrogate Gradients have reduced the gap between Spiking Neuromorphic Computing and Deep Learning, with the consequent energy efficiency gains. Different SG can achieve similar performance, at the expense of potentially costly hyper-parameter search. Our goal was to\n\n8\n\n0.650.700.750.80Test AccuracyIVnoconditionsIIIIIII+III+II+IIII+II+III+IVConditions0.800.850.900.95Validation Accuracy(a)\n\n(b)\n\n(c)\n\nFigure 5: Low dampening, high sharpness and low tail-fatness, lead to higher accuracy on the LIF network. Analysis done on the LIF network over the sl-MNIST task. Panel (a) shows performance for different values of dampening, sharpness set to 1, and viceversa on (b). Dampenings higher than 1 worsen performance while the pattern is the opposite for sharpness. Dashed vertical lines are our theoretical prediction for the exponential SG, (III) for the dampening (γ = 0.20 ± 0.02) and (IV) for the sharpness (β = 1.02 ± 0.17), which agree with the experiments. Panel (c) shows tail-fatness sensitivity of the q-PseudoSpike SG, for β = γ = 1. The theoretical prediction gives a close to optimal q = 1.898 ± 0.002, where the best experimentally was q = 1.56.\n\nreduce the need of such search in the future, with experimental and theoretical insights. We saw that best SG across networks and tasks was the derivative of the fast-sigmoid, also known as SuperSpike, and the SG sensitivity increase with task and network complexity. Incidentally, we reached spiking state-of-the-art on the PTB task with the triangular SG. Best average over 12 seeds had 122.8 ± 10.7 validation and 114.2 ± 9.2 test perplexity, and best seed had 117.2 validation and 109.5 test perplexity. Previous spiking SOTA on PTB was 137.7 test perplexity (Wo ́zniak et al., 2020). Then, we saw the Orthogonal Normal as the best initialization across SG, on the LIF network and the SHD task, and our BiGamma weight distribution reduced final variance. We saw that for some SG, a high initial firing rate, combined with a sparsity encouraging loss term, can improve generalization.\n\nThe literature on optimal SG is growing in activity (Neftci et al., 2019; Zenke and Vogels, 2021; Yin et al., 2021). However, a theoretical framework was needed. We provide a principled method for initialization and SG design, in the form of four conditions. All of them apply to any architecture. We derived the implications on the LIF, and we saw experimentally improved training. The dampening, sharpness and tail-fatness predicted were among the best empirically, Fig. 5. We saw a preferrence for low dampening, high sharpness and low tail-fatness, making the ideal SG close to a delta, with heavy tails. Also, passing gradients for voltages far from zero, could allow the network learn from outliers. Therefore, our method can help reduce the costly hyper-parameter grid search. Conditions (II), (III) and (IV) are not restricted to spiking neurons. (I) was introduced to find the SG maxima, but all deep learning activations, have their regime of interest around zero. So, (I) is also universal. Even if not stated, (Glorot and Bengio, 2010; He et al., 2015) use (I) in the form M ean[vl] = 0, to determine that M ean[Wl] = M ean[bl] = 0 at initialization. Our theoretical solution applies to convolutional Win and Wrec. Take V ar[Win] = nkV ar[win] where nk is the number of presynaptic neurons for each postsynaptic neuron. In our case nk was equal to nl−1. For rD convolutions, nk = krnf , where k is the kernel size, nf the number of filters and typically the convolution dimension is r = 1, 2. Our conditions also apply to different reset methods, App. F. Notice that our conditions could fix γ, β, and q, which turns them into tools for SG design. This leads to a new theoretical understanding of the roles played by dampening and sharpness. The dampening keeps the maximal gradients stable through time, while the sharpness keeps the gradient variances stable through time. In summary, this work is in response to the call made by (Zenke and Vogels, 2021) for a theory of SG choice, and it is a first step in that direction.\n\n9\n\n0.51.01.5Dampening0.90.70.50.30.1Validation Accuracy0.51.01.5Sharpness0.90.70.50.30.1Validation Accuracyq7DLOIDWQHVV9DOLGDWLRQ$FFXUDF\\rectangulartriangularexponentialgaussian sigmoid fast sigmoidq-PseudoSpikeReferences\n\nM. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks.\n\nIn\n\nInternational conference on machine learning, pages 1120–1128. PMLR, 2016.\n\nS. Basu and A. DasGupta. The mean, median, and mode of unimodal distributions: a\n\ncharacterization. Theory of Probability & Its Applications, 41(2):210–223, 1997.\n\nG. Bellec, D. Salaj, A. Subramoney, R. Legenstein, and W. Maass. Long short-term memory\n\nand learning-to-learn in networks of spiking neurons. In NeurIPS, 2018.\n\nP. Blouw, X. Choo, E. Hunsberger, and C. Eliasmith. Benchmarking keyword spotting efficiency on neuromorphic hardware. In Proceedings of the 7th Annual Neuro-inspired Computational Elements Workshop, pages 1–8, 2019.\n\nS. M. Bohte. Error-backpropagation in networks of fractionally predictive spiking neurons.\n\nIn ICANN, pages 60–68. Springer Berlin Heidelberg, 2011. ISBN 978-3-642-21735-7.\n\nB. Cramer, Y. Stradmann, J. Schemmel, and F. Zenke. The heidelberg spiking data sets for the systematic evaluation of spiking neural networks. IEEE Transactions on Neural Networks and Learning Systems, 2020.\n\nM. Davies, A. Wild, G. Orchard, Y. Sandamirskaya, G. A. F. Guerra, P. Joshi, P. Plank, and S. R. Risbud. Advancing neuromorphic computing with loihi: A survey of results and outlook. Proceedings of the IEEE, 109(5):911–934, 2021. doi: 10.1109/JPROC.2021. 3067593.\n\nS. K. Esser, P. A. Merolla, J. V. Arthur, A. S. Cassidy, R. Appuswamy, A. Andreopoulos, D. J. Berg, J. L. McKinstry, T. Melano, D. R. Barch, et al. Convolutional networks for fast, energy-efficient neuromorphic computing. Proceedings of the national academy of sciences, 113(41):11441–11446, 2016.\n\nW. Gerstner, W. M. Kistler, R. Naud, and L. Paninski. Neuronal dynamics: From single\n\nneurons to networks and models of cognition. Cambridge University Press, 2014.\n\nX. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, pages 249–256. JMLR Workshop and Conference Proceedings, 2010.\n\nB. Hanin and D. Rolnick. How to start training: The effect of initialization and architecture.\n\nIn NeurIPS, 2018.\n\nK. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level\n\nperformance on imagenet classification. In ICCV, pages 1026–1034, 2015.\n\nP. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau. Towards the systematic reporting of the energy and carbon footprints of machine learning. JMLR, 21 (248):1–43, 2020.\n\nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nS. Hochreiter, Y. Bengio, P. Frasconi, J. Schmidhuber, et al. Gradient flow in recurrent\n\nnets: the difficulty of learning long-term dependencies, 2001.\n\nM. Horowitz. 1.1 computing’s energy problem (and what we can do about it). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pages 10–14. IEEE, 2014.\n\nI. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv,\n\nneural\n\nnetworks. narized Inc., URL d8330f857a17c53d217014ee776bfd50-Paper.pdf.\n\nIn NeurIPS,\n\n2016a.\n\nand Y. Bengio. Bi29. Curran Associates, https://proceedings.neurips.cc/paper/2016/file/\n\nvolume\n\n10\n\nI. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized neural\n\nnetworks. NeurIPS, 29, 2016b.\n\nR. Hunger. Floating point operations in matrix-vector calculus, volume 2019. Munich Uni-\n\nversity of Technology, Inst. for Circuit Theory and Signal . . . , 2005.\n\nE. M. Izhikevich. Simple model of spiking neurons. IEEE Transactions on neural networks,\n\n14(6):1569–1572, 2003.\n\nP. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. Wilson. Averaging weights leads to wider optima and better generalization. 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018, 2018.\n\nH. Jaeger, M. Lukoˇseviˇcius, D. Popovici, and U. Siewert. Optimization and applications of echo state networks with leaky-integrator neurons. Neural networks, 20(3):335–352, 2007.\n\nJ. Kiefer and J. Wolfowitz. Stochastic estimation of the maximum of a regression function.\n\nThe Annals of Mathematical Statistics, pages 462–466, 1952.\n\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization.\n\nIn Y. Bengio and Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\nL. Lapique. Recherches quantitatives sur l’excitation electrique des nerfs traitee comme une\n\npolarization. Journal of Physiology and Pathololgy, 9:620–635, 1907.\n\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to docu-\n\nment recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n\nM. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large annotated corpus\n\nof English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993.\n\nM. Mehdipour Ghazi, M. Nielsen, A. Pai, M. Modat, M. J. Cardoso, S. Ourselin, and In ICONIP,\n\nL. Sørensen. On the initialization of long short-term memory networks. pages 275–286. Springer, 2019.\n\nE. O. Neftci, H. Mostafa, and F. Zenke. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51–63, 2019. doi: 10.1109/MSP.2019.2931595.\n\nR. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural In International conference on machine learning, pages 1310–1318. PMLR,\n\nnetworks. 2013.\n\nA. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.\n\nImproving language under-\n\nstanding by generative pre-training. 2018.\n\nH. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical\n\nstatistics, pages 400–407, 1951.\n\nD. A. Roberts, S. Yaida, and B. Hanin. The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks. Cambridge University Press, 2022.\n\nA. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics\n\nof learning in deep linear neural networks. In ICLR, 2014.\n\nS. Shrestha and G. Orchard. Slayer: Spike layer error reassignment in time. In NeurIPS,\n\n2018.\n\nS. Wo ́zniak, A. Pantazi, T. Bohnstingl, and E. Eleftheriou. Deep learning incorporating biologically inspired neural dynamics and in-memory computing. Nature Machine Intelligence, 2(6):325–336, 2020.\n\n11\n\nB. Yin, F. Corradi, and S. M. Boht ́e. Effective and efficient computation with multipleIn ICONS, New York, NY, USA, 2020. ISBN 9781450388511. doi: 10.1145/3407197.\n\ntimescale spiking recurrent neural networks. Association for Computing Machinery. 3407225. URL https://doi.org/10.1145/3407197.3407225.\n\nB. Yin, F. Corradi, and S. M. Boht ́e. Accurate and efficient time-domain classification with adaptive spiking recurrent neural networks. Nature Machine Intelligence, 3(10):905–913, 2021.\n\nF. Zenke and S. Ganguli. Superspike: Supervised learning in multilayer spiking neural\n\nnetworks. Neural computation, 30(6):1514–1541, 2018.\n\nF. Zenke and T. P. Vogels. The remarkable robustness of surrogate gradient learning for instilling complex function in spiking neural networks. Neural Computation, 33(4):899– 925, 2021.\n\nJ. Zhuang, T. Tang, Y. Ding, S. C. Tatikonda, N. Dvornek, X. Papademetris, and J. Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. NeurIPS, 33, 2020.\n\n12",
    "reference": "# Summary Of The Paper\n\nThis work finds it unclear to determine which surrogate gradient to apply in different tasks or networks in SNN. Seeking to solve this problem, the authors have done some experiments with different surrogate gradients across tasks and networks, and come to the conclusion that the derivative of fast sigmoid outperforms other chosen surrogate gradients. Besides that, the authors have also done research on how different characteristics of surrogate gradients and initialization affect surrogate gradient learning and found that low dampening, high sharpness, low tail-fatness, orthogonal initialization, and high initial firing rate could help improve the network’s performance. Lastly, the authors provide a theoretical solution based on bounding representations to find a surrogate gradient and initialization that could improve accuracy.\n\n# Strength And Weaknesses\n\nStrength: 1. Training details are explicit, including network architecture and hyper-parameters. 2. Conclusion from the experiments is helpful in choosing surrogate gradient.\n\nWeaknesses: It doesn’t seem reasonable to make the neuron as sensitive to the network history as to new input in section 3.5.2, since the network history is an accumulation of knowlegde learnt from past so it contains more knowledge than new input.\n\nMinor typos:\n\nSec2, line 3 \"y_L the network output\" -> \"y_L is the network output\" or \"y_L means the network output\"\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe authors of this work do not promise any reproducibility in text.\n\n# Summary Of The Review\n\nThe discussion on how to choose SG in SNN is meaningful and missed before.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPROTEIN SEQUENCE DESIGN IN A LATENT SPACE VIA MODEL-BASED REINFORCEMENT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nProteins are complex molecules responsible for different functions in the human body. Enhancing the functionality of a protein and/or cellular fitness can significantly impact various industries. However, their optimization remains challenging, and sequences generated by data-driven methods often fail in wet lab experiments. This study investigates the limitations of existing model-based sequence design methods and presents a novel optimization framework that can efficiently traverse the latent representation space instead of the protein sequence space. Our framework generates proteins with higher functionality and cellular fitness by modeling the sequence design task as a Markov decision process and applying model-based reinforcement learning. We discuss the results in a comprehensive evaluation of two distinct proteins, GFP and His3, along with the predicted structure of optimized sequences using deep learning-based structure prediction.\n\n1\n\nINTRODUCTION\n\nProteins mediate the fundamental processes of cellular fitness and life. Iterated mutations on various proteins and natural selection during the biological evolution diversify traits, eventually accumulating beneficial phenotypes. Similarly, in protein engineering and design, the directed evolution of proteins has proved to be an effective strategy for improving or altering the proteins’ functions or cellular fitness for industrial, research, and therapeutic applications (Yang et al., 2019; Huang et al., 2016). However, the protein sequence space of possible combinations of 20 amino acids is too large to search exhaustively in the laboratory, even with high-throughput screening from the diversified library (Huang et al., 2016). In other words, directed evolution becomes trapped at local fitness maxima where library diversification is insufficient to cross fitness valleys and access neighboring fitness peaks. Moreover, functional sequences in this vast space of sequences are rare and overwhelmed by nonfunctional sequences.\n\nTo tackle the limitations, data-driven methods have been applied to protein sequence design. They used reinforcement learning (RL) (Angermueller et al., 2019), Bayesian optimization (Wu et al., 2017; Belanger et al., 2019; Terayama et al., 2021; Stanton et al., 2022), and generative models (Jain et al., 2022; Kumar & Levine, 2020; Hoffman et al., 2022) in a model-based fashion, i.e., using a protein functionality predictor trained on experimental data to model the local landscape. Despite the advances obtained by these methods, it is still challenging to generate optimized sequences that are experimentally validated. We suggest that the cause for this is two-fold. The first cause is that the optimization process is usually performed by generating candidate sequences directly through amino acid substitutions (Belanger et al., 2019) or additions (Angermueller et al., 2019). Given the vast search space, these methodologies are highly computationally inefficient and commonly lead to the exploration of parts of the space with a low chance of having functional proteins. In designing biological sequences, previous literature explored optimizing a learned latent representation space (G ́omez-Bombarelli et al., 2018; Stanton et al., 2022). Similarly, in this paper we investigate the optimization of sequences via RL directly in a latent representation space rather than in the protein sequence space. Actions, e.g., small perturbations in the latent vector, taken in this representation space can intuitively be understood as walking through a local functionality/fitness landscape.\n\nThe second cause is related to the models used as an oracle for optimization. These models are trained on experimental data obtained for a specific function, covering only a small portion of the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The framework’s overview describes (top left) the encoder-decoder architecture trained to represent protein sequences in a latent space and (top right) the RL framework. The state is defined as the representation in the latent space, while the action is the perturbation in this representation. The perturbed representation is decoded back to a protein sequence using a sequence decoder. The reward is based on the functionality predicted by the functionality predictor (oracle). Bottom row shows three RL-based state and action modeling options.\n\nprotein space, and their accuracy is consequently restricted to this small region. Later, we will demonstrate that even the most advanced model-based optimizations can assign high functionality values for randomly generated protein sequences. These random sequences, however, are unlikely to be functional. To reduce false positives, we suggest augmenting the experimental data with random sequences (i.e., negative examples) assigned to a low functionality value. Such augmentation also helps set boundaries around the experimental data distribution in which the oracle can be trusted.\n\nWe model protein sequence design as a Markov Decision Process (MDP) to optimize a latent representation. Our method trains the optimization policy using a model-based deep reinforcement learning (RL) framework (Fig. 1). At each timestep, the policy updates the latent representation by small perturbations to maximize protein functionality or cellular fitness, i.e., walking uphill through the local landscape until the end of an episode. We evaluate our method in two evaluation tasks, optimizing the functionality of the green fluorescence protein (GFP) and the cell fitness of imidazoleglycerol-phosphate dehydratase (His3). Our results show that the proposed framework design sequences with higher protein functionality and cellular fitness than existing methods. Ablation studies show that, based on the evaluation of various model options for state and action for the RL framework, the proposed latent representation update can successfully optimize the protein and search the vast design space. We provide visual evidence that the trained policy can traverse the local functionality landscape efficiently for GFP and His3. Our method is general and can also be applied to representations learned from protein structures, such as those presented in Zhang et al. (2022); Eguchi et al. (2022).\n\n2 RELATED WORKS\n\nProtein representation learning Representation learning methods aim to learn compact and expressive features describing data. Since a protein is composed of a sequence of distinct amino acids (N=20), it can be interpreted as a large word in which each character is an amino acid. Due to the similarity with natural language processing (NLP) tasks, methods such as BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have been used to train protein language models (Alley et al., 2019; Brandes et al., 2022; Lin et al., 2022; Ferruz et al., 2022; Rives et al., 2021). The protein language model trained with 250 million sequences by Rives et al. (2021) has shown to learn representations containing meaningful information about biological properties and reflecting protein structure. It was shown that the learned representation could be generalized across different applications achiev-\n\n2\n\nEncoderDecoderSequenceSequenceRepresentation•Rich, compressed info •Low dimensionBaseline (learned mutation)SequenceLength (200~)State: sequenceAction: mutationAmino acids (20)Previous approach (learned addition)SequenceLength (200~)State: sequenceAction: additionAmino acids (20)SequenceAdd amino acidPolicySequenceRewardState (Latent vector)Action (Perturbation)EncoderDecoderOur approach State: latent vectorAction: perturbationFunctionality PredictorRepresentationPolicyRepresentation+(L,)(L,)(R,)Under review as a conference paper at ICLR 2023\n\ning state-of-the-art results for supervised prediction of mutational effects. Representation learning has also been applied to learn protein structures (Zhang et al., 2022). An antibody-specific structure encoder was trained using variational autoencoders by Eguchi et al. (2022).\n\nProtein functionality prediction Similar to protein representation learning, for functionality prediction, methods inspired by NLP continue to achieve high performance. For the TAPE benchmark (Rao et al., 2019), long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) and transformers (Vaswani et al., 2017) are the top performing methods. Notin et al. (2022) proposed a transformer-inspired architecture named Tranception for autoregressive functionality prediction that achieve state-of-the-art performance in 87 protein deep mutational scanning (DMS) datasets. Representing the protein as an image and using a convolutional neural network architecture (He et al., 2016) are also investigated in (Rao et al., 2019). Also proposed was the use of representations learned from a protein language model for zero-shot predictions (Alley et al., 2019; Meier et al., 2021). Utilizing protein structures as inputs, graph neural networks have been used to predict functionality in (Gligorijevi ́c et al., 2021). Finally, combining a sequence-based and structure-based representation for functionality prediction using language models and geometric vector perceptrons is proposed in (Wang et al., 2022).\n\nBiological sequence design Various machine learning methods have been used to design biological sequences. DynaPPO is a model-based RL algorithm proposed specifically for biological sequence design (Angermueller et al., 2019), which generates sequence from left to right by adding amino acids. Bayesian optimization was applied for cell fitness maximization in (Brookes et al., 2019; Belanger et al., 2019; Terayama et al., 2021; Swersky et al., 2020; Stanton et al., 2022). From these approaches, Stanton et al. (2022) used Bayesian optimization to optimize the sequence directly in a latent space by training a denoising autoencoder that learns representations for corrupted sequences. It is important to note that our approach, compared to Stanton et al. (2022), does not use the corrupt-and-denoise idea and defines optimization as an episodic task to use reinforcement learning. An evolutionary algorithm for efficient biological sequence optimization was proposed in (Sinai et al., 2020). Methods using generative models to search and sample optimized sequences are investigated in (Brookes et al., 2019; Kumar & Levine, 2020; Jain et al., 2022; Hoffman et al., 2022; Melnyk et al., 2021).\n\n3 METHODOLOGY\n\nThis section describes the primary components of the model-based RL framework, depicted in Fig. 1. Details of network architectures are provided in Sec. 5 and Appendix A.\n\n3.1 PROTEIN REPRESENTATION LEARNING\n\nSequence Encoder Let x ∈ X be a (protein) sequence of length L, where L denotes the number of amino acids in the sequence and X is the sequence space. Each element xi of the sequence is a discrete number representing one of 20 amino acids, and i is the i-th element from x. Given x, the encoder network e produces a representation y of dimensions (R, ). The sequence encoder is divided into two parts: (i) a pre-trained protein language model encoder is used to obtain latent embeddings, (ii) a dimensionality reduction step is used to obtain the final representation.\n\nIn the first part, a pre-trained protein language model is leveraged (see Fig. 2(a)). We employ the ESM-2 model (Lin et al., 2022) trained on the Uniref50 dataset (Suzek et al., 2015). This model uses a BERT (Devlin et al., 2018) encoder and is trained with 150 million parameters in an unsupervised fashion (see Appendix A.1). ESM-2 is utilized to map mutation effects onto a latent space in our model. Given the sequence x as an input, ESM-2 outputs a matrix q ∈ Q of dimensions (L + 2, E), where E is the dimension size of the embeddings, and Q is the embedding space. In ESM-2, a CLS (classification) token is prepended to the sequence, and an EOS (end-of-sequence) token is appended to the sequence when generating the embeddings. They serve as sequence-beginning and sequence-ending characters, respectively.\n\nIn the second part, the output of the encoder with dimensions (L+2, E) is preprocessed before being passed to the policy. We use only the CLS token embedding to reduce the dimension from (L+2, E) to (E, ). However, this could still be large for the action space given the embedding dimension E\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Model Architecture\n\nof even the smallest ESM-2 model is 480. We hence use 1-dimensional adaptive average pooling to further reduce the dimension to (R, ) and obtain the final representation.\n\nSequence decoder Given a reduced representation y of size (R, ), we recover the amino acid sequence x using the sequence decoder d. The representation size is expanded to (E, ) using a linear layer. To recover the original (L + 2, E) dimensions of the embeddings obtained by ESM-2, the dimensions of the reduced representation are first expanded. Then, this matrix is concatenated with the wild-type embeddings obtained using ESM-2 of the pre-trained encoder of size (L + 2, 2E) followed by a linear layer. After passing to a dropout layer, the recovered representation is passed to the decoder’s language model head, which maps the representation back to the sequence space, i.e., predicting each amino acid in the sequence from their associated embeddings. The output is the recovered sequence x.\n\n3.2 PROTEIN FUNCTIONALITY PREDICTION\n\nThe functionality predictor network depicted in Fig. 2(b) is trained as a downstream task of the pre-trained ESM-2 encoder network. Given an input sequence x, the encoder network produces a representation q ∈ Q of dimensions (L + 2, E). We flatten the representation to a vector of size ((L+2)×E, ) that is passed to an architecture with two linear layers followed by GeLU (Hendrycks & Gimpel, 2016) non-linear activation. The final linear layer predicts a value k ∈ R, where k is the functionality metric to be maximized and R is the set of real numbers representing possible values for this metric. While the encoder can be shared since we did not modify the architecture of the encoder nor the weights, the functionality decoder f (x) is assumed to be trained independently for each task. To train the functionality predictor, negative examples are included when sampling the training batch to prevent the model from assigning high values to sequences outside the experimental data distribution. Negative examples are defined as random sequences with a low functionality/fitness value, and are a crucial data enhancement technique for reducing false positives. The augmentation can help set boundaries around the experimental data region that can be trusted.\n\n3.3 PROTEIN SEQUENCE DESIGN VIA MODEL-BASED REINFORCEMENT LEARNING\n\nThe final component trains a policy using an off-policy RL algorithm that models the reward function based on the functionality predictor.\n\nSystem Modeling Consider a policy π that interacts with a fully observable environment during each episode. An episode is defined by the number of timesteps in which the policy takes actions\n\n4\n\nSequenceSequenceWild-typeESM-2 (BERT)ESM-2 (BERT)(a) Sequence encoder-decoder(L,)(L,)(L+2,E)(E,)(R,)PoolingCLSSequence decoderLinear(L+2,E)Concatenate(L+2,2E)Repeat(E,)Linear(L+2,E)Dropout(L+2,E)LM Head(L,)SequenceESM-2 (BERT)(L,)(L+2,E)Reshape((L+2)E,)×Linear(E,)GeLULinear(E,)GeLULinearFunctionalityFunctionality decoder(b) Functionality predictorCLS, EOS(L+2,E)CLS, EOSLatent representationAmino acid sequenceUnder review as a conference paper at ICLR 2023\n\nto optimize the target sequence in the latent space. We sample an initial state s0 ∈ Q from the representation space (state space) at the beginning of each episode. At each timestep t, the agent observes a state st ∈ Q and selects an action at ∈ A according to a stochastic policy π : Q → A, where A is the action space. The action at is defined as a perturbation in which a continuous value atj ∈ [−ε, ε] is chosen for the j-th dimension, and ε is a hyperparameter that can be tuned based on the representation distribution of the experimental data. The hyperparameter ε controls how conservative the policy is when traversing the functionality or fitness landscape. Both st and at have R dimensions.\n\nWe model the environment using a Markov Decision Process (MDP) and assume that the next state st+1 is conditioned only on the current state st and action at. This function calculates the elementwise sum of st and at so that st+1 = st + at. As a result of performing the action at, the agent receives the reward rt. The reward is a function rt = r(st, at), where r is the reward function r : Q → R, and R is the set of real numbers representing reward values. This data is later used to train the policy using an off-policy reinforcement learning algorithm. The agent interacts with the environment until reaching a terminal state, which is one of the followings: (i) when the last timestep T is reached; (ii) when f (d(st)) for the current state is greater than a hyperparameter threshold; and (iii) when st is in a part of the representation space Q where f (d(st)) cannot be trusted. Condition (iii) states how far st is from the experimental data. Each timestep’s data is stored in a replay buffer as a 5-tuple (st, at, st+1, rt, mt) where mt identifies the terminal state.\n\nOff-Policy Reinforcement Learning We further optimize the policy using Soft Actor Critic (SAC) (Haarnoja et al., 2018), an off-policy entropy-regularized reinforcement learning (RL) algorithm for continuous action spaces. Off-policy is a type of RL learning that learns to optimize independently of the agent’s actions. SAC is trained to maximize future rewards and entropy to promote randomness in action space exploration. An optimal policy π∗ following this objective is given as\n\nπ∗ = arg max\n\nπ\n\nEπ\n\n(cid:34) ∞ (cid:88)\n\nt=0\n\nγt(r(st, at) + αH(π(·|st)))\n\n,\n\n(1)\n\n(cid:35)\n\nwhere γ ∈ [0, 1] is a discount factor, α is a temperature hyperparameter and H is entropy. Details regarding SAC optimization and how actions are sampled by the policy π are given in Appendix A.3.\n\n4 RESULTS\n\n4.1 EXPERIMENT SETUP\n\nDatasets We chose proteins of different lengths and functions for robustness in evaluation: the green fluorescent protein (GFP) that can be found naturally in jellyfish and imidazoleglycerolphosphate dehydratase (His3) that is a key enzyme in our human body. The dataset proposed in (Sarkisyan et al., 2016) is used to train the GFP encoder-decoder and its functionality predictor. The dataset contains 54,025 mutant sequences, with log-fluorescence intensity associated with each sequence. The length L of protein sequences is 237. For the His3 protein, we used the dataset in (Pokusaeva et al., 2019) that consists of mutant sequences of its evolutionarily-relevant segments and is associated with its growth rate. We processed the data to 50,000 sequences with the length of L = 30. This dataset is used to train the His3 encoder-decoder and fitness predictor. Datasets are randomly split into non-overlapping training and testing sets with a ratio of 90:10. The encoder, decoder, and functionality predictor use the aforementioned training set. The test set is used for evaluating the trained models and selecting initial states for optimization.\n\nOracles We train two separate oracles for each dataset to prevent the circular use of the functionality predictor: one is exclusively used for optimization (which we call optimization oracle), and another is used to calculate the performance metric in experiments (which we call evaluation oracle). We follow the guideline presented in Kolli et al. (2022) that proposes the use of multiple oracles with different numbers of layers, activation functions, and hyperparameters to improve the reliability of the optimization process. We describe the experiment details and the comparison of evaluation and optimization oracles’ performance in Appendices A.2 and A.7.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nImplementation Details The encoder and decoder use a pre-trained ESM-2 model (Lin et al., 2022) with 150 million trainable parameters. Optimization and evaluation oracles are trained using different ESM-2 models (Lin et al., 2022). The optimization oracle is trained using the model with 150 million parameters while the evaluation oracle uses the model with 35 million parameters. The latent representation space is set to R = 8, which sets the size of state and action vectors. The magnitude of the perturbation ε applied to each element of the action vector is set to 0.1. The episode length T is set to 20 for GFP and 10 for His3. We set three alternative experimental rewards: (1) a dense reward, defined as rt = f (d(st)), (2) an absolute reward, defined as a binary value of rt = 1, if f (d(st)) > rth and 0 if otherwise (values for rth are listed in Table 12), and (3) a binary reward, defined as a binary value of rt = 1, if f (d(st)) > f (d(st−1)) and 0 if otherwise. To set the initial state s0 of GFP, we sampled sequences with low (1.4 to 1.7) and high (3.2 to 3.4) functionality values from the test set. For His3, we sampled sequences with fitness values between 0.5 and 0.8. Appendix A describes other details regarding selecting initial states, network architectures, training hyperparameters, and optimization of our method and baseline methods.\n\nEvaluation Metrics We use three evaluation metrics as reported in Jain et al. (2022): performance, novelty, and diversity. We also consider two additional metrics for robustness: the originality of optimized sequences, named as original, i.e. sequences not contained in the training dataset, and the distance between the optimized sequence and wild-type, named as dist(WT). The performance evaluation metric is calculated as the mean predicted functionality from the top K generated sequences. The predicted functionality is obtained by using the evaluation oracle presented in Section 3.2. Let the generated sequences be contained in the following set G∗ = {g∗ K}, performance is dei ). The novelty evaluation metric is defined to assess if the policy is generating sequences similar to the ones contained in the experimental data. Defining P as the experimental data set containing the wild-type protein sequence, novelty is given as follows:\n\n1, · · · , g∗\n\ni f (g∗\n\nfined as\n\n1 K\n\n(cid:80)\n\n1 K · |P|\n\n(cid:88)\n\n(cid:88)\n\ng∗\n\ni ∈G∗\n\npj ∈P\n\ndist(g∗\n\ni , pj),\n\n(2)\n\nwhere dist is defined as the number of different amino acids of two sequences. The diversity evaluation metric is defined as the mean of the number of amino acids that are different among the optimized sequences and is defined as:\n\n1 K(K − 1)\n\n(cid:88)\n\n(cid:88)\n\ndist(g∗\n\ni , g∗\n\nj ).\n\n(3)\n\ng∗\n\ni ∈G∗\n\ng∗\n\nj ∈G∗−{g∗\n\ni }\n\n(cid:80)\n\nis given as\n\nThe original metric is defined as\n\n1 K\ni ), where w is the wild-type sequence. When testing the protein functionality of GFP, we include the presence of the chromophore region (residues SYG in the wild-type protein) in the optimized sequence, as these residues are related to the ability to emit fluorescence.\n\ni /∈ P]) and the distance from wild-type (WT) metric\n\ni ∈G∗ dist(w, g∗\n\ni 1([g∗\n\n1 K\n\n(cid:80)\n\ng∗\n\n4.2 EXPERIMENT\n\nWe generate 100 sequences for each method, and the 10 highest-performing sequences are evaluated.\n\nGFP Sequence Design The results obtained for GFP sequence design are described in Table 1. We compare with four optimization methods: CbAS (adaptive sampling) (Brookes et al., 2019), BO (bayesian optimization) (Swersky et al., 2020), GFlowNet (generative model) (Jain et al., 2022), and DynaPPO (reinforcement learning) (Angermueller et al., 2019). The term directed evolution refers to the average functionality values of initial states used in the optimization process. We also include random mutations of initial states. It is seen in Table 1 that the proposed method outperforms directed evolution. Only the proposed method and CbAS optimize GFP effectively, whereas BO, GFlowNet, and DynaPPO achieve low performance. Even though our method limits the action to a small step in the latent space, it gets higher novelty and diversity when compared to CbAS, which also limits the search space when optimizing. It was interesting to observe that two of the\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nGFP sequences optimized by the proposed method achieved higher predicted functionality values, 3.9705 and 3.8059, when compared to the experimental wild-type functionality value, 3.72. It is interesting to observe that methods that achieve the highest distance from the wild-type sequence are the ones that achieve the lowest performance.\n\nModel\n\nPerformance Novelty Original\n\ndist(WT) Diversity Chromophore\n\n3.491 ± 0.352 Ours Directed evolution 3.287 ± 0.237 3.155 ± 0.153 CbAS Random-1 2.824 ± 0.100 2.280 ± 0.275 Random-5 Random-P 1.511 ± 0.797 0.581 ± 0.095 BO DynaPPO 0.004 ± 0.003 0.000 ± 0.002 GFlowNet\n\n8.451 7.704 7.712 6.611 13.91 14.71 36.96 218.9 199.4\n\n100% -\n80% 80% 100% 100% 100% 100% 100%\n\n7.700 6.849 6.900 7.186 9.950 14.15 36.70 219.3 200.1\n\n6.311 4.858 1.956 7.716 12.37 14.62 6.867 224.1 12.53\n\n100% 100% 100% 100% 90% 100% 100% 0% 0%\n\nTable 1: Results obtained for GFP sequence design. Random-1 and Random-5 indicates random mutations in 1 and 5 positions, respectively. Random-P indicates a random perturbation in the latent representation. Standard deviation for Novelty and dist(WT) are shown in Table 8.\n\nHis3 Sequence Design Table 2 displays the results for His3, where our framework achieves the best performance. DynaPPO and BO were unable to optimize His3 effectively. Given that the length of the His3 protein is only 30, it is interesting to observe that a single random mutation is an effective strategy. Note, however, that even though the proposed method achieves higher overall performance, its novelty is lower than all other methods. This indicates that the decoder is recovering similar sequences within the representation space. The proposed method achieves higher novelty and diversity when compared to CbAS.\n\nModel\n\nPerformance\n\nNovelty Original\n\ndist(WT) Diversity\n\nOurs CbAS Random-1 Random-5 Directed evolution DynaPPO BO\n\n0.945 ± 0.091 0.749 ± 0.157 0.858 ± 0.058 0.678 ± 0.096 0.616 ± 0.110 -0.201 ± 0.142 -0.313 ± 0.065\n\n8.361 7.287 7.372 9.777 6.889 27.41 26.17\n\n60% 90% 80% 100% -\n100% 100%\n\n10.95 4.700 7.350 8.950 6.710 26.70 27.50\n\n3.521 2.356 7.716 12.37 6.942 27.47 4.756\n\nTable 2: Results obtained for His3 sequence design. Random-1 and Random-5 indicates random mutations in 1 and 5 positions, respectively. Standard deviation for Novelty and dist(WT) are shown in Table 9.\n\n4.3 ABLATION STUDIES\n\nState and Action Modeling Table 3 shows how state and action modeling influence sequence design in GFP. For sequence modeling, we examine two types: a representation in which each amino acid is represented by one-hot encoding and a latent vector that is the encoder’s output. We investigate three types of action modeling: (i) multi discrete sequence generation, (ii) conditional autoregressive addition of amino acids proposed in (Angermueller et al., 2019), and (iii) the proposed perturbation in the latent vector. Only the latent vector as the state and the perturbation as the action can optimize GFP and His3 tasks. Compared to the latent space learned based on the embeddings from ESM-2, using the one-hot encoded mutant sequence as input makes the identification of structural changes related to the protein’s functional site challenging. Also, modeling the action as the addition of amino acids is ineffective for traversing the vast and sparse protein space.\n\nRepresentation Analysis We analyzed the influence of the optimization method on the latent vector as an input. Our method, which employs reinforcement learning for taking actions that update\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nState\n\nAction\n\nGFP\n\nHis3\n\nLatent vector\n\nPerturbation on latent vector\n\n3.491 ± 0.352\n\n0.945 ± 0.091\n\nDirected evolution\n\nSequence Generate sequence Latent vector Generate sequence Sequence\n\nAmino acid addition\n\n3.287 ± 0.237\n\n0.616 ± 0.110\n\n0.006 ± 0.004 0.005 ± 0.003 0.004 ± 0.003\n\n-0.148 ± 0.043 -0.139 ± 0.144 -0.201 ± 0.142\n\nTable 3: Comparison on the performance by the state and action modeling. For rows with the action defined as a mutation in the protein sequence, we train the policy using Proximal Policy Optimization (PPO) (Schulman et al., 2017) to handle the multiple discrete action space. More details regarding the state and action modeling for this ablation are presented in Appendix A.3.\n\na latent vector, is compared to random perturbation and to the aforementioned BO method proposed by Swersky et al. (2020), both of which take a latent vector as input. First, Swersky et al. (2020) performs better when using the latent vector input (2.601 ± 0.912) than when using the sequence as input (0.581 ± 0.095). It implies that the latent vector provides rich information in a low-dimensional space that is easier to optimize than the sequence space. Even with the same representation, our method outperforms Swersky et al. (2020) while achieving competitive novelty.\n\nModel\n\nPerformance\n\nNovelty\n\nDiversity Chromophore\n\n3.491 ± 0.352 8.451 ± 2.05 Ours Directed evolution 3.287 ± 0.237 7.704 ± 2.66 Swersky et al. (2020) on latent space 2.601 ± 0.912 8.077 ± 2.58 1.511 ± 0.797 14.71 ± 5.90 Random perturbation\n\n6.311 4.858 6.600 14.616\n\n100% 100% 100% 100%\n\nTable 4: Comparison on our method and BO when using latent representation as an input.\n\nFigure 3: The predicted fluorescence of random sequences tested using a functionality predictor trained with or without negative examples.\n\nTraining with Negative Examples We trained the GFP functionality predictor with the original training dataset and the training dataset augmented with 40,000 negative examples (i.e., 82% of the size of the original training dataset). We tested both predictors with 10,000 negative examples that were not seen during training. Fig. 3 shows that the functionality predictor trained without negative examples incorrectly predicts a high value for non-functional sequences (mean=4.002), whereas the one trained with negative examples accurately assigns zero functionality (mean=0.0) to random non-functional sequences.\n\n8\n\n40200.00.51.01.52.02.53.03.54.04.55.0Average: 0.000FluorescenceData points (%)Average: 4.002 Fluorescence of GFP proteins Predicted fluorescence on random sequences Predictor trained with negative examples Predictor trained without negative examplesUnder review as a conference paper at ICLR 2023\n\nFigure 4: Visualization of the climbing process through a local functionality and fitness landscape. (a) and (b) Resulting optimization carried out by the trained policy during a single episode for the GFP task. The z-axis represents the intensity of log fluorescence. In (b) it is seen the policy traversing local optima due to the RL-based formulation of the proposed method. (c) Fitness landscape for the His3 dataset. We annotated initial states (red) and optimized states (blue). (d) Optimization performed by the trained policy for the His3 task. The representation space is reduced to two using t-SNE (Van der Maaten & Hinton, 2008). Appendix A.9 describes the methodology used to generate these landscapes. An HTML-formatted interactive version of these graphs can be found in the supplementary materials.\n\n5 DISCUSSION\n\nHow the trained policy traverses the functionality landscape We qualitatively evaluate our trained policy by analyzing its ability to traverse the local landscape. Figs. 4 (a) and (b) visualize the optimization trajectory during one episode of GFP, indicating continued improvement of the fluorescence level. This demonstrates that the model can successfully climb up the local landscape throughout the episode. The policy can also traverse local optima thanks to the MDP formulation (See Appendix A.11). For His3, the cell fitness landscape of the entire dataset is shown in Fig. 4(c), which interestingly depicts a valley region that corresponds to part of the space leading to low fitness. Also, we can observe that the optimized states are scattered throughout the landscape, implying that our policy can design diverse proteins. Fig. 4(d) presents optimization steps taken by the trained policy, which now shows that large (optimistic) perturbations often lead to failure in optimization.\n\n6 CONCLUSION\n\nThis paper addressed the problem of optimizing protein functionality and cellular fitness using datadriven methods. We investigated the limitations of the latest model-based biological sequence design methods and proposed a novel optimization framework that can efficiently traverse the latent representation space, as opposed to optimizing the protein sequence by mutations in its amino acid sequence. Our framework trained a policy that continually updates the latent representation throughout an episode. We modeled the problem as an MDP to maximize future rewards, which allowed the agent to explore the landscape while learning an optimal policy. This special mechanism was critical to efficiently traversing the local functionality and fitness landscape. The proposed framework outperformed other baseline methods in performance and is competitive in terms of novelty. We have also demonstrated that a trained policy can navigate a local landscape effectively and traverse local optima. In the future, we intend to filter the generated sequences and conduct wet lab experiments to validate the proposed framework.\n\n9\n\n3.53.02.52.01.5FitnessFitness1.20.90.60.30.0Optimization leads to higher fitnessOptimization leads to lower fitnessOptimization pathFluorescence(a)(c)(d)Fluorescence(b)Optimized stateInitial stateUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nEthan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church. Unified rational protein engineering with sequence-based deep representation learning. Nature methods, 16(12):1315–1322, 2019.\n\nChristof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy, and Lucy Colwell. Model-based reinforcement learning for biological sequence design. In International conference on learning representations, 2019.\n\nDavid Belanger, Suhani Vora, Zelda Mariet, Ramya Deshpande, David Dohan, Christof Angermueller, Kevin Murphy, Olivier Chapelle, and Lucy Colwell. Biological sequences design using batched bayesian optimization. 2019.\n\nNadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial. Proteinbert: A universal deep-learning model of protein sequence and function. Bioinformatics, 38(8):2102–2110, 2022.\n\nDavid Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling for\n\nrobust design. In International conference on machine learning, pp. 773–782. PMLR, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nRaphael R Eguchi, Christian A Choe, and Po-Ssu Huang. Ig-vae: Generative modeling of protein structure by direct 3d coordinate generation. PLoS computational biology, 18(6):e1010271, 2022.\n\nNoelia Ferruz, Steffen Schmidt, and Birte H ̈ocker. Protgpt2 is a deep unsupervised language model\n\nfor protein design. Nature communications, 13(1):1–10, 2022.\n\nKarl Pearson F.R.S. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559–572, 1901. doi: 10.1080/14786440109462720.\n\nVladimir Gligorijevi ́c, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structurebased protein function prediction using graph convolutional networks. Nature communications, 12(1):1–14, 2021.\n\nRafael G ́omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos ́e Miguel Hern ́andez-Lobato, Benjam ́ın S ́anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al ́an Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268–276, 2018.\n\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nDan Hendrycks and Kevin Gimpel.\n\nGaussian error linear units (gelus).\n\narXiv preprint\n\narXiv:1606.08415, 2016.\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nSamuel C Hoffman, Vijil Chenthamarakshan, Kahini Wadhawan, Pin-Yu Chen, and Payel Das. Optimizing molecules using efficient queries from property evaluations. Nature Machine Intelligence, 4(1):21–31, 2022.\n\nPo-Ssu Huang, Scott E Boyken, and David Baker. The coming of age of de novo protein design.\n\nNature, 537(7620):320–327, 2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMoksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure FP Dossou, Chanakya Ajit Ekbote, Jie Fu, Tianyu Zhang, Michael Kilgour, Dinghuai Zhang, et al. Biological sequence design with gflownets. In International Conference on Machine Learning, pp. 9786–9801. PMLR, 2022.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nSathvik Kolli, Amy X Lu, Xinyang Geng, Aviral Kumar, and Sergey Levine. Data-driven optimization for protein design: Workflows, algorithms and metrics. In ICLR2022 Machine Learning for Drug Discovery, 2022.\n\nAviral Kumar and Sergey Levine. Model inversion networks for model-based optimization. Ad-\n\nvances in Neural Information Processing Systems, 33:5126–5137, 2020.\n\nZeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022.\n\nJoshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. Advances in Neural Information Processing Systems, 34:29287–29303, 2021.\n\nIgor Melnyk, Payel Das, Vijil Chenthamarakshan, and Aurelie Lozano. Benchmarking deep gener-\n\native models for diverse antibody sequence design. arXiv preprint arXiv:2111.06801, 2021.\n\nPascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena Hurtado, Aidan N Gomez, Debora Marks, and Yarin Gal. Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. In International Conference on Machine Learning, pp. 16990– 17017. PMLR, 2022.\n\nVictoria O Pokusaeva, Dinara R Usmanova, Ekaterina V Putintseva, Lorena Espinar, Karen S Sarkisyan, Alexander S Mishin, Natalya S Bogatyreva, Dmitry N Ivankov, Arseniy V Akopyan, Sergey Ya Avvakumov, et al. An experimental assay of the interactions of amino acids from orthologous sequences shaping a complex fitness landscape. PLoS genetics, 15(4):e1008079, 2019.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nRoshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. Evaluating protein transfer learning with tape. Advances in neural information processing systems, 32, 2019.\n\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.\n\nKaren S Sarkisyan, Dmitry A Bolotin, Margarita V Meer, Dinara R Usmanova, Alexander S Mishin, George V Sharonov, Dmitry N Ivankov, Nina G Bozhanova, Mikhail S Baranov, Onuralp Soylemez, et al. Local fitness landscape of the green fluorescent protein. Nature, 533(7603):397–401, 2016.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nSam Sinai, Richard Wang, Alexander Whatley, Stewart Slocum, Elina Locane, and Eric D Kelsic. Adalead: A simple and robust adaptive greedy search algorithm for sequence design. arXiv preprint arXiv:2010.02141, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSamuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, and Andrew Gordon Wilson. Accelerating bayesian optimization for biological sequence design with denoising autoencoders. arXiv preprint arXiv:2203.12742, 2022.\n\nBaris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt Consortium. Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6):926–932, 2015.\n\nKevin Swersky, Yulia Rubanova, David Dohan, and Kevin Murphy. Amortized bayesian optimization over discrete spaces. In Conference on Uncertainty in Artificial Intelligence, pp. 769–778. PMLR, 2020.\n\nKei Terayama, Masato Sumita, Ryo Tamura, and Koji Tsuda. Black-box optimization for automated\n\ndiscovery. Accounts of Chemical Research, 54(6):1334–1346, 2021.\n\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\n\nlearning research, 9(11), 2008.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nZichen Wang, Steven A Combs, Ryan Brand, Miguel Romero Calvo, Panpan Xu, George Price, Nataliya Golovach, Emmanuel O Salawu, Colby J Wise, Sri Priya Ponnapalli, et al. Lm-gvp: an extensible sequence and structure informed deep learning framework for protein property prediction. Scientific reports, 12(1):1–12, 2022.\n\nJian Wu, Matthias Poloczek, Andrew G Wilson, and Peter Frazier. Bayesian optimization with\n\ngradients. Advances in neural information processing systems, 30, 2017.\n\nKevin K Yang, Zachary Wu, and Frances H Arnold. Machine-learning-guided directed evolution for\n\nprotein engineering. Nature methods, 16(8):687–694, 2019.\n\nZuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. arXiv preprint arXiv:2203.06125, 2022.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 SEQUENCE ENCODER-DECODER\n\nNext, we give more information regarding the architecture and the training process of the sequence encoder-decoder. The model architecture is seen in 2(a). The ESM-2 encoder comprises a token embedding layer and 30 transformer layers. Each transformer layer consists of multi-head attention followed by a layer normalization layer and two fully connected layers with GeLU (Hendrycks & Gimpel, 2016) non-linear activation. Language model head comprises of linear layer followed with GeLU activation and the linear layer using the weight of the embedding layer in the encoder. The number of attention heads is set to 20. We used the pre-trained weights provided in (Lin et al., 2022). The ESM-2 encoder weights are not updated during the fine-tuning process. The sequence encoder-decoder is fine-tuned using a masked language model objective. We define the loss function as the mean of cross entropy loss on mutated positions and the mean cross-entropy loss on nonmutated positions to ensure that the decoder also focuses on recovering mutated positions. Since we provide the wild-type representation during sequence recovery and mutants in the GFP dataset have a maximum of 15 mutations, such a loss function is important to prevent the decoder from predicting every sequence as the wild-type. We train the model to the train set of each dataset for 16 epochs using the Adam optimizer (Kingma & Ba, 2014). The initial learning rate of Adam is set to 8e-5, with weight decay set to 1e-5. The learning rate is reduced by 0.8× every epoch. Table 5 shows the performance of a sequence decoder by the chosen embedding size of R.\n\nDataset Embedding size\n\nTop-1 Accuracy Mutated positions Non-mutated positions\n\nGFP\n\nHis3\n\n8 32\n\n8 32\n\n0.463 0.4762\n\n0.6439 0.8194\n\n0.9958 0.9903\n\n0.8407 0.9022\n\nTable 5: Performance of the sequence decoder for GFP and His3.\n\nA.2 FUNCTIONALITY PREDICTOR\n\nFunctionality and fitness predictors for GFP and His3 are also based on the pre-trained ESM-2 protein language model. The ESM-2 encoder has same architecture as described in Appendix A.1, and the decoder is described in Sec. 3. We fine-tuned the predictor on the train set of each dataset for 16 epochs using the Adam (Kingma & Ba, 2014) optimizer with the initial learning rate set to 8e-5 and weight decay set to 1e-5. The ESM-2 encoder weights are not updated during the fine-tuning process. The negative samples for GFP and His3 are associated with a functionality/fitness value of 0. The GFP value of 0 denotes an extremely low fluorescence level (10(0−3.72) = 0.0002 times the brightness of the wild type protein). The His3 value of 0 denotes the minimum fitness value in the dataset, ranging from [0, 1.63]. We used the mean squared error between empirically obtained logfluorescence intensity and predicted fluorescence as the loss function for GFP and the empirically obtained fitness and predicted fitness as the loss function for His3.\n\nA.2.1 OPTIMIZATION ORACLE\n\nIn Table 6 we report the test Spearman’s ρ and mean squared error (MSE) of the optimization oracle used for model-based optimization.\n\nA.2.2 EVALUATION ORACLE\n\nIn Table 7 we report the test Spearman’s ρ and mean squared error (MSE) of the evaluation oracle used for model-based optimization. Compared to the optimization oracle, the evaluation oracle utilize a different pre-trained ESM-2 model which also contains a different number of parameters. The evaluation oracle and optimization oracle are trained with similar methodology.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTask\n\nGFP His3\n\nTest set\n\nNegative test set\n\nSpearman’s ρ MSE\n\n0.8426 0.6635\n\n0.1436 0.0080\n\nMSE\n\n2.930e-6 0.0164\n\nTable 6: Performance of the optimization oracle for GFP and His3\n\nTask\n\nGFP His3\n\nTest set\n\nNegative test set\n\nSpearman’s ρ MSE\n\nMSE\n\n0.8320 0.6820\n\n0.4359 0.0110\n\n6.247e-05 0.0288\n\nTable 7: Performance of the evaluation oracle for GFP and His3\n\nA.2.3 TRAINING CBAS AND DYNAPPO WITH AND WITHOUT NEGATIVE SAMPLES\n\nWe conducted an ablation study by training two baseline optimization methods, CbAS and DynaPPO, using both functionality predictors. The optimized sequences are then visualized using predicted structures obtained by AlphaFold2. As shown in Fig. 5, CbAS managed to propose a valid fluorescent protein when guided by an oracle trained with negative examples but failed when guided by an oracle trained without negative examples. Even though the structure predicted for the CbAS model for this case seems unstable, the oracle trained without negative examples assigned a very high log-fluorescence intensity value (4.014) for the sequence. Therefore, we argue that biological sequence design should be guided and evaluated by an oracle trained with an experimental dataset augmented with negative examples.\n\nA.3 SOFT ACTOR CRITIC (SAC)\n\nRecapitulating, an optimal policy π∗ following the SAC objective is given as (cid:35)\n\nπ∗ = arg max\n\nπ\n\nEπ\n\n(cid:34) ∞ (cid:88)\n\nt=0\n\nγt(r(st, at) + αH(π(·|st)))\n\n,\n\n(4)\n\nwhere γ ∈ [0, 1] is a discount factor, α is a temperature hyperparameter and H is entropy. Following a policy π the Q function or action-value function is defined as\n\nQπ(s, a) = Eπ\n\n(cid:34) ∞ (cid:88)\n\nt=0\n\nγtr(st, at) + α\n\n(cid:35)\n\nγtH(π(·|st))\n\n,\n\n∞ (cid:88)\n\nt=1\n\n(5)\n\nwhere the entropy is added from the first timestep. The Bellman equation for Qπ at a timestep t is given by\n\nQπ(st, at) = Eπ [r(st, at) + γ(Qπ(st+1, ̃at+1) + αH(π(·|st+1))] , (6) in which he state st+1 is used from the replay buffer and ̃at+1 is sampled from the current policy. During training, SAC models three networks: a policy πθ and two Q-functions Qφ1 and Qφ2. The\n\nFigure 5: The structure of optimized sequences without and with negative examples.\n\n14\n\nCbAS optimized GFP Guided by oracle trained without negative examplesGuided by oracle trained with negative examplesDynaPPO optimized GFP Guided by oracle trained without negative examplesGuided by oracle trained with negative examplesUnder review as a conference paper at ICLR 2023\n\ntwo Q functions are due to the clipped double-Q trick, more details can be found in Haarnoja et al. (2018). The loss functions to train Qφ1 and Qφ2 in SAC are defined as\n\nL(φi, D) = E(st,at,rt,st+1,mt)∼D\n\n(cid:104)\n\n(Qφi(st, at) − y(rt, st+1, mt))2(cid:105)\n\n,\n\n(7)\n\nwhere the target y(rt, st+1, dt), substituting the entropy function, is given by (cid:18)\n\n(cid:19)\n\ny(rt, st+1, mt) = rt + γ(1 − mt)\n\nmin j=1,2\n\nQφtarg,j (st+1, ̃at+1) − α log πθ( ̃at+1|st+1)\n\n,\n\n(8)\n\nwhere ̃at+1 ∼ πθ(·|st+1). The policy πθ is trained to maximize the value function of state st. The value function V π is as follows\n\nV π(st) = Eat∼π [Qπ(st, at) − α log π(at|st)] . The policy is reparameterized and then optimized following Haarnoja et al. (2018). Using a squashing Gaussian function, sampling the action is finally defined as\n\n(9)\n\n ̃atθ (st, ξ) = tanh(μθ(st) + σθ(st) ⊙ ξ),\n\nξ ∼ N (0, I).\n\n(10)\n\nA.4\n\nIMPLEMENTATION DETAILS\n\nA.4.1 POLICY\n\nSoft Actor-Critic (SAC) We used three fully connected layers with a hidden size set to 256 for both the actor and critic networks of SAC. The Adam optimizer Kingma & Ba (2014) is used for both actor and critic networks. The entropy temperature α is automatically tuned during training following Haarnoja et al. (2018).\n\nProximal Policy Optimization (PPO) For the ablation studies in Table 3 using the mutation in the protein sequence as output, we use Proximal Policy Optimization (PPO) Schulman et al. (2017) to handle the discrete action space setting. The PPO architecture is similar to the one used for the actor of SAC, using three fully connected layers with a hidden size set to 256. We define the action as a multiple of discrete actions for the third and fifth row in Table 3.\n\nA.4.2 RANDOM PERTURBATION BASELINE\n\nWe added a random value between [−1, 1] to the initial state representation to test the effect of random perturbations on performance. Then, we decoded the sequence using the decoder presented in Sec. 3.1 and predicted the functionality using the functionality predictor presented in Sec. 3.2. Note that the baseline and random performance reported in the Tables 1 and 2 are also calculated on the top K candidates.\n\nA.4.3 PREVIOUS METHODS\n\nFor the comparison with previous baseline methods, we use the FLEXS implementation (Sinai et al., 2020) of DynaPPO, CbAS, and BO. All the models are trained for ten rounds. During training, we observed the performance saturation after the first few rounds. The sequences proposed in the last round are used to evaluate the baseline models. For DynaPPO, its ensemble model is trained for ten rounds, and the environment batch size is set to 10. For CbAS, a CNN architecture with a hidden size equal to 100 and a number of channels equal to 32 is used. Also, the generator block of CbAS is trained for ten epochs.\n\nA.5\n\nINITIAL STATES OF THE OPTIMIZATION PROCESS\n\nWe selected initial states for the optimization process in our framework according to two criteria: (i) sequences with room to improve, (ii) sequences sampled from regions that we can trust the oracle. Since the distribution for the ground truth functionality of GFP is bimodal, we sampled two sets of initial states from each mode, low initial states, and high initial states. For His3, only one set of initial states is chosen. As shown in Fig. 6, we sample the sequences from pre-determined regions that do not include the most functional mutants. Additionally, the sequences are sampled from examples in which the difference between their ground truth and predicted functionality or fitness is less than a threshold set to 0.1.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Ground truth and predicted fluorescence of GFP and fitness of His3.\n\nA.6 EVALUATION ON GFP AND HIS3 DESIGN\n\nIn this section, along with the results presented in Tables 1 and 2 we report results on GFP and His3 on Novelty and dist(WT) with standard deviation.\n\nModel\n\nNovelty\n\ndist(WT)\n\nOurs Directed evolution CbAS Random-1 Random-5 Random-P BO DynaPPO GFlowNet\n\n8.451 ± 2.05 7.704 ± 2.66 7.712 ± 2.05 6.611 ± 1.02 13.91 ± 1.22 14.71 ± 5.90 36.96 ± 5.51 218.9 ± 2.63 199.4 ± 2.00\n\n7.700 ± 0.78 6.849 ± 1.90 6.900 ± 0.83 7.186 ± 2.03 9.950 ± 0.87 14.15 ± 5.76 36.70 ± 5.31 219.3 ± 2.37 200.1 ± 1.87\n\nTable 8: Results obtained for GFP sequence design. Random-1 and Random-5 indicates random mutations in 1 and 5 positions, respectively. Random-P indicates a random perturbation in the latent representation.\n\nModel\n\nNovelty\n\ndist(WT)\n\nOurs CbAS Random-1 Random-5 Directed evolution DynaPPO BO\n\n8.361 ± 2.01 7.287 ± 1.57 7.372 ± 1.56 9.777 ± 1.63 6.889 ± 1.57 27.41 ± 1.12 26.17 ± 1.03\n\n10.95 ± 1.32 4.700 ± 0.64 7.350 ± 1.39 8.950 ± 1.36 6.710 ± 1.57 26.70 ± 1.19 27.50 ± 0.50\n\nTable 9: Results obtained for His3 sequence design. Random-1 and Random-5 indicates random mutations in 1 and 5 positions, respectively.\n\nA.7 COMPARISON OF EVALUATION AND OPTIMIZATION ORACLE\n\nIn Tables 10 and 11, we compare results on GFP and His3 design using evaluation oracle and optimization oracle. It is shown that, for all methods, a decrease in performance using the evaluation oracle is observed. This decline was particularly pronounced for BO (Swersky et al., 2020). It is also observed that the standard deviation of the results increases when using the evaluation oracle.\n\n16\n\nPredicted fluorescencePredicted fitnessGround-truth fluorescenceGround-truth fitnessLow initial statesHigh initial statesInitial statesUnder review as a conference paper at ICLR 2023\n\nModel\n\nEvaluation oracle Optimization oracle\n\nOurs Directed evolution CbAS (Brookes et al., 2019) Random mutation (N=1) Random mutations (N=5) Random perturbation BO (Swersky et al., 2020) DynaPPO (Angermueller et al., 2019) GFlowNet (Jain et al., 2022)\n\n3.491 ± 0.352 3.287 ± 0.237 3.155 ± 0.153 2.824 ± 0.100 2.280 ± 0.275 1.511 ± 0.797 0.581 ± 0.095 0.004 ± 0.003 0.000 ± 0.002\n\n3.531 ± 0.06 3.370 ± 0.013 3.328 ± 0.044 3.410 ± 0.094 2.354 ± 0.522 1.973 ± 0.832 1.231 ± 0.034 0.014 ± 0.001 0.017 ± 0.000\n\nTable 10: Comparison of the results obtained for GFP sequence design using evaluation and optimization oracle.\n\nModel\n\nEvaluation oracle Optimization oracle\n\nOurs CbAS (Brookes et al., 2019) Random mutation (N=1) Directed evolution Random mutations (N=5) DynaPPO (Angermueller et al., 2019) BO (Swersky et al., 2020)\n\n0.945 ± 0.091 0.749 ± 0.157 0.858 ± 0.058 0.616 ± 0.110 0.678 ± 0.096 -0.201 ± 0.142 -0.313 ± 0.065\n\n0.961 ± 0.050 0.889 ± 0.092 0.856 ± 0.070 0.756 ± 0.013 0.518 ± 0.184 -0.067 ± 0.053 -0.089 ± 0.029\n\nTable 11: Comparison of the results obtained for His3 sequence design using evaluation and optimization oracle.\n\nA.8 ABLATION STUDY ON INITIAL STATE AND REWARD MODELING\n\nTable 12 reports the effect of the initial state and reward. Note that, for this ablation study, the performance metric is computed using the optimization oracle. Starting from a state of low or high functionality, our framework optimizes performance relative to the directed evolution. The best performance is obtained when starting from a state of high functionality and training with an absolute reward. Our framework can generate novel sequences even when beginning with a high functionality value. Nonetheless, beginning from a state with low functionality, the trained policy does not produce designs with high functionality. This could mean that there may be a gap between regions with low functionality and regions with high functionality in the representation space that requires additional time steps to be explored in each episode.\n\nInitial state Reward\n\nPerformance* Novelty Diversity\n\nLow\n\nHigh\n\nDense Absolute (rth = 1.8) Binary\n\n1.856 ± 0.631 1.855 ± 0.601 1.546 ± 0.271\n\n70% 50% 100%\n\nDirected evolution\n\n1.492 ± 0.008\n\n-\n\nDense Absolute (rth = 3.3) Binary\n\n3.448 ± 0.094 3.531 ± 0.06 3.452 ± 0.081\n\n100% 100% 95%\n\nDirected evolution\n\n3.370 ± 0.013\n\n-\n\n8.032 5.653 6.347\n\n5.426\n\n5.700 6.311 5.279\n\n4.858\n\nTable 12: Ablation studies investigating the sampling of initial states and the reward modeling for the GFP dataset. The policy trained with absolute reward and starting from high initial states is used for comparisons in Tables 1 and 3. Performance(*) is computed using the optimization oracle.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure\n\nTask\n\nNumber of\n\nRepresentations\n\nSequences\n\nDimension reduction\n\nFig. 4 (a) Fig. 4 (b)\n\nGFP GFP Fig. 4 (c)(d) His3 GFP\n\nFig. 8\n\n1.55M 1.17M 6.43K 1.17M\n\n1403 46 6434 46\n\nt-SNE t-SNE t-SNE PCA\n\nTable 13: Details of the complete functionality landscape.\n\nFigure 7: Structures predicted by AlphaFold2 for sequences optimized by the proposed method, DynaPPO, and CbAs for GFP.\n\nA.9 METHODOLOGY TO OBTAIN THE COMPLETE FUNCTIONALITY LANDSCAPE\n\nThe methodology to plot the Fig. 4 and Fig. 8 is detailed next. First, a range based on the representations obtained during the episode being plotted is defined. Then we decode the distinct sequences from the large number of representations using the decoder presented in Sec. 3.1. Table 13 shows the number of representations and sequences used to plot the complete functionality and fitness landscape. After this step, the reward is decoded for each sequence using the optimization oracle presented in Sec. 3.2. To reduce the dimensionality, t-SNE (Van der Maaten & Hinton, 2008) is applied in Fig. 4 and Principal Component Analysis (PCA) (F.R.S., 1901) is applied in Fig 8 with the two principal components kept to create the local landscape.\n\nA.10 ALPHAFOLD2 PREDICTIONS OF OPTIMIZED SEQUENCES\n\nFig. 7 visualizes the structure of the optimized sequence of GFP based on AlphaFold2 (Jumper et al., 2021). The sequences produced by our method and CbAS maintain the critical chromophore region that is known for emitting fluorescence in GFP and the beta sheets that secure the chromophore region. In contrast, DynaPPO-optimized sequences failed to preserve these essential structures of GFP. The different structures obtained by two reinforcement learning methods highlight the significance of state and action modeling in the design of biological sequences. Learning from a single-hot sequence encoding makes it difficult for an algorithm to identify crucial structural information. On the other hand, the latent vector trained by extracting information from a language model trained with millions of protein sequences can efficiently learn mutational effects and reflect protein structures, as shown in (Rives et al., 2021).\n\n18\n\nOur optimized GFPDynaPPO optimized GFPWild-type GFPCbAs optimized GFPUnder review as a conference paper at ICLR 2023\n\nA.11 POLICY ABILITY TO TRAVERSE LOCAL OPTIMA\n\nWe qualitatively evaluate our trained policy for the GFP by analyzing its ability to traverse local optima of the functionality landscape. The optimization process during one episode is shown in Fig. 8. Until the end of the episode, at timestep t = 10, the policy can traverse the landscape and maximize functionality efficiently. At timestep t = 3, it can escape local optima that do not trigger the conditions for the end of an episode. During timesteps t = 7, 8, 9, it can be seen that the policy is still maximizing the functionality metric until the end of the episode. Additional details and the methodology used to obtain the functionality landscape in Fig. 8 are explained in A.9.\n\nFigure 8: Optimization process performed by the trained policy during one episode for the GFP task. The x-axis and y-axis are the two principal components of the representation space calculated using Principal Component Analysis (F.R.S., 1901). The z-axis is the log-fluorescence intensity.\n\n19\n\nFluorescence3.53.02.5",
    "reference": "# Summary Of The Paper\n\nThe manuscript presents a procedure for protein engineering using a model-based reinforcement approach building on the ESM2 language model. The authors demonstrate that the high dimensional ESM2 representation can be mapped to a lower dimensional representation space which is suitable for optimization, and that full amino acid sequences can be reconstructed from this reduced representation with meaningful accuracy. They then propose a off-policy reinforcement learning procedure to learn how to make updates in the representation space. Finally, the authors evaluate the method on two protein engineering datasets, and report convincing results.\n\n# Strength And Weaknesses\n\nThe paper is well written, the method well described, and the results are convincing. Please see below for detailed comments to specific parts of the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n## Clarity\nThe paper is clearly written, with excellent figures to support the story.\n\n## Quality\nThe quality of the paper is high.\n\n## Reproducibility\nThe authors have not made source code available as part of their submission, making it difficult to fully assess reproducibility. The method is, however, presented at a level of detail that should make it possible to reproduce the results. I strongly encourage the authors to share their source code upon acceptance of their paper.\n\n## Detailed comments\n\nPage 1. \"To tackle this problem, in this paper we propose...\"\nThis paragraph, and especially this sentence, suggests that latent space optimization of proteins is new, ignoring recent work on Bayesian Optimization of proteins on latent spaces, such as \"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\" (ICML 2022). This paragraph should therefore be rephrased.\nThis same ICML2022 reference should also be added to the \"Biological sequence design\" subsection on the next page, and ideally compared to in the results section. I am not affiliated with this paper in any way - but merely suggesting it because the methods are closely related, and both are contestants to constitute the current state of the art - so comparing them head-on would be relevant to the community.\n\nPage 3: \"k is predicted from a representation q with dimensions (L, E), and\". Page 4: \"As a result of performing the action a_t, the agent receives the reward r_t.\". Page 5: \"a dense reward, defined as r_t= f(s_t)\". \nThe last two statements seem to be at odds with the first statement. Originally, f is described as acting on a q representation of size (L,E), but later, the reward is calculated based on s_t, which is only E-dimensional. Please clarify.\n\nPage 4. \"The dataset proposed in (Sarkisyan et al., 2016) is used to train the GFP encoder-decoder and its functionality predictor.\"\nIt would be helpful if you could discuss somewhere in the paper how many parameters are involved in estimating the task-specific projection to lower dimensions (I assume that this is just ExR) and whether this estimation becomes problematic for smaller datasets that the one you studied here. Likewise for the oracle - although one could presumably use a general oracle instead of a task specific one in this case.\n\nPage 4. \"Datasets were split into train and test sets.\" How? Just uniformly?\n\nPage 5. \"We compare with four optimization methods\"\nI assume these are all optimized on the same oracle(?). As mentioned earlier, it would be beneficial to the community if you compared directly to the ICML2022 method here as well - if at all possible.\n\nPage 6. \"The sequence alone cannot easily convey structural information about a protein’s functional sites, making it unsuitable for guiding the optimization process.\"\nI'm surprised by these results. In Table 2, you show that Random mutations work well as an optimization strategy on His3. Why does it fail completely in the experiment in Table 3? Wouldn’t you at least expect to recover the most important sites in the protein? Does this result perhaps primarily indicate a weakness in your policy optimization rather than the representation itself?\n\nPage 7. \"...is compared to random perturbation and BO,\"\nWhen writing “BO”, are you referring to a general Bayesian Optimization procedure - or the specific one that you cite earlier (Swersky et al., 2020)? Since you are using a continuous latent space here, I assume this is now a different BO - in which case you should make the distinction clear - and explain what the setup is - is it a standard GP-based BO with a expected improvement acquisition function? I would also suggest that the authors make it clear that this is just one particular (and perhaps particularly simple) choice of BO.\n\nPage 7. \"Fig. 2 shows that the functionality predictor trained without negative examples incorrectly predicts a high value for non-functional sequences (mean=4.002)\"\nIsn’t it odd that the oracle predicts a higher average functional value than any value it has observed during training? Does this indicate something is flawed with the oracle?\n\nPage 7. \"Fig. 3(d) presents optimization steps taken by the trained policy, which now shows that large (optimistic) perturbations taken by the policy often lead to failure in optimization.\"\nIt was not quite clear to me what this figure is meant to illustrate. Are the large steps failure modes of the learned policy? (i.e. should it have learned to prevent such steps?)\n\nPage 7. Discussion\nIt would be helpful if the authors somewhere in the paper discussed the source of improvement over e.g. Bayesian Optimization. The BO and reinforcement learning literatures have quite different terminology and it can be a bit difficult to see exactly which components make a difference in practice. \nIs it the fact that a policy is learned vs the fixed acquisition function typically used in a BO setting? \n\n### Minor\n\nPage 1. \"The first cause is that the optimization process is usually performed by generating candidate sequences through amino acid substitutions\"\nSlightly odd statement, since any method (including the one proposed here), will ultimately use amino acid substitutions (or insertions/deletion). Perhaps writing \"sequences *directly* through amino acid substitutions\" would be better?\n\nPage 3. \"a 2-dimensional vector representation q ∈ Q of dimensions (L, E)\"\nSlightly confusion that the representation is both 2 dimensional and has dimension L,E. Consider rephrasing.\n\nPage 4. \"Negative examples are defined as random sequences with a zero functionality value.\"\nDo you standardize the functionality values in any way? Otherwise, 0 seems like an arbitrary value.\n\nPage 5. \"We set three experimental rewards\". At this point in the text it is not clear whether these losses will be used simultaneously or as alternatives. Perhaps add \"alternative\" here to make this clear.\n\nPage 5. \"The performance evaluation metric is calculated as the mean log-fluorescence intensity from the top K generated sequences.\"\nFor clarify, perhaps make it clear that this is according to the oracle, and not the ground truth values.\n\nPage 5. \"CbAsoptimize\"\nMissing space\n\n# Summary Of The Review\n\nThe paper proposes a new method for protein engineering. It is well-written, carefully documents its claims, and demonstrates convincing results. I have only minor suggestions for edits to the paper.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLEARNING TO SPLIT FOR AUTOMATIC BIAS DETECTION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nClassifiers are biased when trained on biased datasets. As a remedy, we propose Learning to Split (ls), an algorithm for automatic bias detection. Given a dataset with input-label pairs, ls learns to split this dataset so that predictors trained on the training split cannot generalize to the testing split. This performance gap suggests that the testing split is under-represented in the dataset, which is a signal of potential bias. Identifying non-generalizable splits is challenging since we In this work, we show that the prediction have no annotations about the bias. correctness of each example in the testing split can be used as a source of weak supervision: generalization performance will drop if we move examples that are predicted correctly away from the testing split, leaving only those that are mispredicted. ls is task-agnostic and can be applied to any supervised learning problem, ranging from natural language understanding and image classification to molecular property prediction. Empirical results show that ls is able to generate astonishingly challenging splits that correlate with human-identified biases. Moreover, we demonstrate that combining robust learning algorithms (such as group DRO) with splits identified by ls enables automatic de-biasing. Compared to previous state-of-the-art, we substantially improve the worst-group performance (23.4% on average) when the source of biases is unknown during training and validation. Our code is included in the supplemental materials and will be publicly available.\n\n1\n\nINTRODUCTION\n\nRecent work has shown promising results on de-biasing when the sources of bias (e.g., gender, race) are known a priori Ren et al. (2018); Sagawa et al. (2019); Clark et al. (2019); He et al. (2019); Mahabadi et al. (2020); Kaneko & Bollegala (2021). However, in the general case, identifying bias in an arbitrary dataset may be challenging even for domain experts: it requires expert knowledge of the task and details of the annotation protocols Zellers et al. (2019); Sakaguchi et al. (2020). In this work, we study automatic bias detection: given a dataset with only input-label pairs, our goal is to detect biases that may hinder predictors’ generalization performance.\n\nWe propose Learning to Split (ls), an algorithm that simulates generalization failure directly from the set of input-label pairs. Specifically, ls learns to split the dataset so that predictors trained on the training split cannot generalize to the testing split (Figure 1). This performance gap indicates that the testing split is under-represented among the set of annotations, which is a signal of potential bias.\n\nThe challenge in this seemingly simple formulation lies in the existence of many trivial splits. For example, poor testing performance can result from a training split that is much smaller than the testing split (Figure 2a). Classifiers will also fail if the training split contains all positive examples, leaving the testing split with only negative examples (Figure 2b). The poor generalization of these trivial solutions arise from the lack of training data and label imbalance, and they do not reveal the hidden biases. To ensure that the learned splits are meaningful, we impose two regularity constraints on the splits. First, the size of the training split must be comparable to the size of the testing split. Second, the marginal distribution of the labels should be the similar across the splits.\n\nOur algorithm ls consists of two components, Splitter and Predictor. At each iteration, the Splitter first assigns each input-label pair to either the training split or the testing split. The Predictor then takes the training split and learns how to predict the label from the input. Its prediction performance on the testing split is used to guide the Splitter towards a more challenging split (under the regularity\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nOriginal dataset\n\nY=Samoyeds\n\nY=Samoyeds\n\nY=Samoyeds\n\nY=Samoyeds\n\nls\n\nY=PolarBears\n\nY=PolarBears\n\nY=PolarBears\n\nY=PolarBears\n\nTraining\n\nTesting\n\nY=Samoyeds\n\nY=Samoyeds\n\nY=Samoyeds\n\nY=Samoyeds\n\nY=PolarBears\n\nY=PolarBears\n\nY=PolarBears\n\nY=PolarBears\n\nAn example predictor\n\n100%\n\nIs background white?\n\nN\n\nY\n\nSamoyeds\n\nPolarBears\n\n0%\n\ntrain acc\n\ntest acc \n\nFigure 1: Consider the task of classifying samoyed images vs. polar bear images. Given the set of image-label pairs, our algorithm ls learns to split the data so that predictors trained on the training split cannot generalize to the testing split. The learned splits help us identify the hidden biases. For example, while predictors can achieve perfect performance on the training split by using the spurious heuristic: polar bears live in snowy habitats, they fail to generalize to the under-represented group (polar bears that appear on grass).\n\nconstraints) for the next iteration. Specifically, while we do not have any explicit annotations for creating non-generalizable splits, we show that the prediction correctness of each testing example can serve as a source of weak supervision: generalization performance will decrease if we move examples that are predicted correctly away from the testing split, leaving only those predicted incorrectly.\n\nls is task-agnostic and can be applied to any supervised learning problem, ranging from natural language understanding (Beer Reviews, MNLI) and image classification (Waterbirds, CelebA) to molecular property prediction (Tox21). Given the set of input-label pairs, ls consistently identifies splits across which predictors cannot generalize. For example in MNLI, the generalization performance drops from 79.4% (split by random) to 27.8% (split by ls) for a standard BERT-based predictor. Further analysis reveals that our learned splits coincide with human-identified biases. Finally, we demonstrate that combining group distributionally robust optimization (DRO) with splits identified by ls enables automatic de-biasing. Compared with previous state-of-the-art, we substantially improves the worst-group performance (23.4% on average) when the sources of bias are completely unknown during training and validation.\n\n2 RELATED WORK\n\nDe-biasing algorithms Modern datasets are often coupled with unwanted biases Buolamwini & Gebru (2018); Schuster et al. (2019); McCoy et al. (2019); Yang et al. (2019). If the biases have already been identified, we can use this prior knowledge to regulate their negative impact Kusner et al. (2017); Hu et al. (2018); Oren et al. (2019); Belinkov et al. (2019); Stacey et al. (2020); Clark et al. (2019); He et al. (2019); Mahabadi et al. (2020); Sagawa et al. (2020); Singh et al. (2021). The challenge arises when the source of biases is unknown (Li & Xu, 2021). Recent work has shown that the mistakes of a standard ERM predictor on its training data are informative of the biases (Bao et al., 2021; Sanh et al., 2021; Nam et al., 2020; Utama et al., 2020; Liu et al., 2021a; Lahoti et al., 2020; Liu et al., 2021b; Bao et al., 2022). They deliver robustness by boosting from the mistakes. (Wang & Vasconcelos, 2018; Yoo & Kweon, 2019) also utilize prediction correctness for confidence estimation and active learning. (Creager et al., 2021; Sohoni et al., 2020; Ahmed et al., 2020; Matsuura & Harada, 2020) further analyze the predictor’s hidden activations to identify under-represented groups. However, many other factors (such as the initialization, the representation power, the amount of annotations, etc) can contribute to the predictors’ training mistakes. For example, predictors that lack representation power may simply under-fit the training data.\n\nIn this work, instead of looking at the training statistics of the predictor, we focus on its generalization gap from the training split to the testing split. This effectively balances those unwanted factors.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nTraining\n\nTesting\n\nTraining\n\nTesting\n\nY=Samoyeds\n\nY=Samoyeds\n\nY=Samoyeds\n\nY=Samoyeds\n\nY=Samoyeds\n\nY=Samoyeds\n\nY=PolarBears\n\nY=PolarBears\n\nY=PolarBears\n\nY=PolarBears Y=PolarBears Y=PolarBears\n\nY=Samoyeds\n\nY=Samoyeds\n\nY=PolarBears\n\nY=PolarBears\n\n(a) Predictors cannot generalize if the size of the training split is incomparable to the size of the testing split.\n\n(b) Predictors cannot generalize if the training split contains only samoyeds and the the testing split contains only polar bears.\n\nFigure 2: Splits that are difficult to generalize do not necessarily reveal hidden biases. (a) Predictors cannot generalize if the amount of annotations is insufficient. (b) Predictors fail to generalize when the labels are unbalanced in training and testing. ls poses two regularity constraints to avoid such degenerative solutions: the training split and testing split should have comparable sizes; the marginal distribution of the label should be similar across the splits.\n\nGoing back to the previous example, if the training and test splits share the same distribution, the generalization gap will be small even if the predictors are underfitted. The gap will increase only when the training and testing splits have different prediction characteristics. Furthermore, instead of using a fixed predictor, we iteratively refine the predictor during training so that it faithfully measures the generalization gap given the current Splitter.\n\nHeuristics for data splitting Data splitting strategy directly impacts the difficulty of the underlying generalization task. Therefore, in domains where out-of-distribution generalization is crucial for performance, various heuristics are used to find challenging splits Sheridan (2013); Yang et al. (2019); Bandi et al. (2018); Yala et al. (2021); Taylor et al. (2019); Koh et al. (2021). Examples include scaffold split in molecules and batch split for cells. Unlike these methods, which rely on human-specified heuristics, our algorithm ls learns how to split directly from the dataset alone and can therefore be applied to scenarios where human knowledge is unavailable or incomplete.\n\n3 LEARNING TO SPLIT\n\n3.1 MOTIVATION\n\nGiven a dataset\n\ntotal with input-label pairs\n\n(x, y)\n\ntrain and\n\ntest, such that predictors learned on the training split\n\n, our goal is to split this dataset into two subsets, train cannot generalize to the testing\n\n}\n\nD\n\n{\n\nD\n\nD split\n\nD test.1\n\nD\n\nWhy do we have to discover such splits? Before deploying our trained models, it is crucial to understand the extent to which these models can even generalize within the given dataset. The standard cross-validation approach attempts to measure generalization by randomly splitting the dataset (Stone, 1974; Allen, 1974). However, this measure only reflects the average performance total(x, y). There is no guarantee of performance if our data under the same data distribution P D\ndistribution changes at test time (e.g. increasing the proportion of the minority group). For example, consider the task of classifying samoyeds vs. polar bears (Figure 1). Models can achieve good average performance by using spurious heuristics such as “polar bears live in snowy habitats” and “samoyeds play on grass”. Finding splits across which the models cannot generalize helps us identify underrepresented groups (polar bears that appear on grass).\n\nHow to discover such splits? Our algorithm ls has two components, a Splitter that decides how to split the dataset and a Predictor that estimates the generalization gap from the training split to the testing split. At each iteration, the splitter uses the feedback from the predictor to update its splitting decision. One can view this splitting decision as a latent variable that represents the prediction characteristic of each input. To avoid degenerate solutions, we require the Splitter to satisfy two regularity constraints: the size of the training split should be comparable to the size of the testing split (Figure 2a); and the marginal distribution of the label should be similar across the splits (Figure 2b).\n\n1To prevent over-fitting, we held-out 1/3 of the training split for early-stopping when training the Predictor.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 ls: learning to split (see Algorithm 2 for full details)\n\nInput: dataset Output: data splits\n\nD\n\ntotal\n\ntrain,\n\ntest\n\nD 1: Initialize Splitter to random splitting 2: repeat 3:\n\nApply Splitter to split\n\nD\n\nsplitting decision zi 2{\n\n4: 5:\n\nInitialize Predictor and train Predictor on Evaluate Predictor on racy/AUC. repeat\n\ntrain and\n\nD\n\nD\n\n6: 7: 8: 9: 10: 11: until generalization gap stops increasing\n\nSample a mini-batch from D\nSample another mini-batch from total = Compute the overall loss total stops decreasing\n\nuntil\n\nD L\n\nL\n\nL\n\ntotal into D\n0, 1\n\ntrain, from PSplitter(zi |\n\nD\n\nD\n\n}\n\nxi, yi).\n\ntest. For each input-label pair (xi, yi), sample the\n\ntrain using empirical risk minimization. test. Compute generalization gap = difference in accu-\n\nD\n\ntotal to compute the regularity constraints ⌦1, ⌦2 (Eq 1). gap (Eq 2). test to compute gap +⌦ 1 +⌦ 2. Update Splitter to minimize\n\nL\n\ntotal.\n\nL\n\n3.2 SPLITTER AND PREDICTOR\n\nHere we describe the two key components of our algorithm, Splitter and Predictor, in the context of classification tasks. The algorithm itself generalizes to regression problems as well.\n\nSplitter Given a list of input-label pairs how to partition this dataset into a training split\n\ntotal = [(x1, y2), . . . , (xn, yn)], the Splitter decides test. We can view its splitting decisions as a list of latent variables z = [z1, . . . , zn] where each zi 2{ indicates whether example (xi, yi) is included in the training split or not. In this work, we assume independent selections for simplicity. That is, the Splitter takes one input-label pair (xi, yi) at a time and predicts xi, yi) of allocating this example to the training split. We can factor the probability PSplitter(zi |\n\ntrain and a testing split\n\nthe joint probability of our splitting decisions as\n\n1, 0\n\nD\n\nD\n\nD\n\n}\n\nn\n\nP(z\n\n| D\n\ntotal) =\n\nPSplitter(zi |\n\nxi, yi).\n\ni=1 Y\n\nWe can sample from the Splitter’s predictions PSplitter(zi |\n\ntrain and test. Note that while the splitting decisions are independent across different examples, the Splitter total, from the Predictor during training.\n\nD receives global feedback, dependent on the entire dataset\n\nxi, yi) to obtain the splits\n\nD\n\nD\n\nPredictor The Predictor takes an input x and predicts the probability of its label PP redictor(y |\nx). The goal of this Predictor is to provide feedback for the Splitter so that it can generate more challenging splits at the next iteration.\n\nSpecifically, given the Splitter’s current splitting decisions, we re-initialize the Predictor and train train. This re-initialization step is critical it to minimize the empirical risk on the training split because it ensures that the predictor does not carry over past information (from previous splits) and faithfully represents the current generalization gap. On the other hand, we note that neural networks can easily remember the training split. To prevent over-fitting, we held-out 1/3 of the training split for early stopping. After training, we evaluate the generalization performance of the Predictor on the testing split\n\ntest.\n\nD\n\nD\n\n3.3 REGULARITY CONSTRAINTS\n\nMany factors can impact generalization, but not all of them are of interest. For example, the Predictor may naturally fail to generalize due to the lack of training data or due to label imbalance across the splits (Figure 2). To avoid these trivial solutions, we introduce two soft regularizers to shape the Splitter’s decisions:\n\n⌦1 = DKL(P(z) ⌦2 = DKL(P(y\n\nBernoulli()), k\nz = 1) |\n\nk\n\nP(y)) + DKL(P(y\n\nz = 0)\n\nP(y)).\n\nk\n\n|\n\n(1)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nThe first term ⌦1 ensures that we have sufficient training examples in n\nmarginal distribution P(z) = 1\n\ntrain. Specifically, the xi, yi) represents what percentages of n\ntest. We penalize the Splitter if it moves too far away from the prior D\ndistribution Bernoulli(). Centola et al. (2018) suggest that minority groups typically make up 25 percent of the population. Therefore, we fix  = 0.75 in all experiments.2\n\ni=1 PSplitter(zi = z\n\ntotal are split into\n\ntrain and\n\nP\n\nD\n\nD\n\nD\n\n|\n\nThe second term ⌦2 aims to reduce label imbalance across the splits. It achieves this goal by pushing the label marginals in the training split P(y z = 0) to be close to total. We can apply Bayes’s rule to compute these conditional the original label marginal P(y) in label marginals directly from the Splitter’s decisions PS.(zi | y(yi) PS.(zi = 1 i PS.(zi = 1\n\ny(yi) PS.(zi = 0 i PS.(zi = 0\n\nz = 1) and the testing split P(y\n\n| xi, yi)\n\n| xi, yi)\n\nz = 0) =\n\nz = 1) =\n\n, P(y\n\nxi, yi):\n\nxi, yi)\n\nxi, yi)\n\nP(y\n\nD\n\n|\n\n|\n\n|\n\n.\n\n|\n\ni\n\ni\n\nP\n\n|\n\nP\n\n|\n\n3.4 TRAINING STRATEGY\n\nP\n\nP\n\nThe only question that remains is how to learn the Splitter. Our goal is to produce difficult and nontrivial splits so that the Predictor cannot generalize. However, the challenge is that we don’t have any explicit annotations for the splitting decisions.\n\nThere are a few options to address this challenge. From the meta learning perspective, we can backpropagate the Predictor’s loss on the testing split directly to the Splitter. This process is expensive as it involves higher order gradients from the Predictor’s training. While one can apply episodictraining (Vinyals et al., 2016) to reduce the computation cost, the Splitter’s decision will be biased by the size of the learning episodes (since the Predictor only operates on the sampled episode). From the reinforcement learning viewpoint, we can cast our objectives, maximizing the generalization gap while maintaining the regularity constraints, into a reward function (Lei et al., 2016). However, according to our preliminary experiments, the learning signal from this scalar reward is too sparse for the Splitter to learn meaningful splits.\n\nIn this work, we take a simple yet effective approach to learn the Splitter. Our intuition is that the Predictor’s generalization performance will drop if we move examples that are predicted correctly away from the testing split, leaving only those that are mispredicted. In other words, we can view the prediction correctness of the testing example as a direct supervision for the Splitter.\n\nFormally, let ˆyi be the Predictor’s prediction for input xi: ˆyi = arg maxy PP redictor(y xi). We minimize the cross entropy loss between the Splitter’s decision and the Predictor’s prediction correctness over the testing split:\n\n|\n\ngap =\n\nL\n\n1 test\n\n|D\n\n| X(xi,yi)\n\n2D\n\nCE(PSplitter(zi |\n\ntest L\n\nxi, yi), yi ( ˆyi)).\n\n(2)\n\nCombining with the aforementioned regularity constraints, the overall objective for the Splitter is\n\ntotal =\n\nL\n\nL\n\ngap +⌦ 1 +⌦ 2,\n\n(3)\n\nOne can explore different weighting schemes for the three loss terms (Chen et al., 2018). In this paper, we found that the unweighted summation (Eq 3) works well out-of-the-box across all our experiments. Algorithm 1 presents the pseudo-code of our algorithm. At each outer-loop (line 2-11), test. We train the Predicwe start by using the current Splitter to partition D\ntest. For computation train and evaluate its generalization performance on tor from scratch on efficiency, we sample mini-batches in the inner-loop (line 6-10) and update the Splitter based on Eq equation 3.\n\ntotal into\n\ntrain and\n\nD\n\nD\n\nD\n\nD\n\n2We note that the two regularizers ⌦1 and ⌦2 are introduced to shape the Splitter’s decisions, but the model has the flexibility to deviate from this “prior.” That is, the actual “posteriors” can be different depending on the dataset. For example, the minority group is unlikely to always constitute exactly 25% of the dataset. Therefore, it makes more sense to introduce soft regularizers instead of hard (and exact) constraints. Nevertheless, if users want to allocate exactly 25% of the data into the test set, instead of sampling from the Splitter’s decisions PSplitter(zi\n\nxi, yi), they can simply sort these probabilities and split at the 25th percentile.\n\n|\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nBeer Look\n\nNegative\n\nPositive\n\nBeer Aroma\n\nNegative\n\nPositive\n\nTox21 AR\n\nActive\n\nInactive\n\ntrain ls\n\nD\n\ntest ls\n\nD\n\ntrain ls\n\nD\n\ntest ls\n\nD\n\ntrain ls\n\nD\n\ntest ls\n\nD\n\n100\n\n75\n\n50\n\n25\n\n0\n\ny c\na r\nu c\nc A\n\nTrain\n\nTest\n\n100\n\n75\n\n50\n\n25\n\n0\n\ny c\na r\nu c\nc A\n\nTrain\n\nTest\n\n100\n\n75\n\n50\n\n25\n\n0\n\nC O\nR\n\n-\n\nC U\nA\n\nTrain\n\nTest\n\nrandom\n\nls\n\nrandom\n\nls\n\nrandom\n\nls\n\nWaterbirds\n\nWaterbirds\n\nLandbirds\n\nCelebA\n\nNo_blond_hair\n\nBlond_hair\n\nMNLI\n\nContradiction\n\nEntailment\n\nNetural\n\ntrain ls\n\nD\n\ntest ls\n\nD\n\ntrain ls\n\nD\n\ntest ls\n\nD\n\ntrain ls\n\nD\n\ntest ls\n\nD\n\n100\n\n75\n\n50\n\n25\n\n0\n\ny c\na r\nu c\nc A\n\nTrain\n\nTest\n\n100\n\n75\n\n50\n\n25\n\n0\n\ny c\na r\nu c\nc A\n\nTrain\n\nTest\n\n100\n\n75\n\n50\n\n25\n\n0\n\ny c\na r\nu c\nc A\n\nTrain\n\nTest\n\nrandom\n\nls\n\nrandom\n\nls\n\nrandom\n\nls\n\n val 0.978 test 0.847 gap 0.131 ratio 0.84 ptrain_y 0.14 ptest_y 0.1\n\nFigure 3: Even while the label distributions remain similar (blue), predictors that generalize on random splits fail to generalize on splits identified by ls (green). For both splits, to prevent over-fitting, we held-out 1/3 of the training split for early-stopping. In MNLI (lower right), the generalization gap for a standard BERT-based model is 93.6%\n\n27.8% = 65.8%.\n\nWaterbirds\n\nInputs with land backgrounds\n\ntrain ls\n\nD\n\ntest ls\n\nD\n\nCelebA\n\ntrain ls\n\nD\n\ntest ls\n\nD\n\nInputs with male=True\n\nMNLI\n\ntrain ls\n\nD\n\ntest ls\n\nD\n\nInputs with negation words\n\nWaterbirds\n\nLandbirds\n\nBlond_hair\n\nNo_blond_hair\n\nContradiction\n\nEntailment & Neutral\n\nFigure 4: The splits learned by ls correlate with human-identified biases. For example in Waterbirds (left), ls learns to amplify the spurious association between landbirds and land backgrounds in the train. As a result, predictors will over-fit the background features and fail to generalize training split test) when the spurious correlation is reduced. at test time (\n\nD D\n\n4 EXPERIMENTS\n\nWe conduct experiments over multiple modalities (Section 4.1) and answer two main questions. Can ls identify splits that are not generalizable (Section 4.2)? Can we use the splits identified by ls to reduce unknown biases (Section 4.3)? Implementation details are deferred to the Appendix. Our code is included in the supplemental materials and will be publicly available.\n\n4.1 DATASET\n\nBeer Reviews We use the BeerAdvocate review dataset (McAuley et al., 2012) where each input review describes multiple aspects of a beer and is written by a website user. Following previous work (Lei et al., 2016), we consider two aspect-level sentiment classification tasks: look and aroma. There are 2,500 positive reviews and 2,500 negative reviews for each task. The average word count per review is 128.5. We apply ls to identify spurious splits for each task.\n\nTox21 Tox21 is a property prediction benchmark with 12,707 molecules Huang et al. (2016). Each input is annotated with a set of binary properties that represent the outcomes of different toxicological experiments. We consider the property Androgen Receptor (active or inactive) as our prediction target. We apply ls to identify spurious splits over the entire dataset.\n\n6\n\n1,9581,95154954245.086.696.386.02,0972,04240345850.184.793.488.17,2562312,58716533.478.195.077.2129,08120,2319,4224,03670.295.198.095.43,78099883538170.585.991.785.553,88157,27055,80514,74111,62712,85127.879.493.680.272%18%28%82%50%98%50%2%87%99%13%1%< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nd I\nC /\nq l\n0 j\ny B\no k\nH B\nq N\no +\nZ s\nM B\ny 0\nl k\ns =\n\" >\nA A\nA C\nA 3\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\no t\n7 0\nE i\ny C\np 5\nK I\nq M\ne i\nH j\nx W\ns B\n/ Q\nx L\nL Z\nb t\nq l\nm 0\n3 Y\nn Y\ng l\nB L\nz 4\nV 7\nx 4\nU M\nS r\nf 8\nK b\n/ 8\nZ N\nm 4\nO 2\nP h\nh 4\nv D\nf D\nz D\nw /\n5 k\ny B\nb X\n8 b\np Y\nX F\np e\nW V\n8 m\np l\nb X\n1 j\nc 8\nv c\n3 m\nm p\nK J\nG E\nN k\nn E\nI 9\nn x\ns a\nK c\nC d\no E\nB p\nx 2\nY k\nl x\n6 H\nP a\n9 k\ne X\nu d\n+ +\np 1\nK x\nS N\nz C\nO K\nZ e\ni A\ne C\nB Y\nx g\n0 F\nL P\n3 H\nN D\nD E\nO C\ne X\nq V\n3 a\nU u\n0 A\nd I\nQ W\nI m\ns q\nx n\nV u\n2 a\nP Y\nE 1\nT 5\ny C\nV F\nG B\nR s\n/ 8\nc v\ns R\nS U\nI q\ng H\nC s\nV N\ne x\nY /\nB S\nL I\nE R\nT r\nO K\nm y\ng a\nY z\nL C\nA 9\nr V\nV O\nC Q\nK i\n+ d\n/ J\nB Z\nh 1\nr p\nW 0\nE k\nd Q\nm w\nJ u\nr v\ni R\nS H\nS o\n1 D\nX 3\nf m\nF 6\nt Z\nL x\nf /\n8 7\no J\nB O\nd e\ny k\nS c\nA B\nV k\nu i\nh I\nu A\nW R\nl Q\nd i\n9 Z\nm k\nB P\nh Y\nE 0\nw k\n0 7\nd a\nZ I\ng l\nJ q\nB j\nq +\ng Q\nn N\nm X\n5 0\nn r\nu O\na c\n1 p\ny b\nk 2\nr 9\no o\ni j\nj P\nb R\nA T\np C\nD j\np D\nd X\nS N\nG q\ni J\nC H\np E\nz +\ng V\nv R\nl P\nx o\nv x\nb n\nx M\nW 0\nt G\nM b\nO L\n/ s\nD 4\n/ A\nF 5\nP p\ni 0\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nR p\nT Y\nT V\nz C\nw d\nl l\nt t\nD H\nv i\nZ y\nh s\n/ z\ne c\nE =\n\" >\nA A\nA C\nA n\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\nq p\n7 E\nS 7\nA I\nn k\no i\no h\n6 L\ne v\nB Y\nw X\n5 A\nE 8\nt m\nO 2\n2 X\nb j\n7 Y\nn Y\ng l\nB C\n/ +\nF S\n8 e\nF P\nH q\nr /\nD m\nv 3\nH T\n5 q\nC t\nD w\nY e\n7 8\n0 w\nM 8\n+ L\nB F\nd o\nW d\n9 G\nY W\nF x\na X\nm l\nu F\np a\nW 9\n/ Y\n3 C\np v\n7 z\nR V\nG E\ns G\nD R\na K\nU L\nY 9\nq k\nD w\nA B\nr I\nU U\nA 7\nk k\nB 9\nT 0\nD L\nG 1\n1 m\nf u\ns e\np O\nJ h\nc I\nv j\nC F\ny f\nD g\nL e\n5 4\ny i\nl r\nr l\nP c\ne n\nO G\nR U\nJ F\nf p\nX e\nI g\nP G\nC C\no D\nB N\nu +\nW K\nV b\nU m\nM O\ne J\nn Z\nM K\ny V\nH v\nl r\n+ c\nX s\nh i\nH w\nJ k\ng i\nr V\ns a\n0 I\n3 Y\nR K\n5 E\nx A\nW n\nJ i\nB R\nF l\nI z\nq A\nj q\nY B\n9 U\nG 5\ny e\nS F\n1 D\nz U\nS s\n/ s\nh 1\nJ X\ng O\nZ E\n/ T\n2 R\nU F\n+ p\ns e\n/ p\nz u\nx g\nN e\nt l\n4 n\n9 e\nJ 8\nb +\nu Z\nv w\nI I\no R\nA j\nZ d\n1 I\n+ F\ni a\nG Z\n5 W\nH 2\nu A\nS G\nY q\nw J\nZ Z\nL r\nW 0\n0 2\np J\nI y\n1 K\nm V\nd A\nj 2\n7 M\nv z\np H\nl c\nt U\n+ r\n9 s\n1 J\np X\na R\nx 1\nE k\n+ +\nS A\nH B\nG b\nn J\nE a\nu S\nZ 1\n0 i\nC M\nP J\nJ n\n8 k\nr e\nj C\nf j\nx X\ng 3\nP q\na t\nB S\nO f\n2 S\nV /\nY H\nz +\nA L\ns p\nm E\nw =\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nd I\nC /\nq l\n0 j\ny B\no k\nH B\nq N\no +\nZ s\nM B\ny 0\nl k\ns =\n\" >\nA A\nA C\nA 3\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\no t\n7 0\nE i\ny C\np 5\nK I\nq M\ne i\nH j\nx W\ns B\n/ Q\nx L\nL Z\nb t\nq l\nm 0\n3 Y\nn Y\ng l\nB L\nz 4\nV 7\nx 4\nU M\nS r\nf 8\nK b\n/ 8\nZ N\nm 4\nO 2\nP h\nh 4\nv D\nf D\nz D\nw /\n5 k\ny B\nb X\n8 b\np Y\nX F\np e\nW V\n8 m\np l\nb X\n1 j\nc 8\nv c\n3 m\nm p\nK J\nG E\nN k\nn E\nI 9\nn x\ns a\nK c\nC d\no E\nB p\nx 2\nY k\nl x\n6 H\nP a\n9 k\ne X\nu d\n+ +\np 1\nK x\nS N\nz C\nO K\nZ e\ni A\ne C\nB Y\nx g\n0 F\nL P\n3 H\nN D\nD E\nO C\ne X\nq V\n3 a\nU u\n0 A\nd I\nQ W\nI m\ns q\nx n\nV u\n2 a\nP Y\nE 1\nT 5\ny C\nV F\nG B\nR s\n/ 8\nc v\ns R\nS U\nI q\ng H\nC s\nV N\ne x\nY /\nB S\nL I\nE R\nT r\nO K\nm y\ng a\nY z\nL C\nA 9\nr V\nV O\nC Q\nK i\n+ d\n/ J\nB Z\nh 1\nr p\nW 0\nE k\nd Q\nm w\nJ u\nr v\ni R\nS H\nS o\n1 D\nX 3\nf m\nF 6\nt Z\nL x\nf /\n8 7\no J\nB O\nd e\ny k\nS c\nA B\nV k\nu i\nh I\nu A\nW R\nl Q\nd i\n9 Z\nm k\nB P\nh Y\nE 0\nw k\n0 7\nd a\nZ I\ng l\nJ q\nB j\nq +\ng Q\nn N\nm X\n5 0\nn r\nu O\na c\n1 p\ny b\nk 2\nr 9\no o\ni j\nj P\nb R\nA T\np C\nD j\np D\nd X\nS N\nG q\ni J\nC H\np E\nz +\ng V\nv R\nl P\nx o\nv x\nb n\nx M\nW 0\nt G\nM b\nO L\n/ s\nD 4\n/ A\nF 5\nP p\ni 0\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nR p\nT Y\nT V\nz C\nw d\nl l\nt t\nD H\nv i\nZ y\nh s\n/ z\ne c\nE =\n\" >\nA A\nA C\nA n\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\nq p\n7 E\nS 7\nA I\nn k\no i\no h\n6 L\ne v\nB Y\nw X\n5 A\nE 8\nt m\nO 2\n2 X\nb j\n7 Y\nn Y\ng l\nB C\n/ +\nF S\n8 e\nF P\nH q\nr /\nD m\nv 3\nH T\n5 q\nC t\nD w\nY e\n7 8\n0 w\nM 8\n+ L\nB F\nd o\nW d\n9 G\nY W\nF x\na X\nm l\nu F\np a\nW 9\n/ Y\n3 C\np v\n7 z\nR V\nG E\ns G\nD R\na K\nU L\nY 9\nq k\nD w\nA B\nr I\nU U\nA 7\nk k\nB 9\nT 0\nD L\nG 1\n1 m\nf u\ns e\np O\nJ h\nc I\nv j\nC F\ny f\nD g\nL e\n5 4\ny i\nl r\nr l\nP c\ne n\nO G\nR U\nJ F\nf p\nX e\nI g\nP G\nC C\no D\nB N\nu +\nW K\nV b\nU m\nM O\ne J\nn Z\nM K\ny V\nH v\nl r\n+ c\nX s\nh i\nH w\nJ k\ng i\nr V\ns a\n0 I\n3 Y\nR K\n5 E\nx A\nW n\nJ i\nB R\nF l\nI z\nq A\nj q\nY B\n9 U\nG 5\ny e\nS F\n1 D\nz U\nS s\n/ s\nh 1\nJ X\ng O\nZ E\n/ T\n2 R\nU F\n+ p\ns e\n/ p\nz u\nx g\nN e\nt l\n4 n\n9 e\nJ 8\nb +\nu Z\nv w\nI I\no R\nA j\nZ d\n1 I\n+ F\ni a\nG Z\n5 W\nH 2\nu A\nS G\nY q\nw J\nZ Z\nL r\nW 0\n0 2\np J\nI y\n1 K\nm V\nd A\nj 2\n7 M\nv z\np H\nl c\nt U\n+ r\n9 s\n1 J\np X\na R\nx 1\nE k\n+ +\nS A\nH B\nG b\nn J\nE a\nu S\nZ 1\n0 i\nC M\nP J\nJ n\n8 k\nr e\nj C\nf j\nx X\ng 3\nP q\na t\nB S\nO f\n2 S\nV /\nY H\nz +\nA L\ns p\nm E\nw =\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nd I\nC /\nq l\n0 j\ny B\no k\nH B\nq N\no +\nZ s\nM B\ny 0\nl k\ns =\n\" >\nA A\nA C\nA 3\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\no t\n7 0\nE i\ny C\np 5\nK I\nq M\ne i\nH j\nx W\ns B\n/ Q\nx L\nL Z\nb t\nq l\nm 0\n3 Y\nn Y\ng l\nB L\nz 4\nV 7\nx 4\nU M\nS r\nf 8\nK b\n/ 8\nZ N\nm 4\nO 2\nP h\nh 4\nv D\nf D\nz D\nw /\n5 k\ny B\nb X\n8 b\np Y\nX F\np e\nW V\n8 m\np l\nb X\n1 j\nc 8\nv c\n3 m\nm p\nK J\nG E\nN k\nn E\nI 9\nn x\ns a\nK c\nC d\no E\nB p\nx 2\nY k\nl x\n6 H\nP a\n9 k\ne X\nu d\n+ +\np 1\nK x\nS N\nz C\nO K\nZ e\ni A\ne C\nB Y\nx g\n0 F\nL P\n3 H\nN D\nD E\nO C\ne X\nq V\n3 a\nU u\n0 A\nd I\nQ W\nI m\ns q\nx n\nV u\n2 a\nP Y\nE 1\nT 5\ny C\nV F\nG B\nR s\n/ 8\nc v\ns R\nS U\nI q\ng H\nC s\nV N\ne x\nY /\nB S\nL I\nE R\nT r\nO K\nm y\ng a\nY z\nL C\nA 9\nr V\nV O\nC Q\nK i\n+ d\n/ J\nB Z\nh 1\nr p\nW 0\nE k\nd Q\nm w\nJ u\nr v\ni R\nS H\nS o\n1 D\nX 3\nf m\nF 6\nt Z\nL x\nf /\n8 7\no J\nB O\nd e\ny k\nS c\nA B\nV k\nu i\nh I\nu A\nW R\nl Q\nd i\n9 Z\nm k\nB P\nh Y\nE 0\nw k\n0 7\nd a\nZ I\ng l\nJ q\nB j\nq +\ng Q\nn N\nm X\n5 0\nn r\nu O\na c\n1 p\ny b\nk 2\nr 9\no o\ni j\nj P\nb R\nA T\np C\nD j\np D\nd X\nS N\nG q\ni J\nC H\np E\nz +\ng V\nv R\nl P\nx o\nv x\nb n\nx M\nW 0\nt G\nM b\nO L\n/ s\nD 4\n/ A\nF 5\nP p\ni 0\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nR p\nT Y\nT V\nz C\nw d\nl l\nt t\nD H\nv i\nZ y\nh s\n/ z\ne c\nE =\n\" >\nA A\nA C\nA n\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\nq p\n7 E\nS 7\nA I\nn k\no i\no h\n6 L\ne v\nB Y\nw X\n5 A\nE 8\nt m\nO 2\n2 X\nb j\n7 Y\nn Y\ng l\nB C\n/ +\nF S\n8 e\nF P\nH q\nr /\nD m\nv 3\nH T\n5 q\nC t\nD w\nY e\n7 8\n0 w\nM 8\n+ L\nB F\nd o\nW d\n9 G\nY W\nF x\na X\nm l\nu F\np a\nW 9\n/ Y\n3 C\np v\n7 z\nR V\nG E\ns G\nD R\na K\nU L\nY 9\nq k\nD w\nA B\nr I\nU U\nA 7\nk k\nB 9\nT 0\nD L\nG 1\n1 m\nf u\ns e\np O\nJ h\nc I\nv j\nC F\ny f\nD g\nL e\n5 4\ny i\nl r\nr l\nP c\ne n\nO G\nR U\nJ F\nf p\nX e\nI g\nP G\nC C\no D\nB N\nu +\nW K\nV b\nU m\nM O\ne J\nn Z\nM K\ny V\nH v\nl r\n+ c\nX s\nh i\nH w\nJ k\ng i\nr V\ns a\n0 I\n3 Y\nR K\n5 E\nx A\nW n\nJ i\nB R\nF l\nI z\nq A\nj q\nY B\n9 U\nG 5\ny e\nS F\n1 D\nz U\nS s\n/ s\nh 1\nJ X\ng O\nZ E\n/ T\n2 R\nU F\n+ p\ns e\n/ p\nz u\nx g\nN e\nt l\n4 n\n9 e\nJ 8\nb +\nu Z\nv w\nI I\no R\nA j\nZ d\n1 I\n+ F\ni a\nG Z\n5 W\nH 2\nu A\nS G\nY q\nw J\nZ Z\nL r\nW 0\n0 2\np J\nI y\n1 K\nm V\nd A\nj 2\n7 M\nv z\np H\nl c\nt U\n+ r\n9 s\n1 J\np X\na R\nx 1\nE k\n+ +\nS A\nH B\nG b\nn J\nE a\nu S\nZ 1\n0 i\nC M\nP J\nJ n\n8 k\nr e\nj C\nf j\nx X\ng 3\nP q\na t\nB S\nO f\n2 S\nV /\nY H\nz +\nA L\ns p\nm E\nw =\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nd I\nC /\nq l\n0 j\ny B\no k\nH B\nq N\no +\nZ s\nM B\ny 0\nl k\ns =\n\" >\nA A\nA C\nA 3\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\no t\n7 0\nE i\ny C\np 5\nK I\nq M\ne i\nH j\nx W\ns B\n/ Q\nx L\nL Z\nb t\nq l\nm 0\n3 Y\nn Y\ng l\nB L\nz 4\nV 7\nx 4\nU M\nS r\nf 8\nK b\n/ 8\nZ N\nm 4\nO 2\nP h\nh 4\nv D\nf D\nz D\nw /\n5 k\ny B\nb X\n8 b\np Y\nX F\np e\nW V\n8 m\np l\nb X\n1 j\nc 8\nv c\n3 m\nm p\nK J\nG E\nN k\nn E\nI 9\nn x\ns a\nK c\nC d\no E\nB p\nx 2\nY k\nl x\n6 H\nP a\n9 k\ne X\nu d\n+ +\np 1\nK x\nS N\nz C\nO K\nZ e\ni A\ne C\nB Y\nx g\n0 F\nL P\n3 H\nN D\nD E\nO C\ne X\nq V\n3 a\nU u\n0 A\nd I\nQ W\nI m\ns q\nx n\nV u\n2 a\nP Y\nE 1\nT 5\ny C\nV F\nG B\nR s\n/ 8\nc v\ns R\nS U\nI q\ng H\nC s\nV N\ne x\nY /\nB S\nL I\nE R\nT r\nO K\nm y\ng a\nY z\nL C\nA 9\nr V\nV O\nC Q\nK i\n+ d\n/ J\nB Z\nh 1\nr p\nW 0\nE k\nd Q\nm w\nJ u\nr v\ni R\nS H\nS o\n1 D\nX 3\nf m\nF 6\nt Z\nL x\nf /\n8 7\no J\nB O\nd e\ny k\nS c\nA B\nV k\nu i\nh I\nu A\nW R\nl Q\nd i\n9 Z\nm k\nB P\nh Y\nE 0\nw k\n0 7\nd a\nZ I\ng l\nJ q\nB j\nq +\ng Q\nn N\nm X\n5 0\nn r\nu O\na c\n1 p\ny b\nk 2\nr 9\no o\ni j\nj P\nb R\nA T\np C\nD j\np D\nd X\nS N\nG q\ni J\nC H\np E\nz +\ng V\nv R\nl P\nx o\nv x\nb n\nx M\nW 0\nt G\nM b\nO L\n/ s\nD 4\n/ A\nF 5\nP p\ni 0\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nR p\nT Y\nT V\nz C\nw d\nl l\nt t\nD H\nv i\nZ y\nh s\n/ z\ne c\nE =\n\" >\nA A\nA C\nA n\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\nq p\n7 E\nS 7\nA I\nn k\no i\no h\n6 L\ne v\nB Y\nw X\n5 A\nE 8\nt m\nO 2\n2 X\nb j\n7 Y\nn Y\ng l\nB C\n/ +\nF S\n8 e\nF P\nH q\nr /\nD m\nv 3\nH T\n5 q\nC t\nD w\nY e\n7 8\n0 w\nM 8\n+ L\nB F\nd o\nW d\n9 G\nY W\nF x\na X\nm l\nu F\np a\nW 9\n/ Y\n3 C\np v\n7 z\nR V\nG E\ns G\nD R\na K\nU L\nY 9\nq k\nD w\nA B\nr I\nU U\nA 7\nk k\nB 9\nT 0\nD L\nG 1\n1 m\nf u\ns e\np O\nJ h\nc I\nv j\nC F\ny f\nD g\nL e\n5 4\ny i\nl r\nr l\nP c\ne n\nO G\nR U\nJ F\nf p\nX e\nI g\nP G\nC C\no D\nB N\nu +\nW K\nV b\nU m\nM O\ne J\nn Z\nM K\ny V\nH v\nl r\n+ c\nX s\nh i\nH w\nJ k\ng i\nr V\ns a\n0 I\n3 Y\nR K\n5 E\nx A\nW n\nJ i\nB R\nF l\nI z\nq A\nj q\nY B\n9 U\nG 5\ny e\nS F\n1 D\nz U\nS s\n/ s\nh 1\nJ X\ng O\nZ E\n/ T\n2 R\nU F\n+ p\ns e\n/ p\nz u\nx g\nN e\nt l\n4 n\n9 e\nJ 8\nb +\nu Z\nv w\nI I\no R\nA j\nZ d\n1 I\n+ F\ni a\nG Z\n5 W\nH 2\nu A\nS G\nY q\nw J\nZ Z\nL r\nW 0\n0 2\np J\nI y\n1 K\nm V\nd A\nj 2\n7 M\nv z\np H\nl c\nt U\n+ r\n9 s\n1 J\np X\na R\nx 1\nE k\n+ +\nS A\nH B\nG b\nn J\nE a\nu S\nZ 1\n0 i\nC M\nP J\nJ n\n8 k\nr e\nj C\nf j\nx X\ng 3\nP q\na t\nB S\nO f\n2 S\nV /\nY H\nz +\nA L\ns p\nm E\nw =\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nd I\nC /\nq l\n0 j\ny B\no k\nH B\nq N\no +\nZ s\nM B\ny 0\nl k\ns =\n\" >\nA A\nA C\nA 3\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\no t\n7 0\nE i\ny C\np 5\nK I\nq M\ne i\nH j\nx W\ns B\n/ Q\nx L\nL Z\nb t\nq l\nm 0\n3 Y\nn Y\ng l\nB L\nz 4\nV 7\nx 4\nU M\nS r\nf 8\nK b\n/ 8\nZ N\nm 4\nO 2\nP h\nh 4\nv D\nf D\nz D\nw /\n5 k\ny B\nb X\n8 b\np Y\nX F\np e\nW V\n8 m\np l\nb X\n1 j\nc 8\nv c\n3 m\nm p\nK J\nG E\nN k\nn E\nI 9\nn x\ns a\nK c\nC d\no E\nB p\nx 2\nY k\nl x\n6 H\nP a\n9 k\ne X\nu d\n+ +\np 1\nK x\nS N\nz C\nO K\nZ e\ni A\ne C\nB Y\nx g\n0 F\nL P\n3 H\nN D\nD E\nO C\ne X\nq V\n3 a\nU u\n0 A\nd I\nQ W\nI m\ns q\nx n\nV u\n2 a\nP Y\nE 1\nT 5\ny C\nV F\nG B\nR s\n/ 8\nc v\ns R\nS U\nI q\ng H\nC s\nV N\ne x\nY /\nB S\nL I\nE R\nT r\nO K\nm y\ng a\nY z\nL C\nA 9\nr V\nV O\nC Q\nK i\n+ d\n/ J\nB Z\nh 1\nr p\nW 0\nE k\nd Q\nm w\nJ u\nr v\ni R\nS H\nS o\n1 D\nX 3\nf m\nF 6\nt Z\nL x\nf /\n8 7\no J\nB O\nd e\ny k\nS c\nA B\nV k\nu i\nh I\nu A\nW R\nl Q\nd i\n9 Z\nm k\nB P\nh Y\nE 0\nw k\n0 7\nd a\nZ I\ng l\nJ q\nB j\nq +\ng Q\nn N\nm X\n5 0\nn r\nu O\na c\n1 p\ny b\nk 2\nr 9\no o\ni j\nj P\nb R\nA T\np C\nD j\np D\nd X\nS N\nG q\ni J\nC H\np E\nz +\ng V\nv R\nl P\nx o\nv x\nb n\nx M\nW 0\nt G\nM b\nO L\n/ s\nD 4\n/ A\nF 5\nP p\ni 0\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nR p\nT Y\nT V\nz C\nw d\nl l\nt t\nD H\nv i\nZ y\nh s\n/ z\ne c\nE =\n\" >\nA A\nA C\nA n\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\nq p\n7 E\nS 7\nA I\nn k\no i\no h\n6 L\ne v\nB Y\nw X\n5 A\nE 8\nt m\nO 2\n2 X\nb j\n7 Y\nn Y\ng l\nB C\n/ +\nF S\n8 e\nF P\nH q\nr /\nD m\nv 3\nH T\n5 q\nC t\nD w\nY e\n7 8\n0 w\nM 8\n+ L\nB F\nd o\nW d\n9 G\nY W\nF x\na X\nm l\nu F\np a\nW 9\n/ Y\n3 C\np v\n7 z\nR V\nG E\ns G\nD R\na K\nU L\nY 9\nq k\nD w\nA B\nr I\nU U\nA 7\nk k\nB 9\nT 0\nD L\nG 1\n1 m\nf u\ns e\np O\nJ h\nc I\nv j\nC F\ny f\nD g\nL e\n5 4\ny i\nl r\nr l\nP c\ne n\nO G\nR U\nJ F\nf p\nX e\nI g\nP G\nC C\no D\nB N\nu +\nW K\nV b\nU m\nM O\ne J\nn Z\nM K\ny V\nH v\nl r\n+ c\nX s\nh i\nH w\nJ k\ng i\nr V\ns a\n0 I\n3 Y\nR K\n5 E\nx A\nW n\nJ i\nB R\nF l\nI z\nq A\nj q\nY B\n9 U\nG 5\ny e\nS F\n1 D\nz U\nS s\n/ s\nh 1\nJ X\ng O\nZ E\n/ T\n2 R\nU F\n+ p\ns e\n/ p\nz u\nx g\nN e\nt l\n4 n\n9 e\nJ 8\nb +\nu Z\nv w\nI I\no R\nA j\nZ d\n1 I\n+ F\ni a\nG Z\n5 W\nH 2\nu A\nS G\nY q\nw J\nZ Z\nL r\nW 0\n0 2\np J\nI y\n1 K\nm V\nd A\nj 2\n7 M\nv z\np H\nl c\nt U\n+ r\n9 s\n1 J\np X\na R\nx 1\nE k\n+ +\nS A\nH B\nG b\nn J\nE a\nu S\nZ 1\n0 i\nC M\nP J\nJ n\n8 k\nr e\nj C\nf j\nx X\ng 3\nP q\na t\nB S\nO f\n2 S\nV /\nY H\nz +\nA L\ns p\nm E\nw =\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nd I\nC /\nq l\n0 j\ny B\no k\nH B\nq N\no +\nZ s\nM B\ny 0\nl k\ns =\n\" >\nA A\nA C\nA 3\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\no t\n7 0\nE i\ny C\np 5\nK I\nq M\ne i\nH j\nx W\ns B\n/ Q\nx L\nL Z\nb t\nq l\nm 0\n3 Y\nn Y\ng l\nB L\nz 4\nV 7\nx 4\nU M\nS r\nf 8\nK b\n/ 8\nZ N\nm 4\nO 2\nP h\nh 4\nv D\nf D\nz D\nw /\n5 k\ny B\nb X\n8 b\np Y\nX F\np e\nW V\n8 m\np l\nb X\n1 j\nc 8\nv c\n3 m\nm p\nK J\nG E\nN k\nn E\nI 9\nn x\ns a\nK c\nC d\no E\nB p\nx 2\nY k\nl x\n6 H\nP a\n9 k\ne X\nu d\n+ +\np 1\nK x\nS N\nz C\nO K\nZ e\ni A\ne C\nB Y\nx g\n0 F\nL P\n3 H\nN D\nD E\nO C\ne X\nq V\n3 a\nU u\n0 A\nd I\nQ W\nI m\ns q\nx n\nV u\n2 a\nP Y\nE 1\nT 5\ny C\nV F\nG B\nR s\n/ 8\nc v\ns R\nS U\nI q\ng H\nC s\nV N\ne x\nY /\nB S\nL I\nE R\nT r\nO K\nm y\ng a\nY z\nL C\nA 9\nr V\nV O\nC Q\nK i\n+ d\n/ J\nB Z\nh 1\nr p\nW 0\nE k\nd Q\nm w\nJ u\nr v\ni R\nS H\nS o\n1 D\nX 3\nf m\nF 6\nt Z\nL x\nf /\n8 7\no J\nB O\nd e\ny k\nS c\nA B\nV k\nu i\nh I\nu A\nW R\nl Q\nd i\n9 Z\nm k\nB P\nh Y\nE 0\nw k\n0 7\nd a\nZ I\ng l\nJ q\nB j\nq +\ng Q\nn N\nm X\n5 0\nn r\nu O\na c\n1 p\ny b\nk 2\nr 9\no o\ni j\nj P\nb R\nA T\np C\nD j\np D\nd X\nS N\nG q\ni J\nC H\np E\nz +\ng V\nv R\nl P\nx o\nv x\nb n\nx M\nW 0\nt G\nM b\nO L\n/ s\nD 4\n/ A\nF 5\nP p\ni 0\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nR p\nT Y\nT V\nz C\nw d\nl l\nt t\nD H\nv i\nZ y\nh s\n/ z\ne c\nE =\n\" >\nA A\nA C\nA n\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\nq p\n7 E\nS 7\nA I\nn k\no i\no h\n6 L\ne v\nB Y\nw X\n5 A\nE 8\nt m\nO 2\n2 X\nb j\n7 Y\nn Y\ng l\nB C\n/ +\nF S\n8 e\nF P\nH q\nr /\nD m\nv 3\nH T\n5 q\nC t\nD w\nY e\n7 8\n0 w\nM 8\n+ L\nB F\nd o\nW d\n9 G\nY W\nF x\na X\nm l\nu F\np a\nW 9\n/ Y\n3 C\np v\n7 z\nR V\nG E\ns G\nD R\na K\nU L\nY 9\nq k\nD w\nA B\nr I\nU U\nA 7\nk k\nB 9\nT 0\nD L\nG 1\n1 m\nf u\ns e\np O\nJ h\nc I\nv j\nC F\ny f\nD g\nL e\n5 4\ny i\nl r\nr l\nP c\ne n\nO G\nR U\nJ F\nf p\nX e\nI g\nP G\nC C\no D\nB N\nu +\nW K\nV b\nU m\nM O\ne J\nn Z\nM K\ny V\nH v\nl r\n+ c\nX s\nh i\nH w\nJ k\ng i\nr V\ns a\n0 I\n3 Y\nR K\n5 E\nx A\nW n\nJ i\nB R\nF l\nI z\nq A\nj q\nY B\n9 U\nG 5\ny e\nS F\n1 D\nz U\nS s\n/ s\nh 1\nJ X\ng O\nZ E\n/ T\n2 R\nU F\n+ p\ns e\n/ p\nz u\nx g\nN e\nt l\n4 n\n9 e\nJ 8\nb +\nu Z\nv w\nI I\no R\nA j\nZ d\n1 I\n+ F\ni a\nG Z\n5 W\nH 2\nu A\nS G\nY q\nw J\nZ Z\nL r\nW 0\n0 2\np J\nI y\n1 K\nm V\nd A\nj 2\n7 M\nv z\np H\nl c\nt U\n+ r\n9 s\n1 J\np X\na R\nx 1\nE k\n+ +\nS A\nH B\nG b\nn J\nE a\nu S\nZ 1\n0 i\nC M\nP J\nJ n\n8 k\nr e\nj C\nf j\nx X\ng 3\nP q\na t\nB S\nO f\n2 S\nV /\nY H\nz +\nA L\ns p\nm E\nw =\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nd I\nC /\nq l\n0 j\ny B\no k\nH B\nq N\no +\nZ s\nM B\ny 0\nl k\ns =\n\" >\nA A\nA C\nA 3\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\no t\n7 0\nE i\ny C\np 5\nK I\nq M\ne i\nH j\nx W\ns B\n/ Q\nx L\nL Z\nb t\nq l\nm 0\n3 Y\nn Y\ng l\nB L\nz 4\nV 7\nx 4\nU M\nS r\nf 8\nK b\n/ 8\nZ N\nm 4\nO 2\nP h\nh 4\nv D\nf D\nz D\nw /\n5 k\ny B\nb X\n8 b\np Y\nX F\np e\nW V\n8 m\np l\nb X\n1 j\nc 8\nv c\n3 m\nm p\nK J\nG E\nN k\nn E\nI 9\nn x\ns a\nK c\nC d\no E\nB p\nx 2\nY k\nl x\n6 H\nP a\n9 k\ne X\nu d\n+ +\np 1\nK x\nS N\nz C\nO K\nZ e\ni A\ne C\nB Y\nx g\n0 F\nL P\n3 H\nN D\nD E\nO C\ne X\nq V\n3 a\nU u\n0 A\nd I\nQ W\nI m\ns q\nx n\nV u\n2 a\nP Y\nE 1\nT 5\ny C\nV F\nG B\nR s\n/ 8\nc v\ns R\nS U\nI q\ng H\nC s\nV N\ne x\nY /\nB S\nL I\nE R\nT r\nO K\nm y\ng a\nY z\nL C\nA 9\nr V\nV O\nC Q\nK i\n+ d\n/ J\nB Z\nh 1\nr p\nW 0\nE k\nd Q\nm w\nJ u\nr v\ni R\nS H\nS o\n1 D\nX 3\nf m\nF 6\nt Z\nL x\nf /\n8 7\no J\nB O\nd e\ny k\nS c\nA B\nV k\nu i\nh I\nu A\nW R\nl Q\nd i\n9 Z\nm k\nB P\nh Y\nE 0\nw k\n0 7\nd a\nZ I\ng l\nJ q\nB j\nq +\ng Q\nn N\nm X\n5 0\nn r\nu O\na c\n1 p\ny b\nk 2\nr 9\no o\ni j\nj P\nb R\nA T\np C\nD j\np D\nd X\nS N\nG q\ni J\nC H\np E\nz +\ng V\nv R\nl P\nx o\nv x\nb n\nx M\nW 0\nt G\nM b\nO L\n/ s\nD 4\n/ A\nF 5\nP p\ni 0\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nR p\nT Y\nT V\nz C\nw d\nl l\nt t\nD H\nv i\nZ y\nh s\n/ z\ne c\nE =\n\" >\nA A\nA C\nA n\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\nq p\n7 E\nS 7\nA I\nn k\no i\no h\n6 L\ne v\nB Y\nw X\n5 A\nE 8\nt m\nO 2\n2 X\nb j\n7 Y\nn Y\ng l\nB C\n/ +\nF S\n8 e\nF P\nH q\nr /\nD m\nv 3\nH T\n5 q\nC t\nD w\nY e\n7 8\n0 w\nM 8\n+ L\nB F\nd o\nW d\n9 G\nY W\nF x\na X\nm l\nu F\np a\nW 9\n/ Y\n3 C\np v\n7 z\nR V\nG E\ns G\nD R\na K\nU L\nY 9\nq k\nD w\nA B\nr I\nU U\nA 7\nk k\nB 9\nT 0\nD L\nG 1\n1 m\nf u\ns e\np O\nJ h\nc I\nv j\nC F\ny f\nD g\nL e\n5 4\ny i\nl r\nr l\nP c\ne n\nO G\nR U\nJ F\nf p\nX e\nI g\nP G\nC C\no D\nB N\nu +\nW K\nV b\nU m\nM O\ne J\nn Z\nM K\ny V\nH v\nl r\n+ c\nX s\nh i\nH w\nJ k\ng i\nr V\ns a\n0 I\n3 Y\nR K\n5 E\nx A\nW n\nJ i\nB R\nF l\nI z\nq A\nj q\nY B\n9 U\nG 5\ny e\nS F\n1 D\nz U\nS s\n/ s\nh 1\nJ X\ng O\nZ E\n/ T\n2 R\nU F\n+ p\ns e\n/ p\nz u\nx g\nN e\nt l\n4 n\n9 e\nJ 8\nb +\nu Z\nv w\nI I\no R\nA j\nZ d\n1 I\n+ F\ni a\nG Z\n5 W\nH 2\nu A\nS G\nY q\nw J\nZ Z\nL r\nW 0\n0 2\np J\nI y\n1 K\nm V\nd A\nj 2\n7 M\nv z\np H\nl c\nt U\n+ r\n9 s\n1 J\np X\na R\nx 1\nE k\n+ +\nS A\nH B\nG b\nn J\nE a\nu S\nZ 1\n0 i\nC M\nP J\nJ n\n8 k\nr e\nj C\nf j\nx X\ng 3\nP q\na t\nB S\nO f\n2 S\nV /\nY H\nz +\nA L\ns p\nm E\nw =\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nd I\nC /\nq l\n0 j\ny B\no k\nH B\nq N\no +\nZ s\nM B\ny 0\nl k\ns =\n\" >\nA A\nA C\nA 3\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\no t\n7 0\nE i\ny C\np 5\nK I\nq M\ne i\nH j\nx W\ns B\n/ Q\nx L\nL Z\nb t\nq l\nm 0\n3 Y\nn Y\ng l\nB L\nz 4\nV 7\nx 4\nU M\nS r\nf 8\nK b\n/ 8\nZ N\nm 4\nO 2\nP h\nh 4\nv D\nf D\nz D\nw /\n5 k\ny B\nb X\n8 b\np Y\nX F\np e\nW V\n8 m\np l\nb X\n1 j\nc 8\nv c\n3 m\nm p\nK J\nG E\nN k\nn E\nI 9\nn x\ns a\nK c\nC d\no E\nB p\nx 2\nY k\nl x\n6 H\nP a\n9 k\ne X\nu d\n+ +\np 1\nK x\nS N\nz C\nO K\nZ e\ni A\ne C\nB Y\nx g\n0 F\nL P\n3 H\nN D\nD E\nO C\ne X\nq V\n3 a\nU u\n0 A\nd I\nQ W\nI m\ns q\nx n\nV u\n2 a\nP Y\nE 1\nT 5\ny C\nV F\nG B\nR s\n/ 8\nc v\ns R\nS U\nI q\ng H\nC s\nV N\ne x\nY /\nB S\nL I\nE R\nT r\nO K\nm y\ng a\nY z\nL C\nA 9\nr V\nV O\nC Q\nK i\n+ d\n/ J\nB Z\nh 1\nr p\nW 0\nE k\nd Q\nm w\nJ u\nr v\ni R\nS H\nS o\n1 D\nX 3\nf m\nF 6\nt Z\nL x\nf /\n8 7\no J\nB O\nd e\ny k\nS c\nA B\nV k\nu i\nh I\nu A\nW R\nl Q\nd i\n9 Z\nm k\nB P\nh Y\nE 0\nw k\n0 7\nd a\nZ I\ng l\nJ q\nB j\nq +\ng Q\nn N\nm X\n5 0\nn r\nu O\na c\n1 p\ny b\nk 2\nr 9\no o\ni j\nj P\nb R\nA T\np C\nD j\np D\nd X\nS N\nG q\ni J\nC H\np E\nz +\ng V\nv R\nl P\nx o\nv x\nb n\nx M\nW 0\nt G\nM b\nO L\n/ s\nD 4\n/ A\nF 5\nP p\ni 0\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nR p\nT Y\nT V\nz C\nw d\nl l\nt t\nD H\nv i\nZ y\nh s\n/ z\ne c\nE =\n\" >\nA A\nA C\nA n\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\nq p\n7 E\nS 7\nA I\nn k\no i\no h\n6 L\ne v\nB Y\nw X\n5 A\nE 8\nt m\nO 2\n2 X\nb j\n7 Y\nn Y\ng l\nB C\n/ +\nF S\n8 e\nF P\nH q\nr /\nD m\nv 3\nH T\n5 q\nC t\nD w\nY e\n7 8\n0 w\nM 8\n+ L\nB F\nd o\nW d\n9 G\nY W\nF x\na X\nm l\nu F\np a\nW 9\n/ Y\n3 C\np v\n7 z\nR V\nG E\ns G\nD R\na K\nU L\nY 9\nq k\nD w\nA B\nr I\nU U\nA 7\nk k\nB 9\nT 0\nD L\nG 1\n1 m\nf u\ns e\np O\nJ h\nc I\nv j\nC F\ny f\nD g\nL e\n5 4\ny i\nl r\nr l\nP c\ne n\nO G\nR U\nJ F\nf p\nX e\nI g\nP G\nC C\no D\nB N\nu +\nW K\nV b\nU m\nM O\ne J\nn Z\nM K\ny V\nH v\nl r\n+ c\nX s\nh i\nH w\nJ k\ng i\nr V\ns a\n0 I\n3 Y\nR K\n5 E\nx A\nW n\nJ i\nB R\nF l\nI z\nq A\nj q\nY B\n9 U\nG 5\ny e\nS F\n1 D\nz U\nS s\n/ s\nh 1\nJ X\ng O\nZ E\n/ T\n2 R\nU F\n+ p\ns e\n/ p\nz u\nx g\nN e\nt l\n4 n\n9 e\nJ 8\nb +\nu Z\nv w\nI I\no R\nA j\nZ d\n1 I\n+ F\ni a\nG Z\n5 W\nH 2\nu A\nS G\nY q\nw J\nZ Z\nL r\nW 0\n0 2\np J\nI y\n1 K\nm V\nd A\nj 2\n7 M\nv z\np H\nl c\nt U\n+ r\n9 s\n1 J\np X\na R\nx 1\nE k\n+ +\nS A\nH B\nG b\nn J\nE a\nu S\nZ 1\n0 i\nC M\nP J\nJ n\n8 k\nr e\nj C\nf j\nx X\ng 3\nP q\na t\nB S\nO f\n2 S\nV /\nY H\nz +\nA L\ns p\nm E\nw =\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nd I\nC /\nq l\n0 j\ny B\no k\nH B\nq N\no +\nZ s\nM B\ny 0\nl k\ns =\n\" >\nA A\nA C\nA 3\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\no t\n7 0\nE i\ny C\np 5\nK I\nq M\ne i\nH j\nx W\ns B\n/ Q\nx L\nL Z\nb t\nq l\nm 0\n3 Y\nn Y\ng l\nB L\nz 4\nV 7\nx 4\nU M\nS r\nf 8\nK b\n/ 8\nZ N\nm 4\nO 2\nP h\nh 4\nv D\nf D\nz D\nw /\n5 k\ny B\nb X\n8 b\np Y\nX F\np e\nW V\n8 m\np l\nb X\n1 j\nc 8\nv c\n3 m\nm p\nK J\nG E\nN k\nn E\nI 9\nn x\ns a\nK c\nC d\no E\nB p\nx 2\nY k\nl x\n6 H\nP a\n9 k\ne X\nu d\n+ +\np 1\nK x\nS N\nz C\nO K\nZ e\ni A\ne C\nB Y\nx g\n0 F\nL P\n3 H\nN D\nD E\nO C\ne X\nq V\n3 a\nU u\n0 A\nd I\nQ W\nI m\ns q\nx n\nV u\n2 a\nP Y\nE 1\nT 5\ny C\nV F\nG B\nR s\n/ 8\nc v\ns R\nS U\nI q\ng H\nC s\nV N\ne x\nY /\nB S\nL I\nE R\nT r\nO K\nm y\ng a\nY z\nL C\nA 9\nr V\nV O\nC Q\nK i\n+ d\n/ J\nB Z\nh 1\nr p\nW 0\nE k\nd Q\nm w\nJ u\nr v\ni R\nS H\nS o\n1 D\nX 3\nf m\nF 6\nt Z\nL x\nf /\n8 7\no J\nB O\nd e\ny k\nS c\nA B\nV k\nu i\nh I\nu A\nW R\nl Q\nd i\n9 Z\nm k\nB P\nh Y\nE 0\nw k\n0 7\nd a\nZ I\ng l\nJ q\nB j\nq +\ng Q\nn N\nm X\n5 0\nn r\nu O\na c\n1 p\ny b\nk 2\nr 9\no o\ni j\nj P\nb R\nA T\np C\nD j\np D\nd X\nS N\nG q\ni J\nC H\np E\nz +\ng V\nv R\nl P\nx o\nv x\nb n\nx M\nW 0\nt G\nM b\nO L\n/ s\nD 4\n/ A\nF 5\nP p\ni 0\n< /\nl a\nt e\nx i\nt >\n< l\na t\ne x\ni t\n s\nh a\n1 _\nb a\ns e\n6 4\n= \"\nR p\nT Y\nT V\nz C\nw d\nl l\nt t\nD H\nv i\nZ y\nh s\n/ z\ne c\nE =\n\" >\nA A\nA C\nA n\ni c\nb V\nB N\nS 8\nN A\nE N\n3 U\nr 1\nq /\nq p\n7 E\nS 7\nA I\nn k\no i\no h\n6 L\ne v\nB Y\nw X\n5 A\nE 8\nt m\nO 2\n2 X\nb j\n7 Y\nn Y\ng l\nB C\n/ +\nF S\n8 e\nF P\nH q\nr /\nD m\nv 3\nH T\n5 q\nC t\nD w\nY e\n7 8\n0 w\nM 8\n+ L\nB F\nd o\nW d\n9 G\nY W\nF x\na X\nm l\nu F\np a\nW 9\n/ Y\n3 C\np v\n7 z\nR V\nG E\ns G\nD R\na K\nU L\nY 9\nq k\nD w\nA B\nr I\nU U\nA 7\nk k\nB 9\nT 0\nD L\nG 1\n1 m\nf u\ns e\np O\nJ h\nc I\nv j\nC F\ny f\nD g\nL e\n5 4\ny i\nl r\nr l\nP c\ne n\nO G\nR U\nJ F\nf p\nX e\nI g\nP G\nC C\no D\nB N\nu +\nW K\nV b\nU m\nM O\ne J\nn Z\nM K\ny V\nH v\nl r\n+ c\nX s\nh i\nH w\nJ k\ng i\nr V\ns a\n0 I\n3 Y\nR K\n5 E\nx A\nW n\nJ i\nB R\nF l\nI z\nq A\nj q\nY B\n9 U\nG 5\ny e\nS F\n1 D\nz U\nS s\n/ s\nh 1\nJ X\ng O\nZ E\n/ T\n2 R\nU F\n+ p\ns e\n/ p\nz u\nx g\nN e\nt l\n4 n\n9 e\nJ 8\nb +\nu Z\nv w\nI I\no R\nA j\nZ d\n1 I\n+ F\ni a\nG Z\n5 W\nH 2\nu A\nS G\nY q\nw J\nZ Z\nL r\nW 0\n0 2\np J\nI y\n1 K\nm V\nd A\nj 2\n7 M\nv z\np H\nl c\nt U\n+ r\n9 s\n1 J\np X\na R\nx 1\nE k\n+ +\nS A\nH B\nG b\nn J\nE a\nu S\nZ 1\n0 i\nC M\nP J\nJ n\n8 k\nr e\nj C\nf j\nx X\ng 3\nP q\na t\nB S\nO f\n2 S\nV /\nY H\nz +\nA L\ns p\nm E\nw =\n< /\nl a\nt e\nx i\nt >\nUnder review as a conference paper at ICLR 2023\n\nls-identified training split\n\nls-identified testing split\n\nAll examples\n\nARE active\n\nATAD5 active\n\nHSE active\n\nMMP active\n\np53 active\n\nAhR active\n\nAromatas active\n\nER active\n\nER.LBD active\n\nPPAR.gam active\n\n}\n\n50\n\n25\n\nFigure 5: ls-identified splits correlate with certain spurious properties (ATAD5, AhR) even though they are not provided to algorithm. Here we present the train-test assignment of compounds with AR=active given by ls. In the leftmost bar, we look at all examples: 58% of is in the testing split. For {\neach bar on the right, we look at the subset where an unknown property is active. For exis allocated to the training split and 83% of ample, 17% of\n\nis in the training split and 42% of\n\nAR=active, ATAD5=active\n\nAR=active\n\nAR=active\n\n}\n\n{\n\n}\n\n{ AR=active, ATAD5=active\n\nis in the testing split.\n\n}\n\n{\n\n50\n\n25\n\n0\n\n0\n\n30\n\n15\n\n0\n\n0\n\nBeer Look\n\nBeer Aroma\n\nGeneralization gap\n\nGeneralization gap\n\n10\n\n20\n\n30\n\nWaterbirds\n\n0\n\n0\n\n50\n\n25\n\nCelebA\n\n10\n\n20\n\n30\n\n40\n\nGeneralization gap\n\n10\n\n20\n\n30\n\n0\n\n0\n\nGeneralization gap\n\n10\n\n20\n\n30\n\n40\n\nTox21 AR\n\n70\n\n35\n\nGeneralization gap\n\n10\n\n20\n\n30\n\nMNLI\n\nGeneralization gap\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n0\n\n0\n\n80\n\n40\n\n0\n\n0\n\nFigure 6: Learning curve of ls. X-axis: number of outer-loop iterations. Y-axis: generalization gap from\n\ntest. Error bar represents the standard deviation across 5 random seeds.\n\ntrain to\n\nD\n\nD\n\nWaterbirds Sagawa et al. (2019) created this dataset by combining bird images from the CaltechUCSD Birds-200-2011 (CUB) dataset (Welinder et al., 2010) with backgrounds from the Places dataset (Zhou et al., 2014). The task is to predict waterbirds vs. landbirds. The challenge is that waterbirds, by construction, appear more frequently with a water background. As a result, predictors may utilize this spurious correlation to make their predictions. We combine the official training data and validation data (5994 examples in total) and apply ls to identify spurious splits.\n\nCelebA CelebA is an image classification dataset where each input image (face) is paired with multiple human-annotated attributes Liu et al. (2015). Following previous work (Sagawa et al., 2019), we treat the hair color attribute (y ) as our prediction target. The ). We apply ls to identify label is spuriously correlated with the gender attribute ( spurious splits over the official training data (162,770 examples).\n\nblond, not blond\n\nmale, female\n\n2{\n\n}\n\n}\n\n{\n\nMNLI MNLI is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information (Williams et al., 2018). The task is to classify the relationship between a pair of sentences: entailment, neutral or contradiction. Previous work has found that contradiction examples often include negation words (McCoy et al., 2019). We apply ls to identify spurious splits over the training data (206,175 examples) created by Sagawa et al. (2019).\n\n4.2\n\nIDENTIFYING NON-GENERALIZABLE SPLITS\n\nFigure 3 presents the splits identified by our algorithm ls. Compared to random splitting, ls achieves astonishingly higher generalization gaps across all 6 tasks. Moreover, we observe that the test share similar label learned splits are not degenerative: the training split distributions. This confirms the effectiveness of our regularity objectives.\n\ntrain and testing split\n\nD\n\nD\n\nWhy are the learned splits so challenging for predictors to generalize across? While ls only has access to the set of input-label pairs, Figure 4 and Figure 5 show that the learned splits are informative of human-identified biases. For example, in the generated training split of MNLI, inputs with negation words are mostly labeled as contradiction. This encourages predictors to leverage the\n\n7\n\n60%25%36%38%87%55%46%33%83%55%42%40%75%64%63%13%45%54%67%17%45%58%Under review as a conference paper at ICLR 2023\n\nTable 1: Average and worst-group test accuracy for de-biasing. When using bias annotations on the validation data for model selection, previous work (CVaR DRO (Levy et al., 2020), LfF (Nam et al., 2020), EIIL (Creager et al., 2021), JTT (Liu et al., 2021a)) significantly outperform ERM (that is also tuned using bias annotations on the validation data). However, they underperform the group DRO baseline (Sagawa et al., 2019) that was previously overlooked. When bias annotations are not available for validation, the performances of these methods quickly drop to that of ERM. In contrast, applying group DRO with splits identified by ls substantially improves the worst-group performance. denote numbers reported by Liu et al. (2021a) and Creager et al. (2021) respectively.\n\nand\n\n‡\n\n†\n\nBias annotated\n\nWaterbirds\n\nCelebA\n\nMNLI\n\nin train? in val?\n\nAvg.\n\nWorst\n\nAvg.\n\nWorst\n\nAvg.\n\nWorst\n\nMethod\n\nGroup DRO\n\nERM\n\nCVaR DRO\n\nLfF\n\nEIIL\n\nJTT\n\n3\n\n7\n\n7\n\n7\n\n7\n\n7\n\nGroup DRO (with supervised bias predictor)\n\n7\n\nERM\n\nCVaR DRO\n\nLfF\n\nEIIL\n\nJTT\n\n7\n\n7\n\n7\n\n7\n\n7\n\nGroup DRO (with splits identified by ls)\n\n7\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n7\n\n7\n\n7\n\n7\n\n7\n\n7\n\n93.5%†\n\n91.4%†\n\n92.9%†\n\n88.9%†\n\n81.4%†\n\n77.7%†\n\n97.3%†\n\n72.6%†\n\n95.6%†\n\n47.2%†\n\n82.4%†\n\n67.9%†\n\n96.0%†\n\n75.9%†\n\n82.5%†\n\n64.4%†\n\n82.0%†\n\n68.0%†\n\n91.2%†\n\n78.0%†\n\n85.1%†\n\n77.2%†\n\n80.8%†\n\n70.2%†\n\n96.9%‡\n\n78.7%‡\n\n89.5%\n\n77.8%\n\n79.4%\n\n70.0%\n\n93.3%†\n\n86.7%†\n\n88.0%†\n\n81.1%†\n\n78.6%†\n\n72.6%†\n\n91.4%\n\n88.2% 91.4%\n\n88.9% 79.9%\n\n77.7%\n\n90.7%\n\n64.8%\n\n95.8%\n\n41.1%\n\n81.9%\n\n60.4%\n\n—\n\n—\n\n62.0%†\n\n44.1%†\n\n—\n\n—\n\n36.1%†\n\n81.8%\n\n61.8%\n\n24.4%†\n\n81.1%\n\n62.2%\n\n90.8%\n\n64.5%\n\n95.7%\n\n41.7%\n\n80.3%\n\n64.7%\n\n—\n\n62.5%†\n\n—\n\n40.6%†\n\n81.3%\n\n64.4%\n\n91.2%\n\n86.1% 87.2%\n\n83.3% 78.7%\n\n72.1%\n\npresence of negation words to make their predictions. These biased predictors cannot generalize to the testing split, where inputs with negation words are mostly labeled as entailment or neutral.\n\nConvergence and time-efficiency ls requires learning a new Predictor for each outer-loop iteration. While this makes ls more time-consuming than training a regular ERM model, this procedure guarantees that the Predictor faithfully measures the generalization gap based on the current Splitter. Figure 6 shows the learning curve of ls. We observe that the generalization gap steadily increases as we refine the Splitter and the learning procedure usually converges within 50 outer-loop iterations.\n\n4.3 AUTOMATIC DE-BIASING\n\nOnce ls has identified the spurious splits, we can apply robust learning algorithms to learn models that generalize across the splits. Here we consider group distributionally robust optimization (group DRO) and study three well-established benchmarks: Waterbirds, CelebA and MNLI.\n\nGroup DRO Group DRO has shown strong performance when biases are annotated (Sagawa et al., 2019). For example in CelebA, gender (male, female) constitutes a bias for predicting blond hair. Group DRO uses the gender annotations to partition the training data ,\ninto four groups: }\n. By minimizing the worst-group loss during training, it regularizes {\nthe impact of the unwanted gender bias. At test time, we report the average accuracy and worstgroup accuracy over a held-out test set.\n\nno blond hair, female\n\nno blond hair, male\n\nblond hair, female\n\nblond hair, male\n\n, }\n\n}\n\n{\n\n{\n\n{\n\n}\n\n,\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nGroup DRO with supervised bias predictor Recent work consider a more challenging setting where bias annotations are not provided at train time. CVaR DRO (Levy et al., 2020) up-weights examples that have the highest training losses. LfF (Nam et al., 2020) and JTT (Liu et al., 2021a) train a separate de-biased predictor by learning from the mistakes of a biased predictor. EIIL (Creager et al., 2021) infers the environment information from an ERM predictor and uses group DRO to promote robustness across the latent environments. However, these methods still access bias annotations on the validation data for model selection. With thousands of validation examples (1199 for Waterbirds, 19867 for CelebA, 82462 for MNLI), a simple baseline was overlooked by the community: learning a bias predictor over the validation data (where bias annotations are available) and using the predicted bias attributes on the training data to define groups for group DRO.\n\nGroup DRO with splits identified by ls We consider the general setting where biases are not known during both training and validation. To obtain a robust model, we take the splits identified by ls (Section 4.2) and use them to define groups for group DRO. For example, we have four ,\ngroups in CelebA: }\n. For model selection, we apply the learned Splitter to split the validation\n\nno blond hair, z = 0\n\nno blond hair, z = 1\n\nblond hair, z = 0\n\nblond hair, z = 1\n\n, }\n\n{\n\n}\n\n{\n\n{\n\n,\n\n{ data and measure the worst-group accuracy.\n\n}\n\nResults Table 1 presents our results on de-biasing. We first see that when the bias annotations are available in the validation data, the missing baseline Group DRO (with supervised bias predictor) outperforms all previous de-biasing methods (4.8% on average). This result is not surprising given the fact that the bias attribute predictor, trained on the validation data, is able to achieve an accuracy of 94.8% in Waterbirds (predicting the spurious background), 97.7% in CelebA (predicting the spurious gender attribute) and 99.9% in MNLI (predicting the presence of negation words).\n\nWhen bias annotations are not provided for validation, previous de-biasing methods (tuned based on the average validation performance) fail to improve over the ERM baseline, confirming the findings of Liu et al. (2021a). On the other hand, applying group DRO with splits identified by ls consistently achieves the best worst-group accuracy, outperforming previous methods by 23.4% on average. While we no longer have access to the bias annotations for model selection, the worst-group performance defined by ls can be used as a surrogate (see Appendix C for details).\n\n5 DISCUSSION\n\nSection 4 shows that ls identifies non-generalizable splits that correlate with human-identified biases. However, we must keep in mind that bias is a human-defined notion. Given the set of inputlabel pairs, ls provides a tool for understanding potential biases, not a fairness guarantee. If the support of the given dataset doesn’t cover the minority groups, ls will fail. For example, consider a dataset with only samoyeds in grass and polar bears in snow (no samoyeds in snow or polar bears in grass). ls will not be able to detect the background bias in this case.\n\nWe also note that poor generalization can result from label noise. Since the Splitter makes its decision based on the input-label pair, ls can achieve high generalization gap by allocating all clean examples to the training split and all mislabeled examples to the testing split. Here we can think of ls as a label noise detector (see Appendix D for more analysis). Blindly maximizing the worst-split performance in this situation will enforce the model to memorize the noise.\n\nAnother limitation is running time. Compared to empirical risk minimization, ls needs to perform second-order reasoning, and this introduces extra time cost (see Appendix C for more discussion). Finally, in real-world applications, biases can also come from many independent sources (e.g., gender and race). Identifying multiple diverse splits will be an interesting future work.\n\n6 CONCLUSION\n\nWe present Learning to Split (ls), an algorithm that learns to split the data so that predictors trained on the training split cannot generalize to the testing split. Our algorithm only requires access to the set of input-label pairs and is applicable to general datasets. Experiments across multiple modalities confirm that ls identifies challenging splits that correlate with human-identified biases. Compared to previous state-of-the-art, learning with ls-identified splits significantly improves robustness.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nFaruk Ahmed, Yoshua Bengio, Harm van Seijen, and Aaron Courville. Systematic generalisation with group invariant predictions. In International Conference on Learning Representations, 2020.\n\nDavid M Allen. The relationship between variable selection and data agumentation and a method\n\nfor prediction. technometrics, 16(1):125–127, 1974.\n\nPeter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE transactions on medical imaging, 38(2):550–560, 2018.\n\nYujia Bao, Shiyu Chang, and Regina Barzilay. Predict then interpolate: A simple algorithm to learn\n\nstable classifiers. In International Conference on Machine Learning (ICML), 2021.\n\nYujia Bao, Shiyu Chang, and Regina Barzilay. Learning stable classifiers by transferring unstable\n\nfeatures. In International Conference on Machine Learning, pp. 1483–1507. PMLR, 2022.\n\nYonatan Belinkov, Adam Poliak, Stuart M Shieber, Benjamin Van Durme, and Alexander M Rush. Don’t take the premise for granted: Mitigating artifacts in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 877–891, 2019.\n\nJoy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pp. 77–91, 2018.\n\nDamon Centola, Joshua Becker, Devon Brackbill, and Andrea Baronchelli. Experimental evidence\n\nfor tipping points in social convention. Science, 360(6393):1116–1119, 2018.\n\nZhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. GradNorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 794–803. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/chen18a.html.\n\nChristopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don’t take the easy way out: Ensemble In Proceedings of the 2019 Conference on based methods for avoiding known dataset biases. Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4069–4082, 2019.\n\nElliot Creager, J ̈orn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant\n\nlearning. In International Conference on Machine Learning, pp. 2189–2200. PMLR, 2021.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical\n\nImage Database. In CVPR09, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018.\n\nHe He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting\n\nthe residual. EMNLP-IJCNLP 2019, pp. 132, 2019.\n\nWeihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give robust classifiers? In International Conference on Machine Learning, pp. 2029– 2037. PMLR, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRuili Huang, Menghang Xia, Dac-Trung Nguyen, Tongan Zhao, Srilatha Sakamuru, Jinghua Zhao, Sampada A. Shahane, Anna Rossoshek, and Anton Simeonov. Tox21challenge to build predictive models of nuclear receptor and stress response pathways as mediated by exposure to environmental chemicals and drugs. Frontiers in Environmental Science, 3, 2016. ISSN 2296-665X. doi: 10.3389/fenvs.2015.00085. URL https://www.frontiersin.org/article/10. 3389/fenvs.2015.00085.\n\nMasahiro Kaneko and Danushka Bollegala. Debiasing pre-trained contextualised embeddings. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 1256–1266, 2021.\n\nYoon Kim. Convolutional neural networks for sentence classification.\n\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1746– 1751, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/ D14-1181. URL https://www.aclweb.org/anthology/D14-1181.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637–5664. PMLR, 2021.\n\nMatt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. Advances\n\nin Neural Information Processing Systems, 30, 2017.\n\nPreethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed H Chi. Fairness without demographics through adversarially reweighted learning. arXiv preprint arXiv:2006.13114, 2020.\n\nYann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.\n\nlecun.com/exdb/mnist/.\n\nTao Lei, Regina Barzilay, and Tommi Jaakkola. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 107–117, 2016.\n\nDaniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally robust optimization. Advances in Neural Information Processing Systems, 33:8847–8860, 2020.\n\nZhiheng Li and Chenliang Xu. Discover the unknown biased attribute of an image classifier. In The\n\nIEEE International Conference on Computer Vision (ICCV), 2021.\n\nEvan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In International Conference on Machine Learning, pp. 6781–6792. PMLR, 2021a.\n\nJiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. Heterogeneous risk minimization.\n\narXiv preprint arXiv:2105.03818, 2021b.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pp. 3730–3738, 2015.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017.\n\nRabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. End-to-end bias mitigation by modelling biases in corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8706–8716, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nToshihiko Matsuura and Tatsuya Harada. Domain generalization using a mixture of multiple latent domains. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 11749– 11756, 2020.\n\nAndreas Mayr, G ̈unter Klambauer, Thomas Unterthiner, and Sepp Hochreiter. Deeptox: Toxicity prediction using deep learning. Frontiers in Environmental Science, 3, 2016. ISSN 2296-665X. doi: 10.3389/fenvs.2015.00080. URL https://www.frontiersin.org/article/10. 3389/fenvs.2015.00080.\n\nJulian McAuley, Jure Leskovec, and Dan Jurafsky. Learning attitudes and attributes from multiaspect reviews. In 2012 IEEE 12th International Conference on Data Mining, pp. 1020–1025. IEEE, 2012.\n\nTom McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3428–3448, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1334. URL https://www.aclweb. org/anthology/P19-1334.\n\nTomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. Advances in pre-training distributed word representations. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018), 2018.\n\nJunhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure:\n\nTraining debiased classifier from biased classifier. arXiv preprint arXiv:2007.02561, 2020.\n\nYonatan Oren, Shiori Sagawa, Tatsunori Hashimoto, and Percy Liang. Distributionally robust language modeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4218–4228, 2019.\n\nMengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In International Conference on Machine Learning, pp. 4334–4343, 2018.\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=ryxGuJrFvS.\n\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8732–8740, 2020.\n\nVictor Sanh, Thomas Wolf, Yonatan Belinkov, and Alexander M Rush. Learning from others’ mistakes: Avoiding dataset biases without modeling them. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Hf3qXoiNkR.\n\nTal Schuster, Darsh Shah, Yun Jie Serene Yeo, Daniel Roberto Filizzola Ortiz, Enrico Santus, and Regina Barzilay. Towards debiasing fact verification models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3410–3416, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1341. URL https://www.aclweb.org/anthology/D19-1341.\n\nRobert P Sheridan. Time-split cross-validation as a method for estimating the goodness of prospec-\n\ntive prediction. Journal of chemical information and modeling, 53(4):783–790, 2013.\n\nRicha Singh, Puspita Majumdar, Surbhi Mittal, and Mayank Vatsa. Anatomizing bias in facial\n\nanalysis. arXiv preprint arXiv:2112.06522, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nNimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R ́e. No subclass left behind: Fine-grained robustness in coarse-grained classification problems. Advances in Neural Information Processing Systems, 33, 2020.\n\nJoe Stacey, Pasquale Minervini, Haim Dubossarsky, Sebastian Riedel, and Tim Rockt ̈aschel. Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8281–8291, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.665. URL https://www.aclweb.org/anthology/ 2020.emnlp-main.665.\n\nMervyn Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the\n\nroyal statistical society: Series B (Methodological), 36(2):111–133, 1974.\n\nJ Taylor, B Earnshaw, B Mabey, M Victors, and J Yosinski. Rxrx1: An image set for cellular morphological variation across many experimental batches. In The 7th International Conference on Learning Representations, 2019.\n\nPrasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna Gurevych. Towards debiasing nlu models from unknown biases. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 7597–7610, 2020.\n\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one\n\nshot learning. Advances in neural information processing systems, 29:3630–3638, 2016.\n\nPei Wang and Nuno Vasconcelos. Towards realistic predictors. Conference on Computer Vision (ECCV), pp. 36–51, 2018.\n\nIn Proceedings of the European\n\nP. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD\n\nBirds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.\n\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112–1122. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101.\n\nAdam Yala, Peter G Mikhael, Fredrik Strand, Gigin Lin, Siddharth Satuluru, Thomas Kim, Imon Banerjee, Judy Gichoya, Hari Trivedi, Constance D Lehman, et al. Multi-institutional validation of a mammography-based breast cancer risk model. Journal of Clinical Oncology, pp. JCO–21, 2021.\n\nKevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel GuzmanPerez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molecular representations for property prediction. Journal of chemical information and modeling, 59(8):3370– 3388, 2019.\n\nDonggeun Yoo and In So Kweon. Learning loss for active learning. In Proceedings of the IEEE/CVF\n\nconference on computer vision and pattern recognition, pp. 93–102, 2019.\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791–4800, 2019.\n\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using places database. Advances in neural information processing systems, 27, 2014.\n\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.\n\n13",
    "reference": "# Summary Of The Paper\n\nThis paper proposes Learning to Split (ls), a novel algorithm that automatically detects a potential bias in datasets. More specifically, ls consists of a Splitter and a Predictor. The Splitter learns to divide the dataset into a train split and a test split. Then, the Predictor is trained using the divided train split and measures the generalization error with the test split. The Splitter is then trained in the direction in which the generalization error increases. That is, ls find a train split that cannot generalize well in the test split and then observe the bias existing in the dataset. The authors validated that ls behaves as expected on datasets with bias annotation. Finally, when ls is combined with debiasing methods that require bias annotation during training, such as GroupDRO, these models were trained successfully without bias annotation\n\n# Strength And Weaknesses\n\nStrength\n\n1. The paper is logically well-written, including a discussion of the limitations of ls. In particular, additional experiments related to label noise further enhanced the reliability of the paper.\n\n2. Several experiments have well validated the effect and applicability of ls. In particular, the experimental results using GropuDRO with ls are impressive. It would be interesting to conduct experiments with ls applied to other methods using bias annotations in the validation phase.\n\n3. This research tackles an important question, and I believe it is a helpful research direction for the community.\n\nWeakness\n\n1. I think there should be further experiments and discussion on how the \\delta value affects the results of ls. Therefore, during the review period, I hope the authors validate that the model works robustly even with various \\delta values. Also, it would be helpful to understand ls if the authors address how the model behaves when the percentage of minority groups varies rather than 25%.\n\n2. As the authors mentioned in Section 5, ls seems to have issues with scalability. To further discuss this concern, I think the training time of other debiasing methods should be provided in Table 2 of the Appendix.\n\n3. In Table 1, the source of the results brought from the previous paper is omitted. That information should be included to determine whether the experimental settings of ls are the same as those of the baselines.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe manuscript is logically well-written and easy to follow. Also, to the best of my knowledge, this is the first paper to successfully deal with detecting biases in datasets without bias annotations.\n\n# Summary Of The Review\n\nThe main contribution of this paper is that potential biases present in datasets can be detected without bias annotations. Although there are concerns regarding scalability, I believe it is valuable as the first paper that automatically detects potential biases.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Details Of Ethics Concerns\n\nNone"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCRYSTALBOX: EFFICIENT MODEL-AGNOSTIC EXPLANATIONS FOR DEEP RL CONTROLLERS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nPractical adoption of Reinforcement Learning (RL) controllers is hindered by a lack of explainability. Particularly, in input-driven environments such as computer systems where the state dynamics are affected by external processes, explainability can serve as a key towards increased real-world deployment of RL controllers. In this work, we propose a novel framework, CrystalBox, for generating black-box post-hoc explanations for RL controllers in input-driven environments. CrystalBox is built on the principle of separation between policy learning and explanation computation. As the explanations are generated completely outside the training loop, CrystalBox is generalizable to a large family of inputdriven RL controllers. To generate explanations, CrystalBox combines the natural decomposability of reward functions in systems environments with the explanatory power of decomposed returns. CrystalBox predicts these decomposed future returns using on-policy Q-function approximations. Our design leverages two complementary approaches for this computation: sampling- and learning-based methods. We evaluate CrystalBox with RL controllers in real-world settings and demonstrate that it generates high-fidelity explanations.\n\n1\n\nINTRODUCTION\n\nDeep Reinforcement Learning (DRL) based solutions outperform manually designed heuristics in many computer systems and networking problems in lab settings. DRL agents have been successful in a wide variety of areas, such as Adaptive Bitrate Streaming (Mao et al., 2017), congestion control (Jay et al., 2019), cluster scheduling (Mao et al., 2019b), and network traffic optimization (Chen et al., 2018). However, because DRL agents choose their actions in a black-box manner, systems operators are reluctant to deploy them in real-world systems (Meng et al., 2020). Hence, similar to many ML algorithms, the lack of explainability and interpretability of RL agents has triggered a quest for eXplainable Reinforcement Learning algorithms and techniques (XRL).\n\nThere are two major research directions in explainability of deep RL. The first line of work, which can be described as feature-based methods, transfer established XAI results developed for supervised learning algorithms to deep RL settings. They focus on tailoring commonly used post-hoc explainers for classification and regression tasks, such as saliency maps (Zahavy et al., 2016; Iyer et al., 2018; Greydanus et al., 2018; Puri et al., 2019) or model distillation (Bastani et al., 2018; Verma et al., 2018; Zhang et al., 2020). While such adapted techniques work well for some RL applications, it is becoming apparent that these types of explanations are not sufficient to explain the behavior of complex agents in many real-world settings (Puiutta & Veith, 2020; Madumal et al., 2020). For example, the inherent time-dependent characteristic of RL’s decision making process can not be easily captured by feature-based methods. In the second line of work, XRL techniques help the user to understand the agent’s dynamic behavior (Yau et al., 2020; Cruz et al., 2021; Juozapaitis et al., 2019). The main underlying idea of this class of XRL methods is to reveal to the user how the agent ‘views the future’ as most algorithms compute an explanation using various forms of the agent’s future beliefs like future rewards, goals, etc. For example, (Juozapaitis et al., 2019) proposed to modify a DQN agent to decompose its Q-function into interpretable components. (van der Waa et al., 2018) introduce the concept of explaining two actions by explaining the differences between their future consequences.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nIn this work, we present CrystalBox, a novel framework for extracting post-hoc black-box explanations. CrystalBox is designed to work with input-driven RL environments which is a rich class of RL environments, including systems or networking domains. Input-driven environments have two distinctive characteristics compared to standard RL settings. These environments operate over input data traces (where a trace can be a sequence of network conditions measurements), and often have a decomposable reward function. Traces are difficult to model, and make both policy learning and explainability more challenging: learning a self-explainable policy can lead to significant performance degradation. Hence, we build CrystalBox on the principle of separation between policy learning and explanation computation. Our next key observation is that thanks to the decomposable reward property, we can adapt the idea of decomposable returns (Anderson et al., 2019) as the basis for explanations. Below, we summarize our main contributions.\n\n1. We propose the first post-hoc black box explanation framework for input-driven RL envi-\n\nronments.\n\n2. We demonstrate that decomposable return-based explanations (Anderson et al., 2019) are a good fit for input-driven RL environments and propose a novel method for generating decomposed future returns using on-policy Q-function.\n\n3. We design two complementary approaches to compute on-policy Q-function approxima-\n\ntions outside of the RL agent’s training loop: sampling- and learning-based methods.\n\n4. We implement CrystalBox and evaluate it on input-driven RL environments. We demon-\n\nstrate that CrystalBox produces high-fidelity explanations in real-world settings.\n\n2 SYSTEMS ENVIRONMENTS\n\nSystems environments are a rich class of environments that represent dynamics in computer systems, which are fundamentally different from traditional RL environments. We provide an overview of our representative examples, Adaptive Bitrate Streaming and Congestion Control, and various other systems environments. For a thorough discussion on these environments, please see Appendix A.2 In this section, we highlight the characteristics that we leverage in our explainer, decomposability of reward functions and the notion of traces in these settings.\n\nAdaptive Bitrate Streaming. (ABR) In adaptive video streaming, there are two communicating entities: a client, such as a Netflix subscriber, who is streaming a video over the Internet from a server, such as a Netflix server. In video streaming, the video is typically divided into small secondslong chunks and encoded, in advance, at various discrete bitrates. The goal of the ABR controller is to maximize the Quality of Experience (QoE) of the client by choosing the most suitable bitrate for the next video chunk based on the network conditions. The controller ensures that the client receives an uninterrupted high-quality video stream while minimizing abrupt changes in the video quality and stalling. QoE in this setting is typically defined as a linear combination that awards higher quality and penalizes both quality changes and stalling (Mok et al., 2011).\n\nNote that network conditions are non-deterministic and constitute the main source of uncertainty in this setting. For example, the time taken to send a chunk depends on the network throughput. These network conditions are defined as the trace in ABR. More concretely, a trace is a sequence of network throughput values over time in ABR. Thus, an environment in ABR is modeled using network traces that represent network conditions.\n\nCongestion Control (CC) Congestion control protocols running on end-user devices are responsible for adaptively determining the most suitable transmission rate for data transfers over a shared, dynamic network. When a user transmits data at a rate that the network cannot support, the user experience high queuing delays and packet losses. Deep RL-based solutions have shown superior performance in this setting (Jay et al., 2019; Abbasloo et al., 2020). Similar to the ABR environment, traces in this setting also constitute a timeseries of throughput values. The reward function in congestion control incentivizes higher sending rates and penalizes delay and loss.\n\nOther Systems Environments. Deep RL offers high performance in cluster scheduling (Mao et al., 2019b), network planning (Zhu et al., 2021), network traffic engineering (Chen et al., 2018), database query optimization (Marcus et al., 2019), and several other systems control problems. A\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ncommon theme across these deep RL-based systems controllers is the decomposable reward function. The reason for that is that control in systems settings involves optimization across multiple objectives which are typically represented as the various reward components.\n\n3 FORMALIZATION OF EXPLANATIONS\n\nPreliminaries. In systems environments, we consider an Input-Driven Markov Decision Process (Mao et al., 2018), which is a special class of Markov Decision Processes where the environment transitions depend on an outside process called a trace. Formally, an Input-Driven MDP is defined by the tuple (S, A, Z, Ps, Pz, r, γ), where S is the set of states, A is the set of actions, Z is the set of time-variant traces, r is the reward function, and γ is the discount. Ps(st+1|st, at, zt) is the transition function of the environment that outputs the distribution of the next state, given the current state st, the action at, and the value of the current trace zt. Finally, Pz(zt+1|zt) is the transition function of the traces, which outputs the distribution over the next value of the trace given the current one. For example, in ABR, Pz is a model which determines how the Internet link between the viewer and the platform behaves over time.\n\nExplainablity. We take the perspective of systems operators. It is important to gain an understanding of a controller’s decision-making process. Some of the common questions may be ‘Why does the controller pick action A?’, ‘Why is action A better than action B?’, and ‘What are the measurable consequences of picking an action A?’. Note that these questions span from explanations about a single action to explanations that require reasoning about multiple actions. To answer these questions, we need to define a structure of explanations that is (a) succinct and (b) expressive.\n\nDecomposed future returns (Anderson et al., 2019) is a category of explanations that satisfies these requirements. When each return component is meaningful to the user and the number of return components is small, decomposed future returns provide a concise and expressive explanation. This technique was demonstrated to be effective in learning a self-explainable agent in game environments (Juozapaitis et al., 2019). In systems environments, since the reward functions are naturally decomposable, the future returns are decomposable as well. Moreover, each component represents an aspect of performance or cost that is meaningful to the operator. Thus, decomposed future returns are an apt choice as the units of explanation in this setting. The core challenge then is to generate these decomposed future returns accurately and efficiently.\n\nFuture returns. To build our explanations we require an oracle to compute future returns of a given state st, an action at, and a policy π. We note that this problem is equivalent to computing the decomposed on-policy Qπ(st, at) function, which calculates the expected future returns for taking acting action at in state st and following the policy π thereafter.\n\nWe propose to directly approximate this decomposed on-policy version of the Q-function, Qπ, outside of the policy training process. This separation allows us to build post-hoc explanations for any fixed policy π, even if π is non-deterministic or has continuous action space. We only require to be able to query this policy, without ever having to modify or know its internal structure.\n\nFollowing (Juozapaitis et al., 2019), we define the explainability problem as estimating the decomposed components of the on-policy action-value function Qπ(st, at) = (cid:80) c (st, at), where C is the set of reward components. For example, in ABR, the components are Quality, Quality Change, and Stalling. Each component Qπ c (st, at) computes the expected return of that component for taking action at in state st and following policy π thereafter. It is formally defined as:\n\nc∈C Qπ\n\nQπ\n\nc (st, at) = rc(st, at) + Est+1,at+1,...∼π\n\n∞ (cid:88)\n\n[γ∆trc(st+∆t, at+∆t)], ∀c ∈ C\n\n(1)\n\n∆t=1\n\nWe can obtain empirical samples of this function for all of the different components c by Monte Carlo rollouts. We refer to these Monte Carlo samples of the ground truth as Q\n\nc for convenience.\n\nπ\n\nWe define an explanation for a given state, action, and fixed policy as a tuple of return components:\n\nX (π, st, at) = [Qπ c1\n\n, . . . , Qπ ck\n\n],\n\nc1, . . . , ck ∈ C\n\n(2)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Overview of CrystalBox.\n\nIn general, one can consider more complex explanations that are functions over the return components. The function may depend on concrete environments and user preferences.\n\n4 CRYSTALBOX\n\nIn this section, we present our novel framework, CrystalBox. We start with a high-level description. CrystalBox consists of two main components (Figure 1). The first component is the future returns predictor. It takes as inputs a state, an action, a simulation environment, and a policy. We present two ways to build this component, a sampling-based approach (§ 4.1–4.1.2) and a learningbased approach (§ 4.2). The predictor produces expected returns that are fed to an optional postprocessing module which generates easy-to-understand explanations. As an example, we present a post-processing approach to summarize the returns in Section 5.3.\n\nWe discuss a few assumptions we make about available data. The framework requires four inputs: a state, an action, a policy, and a simulation environment. The first two inputs, a state, and an action, form a pair that we want to explain. The next input, policy, is treated as a black-box that we can only query. We never assume access to the model of the environment or future information such as st+1 or zt. The only assumption we make is that we have access to a simulation environment, the last input. Note that for most input-driven RL environments, these simulation environments are publicly available, e.g., ABR (Mao et al., 2017), CC (Jay et al., 2019), network scheduling (Mao et al., 2019a).\n\nLet us highlight several features of CrystalBox. First, it does implement our design principle which is the separation of policy learning and explanation computations. Second, it is flexible and allows the user to plug and play different environments and policies. To the best of our knowledge, CrystalBox is the first framework that provides such capabilities among the class of reward-based explainers. In the next section, we present a few approaches to design future returns predictors.\n\n4.1 SAMPLING-BASED APPROACH\n\nFigure 2: Examples of future return predictions: Examples of Quality and Stalling return predictions by Naive Sampling and Learned approaches, plotted alongside samples of ground truth returns. We see that Naive sampling fails to capture stalls or quality drops.\n\nOur first approach to designing a predictor is sampling-based: we estimate the individual components of Qπ(st, at) empirically by averaging over the outcomes of running simulations starting from st and taking the action at. However, in practice, Qπ c (st, at) cannot be computed exactly. Thus, to get an empirical approximation of Qπ c , it is necessary to bound the infinite horizon by a fixed length\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Overview of Learning-Based Approach.\n\ntmax. Enforcing this bounded horizon approximates the true Qπ version where the rewards after tmax are effectively assumed to be zero (Sutton & Barto, 2018).\n\nc with a commonly used truncated\n\nTo approximate Qπ c (st, at), we need to sample potential futures of state st for tmax steps. If we have a model of Pz available, we may simply use it to obtain samples of zt, and in turn st. In this case, our sampling would be equivalent to the MC simulation method. However, in input-driven environments, Pz is not available (Mao et al., 2018; 2019a). Therefore, to obtain potential futures of state st we have no choice but to sample traces from Z. Evidently, it is not MC simulation anymore, as these potential futures are ‘guessed’ by our sampling procedure rather than given to us. Z can be sampled using different strategies and we discuss two possible strategies.\n\nBefore diving into sampling strategies, we consider how sampling-based approach would work on ABR. Suppose we need an explanation for a drop in bitrate in ABR. In this case, we roll out the policy π in the environment and consider a set of states with a drop in bitrate for the next chunk. Our goal is to approximate Qπ c (st, at) in these states using our sampling strategies. Note that we can also continue rollouts from this point onwards and compute Q c (§ 3) of each of the future return components. We can use these to gain an initial understanding of how good our approximations are.\n\nπ\n\n4.1.1 NAIVE SAMPLING\n\nc (st, at) values with Q\n\nA simple strategy for sampling involves uniformly random sampling. Given a state st, we randomly sample traces from Z to guess potential futures and compute approximations of Qπ c . Now we can compare computed approximations of Qπ c . Figure 2 shows results of the comparison for the ABR example above focusing on two return components: quality and stalling. We observe that naive sampling-based predictions have low accuracy, especially for stalling predictions. To analyze the poor performance of the naive sampling approach, we took a close look at the sampling procedure. Recall that we randomly sample Z to obtain potential futures, so our estimates depend on the distribution of Z. We observe that the distribution of traces is very unbalanced (see Fig. 8 in Appendix A.1). The dominant traces do not sufficiently represent all relevant scenarios. One remedy to solve this issue is to make our sampling produce distribution aware, e.g. we could weight potential futures that we get from Z.\n\nπ\n\n4.1.2 DISTRIBUTION-AWARE SAMPLING APPROACH\n\nWe propose an improved sampling-based method. As we mentioned in the previous section, a sampling-based approach can benefit from a smarter weighting of potential futures that we obtain from Z. To do so, we take advantage of the features of st and condition our future values by calculating P (zt|st). In practice, this probability distribution cannot be easily computed because of the complexity of the underlying environment. We propose a method to approximate this conditioning. We assume that traces have underlying natural clustering, e.g. clusters may correspond to a set of regions, clients, time, etc. Hence, we cluster all traces in Z and provide a procedure to map the state st to its closest cluster. Finally, we randomly sample a trace within that cluster. In an experimental evaluation, we demonstrate that such conditioning does improve the sampling-based method.\n\n4.2 LEARNING-BASED APPROACH\n\nOur second approach is learning-based. This approach is based on the insight that future returns components of Qπ(st, at) form a function that can be directly parameterized and learned in a modelfree manner by a function approximator.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nThe proposed learning procedure consists of two phases (see Figure 3). In the first phase, we take a policy and a simulation environment and collect trajectories by rolling out policy π in the simulation environment. Next, we pre-process the trajectories to create a dataset of (st, at, Qπ c (st, at)) tuples. In the second phase, we learn our predictor Qπ c,θ for each component, where θ is a set of neural network parameters. We emphasize that we employ deep supervised learning to find the final parameters θ by iteratively updating the function approximator to better approximate the samples of Qπ c,θ(st, at) is the prediction of the neural network, and Qπ c (st, at) is a sample of the true value of the Q-function, calculated in the first phase by looking at the trajectory of states and actions after st and at. As in the sampling-based approach, we use the truncated version of the Qπ\n\nc,θ(st, at)). Here, Qπ\n\nc,θ(st, at) + α(Qπ\n\nc,θ(st, at) ← Qπ\n\nc (st, at) − Qπ\n\nc . Qπ\n\nc function.\n\nThis formulation is a special case of the function approximation version of the Monte Carlo Policy Evaluation algorithm (Silver, 2015; Sutton & Barto, 2018) for estimating Qπ θ is further broken down into smaller return components Qπ θ,c that can be added up to the original value. Therefore, the standard proof of correctness of the Monte Carlo Policy Evaluation applies. Thus, our method converges to the true Qπ function and captures how the policy performs.\n\nθ . In our case, Qπ\n\n4.3 QUALITY OF EXPLANATIONS\n\nNext, we discuss evaluation metrics for explanations. First, we briefly overview commonly used evaluation criteria for explanations: In standard explainability workflow, an explainer takes as input a complex function f (x) and produces an interpretable approximation g(x) as output. For example, g(x) can be a decision tree that explains a neural network f (x). To measure the quality of the approximation, the fidelity metric F D = ∥f (x) − g(x)∥, x ∈ D measures how closely the approximation follows the original function under an input region of interest D.\n\nthe fidelity metric.\n\nLet us consider how these evaluation criteria are applied to our RL settings to evaluate CrystalBox explanations. It turned out that such a translation is rather direct. As above, we have the complex function Qπ c , one per each component c (defined in Section 3.). CrystalBox outputs it approximation, i.e. a predictor Pred(Qπ c ), that also serves as an explanation. Hence, the fidelity metric is defined as a norm between a complex function and its approximation:\n\nF Dc = ∥Qπ\n\nc − Pred(Qπ\n\nc )∥, ∀c ∈ C.\n\n(3)\n\nIn our experiments, we use L2 norm. However, there is one distinction to discuss. Unlike standard settings, Qπ c is neither explicitly given to us as input nor can be efficiently extracted in any realistic environment, e.g. systems environments described in Section 2. Hence, the best we can do is to obtain estimates of Qπ\n\nc using Monte Carlo rollouts.\n\n5 EVALUATION\n\nWe present an experimental evaluation of CrystalBox that consists of two parts. First, we evaluate the fidelity of the returns predictors described in Sections 4.1.1–4.2. Next, we focus on the explainability capabilities of CrystalBox.\n\nWe perform our experiments on two systems environments: ABR and CC. In ABR, the controller decides the video quality of an online stream to show to a client. The controller receives a reward equal to the quality of experience of the client, measured as a weighted sum of three components: quality, quality change, and stalling. In CC, the controller manages the Internet traffic of a connection between a sender and a receiver by adjusting the sending rate of the outgoing traffic from the sender. Here, the controller receives a reward that is a weighted sum of three components: throughput, latency, and loss. For a detailed overview of these environments, see § 2.\n\nWe consider the three return predictors we presented earlier: the naive-sampling based approach (§ 4.1.1), the distribution-aware sampling approach (§ 4.1.2), and the learning-based approach (§ 4.2). Moreover, in some applications, we may have partial access to the policy. For example, we may have access to the embeddings of the states. In this case, it is important for explainability frameworks to take advantage of this additional knowledge to improve explanations. We demonstrate that our approach can do this without any major modifications. Therefore, in addition to the “black-box” setting, we consider a “gray-box” variant of our learned predictor where our predictor reuses the embedding φ(st) of a state st from the policy.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Adaptive Bitrate Streaming\n\n(b) Congestion Control\n\nFigure 4: Evaluation of CrystalBox for factual actions: Distribution of Squared Error of different methods to Monte Carlo samples of the ground truth in Adaptive Bitrate and Congestion Control. Here, we focus on traces that can potentially experience stalling and discuss results on all traces in Figure 14 (Appendix A.6). The Learned approach offers predictions with the lowest error to the ground truth in all three return components of both environments. Note that the values of all the returns are scaled to be in the range zero to one before being measured for error. The y-axis in results for ABR is adjusted due to the inherent tail-ended nature of ABR’s optimization.\n\n5.1 FIDELITY EVALUATION\n\nWe recall that decomposable future returns form the basis for CrystalBox explanations, so it is critically important for us to produce accurate predictions. To measure the quality of these predictions, we turn to the fidelity metric we introduced earlier (§ 4.3), and measure the error between the predictions of different approaches and samples of the true Qπ c function. We generate these samples by rolling out the policy on a held-out set of traces Z ′ to ensure that these samples have not been seen by any of the approaches before.\n\nIn Figure 4, we see that the learned predictor outperforms both of the sampling approaches in producing high-fidelity predictions of all three of the return components in both of the environments. The gray-box predictor narrowly beats the black-box approach at predicting the returns in Adaptive Bitrate Streaming, while achieving similar performance in Congestion Control. Next, we analyze the performance of two sampling-based methods. We see that Distribution-Aware sampling provides dramatic performance improvements over the standard sampling approach, especially, in CC. These results provide additional evidence to confirm our observation that exploiting the information in state st can be vital to producing high-fidelity return predictions.\n\n5.2 EXPLAINABILITY ANALYSIS\n\nHaving established the fidelity of our return predictors, we now turn to evaluating the explanatory power of these predictors. We focus our analysis on the predictors’ ability to answer contrastive questions such as “Why action A instead of action B?”. Recent work (Doshi-Velez et al., 2017; van der Waa et al., 2018; Mittelstadt et al., 2019; Miller, 2019) has highlighted the importance of such questions for human interpretability. Contrastive queries allow the user to differentiate between multiple possible actions to take, e.g. for debugging purposes.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Adaptive Bitrate Streaming: Counter Factual Actions\n\n(b) Congestion Control: Counter Factual Actions\n\nFigure 5: Evaluation of CrystalBox for Counter-factual actions: Distribution of Squared Error to samples of the ground truth decomposed return for ABR and CC. We see that the Learned approach offers the most accurate predictions for both factual and counter-factual actions in all of the different return components. Note that the values of all the returns are again scaled to be in the range zero to one before being measured for error. The y-axis in results for ABR is adjusted due to the inherent tail-ended nature of ABR’s optimization.\n\nCrystalBox supports answering this type of question. Given two actions A and B, we simply need to compute one explanation for A and one for B. The intuition is that by looking at explanations the domain expert can gain insight into why one action, e.g. the action that the policy suggests, is preferred over an alternative action. For example, consider ABR environment. Suppose that we are streaming a high-quality video. In state s1, the policy unexpectedly drops to sending mediumquality video (action A). An alternative action B is to keep the same bitrate value which might be seen as a better action by the operator. To resolve this discrepancy, the user requests explanations for A and B from CrystalBox that we show in the following table:\n\nAction A\nB\n\nExplanation (future returns per component) quality 16.23 16.31\n\nquality change 0.85 2.11\n\nstalling 0.0 0.41\n\nTable 1: Example of predictions of returns\n\nBy comparing future returns, we see that action B is more preferable to action A in terms of video quality but it loses to A in terms of quality change and stalling ( we want these values to be as low as possible for QoE). These indications should convince the operator that action A is more reasonable in this situation. We would like to note that while our explanations operate in terms of very highlevel notions for a given environment, like video quality for ABR, however, we do expect the systems operator to have the basic domain knowledge to draw conclusions given our explanations.\n\nTo quantitatively measure the quality of counterfactual explanations we again use the same fidelity metric. However, there is a difference between factual and counterfactual explanations that we need to take into consideration. Consider a set of trajectories generated in Phase 1 of the learning-based approach (see Figure 3). They come from running a given policy in the environment. Thus, only actions taken by the policy are recorded. However, we envision a range of use cases, like policy\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\ndebugging, where the user might be interested in actions that policy does not frequently take. In this case, such counterfactual actions might be underrepresented in these trajectories leading to poor future returns estimates. To resolve this issue, we again exploited our separation principle between policy training and learning a predictor. Namely, we augmented our dataset by generating additional trajectories where we add an explorative action to the beginning of the trajectory. We use the augmented dataset to train a single predictor. To clarify, the same predictor was used for factual (in Figure 4) and counterfactual explanations. In Figure 5 we observe the distribution of squared error of the different approaches to samples of the true Qπ c function where all the actions at are counterfactual. We emphasize that counterfactual actions can be seen as difficult-to-predict scenarios for the reasons we just explained. First, we see that the learned predictor outperforms samplingbased approaches in almost all cases. Moreover, they provide high-fidelity return predictions for counterfactual actions. Another interesting conclusion is that we see the advantage of the graybox over the black-box learned predictor in the same cases that were not that prominent for factual actions. Consider, for example, results for stalling in factuals in Figure 4a and counterfactuals in Figure 5a. The gray-box learned predictor significantly outperforms all other predictors in the latter plot.\n\n5.3 EVENT DETECTION\n\nCrystalBox provides an optional post-processing capability on top of original future return-based explanations. Consider again the example in Table 1. In many applications, return-based explanations can be sufficient. However, we believe that domain-specific post-processing can be very useful in practice. In some cases, it might not be obvious how to compare two contrastive actions in a state s1 from just their numerical returns. For example, if the stalling return value under action B is 0.1 then it is unclear whether we should interpret it as a sufficient indication of stalling that is not present under action A. Rather than making the user wonder about how to compare these future returns, we can post-process them in a form of binary events, e.g. if a stalling happens or not in the near future.\n\nWe introduce the notion of threshold for demarcating the boundary between binary events along each return component. For example, in ABR environment, we use the 0.3 threshold for stalling. If the return value is greater than 0.3 then the explanation signals that a stalling occurs in the future. Thresholds can be determined based on a variety of factors such as risk tolerance, recovery cost for certain events, etc. In Appendix A.5, we show experimental evaluation of this technique, where we analyze two comparative actions: factual and counter factual from the same state. Overall, we show that all predictors are capable to detect a large portion of events, while the learning-based predictors have better recall of events.\n\n6 DISCUSSION AND FUTURE WORK\n\nWe start our discussion with an applicability scope of CrystalBox and, then, discuss its possible extensions. In this work, CrystalBox targets systems-related applications. However, input-driven environments are not limited to this class of applications. For example, there is a rich class of gamebased environments that are also input-driven (Mao et al., 2018). CrystalBox can be potentially extended to game-based environments, however, such extension is non-trivial. For example, in our learned approach, we used Monte Carlo returns as estimates of the ground-truth Qπ c function. However, in games where rewards can be extremely delayed (only at the end of the episode) or attributed to a large sequence of actions, these returns can be extremely high variance. Such high variance can lead to poor estimates of future returns, hence, low-fidelity explanations. To overcome this variance, several strategies can be explored (Mao et al., 2018; Hessel et al., 2018; Silver et al., 2017). We believe that it is an interesting future direction on its own.\n\nOne interesting direction to explore is whether we can use model distillation techniques to extract an interpretable model of future returns predictors. Another potential avenue is to explore whether we can employ future return predictors during policy learning to facilitate understanding and debugging for human-in-the-loop frameworks.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSoheil Abbasloo, Chen-Yu Yen, and H Jonathan Chao. Classic meets modern: A pragmatic learningbased congestion control for the Internet. In Proceedings of the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication, pp. 632–647, 2020.\n\nAndrew Anderson, Jonathan Dodge, Amrita Sadarangani, Zoe Juozapaitis, Evan Newman, Jed Irvine, Souti Chattopadhyay, Alan Fern, and Margaret Burnett. Explaining reinforcement learning to mere mortals: An empirical study. arXiv preprint arXiv:1903.09708, 2019.\n\nOsbert Bastani, Yewen Pu, and Armando Solar-Lezama. Verifiable reinforcement learning via policy\n\nextraction. Advances in neural information processing systems, 31, 2018.\n\nLi Chen, Justinas Lingys, Kai Chen, and Feng Liu. Auto: Scaling deep reinforcement learning for datacenter-scale automatic traffic optimization. In Proceedings of the 2018 conference of the ACM special interest group on data communication, pp. 191–205, 2018.\n\nFrancisco Cruz, Richard Dazeley, Peter Vamplew, and Ithan Moreira. Explainable robotic systems: Understanding goal-driven actions in a reinforcement learning scenario. Neural Computing and Applications, pp. 1–18, 2021.\n\nFinale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O’Brien, Kate Scott, Stuart Schieber, James Waldo, David Weinberger, et al. Accountability of ai under the law: The role of explanation. arXiv preprint arXiv:1711.01134, 2017.\n\nSamuel Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atari agents. In International conference on machine learning, pp. 1792–1801. PMLR, 2018.\n\nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-second AAAI conference on artificial intelligence, 2018.\n\nRahul Iyer, Yuezhang Li, Huao Li, Michael Lewis, Ramitha Sundar, and Katia Sycara. Transparency In Proceedings of the 2018\n\nand explanation in deep reinforcement learning neural networks. AAAI/ACM Conference on AI, Ethics, and Society, pp. 144–150, 2018.\n\nNathan Jay, Noga Rotman, Brighten Godfrey, Michael Schapira, and Aviv Tamar. A deep reinforcement learning perspective on internet congestion control. In International conference on machine learning, pp. 3050–3059. PMLR, 2019.\n\nZoe Juozapaitis, Anurag Koul, Alan Fern, Martin Erwig, and Finale Doshi-Velez. Explainable reinforcement learning via reward decomposition. In IJCAI/ECAI Workshop on explainable artificial intelligence, 2019.\n\nAdam Langley, Alistair Riddoch, Alyssa Wilk, Antonio Vicente, Charles Krasic, Dan Zhang, Fan Yang, Fedor Kouranov, Ian Swett, Janardhan Iyengar, et al. The quic transport protocol: Design In Proceedings of the conference of the ACM special interest and internet-scale deployment. group on data communication, pp. 183–196, 2017.\n\nPrashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. Explainable reinforcement learning through a causal lens. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 2493–2500, 2020.\n\nHongzi Mao, Ravi Netravali, and Mohammad Alizadeh. Neural adaptive video streaming with pensieve. In Proceedings of the Conference of the ACM Special Interest Group on Data Communication, pp. 197–210, 2017.\n\nHongzi Mao, Shaileshh Bojja Venkatakrishnan, Malte Schwarzkopf, and Mohammad Alizadeh. arXiv preprint\n\nVariance reduction for reinforcement learning in input-driven environments. arXiv:1807.02264, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nHongzi Mao, Parimarjan Negi, Akshay Narayan, Hanrui Wang, Jiacheng Yang, Haonan Wang, Ryan Marcus, Mehrdad Khani Shirkoohi, Songtao He, Vikram Nathan, et al. Park: An open platform for learning-augmented computer systems. Advances in Neural Information Processing Systems, 32, 2019a.\n\nHongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng, and Mohammad In Proceedings of the\n\nAlizadeh. Learning scheduling algorithms for data processing clusters. ACM special interest group on data communication, pp. 270–288. 2019b.\n\nRyan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh, Tim Kraska, Olga Papaemmanouil, and Nesime Tatbul. Neo: A learned query optimizer. arXiv preprint arXiv:1904.03711, 2019.\n\nZili Meng, Minhu Wang, Jiasong Bai, Mingwei Xu, Hongzi Mao, and Hongxin Hu. Interpreting deep learning-based networking systems. In Proceedings of the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication, pp. 154–171, 2020.\n\nTim Miller. Explanation in artificial intelligence: Insights from the social sciences. Artificial intel-\n\nligence, 267:1–38, 2019.\n\nBrent Mittelstadt, Chris Russell, and Sandra Wachter. Explaining explanations in ai. In Proceedings\n\nof the conference on fairness, accountability, and transparency, pp. 279–288, 2019.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\nRicky KP Mok, Edmond WW Chan, and Rocky KC Chang. Measuring the quality of experience In 12th IFIP/IEEE International Symposium on Integrated Network\n\nof http video streaming. Management (IM 2011) and Workshops, pp. 485–492. IEEE, 2011.\n\nTobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden, Gabriel Barth-Maron, Hado Van Hasselt, John Quan, Mel Veˇcer ́ık, et al. Observe and look further: Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593, 2018.\n\nErika Puiutta and Eric Veith. Explainable reinforcement learning: A survey. In International crossdomain conference for machine learning and knowledge extraction, pp. 77–95. Springer, 2020.\n\nNikaash Puri, Sukriti Verma, Piyush Gupta, Dhruv Kayastha, Shripad Deshmukh, Balaji Krishnamurthy, and Sameer Singh. Explain your move: Understanding agent actions using specific and relevant feature attribution. arXiv preprint arXiv:1912.12191, 2019.\n\nDavid Silver. Lectures on reinforcement learning. URL: https://www.davidsilver.uk/\n\nteaching/, 2015.\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nJasper van der Waa, Jurriaan van Diggelen, Karel van den Bosch, and Mark Neerincx. Contrastive explanations for reinforcement learning in terms of expected consequences. arXiv preprint arXiv:1807.08706, 2018.\n\nAbhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri. Programmatically interpretable reinforcement learning. In International Conference on Machine Learning, pp. 5045–5054. PMLR, 2018.\n\nFrancis Y Yan, Hudson Ayers, Chenzhi Zhu, Sadjad Fouladi, James Hong, Keyi Zhang, Philip Levis, In 17th and Keith Winstein. Learning in situ: a randomized experiment in video streaming. USENIX Symposium on Networked Systems Design and Implementation (NSDI 20), pp. 495–511, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nHerman Yau, Chris Russell, and Simon Hadfield. What did you think would happen? explaining agent behaviour through intended outcomes. Advances in Neural Information Processing Systems, 33:18375–18386, 2020.\n\nTom Zahavy, Nir Ben-Zrihem, and Shie Mannor. Graying the black box: Understanding dqns. In\n\nInternational conference on machine learning, pp. 1899–1908. PMLR, 2016.\n\nHengzhe Zhang, Aimin Zhou, and Xin Lin. Interpretable policy derivation for reinforcement learning based on evolutionary feature synthesis. Complex & Intelligent Systems, 6(3):741–753, 2020.\n\nHang Zhu, Varun Gupta, Satyajeet Singh Ahuja, Yuandong Tian, Ying Zhang, and Xin Jin. Network planning with deep reinforcement learning. In Proceedings of the 2021 ACM SIGCOMM 2021 Conference, pp. 258–271, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Examples of Traces in Adaptive Bitrate Streaming. In ABR, a trace is the over-time throughput of the internet connection between a viewer and a streaming platform. In this figure, we present a visualization of a few of those traces for the first 100 seconds. Note that the y-axis is different on each plot due to inherent differences between traces.\n\nA APPENDIX\n\nA.1 TRACES\n\nIn this section, we visualize representative traces in Figure 6 and Figure 7 for ABR and CC applications, respectively.\n\nIn ABR, a trace is the over-time throughput of the internet connection between a viewer and a streaming platform. We obtain a representative set of traces by analyzing the logged data of a public live-streaming platform (Yan et al., 2020). In Figure 6 we present a visualization of a few of\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Examples of Traces in Congestion Control. In CC, a trace is defined as the internet network conditions between a sender and a receiver over time. These conditions can be characterized by many different metrics such as throughput, latency, or loss. In this figure, we represent these traces by the sender’s effective throughput over time. Note that both the x-axis and y-axis are different on each plot due to the inherent differences between traces.\n\nthose traces for the first 100 seconds. Note that the y-axis is different on each plot due to inherent differences between traces. However, even with a naked eye, we can see that some traces are highthroughput traces, e.g. all traces in the third row, while other traces are slow-throughput, e.g. the first plot in the second row. To further analyze these inherent differences, we analyze the distribution of mean and coefficient of variance of throughput within each trace. In Figure 8, we see that a majority of traces have mean throughput well above the requirements of the highest quality video. When we\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Distribution of Traces in ABR. Left: distribution of the mean throughput in traces. Note that the x-axis is log-scaled due to the large differences between all the clients of this server. Middle: distribution of coefficient of variance of the throughput within each trace. Right: The joint distribution of mean and coefficient of variance of throughput. The traces are logged over the course of a couple of months from an online public live-streaming Puffer (Yan et al., 2020). We find that a majority of the traces have mean throughput well above the bitrate of the highest quality video. Only a small percentage of traces represent poor network conditions such as low throughput, high variance, etc.\n\nFigure 9: Distribution of Traces in CC: We analyze the distribution of traces in CC by analyzing the distribution by four key metrics: throughput, latency, maximum queue size and loss. These traces are synthetically generated by sampling from a range of values, similar to the technique employed by (Jay et al., 2019). We observe that traces with especially poor network conditions such as high loss rate or high queuing delay are small in number.\n\nanalyze this jointly with the distribution of throughput coefficient of variance, we see that a majority of those traces also have smaller variance. Only a small number of traces represent poor network conditions such as low throughput or high throughput variance. These observations are consistent with a recent Google study (Langley et al., 2017) that showed that more than 93% of YouTube streams never come to a stall.\n\nIn CC, a trace is the over-time network conditions between a sender and receiver. We obtain a representative set of traces by following (Jay et al., 2019) and synthetically generating them by four key values: mean throughput, latency, queue size, and loss rate. In Figure 7, we demonstrate how these traces may look like from the sender’s perspective by looking at the effective throughput over time. Similar to the traces in ABR, we can visually see that the traces can be greatly different from one another. In Figure 9, we analyze the effective distribution of these traces. We observe that while the distribution isn’t nearly as unbalanced as it is in ABR, there are still only a small number of traces that have exceedingly harsh network conditions.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nA.2 ENVIRONMENTS IN THE SYSTEMS DOMAINS\n\nIn system domains, traditional environment simulators are not available due to the complexity of the underlying input-process z. To circumvent this issue, the state-of-the-art training environments replay specific logged runs of the system called “traces”. The training environment selects a specific input-trace z from the set of given traces, and generates the successive state st by simply looking up the logged value zt to compute P (st+1|st, at, zt). This replaying process circumvents the need to explicitly calculate or approximate the behavior of the trace P (zt|zt−1). However, this replaying process is not available when the policy is deployed in the real world. In the real world, we do not know the future value of the input-trace zt, nor do we have a way to approximate it using zt−1.\n\nTo summarize, we highlight the key differences between traditional simulators and “simulators” in input-driven environments.\n\n• Rollout mechanism during training: In traditional simulators, future states can be sampled. In trace-based simulators, it is not obvious how to obtain future states in a similar manner because they depend on an underlying process for which we lack a model. To overcome this issue, the state-of-the-art solution is to select a specific trace and replay it. This is a reasonable substitute for having a model of the underlying process and is efficient.\n\n• Complexity of rollouts: Rollouts can be expensive in traditional simulators. However, in trace-based simulators, given a specific trace, trace replay in the environment is relatively inexpensive, we just replay this trace.\n\n• Rollout mechanism during evaluation: In traditional simulators, this mechanism is not different from the one during training. Therefore, Monte Carlo sampling of the future states is trivial. By contrast, in trace-based environments, the rollout mechanism from point 1 cannot be used, because we are not given the future nor do we have a method to model it.\n\nA.3 EXAMPLES OF FUTURE RETURN PREDICTIONS (ADDITIONAL FIGURES)\n\nWe visualize all three components of future returns in Figure 10 which is an extension of Figure 2.\n\nFigure 10: Examples of future return predictions: We visualize the Quality Change component of the future return predictions we presented in Fig. 2. We present Quality and Stalling here again for clarity. Unlike the other two return components where Naive sampling achieves dramatically poor performance, we see that Naive Sampling can detect quality changes to a certain degree.\n\nA.4 RUNTIME ANALYSIS\n\nWe visualize the latency of Sampling-based and Learning-based techniques to generate an explanation. In sampling-based techniques, we empirically estimate the return components Qπ c by rolling out the policy for tmax steps under a number of sampled traces z1, z2, z3, .... During this rollout process, we repeatedly query the simulator and policy at each step. By contrast, in Learning-based techniques, we directly generate the return components by querying a neural network once. In Figure 11, we see the impact of this reduced overhead: learning-based techniques offer drastically lower latency than sampling-based techniques, reducing the computation time from 50-250 ms to just 5-6 ms.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 11: Runtime Analysis: We visualize the amount of time taken by Sampling-based and Learning-based techniques to generate an explanation in both ABR and CC environments. We see that the Learning-based techniques significantly reduce the compute time necessary to generate an explanation, reducing it by an order of magnitude on average.\n\nA.5 EVENT DETECTION\n\nWe present our evaluation of an event-based post-processing technique. We recall that the goal of the post-processing technique is to make an explanation easier for the operator to interpret. To do so, we introduce a notion of a threshold to convert numerical returns to event-based returns. For example, if the value of future return for quality change is greater than 0.2, we say that an event of quality change is going to be seen in the near future.\n\nTo evaluate the effectiveness of this post-processing technique, we consider the scenario where the operator may want to understand the impact of two comparative actions in the same state. We select a trace z, rollout the policy in that trace, and select a state st to focus on. Within that state, we simulate taking two actions, at1 and at2, and continue rolling out the policy using z under both the actions to obtain two Q\n\nπ\n\nc vectors.\n\nOur event-based post-processing allows us to treat the explainer as an event predictor model. Hence, π\nwe can evaluate it using standard metrics. The ground truth events are obtained from the two Q c\nvectors using the same threshold. Note that threshold values are parameters of this post-processing method. We manually tried out a few threshold values and choose one that balances recall and false positives among those we tried on the training set.\n\nFigure 12 shows our results for ABR (the first row) and CC (the second row). The first plot in each row shows results for factual and the second for counterfactual explanations. For ABR we used the following event threshold values: {quality below 0.55, quality change above 0.2, stalling above 0.3}. For CC we used the following event threshold values:{throughput below 0.4, latency above 0.15, loss above 0.025}. On the y-axis, we show the percentage of events that were correctly detected by each explanation generation method. For example, if there are 10 quality drop events and naive sampling detects 5 of them then the percentage value is 50 as this method detects 50% of events. Consider ABR results first. For factual explanations, sampling-based methods are better in detecting large quality change events. However, learning-based methods are better in detecting quality drop and long stall events. In fact, sampling-based method misses all long stall events. If we consider results for counterfactuals we see that learning-based methods outperform sampling methods. We observe a slightly different picture in CC environment. While the learned-based approaches outperform all other at detecting high latency, all the predictors provide high performance in other scenarios.\n\nNext, we compute the false positive rate for each predictor. Figure 13 shows these results for ABR and CC environments. By false-positive we mean a situation when a predictor signals that an event will happen but this is not the case w.r.t. Q . As can be seen from the plot, the false-positive rate is low for all predictors overall. The only exception that stands out is the high loss events in CC. Note that learning-based methods mostly manage to avoid false positives on factual explanations in CC. However, they do have a high false positive rate on counterfactuals.\n\nπ\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n(a) ABR: Factual Actions\n\n(b) ABR: Counter Factual Actions\n\n(c) CC: Factual Actions\n\n(d) CC: Counter Factual Actions\n\nFigure 12: Event Detection: We analyze the efficacy of different predictors at detecting events. We identify events happening by detecting if samples of the ground-truth return exceed a threshold. For a detailed discussion, see Section 5.3. We evaluate their efficacy by analyzing their recall under both factual and counterfactual actions.\n\n(a) ABR: Factual Actions\n\n(b) ABR: Counter Factual Actions\n\n(c) CC: Factual Actions\n\n(d) CC: Counter Factual Actions\n\nFigure 13: False Positive Rates on Event Detections: The false positive rate of the different predictors under both factual and counterfactual actions. We consider predictions false-positive when ground-truth samples of returns do not exceed the threshold, but the predictions exceed it.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nA.6 FIDELITY EVALUATION (ADDITIONAL RESULTS FOR ABR)\n\nWe present our evaluation of CrystalBox explanations on all traces. Figure 14 shows our results. We can see that all predictors perform really well. The learned-based predictors do slightly outperform sampling-based but the difference is not that prominent compared to results on traces that might experience stalling (see Figure 4a). For high throughput traces, the optimal policy for the controller is simple: send the highest bitrate. Therefore, all predictors do well on these traces.\n\n(a) Factual Actions\n\n(b) Counter Factual Actions\n\nFigure 14: Evaluation of CrystalBox in ABR across all traces. Distribution of Squared Error to samples of the ground truth decomposed return predictions for all traces in ABR. We observe that the differences in distribution of error for all of the return predictors shrink, but the relative ordering remains the same. Learned approach offers the best predictions for both factual and counterfactual actions.\n\nA.7 MONTE CARLO ROLLOUTS\n\nWe collect samples of the ground truth values of the decomposed future returns by rolling out the policy in a simulation environment. That is, we let the policy interact with the environment under an offline set of traces Z, and observe sequences of the tuple (s, a, ⃗r). With these tuples, we can calculate the decomposed return Qπ c (st, at) for each timestamp. However, for a given episode, these states and returns can be highly correlated (Mnih et al., 2013). Thus, to efficiently cover a wide variety of scenarios, we do not consider the states and returns Qπ c (st, at) after st for tmax steps. Moreover, when attempting to collect samples for a counterfactual action a′ t, we ensure the rewards and actions from timestamp t onwards are not used in calculation for any state-action pair before (st, a′ t). This strategy avoids adding any additional noise to samples of Qπ due to exploratory actions.\n\ntmax is a hyper-parameter for each environment. In systems environments, we usually observe an effect of each action within a short time horizon. For example, if a controller drops bitrate, then the user experiences lower quality video in one step. Therefore, it is only required to consider rollouts of a few steps to capture consequences of each action, so tmax equals to five is sufficient for our environments.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nA.8 LEARNING-BASED APPROACH\n\nPreprocessing. We employ Monte Carlo Rollouts to get samples of Qπ c for training our learned predictor. By themselves, the return components can vary across multiple orders of magnitude. Thus, similar to the standard reward clipping (Mnih et al., 2013) and return normalization (Pohlen et al., 2018) techniques widely employed in Q-learning, we normalize all the returns to be in the range [0, 1].\n\nNeural Architecture Design. We design the neural architecture of our learned predictors to be compact and sample efficient. We employ shared layers that feed into separate fully connected ‘tails’ that then predict the return components. We model the samples of Qπ c as samples from a Gaussian distribution, and predict the parameters (mean and standard deviation) to this distribution in each tail. To learn to predict these parameters, we minimize the negative log likelihood loss of each sample of Qπ c .\n\nFor the fully connected layers, we perform limited tuning to choose the units of these layers from {64, 128, 256, 512}. We found that a smaller number of units is enough in both of our environments. We present a visualization of our architectures in Figures 15-18.\n\nLearning Parameters. We learn our predictors in two stages. In the first stage, we train our network end-to-end. In the second stage, we freeze the shared weights in our network, and fine-tune our predictors with a smaller learning rate. We use an Adagrad optimizer, and experimented with learning rates from 1e-6 to 1e-4, with decay from 1e-10 to 1e-9. We tried batch sizes from {50, 64, 128, 256, 512}. We found that small batch sizes, learning rates and decay work best.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 15: Neural Architecture of the Black-box Learned Predictor in ABR.\n\nFigure 16: Neural Architecture of the Gray-box Learned Predictor in ABR.\n\nFigure 17: Neural Architecture of the Black-box Learned Predictor in CC.\n\nFigure 18: Neural Architecture of the Gray-box Learned Predictor in CC.\n\n21",
    "reference": "# Summary Of The Paper\n\nThis work considers the problem of providing interpretability for RL agents in input-driven environments. Specifically, this work proposes CrystalBox, a method for decomposing expected future returns into multiple components, when the reward itself is composed of multiple components. This work presents two approaches for providing the decomposed expected future returns: 1) A sampling-based approach that estimates the future returns with Monte Carlo simulation, assuming access to a simulator that enables multiple roll-outs from the current state; 2) A learning-based approach, which implements Monte Carlo policy evaluation.\n\n# Strength And Weaknesses\n\n## Strengths\n\nThis work's main strengths are that it studies an interesting and important problem of providing interpretability to RL policies, which are generally difficult to interpret, and that the presentation is quite clear.\n\n## Weaknesses\n\n- My main concern is that there is very limited technical contribution of interest to the ML community. Ultimately, the two proposed variants of CrystalBox leverage extremely well-understood classic techniques in fairly straightforward ways that can be found in RL textbooks, e.g., Sutton & Barto. I appreciate that these techniques are being used to decompose a value function into components, which is somewhat novel, but also still explored in prior work, e.g., Anderson et al., '19.\n- The sampling-based method is also quite expensive and relies on fairly strong assumptions in general. Specifically, it requires many Monte Carlo roll-outs (which are expensive), and also requires the ability to reset to a given state, which is generally not possible across RL tasks, although it is true that in the considered input-driven tasks, there is access to a simulator that enables this. Yet, I assume that a downstream goal is to apply this in production systems, rather that merely on simulators, in which case, it would not be possible.\n- Though this work claims that directly decomposing the rewards in the policy can \"lead to significant performance degradation,\" it's not actually clear to me that this is the case. It seems quite possible that directly learning a separate value function for each reward component, or predicting different components as an auxiliary task could yield a better policy. At least, this requires some empirical investigation, and raises the question of why we need a separate framework to estimate these returns.\n- Finally, a major concern is that it's difficult to assess the practical utility of the system from the current set of experiments. While the cdf error curves are interesting and show an ordering between the different proposed variants, it's unclear what amount of squared error is acceptable and useful for actually using this system for interpretability. Compounding this issue is the fact that even the computed squared errors are only estimates because the \"ground-truth\" values themselves are MC estimates. This work proposes reasonable ways that such a system might be used, but the key question remains: Is this system in fact useful for e.g., network engineers? A user study could potentially be useful here, although I do not have the expertise to understand how a network engineer might want to change the ABR or CC policy or its decisions based on the outputs of the interpretability system. However, determining how the interpretability system can beneficially impact future decision making is crucial -- if no decisions can practically be made from its outputs, then how is the interpretability helping?\n- Minor comment: Section 4 states that CrystalBox consists of 2 components, but only describes 1 component.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis work is extremely clear and seems to be reproducible. The quality is fairly high in that the proposals are technically sound, though the experimental evaluation could be tightened. The novelty is on the lower side, as the core techniques and ideas have been explored previously, though not in conjunction.\n\n# Summary Of The Review\n\nOverall, I am mainly concerned about the relevance and the impact of this work for the ML community. Therefore, I cannot recommend acceptance.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nBANDWIDTH ENABLES GENERALIZATION IN QUANTUM KERNEL MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nQuantum computers are known to provide speedups over classical state-of-the-art machine learning methods in some specialized settings. For example, quantum kernel methods have been shown to provide an exponential speedup on a learning version of the discrete logarithm problem. Understanding the generalization of quantum models is essential to realizing similar speedups on problems of practical interest. Recent results demonstrate that generalization is hindered by the exponential size of the quantum feature space. Although these results suggest that quantum models cannot generalize when the number of qubits is large, in this paper we show that these results rely on overly restrictive assumptions. We consider a wider class of models by varying a hyperparameter that we call quantum kernel bandwidth. We analyze the large-qubit limit and provide explicit formulas for the generalization of a quantum model that can be solved in closed form. Specifically, we show that changing the value of the bandwidth can take a model from provably not being able to generalize to any target function to good generalization for well-aligned targets. Our analysis shows how the bandwidth controls the spectrum of the kernel integral operator and thereby the inductive bias of the model. We demonstrate empirically that our theory correctly predicts how varying the bandwidth affects generalization of quantum models on challenging datasets, including those far outside our theoretical assumptions. We discuss the implications of our results for quantum advantage in machine learning.\n\n1\n\nINTRODUCTION\n\nQuantum computers have the potential to provide computational advantage over their classical counterparts (Nielsen & Chuang, 2011), with machine learning commonly considered one of the most promising application domains. Many approaches to leveraging quantum computers for machine learning problems have been proposed. In this work, we focus on quantum machine learning methods that only assume classical access to the data. Lack of strong assumptions on the data input makes such methods a promising candidate for realizing quantum computational advantage. Specifically, we consider an approach that has gained prominence in recent years wherein a classical data point is embedded into some subspace of the quantum Hilbert space and learning is performed using this embedding. This class of methods includes so-called quantum neural networks (Mitarai et al., 2018; Farhi & Neven, 2018) and quantum kernel methods (Havlíˇcek et al., 2019; Schuld & Killoran, 2019). Quantum neural networks are parameterized quantum circuits that are trained by optimizing the parameters to minimize some loss function. In quantum kernel methods, only the inner products of the embeddings of the data points are evaluated on the quantum computer. The values of these inner products (kernel values) are then used in a model optimized on a classical computer (e.g., support vector machine or kernel ridge regression). The two approaches are deeply connected and can be shown to be equivalent reformulations of each other in many cases (Schuld, 2021). Since the kernel perspective is more amenable to theoretical analysis, in this work we focus only on the subset of models that can be reformulated as kernel methods. A support vector machine (SVM) with a quantum kernel based on Shor’s algorithm has been shown to provide exponential (in the problem size) speedup over any classical algorithm for a version of the discrete logarithm problem (Liu et al., 2021), suggesting that a judicious embedding of classical data into the quantum Hilbert space can enable a quantum kernel method to learn functions that would be hard to learn otherwise.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nWhile the quantum kernels provide a much larger class of learnable functions compared to their classical counterpart, the ability of quantum kernels to generalize when the number of qubits is large has been called into question. Informally, Kübler et al. (2021) show that generalization is impossible if the largest eigenvalue of the kernel integral operator is small, and Huang et al. (2021) show that generalization is unlikely if the rank of the kernel matrix is large. The two conditions are connected since for a positive-definite kernel with fixed trace, a small value of the largest eigenvalue implies that the spectrum of the kernel is “flat” with many nonzero eigenvalues. Under the assumptions used by Kübler et al. (2021); Huang et al. (2021), as the number of qubits grows, the largest eigenvalue of the integral operator gets smaller and the spectrum becomes “flat”. Therefore, Kübler et al. (2021); Huang et al. (2021) conclude that learning is impossible for models with a large number of qubits unless the amount of training data provided grows exponentially with qubit count. This causes the curse of “exponential” dimensionality (Schölkopf et al., 2002) inherent in quantum kernels. However, Shaydulin & Wild (2021) show that if the class of quantum embeddings is extended by allowing a hyperparameter (denoted “kernel bandwidth”) to vary, learning is possible even for high qubit counts. While extensive numerical evidence for the importance of bandwidth is provided, no analytical results are known that explain the mechanism by which bandwidth enables generalization.\n\nIn this work, we show analytically that quantum kernel models can generalize even in the limit of large numbers of qubits (and exponentially large feature space). The generalization is enabled by the bandwidth hyperparameter (Schölkopf et al., 2002; Silverman, 2018) which controls the inductive bias of the quantum model. We study the impact of the bandwidth on the spectrum of the kernel using the framework of task-model alignment developed in Canatar et al. (2021), which is based on the replica method of statistical physics (Seung et al., 1992; Dietrich et al., 1999; Mezard & Montanari, 2009; Advani et al., 2013). While nonrigorous, this framework was shown to capture various generalization phenomena accurately compared with the vacuous bounds from statistical learning theory. Together with the spectral biases of the model, task-model alignment quantifies the required amount of samples to learn a task correctly. A “flat” kernel with poor spectral bias implies large sample complexities to learn each mode in a task, while poor task-model alignment implies a large number of modes to learn. On an analytically tractable quantum kernel, we use this framework to show generalization of bandwidth-equipped models in the limit of an infinite number of qubits. Generalization in this infinite-dimensional limit contrasts sharply with previous results suggesting that high dimensionality of quantum Hilbert spaces precludes learning.\n\nOur main contribution is an analysis showing explicitly the impact of quantum kernel bandwidth on the spectrum of the corresponding integral operator and on the generalization of the overall model. On a toy quantum model, we first demonstrate this analytically by deriving closed-form formulas for the spectrum of the integral operator, and show that larger bandwidth leads to larger values of the top eigenvalue and to a less “flat” spectrum. We show that for an aligned target function the kernel can generalize if bandwidth is optimized, whereas if bandwidth is chosen poorly, generalization requires an exponential number of samples on any target. Furthermore, we provide numerical evidence that the same mechanism allows for successful learning for a much broader class of quantum kernels, where analytical derivation of the integral operator spectrum is impossible. While our results do not necessarily imply quantum advantage, the evidence we provide suggests that, even with a compatible, well-aligned task, the quantum machine learning methods require a form of spectral bias to escape the curse of dimensionality and enable generalization.\n\n2 BACKGROUND\n\nWe begin by reviewing relevant classical and quantum machine learning concepts and establishing the notation used throughout the paper. We study the problem of regression, where the goal is to learn xμ, yμ μ=1 containing P a target function from data. Specifically, the input is the training set R defined on observations, with x drawn from some marginal probability density function p :\n\n=\n\n{\n\nP\n\nRn and y produced by a target function ̄f :\n\nX ⊂\n\nD R as y = ̄f (x).\n\nX →\n\n} X →\n\nLearning with kernels Given data in p :\n\ndistributed according to a probability density function R, we consider a finite-dimensional complex reproducing kernel Hilbert space (RKHS) . This feature map gives rise to a kernel function\n\nX → and a corresponding feature map ψ : H\nψ(x), ψ(x′) k(x, x′) = H. The RKHS ⟩\n⟨ satisfying the reproducing property and comprises functions f :\n\nX → H H\n\nassociated with k is endowed with an inner product\n\nR such that\n\n, ⟨·\n\nX\n\n·⟩\n\nH\n\nX →\n\nf, f ⟨\n\n⟩H <\n\n∞\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n(Schölkopf et al., 2002). Given a set of P data points xμ , the positive semidefinite Gram matrix is defined elementwise by Kμν = k(xμ, xν). The continuous analogue to the Gram matrix K is the integral kernel operator Tk : L2(\n\n) defined according to its action:\n\n∈ X\n\nL2(\n\n)\n\nX\n\n→\n\nX (cid:90)\n\n(Tkf )(x) =\n\nk(x, x′)f (x′)p(x)dx.\n\n(1)\n\nX\n\n⟨\n\nφk, φl ⟩\nk ηkφk(x)φ∗\n\n= δkl), span L2( k(x′), where ηk\n\nBy Mercer’s theorem (Schölkopf et al., 2002), the eigenfunctions of Tk satisfying Tkφk = ηkφk are ), and give rise to an eigendecomposition of k given by orthonormal (i.e., k(x, x′) = (cid:80) are real-valued, nonnegative eigenvalues of the integral ,\nH in the RKHS of k may be computed with operator due to its Hermiticity. The inner product ·⟩ ⟨· f, g H = respect to the integral kernel operator of Eq. 1 as ⟩L2(X ), with the null space of ⟩\nk . From the kernel eigendecomposition, any target ̄f Tk ignored in computing T −1 ) that lies L2( ∈\nX in the RKHS may therefore be decomposed as ̄f (x) = (cid:80) ̄f , φk k ̄akφk(x), where ̄ak = H are the ⟩\n⟨ target weights. We comment on the case where the target lies outside of the RKHS in Appendix C.\n\nf, T −1 ⟨\n\nk g\n\nX }\n\n{\n\n⟨\n\nKernel ridge regression (KRR) is a convex optimization problem over functions that belong to a Hilbert space\n\nand is stated as follows:\n\nH\n\nf ∗ = arg min\n\nf ∈H\n\n1 2\n\nP (cid:88)\n\nμ=1\n\n(cid:0)f (xμ)\n\nyμ(cid:1)2\n\n+\n\n−\n\nλ 2 ∥\n\nf\n\nH,\n\n2 ∥\n\n(2)\n\n≥\n\nwhere and λ\n\n∥·∥H denotes the norm with respect to the inner product\n\n0 is the ridge parameter introduced for regularizing the solution.\n\n, ⟨·\n\n·⟩H defined on the Hilbert space\n\nUsing these definitions, one can show that the solution to the regression problem takes the form f ∗(x) = k(x)⊤ (K + λI)−1 ̄y, where k(x) is a vector with elements k(x)μ = k(x, xμ) and ̄yμ = ̄yμ. The kernel trick (Schölkopf et al., 2002) allows one to perform regression without explicitly computing the features if one has access to the analytical kernel function. While providing a rich class of kernels, quantum kernels are typically not expressible analytically and hence require explicit representations of the feature maps.\n\nρ\n\n≻\n\nH\n\nH\n\n{ →\n\nHS = Tr(cid:8)A†B(cid:9) for A, B ⟩\n\nMachine learning with quantum computers The central motivation for applying quantum computers to problems in machine learning is to leverage the ability of quantum systems to efficiently perform computation in a high-dimensional quantum Hilbert space. We consider quantum systems defined on n qubits whose dynamics may be described using complex-valued linear operators. A 2n density matrix with general quantum state on n qubits may be described by a positive definite 2n ×\nL(C2n =\n0, Tr(ρ) = 1, ρ ρ\nunit trace and contained in the quantum Hilbert space ,\n| }\nwhere L(Cd) denotes bounded linear operators of the form Cd Cd or, equivalently, d d complex matrices. H given by the Hilbert–Schmidt inner product is endowed with the inner product ⟩\nA, B ⟨\nThe dynamics of quantum states are described by applying linear operators to ρ, and we here will specifically consider unitary operations (represented by a 2n 2n unitary matrix U ). Then, the quantum states that we are interested in may be prepared by applying a unitary operator to the vacuum . When n = 1, state ⟩\n= 1 and ρ = ρ† mean quantum states may be represented by the Bloch sphere: the constraints Tr that the components of a density matrix are parameterized by three real parameters n = (nx, ny, nz) and the density matrix can be written in terms of Pauli matrices ⃗σ as ρ = 1 ⃗σ). Since = 1, we can identify any single-qubit state ρ with a ∥\n≥ R3. Similarly, any unitary operation acting on a single qubit may be vector n in the unit sphere S2 represented as a sequence of rotations on the Bloch sphere (Nielsen & Chuang, 2011), thus making this representation convenient for visualizing feature maps associated with quantum kernels.\n\nrepresents the jth standard basis vector ˆej in R2n\n\n0 , where the “ket” notation ⟩\n\n1 is required for ρ\n\n0 subject to Tr\n\n2 (1 + n\n\nρ, ρ′ ⟨\n\nL(Cd).\n\n∈ ×\n\n∥ ≤\n\n×\n\n⊂\n\n∈\n\nn\n\nρ\n\nρ\n\n}\n\n{\n\n}\n\n{\n\nj\n\n)\n\n|\n\n|\n\n·\n\nBy associating the quantum Hilbert space with a feature space , we can define a feature map that gives rise to a quantum kernel (Havlíˇcek et al., 2019; Schuld & Killoran, 2019). We consider U †(x), a data-dependent unitary operator U (x) and prepare a density matrix ρ(x) = U (x) 2n matrix with ”1” in the top left corner and zeros elsewhere; later where we discuss examples of how to construct data-dependent unitary operators. A natural choice for a H = Tr(cid:8)ρρ′(cid:9). Under feature map is then ψ(x) = ρ(x) with the corresponding inner product ⟩\n\nrepresents the 2n\n\nρ, ρ′ ⟨\n\n0 |\n\n0 |\n\n0 |\n\n0 |\n\nH\n\n⟩⟨\n\n⟩⟨\n\n×\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nH = Tr(cid:8)ρ(x)ρ(x′)(cid:9), ψ(x), ψ(x′) this feature map, the quantum kernel is defined as k(x, x′) = ⟩\n, which inherits symmetry in its arguments from H. With this association between the feature ⟨· map ψ(x) and the quantum state ρ(x), we can freely apply existing theory for kernel methods in terms of complex vector spaces to study the spectra and generalization behavior of quantum kernels. In Appendix A we provide further details on the theory of quantum states and kernels, such as construction of the RKHS from Hermitian linear operators (or observables) and the geometry and probabilistic nature of quantum operations. In practice, a quantum kernel method computes the kernel matrix entries on a quantum computer by evaluating the value of the observable on the state U (x)U (x′)\n\nU †(x′)U †(x).\n\n0 |\n\n⟩ ⟨\n\n·⟩\n\n0\n\n0\n\n0\n\n⟨\n\n|\n\n|\n\n⟩ ⟨\n\n|\n\n3 MOTIVATING EXAMPLE: NO GENERALIZATION WITHOUT BANDWIDTH\n\nDespite existing techniques for empirically evaluating the potential performance of quantum kernels on classical datasets (Huang et al., 2021) and examples of successful implementation on currently available quantum computers (Glick et al., 2021; Peters et al., 2021; Wu et al., 2021; Hubregtsen et al., 2021), it is often still unclear how to construct a quantum kernel that will be suitable for learning on a real-world classical dataset. This uncertainty in the potential performance of quantum machine learning methods is compounded by regimes in which generalization is apparently impossible with a subexponential amount of training data. These regimes arise from the same feature of quantum computing that originally motivated quantum kernel methods: the availability of exponentially large feature spaces. In this section we discuss a simple example where the high dimensionality of the feature space precludes learning with fixed quantum embeddings, and we show that the introduction of bandwidth enables generalization.\n\n⊗\n\nO\n\nOne example of how high dimensionality of the feature space precludes learning is provided by random feature maps. Given a feature map ψ consisting entirely of independent Gaussian features, the operator Σ = EX [ψψ†] is proportional to the identity operator. Since Σ shares eigenvalues with Tk (P −1/2) (Rosasco of Eq. 1 (Appendix A), the eigenvalues of K concentrate around unity at a rate et al., 2010). Analogously, we consider a quantum feature map where states ρ(x) are prepared by 2n-dimensional random unitaries U (x), with the uniform distribution over unitaries being described by the Haar measure (e.g., Collins & Nechita (2016)). Then, identifying a correspondence Σ →\nEU ∼U (2n)[ρ(x) ρT (x)] in the quantum case and applying standard results from measure theory, one can directly compute the spectrum of Σ (Appendix A). This computation yields 2n−1(2n +1) nonzero eigenvalues with magnitude 21−n(2n + 1)−1, and thus the nonzero eigenvalues of K again become uniform as n . Since the largest eigenvalue is exponentially small in n, generalization requires the number of data points to be exponentially large in n. The connection between the magnitude of the largest eigenvalue and the required number of training samples can be seen directly from Kübler et al. (2021, Theorem 3), although it is a straightforward consequence of many older results, for example, (Dietrich et al., 1999; Bengio et al., 2005; Liang et al., 2019; Bordelon et al., 2020; Canatar et al., 2021). In other words, efficient generalization becomes impossible when the quantum feature map uses the full extent of the quantum state space uniformly. Our results demonstrate that the converse is true: restricting embeddings to a smaller region of quantum state space recovers the possibility of efficient learning.\n\n→ ∞\n\n3.1 LIMITATIONS OF FIXED QUANTUM EMBEDDINGS\n\nTo make the example concrete, we consider the following learning problem. This learning problem and the failure of fixed quantum embeddings on it were considered in Huang et al. (2021, Supplementary\n\nInformation 9). For an input x\n\nx(n)(cid:17) where x(n) is the last element of the vector x. Since learning this function is equivalent to learning the value of the last element of x, it is trivial classically, and a simple linear regression succeeds. We now show how KRR with a badly designed quantum kernel fails on this trivial task, and we show how the introduction of bandwidth allows the KRR with a quantum kernel to learn the target.\n\nn, the goal is to learn a target function ̄f (x) = cos }\n\n0, π\n\n∈ {\n\n(cid:16)\n\n,\n\nConsider a quantum kernel equipped with feature map\n\nU (x) =\n\nn (cid:79)\n\nj=1\n\nU (x(j)),\n\nU (x(j)) = Rx\n\n(cid:16)\n\nx(j)(cid:17)\n\n=\n\n4\n\n\n\n \n\n(cid:16)\n\ncos\n\n(cid:17)\n\n(cid:17)\n\nx(j)/2 (cid:16)\n\nx(j)/2\n\ni sin\n\n(cid:17)\n\n(cid:16)\n\ni sin\n\n(cid:16)\n\ncos\n\nx(j)/2 (cid:17)\n\nx(j)/2\n\n\n\n  ,\n\n(3)\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: A A quantum kernel method involves embedding data using a quantum circuit, often involving rotation by some angle about an axis. When angles are not rescaled properly, data can be embedded far apart in a space with dimensionality O(2n) (Sec. 3). Similarly, λmax is suppressed as the mean embedding EX [ρ(x)] approaches the center of the Bloch sphere (Kübler et al., 2021). B High-dimensional feature space results in k(x, x′) with narrow width (Eq. 6 with n = 50 and (top) c = 1 (bottom) c = 0.25) Tuning bandwidth escapes the “curse of dimensionality” associated with high-dimensional feature space. For n = 50 and ̄f (x) = cos , quantum features that are nearly orthogonal result in a narrow kernel (c = 1, top) and failure to generalize, while tuning bandwidth (c < 1, bottom) recovers KRR generalization performance.\n\nx(50)(cid:17)\n\n(cid:16)\n\nwhere (cid:78)n j=1 is the tensor product of single-qubit operations and Rx(θ) represents a rotation of θ about the x-axis of the single-qubit Bloch sphere (see Fig. 1A). For this feature map, the embedding factorizes over qubits as ρ(x) = U (x)\n\nU †(x) = (cid:78)n\n\nj=1 ρ(x(j)), and\n\n0 |\n\n⟩⟨\n\n0 |\n\n\n\nρ(x(j)) =\n\n \n\ni cos\n\n(cid:16)\n\nwith the kernel given by\n\ncos2(x(j)/2) (cid:16) (cid:17)\n\nx(j)/2\n\nsin\n\nx(j)/2\n\n(cid:17)\n\n(cid:16)\n\ni cos\n\n(cid:17)\n\nx(j)/2\n\n(cid:16)\n\nsin\n\n(cid:17)\n\nx(j)/2\n\nsin2(x(j)/2)\n\n−\n\n\n\n  ,\n\nk(x, x′) = Tr(cid:0)ρ(x)ρ(x′)(cid:1) =\n\nn (cid:89)\n\nj=1\n\n(cid:18)(cid:16)\n\nx(j)\n\ncos2\n\nx′(j)(cid:17)\n\n/2\n\n(cid:19) .\n\n−\n\n(4)\n\n(5)\n\nWhile this kernel is obtained via quantum operations, the fact that it has a closed form expression makes it classically easy to simulate. Nevertheless, it is a useful toy model (Kübler et al., 2021; Huang et al., 2021) for the analytical analysis of exponentially large quantum feature spaces. Since n, the kernel becomes a delta function: k(x, x′) = δx,x′. Therefore, the input data is x }\n= x′ are orthogonal and the kernel cannot any two points in the feature space ρ(x) and ρ(x′) for x capture the correlations in data. KRR with this kernel simply memorizes the training set and cannot generalize to any target function with a subexponential (in n) training set size.\n\n∈ {\n\n0, π\n\n3.2 BANDWIDTH ENABLES LEARNING\n\nThe preceding example highlights how quantum kernel methods utilizing high-dimensional spaces can fail to generalize. Our central technique for mitigating this limitation will be to introduce bandwidth to quantum kernels (Shaydulin & Wild, 2021). We now reconsider the kernel with the feature map [0, 1] that controls the bandwidth of the kernel. The of Eq. 3 and introduce a scaling parameter c feature map and the kernel become\n\n∈\n\nU (x(j)) = Rx\n\n(cid:16)\n\ncx(j)(cid:17)\n\n,\n\nk(x, x′) =\n\nn (cid:89)\n\nj=1\n\n(cid:18)\n\nc\n\n(cid:16)\n\nx(j)\n\ncos2\n\nx′(j)(cid:17)\n\n(cid:19)\n\n/2\n\n.\n\n−\n\n(6)\n\nGeometrically, the factor c restricts features ρ(x) to a smaller region of the Bloch sphere (Fig. 1A). Consequently, the kernel matrix is no longer diagonal, and we can straightforwardly check that simply tuning c allows KRR with kernel Eq. 6 to generalize (see Fig. 1B).\n\n5\n\n̸ Under review as a conference paper at ICLR 2023\n\n4 EFFECT OF BANDWIDTH ON SCALING AND SPECTRA\n\nIn the preceding section we provided a qualitative mechanism for how bandwidth improves the generalization of quantum kernels. We now analyze the expected generalization error of quantum kernels equipped with bandwidth. We derive explicitly the spectrum of the bandwidth-equipped quantum kernel with the feature map Eq. 6 and show how the bandwidth makes the spectrum less “flat,” thereby enabling learning. Our main tool is the theory developed in Bordelon et al. (2020); Canatar et al. (2021) where the generalization error, as a function of the training set size, is analytically obtained from the eigenvalues of the kernel and the projection of the target function on the RKHS defined by the kernel.\n\n4.1 EXPLICIT FORMULAS FOR GENERALIZATION OF BANDWIDTH-EQUIPPED KERNELS\n\nUnif([\n\nWe consider the quantum kernel with the feature map given by Eq. 6 and the input distribution π, π]n), as previously studied by Kübler et al. (2021). For a single qubit, the kernel x\nbecomes k(x, x′) = cos2(c(x x′)/2) with bandwidth parameter c, and the eigenvalues can be computed as follows (see Appendix B):\n\n∼\n\n−\n\n−\n\nλ1 =\n\nλ3 =\n\n3 8\n3 8\n\n+\n\n+\n\n1 8\n1 8\n\nsinc(2πc) +\n\nsinc(2πc)\n\n−\n\n(cid:112)(1\n\n(cid:112)(1\n\n1 8\n1 8\n\n−\n\n−\n\nsinc(2πc))2 + 16sinc(πc)2,\n\nλ2 =\n\n1 4 −\n\n1 4\n\nsinc(2πc),\n\nsinc(2πc))2 + 16sinc(πc)2,\n\nλ4 = 0.\n\n(7)\n\n∼ O\n\nn(1) and makes generalization impossible with P\n\nFor an n-qubit system, the largest eigenvalue ηmax of the kernel in Eq. 6 falls exponentially with n for c poly(n) amount data (Kübler et al., 2021) (see Appendix. B). To prevent ηmax decreasing with n, we choose a bandwidth that scales with n (i.e., c = an−α with a 2 is required for ∼ O generalization. We consider α = 1/2 henceforth. Then, all nonzero eigenvalues of the n-qubit kernel Eq. 6 can be expressed as λk1 1 λk2 3 with k1 + k2 + k3 = n, where the single-qubit eigenvalues asymptotically (with n) look like\n\n(1) and α > 0). In Appendix B we show that α\n\n2 λk3\n\n≥\n\n∼\n\n1\n\na2π2 6n Starting from k1 = n and k2 = k3 = 0, the hierarchy of eigenvalues obtained in this way are given in Table 1. We denote each of these eigenvalues as ηk,z and their corresponding eigenfunctions as\n\na4π4 180n2 .\n\n(8)\n\nλ1\n\nλ2\n\nλ3\n\n≈\n\n≈\n\n≈\n\n1,\n\n,\n\nTable 1: Hierarchy of eigenvalues based on their scaling. N (n, k) denotes the degeneracy of eigenvalues with scaling n−k, and\n\ndenotes the form of the corresponding eigenstates.\n\nn, k |\n\n⟩\n\nn−k Degeneracy N (n, k) Eigenstate n0 n−1 n−2 n−3\n\n(cid:1) (cid:1) + (cid:0)n (cid:1) + (cid:0)n−1\n\n⊗n ⟩\n⊗(n−1) ⟩\n⊗(n−2) ⟩\n⊗(n−3) ⟩\n\nψ1 ψ1 ψ1 ψ1\n\n1 (cid:0)n 1\n(cid:0)n 2\n(cid:0)n 3\n\n(cid:1)(cid:0)n 1\n\n| |\n| |\n\n(cid:1)\n\n(cid:1)\n\n1\n\n1\n\nn, k |\n\n⟩\n\nψ2 |\nψ2 |\nψ2 |\n\n⟩ ⟩\n⟩\n\n⊗(n−1)\n\n⊗(n−2)\n\n⊗2 , ⊗3 ,\n\nψ1 |\nψ1 |\n\n⟩ ⟩\n\nψ3 ψ2\n\n| |\n\n⟩ ⟩ |\n\nψ3\n\n⟩\n\n∼ O\n\n(n−k) for each k. Notice that the quantity ̄ηk,z = N (n, k)ηk,z\n\n ̄ηk,z′ for all pairs z, z′ since the spectrum is almost flat at each scaling k (see Figure 2A).\n\nφk,z(x), where k indexes the overall scaling of the eigenvalue and z indexes each of the individual (nk) denotes the number of eigenvalues eigenvalues with scaling n−k. The degeneracy N (n, k) ηk,z n(1) with respect to the input dimension and its value depends on a and k. For each k, we will further make the approximation ̄ηk,z We obtain the projections of target function ̄f (x) on the kernel eigenfunctions as ̄ak,z = (cid:82) ̄f (x)φk,z(x)p(x)dx. We also define the total target power at each scaling ̄a2 k,z. With the eigenvalues and the target weights ̄a2 k, the generalization error is given by (Canatar et al., 2021) (Appendix C):\n\n(cid:80)N (n,k) z=1\n\nk ≡\n\n∼ O\n\n∼ O\n\n ̄a2\n\n≈\n\nEg =\n\nκ2\n\n−\n\nγ\n\n1\n\n(cid:88)\n\nk\n\n ̄a2 k\n(cid:0)κ + αk ̄ηk\n\n(cid:1)2 ,\n\nκ = λ + κ\n\n(cid:88)\n\nk\n\n ̄ηk κ + αk ̄ηk\n\n,\n\n(cid:88)\n\nγ =\n\nk\n\nαk ̄η2 (κ + αk ̄ηk)2 ,\n\nk\n\n(9)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nwhere λ is the KRR regularization parameter, κ should be solved self-consistently, and we have defined αk = P ) while (nl)), the generalization error decouples keeping αl across different scaling limits and becomes\n\nN (n,k) . Taking the large qubit and large data limit (n\n\nn(1) for some mode l (meaning P\n\nand P\n\n→ ∞\n\n→ ∞\n\n∼ O\n\n∼ O\n\n\n\n\n\nEg(αl) =\n\nκ2\n\n−\n\nγ\n\n1\n\n ̄a2 l\n(cid:0)κ + αl ̄ηl\n\n(cid:1)2 +\n\n1\n\nγ\n\n−\n\n(cid:88)\n\nk>l\n\nγ\n\n\n\n ̄a2\n\nk\n\n +\n\n ̄a2 k.\n\n(cid:88)\n\nk>l\n\n(10)\n\nThe target modes with k > l remain unlearned since αk>l = 0 and the modes k < l have already . This scaling defines learning stages where been learned using the provided data since αk<l = at each stage a single mode l is being learned, and the remaining modes contribute as constant error. The term in the parentheses goes to zero as αl (see Appendix C). Hence, when the mode l is completely learned, the ratio of the generalization error to its initial value becomes Eg(αl=∞) (nl), the quantum kernel k). Therefore, given a data budget P is guaranteed to generalize to target functions whose weights beyond mode l are vanishing. The , called the cumulative power (Canatar et al., 2021), describes the quantity C(l) = 1 amount of power placed in the first l modes and quantifies the task-model alignment.\n\nEg(0) = ((cid:80)\n\nEg(αl=∞) Eg(0)\n\nk)/((cid:80)\n\nk>l ̄a2\n\n→ ∞\n\nk ̄a2\n\n∼ O\n\n∞\n\n−\n\nIt was shown in Canatar et al. (2021) that kernels generalize better for target functions with sharply (nl) samples, it is a necessary but not sufficient rising C(l). In our case, for generalizability at P condition for target functions to have good task-model alignment for which C(l) 1. Note that the trace of this kernel is (cid:82) k(x, x)p(x)dx = 1, and therefore the eigenvalues satisfy (cid:80) ≈\nk,z ηk,z = 1. (3−n) since there are 3n nonzero modes of the Flatness of the spectrum, then, implies ηk,z kernel in Eq. 6 (see Appendix B). Equation 9 suggests that even if the target is aligned well with the (3n) samples to learn each mode, and so generalization becomes impossible kernel, it requires P with polynomial sample complexity P\n\n(nl).\n\n∼ O\n\n∼ O\n\n∼ O\n\n∼ O\n\n∼ O\n\n(nl) samples yield an excess generalization error Eg\n\nOn the other hand, bandwidth enables sufficient decay in the spectrum of the kernel, and Eq. 9 shows that P C(l). In Figure 2, we show the results of simulating the kernel of Eq. 6 with n = 40 qubits for a target function given by ̄f (x) = e−∥x∥2/n2 . For c = 1, the kernel has a flat spectrum (Figure 2A) with poor alignment with the task (Figure 2B). On the other hand, bandwidth introduces sufficient decay in the eigenspectrum so that polynomial time learning becomes possible. Surprisingly, bandwidth also improves the task-model alignment which, together with spectral bias, implies generalizability with better sample efficiency. In Figure 2C, we perform kernel regression with our toy kernel and confirm that generalization improves with bandwidth up to an optimal value after which it degrades again. This is due to the fact that larger bandwidths cause underfitting of the data (Silverman, 2018) since only a very few eigenmodes become learnable while the target cannot be fully explained by those modes. In Figure 2, we find that the optimal bandwidth parameter is c∗\n\n−\n\n≈\n\n1\n\n2\n\nn (see Appendix E).\n\n≈\n\nπ, π]40). Learnability Figure 2: A Eigenvalues of the kernel Eq. 6 with respect to data x is explained by the preservation of large-eigenvalue eigenspaces; without bandwidth, the model provably cannot learn in poly(n) complexity due to the flat spectrum. B The projections of the target ̄f (x) = e−∥x∥2/n2 on the eigenvectors of the kernel for each bandwidth. Apart from the flatness of the c = 1 kernel, its eigenfunctions align poorly with the target. C Generalization error as a function of the number of training samples computed by using theory (solid lines) (Eq. 9) and performing kernel ridge regression empirically (dots). Bandwidth c = 1 yields a constant learning curve. While all c < 1 kernels provide improvement, an optimal bandwidth parameter c∗ 2/n gives the best task-model alignment.\n\nUnif([\n\n−\n\n∼\n\n≈\n\n7\n\n101102103P10−1010−810−610−410−2100Eg(P)c=1.00c=0.10c∗=0.05c=0.03c=0.01101102103k10−410−310−210−1100C(k)c=1.00c=0.10c∗=0.05c=0.03c=0.01100101102103k10−1110−910−710−510−310−1ηkpoly(n2)c=1.00c=0.10c∗=0.05c=0.03c=0.01ABCUnder review as a conference paper at ICLR 2023\n\n4.2 EVIDENCE OF PERFORMANCE GAINS IN REAL DATASETS\n\nTo evaluate our theory in a practical setting, we consider two previously proposed quantum kernels that have been conjectured to be hard to simulate classically: a kernel with a feature map inspired by instantaneous quantum polynomial-time (IQP) circuit (Shepherd & Bremner, 2009; Havlíˇcek et al., 2019; Huang et al., 2021) and a kernel with Hamiltonian evolution (EVO) feature map (Huang et al., 2021; Shaydulin & Wild, 2021) (see Appendix E for details). Unlike the kernel considered in the preceding section, the spectrum cannot be derived analytically for these kernels.\n\nWe evaluate these kernels on binary classification versions of FMNIST (Xiao et al., 2017), KMNIST (Clanuwat et al., 2018), and PLAsTiCC (The PLAsTiCC team et al., 2018) datasets with the input data downsized to n = 22 dimensions, which were previously used to evaluate quantum kernel performance in Shaydulin & Wild (2021); Huang et al. (2021); Peters et al. (2021). We use the kernel values reported in Shaydulin & Wild (2021), which were evaluated with high precision using an idealized (noiseless) simulator. In practice, an additive error is introduced when evaluating the kernel values on a fault-tolerant quantum computer. Bandwidth-equipped kernels are robust against this error; see the discussion in Shaydulin & Wild (2021). We perform SVM for binary classification using these kernels with varying bandwidths. In Table 2, we report the test accuracies with bandwidth parameter c = 1 for the IQP and EVO kernels. We also report the test performance with bandwidth parameters c∗ < c optimized by hyperparameter tuning for each kernel using cross validation (see Appendix E). For both kernels, bandwidth significantly improves the test performance.\n\nIQP\n\nEVO\n\nRandom Guess\n\nFMNIST\n\nKMNIST\n\nPLAsTiCC\n\nc∗\n\n0.926\n\n0.915\n\n0.789\n\nc = 1\n\n0.542\n\n0.629\n\n0.5\n\nc∗\n\n0.916\n\n0.914\n\n0.788\n\nc = 1\n\n0.643\n\n0.600\n\n0.613\n\n0.5\n\n0.5\n\n0.5\n\nTable 2: Bandwidth tuning recovers significant performance gains over out-of-the-box quantum models, opening up the possibility of better workflows for general quantum machine learning on many qubits via hyperparameter tuning.\n\nTo test our intuition, we present in Fig. 3A the shape of the IQP kernel where the kernel clearly sharpens for larger values of bandwidth parameter implying flat spectrum. In Fig. 3B, we further confirm that the spectrum decay improves with bandwidth when the IQP kernel is evaluated on the FMNIST dataset. We also find that the task-model alignment improves greatly with the bandwidth (Fig. 3C), thus implying, together with the previous point, possible generalization.\n\nFigure 3: A IQP kernel function example: Intuitive behavior of bandwidth persists even when quantum kernel is not strictly shift-invariant. B IQP with the addition of bandwidth is capable of recovering significant target alignment with FMNIST dataset. C While the spectrum for c = 1 IQP is flat, it also has poor alignment with the target (see Appendix E).\n\n5 RELATED WORK\n\nHuang et al. (2021) introduce the idea that the exponential dimensionality of quantum feature spaces precludes generalization of quantum kernel methods, and they provide an upper bound on generalization error that includes the dimension of the space spanned by the training set. They connect their results to the spectrum of the kernel by introducing a measure of “approximate dimension” of\n\n8\n\n−1.0−0.50.00.51.0x0.00.20.40.60.81.0K(x)c=0.01c=0.1c=0.3c=0.5c=1.0100101102103k10−1010−710−410−1ηkc=0.01c=1100101102103k0.60.81.0C(k)ABCUnder review as a conference paper at ICLR 2023\n\n(cid:16) 1\n\n(cid:17)\n\n×\n\nk=1\n\n(cid:80)m\n\nthe span of the training set given by (cid:80)n l=1 are the eigenvalues of the n n kernel matrix. This number is effectively a measure of the flatness of the spectrum of the kernel and is used in numerical experiments in Huang et al. (2021). An alternative analysis is given by Banchi et al. (2021), who derive bounds on generalization error using quantum information techniques and show that near-identity kernels resulting from large dimensionality of the feature space preclude generalization. We use the construction from Huang et al. (2021, Appendix I) as our motivating example in Sec. 3.\n\n, where\n\nl=k tl\n\nn−k\n\ntl\n\n}\n\n{\n\nn\n\nKübler et al. (2021) introduce techniques for deriving the spectrum of quantum kernels and obtain the spectrum of the kernel Eq. 5. They then use these techniques to show that the purity of the mean embedding provides an upper bound on the largest eigenvalue of the quantum kernel integral operator and consequently that quantum kernel methods with feature maps that utilize the full quantum Hilbert space require an exponential number of qubits of data to learn. However, Kübler et al. (2021) did not consider bandwidth as a method for controlling the inductive bias of quantum kernels. Our contribution is using the techniques of Kübler et al. (2021) to derive the spectrum of the bandwidth-equipped kernel Eq. 6 and explicit formulas for the expected generalization error.\n\nShaydulin & Wild (2021) introduce the concept of quantum kernel bandwidth and provide numerical evidence that bandwidth tuning (equivalent to rescaling of the input data) improves the generalization of SVM with quantum kernels. An analogous mechanism has been observed to improve trainability of quantum neural networks (Zhang et al., 2022). However, previous results make no connection between the bandwidth and the spectrum of the kernel and provide no analytical results for generalization. We reinterpret the data from Shaydulin & Wild (2021) in Sec. 4.2 and show how the bandwidth controls the spectrum of the kernel and enables generalization.\n\n6 DISCUSSION\n\nIn this work, we studied the kernels induced by quantum feature embeddings of data and their generalization potential. Recent work suggests that machine learning models built in this way suffer from the curse of dimensionality, requiring exponentially large training sets to learn. Note that 103 for standard datasets) which embedding n-dimensional data requires at least n qubits (where n span a 2n dimensional feature space. While quantum models may possess potentially powerful and classically inaccessible representations for certain tasks, utilization of those necessarily requires a control over the large space they span in order to generalize.\n\n∼\n\nHere we showed that the bandwidth hyperparameter enables generalization of quantum kernel methods when the numbers of qubits is large, and provided explicit formulas for the resulting expected generalization error on a toy model. Our results open up promise for quantum machine learning beyond intermediate numbers of qubits. A central lesson provided by our work is that thoughtfully chosen hyperparameters can significantly improve the performance of quantum machine learning methods. Identifying such hyperparameters that control the inductive bias of quantum models is essential to realizing the full potential of quantum machine learning methods.\n\nUnlike prior works, we focus not just on the spectrum of the quantum kernel but on the alignment between the kernel and the real-world datasets. Our empirical results imply that scaling the bandwidth not only makes the spectrum less flat, but also improves the alignment with real-world target functions. These observations make us optimistic about the potential of quantum kernel methods to solve classically challenging problems.\n\nAn important limitation of our results is that while bandwidth scaling is guaranteed to improve the spectrum, it does not necessarily lead to good alignment. For example, in the limit of c 0 the spectrum has only one nonzero eigenvalue, although learning is still not possible (see Appendix D). This suggests that optimizing bandwidth as a hyperparameter during training can balance triviality of the feature map (c 0) with greater utilization of the quantum state space. At the same time, if the feature map is chosen poorly, varying bandwidth would not lead to good generalization.\n\n→\n\n→\n\nIf the kernel values are evaluated on a noisy quantum computer, the quantum hardware noise would affect the spectrum of the kernel. Hardware noise reduces the purity of embedding, leading to a trivial lower bound on generalization error from Kübler et al. (2021, Theorem 1). Heyraud et al. (2022) give a more detailed analysis. While outside the scope of this work, the impact of noise on generalization is nonetheless an important consideration for near-term prospects of quantum kernel methods.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nHéctor Abraham, Ismail Yunus Akhalwaya, Gadi Aleksandrowicz, Thomas Alexander, G Alexandrowics, E Arbel, A Asfaw, C Azaustre, P Barkoutsos, G Barron, et al. Qiskit: An open-source framework for quantum computing. URL https://doi. org/10.5281/zenodo, 2562111, 2019. doi: 10.5281/zenodo.2562111.\n\nMadhu Advani, Subhaneil Lahiri, and Surya Ganguli. Statistical mechanics of complex neural systems and high dimensional data. Journal of Statistical Mechanics: Theory and Experiment, 2013(03):P03014, 2013. doi: 10.1088/1742-5468/2013/03/P03014.\n\nLeonardo Banchi, Jason Pereira, and Stefano Pirandola. Generalization in quantum machine learning: A quantum information standpoint. PRX Quantum, 2(4), November 2021. doi: 10.1103/ prxquantum.2.040321. URL https://doi.org/10.1103/prxquantum.2.040321.\n\nYoshua Bengio, Olivier Delalleau, and Nicolas Le Roux.\n\nThe curse of dimensionality Technical Report TR-1258, Université de Montréal, March URL https://www.microsoft.com/en-us/research/publication/\n\nfor local kernel machines. 2005. the-curse-of-dimensionality-for-local-kernel-machines/.\n\nBlake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In International Conference on Machine Learning, pp. 1024–1034. PMLR, 2020. URL https://proceedings.mlr.press/v119/ bordelon20a.html.\n\nAbdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. Nature Communications, 12(1):1–12, 2021. doi: 10.1038/s41467-021-23103-1.\n\nTarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep learning for classical Japanese literature. arXiv:1812.01718, 2018. doi: 10.20676/ 00000341.\n\nBenoît Collins and Ion Nechita. Random matrix techniques in quantum information theory. Journal\n\nof Mathematical Physics, 57(1):015215, January 2016. doi: 10.1063/1.4936880.\n\nRainer Dietrich, Manfred Opper, and Haim Sompolinsky. Statistical mechanics of support vector\n\nnetworks. Physical Review Letters, 82(14):2975, 1999. doi: 10.1103/PhysRevLett.82.2975.\n\nEdward Farhi and Hartmut Neven. Classification with quantum neural networks on near term\n\nprocessors. arXiv:1802.06002, 2018. doi: 10.48550/arXiv.1802.06002.\n\nJennifer R. Glick, Tanvi P. Gujarati, Antonio D. Corcoles, Youngseok Kim, Abhinav Kandala, Jay M. Gambetta, and Kristan Temme. Covariant quantum kernels for data with group structure. arXiv:2105.03406, 2021. doi: 10.48550/arXiv.2105.03406.\n\nVojtˇech Havlíˇcek, Antonio D. Córcoles, Kristan Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gambetta. Supervised learning with quantum-enhanced feature spaces. Nature, 567(7747):209–212, March 2019. doi: 10.1038/s41586-019-0980-2.\n\nValentin Heyraud, Zejian Li, Zakari Denis, Alexandre Le Boité, and Cristiano Ciuti. Noisy quantum\n\nkernel machines. arXiv:2204.12192, 2022. doi: 10.48550/arXiv.2204.12192.\n\nZoë Holmes, Kunal Sharma, M. Cerezo, and Patrick J. Coles. Connecting ansatz expressibility to gradient magnitudes and barren plateaus. PRX Quantum, 3(1), January 2022. doi: 10.1103/ prxquantum.3.010313.\n\nHsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, Hartmut Neven, and Jarrod R. McClean. Power of data in quantum machine learning. Nature Communications, 12(1), May 2021. doi: 10.1038/s41467-021-22539-9.\n\nThomas Hubregtsen, David Wierichs, Elies Gil-Fuster, Peter-Jan H. S. Derks, Paul K. Faehrmann, and Johannes Jakob Meyer. Training quantum embedding kernels on near-term quantum computers. arXiv:2105.02276, 2021. doi: 10.48550/arXiv.2105.02276.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJonas Kübler,\n\nSimon Buchholz,\n\nof quantum kernels. 2021. URL 69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf.\n\nand Bernhard Schölkopf.\n\ninductive bias 34, https://proceedings.neurips.cc/paper/2021/file/\n\nInformation Processing Systems,\n\nin Neural\n\nAdvances\n\nThe\n\nTengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels. arXiv:1908.10292, 2019. doi: 10.48550/ arXiv.1908.10292.\n\nYunchao Liu, Srinivasan Arunachalam, and Kristan Temme. A rigorous and robust quantum speed-up in supervised machine learning. Nature Physics, 17(9):1013–1017, July 2021. doi: 10.1038/ s41567-021-01287-z.\n\nJarrod R. McClean, Sergio Boixo, Vadim N. Smelyanskiy, Ryan Babbush, and Hartmut Neven. Barren plateaus in quantum neural network training landscapes. Nature Communications, 9(1), November 2018. doi: 10.1038/s41467-018-07090-4.\n\nM. Mezard and A. Montanari. Information, Physics, and Computation. Oxford University Press,\n\n2009. doi: 10.1093/acprof:oso/9780198570837.001.0001.\n\nK. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. Quantum circuit learning. Physical Review A, 98\n\n(3), September 2018. doi: 10.1103/physreva.98.032309.\n\nMichael A Nielsen and Isaac L Chuang. Quantum Computation and Quantum Information. Cam-\n\nbridge University Press, 2011. doi: 10.1017/CBO9780511976667.\n\nEvan Peters, João Caldeira, Alan Ho, Stefan Leichenauer, Masoud Mohseni, Hartmut Neven, Panagiotis Spentzouris, Doug Strain, and Gabriel N. Perdue. Machine learning of high dimensional data on a noisy quantum processor. npj Quantum Information, 7(1), November 2021. doi: 10.1038/s41534-021-00498-9.\n\nZbigniew Puchała and Jarosław Adam Miszczak. Symbolic integration with respect to the Haar measure on the unitary group. Bulletin of the Polish Academy of Sciences: Technical Sciences, 65 (1):21–27, 2017. doi: 10.1515/bpasts-2017-0003.\n\nLorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators. Journal of Machine Learning Research, 11(30):905–934, 2010. URL http://jmlr.org/papers/ v11/rosasco10a.html.\n\nBernhard Schölkopf, Alexander J Smola, Francis Bach, et al. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, 2002. doi: 10.7551/mitpress/ 4175.001.0001.\n\nMaria Schuld. Supervised quantum machine learning models are kernel methods. arXiv:2101.11020,\n\n2021. doi: 10.48550/arXiv.2101.11020.\n\nMaria Schuld and Nathan Killoran. Quantum machine learning in feature Hilbert spaces. Physical\n\nReview Letters, 122(4), February 2019. doi: 10.1103/physrevlett.122.040504.\n\nH. S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples.\n\nPhysical Review A, 45:6056–6091, Apr 1992. doi: 10.1103/PhysRevA.45.6056.\n\nJ. Shawe-Taylor, C.K.I. Williams, N. Cristianini, and J. Kandola. On the eigenspectrum of the Gram matrix and the generalization error of kernel-pca. IEEE Transactions on Information Theory, 51 (7):2510–2522, 2005. doi: 10.1109/TIT.2005.850052.\n\nRuslan Shaydulin and Stefan M. Wild. Importance of kernel bandwidth in quantum machine learning.\n\narXiv:2111.05451, 2021. doi: 10.48550/arXiv.2111.05451.\n\nDan J. Shepherd and Michael J. Bremner. Temporally unstructured quantum computation. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 465:1413–1439, 2009. doi: 10.1098/rspa.2008.0443.\n\nBernard W Silverman. Density estimation for statistics and data analysis. Routledge, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nThe PLAsTiCC team, Tarek Allam, Anita Bahmanyar, Rahul Biswas, Mi Dai, Lluís Galbany, Renée Hložek, Emille E. O. Ishida, Saurabh W. Jha, David O. Jones, Richard Kessler, Michelle Lochner, Ashish A. Mahabal, Alex I. Malz, Kaisey S. Mandel, Juan Rafael Martínez-Galarza, Jason D. McEwen, Daniel Muthukrishna, Gautham Narayan, Hiranya Peiris, Christina M. Peters, Kara Ponder, Christian N. Setzer, The LSST Dark Energy Science Collaboration, The LSST Transients, and Variable Stars Science Collaboration. The photometric LSST astronomical timeseries classification challenge (PLAsTiCC): Data set. arXiv:1810.00001, 2018. doi: 10.48550/ arXiv.1810.00001.\n\nJohn Watrous. The Theory of Quantum Information. Cambridge University Press, 2018. doi:\n\n10.1017/9781316848142.\n\nSau Lan Wu, Shaojun Sun, Wen Guan, Chen Zhou, Jay Chan, Chi Lung Cheng, Tuan Pham, Yan Qian, Alex Zeng Wang, Rui Zhang, Miron Livny, Jennifer Glick, Panagiotis Kl. Barkoutsos, Stefan Woerner, Ivano Tavernelli, Federico Carminati, Alberto Di Meglio, Andy C. Y. Li, Joseph Lykken, Panagiotis Spentzouris, Samuel Yen-Chi Chen, Shinjae Yoo, and Tzu-Chieh Wei. Application of quantum machine learning using the quantum kernel algorithm on high energy physics analysis at the LHC. Physical Review Research, 3(3), sep 2021. doi: 10.1103/physrevresearch.3.033221.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. arXiv:1708.07747, 2017. doi: 10.48550/arXiv.1708.07747.\n\nKaining Zhang, Min-Hsiu Hsieh, Liu Liu, and Dacheng Tao. Gaussian initializations help deep variational quantum circuits escape from the barren plateau. arXiv:2203.09376, 2022. doi: 10.48550/arXiv.2203.09376.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA REVIEW OF CLASSICAL AND QUANTUM DATA OPERATORS\n\nHere we discuss covariance operators and integral kernel operators for two types of data: classical real-valued vectors and finite-dimensional complex Hermitian operators. The goal is to relate the spectra of these operators in order to better understand how feature maps and distributions of input data combine to affect the learnability of a distribution.\n\nGiven a symmetric positive semidefinite kernel function k : Tk : L2(\n\n) is defined according to its action on f\n\nL2(\n\n)\n\nX\n\n→\n\nX\n\nX × X → X\nk(x, x′)f (x′)μ(dx′).\n\nL2(\n\n∈\n\nR, the integral kernel operator ) as\n\n(A.1)\n\n(Tkf )(x) =\n\n(cid:90)\n\nX\n\nA quantum kernel is defined as the inner product of two quantum states,\n\nk(x, x′) =\n\nρ(x), ρ(x′) ⟩\n⟨\n\nH = Tr(cid:8)ρ(x)ρ(x′)(cid:9) = Vec (cid:0)ρ(x)(cid:1)†\n\nVec (cid:0)ρ(x′)(cid:1) ,\n\nwhere we have introduced the vectorization map (e.g., Watrous (2018)) Vec : L(Cd) , which 1 column vector and where L(Cd) denotes the space of stacks rows of a d Cd. Throughout this section we will consider d-dimensional linear operators of the form Cd quantum states and operations (where d = 2n in the case of n qubits). We will frequently use the identity\n\nd matrix into a d2\n\n→\n\n→\n\n×\n\n×\n\nCd2\n\nVec (A)† Vec (B) =\n\n(cid:110)\n\nA†B\n\n(cid:111) ,\n\n= Tr\n\nA, B ⟨\n\n⟩\n\nA, B\n\nL(Cd).\n\n∀\n\n∈\n\nObserving that the RKHS of a kernel k is defined (see, e.g., Schuld (2021)) by functions of the form\n\nwe can rewrite Eq. A.1 as\n\nf (x) =\n\nρ(x), H\n\n, ⟩\n\n⟨\n\n(Tkf )(x) = Vec (cid:0)ρ(x)(cid:1)† (cid:90)\n\nVec (cid:0)ρ(x′)(cid:1) Vec (cid:0)ρ(x′)(cid:1)†\n\nVec (H) μ(dx′)\n\nX\n\n=\n\nVec (cid:0)ρ(x)(cid:1) , Σ Vec (H) ⟩\n⟨\n\nwhere we have defined the covariance operator,\n\n,\n\n(A.2)\n\nΣ =\n\n(cid:90)\n\nX\n\nVec (cid:0)ρ(x′)(cid:1) Vec (cid:0)ρ(x′)(cid:1)†\n\nμ(dx′) =\n\n(cid:90)\n\nX\n\nρ(x′)\n\n⊗\n\nρ(x′)T μ(dx′),\n\n(A.3)\n\n⟩⟨\n\nfor some\n\nψ(x) |\n\nψ(x) |\n\nand the last equality in Eq. A.2 holds whenever the quantum embedding is a pure state (i.e., ρ(x) = ψ(x) ). We can intuitively understand this operator by analogy to linear |\n⟩ Rn and assume x are feature maps: If we consider n-dimensional real-valued input vectors x centered such that EX [x] = 0, then the covariance operator is defined as Σ = EX [xxT ] such that (Σ)ij = E[xixj]. Equation A.3 therefore describes a kind of quantum covariance operator, although this is an imperfect analogy since a centered quantum feature map (EX [ρ(x)] = 0) would violate the requirement Tr(cid:8)ρ(x)(cid:9) = 1.\n\n∈ X ⊂\n\nWe are interested in the eigenvalue equations\n\nΣ Vec (H) = η Vec (H) (Tkφ)(x) = ηφ(x).\n\nGiven the operators defined in Eq. A.1 and Eq. A.3, one may verify (see, e.g., Shawe-Taylor et al. Vec (cid:0)ρ(x)(cid:1) described (2005)) that the following statements hold under the identification ψ(x) above.\n\n→\n\n(S1) For every eigenfunction φ satisfying (Tkφ)(x) = ηφ(x), there is a corresponding eigenvec-\n\ntor Vec (H) of Σ given by\n\nVec (H) =\n\n(cid:90)\n\nX\n\nφ(x) Vec (cid:0)ρ(x)(cid:1) μ(dx)\n\nsuch that Σ Vec (H) = η Vec (H). Furthermore, from the bijectivity of the Vec operation and the fact that Hermiticity is preserved under convex combination, it follows that H is Hermitian.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n(S2) For every eigenvector Vec (H) of Σ satisfying Σ Vec (H) = η Vec (H), there is a corre-\n\nsponding eigenfunction of Tk given by\n\nφ(x) = Tr(cid:8)ρ(x)H(cid:9) = Vec (cid:0)ρ(x)(cid:1)†\n\nVec (H)\n\nsuch that (Tkφ)(x) = ηφ(x).\n\n(S3) Assuming that eigenfunctions φk of Tk indexed by eigenvalue ηk are orthonormal, (cid:90)\n\nφk, φl\n\n⟩L2(X ) =\n\n⟨\n\nX\n\nφk(x)∗φl(x)μ(dx) = δkl,\n\nthe eigenvectors Vec (Hk) of Σ indexed according to eigenvalue ηk satisfy\n\nVec (Hk)† Vec (Hl) = Tr\n\n(cid:110)\n\nH †\n\nkHl\n\n(cid:111)\n\n= η−1\n\nk δkl\n\nwhenever ηk > 0 (when ηk = 0, we may safely ignore Vec (Hk) in the null space of Σ).\n\nLetting spec (A) denote the sequence of eigenvalues of an operator A sorted in nonincreasing order, it then follows from (S1) and (S2) that\n\nspec (Tk) = spec (Σ) .\n\nThe mean embedding is defined as the average state with respect to a distribution over\n\nρμ =\n\n(cid:90)\n\nX\n\nρ(x)μ(dx).\n\n:\n\nX\n\nAs shown in Kübler et al. (2021, Lemma 1), the inequality ηmax(Σ) provides a bound for the largest eigenvalue of Σ. Let n be the Bloch vector for ρμ defined on n = 1 qubits, which can be parameterized as\n\n≤\n\n(cid:113)\n\nTr(cid:8)ρμ\n\n(cid:9)2\n\nn = (sin θ cos φ, sin θ sin φ, cos φ)\n\n(A.4)\n\n(cid:110)\n\n(cid:111)\n\n∥\n\nn\n\nρ2 μ\n\n= (1 +\n\nThen Tr\n\n2)/2. This provides a geometric argument for the use of bandwidth in ∥\na single-qubit system (or product thereof). For instance, consider each shaded region in Fig. 1A representing the subspace spanned by quantum states associated with the feature map giving rise to k. 2)1/2/√2 Then we have that the maximum eigenvalue of Σ is suppressed like ηmax(Σ) as the centroid n of each region (representing ρμ) approaches the center of the Bloch sphere. By limiting bandwidth (and therefore restricting the shaded region of Fig. 1B to a polar cap of the Bloch sphere), the upper bound on ηmax(Σ) is lifted, and the possibility of learning on data is restored.\n\n(1 +\n\n≤\n\nn\n\n∥\n\n∥\n\nA.1 SPECTRUM OF A RANDOM QUANTUM EMBEDDING\n\nA foundational observation for this work is that using a kernel associated with a feature map that completely utilizes a high-dimensional feature space leads to poor learning guarantees due to the flatness of the corresponding spectrum. Here, we present an extreme (and somewhat contrived) example where a quantum feature map associated with random state vectors leads to a flat kernel U †(x) spectrum. We assume the existence of a data distribution and feature map x for which unitaries U (x) are sampled uniformly over the space of n-qubit unitaries U (2n). Such a distribution may be achieved by sampling uniformly with respect to the Haar measure μ(dU ) (e.g., Collins & Nechita (2016)). As described in Rosasco et al. (2010), the spectrum of K concentrates around the spectrum of Σ. We therefore explicitly compute the spectrum of Σ corresponding to random quantum features. Using standard results for integration with respect to the Haar measure (Puchała & Miszczak, 2017), we compute the average elements of Σ as\n\nU (x) |\n\n0 |\n\n→\n\n⟩⟨\n\n0\n\nΣ\n\nij ⟨\n\n|\n\n|\n\nkl\n\n⟩\n\n=\n\n(cid:90)\n\nU (2n)⟨\n\nρ(x) i\n|\n\nk |\n\nj\n\nρ(x) |\n\nl ⟩\n|\n\n⟩⟨\n\nμ(dU )\n\nUi0U ∗\n\nk0Uj0U ∗\n\nl0μ(dU )\n\n(cid:0)δikδjl + δilδjk\n\n(cid:1) .\n\n1)\n\n(A.5)\n\n(cid:90)\n\n=\n\n=\n\nU (2n) 2n −\n2n(22n\n\n1\n\n−\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nThe terms associated with δikδjl = 1 correspond to the identity operator I4n , while the terms δilδjk contribute to either 2n-many diagonal components Σii,ii for i = 1, . . . , 2n or 2n(2n 1)-many off-diagonal components Σij,ji whenever j\n\n= i:\n\n−\n\nΣ\n\n→\n\nI4n +\n\n2n (cid:88)\n\ni=1\n\nii\n\n⟩⟨\n\n+\n\nii\n\n|\n\n|\n\n2n (cid:88)\n\n(cid:88)\n\ni=1\n\nj̸=i\n\nij\n\nji\n\n. |\n\n⟩⟨\n\n|\n\n(A.6)\n\nEquation A.6 is therefore a direct sum of subspaces proportional to 2I2 and subspaces proportional to I2 + X2, the latter of which may be easily diagonalized. Combining with Eq. A.5 yields the spectrum\n\n(cid:40)\n\nspec (Σ) =\n\n0\n\n2\n\n2n(2n+1) with multiplicity 2n + 2n(2n−1)\n\nwith multiplicity 2n(2n−1)\n\n2\n\n2 .\n\nCoincidentally, the mean embedding associated with this feature map is proportional to the identity; that is, ρμ = EX [ρ(x)] = I/2n.\n\nB SPECTRUM OF THE BANDWIDTH-EQUIPPED KERNEL\n\nHere we derive the spectrum of the integral operator for kernel discussed in the main text. We follow the technique of diagonalizing Σ described in Appendix A and used in Kübler et al. (2021, Appendix C). We first derive the spectrum for the single-qubit (one-dimensional data) case. Then we leverage the observation that the kernel factorizes over the qubits to obtain the full spectrum for the general n-dimensional case.\n\nWe begin by considering the input x by the unitary\n\n∈\n\nUniform[\n\nπ, π]. Recall that the feature map ρ(x) is generated\n\nU (x) = cos\n\n− (cid:18) cx (cid:19) 2\n\nI + i sin\n\n(cid:19)\n\n(cid:18) cx 2\n\nX.\n\nThen, the vectorization of the image of a data point x in feature space ρ(x) = U (x)\n\n0\n\n0 |\n\n|\n\n⟩ ⟨\n\nVec (cid:0)ρ(x)(cid:1) =\n\n\n\n \n \n\ni cos (cid:0) cx −\ni cos (cid:0) cx\n\n2\n\ncos2 (cid:0) cx\n\n2\n\n(cid:1) (cid:1) (cid:1) sin (cid:0) cx 2\n(cid:1) (cid:1) sin (cid:0) cx (cid:1)\n\n2\n\n2\n\nsin2 (cid:0) cx\n\n2\n\n\n\n \n \n\n,\n\nand the corresponding kernel is\n\nk(x, x′) = Vec (cid:0)ρ(x)(cid:1)†\n\nVec (cid:0)ρ(x′)(cid:1) = cos2\n\n(cid:18)\n\nc\n\n(x\n\n− 2\n\n(cid:19)\n\nx′)\n\n.\n\nOur goal is to compute the eigenvalues of the integral operator Tk defined as (cid:90)\n\n(Tkφk)(x) =\n\nμ(dx′)k(x, x′)φk(x′) = λkφk(x).\n\nU (x)† is\n\n(A.7)\n\n(A.8)\n\nHere we refer to single-qubit eigenvalues as λk and many-qubit eigenvalues as ηk. As described in Appendix A, this is equivalent to computing the eigenvalues of the covariance operator Σ given by\n\n2 ) sin( cy 2 ) sin2( cy 2 ) sin2( cy 2 ) sin3( cy\n\n2 ) −i cos3( cy 2 ) − cos2( cy 2 ) cos2( cy 2 ) −i cos( cy\n\n2 ) cos2( cy 2 ) sin( cy 2 ) sin2( cy 2 ) −i cos( cy 2 ) sin2( cy 2 ) i cos( cy 2 ) 2 ) sin3( cy\n\n2 ) sin2( cy 2 ) 2 ) sin3( cy 2 ) 2 ) sin3( cy 2 ) 2 )\n\nsin4( cy\n\n\n\n \n\n\ndy\n\nΣ =\n\n=\n\n=\n\n(cid:90)\n\nX\n\n1 2π\n\n\n\n \n\n\nVec (cid:0)ρ(y)(cid:1) Vec (cid:0)ρ(y)(cid:1)†\n\nμ(dy)\n\n\n\n \n\n\n(cid:90) π\n\n−π\n\ncos4( cy\n\n2 ) 2 ) sin( cy 2 ) sin( cy 2 ) sin2( cy\n\n−i cos3( cy i cos3( cy cos2( cy\n\ni cos3( cy 2 ) cos2( cy 2 ) − cos2( cy 2 ) i cos( cy \n\na1 0\n0 a2\n\n0 a2 a2 −\n0\n\n0 a2 −\na2 0\n\na2 0\n0 a3\n\n,\n\n \n\n\n15\n\n̸ Under review as a conference paper at ICLR 2023\n\nwhere\n\na1 =\n\na2 =\n\na3 =\n\n+\n\n3 8\n1 8 − 3\n8 −\n\n1 2\n1 8\n1 2\n\nsinc(cπ) +\n\nsinc(2cπ)\n\nsinc(cπ) +\n\n1 8\n\n1 8\n\nsinc(2cπ)\n\nsinc(2cπ).\n\nWe can easily obtain the eigenvalues of this matrix, which are\n\n+\n\nλ1 =\n\nλ2 =\n\n3 8\n1 4 − 3\n8 λ4 = 0,\n\nλ3 =\n\n+\n\n1 8\n1 4\n1 8\n\nsinc(2πc) +\n\nsinc(2πc)\n\nsinc(2πc)\n\n−\n\n1 8\n\n1 8\n\n(cid:112)(1\n\n−\n\n(cid:112)(1\n\n−\n\nsinc(2πc))2 + 16sinc(πc)2\n\nsinc(2πc))2 + 16sinc(πc)2\n\n(A.9)\n\n≥\n\nλ3 > λ4 for all c\n\n[0, 1]. With c = 1, the eigenvalues become 1\n\nwhere λ1 > λ2 4 , 0, respectively. Now we can examine the impact of bandwidth on the eigenvalues (see Eq. A.9) of the integral operator. We first observe that for c 0, all eigenvalues except the top eigenvalue become 0. This confirms our intuition about the impact of bandwidth zero; that is, λ1 on the spectrum of the integral operator. This also implies that for small c the approximate dimension of the space spanned by the training data will be 1, which is consistent with the observation that in this limit the feature maps become constant.\n\n1 and λ2, λ3, λ4\n\n2 , 1\n\n4 , 1\n\n→\n\n→\n\n→\n\n∈\n\nFor an n-qubit system and an input data distribution that factorizes over the dimensions, the kernel simply becomes the direct product of the n copies of the single-qubit system. Since the n qubits are completely decoupled, the resulting kernel in Eq. 6 has eigenvalues of the form\n\nηn1n2n3n4 = λn1\n\n1 λn2\n\n2 λn3\n\n3 λn4\n\n4 , n1 + n2 + n3 + n4 = n, n1, n2, n3, n4\n\nZ+\n\n0\n\n, }\n\n∪ {\n\n∈\n\nwhere the nonzero eigenvalues are obtained by setting n4 = 0 since λ4 = 0. Here, we note that the number of zero eigenvalues grows exponentially with number of qubits as 4n 3n. However, its ratio to the total number of eigenvalues vanishes since\n\n−\n\n#\n\nηn1n2n3n4 = 0 {\nηn1n2n3n4 } #\n\n{\n\n}\n\n4n\n\n=\n\n3n\n\n− 4n\n\nn→∞ −−−−→\n\n0;\n\ntherefore the bulk of the spectrum remains nonzero.\n\nFor c where the eigenvalues are given by\n\nn(1), the spectrum remains flat as n\n\n∼ O\n\n. This can be easily seen with the case c = 1\n\n→ ∞\n\nηk = 2−n2−k,\n\n(cid:1), and of course (cid:80)n\n\n. Each eigenvalue ηk is degenerate with N (n, k) = where k = n2 + n3 and takes values in 2k(cid:0)n k=0 N (n, k) = 3n gives the number of nonzero eigenvalues. To obtain a nonflat spectrum, we need eigenvalues to scale with the number of qubits n. In the next section, we do so by imposing scaling conditions on the eigenvalues.\n\n0, . . . , n\n\n}\n\n{\n\nk\n\nFor completeness, we also provide the unnormalized eigenfunctions of the kernel using the eigenvectors of Σ. Inverting the Vec operation, we get the matrices\n\nH1 =\n\nH3 =\n\n(cid:32) 4sinc(πc)+√(1−sinc(2πc))2+16sinc(πc)2\n\n1−sinc(2πc) 0\n(cid:32) 4sinc(πc)−√(1−sinc(2πc))2+16sinc(πc)2\n\n1−sinc(2πc) 0\n\n(cid:33)\n\n0 1\n\n(cid:33)\n\n0 1\n\n, H2 =\n\n, H4 =\n\n(cid:18)0 1\n\n(cid:18)0 1\n\n(cid:19)\n\n1 −\n0\n\n(cid:19)\n\n1 0\n\n.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nThe corresponding eigenfunctions are given by φ1(x) = Tr(cid:0)ρ(x)Hi\n\n(cid:1) and become\n\nφ1(x) = sin2\n\n(cid:18) cx 2\nφ2(x) = i sin(cx) (cid:18) cx 2\n\nφ3(x) = sin2\n\nφ4(x) = 0.\n\n(cid:19)\n\n(cid:19)\n\n+ cos2\n\n(cid:18) cx 2\n\n(cid:19) 4sinc(πc) + (cid:112)(1 1\n\n+ cos2\n\n(cid:18) cx 2\n\n(cid:19) 4sinc(πc)\n\n(cid:112)(1 1\n\n−\n\n− −\n\n− −\n\nsinc(2πc))2 + 16sinc(πc)2 sinc(2πc)\n\nsinc(2πc))2 + 16sinc(πc)2 sinc(2πc)\n\nSetting c = 1, we obtain the eigenfunctions given in Kübler et al. (2021): φ1(x) = 1, φ2(x) = i sin(x), φ3(x) = cos(x), and φ4 = 0. Note that unlike the c = 1 case, the eigenfunction φ1(x) in general is not constant.\n\n−\n\nB.1 SCALING RESTRICTIONS TO THE BANDWIDTH\n\nThe argument given by Kübler et al. (2021, Theorem 1) is that when the largest eigenvalue of the kernel is sufficiently small compared with the sample size, the generalization error is lower bounded by the L2 norm of the target function with probability at least 1\n\nε as\n\nEg\n\n(1\n\n−\n\n≥\n\nε)(cid:13)\n\n(cid:13) ̄f (cid:13) 2\n(cid:13)\n\n= (1\n\nε)\n\n−\n\n−\n\n(cid:88)\n\n ̄a2 k,\n\nk\n\nwhich matches our result from Sec. 4. The result in Kübler et al. (2021) depends on the exponentially small largest eigenvalue of the kernel compared with the amount of training samples. In fact, from nl and exponentially Eq. 9 it easily follows that for a polynomial number of training samples P suppressed largest eigenvalue ηmax . Kübler et al. (2021, Lemma 1) proves that the largest eigenvalue of the kernel is upper bounded by the so-called purity:\n\n2−n, the learning is impossible as n\n\n∼ → ∞\n\n∼\n\nηmax\n\n(cid:112)Mμ,\n\n≤ where purity is given by Mμ = (cid:82) μ(dx)μ(dx′)k(x, x′). We also demonstrate their proof here for the reader’s convenience. Consider a normalized kernel satisfying k(x, x) = 1 for all x. The normalized eigenfunction φmax(x) corresponding to the largest eigenvalue ηmax is L2 bounded by the constant function η−1/2\n\nmax 1(x) since\n\n1 = k(x, x) > ηmaxφmax(x)2.\n\nHere 1(x) = 1 is the constant function. This immediately implies that\n\n(cid:90)\n\nηmax =\n\nμ(dx)μ(dx′)k(x, x′)φmax(x)φmax(x′)\n\n(cid:90)\n\nη−1\n\nmax\n\n≤\n\nμ(dx)μ(dx′)k(x, x′)1(x)1(x′) = η−1\n\nmaxMμ.\n\nGiven this bound, we demand that the bandwidth should scale such that the purity stays constant with respect to the number of qubits n. For our example kernel, this condition translates into\n\nMμ =\n\n1 2n\n\n(cid:0)1 + sinc(πc(n))(cid:1)n\n\n.\n\nInverting this equation is not possible. However, its numerical solution yields a scaling for bandwidth as\n\nc(n) =\n\n,\n\n(A.10)\n\na √n\n\n∼ O\n\nwhere a n(1) depends on the fixed purity Mμ. Note that this is a lower bound for the bandwidth to keep purity from inversely scaling with n. In principle, we could allow purity to increase with n depending on the target function. For example, c(n) = an−∞ 0 yields perfect purity since there is only a single mode. Hence, we conclude that the bandwidth should at least scale as\n\n→\n\nWe remark, however, that the spectrum will collapse to a single mode for large exponents α and generalization will not be possible except for very specific target functions.\n\nc = an−α, α\n\n1 2\n\n.\n\n≥\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nB.2 SCALING OF EIGENVALUES WITH BANDWIDTH\n\nUsing the bandwidth scaling derived in Eq. A.10, we now study the scaling of eigenvalues at the large qubit limit n\n\n. Asymptotically, the kth power of these eigenvalues looks like\n\n→ ∞\n\nλk\n\n1 ≈\n\n1\n\n−\n\na2π2 6n\n\nk +\n\n(cid:19)\n\n(cid:18) 1 n2\n\nO\n\n(cid:32)\n\n(cid:32)\n\nλk\n\n2 ≈\n\nλk\n\n3 ≈\n\na2π2 6n\n\n(cid:33)k (cid:32)\n\n1\n\n−\n\na2π2 5n\n\nk +\n\n(cid:18) 1 n2\n\nO\n\n(cid:19)(cid:33)\n\na4π4 180n2\n\n(cid:33)k (cid:32)\n\n1 +\n\na2π2 42n\n\nk +\n\n(cid:18) 1 n2\n\nO\n\n(cid:19)(cid:33)\n\n.\n\nNotice that with this scaling of the bandwidth parameter, the largest eigenvalue remains constant asymptotically. We also remark that eigenvalues composed of a large number (k n) of λ2 and λ3 scale as e−n log n, and we consider them decoupled since none of these modes can be learned with a polynomial amount of data. Hence, we conclude that the spectral bias induced by the bandwidth restricts the space of learnable functions to lie in the space spanned by eigenfunctions of the form\n\n≈\n\nψ1 |\n\n⊗(n−n2−n3) ⟩\n\nψ2 |\n\n⊗n2 ⟩\n\nψ3 |\n\n⊗n3 , ⟩\n\n≪\n\nn. It can be shown that the hierarchy of eigenvalues obtained in this way scale such that n2 + n2 polynomially with n, as discussed in Sec. 4. In Table B.2, we present the first few eigenvalue scalings where N (n, k) denotes the number of eigenvalues with scaling ηk,z denotes the corresponding states. We denote each eigenvalue ηk,z with two indices: k corresponds to the scaling of the eigenvalue as n−k, and z = 1, . . . , N (n, k) indexes the N (n, k) eigenvalues with the same scaling.\n\n(n−k) and\n\nΨ |\n\n∼ O\n\n⟩\n\nTable 3: Degeneracies N (n, k) of quantum states for the first few scalings.\n\nDegenerate States and Spectrum Scaling\n\nScaling n−k n0 n−1 n−2 n−3 n−4\n\nDegeneracy N (n, k)\n\nStates\n\nΨ |\n⊗n\n\n⟩\n\n1 (cid:0)n 1\n(cid:0)n 2\n(cid:0)n 3\n(cid:0)n 4\n\n(cid:1)\n\n(cid:1) (cid:1) + (cid:0)n (cid:1) + (cid:0)n−1 (cid:1)+(cid:0)n−1\n\n1\n\n1\n\n2\n\n(cid:1)(cid:0)n 1\n(cid:1)(cid:0)n 1\n\n(cid:1) (cid:1)+(cid:0)n\n\n2\n\nψ1 |\nψ1 |\nψ1 |\nψ1 |\nψ1 |\n\n(cid:1)\n\n⊗(n−1)\n\n⊗(n−2)\n\n⟩ ⟩\n⟩ ⊗(n−3) ⟩\n⊗(n−4) ⟩\n\nψ2 |\nψ2 |\nψ2 |\nψ2 |\n\n⟩ ⊗2 , ⟩\n⊗3 , ⟩\n⊗4 , ⟩\n\nψ1 ψ1 ψ1\n\n| |\n|\n\n⊗(n−1) ⟩\n⊗(n−2) ⟩\n⊗(n−3) ⟩\n\nψ3 |\nψ2 |\nψ2 |\n\nψ3\n\n⟩ ⟩ | ⊗2 ⟩\n\n⟩ ψ3 |\n\n,\n\n⟩\n\nψ1 |\n\n⟩\n\n⊗(n−2)\n\nψ3\n\n|\n\n⊗2 ⟩\n\nB.3 BANDWIDTH AND PROJECTED (BIASED) KERNELS\n\nAn alternative way to control the inductive bias of the quantum model is to define the kernel in terms of the reduced density matrix (e.g., single-qubit): ̃ρ(x) = Tr[1...n−1] (ρ(x)), where Tr[1...n−1] ( )\n· denotes the partial trace over qubits 1, . . . , n 1 of a n-qubit system. For a detailed discussion of such kernels, the interested reader is referred to Kübler et al. (2021); Huang et al. (2021). Here, we briefly comment on the similarities and differences between the impacts of projection and bandwidth tuning on the spectrum of the kernel.\n\n−\n\nAs shown in (Kübler et al., 2021, Theorem 2), the spectrum of a generic projected kernel has one constant (with n) eigenvalue, and the rest are exponentially small with n. Contrast that with the spectrum of the bandwidth-equipped kernel given in Table B.2. Similarly to projected kernels, in bandwidth-equipped kernels the first eigenvalue stays constant as the number of qubits grows. However, the spectrum decay behavior is different, since the eigenvalues decay polynomially and not exponentially with n. This leads to a qualitatively different inductive bias, which may be beneficial in some settings.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nB.4 BANDWIDTH AND TRAINABILITY OF QUANTUM NEURAL NETWORKS\n\nThe phenomenon of flat spectrum of the quantum kernels is deeply connected to the barren plateaus phenomenon in quantum neural networks (QNNs) (McClean et al., 2018) since both stem from the exponential dimensionality of the space in which the classical data points are embedded (Kübler et al., 2021; Holmes et al., 2022). “Barren plateaus” in the context of QNNs refers to the gradients of the loss function becoming exponentially small with the number of qubits for sufficiently deep QNNs because of the loss function concentrating around its mean in high-dimensional quantum Hilbert space. Notably, a mechanism analogous to rescaling the bandwidth has been observed to enable training of quantum neural networks. Zhang et al. (2022) show that if the parameters are initialized from a Gaussian distribution with zero mean and variance O( 1 L ) for circuits of depth L, the barren plateaus are provably avoided. Rescaling the initialization of trainable parameters avoids barren plateaus by limiting the effective dimensionality of the subspace of the quantum Hilbert space being used. This is analogous to how scaling down of the data controls the bandwidth and the spectrum of quantum kernels, with the connection coming from the equivalency between quantum kernel methods and infinitely deep QNNs (Schuld, 2021).\n\nC GENERALIZATION ERROR IN KERNEL RIDGE REGRESSION\n\nIn this section we review the theoretical generalization error curves for kernel ridge regression developed by Bordelon et al. (2020); Canatar et al. (2021) and extend our results to the cases with a nonzero ridge parameter and noise on the labels. For kernel machines, a reproducing kernel Hilbert defines a set of functions over which the empirical loss function is minimized. Consider space H\nμ=1, where the inputs are drawn i.i.d. from a distribution p(x) on a training set and the labels are generated through a target function ̄f (x) as yμ = ̄f (xμ) + εμ, where x\nεμ (0, σ2) is an additive noise with variance σ2. Then the predictor is given by minimizing the empirical mean-squared-error over\n\n∈ X ∼ N\n\nxμ, yμ\n\n=\n\nD\n\n{\n\n}\n\nP\n\n:\n\nH\n\nf ∗(x) = arg min\n\nf ∈H\n\n1 2\n\nP (cid:88)\n\nμ=1\n\n(cid:0)f (xμ)\n\nyμ(cid:1)2\n\n+\n\n−\n\nλ 2 ∥\n\nf\n\n∥\n\n2\n\nH,\n\n(A.11)\n\nwhere λ is the ridge parameter regularizing the Hilbert norm of the predictor. Associated with the is a positive semi-definite kernel k(x, x′) satisfying the reproducing property: RKHS\n\nH\n\nk(x, ⟨\n\n) ), f ( ·\n·\n\n⟩H = f (x),\n\nwhere eigenvalue problem with respect to the input distribution p(x):\n\n·⟩H is the Hilbert inner product on\n\n. A basis for\n\n, ⟨·\n\nH\n\nH\n\ncan be obtained by solving the integral\n\n(cid:90)\n\nk(x, x′)φk(x′)p(x′)dx′ = ηkφk(x),\n\n(cid:90)\n\nφk(x)φl(x)p(x)dx = δkl,\n\nφk(x)\n\nis a basis for L2(\n\n) with respect to the distribution p(x). A normalized basis for the\n\nwhere RKHS\n\n{ H\n\n}\n\nis obtained by the features ψk(x)\n\n≡ )\n), ψl( ·\n· With these bases, the kernel can be decomposed as\n\nψk(\n\n⟨\n\nX\n\n√ηkφk(x) that satisfy\n\n⟩H = δkl.\n\nk(x, x′) =\n\n(cid:88)\n\nk\n\nηkφk(x)φk(x′) =\n\nψk(x)ψk(x′).\n\n(cid:88)\n\nk\n\nFurthermore, any target function in L2(\n\nX\n\n) can be decomposed as\n\n ̄f (x) =\n\n(cid:88)\n\n ̄akφk(x) =\n\nk\n\n(cid:88)\n\nk\n\n ̄ak √ηk\n\nψk(x),\n\n2\n\nH =\n\nf\n\n∥\n\n∥\n\n ̄a2 k\nηk\n\n.\n\n(cid:88)\n\nk\n\n. In the case where K(x, x′) has Note that a function belongs to the RKHS only if zero eigenvalues while the function f has components along the corresponding eigenfunctions, this function is said to be out-of-RKHS (since ), and a kernel machine can learn only the components along the nonzero eigenvalues. We show that the formula for generalization error in\n\n∥H <\n\n∥H =\n\n∞\n\n∞\n\n∥\n\n∥\n\nf\n\nf\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nCanatar et al. (2021, Eq. 4) also extends to out-of-RKHS targets by appropriately taking the ηk limit.\n\n0\n\n→\n\nGiven the P Eq. A.11 is\n\n×\n\nP kernel Gram matrix Kμν = k(xμ, xν), the solution to the kernel regression problem\n\nf ∗(x) = k(x)⊤ (K + λI)−1 y, where k(x) is a P -dimensional vector with components k(x)μ = k(x, xμ) and yμ = yμ is a P - dimensional vector of the labels. Then generalization error, as function of number of training samples P and the dataset\n\n, is defined as\n\nD\n\nEg(P,\n\n) =\n\nD\n\n(cid:90)\n\n(cid:0)f ∗(x)\n\n−\n\n ̄f (x)(cid:1)2\n\np(x)dx.\n\nHowever, the dependency of this quantity to the particular choices of datasets of size P makes it )\nanalytically intractable. Instead, we are interested in the averaged generalization error ⟩D over the datasets of size P , which has been calculated using replica theory in Canatar et al. (2021).\n\nEg(P, ⟨\n\nD\n\nAs a function of number of training samples P , ridge parameter λ, and variance of label noise σ2 as well as the kernel eigenvalues ⟩D becomes (Canatar et al., 2021):\n\nand the target weights\n\n, the result for\n\nEg(P,\n\n ̄ak\n\nηk\n\nD\n\n{\n\n}\n\n}\n\n{\n\n)\n\n⟨\n\nEg(P ) =\n\nκ2\n\n−\n\nγ\n\n1\n\n(cid:88)\n\nk\n\nwhere\n\n ̄a2 k\n\n(P ηk + κ)2 + σ2 γ\n\n1\n\n−\n\n,\n\nγ\n\n(A.12)\n\nκ = λ + κ\n\n(cid:88)\n\nk\n\nηk P ηk + κ\n\n,\n\n(cid:88)\n\nγ =\n\nk\n\nP η2 k\n(P ηk + κ)2 .\n\nHere κ is to be solved self-consistently, and it acts as an effective ridge parameter that depends on the kernel eigenvalues and number of training samples. Even in the absence of an explicit ridge parameter (i.e. λ = 0), implicit regularization prevents the predictor from having large variance.\n\nC.1 DERIVATION OF EQ. 9 IN MAIN TEXT\n\nIn Sec. 4 of the main text and in Appendix B, we have shown that the top eigenvalues ηk,z of the (n−k), kernel in Eq. 6 scale polynomially with the number of input dimensions, that is, ηk,z where the index k = 1, 2, . . . represents different scalings and index z = 1, . . . , N (n, k) represents the degenerate modes in scaling k. Note that the number of degenerate modes N (n, k) grows as (1). We also decomposed the target function onto the kernel\n\n(nk) such that ̄ηk\n\nN (n, k)ηk\n\n∼ O\n\nO basis as\n\n≡\n\n∼ O\n\n(cid:90)\n\n ̄ak,z =\n\n ̄f (x)φk,z(x)p(x)dx\n\nand defined ̄a2 we get\n\nk ≡\n\n(cid:80)N (n,k) z=1\n\n ̄a2\n\nk,z as the total weight at scaling k. Plugging these quantities in Eq. A.12,\n\nEg(P ) =\n\nκ2\n\n−\n\nγ\n\n1\n\nN (n,k) (cid:88)\n\n(cid:88)\n\nk\n\nz=1\n\n ̄a2\n\nk,z\n\n(P ηk,z + κ)2 + σ2 γ\n\n1\n\n−\n\n,\n\nγ\n\nκ = λ + κ\n\nN (n,k) (cid:88)\n\n(cid:88)\n\nk\n\nz=1\n\nηk,z P ηk,z + κ\n\n,\n\nγ =\n\nk,z\n\nN (n,k) (cid:88)\n\n(cid:88)\n\nP η2 (P ηk,z + κ)2 . ηk,z′ for all z, z′ since, in large n limit, modes n(1) quantity. Hence, we drop the index z.\n\nz=1\n\nk\n\nFurthermore, we made the approximation that ηk,z ≈\nin the same scaling differ from each other with some Then, in terms of ̄ηk\n\nN (n, k)ηk, generalization error simplifies to\n\nO\n\n≡\n\nEg(P ) =\n\nκ = λ + κ\n\nκ2\n\n(cid:88)\n\nγ\n\n1\n\n− (cid:88)\n\nk ̄ηk αk ̄ηk + κ\n\nk\n\n ̄a2 k\n\n(αk ̄ηk + κ)2 + σ2 γ\n\n1\n\n,\n\nγ\n\n,\n\nγ =\n\n(cid:88)\n\nk\n\n− αk ̄η2 (αk ̄ηk + κ)2 ,\n\nk\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nwhere we define αk to scale with\n\nN (n,k) denoting the learning stage. Now, consider the number of samples P the quantity αk becomes\n\n(nl) for some integer l. Since N (n, k)\n\n(nl), as n\n\n≡\n\nP\n\nO\n\n→ ∞\n\n∼ O ,\n∞\n\n(1),\n\nαk<l = αk=l αk<l\n\n∼ O 0.\n\n≈\n\nTherefore, in the large n limit, generalization error simplifies greatly and becomes\n\nwhere ̃σ2 = σ2 + (cid:80) \n\nEg(P ) =\n\nκ2\n\n1\n\nγ\n\n ̄a2 l\n\n(αl ̄ηl + κ)2 + ̃σ2 γ\n\n1\n\nγ\n\n(cid:88)\n\n+\n\n ̄a2 k,\n\nγ =\n\nαl ̄η2 (αl ̄ηl + κ)2 ,\n\nl\n\n− k>l ̄a2 k is the effective noise and κ has an explicit solution:\n\n−\n\nk>l\n\nκ =\n\n1\n\n2 ( ̃λ + ̄ηl\n\n1\n\n2 ( ̃λ + ̄ηl\n\n−\n\n−\n\n(cid:32)\n\n(cid:114)\n\n ̄ηlαl)\n\n1 +\n\n1 +\n\n4 ̃λ ̄ηlαl ( ̃λ+ ̄ηl− ̄ηlαl)2\n\n(cid:32)\n\n(cid:114)\n\n ̄ηlαl)\n\n1\n\n−\n\n1 +\n\n4 ̃λ ̄ηlαl ( ̃λ+ ̄ηl− ̄ηlαl)2\n\n(cid:33)\n\n(cid:33)\n\n1 + ̃λ/ ̄ηl\n\n,\n\n1 + ̃λ/ ̄ηl\n\nαl\n\nαl\n\n≤\n\n≥\n\n \n\nwhere ̃λ = λ + (cid:80) k>l ̄ηk is the effective ridge parameter and describes the implicit regularization of the kernel model. Note that the total power beyond mode-l acts as label noise and also irreducible error.\n\nC.2 OUT-OF-RKHS TARGET FUNCTIONS AND LABEL NOISE\n\nWe treat the case where target function has out-of-RKHS components by setting eigenvalues with indices in an index set\n\nin the generalization error formula to zero; that is, ηk∈I = 0:\n\nI\n\nκ2\n\n−\n\nκ2\n\n−\n\nγ\n\nγ\n\n1\n\n1\n\n(cid:88)\n\nk\n\n(cid:88)\n\nk̸∈I\n\nEg(P ) =\n\n=\n\nwhere κ and γ are again\n\n ̄a2 k\n\n(P ηk + κ)2 + σ2 γ\n\n1\n\nγ\n\n−\n\n σ2 +\n\n ̄a2 k\n(P ηk + κ)2 +\n\n\n\n ̄a2\n\nk\n\n\n\n(cid:88)\n\nk∈I\n\nγ\n\n−\n\n1\n\n+\n\nγ\n\n ̄a2 k,\n\n(cid:88)\n\nk∈I\n\nκ = λ + κ\n\n(cid:88)\n\nk̸∈I\n\nηk P ηk + κ\n\n,\n\nγ =\n\nP η2 k\n(P ηk + κ)2 .\n\n(cid:88)\n\nk̸∈I\n\nNotice that target power placed on the modes corresponding to zero eigenvalues act both as label noise and irreducible error (Canatar et al., 2021). This implies that the inaccessible modes in target function due to small training set sizes simply lie outside the effective RKHS defined by the accessible, large eigenvalues. This also implies that very large bandwidths can also impair generalization; for c\n0 only a single non-zero eigenvalue survives and therefore generic targets which may have many modes corresponding to the remaining zero eigenvalues lie outside the effective RKHS. Therefore, the extreme case of very large bandwidth also creates a problem. This is just a restatement of the well-known bias-variance trade-off.\n\n≈\n\nD BANDWIDTH MAKES BOUNDS ON GENERALIZATION VACUOUS\n\nThe following is a sketch of how the main theorem of Kübler et al. (2021) becomes vacuous (“fails”) in a certain context where we can guarantee that k(x, x′) is lower bounded by some bandwidthdependent constant.\n\nWe first note that this cannot be a positive proof: nothing about lower bounding k(x, x′) can provide a guarantee on classifier accuracy. Rather, this scenario just shows a context in which a lower bound on the ridge regression classifier accuracy based on the largest eigenvalue ηmax is no longer effective at showing a failure of quantum kernel methods.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nNow, we suppose that we can use bandwidth to require that states are “not too far away” in Hilbert space. More specifically, we suppose that there is some function ∆c such that\n\nk(xμ, xν)\n\n∆c.\n\n≥\n\n(A.13)\n\nNote that this does not interfere with the requirement that k is Lμ 2 integrable since we assume that k π, π]n (without this assumption, k is not integrable in a more is defined on a restricted support x general treatment). We will show that some choice of ∆c always ensures that the largest eigenvalue of K (and therefore the largest eigenvalue of Tk with high probability) is bounded. Denote the eigenvalues of K as ηk, and in particular let ηmax be the largest eigenvalue of K and Ku = ηmaxu RP , and in for some eigenvector u particular\n\nRP . By definition, ηmax = max∥v∥=1⟨\n\nfor all v\n\nv, Kv\n\n−\n\n∈\n\n∈\n\n∈\n\n⟩\n\n[\n\nηmax = ⟨\n\nu, Ku u, u ⟩\n⟨\n\n⟩\n\n≥\n\n⟨\n\n1, K1 1, 1 ⟨\n\n⟩\n\n⟩\n\n=\n\n1 P\n\nP (cid:88)\n\nμ,ν=1\n\nKμν\n\n(P\n\n≥\n\n−\n\n1)∆c,\n\nwhere 1 is the vector of all ones and we have applied the inequality of Eq. A.13. Proposition 10 of Rosasco et al. (2010) states that\n\n\n\nPr\n\n sup\n\nk\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nηk P −\n\n(cid:12) (cid:12) (cid:12) (cid:12) ≤\n\n2\n\nγk\n\n(cid:115)\n\nlog(cid:0)4/δ2(cid:1) P\n\n\n\n  ≥\n\n1\n\n−\n\nδ,\n\nwhere we contrast the empirical eigenvalues ηk with the eigenvalues γk of the integral operator Tk. Note that the empirical eigenvalues ηk δ it holds that\n\n. Hence, with probability at least 1\n\nγk as P\n\n−\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nηmax\n\n−\n\n→ (cid:18) P\n\n(cid:19)\n\n1\n\n∆c\n\n− P\n\n(cid:115)\n\n→ ∞ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤\n\n2\n\nlog(cid:0)4/δ2(cid:1) P\n\n.\n\n(A.14)\n\nTheorem 1 of Kübler et al. (2021) states that, with probability at least 1 risk for KRR with penalty λ and P training data is lower bounded by\n\nε\n\n−\n\n−\n\nηmaxP 4, the empirical\n\n(cid:32)\n\n(cid:114)\n\nRemp(f λ\n\nm)\n\n1\n\n−\n\n≥\n\n(cid:33)\n\n2ηmaxP 2 ε\n\nf\n\n∥\n\n2.\n\n∥\n\n(A.15)\n\nFrom Eq. A.14, however, we see that ηmax approaches a constant ∆c at a rate of O(1/√P ); for sufficiently large P , Eq. A.15 holds with vanishing probability, and for all other choices of P the bound becomes vacuous with high probability under the condition that (cid:112)2ηmaxP 2/ε 1, which may be achieved within O(1/√P ) precision by choosing (cid:114) ε\n\n≥\n\nP\n\n∆c\n\n≥\n\nP\n\n1\n\n−\n\n2P 2 .\n\nImportantly, this outcome does not guarantee that KRR using k satisfying the inequality in Eq. A.13 will achieve good generalization error. In particular, the choice ∆c = 1 will result in Tk having a single nonzero eigenvalue associated with the constant function. Rather, this demonstration reemphasizes that there are two conditions to successful classification using quantum kernels: (i) the kernel should be chosen such that the eigenfunctions of Tk align with the target function, and (ii) the corresponding eigenvalues should be large. By lower bounding k in this way, we can guarantee condition (ii); however, successful generalization will still depend on a choice of k satisfying condition (i).\n\nE NUMERICAL METHODS\n\nE.1 EXPERIMENTS WITH TOY MODEL\n\nIn Figs. 1 and 2 in the main text, we consider the toy kernel k(x, x′) = (cid:81)n varying bandwidth parameter c and n-dimensional input data x\n\nfor π, π]n and drawn uniformly\n\ni=1 cos\n\nc (xi−x′ i)\n\n2\n\n(cid:16)\n\n(cid:17)\n\n[ −\n\n∈\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\n(Kübler et al., 2021). We generate a dataset of size P by uniformly sampling P input points and computing the corresponding labels using a target function ̄f (x). We denote the vector of labels by RP and denote the kernel Gram matrix by K whose elements are Kμν = k(xμ, xν). We obtain ̄y the eigenvalues and eigenvectors of the kernel by solving the empirical eigenvalue problem:\n\n∈\n\n1 P\n\nP (cid:88)\n\nν=1\n\nKμνΦν,k = ηkΦμ,k,\n\n1 P\n\nP (cid:88)\n\nμ=1\n\nΦμ,kΦμ,l = δkl,\n\nP\n\nηk\n\nwhere Φμ,k is the matrix of eigenvalues whose columns are the orthonormal eigenvectors and k=1 are the eigenvalues. Note that we obtain at most P eigenmodes with P samples and hence k {\nruns from 1, . . . , P . Finally, we obtain the target weights by projecting the targets on the eigenvectors of K:\n\n}\n\na =\n\n1 P\n\nΦ⊤ ̄y.\n\nP\n\n}\n\n{\n\nηk\n\nk=1 and target weights a, we directly compute the generalization error by Using the eigenvalues plugging them in Eq. A.12. To perform the experiments, we used the Kernel Generalization code by Canatar et al. (2021) and utilized a single NVIDIA V100 GPU with 32 GB of RAM. In Fig. E.1, we present the same experiment as Fig. 2 but for different input dimensions n. We find that the optimal bandwidth parameter is c∗ = 2/n. Therefore, the optimal scaling of the bandwidth parameter is (n−α) for α = 1 in this special case. Note that bandwidth changes both the eigenvalues and the O\neigenfunctions of the kernel that affect spectral bias and task-model alignment, respectively. For certain tasks, faster decaying bandwidths might improve the task-model alignment and hence yield better generalization.\n\nFigure A.1: Generalization error as a function of the number of training samples computed by using both theory (solid lines) and performing kernel ridge regression empirically (dots). The target function is ̄f (x) = e−∥x∥2/n2 π, π]n) for n = 20, 40, 80, 200. Bandwidth c = 1 yields a constant learning curve. While all c < 1 kernels provide improvement, there is an optimal bandwidth parameter c∗ 2/n that gives the best task-model alignment. Regularization with a small ridge parameter λ = 10−10 is applied for numerical stability.\n\n, and data is drawn uniformly from Unif([\n\n−\n\n≈\n\n23\n\n101102103P108106104102100Eg(P)n = 20c = 1e+00c = 1e-01c = 5e-02c = 3e-02c = 1e-02101102103P1010108106104102100Eg(P)n = 40c = 1e+00c = 1e-01c = 5e-02c = 3e-02c = 1e-02101102103P1011109107105103101Eg(P)n = 80c = 1e+00c = 1e-01c = 5e-02c = 3e-02c = 1e-02101102103P10131011109107105103101Eg(P)n = 200c = 1e+00c = 1e-01c = 5e-02c = 3e-02c = 1e-02Under review as a conference paper at ICLR 2023\n\nE.2 BANDWIDTH IN QUANTUM MACHINE LEARNING ARCHITECTURES\n\nWe use the experimental data provided by Shaydulin & Wild (2021).1 We use the kernel given by the instantaneous quantum polynomial-time (IQP) circuit feature map (Shepherd & Bremner, 2009; Havlíˇcek et al., 2019; Huang et al., 2021) and Hamiltonian evolution circuit (EVO) feature map(Huang et al., 2021; Shaydulin & Wild, 2021) and real datasets FMNIST (Xiao et al., 2017), KMNIST (Clanuwat et al., 2018), and PLAsTiCC (The PLAsTiCC team et al., 2018).\n\nSince the dimensionality of the datapoints in these datasets is too large (e.g., 784 for FMNIST and KMNIST) and leads to quantum circuits that cannot be simulated by using available tools, the inputs were downsized to 22-dimensions by using PCA (Shaydulin & Wild, 2021). Following Shaydulin & Wild (2021); Huang et al. (2021), we consider a binary classification problem where for each dataset, only two classes were chosen (see Shaydulin & Wild (2021) for the details on data preprocessing).\n\nFor n-dimensional inputs, the quantum circuit used to compute the n-qubit IQP kernel is given by\n\nUIQP(x) = UZ(x)H ⊗nUZ(x)H ⊗n, UZ(x) = exp\n\n\n\nc\n\nn (cid:88)\n\nj=1\n\nxj Zj + c2\n\nn (cid:88)\n\nj,j′=1\n\n\n\nxjxj′ Zj Zj′\n\n ,\n\nwhere H is the Hadamard gate and Z is the Pauli Z-gate (see Havlíˇcek et al. (2019)). This unitary acts ⊗n. Then on the n-qubit ground state to embed an input xμ to a quantum state = UIQP(xμ) the resulting feature map is given by ρIQP(xμ) = with the corresponding quantum kernel KIQP(xμ, xν) = Tr (cid:0)ρIQP(xμ)ρIQP(xν)(cid:1). For n-dimensional inputs, the quantum circuit used to compute the EVO kernel (Huang et al., 2021; Shaydulin & Wild, 2021) has n + 1 qubits and is given by\n\nxμ |\n\nxμ |\n\n0 ⟩\n\nxμ\n\n⟩ ⟨\n\n⟩\n\n|\n\n|\n\nUEVO(x) =\n\nn (cid:89)\n\nj=1\n\ne−icxij (Xj Xj+1+Yj Yj+1+Zj Zj+1).\n\nHere, c parameterizes time evolution and corresponds to the bandwidth of the resulting kernel. The initial (n + 1)-qubit state is given by\n\n=\n\nΨ0\n\n|\n\n⟩\n\nn+1 (cid:79)\n\nj=1\n\n,\n\nψj |\n\n⟩\n\nψj where each |\n2021). Then the quantum embedding of a sample xμ is ρEVO(xμ) =\n\nΨ0 |\nand kernel KEVO(xμ, xν) = Tr (cid:0)ρEVO(xμ)ρEVO(xν)(cid:1).\n\nis randomly generated with respect to a single-qubit Haar measure (Huang et al., with the feature map\n\n= UEVO(xμ)\n\n⟩ xμ\n\nxμ\n\nxμ\n\n⟩\n\n⟩\n\n|\n\n|\n\n⟩ ⟨\n\n|\n\nIn both cases, the resulting kernel is conjectured to be intractable to compute analytically, and both models utilize Hilbert spaces that are exponentially large in the number of qubits n. These quantum circuits were simulated in Shaydulin & Wild (2021) using Qiskit (Abraham et al., 2019) software to compute the kernel Gram matrices on the data. Then the resulting kernels were used to perform SVM for the binary classification task.\n\nThe datasets were split into 800 training sets and 200 test sets. Each input was downsized to 22 dimensions using PCA, which leads to 222- and 223-dimensional Hilbert spaces for IQP and EVO circuits, The code for accessing and processing the data was obtained at https://github.com/rsln-s/ Importance-of-Kernel-Bandwidth-in-Quantum-Machine-Learning/.\n\nrespectively (Shaydulin & Wild, 2021).\n\nKERNEL RIDGE REGRESSION WITH REALISTIC QUANTUM KERNELS\n\nApart from the classification task shown in the main text, we also use the same quantum kernels to perform kernel ridge regression on real data as shown in Figure A.2. We again find that there is an optimal bandwidth for both kernels signaled by low test loss.\n\n1The code and the data are publicly provided by Shaydulin & Wild (2021) at https://github.com/\n\nrsln-s/Importance-of-Kernel-Bandwidth-in-Quantum-Machine-Learning/.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nFigure A.2: Kernel Ridge Regression performed with k-fold cross-validation and λ = 0.1 ridge parameter yields the optimal bandwidth. The vertical axis shows the test loss of the estimator when evaluated on held-out data.\n\nOPTIMAL BANDWIDTH IN REALISTIC QUANTUM KERNELS\n\nWe have obtained the optimal bandwidths through k-fold cross-validation for the SVM task. In Figure A.3, we report our results for each kernel method on all three datasets. Furthermore, we present the eigenvalues and target weights corresponding to both quantum model on all three dataset domains in Figure A.4 and Figure A.5, respectively. We find that the spectrum in all cases are flat without the bandwidth, and that the bandwidth improves the spectral properties in all cases. However, the task alignment with the quantum kernels depends significantly on the choice of the dataset, and it remains poor even with the bandwidth cure. This is in agreement with our arguments that bandwidth does not guarantee generalization, but only enables a model to potentially generalize if the target is suitable.\n\nHere, we also empirically show that the optimal bandwidth scales inversely with the number of qubits n as c∗ 0.5. Using the data provided by Shaydulin & Wild (2021), we first extract the maximum kernel eigenvalue for the IQP kernel on FMNIST dataset. The IQP kernel is evaluated for various input dimensions n (also the number of qubits) and various bandwidth parameters.\n\n(n−α), where α\n\n∼ O\n\n≥\n\nFirst, we normalize each top eigenvalue ηmax(n) as a function of n with the maximum eigenvalue corresponding to the smallest qubit size ηmax(n0), and define the quantity\n\n ̃ηmax(n) =\n\nηmax(n) ηmax(n0)\n\n,\n\nwhere n0 = 4 in this case. In Figure A.6a, we plot these normalized eigenvalues against the number of qubits, and we fit exponential curves to extrapolate the behavior for large qubits which are not accessible experimentally. As the number of qubits increase, the maximum eigenvalue corresponding to c = 1 kernel falls much faster than the kernels with c < 1 bandwidth as expected. Note that in Appendix B we found that the bandwidth prevents the maximum eigenvalues to fall exponentially fast in n using our toy model, and this is also what we observe here.\n\nNext, we consider a fixed eigenvalue η0 = 0.8 line in Figure A.6a, and in Figure A.6b we analyze where this line intersects each of the normalized eigenvalues ̃ηmax(n) corresponding to different bandwidth parameters c. Similar to our calculation in Appendix B for the toy kernel, we aim to find the scaling of the bandwidth with the number of qubits such that the maximum eigenvalue stays constant. By identifying the intersection points in Figure A.6b, we obtain a relation between the optimal bandwidth and the number of qubits as shown in Figure A.6c. In this case, we numerically find that the optimal bandwidth scales as:\n\nc∗(n)\n\n∝\n\nn−0.506.\n\n25\n\n(A.16)\n\nUnder review as a conference paper at ICLR 2023\n\nThis is in agreement with the bound for the exponent we derived in Eq. A.10. Finally, in Figure A.7 and Figure A.8, for both models when evaluated on the FMNIST data, we show the empirical scaling of the optimal bandwidth to keep their top eigenvalues constant for varying η0’s. Again, we find that the decay exponent never violates α\n\n0.5.\n\n≥\n\nFigure A.3: SVM performed with k-fold cross-validation yields the optimal bandwidth.\n\nFigure A.4: Eigenvalues of the quantum kernels are always flat when the bandwidth is not tuned.\n\n26\n\n103102101100Bandwidth c5×1016×1017×1018×1019×101Test ScoresIQP_fmnist2-fold-cv5-fold-cv10-fold-cvtest_scores103102101100Bandwidth c5×1016×1017×1018×1019×101Test ScoresIQP_kmnist2-fold-cv5-fold-cv10-fold-cvtest_scores103102101100Bandwidth c7×1018×101Test ScoresIQP_plasticc2-fold-cv5-fold-cv10-fold-cvtest_scores103102101100Bandwidth c5×1016×1017×1018×1019×101Test ScoresEVO_fmnist2-fold-cv5-fold-cv10-fold-cvtest_scores103102101100Bandwidth c5×1016×1017×1018×1019×101Test ScoresEVO_kmnist2-fold-cv5-fold-cv10-fold-cvtest_scores103102101100Bandwidth c6.6×1016.8×1017×1017.2×1017.4×1017.6×101Test ScoresEVO_plasticc2-fold-cv5-fold-cv10-fold-cvtest_scores100101102103k1011108105102Eigenvalue kIQP_fmnistc=0.01c=0.05c=0.1c=0.5c=1.0100101102103k1010107104101Eigenvalue kIQP_kmnistc=0.01c=0.05c=0.1c=0.5c=1.0100101102103k1011108105102Eigenvalue kIQP_plasticcc=0.01c=0.05c=0.1c=0.5c=1.0100101102103k1011108105102Eigenvalue kEVO_fmnistc=0.01c=0.05c=0.1c=0.5c=1.0100101102103k1010107104101Eigenvalue kEVO_kmnistc=0.01c=0.05c=0.1c=0.5c=1.0100101102103k1011108105102Eigenvalue kEVO_plasticcc=0.01c=0.05c=0.1c=0.5c=1.0Under review as a conference paper at ICLR 2023\n\nFigure A.5: Task-model alignment improves with bandwidth tuning. Note that a decaying eigenspectrum is not enough alone for generalizability. Empirically, we find that bandwidth improves both the eigenspectrum and the task-model alignment.\n\nFigure A.6: Empirical scaling of the optimal bandwidth with the number of qubits.\n\n27\n\n100101102103k0.40.60.81.0C(k)IQP_fmnistc=0.01c=0.05c=0.1c=0.5c=1.0100101102103k0.50.60.70.80.91.0C(k)IQP_kmnistc=0.01c=0.05c=0.1c=0.5c=1.0100101102103k0.50.60.70.80.91.0C(k)IQP_plasticcc=0.01c=0.05c=0.1c=0.5c=1.0100101102103k0.00.20.40.60.81.0C(k)EVO_fmnistc=0.01c=0.05c=0.1c=0.5c=1.0100101102103k0.20.40.60.81.0C(k)EVO_kmnistc=0.01c=0.05c=0.1c=0.5c=1.0100101102103k0.40.60.81.0C(k)EVO_plasticcc=0.01c=0.05c=0.1c=0.5c=1.0a)c)b)Under review as a conference paper at ICLR 2023\n\nFigure A.7: IQP Kernel: Empirical scaling of the optimal bandwidth with the number of qubits for n−α. all η0. Note that the empirical scaling never exceeds α = 0.5 for the optimal bandwidth c∗\n\n≈\n\nFigure A.8: EVO Kernel: Empirical scaling of the optimal bandwidth with the number of qubits for n−α. all η0. Note that the empirical scaling never exceeds α = 0.5 for the optimal bandwidth c∗\n\n≈\n\n28",
    "reference": "# Summary Of The Paper\n\nThis paper studied the failure of the generalization of the quantum kernel method by focusing on the major cosine kernel function. The authors proposed introducing the bandwidth to control the spectrum of the cosine kernel. The authors derived the formula of the spectrum for that kernel. Then the authors showed that introducing the bandwidth improves the generalization by combining the generalization error bound of the existing work and deriving the spectrum formula.\n\n# Strength And Weaknesses\n\n# Strength\n- The authors rigorously evaluated the eigenvalues of cosine kernel under the large qubit settings and showed how the bandwidth affects the spectrum. Then the authors also derived the scaling laws of the spectrum. As far as I know, the derived scaling laws are new and meaningful.\n- Combined with the spectrum, the authors derived the generalization error of the kernel regression task and showed that introducing the bandwidth improves the generalization. \n\n# Weakness\n- The idea of using bandwidth is not new.\n- The generalization analysis is not novel; it just substitutes the derived spectrum formula into the Theorem in Canatar 2021.\n- The analysis and numerical experiments are not consistent. The generalization analysis focuses on the regression task, but numerical experiments are the classification task. Also, the authors used different kernel functions from the theoretical analysis. Thus, it is hard to judge how significant the theoretical analysis is in a real-world application.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI think the layout of tables and figures should be improved. For example, the caption of Table 2 and Figure 3 are hard to distinguish from the main text.\n\n# Summary Of The Review\n\nAs I wrote in the weakness, the contribution of this paper is not significant and novel. Using the bandwidth is not new and the derived generalization error bound is the simple application of existing work. The derived scaling law of the spectrum seems new and interesting, but the numerical experiments are conducted with different kernel functions from the theory; thus hard to see the usefulness of the theoretical analysis.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "GEASS: NEURAL CAUSAL FEATURE SELECTION FOR HIGH-DIMENSIONAL BIOLOGICAL DATA\n\nMingze Dong Yale University mingze.dong@yale.edu\n\nYuval Kluger Yale University yuval.kluger@yale.edu\n\nABSTRACT\n\nIdentifying nonlinear causal relationships in high-dimensional biological data is an important task. However, current neural network based causality detection approaches for such data suffer from poor interpretability and cannot scale well to the high dimensional regime. Here we present GEASS (Granger fEAture Selection of Spatiotemporal data), which identifies sparse Granger causal interacting features of high dimensional spatiotemporal data by a single neural network. GEASS maximizes sparsity-regularized modified transfer entropy with a theoretical guarantee of recovering features with spatial/temporal Granger causal relationships. The sparsity regularization is achieved by a novel combinatorial stochastic gate layer to select sparse non-overlapping feature subsets. We demonstrate the efficacy of GEASS in several synthetic datasets and real biological data from single-cell RNA sequencing and spatial transcriptomics.\n\n1\n\nINTRODUCTION\n\nAdvances in single-cell omics research enable full characterizations of high-dimensional gene dynamics in biological systems on a either temporal or spatial scale. An example for the temporal case is single-cell RNA sequencing (scRNA-seq) trajectories, where cells are sampled from a dynamical biological process, sequenced, and ordered based on either real sampled time or inferred pseudo-time (Cannoodt et al., 2016; Saelens et al., 2019). Gene dynamics along the specified cell order encodes information of causal regulation for the underlying biological process. An example for the spatial case is single-cell level spatial transcriptomics (e.g. SeqFISH+ (Eng et al., 2019), Merfish (Fang et al., 2022)), in which cells from a tissue slice are sequenced with their spatial coordinates preserved (Moses and Pachter, 2022; Rao et al., 2021; Palla et al., 2022). Spatial profiling allows investigations of the cellular interplay, corresponding to conditional gene expression change caused by neighborhood phenotypic states. However, despite the potential significance, data-driven causal discovery for such data remains largely unexplored, especially for the spatial omics data.\n\nIdentifications of causal regulatory patterns in such data can be reformulated into the general task of causal feature selection in observational data with intrinsic structures, e.g. spatial data or temporal data. Identifications of causal interactions in time-series has lead to valuable findings in multiple disciplines, including but not limited to, economy, climate science, and biology (Hoover, 2006; Kami ́nski et al., 2001; Runge et al., 2019a).\n\nLearning directed causal relationships in temporal/spatial data is feasible as time and space both induce asymmetric dependencies. In the case of time-series data, a feature in the future cannot have effect on past values of other features. For spatial data, a similar definition of causal dependency can be established (Herrera Gómez et al., 2014).\n\nThe concept of Granger causality is proposed in order to uncover the assymetric causal dependency (Granger, 1969; Shojaie and Fox, 2022). In time-series data, this would translate to identifying one variable’s causal relationship with other variables based on how well the historical observations of other variables can predict the variable’s present value. The application of Granger causality in a spatial context corresponds to predicting significant relationships between neighboring observations of other variables and the specified variable (Mielke et al., 2020), which is a key insight used in recent works aimed to discover cellular interaction patterns in spatial omics data (Fischer et al., 2021; Valdés-Sosa et al., 18).\n\n1\n\nIn the nonlinear regime, information-theoretic measures such as directed information, transfer entropy (Schreiber, 2000), and partial transfer entropy (Staniek and Lehnertz, 2008), are used as a counterpart of linear Granger causality. Moreover, some works consider modeling conditional independence (CI) in time-series data to identify the underlying causal graph (Entner and Hoyer, 2010; Malinsky and Spirtes, 2018; Moneta et al., 2011; Runge et al., 2019a; Pfister et al., 2019; Mastakouri et al., 2021). Two examples are VarLINGAM (Hyvärinen et al., 2010) and PCMCI (Runge et al., 2019b), which are generalizations of LINGAM (Shimizu et al., 2006) and PC (Spirtes et al., 2000) respectively. Finally, multiple recent works have proposed to use neural network approaches to model the nonlinear Granger causality, including MLP, LSTM, and neural-ODE based approaches, resulting in improved prediction power for nonlinear time-series dynamics (Li et al., 2017; Tank et al., 2021; Nauta et al., 2019; Yin and Barucca, 2022; Bellot et al., 2021).\n\nDespite the success of these methods in various systems of interest, multiple challenges limit their use in high-dimensional biological datasets.\n\n• Although linear methods (LINGAM, linear Granger causality) have succeeded in various settings and can potentially scale to high feature numbers, these methods may completely fail when the feature dependency in data is highly complex and nonlinear.\n\n• As the number of conditional independencies generally scales exponentially or at least polynomially with the feature size, applying causal discovery methods which are based on CI tests to high-dimensional data is not realistic. Distinctively, Granger-causality based methods are built with a prediction model for each feature in the data. The time complexity of solving the stacked prediction model for all features is of polynomial level with respect to the feature size.\n\n• In previous methods, the number of causal edges between features is assumed to be sparse (edge sparsity) to maximize interpretability of the identified causal graph. However, in biological data, there exists a large proportion of nuisance features. Also, one functional gene may activate a large number of downstream genes in neighboring cells. Sparsifying the number of interacting features (feature sparsity) has the potential to improve causal discovery in biological systems, which remains to be explored.\n\n• While a large number of methods are designed for causal discovery in time-series data, only a limited number of present works aim for causal discovery in general graph-structured data. Time-series based methods cannot be directly adopted on data with multi-branch trajectory dynamics or spatial structures.\n\nOur contribution. In this work, we present GEASS (Granger fEAture Selection of Spatiotemporal data), which identifies causally interacting features of high dimensional temporal / spatial data by a single neural network. GEASS considers the aforementioned feature sparsity instead of edge sparsity, thus selects most significant interacting features for downstream causal discovery. Our contributions are three-folds.\n\n1. Instead of direct causal discovery in data, we formulate the task as two steps of causal feature selection and causal graph identification. We provide a novel solution of causal feature selection problem in general graph-structured data by the use of modified transfer entropy maximization with theoretical guarantees.\n\n2. In order to solve our proposed optimization problem, we design a novel combinatorial stochastic gate layer to select non-overlapping sparse feature sets with a newly designed initialization procedure.\n\n3. We demonstrate the power of our method by benchmarking it on both temporal data and spatial data of multiple settings. Our method gives accurate and robust causal feature identification and reveals novel biology in real datasets.\n\n1.1 RELATED WORKS\n\nNeural Granger causality. Despite the large body of work based on linear Granger causal discovery, neural Granger causality still remains an active area of research. Various neural network architectures, such as MLP, sequential model, and attention-based architecture (Tank et al., 2021; Nauta et al., 2019; Khanna and Tan, 2019; Sun et al., 2021), have been proposed for nonlinear Granger causality\n\n2\n\ndiscovery. A recent work uses the information of proxy variable to learn latent confounder for Granger causality by a dual-decoder neural network (Yin and Barucca, 2022). One recent biologyoriented work extends the definition of Granger causality to DAGs, where the use of a linear graph neural network is proposed to model underlying Granger causality (Wu et al., 2021). Meanwhile, a neural-ODE based approach has been proposed to reformulate the Granger causality problem in terms of local dependence graph identification (Bellot et al., 2021).\n\nCausal feature selection. The task of causal feature selection has been considered by multiple groups. Most works in this category uses constraint-based methods to identify each feature’s causal relation with all other features, equivalent of identifying the whole causal graph structure, including VARLINGAM, tsFCI, SVAR-FCI, and PCMCI (Hyvärinen et al., 2010; Entner and Hoyer, 2010; Malinsky and Spirtes, 2018; Moneta et al., 2011; Runge et al., 2019a). Meanwhile, seqICP focus on identifying the direct or indirect cause for each feature assuming sufficient interventions in the dataset (Pfister et al., 2019). SyPI tackles the causal feature selection problem without the assumption of causal sufficiency and avoids issues in multi-hypothesis testing by construction of the correct conditional set (Mastakouri et al., 2021). Finally, Guo et al. (2022) considers dual correction of causal feature selection to control both false positive rates and false negative rates.\n\n2 MODIFIED TRANSFER ENTROPY (MTE)\n\nIn order to tackle the issue that a neural network may overfit each model therefore overestimates the number of causal interactions, we need a prediction-free loss function that directly indicates causal signficance. In this work, we propose a novel function, modified transfer entropy (mTE), based on transfer entropy (Schreiber, 2000) as a metric of causal interaction significance.\n\nTransfer entropy is a information-theoretic measure of cross dependence (Schreiber, 2000). Consider two vectorized time series xt and yt for t ∈ 1, ..., T . In a Markovian model, the transfer entropy from x to y at time t is defined as the mutual information between the present value xt and the future value yt+1, conditioning on yt to eliminate possible autocorrelation: TEt(x, y) = I(xt; yt+1|yt).\n\nBy the use of mutual information, transfer entropy is able to model general nonlinear dependencies beyond linear Granger causality. In this work, we further consider the generalization of transfer entropy on graph structured xi and yi, where i denotes a vertex on the data graph G = (V, E):\n\nTEi(x, y) := I(xi; yN (i)|yi), where N (i) := {j|(i, j) ∈ E}.\n\n(1)\n\nNote here the graph can be either directed (the time-series case) or undirected (the spatial case). In this study, we introduce a novel function, modified transfer entropy, that enables the application of bivariate transfer entropy for causal discovery in high-dimensional data. Our key insight is to consider two feature subsets in the dataset that maximizes the mutual information difference: Definition 2.1. Let X = [x1x2 . . . xn] ∈ Rp×n be a matrix containing graph-structured vector series xi, with i as vertices of the data graph G = (V, E). Suppose S1 and S2 be two subsets of {1, 2, ..., p}. The modified transfer entropy mTEi(S1, S2) and its maximum mTE∗\n\ni are defined by\n\nmTEi(S1, S2) := I(xi\n\nS1\n\n; xN (i)\n\nS2\n\n) − I(xi\n\nS1\n\n; xi\n\nS2\n\n); mTE∗\n\ni := max S1,S2\n\nmTEi(S1, S2).\n\n(2)\n\nNote the mTE function requires strictly stronger dependence than the analogically defined transfer entropy TEi(S1, S2), as shown by the proposition below (The proof can be seen at Appendix A.1): Proposition 2.2. ∀S1, S2 ⊂ {1, ..., p}, mTEi(S1, S2) > 0 ⇒ TEi(S1, S2) > 0.\n\n1 , S∗\n\n2 ) be one of the maximizers with the smallest size of |S1 ∪ S2|, and denote S∗ := S∗\n\n1 ∪ S∗ Let (S∗ (note (S∗ 2 ) may not be unique). Under some mild assumptions listed below, we are able to provide the theoretical justification for mTE maximization in the time-series setting (Theorem 2.4). A proof can be seen in Appendix A.3.\n\n1 , S∗\n\n2\n\nAssumptions:\n\nA1-A3 Causal Markov assumption, faithfulness, and causal sufficiency for the causal graph.\n\nA4 Ergodicity and Stationarity of the stochastic process defined by the causal graph, meaning the ensemble average equals time average, and the functional relationships encoded by the causal graph do not change by time (or location). This also leads to mTEi(S1, S2) is constant across i.\n\n3\n\nA5 DAG causal graph: We assume X T = [t1, ..., tm, um+1, ..., up] up to a permutation, where ti are causally interacting features forming a directed acyclic graph (DAG), and uk are nuisance features that may correlate with ti. An illustration based on the time series setting can be seen in Figure 1.\n\nui 4\n\nti 1\n\nti 2\n\nti 3\n\nui+1\n\n4\n\nti+1\n\n1\n\nti+1\n\n2\n\nti+1\n\n3\n\ncorrelates\n\ncauses\n\ncauses\n\nu4\n\nt2\n\nt1\n\nt3\n\nFigure 1: Dependence graph for a single forward step (left) and the underlying causal graph (right).\n\nA6 Interaction regularity: Given two disjoint feature sets A, B, such that A is a subset of the parent features of B or B is a subset of child features of A. Then conditioning on any other feature set C such that I(Ai, BN (i)|C i), I(Ai, BN (i)|C N (i)) > 0, we have:\n\n∀i, min{I(Ai, BN (i)|C i), I(Ai, BN (i)|C N (i))} > I(Ai, Bi|C i). Remark 2.3. Here our only additional assumption from prevalent literatures (Pearl, 2009; Spirtes et al., 2000) is A6, which aims to filter out features with spurious causations and regularize the algorithmic complexity of causal interactions, thus enabling information-theoretic analysis. A6 has direct connections with the concept of conditional transfer entropy (Faes et al., 2016; Shahsavari Baboukani et al., 2020); further discussions can be seen at Appendix A.2. Theorem 2.4. Given A1-A6, S∗ := (S∗ described in A5). Moreover, each feature in S∗ is connected to other features in the set S∗.\n\n2 ) ⊆ {1, ..., m} (the index set of true interacting features\n\n1 ∪ S∗\n\n(3)\n\n3 NEURAL OPTIMIZATION OF MODIFIED TRANSFER ENTROPY\n\nWith Theorem 3.1 stated below, we are able to give a theoretical guarantee of the l0-penalized optimization of mTE. A proof can be seen at Appendix A.4. Here ⊙ stands for the Hardmard product. Theorem 3.1. Assume A1-A6 holds and f, g, h define one-to-one mappings on X ⊙1S1(for f ) or X ⊙ 1S2 (for g, h). Then ∃λ > 0, such that for (4), any solution (S∗ 2 ) ⊆ {1, ..., m}. Moreover, each feature in S∗ is connected to other features in the set.\n\n2 ) satisfies S∗ := (S∗\n\n1 ∪ S∗\n\n1 ∪ S∗\n\nmin f,g,h,S1,S2\n\n−(I(f (xi ⊙ 1S1); h(xN (i) ⊙ 1S2 )) − I(f (xi ⊙ 1S1 ); g(xi ⊙ 1S2 ))) + λ|S1 ∪ S2| (4)\n\nRemark 3.2. The estimation of mutual information by various approaches is an active field itself (Belghazi et al., 2018; Hjelm et al., 2018; McAllester and Stratos, 2020; Zhang et al., 2019). In contrast, by this theorem, we show that an accurate estimation of the transfer entropy (such as in (Zhang et al., 2019)) may not be needed as optimizing the upper bound of the modified transfer entropy automatically gives the best feature subset selection. Remark 3.3. Our theoretical guarantee is derived based on one-to-one embeddings f, g, h. In a neural network, the injectivity may be enforced with various architecture designs yet may not perfectly hold. Empirically, we have found that the optimization of mTE is robust to the embedding injectivity, compared with the original transfer entropy. This is due to our stricter design of the mTE function (Proposition 2.2) and is further illustrated by our experiments in the next section.\n\nGiven Theorem 3.1, we are able to construct a neural network for optimizing the proposed loss function. However, the estimation of mutual information is not directly tractable. In this case, because mutual information is invariant by one-to-one transforms, we can restrict the function class of f, g, h in the optimization problem (4) as flows transforming the original feature distributions into Gaussian distributions with fixed dimensionality. We are able to formulate the target for neural network optimization by the explicit formula for mutual information between Gaussians: I(X, Y ) = 2 log det ΣX det ΣY 1\n. The Gaussian regularization can be applied either by regularizing over the discrepancy between embedding distributions [f, g, h] and Gaussian distributions or by applying a adversarial training procedure. In this work, we have implemented the former approach, constructing means and covariance matrices for the concatenated embedding as learnable parameters and minimize the cross entropy between target distributions and the parametrized Gaussian distributions.\n\ndet Σ[X,Y ]\n\n4\n\n3.1 COMBINATORIAL STOCHASTIC GATES\n\nIn order to solve the optimization problem, we need to learn two sparse sets S1, S2, which involves combinatorial optimization, making the task impractical for high-dimensional data. To overcome this issue, we use a stochastic gate based approach (Yamada et al., 2020; Lindenbaum et al., 2021), which performs probabilistic relaxation of deterministic l0 norms. In order to explicitly construct S1 and S2 by stochastic gates, we define two random vectors T 1 and T 2 ranging in [0, 1] with lengths equal to the feature number, with each element independently sampled from STG distribution defined as: T i d is a parameter trainable by reparametrization (Miller et al., 2017; Figurnov et al., 2018).\n\ni ) is i.i.d. sampled with fixed variance and μi\n\nd = max(0, min(1, μi\n\nd)), where εi\n\nd ∼ N (0, σ2\n\nd + εi\n\nThe new loss function applying stochastic gates can be formulated as:\n\nET 1,T 2 − [ ˆI(f ( ̃XS1 ); h(W ̃XS2)) − ˆI(f ( ̃XS1 ); g( ̃XS2 ))] +\n\np (cid:88)\n\nd=1\n\n[λ1P(T 1\n\nd > 0) + λ2P(T 2\n\nd ∈ (0, 1))],\n\ns.t.\n\n ̃XS1 = X ⊙ T 1 ⊙ T 2,\n\n ̃XS2 = X ⊙ T 1 ⊙ (1 − T 2).\n\nHere ˆI is defined as the empirical Gaussian mutual information: ˆI(X, Y ) = 1\n\n2 log det ˆΣX det ˆΣY\n\ndet ˆΣ[X,Y ]\n\n(5)\n\n, and\n\nW is defined as the graph diffusion operator: W xi = xN (i). In our construction, T 1 controls the sparsity of feature selection, while T 2 controls the expectation of overlap between ̃XS1 and ̃XS2. Denoting the Gaussian error function as erf(), the regularization term for the first layer is of form: p\n(cid:88)\n\np (cid:88)\n\nP(T 1\n\nd > 0) =\n\nd=1\n\ni=1\n\n(\n\n1 2\n\n−\n\nerf(\n\n1 2\n\nμ1 d√\n\n2σ1\n\n)).\n\n(6)\n\nThe regularization term for the second layer can be expressed as:\n\np (cid:88)\n\nd=1\n\nP(T 2\n\nd ∈ (0, 1)) =\n\np (cid:88)\n\nd=1\n\nP(T 2\n\nd > 0) − P(T 2\n\nd ≥ 1) =\n\n1 2\n\np (cid:88)\n\nd=1\n\n(erf(\n\nμ2 d√\n\n2σ2\n\n) − erf(\n\nμ2 d − 1 √\n2σ2\n\n)).\n\n(7)\n\nWe are able to show strong consistency for our stochastic-gate based feature selection scheme by the theorem below (A proof can be seen at Appendix A.5): Theorem 3.4. Assume A1-A6 and f, g, h are one-to-one Gaussian embeddings as described above. For the optimal solution of (5), denote a sample of stochastic gate as T 1, T 2 and denote the ground truth interacting feature set as S, then there exists λ1, λ2 > 0 for (5) such that as n → ∞,\n\n∀i ∈ {0, 1}, P(Bi ⊆ S) a.s.−−→ 1, where Bi := {d|T 1\n\nd > 0, T 2\n\nd = i}.\n\n(8)\n\nIn practice, we also have observed the method’s solution highly depends on the stochastic gate initialization. Here we provide a heuristic initialization scheme that shows superior empirical performance. Details of the initialization scheme can be seen in Appendix B.\n\n3.2 PROPOSED NETWORK ARCHITECTURE\n\nOur proposed network architecture is summarized in Figure 2. For an input dataset X ∈ Rp×n and its corresponding graph adjacency matrix A ∈ Rn×n, we first pass each feature through two sequential stochastic gate layers T 1, T 2. The l0 penalty is conducted on the first STG layer, while the second STG layer is regularized with the 0-1 penalty, consistent with the descriptions in the previous section.\n\ni = 1 − T 2\n\nAfter passing each feature, denote ˆT 2 i , we have two intermediate embeddings defined by ̃XS1 = X ⊙ T 1 ⊙ T 2 and ̃XS2 = X ⊙ T 1 ⊙ ˆT 2 respectively. Then these two embeddings are passed through MLP1 (f ) and MLP2 (g) to generate Gaussian embeddings f ( ̃XS1 ), g( ̃XS2 ) corresponding to (5). For the design of function h, we consider two crucial elements: 1. an additional layer to aggregate the information from different nodes in xN (i); 2. the injectivity of mappings f, g, h. Note f, h in (5) are automatically enforced to be injective on interacting features to maximize the first term of mTE, but g is not. Therefore, our final design of h is the composition of first applying g (enforcing the injectivity of g), a mean aggregation layer without self-loop consistent with the GCN design (Kipf and Welling, 2016) by multiplying the adjacency matrix A, and another MLP layer (MLP3). Finally, we compute the minus empirical Gaussian mTE ˆI(f, g) − ˆI(f, h) and add the cross-entropy penalty between the concatenated embedding distribution and a learnable Gaussian distribution.\n\n5\n\nFigure 2: Illustration of the proposed GEASS network architecture.\n\n3.3 OUTPUT INTERPRETATION\n\nUpon the algorithm convergence, GEASS provides both outputs of active features (B0 ∪ B1) and embeddings (f, g, h) produced by causally interacting features. In this paper, we emphasize the use of the identified interacting features B0 ∪ B1. The output of embeddings (f, g, h) may be complex and nonlinear, potentially requiring additional architectures to maximize its interpretability.\n\nBy the construction of GEASS, we are able to get two separate sparse feature subsets as source features B1 and sink features B0. These features may be used as inputs to further proper causal analysis, such as LPCMCI (Gerhardus and Runge, 2020) for time-series data, which despite its statistical power in depicting possible lags, identifying latent confounders, and allowing nonlinear tests, can only work on data with moderate feature sizes. Also, these features may be used in other machine learning models for improved model interpretability.\n\n4 EXPERIMENTS\n\n4.1 GAUSSIAN TIME-SERIES WITH POSSIBLE NONLINEARITY\n\nIn order to benchmark the method in time-series data, we consider two settings: 1. Minor effect of latent processes, with autocorrelation present; 2. Significant effect of latent processes, with autocorrelation present. Both settings are modeled by Gaussian structural processes with an underlying causal graph. Further details can be seen in Appendix C.1.\n\nWe test the false discovery rate (FDR) and F1 score between ground truth interacting features and recovered features as two metrics for high-dimensional data causal discovery. We compare GEASS with two categories of methods, namely conditional independence based (CI-based) methods and Granger causality based (GC-based) methods respectively. The first method category includes VAR-LINGAM (Hyvärinen et al., 2010), PCMCI (Runge et al., 2019b), and LPCMCI (Gerhardus and Runge, 2020). Among them, despite the statistical power, LPCMCI is not included in our experiment as it fails to converge in given time in our preliminary experiments. The second method category includes a neural-network based generalized vector autoregression model GVAR Granger (Marcinkeviˇcs and Vogt, 2021), and Grid-net which generalizes the definition of Granger causality to Directed Acyclic Graph (DAG) (Wu et al., 2021); moreover we include two state-of-the-art approaches, DCM and NGM implemented in (Bellot et al., 2021) that use neural ODE to model nonlinear dependence graph.\n\nTable 1 shows our benchmarking results. Among the alternative methods, GVAR and GrID-net fail in all settings as they are not designed for causal feature selection. VAR-LINGAM achieves high accuracy in linear settings while fails in nonlinear settings. In contrast, PCMCI fails when latent processes contribute to both true causally interacting features and nuisance features, creating spurious correlations. Empirically we also observe that DCM and NGM achieves comparable performance\n\n6\n\nSTG layer 1STG layer 2MLP 1 (f)MLP 2 (g)MeanAggregationMLP 3 + penalties............EmbeddingdimensionColor barLow valuesHigh valueswhen the dynamics are linear but performs worse in the nonlinear setting, where the dynamics are more irregular. Finally, GEASS consistently gives accurate causal feature identifications (high F1) and low false discovery rate (low FDR) in all settings considered.\n\nTable 1: Comparison of methods on Gaussian linear / nonlinear time-series data with different feature numbers and different nuisance feature settings.\n\nWeak confounding interactions\n\nStrong confounding interactions\n\nLinear\n\nnonlinear\n\nLinear\n\nnonlinear\n\nMethods\n\nFDR\n\nF1 score\n\nFDR\n\nF1 score\n\nFDR\n\nF1 score\n\nFDR\n\nLINGAM (CI) PCMCI (CI)\n\nGVAR (GC) GrID-net (GC) DCM (GC) NGM (GC) GEASS (Ours)\n\n.00 (.00) .17 (.01)\n\n.94 (.00) 1.0 (.00) .12 (.20) .07 (.08) .05 (.15)\n\n.94 (.04) .81 (.04)\n\n.11 (.00) .00 (.00) .88 (.20) .88 (.04) .97 (.10)\n\n.83 (.00) .12 (.08)\n\n.94 (.00) 1.0 (.00) .65 (.12) .48 (.17) .03 (.06)\n\n.17 (.00) .85 (.05)\n\n.11 (.00) .00 (.00) .35 (.12) .50 (.17) .92 (.05)\n\n.00 (.00) 1.0 (.00)\n\n.94 (.00) 1.0 (.00) .18 (.09) .00 (.00) .03 (.07)\n\n.94 (.04) .00 (.00)\n\n.11 (.00) .00 (.00) .82 (.09) .91 (.00) .90 (.04)\n\n.83 (.00) .63 (.23)\n\n.94 (.00) 1.0 (.00) .93 (.11) .62 (.25) .00 (.00)\n\nF1 score\n\n.17 (.00) .36 (.22)\n\n.11 (.00) .00 (.00) .07 (.11) .38 (.25) .91 (.00)\n\nFurthermore, we evaluate different methods’ scalability with respect to the feature size. (Experimental details can be seen at Appendix C.1.2). As described before, we anticipate high computational complexity of both conditional independence based methods and neural network based methods with respect to the feature size, which prohibits further use of these methods for high-dimensional biological data analysis, where the feature number is typically at the scale of 103 − 104. Meanwhile, GEASS constructs a single neural network with parameters approximately proportional to p, thus largely reducing the complexity in the high-dimensional regime. We benchmark PCMCI, GVAR, GrID-net, NGM, GEASS, and an additional combination of GEASS with a downstream CI-test based causal graph identification method LPCMCI. Our experimental result shows the superior performance of GEASS as well as GEASS+LPCMCI in time complexity, consistent with our qualitative analysis (Figure 3).\n\nFigure 3: Running time comparison for methods on time-series data with different feature sizes.\n\n4.2 SIMULATED SPATIAL OMICS DATA WITH CELL TYPE CONFOUNDER\n\nIn order to jointly consider spatial confounders and corresponding autocorrelation patterns that are potentially enriched in specific niches, we consider the case of spatial omics data, where the autocorrelation is modeled by a higher likelihood of same type of cells in the neighborhood, and the confounder (nuisance features) is modeled by a coherent shift of global gene expression for each cell type. We first simulate scRNA-seq datasets, then each synthetic scRNA-seq dataset is assigned to a fixed size grid with cell type labels simulated by Ising model simulation. We then add artificial genes that are spatially correlated with neighboring cell’s given gene set. Finally each dataset is normalized and log1p transformed as the standard pipeline in Scanpy (Wolf et al., 2018).\n\nThe majority of methods are not available as their focus is on time-series data. Therefore in order to perform our benchmarking study, we compare GEASS with Lasso Granger, as well as our implemented L1-regularized version of NCEM, an approach proposed to detect interactions in spatial omics data (Fischer et al., 2021). Finally, we also implemented a method that maximizes over the original transfer entropy to select causal features (TE).\n\nAs shown in Table 2, the original LASSO cannot identify causal features because of the strong correlation between features. L1-NCEM alleviates the issue by conditioning on cell type labels in regression. TE outperforms linear methods yet generates a number of false positives, as it may learn spurious causations as discussed in Remark 3.3. Finally, GEASS consistently outperforms over other methods in identifying causal features of data as shown by both high F1 score and low FDR.\n\n7\n\nTable 2: Comparison of methods on simulated spatial transcriptomics data.\n\nLinear\n\nnonlinear\n\nMethods\n\nFDR\n\nF1 score\n\nFDR\n\nF1 score\n\nLasso L1-NCEM TE GEASS (Ours)\n\n0.950±0.055 0.380±0.138 0.190±0.127 0.095±0.128\n\n0.050±0.055 0.620±0.138 0.767±0.070 0.787±0.088\n\n0.970±0.040 0.535±0.134 0.141±0.087 0.000±0.000\n\n0.030±0.040 0.465±0.134 0.761±0.060 0.775±0.110\n\n4.3\n\nSCRNA-SEQ PANCREATIC ENDOCRINOGENESIS TRAJECTORY\n\nWe test GEASS on the pancreatic endocrinogenesis trajectory data, which is a standard dataset for scRNA-seq trajectory inference task (Bergen et al., 2020; Bastidas-Ponce et al., 2019). The pancreas trajectory data contains 3696 cells and 27998 genes. After preprocessing, lowly-expressed genes are filtered out as the standard pipeline in scVelo (Bergen et al., 2020), with remaining 2000 genes for further analysis. We aim to use GEASS to identify causally-related genes along the developmental trajectory to reveal underlying biology. (See Appendix C.3 for experimental details).\n\nscRNA-seq data provides a snapshot of cell population distribution therefore time-series based analysis methods cannot be directly applied. However, due to GEASS’s flexible setting in forward operator W , we are able to define the time flow by RNA velocity analysis. RNA velocity analysis uses the additional information of intron RNAs to infer the underlying dynamics of gene expression change. Thus, we are able to define a velocity kernel matrix Avelo, which provides weighted adjacency relationships of cells based on velocity direction and cell phenotypic proximity.\n\nGEASS identifies 50 causally-related features with high biological relevance. For example, the gene list includes the key transcriptional regulator NEUROG3, which is required for the specification of a common precursor of the 4 pancreatic terminal states (uni, 2021). As the ground truth causal interactions here are unknown, for further quantitative validation, we assume the underlying biological process is driven by a causal cascade of gene interactions, meaning target genes activated in earlier phases of the trajectory further cause downstream gene activation at later phases. In this case, the higher a gene velocity is, the more likely the gene is associated with causal gene-gene relationships. Our benchmarking result here suggests GEASS achieves the best performance in selecting genes with high mean velocity likelihood, compared with alternative gene selection schemes with fixed gene number (50) including high-expressed genes (HEG), highly-variable genes (HVG), and genes having high correlation with inferred latent time (HCG) (Figure 4).\n\nMean RNA velocity likelihood\n\nHEG HVG HCG GEASS\n\n0.0528 0.1753 0.1889 0.2366\n\nFigure 4: Visualization of pancreas trajectory dataset and comparisons of gene selection criterions.\n\n4.4 MERFISH HUMAN CORTEX SINGLE-CELL LEVEL SPATIAL TRANSCRIPTOMICS\n\nSpatial transcriptomics represent a wide category of method that can achieve spatial profiling of gene expression in tissues (Moses and Pachter, 2022; Rao et al., 2021; Palla et al., 2022). By the additional information of spatial locations, such measurements enable deeper understandings of cellular interactions (Palla et al., 2022; Jerby-Arnon and Regev, 2022; Fischer et al., 2021). However, current computational methods revealing interaction modules (Jerby-Arnon and Regev, 2022) or niche effects (Fischer et al., 2021; Raredon et al., 2023) for spatial omics data lacks causal interpretation. Applying GEASS, we aim to reveal underlying causal intercellular patterns to fully utilize the potential of spatial omics data for biological discovery.\n\n8\n\nHere we use GEASS on a recent published MERFISH dataset measuring spatially-resolved single-cell gene expression of human cortex (Fang et al., 2022). The dataset we used comprises of 3044 cells and 4000 genes; each cell is annotated as one of the eight cell types: excitatory neurons (EXC), inhibitory neurons (INC), astrocytes (ASC), microglial cells (MGC), oligodendrocytes (OGC), oligodendrocyte progenitor cells (OPC), endothelial cells (ENDO), and mural cells (MURAL) as shown by the first panel of Figure 6 in Appendix D. Our GEASS analysis selects 9 genes, namely FILIP1, SLC17A7, MYH11, RP11-10j21.2, PIRT, C3ORF67, TRDMT1, RGS8, SPTLC2 (Appendix Figure 6), with further experimental details available in Appendix C.4. Among these genes, MYH11, RP11-10j21.2, and TRDMT1 are enriched at the endothelial cells adjacent with mural cells, corresponding to underlying vascular structures (marked by ellipses in the first panel of Appendix Figure 6). We next aim to verify if their expression difference with those of non-adjacent endothelial cells is statistically significant. Indeed, by applying the Wilcoxon rank-sum test, we have found significant enrichments for both MYH11 and TRDMT1, with p-values 0.003 and 0.015 respectively, while the p-value for the gene RP11-10j21.2 is not significant (0.5) due to the gene expression sparsity. The finding is consistent with the MERFISH images, which reveals rich cellular interactions between neuronal cells and the blood vessels (Fang et al., 2022). Therefore, these identified marker genes of vascular structure may encode underlying meaningful cellular interactions.\n\nNext, we focus on two GEASS identified genes, C3ORF67 and PIRT, which are highly expressed at nearby spatial locations. In order to confirm the possible causal relationship between the two genes, we consider three models: 1. the two genes are expressed in the same cell without spatial causal relationships; 2. The expression of C3ORF67 in each cell causes the expression of PIRT in neighboring cells (C3ORF67 → PIRT); 3. The expression of PIRT in each cell causes the expression of C3ORF67 in neighboring cells (PIRT → C3ORF67). To this end, we first compare Pearson and Spearman p-values of intracellular correlation (model 1), C3ORF67 to neighboring PIRT (model 2), and PIRT to neighboring C3ORF67 (model 3). Our comparison shows for the p-values of both correlation measures, model 3 is favored (0.004, 0.001) over model 1 (0.014, 0.003) and model 2 (0.049, 0.004). The validity of model 3 (PIRT → C3ORF67) is further supported by a linear model predicting C3ORF67 expression by both intracellular and neighbor expression of PIRT, where the neighboring cell effect coefficient is significant at the confidence level of 0.01 by bootstrap, while the alternative model’s corresponding coefficient is not significant. Our finding is consistent with the predicted role of PIRT in transmembrane transporter binding and phosphatidylinositol-mediated signaling (Safran et al., 2021). As the role of C3ORF67 in human cortex remains unclear, this revealed causal link may lead to novel biological discoveries with further experimental validations.\n\nFigure 5: Normalized spatial expression levels of genes C3orf67 and PIRT.\n\n5 CONCLUSIONS\n\nIn this work, we present GEASS, a causal feature selection method based on information-theoretic tools and neural networks. GEASS is able to scale to high dimensions and identify sparse interacting features. We provide both theoretical gaurantees and empirical validations of GEASS on synthetic and real biological data. Our results show GEASS can be integrated into high-dimensional spatiotemporal data analysis pipelines to provide unique insights for further findings.\n\nLimitations. GEASS is a method designed for nonlinear causal feature selection. GEASS does not provide a causal graph itself as it optimizes a latent embedding corresponding to different causal mechanisms. Therefore, in applications where a causal graph output is favored, constraint-based methods may need to be applied after GEASS. Moreover, when underlying causal graph has a large number of vertices, the sparsity assumption is violated and GEASS is not gauranteed to work. Also, further efforts may be taken to incorporate lag selections for GEASS.\n\nBroader impact. We anticipate a wide use of GEASS in high-dimensional graph-structured data, especially for high-dimensional biological data such as single cell trajectories and spatial omics measurements. Applying GEASS along with causal graph identification methods to a wider range of real biological data may greatly facilitate downstream biological discoveries.\n\n9\n\nACKNOWLEDGEMENTS\n\nThe authors thank Ofir Lindenbaum, Boaz Nadler, Yifei Min, and Ronen Basri for helpful discussions. Y.K. acknowledges support by NIH grants R01GM131642, UM1DA051410, U54AG076043, P50CA121974, and U01DA053628.\n\nREFERENCES\n\nRobrecht Cannoodt, Wouter Saelens, and Yvan Saeys. Computational methods for trajectory inference from single-cell transcriptomics. European journal of immunology, 46(11):2496–2506, 2016.\n\nWouter Saelens, Robrecht Cannoodt, Helena Todorov, and Yvan Saeys. A comparison of single-cell\n\ntrajectory inference methods. Nature biotechnology, 37(5):547–554, 2019.\n\nChee-Huat Linus Eng, Michael Lawson, Qian Zhu, Ruben Dries, Noushin Koulena, Yodai Takei, Jina Yun, Christopher Cronin, Christoph Karp, Guo-Cheng Yuan, et al. Transcriptome-scale super-resolved imaging in tissues by rna seqfish+. Nature, 568(7751):235–239, 2019.\n\nRongxin Fang, Chenglong Xia, Jennie L Close, Meng Zhang, Jiang He, Zhengkai Huang, Aaron R Halpern, Brian Long, Jeremy A Miller, Ed S Lein, et al. Conservation and divergence of cortical cell organization in human and mouse revealed by merfish. Science, 377(6601):56–62, 2022.\n\nLambda Moses and Lior Pachter. Museum of spatial transcriptomics. Nature Methods, 19(5):534–546,\n\n2022.\n\nAnjali Rao, Dalia Barkley, Gustavo S França, and Itai Yanai. Exploring tissue architecture using\n\nspatial transcriptomics. Nature, 596(7871):211–220, 2021.\n\nGiovanni Palla, David S Fischer, Aviv Regev, and Fabian J Theis. Spatial components of molecular\n\ntissue biology. Nature Biotechnology, 40(3):308–318, 2022.\n\nKevin D Hoover. Causality in economics and econometrics. 2006.\n\nMaciej Kami ́nski, Mingzhou Ding, Wilson A Truccolo, and Steven L Bressler. Evaluating causal relations in neural systems: Granger causality, directed transfer function and statistical assessment of significance. Biological cybernetics, 85(2):145–157, 2001.\n\nJakob Runge, Sebastian Bathiany, Erik Bollt, Gustau Camps-Valls, Dim Coumou, Ethan Deyle, Clark Glymour, Marlene Kretschmer, Miguel D Mahecha, Jordi Muñoz-Marí, et al. Inferring causation from time series in earth system sciences. Nature communications, 10(1):1–13, 2019a.\n\nMarcos Herrera Gómez, Manuel Ruiz Marín, and Jesús Mur Lacambra. Testing spatial causality in\n\ncross-section data. 2014.\n\nClive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.\n\nEconometrica: journal of the Econometric Society, pages 424–438, 1969.\n\nAli Shojaie and Emily B Fox. Granger causality: A review and recent advances. Annual Review of\n\nStatistics and Its Application, 9:289–319, 2022.\n\nKonrad P Mielke, Tom Claassen, J Huijbregts, Aafke M Schipper, and Tom M Heskes. Discovering cause-effect relationships in spatial systems with a known direction based on observational data. In International Conference on Probabilistic Graphical Models, pages 305–316. PMLR, 2020.\n\nDavid S Fischer, Anna C Schaar, and Fabian J Theis. Learning cell communication from spatial\n\ngraphs of cells. BioRxiv, 2021.\n\nPA Valdés-Sosa, JM Bornot-Sánchez, M Vega-Hernández, L Melie-García, A Lage-Castellanos, and E Canales-Rodríguez. granger causality on spatial manifolds: applications to neuroimaging. handbook of time series analysis: recent theoretical developments and applications, 18.\n\nThomas Schreiber. Measuring information transfer. Physical review letters, 85(2):461, 2000.\n\n10\n\nMatthäus Staniek and Klaus Lehnertz. Symbolic transfer entropy. Physical review letters, 100(15):\n\n158101, 2008.\n\nDoris Entner and Patrik O Hoyer. On causal discovery from time series data using fci. Probabilistic\n\ngraphical models, pages 121–128, 2010.\n\nDaniel Malinsky and Peter Spirtes. Causal structure learning from multivariate time series in settings In Proceedings of 2018 ACM SIGKDD workshop on causal\n\nwith unmeasured confounding. discovery, pages 23–47. PMLR, 2018.\n\nAlessio Moneta, Nadine Chlaß, Doris Entner, and Patrik Hoyer. Causal search in structural vector autoregressive models. In NIPS Mini-Symposium on Causality in Time Series, pages 95–114. PMLR, 2011.\n\nNiklas Pfister, Peter Bühlmann, and Jonas Peters. Invariant causal prediction for sequential data.\n\nJournal of the American Statistical Association, 114(527):1264–1276, 2019.\n\nAtalanti A Mastakouri, Bernhard Schölkopf, and Dominik Janzing. Necessary and sufficient conditions for causal feature selection in time series with latent common causes. In International Conference on Machine Learning, pages 7502–7511. PMLR, 2021.\n\nAapo Hyvärinen, Kun Zhang, Shohei Shimizu, and Patrik O Hoyer. Estimation of a structural vector autoregression model using non-gaussianity. Journal of Machine Learning Research, 11(5), 2010.\n\nJakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino Sejdinovic. Detecting and quantifying causal associations in large nonlinear time series datasets. Science advances, 5 (11):eaau4996, 2019b.\n\nShohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, Antti Kerminen, and Michael Jordan. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006.\n\nPeter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction,\n\nand search. MIT press, 2000.\n\nYaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network:\n\nData-driven traffic forecasting. arXiv preprint arXiv:1707.01926, 2017.\n\nAlex Tank, Ian Covert, Nicholas Foti, Ali Shojaie, and Emily B Fox. Neural granger causality. IEEE\n\nTransactions on Pattern Analysis and Machine Intelligence, 44(8):4267–4279, 2021.\n\nMeike Nauta, Doina Bucur, and Christin Seifert. Causal discovery with attention-based convolutional\n\nneural networks. Machine Learning and Knowledge Extraction, 1(1):19, 2019.\n\nZexuan Yin and Paolo Barucca. Deep recurrent modelling of granger causality with latent confound-\n\ning. arXiv preprint arXiv:2202.11286, 2022.\n\nAlexis Bellot, Kim Branson, and Mihaela van der Schaar. Neural graphical modelling in continuoustime: consistency guarantees and algorithms. In International Conference on Learning Representations, 2021.\n\nSaurabh Khanna and Vincent YF Tan. Economy statistical recurrent units for inferring nonlinear\n\ngranger causality. arXiv preprint arXiv:1911.09879, 2019.\n\nXiangyu Sun, Guiliang Liu, Pascal Poupart, and Oliver Schulte. Nts-notears: Learning nonparametric temporal dags with time-series data and prior knowledge. arXiv preprint arXiv:2109.04286, 2021.\n\nAlexander P Wu, Rohit Singh, and Bonnie Berger. Granger causal inference on dags identifies genomic loci regulating transcription. In International Conference on Learning Representations, 2021.\n\nXianjie Guo, Kui Yu, Lin Liu, Fuyuan Cao, and Jiuyong Li. Causal feature selection with dual\n\ncorrection. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n\nJudea Pearl. Causality. Cambridge university press, 2009.\n\n11\n\nLuca Faes, Daniele Marinazzo, Giandomenico Nollo, and Alberto Porta. An information-theoretic framework to map the spatiotemporal dynamics of the scalp electroencephalogram. IEEE Transactions on Biomedical Engineering, 63(12):2488–2496, 2016.\n\nPayam Shahsavari Baboukani, Carina Graversen, Emina Alickovic, and Jan Østergaard. Estimating conditional transfer entropy in time series using mutual information and nonlinear prediction. Entropy, 22(10):1124, 2020.\n\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.\n\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.\n\nDavid McAllester and Karl Stratos. Formal limitations on the measurement of mutual information. In International Conference on Artificial Intelligence and Statistics, pages 875–884. PMLR, 2020.\n\nJingjing Zhang, Osvaldo Simeone, Zoran Cvetkovic, Eugenio Abela, and Mark Richardson. Itene:\n\nIntrinsic transfer entropy neural estimator. arXiv preprint arXiv:1912.07277, 2019.\n\nYutaro Yamada, Ofir Lindenbaum, Sahand Negahban, and Yuval Kluger. Feature selection using stochastic gates. In International Conference on Machine Learning, pages 10648–10659. PMLR, 2020.\n\nOfir Lindenbaum, Moshe Salhov, Amir Averbuch, and Yuval Kluger. L0-sparse canonical correlation\n\nanalysis. In International Conference on Learning Representations, 2021.\n\nAndrew Miller, Nick Foti, Alexander D’Amour, and Ryan P Adams. Reducing reparameterization\n\ngradient variance. Advances in Neural Information Processing Systems, 30, 2017.\n\nMikhail Figurnov, Shakir Mohamed, and Andriy Mnih.\n\nImplicit reparameterization gradients.\n\nAdvances in neural information processing systems, 31, 2018.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.\n\narXiv preprint arXiv:1609.02907, 2016.\n\nAndreas Gerhardus and Jakob Runge. High-recall causal discovery for autocorrelated time series with latent confounders. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 12615–12625. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 94e70705efae423efda1088614128d0b-Paper.pdf.\n\nRiˇcards Marcinkeviˇcs and Julia E Vogt.\n\nInterpretable models for granger causality using self-\n\nexplaining neural networks. arXiv preprint arXiv:2101.07600, 2021.\n\nF Alexander Wolf, Philipp Angerer, and Fabian J Theis. Scanpy: large-scale single-cell gene\n\nexpression data analysis. Genome biology, 19(1):1–5, 2018.\n\nVolker Bergen, Marius Lange, Stefan Peidli, F Alexander Wolf, and Fabian J Theis. Generalizing rna velocity to transient cell states through dynamical modeling. Nature biotechnology, 38(12): 1408–1414, 2020.\n\nAimée Bastidas-Ponce, Sophie Tritschler, Leander Dony, Katharina Scheibner, Marta Tarquis-Medina, Ciro Salinno, Silvia Schirge, Ingo Burtscher, Anika Böttcher, Fabian J Theis, et al. Comprehensive single cell mrna profiling reveals a detailed roadmap for pancreatic endocrinogenesis. Development, 146(12):dev173849, 2019.\n\nUniprot: the universal protein knowledgebase in 2021. Nucleic acids research, 49(D1):D480–D489,\n\n2021.\n\nLivnat Jerby-Arnon and Aviv Regev. Dialogue maps multicellular programs in tissue from single-cell\n\nor spatial transcriptomics data. Nature Biotechnology, pages 1–11, 2022.\n\n12\n\nMicha Sam Brickman Raredon, Junchen Yang, Neeharika Kothapalli, Wesley Lewis, Naftali Kaminski, Laura E Niklason, and Yuval Kluger. Comprehensive visualization of cell–cell interactions in single-cell and spatial transcriptomics with niches. Bioinformatics, 39(1):btac775, 2023.\n\nMarilyn Safran, Naomi Rosen, Michal Twik, Ruth BarShir, Tsippi Iny Stein, Dvir Dahary, Simon Fishilevich, and Doron Lancet. The genecards suite. In Practical guide to life science databases, pages 27–56. Springer, 2021.\n\nThomas M Cover. Elements of information theory. John Wiley & Sons, 1999.\n\nDylan Kotliar, Adrian Veres, M Aurel Nagy, Shervin Tabrizi, Eran Hodis, Douglas A Melton, and Pardis C Sabeti. Identifying gene expression programs of cell-type identity and cellular activity with single-cell rna-seq. Elife, 8, 2019.\n\nAPPENDIX\n\nA PROOFS\n\nA.1 PROOF OF PROPOSITION 2.2.\n\nProposition 2.2. ∀S1, S2 ⊂ {1, ..., p}, mTEi(S1, S2) > 0 ⇒ TEi(S1, S2) > 0.\n\nProof. By standard properties of mutual information (Cover, 1999) we have\n\nTEi(XS1, XS2) = I(X i = I(X i\n\nS1\n\nS1\n\n; X j:(i,j)∈E\n\nS2\n\n; X j:(i,j)∈E\n\nS2\n\n= I(X i\n\nS1\n\n; X j:(i,j)∈E\n\nS2\n\n)\n\n|X i\n\nS2 , X i\n\nS2 ) − I(X i\n\n) − I(X i\n\nS1\n\n; X i\n\nS2\n\nS1\n\n)\n\n; X i\n\nS2 ) + I(X i\n\nS1\n\n(9)\n\n; X i\n\nS2\n\n|X j:(i,j)∈E\n\nS2\n\n).\n\nTherefore TEi(S1, S2) ≥ mTEi(S1, S2) holds, thus mTEi(S1, S2) > 0 ⇒ TEi(S1, S2) > 0.\n\nA.2 DISCUSSION OF ASSUMPTION A6.\n\nOur assumption A6 is based on the concept of conditional mutual entropy, which aims to filter out possible indirect causal relationships.\n\nHere are two simple examples to see why TE/mTE can have problems with indirect causal interactions in the time-series setting: consider the relationships: st → wt → vt+1; st → wt+1 → vt+1. Then in both cases, we may have: I(st, vt+1) − I(st, vt) > 0 and I(st, vt+1|vt) > 0 although there are no direct causal relationship between s and v. Note in our setting, we include the possibility of such indirect interaction by allowing correlation between nuisance features and true interacting features.\n\nThe issue can be resolved by considering the conditional mutual information I(st, vt+1|wt) or I(st, vt+1|wt+1), which equals 0. This insight is also addressed the concept of conditional transfer entropy:\n\nDefinition (Conditional transfer entropy) (Shahsavari Baboukani et al., 2020). Assume X and Y are the features of interest and the conditioning features are Z. Denote − as [1, 2, ..., t], then we have\n\ncTEt(X, Y, Z) = I(Yt+1, X−|Y−, Z−).\n\nThe classical formulation of conditional transfer entropy is widely used in high-dimensional observational data to learn direct causal dependencies (Faes et al., 2016; Shahsavari Baboukani et al., 2020). It implicitly assumes that, there is direct causal relationship between X and Y if ∀Z, t, cTEt(X, Y, Z) > 0. Here, we extend this assumption in the context of conditional mTE covering both examples described above. The conditional mTEs are defined in analogy to cTE for generalized graph-structured data in the Markovian model setting:\n\nDefinition (Two forms of conditional mTE). Assume X and Y are the feature sets of interest and the conditioning features are Z. Then we have\n\ncmTE1\n\ni (X, Y, Z) = I(X i, Y N (i)|Z i) − I(X i, Y i|Z i);\n\n13\n\ncmTE2\n\ni (X, Y, Z) = I(X i, Y N (i)|Z N (i)) − I(X i, Y i|Z i);\n\nBy controlling the two forms of conditional mTE to be larger than zero, we rule out both possibilities of X i → Z i → Y N (i) and X i → Z N (i) → Y N (i), as mTE is a stricter version of the original transfer entropy as discussed in Proposition 2.2. In summary, our A6 can be reformulated as ∀Z, i, cmTE1 i (X, Y, Z) > 0 for ground truth interacting X, Y in non-degenerating cases, where Z does not fully overlap with X/Y in the same point.\n\ni (X, Y, Z) > 0; cmTE2\n\nA.3 PROOF OF THEOREM 2.4.\n\nTheorem 2.4. Given A1-A6, S∗ := (S∗ described in A5). Moreover, each feature in S∗ is connected to other features in the set S∗.\n\n2 ) ⊆ {1, ..., m} (the index set of true interacting features\n\n1 ∪ S∗\n\nProof. Step 1. First we prove S∗ we denote N (i) := {j|(i, j) ∈ E}, A = XS∗\n\n1 ∩S∗\n\n, B = XS∗\n\n2 . Then we have\n\n1\n\n2 = ∅. If not, assume p is an overlapping element. For simplicity,\n\nmTE(S∗\n\n1 , S∗\n\n2 ) − mTE(S∗\n\n1 \\ p, S∗ 2 )\n\n= I(Ai \\ pi, pi; BN (i) \\ pN (i), p{N (i)}) − I(Ai, pi; Bi, pi) − I(Ai \\ pi; BN (i) \\ pN (i), pN (i)) + I(Ai; Bi, pi)\n\n= I(pi; BN (i) \\ pN (i), pN (i)|Ai \\ pi) − I(pi; Bi \\ pi, pi|Ai \\ pi) < 0.\n\n(10)\n\nTherefore removing p would increase the value of mTE, leading to a contradiction.\n\nStep 2. Now we prove nuisance signals cannot be in either S∗ 2 . Otherwise, first we assume a set of nuisance signals U is in S∗ 2 . As U only interacts with variables at the same time point, U can only interact with BN (i) via indirect links through a subset of interacting features at i. Denote this feature set as P aU (B)i ⊆ {ti m}, and the difference set P a− U (B)i cannot be an empty set. Otherwise, denote S1 := S∗\n\nU (B)i := P aU (B)i \\ Bi. Then we first note P a−\n\n1 \\ U , noting the non-overlapness between A and B we would have\n\n1 . Here we denote A := XS∗\n\n1 or S∗ , B := XS∗\n\n1, ..., ti\n\n1\n\nmTE(S∗\n\n1 , S∗\n\n2 ) − mTE(S1, S∗ 2 )\n\n= I(Ai \\ U i, U i; BN (i)) − I(Ai \\ U i, U i; Bi) − I(Ai \\ U i; BN (i)) + I(Ai \\ U i; Bi)\n\n= I(U i; BN (i)|Ai \\ U i) − I(U i; Bi|Ai \\ U i)\n\n(11)\n\n= −h(U i|BN (i), Ai \\ U i) + h(U i|Bi, Ai \\ U i) ≤ −h(U i|BN (i), Ai \\ U i) + h(U i|P aU (B)i, Ai \\ U i) (Conditioning reduces entropy) ≤ 0.\n\nThis means (S1, S∗ to a contradiction. Then because P a−\n\n2 )’s mTE is not smaller than (S∗\n\n2 )’s while having a smaller union size, leading U (B) does not overlap with either U and B, with A6 we have\n\n1 , S∗\n\nmTE(S∗\n\n1 , S∗\n\n2 ) − mTE(S∗\n\n1 ∪ Index(P a−\n\nU (B)), S∗ 2 )\n\n= I(Ai \\ U i, U i; BN (i)) − I(Ai \\ U i, U i; Bi) − I(Ai \\ U i, U i, P a− + I(Ai \\ U i, U i, P a−\n\nU (B)i; Bi)\n\nU (B)i; BN (i))\n\n(12)\n\n= I(P a−\n\nU (B)i; Bi|Ai) − I(P a−\n\nU (B)i; BN (i)|Ai)\n\nA6 ≤ 0.\n\n14\n\nThe equal sign above is taken iff. P a−\n\nU (B)i ⊆ Ai. Further we have\n\nmTE(S∗\n\n1 ∪ Index(P a− = I(Ai \\ U i, U i, P a−\n\n2 ) − mTE(S1 ∪ Index(P a− U (B)), S∗ U (B)i; BN (i)) − I(Ai \\ U i, U i, P a−\n\nU (B)), S∗ 2 ) U (B)i; Bi)\n\n− I(Ai \\ U i, P a−\n\nU (B)i; BN (i)) + I(Ai \\ U i, P a−\n\nU (B)i; Bi)\n\n= I(U i; BN (i)|P a−\n\nU (B)i, Ai \\ U i) − I(U i; Bi|P a−\n\nU (B)i, Ai \\ U i)\n\n(13)\n\n= −h(U i|BN (i), P a−\n\n≤ −h(U i|BN (i), P a−\n\nU (B)i, Ai \\ U i) + h(U i|Bi, P a− U (B)i, Ai \\ U i) U (B)i, Ai \\ U i) + h(U i|P aU (B)i, Ai \\ U i) ≤ 0.\n\nTherefore, in all possible cases, mTE(S1 ∪ Index(P a− mTE(S∗\n\n2 ) or equal with mTE(S∗\n\n1 , S∗\n\n1 , S∗\n\nU (B)i), S∗\n\n2 ) is either strictly larger than\n\n2 ) but with smaller union size, leading to a contradiction.\n\nNext, given the result above, we assume a nuisance signal set U is in S∗ 1 does not include any nuisance features. Then as U only interacts with variables at the same time point, U N (i) can only interact with S∗ 1 via indirect links through a subset of interacting features at N (i). Denote the whole intermediate feature set for A as ChU (A)N (i) ⊆ {tN (i) U (A)N (i) := ChU (A)N (i) \\ AN (i). Then same as above, denote S2 = S∗ U (A) is an empty set we would have\n\n, ..., tN (i) 2 \\ U , if Ch−\n\nm }, and Ch−\n\n2 , and S∗\n\n1\n\nmTE(S∗\n\n1 , S∗\n\n2 ) − mTE(S∗\n\n1 , S2)\n\n= I(Ai; BN (i) \\ U N (i), U N (i)) − I(Ai; Bi \\ U i, U i) − I(Ai; BN (i) \\ U N (i)) + I(Ai; Bi \\ U i, U i)\n\n= I(Ai; U N (i)|BN (i) \\ U N (i)) − I(Ai; U i|Bi \\ U i)\n\n= −h(U N (i)|BN (i) \\ U N (i), Ai) + h(U i|Bi \\ U i, Ai) ≤ −h(U N (i)|BN (i) \\ U N (i), Ai) + h(U i|ChU (A)i, Bi \\ U i) ≤ 0.\n\n(14)\n\nAbove derivation holds due to stationarity (as |N (i)| ≡ 1 in the time series setting). Therefore Ch− U (A) and either A or U , with A6, we have\n\nU (A) cannot be an empty set. Because of the non-overlapness between Ch−\n\nmTE(S∗\n\n1 , S∗\n\n2 ) − mTE(S∗\n\n1 , S2 ∪ Index(Ch−\n\nU (A)))\n\n= I(Ai; BN (i) \\ U N (i), U N (i)) − I(Ai; Bi \\ U i, U i)\n\n− I(Ai; BN (i) \\ U N (i), U N (i), Ch−\n\nU (A)N (i)) + I(Ai; Bi \\ U i, U i, Ch−\n\nU (A)i)\n\n= I(Ai; Ch−\n\nU (A)i|Bi) − I(Ai; Ch−\n\nU (A)N (i)|BN (i))\n\nA6 ≤ 0.\n\nThe equal sign above is taken iff. Ch−\n\nU (A)i ⊆ Bi. Further we have\n\n1 , S∗\n\nmTE(S∗\n\n2 ∪ Index(Ch− = I(Ai; BN (i) \\ U N (i), U N (i), Ch−\n\nU (A))) − mTE(S∗\n\n1 , S2 ∪ Index(Ch− U (A)N (i)) − I(Ai; Bi \\ U i, U i, Ch−\n\nU (A)))\n\nU (A)i)\n\n− I(Ai; BN (i) \\ U N (i), Ch−\n\nU (A)N (i)) + I(Ai; Bi \\ U i, Ch−\n\nU (A)i)\n\n= I(Ai; U N (i)|BN (i) \\ U N (i), Ch−\n\nU (A)N (i)) − I(Ai; U i|Bi \\ U i, Ch−\n\nU (A)i) ≤ 0.\n\n(15)\n\n(16)\n\nTherefore, in all possible cases, mTE(S∗ mTE(S∗\n\n2 ) or equal with mTE(S∗\n\n1 , S∗\n\n1 , S∗\n\n1 , S2 ∪ Index(Ch−\n\nU (A))) is either strictly larger than\n\n2 ) but with smaller union size, leading to a contradiction.\n\nStep 3. Moreover, if there exists a component in S∗ 2 not connected to any other feature components, denote the feature as q. Then, in this case with A1-4, the feature q is independent of any other features in S∗ 2 . Therefore in this case, we have mTE(S∗ 1 , S∗ 2 ) thus leading to the contradiction of finding an (S1, S2) with the same mTE but smaller |S1 ∪ S2|.\n\n2 . From step 1 it can be deduced that q cannot be in both S∗\n\n2 − q) = mTE(S∗\n\n1 − q, S∗\n\n1 ∪ S∗\n\n1 ∪ S∗\n\n1 , S∗\n\n15\n\nA.4 PROOF OF THEOREM 3.1.\n\nTheorem 3.1. Assume A1-A6 holds and f, g, h define one-to-one mappings on X ⊙1S1 (for f ) or X ⊙ 1S2 (for g, h). Then ∃λ > 0, such that for (4), any solution (S∗ 2 ) ⊆ {1, ..., m}. Moreover, each feature in S∗ is connected to other features in the set.\n\n2 ) satisfies S∗ := (S∗\n\n1 ∪ S∗\n\n1 ∪ S∗\n\nmin f,g,h,S1,S2\n\n−(I(f (xi ⊙ 1S1 ); h(xN (i) ⊙ 1S2 )) − I(f (xi ⊙ 1S1 ); g(xi ⊙ 1S2))) + λ|S1 ∪ S2|\n\nProof. With A4 (ergodicity and stationarity), the optimization problem 4 is equivalent to\n\nmin f,g,h,S1,S2\n\n−(I(f (xi\n\nS1\n\n); h(xN (i)\n\nS2\n\n)) − I(f (xi\n\nS1\n\n); g(xi\n\nS2\n\n))) + λ|S1 ∪ S2|.\n\n(17)\n\nGiven the assumption that f, g, h define injective mappings on xi respectively, and one-to-one transformation does not change mutual information, we have the optimization problem is equivalent to\n\n, xi\n\nS2\n\nS1\n\nmin S1,S2\n\n−(I(xi\n\n; xN (i)\n\nS2\n\nS1\n\n) − I(xi\n\n; xi\n\nS2\n\nS1\n\n)) + λ|S1 ∪ S2|.\n\n(18)\n\n1 ∪ S∗\n\n2 ) ⊆ {1, ..., m}. Moreover, each feature in S∗\n\nUsing Theorem 2.4, a minimizer of the mTE term with the smallest union size satisfies S∗ := (S∗ 2 is connected to other features in the set. Note that with our definition of optimal S1, S2, the minimal gap between mTE(S∗ 2 ) and any other value mTE(S1, S2) with smaller |S1 ∪ S2| size is larger than zero. Denote the minimal gap as δ, and take λ <\n\n2 | , then for these other solutions, we have\n\n1 ∪ S∗\n\nδ 1 ∪S∗\n\n1 , S∗\n\n|S∗\n\n− mTE(S1, S2) + λ|S1 ∪ S2| 1 , S∗ ≥ −mTE(S∗ 1 , S∗ ≥ −mTE(S∗ 1 , S∗ > −mTE(S∗\n\n2 ) + δ + λ|S1 ∪ S2| 2 ) + δ 2 ) + λ|S∗\n\n1 ∪ S∗\n\n2 |.\n\nMeanwhile, for the (S1, S2) with larger union size, with the definition of the mTE, we have\n\n− mTE(S1, S2) + λ|S1 ∪ S2| 1 , S∗ ≥ −mTE(S∗ = −mTE(S∗ 1 , S∗ 1 , S∗ > −mTE(S∗\n\n2 ) + λ|S1 ∪ S2| 2 ) + λ(|S1 ∪ S2| − |S∗ 1 ∪ S∗ 2 ) + λ|S∗\n\n2 |.\n\n1 ∪ S∗\n\n2 |) + λ|S∗\n\n1 ∪ S∗ 2 |\n\n(19)\n\n(20)\n\nδ Therefore, when taking λ ∈ (0, 1 ∪S∗ the constructed optimization problem.\n\n|S∗\n\n2 | ), the desired optimal S1, S2 by mTE is the optimal output of\n\nA.5 PROOF OF THEOREM 3.4.\n\nTheorem 3.4. Assume A1-A6 and f, g, h are one-to-one Gaussian embeddings as described above. Denote for the optimal solution of (5), a sample of stochastic gate is given by T 1, T 2 and denote the ground truth interacting feature set as S, then there exists λ1, λ2 > 0 for (5) such that as n → ∞,\n\n∀i ∈ {0, 1}, P(Bi ⊆ S) a.s.−−→ 1, where Bi := {d|T 1\n\nd > 0, T 2\n\nd = i}.\n\nProof. In the following proof for simplicity we denote ̃xS1 = x⊙T 1 ⊙T 2; ̃xS2 = x⊙T 1 ⊙(1−T 2).\n\nStep 1. Given f, g, h projects input distributions into joint Gaussian distributions with fixed dimensionality, by convergence of Gaussian covariance matrices, we have:\n\nˆΣ(f ( ̃xi\n\nS1\n\n), h( ̃xN (i)\n\nS2\n\n)) =\n\nˆΣ(f ( ̃xi\n\nS1\n\n), g( ̃xi\n\nS2\n\n)) =\n\n1 n\n\n1 n\n\nn (cid:88)\n\n[f ( ̃xi\n\nS1\n\ni=1 n\n(cid:88)\n\n[f ( ̃xi\n\nS1\n\ni=1\n\n); h( ̃xN (i)\n\nS2\n\n)][f ( ̃xi\n\nS1\n\n); h( ̃xN (i)\n\nS2\n\n)]T a.s.−−→ Σf ( ̃xi\n\nS1\n\n),h( ̃xN (i)\n\nS2\n\n);\n\n); g( ̃xi\n\nS2\n\n)][f ( ̃xi\n\nS1\n\n); g( ̃xi\n\nS2\n\n)]T a.s.−−→ Σf ( ̃xi\n\nS1\n\n),g( ̃xi\n\nS2\n\n).\n\n(21)\n\n16\n\nAs in the Gaussian case, the mutual information between jointly Gaussian r.v.s is a function of the covariance matrix, we have\n\nˆI(f ( ̃xi\n\n); h( ̃xN (i)\n\nS1 ˆI(f ( ̃xi\n\nS1\n\n)) a.s.−−→ I(f ( ̃xi )) a.s.−−→ I(f ( ̃xi\n\nS1\n\n); h( ̃xN (i)\n\nS2 ); g( ̃xi\n\nS2\n\nS1\n\n)) = I( ̃xi\n\nS1\n\n; ̃xN (i)\n\nS2\n\n);\n\n)) = I( ̃xi\n\nS1\n\n; ̃xi\n\nS2\n\n);\n\n(22)\n\nEmpirical mTE = mTE) = 1.\n\nS2 ); g( ̃xi S2 P( lim\n\nN→∞\n\nStep 2. Importantly, in our formulation eq (5), the T1, T2 are sampled once in one epoch, meaning they are fixed across features for computing mTE. Further note that (cid:80)p d > 0) = E||T 1||0; (cid:80)p d ∈ (0, 1)) = E||1T 2∈(0,1)||0. This means denoting the value of eq (5) as L, we have\n\nP(T 2\n\nP(T 1\n\nd=1\n\nd=1\n\nL a.s.−−→ ET 1,T 2[−mTE(1T 1⊙T 2>0, 1T 1⊙(1−T 2)>0) + λ1||T 1||0 + λ2||1T 2∈(0,1)||0]\n\n≥ min T 1,T 2\n\n−mTE(1T 1⊙T 2>0, 1T 1⊙(1−T 2)>0) + λ1||T 1||0 + λ2||1T 2∈(0,1)||0.\n\nNote with step 1 of the proof of theorem 2.4, for any T1 we have\n\n− mTE(1T 1⊙T 2>0, 1T 1⊙(1−T 2)>0) + λ1||T 1||0 + λ2||1T 2∈(0,1)||0 ≥ −mTE(1T 1⊙T 2>0, 1T 1⊙(1−T 2)>0) + λ1||T 1||0,\n\nwhich is taken when ∀d, P(T 2\n\nd = 1) = 0/1, P(T 2\n\nd = 0) = 1/0. In this case,\n\n||T 1||0 = ||T 1 ⊙ T 2||0 + ||T 1 ⊙ (1 − T 2)||0.\n\nApplying theorem 3.1, we have for λ1 = λ in theorem 3.1,\n\nmin T 1,T 2\n\n−mTE(1T 1⊙T 2>0, 1T 1⊙(1−T 2)>0) + λ1||T 1||0 + λ2||1T 2∈(0,1)||0\n\n= −mTE(S∗\n\n1 , S∗\n\n2 ) + λ|S∗\n\n1 ∪ S∗\n\n2 | := L∗.\n\n(23)\n\n(24)\n\n(25)\n\n1 , S∗\n\nHere (S∗ 1 , S∗ denote the set containing all minimizers as {(S∗ if and only if P((1T 1⊙T 2>0, 1T 1⊙(1−T 2)>0) ∈ {(S∗ 1) = 0/1, P(T 2 convergence, we finally have\n\n2 ) satisfies properties described by theorem 3.1. Note the minimizer may not be unique, 2 )}. Then the equal sign in eq (23) holds 2 )}) = 1. Further noting ∀d, P(T 2 1 , S∗ d = d = 0) = 1/0, and our analysis above holds as n → ∞ with probability 1 by a.s.\n\nP( lim\n\nN→∞\n\nP(B1 ⊆ S) = 1) = 1; P( lim\n\nN→∞\n\nP(B0 ⊆ S) = 1) = 1\n\nholds.\n\nB GATE INITIALIZATION\n\nOur proposed initialization scheme is based on analysis of the linear case. Assume\n\nwhere a, b ∈ Rp represents two feature loadings. Then:\n\nf (XS1) = Xa, g(XS2 ) = Xb,\n\n1. a, b should be non-overlapping, therefore we expect |aT b| to be small.\n\n2. We should have f (X) ≈ W g(X) to maximize the mTE.\n\nThe constraint can be formulated into a regression problem W Xb = Xa, therefore a natural solution is given by a = X †W Xb = (X T X)−1X T W Xb. In this case, ||aT b|| = ||bT (X T X)−1X T W Xb|| = ||b||2 (X T X)−1X T W X . Given b is normalized, it can be shown that the optimal b corresponds to the eigenvector with least absolute eigenvalue of matrix (X T X)−1X T W X.\n\nAfter getting a, b, we select a quantile threshold over a/(a + b) to initialize the second stochastic gate layer. The first stochastic gate layer is initialized with uniform weights.\n\n17\n\nC EXPERIMENTAL DETAILS\n\nC.1 TIME-SERIES BENCHMARKING STUDY\n\nIn the study the causal processes is simulated with Python package Tigramite. Among the total 100 features, there are 6 interacting features {1, 2, 3, 4, 5, 6}. The causal links are: 1->2 with time lag 2, 2->3 with time lag 1, 5->4 with time lag 1, 1->5 with time lag 1, 3->6 with time lag 3. These features also have autocorrelations with time lags ranging from 1 to 3. There is also a latent confounder modeled by Tigramite interacting with feature 0 and feature 2. In the case of strong latent process, the latent confounder also have effects on other 43 features. All other features (93/50) not mentioned above are nuisance features with white noise dynamics. The forward operator is defined by 5-neighbor lower triangular matrix.\n\nC.1.1 ALGORITHM IMPLEMENTATION\n\nIf not particularly mentioned, default settings of the algorithms are used throughout.\n\n• VAR-LINGAM. The VAR-LINGAM algorithm is implemented in the Python package LINGAM, available at https://github.com/cdt15/lingam. VAR-LINGAM gives a weighted matrix as output. Therefore in our benchmarking study, we choose the most significant edge corresponding features with the number matching the sparsity level.\n\n• PCMCI. The PCMCI algorithm is implemented in the Python package Tigramite, which gives a weighted matrix as output. We choose the most significant edge corresponding features with the number matching the sparsity level.\n\n• GVAR. The GVAR algorithm is implemented at https://github.com/i6092467/GVAR. The sparsity parameter is set to be 1. We use the stable training option in GVAR, which trains the first and second half of the time series respectively to optimize over edge selection sparsity level then train on the whole time series, giving a binary output and no threshold selection is needed.\n\n• Grid-net. The Grid-net algorithm is implemented at https://github.com/alexw16/gridnet. The parameter set: order=5, hidden_layer_size = 10, end_epoch=50, batch_size = 50, lmbd=1 is used throughout our study. After the training finishes, we choose the most significant edge corresponding features with the number matching the sparsity level.\n\n• DCM, NGM. The two algorithms are both implemented at https://github.com/alexisbellot/ Graphical-modelling-continuous-time. For DCM, the default setting is used, and we use hidden dim = 10 for NGM. After both training finishes, we choose the most significant edge corresponding features with the number matching the sparsity level.\n\n• GEASS. We use the same training parameters in all time-series settings, with the key sparsity regularization parameter λ1 set with 0.04/0.05 based on a validation set, and the rest parameter settings are consistent with default.\n\nC.1.2 SCALABILITY ANALYSIS\n\nWe test PCMCI, GVAR, GrID-net, NGM, GEASS, GEASS+LPCMCI’s running time with consistent settings described in the above section. (LPCMCI’s setting is consistent with PCMCI’s setting). We use the same data generation pipeline and select the set of the total feature numbers as [100, 200, 400, 800, 1600].\n\nC.2 SIMULATED SPATIAL OMICS DATA BENCHMARKING STUDY\n\nIn the study the spatial omics data is simulated with Python package Scsim (Kotliar et al., 2019). 1000 genes are simulated in total, while 990 genes are cell-type-specificly expressed. The rest 10 genes each has a functional relationship (linear/nonlinear) with one cell-type-specific genes plus the noise term in order to model the cell-type-specific interactions. The data is then normalized and log-transformed according to the standard Scanpy pipeline (Wolf et al., 2018). The forward operator is defined by 4-neighbor adjacency matrix.\n\n18\n\nC.2.1 ALGORITHM IMPLEMENTATION\n\nIf not particularly mentioned, default settings of the algorithms are used throughout.\n\n• Lasso Granger. The Lasso algorithm is implemented by Scipy with tuned α (0.12) to match\n\nthe sparsity level.\n\n• NCEM. NCEM (Linear) is a linear graph neural network, which in the grid case corresponds to a standard linear regression based on neighbors and the cell type label. Based on the original work, we implemented our equivalent version by Lasso regression with α = 0.019 to match the sparsity level.\n\n• GEASS. We use the same training parameters in all settings, with the key sparsity regularization parameter λ1 set with 0.02 based on a validation set, and the latent dimension number is set to be 64.\n\n• TE. To give a fair comparison, we use the same architecture as GEASS except for the loss function is changed. We use the same training parameters in all time-series settings, with the key sparsity regularization parameter λ1 set with 0.05 based on a validation set, and the latent dimension number is set to be 64 consistent with GEASS.\n\nC.3\n\nSCRNA-SEQ PANCREAS TRAJECTORY\n\nThe data preprocessing is consistent with the scVelo tutorial: https://scvelo.readthedocs.io/ VelocityBasics/ (Bergen et al., 2020). The parameter set: λ1 = 0.06, λ2 = 0.1. Here because the gene regulatory network is fully connected and activated in cascade along the developmental trajectory, we consider the opposite initialization with b be the largest eigenvalues corresponding eigenvectors of the matrix (X T X)−1X T W X.\n\nC.4 MERFISH SPATIAL TRANSCRIPTOMICS DATA\n\nThe data is downloaded from Dryad and preprocessed with the standard Scanpy pipeline (Wolf et al., 2018): first normalize and log-transform the data by default functions in Scanpy then select 1000 highly variable genes by default functions in Scanpy (Wolf et al., 2018). The forward operator is defined by 5-neighbor adjacency matrix. The GEASS parameter set is consistent with those used in the spatial omics benchmarking.\n\nD ADDITIONAL EXPERIMENTAL RESULTS\n\nFigure 6: Spatial profiling of MERFISH human cortex slice, colored by cell type annotation and GEASS identified gene expressions. The ellipses in the first panel represent examples of vascular structures.\n\n19",
    "reference": "# Summary Of The Paper\n\nThe manuscript proposed a feature selection model for the causal discovery task based on the optimization of multi-dimensional transfer entropy of the input data by combinatorial stochastic gates. The proposed model was evaluated on by synthetic time series datasets with the presence of latent process, synthetic spatial progression of the scRNA-seq data, as well as the real (pancreatic endocrinogenesis trajectory) data. Experiment results indicate superior performance of the model over both classic and recent causal discovery methods.\n\n# Strength And Weaknesses\n\nStrength: The methodology involved in the optimized feature subset selection for causal discovery (Theorem 3.1, 3.4) and the design of the stochastic gate-based approach is important to the field of data mining. Experiment results show superior performance on both synthetic and real data, further validating the effectiveness of the model.\n\nWeakness: First of all, in section 3.2, the author mentioned that “GEASS provides both outputs of active features and embeddings produced by causally interacting features. In this paper, we emphasize the use of the former as the latter embedding output may be complex and nonlinear, potentially requiring additional architectures to maximize its interpretability.” The statement is confusing as it did not specify where the “embeddings produced by causally interacting features” were obtained: whether they were from the stochastic gates, or the MLPs indicated in Fig. 2? Further, there are no explanations for the design and purpose of the MLPs in Fig. 2.  Also, the term “STG layer” in Fig. 2 has never been mentioned throughout the manuscript. The reviewer guesses it is referring to the stochastic gates but is not sure about it.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: The manuscript is clearly written, although a full understanding of the algorithm details would need a frequent reference to the appendix and cited literature. \n\nQuality: The quality of the manuscript is good, with an extensive investigation of the algorithm design, proof of the key theorems, and insights into the model development. \n\nReproducibility: The manuscript did not provide any code repositories associated with the model. With the current description of the methodology, the work can be potentially reproduced, yet not guaranteed.\n\n# Summary Of The Review\n\nA causal discovery framework with fundamental and important algorithmic development (Theorem 3.1 and 3.4), with good practical value as well.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nONLINE BLACK-BOX ADAPTATION TO LABEL-SHIFT IN THE PRESENCE OF CONDITIONAL-SHIFT\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe consider an out-of-distribution setting where trained predictive models are deployed online in new locations (inducing conditional-shift), such that these locations are also associated with differently skewed target distributions (labelshift). While approaches for online adaptation to label-shift have recently been discussed by Wu et al. (2021), the potential presence of concurrent conditionalshift has not been considered in the literature, although one might anticipate such distributional shifts in realistic deployments. In this paper, we empirically explore the effectiveness of online adaptation methods in such situations on three synthetic and two realistic datasets, comprising both classification and regression problems. We show that it is possible to improve performance in these settings by learning additional hyper-parameters to account for the presence of conditional-shift by using appropriate validation sets.\n\n1\n\nINTRODUCTION\n\nWe consider a setting where we have black-box access to a predictive model which we are interested in deploying online in different places with skewed label distributions. For example, such situations can arise when a cloud-based, proprietary service trained on large, private datasets (like Google’s Vision APIs) serves several clients real-time in different locations. Every new deployment can be associated with label-shift. Recently, Wu et al. (2021) discuss the problem of online adaptation to label-shift, proposing two variants based on classical adaptation strategies – Online Gradient Descent (OGD) and Follow The Leader (FTH). Adapting the output of a model to a new label-distribution without an accompanying change in the label-conditioned input distribution only requires an adjustment to the predictive distribution (in principle). Therefore, both methods lend themselves to online black-box adaptation to label-shift, which makes on-device, post-hoc adjustments to the predictive distribution feasible under resource constraints.\n\nIn this paper, we empirically explore such methods when the underlying assumption of an invariant conditional distribution is broken. Such situations are likely to arise in reality. For example, in healthcare settings there are often differing rates of disease-incidence (label-shift) across different regions (Vos et al., 2020) accompanied by conditional-shift in input features at different deployment locations, for example in diagnostic radiology Cohen et al. (2021). In notation, for input variable x and target variable y, we have that P new(x | y) ̸= P (x | y) and P new(y) ̸= P (y), for a training distribution P and a test distribution P new in a new deployment location.\n\nContributions Our contributions are as follows.\n\n• We conduct an empirical study of the FTH and OGD methods introduced by Wu et al. (2021) in black-box label-shift settings with concurrent conditional-shift, a situation likely to arise in realistic deployments.\n\n• We explore the question of how to potentially improve performance in such practical settings by computing confusion matrices on OOD validation sets, and show that adding extra hyper-parameters can contribute to further improvements.\n\n• We reinterpret a simplified variant of FTH under a more general Bayesian perspective, enabling us to develop an analogous baseline for online adaptation in regression problems.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n2 BACKGROUND\n\nWe begin with a brief review of online adaptation methods for label-shift for classification problems, based on the recent discussion in Wu et al. (2021). While their motivation is temporal drift in label-distributions, we consider the case where a single model is serving several clients online in different locations, each with their own skewed label-distribution that does not change even further with time. If the training set label-distribution is P (y) and the label-distribution in the new location is P new(y), and if we assume P new(x | y) = P (x | y), then the following holds\n\nP new(y | x) =\n\nP (x | y)P new(y) P new(x)\n\n=\n\nP (y | x)P (x) P (y)\n\nP new(y) P new(x)\n\n∝\n\nP new(y) P (y)\n\nP (y | x),\n\n(1)\n\ni.e., the location-adjusted output distribution is simply a reweighting of the output distribution from the base underlying predictive model. Wu et al. (2021) follow along past work on label-shift adaptation by restricting the hypothesis space for f to be that of re-weighted classifiers, since Eq. 1 implies that one only needs to re-weight the predictive distribution to account for label-shift. The parameter vector for this classifier is simply the vector of probabilities in P new(y), henceforth referred to as p, and we will similarly use q to represent the training-set probability distribution, P (y). Given an underlying predictive model f , the adjusted classifier rule is therefore given by\n\ng(x; f, q, p) = arg max\n\ny∈[K]\n\np[y] Pf (y | x) q[y]\n\n,\n\n(2)\n\nwhere Pf (y | x) is the predictive distribution produced by an underlying base model f ; for example, a softmax distribution produced by a neural network, and there are K classes in our dataset.\n\n2.1 ONLINE ADAPTATION ALGORITHMS\n\nWu et al. (2021) present two online updating methods to estimate p – Online Gradient Descent (OGD) and Follow The History (FTH).\n\nIf we assume knowledge of a confusion matrix for a classifier f in a new location, C new(f ) ∈ RK×K, such that C new [i, j] = Px∼P new(x|y=i)(f (x) = j), then Wu et al. (2021) show that the expected error rate in this new location can be derived as a function of the label-distribution P new(y). If we represent P new(y) as a K-dimensional probability vector qnew, the expected error rate is given as\n\nf\n\nlnew(f ) =\n\nK (cid:88)\n\ni=1\n\n(cid:0)1 − Px∼P new(x|y=i)(f (x) = i)(cid:1) · qnew[i] = ⟨1 − diag(C new\n\nf\n\n), qnew⟩,\n\n(3)\n\nwhere 1 is the all-ones vector. Since we have assumed no conditional-shift so far, C new f = Cf , i.e. the confusion matrix remains invariant under label-shift. This implies one can optimize the expected error rate in the new deployment location using a confusion matrix estimated from a large in-distribution validation set, Cf , in place of C new\n\nin Eq. 3.\n\nf\n\nOnline Gradient Descent (OGD) Assuming that diag(Cf ) is differentiable wrtf , we can update f to minimize the expected error rate. We would typically not be aware of the true label-distribution in the new deployment location. However, when the confusion matrix Cf is invertible, we can compute an unbiased estimate of this distribution, given as ˆqnew = (cid:0)C ⊤ e, where e is a one-hot vector for the predicted category. Using this, Wu et al. (2021) present an unbiased gradient of lnew(f ),\n\n(cid:1)−1\n\nf\n\n∇f\n\nˆlnew(f ) = EP new\n\n(cid:2) ∂ ∂f\n\n[1 − diag(Cf )]⊤ · ˆqnew(cid:3).\n\n(4)\n\nWhen the hypothesis space is restricted to the space of re-weighted classifiers g (Eq. 2) this gradient is only over p. Wu et al. (2021) show how we might use effective numerical methods to estimate this gradient. In the online setting, p is updated after seeing new examples, hence the t + 1-th gradient update is performed by computing the gradient at the current point pt, followed by a projection to the probability simplex,\n\n∇p\n\n(cid:12) ˆlnew(p) (cid:12) (cid:12)p=pt\n\n= EP new\n\n(cid:2) ∂ ∂p\n\n[1 − diag(Cg)]⊤ · ˆqnew(cid:3)(cid:12)\n\n(cid:12) (cid:12)p=pt\n\n(5)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\npt+1 = Proj∆K−1\n\npt − η · ∇p\n\n(cid:18)\n\nˆlnew(p)\n\n(cid:19)\n\n,\n\n(cid:12) (cid:12) (cid:12)p=pt\n\n(6)\n\nwhere η is the learning rate and Proj is the projection operator.\n\nFollow The History (FTH) The update rule for pt in FTH is simpler and more efficient (in terms of memory and time complexity), given by\n\npt+1 =\n\n1 t\n\nt (cid:88)\n\nτ =1\n\nˆqnew τ\n\n,\n\n(7)\n\nτ\n\nwhere ˆqnew is the estimate for the label distribution at the τ -th iteration. Empirical evidence in Wu et al. (2021) suggests that FTH performs very competitively with OGD, and might be preferred in highly resource-constrained settings.\n\n3 UNMET ASSUMPTIONS IN PRACTICE\n\nWe now consider applying the above strategies in cases where some of the assumptions in the above section are broken. While it is difficult to make conclusive theoretical statements in situations when these assumptions break, we propose some heuristics which we evaluate empirically.\n\n3.1 THE ASSUMPTION OF INVARIANT P (x | y) CAN BREAK\n\nIn realistic deployments in new locations, it is likely that along with a differently skewed labeldistribution, the conditional distribution will change as well, i.e. P new(x | y) ̸= P (x | y). In our study, we will assume that this distributional shift only takes place within the same domain, and along (potentially spuriously-correlated) non-semantic features, leaving the semantic features intact, a setting likely to be manifested in different deployment locations.\n\nHEURISTIC 1 One possibility to adapt the above methods to settings with concurrent conditionalshift is to estimate the confusion matrix on an OOD validation set. Intuitively, an IID-estimated confusion matrix is likely to be over-confident, and a surrogate-OOD validation set can better reflect performance at test-time OOD settings.\n\nHEURISTIC 2 We propose to add extra scaling hyper-parameters in the decision rule in Eq. 2. Specifically, we add the scaling hyper-parameters λu and λy before making a test prediction,\n\n ̃g(x; f, q, p) = arg max\n\nlog Pf (y | x) + λu log p[y] − λy log q[y],\n\n(8)\n\ny∈[K]\n\nwhere we have rewritten the rule in log-space. In this formulation, log Pf (y | x) = logit[y] − Z(x), so we can drop the normalizing term. This results in a predictive rule that is a form of logitadjustment (Menon et al., 2021). Intuitively, these hyper-parameters play the role of determining how much of the training prior to “subtract”, and how much weight to assign to the pseudo-label based re-adjustment. When these magnitudes are learned on validation sets representing a combination of label-shift and conditional-shift, one can hope to further improve at novel test-time deployments.\n\n3.2 CONFUSION MATRICES CAN BE NON-INVERTIBLE\n\nExisting work on label-shift based on confusion matrices rely on a significantly large held-out validation set to estimate a robust confusion matrix. When the underlying dataset is highly classimbalanced with several categories and limited-size validation sets, one can easily end up with a non-invertible confusion matrix. Lipton et al. (2018) suggests two main possibilities – use of a soft confusion matrix, or a pseudo-inverse. In our experiments on a large-scale realistic dataset, we find both choices to lead to degraded performance. We find that simply using an identity matrix approximation can recover some of the performance drops (see Appendix E). When using FTH with an identity Cf , this corresponds to simply using the pseudo-labels up to time t to estimate the label-distribution. However, naively using the identity matrix in Eq. 7 might lead to a practical problem: after seeing the first data-point, p would be a one-hot vector, and thus enforce the same\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nprediction at the next iteration when using Eq. 2. A fix would be to use a “pseudo-count” to smooth initial conditions, which is reminiscent of Bayesian posterior updates. In the next section, we use this realization as a starting point to suggest a simpler as well as more general framework. This framework then enables us to develop an equivalent online label-shift adaptation method for regression problems.\n\n4 A BAYESIAN PERSPECTIVE\n\nIf we use the vector α to keep online counts of predictions, with an initialized α0, such that\n\nαt[k] =\n\nt (cid:88)\n\nτ =1\n\n1[ˆyτ = k] + α0 = 1[ˆyt = k] + αt−1[k],\n\nthen using an identity confusion matrix in Eq. 7 corresponds to the following update rule,\n\npt+1[k] =\n\nαt[k]\n\n(cid:80)K\n\nk′=1 αt[k′]\n\n.\n\n(9)\n\n(10)\n\nWe recognize that this update-rule corresponds exactly to the posterior predictive distribution computed using a Categorical likelihood with a Dirichlet prior, and using a recursive rule for updating the posterior. More precisely, if we use\n\nφ ∼ Dir(α), y | φ ∼ Cat(φ),\n\n(11)\n\n(12)\n\nwhere φ ∈ ∆K−1 are the parameters of the Categorical distribution, in the following update equations\n\nPt(φ) ∝ P (yt | φ) Pt−1(φ),\n\nPt+1(y) =\n\n(cid:90)\n\nφ\n\nP (y | φ) Pt(φ) dφ,\n\n(13)\n\n(14)\n\nthen we arrive at Eq. 10 using Eq. 14, and Eq. 9 using Eq. 13. See Appendix A for a derivation of Eq. 13. In practice, yt is not available to us, and we use the pseudo-label ˆyt instead, as in FTH.\n\n4.1 EXTENSION TO REGRESSION PROBLEMS\n\nWhile adaptation for regression problems has been discussed more generally (Cortes & Mohri, 2011; 2014; Zhang et al., 2013), an analogous discussion for online black-box label-shift adaptation is missing for regression. We adapt the general online update rules in Eq. 13, 14 for regression problems undergoing similar concurrent test-time distributional shifts. A natural choice is to use Gaussians to model the distributions over the continuous target variable,\n\nPf (y | x) ∝ exp\n\nP (y) ∝ exp\n\n(cid:16)\n\n(cid:16)\n\n−\n\n−\n\nλx 2\n\nλy 2\n\n(cid:18)\n\n(cid:18)\n\ny − f (x)\n\n(cid:19)2(cid:17)\n\n,\n\n(cid:19)2(cid:17)\n\n,\n\ny − m\n\n(15)\n\n(16)\n\nwhere λx, λy are the precision parameters and m is the training set mean. The parameters φ in Eq. 13 are now the mean and precision parameters for y in the new deployment location. We use the Normal-Gamma distribution to model the posterior over these parameters, since this is the conjugate distribution for Gaussians with unknown mean and precision (DeGroot, 2004),\n\nP (μnew, λnew) = N\n\n(cid:16)\n\nμnew | μ,\n\n(cid:17)\n\n1 κλnew\n\nGa(λnew | a, b).\n\n(17)\n\nCombined with the Gaussian likelihood in Eq. 14, this yields P new(y) in the form of a Student’s t-distribution,\n\n(cid:33)− 2a+1\n\n2\n\n(y − μ)2\n\n,\n\n(18)\n\n(cid:32)\n\nP new(y) ∝\n\n1 +\n\nL 2a\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTrain (r = 0.99)\n\nValidation (opposite colors with r = 0.75)\n\nTest (r = −1.0)\n\n(a) Synthetic variant of the MNIST dataset constructed by using colors to correspond to sources with skewed label-distributions. The colors are flipped for validation and test with different correlation strengths, corresponding to (almost completely) reversing the label-skew at the sources at test-time.\n\n(b) Synthetic MIX-OF-GAUSSIANS data. Differently colored regions along the x-axis correspond to training, validation and test samples, with different regions of the same color corresponding to different sources/locations.\n\nFigure 1: Synthetic MNIST and Gaussian datasets.\n\nwhere 2a is the number of degrees of freedom, and L = aκ (in log-space) takes the form\n\nb(κ+1) . Using these, our predictive function\n\narg min y\n\n(cid:16)\n\nλx 2\n\ny − f (x)\n\n(cid:17)2\n\n−\n\n(cid:16)\n\nλy 2\n\ny − m\n\n(cid:17)2\n\n+\n\n2a + 1 2\n\n(cid:16)\n\n1 +\n\nlog\n\nL 2a\n\n(y − μ)2(cid:17)\n\n.\n\n(19)\n\nSetting the derivative wrt y to zero yields a cubic equation (see Appendix B.1), which we can solve to find roots. A positive sign of the second derivative of the objective tells us if a solution is a (local) minima. When we have one real solution with a positive second derivative, we use this; when we have multiple real solutions with positive second derivatives, we pick the one that corresponds to the smallest objective; when we have no real solutions with positive second derivatives, we do not update P(y | x), retaining f (x) as the solution. Empirically, we find that the condition for no local minima does not arise for optimal choices of hyper-parameters (also see Appendix B.2).\n\nThe update equations at the t-th step follow from the computation of the posterior using Eq. 13 (see Murphy (2007), for example, for the derivation of these update steps) and are given as:\n\nat+1 = at + 1/2; κt+1 = κt + 1; μt+1 =\n\nκtμt + ˆyt+1 κt + 1\n\n; bt+1 = bt +\n\nκt(ˆyt+1 − μt)2 2(κt + 1)\n\n.\n\n(20)\n\nThe hyper-parameters λx (output precision) and κ (equivalent of the smoothing pseudo-count α0 in classification) are picked on the validation set, along with a scaling pre-multiplier for the precision λy (analogous to the classification setup). In order to place uniform priors over the output range, we will simulate a uniform set of samples over the output range. μ = E[ypseudo] is the mean of the pseudo-samples, and β is initialized as 0.5(κ − 1)Var(ypseudo) (see Appendix B.3 for details).\n\n5 EXPERIMENTS\n\nWe compare variants of online label-shift methods based on our discussion above on a mix of synthetic and realistic datasets to the un-adjusted model performance (BASE).\n\n• FTH and OGD: These are the variants proposed in Wu et al. (2021). We evaluate both for two choices of confusion matrices each – computed using the in-distribution validation set, and using the out-of-distribution validation set (our HEURISTIC 1). We refer to these two alternatives as (C-IID) and (C-OOD).\n\n• FTH-H and OGD-H: These are our modifications of FTH and OGD using the scaling hyper-parameters proposed in HEURISTIC 2. For both variants, we again evaluate two versions each, using (C-IID) and (C-OOD).\n\n• FTH-H-B: This is our modification of FTH, with an additional pseudo-count hyperparameter added for smoothing. The hyper-parameters are learned on the OOD validation sets. We call the regression variant FTH-H-B (R).\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Skewed COCO-on-Places: Synthetic dataset constructed by superimposing COCO objects (Lin et al., 2014) on scenes from the Places dataset (Zhou et al., 2017). The 5 columns correspond to 5 sources of data, where the backgrounds correspond to examples of particular scenes, and the skew in number of examples per row correspond to the skew in label distribution we impose. Different background scenes are used for training, validation, and test sets.\n\n• OPTIMAL FIXED CLASSIFIERS: These oracle methods are derived by replacing p in Eq. 2 with the empirical location-wise label distributions, providing a sense of achievable gains if one were aware of the true label-distributions from the get-go. We include two variants – OFC, which uses Eq. 2, and OFC-H, which uses the modified update rule in Eq. 8 where the hyper-parameters are oracle hyper-parameters learned on the test-set.\n\nWhen using OGD, we use the surrogate loss implementation in Wu et al. (2021) since it is both better-performing as well as much faster. This variant involves using a smooth approximation of the 0-1 loss allowing for direct gradient computation instead of a numerical approximation.\n\n5.1 CLASSIFICATION PROBLEMS\n\n5.1.1 SYNTHETIC: SKEWED-MNIST\n\nWe split MNIST classes into two subsets: [0, 1, 2, 5, 9] and [3, 4, 6, 7, 8]. We use different colors to correspond to different deployment locations, similar to Arjovsky et al. (2019). In the training set, we color digits in a particular subset a particular color 99% of the time. This corresponds to a 99% skew in label-distributions across the two locations. The 1% cross-over instructs some color-invariance but not strongly enough to completely overcome the bias. The validation set uses opposing colours for the subsets, but with a 75% correlation – this represents a scenario where the class-distributions in different locations change from that in training. Finally, the test set uses completely flipped colors in the two subsets compared to the training set – this implies reversed label-distributions, resulting in poorer baseline performance.\n\nSince the overall class frequencies are balanced in the training set, we drop the P (y) from the update rule in Eq. 2 and 8. With a 3-layer CNN trained for 20 epochs to 100% training set accuracy and 99.6% in-distribution test set accuracy, we find, in Table 1, that using online adjustments at test-time can lead to marked improvements for the base model in the test set. The numbers are averaged over 5 independent rounds of base-model training, with validation and test sets randomly shuffled for 5 trials for each round of training. (More details about dataset construction in Appendix C.1)\n\n5.1.2 SYNTHETIC: SKEWED-COCO-ON-PLACES\n\nWe construct a second, more photo-realistic, synthetic dataset by superimposing segmented objects from COCO Lin et al. (2014) on to scenes from the PLACES dataset Zhou et al. (2017), as in Ahmed et al. (2021). The scenes correspond to the notion of a deployment location, albeit with significant intra-location variation. For every such scene-represented source, we use a different class-distribution to simulate source-specific skews in the label distribution. In Fig. 2 the relative number of images per row represent the relative frequency of a particular class at a specific source. There are a total of ∼ 10K training images, ∼ 2.5K validation images (each for seen and unseen sources), and ∼ 6K test images (each for seen and unseen sources).\n\nThe validation and test sets are constructed similarly. For in-distribution validation and test sets, the same set of scenes as for training is used (with different instances), and for new-location validation and test sets, different sets of scenes are used. See Appendix C.3 for details about dataset construction. We train a ResNet-50 for 400 epochs with SGD+Momentum for the underlying model, achieving an in-distribution test accuracy of ∼ 75%. Since the overall distribution of classes is close to uniform,\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Classification problems: Average accuracy on SKEWED-MNIST, SKEWED-COCO-ONPLACES, and WILDS-IWILDCAM (also reporting macro F1-score for IWILDCAM). Overall trends indicate that our heuristics are helpful, and FTH-H-B is competitive or better without needing a confusion matrix.\n\nMethod\n\nBASE\n\nS-MNIST\n\nS-COCO-ON-PLACES\n\nIWILDCAM (Avg.)\n\nIWILDCAM (F1)\n\n82.59 ± 1.82\n\n56.09 ± 0.66\n\n73.10 ± 3.26\n\n32.70 ± 0.16\n\nFTH (C-IID). FTH (C-OOD)\n\nOGD (C-IID) OGD (C-OOD)\n\nFTH-H (C-IID) FTH-H (C-OOD)\n\n93.12 ± 1.57 96.04 ± 1.03\n\n88.32 ± 2.06 95.75 ± 0.70\n\n98.21 ± 0.47 98.69 ± 0.31\n\nOGD-H (C-IID) OGD-H (C-OOD)\n\n96.07 ± 1.76 98.91 ± 0.20\n\n58.50 ± 0.55 58.94 ± 0.63\n\n57.37 ± 0.51 57.75 ± 0.29\n\n56.72 ± 0.84 57.81 ± 0.74\n\n57.58 ± 0.79 57.12 ± 0.15\n\n71.41 ± 4.91 71.41 ± 4.91\n\n71.66 ± 4.56 73.11 ± 3.05\n\n73.75 ± 3.77 73.75 ± 3.77\n\n72.89 ± 3.30 73.36 ± 3.51\n\n29.57 ± 0.93 29.57 ± 0.93\n\n32.56 ± 0.27 32.49 ± 0.41\n\n32.46 ± 0.31 32.46 ± 0.31\n\n31.74 ± 0.51 31.36 ± 0.41\n\nFTH-H-B\n\nOFC OFC-H\n\n97.46 ± 0.64\n\n58.42 ± 0.49\n\n74.10 ± 3.56\n\n33.33 ± 1.31\n\n99.24 ± 0.20 99.26 ± 0.20\n\n75.88 ± 0.33 75.88 ± 0.33\n\n79.19 ± 1.76 81.07 ± 0.79\n\n48.61 ± 0.27 48.61 ± 0.27\n\nwe again drop the marginal P (y) term in Eq. 2 and 8. In Table 1 we again find improved performance over the unadjusted base model for all variants. Accuracy is aggregated across 20 random orderings of the test set (since the test-sets are smaller for this specific dataset), for 3 rounds of base-model training each.\n\n5.1.3 WILDS-IWILDCAM\n\nWe use the variant of the IWILDCAM 2020 dataset Beery et al. (2021) curated by the WILDS set of benchmarks for out-of-distribution (OOD) generalization Koh et al. (2021). The data consists of burst images taken at camera traps, triggered by animal motion. The task is to identify the species in the picture, and the locations correspond to the unique camera trap the pictures are from. There are a total of 182 species in this version of the dataset across a total of 323 camera traps. There is significant skew in terms of species distribution across different camera traps, as well as the number of images available for each trap. The training set consists of ∼ 130K images from 243 traps; the in-distribution validation set consists of ∼ 7.3K images from the same traps as that in the training set but on different dates; the OOD validation set consists of ∼ 15K images taken at 32 traps that are different from the ones in the training set; the in-distribution test set consists of ∼ 8.1K images taken by the same camera traps as in the training set, but on different dates from both training and validation; finally, the OOD test set consists of ∼ 43K images taken at 48 camera traps that are different from those for all other splits.\n\nKoh et al. (2021) trained ResNet-50 based models along with their curation of this dataset, also evaluating several methods for OOD generalization and releasing all models. We use their models trained with the domain generalization method CORAL Sun & Saenko (2016), since this model has improved performance over the ERM baseline. They released three sets of weights, trained with three random seeds. We evaluate all variants for each of the three seeds, with 3 random orderings each of the test set, and report aggregates in Table 1. Koh et al. (2021) recommend evaluation with both average accuracy as well as macro-F1 (since some species in the dataset are rare). We perform evaluation with both metrics, but use our own trained models for average accuracy – this is because Koh et al. (2021) trained their models optimizing for macro F1. We similarly trained CORAL-augmented base models optimizing the penalty coefficient and choice of early stopping.\n\nWe replace the confusion matrix with an identity matrix for evaluating methods on this dataset (for methods where a validation-set estimated confusion matrix is required). Confusion matrices evaluated on the validation sets are non-invertible for this dataset due to sparse class-representation and we found common alternatives to perform poorly (see Appendix E).\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Regression problems: For the GAUSSIANS dataset the metric is mean squared error (lower is better), and for the PovertyMap folds the metric is Pearson’s correlation co-efficient (higher is better), computed separately for average (ALL) and worst-group (WG) performance.\n\nDataset\n\nBASE\n\nFTH-H-B (R)\n\nMIX-OF-GAUSSIANS\n\n9.17 ± 2.17\n\n4.35 ± 1.48\n\nPOVERTYMAP Fold\n\nBASE\n\nFTH-H-B (R)\n\nPOVERTYMAP Fold\n\nBASE\n\nFTH-H-B (R)\n\nA (ALL) B (ALL) C (ALL) D (ALL) E (ALL)\n\n0.84 0.83 0.80 0.77 0.75\n\n0.84 ± 0.00 0.82 ± 0.00 0.83 ± 0.00 0.77 ± 0.00 0.75 ± 0.00\n\nA (WG) B (WG) C (WG) D (WG) E (WG)\n\n0.42 0.52 0.42 0.50 0.34\n\n0.43 ± 0.00 0.50 ± 0.01 0.56 ± 0.01 0.56 ± 0.01 0.37 ± 0.00\n\n5.2 REGRESSION PROBLEMS\n\n5.2.1 SYNTHETIC: MIX-OF-GAUSSIANS\n\nWe create a synthetic regression dataset by constructing a curve from a mixture of Gaussians. We pick regions on the x-axis to correspond to training, validation, and test sets, such that every set samples data from two regions each, corresponding to two locations (see Appendix C.2). In Figure 1b, we depict the curve, along with sampling indicators for the different sets and sources. The points have been placed at different heights for clearer visualization of overlaps. 500 points are sampled from the two training regions, and 250 each for the validation and test sets from their assigned regions. We train a 3-layer MLP with BatchNorm and ReLU activations and a mean squared loss for 100 epochs, yielding an in-distribution test mean squared error (MSE) of ∼ 0.15. In Table 2 we find that online updating reduces the OOD test MSE significantly. Results are aggregates over five trials, with a different random sampling of all data, followed by training and validation each time. Full results and more experimental details are in Appendix C.2).\n\n5.2.2 WILDS-POVERTYMAP\n\nWe use the WILDS variant of a poverty mapping dataset Yeh et al. (2020). This is a dataset for estimating average household economic conditions in a region through satellite imagery, measured by an asset wealth index computed from survey data. The data comprises 8-channel satellite images with data from 23 African countries. The locations here correspond to different countries. Due to the smaller size of the dataset, Koh et al. (2021) recommend a five-fold evaluation, where every fold is approximately constructed as follows – 10K images from 13-14 countries in the training set; 1K images from the same countries for in-distribution validation; 1K images from these countries for in-distribution testing; 4K images from 4-5 countries not in the training set for OOD validation; and 4K images from 4-5 countries in neither training nor validation sets for OOD test.\n\nThe evaluation metric is Pearson’s correlation between predicted economic index vs. actual index, as is standard in the literature (Yeh et al., 2020). Following Koh et al. (2021), we split the assessment into overall average as well as worst-group performance, which picks the worst performance across rural/urban subgroups. As with IWILDCAM, we use the CORAL-augmented base networks and weights released by Koh et al. (2021), but with our retrained versions for average correlation coefficient (since the validation choices for the released weights were for worst group performance). We evaluate separately for each fold (which have quite a bit of variance in base performance) with 5 random orderings of each of the test sets. In Table 2, we find that while there seems generally little to no improvement for average correlation, there are more significant improvements for three of five folds in terms of worst-group performance. As noted in Koh et al. (2021), a wide range of differences along many dimensions such as infrastructure, agriculture, development, cultural aspects play a role not only in determining wealth-distribution, but also in terms of how the features manifest in different places. Such real-world issues imply that validating for OOD performance is bound to be sensitive to problem types and the specific choices of validation sets used to tune hyper-parameters, and the differences that may arise between an OOD validation set and an OOD test set. This issue extends generally to all attempts at OOD generalization.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n5.3 TAKEAWAYS\n\nOur experiments are generally suggestive of the following takeaways.\n\n• While invertible confusion matrices are not always achievable due to data scarcity (as modelled in our experiments with WILDS-IWILDCAM), a practitioner can adopt confusionmatrix free methods such as FTH-H-B, which we find to provide competitive or improved performance. Using OOD validation sets to estimate confusion matrices can improve results relative to using an IID validation set, although confusion matrices estimated on smaller-sized sets can be noisy.\n\n• Learning additional scaling hyper-parameters can be useful for further improvements. We find this trend to not hold for SKEWED-COCO-ON-PLACES (FTH outperforms FTH-H and FTH-H-B). We suspect this is likely due to instability from the relatively smaller size of the validation set – when picking oracle scaling hyper-parameters on the test set, we achieve an accuracy of 59.37 ± 0.89. In Appendix D we compare performance when learning hyper-parameters on different validation sets – IID/OOD/test (oracle).\n\n6 RELATED WORK\n\nLabel-shift for classifiers Saerens et al. (2002) provides a seminal discussion about adapting the output distribution of a classifier when the test set undergoes label-shift. This approach presumes access to the entire test set up front, or a sufficiently representative sample. More recent works have investigated other ways to estimate label-shift (Lipton et al., 2018; Azizzadenesheli et al., 2019) using confusion matrices, which partially inspired the methods in Wu et al. (2021) that we use as our foundation. It has been recently suggested (Alexandari et al., 2020; Garg et al., 2020) that the simple correction method in Saerens et al. (2002) often outperforms these later methods when combined with calibration. While Alexandari et al. (2020) perform their calibration using a held-out IID validation set for their iterative method, we adapt this strategy to the out-of-distributions setting by picking scaling hyper-parameters on an OOD validation set.\n\nTest-time training Another emerging line of literature focuses on updating neural network parameters using test data without being able to match training statistics with test statistics, due to the potential lack of access to training data for the same topical reasons – data privacy and large datasets. Some examples include updating the Batch-Norm statistics optimizing for minimum test-time entropy Wang et al. (2021), or using self-supervised pseudo-labels to adapt the feature extraction part of the network Liang et al. (2020). Our setup here can be viewed as a form of test-time training, but in a more constrained setting, with inaccessible model parameters and no resources to replicate an onsite-model by querying the black-box model, e.g. using distillation (Hinton et al., 2015).\n\nOut-of-distribution generalization There has been a recent surge in interest for methods aiming to learn stable or invariant features across different domains/environments/groups Sun & Saenko (2016); Arjovsky et al. (2019); Krueger et al. (2020); Sagawa et al. (2020). Such approaches have been demonstrated to be useful for certain types of distributional shifts, such as with improved minority group robustness Sagawa et al. (2020) and systematic generalization Ahmed et al. (2021). Our discussion in this paper is complementary to this set of methods in OOD generalization research. One can use an underlying model trained with cross-group penalties that result in improved OOD generalization, and further improve performance by factoring in useful contextual information.\n\n7 CONCLUSION\n\nIn this paper, we empirically investigated the effectiveness of online black-box adaptation methods for label-shift when a key underlying assumption of invariant class-conditional input distributions is broken. We found that while existing methods can be effective to an extent regardless of conditionalshift, performance can be improved by adopting intuitive heuristics – in particular, estimating confusion matrices on OOD validation sets, and learning additional scaling hyper-parameters in the output adjustment step to account for shifting distributions.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nFaruk Ahmed, Yoshua Bengio, Harm van Seijen, and Aaron Courville. Systematic generalisation with group invariant predictions. In 9th International Conference on Learning Representations (ICLR), 2021.\n\nAmr Alexandari, Anshul Kundaje, and Avanti Shrikumar. Maximum likelihood with bias-corrected In International Conference on Machine\n\ncalibration is hard-to-beat at label shift adaptation. Learning, pp. 222–232. PMLR, 2020.\n\nMartin Arjovsky, L ́eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\n\nCoRR, 2019.\n\nKamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning\n\nfor domain adaptation under label shifts. arXiv preprint arXiv:1903.09734, 2019.\n\nSara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh Birodkar. The iwildcam 2021 competition\n\ndataset. arXiv preprint arXiv:2105.03494, 2021.\n\nJoseph Paul Cohen, Tianshi Cao, Joseph D Viviano, Chin-Wei Huang, Michael Fralick, Marzyeh Ghassemi, Muhammad Mamdani, Russell Greiner, and Yoshua Bengio. Problems in the deployment of machine-learned models in health care. CMAJ, 193(35):E1391–E1394, 2021.\n\nCorinna Cortes and Mehryar Mohri. Domain adaptation in regression. In International Conference\n\non Algorithmic Learning Theory, pp. 308–323. Springer, 2011.\n\nCorinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and\n\nalgorithm for regression. Theoretical Computer Science, 519:103–126, 2014.\n\nMorris H. DeGroot. Optimal Statistical Decisions, chapter 9, pp. 155–189. John Wiley Sons, Ltd,\n\n2004.\n\nSaurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary C Lipton. A unified view of label\n\nshift estimation. arXiv preprint arXiv:2003.07554, 2020.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv\n\npreprint arXiv:1503.02531, 2(7), 2015.\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637–5664. PMLR, 2021.\n\nDavid Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). CoRR, 2020.\n\nJian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning, pp. 6028–6039. PMLR, 2020.\n\nTsung-Yi Lin, M. Maire, Serge J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll ́ar, and C. L.\n\nZitnick. Microsoft coco: Common objects in context. ArXiv, abs/1405.0312, 2014.\n\nZachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box predictors. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3122–3130. PMLR, 10–15 Jul 2018.\n\nAditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and\n\nSanjiv Kumar. Long-tail learning via logit adjustment. ICLR, 2021.\n\nKevin P Murphy. Conjugate bayesian analysis of the gaussian distribution. https://www. cs.ubc.ca/ ̃murphyk/Papers/bayesGauss.pdf, 2007. [Online; accessed 19-January2022].\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMarco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to new a priori probabilities: A simple procedure. Neural Comput., 14(1):21–41, jan 2002. ISSN 0899-7667.\n\nShiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. ICLR, 2020.\n\nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation.\n\nComputer Vision – ECCV 2016 Workshops, pp. 443–450, 2016.\n\nTheo Vos et al. Global burden of 369 diseases and injuries in 204 countries and territories, 1990-2019: a systematic analysis for the global burden of disease study 2019. Lancet, 396(10258):1204–1222, 2020.\n\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully testtime adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c.\n\nRuihan Wu, Chuan Guo, Yi Su, and Kilian Q Weinberger. Online adaptation to label distribution\n\nshift. Advances in Neural Information Processing Systems, 34, 2021.\n\nChristopher Yeh, Anthony Perez, Anne Driscoll, George Azzari, Zhongyi Tang, David Lobell, Stefano Ermon, and Marshall Burke. Using publicly available satellite imagery and deep learning to understand economic well-being in africa. Nature communications, 11(1):1–11, 2020.\n\nKun Zhang, Bernhard Sch ̈olkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under target and conditional shift. In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28, 2013.\n\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA POSTERIOR UPDATE\n\nWe derive the posterior update equation (Eq. 13), specifying the conditions under which this rule holds. The key assumption is that in the new deployment location, categories are encountered in an IID manner in the location, i.e., yj ⊥⊥ yk. Pt(φ) = P (φ | y1, · · · , yt),\n\n(21)\n\n=\n\nP (y1, · · · , yt | φ) P (φ) P(y1, · · · , yt) ∝ P (y1, · · · , yt | φ) P (φ),\n\n,\n\n(Bayes rule)\n\n(dropping terms independent of φ)\n\n=\n\nt (cid:89)\n\ni=1\n\nP (yi | φ) P (φ)\n\n(using assumption yj ⊥⊥ yk)\n\n(cid:32) t−1 (cid:89)\n\n= P (yt | φ)\n\ni=1\n\n(cid:33)\n\nP (yi | φ) P (φ)\n\n(regrouping terms)\n\n= P (yt | φ) Pt−1(φ),\n\n(by definition)\n\nB REGRESSION MODEL\n\nB.1 FINDING THE OPTIMAL SOLUTION FROM THE PREDICTIVE RULE\n\nThe required distributions are defined as\n\n(cid:32)\n\nP (y | x) ∝ exp\n\n−\n\n(cid:17)2(cid:33) ,\n\ny − f (x)\n\n(cid:16)\n\nλx 2\n\n(cid:32)\n\nP new(y) ∝\n\n1 +\n\nL 2a\n\n(y − μ)2\n\n(cid:33)− 2a+1\n\n2\n\n,\n\n,\n\n(cid:32)\n\nP (y) ∝ exp\n\n−\n\n(cid:17)2(cid:33)\n\ny − m\n\n(cid:16)\n\nλy 2\n\nwhich gives us the objective J = − log P (y | x) expressed as J = − log P (y | x) − log P new(y) + log P (y)\n\nλx λy 2\n2 The derivative of this objective wrt y is\n\ny − f (x)\n\n=\n\n−\n\n(cid:17)2\n\n(cid:16)\n\n(cid:16)\n\ny − m\n\n(cid:17)2\n\n+\n\n2a + 1 2\n\n(cid:16)\n\n1 +\n\nlog\n\n(y − μ)2(cid:17)\n\nL 2a\n\n∂J ∂y\n\n= λx(y − f (x)) − λy(y − m) +\n\n= λx(y − f (x)) − λy(y − m) +\n\nL\n\n2a+1 (cid:3)2 1 + L\n\n2a .(cid:1)2.(y − μ) 2a (y − μ)2\n\n2a (y − μ)\n\n(2a + 1) L 1 + L\n\n2a (y − μ)2\n\nA (cid:122) (cid:125)(cid:124) (cid:123) (2a + 1)\n\nM (cid:122)(cid:125)(cid:124)(cid:123) L\n2a\n\n(y − μ)\n\n1 + L\n\n2a (y − μ)2\n\n+\n\n= (cid:0)λx − λy\n\n(cid:124)\n\n(cid:123)(cid:122) τd\n\n(cid:1)\n\n(cid:125)\n\ny + (cid:0)λym − λxf (x)(cid:1) (cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) τμ\n\n= τdy + τμ +\n\nAM (y − μ) 1 + M (y − μ)2\n\nSetting to zero, we have\n\n(cid:16)\n\nτdy + τμ\n\n(cid:17)(cid:16)\n\n1 + M (y − μ)2(cid:17)\n\n+ AM (y − μ) = 0\n\n12\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:16)\n\n=⇒\n\nτdy + τμ\n\n(cid:17)(cid:16)\n\n1 + M y2 + M μ2 − 2M μy\n\n(cid:17)\n\n+ AM (y − μ) = 0\n\n(38)\n\n=⇒ τdy + M τdy3 + M μ2τdy − 2M μτdy2 + τμ + M τμy2 + M τμμ2 − 2M μτμy + AM y − AM μ = 0\n\n(39)\n\n=⇒ M τdy3 + (M τμ − 2M μτd)y2 + (τd + M μ2τd − 2M μτμ + AM )y + (τμ + M τμμ2 − AM μ) = 0\n\n(40)\n\nwhich is the equation we shall solve for y. We use NUMPYs polynomial solver to find roots. A cubic equation either has one real and a pair of conjugate imaginary roots, or all real roots. We test the real solutions for a positive curvature (implying local minima), and pick the minima resulting in smallest value of the objective J.\n\nB.2 SECOND DERIVATIVE TEST FOR SOLUTIONS\n\nThe second derivative of J is given by\n\nτd −\n\n2AM 2(y − μ)2 (1 + M (y − μ)2)2 +\n\nAM 1 + M (y − μ)2\n\n(41)\n\nWriting y − μ as D, we have\n\nτd +\n\nAM (1 + M D2)\n\n−\n\n2AM 2D2 (1 + M D2)2 = τd +\n\nAM 1 + M D2\n\n(cid:16)\n\n1 −\n\n2M D2 1 + M D2\n\n(cid:17)\n\n= τd +\n\nAM (1 − M D2) (1 + M D2)2\n\n(42)\n\nWhen this expression is positive, we have a local minima.\n\nFor the first term to be positive, we require that τd > 0, which has a straightforward intuitive interpretation: τx > τy, i.e. output precision should be higher than marginal-adjustment precision. This is a reasonable condition which we expect to be fulfilled, since we typically expect to rely more strongly on the underlying predictive model than simply the marginal.\n\nIn the second term, AM is always non-negative, for a positive pseudo-count. The denominator is always positive. Substituting in expressions for the values after the t-th update, we have\n\nM D2 =\n\nκt\n\nκt+1 (y − μt)2 κτ +1 (ˆyτ +1 − μτ )2\n\nκτ\n\n(cid:80)t−1 τ =0\n\n.\n\n(43)\n\nWhen this term is ≤ 1, we are guaranteed positivity (strictly speaking, τd provides the second term with some room for negative values, but we ignore it for simplified reasoning). This condition implies\n\n(y − μt)2 ≤\n\nκt + 1 κt\n\nt−1 (cid:88)\n\nτ =0\n\nκτ κτ + 1\n\n(ˆyτ +1 − μτ )2,\n\n(44)\n\nwhich then implies that the following range for y allows local minima\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nμt −\n\nκt + 1 κt\n\nt−1 (cid:88)\n\nτ =0\n\nκτ κτ + 1\n\n(ˆyτ +1 − μτ )2 ≤ y ≤ μt +\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nκt + 1 κt\n\nt−1 (cid:88)\n\nτ =0\n\nκτ κτ + 1\n\n(ˆyτ +1 − μτ )2.\n\n(45)\n\nAn intuitive interpretation of this condition is that valid updates are allowed within an increasing range as a function of the total observed variances up to the t-th test example. In practice, we find that validation tends to pick values for τx > τy, and that the case for no-local-minima typically does not arise for the optimal hyper-parameters in our experiments.\n\nB.3\n\nINITIALIZING PRIORS\n\nFor initializing priors, we might endeavour to stay unbiased, since we assume that deployment locations can have significantly different target distributions than we might anticipate from the marginal over the training set. For classification, we built this in by using a uniform pseudo-count for all classes and sources. For regression, we simulate a pseudo-count of uniform samples from the output range.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nIf we start with a reference prior for the Normal-Gamma distribution with parameter settings\n\nμ = ., κ = 0, α = −0.5, β = 0,\n\n(46)\n\nthen after observing a N data-points {y1, · · · , yN }, yi ∼ U[L, H] (the uniformly sampled points we will simulate), the resulting posterior is\n\nμ =\n\n1 N\n\nN (cid:88)\n\ni=1\n\nyi,\n\nκ = N,\n\nα =\n\nβ =\n\n,\n\nN − 1 2\nN (cid:88)\n\n1 2\n\ni=1\n\n(yi − μ)2.\n\n(47)\n\n(48)\n\n(49)\n\n(50)\n\nIn this view, κ corresponds to the pseudo-count (as per the interpretation of the parameters of the Normal-Gamma conjugate prior as in Murphy (2007)). α is defined in terms of κ. To improve stability, we will set μ to the middle of the output range rather than actually estimate the mean of our uniform pseudo-samples. Likewise, we will set β by estimating its value as a function of κ and using the expression for variance of a uniform distribution,\n\nE[β] =\n\n1 2\n\n(κ − 1)Var(yi) = (κ − 1)\n\n(H − L)2 24\n\n.\n\n(51)\n\nC EXPERIMENTAL DETAILS\n\nC.1 SYNTHETIC MNIST\n\nThe splitting of digits into two sets is performed by observing mis-classification matrices after 200 iterations of training a neural network averaged across a 100 runs – digits are put into opposing sets if they tend to be confused, while also trying to keep the set-sizes balanced.\n\nThe network architecture consists of 3 CONV layers with 64, 128 and 256 channels, each followed by MAXPOOL, BATCHNORM, and RELU. After the third layer, we spatially mean-pool activations and use a linear layer to map to the logits. A weight-decay of 5e − 4 is applied on all parameters. Training is conducted for 20 epochs with batches of size 256 where training accuracy saturates to 100%. An initial learning rate of 0.1 is used, which is cut by 5 at the 6-th, 12-th and 16-th epochs.\n\nThe datapoint-counts in the train/val/test environments are as follows.\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nTrain\n\nIID validation\n\nOOD validation\n\nOOD Test\n\nred cyan\n\nred cyan\n\ncyan red\n\ncyan red\n\n4889 43\n\n989 2\n\n687 304\n\n980 0\n\n5614 64\n\n1052 12\n\n714 350\n\n1135 0\n\n4915 53\n\n985 5\n\n689 301\n\n1032 0\n\n38 5063\n\n7 1023\n\n313 717\n\n0 1010\n\n49 4810\n\n4664 42\n\n57 4894\n\n10 973\n\n304 679\n\n0 982\n\n904 11\n\n635 280\n\n892 0\n\n12 955\n\n310 657\n\n0 958\n\n59 5116\n\n8 1082\n\n315 775\n\n0 1028\n\n41 4801\n\n4946 42\n\n12 997\n\n301 708\n\n0 974\n\n949 12\n\n664 297\n\n1009 0\n\nC.2 SYNTHETIC GAUSSIAN\n\nThe synthetic data for this experiment is generated with the following function\n\ny(x) = 10N (y | x; μ = −2, σ = 0.5) + 3N (y | x; μ = 2, σ = 0.5) + 6N (y | x; μ = 0, σ = 1)\n\nTraining points: Training points are sampled from two regions on the x-axis, x ∼ N (−2, 0.4) and x ∼ N (2, 0.2), with 250 points each.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nOOD validation points: OOD validation points are sampled from N (−3.5, 0.2) and N (1, 0.2), with 250 points each.\n\nOOD test points: OOD test points are sampled from N (0, 0.2) and N (3, 0.2), with 250 points each.\n\nFor OOD sets, the different sampling distributions correspond to different locations. For different trials, we repeat the whole experiment from scratch, sampling new training, validation, and test sets, and performing validation every time.\n\nThe network architecture is a 3 layer MLP with 128 hidden units, with BATCHNORM and RELU after hidden activations. A weight decay of 1e − 8 is applied on all parameters. We train for a 100 epochs with batch-sizes of 100, with SGD + Momentum (0.9), starting with an initial learning rate of 0.01 and scaling it by 0.95 after every epoch.\n\nWe include the non-aggregated MSEs below to confirm that there are consistent improvements over every base model/data-sampling individually.\n\nSeed\n\nIID-Base OOD-Base OOD-Online\n\n0 1\n2 3\n4\n\n0.08 0.13 0.16 0.19 0.21\n\n11.23 12.37 6.13 9.14 7.00\n\n3.14 3.82 3.00 5.50 6.31\n\nC.3 SYNTHETIC SKEWED-COCO-ON-PLACES\n\nWe chose the following objects for this synthetic classification task: bicycle, train, cat, chair, horse, motorcycle, bus, dog, couch, and zebra; and the following scenes to simulate different sources.\n\nTraining: beach, canyon, building facade, desert/sand, iceberg\n\nOOD validation: oast house, orchard, crevasse, ball pit, viaduct\n\nOOD test: water tower, staircase, waterfall, bamboo forest, zen garden\n\nWhen there are multiple instances of a class in an image, we pick the instance occupying largest area, such that only images with objects occupying at least 10K pixels are retained. All images are resized to 256 × 256.\n\nAcross the 5 sources, the number of examples for training, validation, and test sets are as follows.\n\nTable 3: Training set\n\nbicycle\n\ntrain\n\nbeach canyon building facade desert/sand iceberg\n\n669 135 5\n0 0\n\n669 329 34 0\n0\n\ncat\n\n429 513 132 6\n0\n\nchair\n\nhorse motorcycle\n\n176 513 322 35 0\n\n46 329 503 135 7\n\n7 135 503 329 46\n\nbus\n\n0 35 322 513 176\n\ndog\n\n0 6\n132 513 429\n\ncouch\n\nzebra\n\n0 0\n34 329 669\n\n0 0\n5 135 669\n\nTable 4: Validation sets\n\nbicycle\n\ntrain\n\nbeach canyon building facade desert/sand iceberg\n\n167 33 1\n0 0\n\n167 82 8\n0 0\n\ncat\n\n107 128 33 1\n0\n\nchair\n\nhorse motorcycle\n\n44 128 80 8\n0\n\n11 82 125 33 1\n\n1 33 125 82 11\n\nbus\n\n0 8\n80 128 44\n\ndog\n\n0 1\n33 128 107\n\ncouch\n\nzebra\n\n0 0\n8 82 167\n\n0 0\n1 33 167\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Test sets\n\nbicycle\n\ntrain\n\nbeach canyon building facade desert/sand iceberg\n\n401 81 3\n0 0\n\n401 197 20 0\n0\n\ncat\n\n257 308 79 3\n0\n\nchair\n\nhorse motorcycle\n\n105 308 193 21 0\n\n27 197 302 81 4\n\n4 81 302 197 27\n\nbus\n\n0 21 193 308 105\n\ndog\n\n0 3\n79 308 257\n\ncouch\n\nzebra\n\n0 0\n20 197 401\n\n0 0\n3 81 401\n\nNote that the pattern of label-shift is the same across validation and test subsets (albeit of a smaller size). This proof-of-concept experiment is intended as a middle-ground between the COLORED MNIST and WILDS-IWILDCAM experiments, in that the potential of learning hyper-parameters to account for conditional shift is tested while keeping label-shift pattern fixed).\n\nWe train for 400 epochs with SGD + Momentum (0.9), using batch sizes of 128, with an initial learning rate of 0.1 which is cut by 5 at the 240th, 320th, 360th epochs. An L2 weight decay regulariser is applied on all parameters with a coefficient of 5e−4. We normalize images with the training set mean and standard deviation per channel, and apply data augmentation of random crops to 224 × 224 and random horizontal reflections.\n\nD HYPER-PARAMETER SELECTION\n\nWe contrast performance when methods use IID validation sets vs. OOD validation sets vs. the test set itself, in Table 6 . We observe that, generally speaking, OOD validation can improve over IID validation.\n\nE IDENTITY APPROXIMATION FOR CONFUSION MATRIX\n\nDegenerate confusion matrices can arise when there are missing categories in the validation set used to compute it (leading to zero-rows), or if two or more rows are exactly the same (for example, when multiple rare categories both get categorized the same way). Two options are to use a soft-confusion matrix, or a pseudo-inverse (Lipton et al., 2018). Since the IWILDCAM dataset is significantly long-tailed, with a large number of classes not represented in the validation sets, we end up with a number of zero rows for the soft-confusion matrix. For such rows, we simply placed a 1 in the diagonal element.\n\nIn Table 7, we find these alternatives to result in degraded performance for IWILDCAM, generally much worse than our identity approximation. We hypothesize that part of the reason is to do with the fact that both our zero-confusion heuristic for dealing with missing classes for the soft-confusion matrix, as well as the same underlying effect being applied by the pseudo-inverse results in a misleading effect: rare classes, absent from validation sets, are in fact more likely to be confused than the frequent ones. This is one possibility for why the less presumptive identity approximation performs better. The inherent difficulty is estimating robust confusion matrices has been recognized in the literature, with the typical approach being to hold out significantly large validation sets in order to reliably estimate less noisy confusion matrices. In Table 8, we include numbers from an identity approximation in the synthetic datasets where the confusion matrices were invertible.\n\nOn the whole, we suggest to practitioners that in difficult, real-life situations, simpler approximations might continue to serve us well, while more sophisticated methods can pose specific requirements to be successful.\n\nF HYPERPARAMETERS, COMPUTE, AND CODE AND DATA LICENSES.\n\nThe hyper-parameters involved are the two calibration terms λu, λy and the pseudo-count term α0 for classification, and λx, λy, κ for the regression problems. These were picked via grid-search on the OOD validation sets, optimizing for OOD performance in all cases. For OGD methods, an additional\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: (top) Classification problems: Performance when picking hyper-parameters on IID, OOD validation sets, or on (Oracle) test sets. (bottom) Regression problems: Performance when picking hyper-parameters on IID, OOD validation sets, or on (Oracle) test sets. For MIX-OF-GAUSSIANS, we use mean squared error as the metric (lower is better), while for POVERTYMAP the metric is the Pearson’s correlation co-efficient (higher is better).\n\nDatasets\n\nMethods\n\nIID validation OOD validation\n\nOracle\n\nS-MNIST\n\nS-COCO-ON-PLACES\n\nIWILDCAM (AVG)\n\nIWILDCAM (F1)\n\nFTH-H OGD OGD-H FTH-H-B\n\nFTH-H OGD OGD-H FTH-H-B\n\nFTH-H OGD OGD-H FTH-H-B\n\nFTH-H OGD OGD-H FTH-H-B\n\n82.67 ± 1.79 82.75 ± 1.77 82.59 ± 1.82 83.00 ± 1.79\n\n57.42 ± 0.53 57.72 ± 0.31 57.31 ± 0.68 58.59 ± 1.02\n\n73.52 ± 3.36 69.42 ± 5.10 73.41 ± 3.42 73.90 ± 3.93\n\n31.93 ± 1.56 29.37 ± 2.15 32.09 ± 0.29 32.73 ± 2.78\n\n98.69 ± 0.30 95.75 ± 0.70 98.91 ± 0.20 97.46 ± 0.64\n\n57.81 ± 0.74 57.75 ± 0.29 57.12 ± 0.15 58.42 ± 0.49\n\n73.75 ± 3.77 73.11 ± 3.05 73.36 ± 3.51 74.10 ± 3.56\n\n32.46 ± 0.31 32.49 ± 0.41 31.36 ± 0.41 33.33 ± 1.31\n\n98.69 ± 0.30 95.75 ± 0.70 98.91 ± 0.20 98.35 ± 0.52\n\n59.05 ± 0.53 57.75 ± 0.29 58.10 ± 0.85 59.37 ± 0.89\n\n74.13 ± 3.54 73.16 ± 3.15 73.53 ± 3.29 74.41 ± 3.65\n\n33.81 ± 0.30 32.72 ± 0.06 32.72 ± 0.15 33.33 ± 1.31\n\nDatasets\n\nIID validation OOD validation Oracle validation\n\nMIX-OF-GAUSSIANS\n\n9.24 ± 2.76\n\n4.35 ± 1.48\n\nPOVERTYMAP-A (ALL) POVERTYMAP-B (ALL) POVERTYMAP-B (ALL) POVERTYMAP-B (ALL) POVERTYMAP-B (ALL)\n\nPOVERTYMAP-A (WG) POVERTYMAP-A (WG) POVERTYMAP-A (WG) POVERTYMAP-A (WG) POVERTYMAP-A (WG)\n\n0.80 ± 0.00 0.82 ± 0.00 0.82 ± 0.00 0.78 ± 0.01 0.72 ± 0.01\n\n0.43 ± 0.00 0.33 ± 0.03 0.50 ± 0.01 0.46 ± 0.04 0.36 ± 0.02\n\n0.84 ± 0.00 0.82 ± 0.00 0.83 ± 0.00 0.77 ± 0.00 0.75 ± 0.00\n\n0.43 ± 0.00 0.50 ± 0.01 0.56 ± 0.01 0.56 ± 0.01 0.37 ± 0.00\n\n1.76 ± 0.59\n\n0.84 ± 0.00 0.83 ± 0.00 0.83 ± 0.00 0.78 ± 0.00 0.75 ± 0.00\n\n0.45 ± 0.02 0.52 ± 0.00 0.58 ± 0.02 0.57 ± 0.02 0.37 ± 0.00\n\nhyper-parameter is the learning rate used for updating p. This learning rate is searched over a range from 1e-8 to 10 in steps of ×10.\n\nV100 GPUs were used to train base models (in cases where we trained our own models), and the online adjustment experiments were performed on an Apple Macbook Air with saved outputs from the models.\n\nWe reused code from https://github.com/p-lambda/wilds, released under the MIT License, and code from https://github.com/wrh14/online_adaption_to_label_ distribution_shift, publicly released by Wu et al. (2021). We also used data from MS-COCO, released under the CREATIVE COMMONS ATTRIBUTION 4.0 LICENSE. WILDSIWILDCAM is under COMMUNITY DATA LICENSE AGREEMENT – PERMISSIVE – V1.0, and the WILDS-POVERTYMAP data is U.S. PUBLIC DOMAIN (LANDSAT/DMSP/VIIRS).\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: We compare use of a soft-confusion matrix and the pseudo-inverse with our approximation with an identity matrix for IWILDCAM. We find that FTH performance drops strongly, and for OGD, the optimal learning rate is most often zero, leading to no differences with base performance. For OGD, we find the optimal learning rate on the test-set for all choices of confusion matrix, reporting best-case performance.\n\nDataset\n\nMethod\n\nSoft confusion matrix\n\nPseudo-Inverse\n\nIdentity\n\nIWILDCAM (AVG)\n\nIWILDCAM (MACRO-F1)\n\nFTH (C-IID) FTH (C-OOD) OGD (C-IID) OGD (C-OOD)\n\nFTH (C-IID) FTH (C-OOD) OGD (C-IID) OGD (C-OOD)\n\n43.41 ± 21.80 34.56 ± 16.71 73.10 ± 3.26 73.10 ± 3.26\n\n22.42 ± 4.33 23.73 ± 3.36 32.71 ± 0.18 32.71 ± 0.14\n\n37.23 ± 19.34 28.20 ± 13.74 73.29 ± 3.04 73.10 ± 3.26\n\n11.33 ± 0.26 10.82 ± 4.64 32.70 ± 0.16 32.70 ± 0.16\n\n71.41 ± 4.91 71.41 ± 4.91 73.16 ± 3.33 73.17 ± 3.18\n\n29.57 ± 0.93 29.57 ± 0.93 32.75 ± 0.17 32.70 ± 0.16\n\nTable 8: Identity approximation with S-MNIST and S-COCO-ON-PLACES, with test-time performance using the original confusion matrix Cf for reference. When using the identity approximation, OGD (IID) uses the IID validation set to estimate Cg and OGD (OOD) uses the OOD validation set.\n\nDataset\n\nMethod\n\nIdentity approximation\n\nOriginal\n\nS-MNIST\n\nS-COCO-ON-PLACES\n\nFTH OGD (IID) OGD (OOD)\n\nFTH OGD (IID) OGD (OOD)\n\n96.02 ± 1.07 89.47 ± 1.96 95.70 ± 0.68\n\n59.27 ± 0.64 57.48 ± 0.52 56.02 ± 0.35\n\n96.04 ± 1.03 88.32 ± 2.06 95.75 ± 0.70\n\n58.94 ± 0.63 57.37 ± 0.51 57.75 ± 0.29\n\n18",
    "reference": "# Summary Of The Paper\n\nIt is well known that the performance of machine learning models is highly dependent on the distribution of the data on which it is evaluated: model performance deteriorates when tested on data generated from a distribution shifted with respect to the training data generating process. Identifying and mitigating the effects of distribution shifts is a major open challenge for machine learning practitioners, as distribution shifts are ubiquitous in an ever-changing world. In the supervised learning context, evaluating test performance and mitigating it usually require labelled testing data, which is often difficult or impossible to obtain. \n\nLittle can be done about arbitrary distribution shifts – generalization from training to test data is only possible if the shift leaves some structure in the data unchanged. Label shift is a basic example of such a distribution shift, where the conditional probability P(X|Y) remains fixed, and only P(Y) changes. Here X are the covariates, Y the label, and P(X,Y) = P(X|Y)P(Y) is their joint distribution. Recent years saw much progress with the analysis of label shift, and methods have been developed to mitigate its impact on black box models – with deep learning a primary application -- in both offline and online settings. Essentially, these methods rely on re-weighting model predictions using the distribution of predicted (pseudo-)labels, and thus do not require true labels for the test data. \n\nThe current paper follows three goals related to label shift adaptation:\n\n1. The paper’s main effort focuses on examining how previously proposed label shift mitigation methods perform on shifted distributions do not satisfy the label shift condition – a scenario highly relevant to real-world applications, where often as pure label shifts are rare. In an online learning setting, albeit one in which the distribution does not shift continuously, the paper examines empirically how recently proposed algorithms for online adaptation to label shift perform on a few synthetic and realistic datasets that exemplify different kinds of “non-label” distribution shift. The empirical investigation also considers a couple of heuristically-motivated extensions to these algorithms, most notably performing model selection on an OOD validation set which is shifter with respect to both the training and test sets. The findings of these investigations are not clear cut, but suggest that in some cases, the proposed algorithms provide an improved adaptation to the distribution shift. The takeaway is that label shift adaptation methods (or some heuristic generalization thereof) might sometimes be useful to mitigate general distribution shifts, even if this practice has no known theoretical justification.\n\nThe paper considers two further issues related to label shift adaptation:\n\n2. Past work on label shift adaptation has mostly focused on classification problems. The paper proposes an algorithm for label shift adaptation in regression settings, and studies it empirically.\n3. Past algorithms for online label shift adaptation require the inversion of an empirically measured confusion matrix. The paper suggests a heuristic fix for the case when this matrix is non-invertible and studies it empirically. \n\nThe latter two issues are discussed briefly (compared to the main topic of the paper), and here too the investigations do not provide clear cut conclusions on the efficacy of the proposed methods, but in some cases these methods perform better than the baseline.\n\n# Strength And Weaknesses\n\nMajor Strengths: \n\n1. The problem investigated is well motivated. Distribution shifts are indeed a big and relevant problem when machine learning models are deployed in the real world. Much of the work to date has focused on idealized types of shifts, like label or covariate shift. It is natural to wonder how much methods developed for idealized shifts might be useful in more realistic settings. Furthermore, if label shift adaptation methods generalize to realistic shift scenarios, they are attractive from a practical standpoint, as they do not require labelled test data. \n2. Empirical results are, for the most part (except for some comments below), clearly presented: I could understand what was done and believe I have enough information to attempt to reproduce the results. \n3. The paper is quite honest about the inconclusive nature of much of the results, and does not try to oversell the proposed methods. \n\nMajor Weaknesses: \n\n1. A systematic or principled approach to the types of distribution shifts considered is missing. Distributions can shift in many ways and for many reasons. Adding conditional shift to label shift is tantamount to considering general distribution shifts. Indeed, the paper considers two examples with no label shift (P(Y) is not changed in the synthetic MNIST and COCO-on-Places datasets), an example with covariate shift (Mixture of Gaussians), and two general distribution shifts (from the WILDS dataset). Framing the issue as “label shift in the presence of conditional shift” might give a wrong impression that the conditional shift is a perturbation of the label shift condition. I find it clearer to state that general distribution shifts are considered. \nLittle can be said about distribution shifts in general, without focusing on particular types or characteristics of the shifts, such as label/covariate shift, subpopulation shift [6], or shifts where the data generating process has a fixed known causal structure [3]-[5]. Since experiments in the paper do not belong to a particular type of shift, it is hard to compare results or to generalize from them to general shifts. \nThe lack of a systematic approach to general label shifts is reflected also in the absence of discussion of relevant work on this issue, including refs [1]—[6].\n\n2. Given the vast scope of possible distribution shifts, with no systemic understanding of how they relate to or differ from label shift, and with heuristic methods lacking a theoretical foundation – given these, a major and comprehensive empirical study is necessary in order to ascertain the usefulness of the proposed methods. The paper offers modest experiments, in terms of types and strengths of shift, types of data, and alternative baselines/methods. This severely limits the usefulness of the results, as it is unclear when the suggested methods can be expected to improve upon baselines, and how good such improvement are compared to alternative methods. As it stands, few generalizable insights can be drawn from the empirical scope of the paper. The paper itself is honest about the modest and tentative nature of the findings, when it concludes that the experiments are “suggestive” that the proposed methods show “promising trends for the most part” in the limited scope in which they were tested. \nConcretely, for the experiments performed, here are some suggestions of baselines/methods that might provide a wider context for obtained results: \n  a. An optimal fixed classifier, as considered by Wu et al. (2021).\n  b. Results obtained from offline domain adaptation methods (Garg et al., 2020).\n  c. Results obtained from known domain generalization methods such as those mentioned in the related works section of the paper, or the ones surveyed by Gulrajani & Lopez-Paz (2020). In particular, if I understand correctly, CORAL was used for the two WILDS datasets considered in the paper, but not the others. It might be more informative to test all datasets with and without CORALS (and/or other domain adaptation methods). \n  d. The paper emphasizes the importance of the use of an OOD validation set. It would thus be useful to test the effect of this OOD validation set on test performance by considering the effect of different validation sets, preferably with different characteristics. For example, for the synthetic colored MNIST dataset, one could use validation sets that are more or less correlated with the test sets. \n\n3. Goals 2+3 above are not explored in detail in the paper. No references are given to prior work on regression label shift / domain adaptation (e.g., [7]-[8] below), nor to the discussion in Lipton et al. (2018, section 7) about remedies to non-invertible empirical confusion matrices. The corresponding experiments provide only an initial investigation into them. The paper provides some interesting but embryonic discussion/exploration of both. Their inclusion in the current form of the paper \n\n4. Some key definitions and explanations are lacking in the paper, making it difficult to understand some sections of it.\n  a. “Conditional shift” is not defined. While it is a term used in the literature and whose meaning might be intuitive, many other terms are used in the literature as well. To guarantee that there are no misunderstandings regarding this central concept, its definition should be provided. \n  b. Method FTH-H-B and FTH-H-B (R) are never clearly defined (what is the “pseudo-count hyper-parameter” mentioned? I did not understand).\n  c. In equation (3), the definition of the expected error rate \\ell^{\\test{new}} is only given in words, not in a formula. \n  d. In section 4.1, what are a, b, kappa, and mu?   \n  e. In appendix A, none of the notation is defined, and in fact no information is given about the context and goal of the derivation there. \n\nFurther comments\n\n1. Online vs offline methods. The scope of label shifts considered in this paper is more limited than those considered by Wu et al.: here only constant shifts are considered (test data is drawn from a fixed shifted distribution), whereas Wu et al considered distributions that keep changing throughout training. An important strength of online methods are their ability to deal with continual changes. Considering only constant changes reduces (but does not invalidate) the usefulness of online methods compared to offline ones. The decision to focus on online methods should be motivated in the paper. \n\n2. OOD validation: the concept of OOD validation is introduced in Heuristic 1 without being properly defined/explained. As this is a central tenet in the proposed methods, the idea and procedure should have a clear and detailed explanation. Furthermore, in Heuristic 1 it is written that OOD validation is a standard practice of model selection, with a reference to Gularjani & Lopez-Paz (2020). As far as I can tell, this reference (which emphasizes the importance of validation set details in the context of domain generalization) does not advocate the use of validation on a separate OOD set. Rather, it attributes this method to Krueger et al. (2020), who indeed mention it in an appendix. \nRegarding the method itself OOD validation itself: why should it work? I can understand that it might be useful when the shifts in the validation and test sets are somehow related (like the Skewed-MNIST example where test is a more severe shift of the same type as validation), but why would it help in examples like the mixture of Gaussians, or the WILDS datasets? Looking at the experiment results, it indeed seems to me that OOD validation is helpful only for the skewed-MNIST example. If my reading is correct, this should be stated clearly, and the appropriate qualifications should be made in the conclusions about the merits of OOD validation. Currently, section 5.3 states that “Using OOD validation sets … improves results on the whole” – but for S-COCO-on-Places and iWildCam (Avg) I do not see any improvement more significant than the noise level, and for iWildCam (F1) there is a small deterioration (which is also consistent with noise). \nFrom a practical perspective, performing OOD validation is not always possible as it requires more labelled data – it would be useful to emphasize this fact. Technically – what are all the optimization steps performed on this validation set? I.e., which hyper-parameters are calculated on this validation set, other than the confusion matrix? \n\n3. Non-invertible confusion matrices. The methods proposed in Heuristic 3 surely generate invertible matrices, but why would they be expected to work for label shift and general distribution shift adaptation? They seem to me ad-hoc and unmotivated. What would be their merit compared to using a pseudo inverse, or the soft probability matrix suggested by Lipton et al. (2018)? \n\n4. Section 4: The role of this Bayesian discussion is not clear to me. What insights are gained from this Bayesian perspective? Are these insights relevant also to cases of pure-label shift, or only general distribution shifts? I found the discussion around equations (11)-(14) confusing on first reading. The notation in equations (11)-(12) is confusing, perhaps Y|\\phi ~ Cat(\\alpha) and \\phi ~ Dir(\\alpha). The notation in equation (13)-(14) – P_t(\\phi), P^{new}_{t+1} is not defined anywhere. \nI found the whole of section 4.1 confusing. How is the discussion related to label shifts in regression problems? What are the takeaways or results of this section? Are the results valid only for the Gaussian example with a conjugate prior, or more generally applicable? What kind of calibration is performed in this section, and why is it useful?\n\n5. Experiment details. Right before section 5.1: \n  a. It would be worthwhile to provide the details of “the surrogate loss implementation of Wu et al.” \n  b. What are the details of the grid search used for the parameter of OGD? On which validation set is it taking place.\n  c. Skewed-MNIST should reference the inspiration from color MNIST of Arjovsky et al. (2019). A table with the makeup (number of digits of each color) of each of the train/val/test datasets would be useful. It is stated that “Since the overall class frequencies are balanced … we drop the P(Y)”. Drop it from where (same comment for skewed COCO on Places)? Appendix C.1 describes how digits were split into two sets – was there a precise protocol for this? How is the “tend(ency) to be confused” measured context? What was the optimizer used for training  - SGD? \n  d. WILDS-iWildCam: it is stated that “We use Heuristic 3 for evaluating methods on this dataset. Heuristic 3 mentions several approaches: adding a tunable scalar to the diagonal? Using the identity matrix? Using a “pseudo-count”? \n  e. Table 2: How are the error estimates estimated relevant to all tables)? Why are the error estimates here +- 0? Are the quantities really measured to perfect accuracy? \n\nMinor comments\n\n1. Before equation (4): “where e is a one hot vector for the predicted category” – the description and notation there can be clarified: it was initially unclear to me which predicted category is referred to, and only after reading Wu et. al (2021) did I understand that these are calculated for each step I separately. \n\n2. After equation (4), it is stated that calculating the gradients is tricky. Why is it so? For self-containedess, the statement should be explained. Similarly, before equation (7) it is stated that FTH is more efficient than OGD – efficient in which sense? Compute time? Memory? Data complexity? \n\n3. Right before 5.1.3, it is mentioned that “test-sets are smaller”. Smaller than what? \n\n4. Typos: \n- Heuristic 1, line 2: shiftis -> shift is\n- Two lines below equation (19): minimum -> minima\n- Last line of page 6: there’s a superfluous ). \n- 5.2.2, last line of first paragraph, should read “in neither training nor validation sets for OOD test.”\n- The reference to Sun and Saenko (2016) is missing bibliographic info (journal name).\n- Appendix A: equations (23) and (24) seem to be the same\n\nReferences \n[1] Storkey, When training and test sets are different, in:Quinonero Candela et al., Dataset Shift in Machine Learning, 2009\n[2] Moreno-Torresa et al., A unifying view on dataset shift in classification  (2012)\n[3] Schoelkopf et al., On Causal and Anticausal Learning (2012)\n[4] Zhang et al., Domain adaptation under target and conditional shift (2012) \n[5] Kull and Flach, Patterns of dataset shift (2014)\n[6] Breeds: Benchmarks for subpopulation shift, Santurkar et al. (2020)\n[7] Cortes and Mohri, Domain Adaptation in Regression (2011)\n[8] Cortes and Mohri, Domain adaptation and sample bias correction theory and algorithm for regression (2014)\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper, while being short and concise, is for the most part easily readable. Some sections that I found to be more difficult to understand are listed above. \n\nExperiments are described clearly and seem reproducible. Some minor misunderstandings that I had regarding experimental protocols are listed above. \n\nAs far as I can tell, the paper's examination of online labels+conditional shift adaptation of neural networks is novel, as are the experiments performed here. \n\nAs detailed above, the quality of the paper can in my opinion be greatly improved if more context was provided about the distribution shifts considered, a more thorough empirical investigation was conducted, and the unclear/undefined terms and sections are clarified.\n\n# Summary Of The Review\n\nThe work presented in this paper is novel, seems technically correct, and addresses a key problem to many real-world scenarios. I believe that the work in its current state with some corrections/improvements could and should merit publication in some venue. However, with the flaws described above, I do not believe this paper is ready for publication.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMERMAIDE: LEARNING TO ALIGN LEARNERS USING MODEL-BASED META-LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDesigning mechanisms like auctions or taxation policies can be formulated as a general-sum game between a principal and a self-interested learning agent. The principal aims to induce desirable outcomes in such games and may do so, for example, by dynamically intervening on the agent’s learning objective. The intervention policy should generalize well to agents with unseen learning behaviors; in the real world, the principal may not know the agent’s learning algorithm nor its rewards. Moreover, interventions may be costly, e.g., enforcing a tax might require extra labor; hence, interventions should be few-shot adaptable (only needs to retrain on few agents at test-time) and cost-efficient (uses few interventions). Here, we introduce a model-based meta-learning framework to train a principal that can quickly adapt when facing out-of-distribution agents with different learning strategies and reward functions. First, in a simple Stackelberg game between the principal and a greedy agent, we show that meta-learning allows adapting to the theoretically known and appropriate Stackelberg equilibrium at meta-test time, with few interactions with the agent. Second, we show empirically that our approach yields strong meta-test time performance against bandit agents with various unseen explore-exploit behaviors. Finally, we outperform baselines that separately use either meta-learning or agent behavior modeling to learn a cost-effective intervention policy that is K-shot adaptable with only partial agent information.\n\n1\n\nINTRODUCTION\n\nGeneral-sum games provide a framework to study diverse applications involving a principal that aims to incentivize an adaptive agent (both are learners) to achieve the principal’s goal, e.g., maximizing revenue in auctions (Milgrom & Milgrom, 2004), optimizing social welfare with economic policy (Zheng et al., 2022), or optimizing skill acquisition in personalized education (Maghsudi et al., 2021). In this work, we focus on a principal that directly intervenes on the rewards of the agent. For instance, a government may want to incentivize the use of environmentally-friendly (“clean”) products by levying green taxes, but needs to understand how people (strategically) change their consumption behavior as taxes change. Here, existing models of human adaptation that assume rational learning (or use simplified models of bounded rationality) often do not suffice. Hence, interacting with the agents is required to learn (how they change) their behavior, but such interactions are not “free”. For example, a tax policy may require effort to apply it fairly and to measure its impact on consumers.\n\nTo mitigate the need for costly real-world interactions, we can use simulations with deep reinforcement learning (RL) agents. This is an attractive solution framework: deep neural network behavioral models are expressive enough to emulate real-world entities and simulations can be run safely and as often as needed. Moreover, we can use deep RL to learn intervention policies that are effective even in the face of complex agent behaviors in sequential general-sum games.\n\nHowever, this approach also faces several challenges. When deploying the learned policies in the real world, interventions can typically only be applied a few times, due to implementation costs, and rarely under identical circumstances; in contrast to simulations, we cannot reset the real world. Even though principals may adapt their policies to new conditions, they cannot realistically know the true rewards or learning strategy of the agent. Hence, our goal is to learn policies in general-sum games\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthat 1) perform well even when agents learn, 2) can be quickly adapted, 3) are robust to distribution shifts in agent behaviors, and 4) are effective despite having only partial information.\n\nContributions. To address these challenges, we propose MERMAIDE (Meta-learning for Modelbased Adaptive Incentive Design), a deep RL approach that 1) learns a world model and 2) uses gradient-based meta-learning to learn a principal policy that can be quickly adapted to perform well on unseen test agents. We consider two-player general-sum games between a principal and an agent wherein the principal intervenes at a cost on the agent’s learning process to incentivize the agents to learn to act to achieve the principal’s objective. We assume that the agent behaves in a first-order strategic manner and the principal in a second-order strategic manner. Here, the agents optimize their experienced rewards and minimize their regret, but do not account for their influence on the principal’s actions. In contrast, the principal intervenes explicitly as to influence the agent’s actions.\n\nWe first analyze the one-shot adaptation performance of a meta-learned principal in a matrix game setting, under both perfect and noisy observations for the agent and the principal. We show that meta-training reliably finds solutions that one-shot adapt well, and characterize how the principal’s out-of-distribution performance depends on its observable information about the agent.\n\nWe next develop and empirically verify these insights with more adaptive agents and propose MERMAIDE which finds well-performing reward intervention policies in the sequential bandit setting. Here, MERMAIDE performs well against out-of-distribution bandit learners, with test-time performance and robustness depending on the agents’ level of exploration and their pessimism in the face of uncertainty, confirming and extending the analysis and conclusions from the single-round setting.\n\n2 RELATED WORK\n\nBilevel optimization. Learning a mechanism with agents who also learn is a bilevel optimization problem, which is NP-hard (Ben-Ayed & Blair, 1990; Sinha et al., 2017). Possible solution techniques include branch-and-bound and trust regions (Colson et al., 2007). In particular, solving bilevel optimization using joint learning of the mechanism and the agents can be unstable, as the agents continuously adapt their behavior to changes in the mechanism. This can be stabilized using curriculum learning (Zheng et al., 2020), but generally bilevel problems remain challenging, especially with nonlinear objectives or constraints.\n\nMeta-learning and distribution shift. In recent years, gradient-based meta-learning has proven effective in learning initializations for complex policy models that generalize well to unseen tasks (Finn et al., 2017a; Nagabandi et al., 2018). Luketina et al. (2022) showed that context-conditioned meta-gradients are effective for adapting in environments with controlled sources of non-stationarity, but they do not account for non-stationarity from interactions between strategic agents that learn. Prior works in imitation learning (Argall et al., 2009) and inverse RL (Abbeel & Ng, 2004) assume access to expert demonstrations with a fixed policy that the (RL) agent wants to emulate. In contrast, our principal aims to learn a policy that can strategically alter the behavior of such demonstrators (our agents), who are themselves learning during an episode of the demonstration. Recently, Boutilier et al. (2020) studied meta-learning for bandit policies, while Guo et al. (2021) introduced the inverse bandit setup for learning from low-regret demonstrators. However, these works do not consider shifts in the bandit learning algorithm between training and test time.\n\nModeling agents. A key challenge in multi-agent learning is that each agent experiences a nonstationary environment if other agents are learning. As such, agents can benefit from having a world model, e.g., to know what the policy or value function of the other agents are. World models can stabilize multi-agent RL (Lowe et al., 2017) and enable higher-order learning methods (Foerster et al., 2018), and can be seen as a form of model-based RL. However, this may require a large amount of observational data or prior knowledge, which may be hard to acquire.\n\nAdaptive incentive design. Principal-Agent problems (Eisenhardt, 1989) involve design of incentive structures, often under information asymmetry, but are usually not concerned with learning how to learn to incentivize across agents of different types. Pardoe et al. (2006) found that a form of meta-learning that adapts the learning process itself can design English auctions (sequential bidding)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nthat perform better with adaptive bidders who are loss-averse, and is still effective when the distribution of bidder behaviors (slowly) shifts. Our work expands on this theme by explicitly modeling agents that learn, considering shifts in the learning algorithm of the agents, and using deep RL with gradient-based meta-learning. The combination of these techniques enable learning incentivization policies that generalize well across more complex tasks.\n\n3 LEARNING TO ALIGN AGENTS BY REWARD INCENTIVIZATION\n\nOverview. We model a principal who aims to incentivize an agent to (learn to) execute the principal’s preferred action. To do so, the principal can intervene and change the agent’s rewards at a cost. Without interventions, the agent may learn to prefer an action different than the principal’s.\n\nFor example, consider consumers who can use either environmentally “clean” or “dirty” goods. Indifferent at first, consumers may gradually learn to prefer dirty goods if those are consistently cheaper than clean ones, whereas the government may want them to prefer clean goods. Here, the agent’s reward is the negative of the cost of consumption, for instance, and an intervention changes the price of goods through taxes or subsidies. If we have access to a simulation, the principal can compute an optimal intervention. However, the simulation might be inaccurate and real-world agents might behave differently. As an example of such test-time distribution shift, simulated agents may be fast to change their consumption preferences, while real agents may be slow. A “good” principal (trained in a simulation) could quickly be fine-tuned to intervene more in the latter case and adapt quickly if such behavior is observed during deployment.\n\nIn particular, we focus on learning a principal policy that needs to be adapted quickly following a single round of test-time game play (e.g., taxes and subsidies are deployed in the real world), and that is effective when the agent’s learning algorithm differs from that seen during train-time.\n\nWe now formalize this setting. In this work, we focus on agents in a stateless environment for ease of exposition. For all variables and their meaning, see Tables 2 and 3 in the Appendix.\n\nThe agent. The agents are characterized by their action space A and a base reward function r : A → R. We call it the base reward because the agent experiences an intervened reward\n\n ̃rt (at) = r (at) + r′\n\nt (at) ,\n\n(1)\n\nwhere the intervention r′ t is provided externally (by the principal) for the agent action at. We index time as t = 1, . . . , T . At each time step t, the agent’s policy πt computes a distribution over its actions based on the observations for the agent up to timestep t and executes at ∼ πt. We assume that the principal has a preferred action a∗ that the agent should execute, whereas the agent’s optimal policy can prefer a different action than a∗ without intervention. Finally, at time t, the agent learns using an update rule f : (πt, at, ̃rt) (cid:55)→ πt+1 to maximize the agent’s intervened rewards, e.g., under UCB (Lai et al., 1985), f updates the confidence bounds for the action selected at time t.\n\nIn this work, from the principal’s point of view, the world (environment) consists The principal. of the agent who maximizes ̃r. A standard assumption is that agents are rational and they may have a private state (referred to as its type) which the principal cannot see. Although the agent faces a stateless problem, the principal faces a stateful problem with partial observability. The full state s ∈ S includes the principal’s internal state hp t (e.g., the principal’s belief about the value of the private agent information), and all information about the agent, including its past actions, reward function, and policy model; often, the latter two are private.\n\nMore formally, the principal can be modeled as a POMDP (S, op, Ap, rp, γ, P). The observation function op determines what part of a world state s is visible to the principal, Ap is its action space of interventions, rp is its reward, γ is a discounting factor, and P are the environment dynamics, e.g., (cid:1) t ∼ πp (cid:0)ap as caused by the agent’s actions. At time t, the principal samples an action ap (cid:104) which determines its intervention on each possible agent action a, i.e. ap 1, . . . , r′ r′\n\nt−1, hp t |op (cid:105) .\n\nt =\n\nt−1\n\n|A|\n\nAdaptive intervention policy learning To model distribution shift at test time, we follow the meta-learning terminology (Finn et al., 2017b) and view each distinct agent as a task τ i. The principal has access to a meta-train set of agents τ i ∈ Ttrain; i = 1, . . . , ntrain and is evaluated on a meta-test\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nset of agents τ i ∈ Ttest; i = 1, . . . , ntest. We emphasize that during a task, both the principal and agent may learn and adapt, both at meta-train and meta-test time.\n\nHere, we focus on two key challenges: K-shot adaptation and distribution shift. First, the principal gets only K episodes for fine-tuning for each meta-test task (but can train indefinitely for each metatrain task). Second, the principal faces two types of distribution shift: 1) across tasks and 2) intratask non-stationarity. The meta-train and meta-test tasks may differ (significantly) in their temporal distribution of actions, e.g., due to different agent updates f or the agent rewards rt being centered around different values (e.g., average price levels are higher in the real world vs in the simulation). Within a task, the agent’s learning is affected by the principal’s interventions that change its reward ̃r. This gives rise to non-stationarity in the agent’s environment, as its learning objective may shift over time. These forms of distribution shift distinguish our adaptive intervention policy learning setting from most prior work in meta-learning, which often assume stationarity within a task and also assume similar task distributions at meta-train and meta-test times.\n\nObjectives. The principal’s objective is to maximize how often meta-test-time agents choose a∗ during learning and have them converge to a policy that always chooses a∗. To do so, the principal (cid:105) aims to maximize the cost-adjusted test-time return J p ,\nwhere the agent executes its (optimal) policy πi [πp] in response to πp:\n\n(cid:0)πp, πi(cid:1) = ETtest\n\nt=1 γt−1(rp\n\nt − αct)\n\n(cid:104)(cid:80)T\n\ntest\n\narg max πp\n\nEτ i∈Ttest\n\nEπp Eπi[πp]\n\n(cid:34) T\n\n(cid:88)\n\nt=1\n\n(cid:35)\n\nγt−1(rp\n\nt − αct)\n\n,\n\nrp t = 1 [at = a∗] , α > 0,\n\n(2)\n\nwhere the principal incurs a cost ct if it intervenes. A simple cost function is ct = 1 [r′ t ̸= 0], i.e., the cost is constant across non-trivial interventions, where α > 0 is a constant. Note that if intervention were free (ct = 0), a trivial solution is to always add a large r′ (a∗) ≫ 0 for its preferred action a∗, such that it always yields the highest reward. Hence, we focus on learning non-trivial strategies when intervention is costly, which forces the principal to strategically alter the agent’s learning behavior.\n\n(cid:104) (cid:80)T\n\nDuring an episode of T time steps, each agent i starts with a uniformly initialized action probability distribution πi t subject to interventions πp to maximize its return: t, ap EπiEπp . Here, we assume that T and γ are sufficiently large so the agent converges to its optimal policy under ̃r, using its learning algorithm f . That is, we assume that the objective in Eq. (2) is sufficient to describe the principal’s objective of ensuring the agent converges to preferring a∗ at some t < T .\n\n0 and optimizes πi (cid:1) (cid:105)\n\nt=1 ̃ri\n\n(cid:0)ai\n\nt\n\nt\n\nIn the K-shot adaptation setting, at meta-test time, the principal gets K episodes to interact with any agent, each episode of length T steps. The principal has a fixed policy during an episode and it can update its policy at the end of an episode. The agent is reset across episodes, and within each episode, the agent follows its own learning strategy in response to the principal’s interventions. On the K + 1th episode, the principal evaluates its K-shot adapted policy on the agent. Note this assumes that the principal has a separate copy of the meta-test time agent for evaluation.\n\n4 ANALYSIS IN THE MATRIX GAME SETTING\n\nWe first study robust adaptive intervention policy learning with strategic agents using a simple 2player game between a principal and an agent. The agent’s actions are “cooperate” and “defect”, while the principal can choose whether or not to intervene. Assuming the row player is the agent and the column player is the principal, the 2 × 2 payoff matrix is given by\n\nCooperate Defect\n\n(cid:16)\n\nNo intervention u, 1 1 − u, 0\n\nIntervene u + 1, 1 − c −u, −c\n\n(cid:17)\n\n,\n\n(3)\n\nwhere u ∈ (0, 1) and c is the cost of intervention (c < 1). The principal prefers cooperation: it gets 1 if the agent cooperates and 0 if the agent defects (minus the cost c if it intervenes). The agent’s base payoff u is its type. Notice that an intervention incentivizes the agent to cooperate (u + 1 > −u).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\nFigure 1: Single round game. REINFORCE (RL) does not adapt to expected Stackelberg equilibrium during evaluation. MAML’s adaptability suffers under observation noise.\n\nIt is natural to consider the Stackelberg setting where the principal is the leader and acts first (intervene or not), and the agent acts second (Von Stackelberg, 2010). Our goal is to learn an intervention policy that can adapt to different agent types and find the Stackelberg equilibrium. We now analyze three scenarios with increasing difficulty:\n\n1. First, we assume that the principal knows u. Here, there is a unique Stackelberg equilibrium\n\nat (Cooperate, No Intervention) when u ≥ 1\n\n2 , and at (Cooperate, Intervene) when u < 1 2 .\n\n2. Second, the principal can observe a noisy version of u. In both these cases, the agent first observes the principal’s action and plays the best response according to its payoff matrix.\n\n3. Finally, we consider a repeated multi-stage game where the agent cannot observe the principal’s action. Instead, we assume that the agent keeps a running average for the experienced payoffs for each of its actions. In a single-round setting this would correspond to the principal committing to a mixed action and then the agent choosing its best response. When 2 , the Stackelberg equilibrium occurs at (Cooperate, No Intervention). When u < 1 u ≥ 1 2 , at the Stackelberg equilibrium for this game the principal has a mixed action where it chooses to intervene for 2u+1 fraction of times and the agent chooses to always cooperate.\n\n2\n\nGiven this equilibrium analysis, we learn a neural network policy for the principal that predicts its probability of intervention and compare the behavior of the learned policy when trained using standard policy gradients (RL) versus meta-learning (MAML, (Finn et al., 2017b)). We set c = 0.75.\n\nEap∼πp\n\nWith perfect observability. Here, we study whether meta-learning finds a better initialization θmeta for adaptation on unseen agents. In this setting, we assume that the principal observes an agent’s exact payoff parameter u. It learns a stochastic policy πp θ (u) which determines its probability of intervening in a single-round game with an agent of type u. Given a set of training agents with different types u ∼ U (0, 1), for each u, the principal learns the optimal policy parameters θ∗ (u) = θ (u) [rp (ap)], where rp (ap) is the principal’s payoff for action ap with agent u. arg maxθ The planner then learns θmeta using the meta-learning algorithm in Appendix B and one-shot adapts on a meta-test set of agents with different us than at training. Note that we’re studying the quality of the initialization, not the generalization performance of an already trained policy. Fig. 1 shows the principal’s meta-test time probability of intervening with 3 different agents from the test set, across training epochs. The principal and agent should be at different Stackelberg equilibria depending on the type u, as discussed above. We see that a principal trained from scratch on the test agents using standard policy gradients is unable to adapt to different agents in a single-shot adaptation setting. In contrast, with meta-learning, the principal learns a better policy that is one-shot adaptable to agents of different types and converges to the correct Stackelberg equilibrium at meta-test time.\n\nWith noisy observations for the principal. Here, we emulate a principal with partial observability of the agent, by letting the principal observe u with added i.i.d. Gaussian noise. The agent can see all payoffs and chooses the best response to achieve a Stackelberg equilibrium. Fig. 1 shows that with noisy observations, the meta-learned principal policy requires more training time to be one-shot adaptable to the optimal intervention policy. This empirically indicates the increased difficulty of learning an adaptive intervention policy due to incomplete information about the agent, especially under limited adaptation time with unseen agents. It therefore motivates us to adopt a model-based approach for the principal to better estimate the agent type and learn an adaptive intervention policy.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\nFigure 2: Multi-round game. (a) Principal’s optimization trajectory in the expected payoff landscape during training. Axes are PCA directions in the policy parameter space. (b)(c) MAML adapts (single shot) to Stackelberg equilibrium with a best response agent in a simplified form of MD.\n\nFigure 3: Overview of MERMAIDE. Left: Flow of principal and agent observables, rewards, and actions. Right: The principal’s world model and intervention policy. Also see Algorithm 1.\n\nComparing Fig. 1b and Fig. 1c, we also see that when u < 1 2 , the difference in unintervened payoffs between the principal’s preferred action (u) and the agent’s preferred action (1 − u) also impacts the one-shot adaptability of the principal receiving noisy observations. This observation informs our analysis of the bandit setting in Section 6.\n\nMulti-round repeated game with noisy rewards. In this setting, the principal and agent repeatedly play an iterated game over T = 100 steps. In each round, the principal observes the agent’s type u with added i.i.d. Gaussian noise. The agent cannot observe the principal’s actions, and plays a best response for its current estimate of the action payoffs. Whenever the agent selects an action, it receives a noisy observation of the true payoff and updates its estimate. Compared to the single-round setting, here the agent’s best response behavior may change across rounds in the game depending on its observed payoffs, giving rise to non-stationarity in the principal’s environment. The planner, in turn, has to learn to intervene so that the agent’s best response is to cooperate.\n\nFig. 2a compares the optimization trajectory followed using 1) standard policy gradients and 2) meta-learning for the principal. Starting from the same initialization, the meta-learned policy’s parameters lie in a region of the payoff landscape with a higher expected value over the training agents. Moreover, Fig. 2b and Fig. 2c show the one-shot adaptability of the principal’s policy for two different agent types at meta-test time. Meta-learning helps learn a better intervention strategy that is robust to the principal’s observation noise as well as the agent’s evolving best response strategy.\n\n5 MERMAIDE: LEARNING TO ALIGN LEARNERS\n\nMotivated by our findings from Section 4, we now present MERMAIDE (Fig. 3), consisting of:\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 MERMAIDE (Notations also in Table 3)\n\n0, hp 0.\n\nUpdate world model parameters ω = ωe using Eq. (4). for agents (tasks) i = 1, . . . , ntrain do\n\n1: Initialize principal (θ0, ω0), and hidden states hi 2: for meta-train epoch e = 1, . . . , Etrain do 3: 4: 5: 6: 7: 8:\n\nInitialize agent: (μi, πi for k = 1, . . . , Ktrain do\n\nfor time t = 1, . . . , T do\n\n(cid:0)ai\n\nt = arg maxai\n\nPredict ˆai ˆπω Intervention: ̃μi = μi + ap t ,\n\nt\n\n0), task specific principal policy parameter θ (cid:0)τ i\n\n0\n\n(cid:1) = θe.\n\n▷ Inner loop for Ktrain episodes. ▷ For each episode with T principal-agent interactions\n\nt−1\n\nt−1, hi\n\nt−1, ap t|ai t ∼ πp ap\n\n(cid:1) using the world model. (cid:0)ap t−1, ap t−1, ˆai k) θ(τ i t ∼ N (cid:0) ̃μi, σ2(cid:1). πi t and receives reward ri (cid:1).\n\nt, hp t (cid:55)→ πi\n\nt |ai\n\n(cid:1).\n\nt−1\n\n(cid:1) (cid:55)→ θ (cid:0)τ i\n\nk+1\n\nt+1. ▷ Using REINFORCE. (cid:0)τ i(cid:1) = {}\n\nt−1\n\nt−1, hi\n\n▷ Rollout for meta-update; Dmeta (cid:1) using the world model. (cid:0)ap (cid:1). t−1, ˆai t |ai t ∼ N (cid:0) ̃μi, σ2(cid:1). Updates πi (cid:27)\n\nt−1, ap\n\nt (cid:55)→ πi\n\nt, hp\n\nKtrain\n\nt−1\n\nτ i\n\n(cid:16)\n\n(cid:17)\n\nθ\n\nt+1.\n\nAgent acts: ai Locally update θ (cid:0)τ i\n\nt ∼ πi\n\nk\n\nfor t = 1, . . . , T do\n\nt = arg maxai\n\nPredict ˆai ˆπω Intervention: ̃μi = μi + ap t ,\n\nt\n\n(cid:0)ai\n\nt−1, ap t|ai t ∼ πp ap\n\nAgent acts: ai\n\nCollect Dmeta\n\nt ∼ πi (cid:0)τ i(cid:1) ∪\n\nt, receives reward ri t , πp\n\nt, ap ai\n\n(cid:26)\n\n(cid:16)\n\n(cid:17)\n\nθ\n\nτ i\n\nKtrain\n\nMeta-update θe (cid:55)→ θe+1 using Dmeta = ∪τ iDmeta\n\n(cid:0)τ i(cid:1).\n\n▷ Using MAML.\n\n9:\n\n10:\n\n11:\n\n12:\n\n13:\n\n14:\n\n15:\n\n16:\n\n17:\n\n• a recurrent world model parameterized by ω that outputs a distribution over an agent i’s (cid:1), conditioned on the planner’s int−1, ap\n\nt−1, hi actions at the next time step t: ˆπω tervention and the observed agent action at t − 1. hi\n\nt−1 is the hidden world model state.\n\nt|ai\n\n(cid:0)ai\n\nt−1\n\nintervention policy which outputs a distribution over t |ai\n\n• a recurrent (cid:1), conditioned on its previous intervention, ap t ∼ πp observed agent action and the world model’s predicted next agent action ˆai maxa ˆπω\n\nt−1 is the hidden state of the policy network.\n\ninterventions the t =\n\nt−1, ap\n\nt−1, ap\n\nt−1, hi\n\nt−1, ˆai\n\n(cid:0)a|ai\n\n(cid:1). hp\n\nt, hp\n\n(cid:0)ap\n\nt−1\n\nt−1\n\nθ\n\nWe train this using gradient-based meta-learning and RL, see Algorithm 1. Here, the principal maximizes the meta-train objective J p train similar to the objective in Eq. (2). The base RL algorithm is REINFORCE (Williams, 1992) and the meta-learning update uses MAML (Finn et al., 2017b). The agent optimizes its cumulative intervened reward, see Section 6 for details. The world model ˆπω trains by maximizing the log-likelihood of the observed ai t, using Adam (Kingma & Ba, 2014):\n\narg max ω\n\nEap∼πp Eai∼πi\n\n(cid:34) T\n\n(cid:88)\n\nt=1\n\nlog ˆπω\n\n(cid:0)ai\n\nt|ai\n\nt−1, ap\n\nt−1, hi\n\nt−1\n\n(cid:35)\n\n(cid:1)\n\n.\n\n(4)\n\nNote that the principal’s parameters θ are updated after each T -step episode, while the agent continuously learns during each episode. Also, the agent is reset in between episodes. At time 0, the world model makes a prediction based on zero initialization.We use a single world model for all agents. At meta-test time, only the intervention policy is updated by one-shot adaptation to a new agent.\n\n6 EXPERIMENTAL VALIDATION IN THE BANDIT SETTING\n\nWe now study a sequential general-sum game between the principal and an adaptive no-regret learner agent, modeled by an |A|-armed bandit instance with action set A having base reward r = (cid:2)r1, . . . , r|A| (cid:3). At each time step t, the agent chooses an arm a and gets a reward sampled from N (cid:0)ra, σ2(cid:1). We assume ra ∈ (0, 1) ∀a. The agent aims to maximize its cumulative reward over a horizon of T steps. The agent can only observe the reward for the chosen action, and hence faces a explore-exploit dilemma addressed by bandit algorithms like UCB (Lai et al., 1985). We assume there is a unique arm ̃a with the highest base reward: ̃a = arg maxa ra, i.e., the agent’s preferred action without any intervention.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nCostly interventions. To analyze the effect of the cost of intervention ct on the principal’s learnt policy, we assume that the principal decides among three different intervention levels |r′| ∈ {0, 0.5, 1} such that ct = |r′|. Across different bandit agent tasks τ i with distinct base rewards ri and reward gaps δ = maxa∈A ri[a] − ri[a∗], the principal should learn to appropriately incentivize the agent while minimizing the total cost of intervening. We then define the experienced reward as:\n\n ̃rt[a∗] = ri[a∗] + r′ t;\n\n ̃rt[a] = ri[a] − r′ t,\n\n∀a ̸= a∗, (a, a∗ ∈ A).\n\n(5)\n\nNote that this ensures the agent always experiences an intervention, no matter which action it chooses. During each episode, the agent learns but the principal’s policy is fixed; the principal can update its policy only at the end of each episode (Algorithm 1). Also, we assume that the principal can only observe the agent’s actions ai t but not its base reward ri or policy update function f i. We measure the performance of the principal using Eq. (2), with γ = 1.\n\nWorld model. The world model predicts the agent’s next action (given the principal’s prior observations) to characterize the agent’s behavior. We do not train the principal’s world model to estimate the base rewards, because bandit agents with distinct base rewards could still execute the same sequence of actions, depending on the agent’s explore-exploit algorithm and its observations.\n\nChallenges in the sequential setting. Compared to the simple game setting in Section 4, principal’s intervention policy learning with sequential (bandit) learners creates additional challenges:\n\n• Bandit agents may follow different strategies for action selection to maximize their experienced reward. The agent’s rate of exploration may be constant (e.g., ε-greedy) or it can reduce with time (e.g., UCB) within an episode, depending on its observations. This creates a highly non-stationary environment for the principal wherein its decision to intervene must adapt to different explore-exploit behaviors for the same agent within an episode. When the agent explores a larger action space, it further exacerbates the challenges in estimating the agent’s behavior since the principal only has partial information about the agent.\n\n• Bandit agents are sequential learners and feedback (ai\n\nt) can update the policy πi differently at different steps t. The update may depend on how optimistic (e.g., UCB) or pessimistic (e.g., EXP3) the bandit agents are about their reward estimates. Hence, an intervention ap may not equally incentivize the agent at different t. Since the principal’s interventions have different costs, a strategic principal must decide when to intervene and how much (|r′|) depending on its observations of the agent’s actions.\n\nt, ̃ri\n\nResults. In the following experiments, we use 15 bandit agents for training and 10 bandit agents for testing, each with different base rewards (both within and across train and test sets). |A| = 10. We consider two agent learning algorithms (UCB and ε-greedy). In each experiment, the train and test agents use the same algorithm, but with different tendencies for exploration vs exploitation, determined by their exploration coefficients: β ∈ {0.17, 0.27, 0.42, 0.5, 0.67} for UCB (higher β gives more exploration) and ε ∈ {0.1, 0.2, 0.3, 0.4, 0.5} for ε-greedy (higher ε gives more exploration). These constants were chosen such that they afford, on the average, the same number of exploratory actions when following either UCB or ε-greedy strategy without any intervention (see Appendix B).\n\nIn Table 1, we show the one-shot adapted principal’s score on each test set over T = 200 time steps. We compare MERMAIDE against 1) model-free baselines (MF-RL using REINFORCE and MFMAML using MAML), as well as 2) REINFORCE with world model (WM-RL) (see Appendix B for details). We also include a “No Intervention” baseline to show how agents behave by default.\n\nOut-of-distribution performance. Table 1 shows the principal’s score when evaluated on test agents having a different exploration constant than train agents. Using meta-learning for the intervention policy (MF-MAML) and using a world model to predict the agent’s behavior (WM-RL) both have advantages for training a robust and one-shot adaptable intervention policy. A world model is advantageous when 1) the test agent is more exploratory than the train set (e.g., ε = 0.1 at training, ε = 0.4 at test), or 2) the agent explores throughout an episode and is likely to often select actions other than the one with its current maximum mean reward estimate (e.g., ε = 0.5 at training). Because we evaluate on K = 1, fine-tuning on only a single test-time episode, a trained world model provides a useful prior belief representation for the principal. Indeed, the MF-RL results show the\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTrain on UCB, β = 0.17 No intervention MF-RL MF-MAML WM-RL MERMAIDE (ours)\n\nTrain on ε-greedy, ε = 0.1 No intervention MF-RL MF-MAML WM-RL MERMAIDE (ours)\n\nTrain on UCB, β = 0.67 No intervention MF-RL MF-MAML WM-RL MERMAIDE (ours)\n\nTrain on ε-greedy, ε = 0.5 No intervention MF-RL MF-MAML WM-RL MERMAIDE (ours)\n\nTest on β = 0.17 3 (0) 119 (2) 133 (2) 123 (7) 154 (2)\n\nε = 0.1 3 (0) 115 (5) 122 (4) 115 (4) 134 (1)\n\nβ = 0.17 3 (0) 103 (3) 124 (2) 100 (4) 131 (2)\n\nε = 0.1 3 (0) 4 (5) 2 (0) 102 (6) 87 (42)\n\nβ = 0.27 5 (0) 109 (2) 125 (3) 112 (6) 141 (1)\n\nε = 0.2 4 (1) 94 (4) 97 (3) 94 (5) 108 (1)\n\nβ = 0.27 5 (0) 101 (3) 116 (1) 89 (0) 125 (2)\n\nε = 0.2 4 (1) 2 (3) 4 (0) 79 (10) 102 (3)\n\nβ = 0.42 8 (0) 98 (2) 107 (1) 100 (4) 115 (1)\n\nε = 0.3 7 (0) 54 (19) 58 (5) 70 (1) 85 (1)\n\nβ = 0.42 8 (0) 92 (2) 102 (1) 85 (1) 109 (1)\n\nε = 0.3 7 (0) 5 (0) 6 (0) 68 (3) 78 (6)\n\nβ = 0.5 10 (0) 90 (2) 97 (1) 92 (2) 103 (0)\n\nε = 0.4 9 (1) 39 (6) 40 (2) 55 (3) 57 (7)\n\nβ = 0.5 10 (0) 85 (1) 94 (1) 85 (1) 101 (1)\n\nε = 0.4 9 (1) 11 (5) 8 (1) 47 (1) 69 (1)\n\nβ = 0.67 12 (0) 77 (1) 77 (0) 75 (1) 80 (1)\n\nε = 0.5 11 (0) 22 (9) 12 (1) 38 (1) 29 (1)\n\nβ = 0.67 12 (0) 74 (1) 80 (1) 74 (0) 85 (1)\n\nε = 0.5 11 (0) 7 (1) 11 (1) 30 (2) 46 (2)\n\nTable 1: Test-time principal mean and standard error scores across 3 random seeds. Left column: Principal’s algorithm (e.g., MERMAIDE), training agent type (e.g., UCB with β = 0.17). Other columns: Test-time scores on agents with the same algorithm, but different hyperparameters.\n\nhidden state representation of the model-free principal might be unable to adapt to high environment non-stationarity without a trained next-agent-action world model.\n\nCompared to an ε-greedy agent, the UCB agent explores mostly at the start of an episode, for all β. Hence, with UCB agents, the principal learns an effective one-shot adaptable intervention policy using meta-learning (MF-MAML) only (even without a world model), as the agents cause less distribution shift across different c. It further emphasizes the effectiveness of meta-learning for adaptive policy learning: unlike MF-MAML, neither the world model nor the intervention policy is meta-learned in WM-RL. Moreover, it also shows that for the same amount of distribution shift (characterized in Appendix B), the relative benefit of a world model or meta-learning the principal’s policy depends on the nature of the agent’s exploration strategy (which is unknown to the principal).\n\nIn all, these results show that MERMAIDE combines the best of both techniques: the principal obtains a higher score across agents with different learning algorithms and explore-exploit behaviors.\n\nAgent exploration vs intervention cost. In order to intervene effectively, the principal should learn when to intervene and how much to incentivize the agent while minimizing its incurred cost. This is a challenging learning problem for the principal not just during meta-training, but more so during one-shot adaptation at meta-test time. Bandit algorithms like EXP3 (Auer et al., 2002) use pessimism in the face of uncertainty, and encourage continued exploration. This increases the non-stationarity for the principal. In order to effectively incentivize such agents to prefer a∗, the principal needs to accurately predict the agent’s policy from its observations; otherwise it can incur a high cost for intervening ineffectively and lowering its score, and learn to stop intervening. Indeed, our results when training on ε = 0.5-greedy agents show that the MF-RL and MF-MAML principal stop intervening. In contrast, in that setting, MERMAIDE learns an effective intervention policy that outperforms all baselines, even under distribution shift between meta-train and meta-test agents.\n\n7 DISCUSSION AND FUTURE WORK\n\nWe have shown that MERMAIDE is an effective framework to learn principal intervention policies that generalize well to agents with unseen learning behavior. Future work could extend MERMAIDE to settings with multiple learning agents who may coordinate, compete, or a combination thereof. Moreover, it is interesting to extend MERMAIDE to agents that adapt adversarially to the principal’s intervention policy, which poses a challenging non-stationary problem for the principal.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nPieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In\n\nProceedings of the twenty-first international conference on Machine learning, pp. 1, 2004.\n\nBrenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning\n\nfrom demonstration. Robotics and autonomous systems, 57(5):469–483, 2009.\n\nPeter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-\n\narmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002.\n\nOmar Ben-Ayed and Charles E Blair. Computational difficulties of bilevel linear programming.\n\nOperations Research, 38(3):556–560, 1990.\n\nCraig Boutilier, Chih-wei Hsu, Branislav Kveton, Martin Mladenov, Csaba Szepesvari, and Manzil Zaheer. Differentiable meta-learning of bandit policies. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2122–2134. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/171ae1bbb81475eb96287dd78565b38b-Paper.pdf.\n\nBenoˆıt Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization. Annals of\n\noperations research, 153(1):235–256, 2007.\n\nKathleen M Eisenhardt. Agency theory: An assessment and review. Academy of management\n\nreview, 14(1):57–74, 1989.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation In International conference on machine learning, pp. 1126–1135. PMLR,\n\nof deep networks. 2017a.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. arXiv:1703.03400 [cs], July 2017b. URL http://arxiv.org/ abs/1703.03400.\n\nJakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with Opponent-Learning Awareness. arXiv:1709.04326 [cs], September 2018. URL http://arxiv.org/abs/1709.04326.\n\nWenshuo Guo, Kumar Krishna Agrawal, Aditya Grover, Vidya Muthukumar, and Ashwin Pananjady. Learning from an exploring demonstrator: Optimal reward estimation for bandits, 2021. URL https://arxiv.org/abs/2106.14866.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nTze Leung Lai, Herbert Robbins, et al. Asymptotically efficient adaptive allocation rules. Advances\n\nin applied mathematics, 6(1):4–22, 1985.\n\nRyan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. MultiAgent Actor-Critic for Mixed Cooperative-Competitive Environments. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6379–6390. Curran Associates, Inc., 2017.\n\nJelena Luketina, Sebastian Flennerhag, Yannick Schroecker, David Abel, Tom Zahavy, and Satinder Singh. Meta-gradients in non-stationary environments. In ICLR Workshop on Agent Learning in Open-Endedness, 2022.\n\nSetareh Maghsudi, Andrew Lan, Jie Xu, and Mihaela van Der Schaar. Personalized education in the artificial intelligence era: what to expect next. IEEE Signal Processing Magazine, 38(3):37–50, 2021.\n\nPaul Milgrom and Paul Robert Milgrom. Putting auction theory to work. Cambridge University\n\nPress, 2004.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAnusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through metareinforcement learning. In International Conference on Learning Representations, 2018.\n\nDavid Pardoe, Peter Stone, Maytal Saar-Tsechansky, and Kerem Tomak. Adaptive mechanism deIn Proceedings of the 8th International Conference on Elecsign: A metalearning approach. tronic Commerce: The New e-Commerce: Innovations for Conquering Current Barriers, Obstacles and Limitations to Conducting Successful Business on the Internet, ICEC ’06, pp. 92–102, New York, NY, USA, 2006. Association for Computing Machinery. ISBN 1595933921. doi: 10.1145/1151454.1151480. URL https://doi.org/10.1145/1151454.1151480.\n\nAnkur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: from classical to evolutionary approaches and applications. IEEE Transactions on Evolutionary Computation, 22 (2):276–295, 2017.\n\nRichard S Sutton and Andrew G Barto. Introduction to reinforcement learning, 1998.\n\nHeinrich Von Stackelberg. Market structure and equilibrium. Springer Science & Business Media,\n\n2010.\n\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. Machine learning, 8(3):229–256, 1992.\n\nStephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck, David C. Parkes, and Richard Socher. The AI Economist: Improving Equality and Productivity with AI-Driven Tax Policies. arXiv:2004.13332 [cs, econ, q-fin, stat], April 2020. URL http://arxiv. org/abs/2004.13332.\n\nStephan Zheng, Alexander Trott, Sunil Srinivasa, David C. Parkes, and Richard Socher. The ai economist: Taxation policy design via two-level deep multiagent reinforcement learning. Science Advances, 8(18):eabk2607, 2022. doi: 10.1126/sciadv.abk2607. URL https://www. science.org/doi/abs/10.1126/sciadv.abk2607.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nVariable Time Principal Agent State State vector State space Agent’s action space Principal’s action space Action sequence Agent i’s reward sequence Principal’s reward sequence Transition function Agent i’s policy Principal’s intervention policy Agent’s mean estimate of intervened rewards for action a Number of adaptation steps Number of meta-tasks for the planner Principal’s history of interventions and observed agent actions upto time t Agent’s history of actions taken and rewards observed upto time t\n\nSymbol t\np i\ns s\nS A\nAp a1:T = {a1, a2, . . . , aT } 1:T = (cid:8) ̃ri ̃ri rp 1:T = {rp P\nπi πp ̃μa\n\n1, . . . , ̃ri 1, . . . , rp\n\nT }\n\n(cid:9)\n\nT\n\nK N\nHp\n\nt = (cid:8)ap\n\n1, ai\n\n1, ap\n\n2, ai\n\n2, . . . , ap\n\nt−1, ai\n\nt−1\n\n(cid:9)\n\nHi\n\nt = (cid:8)ai\n\n1, ̃ri\n\n1, ai\n\n2, ̃ri\n\n2, . . . , ai\n\nt−1, ̃ri\n\nt−1\n\n(cid:9)\n\nTable 2: Overview of notation.\n\nPrincipal’s policy parameter Agent i’s learning algorithm Agent i’s true action mean rewards Agent i’s intervened action mean rewards Principal’s action at time t Hidden state space of the principal’s recurrent world model Agent’s action at time t Agent’s reward at time t Principal’s world model estimate of the agent’s action probability distribution Principal’s world model estimate of the latent state of the environment Principal’s world model hidden state embeding in the LSTM architecture\n\nθ ∈ Θ f i ∈ F μi ∼ U ̃μi ap t ∼ πp H\n\nθ\n\n(cid:0)ap\n\nt |ai\n\nt−1, ap\n\nt−1, ˆai\n\nt, hp\n\nt−1\n\n(cid:1)\n\nt\n\n(cid:1) , t = 1, . . . , T\n\n(cid:0)ai t|Hi t ∼ πi ai t ∼ N (cid:0) ̃μi, σ2(cid:1) ri ω : A × Ap × H → ∆ (A) , ˆπi ˆπi\n\nt\n\nω,0 ∈ A\n\nω : A × Ap × H → H, gi gi\n\nω,0 ∈ ∆ (H)\n\nt = gi\n\nω\n\n(cid:0)ai\n\nt−1, ap\n\nt−1, ht−1\n\n(cid:1) , t = 2, . . . , T hi\n\n1 =\n\nhi gi\n\nω,0\n\nTable 3: Notation for MERMAIDE See Section Section 5 for their use.\n\nA NOTATION\n\nFor an overview of all symbols and variables used in this work, see Table 2 and Table 3.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nB ADDITIONAL RESULTS\n\nB.1 DESCRIPTION OF THE BANDIT ALGORITHMS\n\nWe provide a brief overview of the learning algorithms referred to in Section 6.\n\nUCB. This is an Upper Confidence Bound based exploration-exploitation algorithm that follows the principle of optimism in the face of uncertainty. At each time step t, the bandit agent selects an action\n\nat = arg max\n\na\n\n ̃μa + β\n\n(cid:114) log t na\n\n(6)\n\nwhere na is the number of steps until t in which it previously selected the action a, ̃μa is its corresponding mean estimate for the experienced rewards ̃r for action a and β is the exploration constant that balances the amount of exploration vs. exploitation across a time horizon T . A higher value of β makes the agent less optimistic and explore its action space more. The UCB agent’s tendency to explore is also affected by the difference in the mean reward estimates of its actions. In the context of our mechanism design problem formulation, if the UCB agent has a larger value of δ = maxa ra − ra∗ , without any intervention at the beginning of an episode, its confidence bounds would quickly converge to exploiting the action arg maxa ra. So a principal that intervenes only towards the later stages of an episode with this agent would have to provide much more incentives (higher r′) to alter the agent’s preferred action to be a∗, thus incurring a larger cost c as compared to a principal that intervenes more at the beginning of an episode when the UCB agent is still exploring its action space. This is also illustrated in Section 4 with a simpler best response agent in the single round game setting. As shown in Fig. 1b and Fig. 1c, under observation noise (partial information), the meta-trained principal has a better one-shot meta-test-time performance when the agent’s base payoff has a higher difference between the principal’s preferred action and the agent’s intrinsic preference without any intervention.\n\nε-greedy. A simple exploration-exploitation strategy in the bandit setting is the ε-greedy rule (Sutton & Barto, 1998) wherein the agent selects with probability 1 − ε the action at = arg maxa ̃μa and with probability ε it selects a random action. In our setting, we consider ε to be constant during an episode, which results in a uniform exploration rate throughout. In contrast to the UCB agent, the εgreedy algorithm simulates a less optimistic, more exploratory agent for which the principal requires a robust belief representation of the agent’s predicted behavior conditioned on the principal’s past observations (Table 1). Since there is a uniform exploration rate for the agent, the principal has to continue intervening intermittently throughout an episode, especially when δ is large and the agent could obtain a higher reward for an action a ̸= a∗ by exploring its action space when the principal does not intervene.\n\nEXP3. The Exponential-weight algorithm for Exploration and Exploitation (EXP3) (Auer et al., 2002) follows a more pessimistic approach to exploration-exploitation in the bandit setting. It maintains a set of weights for each agent action a ∈ A which are updated using the experienced rewards ̃r as follows:\n\nπt(at) =\n\nw |A|\n\n+ (1 − w)\n\nwhere\n\nη exp (Sat,t)\n\n(cid:80)\n\nat∈|A| η exp (Sat,t)\n\nSat,t =\n\nt (cid:88)\n\nl=1\n\n1 {al = at}\n\n ̃rat,l πl\n\n, η =\n\nw |A|\n\n.\n\n,\n\n(7)\n\n(8)\n\nHere, w is the variable that determines the extent of uniform random exploration in the action space. This presents a very challenging problem to learn a suitable belief representation for such agents that can be utilized by a principal to guide its intervention policy. In Section 6, we exclude EXP3 from Table 1 since it is primarily designed for an adversarial bandit setup, whereas we do not consider an agent to have such biases under our current problem formulation.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nβ\n\n0.17 0.27 0.42 0.50 0.67\n\nUCB\n\nε-greedy\n\n33 (0) 47 (0) 70 (0) 80 (0) 99 (0)\n\n33 (0) 47 (4) 68 (9) 81 (3) 99 (1)\n\nε\n\n0.10 0.20 0.30 0.40 0.50\n\nTable 4: Experiment design choice. Frequency of agent selecting at ̸= arg maxa ra with UCB and ε-greedy algorithms on the same set of base rewards (without any intervention) with a horizon T = 200, averaged across 3 random seeds.\n\nB.2 CHARACTERIZING THE DISTRIBUTION SHIFT IN OUR EVALUATION SETUP\n\nBandit agents having the same base reward r make different explore-exploit decisions depending on their algorithm (eg. UCB, ε-greedy) and also their prior observations. In Section 6, we consider agents with the same set of base rewards, but following different bandit algorithms. Both UCB and ε-greedy have tunable parameters that determine their explore-exploit tradeoff. In order to measure the robustness of the learnt principal policy to different agent behavior (leading to different levels of non-stationarity in the principal’s environment between training and test agents), we vary the amount of exploration performed by the agent by varying the respective parameters: β for the UCB agent and ε for the ε-greedy agent. Table 4 shows the average (and standard error) frequency of exploration by the agents for our choices of β and ε in Section 6. We vary β and ε such that they are pairwise comparable in Table 1 and would lead to similar change in exploration frequency for both UCB and ε-greedy agents. In other words, following Table 1, a principal trained with UCB agents having β = 0.17 when evaluated with UCB agents having β ∈ {0.17, 0.27, 0.42, 0.50, 0.67} will encounter a similar shift in the agent’s exploration frequency as in the case of training with ε-greedy agents with ε = 0.1 and evaluating on ε-greedy agents having ε ∈ {0.1, 0.2, 0.3, 0.4, 0.5}. In that case, the difference in achieved scores between the UCB and ε-greedy agents can be attributed to the way in which they distribute their exploratory actions: UCB agent being more optimistic focuses most of its exploration at the beginning of an episode, whereas the ε-greedy agent is more stochastic with uniform random exploration throughout.\n\nB.3 DESCRIPTION OF BASELINES\n\nWe now describe the details of our evaluated baselines in Section 6 along with their variations that assume access to an agent state oracle.\n\nRule based mechanism with an agent state oracle (RB): Given an oracle that correctly identifies the action at to be taken by an agent in the next time step, a simple rule based approach is for the principal to intervene at time t when at ̸= a∗. We assume that the principal always intervenes with a fixed incentive (r′ = 0.5 or 1) and we compute the principal’s maximum possible score. Note that this is not a realistic solution for the principal since it is impractical to expect the availability of such an oracle, especially for out of distribution test agents.\n\n(cid:0)ap\n\nModel-free learning based mechanism: the plant ∼ ner has a recurrent (cid:1), conditioned on the planner’s intervention and observed agent action at t−1, hp πp t − 1. The policy network is trained using REINFORCE for the MF-RL baseline and using MAML for the MF-MAML baseline.\n\nintervention policy that outputs a distribution over interventions ap\n\nIn this framework, we assume that\n\nt−1, ap\n\nt |ai\n\nt−1\n\nθ\n\nLearning based mechanism with an agent state oracle: recurrent intervention policy that outputs a distribution over interventions ap conditioned on the true agent action at time t provided by an oracle. The policy network is trained using REINFORCE for the SB-RL baseline and MAML for the SB-MAML baseline.\n\nIn this setting, the principal learns a (cid:1) t ∼ πp\n\nt, hp\n\nt |ai\n\n(cid:0)ap\n\nt−1\n\nθ\n\nLearning based mechanism with a world model without meta-learning (WM-RL): In this setting, we use our proposed recurrent world model with a recurrent intervention policy trained\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTrain on UCB, β = 0.17 No intervention RB SB-RL SB-MAML\n\nTest on β = 0.17 3 (0) 173 (0) 168 (3) 169 (3)\n\nTrain on ε-greedy, ε = 0.1 No intervention RB SB-RL SB-MAML\n\nTrain on UCB, β = 0.67 No intervention RB SB-RL SB-MAML\n\nTrain on ε-greedy, ε = 0.5 No intervention RB SB-RL SB-MAML\n\nε = 0.1 3 (0) 156 (3) 148 (2) 152 (1)\n\nβ = 0.17 3 (0) 173 (0) 166 (3) 173 (1)\n\nε = 0.1 3 (0) 156 (3) 49 (46) 93 (45)\n\nβ = 0.27 5 (0) 166 (0) 138 (27) 169 (1)\n\nε = 0.2 4 (1) 130 (1) 119 (3) 126 (2)\n\nβ = 0.27 5 (0) 166 (0) 163 (2) 170 (0)\n\nε = 0.2 4 (1) 130 (1) 51 (35) 62 (32)\n\nβ = 0.42 8 (0) 154 (0) 128 (26) 155 (2)\n\nε = 0.3 7 (0) 105 (4) 87 (4) 105 (3)\n\nβ = 0.42 8 (0) 154 (0) 150 (3) 159 (0)\n\nε = 0.3 7 (0) 105 (4) 64 (29) 32 (13)\n\nβ = 0.5 10 (0) 146 (0) 122 (24) 148 (1)\n\nε = 0.4 9 (1) 87 (4) 75 (6) 66 (3)\n\nβ = 0.5 10 (0) 146 (0) 146 (2) 152 (0)\n\nε = 0.4 9 (1) 87 (4) 61 (15) 58 (25)\n\nβ = 0.67 12 (0) 126 (0) 107 (22) 128 (2)\n\nε = 0.5 11 (0) 62 (6) 50 (2) 30 (9)\n\nβ = 0.67 12 (0) 126 (0) 128 (2) 133 (0)\n\nε = 0.5 11 (0) 62 (6) 28 (17) 24 (17)\n\nTable 5: Principal (with oracle agent state input) scores across 3 random seeds. These baselines are not applicable in practice since they cheat by assuming access to an oracle that always informs them of the agent’s next action. We include them here as a form of standardization with respect to a (perfect) system that does not face the challenges of partial observability or out-of-distribution generalization for mechanism design.\n\nusing REINFORCE. Here, the policy network outputs a distribution over interventions ap πp t|ai\n\nt = arg maxa ˆπω\n\n(cid:1) where ˆai\n\nt−1, ap\n\nt−1, hi\n\nt, hp\n\nt |ˆai\n\n(cid:0)ap\n\n(cid:0)ai\n\n(cid:1).\n\nt−1\n\nt−1\n\nθ\n\nt ∼\n\nWe would like to highlight an implementation detail in our baselines indicated ‘RL’ in Section 6. Since we evaluate our learnt principal policy in the K-shot adaptation setting which is common in the meta-learning literature, we ensure that the principal policies that are not meta-trained are also allowed to K-shot adapt at test time. This means that the ‘RL’ policies are also updated at test time, before evaluation, using K rounds of principal-agent interactions. This is contrast to Section 4 where ‘RL’ was trained from scratch during test time adaptation. It further shows that even with pre-training (on the same set of train agents as used by ‘MAML’), standard policy gradient update does not lead to test time K-shot adaptation on test agents.\n\nIn Table 5, we compare the test time scores for the principal policy having access to a state based oracle. We observe that overall, the meta-trained principal policy (SB-MAML) achieves a higher score even with distribution shift across different bandit algorithms and different levels of exploration, compared to the SB-RL baseline. The rule based baseline also shows strong performance but we note its scores do not reflect adaptation to distribution shift. However, none of these baselines that assume the principal has access to an oracle that correctly predicts the agent’s action at the next time step are realistic. We can only treat the scores in Table 5 as gold standards in a perfect system that does not account for the challenges faced by a principal in practice.\n\nIn Section 4, the principal policy πp is a fully connected neural network (MLP) Training details. with one hidden layer and ReLU activation. Given an (noisy) observed value of the agent type as input, it predicts the probability of intervention: πp t ∼ Bern (πp\n\nt . The principal’s action at time t is ap\n\nt ).\n\nFor the ‘RL’ principal, it is trained on the test agents starting from scratch over K episodes before evaluation. For the MAML principal, it is meta-trained to learn an initial parameterization with a different set of training agents and evaluated with K-shot adaptation on the test agents.\n\nIn Section 6, the recurrent world model and policy networks are GRUs with 2 layers and hidden state dimension 128. For meta-training, the inner gradient update loop uses SGD optimizer with a learning rate of 7 × 10−4 whereas the meta-update step uses Adam with a learning rate of 0.001. The world model is trained only with the set of training agents, it is not adapted at test time: only the policy network is K-shot adapted.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 MERMAIDE (K-shot Adaptation)\n\nInitialize agent: (μi, πi for k = 1, . . . , K do\n\n0), task specific intervention policy parameter θ (cid:0)τ i\n\n1: Initialize principal with trained parameters (θmeta, ωtrain), and hidden states hi 2: for agents (tasks) i = 1, . . . , ntest do 3: 4: 5: 6:\n\n0, hp 0. (cid:1) = θmeta. ▷ Inner loop for K episodes. ▷ For each episode with T principal-agent interactions (cid:0)ai t|ai t ∼ πp ap\n\n(cid:1) using the world model. t−1, hi (cid:0)ap t, hp t−1, ap t−1, ˆai k) θ(τ i t ∼ N (cid:0) ̃μi, σ2(cid:1). πi t (cid:55)→ πi t and receives reward ri (cid:1). ▷ Using REINFORCE.\n\nPredict ˆai ˆπωtrain Intervention: ̃μi = μi + ap t ,\n\nAgent acts: ai Locally update θ (cid:0)τ i\n\nfor time t = 1, . . . , T do\n\nt = arg maxai\n\n(cid:1) (cid:55)→ θ (cid:0)τ i\n\nt−1, ap\n\nt ∼ πi\n\nt |ai\n\nt+1.\n\n(cid:1).\n\nt−1\n\nt−1\n\n0\n\nt\n\nk\n\nk+1\n\nfor t = 1, . . . , T do\n\n▷ Rollout for evaluation\n\nt = arg maxai\n\nPredict ˆai ˆπωtrain Intervention: ̃μi = μi + ap t ,\n\nt\n\nt−1, ap\n\n(cid:0)ai t|ai t ∼ πp ap t, receives reward ri\n\nt−1, hi (cid:0)ap\n\n(cid:1) using the world model. t, hp t−1, ap θ(τ i t ∼ N (cid:0) ̃μi, σ2(cid:1). Updates πi\n\nt (cid:55)→ πi\n\nt−1, ˆai\n\nt |ai\n\nK)\n\n(cid:1).\n\nt−1\n\nt−1\n\nt+1.\n\nAgent acts: ai Update principal’s score.\n\nt ∼ πi\n\n7:\n\n8:\n\n9:\n\n10: 11:\n\n12:\n\n13: 14:\n\nB.4 OVERVIEW OF K-SHOT ADAPTATION WITH MERMAIDE:\n\nAlgorithm 2 outlines our framework for K-shot adaptation of the meta-trained principal to test agents. In our experiments, K = 1.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTrain on UCB, β = 0.17 No intervention MF-RL MF-MAML WM-RL MERMAIDE (ours) MERMAIDE (ours) - 2nd set MERMAIDE (K = 0) WM-RL (K = 0)\n\nTrain on ε-greedy, ε = 0.1 No intervention MF-RL MF-MAML WM-RL MERMAIDE (ours) MERMAIDE (ours) - 2nd set MERMAIDE (K = 0)\n\nTrain on UCB, β = 0.67 No intervention MF-RL MF-MAML WM-RL MERMAIDE (ours) MERMAIDE (ours) - 2nd set MERMAIDE (K = 0) WM-RL (K = 0)\n\nTrain on ε-greedy, ε = 0.5 No intervention MF-RL MF-MAML WM-RL MERMAIDE (ours) MERMAIDE (ours) - 2nd set MERMAIDE (K = 0)\n\nTest on β = 0.17 3 (0) 119 (2) 133 (2) 123 (7) 154 (2) 144 (3) 148 (2) -\n\nε = 0.1 3 (0) 115 (5) 122 (4) 115 (4) 134 (1) 132 (3) 133 (2)\n\nβ = 0.17 3 (0) 103 (3) 124 (2) 100 (4) 131 (2) 119 (7) 115 (3) 104 (8)\n\nε = 0.1 3 (0) 4 (5) 2 (0) 102 (6) 87 (42) -\n113 (20)\n\nβ = 0.27 5 (0) 109 (2) 125 (3) 112 (6) 141 (1) 133 (3) 138(1) 109 (1)\n\nε = 0.2 4 (1) 94 (4) 97 (3) 94 (5) 108 (1) 111 (3) 109 (3)\n\nβ = 0.27 5 (0) 101 (3) 116 (1) 89 (0) 125 (2) 118 (5) 114 (3) 90 (-)\n\nε = 0.2 4 (1) 2 (3) 4 (0) 79 (10) 102 (3) 89 (17) 85 (15)\n\nβ = 0.42 8 (0) 98 (2) 107 (1) 100 (4) 115 (1) 122 (2) 120(1) 92 (-)\n\nε = 0.3 7 (0) 54 (19) 58 (5) 70 (1) 85 (1) 89 (1) 86 (2)\n\nβ = 0.42 8 (0) 92 (2) 102 (1) 85 (1) 109 (1) 110 (3) 103 (4) -\n\nε = 0.3 7 (0) 5 (0) 6 (0) 68 (3) 78 (6) 65 (12) 71 (16)\n\nβ = 0.5 10 (0) 90 (2) 97 (1) 92 (2) 103 (0) 108 (2) 103(2) -\n\nε = 0.4 9 (1) 39 (6) 40 (2) 55 (3) 57 (7) 68 (1) 65 (3)\n\nβ = 0.5 10 (0) 85 (1) 94 (1) 85 (1) 101 (1) 104 (3) 100 (3) 69 (1)\n\nε = 0.4 9 (1) 11 (5) 8 (1) 47 (1) 69 (1) 47 (20) 48 (14)\n\nβ = 0.67 12 (0) 77 (1) 77 (0) 75 (1) 80 (1) 81 (2) 89(1) -\n\nε = 0.5 11 (0) 22 (9) 12 (1) 38 (1) 29 (1) 45 (1) 37 (1)\n\nβ = 0.67 12 (0) 74 (1) 80 (1) 74 (0) 85 (1) 87 (2) 89 (3) -\n\nε = 0.5 11 (0) 7 (1) 11 (1) 30 (2) 46 (2) 20 (15) 21 (15)\n\nTable 6: Test-time principal mean and standard error scores. Left column: Principal’s algorithm (e.g., MERMAIDE), training agent type (e.g., UCB with β = 0.17). Other columns: Test-time scores on agents with the same algorithm, but different hyperparameters. Gold-colored values represent testing with a principal that is not updated during test-time, i.e., K = 0-shot generalization. We see that the principal can perform on par when trained on β = 0.17 and ε = 0.1, but that 0-shot generalization does not work so well when the principal was trained on more exploratory hyperparameter values, i.e., β = 0.67 and ε = 0.5. To compare the levels of exploration between different hyperparameter settings, please refer to Appendix B.2 and Table 4. All results are based on 3 random seeds. Note that the results in blue use 3 random seeds with the same settings as the rows above; as such, there are two sets of 3 random seeds for MERMAIDE. We see that the results are similar between the two sets of 3 random seeds.\n\nC ADDITIONAL EXPERIMENTAL RESULTS WITH BANDIT AGENTS\n\nAdditional seeds for Table 1. We ran the same set of experiments for MERMAIDE from Section 6 with 3 additional results. At the time of the rebuttal deadline, some of the runs have not converged so we are reporting these additional results in the row marked in blue in Table 7. We will update these with final converged values and more seeds if requested for the camera ready version if the paper is accepted. On the basis of our current results, we do not expect a significant variation from the values originally reported in Table 1 even with more seeds.\n\nZero-shot evaluation results with MERMAIDE. As requested by reviewer 4J8Y, we evaluated the policy is not updated at meta-test time (K = 0). MERMAIDE in the zero-shot setting i.e. Table 7 shows the preliminary results for these experiments in gold. Some of these experiments have not coverged yet, but we do not expect a lot of improvement compared to the reported scores. We see that the principal can perform on par when trained on β = 0.17 and ε = 0.1, but that 0-shot generalization does not work so well when the principal was trained on more exploratory hyperparameter values, i.e., β = 0.67 and ε = 0.5. To compare the levels of exploration between different hyperparameter settings, please refer to Appendix B.2 and Table 4.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTrain on UCB, β = 0.42 No intervention WM-RL MERMAIDE (ours)\n\nTest on ε = 0.1 3 (0) 91 (4) 103 (1)\n\nTrain on ε-greedy, ε = 0.3 No intervention WM-RL MERMAIDE (ours)\n\nTest on β = 0.17 3 (0) 127 (7) 138 (2)\n\nε = 0.2 4 (1) 62 (8) 67 (2)\n\nβ = 0.27 5 (0) 95 (2) 102 (6)\n\nε = 0.3 7 (0) 68 (1) 30 (2)\n\nβ = 0.42 8 (0) 80 (5) 116 (2)\n\nε = 0.4 9 (1) 28 (4) 8 (1)\n\nβ = 0.5 10 (0) 80 (5) 96 (5)\n\nε = 0.5 11 (0) -\n-\n\nβ = 0.67 12 (0) 61 (4) 77 (2)\n\nTable 7: Test-time principal mean and standard error scores across 3 random seeds, K = 0. Left column: Principal’s algorithm (e.g., MERMAIDE), training agent type (e.g., UCB with β = 0.42). Other columns: Test-time scores on agents with different algorithm and different hyperparameters. We see that MERMAIDE generalizes well when tested on agents that explore the same amount or less than the train-time agents. For clarity, note that higher β and ε lead to more exploration. More generally, a principal that is trained on a stochastic agent generalizes well to an equal or less stochastic agent, e.g., training on ε = 0.3 and testing on UCB with β = 0.5, 0.67; note that the behavior of UCB is less stochastic than ε-greedy.\n\nCross algorithm evaluation. Table 7 indicates the mean and standard error scores for evaluation in the K = 0-shot generalization setting when the training agent and test agents are of different types. Note that the behavior of UCB agents is less stochastic than ε-greedy agents. We observe that when trained with UCB agents, MERMAIDE outperforms WM-RL for generalizing to ε-greedy agents that have a lower exploration coefficient ε = 0.1 or 0.2. In contrast, when trained with ε-greedy agents, MERMAIDE outperforms WM-RL for generalizing to UCB agents with both higher and lower levels of exploration. More generally, a meta-learning principal that is trained on a stochastic agent generalizes well to an equal or less stochastic agent in the zero shot setting.\n\n18",
    "reference": "# Summary Of The Paper\n\nThe paper explores the problem of mechanism design which studies how to design reward functions and environmental rules defining mathematical games. The applications of mechanism design spans across many domains from optimizing social welfare with economic policies to designing governmental policies. The conventional problem in this space is that it is often expensive to understand the effect of changes to a mechanism design in the real world. Thus, it is often convenient to study the mechanisms in simulations before deploying them in real world. \n\nThe paper presents a deep RL approach to mechanism design that learns a world model and uses meta-learning to learn a mechanism that can be adapted quickly to unseen test agents. The approach called MERMADE consists of a planner that has an associated cost for intervening an agent and the goal of this planner is to achieve the designer’s objective. The learning agents maximize the rewards they experience from an environment. \n\nThe approach is evaluated on one-shot adaptation performance of the planner in matrix game setups.\n\n# Strength And Weaknesses\n\nStrengths:\nThe paper is well-written.\nPresents the idea of mechanism design with MERMADE in a clear manner.\nExperiment setups and results are well-presented and the performance of the approach is demonstrably strong wrto baselines. \n\nWeaknesses: \nExperiments are limited to Matrix games and Bandit settings.\nMore random seeds and baselines are needed in the experiments.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clear and is novel as far as I can tell. \nMost of the experiment setup and description of the algorithm is present in the main text, making it easier to reproduce the results.\n\n# Summary Of The Review\n\nMERMADE is interesting because it merges meta-learning with mechanism design. The idea looks very promising. \nHow do these ideas scale to larger/challenging domains (for example, in MDPs with continuous state spaces, with multi-agents environments)? What kind of research questions need to be addressed to make this idea to scale?\n\nHow about including a baseline that is not adapted at test time? What does the performance of the baseline look like when it is evaluated in a zero-shot manner at test time? How large is the gap between MERMADE and this baseline? This will be useful to understand the contribution made by MERMADE over a baseline that is trained similarly but held fixed at test time.\n\nThe experiment results seem to be averaged across 3 random seeds. The experiment setup should run fast and should not be a challenge to report results from many more random seeds. Have the authors considered looking into reporting results from more seeds?\n\nHow are the hyperparameters tuned for MERMADE and for the baseline methods? \n\nIn the experiments, it seems like it is possible to measure the optimal performance of an oracle in mechanism design. It would be interesting to see the difference between MERMADE and such an oracle in the experiments.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPROTFIM: FILL-IN-MIDDLE PROTEIN SEQUENCE DESIGN VIA PROTEIN LANGUAGE MODELS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFollowing the investigation that protein sequence determines its structure and function, engineering protein sequences allows us to optimize the functions of proteins for specific purposes such as enhancement of catalytic activity or binding affinity maturation. In protein engineering, there are many cases where the amino acids in the middle of a protein sequence are changed while maintaining the remaining residues to avoid unwanted functional changes from remaining residues. However, existing research on protein sequence design via protein language models (pLMs) has focused on modifying suffix residues by prompting prefix residues to the model or mutating the overall sequence residues. This is unsuitable for scenarios where the residues located in the middle of the sequence are to be optimized. In this work, we suggest a pLM-based framework to solve the fill-inmiddle (FIM) protein engineering tasks. To evaluate the performance of pLMs on the FIM tasks, we design a novel evaluation scheme where pLMs are tasked to generate new sequences while maintaining the secondary structures. Also, we propose a new PROTein language model specialized for the Fill-In-Middle task, ProtFIM. Experiments confirm that ProtFIM performs FIM engineering efficiently, especially for alpha-helix structures, and provides decent protein representations of sequence-function relationships. Finally, we demonstrate an artificial protein sequence design framework composed of ProtFIM and a high-quality structure predictor as a novel tool to optimize protein sequences.\n\n1\n\nINTRODUCTION\n\nProteins play a crucial role in various parts of biological processes, and the ensemble of diverse functioning proteins is the basis of life’s activities, such as immune response and metabolism. Such essential and versatile functions of proteins are encoded in protein sequences which are the arrangement of amino acid residues. The sequences determine their structures via complex biophysical interactions between residues and these structures are directly linked to the functions of proteins. Thus, optimizing the protein’s function by changing amino acid residues of protein of interest, called protein engineering, has been of great interest in diverse industries such as biofuel (Wen et al., 2009), pharmaceuticals (H Tobin et al., 2014), and agriculture (Rao, 2008).\n\nOne of the representatives of protein sequence design methods is a mutagenesis technique, which gives evolutionarily plausible candidate protein sequence libraries with the help of genetic engineering (Arnold, 1998). However, this approach requires substantial efforts in high-throughput screening experiments. Recently, machine learning-guided protein sequence design strategies have been proposed to achieve a more efficient sequence space search using experimentally acquired labeled data (Yang et al., 2019a).\n\nWith both advances in high-throughput sequencing technologies and language modeling in the field of natural language processing (NLP), protein language models (pLMs), which are trained in an unsupervised manner using tremendous sets of unlabeled protein sequences (Consortium, 2019), have been developed for generating de novo protein sequence (Madani et al., 2020; Hesslow et al., 2022; Moffat et al., 2022; Ferruz et al., 2022; Nijkamp et al., 2022). Existing generative pLMs are trained using an auto-regressive (AR) strategy (Radford et al., 2019; Brown et al., 2020), and generate sequences conditioning on the prefix protein sequences. Unfortunately, if the target region where we want to change amino acid residues is located at the front, existing pLMs uses only\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: An illustrative example of FIM protein engineering. The changed sequences for the target region are generated by generative pLMs or the like, and the structures are altered accordingly.\n\na few preceding amino acid residues (“prompts”) for sequence generation. The interaction sites, positions that interact with other proteins or molecules to perform their functions and are mainly modified to improve functionality, are evenly located on the protein sequence. To prove this, we collect 3D protein structures from Protein Data Bank (PDB) database (Sussman et al., 1998) and calculate the relative positions of protein-protein interaction sites on the protein sequences (see details in Appendix A.1). As illustrated in Figure 5, interacting sites are evenly present on the protein sequence. This result suggests that in protein engineering, modifying the amino acid sequence will be done for the middle part of the sequence in many cases. In this case, existing pLMs may not effectively utilize the information behind them, which can result in poor quality of generation.\n\nIn this work, we regard the middle protein engineering as a fill-in-middle (FIM) sequence generation problem as in Figure 1 and investigate the possibility of pLMs in FIM protein engineering framework. With the emergence of highly accurate protein structure predictors (Jumper et al., 2021; Baek et al., 2021), protein structures are predicted very quickly and accurately with a low cost. Using these advances, we propose a new evaluation scheme, Secondary structurE InFilling rEcoveRy, SEIFER, for FIM protein sequence generation. The secondary structures are usually desirable to be preserved (Rubio et al., 2019) since the binding pockets of other interacting proteins or molecules are fixed to some extent. In SEIFER, models are tasked to recommend protein sequences and achieve two conditions: the new sequences must be different from the original sequences and their secondary structures must be fully maintained. So, SEIFER can assess both the diversity and structure of new sequences simultaneously and we believe that SEIFER is suitable for assessing generated sequences in the field of protein engineering which modifies the amino acid residues of original sequences to improve functions. Also, inspired by the latest research in the field of language models (Bavarian et al., 2022b), we propose a new Protein language model specialized for the Fill-In-Middle task, ProtFIM. Compared to existing pLMs, our proposed ProtFIM use both front (“prefix”) and back (“suffix”) sequence information during training and inference.\n\nThrough SEIFER evaluation, we show that ProtFIM can generate diverse sequences while maintaining secondary structure, especially for α-helix. Furthermore, ProtFIM outperforms when engineering on residues positioned in the front part of a protein sequence compared to existing pLMs, proving that the FIM training is more suitable for FIM engineering compared to AR pLMs. Finally, through analysis and visualization, we prove that ProtFIM has decent representations of protein sequences and can serve as a sequence optimization tool accompanied by AlphaFold2. In summary, our contributions are:\n\n• We define FIM protein engineering as protein sequence infilling tasks and provide the ap-\n\nplicability of protein language models on the task.\n\n• We propose a new evaluation scheme, SEIFER, that can be used to evaluate the performance of pLMs on protein infilling sequence design tasks by considering structural conservation. Through this evaluation, we find that existing AR pLMs are capable of sequence design having α-helix structure.\n\n• We propose a new type of pLM, ProtFIM, that has both AR and FIM capability. Comprehensive results show that ProtFIM has efficient and comparable performances in protein infilling and protein representations for protein engineering compared to other pLMs.\n\n2\n\nFIM engineered proteinsKPHFPLLQLREEGYDKFAIHOriginal sequenceTarget regionTDAARAIYDEOriginal structureWell-structured sequencePoorly-structured sequenceOriginal proteinSequence generation...VISWHLASDMDCVVTLT TDAARAIYDE TQGRQQVLPLDSIYRKTL...Under review as a conference paper at ICLR 2023\n\n• We show that the ProtFIM acts as a sequence optimizer, which generates novel sequences with high pLDDT of AlphaFold2 while maintaining the structures essential for the function of the protein.\n\n2 RELATED WORK\n\nProtein language models Pretraining-based language modeling such as Transformer (Vaswani et al., 2017), BERT (Devlin et al., 2018), and GPT (Ferruz et al., 2022) have revolutionized natural language processing and shown remarkable performance on various tasks such as language understanding, sentence generation, and infilling over the last few years. With a huge increase in the amount of unlabeled protein sequences (Consortium, 2019) produced by high throughput sequencing technologies, pLMs have been introduced and resolved the challenges in protein science and engineering by learning protein languages. BERT-style models primarily provide protein embeddings to solve prediction problems, including protein structure prediction (Rao et al., 2020; Jumper et al., 2021; Lin et al., 2022), function prediction (Brandes et al., 2022), and property prediction (Rives et al., 2021). GPT-style architectures are utilized in resolving generation challenges such as protein sequence design (Madani et al., 2020; Hesslow et al., 2022; Moffat et al., 2022; Ferruz et al., 2022; Nijkamp et al., 2022).\n\nProtein sequence design Attempts to efficiently design protein sequences can be divided into two categories: a method for conducting a large number of high-throughput experiments with mutagenesis and a machine learning-based sequence generation method. Recent advances in experimentalbased methods (Fowler & Fields, 2014) allow us to assess the functional changes of mutated protein sequences at a large scale and produce a lot of labeled data. Many machine learning-based sequence design methods generate the optimized sequences iteratively based on the feedback of labeled data (Yang et al., 2019a; Xu et al., 2020; Wu et al., 2021; Shin et al., 2021). Unfortunately, both approaches require a lot of cost and effort in experiments. Recently, several works generate protein sequences conditioned on given 3D structures using a single energy function (Alford et al., 2017), convolutional neural networks (Zhang et al., 2020; Qi & Zhang, 2020), graph neural networks (Ingraham et al., 2019; Jing et al., 2020; Strokach et al., 2020; Dauparas et al., 2022), or Transformers(Hsu et al., 2022). Since these works require 3D coordinate information to generate sequences, generation may be limited only to areas where high-quality structures exist. Also, in these works, CATH (Orengo et al., 1997) is used to evaluate how similar the generated sequences are to the original sequence. This evaluation method may not be suitable for protein engineering, which aims to change the sequence to have a better function. In parallel, generative pLMs such as RITA (Hesslow et al., 2022), DARK (Moffat et al., 2022), ProtGPT2 (Ferruz et al., 2022), and Progen2 (Nijkamp et al., 2022) have been developed. These generative pLMs generate protein sequences having well-folded and viable structures even though these methods do not employ any structural information. However, due to the nature of the AR model itself, these methods utilize only the preceding sequence information during sequence generation. Our proposed ProtFIM has both AR and FIM property, resulting in efficient FIM protein engineering.\n\n3 METHOD\n\nProblem Setup In NLP, infilling is defined as generating complete text x given incomplete text ̃x, including one or more missing spans. Similarly, we can regard protein engineering on middle residues as an infilling task where models are tasked to return new protein sequences s given incomplete protein sequence ̃s containing missing residues on the target region. Additionally, in the protein infilling task, there is a special structure conservation constraint where the secondary structure of the target site is maintained to approximate the protein engineering scenario properly. Taken together, our goal is to develop a pLM, f ( ̃s; θ), which outputs complete protein sequence s based on a distribution p(s| ̃s) and sequence s must have different residues while having the same secondary structure as that of original residues.\n\n3.1 MODEL REQUIREMENTS\n\nWe suggest four key characteristics of pLMs suitable for protein infilling tasks as follows:\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n• Dynamic property: The model can handle various lengths of protein sequences because the\n\nlengths of the middle sites are diverse depending on various applications.\n\n• Causal modeling: Previous studies reveal that AR pLMs have data-driven co-evolutionary rules across natural protein sequences and generate plausible sequences that tend to be well-folded. So, pLMs which have both AR and infilling capability would be optimal.\n\n• Efficiency: Various strategies, such as pre-processing training data, modifying the model architecture, and using special tokens for controlling, can be used. However, these approaches must be fulfilled as efficiently as possible because protein sequence length is relatively long (we use the maximum length of residues as 1024 in this work).\n\n• Diversity: Because there are many combinations giving the same secondary structures, pLMs which generate diverse sequences different from existing sequences are preferred.\n\nTo achieve the above characteristics, we adopt the idea of FIM transformation, which is a very recently proposed FIM causal language modeling strategy by Bavarian et al. (2022a). The following section explains how to develop FIM pLMs and generate protein sequences using the model.\n\n3.2 MODEL DEVELOPMENT\n\nFIM training In FIM transformation, a span of text from the middle of a whole sentence is moved to its end, and additional special tokens are introduced for marking where spans are from. The transformation is stochastically fulfilled during causal language modeling training. Intriguingly, this simple and straightforward transformation successfully gives fill-in-the-middle (FIM) capability to the model without modifying model architecture and sacrificing left-to-right causal generation capacity. The transformation is easily applied to protein sequence modeling as follows. First, we tokenize each residue R of a protein sequence S with length N to the sequence consisting of corresponding tokens T (see eqn. 1 and 2).\n\nS = (R1, R2, ..., RN )\n\nSt = (T1, T2, ..., TN )\n\n(1)\n\n(2)\n\nSecond, we conduct uniform sampling to get the start position K of the middle span of length L and add special tokens [PRE], [MID], and [SUF] at the beginning of each prefix, middle, and suffix part, respectively. Finally, FIM-transformed sentences are created by concatenating prefix, suffix middle in order as eqn. 3.\n\n′\n\nS\n\nt = ([P RE], R1, ..., RK−1, [SU F ], RK+L+1, ..., RN , [M ID], RK, ..., RK+L)\n\n(3)\n\nBecause several residues are needed to form a secondary structure, the middle residue sampling is conducted so that both prefix and suffix parts have at least four residues. The traditional GPT2 architecture from Hugging Face (Wolf et al., 2019) is used for training, and FIM transformation is applied to the input with a 50% frequency. We denote pLMs trained using FIM transformation as ProtFIM in this work. More details are written in Appendix A.4.\n\nFIM inference for middle residue engineering For generating complete sequences in protein infilling tasks, we consider the target region as the middle part, and the front and back regions to the target region are prefixes and suffixes. Then, we make a prompt for FIM generation by concatenating prefix part, suffix part, and [MID] token as eqn. 4.\n\nP\n\n′\n\nt = ([P RE], R1, ..., RK−1, [SU F ], RK+L+1, ..., RN , [M ID])\n\n(4)\n\n4 EXPERIMENTS\n\nSection 4.1 illustrates our proposed evaluation scheme, SEIFER, specially designed for protein infilling tasks. Section 4.2 describes metrics and various baseline models covering representative language modeling approaches such as causal language modeling (CLM) and permutation language modeling (PLM). Section 4.3 includes evaluation results of SEIFER tasks. Then, section 4.4 and 4.5\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Illustration of our SEIFER evaluation scheme, which estimates the recovery rates of the secondary structures of the generated structure for the original secondary structure.\n\nprovide ablation studies of the SEIFER task concerning relative position and length of target region. Additionally, other metrics for evaluating pLMs, such as perplexity and sequence recovery, are provided in AppendixA.6.\n\n4.1 EVALUATION\n\nProtein secondary structure Protein secondary structures play a key role as an intermediate between the primary sequences and functional tertiary structures that determines the function of proteins in a variety of biological process. Therefore, designing properly optimized combinations of residues having the same secondary structures can lead to enhanced function of protein (Rubio et al., 2019). Protein secondary structures are categorized into regular and irregular categories. First, the regular structure includes α-helix (H), β-sheet (E) (Pauling et al., 1951), and the irregular structure type is a coil (C). In this work, we adopt a 3-class secondary structure definition and those structures are calculated via DSSP (Kabsch & Sander, 1983).\n\nProtein secondary structure recovery via infilling In this work, we propose a new evaluation scheme, Secondary structurE InFilling rEcoveRy, called SEIFER, evaluating the sequence generation and structure conservation simultaneously. In the task, first, models are tasked to generate various sequences to fill the provided target sites. Since secondary structures are calculated based on three-dimensional structural information, the characterization of tertiary protein structures for each generated sequence must be preceded. Unfortunately, conducting experimental characterization on all the new sequences is practically impossible. Instead of this, we utilize Alphafold2 (Jumper et al., 2021), which has shown near-experiments prediction performance, to predict tertiary structures of all generated sequences. Then, secondary structures of each new sequence are calculated via DSSP algorithm using DSSP module of Biopython (Cock et al., 2009). Finally, the secondary structures of new sequences are compared to the original secondary structures. We assign a positive value, 1, on the case where all new residues have the same secondary structure as the original sequences. And all other cases are negative, 0. We illustrate the process of SEIFER in Figure 2. We use proteins presented in CASP14 to obtain candidate middle sites for SEIFER tasks. And, we argue that our experimental setting is reliable because AlphaFold2 was stringently assessed and proved by remarkable prediction performance on the proteins in the CASP14. Additionally, we use the middle sites, which have minimum lengths of 10, 6, and 6 for helix, beta, and coil structures, respectively, considering the average number of residues for the structures.\n\nDifference of SEIFER over protein residue recovery task Sequence recovery has been widely used to evaluate the generation performance of protein generative language models (Ingraham et al., 2019). However, considering that the objective of sequence optimization is to design new sequences with better target properties, recovery of original residues would not be proper. So, a metric is needed to evaluate whether the model can generate a variety of sequences while maintaining the function of the protein. Because the function of protein is directly linked to local structure, evaluating the model ability that generates different residues with the same local structure is a promising way. So, we argue that our proposed SEIFER tasks are appropriate for simulating sequence engineer-\n\n5\n\nFIM generator Generated sequences ...GGEEGYDKFAIH... ...GHKPHFPLLQLR...Original sequence ...KETDAARAIYDE...Generated structures Original structure Generated secondary structures ...EEEEHHHHCCCC... well ...EEEECCCCCCCC... poorOriginal secondary structure ...EEEEHHHHCCCC... AlphaFoldExperiment or AlphaFoldDSSPDSSPRecovery evaluationAlpha Helix (H)Beta sheet (E) Coil (C)Ground truthSequence generationUnder review as a conference paper at ICLR 2023\n\ning scenarios, because in SEIFER tasks models are tasked to recover the protein’s local structure, especially, secondary structures, not residues.\n\n4.2 EXPERIMENTAL SETUP\n\nBaseline We compare our ProtFIM with other pLMs covering diverse generation strategies.\n\n• ProGPT2-C: To prove the effect of FIM over AR in protein engineering, we train AR-based pLMs using the same data, hyperparameters, and the number of parameters compared to ProtFIM. It is similar to the previous AR model, ProtGPT2 (Ferruz et al., 2022), but our model is trained in the amino acid level. Thus, we name the amino acid residue-level (character-level) version of ProtGPT as ProtGPT2-C.\n\n• ProGen2: ProGen2 (Nijkamp et al., 2022) is a concurrently released suite of AR pLMs\n\nwith various parameters.\n\n• ProtXLNet: XLNet(Yang et al., 2019b) can protein sequences using prefix and suffix information. Like FIM, sequences of target sites are generated auto-regressively with conditioning on bidirectional context from both the prefix and suffix. We borrow the publicly released ProtXLNet model, a variant of XLNet for protein (Elnaggar et al., 2022).\n\n• Random generator: This generator is used to approximate the random mutagenesis technique, error-prone PCR (McCullum et al., 2010), which is still commonly used in protein engineering (Dror et al., 2014).\n\nEvaluation metrics SEIFER measures how many sequences with the same secondary structure exist among new sequences created by a generative model. It is like a retrieval or recommendation engine for protein sequences. In the SEIFER task, all models generate K sequences for N middle sites, and all sequences are evaluated by whether the whole secondary structures at each target site are recovered. If the whole secondary structures are recovered, it is a true positive (TP). Then, Precision@K is the mean of TP/K for N sites. Also, we use Retrieval@K, which assumes a positive case where any true positive sequence exists in generated K sequences, zero otherwise. Thus, Retrieval@K is (the number of sites having TP among K)/N.\n\n4.3 EXPERIMENTAL RESULTS ON SEIFER TASKS\n\nAs shown in Table 1 and 2, all pLMs have better performance than the random generator in helix structure recovery in views of both retrieval and precision. These results describe that pLMs are promising tools to fill in middle residues of target protein during protein engineering on helix structure. In contrast, all pLMs perform similarly or worse than the random generator in the β-sheet and coil recovery. To investigate the result, we check the distribution of secondary structures for the proteins with known structures by calculating the distribution of secondary structures in proteins from PDB (details are described in AppendixA.2). Figure 6a and 6b illustrate 3-classes and 4-classes secondary structure distribution, showing that the α-helix structures are dominant in natural protein structures. This empirical result is consistent with the widely known observation in the protein community. We conjecture that this imbalance gives unwanted α-helix bias in existing protein sequence datasets. Additionally, the coil usually has unordered noisy structures. Taking the above facts together, it is possible to say that the similar or worse performances of pLMs in β-sheets and coil cases are reasonable because helix bias makes it models hard to learn the rules of generating residues consisting of the coil and β-sheet.\n\nMeanwhile, we compare ProtFIM over ProtGPT2-C to see the effectiveness of FIM compared to CLM. As shown in each table’s fifth and sixth rows, ProtFIM performs better than ProtGPT2-C in helix recovery. Because both models are trained using the same data, hyperparameters, and the number of parameters except for the utilization of fill-in-middle transformation, these results support that conditioning on both prefixes and suffixes during generation is essential for better sequence design for protein engineering.\n\nWe also compare ProtFIM with other pLMs, such as ProGen2 and ProtXLNet. XLNet is another possible model which is able to fill-in-middle protein engineering using both the prefix and suffix. It is found that ProtXLNet shows strong performance in α-helix compared to the similar scale of\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Model performances on SEIFER tasks in terms of retrieval.\n\nModel\n\n#Params Objective\n\nRandom Generator ProGen2-small ProGen2-medium ProGen2-large ProtXLNet ProtGPT2-C ProtFIM (ours)\n\n- 151M 764M 2.7B 409M 85M 85M\n\n- CLM CLM CLM PLM CLM FIM\n\nE (β-sheet) H (α-helix) R@3 R@5 R@3 R@5 R@3 R@5\n\nC (Coil)\n\n0.46 0.59 0.54 0.59 0.61 0.58 0.57\n\n0.60 0.71 0.66 0.64 0.69 0.66 0.71\n\n0.76 0.67 0.69 0.70 0.66 0.71 0.74\n\n0.82 0.64 0.79 0.79 0.75 0.79 0.82\n\n0.76 0.69 0.70 0.69 0.66 0.69 0.74\n\n0.80 0.80 0.79 0.82 0.76 0.74 0.81\n\nTable 2: Model performances on SEIFER tasks in terms of precision.\n\nModel\n\n#Params Objective\n\nRandom Generator ProGen2-small ProGen2-medium ProGen2-large ProtXLNet ProtGPT2-C ProtFIM (ours)\n\n- 151M 764M 2.7B 409M 85M 80M\n\n- CLM CLM CLM PLM CLM FIM\n\nH (α-helix) P@5 P@3\n\nE (β-sheet) P@5 P@3\n\nC (Coil)\n\nP@3\n\nP@5\n\n0.25 0.32 0.31 0.36 0.37 0.31 0.31\n\n0.25 0.32 0.32 0.34 0.36 0.31 0.32\n\n0.49 0.45 0.45 0.44 0.42 0.48 0.45\n\n0.47 0.43 0.47 0.45 0.42 0.47 0.46\n\n0.47 0.47 0.49 0.50 0.49 0.45 0.48\n\n0.45 0.48 0.47 0.51 0.49 0.45 0.48\n\nmodels, such as ProGen2-small and ProGen2-large. These results prove that sequence design needs to be conducted using the surrounding context of target sites. On the other hand, our proposed ProtFIM shows comparable performance in term of retrieval and competitive performances in term of precision compared to other larger models by 2-20 times. These results show that the FIM scheme is parametrically efficient for protein middle engineering tasks.\n\n4.4 PERFORMANCE WITH REGARD TO THE POSITION OF TARGET REGION\n\nWe start with an assumption that previous AR pLMs would be weak in FIM protein engineering because the sequence generation of AR pLMs is fulfilled by conditioning on only prefixes residues. To verify whether this phenomenon occurs, we ablate the SEIFER performance concerning the relative position of the target middle sites. After dividing each protein sequence into four parts, the α-helix recovery performances of each model corresponding to each part are averaged and illustrated in Figure 3a and Figure 7a. Interestingly, in the first part (front part), only two models, ProtFIM and ProtXLNet, which consider both prefix and suffix part outperforms the random generator, while AR models such as ProtGPT2-C and ProGen-series do not. These results prove our assumption empirically.\n\nAdditionally, the fact that ProtFIM outperforms ProtXLNet in the front part shows the effectiveness of the FIM training scheme because ProtFIM has five times fewer parameters. Meanwhile, it is found that PLMs are generally better than the random generator in other parts, supporting the effectiveness of pLMs on protein middle engineering. In addition, it can be seen that the model’s performance is not uniform over positions. We think that it is due to the lack of an evaluation dataset because the number of used CASP proteins is 28. However, since the models are compared under the same conditions, the insight obtained from the performance comparison in the experiments is reliable.\n\n4.5 PERFORMANCE WITH REGARD TO LENGTH OF TARGET REGION\n\nWe can see that the random generator shows comparable performances to pLMs in several tests in the above results. To investigate this phenomenon, we ablate the SEIFER performances according to the length of the middle sites. We partition the range of lengths into four parts, and plot corresponding averaged Recall@K and Precision@K as in Figure 3b and Figure 7b. Interestingly, the random generator performs similarly to pLMs in the first quarter (short length size). However, the perfor-\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 3: Performance changes in the term of retrieval with regard to (a) relative positions and (b) length of middle sites.\n\nmance of the random generator drastically drops as the length of the target sites becomes longer, and it fails all predictions when the middle sites are longer than 30.\n\nMeanwhile, the performance of pLMs degrades gradually and fails at the last part, where the middle sites are longer than 40. All the results imply that the length of the middle sites is the main factor for model performance. We explain this using the degree of freedom on possible protein structures of target middle sites. Since the high-dimensional interactions between amino acids make the structure of the protein, the structure is determined to some extent by the structural context from other residues except residues of the middle sites. In other words, the degree of freedom in the structure of the middle sites is relatively small due to the non-target residues. Considering that any amino acid is a building block of a α-helix, β-sheet, and coil structure, even if the amino acid is randomly sampled, there will be a high probability of obtaining the desired original structure in FIM scenarios. Meanwhile, the observation that pLMs still work at the longer middle sites shows that pLMs would be a promising solution for long FIM protein sequence design, giving efficient sequence search compared to random generator.\n\n5 ANALYSIS AND VISUALIZATION\n\nTable 3: Zero-shot fitness prediction on FLIP tasks. All scores are Spearman correlation.\n\nModel\n\n#Params Objective AAV GB1 Meltome Meta Avg.\n\nESM-1b (mean) ESM-1v (mean) ProGen2-small ProGen2-medium ProGen2-large ProtXLNet ProtGPT2-C ProtFIM\n\n750M 750M 151M 764M 2.7B 409M 80M 80M\n\nMLM MLM CLM CLM CLM PLM CLM FIM\n\n0.36 0.33 0.39 0.18 0.41 0.33 0.40 0.39\n\n0.34 0.38 -0.21 -0.11 0.24 0.29 0.18 0.25\n\n0.71 0.72 0.56 0.59 0.68 0.47 0.53 0.60\n\n0.47 0.48 0.25 0.22 0.44 0.36 0.37 0.41\n\n5.1 REPRESENTATION QUALITY\n\nCollecting experimental functional properties of protein sequence gives insights into a sequence-tofunction relationship called fitness landscape. In protein engineering, the fitness landscape is used to rank designed sequences. To this end, pLMs can provide sequence representation for fitness prediction. Recently, FLIP benchmarks have been introduced to assess the quality of representations of pLMs (Dallago et al., 2021). Using FLIP, we compare the embeddings of ProtFIM with baselines.\n\n8\n\n(0.0, 0.25](0.25, 0.5](0.5, 0.75](0.75, 1.0]Relative Position Bins0.30.40.50.60.70.8R@3ProtFIMProtGPT2-CRandomProGen2-smallProGen2-mediumProGen2-largeProtXLNet(10, 20](20, 30](30, 40](40, 50]Length Bins0.00.10.20.30.40.50.60.7R@3ProtFIMProtGPT2-CRandomProGen2-smallProGen2-mediumProGen2-largeProtXLNetUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 4: (a) Cumulative density plot on pLDDT change and (b) an example of a case where pLDDT increases or decreases after protein sequence design via ProtFIM.\n\nAdditionally, ESM-1b and 1v (Rao et al., 2020) are added to compare FIM with masked language modeling (MLM). In FLIP, embeddings of sequences are directly used to predict fitness without fine-tuning pLMs. For a fair comparison, embeddings are obtained via averaging of all residue representations. Table 3 shows that ProtFIM has comparable zero-shot fitness prediction performance even if the ProtFIM capacity is multiple times smaller than other models. This result implies that the embeddings of ProtFIM are effective for both FIM protein engineering and zero-shot fitness prediction. Detailed scores are included in Appendix A.8.\n\n5.2\n\nPLDDT CHANGE AND VISUALIZATION\n\nAlphaFold2 gives a per-residue confidence metric called the predicted local distance difference test (pLDDT) ranging from 0 to 100. Recently, several works have used the metric as a scoring criterion to assess designed protein sequence by assuming that the higher pLDDT, the better and more plausible structure (Moffat et al., 2022; Wang et al., 2022). To assess the FIM engineering performance of models in terms of pLDDT, we visualize the difference between pLDDT of the structure of both new sequences and the corresponding original sequence using a cumulative density plot. Figure 4a reveals that positive cases where pLDDT increases after FIM engineering are rare for all models, but pLMs have more chance to get sequences with higher pLDDT. We cherry-pick a protein and visualize the original structure and modified structures through ProtFIM as shown in Figure 4b. The new two sequences of middle sites are different from the original sequences, but all have α-helix. Interestingly, in-depth visualization considering the side-chain unveils the subtle difference, resulting in well or poorly-optimized sequences. All the above results demonstrate that our model, with the help of AlphaFold2, can serve as a sequence design framework, which optimizes the target sequence while maintaining the structures essential for the protein’s function.\n\n6 CONCLUSION\n\nIn this work, we show the FIM protein sequence design framework via pLMs and propose a new protein language model, ProtFIM, which is specialized for the framework. By evaluating various models via our proposed new evaluation scheme, SEIFER, ProtFIM performs FIM protein sequence design efficiently compared to existing pLMs. Additional analysis and visualization also prove that ProtFIM is a promising tool for practical protein engineering such as fitness prediction and sequence optimization.\n\n9\n\n−40−30−20−100ΔpLDDT0.00.20.40.60.81.0Cumulative DensityProtFIMProGen-largeRandomRLQALREIARVpLDDT: 97.05ARAGLLEIARVpLDDT: 97.14KADALREMVRVpLDDT: 96.68Original structure ...MPDRLQALREIARVLRH...Generated structuresUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRebecca F Alford, Andrew Leaver-Fay, Jeliazko R Jeliazkov, Matthew J O’Meara, Frank P DiMaio, Hahnbeom Park, Maxim V Shapovalov, P Douglas Renfrew, Vikram K Mulligan, Kalli Kappel, et al. The rosetta all-atom energy function for macromolecular modeling and design. Journal of chemical theory and computation, 13(6):3031–3048, 2017.\n\nFrances H Arnold. Design by directed evolution. Accounts of chemical research, 31(3):125–131,\n\n1998.\n\nMinkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie Lee, Jue Wang, Qian Cong, Lisa N Kinch, R Dustin Schaeffer, et al. Accurate prediction of protein structures and interactions using a three-track neural network. Science, 373(6557):871– 876, 2021.\n\nMohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022a.\n\nMohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022b.\n\nNadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial. Proteinbert: A universal deep-learning model of protein sequence and function. Bioinformatics, 38(8):2102–2110, 2022.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nDrew H Bryant, Ali Bashir, Sam Sinai, Nina K Jain, Pierce J Ogden, Patrick F Riley, George M Church, Lucy J Colwell, and Eric D Kelsic. Deep diversification of an aav capsid protein by machine learning. Nature Biotechnology, 39(6):691–696, 2021.\n\nPeter JA Cock, Tiago Antao, Jeffrey T Chang, Brad A Chapman, Cymon J Cox, Andrew Dalke, Iddo Friedberg, Thomas Hamelryck, Frank Kauff, Bartek Wilczynski, et al. Biopython: freely available python tools for computational molecular biology and bioinformatics. Bioinformatics, 25(11):1422–1423, 2009.\n\nUniProt Consortium. Uniprot: a worldwide hub of protein knowledge. Nucleic acids research, 47\n\n(D1):D506–D515, 2019.\n\nChristian Dallago, Jody Mou, Kadina E Johnston, Bruce J Wittmann, Nicholas Bhattacharya, Samuel Goldman, Ali Madani, and Kevin K Yang. Flip: Benchmark tasks in fitness landscape inference for proteins. bioRxiv, 2021.\n\nJustas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learning– based protein sequence design using proteinmpnn. Science, pp. eadd2187, 2022.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAdi Dror, Einav Shemesh, Natali Dayan, and Ayelet Fishman. Protein engineering by random mutagenesis and structure-guided consensus of geobacillus stearothermophilus lipase t6 for enhanced stability in methanol. Applied and environmental microbiology, 80(4):1515–1527, 2014.\n\nAhmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard Rost. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):7112–7127, 2022. doi: 10.1109/TPAMI.2021.3095381.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nNoelia Ferruz, Steffen Schmidt, and Birte H ̈ocker. Protgpt2 is a deep unsupervised language model\n\nfor protein design. Nature communications, 13(1):1–10, 2022.\n\nDouglas M Fowler and Stanley Fields. Deep mutational scanning: a new style of protein science.\n\nNature methods, 11(8):801–807, 2014.\n\nPeter H Tobin, David H Richards, Randolph A Callender, and Corey J Wilson. Protein engineering:\n\na new frontier for biological therapeutics. Current drug metabolism, 15(7):743–756, 2014.\n\nDaniel Hesslow, Niccol ́o Zanichelli, Pascal Notin, Iacopo Poli, and Debora Marks. Rita: a study on\n\nscaling up generative protein sequence models. arXiv preprint arXiv:2205.05789, 2022.\n\nChloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexan-\n\nder Rives. Learning inverse folding from millions of predicted structures. bioRxiv, 2022.\n\nJohn Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for graph-\n\nbased protein design. Advances in neural information processing systems, 32, 2019.\n\nAnna Jarzab, Nils Kurzawa, Thomas Hopf, Matthias Moerch, Jana Zecha, Niels Leijten, Yangyang Bian, Eva Musiol, Melanie Maschberger, Gabriele Stoehr, et al. Meltome atlas—thermal proteome stability across the tree of life. Nature methods, 17(5):495–503, 2020.\n\nBowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. arXiv preprint arXiv:2009.01411, 2020.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.\n\nWolfgang Kabsch and Christian Sander. Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features. Biopolymers: Original Research on Biomolecules, 22(12):2577–2637, 1983.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nZeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017.\n\nAli Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R Eguchi, Po-Ssu Huang, and Richard Socher. Progen: Language modeling for protein generation. arXiv preprint arXiv:2004.03497, 2020.\n\nElizabeth O McCullum, Berea AR Williams, Jinglei Zhang, and John C Chaput. Random mutagen-\n\nesis by error-prone pcr. In In vitro mutagenesis protocols, pp. 103–109. Springer, 2010.\n\nLewis Moffat, Shaun M Kandathil, and David T Jones. Design in the dark: Learning deep generative\n\nmodels for de novo protein design. bioRxiv, 2022.\n\nErik Nijkamp, Jeffrey Ruffolo, Eli N Weinstein, Nikhil Naik, and Ali Madani. Progen2: exploring\n\nthe boundaries of protein language models. arXiv preprint arXiv:2206.13517, 2022.\n\nChristine A Orengo, Alex D Michie, Susan Jones, David T Jones, Mark B Swindells, and Janet M Thornton. Cath–a hierarchic classification of protein domain structures. Structure, 5(8):1093– 1109, 1997.\n\nLinus Pauling, Robert B Corey, and Herman R Branson. The structure of proteins: two hydrogenbonded helical configurations of the polypeptide chain. Proceedings of the National Academy of Sciences, 37(4):205–211, 1951.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYifei Qi and John ZH Zhang. Densecpd: improving the accuracy of neural-network-based computational protein sequence design with densenet. Journal of chemical information and modeling, 60(3):1245–1252, 2020.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nA Gururaj Rao. The outlook for protein engineering in crop improvement. Plant physiology, 147\n\n(1):6–12, 2008.\n\nRoshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. Transformer\n\nprotein language models are unsupervised structure learners. Biorxiv, 2020.\n\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.\n\nMarcelo Ventura Rubio, C ́esar Rafael Fanchini Terrasan, Fabiano Jares Contesini, Mariane Paludetti Zubieta, Jaqueline Aline Gerhardt, Leandro Cristante Oliveira, Any Elisa de Souza Schmidt Gonc ̧alves, Fausto Almeida, Bradley Joseph Smith, Gustavo Henrique Martins Ferreira De Souza, et al. Redesigning n-glycosylation sites in a gh3 β-xylosidase improves the enzymatic efficiency. Biotechnology for biofuels, 12(1):1–14, 2019.\n\nJung-Eun Shin, Adam J Riesselman, Aaron W Kollasch, Conor McMahon, Elana Simon, Chris Sander, Aashish Manglik, Andrew C Kruse, and Debora S Marks. Protein design and variant prediction using autoregressive generative models. Nature communications, 12(1):1–11, 2021.\n\nAlexey Strokach, David Becerra, Carles Corbi-Verge, Albert Perez-Riba, and Philip M Kim. Fast and flexible protein design using deep graph neural networks. Cell systems, 11(4):402–411, 2020.\n\nJoel L Sussman, Dawei Lin, Jiansheng Jiang, Nancy O Manning, Jaime Prilusky, Otto Ritter, and Enrique E Abola. Protein data bank (pdb): database of three-dimensional structural information of biological macromolecules. Acta Crystallographica Section D: Biological Crystallography, 54 (6):1078–1084, 1998.\n\nBaris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt Consortium. Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6):926–932, 2015.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nJue Wang, Sidney Lisanza, David Juergens, Doug Tischer, Joseph L Watson, Karla M Castro, Robert Ragotte, Amijai Saragovi, Lukas F Milles, Minkyung Baek, et al. Scaffolding protein functional sites using deep learning. Science, 377(6604):387–394, 2022.\n\nFei Wen, Nikhil U Nair, and Huimin Zhao. Protein engineering in designing tailored enzymes and microorganisms for biofuels production. Current opinion in biotechnology, 20(4):412–419, 2009.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ́emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n\nNicholas C Wu, Lei Dai, C Anders Olson, James O Lloyd-Smith, and Ren Sun. Adaptation in\n\nprotein fitness landscapes is facilitated by indirect paths. Elife, 5:e16965, 2016.\n\nZachary Wu, Kadina E Johnston, Frances H Arnold, and Kevin K Yang. Protein sequence design\n\nwith deep generative models. Current opinion in chemical biology, 65:18–27, 2021.\n\nYuting Xu, Deeptak Verma, Robert P Sheridan, Andy Liaw, Junshui Ma, Nicholas M Marshall, John McIntosh, Edward C Sherer, Vladimir Svetnik, and Jennifer M Johnston. Deep dive into machine learning models for protein engineering. Journal of chemical information and modeling, 60(6): 2773–2790, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nKevin K Yang, Zachary Wu, and Frances H Arnold. Machine-learning-guided directed evolution for\n\nprotein engineering. Nature methods, 16(8):687–694, 2019a.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019b.\n\nYuan Zhang, Yang Chen, Chenran Wang, Chun-Chao Lo, Xiuwen Liu, Wei Wu, and Jinfeng Zhang. Prodconn: Protein design using a convolutional neural network. Proteins: Structure, Function, and Bioinformatics, 88(7):819–829, 2020.\n\nA APPENDIX\n\nA.1\n\nINTERACTION SITES EXTRACTION\n\nFigure 5: Relative positions of the interacting sites on the protein sequences.\n\nWe download 3D protein structures from PDB database (Sussman et al., 1998) and extract protein structures satisfying several conditions: having more than two protein chains; having UniProt ID and a length of the entire sequence in mmCIF dictionaries from MMCIF2Dict module of Biopython (Cock et al., 2009). We hypothesize that the two residue pairs of two different chains would be involved in the interaction if any atom excluding hydrogen of the residues were at a Euclidean distance of 8 ̊A or less. Then, we identify all residues which are likely to be involved in the interactions and find where these residues are located on the entire protein sequence.\n\nA.2 SECONDARY STRUCTURE STATISTICS\n\nWe analyze the secondary structures of 166,512 structures that can be processed through a DSSP module of Biopython. Biopython classifies the secondary structures as eight classes by default: alpha helix (4-12) (code: ‘H’), isolated beta-bridge residue (code: ‘B’), strand (code: ‘E’), 3-10 helix (code: ‘G’), pi helix (code: ‘I’), turn (code: ‘T’), bend (code: ‘S’), and none (code: ‘-’). In our study, The eight classes are mapped to the three classes as follows: ‘H’, ‘G’, and ‘I’ are mapped to the α-helix class ‘H’; ‘B’ and ‘E’ are mapped to the β-sheet class ‘E’; ‘T’, ‘S’, ‘C’, and ‘-’ are mapped to the coil class ‘C’. In addition, in Figure 6b, ‘-’ is displayed separately.\n\nFigure 6a and 6b show that α-helix substructures are dominant in natural proteins, meaning imbalance. Furthermore, coil structures have rules that are difficult to capture. Therefore, the model trained using the existing natural protein database would be familiar with the α-helix generation. Therefore, a preprocessing or encoding technique that can alleviate the α-helix bias can be a good research topic in the future.\n\n13\n\n0.00.20.40.60.81.0Relative position0.00.20.40.60.81.0DensityUnder review as a conference paper at ICLR 2023\n\n(a) 3-classes\n\n(b) 3-classes and none\n\nFigure 6: Secondary structure distribution of proteins in PDB database. H, E, C, and - correspond to α-helix, β-sheet, coil, and none-type. (a) describes 3-classes secondary structure distribution. And, because coil can be divided into two categories, the coil and none-type structure in DSSP algorithm, we can calculate 4-classes distribution as shown in (b).\n\nA.3 TRAINING DATASETS\n\nFor training, protein sequences from UniRef50(Suzek et al., 2015) dated March 28, 2018 version are used to avoid leakage of CASP13, 14 and conduct a fair comparison with other models. 5% of protein sequences in the UniRef50 are randomly selected as a held-out validation set. The total number of sequences in training data is 25M.\n\nA.4 TRAINING DETAILS\n\nProtFIM is trained with a batch size of 128. The maximum length of each protein sequence we used for training is 1024. For ProtFIM optimization, we use AdamW optimizer Kingma & Ba (2014); Loshchilov & Hutter (2017) with a weight decay ratio of 1e-5. The learning rate is scheduled using cosine-warmup strategy. The total optimization step is 500k with 1k warmup steps. We train the model on 8 NVIDIA A100s in 4 days. FIM transformation is applied with 50% of probability. The model consists of 12 layers with a feature dimension of 768. The architecture is based on the released GPT2-base model by HuggingFace (Wolf et al., 2019).\n\nA.5 GENERATION HYPER-PARAMETERS\n\nWe conduct sequence generation using HuggingFace generation API. Th topK and topP values are set to 100 and 0.95. We set the temperature as 1.0. After sequence generation, we select top-K sequences. If shorter sequences are generated compared to the length of middle sites, we increase topK by 10 and conduct generation until K sentences are collected. We use the default option of HuggingFace API for other hyper-parameters. These hyper-parameters and generation processes are applied on ProtFIM, ProtGPT2, and ProGen2 models for a fair comparison. Also, we use ProtXLNet to generate sequences of target sites auto-regressively with conditioning on bidirectional context using topK sampling as other AR models.\n\nA.6 PERPLEXITY AND SEQUENCE RECOVERY\n\nWe also add other evaluation metrics, such as perplexity and sequence recovery rates, which are widely used for evaluating language models in inverse folding. Table 4 shows the result of perplexity and sequence recovery rates. ProtFIM performs poorly in terms of perplexity and sequence recovery rates. In the FIM paper (Bavarian et al., 2022b), some experiments find that perplexity alone is insufficient for evaluating the infilling task because infilling is conducted in a somewhat different nature compared to conventional left-to-light generation as expressed like PF IM (M | P, S) > PAR(M | P ) where P, M, S indicate prefix, middle, and suffix part, respectively.\n\n14\n\nHECSecondary structure012345Counts1e7HEC-Secondary structure01234Counts1e7Under review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 7: Performance changes in the metric of Precision@3 with regard to relative positions and length of middle sites.\n\nAdditionally, our targeted infilling task aims to design various sequences in the presence of local structure constraints considering the surrounding context, which is quite different from restoring residues as much as possible. So, there needs to be an appropriate evaluation scheme simulating protein middle engineering tasks that change amino acid residues of local parts of the protein to optimize the target protein, such as enzymes and antibodies. So, sequence residue recovery rate, a widely used metric to evaluate models’ sequence design performance, is insufficient for the protein infilling task. Based on the above results and descriptions, we argue that our proposed SEIFER tasks are more appropriate for evaluating protein infilling tasks than existing metrics such as perplexity and sequence recovery rates.\n\nTable 4: Perplexity and sequence recovery rates\n\nModel\n\n#Params Objective\n\nPerplexity (↓) Recovery rate (%) (↑)\n\nProGen2-small ProGen2-medium ProGen2-large ProtXLNet ProtGPT2-C ProtFIM\n\n151M 764M 2.7B 409M 80M 80M\n\nCLM CLM CLM PLM CLM FIM\n\n16.88 16.17 16.24 16.58 17.08 17.04\n\n8 10 9\n8 8\n9\n\nA.7 PRECISION@K WITH REGARD TO POSITION AND LENGTH\n\nTable 7a and 7b include ablation studies of SEIFER performance in term of precision according to relative positions and length of target sites in a protein.\n\nA.8 FLIP\n\nTable 5, 6, and 7 contain the zero-shot fitness prediction performances of various pLMs on three fitness landscapes.\n\n15\n\n(0.0, 0.25](0.25, 0.5](0.5, 0.75](0.75, 1.0]Relative Position Bins0.150.200.250.300.350.400.450.50Precision@3ProtFIMProtGPT2-CRandomProGen2-smallProGen2-mediumProGen2-largeProtXLNet(10, 20](20, 30](30, 40](40, 50]Length Bins0.000.050.100.150.200.250.300.35Precision@3ProtFIMProtGPT2-CRandomProGen2-smallProGen2-mediumProGen2-largeProtXLNetUnder review as a conference paper at ICLR 2023\n\nTable 5: Zero-shot fitness prediction on adeno-associated virus (AAV) capsid proteins (Bryant et al., 2021). All scores are Spearman correlation.\n\nModel\n\n#Params Objective Mut-Des Des-Mut\n\n1-vs-rest\n\n2-vs-rest\n\n7-vs-rest\n\nlow-vs-high Avg.\n\nESM-1b (mean) ESM-1v (mean) ProGen2-small ProGen2-medium ProGen2-large ProtXLNet ProtGPT2-C ProtFIM\n\n750M 750M 151M 764M 2.7B 409M 80M 80M\n\nMLM MLM CLM CLM CLM PLM CLM FIM\n\n0.63 0.55 0.38 0.19 0.68 0.55 0.59 0.53\n\n0.59 0.44 0.53 0.25 0.67 0.58 0.66 0.56\n\n0.04 0.18 0.39 0.14 0.33 0.21 0.24 0.32\n\n0.26 0.16 0.47 0.30 0.20 0.02 0.34 0.24\n\n0.46 0.45 0.43 0.22 0.42 0.42 0.41 0.44\n\n0.18 0.20 0.14 0.00 0.13 0.20 0.16 0.28\n\n0.36 0.33 0.39 0.18 0.41 0.33 0.40 0.39\n\nTable 6: Zero-shot fitness prediction on adeno-associated virus GB1 landscape (Wu et al., 2016). All scores are Spearman correlation.\n\nModel\n\n#Params Objective\n\n1-vs-rest\n\n2-vs-rest\n\n3-vs-rest\n\nlow-vs-high Avg.\n\nESM-1b (mean) ESM-1v (mean) ProGen2-small ProGen2-medium ProGen2-large ProtXLNet ProtGPT2-C ProtFIM\n\n750M 750M 151M 764M 2.7B 409M 80M 80M\n\nMLM MLM CLM CLM CLM PLM CLM FIM\n\n0.32 0.32 -0.27 -0.06 0.19 0.18 0.02 0.01\n\n0.36 0.32 -0.30 -0.16 0.28 0.33 0.05 0.18\n\n0.54 0.77 -0.26 -0.12 0.44 0.44 0.44 0.63\n\n0.13 0.10 -0.03 -0.10 0.06 0.21 0.20 0.18\n\n0.34 0.38 -0.21 -0.11 0.24 0.29 0.18 0.25\n\nTable 7: Zero-shot fitness prediction on landscape from the Meltome Atlas (Jarzab et al., 2020). All scores are Spearman correlation.\n\nModel\n\n#Params Objective Mixed Human Human-Cell Avg.\n\nESM-1b (mean) ESM-1v (mean) ProGen2-small ProGen2-medium ProGen2-large ProtXLNet ProtGPT2-C ProtFIM\n\n750M 750M 151M 764M 2.7B 409M 80M 80M\n\nMLM MLM CLM CLM CLM PLM CLM FIM\n\n0.68 0.67 0.46 0.49 0.67 0.44 0.49 0.51\n\n0.70 0.75 0.63 0.66 0.70 0.52 0.55 0.66\n\n0.75 0.74 0.59 0.62 0.66 0.47 0.54 0.63\n\n0.71 0.72 0.56 0.59 0.68 0.47 0.53 0.60\n\n16",
    "reference": "# Summary Of The Paper\n\nThe paper's aim is sequence-based protein engineering based on protein language models (PLMs). For this purpose, it uses a recently developed self-supervised in-filling language model. The model rearranges the middle (to be infilled) part of a sequence and move it to the end of the sequence which enables the use of standard forward prediction. The paper then tests these PLM models for identifying other possible residue sequences in the middle of a protein sequence that preserves the general 3D structure of the protein (secondary structural type).\n\n# Strength And Weaknesses\n\n- the paper provides a novel method for PLM especially suitable for sequence-based protein optimization.\n- despite being a language model, the proposed architecture is relatively small in number of parameters which enables efficient use at test time and enables research for protein-engineering with limited computational resources.\n- the additional experiments regarding the quality and the general analysis of the learnt representations are interesting, encouraging and informative.\n\n*Weaknesses*:\n- For an ML conference there seems to be no technical novelty, neither fundamental, nor incremental. The model is almost an exact copy of the standard LM with the addition of the structure constraint.\n- The number of available protein (and non protein) language models are vast. The paper only compares with one method while it could (and should have) compared with other PLMs but also possibly other language models with the same trick (moving the missing part to the end).\n- the main results that is the goal of the work for protein engineering seem quite comparable with the only baseline that is used (ProGen).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper can be improved in clarity in general. While the idea of the paper is straightforward and thus easy to understand. The organization of the content, the notation, and figures can be revised to improve the flow. For instance, the introduction section can become sharper and more concise while the method section can benefit from more formal and structured content of the general problem, possibly the existing variants and the proposed approach.\n\n# Summary Of The Review\n\nThe paper uses a novel method for protein language models that is quite efficient in number of parameters and shows the results are comparable for protein sequence engineering compared to one other recent PLM baseline. However, considering that the results are clearly positive compared to the possible baselines, the lack of technical novelty (even in form of an increment on existing methods), the lack of a thorough set of baselines, and presentation of the work, I believe the paper is not ready for publication, at least at a ML venue. More developments on the method and/or a more thorough empirical evidence will increase the quality of the paper in another revision.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nHESSCALE: SCALABLE COMPUTATION OF HESSIAN DIAGONALS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nSecond-order optimization uses curvature information about the objective function, which can help in faster convergence. However, such methods typically require expensive computation of the Hessian matrix, preventing their usage in a scalable way. The absence of efficient ways of computation drove the most widely used methods to focus on first-order approximations that do not capture the curvature information. In this paper, we develop HesScale, a scalable approach to approximating the diagonal of the Hessian matrix, to incorporate second-order information in a computationally efficient manner. We show that HesScale has the same computational complexity as backpropagation. Our results on supervised classification show that HesScale achieves high approximation accuracy, allowing for scalable and efficient second-order optimization.1\n\n1\n\nINTRODUCTION\n\nFirst-order optimization offers a cheap and efficient way of performing local progress in optimization problems by using gradient information. However, their performance suffers from instability or slow progress when used in ill-conditioned landscapes. Such a problem is present because firstorder methods do not capture curvature information which causes two interrelated issues. First, the updates in first-order have incorrect units (Duchi et al. 2011), which creates a scaling issue. Second, first-order methods lack parameterization invariance (Martens 2020) in contrast to secondorder methods such as natural gradient (Amari 1998) or Newton-Raphson methods. Therefore, some first-order normalization methods were developed to address the invariance problem (Ba et al. 2016, Ioffe & Szegedy 2015, Salimans & Kingma 2016). On the other hand, some recent adaptive stepsize methods try to alleviate the scaling issue by using gradient information for first-order curvature approximation (Luo et al. 2019, Duchi et al. 2011, Zeiler 2012, Reddi et al. 2018, Kingma & Ba 2015, Tran & Phong 2019, Tieleman et al. 2012). Specifically, such methods use the empirical Fisher diagonals heuristic by maintaining a moving average of the squared gradients to approximate the diagonal of the Fisher information matrix. Despite the huge adoption of such methods due to their scalability, they use inaccurate approximations. Kunstner et al. (2019) showed that the empirical Fisher does not generally capture curvature information and might have undesirable effects. They argued that the empirical Fisher approximates the Fisher or the Hessian matrices only under strong assumptions that are unlikely to be met in practice. Moreover, Wilson et al. (2017) presented a counterexample where the adaptive step-size methods are unable to reduce the error compared to non-adaptive counterparts such as stochastic gradient descent.\n\nAlthough second-order optimization can speed up the training process by using the geometry of the landscape, its adoption is minimal compared to first-order methods. The exact natural gradient or Newton-Raphson methods require the computation, storage, and inversion of the Fisher information or the Hessian matrices, making them computationally prohibitive in large-scale tasks. Accordingly, many popular second-order methods attempt to approximate less expensively. For example, a type of truncated-Newton method called Hessian-free methods (Martens 2010) exploits the fact that the Hessian-vector product is cheap (Bekas et al. 2007) and uses the iterative conjugate gradient method to perform an update. However, such methods might require many iterations per update or some tricks to achieve stability, adding computational overhead (Martens & Sutskever 2011).\n\n1Code will be available.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nSome variations try to approximate only the diagonals of the Hessian matrix using stochastic estimation with matrix-free computations (Chapelle & Erhan 2011, Martens et al. 2012, Yao et al. 2021). Other methods impose probabilistic modeling assumptions and estimate a block diagonal Fisher information matrix (Martens & Grosse 2015, Botev et al. 2017). Such methods are invariant to reparametrization but are computationally expensive since they need to perform matrix inversion for each block.\n\nDeterministic diagonal approximations to the Hessian (LeCun et al. 1990, Becker & Lecun 1989) provide some curvature information and are efficient to compute. Specifically, they can be implemented to be as efficient as first-order methods. We view this category of approximation methods as scalable second-order methods. In neural networks, curvature backpropagation (Becker & Lecun 1989) can be used to backpropagate the curvature vector. We distinguish this efficient method from other expensive methods (e.g., Mizutani & Dreyfus 2008, Botev et al. 2017) that backpropagate the full Hessian matrices. Although these diagonal methods show a promising direction for scalable second-order optimization, the approximation quality is sometimes poor with objectives such as cross-entropy (Martens et al. 2012). A scalable second-order method with high quality approximation is still needed.\n\nIn this paper, we present HesScale, a high-quality approximation method for the Hessian diagonals. Our method is also scalable and has little memory requirement with linear computational complexity while maintaining high approximation accuracy.\n\n2 BACKGROUND\n\nIn this section, we describe the Hessian matrix for neural networks and some existing methods for estimating it. Generally, Hessian matrices can be computed for any scalar-valued function that are twice differentiable. If f : Rn → R is such a function, then for its argument ψ ∈ Rn, the Hessian matrix H ∈ Rn×n of f with respect to ψ is given by Hi,j = ∂2f (ψ)/∂ψi∂ψj. Here, the ith element of a vector v is denoted by vi, and the element at the ith row and jth column of a matrix M is denoted by Mi,j. When the need for computing the Hessian matrix arises for optimization in deep learning, the function f is typically the objective function, and the vector ψ is commonly the weight vector of a neural network. Computing and storing an n × n matrix, where n is the number of weights in a neural network, is expensive. Therefore, many methods exist for approximating the Hessian matrix or parts of it with less memory footprint, computational requirement, or both. A common technique is to utilize the structure of the function to reduce the computations needed. For example, assuming that connections from a certain layer do not affect other layers in a neural network allows one to approximate a block diagonal Hessian. The computation further simplifies when we have piece-wise linear activation functions (e.g., ReLU), which result in a Generalized Gauss-Newton (GGN) (Schraudolph 2002) approximation that is equivalent to the block diagonal Hessian matrix with linear activation functions. The GGN matrix is more favored in second-order optimization since it is positive semi-definite. However, computing a block diagonal matrix is still demanding.\n\nMany approximation methods were developed to reduce the storage and computation requirements of the GGN matrix. For example, under probabilistic modeling assumptions, the Kronecker-factored Approximate Curvature (KFAC) method (Martens & Grosse 2015) writes the GGN matrix G as a Kronecker product of two matrices of smaller sizes as: G = A ⊗ B, where A = E[hh⊤], B = E[gg⊤], h is the activation output vector, and g is the gradient of the loss with respect to the activation input vector. The A and B matrices can be estimated by Monte Carlo sampling and an exponential moving average. KFAC is more efficient when used in optimization since it requires inverting only the small matrices using the Kronecker-product property (A ⊗ B)−1 = A−1 ⊗ B−1. However, KFAC is still expensive due to the storage of the block diagonal matrices and computation of Kronecker product, which prevent it from being used as a scalable method.\n\nComputing the Hessian diagonals can provide some curvature information with relatively less computation. However, it has been shown that the exact computation for diagonals of the Hessian typically has quadratic complexity with the unlikely existence of algorithms that can compute the exact diagonals with less than quadratic complexity (Martens et al. 2012). Some stochastic methods provide a way to compute unbiased estimates of the exact Hessian diagonals. For example, the AdaHessian (Yao et al. 2021) algorithm uses the Hutchinson’s estimator diag(H) = E[z ◦ (Hz)], where z is a multivariate random variable with a Rademacher distribution and the expectation can\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nbe estimated using Monte Carlo sampling with an exponential moving average. Similarly, the GGNMC method (Dangel et al. 2020) uses the relationship between the Fisher information matrix and the Hessian matrix under probabilistic modeling assumptions to have an MC approximation of the diagonal of the GGN matrix. Although these stochastic approximation methods are scalable due to linear or O(n) computational and memory complexity, they suffer from low approximation quality, improving which requires many sampling and factors of additional computations.\n\n3 THE PROPOSED HESSCALE METHOD\n\nIn this section, we present our method for approximating the diagonal of the Hessian at each layer in feed-forward networks, where a backpropagation rule is used to utilize the Hessian of previous layers. We present the derivation of the backpropagation rule for fully connected and convolutional neural networks in supervised learning. Similar derivation for fully connected networks with mean squared error is presented before (LeCun et al. 1990, Becker & Lecun 1989). However, we use the exact diagonals of the Hessian matrix at the last layer with some non-linear and non-element-wise output activations such as softmax and show that it can still be computed in linear computational complexity. We show the derivation for Hessian diagonals for fully connected networks in the following and provide the derivation for the convolutional neural networks in Appendix B.\n\nWe use the supervised classification setting where there is a collection of data examples. These data examples are generated from some target function f ∗ mapping the input x to the output y, where the k-th input-output pair is (xk, yk). In this task, the learner is required to predict the output class y ∈ {1, 2, ..., m} given the input vector x ∈ Rd by estimating the target function f ∗. The performance is measured with the cross-entropy loss, L(p, q) = − (cid:80)m i=1 pi log qi, where p ∈ Rm is the vector of the target one-hot encoded class and q ∈ Rm is the predicted output. The learner is required to reduce the cross-entropy by matching the target class.\n\nConsider a neural network with L layers that outputs the predicted output q. The neural network is parametrized by the set of weights {W1, ..., WL}, where Wl is the weight matrix at the l-th layer, and its element at the ith row and the jth column is denoted by Wl,i,j. During learning, the parameters of the neural network are changed to reduce the loss. At each layer l, we get the activation output hl by applying the activation function σ to the activation input al: hl = σ(al). .\nWe simplify notations by defining h0 = x. The activation output hl is then multiplied by the weight matrix Wl+1 of layer l + 1 to produce the next activation input: al+1,i = (cid:80)|hl| j=1 Wl+1,i,jhl,j. We assume here that the activation function is element-wise activation for all layers except for the final layer L, where it becomes the softmax function. The backpropagation equations for the described network are given as follows Rumelhart et al. (1986):\n\n∂L ∂al,i\n\n∂L ∂Wl,i,j\n\n=\n\n=\n\n|al+1| (cid:88)\n\nk=1 ∂L ∂al,i\n\n∂L ∂al+1,k\n\n∂al+1,k ∂hl,i\n\n∂hl,i ∂al,i\n\n= σ′(al,i)\n\n|al+1| (cid:88)\n\nk=1\n\n∂L ∂al+1,k\n\nWl+1,k,i,\n\n∂al,i ∂Wl,i,j\n\n=\n\n∂L ∂al,i\n\nhl−1,j.\n\n(1)\n\n(2)\n\nIn the following, we write the equations for the exact Hessian diagonals with respect to weights ∂2L/∂W 2\n\nl,i first:\n\nl,i,j, which requires the calculation of ∂2L/∂a2 \n\n∂2L ∂a2 l,i\n\n=\n\n∂ ∂al,i\n\n σ′(al,i)\n\n|al+1| (cid:88)\n\nk=1\n\n∂L ∂al+1,k\n\nWl+1,k,i\n\n\n\n= σ′(al,i)\n\n|al+1| (cid:88)\n\n|al+1| (cid:88)\n\nk=1\n\np=1\n\n∂2L ∂al+1,k∂al+1,p\n\n∂al+1,p ∂al,i\n\nWl+1,k,i + σ′′(al,i)\n\n|al+1| (cid:88)\n\nk=1\n\n∂L ∂al+1,k\n\nWl+1,k,i\n\n= σ′(al,i)2\n\n|al+1| (cid:88)\n\n|al+1| (cid:88)\n\nk=1\n\np=1\n\n∂2L ∂al+1,k∂al+1,p\n\nWl+1,p,iWl+1,k,i + σ′′(al,i)\n\n|al+1| (cid:88)\n\nk=1\n\n∂L ∂al+1,k\n\nWl+1,k,i,\n\n∂2L ∂W 2\n\nl,i,j\n\n=\n\n∂ ∂Wl,i,j\n\n(cid:18) ∂L ∂al,i\n\n(cid:19)\n\nhl−1,j\n\n=\n\n∂ ∂al,i\n\n(cid:18) ∂L ∂al,i\n\n(cid:19) ∂al,i ∂Wl,i,j\n\nhl−1,j =\n\n∂2L ∂a2 l,i\n\nh2\n\nl−1,j.\n\n(3)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nSince, the calculation of ∂2L/∂a2 l,i depends on the off-diagonal terms, the computation complexity becomes quadratic. Following Becker and Lecun (1989), we approximate the Hessian diagonals by ignoring the off-diagonal terms, which leads to a backpropagation rule with linear computational\n\ncomplexity for our estimates\n\n(cid:92)∂2L ∂W 2\n\nl,i,j\n\nand (cid:100)∂2L ∂a2\n\nl,i\n\n:\n\n(cid:100)∂2L ∂a2 l,i\n\n. = σ′(al,i)2\n\n|al+1| (cid:88)\n\nk=1\n\n(cid:92)∂2L ∂a2\n\nl+1,k\n\nW 2\n\nl+1,k,i + σ′′(al,i)\n\n|al+1| (cid:88)\n\nk=1\n\n∂L ∂al+1,k\n\nWl+1,k,i,\n\n(cid:92)∂2L ∂W 2\n\nl,i,j\n\n. =\n\n(cid:100)∂2L ∂a2 l,i\n\nh2\n\nl−1,j.\n\n(4)\n\n(5)\n\nHowever, for the last layer, we use the exact Hessian diagonals\n\n(cid:91)∂2L ∂a2\n\nL,i\n\n. = ∂2L ∂a2\n\nL,i\n\nsince it can be\n\ncomputed in O(n) for the softmax activation function and the cross-entropy loss. More precisely, the exact Hessian diagonals for cross-entropy loss with softmax is simply q − q ◦ q, where q is the predicted probability vector and ◦ denotes element-wise multiplication. We found empirically that this small change makes a large difference in the approximation quality, as shown in Fig. 1a. Hence, unlike Becker and Lecun (1989) who use a Hessian diagonal approximation of the last layer by Eq. 4, we use the exact values directly to achieve more approximation accuracy. We call this method for Hessian diagonal approximation HesScale and provide its pseudocode for supervised classification in Algorithm 1. HesScale is not specific to cross-entropy loss as the exact Hessian diagonals can\n\nAlgorithm 1 HesScale: Computing Hessian diagonals of a neural network layer in classification\n\nRequire: Neural network f and a layer number l\n\nRequire: First and second order information\n\nRequire: Input-output pair (x, y)\n\n(cid:92)∂L ∂al+1,i\n\nand\n\n(cid:92)∂2L ∂a2\n\nl+1,i,j\n\n, unless l = L\n\nSet loss function L to cross-entropy loss Compute preference vector aL ← f (x) and target one-hot-encoded vector p ← onehot(y) Compute the predicted probability vector q ← σ(aL) using softmax function σ Compute the error L(p, q) if l = L then\n\n▷ Computing Hessian diagonals exactly for the last layer\n\nCompute ∂L ∂aL Compute ∂L ∂WL\n\n← q − p\n\nusing Eq. 2\n\nusing Eq. 5\n\n(cid:100)∂2L ∂a2 L\n\nCompute\n\n← q − q ◦ q (cid:91)∂2L ∂W 2 L\nelse if l ̸= L then Compute ∂L ∂al Compute (cid:100)∂2L ∂a2 l\n\nand\n\nend if return ∂L ∂Wl\n\n,\n\n(cid:91)∂2L ∂W 2 l\n\n, ∂L ∂al\n\n, and (cid:100)∂2L ∂a2 l\n\nand ∂L/∂Wl using Eq. 1 and Eq. 2\n\n(cid:91)∂2L ∂W 2 l\n\nusing Eq. 4 and Eq. 5\n\n▷ ∂L ∂aL\n\n▷ ∂L ∂WL ▷ (cid:100)∂2L ∂a2 L\n(cid:91)∂2L ∂W 2 L\n\n▷\n\nconsists of elements\n\nconsists of elements ∂L ∂aL,i ∂L ∂WL,i,j (cid:91)∂2L ∂a2 (cid:92)∂2L ∂W 2\n\nconsists of elements\n\nconsists of elements\n\nL,i\n\nL,i,j\n\nbe calculated in O(n) for some other widely used loss functions as well. We show this property for negative log-likelihood function with Gaussian and softmax distributions in Appendix A. The computations can be reduced further using a linear approximation for the activation functions (by dropping the second term in Eq. 4), which corresponds to an approximation of the GGN matrix. We call this variation of our method HesScaleGN.\n\nBased on HesScale, we make a stable optimizer, which we call AdaHesScale, given in Algorithm 2. We use the same style introduced in Adam (Kingma & Ba 2015), using the squared diagonal approximation instead of the squared gradients to update the moving average. Moreover, we introduce another optimizer based on HesScaleGN, which we call AdaHesScaleGN. We refer the reader to the convergence proof for methods with Hessian diagonals, which was presented by Yao et al. (2021).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 AdaHesScale for optimization\n\nRequire: Neural network f with weights {W1, ..., WL} and a dataset D Require: Small number ε ← 10−8 Require: Exponential decay rates β1, β2 ∈ [0, 1) Require: step size α Require: Initialize {W1, ..., WL}\n\nInitialize time step t ← 0. for l in {L, L − 1, ..., 1} do Ml ← 0; Vl ← 0\n\nend for for (x, y) in D do t ← t + 1 rL+1 ← sL+1 ← ∅ for l in {L, L − 1, ..., 1} do\n\n▷ Set exponential moving averages at time step 0 to zero ▷ Same size as Wl\n\n▷ rl and sl stand for ∂L ∂al\n\nand (cid:100)∂2L ∂a2 l\n\n, respectively\n\nFl, Sl, rl, sl ← HesScale(f, x, y, l, rl+1, sl+1). Ml ← β1Ml + (1 − β1)Fl\n\nl\n\nVl ← β2Vl + (1 − β2)S2 ˆMl ← Ml/(1 − βt 1) ˆVl ← Vl/(1 − βt 2) Wl ← Wl − α ˆMl ⊘ ( ˆVl + ε)◦ 1\n\n2\n\n▷ Sl stands for\n\n▷ Check Algorithm 1 ▷ Fl stands for ∂L ∂Wl (cid:91)∂2L ∂W 2 l\n▷ Bias-corrected estimate for Fl ▷ Bias-corrected estimate for Sl ▷ ⊘ is element-wise division 2 is element-wise square root of A\n\n▷ A◦ 1\n\nend for\n\nend for\n\n4 APPROXIMATION QUALITY & SCALABILITY OF HESSCALE\n\nIn this section, we evaluate HesScale for its approximation quality and computational cost and compare it with other methods. These measures constitute the criteria we look for in scalable and efficient methods. For our experiments, we implemented HesScale using the BackPack framework (Dangel et al. 2020), which allows easy implementation of backpropagation of statistics other than the gradient.\n\nWe start by studying the approximation quality of Hessian diagonals compared to the true values. To measure the approximation quality of the Hessian diagonals for different methods, we use the L1 distance between the exact Hessian diagonals and their approximations. Our task here is supervised classification, and data examples are randomly generated. We used a network of three hidden layers with tanh activations, each containing 16 units. The network weights and biases are initialized randomly. The network has six inputs and ten outputs. For each example pair, we compute the exact Hessian diagonals for each layer and their approximations from each method. All layers’ errors are summed and averaged over 1000 data examples for each method. In this experiment, we used 40 different initializations for the network weights, shown as colored dots in Fig. 1a. Each point represents the summed error over network layers, averaged over 1000 examples for each different initialization. In this figure, we show the average error incurred by each method normalized by the average error incurred by HesScale. Any approximation that incurs an averaged error above 1 has a worse approximation than HesScale, and any approximation with an error less than 1 has a better approximation than HesScale. Moreover, we show the layer-wise error for each method in Fig. 1b.\n\nDifferent Hessian diagonal approximations are considered for comparison with HesScale. We included several deterministic and stochastic approximations for the Hessian diagonals. We also include the approximation of the Fisher Information Matrix done by squaring the gradients and denoted by g2, which is highly adopted by many first-order methods (e.g., Kingma and Ba, 2015). We compare HesScale with three stochastic approximation methods: AdaHessian (Yao et al. 2021), Kronecker-factored approximate curvature (KFAC) (Martens & Grosse 2015), and the Monte-Carlo (MC) estimate of the GGN matrix (GGN-MC) (Dangel et al. 2020). We also compare HesScale with two deterministic approximation methods: the diagonals of the exact GGN matrix (Schraudolph 2002) (diag(G)) and the diagonal approximation by Becker and Lecun (1989) (BL89). In\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Normalized L1 error with respect to HesScale\n\n(b) Layer-wise L1 error\n\nFigure 1: The averaged error for each method is normalized by the averaged error incurred by HesScale. We show 40 initialization points with the same colors across all methods. The norm of the vector of Hessian diagonals |diag(H)| is shown as a reference.\n\nKFAC, we extract the diagonals from the block diagonal matrix and show the approximation error averaged over 1 MC sample (KFAC-MC1) and over 50 MC samples (KFAC-MC50), both per each data example. Since AdaHessian and GGN-MC are already diagonal approximations, we use them directly and show the error with 1 MC sample (GGN-MC1 & AdaHessian-MC1) and with 50 MC samples (GGN-MC50 & AdaHessian-MC50). We refer the reader to Appendix C for an additional experiment with MNIST data points.\n\nHesScale provides a better approximation than the other deterministic and stochastic methods. For stochastic methods, we use many MC samples to improve their approximation. However, their approximation quality is still poor. Methods approximating the GGN diagonals do not capture the complete Hessian information since the GGN and Hessian matrices are different when the activation functions are not piece-wise linear. Although these methods approximate the GGN diagonals, their approximation is significantly better than the AdaHessian approximation. And among the methods for approximating the GGN diagonals, HesScaleGN performs the best and is close to the exact GGN diagonals. This experiment clearly shows that HesScale achieves the best approximation quality compared to other stochastic and deterministic approximation methods.\n\nNext, we perform another experiment to evaluate the computational cost of our optimizers. Our Hessian approximation methods and corresponding optimizers have linear computational complexity, which can be seen from Eq. 4 and Eq. 5. However, computing second-order information in optimizers still incurs extra computations compared to first-order optimizers, which may impact how the total computations scale with the number of parameters. Hence, we compare the computational cost of our optimizers with others for various numbers of parameters. More specifically, we measure the update time of each optimizer, which is the time needed to backpropagate first-order and second-order information and update the parameters.\n\nWe designed two experiments to study the computational cost of first-order and second-order optimizers. In the first experiment, we used a neural network with a single hidden layer. The network has 64 inputs and 512 hidden units with tanh activations. We study the increase in computational time when increasing the number of outputs exponentially, which roughly doubles the number of parameters. The set of values we use for the number of outputs is {24, 25, 26, 27, 28, 29}. The results of this experiment are shown in Fig. 2a. In the second experiment, we used a neural network with multi-layers, each containing 512 hidden units with tanh activations. The network has 64 inputs and 100 outputs. We study the increase in computational time when increasing the number of layers exponentially, which also roughly doubles the number of parameters. The set of values we use for the number of layers is {1, 2, 4, 8, 16, 32, 64, 128}. The results are shown in Fig. 2b. The points in Fig. 2a and Fig. 2b are averaged over 30 updates. The standard errors of the means of these points are smaller than the width of each line. On average, we notice that the cost of AdaHessian, AdaHesScale, and AdaHesScaleGN are three, two, and 1.25 times the cost of Adam, respectively.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nIt is clear that our methods are among the most computationally efficient approximation method for Hessian diagonals.\n\n(a) Increasing number of outputs in a neural network\n\n(b) Increasing number of layers in a neural network\n\nFigure 2: The average computation time for each step of an update is shown for different optimizers. The computed update time is the time needed by each optimizer to backpropagate gradients or second-order information and to update the parameters of the network. GGN overlaps with H in (a).\n\n5 EMPIRICAL PERFORMANCE OF HESSCALE IN OPTIMIZATION\n\nsection, we\n\nIn this and AdaHesScaleGN—with three second-order optimizers: BL89, GGNMC, and AdaHessian. We also include comparisons to two first-order methods: Adam and SGD. We exclude KFAC and the exact diagonals of the GGN matrix from our comparisons due to their prohibitive computations.\n\nthe performance of our optimizers—AdaHesScale\n\ncompare\n\nOur optimizers are evaluated in the supervised classification problem with a series of experiments using different architectures and three datasets: MNIST, CIFAR-10, and CIFAR-100. Instead of attempting to achieve state-of-the-art performance with specialized techniques and architectures, we follow the DeepOBS benchmarking work (Schneider et al. 2019) and compare the optimizers in their generic and pristine form using relatively simple networks. It allows us to perform a more fair comparison without extensively utilizing specialized knowledge for a particular task. In the first experiment, we use the MNIST-MLP task from DeepOBS. The images are flattened and used as inputs to a network of three fully connected layers (1000, 500, and 100 units) with tanh activations. We train each method for 100 epochs with a batch size of 128. We show the training plots in Fig. 7a with their corresponding sensitivity plots in Appendix D, Fig. 9a. In the second experiment, we use the CIFAR10-3C3D task from the DeepOBS benchmarking tasks. The network consists of three convolutional layers with tanh activations, each followed by max pooling. After that, two fully connected layers (512 and 256 units) with tanh activations are used. We train each method for 100 epochs with a batch size of 128. We show the training plots in Fig. 7b with their corresponding sensitivity plots in Fig. 9b. In the third experiment, we use the CIFAR100-3C-3D task from DeepOBS. The network is the same as the one used in the second task except for the activations are ELU. We train each method for 200 epochs with a batch size of 128. We show the training plots in Fig. 8b with their corresponding sensitivity plots in Fig. 10b. In the fourth experiment, we use the CIFAR100-ALLCNN task from DeepOBS with the ALL-CNN-C network, which consists of 9 convolutional layers (Springenberg et al. 2015) with ELU activations. We use tanh and ELU instead of ReLU, which is used in DeepOBS, to differentiate between the performance of AdaHesScale and AdaHesScaleGN. We show the training plots in Fig. 8a with their corresponding sensitivity plots in Fig. 10a.\n\nIn the MNIST-MLP and CIFAR-10-3C3D experiments, we performed a hyperparameter search for each method to determine the best set of β1, β2, and α. The range of β2 is {0.99, 0.999, 0.9999} and the range of β1 is {0.0, 0.9}. The range of step size is selected for each method to create a convex curve. Our criterion was to find the best hyperparameter configuration for each method in the search space that minimizes the area under the validation loss curve. The performance of each method was averaged over 30 independent runs. Each independent run had the same initial representation for the algorithms used in an experiment. Using each method’s best hyperparameter configuration on the validation set, we show the performance of each method against the time in seconds needed to complete the required number of epochs, which better depicts the computational efficiency of the\n\n7\n\n1718192021Number of parameters (2x)10−310−210−1100101102Time in secondsAdamSGDBL89AdaHessianGGNMCAdaHesScaleGNAdaHesScaleGGNKFACH19202122232425Number of parameters (2x)10−310−210−1100101102Time in secondsAdamSGDBL89AdaHessianGGNMCAdaHesScaleGNAdaHesScaleGGNKFACHUnder review as a conference paper at ICLR 2023\n\nmethods. Fig. 3a and Fig. 3b show these results on MNIST-MLP and CIFAR-10 tasks. Moreover, we show the sensitivity of each method to the step size in Fig. 5a and Fig. 5b.\n\n(a) MNIST-MLP\n\n(b) CIFAR-10 3C3D\n\nFigure 3: MNIST-MLP and CIFAR-10 3C3D classification tasks. Each method is trained for 100 epochs. We show the time taken by each algorithm in seconds (left) and we show the learning curves in the number of epochs (right). The performance of each method is averaged over 30 independent runs. The shaded area represents the standard error.\n\n(a) CIFAR-100 3C3D\n\n(b) CIFAR-100 All-CNN-C\n\nFigure 4: CIFAR-100 3C3D and CIFAR-100 ALL-CNN classification tasks. Each method from the first task is trained for 200 epochs and each method from the second task is trained for 350 epochs. We show the time taken by each algorithm in seconds (left) and we show the learning curves in the number of epochs (right). The performance of each method is averaged over 30 independent runs. The shaded area represents the standard error.\n\nIn the CIFAR-100-ALL-CNN and CIFAR-100-3C3D experiments, we used the set of β1 and β2 that achieved the best robustness in the previous two tasks, which were 0.9 and 0.999 respectively. We did a hyperparameter search for each method to determine the best step size using the specified β1 and β2. The rest of the experimental details are the same as the first two experiments. Using each method’s best hyperparameter configuration on the validation set, we show the performance of each method against the time in seconds needed to complete the required number of epochs. Fig. 4a and Fig. 4b show these results on CIFAR-100-ALL-CNN and CIFAR-100-3C3D tasks. We summarize the results in Appendix E.\n\nOur results show that all optimizers except for BL89 performed well on the MNIST-MLP task. However, in CIFAR-10, CIFAR-100 3c3d, and CIFAR-100 ALL-CNN, we notice that AdaHessian performed worse than all methods except BL89. This result is aligned with AdaHessian’s inability\n\n8\n\n05001000150020002500Time in seconds0.97500.97750.98000.98250.9850Test AccuracyGGNMCAdaHesScaleAdaHesScaleGNSGDBL89AdaHessianAdam020406080100Epochs0.9700.9750.9800.98501000200030004000500060007000Time in seconds0.650.700.750.80Test AccuracyGGNMCAdaHesScaleAdaHesScaleGNSGDBL89AdaHessianAdam020406080100Epochs0.650.700.750.8002000400060008000100001200014000Time in seconds0.350.400.450.50Test AccuracyGGNMCAdaHesScaleAdaHesScaleGNSGDAdaHessianAdam0255075100125150175200Epochs0.350.400.450.500500010000150002000025000Time in seconds0.450.500.55Test AccuracyGGNMCAdaHesScaleAdaHesScaleGNSGDAdaHessianAdam050100150200250300350Epochs0.450.500.55Under review as a conference paper at ICLR 2023\n\n(a) MNIST\n\n(b) CIFAR-10\n\nFigure 5: Sensitivity of the step size for each method on MNIST-MLP and CIFAR-10 3C3D tasks. We select the best values of β1 and β2 for each step size α.\n\nto accurately approximate the Hessian diagonals, as shown in Fig. 1. Moreover, AdaHessian required more computational time compared to all methods, which is also reflected in Fig. 2. While being time-efficient, AdaHesScaleGN consistently outperformed all methods in CIFAR-10-3C3D and CIFAR-100-3C3D, and it outperformed all methods except AdaHesScale in CIFAR-100 ALLCNN. This result is aligned with our methods’ accurate approximation of Hessian diagonals. Our experiments indicate that incorporating HesScale and HesScaleGN approximations in optimization methods can be of significant performance advantage in both computation and accuracy. AdaHesScale and AdaHesScaleGN outperformed other optimizers likely due to their accurate approximation of the diagonals of the Hessian and GGN, respectively.\n\n6 CONCLUSION\n\nHesScale is a scalable and efficient second-order method for approximating the diagonals of the Hessian at every network layer. Our work is based on the previous work done by Becker and Lecun (1989). We performed a series of experiments to evaluate HesScale against other scalable algorithms in terms of computational cost and approximation accuracy. Moreover, we demonstrated how HesScale can be used to build efficient second-order optimization methods. Our results showed that our methods provide a more accurate approximation and require small additional computations.\n\n7 BROADER IMPACT\n\nSecond-order information is used in domains other than optimization. For example, some works alleviating catastrophic forgetting use a utility measure for the network’s connections to protect them. Typically, an auxiliary loss is used between such connections, and their old values are weighted by their corresponding importance. Such methods (LeCun et al. 1990, Hassibi & Stork 1993, Dong et al. 2017, Kirkpatrick et al. 2017, Schwarz et al. 2018, Ritter et al. 2018) use the diagonal of the Fisher information matrix or the Hessian matrix as a utility measure for each weight. The quality of these algorithms depends heavily on the approximation quality of the second-order approximation. Second-order information can also be used in neural network pruning. Molchanov et al. (2019) showed that second-order approximation with the exact Hessian diagonals could closely represent the true measure of the utility of each weight.\n\nThe accurate and efficient approximation for the diagonals of the Hessian at each layer enables HesScale to be used in many important lines of research. Using this second-order information provides a reliable measure of connection utility. Therefore, using HesScale in these types of problems can potentially improve the performance of neural network pruning methods and regularization-based catastrophic forgetting prevention methods.\n\n9\n\n10−910−710−510−310−1α0.00.51.01.52.02.5Test LossGGNMCAdaHesScaleAdaHesScaleGNSGDBL89AdaHessianAdam10−810−610−410−2100α1.01.52.02.53.03.5Test LossGGNMCAdaHesScaleAdaHesScaleGNSGDBL89AdaHessianAdamUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAmari, S. (1998). Natural Gradient Works Efficiently in Learning. Neural Computation, 10(2),\n\n251–276.\n\nBa, J. L., Kiros, J. R., & Hinton, G. E. (2016).\n\nLayer normalization.\n\narXiv preprint\n\narXiv:1607.06450.\n\nBecker, S. & Lecun, Y. (1989).\n\nImproving the convergence of back-propagation learning with second-order methods. Proceedings of the 1988 Connectionist Models Summer School (pp. 29–37).\n\nBekas, C., Kokiopoulou, E., & Saad, Y. (2007). An estimator for the diagonal of a matrix. Applied\n\nNumerical Mathematics, 57(11), 1214–1229.\n\nBotev, A., Ritter, H., & Barber, D. (2017). Practical Gauss-Newton optimisation for deep learning.\n\nProceedings of the 34th International Conference on Machine Learning, 70, 557–565.\n\nChan, A., Silva, H., Lim, S., Kozuno, T., Mahmood, A. R., & White, M. (2022). Greedification operators for policy optimization: Investigating forward and reverse KL divergences. Journal of Machine Learning Research, 23(253), 1-79.\n\nChapelle, O. & Erhan, D. (2011). Improved preconditioner for hessian free optimization. NIPS\n\nWorkshop on Deep Learning and Unsupervised Feature Learning.\n\nDangel, F., Kunstner, F., & Hennig, P. (2020). BackPACK: Packing more into backprop. Interna-\n\ntional Conference on Learning Representations.\n\nDong, X., Chen, S., & Pan, S. J. (2017). Learning to prune deep neural networks via layer-wise optimal brain surgeon. Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 4860-4874).\n\nDuchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and\n\nstochastic optimization. Journal of Machine Learning Research, 12(61), 2121–2159.\n\nHassibi, B. & Stork, D. (1993). Second order derivatives for network pruning: Optimal brain\n\nsurgeon. Advances in Neural Information Processing Systems, 5.\n\nIoffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. International conference on machine learning (pp. 448-456).\n\nKingma, D. P. & Ba, J. (2015). Adam: A method for stochastic optimization.\n\nInternational\n\nConference on Learning Representations.\n\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13), 3521–3526.\n\nKunstner, F., Hennig, P., & Balles, L. (2019). Limitations of the empirical fisher approximation for\n\nnatural gradient descent. Advances in Neural Information Processing Systems, 32.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nLeCun, Y., Denker, J., & Solla, S. (1990). Optimal brain damage. Advances in Neural Information\n\nProcessing Systems, 2.\n\nLuo, L., Xiong, Y., & Liu, Y. (2019). Adaptive gradient methods with dynamic bound of learning\n\nrate. International Conference on Learning Representations.\n\nMartens, J. (2010). Deep learning via hessian-free optimization.\n\nInternational Conference on\n\nMachine Learning (pp. 735–742).\n\nMartens, J. (2020). New insights & perspectives on the natural gradient method. Journal of\n\nMachine Learning Research, 21(146), 1-76.\n\nMartens, J. & Grosse, R. (2015). Optimizing neural networks with kronecker-factored approximate\n\ncurvature. International Conference on Machine Learning (pp. 2408–2417).\n\nMartens, J. & Sutskever, I. (2011). Learning recurrent neural networks with hessian-free optimiza-\n\ntion. International Conference on Machine Learning (pp. 1033-1040).\n\nMartens, J., Sutskever, I., & Swersky, K. (2012). Estimating the hessian by back-propagating International Conference on International Conference on Machine Learning(pp.\n\ncurvature. 963–970).\n\nMizutani, E. & Dreyfus, S. E. (2008). Second-order stagewise backpropagation for hessian-matrix\n\nanalyses & investigation of negative curvature. Neural Networks, 21(2), 193–203.\n\nMolchanov, P., Mallya, A., Tyree, S., Frosio, I., & Kautz, J. (2019). Importance estimation for IEEE/CVF Conference on Computer Vision and Pattern Recognition\n\nneural network pruning. (pp. 11264–11272).\n\nReddi, S. J., Kale, S., & Kumar, S. (2018). On the convergence of Adam and beyond. International\n\nConference on Learning Representations.\n\nRitter, H., Botev, A., and Barber, D. (2018). Online structured Laplace approximations for overcoming catastrophic forgetting. International Conference on Neural Information Processing Systems (pp. 3742–3752).\n\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-\n\npropagating errors. Nature, 323(6088), 533–536.\n\nSalimans, T. and Kingma, D. P. (2016). Weight normalization: A simple reparameterization to accelerate training of deep neural networks. Advances in neural information processing systems, 29, 901–909.\n\nSchneider, F., Balles, L., and Hennig, P. (2019). DeepOBS: A deep learning optimizer benchmark\n\nsuite. International Conference on Learning Representations.\n\nSchraudolph, N. N. (2002). Fast curvature matrix-vector products for second-order gradient\n\ndescent. Neural Computation, 14(7), 1723–1738.\n\nSchwarz, J., Czarnecki, W., Luketina, J., Grabska-Barwinska, A., Teh, Y. W., Pascanu, R., and Hadsell, R. (2018). Progress & compress: A scalable framework for continual learning.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nInternational Conference on Machine Learning (pp. 4528–4537).\n\nSpringenberg, J., Dosovitskiy, A., Brox, T., and Riedmiller, M. (2015). Striving for simplicity: The\n\nall convolutional net. International Conference on Learning Representations [Workshop].\n\nTieleman, T., Hinton, G., et al. (2012). Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 26–31.\n\nTran, P. T. and Phong, L. T. (2019). On the convergence proof of AMSGrad and a new version.\n\nIEEE Access, 7, 61706–61716.\n\nWilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. (2017). The marginal value of adaptive gradient methods in machine learning. International Conference on Neural Information Processing Systems (pp. 4151–4161).\n\nYao, Z., Gholami, A., Shen, S., Keutzer, K., and Mahoney, M. W. (2021). Adahessian: An adaptive second order optimizer for machine learning. AAAI Conference on Artificial Intelligence, 35(12), 10665-10673.\n\nZeiler, M. D. (2012). Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA HESSIAN DIAGONALS OF THE LOG-LIKELIHOOD FUNCTION FOR TWO\n\nCOMMON DISTRIBUTIONS\n\nHere, we provide the diagonals of the Hessian matrix of functions involving the log-likelihood of two common distributions: a normal distribution and a categorical distribution with probabilities represented by a softmax function, which we refer to as a softmax distribution. We show that the exact computations of the diagonal can be computed with linear complexity since computing the diagonal elements does not depend on off-diagonals in these cases. In the following, we consider the softmax and normal distributions, and we write the exact Hessian diagonals in both cases.\n\nA.1 SOFTMAX DISTRIBUTION\n\ni=1 pi log qi(θ), Consider a cross-entropy function for a discrete probability distribution as f where q is a probability vector that depends on a parameter vector θ, and p is a one-hot vector for the i=1 eθi. target class. For softmax distributions, q is parametrized by a softmax function q In this case, we can write the gradient of the cross-entropy function with respect to θ as\n\n. = eθ/ (cid:80)|q|\n\n. = − (cid:80)|q|\n\n∇θf (θ) = q − p.\n\nNext, we write the exact diagonal elements of the Hessian matrix as follows:\n\ndiag(Hθ) = diag(∇θ(q − p)) = q − q2,\n\nwhere q2 denotes element-wise squaring of q, and ∇ operator applied to a vector denotes Jacobian. Computing the exact diagonals of the Hessian matrix depends only on vector operations, which means that we can compute it in O(n). The cross-entropy loss is used with softmax distribution in many important tasks, such as supervised classification and discrete reinforcement learning control with parameterized policies (Chan et al. 2022).\n\nA.2 MULTIVARIATE NORMAL DISTRIBUTION WITH DIAGONAL COVARIANCE\n\nFor a multivariate normal distribution with diagonal covariance, the parameter vector θ is determined .\n= (μ, σ2). The log-likelihood of a random vector x drawn from by the mean-variance vector pair: θ this distribution can be written as\n\nlog q(x; μ, σ2) = −\n\n= −\n\n1 2\n\n1 2\n\n(x − μ)⊤D(σ2)−1(x − μ) −\n\n(x − μ)⊤D(σ2)−1(x − μ) −\n\n1 2\n\n1 2\n\nlog(|D(σ2)|) + c\n\n|σ| (cid:88)\n\nlog(\n\ni=1\n\nσ2\n\ni ) + c,\n\nwhere D(σ2) gives a diagonal matrix with σ2 in its diagonal, |M | is the determinant of a matrix M and c is some constant. We can write the gradients of the log-likelihood function with respect to μ and σ2 as follows:\n\n∇μ log q(x; μ, σ2) = D(σ2)−1(x − μ) = (x − μ) ⊘ σ2,\n\n∇σ2 log q(x; μ, σ2) =\n\n1 2\n\n(cid:2)(x − μ)2 ⊘ σ2 − 1(cid:3) ⊘ σ2,\n\nwhere 1 is an all-ones vector, and ⊘ denotes element-wise division. Finally, we write the exact diagonals of the Hessian matrix as\n\ndiag(Hμ) = diag(∇μ(x − μ) ⊘ σ2) = −1 ⊘ σ2,\n\ndiag(Hσ2 ) = diag\n\n(cid:16)\n\n∇σ2\n\n(cid:2) 1 2\n\n[(x − μ)2 ⊘ σ2 − 1] ⊘ σ2(cid:3)(cid:17)\n\n= (cid:2)0.51 − (x − μ)2 ⊘ σ2(cid:3) ⊘ σ4.\n\nClearly, the gradient and the exact Hessian diagonals can be computed in O(n). Log-likelihood functions for normal distributions are used in many important problems, such as variational inference and continuous reinforcement learning control.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB HESSCALE WITH CONVOLUTIONAL NEURAL NETWORKS\n\nHere, we derive the Hessian propagation for convolutional neural networks (CNNs). Consider a CNN with L − 1 layers followed by a fully connected layer that outputs the predicted output q. The CNN filters are parameterized by {W1, ..., WL}, where Wl is the filter matrix at the l-th layer with the dimensions kl,1 × kl,2, and its element at the ith row and the jth column is denoted by Wl,i,j. For the simplicity of this proof, we assume that the number of filters at each layer is one; the proof can be extended easily to the general case. The learning algorithm learns the target function f ∗ by optimizing the loss L. During learning, the parameters of the neural network are changed to reduce the loss. At the layer l, we get the activation output matrix Hl by applying the activation function σ to the activation input Al: Hl = σ(Al). We assume here that the activation function is element-wise activation for all layers except for the final layer L, where it becomes the softmax .\nfunction. We simplify notations by defining H0 = X, where X is the input sample. The activation output Hl is then convoluted by the weight matrix Wl+1 of layer l + 1 to produce the next activation input: Al+1,i,j = (cid:80)kl,1−1 n=0 Wl+1,m,nHl,(i+m),(j+n). We denote the size of the activation output at the l-th layer by hl × wl. The backpropagation equations for the described network are given following Rumelhart et al. (1986):\n\n(cid:80)kl,2−1\n\nm=0\n\n∂L ∂Al,i,j\n\nkl+1,1−1 (cid:88)\n\nkl+1,2−1 (cid:88)\n\nm=0\n\nn=0\n\nkl+1,1−1 (cid:88)\n\nkl+1,2−1 (cid:88)\n\nm=0\n\nn=0\n\nkl+1,1−1 (cid:88)\n\nkl+1,2−1 (cid:88)\n\n=\n\n=\n\n=\n\nm=0\n\nn=0\n\n∂L ∂Al+1,(i−m),(j−n)\n\n∂Al+1,(i−m),(j−n) ∂Al,i,j\n\n∂L ∂Al+1,(i−m),(j−n)\n\nkl+1,1−1 (cid:88)\n\nkl+1,2−1 (cid:88)\n\nm′=0\n\nn′=0\n\nWl+1,m′,n′\n\n∂Hl,(i−m+m′),(j−n+n′) ∂Al,i,j\n\n∂L ∂Al+1,(i−m),(j−n)\n\nWl+1,m,nσ′(Al,i,j)\n\n= σ′(Al,i,j)\n\nkl+1,1−1 (cid:88)\n\nkl+1,2−1 (cid:88)\n\nm=0\n\nn=0\n\n∂L ∂Al+1,(i−m),(j−n)\n\nWl+1,m,n,\n\n(6)\n\n∂L ∂Wl,i,j\n\n=\n\nhl−kl,1 (cid:88)\n\nwl−kl,2 (cid:88)\n\nm=0\n\nn=0\n\n∂L ∂Al,m,n\n\n∂Al,m,n ∂Wl,i,j\n\n=\n\nhl−kl,1 (cid:88)\n\nwl−kl,2 (cid:88)\n\nm=0\n\nn=0\n\n∂L ∂Al,m,n\n\nHl−1,(i+m),(j+n).\n\n(7)\n\nIn the following, we write the equations for the exact Hessian diagonals with respect to weights ∂2L/∂W 2\n\nl,i,j, which requires the calculation of ∂2L/∂A2\n\nl,i,j first:\n\n∂2L\n\n∂A2\n\nl,i,j\n\n=\n\n∂ ∂Al,i,j\n\n(cid:34)\n\nσ′(Al,i,j)\n\nkl+1,1−1 (cid:88)\n\nkl+1,2−1 (cid:88)\n\nm=0\n\nn=0\n\n∂L ∂Al+1,(i−m),(j−n)\n\nWl+1,m,n\n\n(cid:35)\n\n= σ′(Al,i,j)\n\n+ σ′′(Al,i,j)\n\nkl+1,2−1 (cid:88)\n\nkl+1,2−1 (cid:88)\n\nm,p=0\n\nn,q=0\n\nkl+1,2−1 (cid:88)\n\nkl+1,2−1 (cid:88)\n\nm=0\n\nn=0\n\n∂2L ∂Al+1,(i−m),(j−n)∂Al+1,(i−p),(j−q)\n\n∂Al+1,(i−p),(j−q) ∂Al,i,j\n\nWl+1,m,n\n\n∂L ∂Al+1,(i−m),(j−n)\n\nWl+1,m,n\n\n∂2L ∂W 2\n\nl,i,j\n\n=\n\n=\n\n∂ ∂Wl,i,j\n\n(cid:34) hl−kl,1 (cid:88)\n\nwl−kl,2 (cid:88)\n\nm=0\n\nn=0\n\n∂L ∂Al,m,n\n\n(cid:35)\n\nHl−1,(i+m),(j+n)\n\nhl−kl,1 (cid:88)\n\nwl−kl,2 (cid:88)\n\nm,p=0\n\nn,q=0\n\n∂2L ∂Al,m,n∂Al,p,q\n\n∂Al,p,q ∂Wl,i,j\n\nHl−1,(i+m),(j+n)\n\nSince the calculation of ∂2L/∂A2 l,i,j depend on the off-diagonal terms, the computation complexity becomes quadratic. Following Becker and Lecun (1989), we approximate the\n\nl,i,j and ∂2L/∂W 2\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nHessian diagonals by ignoring the off-diagonal terms, which leads to a backpropagation rule with\n\nlinear computational complexity for our estimates\n\n(cid:92)∂2L ∂W 2\n\nl,i,j\n\nand\n\n(cid:92)∂2L ∂A2\n\nl,i,j\n\n:\n\n(cid:92)∂2L ∂A2\n\nl,i,j\n\n. = σ′(Al,i,j)2\n\nkl+1,2−1 (cid:88)\n\nkl+1,2−1 (cid:88)\n\n(cid:92)∂2L\n\n+ σ′′(Al,i,j)\n\nm=0\n\nn=0\n\nkl+1,2−1 (cid:88)\n\nkl+1,2−1 (cid:88)\n\nm=0\n\nn=0\n\n∂A2\n\nl+1,(i−m),(j−n)\n\nW 2\n\nl+1,m,n\n\n∂L ∂Al+1,(i−m),(j−n)\n\nWl+1,m,n,\n\n(cid:92)∂2L ∂W 2\n\nl,i,j\n\n. =\n\nhl−kl,1 (cid:88)\n\nwl−kl,2 (cid:88)\n\nm=0\n\nn=0\n\n(cid:92)∂2L ∂A2\n\nl,m,n\n\nH 2\n\nl−1,(i+m),(j+n).\n\nC APPROXIMATION QUALITY WITH MNIST DATA\n\n(8)\n\n(9)\n\nWe repeat the experiment shown in Fig. 1 with MNIST data points instead of random data points. The experimental details are the same except for two changes. First, we used a larger network where we changed the number of units in each hidden layer to 32 instead of 16. Second, we performed an optimization update with SGD at each data point. The results shown in Fig. 6 are similar to the results shown in Fig. 1 where HesScale gives a better approximation quality than other methods. This experiment shows that our results hold for realistic settings where learning is involved.\n\n(a) Normalized L1 error with respect to HesScale\n\n(b) Layer-wise L1 error\n\nFigure 6: The averaged error for each method is normalized by the averaged error incurred by HesScale for data points coming from MNIST. We show 40 initialization points with the same colors across all methods. The norm of the vector of Hessian diagonals |diag(H)| is shown as a reference.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nD OPTIMIZATION PLOTS IN THE NUMBER OF EPOCHS\n\nWe give the training loss, training accuracy, validation loss, validation accuracy, test loss, and test accuracy for each of the methods we include in our comparison in Fig. 7 and Fig. 8. Moreover, we give the sensitivity plots for β1, β2, and α for each method in Fig. 9 and Fig. 10.\n\n(a) MNIST\n\n(b) CIFAR-10\n\nFigure 7: Learning curves of each algorithm on two tasks, MNIST-MLP and CIFAR-10 3C3D, for 100 epochs. We show the best configuration for each algorithm on the validation set. The best parameter configuration for each algorithm is selected based on the area under the curve for the validation loss.\n\n16\n\n0.00.10.20.30.40.5Test LossGGNMCAdaHesScaleAdaHesScaleGNSGDBL89AdaHessianAdam0.00.10.20.30.40.5Train Loss0.940.950.960.970.98Test Accuracy020406080100Epochs0.940.950.960.970.980.991.00Train Accuracy1.01.52.02.53.03.54.04.55.0Test LossGGNMCAdaHesScaleAdaHesScaleGNSGDBL89AdaHessianAdam1.01.52.02.53.03.54.04.55.0Train Loss0.30.40.50.60.70.8Test Accuracy020406080100Epochs0.30.40.50.60.70.8Train AccuracyUnder review as a conference paper at ICLR 2023\n\n(a) CIFAR-100 All CNN\n\n(b) CIFAR-100 3C3D\n\nFigure 8: Learning Curves of each algorithm on CIFAR-100 with All-CNN and 3C3D architectures, for 100 epochs. We show the best configuration for each algorithm on the validation set. The best parameter configuration for each algorithm is selected based on the area under the curve for the validation loss.\n\n17\n\n2.22.42.62.83.03.23.4Test LossGGNMCAdaHesScaleAdaHesScaleGNSGDAdaHessianAdam1.01.52.02.53.03.5Train Loss0.3500.3750.4000.4250.4500.4750.5000.5250.550Test Accuracy050100150200250300350Epochs0.350.400.450.500.550.600.650.700.75Train Accuracy2.53.03.54.04.55.0Test LossGGNMCAdaHesScaleAdaHesScaleGNSGDAdaHessianAdam2.002.252.502.753.003.253.503.754.00Train Loss0.300.350.400.450.500.55Test Accuracy0255075100125150175200Epochs0.300.350.400.450.500.550.600.650.70Train AccuracyUnder review as a conference paper at ICLR 2023\n\n(a) MNIST\n\n(b) CIFAR-10\n\nFigure 9: Parameter Sensitivity study for each algorithm on two data sets, MNIST and CIFAR-10. The range of β2 is {0.99, 0.999, 0.9999} and the range of β1 is {0.0, 0.9}. Each point for each algorithm represents the average test loss given a set of parameters.\n\n18\n\n10−910−710−510−310−10.00.51.01.52.02.5β1=0.0β2=0.99GGNMCAdaHesScaleAdaHesScaleGNBL89AdaHessianAdam10−910−710−510−310−10.00.51.01.52.02.5β2=0.99910−910−710−510−310−10.00.51.01.52.02.5β2=0.999910−910−710−510−310−10.00.51.01.52.02.5β1=0.910−910−710−510−310−10.00.51.01.52.02.510−910−710−510−310−10.00.51.01.52.02.510−710−510−310−1α0.00.51.01.52.02.510−810−610−410−21234β1=0.0β2=0.99GGNMCAdaHesScaleAdaHesScaleGNBL89AdaHessianAdam10−810−610−410−21234β2=0.99910−810−610−410−21234β2=0.999910−810−610−410−21234β1=0.910−810−610−410−2123410−810−610−410−2123410−510−410−310−210−1100α1234Under review as a conference paper at ICLR 2023\n\n(a) CIFAR-100 All-CNN\n\n(b) CIFAR-100 3C3D\n\nFigure 10: Parameter Sensitivity study for each algorithm on CIFAR-100 with All-CNN and 3C3D architectures. The range of step size is {10−5, 10−4, 10−3, 10−2, 10−1, 100}. We choose β1 to be equal to 0.9 and β2 to be equal to 0.999. Each point for each algorithm represents the average test loss given a set of parameters.\n\nE SUMMARY OF OPTIMIZATION RESULTS\n\nWe summarize the final performance of AdaHesScale and AdaHesScaleGN against other optimizers on the train sets and test sets, in Table 1 and Table 2 respectively.\n\nTable 1: Performance of optimization methods on the train sets of different problems.\n\nMNIST CIFAR-10 CIFAR-100 3C3D CIFAR-100 ALL-CNN-C\n\nAdaHesScale\n\n99.68%\n\nAdaHesScaleGN 99.64%\n\nGGN-MC\n\nSGD\n\nAdam\n\nAdaHessian\n\nBL89\n\n99.64%\n\n99.65%\n\n99.67%\n\n99.71%\n\n96.99%\n\n78.19%\n\n82.11%\n\n78.92%\n\n77.93%\n\n80.24%\n\n74.94%\n\n33.89%\n\n57.29%\n\n65.39%\n\n64.62%\n\n61.37%\n\n58.36%\n\n46.92%\n\n-\n\n71.89%\n\n58.10%\n\n59.14%\n\n63.37%\n\n60.82%\n\n62.43%\n\n-\n\nTable 2: Performance of optimization methods on the test sets of different problems.\n\nMNIST CIFAR-10 CIFAR-100 3C3D CIFAR-100 ALL-CNN-C\n\nAdaHesScale\n\n98.17%\n\nAdaHesScaleGN 98.17%\n\nGGN-MC\n\nSGD\n\nAdam\n\nAdaHessian\n\nBL89\n\n97.99%\n\n98.02%\n\n98.10%\n\n98.23%\n\n95.13%\n\n77.98%\n\n79.79%\n\n77.93%\n\n77.28%\n\n79.07%\n\n73.91%\n\n36.74%\n\n49.13%\n\n49.99%\n\n49.41%\n\n47.65%\n\n47.30%\n\n45.10%\n\n-\n\n53.59%\n\n50.16%\n\n48.06%\n\n49.13%\n\n47.97%\n\n47.63%\n\n-\n\n19",
    "reference": "# Summary Of The Paper\n\nThe authors present a modification of a method of Becker and Lecun (1989) using a diagonal Hessian approximation to improve the convergence of stochastic gradient descent schemes.  The scheme takes about the same amount of time as standard back-propagation, unlike some other approximation schemes.  Based on this modified Hessian approximation, they introduce the AdaHessian method, which compares favorably to other standard optimizers on a set of test problems.\n\n# Strength And Weaknesses\n\nTo the extent that I can read the figures, the approximation scheme does indeed seem to lead to better convergence results than competitors on the test problems.  The modification compared to B&L 89 is not large, but the authors point out that it makes a big difference in the convergence of the final method.\n\nThere is no new theory behind this, or at least none given in the paper.  The verification of the quality is purely empirical.  It would be interesting if there was a theoretical argument for why the modification makes as much difference as it does.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe written presentation is quite clear.  There is not a lot of novelty here, but the improvement is clear; also (though the authors don't say much about it), the 1989 Becker-Lecun paper does not integrate with the Adam ideas, so there is a new combination of the Hessian approximation and modern SGD-style algorithms.\n\nAs a minor but key complaint: between the small fonts and the use of many colored lines, I found the plots very difficult to read.  This is surely made more difficult by my poor color vision, and I understand the constraints of a page limit -- but I would have been a much happier reader with something a little larger!\n\n# Summary Of The Review\n\nWith a small-but-critical change to a 1989 scaling technique, together with adopting modern SGD framework, the authors introduce a new SGD-style optimizer for NN training that includes second-order information \"for cheap\" and seems to lead to better training performance (in terms of test accuracy vs time) than natural competitors.  I am a fan of \"not a big change, but the right change\" work, and would like to see this published.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDYNAMIC UPDATE-TO-DATA RATIO: MINIMIZING WORLD MODEL OVERFITTING\n\nNicolai Dorka1 1University of Freiburg dorka@cs.uni-freiburg.de\n\nTim Welschehold1\n\nWolfram Burgard2 2University of Technology Nuremberg\n\nABSTRACT\n\nEarly stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving. As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on underand overfitting detection on a small subset of the continuously collected experience not used for training. We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari 100k benchmark. The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search which is not feasible for many applications. Our method eliminates the need to set the UTD hyperparameter by hand and even leads to a higher robustness with regard to other learning-related hyperparameters further reducing the amount of necessary tuning.\n\n1\n\nINTRODUCTION\n\nIn model-based reinforcement learning (RL) the agent learns a predictive world model to derive the policy for the given task through interaction with its environment. Previous work has shown that model-based approaches can achieve equal or even better results than their model-free counterparts Silver et al. (2018); Schrittwieser et al. (2020); Chua et al. (2018); Hafner et al. (2021). An additional advantage of using a world model is, that once it has been learned for one task, it can directly or after some finetuning be used for different tasks in the same environment potentially making the training of multiple skills for the agent considerably cheaper. Learning a world model is in principle a supervised learning problem. However, in contrast to the standard supervised learning setting, in model-based RL the dataset is not fixed and given at the beginning of training but is gathered over time through the interaction with the environment which raises additional challenges.\n\nA typical problem in supervised learning is overfitting on a limited amount of data. This is well studied and besides several kinds of regularizations a common solution is to use a validation set that is not used for training but for continual evaluation of the trained model during training. By considering the learning curve on the validation set it is easy to detect if the model is under- or overfitting the training data. For neural networks a typical behavior is that too few updates lead to underfitting while too many updates lead to overfitting. In this context, the validation loss is a great tool to balance those two and to achieve a small error on unseen data.\n\nFor learning a world model on a dynamic dataset there unfortunately is no established method to determine if the model is under- or overfitting the training data available at the given point in time. Additionally, in model-based RL a poorly fit model can have a dramatic effect onto the learning result as from it the agent derives the policy, which influences the future collected experience which again influences the learning of the world model. So far, in model-based RL this is commonly addressed with some form of regularization and by setting an update-to-data (UTD) ratio that specifies how many update steps the model does per newly collected experience, similar to selecting the total number of parameter updates in supervised learning. Analogously to supervised learning, a higher\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nUTD ratio is more prone to overfit the data and a lower one to underfit it. State-of-the-art methods set the UTD ratio at the beginning of the training and do not base the selection on a dynamic performance metric. Unfortunately, tuning this parameter is very costly as the complete training process has to be traversed several times. Furthermore, a fixed UTD ratio is often sub-optimal because different values for this parameter might be preferable at different stages of the training process.\n\nIn this paper, we propose a general method – called Dynamic Update-to-Data ratio (DUTD) – that adjusts the UTD ratio during training. DUTD is inspired by using early stopping to balance under- and overfitting. It stores a small portion of the collected experience in a separate validation buffer not used for training but instead used to track the development of the world models accuracy in order to detect under- and overfitting. Based on this, we then dynamically adjust the UTD ratio.\n\nWe evaluate DUTD applied to DreamerV2 Hafner et al. (2021) on the DeepMind Control Suite and the Atari100k benchmark. The results show that DUTD increases the overall performance relative to the default DreamerV2 configuration. Most importantly, DUTD makes searching for the best UTD rate obsolete and is competitive with the best value found through extensive hyperparameter tuning of DreamerV2. Further, our experiments show that with DUTD the world model becomes considerably more robust with respect to the choice of the learning rate.\n\nIn summary, this paper makes the following contributions: i) we introduce a method to detect under- and overfitting of the world model online by evaluating it on hold-out data; ii) We use this information to dynamically adjust the UTD ratio to optimize world model performance; iii) Our method makes tuning the UTD hyperparameter obsolete; iv) We exemplarily apply our method to a state-of-the-art model-based RL method and show that it leads to an improved overall performance and higher robustness compared to its default setting and reaches a competitive performance to an extensive hyperparameter search.\n\n2 RELATED WORK\n\n1\n\nFigure 1: Overview of DUTD. A small subset of the experience collected from the environment is stored in a validation set not used for training. The world model is trained for one update after every UTD ratio many environment steps. From time to time, e.g., after an episode ended, the UTD ratio is adjusted depending on the detection of under- or overfitting of the world model on the validation data. The policy is obtained from the world model either by planning or learning and collects new data in the environment.\n\nIn reinforcement learning there are two forms of generalization and overfitting. Inter-task overfitting describes overfitting to a specific environment such that performance on slightly different environments drops significantly. This appears in the context of sim-to-real, where the simulation is different from the target environment on which a well performing policy is desired, or when the environment changes slightly, for example, because of a different visual appearance Zhang et al. (2018b); Packer et al. (2018); Zhang et al. (2018a); Raileanu et al. (2020); Song et al. (2020). In contrast, intra-task overfitting appears in the context of learning from limited data in a fixed environment when the model fits the data too perfectly and generalizes poorly to new data. We consider intra-task opposed to inter-task generalization.\n\nIn model-based reinforcement learning, there is also the problem of policy overfitting on an inaccurate dynamics model Arumugam et al. (2018); Jiang et al. (2015). As a result, the policy optimizes over the inaccuracies of the model and finds exploits that do not work on the actual environment. One approach is to use uncertainty estimates coming from an ensemble of dynamics models to be more conservative when the estimated uncertainty is high Chua et al. (2018). Another approach to prevent the policy from exploiting the model is to use different kinds of regularization on the plans the policy considers Arumugam et al. (2018). In contrast to these previous works, we directly\n\n2\n\nEnvironmentTrainingDataWorldModelTrain every1UTD ratioEvaluateupdatePolicyValidationDataPublished as a conference paper at ICLR 2023\n\ntackle the source of the problem by learning a better dynamics model. Consequently, our method is orthogonal to and can easily be combined with the just mentioned line of work.\n\nDirectly targeting the overfitting of the dynamics model can be done through the usage of a Bayesian dynamics model and the uncertainties that come with such a model. Gaussian processes have been used successfully in this context Deisenroth & Rasmussen (2011) although it is difficult to scale this to high-dimensional problems. Another way to reduce overfitting of the dynamics model is to use techniques from supervised learning. This includes for example regularization of the weights, dropout Srivastava et al. (2014), or data augmentation Laskin et al. (2020); Schwarzer et al. (2021). All of these are also orthogonal to our method and can be combined with it to learn an even better dynamics model. Another popular approach is early stopping Strand (1974); Anderssen & Prenter (1981); Morgan & Bourlard (1989), where the training is stopped before the training loss converges. Our method can be regarded as the analogy of early stopping in a dynamic dataset scenario.\n\nReducing the number of model parameters can prevent overfitting but can decrease performance compared to the right amount of training steps with more parameters. Our method overcomes this problem by automatically choosing the right amount of training steps for a given network.\n\nHyperparameter optimization for RL algorithms is also related to our work. For example, AlphaStar Silver et al. (2018) has been improved by using Bayesian optimization Chen et al. (2018). Zhang et al. (2021) demonstrated that model-based RL algorithms can be greatly improved through automatic hyperparameter optimization. A recent overview on automated RL is given by Parker-Holder et al. (2022). However, most of these approaches improve hyperparameters by training the RL agent on the environment in an inner loop while keeping the hyperparameters fixed during each run. Our work deviates from that by adapting a hyperparameter online during training of a single run. The approach of Schaul et al. (2019) also falls into this category and dynamically adapts behaviorrelated parameters such as stochasticity and optimism. Similarly, the algorithm Agent57 Badia et al. (2020) adaptively chooses from a set of policies with different exploration strategies and achievs human level performance on all 57 Atari games Bellemare et al. (2013). Another approach adapts a hyperparameter that controls under- and overestimation of the value function online resulting in a model-free RL algorithm with strong performance on continuous control tasks Dorka et al. (2021).\n\nIn contrast to these approaches, our method directly learns a better world model by detecting underand overfitting online on a validation set and dynamically adjusts the number of update steps accordingly. This renders the need to tune the UTD ratio hyperparameter unnecessary and further allows to automatically have its value being adapted to the needs of the different training stages.\n\n3 THE DUTD ALGORITHM\n\nIn this section, we will first introduce the general setup, explain early stopping in the context of finding the right data fit and propose a new method that transfers this technique to the online learning setting. Lastly, we explain how the method can be applied to DreamerV2.\n\n3.1 MODEL-BASED REINFORCEMENT LEARNING\n\nWe use the classical RL framework Sutton & Barto (2018) assuming a Markov decision process (S, A, P, R). In this framework, the agent sequentially observes the current state st ∈ S in which it executes an action at ∈ A, receives a scalar reward rt according to the reward function R, and transitions to a next state st+1 generated by the unknown transition dynamics P. The goal is to learn a policy that selects actions in each state such that the total expected return (cid:80)T\n\ni=t ri is maximized. Model-based RL approaches learn a world model ˆP(st+1 | st, at) – also called dynamics model – and a reward model ˆR(rt | st) that attempt to reflect their real but unknown counterparts. These models can then be used to learn a good policy by differentiating through the world model or by generating imaginary rollouts on which an RL algorithm can be trained. Alternatively, the learned model can be used in a planning algorithm to select an action in the environment.\n\n3.2 UNDER- AND OVERFITTING\n\nA well-known problem in supervised learning is that of overfitting, which typically corresponds to a low error on the training data and a high error on test data not seen during training. Usually, this\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nhappens if the model fits the training data too perfectly. In contrast to this, underfitting corresponds to the situation in which the model even poorly fits the training data and is characterized by both a high training and test error. To measure the performance of the model on unseen data, the available data is often split into a training and a validation set. Generally, only the training set is used to train the model while the validation set is used to evaluate its performance on new data.\n\nFor iterative training methods – like gradient descent based methods – overfitting is often detected by observing the learning curves for training and validation error against the number of training steps. A typical behavior is that in the beginning of the training both training and validation loss are decreasing. This is the region where the model is still underfitting. At some point, when the model starts overfitting the training data, only the training loss decreases further while the validation loss starts to increase. The aforementioned early stopping method balances under- and overfitting by stopping the training once the validation loss starts to increase.\n\nWhile in supervised learning one can easily select a well fit model by using the validation loss, in reinforcement learning one cannot apply this technique as the dataset is not fixed but dynamic and is constantly growing or changing. Furthermore, the quality of the current policy influences the quality of the data collected in the future. Even though learning a world model is in principle a supervised task, this problem also occurs in the model-based RL framework.\n\n3.3 DYNAMIC UPDATE-TO-DATA RATIO\n\nA typical hyperparameter in many RL algorithms is the update-to-data (UTD) ratio which specifies the number of update steps performed per environment step (i.e., per new data point). This ratio can in principle be used to balance under- and overfitting as one can control it in a way that not too few or too many updates steps are done on the currently available data. However, several problems arise while optimizing this parameter. First, it is very costly to tune this parameter as it requires to run the complete RL training several times making it infeasible for many potential applications. Second, the assumption that one fixed value is the optimal choice during the entire training duration does not necessarily hold. For example, if data from a newly explored region of the environment is added to the replay buffer it might be beneficial to increase the number of update steps.\n\nTo address these problems, we propose – DUTD – a new method that dynamically adjusts the UTD ratio during training. It is inspired by the early stopping criterion and targets at automatically balancing under- and overfitting online by adjusting the number of update steps. As part of the method, we store some of the experience in a separate validation buffer not used for training. Precisely, every d environment steps we collect s consecutive transitions from a few separate episodes dedicated to validation and every k environment steps the world model is evaluated on the validation buffer, where k should be much smaller than d. As the world model learning task is supervised this is easily done by recording the loss of the world model on the given validation sequences. The current validation loss is then compared to the validation loss of the previous evaluation. If the loss has decreased, we assume the model is still in the underfitting regime and increase the UTD rate by a specified amount. If the loss has increased, we assume the model to be in an overfitting regime and hence reduce the UTD rate. To allow for a finer resolution at the high-update side of the allowed interval we adjust the UTD rate in log-space, meaning it is increased or decreased by multiplying it with a value of c or 1/c respectively, where c is slightly larger than 1. The update formula at time step t then becomes\n\nutd ratiot = utd ratiot−k · b;\n\nb =\n\n(cid:26)c, c ,\n\n1\n\nif validation losst < validation losst−k, if validation losst ≥ validation losst−k.\n\n(1)\n\nDUTD is a general method that can be applied to any model-based RL algorithm that learns a world model in a supervised way. The implementation can be either in terms of the UTD ratio or the datato-update ratio which is its inverse and which we call IUTD (i.e., the number of environment steps per update step). It is more convenient to use the UTD ratio if several updates are performed per environment step and the IUTD if an update step is only performed after some environment steps. Methodologically, the two settings are the same as the two ratios describe the same quantity and are just the inverse of each other.\n\nA high-level overview of DUTD is shown in Figure 1 and the pseudocode is described in Algorithm 1, both explained in terms of the IUTD ratio as we will apply DUTD to the DreamerV2\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nalgorithm Hafner et al. (2021) for which several update steps per environment step become computationally very costly. However, in both framework both scenarios can be addressed by letting the ratio be a fractional.\n\n3.4 APPLYING DUTD TO DREAMERV2\n\nWe apply DUTD to DreamerV2 Hafner et al. (2021), which is a model-based RL algorithm that builds on Dreamer Hafner et al. (2020) which again builds on PlaNet Hafner et al. (2019). DreamerV2 learns a world model through latent imagination. The policy is learned purely in the latent space of this world model through an actor-critic framework. It is trained on imaginary rollouts generated by the world model. The critic is regressed onto λ-targets Schulman et al. (2015); Sutton & Barto (2018) and the actor is trained by a combination of Reinforce Williams (1992) and a dynamics backpropagation loss. The world model learns an image encoder that maps the input to a categorical latent state on which a Recurrent State-Space Model Hafner et al. (2019) learns the dynamics. Three predictors for image, reward, and discount factor are learned on the latent state. The total loss for the world model is a combination of losses for all three predictors and a Kullback–Leibler loss between the latents predicted by the dynamics and the latents from the encoder.\n\nAlgorithm 1 DUTD (in terms of inverted UTD ratio)\n\nInput: Initial inverted UTD ratio iutd ratio; number of steps after which additional validation data is collected d, number of validation transitions collected s, steps after which the iutd ratio is updated k, iutd update increment c for t = 1 to total num of env steps do\n\nAct according to policy π(a | s) and observe next state if t mod d == 0 then\n\nCollect s transitions and store experience in a separate validation buffer; increment t = t + s\n\nend if if t mod iutd ratio == 0 then\n\nPerform one training step of the transition model\n\nend if if t mod k == 0 then\n\nCompute model loss L on validation dataset if L ≥ Lprevious then\n\n# Overfitting\n\niutd ratio = iutd ratio · c\n\nelse\n\n# Underfitting\n\niutd ratio = iutd ratio/c\n\nend if Lprevious = L\n\nend if end for\n\nTo apply DUTD we evaluate the image reconstruction loss on the validation set. Other choices are also possible but we speculate that the image prediction is the most difficult and important part of the world model. One could also use a combination of different losses but then one would potentially need a scaling factor for the different losses. As we want to keep our method simple and prevent the need of hyperparameter tuning for our method, we employ the single image loss. The source code of our implementation is publicly available 1.\n\n4 EXPERIMENTS\n\nWe evaluate DUTD applied to DreamerV2 on the Atari 100k benchmark Kaiser et al. (2019) and the DeepMind Control Suite Tassa et al. (2018). For each of the two benchmarks we use the respective hyperparameters provided by the authors in their original code base. Accordingly, the baseline IUTD ratio is set to a value of 5 for the control suite and 16 for Atari which we also use as initial value for our method. This means an update step is performed every 5 and 16 environment steps respectively. For both benchmarks we set the increment value of DUTD to c = 1.3 and the IUTD ratio is updated every 500 steps which corresponds to the length of one episode in the control suite (with a frameskip of 2). Every 100, 000 steps DUTD collects 3, 000 transitions of additional validation data. We cap the IUTD ratio in the interval [1, 15] for the control suite and in [1, 32] for Atari. This is in principle not necessary and we find that most of the time the boundaries, especially the upper one, is not reached. A boundary below 1 would be possible by using fractions and doing several updates per environment step, but this would be computationally very expensive for DreamerV2. All other hyperparameters are reported in the Appendix. They were not extensively tuned and we observed that the performance of our method is robust with respect to the specific choices. The environment steps in all reported plots also include the data collected for the validation set.\n\n1https://github.com/Nicolinho/dutd\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Aggregated metrics over 5 random seeds on the 26 games of Atari 100k with 95% confidence intervals according to the method presented in Agarwal et al. (2021). The intervals are estimated by the percentile bootstrap with statified sampling. Higher mean, median, interquantile mean (IQM) and lower optimality gap are better.\n\nThe Atari 100k benchmark Kaiser et al. (2019) includes 26 games from the Arcade Learning Environment Bellemare et al. (2013) and the agent is only allowed 100, 000 steps of environment interaction per game, which are 400, 000 frames with a frame-skip of 4 and corresponds to roughly two hours of real-time gameplay. The final performance per run is obtained by averaging the scores of 100 rollouts with the final policy after training has ended. We compute the human normalized score of each run as agent score−random score human score−random score . The DeepMind Control Suite provides several environments for continuous control. Agents receive pixel inputs and operate with a frame-skip of 2 as in the original DreamerV2. We trained for 2 million frames on most environments and to save computation cost for 1 million frames if standard DreamerV2 already achieves its asymptotic performance well before that mark. The policy is evaluated every 10, 000 frames for 10 episodes. For both benchmarks, each algorithm is trained with 5 different seeds on every environment.\n\nOur experiments are designed to demonstrate the following:\n\n• The UTD ratio can be automatically adjusted using our DUTD approach\n\n• DUTD generally increases performance (up to 300% on Atari100k) by learning an improved\n\nworld model compared to the default version of DreamerV2\n\n• DUTD increases the robustness of the RL agent with regard to learning-related hyperparameters\n\n• DUTD is competitive with the best UTD hyperparameter found by an extensive grid search\n\n4.1 PERFORMANCE OF DUTD COMPARED TO STANDARD DREAMERV2\n\nFor Atari100k, Figure 2 shows results aggregated over the 26 games with the method of Agarwal et al. (2021), where the interquantile mean (IQM) ignores the bottom and top 25% of the runs across all games and computes the mean over the remaining. The optimality gap describes the amount by which a minimal value of human level performance is not In Figure 11 we present the learning curves for reached. each environment. The results show that DUTD achieves a drastically stronger performance on all considered metrics compared to DreamerV2 with the fixed default IUTD ratio of 16. It increases the interquantile mean (IQM) score by roughly 300% and outperforms the human baseline in terms of mean score without any data augmentation.\n\nFigure 3 shows the aggregated results for two million frames over ten environments of the Control Suite, which we list in the Appendix. The curves per environment are presented in Figure 11 of the Appendix further including results for ten more environments on which the algorithms run until one million frames. Compared to the manually set default UTD ratio, DUTD matches or improves the performance on every environment. Overall, DUTD improves the performance significantly although its average IUTD rate over all games and checkpoints is 5.84 similar to the default rate of 5 showing that DUTD better exploits the performed updates.\n\n6\n\nFigure 3: Sample efficiency curves aggregated from the results for ten environments of the DeepMind Control Suite for DreamerV2 with the default UTD ratio and when it is adjusted with DUTD. The IQM score at different training steps is plotted against the number of environment steps. Shaded regions denote pointwise 95% stratified bootstrap confidence intervals according to the method by Agarwal et al. (2021).\n\n0.10.20.3DreamerV2DUTD-DreamerV2Median0.160.240.32IQM0.60.81.01.2Mean0.600.650.700.75Optimality GapHuman Normalized Score0.00.51.01.52.0Environment Steps (in millions)0200400600800IQM ScoreDreamerV2-IUTD_5DUTD-DreamerV2Published as a conference paper at ICLR 2023\n\nFigure 4: Learning curves for five environments of the Control Suite for DUTD-DreamerV2 and standard DreamerV2 when non-default learning rates are used. The first row shows the results for a lower than default learning rate of 0.0001 and the second row for a higher one of 0.001. The default learning rate is 0.0003 and its results are shown in Figure 12. The solid line represents the mean and the shaded region a pointwise standard deviation in each direction computed over 5 runs.\n\nFigure 5: Aggregated metrics over 5 random seeds on the 26 games of Atari 100k, cf. Figure 2 for the methodology. DUTD is compared to Dreamer with different choices for the IUTD rate.\n\n4.2\n\nINCREASED ROBUSTNESS WITH DUTD\n\nAs DUTD dynamically adjusts the UTD ratio which allows to modify the training process online, we formed the hypothesis that with DUTD the underlying RL algorithm is more robust to suboptimal learning hyperparameters. Similar to supervised learning on a fixed dataset the optimal number of updates to tradeoff between under- and overfitting will be highly dependent on hyperparameters like the learning rate. To investigate this, we evaluated DreamerV2 with and without our method for different learning rates of the dynamics model. The standard learning rate on the control suite is 0.0003. Hence, we trained with both a higher learning rate of 0.001 and a lower one of 0.0001 on a subset of the environments. The resulting learning curves are displayed in Figure 4. Compared to the default learning rate the performance of DreamerV2 with the standard fixed IUTD ratio of 5 is overall lower and decreases substantially for some of the environments for both non-default learning rates. However, using DUTD the algorithm achieves considerably stronger results. This shows that using DUTD the algorithm is more robust to the learning rate, which is an important property when the algorithm is applied in real world settings such as robotic manipulation tasks, since multiple hyperparameter sweeps are often infeasible in such scenarios. The need for more robustness as offered by DUTD is demonstrated by the performance drop of DreamerV2 with a learning rate differing by a factor of 3 and the fact that on Atari a different learning rate is used.\n\n4.3 COMPARING DUTD WITH EXTENSIVE HYPERPARAMETER TUNING\n\nIn the previous sections, we showed that DUTD improves the performance of DreamerV2 with its default IUTD ratio significantly. Now we want to investigate how well DUTD compares to the best hyperparameter value for IUTD that can be found through an extensive grid search on each benchmark. While for many applications such a search is not feasible we are interested in what can be expected of DUTD relative to what can be regarded as the highest achievable performance.\n\n7\n\n0.00.20.40.60.81.01e6100200300400500600Average Evaluation ReturnAcrobot SwingupDUTD-DreamerV2_lr1e-4DreamerV2_lr1e-40.00.20.40.60.81.01e602004006008001000Cartpole Swingup0.00.20.40.60.81.01e60200400600800Cheetah Run0.00.20.40.60.81.01e6050100150200250300Hopper Hop0.00.20.40.60.81.01e60100200300400500Quadruped Walk0.00.20.40.60.81.0Environment Steps1e650100150200250300Average Evaluation ReturnDUTD-DreamerV2_lr1e-3DreamerV2_lr1e-30.00.20.40.60.81.0Environment Steps1e62004006008000.00.20.40.60.81.0Environment Steps1e6020040060080010000.00.20.40.60.81.0Environment Steps1e6500501001502002503000.00.20.40.60.81.0Environment Steps1e601002003004005006007000.10.20.3DreamerV2_IUTD1DreamerV2_IUTD2DreamerV2_IUTD4DreamerV2_IUTD7DreamerV2_IUTD10DreamerV2_IUTD16DUTD-DreamerV2Median0.160.240.32IQM0.751.001.25Mean0.600.660.72Optimality GapHuman Normalized ScorePublished as a conference paper at ICLR 2023\n\nFigure 7: IUTD ratio against environment steps for DUTD and the standard DreamerV2 on five environments. For each environment the mean over 5 runs is plotted as the solid line and the shaded region represents one pointwise standard deviation in each direction.\n\nOn the Atari 100k benchmark we evaluate DreamerV2 with IUTD rates of 1, 2, 4, 7, 10 and 16 (the default value) and denote the algorithms with DreamerV2-IUTD 1, DreamerV2-IUTD 2, etc. The aggregated results over all games and seeds in Figure 5 show an increase in performance when the number of updates increases up to an IUTD rate of 2. Increasing it further to 1 leads to declining results. Thus, there is a sweet spot and one can not simply set the IUTD rate very low and expect good results. Averaged over all runs and checkpoints the IUTD rate of DUTD is at 3.91 which is in the region of the best performing hyperparameters of 2 and 4. This is also reflected by the fact that DUTD achieves similar performance to these two optimal choices.\n\nWe further evaluate DreamerV2 with IUTD ratios of 2, 5 (the default one), 10, and 15 on ten environments of the control suite. An IUTD value below 2 is not possible as a single run would take roughly two weeks to run on our hardware. The aggregated sample efficiency curves in Figure 6 further support the hypothesis that DUTD is competitive with the results of an extensive grid search. Only an IUTD choice of 2 gives slightly better sample efficiency but reaches a lower final performance. To further investigate the behaviour of DUTD we report the adjusted inverted UTD ratio over time for five environments in Figure 7, and for all environments in Figure 13 in the Appendix. Interestingly, the behavior is similar for all the environments. At the start of the training, the ratio is very low and then it quickly oscillates around a value of roughly 5 for most environments and an even higher value for a few others. On cheetah run and hopper hop, the IUTD oscillates around the default value of 5 most of the time and still, DUTD reaches a higher performance than Dreamer as can be seen in the single environment plot in Figure 12 of the Appendix. This result supports the hypothesis that a static IUTD rate can be suboptimal for some environments and that DUTD successfully balances over- and underfitting during the training process.\n\nFigure 6: Sample efficiency curves showing the IQM score aggregated from the results for ten environments of the DeepMind Control Suite for DreamerV2 with different choices for the IUTD ratio. Shaded regions denote pointwise 95% stratified bootstrap confidence intervals.\n\n4.4 EVALUATION FOR A HIGH NUMBER OF SAMPLES\n\nNext we investigate the behaviour of DUTD if training is continued for many environment steps. We randomly selected 5 games from the Atari benchmark and trained the algorithms for 40 million frames. The resulting learning curves displayed in Figure 8 show that DUTD maintains its advantage also in this setting. The significantly improved performance is achieved with an IUTD ratio of 13.51 averaged over all games and checkpoints. In Figure 14 of the Appendix we show the development of the IUTD ratio over time for each environment. We can see that with DUTD after an initial phase with a lower IUTD ratio it oscillates around a value not too far from the highly tuned default ratio of 16. This means DUTD significantly improves performance over plain DreamerV2 without requiring substantially more updates. The experiment further highlights the benefits of DUTD. Evaluating different choices for a fixed IUTD ratio in this setting is highly expensive and for low values of the IUTD ratio almost impossible as a single run with the default value takes already several days\n\n8\n\n0.00.51.01.52.0Environment Steps1e62468101214Inverted UTD RatioAcrobot SwingupDUTD-DreamerV2DreamerV20.00.20.40.60.81.0Environment Steps1e62468101214Cartpole Swingup0.00.51.01.52.0Environment Steps1e62468101214Cheetah Run0.00.51.01.52.0Environment Steps1e62468101214Hopper Hop0.00.51.01.52.0Environment Steps1e62468101214Quadruped Walk0.00.51.01.52.0Environment Steps (in millions)0200400600800IQM ScoreDUTD-DreamerV2DreamerV2-IUTD_2DreamerV2-IUTD_5DreamerV2-IUTD_10DreamerV2-IUTD_15Published as a conference paper at ICLR 2023\n\nFigure 8: Learning curves for DreamerV2 with and without DUTD on 5 randomly selected environments of the Atari benchmark. For each environment the mean over 3 runs is plotted as the solid line and the shaded region represents one pointwise standard deviation in each direction.\n\nto train. DUTD improves upon the highly tuned default choice and removes the need to tune this hyperparameter in an inner loop.\n\n4.5 GENERALITY OF DUTD\n\nTo demonstrate the generality of DUTD we applied it to PlaNet Hafner et al. (2019) which is another model-based RL algorithm. We evaluated the resulting method on three environments of the DeepMind Control Suite using the same hyperparameters for DUTD as for Dreamer. The results in Figure 9 of the appendix show that DUTD also improves the performance of PlaNet validating that DUTD is a general method and indicating its usefulness for different base algorithms.\n\n5 DISCUSSION\n\nWe presented a novel and general method denoted as DUTD that is designed to detect under- and overfitting on evolving datasets and is able to dynamically adjust the typically hand-set UTD ratio in an automated fashion. As in early stopping, the underlying rationale is that too many updates can lead to overfitting while too few updates can lead to underfitting. DUTD quickly identifies such trends by tracking the development of the world model performance on a validation set. It then accordingly increases or decreases the UTD ratio in the case of underfitting or overfitting.\n\nIn our experiments, we demonstrated how to successfully apply DUTD to a model-based RL algorithm like DreamerV2. The experiments show that DUTD can automatically balance between the under- and overfitting of the world model by adjusting the UTD ratio. As a result, DUTD removes the burden of manually setting the UTD ratio, which otherwise needs to be tuned for new environments making it prohibitively expensive to apply such algorithms in many domains. At the same time, DUTD increases the performance of DreamerV2 significantly compared to its default UTD rate and is competitive with the best hyperparameter found for each domain through an extensive hyperparameter search. Moreover, a notable property of DUTD-DreamerV2 is its robustness to changes in the learning rate. This is important, as the learning rate often has to be tuned for new environments. For example, in DreamerV2 the default learning rate differs between Atari and the DeepMind Control Suite. In the context of real world problems such tuning is undesirable and often too costly. At the same time, the hyperparameters of DUTD can easily be set and do not have a big influence on the final performance. We recommend updating the UTD rate after a fixed time interval that is similar to the average episode length. The data used for validation should not exceed 10% of all data.\n\nAn interesting avenue for future work would be to explore non-supervised objectives for model-free RL algorithms that can be used for evaluation on the validation set. This would allow the usage of DUTD to adjust the UTD ratio of such algorithms. Another potential way to further boost the performance of our method is to use k-fold cross-validation with an ensemble of world models such that every transition can be used for training.\n\nWe are convinced that DUTD is a further step in the direction of autonomy and the easy applicability of RL algorithms to new real world problems without the need to tune any hyperparameters in an inner loop. More generally, our work shows that it might be fruitful to use knowledge about the underlying learning dynamics to design algorithms that dynamically adjust parts of the learning algorithm.\n\n9\n\n01234Environment Frames1e710001500200025003000Average Evaluation ReturnAlienDreamerV2DUTD-DreamerV201234Environment Frames1e72004006008001000Amidar01234Environment Frames1e7100020003000400050006000Ms Pacman01234Environment Frames1e70100002000030000400005000060000Qbert01234Environment Frames1e725000250050007500100001250015000SeaquestPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work was supported by the European Union’s Horizon 2020 Research and Innovation Program under Grant 871449-OpenDR.\n\nREFERENCES\n\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems, 34, 2021.\n\nRS Anderssen and PM Prenter. A formal comparison of methods proposed for the numerical solution\n\nof first kind integral equations. The ANZIAM Journal, 22(4):488–500, 1981.\n\nDilip Arumugam, David Abel, Kavosh Asadi, Nakul Gopalan, Christopher Grimm, Jun Ki Lee, Lucas Lehnert, and Michael L Littman. Mitigating planner overfitting in model-based reinforcement learning. arXiv preprint arXiv:1812.01129, 2018.\n\nAdri`a Puigdom`enech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. In International Conference on Machine Learning, pp. 507–517. PMLR, 2020.\n\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253–279, 2013.\n\nYutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in alphago. arXiv preprint arXiv:1812.06855, 2018.\n\nKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Advances in Neural Information Processing Systems, 31, 2018.\n\nMarc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp. 465–472. Citeseer, 2011.\n\nNicolai Dorka, Joschka Boedecker, and Wolfram Burgard. Adaptively calibrated critic estimates for\n\ndeep reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021.\n\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pp. 2555–2565. PMLR, 2019.\n\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=S1lOTC4tDS.\n\nDanijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=0oabwyZbOu.\n\nNan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pp. 1181–1189. Citeseer, 2015.\n\nŁukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Bła ̇zej Osi ́nski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model based reinforcement learning for atari. In International Conference on Learning Representations, 2019.\n\nMisha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. Advances in Neural Information Processing Systems, 33, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nNelson Morgan and Herv ́e Bourlard. Generalization and parameter estimation in feedforward nets:\n\nSome experiments. Advances in neural information processing systems, 2:630–637, 1989.\n\nCharles Packer, Katelyn Gao, Jernej Kos, Philipp Kr ̈ahenb ̈uhl, Vladlen Koltun, and Dawn Song. Assessing generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282, 2018.\n\nJack Parker-Holder, Raghu Rajan, Xingyou Song, Andr ́e Biedenkapp, Yingjie Miao, Theresa Eimer, Baohe Zhang, Vu Nguyen, Roberto Calandra, Aleksandra Faust, et al. Automated reinforcement learning (autorl): A survey and open problems. arXiv preprint arXiv:2201.03916, 2022.\n\nLuis Pineda, Brandon Amos, Amy Zhang, Nathan O. Lambert, and Roberto Calandra. Mbrl-lib: A modular library for model-based reinforcement learning. Arxiv, 2021. URL https://arxiv. org/abs/2104.10159.\n\nRoberta Raileanu, Maxwell Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic\n\ndata augmentation for generalization in reinforcement learning. 2020.\n\nTom Schaul, Diana Borsa, David Ding, David Szepesvari, Georg Ostrovski, Will Dabney, and Simon Osindero. Adapting behaviour for learning progress. arXiv preprint arXiv:1912.06910, 2019.\n\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.\n\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. HigharXiv preprint\n\ndimensional continuous control using generalized advantage estimation. arXiv:1506.02438, 2015.\n\nMax Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=uCQfPZwRaUu.\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140– 1144, 2018.\n\nXingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, and Behnam Neyshabur. Observational overfitting in reinforcement learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJli2hNKDH.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Journal of Machine Dropout: A simple way to prevent neural networks from overfitting. Learning Research, 15(56):1929–1958, 2014. URL http://jmlr.org/papers/v15/ srivastava14a.html.\n\nOtto Neall Strand. Theory and methods related to the singular-function expansion and landweber’s iteration for integral equations of the first kind. SIAM Journal on Numerical Analysis, 11(4): 798–825, 1974.\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT Press,\n\n2018. ISBN 78-0262039246.\n\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.\n\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. Machine learning, 8(3):229–256, 1992.\n\nAmy Zhang, Nicolas Ballas, and Joelle Pineau. A dissection of overfitting and generalization in\n\ncontinuous reinforcement learning. arXiv preprint arXiv:1806.07937, 2018a.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nBaohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, Andr ́e Biedenkapp, Kurtland Chua, Frank Hutter, and Roberto Calandra. On the importance of hyperparameter optimization for model-based reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pp. 4015–4023. PMLR, 2021.\n\nChiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overfitting in deep\n\nreinforcement learning. arXiv preprint arXiv:1804.06893, 2018b.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA FURTHER RESULTS\n\nA.1 APPLYING DUTD TO PLANET\n\nTo demonstrate the generality of DUTD we additionally applied it to PlaNet Hafner et al. (2019) with the same hyperparameters for DUTD as we also used for DreamerV2. As base source code on which we implemented DUTD we used Pineda et al. (2021). We evaluated the resulting algorithm on three environments of the DeepMind Control Suite that were also used in the original publication of PlaNet. We used 5 seeds and evaluated the algorithms every 25000 environment frames. The results in Figure 9 show that DUTD also improves the performance of PlaNet. This is further evidence for the generality of DUTD.\n\nFigure 9: Learning curves for PlaNet with and without DUTD on three environments of the DeepMind Cotrol Suite. The solid line is the mean over 5 seeds and the shaded area represents one pointwise standard deviation. We used a uniform filter of size 3.\n\nA.2 DETAILED RESULTS FOR APPLYING DUTD TO DREAMERV2\n\nThe ten environments of the DeepMind Control Suite used to generate the aggregated curves in the Figures 3 and 6 are: acrobot swingup, cheetah run, finger turn easy, finger turn hard, hopper hop, quadruped run, quadruped walk, reacher hard, walker walk, and walker run.\n\nWe evaluated on all 20 environments used in the original Dreamer paper Hafner et al. (2020) but to save computation stopped training for ten environments at 1 million steps because standard Dreamer already reaches its asymptotic performance well before that mark. The aggregated curves are generated from the other 10 environments for which training ran until 2 million steps. Figure 12 shows the single learning curves for all environments. Please note, that on the 1 million steps environments with DUTD the asymptotic performance is reached much faster - often twice as fast.\n\nIn the Figures 10, 11, 12, 13, and 15 we present the more detailed results of our experiments for each single environment.\n\nFigure 10: Learning curves for different choices of the IUTD ratio for each of the environments. The solid line is the mean over 5 seeds and the shaded area represents one pointwise standard deviation.\n\n13\n\n050000100000150000200000250000Environment Frames200300400500600700800900Average Evaluation ReturnCartpole SwingupDUTD-PlaNetPlaNet050000100000150000200000250000Environment Frames0100200300400500Cheetah Run050000100000150000200000250000Environment Frames100200300400500Walker Walk0.00.51.01.52.01e60100200300400500600700Average Evaluation ReturnAcrobot Swingup0.00.51.01.52.01e602004006008001000Cheetah Run0.00.51.01.52.01e602004006008001000Finger Turn Easy0.00.51.01.52.01e602004006008001000Finger Turn Hard0.00.51.01.52.01e60100200300400500Hopper Hop0.00.51.01.52.0Environment Steps1e60200400600800Average Evaluation ReturnQuadruped Run0.00.51.01.52.0Environment Steps1e602004006008001000Quadruped Walk0.00.51.01.52.0Environment Steps1e60200400600800Reacher Hard0.00.51.01.52.0Environment Steps1e60200400600800Walker Run0.00.51.01.52.0Environment Steps1e602004006008001000Walker WalkDUTD-DreamerV2DreamerV2_IUTD15DreamerV2_IUTD10DreamerV2_IUTD5DreamerV2_IUTD2Published as a conference paper at ICLR 2023\n\nFigure 11: Learning curves for DreamerV2 with and without DUTD on the 26 environments of the Atari 100k benchmark. The solid line is the mean over 5 seeds and the shaded area represents one pointwise standard deviation.\n\n14\n\n012341e502004006008001000Average Evaluation ReturnAlien012341e5020406080100120Amidar012341e5020040060080010001200Assault012341e5100200300400500600700800Asterix012341e50100200300400500600700Bank Heist012341e5020004000600080001000012000Average Evaluation ReturnBattle Zone012341e5604020020406080100Boxing012341e51012345Breakout012341e50200400600800Chopper Command012341e5020000400006000080000Crazy Climber012341e5050100150200250300350Average Evaluation ReturnDemon Attack012341e5505101520253035Freeway012341e50250500750100012501500Frostbite012341e505001000150020002500Gopher012341e501000200030004000Hero012341e5050100150200250300Average Evaluation ReturnJamesbond012341e50100020003000400050006000Kangaroo012341e502000400060008000Krull012341e5050001000015000200002500030000Kung Fu Master012341e502505007501000125015001750Ms Pacman012341e5201510505101520Average Evaluation ReturnPong012341e51000500050010001500Private Eye012341e50100200300400500600700800Qbert012341e50200040006000800010000120001400016000Road Runner012341e50100200300400500600Seaquest01234Environment Frames1e51000001000020000300004000050000Average Evaluation ReturnUp N Down0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0Published as a conference paper at ICLR 2023\n\nFigure 12: Learning curves for DreamerV2 with and without DUTD for 20 environments of the DeepMind Control Suite. The solid line is the mean over 5 seeds and the shaded area represents one pointwise standard deviation.\n\n15\n\n0.00.51.01.52.01e60100200300400500600Average Evaluation ReturnAcrobot SwingupDUTD-DreamerV2DreamerV20.00.20.40.60.81.01e62003004005006007008009001000Cartpole Balance0.00.20.40.60.81.01e602004006008001000Cartpole Balance Sparse0.00.20.40.60.81.01e6200400600800Cartpole Swingup0.00.20.40.60.81.01e60200400600800Cartpole Swingup Sparse0.00.51.01.52.01e602004006008001000Average Evaluation ReturnCheetah Run0.00.20.40.60.81.01e602004006008001000Cup Catch0.00.20.40.60.81.01e60100200300400500600700800Finger Spin0.00.51.01.52.01e602004006008001000Finger Turn Easy0.00.51.01.52.01e602004006008001000Finger Turn Hard0.00.51.01.52.01e60100200300400500Average Evaluation ReturnHopper Hop0.00.20.40.60.81.01e602004006008001000Hopper Stand0.00.20.40.60.81.01e602004006008001000Pendulum Swingup0.00.51.01.52.01e60200400600800Quadruped Run0.00.51.01.52.01e602004006008001000Quadruped Walk0.00.20.40.60.81.0Environment Steps1e602004006008001000Average Evaluation ReturnReacher Easy0.00.51.01.52.0Environment Steps1e60200400600800Reacher Hard0.00.51.01.52.0Environment Steps1e60200400600800Walker Run0.00.20.40.60.81.0Environment Steps1e62004006008001000Walker Stand0.00.51.01.52.0Environment Steps1e62004006008001000Walker WalkPublished as a conference paper at ICLR 2023\n\nFigure 13: IUTD ratio against environment steps for DUTD and the standard DreamerV2 on all environments. For each environment the mean over 5 runs is plotted as the solid line and the shaded region shows represents one pointwise standard deviation in each direction.\n\nIUTD ratio against environment steps for DUTD and the standard DreamerV2 on 5 Figure 14: environments of Atari for which the algorithms were trained until 40 million frames. For each environment the mean over 3 runs is plotted as the solid line and the shaded region shows represents one pointwise standard deviation in each direction.\n\n16\n\n0.00.51.01.52.01e62468101214Inverted UTD RatioAcrobot SwingupDUTD-DreamerV2DreamerV20.00.20.40.60.81.01e62468101214Cartpole Balance0.00.20.40.60.81.01e62468101214Cartpole Balance Sparse0.00.20.40.60.81.01e62468101214Cartpole Swingup0.00.20.40.60.81.01e62468101214Cartpole Swingup Sparse0.00.51.01.52.01e62468101214Inverted UTD RatioCheetah Run0.00.20.40.60.81.01e62468101214Cup Catch0.00.51.01.52.01e62468101214Finger Spin0.00.51.01.52.01e62468101214Finger Turn Easy0.00.51.01.52.01e62468101214Finger Turn Hard0.00.51.01.52.01e62468101214Inverted UTD RatioHopper Hop0.00.20.40.60.81.01e62468101214Hopper Stand0.00.51.01.52.01e62468101214Pendulum Swingup0.00.51.01.52.01e62468101214Quadruped Run0.00.51.01.52.01e62468101214Quadruped Walk0.00.51.01.52.0Environment Steps1e62468101214Inverted UTD RatioReacher Easy0.00.51.01.52.0Environment Steps1e62468101214Reacher Hard0.00.51.01.52.0Environment Steps1e62468101214Walker Run0.00.20.40.60.81.0Environment Steps1e62468101214Walker Stand0.00.20.40.60.81.0Environment Steps1e62468101214Walker Walk01234Environment Frames1e746810121416182022IUTD RatioAlienDreamerV2DUTD-DreamerV201234Environment Frames1e746810121416182022Amidar01234Environment Frames1e746810121416182022Ms Pacman01234Environment Frames1e746810121416182022Qbert01234Environment Frames1e746810121416182022SeaquestPublished as a conference paper at ICLR 2023\n\nFigure 15: Learning curves for different choices of the IUTD ratio for each of the 26 environments of the Atari 100k benchmark. The solid line is the mean over 5 seeds and the shaded area represents one pointwise standard deviation.\n\n17\n\n012341e5020040060080010001200Average Evaluation ReturnAlien012341e5020406080100120140Amidar012341e5020040060080010001200Assault012341e5200400600800Asterix012341e502004006008001000Bank Heist012341e5020004000600080001000012000Average Evaluation ReturnBattle Zone012341e5604020020406080100Boxing012341e502468Breakout012341e502004006008001000120014001600Chopper Command012341e5020000400006000080000100000Crazy Climber012341e51000100200300400500Average Evaluation ReturnDemon Attack012341e5010203040Freeway012341e525002505007501000125015001750Frostbite012341e505001000150020002500Gopher012341e50200040006000800010000Hero012341e50100200300400Average Evaluation ReturnJamesbond012341e501000200030004000500060007000Kangaroo012341e502000400060008000Krull012341e5050001000015000200002500030000Kung Fu Master012341e50500100015002000Ms Pacman012341e5201001020Average Evaluation ReturnPong012341e51000500050010001500Private Eye012341e502505007501000125015001750Qbert012341e505000100001500020000Road Runner012341e50100200300400500600700Seaquest01234Environment Frames1e520000020000400006000080000Average Evaluation ReturnUp N DownDUTD-DreamerDreamer_IUTD16Dreamer_IUTD1Dreamer_IUTD2Dreamer_IUTD4Dreamer_IUTD7Dreamer_IUTD100.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0Published as a conference paper at ICLR 2023\n\nB HYPERPARAMETERS\n\nIn Table 1 we give an overview of all hyperparameters related to DUTD. All other hyperparameters are the standard DreamerV2 hyperparameters as given in the open source codebase 2. On the DM Control Suite we reduced the number of steps d after which to collect new data for the validation set by a half during the first 400k steps as for some environments a strong policy is learned very quickly and hence a validation set with more recent transitions that better represent the kind of transitions the agent encounter makes more sense. We have because we started our first experiments with this but from some limited additional experiments it seems not to have a big impact on performance.\n\nTable 1: Hyperparameters values for DUTD applied to DreamerV2 and the corresponding hyperparameter in the original DreamerV2.\n\nHYPERPARAMETER INITIAL IUTD RATIO LOWER BOUNDARY FOR THE IUTD RATIO UPPER BOUNDARY FOR THE IUTD RATIO IUTD UPDATE INCREMENT — c NUMBER OF STEPS AFTER WHICH TO UPDATE THE IUTD RATIO — k VALIDATION SET MAXIMUM SIZE — k NUMBER OF STEPS AFTER WHICH TO COLLECT NEW DATA FOR THE VALIDATION SET — d NUMBER OF ADDITIONAL TRANSITIONS FOR THE VALIDATION SET EACH TIME NEW VALIDATION DATA IS COLLECTED — s\n\nIUTD RATIO\n\nATARI 16 1\n32 1.3 500 12,000\n\nDM CONTROL 5\n1 15 1.3 500 10,000\n\n100,000\n\n100,000\n\n3,000 STANDARD DREAMERV2\n\n3,000\n\n16\n\n5\n\nC HYPERPARAMETER SENSITIVITY OF DUTD\n\nMost hyperparameters of our method are straightforward to set and do not need any tuning. Updating the UTD ratio after the maximum episode length of 500 in DM Control Suite (DMC) is a value that we directly transferred to the Atari benchmark without further tuning. The initial value for the UTD ratio has no effect, as it gets quickly adjusted. The lower and upper limits for the UTD ratio are not reached often and hence do not affect performance given they are chosen lavish enough. We did not tune those. We tried a few choices for the number of additional transitions each time new validation data is collected and the number of steps after which we do so but did not find it to affect performance a lot and fixed one choice for both benchmarks.\n\nThe multiplicative factor c is the most important hyperparameter of our method and we hence conducted an additional experiment evaluating its sensitivity on the Atari100k benchmark over 5 random seeds. We show the aggregated metrics for different multiplicative factors in Figure 16.\n\nThe results show that seen over all metrics and relative to the baseline results the performance is not very sensitive with respect to the choice of the multiplicative factor. For the mean our default factor of 1.3 even gives slightly worse results than all other factors. Further, we argue the fact that the same setting of hyperparameters of DUTD works for very different benchmarks, Atari and DMC, shows that DUTD is not very sensitive to its hyperparameters and that the default values given by us will most likely work for a wide range of tasks. While an extensive hyperparameter search for the optimal UTD ratio might give slightly better results than DUTD with some fixed multiplicative factor, DUTD is still favorable for many real world applications where such tuning is too costly.\n\n2https://github.com/danijar/dreamerv2\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nFigure 16: Aggregated metrics over 5 random seeds on the 26 games of Atari 100k, cf. Figure 2 for the methodology. We investigate the sensitivity of DUTD to its own most important hyperparameter c for values of 1.1, 1.2, 1.3 (default one used in the main experiments), 1.4, and 1.5 .\n\n19\n\n0.10.20.3DreamerV2_IUTD16DUTD-1.1DUTD-1.2DUTD-1.3DUTD-1.4DUTD-1.5Median0.160.240.32IQM0.60.91.21.5Mean0.600.650.700.75Optimality GapHuman Normalized Score",
    "reference": "# Summary Of The Paper\n\nThis paper aims to improve the training of Model Based Reinforcement Learning methods by introducing an automatic method of setting/dynamically tuning the update-to-data (UTD) ratio, a key hyper parameter which controls how much the world model over- or under-fits the training data.  They propose using a held out set of validation samples, updated throughout training, to estimate whether the world model is over- or under-fitting and to update the UTD ratio accordingly.  This method both enables the parameter to be set automatically, avoiding the need for hyper parameter tuning, and also allows the parameter to be dynamically updated as the training stage evolves.  They present results using their approach with DreamerV2 on both the DeepMind Control Suite and the Atari100k benchmark, demonstrating comparable performance on Atari100k and a slight improvement on DeepMindControl Suite when compared to training with the best fixed value of UTD when it is carefully tuned.  Additionally they show that the final performance on both environment is highly dependent on carefully tuning UTD when using a fixed value, but the performance of their method is robust to a wide range of values for its primary hyper-parameter.\n\n# Strength And Weaknesses\n\nStrengths:\n- The method, though intuitive, is not standard practice and they make strong case for why DUTD could benefit all MBRL practitioners by improving their training performance while reducing the amount of manual tuning necessary.  Such improvements also enable fairer comparisons between future methods by removing one of many possible confounding factors in their evaluation.\n- Thorough experimentation and presentation of findings, including their many plots in the Appendix supports both drawing additional conclusions and reproducibility. Reporting results using methods described in Agarwal et al 2021 supports fair comparisons and a stronger ability to draw meaningful conclusions.\n- Not mentioned in the work, but the method also allows for the UTD parameter to be adaptively set for each individual task automatically, with different tasks using substantially different average values (as seen in Figure 12).\n\nWeaknesses:\n- These results are only evaluated on DreamerV2, therefore it is unclear if they extend to other MBRL methods or are Dreamer specific.\n- The claim that DUTD is more robust to hyper parameters than a fixed value of UTD is supported by comparing DUTD to the default fixed value of UTD which was already demonstrated to be sub-optimal.  Since it was already demonstrated that DUTD outperforms the default UTD with the default hpms then it is unsurprising that this continues to be the case when changing the hpms.  To isolate whether the adaptive choice of UTD as done by DUTD is more robust to varying hpms (the goal of this ablation), it should be compared to the best fixed UTD value as chosen by careful tuning.\n- Similarly, it seems a possible takeaway from the longer runs is the value of adaptive UTD setting as we see in Figure 13 that the value chosen by DUTD varies throughout training.  However, because the comparison in Figure 8 is to the default value instead of the best tuned value of UTD, it is unclear whether there is benefit from adaptively setting UTD throughout training or just from choosing a good value of UTD.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- The method, primary contributions, and demonstration of effectiveness is presented clearly.  \n- The experiments demonstrate the high quality of their method for automatically tuning the UTD parameter and the importance of this choice.\n- The idea of using a validation set to identify overfitting and perform early stopping is not novel, but their method for applying this concept to MBRL is new as far as I know.\n- Their thorough description of their method, hyperparameters and results, and their use of multiple seed so provide error bars makes this work easily reproducible, as will their code release. \n\nSmall Note:\n- In the final paragraph in Section 2, missing word: “In contrast to these approaches we [present] our method”\n\n# Summary Of The Review\n\nThis paper (1) provides strong evidence that tuning the UTD parameter substantially impacts the outcomes of DreamerV2 training (2) provides a method for automatically tuning the UTD parameter that consistently matches or improves the performance of the best fixed value of UTD when chosen with careful tuning.  \n\nThis method is likely broadly applicable to MBRL practitioners and can be used as a standard tool for effectively training world models, saving time/resources in hyper parameter tuning and making results more directly comparable by removing the confounding factor of how well tuned the UTD parameter is.  However, it is not clear that this is the case since the method has only been evaluated on one such method, DreamerV2 (my main concern when making my acceptance recommendation).  Additionally, there isn’t clear evidence for the claim that *adaptively* setting UTD throughout training is more beneficial than choosing the optimal fixed UTD (my main concern when evaluating correctness at 3 out of 4).\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Details Of Ethics Concerns\n\nN/A"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nUTS: WHEN MONOTONIC VALUE FACTORISATION MEETS NON-MONOTONIC AND STOCHASTIC TARGETS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nIn the paradigm of centralised training with decentralised execution, monotonic value decomposition is one of the most popular methods to guarantee consistency between centralised and decentralised policies. This method always underestimates the value of the optimal joint action and converges to the suboptimal because it can only represent values in the restricted monotonic space. A possible way to rectify this issue is to introduce a weighting function to prioritise the real optimal joint action and learn biased joint action-value functions. However, there may not exist an appropriate weight to solve more general tasks with non-monotonic and stochastic target joint action-values. To solve this problem, we propose a novel value factorisation method named uncertainty-based target shaping (UTS), which projects the original target to the space that monotonic value factorisation can represent based on its stochasticity. First, we employ networks to predict the reward and the embedding of the next state, where the prediction error quantifies the stochasticity. Then, we introduce a target shaping function to replace the targets for deterministic suboptimal with the best per-agent value. Since we remain the optimal policy unchanged during shaping, monotonic value decomposition can converge to the real optimal with any original targets. Theoretical and empirical results demonstrate the improved performance of UTS in the task with non-monotonic and stochastic target action-value functions.\n\n1\n\nINTRODUCTION\n\nRecent progress in cooperative multi-agent reinforcement learning (MARL) has shown attractive prospects for various real-world applications, such as the smart grid management (Aladdin et al., 2020) and autonomous vehicles (Zhou et al., 2021). Due to practical communication constraints and intractably large joint action space, decentralised policies are often used in MARL. It is possible to use extra information from the environment and other agents in a simulated or laboratory setting. Exploiting this information can significantly benefit policy optimisation and improve learning performance (Foerster et al., 2016; 2018; Rashid et al., 2020).\n\nIn the paradigm of centralised training with decentralised execution (CTDE), agents’ policies are trained with access to global information in a centralised way and executed only based on local histories in a decentralised way (Oliehoek et al., 2008; Kraemer & Banerjee, 2016). One of the most significant challenges is to guarantee the consistency between the individual policies and the centralised policy, i.e., the Individual-Global Max (IGM) principle (Son et al., 2019). In value decomposition methods, QMIX (Rashid et al., 2018) applies a monotonic mixing network to factorise the joint Q-value function, which naturally meets the IGM principle. Inspired by QMIX, many algorithms are proposed to improve coordination from different perspectives, e.g., multi-agent exploration (Mahajan et al., 2019), role-based learning (Wang et al., 2020b;c), and policy-based algorithms (Wang et al., 2020d). However, they can represent the same class of joint Q-values as QMIX because they use the same monotonic mixing network.\n\nHowever, since QMIX can only represent values in the restricted monotonic space, there exists a gap between the approximated joint Q-values and the non-monotonic target values Q from the environment. In some special tasks, QMIX can underestimate the value of the real optimal joint action and converge to a suboptimal (Son et al., 2019; Mahajan et al., 2019; Rashid et al., 2020). Recent works try to solve this representational limitation from two different perspectives. This first category\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nintroduces the joint actions (Wang et al., 2020a; Mahajan et al., 2021) or pairwise interactions (B ̈ohmer et al., 2020; Li et al., 2021) into centralised learning to achieve full representational capacity for target Q-values. However, learning such centralised values is difficult due to the large joint action space. Another category is to prioritise the real optimal joint action and learn biased joint Q-value functions. WQMIX (Rashid et al., 2020) introduces a weighting function into the projection from the target value functions to the joint Q-values and uses it to down-weight every suboptimal action whose target value is less than the current estimate. However, the poor empirical results on decentralised micromanagement tasks in StarCraft II show that it is difficult to apply the weighting function to more general tasks (Rashid et al., 2020; Wang et al., 2020a). We prove that the weight for the suboptimal should be small to help QMIX focus on the representation of the optimal joint Q-value and solves the non-monotonic targets. In addition, the weight for each action should be uniform to avoid overestimating the suboptimal whose target is large with a low probability. Due to this contradiction, there may not exist an appropriate weight to recover the optimal policy when the target is non-monotonic and stochastic.\n\nThis paper aims to take a step towards the latter category. We propose a novel value factorisation method named uncertainty-based target shaping (UTS), which projects the original target to the space that monotonic value factorisation can represent based on its stochasticity. First, we formulate two prediction problems and use the prediction error to quantify the stochasticity of the target joint Q-values. We employ a reward predictor and a state predictor to approximate the standard deviation of the reward and the embedding of the following state. The predicted standard deviation of the reward and the error of the state are expected to be significant if the pair leads to stochastic reward and stochastic state transition, respectively. Then, we introduce a shaping function to project the original targets Q to the monotonic space and keep the optimal policy unchanged. In practice, the best action value network is applied to predict the action value when each agent gets the cooperation of others. We use the minimal best per-agent values over all agents to replace the suboptimal target. We prove that this shaping can guarantee that all shaped targets are tractable for monotonic value decomposition. In addition, the optimal policy is the same for the original and the shaped targets. Therefore, QMIX can achieve full representational capacity for the shaped target Q-values rather than the original ones and converge to the real optimal.\n\nWe list our main contributions as follows:\n\n• We first analyse the limitations of the weighting function in value decomposition methods and show that it cannot guarantee convergence to the optimal when target Q-value functions are non-monotonic and stochastic.\n\n• We introduce a target shaping function to project the original targets to a monotonic space,\n\nwhich ensures that QMIX can converge to the optimal with any original targets.\n\n• We propose uncertainty-based target shaping and empirically show its improved performance\n\nin practice, especially in tasks with non-monotonic and stochastic targets.\n\n2 BACKGROUND\n\nA fully cooperative multi-agent task in the partially observable setting can be formulated as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP) (Oliehoek & Amato, 2016), consisting of a tuple G = ⟨A, S, Ω, O, U, P, R, n, γ⟩, where a ∈ A ≡ {1, . . . , n} describes the set of agents, S denotes the set of states, Ω denotes the set of joint observations, and R denotes the set of rewards. At each time step, an agent obtains its observation o ∈ Ω based on the observation function O (s, a) : S × A → Ω, and an action-observation history τa ∈ T ≡ (Ω × U )∗. Each agent a chooses an action ua ∈ U by a stochastic policy πa (ua|τa) : T × U → [0, 1], forming a joint action u ∈ U, which leads to a transition on the environment through the transition function P (s′, r|s, u) : S × U × S × R → [0, 1], where r ∈ R is the team reward. The goal of the task is to find the joint policy π which can maximise the joint Q-value function Qπ(st, ut) = Est+1:∞,ut+1:∞ [Gt|st, u], where Gt = (cid:80)∞\n\ni=0 γirt+i is the discounted return.\n\nVDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), and WQMIX (Rashid et al., 2020) are Q-learning algorithms for the fully cooperative multi-agent tasks, which estimate the joint Q-value function Q(s, u) as Qtot with specific forms. Considering only a fully-observable setting for ease of representation. VDN factorises Qtot into a sum of individual Q-value functions. By contrast,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nQMIX applies a state-dependent monotonic mixing network to combine per-agent Q-value functions with the joint Q-value function. The restricted spaces of all Qtot that linear and monotonic value factorisations can represent are:\n\nQlvf := {Qtot|Qtot(s, u) =\n\nn (cid:88)\n\na=1\n\nQa(s, ua), Qa(s, u) ∈ R}\n\n(1)\n\nQmvf := {Qtot|Qtot(s, u) = fs(Q1(s, u1), ..., Qn(s, un)),\n\n∂fs ∂Qa\n\n⩾ 0, Qa(s, u) ∈ R},\n\n(2)\n\nwhere the monotonic mixing function fs is parametrised as a feedforward network, whose nonnegative weights are generated by hypernetworks that take the state as input.\n\nu∈U(y(s, u) − Qtot(s, u))2, WQMIX views QMIX as an operator ΠQmixQ := arg minQtot∈Qmvf which can be separated into two parts: the first computes the Q-learning target y(s, u) := E[r + γ maxu′ Qtot(s′, u′)], and the second projects the target into Qmvf . Then, WQMIX introduces the weighting function into the projection to prioritise the optimal joint Q-value:\n\n(cid:80)\n\nΠWQmixQ := arg min Qtot∈Qmvf\n\n(cid:88)\n\nu∈U\n\nw(s, u)(y(s, u) − Qtot(s, u))2\n\n(3)\n\nSince it is computationally infeasible to obtain the optimal joint Q-value, WQMIX proposes CentrallyWeighted QMIX (CW-QMIX) and Optimistically-Weighted QMIX (OW-QMIX) to place more importance on better joint actions. In practice, the weighting function w : S × U → (0, 1] is:\n\n(cid:40)\n\nwcw(s, u) =\n\ny(s, u) > ˆQ∗(s, ˆu∗) or u = ˆu∗\n\n1 α otherwise\n\n, wow(s, u) =\n\n(cid:26)1\n\ny(s, u) > Qtot(s, u)\n\nα otherwise\n\n, (4)\n\nwhere ˆu∗ = arg maxu Qtot(s, u) is the current greedy joint action, ˆQ∗(s, ˆu∗) is an approximation of the true optimal value function which is not constrained to be monotonic.\n\n3 CASE STUDIES\n\nIn this section, we examine the weightings of WQMIX when the Q-learning targets are non-monotonic and stochastic. Since the non-linear mixing network in WQMIX makes many analysis methods inapplicable, we use WVDN to analyse the fundamental limitations of the weighting. The WVDN u∈U wow(s, u)(y(s, u)−Qtot(s, u))2, where operator is defined as ΠWVDNQ := arg minQtot∈Qlvf wow(s, u) is the optimistic weighting from (4).\n\n(cid:80)\n\n3.1 NON-MONOTONIC TARGETS\n\nQMIX cannot represent joint Q-value functions that are characterised as non-monotonic. Although this representational limitation will not necessarily result in the convergence to the suboptimal, QMIX fails to recover the optimal policy with some specific non-monotonic targets (Son et al., 2019). To accurately represent the optimal joint Q-value rather than the suboptimal ones, Rashid et al. (2020) introduce the weighting function in (4) to down-weight every suboptimal action. However, the choice of weight α is crucial to ensure that WQMIX can overcome the limitations of QMIX.\n\nThe weight α should be small enough to prioritise estimating the real optimal action value. We start by analysing a matrix game in Figure 1a, where each joint action receives a team reward. Our study focuses on the greatest value α to converge to the optimal when the policy is initialised to be suboptimal. Suppose the training dataset has a ε-greedy distribution, where the greedy action is (u2 to recover the optimal joint policy. The proof can be found in Appendix A.2. In Figure 1b, We show that α for WQMIX also should be smaller than a specific value to overcome the non-monotonic targets. The complete results are provided in Appendix B.\n\n2). We find that α for WVDN should be smaller than 3ε(a−c)+ε2(c−b)\n\n(ε−3)2(c−b)\n\n1, u2\n\n3.2 STOCHASTIC TARGETS\n\nThe stochasticity of the target value functions comes from both rewards and transitions, i.e., P (s′, r|s, u) is not deterministic. Traditional Q-learning algorithms solve this problem by calculating\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:80)\n\nthe weighted sum of possible rewards and values of the next state multiplied by their probabilities, i.e., Q(s, u) = (cid:80) r[P (s′, r|s, u)r + γP (s′, r|s, u) maxu′ Q(s′, u′)]. By contrast, the weighting function in WQMIX prioritises actions whose targets are greater than the estimated joint Q-value from the current mixing network. This weighting can be seen as a type of distribution shaping, leading to potential risks in policy learning.\n\ns′\n\nThe weight α should be large to anchor down the overestimated value for the suboptimal whose target is huge with a small probability. Since r(s, u) and maxu′ Q(s′, u′) play an equal role in the target value function, we take the matrix game with stochastic rewards as an example. To get a better understanding of the effect of the weight α in the stochastic environment, we show a numerical example in Figure 1c, where the suboptimal action us = (u2 2) receives 12 with probability 0.5 and 0 with 0.5, where the optimal is u∗ = (u1 2). Suppose the training dataset is fixed, where the data has a ε-greedy distribution and the greedy action is us. Let Q(u) denote the true joint Q-value. Since this matrix game only involves one state, we omit the state s in the value function for simplicity.\n\n1, u1\n\n1, u2\n\nFigure 1: (a) The payoff matrix for a one-step non-monotonic game, where a > c > b. (b) The minimal 1/α to recover the optimal policy with different ε in the non-monotonic game in (a), where a = 8, b = −12, c = 6. (c) The payoff matrix for a one-step stochastic game. (d-h) The estimated joint Q-values Qtot returned from QMIX and WQMIX. Boldface means the optimal joint action from the payoff matrix or the greedy joint action from the Qtot.\n\nFigure 1d-h demonstrate the joint Q-values Qtot(u) returned from QMIX and WQMIX with different α. QMIX is a special case of WQMIX (α = 1) and thus represents the joint Q-value perfectly. By contrast, WQMIX with a small weight is drawn to the suboptimal us and thus has an incorrect argmax. OW-QMIX returns an overestimated Qtot(us) = 10.9 > Qtot(u∗) and thus converge to the suboptimal ˆu∗ = us. In particular, CW-QMIX places more importance on us when it is not the greedy action and receives the reward of 12, leading to an overestimated value Qtot(us) > Q(us) = 6. With the increase of Qtot(us), us becomes the greedy action once Qtot(us) > Qtot(u∗). In this situation, CW-QMIX does not place the α for us when it receives the reward of 0, leading to the decrease of Qtot(us). As a result, CW-QMIX is stuck in this loop and cannot converge to any policy.\n\nu∈U wow(s, u)(Q(s, u) − Qtot(s, u))2 and Theorem 1 Let ΠWVDNQ := arg minQtot∈Qlvf wow(s, u) is the optimistic weighting from (4). Then ∃Q such that arg max ΠWVDNQ ̸= arg max Q for any α ∈ (0, 1].\n\n(cid:80)\n\nThe proof is provided in Appendix A.2. An intuitive understanding is that the weight α in WVDN has an upper bound to deal with non-monotonic Q-learning targets and has a lower bound to anchor down the overestimated suboptimal value due to undesirable weights in the stochastic environment. The empirical results in Appendix B show that WQMIX also suffers from this contradiction and fails to converge to the optimal.\n\n4 METHOD\n\nIn this section, we introduce a shaping function to generate the shaped target Qf (s, u) to replace the original target Q(s, u). The shaping function has the following properties: 1) policy invariance, which means the optimal policy remains unchanged during shaping, i.e., arg maxu Qf (s, u) = arg maxu Q(s, u). 2) full representational capacity, i.e., all shaped joint Q-values should belong\n\n4\n\n(a) A non-monotonic game.u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 babcccbbcbabcccbbcu11 U1 U2 u21 u12 u13 u22 u23 babcccbbc(c) A stochastic game.6866666612/0u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 6866666612/0u11 U1 U2 u21 u12 u13 u22 u23 (c) A stochastic game.6866666612/0u11 U1 U2 u21 u12 u13 u22 u23 (d) QMIX (w=1).68666666686666666686666666u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 686666666u11 U1 U2 u21 u12 u13 u22 u23 (d) QMIX (w=1).686666666u11 U1 U2 u21 u12 u13 u22 u23 α α (h) CW-QMIX (w=0.1).(h) CW-QMIX (w=0.1).7.987.987.987.986.336.336.336.335.51u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 7.987.987.987.986.336.336.336.335.51u11 U1 U2 u21 u12 u13 u22 u23 α α (f) CW-QMIX(w=0.5).(f) CW-QMIX(w=0.5).7.337.257.257.26.336.286.336.285.45u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 7.337.257.257.26.336.286.336.285.45u11 U1 U2 u21 u12 u13 u22 u23 α α (g) OW-QMIX (w=0.1).(g) OW-QMIX (w=0.1).7.297.676.426.626.435.737.6710.96.63u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 7.297.676.426.626.435.737.6710.96.63u11 U1 U2 u21 u12 u13 u22 u23 α α (e) OW-QMIX(w=0.5).(e) OW-QMIX(w=0.5).7.347.296.066.056.055.927.297.236.03u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 7.347.296.066.056.055.927.297.236.03u11 U1 U2 u21 u12 u13 u22 u23 α α (b) Minimal 1/a .AlgoAnalysis3.015.052.6WVDN10.542.2100.5OW-QMIX12.243.2103.2CW-QMIX11.529.683.51.00.50.25ε AlgoAnalysis3.015.052.6WVDN10.542.2100.5OW-QMIX12.243.2103.2CW-QMIX11.529.683.51.00.50.25ε α α Under review as a conference paper at ICLR 2023\n\nto a subset of Qmvf . Note that we achieve the full representational capacity w.r.t the shaped targets rather than the original ones. QMIX can recover the optimal joint policy with arbitrary original targets based on these properties. To this end, we first predict the standard deviation of the reward and the embedding of the next state. We then split the stochastic joint Q-values from the sampled data according to the prediction error. Next, we employ a network that estimates the best action value for each agent. Based on the prediction error and the best action values, we use the shaping function to generate shaped target joint Q-values, which can be represented by monotonic value decomposition.\n\n4.1 UNCERTAINTY ESTIMATION\n\nThe stochasticity of the target joint Q-value comes from the stochastic transition of the reward and the state, i.e., P (s′, r|s, u), which can be viewed as a type of the aleatoric uncertainty (Der Kiureghian & Ditlevsen, 2009). We propose a solution to this undesirable stochasticity using two prediction problems, where the output is the standard deviation of the reward and the embedding of the following state of a given state-action pair. This involves three neural networks: a reward predictor network, a state predictor, and a fixed and randomly initialised target network. The reward predictor network takes the state and joint action to the reward ˆr(s, u) : S × U → R and the uncertainty, i.e., the standard deviation ˆσ(s, u) : S × U → R. Based on the Bayesian deep learning method (Kendall & Gal, 2017), the reward predictor network is trained by minimising the following loss:\n\nB (cid:88)\n\n(\n\nb=1\n\n1\n\n2ˆσb(s, u)2 (rb(s, u) − ˆrb(s, u))2 +\n\n1 2\n\nlog ˆσb(s, u)2),\n\n(5)\n\nwhere B denotes the batch size of transitions sampled from the replay buffer. The standard deviation is expected to be higher if the state-action pair leads to stochastic rewards.\n\nInspired by RND (Burda et al., 2018), the target network for the second prediction problem takes a state to an m-dimensional embedding g(s) : S → Rm, where g(s) is a fixed and randomly initialised network. The state predictor h(s, u; θh) : S × U → Rm is trained to minimise the prediction error E(s, u, s′) = ∥g(s′) − h(s, u)∥2 with respect to its parameters θh, where s′ denotes the next state in the sampled trajectory. The prediction error is expected to be higher if the state-action pair results in stochastic transitions. Combining the reward and the state predictors, we define an indicator function T (s, u) = 1(ˆσ(s,u)>D (cid:83) E(s,u,s′)>E), where D and E are hyper-parameters.\n\n4.2 SHAPING FUNCTION\n\nQMIX cannot represent the non-monotonic target Q-value because it imposes the monotonic constraint on the relationship between the joint Q-value and the individual Q-values. Therefore, we can represent the suboptimality of uncooperative actions rather than their exact values. To quantify the suboptimality of actions, we define the best action value function qa(s, τa, ua) = maxu−a Q(s, ua, u−a) as the greatest Q-value of ua when others perform optimally to cooperate with it. qa(s, τa, ua) is less than the optimal Q value if ua is not part of the real optimal. Inspired by BAIL (Chen et al., 2020), we approximate qa(s, τa, ua) by minimising the following loss:\n\nB (cid:88)\n\nn (cid:88)\n\nb=1\n\na=1\n\nK(s, τa, ua)[qa(s, τa, ua) − yb\n\na(s, ua)]2 + λ∥θqa ∥,\n\n(6)\n\na\n\na, u′\n\nqa(s′, τ ′\n\nwhere B denotes the batch size of transitions sampled from the replay buffer, yb a(s, ua) = a) is the target, λ is the regularisation coefficients, K(s, τa, ua) = 1 ˆr(s, u) + γ maxu′ for the Q-value whose target is stochastic or smaller than the current estimate, i.e., yb a(s, ua) < qa(s, τa, ua) (cid:83) T (s, u) = 1, and K(s, τa, ua) = k ≫ 1 otherwise. Equipped with qa(s, τa, ua), we can compute the original target y(s, τ , u) for joint Q-value Qtot and projects it into yf (s, τ , u). We define the corresponding shaping function as follows: (cid:40) min a∈ A y(s, τ , u)\n\nqa(s, τa, ua) T (s, u) = 0 and y(s, τ , u) ⩽ Qtot(s, τ , ˆu∗)\n\nyf (s, τ , u) =\n\notherwise\n\n(7)\n\n,\n\nwhere Qtot(s, τ , u; θ) = fs(Q1(τ1, u1), ..., Qn(τn, un)) is the joint Q-values parametrized by θ, ⩾ 0, y(s, τ , u) = r(s, u) + γ maxu′ Qtot(s′, τ ′, u′; θ′), ˆu∗ = arg maxu Qtot(s, u), and θ′ ∂fs ∂Qa denotes the parameters of a target network that are periodically copied from θ.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nThen, we introduce a shaping function f (s, u) to project the target Q(s, τ , u) into the restricted monotonic space that QMIX can represent. In practice, we replace the target Q(s, τ , u) by the minimal value of qa(s, τa, ua) for deterministic and suboptimal joint Q-values. The goal is to ensure that all shaped target value functions at a given state belong to the restricted monotonic space and that the target for the optimal is greater than the suboptimal. We do not change the stochastic targets because QMIX can deal with them already. To improve learning efficiency, we also remain the target unchanged if it exceeds the current greedy action value estimate and is deterministic. Qtot(s, τ , u; θ) is trained by minimising the following loss:\n\nB (cid:88)\n\nb=1\n\n(Qtot(s, τ , u; θ) − Qf\n\nb (s, τ , u))2.\n\n(8)\n\nMonotonic value factorisation with the original target joint Q-values Q induces representational limitation, but it is unclear if the shaped targets do the same. In the following, we give a profound understanding of this. We consider only a fully-observable setting for ease of representation, and our notations do not distinguish the concepts of states and observation-action histories.\n\nTheorem 2 Let Q and Qf be the orignal and the shaped action value based on (7). Then ∀s ∈ S and ∀u ∈ U such that arg maxu Q(s, u) is unique, arg maxu Qf (s, u) = arg maxu Q(s, u) and Qf (s, u) ∈ Qmvf .\n\nThe proof can be found in Appendix A.3. Since all shaped targets Qf (s, u) at a given state s lie in the restricted monotonic space Qmvf , monotonic value factorisation can perfectly represent these values. In addition, the optimal policy remains unchanged during shaping. Therefore, QMIX can now converge to the optimal for any original target Q-value function.\n\nFigure 2: The performance of QMIX with the original and the shaped target values.\n\nIn Figure 2, we compare the performance of QMIX with the original and the shaped target joint Q-values. Consider a 10 × 10 matrix game, where the suboptimal is filled with random numbers generated uniformly between -10 and 9, and the unique optimal value is +10. As shown in Figure 2a-b, the original targets are the rewards from this matrix game, and the shaped targets are obtained based on 7. Figure 2c-d demonstrates that QMIX can perfectly represent all shaped targets and thus converge to the optimal by showing the loss is almost 0 and the test reward is +10 after 400 iterations. By contrast, QMIX with the original targets in Figure 2a keeps fluctuating during training.\n\n5 RELATED WORK\n\nIn this section, we briefly introduce recent related work on cooperative multi-agent reinforcement learning (MARL) in the paradigm of centralised training with decentralised execution (CTDE). One of the most significant challenges in CTDE is to ensure the correspondence between the individual Q-value functions and the joint Q-value function Qtot, i.e., the Individual-Global Max (IGM) principle (Son et al., 2019). VDN (Sunehag et al., 2018) and QMIX (Rashid et al., 2018) learn the joint Q-values and factorise them into individual Q-value functions in an additive and a monotonic fashion, respectively. Qatten (Yang et al., 2020b) is a variant of QMIX, which applies a multi-head attention structure to the mixing network. QPD (Yang et al., 2020a) utilises integrated gradients to decompose Qtot along trajectory paths. SMIX(λ) (Yao et al., 2021) changes the one-step Q-learning target with a SARSA(λ) target for QMIX. RODE (Wang et al., 2020c) decomposes joint action\n\n6\n\nOriginal target valuesShaped target values10-105-50102846U1 U2 U1 U2 (a) Original target values.(b) Shaped target values.(c) Loss for approximating the target values.(d) Reward for the current greedy action.Iterations051015202530Loss02004006008001000Iterations2004006008001000002.557.510QMIX with original targetsQMIX with shaped targetsQMIX with original targetsQMIX with shaped targetsTest rewardUnder review as a conference paper at ICLR 2023\n\nspaces into restricted role action spaces to boost learning efficiency and policy generalisation. These methods apply the same monotonic mixing network and thus can only represent the same class as QMIX. However, as many previous studies pointed out, monotonic value factorisation limits the representational capacity of Qtot, and fail to learn the optimal policy when the target Q-value functions are non-monotonic (Mahajan et al., 2019; Son et al., 2019; Rashid et al., 2020).\n\nSome recent works try to achieve the full representational capacity of Qtot to solve this problem. QPLEX (Wang et al., 2020a) achieves the full representation under the IGM principle theoretically through a duelling mixing network, in which the weights are produced through joint actions. However, it also induces the “vanishing gradient” problem, leading to slow convergence and learning instability. More discussions can be found in Appendix B.2. Deep coordination graph (B ̈ohmer et al., 2020) decompose the joint state-action value into individual utilities and payoff contributions based on the actions of the agents connected by the (hyper-)edges. Tesseract (Mahajan et al., 2021) decomposes the Q-tensor across agents and utilises low-rank tensor approximations to model agent interactions relevant to the task, and thus learns a compact approximation of the target Q-value function. However, since the dimension of the state-action space increases exponentially as the number of agents grows, it is not easy to achieve the full representational capacity in complex MARL tasks.\n\nAnother solution is to learn a biased Qtot by prioritising the optimal joint action. QTRAN (Son et al., 2019) uses two soft regularisations to align the greedy action selections between the joint Q-value and the individual values. WQMIX (Rashid et al., 2020) introduces a weighting mechanism similar to Hysteretic Q-learning (Matignon et al., 2007), which places more importance on better joint actions. QTRAN can be viewed as a variant of WQMIX, which additionally uses the weight 0 for the joint Q-value whose target is smaller than the current estimate and the chosen action is not greedy. QTRAN approximates ˆQ∗ as the target instead of the original target y and thus can deal with stochasticity. However, due to the 0 weight for overestimated Q-values, QTRAN is empirically hard to scale to more complex tasks (Samvelyan et al., 2019). We also prove that there may not exist an appropriate weight when the targets are non-monotonic and stochastic.\n\nIn addition, there have been many developments in policy-based methods under CTDE settings. MAPPO (Yu et al., 2021) applies PPO (Schulman et al., 2017) into MARL and shows strong empirical performance. However, Kuba et al. (2021) points out MAPPO suffers from instability arising from the non-stationarity induced by simultaneously learning and exploring agents. Therefore, they introduce the multi-agent advantage decomposition lemma and the sequential policy update scheme to achieve monotonic improvement on the joint policy, which can converge to one of the Nash Equilibriums. However, the monotonic target in value decomposition methods can be interpreted as a static multivariable optimisation problem, where there are non-optimal solutions that cannot be improved upon by coordinate descent. The sequential policy update scheme cannot guarantee convergence to the optimal in this optimisation problem (Bertsekas, 2019).\n\nRelationship To Reward Shaping. Reward shaping is a common technique for improving singleagent learners’ performance by integrating expert knowledge into MDP (Gullapalli & Barto, 1992). It also has been adopted to the MARL setting in recent work (Devlin et al., 2011; Xiao et al., 2021). Since it is not always possible to find an expert with complete domain knowledge, it is not easy to set rewards for complex MARL tasks in advance. Therefore, adaptively shaping the reward is a more attractive way. Some reward randomisation methods (Gupta et al., 2021; Tang et al., 2021) are proposed to convert the original MDP to a new MDP with random rewards through universal successor features. However, these methods require a massive number of randomisation to find a desirable reward function because they cannot guarantee policy invariance. GVR (Wan et al., 2022) proposes inferior target reshaping and superior experience replay to eliminate the non-optimal self-transition nodes, which is similar to WQMIX and ignores the stochasticity of the transition. Compared with these methods, UTS is more efficient because it can perfectly represent the expected value for stochastic targets and guarantee policy invariance during target shaping.\n\n6 EXPERIMENTS\n\nIn this section, we conduct empirical experiments to answer the following questions: (1) Is UTS better than the existing methods when the target is seriously non-monotonic? (2) Can UTS perform well when the target is non-monotonic and stochastic? (3) Can UTS perform efficient coordination in challenging multi-agent tasks? We compare UTS with value decomposition baselines, including\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nVDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), QTRAN (Son et al., 2019), QPLEX (Wang et al., 2020a) and WQMIX (Rashid et al., 2020) on a matrix game, a multi-agent Markov Decision Process, predator-prey (Son et al., 2019), predator-stag-hare, and StarCraft II Multi-agent Challenge (Samvelyan et al., 2019). We also conduct ablation studies (Appendix C) and compare UTS with GVR (Wan et al., 2022) and WQMIX with different weights (Appendix D), as well as more CTDE-based algorithms (Appendix E). More detailed experimental settings are included in Appendix F.\n\n6.1 MATRIX GAMES\n\nIn this subsection, we consider a matrix game in Figure 3a, where the local optimal is difficult to jump out due to the considerable miscoordination penalties and stochastic reward. The optimal joint strategy is to perform (u1 2) simultaneously. We adopt a uniform exploration strategy to approximate the uniform data distribution and eliminate the challenge of exploration and sample complexity. As shown in Figure 3a, UTS and QTRAN can achieve optimal performance, while other algorithms fall into a local optimum. CW-QMIX fluctuates dramatically and cannot converge to any policy, which is consistent with our analysis in Section 3.2.\n\n1, u1\n\nFigure 3: Mean test return on a stochastic matrix game and an MMDP.\n\nWe also consider a multi-agent Markov Decision Process (MMDP) with non-monotonic and stochastic targets in Figure 3b. This MMDP involves two agents, three actions and a team reward. Two agents start at state s1 and explore extrinsic rewards for ten environment steps. The state transits from s1 to s2 with probability 50% and s3 with 50% if agents perform u = (u2 2) at s1, and remains unchanged otherwise. The team continually obtains a deterministic reward of +10 and -10 at s2 and s3, respectively. In this task, since VDN, QMIX, and QPLEX estimates the stochastic targets well but cannot deal with the non-monotonicity, they converge to the suboptimal with the return of +60. The negative results for CW-QMIX and OW-QMIX show that they converge to the stochastic suboptimal due to the undesirable weights. By contrast, UTS and QTRAN can achieve optimal performance.\n\n1, u2\n\n6.2\n\nPREDATOR-PREY\n\nPredator-prey (PP) is a partially observable environment containing eight predators (agents) and eight prey in a 10 × 10 grid world. Each agent observes a 5 × 5 sub-grid around it and can perform five actions, i.e., up, down, left, right, and catch. The team receives a miscoordination penalty when two agents surround a prey and only one tries to catch it. By contrast, they earn a bonus of 10 if they catch the prey simultaneously. After a successful catch, the catching predators and prey will be removed from the grid. Figure 4a-b shows that VDN, QMIX, and QPLEX fail to return positive returns with a significant penalty because the penalty makes these methods underestimate the per-agent action value of the real optimal. CW-QMIX and OW-QMIX can achieve positive test returns when the penalty is -2 because they place more weight on better joint actions. QTRAN can also solve this task through its regularisation terms. However, CW-QMIX, OW-QMIX, and QTRAN fail to solve the task when the penalty is -10, suggesting difficulties in choosing the pre-defined weight and the network architecture. By contrast, UTS solves tasks quickly and steadily in all penalty settings.\n\nWe extend predator-prey to a stochastic version named predator-stag-hare (PSH), which contains eight predators, four stags and four hares. The agents receive the same miscoordination penalty in predator-prey when they try to catch a stag. By contrast, a catch of hare results in a non-deterministic reward, i.e., the probabilities of receiving SR∈ R+ and -SR are both 50%. As shown in Figure 4c-d, VDN and QMIX can quickly recover the optimal policy when the penalty is 0 because they can\n\n8\n\n(a) A non-monotonic game with stochastic reward.s2 s2 s3 s3 u≠(u12,u22) P s2 s1,u12,u22 =P s3 s1,u12,u22 =0.5 r=−10 r=+10 -128-12666-12-1212/0u11 U1 U2 u21 u12 u13 u22 u23 -128-12666-12-1212/0u11 U1 U2 u21 u12 u13 u22 u23 (b) A non-monotonic MMDP with stochastic state transition.s1 s1 -128-12666-12-126u11 U1 U2 u21 u12 u13 u22 u23 -128-12666-12-126u11 U1 U2 u21 u12 u13 u22 u23 The payoff matrix at .s1 The payoff matrix at .s1 -128-12666-12-126u11 U1 U2 u21 u12 u13 u22 u23 The payoff matrix at .s1 Under review as a conference paper at ICLR 2023\n\naccurately evaluate the stochastic reward. When the penalty is -2, VDN, QMIX, QPLEX, and WQMIX fail to solve this task. QTRAN also struggles with the suboptimal in this setting. By contrast, UTS takes advantage of the uncertainty estimation module to avoid overestimating the stochastic action values, and uses the shaping function to eliminate the adverse impact of the miscoordination penalty. Therefore, it learns quickly and reliably in these tasks. The detailed comparison between UTS and WQMIX with different weights can be found in Appendix D.\n\nFigure 4: Mean test return on predator-prey and predator-stag-hare.\n\n6.3 STARCRAFT MULTI-AGENT CHALLENGE\n\nWe also compare UTS with baselines on eight hard and super hard maps in the decentralised micromanagement tasks in StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019). Figure 5 shows the improved performance of UTS, which indicates that the target shaping function can be applied to more general benchmarks.\n\nFigure 5: Mean test win rate on SMAC tasks.\n\n7 CONCLUSION AND FUTURE WORK\n\nThis paper presents uncertainty-based target shaping (UTS), which was inspired by analysing the limitation of Weighted QMIX. We prove that there may not exist an appropriate weighting function for Weighted QMIX to recover the optimal policy when the targets are stochastic and non-monotonic. To solve this problem, UTS first identifies whether a target value is stochastic by predicting the standard deviation of the reward and the prediction error between the predicted and the target embedding of the following state. Then we propose a shaping function to project original target values into the monotonic space that QMIX can represent by replacing the deterministic suboptimal target with the minimal best per-agent values. Empirical results demonstrate the improved ability of UTS to deal with the task with non-monotonic and stochastic targets, as well as more complicated tasks. The assumption of “unique optimal policy” is required in our proofs. In the environment with multiple optimal, the more efficient exploration method could be considered, as opposed to the simplistic ε-greedy exploration scheme we used in this paper.\n\n9\n\n00007LPHVWHSV0HDQ7HVW5HWXUQD333HQDOW\\ 00007LPHVWHSV0HDQ7HVW5HWXUQE333HQDOW\\ 00007LPHVWHSV1XPEHURI&DSWXUHG6WDJVF36+65 3HQDOW\\ 00007LPHVWHSV1XPEHURI&DSWXUHG6WDJVG36+65 3HQDOW\\ 876&:40,;2:40,;43/(;475$140,;9'1876&:40,;2:40,;43/(;475$140,;9'100007LPHVWHSV0HDQ7HVW:LQ5DWHDFBYVB]J00007LPHVWHSV0HDQ7HVW:LQ5DWHE00000007LPHVWHSV0HDQ7HVW:LQ5DWHFVBYVB]00007LPHVWHSV0HDQ7HVW:LQ5DWHGPBYVBP000007LPHVWHSV0HDQ7HVW:LQ5DWHHV]BYVBV]00007LPHVWHSV0HDQ7HVW:LQ5DWHIPBYVBP000007LPHVWHSV0HDQ7HVW:LQ5DWHJKBYVB]000007LPHVWHSV0HDQ7HVW:LQ5DWHKFRUULGRU876&:40,;2:40,;43/(;475$140,;9'1876&:40,;2:40,;43/(;475$140,;9'1Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSally Aladdin, Samah El-Tantawy, Mostafa M. Fouda, and Adly S. Tag Eldien. MARLA-SG: multiagent reinforcement learning algorithm for efficient demand response in smart grid. IEEE Access, 8:210626–210639, 2020.\n\nDimitri Bertsekas. Multiagent rollout algorithms and reinforcement learning. arXiv preprint\n\narXiv:1910.00120, 2019.\n\nWendelin B ̈ohmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International\n\nConference on Machine Learning, pp. 980–991. PMLR, 2020.\n\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network\n\ndistillation. arXiv preprint arXiv:1810.12894, 2018.\n\nXinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Bestaction imitation learning for batch deep reinforcement learning. Advances in Neural Information Processing Systems, 33:18353–18363, 2020.\n\nArmen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? Structural safety,\n\n31(2):105–112, 2009.\n\nSam Devlin, Daniel Kudenko, and Marek Grze ́s. An empirical study of potential-based reward shaping and advice in complex, multi-agent systems. Advances in Complex Systems, 14(02): 251–278, 2011.\n\nJakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial intelligence, volume 32.1, 2018.\n\nJakob N Foerster, Yannis M Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 2145–2153, 2016.\n\nVijaykumar Gullapalli and Andrew G Barto. Shaping as a method for accelerating reinforcement learning. In Proceedings of the 1992 IEEE international symposium on intelligent control, pp. 554–559. IEEE, 1992.\n\nTarun Gupta, Anuj Mahajan, Bei Peng, Wendelin B ̈ohmer, and Shimon Whiteson. Uneven: Universal value exploration for multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 3930–3941. PMLR, 2021.\n\nAlex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer\n\nvision? Advances in neural information processing systems, 30, 2017.\n\nLandon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for\n\ndecentralized planning. Neurocomputing, 190:82–94, 2016.\n\nJakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv preprint arXiv:2109.11251, 2021.\n\nSheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deep implicit coordination graphs for multi-agent reinforcement learning. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 764–772, 2021.\n\nAnuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent\n\nvariational exploration. Advances in Neural Information Processing Systems, 32, 2019.\n\nAnuj Mahajan, Mikayel Samvelyan, Lei Mao, Viktor Makoviychuk, Animesh Garg, Jean Kossaifi, Shimon Whiteson, Yuke Zhu, and Animashree Anandkumar. Tesseract: Tensorised actors for multiagent reinforcement learning. In International Conference on Machine Learning, pp. 7301–7312. PMLR, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nLa ̈etitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Hysteretic q-learning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 64–69. IEEE, 2007.\n\nFrans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs. Springer,\n\n2016.\n\nFrans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value functions\n\nfor decentralized pomdps. Journal of Artificial Intelligence Research, 32:289–353, 2008.\n\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018.\n\nTabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in neural information processing systems, 33:10199–10210, 2020.\n\nMikayel Samvelyan, Tabish Rashid, Christian Schr ̈oder de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob N Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. In AAMAS, 2019.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nMax Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular\n\nmdps. Advances in Neural Information Processing Systems, 32, 2019.\n\nKyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 5887–5896. PMLR, 2019.\n\nPeter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085–2087, 2018.\n\nZhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon Shaolei Du, Yu Wang, and Yi Wu. Discovering diverse multi-agent strategic behavior via reward randomization. In International Conference on Learning Representations, 2021.\n\nLipeng Wan, Zeyang Liu, Xingyu Chen, Xuguang Lan, and Nanning Zheng. Greedy based value representation for optimal coordination in multi-agent reinforcement learning. In Proceedings of the 39th International Conference on Machine Learning, volume 162, pp. 22512–22535. PMLR, 17–23 Jul 2022.\n\nJianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling\n\nmulti-agent q-learning. In International Conference on Learning Representations, 2020a.\n\nJianhao Wang, Zhizhou Ren, Beining Han, Jianing Ye, and Chongjie Zhang. Towards understanding\n\ncooperative multi-agent q-learning with value factorization, 2021.\n\nTonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Roma: Multi-agent reinforcement\n\nlearning with emergent roles. arXiv preprint arXiv:2003.08039, 2020b.\n\nTonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang. Rode: Learning roles to decompose multi-agent tasks. arXiv preprint arXiv:2010.01523, 2020c.\n\nYihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Off-policy multiagent decomposed policy gradients. In International Conference on Learning Representations, 2020d.\n\nBaicen Xiao, Bhaskar Ramasubramanian, and Radha Poovendran. Shaping advice in deep multi-agent\n\nreinforcement learning. arXiv preprint arXiv:2103.15941, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYaodong Yang, Jianye Hao, Guangyong Chen, Hongyao Tang, Yingfeng Chen, Yujing Hu, Changjie Fan, and Zhongyu Wei. Q-value path decomposition for deep multiagent reinforcement learning. In International Conference on Machine Learning, pp. 10706–10715. PMLR, 2020a.\n\nYaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. CoRR, abs/2002.03939, 2020b.\n\nXinghu Yao, Chao Wen, Yuhui Wang, and Xiaoyang Tan. Smix (λ): Enhancing centralized value functions for cooperative multiagent reinforcement learning. IEEE Transactions on Neural Networks and Learning Systems, 2021.\n\nChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.\n\nMeng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit assignment for cooperative multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 33:11853–11864, 2020.\n\nMing Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang, Montgomery Alban, IMAN FADAKAR, Zheng Chen, et al. Smarts: An open-source scalable multi-agent rl training school for autonomous driving. In Conference on Robot Learning, pp. 264–285. PMLR, 2021.\n\nA OMITTED PROOFS\n\nA.1 WVDN AND WEIGHTED LINEAR REGRESSION PROBLEM\n\nWVDN operator is defined as\n\nΠWVDNQ := arg min Qtot∈Qlvf\n\n(cid:88)\n\nu∈U\n\nwow(s, u)(y(s, u) − Qtot(s, u))2,\n\n(9)\n\nwhere Qlvf := {Qtot|Qtot(s, u) = (cid:80)n weighting:\n\na=1 Qa(s, ua), Qa(s, u) ∈ R}, wow(s, u) is the optimistic\n\nwow(s, u) =\n\n(cid:26)1\n\ny(s, u) > Qtot(s, u)\n\nα otherwise\n\n.\n\nThe optimising process of WVDN can be constructed as a weighted linear regression problem:\n\n√\n\n∥\n\nw⊤ ·\n\n(cid:112)\n\nmin x\n\np⊤ · (Ax − b)∥,\n\n(10)\n\n(11)\n\nwhere A ∈ Rmn×mn, x ∈ Rmn, w,p,b ∈ Rmn size for each agent, n denotes the number of agents. w⊤ · (cid:112)p⊤ · A, bw =\n\nDenote Aw =\n\n√\n\n√\n\nw⊤ · (cid:112)p⊤ · b. The optimal solution is:\n\n, m,n ∈ Z +. In particular, m dentoes the action space\n\nx∗ = Aw,†bw,\n\n(12)\n\nwhere Aw,† = V S†U ⊤ is the pseudo-inverse of Aw according to the Singular Value Decomposition. U ∈ Rmn×mn and V ∈ Rmn×mn are real orthogonal matrices, S† ∈ Rmn×mn is a diagonal matric with non-negative real numbers on the diagonal.\n\nThe proof is trivial.\n\nConsider the full exploration scheme (i.e., ε-greedy exploration with ε = 1), and the non-monotonic game in Figure 6a, where a = 8, b = −12, c = 6.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFor VDN (a special case of WVDN where α = 1), we have\n\nA =\n\n\n\n \n \n \n \n \n\n\n1 1\n1 0\n0 0\n0 0\n0\n\n0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0\n\n0 1\n0 0\n1 0\n0 1\n0\n\n\n\n \n \n \n \n \n\n\n0 0\n1 0\n0 1\n0 0\n1\n\n, x =\n\nAccording to (12), the optimal solution is\n\n\n\n \n \n \n\n\n\n \n \n \n\nQ1(s, u1 1) Q1(s, u2 1) Q1(s, u3 1) Q2(s, u1 2) Q2(s, u2 2) Q2(s, u3 2)\n\n, b =\n\n\n\n \n \n \n \n \n\n\n8 −12 −12 −12 6\n6 −12 6\n6\n\n\n\n \n \n \n \n \n\n\n, p = w =\n\n\n\n \n \n \n \n \n\n\n.\n\n(13)\n\n\n\n \n \n \n \n \n\n\n1 1\n1 1\n1 1\n1 1\n1\n\nx∗ = (−4.44\n\n0.89\n\n0.89 −4.44\n\n0.89\n\n0.89)⊤ .\n\n(14)\n\nClearly, Qi(u1 to the optimal.\n\ni ) < max{Qi(u2\n\ni ), Q1(u3\n\ni )}, ∀i ∈ {1, 2}, which indicates that VDN fails to converge\n\nA.2 OMITTED PROOFS IN SECTION 3\n\nWVDN operator is defined in Section A.1.\n\nDenote the joint action u = [u1, ..., ua, ..., un], where n is the number of agents. Denote ui i-th action of ua.\n\na as the\n\nConsider matrix games in Figure 6. Suppose the training dataset is fixed and the data has a ε-greedy distribution, where the greedy action is (u2\n\n2) and ε ∈ (0, 1].\n\n1, u2\n\nBased on the ε-greedy distribution, the probabilities of each joint action p(ui\n\n1, uj\n\n2) is:\n\nP r(ui\n\n1, uj\n\n2) =\n\n\n\n \n\n,\n\nε2 9\n(3 − 2ε)2 9\n(3 − 2ε)ε 9\n\n,\n\n,\n\n(i, j) ∈ {(1, 1), (1, 3), (3, 1), (3, 3)},\n\n(i, j) = (2, 2),\n\notherwise.\n\n(15)\n\n(a) is a non-monotonic matrix game, where a > c > b; Figure 6: Payoffs of matrix games. (b) is a stochastic matrix game, where (u2 2) receives c with probability p and d with (1 − p), c > a > max{b, d, e} and a > pc + (1 − p)d; (c-d) are two numerical examples for stochastic matrix games, where the reward in (c) is monotonic and in (d) is non-monotonic.\n\n1, u2\n\nTo recover the optimal policy, the weight α for WVDN is bounded by ε and the reward space. We start by analysing the matrix game in Figure 6a. Since the matrix game only involves one state, we omit s for the input of value functions in the following proofs.\n\nProposition 1 For WVDN, the weight α should be smaller than 3ε(a−c)+ε2(c−b) optimal joint policy on the non-monotonic matrix game in Figure 6a.\n\n(ε−3)2(c−b)\n\nto recover the\n\n13\n\n(a) A non-monotonic matrix game.babcccbbc(b) A stochastic matrix game.babeeebbc/d(c) Payoff of a stochastic matrix game.6866666612/0-128-12666-12-12-128-12666-12-1212/0(d) Payoff of a stochastic and non-monotonic game.(d) Payoff of a stochastic and non-monotonic game.u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 Under review as a conference paper at ICLR 2023\n\nProof. First compute the gradient for Q(u1\n\n1), Q(u2\n\n1), Q(u3\n\n1):\n\n▽Q(u1\n\n1) =\n\nε 9\n\n[(ε + (3 − ε)α)Q(u1\n\n1) + εQ(u1\n\n2) + (3 − 2ε)αQ(u2\n\n2) + εαQ(u3 2)\n\n− εa − α(3 − ε)b]\n\n(3 − 2ε)α 9\n\n▽Q(u2\n\n1) =\n\n▽Q(u3\n\n1) =\n\nεα 9\n\n[3Q(u2\n\n1) + ε(Q(u1\n\n2) + Q(u3\n\n2)) + (3 − 2ε)Q(u2\n\n2) − εb − (3 − ε)c]\n\n[3Q(u3\n\n1) + ε(Q(u1\n\n2) + Q(u3\n\n2)) + (3 − 2ε)Q(u2\n\n2) − εb − (3 − ε)c]\n\nLet ▽Q(u1\n\n1) = ▽Q(u2\n\n1) = ▽Q(u3 εa + α(3 − ε)b − εQ(u1\n\n1) = 0:\n\nQ(u1\n\n1) =\n\n2) − (3 − 2ε)αQ(u2 ε + 3α − εα\n\n2) − εαQ(u3 2)\n\nQ(u2\n\n1) = Q(u3\n\n1) =\n\nεb + (3 − ε)c − ε(Q(u1\n\n2) + Q(u3\n\n2)) − (3 − 2ε)Q(u2 2)\n\n3\n\nTo recover the optimal joint policy, the following conditions should be satisfied:\n\nQ(u1\n\n1) − max{Q(u2\n\n1), Q(u3\n\n1)} > 0, Q(u1\n\n2) − max{Q(u2\n\n2), Q(u3\n\n2)} > 0\n\nDenote t = ε + 3α − εα. Then\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\nQ(u1\n\n1), Q(u3 1) − max{Q(u2 εa + α(3 − ε)b − εQ(u1\n\n1)} 2) − (3 − 2ε)αQ(u2\n\n2) − εαQ(u3 2)\n\n=\n\n−\n\n<\n\nεb + (3 − ε)c − ε(Q(u1\n\n2)) − (3 − 2ε)Q(u2 2)\n\nt 2) + Q(u3\n\n3\n\n(\n\n(cid:124)\n\nt + εα − ε − 3α t\n(cid:123)(cid:122) = 0\n\n)\n\nQ(u1 (cid:125)\n\n2) +\n\nε t\n\na +\n\n(9 − 3ε)α − εt 3t\n\nb +\n\nε − 3 3\n\nc.\n\n(22)\n\nLet Q(u1\n\n1) − max{Q(u2 Then α < 3ε(a−c)+ε2(c−b)\n\n(ε−3)2(c−b)\n\n. □\n\n1), Q(u3\n\n1)} > 0 and Q(u1\n\n2) − max{Q(u2\n\n2), Q(u3\n\n2)} > 0.\n\nProposition 2 For WVDN, the weight α should be large enough to recover the optimal joint policy for the stochastic matrix game in Figure 6b.\n\nu∈U wow(s, u)(Q(s, u) − Qtot(s, u))2 and Theorem 1 Let ΠWVDNQ := arg minQtot∈Qlvf wow(s, u) is the optimistic weighting from (4). Then ∃Q such that arg max ΠWVDNQ ̸= arg max Q for any α ∈ (0, 1].\n\n(cid:80)\n\nProof. Since the proof of Proposition 2 and Theorem 1 contains a significant overlap, we will merge them both into a single proof.\n\nFirst compute the gradient for Q(u1\n\n1), Q(u2\n\n1), Q(u3\n\n1):\n\n▽Q(u1\n\n1) =\n\nε 9\n\n[(ε + 3α − εα)Q(u1\n\n1) + εQ(u1\n\n2) + (3 − 2ε)αQ(u2\n\n2) + εαQ(u3\n\n2)−\n\nεa − α(3 − ε)b]\n\n▽Q(u2\n\n1) =\n\n3 − 2ε 9\n\n[((3 − 2ε)(1 − α)p + 3α)Q(u2\n\n1) + εα(Q(u1\n\n2) + Q(u3\n\n2))\n\n+ (3 − 2ε)(p + α − pα)Q(u2\n\n2) − εαb − (3 − 2ε)(pc + αd − αpd) − εαe]\n\n[3Q(u3\n\n1) + ε(Q(u1\n\n2) + Q(u3\n\n2)) + (3 − 2ε)Q(u2\n\n2) − εb − (3 − ε)e]\n\n▽Q(u3\n\n1) =\n\nεα 9\n\n(23)\n\n(24)\n\n(25)\n\nTo recover the optimal joint policy, the following requirements should be satisfied:\n\nQ(u1\n\n1) − max{Q(u2\n\n1), Q(u3\n\n1)} > 0, Q(u1\n\n2) − max{Q(u2\n\n2), Q(u3\n\n2)} > 0\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nLet ▽Q(u1 Then\n\n1) = ▽Q(u2\n\n1) = ▽Q(u3\n\n1) = 0. Denote t = ε + 3α − εα, m = (3 − 2ε)(1 − α)p + 3α.\n\nQ(u1\n\n1) − Q(u2\n\n1) <\n\nQ(u1\n\n1) − Q(u3\n\n1) <\n\nε t\nε t\n\na +\n\na +\n\n(3 − ε)α t\n(3 − ε)α t\n\nb −\n\nb −\n\nεα m\nε 3\n\nb −\n\n(3 − ε)p 3\n\ne.\n\n(b + e) −\n\n(3 − 2ε)p m\n\nc −\n\n(3 − 2ε)(1 − p)α m\n\nd,\n\n(26)\n\n(27)\n\nFor the matrix game in Figure 6b, consider the following situations:\n\n(1) The target is monotonic but stochastic. For example, in Figure 6c, a = 8, b = e = 6, c = 12, d = 0, and p = 0.5. To recover the optimal joint action, the weight α should be greater than (0.434, 0.667, 0.827, 0.932) when ε = (1, 0.75, 0.5, 0.25), respectively.\n\n(2) The target is non-monotonic and stochastic. For example, in Figure 6d, a = 8, b = −12, c = 12, d = 0, e = 6, and p = 0.5. When ε = (1, 0.75, 0.5, 0.25), there does not exist the weight α ∈ (0, 1] making Q(u1 Thus, ∃Q such that arg max ΠWVDNQ ̸= arg max Q for any α ∈ (0, 1]. □\n\n1)} > 0, i.e., WVDN cannot converge to the optimal.\n\n1) − max{Q(u2\n\n1), Q(u3\n\nMore experimental results can be found in Appendix B.1.\n\nA.3 OMITTED PROOFS OF RESHAPING FUNCTION\n\nTheorem 2 Let Q and Qf be the orignal and the shaped action value based on (7). Then ∀s ∈ S and ∀u ∈ U such that arg maxu Q(s, u) is unique, arg maxu Qf (s, u) = arg maxu Q(s, u) and Qf (s, u) ∈ Qmvf .\n\nProof. Considering only a fully-observable setting for ease of representation. Thus our notations do not distinguish the concepts of states and observation-action histories. In addition, when more than one optimal policy exists, most Q-learning algorithms fail to converge to a stable point (Simchowitz & Jamieson, 2019; Wang et al., 2020a; Rashid et al., 2020). Thus, consider a s ∈ S, we assume u∗\n\nQ = arg maxu Q(s, u) is unique.\n\nAssume maxu−a Q(s, ua, u−a) for all ua are perfectly represented. Then Q(s, u) ⩽ mina∈ A{maxu−a Q(s, ua, u−a)}. By the shaping function from (7): \n\n\nQf (s, u∗\n\nQ) = Q(s, u∗ Q) Qf (s, u) = min{max u−i\n\n\n\nQ(s, ui, u−i)}, ∀ui ∈ u ̸= u∗\n\nQ and ui /∈ u∗\n\nQ\n\n.\n\n(28)\n\nUnder the assumption of the unique optimal action, we have Qf (s, u∗ Therefore, arg maxu Qf (s, u) = arg maxu Q(s, u). Suppose ∃fs(s, Q1(s, u1), ..., Qn(s, un)) = Qf (s, u), ∀u = (u1, ..., un) ∈ U. Then\n\nQ) > Qf (s, u) for all u ̸= u∗ Q.\n\n \n\n\n\nfs(s, Qa(ua), Q−a(u−a)) = min{max u−a\n\nQ(s, ua, u−a), max u−i\n\nQ(s, ui, u−i)}, ∀i ∈ −a\n\nfs(s, Qa(u′\n\na), Q−a(u−a)) = min{max u−a\n\nQ(s, u′\n\na, u−a), max u−i\n\nQ(s, ui, u−i)}, ∀i ∈ −a\n\n,\n\n(29)\n\nwhere Q−a(u−a) = {Qi(s, ui)}i∈−a. Let Qa(s, ua) ⩾ Qa(s, u′ Then, according to (29), fs(s, Qa(ua), Q−a(u−a)) ⩾ fs(s, Qa(u′ Thus we can find fs(s, Q1(s, u1), ..., Qn(s, un)) to represent Qf (s, u), ∀u = (u1, ..., un) ∈ U, where ∂fs ∂Qa\n\na) if maxu−a Q(s, ua, u−a) ⩾ maxu−a Q(s, u′\n\n⩾ 0. And so Qf (s, u) ∈ Qmvf , ∀s ∈ S, ∀u ∈ U. □\n\na, u−a), ∀a ∈ A.\n\na), Q−a(u−a)).\n\nNow we talk about the case of multiple optimal. When more than one optimal policy exists in the Markov Decision Process, many value-based algorithms fail to converge on a stable point (Simchowitz & Jamieson, 2019). However, multiple optimal may be very common in a multi-agent setting. For\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nexample, when two agents arrive at a crossroads, the optimal solution is that one of them chooses to move and another chooses to make way. In this situation, the team can solve this simple task no matter which agent chooses to move.\n\nFigure 7: (a) The payoff matrix for a game with multiple optimal. (b-h) The estimated joint Q-values Qtot returned from QMIX, VDN, and UTS with different exploration rate. Boldface means the optimal joint action from the payoff matrix, or the greedy joint action from the Qtot.\n\nWe take a simple one-step matrix game with multiple optimal as an example. The payoff matrix is shown in Figure 7a. We investigate the performance of VDN, QMIX, QPLEX, and UTS on this simple task. Since this matrix game is non-monotonic, the target values cannot be perfectly represented in Qlvf and Qmvf . When the full exploration scheme (i.e., ε-greedy with ε = 1) is applied, none of these methods can converge to the optimal. Although QPLEX achieves the full representational capacity (i.e., the joint Q-values Qtot is the same as the payoff), it still cannot converge because all per-agent Q-values are optimal (because of the Individual-Global-Max). However, in practice, we can solve this problem by ε-greedy with decayed ε. In particular, we show the Qtot returned from UTS to illustrate the effect of the ε. As shown in Figure 7e-h, we can see the gap between the suboptimal and the optimal increases with the decrease of the ε, which improves the learning stability. However, UTS inherits the characteristic of QMIX and thus is not a contraction, i.e., it would return two distinct Qtot and converge to any one of the optimal.\n\nB RESULTS ON MATRIX GAMES\n\nB.1 WQMIX WITH DIFFERENT WEIGHTS\n\nIn Section 3.1, we demonstrate the complete data of minimal 1/α for WVDN and WQMIX to recover the optimal policy, where α denotes the weight on the suboptimal. The payoff matrix is shown in 6a. The dataset is collected by the ε-greedy exploration, where the greedy action is set to us = (u2 2).\n\n1, u2\n\nFigure 8: (a)-(b) shows the performance of WVDN and WQMIX on the non-monotonic game in Figure 6a, and (c)-(d) shows the performance on the non-monotonic and stochastic game in Figure 9a.\n\nWe select a = 8, b = −12, and (1) three reward assignments, c ∈ {−3, 3, 6}, ε = 0.25, as well as (2) three exploration rate assignments, ε ∈ {0.5, 0.75, 1}, c = 6, and run WVDN and WQMIX with five random seeds on a fixed dataset. We show the gap between the approximated real optimal action value ˆQ1(u∗ 1) for agent 1, which should be zero if the algorithm converges to the optimal. Figure 8a-b shows that the gap reduces with the increase of 1/α, indicating\n\n1) and greedy action value ˆQ1(ˆu∗\n\n16\n\n(a) Payoff of a game with multiple optimal.u11 U1 U2 u21 u12 u22 0.510.51u11 U1 U2 u21 u12 u22 0.510.51u11 U1 U2 u21 u12 u22 0.750.750.760.76u11 U1 U2 u21 u12 u22 0.750.750.760.76u11 U1 U2 u21 u12 u22 0.750.740.740.75u11 U1 U2 u21 u12 u22 0.750.740.740.75u11 U1 U2 u21 u12 u22 0.950.950.970.97u11 U1 U2 u21 u12 u22 0.950.950.970.97u11 U1 U2 u21 u12 u22 0.960.950.961.02u11 U1 U2 u21 u12 u22 0.960.950.961.02u11 U1 U2 u21 u12 u22 0.930.920.921.02u11 U1 U2 u21 u12 u22 0.930.920.921.02u11 U1 U2 u21 u12 u22 0.760.730.761.01u11 U1 U2 u21 u12 u22 0.760.730.761.01u11 U1 U2 u21 u12 u22 0.650.610.661.00u11 U1 U2 u21 u12 u22 0.650.610.661.00(b) QMIX, 1.(c) VDN, 1.(d) UTS, 1.(e) UTS, 0.75.(f) UTS, 0.5.(g) UTS, 0.25.(h) UTS, 0.05.ε= ε= ε= ε= ε= ε= ε= ε= ε= ε= ε= ε= ε= ε= 1u1∗=u 1∗ 1/α 1/α (c) The value gap between the real optimal action and the greedy action. (d) The indicator function that shows whether the greedy action is the optimal. Q 1 u1∗ −Q 1 u 1∗ (a) The performance with different c, where a=8,b=−12,ε=0.25. 1/α 1/α (b) The performance with different ε, where a=8,b=−12,c=6. c=6 c=3 c=−3 c=6 c=3 c=−3 ε=0.5 ε=0.75 ε=1.0 ε=0.5 ε=0.75 ε=1.0 Q 1 u1∗ −Q 1 u 1∗ Q 1 u1∗ −Q 1 u 1∗ Under review as a conference paper at ICLR 2023\n\nthat the weight α should be small enough to deal with non-monotonicity. In addition, we can see that α should be small enough to converge to the optimal when the suboptimal value is large or the exploration rate is low, which is consistent with our analysis.\n\nIn Section 3.2 and Section A.2, we prove that the contradiction of the choice of α may result in converging on the suboptimal for WQMIX. We also conduct experiments to verify our analysis. We consider the matrix game in Figure 9a, and train WVDN and WQMIX on a fixed dataset. The dataset is collected by the ε-greedy exploration, where the greedy action is set to us = (u2 2) and ε = 0.5. We show the gap between the approximated real optimal action value ˆQ1(u∗ 1) and greedy action value ˆQ1(ˆu∗ 1) for agent 1, which should be zero if the algorithm converges to the optimal. We also show whether the greedy action is optimal by an indicator function 1 ˆQ1(u∗\n\n1, u2\n\n1 )= ˆQ1(ˆu∗\n\n1 ).\n\nAs shown in Figure 8c-d, WQMIX and WVDN cannot converge to the optimal with different α when the payoff is non-monotonic and stochastic. Since CW-QMIX does not place α on the current greedy joint action, it can get rid of the stochastic suboptimal us when us is the current greedy action, i.e., ˆu∗ = us. However, with the decrease of Qtot(us), CW-QMIX falls into this suboptimal again once ˆu∗ ̸= us. Therefore, we can see that the gap between ˆQ1(u∗ 1) is tiny. However, as shown in Figure 8d, CW-QMIX cannot converge to the optimal, which is consistent with our analysis.\n\n1) and ˆQ1(ˆu∗\n\nB.2 THE APPROXIMATED JOINT ACTION-VALUES\n\nWe also show the joint Q-value returned from our baselines on a matrix game with non-monotonic and stochastic rewards in Figure 9a. We adopt a uniform exploration strategy to approximate the uniform data distribution (i.e., ε-greedy exploration with ε = 1). As shown in Figure 9, UTS and QTRAN can recover the optimal policy, while other methods fail to solve this simple task. Since we have discussed QMIX and WQMIX in previous sections, here we analyse QPLEX and GVR.\n\nFigure 9: The results on a matrix game with non-monotonic and stochastic rewards. Boldface means the optimal joint action from the payoff matrix, or the greedy joint action from the Qtot.\n\nIn QPLEX, the joint Q-value is:\n\nQtot(τ , u) = Vtot(τ ) + Atot(τ , u) =\n\nn (cid:88)\n\ni=1\n\nVi(τ ) +\n\nn (cid:88)\n\ni=1\n\nλi(τ , u)Ai(τ , ui)\n\nn (cid:88)\n\n[wi(τ )Vi(τi) + bi(τ )] +\n\n=\n\ni=1\n\nn (cid:88)\n\ni=1\n\nλi(τ , u)[Qi(τ , ui) − Vi(τ )]\n\n(30)\n\n1, ..., u∗\n\ni , ..., u∗\n\nwhere Vi(τi) = maxui Qi(τi, ui), wi(τ ) ≥ 0, λi(τ , u) > 0. Let u∗ = [u∗ n] denote the optimal and one of the suboptimal joint action, where n is the number of agents. If QPLEX is initialised to this joint action, i.e., Vtot(τ ) = (cid:80)n i ), it is difficult to jump out from this suboptimal for QPLEX. Consider Q(s, us) is the real second greatest action value at a given state, where us i , ∀i ∈ A. Therefore, joint Q-values whose targets are smaller than Qtot(τ , us) can be perfectly represented by QPLEX.\n\ni=1 Qi(τi, us\n\nn], us = [us\n\ni , ..., us\n\n1, ..., us\n\ni ̸= u∗\n\n17\n\n(a) A game with non-monotonic and stochastic rewards.1.681.681.686.331.685.931.6816.86.33u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 1.681.681.686.331.685.931.6816.86.33u11 U1 U2 u21 u12 u13 u22 u23 866666666u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 866666666u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 -128-12666-12-12-128-12666-12-12u11 U1 U2 u21 u12 u13 u22 u23 -128-12666-12-12(e) QMIX (w=1).(e) QMIX (w=1).-8.29u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 2.722.722.725.972.725.972.725.975.97u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 2.722.722.725.972.725.972.725.975.97u11 U1 U2 u21 u12 u13 u22 u23 (h) QPLEX.(h) QPLEX.4.37-8.06-6.931.61-8.454.37-8.094.374.37u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 4.37-8.06-6.931.61-8.454.37-8.094.374.37u11 U1 U2 u21 u12 u13 u22 u23 α α 20-20(b) UTS.(c) OW-QMIX (w=0.1).α α (c) OW-QMIX (w=0.1).α (d) CW-QMIX (w=0.1).α α (d) CW-QMIX (w=0.1).α -8.29-8.29-8.29-8.291.495.004.648.155.315.335.356.035.336.035.356.036.03u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 5.315.335.356.035.336.035.356.036.03u11 U1 U2 u21 u12 u13 u22 u23 (g) QTRAN.(g) QTRAN.8.09-12.32-12.32-2.27-12.245.76-12.245.765.62u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 8.09-12.32-12.32-2.27-12.245.76-12.245.765.62u11 U1 U2 u21 u12 u13 u22 u23 (f) GVR.Under review as a conference paper at ICLR 2023\n\nHowever, when (τ , u∗) is sampled, QPLEX tries to minimising the following loss:\n\nn (cid:88) (\n\ni=1\n\nλi(τ , u∗)[Qi(τ , u∗\n\ni ) − Vi(τ )] − [Q(τ , u∗) − Vtot(τ )])2,\n\n(31)\n\ni ) − Vi(τ ) ≤ 0, and λi(τ , u∗) > 0. As a result, λi(τ , u∗) where Q(τ , u∗) − Vtot(τ ) > 0, Qi(τ , u∗ decreases to almost zero, and Qi(τ , u∗ i ) gradually reaches to Vi(τ ). However, the process is very time consuming and Qi(τ , u∗ i ) cannot exceed Vi(τ ) in most cases due to the low weight λi(τ , u∗), i.e., the vanishing gradient problem. Consequently, as shown in Figure 9h, the approximated joint Q-values for the optimal u∗ = (u1 2)} are 4.37 in QPLEX.\n\n2) and three suboptimal {(u2\n\n2), (u3\n\n2)(u3\n\n1, u3\n\n1, u3\n\n1, u2\n\n1, u1\n\nGVR uses inferior target shaping and superior experience replay to eliminate the non-optimal selftransition nodes. However, it ignores the stochasticity at all as WQMIX, and thus behaves as CW-QMIX and cannot converge to any joint action (Figure 9f). More comparison among GVR, WQMIX, and UTS can be found in Appendix D.\n\nC ABLATION STUDIES\n\nIn this section, we conduct ablation studies to investigate the influence of target shaping in UTS. The following methods are included in the evaluation: 1) QMIX, which is the natural ablation baseline of UTS, 2) UTS without target shaping (denoted by UTS-wo-TS), i.e., agents are trained only according to the best action value function qa(s, τa, ua), 3) UTS without the reward predictor (denoted by UTS-wo-R), 4) UTS without the state predictor (denoted by UTS-wo-S), and 5) the original UTS.\n\nC.1 THE REWARD AND THE STATE PREDICTORS\n\nFirst, we investigate the role of the reward and the state predictors. We compare UTS-wo-R, UTSwo-S, and UTS on matrix game (stochastic non-monotonic rewards), multi-agent Markov Decision Process (MMDP) (stochastic state transition and non-monotonic returns), predator-prey (deterministic non-monotonic returns), and predator-stag-hare (stochastic non-monotonic returns). As shown in Figure 10, UTS-wo-R cannot cope with stochastic rewards and thus fail to solve matrix game, MMDP, and predator-stag-hare. UTS-wo-S cannot identify whether the state-action pair leads to stochastic transition and fail to solve MMDP. However, since predator-prey only involves deterministic targets, the target shaping function does not overestimate the suboptimal. As a result, both UTS-wo-R and UTS-wo-S can solve predator-prey, indicating the effectiveness of the target shaping function.\n\nFigure 10: The comparison of UTS-wo-R, UTS-wo-S, and UTS.\n\nThe additional models (the reward and the state predictors) in UTS are introduced to identify stochasticity, which are necessary to cope with any target state-action values. When the task does not involve stochasticity, UTS learns slower because the target shaping function relies on the results from these models. When we have complete domain knowledge of the task, a suitable threshold for the model prediction error can be set to improve the learning speed.\n\nC.2 THE TARGET SHAPING FUNCTION\n\nNow we show the effectiveness of the target shaping function. We compare QMIX, UTS-wo-TS, and UTS on predator-prey and predator-stag-hare. As shown in Figure 11, since the miscoordination penalty is large, QMIX cannot deal with non-monotonic targets and thus fail to solve these tasks.\n\n18\n\nNNNNN0HDQ7HVW5HWXUQD0DWUL[*DPHNNNNN0HDQ7HVW5HWXUQE00'300000HDQ7HVW5HWXUQF33SHQDOW\\ 00001XPEHURI&DSWXUHG6WDJVG36+65 3HQDOW\\ 876876ZR5876ZR6Under review as a conference paper at ICLR 2023\n\nUTS-wo-TS and UTS can learn to catch the prey (stags), whereas UTS learns more quickly. Since the estimated best action value introduces additional estimation error that may cause a significant variance, we also show the standard deviation of per-agent action values Qa. As shown in Figure 11, we can see that UTS has a lower standard deviation than UTS-wo-TS.\n\nTo better understand why UTS can achieve better performance, first, we discuss the advantage of value factorisation methods. Linear value factorisation implicitly achieves credit assignment using a counterfactual baseline (Wang et al., 2021). The individual Q-value function Qi(s, ui) is updated by:\n\nEu′\n\n−i∼π[y(s, ui, u−i)] −\n\nn − 1 n\n\nEu′∼π[y(s, u′)] + wi(s),\n\n(32)\n\nwhere the residue term w ≡ [w1(s), ..., wn(s)] denotes an arbitrary vector satisfying ∀s, (cid:80)n 1 wi(s) = 0. As a result, the individual Q-value in linear value factorisation can be viewed as a type of advantage function, where the per-agent baseline reduces variance but does not affect the convergence. Due to the non-linear mixing function in QMIX, a more general theoretical understanding of monotonic value factorisation is still insufficient. However, previous work shows that QMIX also implicitly performs credit assignment and outperforms linear value factorisation (Rashid et al., 2018; 2020; Zhou et al., 2020).\n\nFigure 11: The comparison of QMIX, UTS, and UTS-wo-TS.\n\nFigure 12: Per-agent action values from QMIX, UTS, and UTS-wo-TS.\n\nA numerical example is provided in Figure 12. Figure 12a shows the payoff of a one-step non-monotonic game. First, we show the standard deviation of per-agent action values Qa. QMIX converges to the suboptimal when it meets non-monotonic targets of the joint Q-value function. As shown in Figure 12b, QQM IX (u1 1) keeps reducing and is smaller than the values of other actions, which implies that QMIX converges to the suboptimal due to representational limitation. Both UTS and UTS-wo-TS can recover the optimal policy. The action values in UTS-wo-TS show similar trends, and thus UTS-wo-TS can be stable until it perfectly approximates all action values. By contrast, UTS shapes the targets to ensure they can be represented by monotonic value factorisation, where the optimal policy is invariant during this shaping. UTS performs credit assignment with the shaped targets, the action values in UTS can be viewed as a type of advantage and thus are much smaller than UTS-wo-TS. The action values in\n\n1\n\n19\n\nFigure 13: Standard deviation of per-agent action values from QMIX, UTS, and UTS-wo-TS on a random 10 × 10 matrix game.\n\n00007LPHVWHSV1XPEHURI&DSWXUHG6WDJVD333HQDOW\\ 00007LPHVWHSV6WDQGDUG'HYLDWLRQV00007LPHVWHSV1XPEHURI&DSWXUHG6WDJVE36+65 3HQDOW\\ 00007LPHVWHSV6WDQGDUG'HYLDWLRQV87640,;876ZR76(a) Payoff matrix.Q1(u11) Q1(u12) Q1(u13) Q1(u11) Q1(u12) Q1(u13) (b) QMIX.(c) UTS.(d) UTS-wo-TS.-12866612/0u11 U1 U2 u21 u12 u13 u22 u23 u11 U1 U2 u21 u12 u13 u22 u23 -12-12-12-12866612/0u11 U1 U2 u21 u12 u13 u22 u23 -12-12-12Under review as a conference paper at ICLR 2023\n\nUTS show two different trends, where QU T S when the algorithm converges.\n\n1\n\n(u1\n\n1) is positive, QU T S\n\n1\n\n(u2\n\n1) and QU T S\n\n1\n\n(u3\n\n1) are negative\n\nWe also show the standard deviation of per-agent action values from QMIX, UTS, and UTS-wo-TS on a random 10 × 10 matrix game, where the suboptimal is filled with random numbers generated uniformly between -20 and 19, and the unique optimal value is +20. As shown in Figure 13, UTS reduces variance compared to UTS-wo-TS, thus improving learning efficiency and stability.\n\nD THE COMPARISON WITH WQMIX AND GVR\n\nIn Section 3.1, we prove that WQMIX with a low α can overcome any non-monotonic tasks. However, we also prove that WQMIX is particularly brittle with stochasticity because it overestimates the expected value of the suboptimal whose target is large with a low probability. GVR also ignores the stochasticity and cannot accurately identify whether a given action is inferior. As a result, WQMIX with fine-tuned α and GVR are able to solve any non-monotonic targets, but cannot deal with stochasticity.\n\nFigure 14: The comparison of GVR, UTS, and WQMIX with different weights.\n\nTo further show the difference between these methods and UTS, we compare them on predator-prey with different level of miscoordination penalties and stochasticity. Figure 14 demonstrates that WQMIX with a very low α and GVR can obtain the positive test return in all penalty settings on predator-prey. However, they cannot recover the optimal joint policy on stochastic predator-stag-hare. By contrast, UTS outperforms WQMIX and GVR in these tasks.\n\nE THE COMPARISON WITH MORE ALGORITHMS\n\nWe also compare our method with more algorithms to further show our motivation. We consider recent multi-agent reinforcement learning works, including RODE (Wang et al., 2020c), MAPPO (Yu et al., 2021), and HAPPO (Kuba et al., 2021).\n\nFigure 15a-d shows that RODE, MAPPO, and HAPPO fail to recover the optimal policy in the matrix game and the MMDP with stochastic and non-monotonic targets, where the payoff is shown in Section 6.1. In addition, they also cannot solve predator-prey and predator-stag-hare when the penalty is large. This is because that RODE applies the same monotonic mixing network as QMIX and thus can only represent values in the restricted monotonic space. This representational limitation makes RODE cannot cope with non-monotonic targets. MAPPO is an independent learning approach with many valuable techniques, but it cannot deal with these tasks due to the non-stationarity of the environment. HAPPO applies an agent-by-agent update scheme and improves the performance\n\n20\n\n00007LPHVWHSV0HDQ7HVW5HWXUQD333HQDOW\\ 00007LPHVWHSV0HDQ7HVW5HWXUQE333HQDOW\\ 00007LPHVWHSV0HDQ7HVW5HWXUQF333HQDOW\\ 00007LPHVWHSV0HDQ7HVW5HWXUQG333HQDOW\\ 00007LPHVWHSV1XPEHURI&DSWXUHG6WDJVH36+3HQDOW\\ 00007LPHVWHSV1XPEHURI&DSWXUHG6WDJVI36+3HQDOW\\ 00007LPHVWHSV1XPEHURI&DSWXUHG6WDJVJ36+3HQDOW\\ 00007LPHVWHSV1XPEHURI&DSWXUHG6WDJVK36+3HQDOW\\ 876&:40,; &:40,; &:40,; 2:40,; 2:40,; 2:40,; *95876&:40,; &:40,; &:40,; 2:40,; 2:40,; 2:40,; *95Under review as a conference paper at ICLR 2023\n\ncompared to MAPPO. This update scheme can only guarantee convergence to one of the Nash Equilibriums in theory (each agent is treated to receive the equal reward under the team reward setting). However, the monotonic target in value decomposition methods can be interpreted as a static multivariable optimisation problem, where there are non-optimal solutions that cannot be improved upon by coordinate descent. The sequential policy update scheme cannot guarantee convergence to the optimal in this optimisation problem (Bertsekas, 2019). Or we can also view the task of predator-prey and predator-stag-hare as a Pareto Selection problem, and HAPPO cannot ensure convergence to the real optimal (the Pareto optimal) and thus fail to solve these tasks.\n\nThe representational limitation of monotonic value decomposition will not necessarily result in converging to suboptimal. We can see that RODE outperforms UTS in some SMAC maps. However, UTS still achieves comparable performance with RODE in many SMAC tasks, indicating that the target shaping function is more general than the weighting function in WQMIX.\n\nFigure 15: The performance of UTS, RODE, MAPPO, and HAPPO on matrix game, MMDP, predatorprey, predator-stag-hare, and many SMAC maps.\n\nThe sampling efficiency of MAPPO and HAPPO is lower than Q-learning based algorithms because they are on-policy algorithms. Therefore, Figure 15e-h shows their poor performance compared with RODE and UTS with limited environment steps. Not that we use SC2.4.6.2.69232 (the version of StarCraft II which VDN, QMIX, and RODE used in their papers) instead of SC2.4.10 (the version which policy-based algorithm always used). Performance is not always comparable between versions.\n\nWe emphasise that our work is to provide a concrete analysis of the relationship between the nonmonotonic and stochastic targets and the monotonic value decomposition. We propose a target shaping function to eliminate the adverse impact from representational limitation, where the approximation of all original targets is not required. The shaping function project all original targets to a monotonic space and keeps the optimal policy unchanged. Since all shaped targets are tractable to monotonic value decomposition, QMIX can recover the optimal policy with any original target. That is to say, we relax the monotonic assumption of the relationship between the joint Q-value and individual Q-values.\n\nF EXPERIMENTAL SETUP\n\nWe adopt the same architecture for VDN, QMIX, QTRAN, QPLEX, and WQMIX as (Samvelyan et al., 2019; Rashid et al., 2020). Each agent independently learns a policy with fully shared parameters between all policies. We used RMSProp with a learning rate of 5 × 10−4 and γ = 0.99, buffer size 5000, mini-batch size 32 for all algorithms. We use an ε-greedy strategy, and ε is linearly annealed from 1 to 0.05 over 50k timesteps (unless specified otherwise). The dimension of each agent’s GRU hidden state is set to 64.\n\nFor WQMIX, we use α = 0.1 on predator-prey and predator-stag-hare (unless specified otherwise), and α = 0.5 in SMAC.\n\n21\n\nNNNNN7LPHVWHSV0HDQ7HVW5HWXUQD0DWUL[*DPHNNNNN7LPHVWHSV0HDQ7HVW5HWXUQE00'300007LPHVWHSV0HDQ7HVW5HWXUQF333HQDOW\\ 00007LPHVWHSV1XPEHURI&DSWXUHG6WDJVG36+65 3HQDOW\\ 00007LPHVWHSV0HDQ7HVW:LQ5DWHHFBYVB]J000007LPHVWHSV0HDQ7HVW:LQ5DWHIFRUULGRU000007LPHVWHSV0HDQ7HVW:LQ5DWHJKBYVB]00007LPHVWHSV0HDQ7HVW:LQ5DWHKPBYVBP8760$332+$33252'(8760$332+$33252'(Under review as a conference paper at ICLR 2023\n\nFor UTS, we use the same agent and mixing network in WQMIX as (Rashid et al., 2020). For the reward and the state predictors, we use a multilayer perceptron (MLP) with ReLU non-linearities and a gated recurrent unit (GRU) to extract features, where the hidden dim is 128. Then we use another MLP to output the predicted reward, the standard deviation and the embedding of the next state. The first and the GRU parameters are shared. The dimension of the target embedding for the state m is set to 16. We use Adam with a learning rate of 1 × 10−3 to train the predictors. For the best Q-value function, we use feed-forward networks with three hidden layers of dim {64, 64} and ReLU non-linearities, where the optimiser is RMSProp with a learning rate of 5 × 10−4.\n\nWe conduct experiments on an NVIDIA RTX 3090 GPU. Each task needs to train for about 5 hours on predator-prey, and about 8 to 16 hours on SMAC, depending on the number of agents and episode length limit of each map. We evaluate 32 episodes with decentralised greedy action selection every 10k timesteps for each algorithm.\n\nWe use an ε-greedy policy in which ε decreases from 1 to 0.05 over 1M timesteps for 3s5z vs 3s6z, 6h vs 8z and corridor, and over 50k timesteps for others.\n\nAll the learning curves in the experiments are plotted based on five training runs with different random seeds using mean and standard deviation with confidence internal 95%.\n\n22",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a new value factorization method for cooperative MARL. The technique brings out an alternate way of value factorization (to WQMIX and QMIX) when the target values are non-monotonic and stochastic. In particular, prior approaches is the inability to recover the optimal Qtot value when the joint value function is non-monotonic and stochastic. A popular matrix game example variant shows that the previous approaches frequently overestimate or underestimate joint Q values in stochastic and non-monotonic settings. An alternate shaping function is introduced to generate the shaped joint action-value that can consider the stochasticity in the reward and state function and place appropriate weight on the different joint actions. To this end, a neural network is trained to predict standard deviation and error in the reward and the embedding of the next state. Another neural network is trained to estimate the best action value for each agent, assuming the agent gets the cooperation of other agents. Finally, based on the prediction error and the best action values, the shaping function generates monotonic target joint action-values for QMIX.\n\n# Strength And Weaknesses\n\nStrengths\n•\tIncorporates stochasticity in reward and state representation\n•\tGood empirical performance\n•\tLimitations of prior works is explained through theory and the example of matrix game\n\nWeaknesses\n•\tThe overall approach is a bit of a kitchen sink with many hyperparameters to be tuned and models to be trained. QMIX alone is already known for being challenging on this front with larger number of agents.  See, e.g., the below reference.\n•\tGoal of estimating the stochasticity of rewards and next states seems similar in spirit to distributional RL, but there is no comparison to related work from this literature.\n•\tWhile the experiments include some ablations, the role of various parts of the overall approach is not fully explored.  For example, there are two approaches used in combination to detect stochasticity (reward predictor and state predictor).  What if you only include one?  Or what happens if you use different rules about when to apply or not apply the change of target than that in Equation (7) since at the very least it has two different cases.\n\n@article{avalos2021local,\n  title={Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning},\n  author={Avalos, Rapha{\\\"e}l and Reymond, Mathieu and Now{\\'e}, Ann and Roijers, Diederik M},\n  journal={arXiv preprint arXiv:2112.12458},\n  year={2021}\n}\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe use of random embedding for estimating stochasticity seems similar to some work on exploration such as the following (although with a different goal).  Is there a relation?\n\n@article{burda2018exploration,\n  title={Exploration by random network distillation},\n  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},\n  journal={arXiv preprint arXiv:1810.12894},\n  year={2018}\n}\n\nThe first sentence of 4.2 is confusing to me.  I thought Q^{mvf} is intended to be the set of things that QMIX can in principle learn.  But the phrasing suggests that there are things in it that QMIX cannot represent.  Is there a typo here or something that needs to be better explained?\n\nThe reason of predicting each agent’s best action value function is not well explained in the main text and required studying the proof of Theorem 2 for me to understand.  My intuition is the goal is quantify the suboptimality of actions because, by definition, the the join action is suboptimal (and the optimal action is unique) then for every agent not playing their part of the optimal action their q_a will be less than the optimal Q value.  \n\nThe statement of Theorem 2 seems to be missing something.  It contains a “such that,” with no indication what the assumption is.  From the proof I’m guessing it is that the optimal policy is unique and the discussion of this issue in the proof should be discussion and not actually part of the proof?  Relatedly, I don’t know where there is a quantification over joint actions u in the theorem statement.\n\nThe intuition for the decision not to change the target if it is highly stochastic is not clear to me. (Discussion before (8))\n\n# Summary Of The Review\n\nA paper with strong empirical performance but room for improvement in the throughness and clarity of the exposition.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nA REPRODUCIBLE AND REALISTIC EVALUATION OF PARTIAL DOMAIN ADAPTATION METHODS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nUnsupervised Domain Adaptation (UDA) aims at classifying unlabeled target images leveraging source labeled ones. In this work, we consider the Partial Domain Adaptation (PDA) variant, where we have extra source classes not present in the target domain. Most successful algorithms use model selection strategies that rely on target labels to find the best hyper-parameters and/or models along training. However, these strategies violate the main assumption in PDA: only unlabeled target domain samples are available. Moreover, there are also inconsistencies in the experimental settings - architecture, hyper-parameter tuning, number of runs - yielding unfair comparisons. The main goal of this work is to provide a realistic evaluation of PDA methods with the different model selection strategies under a consistent evaluation protocol. We evaluate 7 representative PDA algorithms on 2 different real-world datasets using 7 different model selection strategies. Our two main findings are: (i) without target labels for model selection, the accuracy of the methods decreases up to 30 percentage points; (ii) only one method and model selection pair performs well on both datasets. Experiments were performed with our PyTorch framework, BenchmarkPDA, which we open source.\n\n1\n\nINTRODUCTION\n\nDomain adaptation. Deep neural networks are highly successful in image recognition for indistribution samples (He et al., 2016) with this success being intrinsically tied to the large number of labeled training data. However, they tend to not generalize as well on images with different background or colors not seen during training. Such shift in the samples is referred to as domain shift in the literature. Unfortunately, enriching the training set with new samples from different domains is challenging as labeling data is both an expensive and time-consuming task. Thus, researchers have focused on unsupervised domain adaptation (UDA) where we have access to unlabelled samples from a different domain, known as the target domain. The purpose of UDA is to classify these unlabeled samples by leveraging the knowledge given by the labeled samples from the source domain (Pan & Yang, 2010; Patel et al., 2015). In the standard UDA problem, the source and target domains are assumed to share the same classes. In this paper, we consider a more challenging variant of the problem called partial domain adaptation (PDA): the classes in the target domain Yt form a subset of the classes in the source domain Ys (Cao et al., 2018), i.e., Yt ⊂ Ys. The number of target classes is unknown as we do not have access to the labels. The extra source classes, not present in the target domain, make the PDA problem more difficult: simply aligning the source and target domains forces a negative transfer where target samples are matched to outlier source-only labels.\n\nRealistic evaluations. Most recent PDA methods report an increase of the target accuracy up to 15 percentage points on average when compared to the baseline approach that uses only source domain samples. While these successes constitute important breakthroughs in the DA research literature, target labels are used for model selection, violating the main UDA assumption. In their absence, the effectiveness of PDA methods remains unclear and model selection constitutes a yet to be solved problem as we show in this work. Moreover, the hyper-parameter tuning is either unknown or lacks details and sometimes requires labeled target data, which makes it challenging to apply PDA methods to new datasets. Recent work has highlighted the importance of model selection in the presence of domain shift. Gulrajani & Lopez-Paz (2021) showed that when evaluating domain generalization (DG) algorithms, whose goal is to generalize to a completely unseen domain, in a consistent and realistic setting no method outperforms the baseline ERM method by more than 1\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nDATASET\n\nModel Selection\n\nS. ONLY\n\nPADA\n\nSAFN\n\nBA3US\n\nAR\n\nJUMBOT\n\nMPOT\n\nOFFICEHOME\n\nWorst (w/o target labels) Best (w/o target labels) ORACLE\n\nVISDA\n\nWorst (w/o target labels) Best (w/o target labels) ORACLE\n\n59.55 (-2.31) 52.72 (-11.00) 61.37 (-1.93) 62.25 (-13.73) 64.32 (-8.42) 61.28 (-15.87) 46.92 (-30.38) 75.37 (-0.61) 74.61 (-2.54) 66.24 (-11.07) 60.73 (-1.14) 63.08 (-0.64) 75.98\n\n70.58 (-2.16) 72.73\n\n62.59 (-0.71) 63.30\n\n61.87\n\n63.72\n\n77.31\n\n77.15\n\n55.02 (-4.46) 32.32 (-22.26) 42.83 (-19.81) 51.07 (-16.60) 55.69 (-18.15) 59.86 (-24.15) 61.62 (-25.33) 78.40 (-8.54) 65.58 (-2.09) 55.24 (-4.24) 86.95 67.67 59.48\n\n67.20 (-6.65) 73.85\n\n77.69 (-6.31) 84.01\n\n58.62 (-4.02) 62.64\n\n56.83 (2.26) 54.57\n\nTable 1: Task accuracy average computed over three different seeds (2020, 2021, 2022) on Partial OFFICE-HOME and Partial-VISDA. For each dataset and PDA method, we display the results of the worst and best performing model selection that do not use target labels as well as the ORACLE model selection strategy. All results can be found in Table 6.\n\npercentage point. They argue that DG methods without a model selection strategy remain incomplete and should therefore be specified as part of the method. A similar recommendation was done by Saito et al. (2021) for domain adaptation.\n\nPDA methods have been designed using target labels at test time to select the best models. Parallel work (Saito et al., 2021; You et al., 2019) on model selection strategies for domain adaptation claimed to select the best models without using target labels. However, a realistic empirical study of these strategies in PDA is still lacking. In this work, we conduct extensive experiments to study the impact of model selection strategies on the performance of partial domain adaptation methods. We evaluate 7 different PDA methods over 7 different model selection strategies, 4 of which do not use target labels, and 2 different datasets under the same experimental protocol for a fair comparison. We list below our major findings:\n\n• The accuracy attained by models selected without target labels can decrease up to 30 percentage points compared to the one reported using target labels (See Table 1 for a summary of results).\n\n• Only 1 pair of PDA methods and target label-free model selection strategies achieve comparable\n\naccuracies to when target labels are used, while still improving over a source only baseline.\n\n• Random seed plays an important role in the selection of hyper-parameters. Selected parameters are not stable across different seeds and the standard deviation between accuracies on the same task can be up to 8.4% even when relying on target labels for model selection.\n\n• Under a more realistic scenario where some target labels are available, 100 random samples is enough to see only a drop of 1 percentage point in accuracy (when compared to using all target samples). However, the extreme case of using only one labeled target sample per class leads a to significant drop in performance.\n\nOutline. In Section 2, we provide an overview of the different model selection strategies considered in this work. Then in Section 3, we discuss the PDA methods that we consider. In Section 4 we describe the training procedures, hyper-parameter tuning and evaluation protocols used to evaluate all methods fairly. In Section 5, we discuss the results of the different benchmarked methods and the performance of the different model selection strategies. Finally in Section 6, we give some recommendations for future work in partial domain adaptation.\n\n2 MODEL SELECTION STRATEGIES: AN OVERVIEW\n\nModel selection (choosing hyper-parameters, training checkpoints, neural network architectures) is a crucial part of training neural networks. In the supervised learning setting, a validation set is used to estimate the model’s accuracy. However, in UDA such approach is not possible as we have unlabeled target samples. Several strategies have been designed to address this issue. Below, we discuss the ones used in this work.\n\nSource Accuracy (S-ACC). Ganin & Lempitsky (2015) used the accuracy estimated on a small validation set from the source domain to perform the model selection. While the source and target accuracies are related, there are no theoretical guarantees. You et al. (2019) showed that when the domain gap is large this approach fails to select competitive models.\n\nDeep Embedded Validation (DEV). Sugiyama et al. (2007) and Long et al. (2018) perform model selection through Importance-Weighted Cross-Validation (IWCV). Under the assumption that the\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nsource and target domain follow a covariate shift, the target risk can be estimated from the source risk through importance weights that give increased importance to source samples that are closer to target samples. These importance weights correspond to the ratio of the target and source densities and are estimated using Gaussian kernels. Recently, You et al. (2019) proposed an improved variant, Deep Embedded Validation (DEV), that controls the variance of the estimator and estimates the importance weights with a discriminative model that distinguish source samples from target samples leading to a more stable and effective method.\n\nEntropy (ENT). While minimizing the entropy of the target samples has been used in domain adaptation to improve accuracy by promoting tighter clusters, Morerio et al. (2018) showed that it can also be used for model selection. The intuition is that a lower entropy model corresponds to a highly confident model with discriminative target features and therefore reliable predictions.\n\nSoft Neighborhood Density (SND). Saito et al. (2021) argue that a good UDA model will have a cluster structure where nearby target samples are in the same class. They claim that entropy is not able to capture this property and propose the Soft Neighborhood Density (SND) score to address it.\n\nTarget Accuracy (ORACLE). We consider as well the target accuracy on all target samples. While we emphasize once again its use is not realistic in unsupervised domain adaptation (hence why we will refer to it as ORACLE), it has nonetheless been used to report the best accuracy achieved by the model along training in several previous works (Cao et al., 2018; Xu et al., 2019; Jian et al., 2020; Gu et al., 2021; Nguyen et al., 2022). Here, we use it as an upper bound for all the other model selection strategies and to check the reproducibility of previous works.\n\nSmall Labeled Target Set (1-SHOT and 100-RND). For real-world applications in an industry setting, it is unlikely that a model will be deployed without the very least of an estimate of its performance for which target labels are required. Therefore, one can imagine a situation where a PDA method is used and a small set of target samples is available. Thus, we will compute the target accuracy with 1 labeled sample per class (1-SHOT) and 100 random labeled target samples (100RND) as model selection strategies. One could argue that the 100 random samples could have been used in the training with semi-supervised domain adaptation methods. However, note that we do not know how many classes we have on the target domain so it is hard to form a split when we have uncertainty of classes. For instance, 100-RND represents possibly less than 2 samples per class for one of our real-world dataset, as we do not know the number of classes, making a potential split between a train and validation target sets not possible.\n\n3 PARTIAL DOMAIN ADAPTATION METHODS\n\nIn this section, we give a brief description of the PDA methods considered in our study. They can be grouped into two families: adversarial training and divergence minimization.\n\nAdversarial training. To solve the UDA problem, Ganin et al. (2016) aligned the source and target domains with the help of a domain discriminator trained adversarially to be able to distinguish the samples from the two domains. However, when applied to the PDA problem this strategy leads to negative transfer and the model performs worse than a model trained only on source data. Cao et al. (2018) proposed PADA that introduces a PDA specific solution to adversarial domain adaptation: the contribution of the source-only class samples to the training of both the source classifier and the domain adversarial network is decreased. This is achieved through class weights that are calculated by simply averaging the classifier prediction on all target samples. As the source-only classes should not be predicted in the target domain, they should have lower weights. More recently, Jian et al. (2020) proposed BA3US which augments the target mini-batch with source samples to transform the PDA problem into a vanilla DA problem. In addition, an adaptive weighted complement entropy objective is used to encourage incorrect classes to have uniform and low prediction scores.\n\nDivergence minimization. Another standard direction to align the source and target distributions in the feature space of a neural network is to minimize a given divergence between distributions of domains. Xu et al. (2019) empirically found that target samples have low feature norm compared to source samples. Based on this insight, they proposed SAFN which progressively adapts the feature norms of the two domains by minimizing the Maximum Mean Feature Norm Discrepancy (Gretton et al., 2012). Other approaches are based on optimal transport (OT) (Bhushan Damodaran et al., 2018) with mini-batches (Peyr ́e & Cuturi, 2019; Fatras et al., 2020; 2021b). For the PDA problem\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nPDA Methods\n\nPADA, SAFN, BA3US AR, JUMBOT, MPOT\n\nModel Selection Strategies\n\nS-ACC, ENT, DEV, SND, 1-SHOT, 100-RND, ORACLE\n\nArchitecture\n\nResNet50 backbone ⊕ linear bottleneck ⊕ linear classification head\n\nExperimental protocol\n\n3 seeds on the 12 tasks of OFFICE-HOME and 2 tasks of VISDA\n\nTable 2: Summary of our considered methods, model selection strategies, architecture and datasets.\n\nMethod\n\nPADA SAFN BA3US AR JUMBOT MPOT\n\nArchitecture (bottleneck)\n\nRuns per task\n\nModel Selection\n\nHyper-Parameters\n\nAlong Training\n\nLinear Non-Linear Linear Non-Linear Linear Linear\n\n1 3\n3 1\n1 3\n\nIWCV (lacks details) Unknown Unknown IWCV (lacks details) ORACLE Unknown\n\nORACLE ORACLE ORACLE ORACLE FINAL ORACLE\n\nTable 3: Summary of the experimental protocol used for SOTA partial domain adaptation methods. We refer to Appendix A.1 for additional details.\n\nin specific, (Fatras et al., 2021a) developed JUMBOT, a mini-batch unbalanced optimal transport that learns a joint distribution of the embedded samples and labels. The use of unbalanced OT is critical for the PDA problem as it allows to transport only a portion of the mass limiting the negative transfer between distributions. Based on this work, (Nguyen et al., 2022) investigated the partial OT variant (Chapel et al., 2020), a particular case of unbalanced OT, proposing M-POT. Finally, another line of work is to use the Kantorovich-Rubenstein duality of optimal transport to perform the alignment similarly to WGAN (Arjovsky et al., 2017). This is precisely the work of Gu et al. (2021) that proposed, AR. In addition, source samples are reweighted in order to reduce the negative transfer from the source-only class samples. The Kantorovich-Rubenstein duality relies on a one Lipschitz function which is approximated using adversarial training like the PDA methods described above.\n\n4 EXPERIMENTAL PROTOCOL\n\nIn this section, we discuss our choices regarding the training details, datasets and neural network architecture. We then discuss the hyper-parameter tuning used in this work. We summarize the PDA methods, model selection strategies and experimental protocol used in this work in Table 2. The main differences in the experimental protocol of the different published state-of-the-art (SOTA) methods is summarized in Table 3. To perform our experiments we developed a PyTorch (Paszke et al., 2019) framework: BenchmarkPDA. We make it available for other researchers to use and contribute with new algorithms and model selection strategies:\n\nhttps://anonymous.4open.science/r/BenchmarkPDA-7F73\n\nIt is the standard in the literature when proposing a new method to report directly the results of its competitors from the original papers (Cao et al., 2018; Xu et al., 2019; Jian et al., 2020; Gu et al., 2021; Nguyen et al., 2022). As a result some methods differ for instance in the neural network architecture implementation (AR (Gu et al., 2021), SAFN (Xu et al., 2019)) or evaluation protocol JUMBOT (Fatras et al., 2021a) with other methods. These changes often contribute to an increased performance of the newly proposed method leaving previous methods at a disadvantage. Therefore we chose to implement all methods with the same commonly used neural network architecture, optimizer, learning rate schedule and evaluation protocol. We discuss the details below.\n\n4.1 METHODS, DATASETS, TRAINING AND EVALUATION DETAILS\n\nMethods. We implemented 7 PDA methods by adapting the code from the Official GitHub repositories of each method: Source Only, PADA (Cao et al., 2018), SAFN (Xu et al., 2019), BA3US (Jian et al., 2020), AR (Gu et al., 2021), JUMBOT (Fatras et al., 2021a), MPOT (Nguyen et al., 2022). We\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nMETHOD\n\nA2C A2P A2R C2A C2P\n\nC2R\n\nP2A P2C\n\nP2R\n\nR2A R2C\n\nR2P\n\nAvg\n\nS. ONLY†\n\n46.33 67.51 75.87 59.14 59.94 62.73 58.22 41.79 74.88 67.40 48.18 74.17 61.35 S. ONLY (Ours) 45.43 68.91 79.53 55.59 57.42 65.23 59.32 40.80 75.80 69.88 47.20 77.31 61.87\n\nPADA† PADA (Ours)\n\nSAFN†∗ SAFN* (Ours) SAFN (Ours)\n\nBA3US† BA3US (Ours)\n\nAR†∗ AR* (Ours) AR (Ours)\n\n51.95 67.00 78.74 52.16 53.78 59.03 52.61 43.22 78.79 73.73 56.60 77.09 62.06 50.53 67.45 80.14 57.30 54.47 64.55 61.07 40.94 79.55 73.09 54.63 80.93 63.72\n\n58.93 76.25 81.42 70.43 72.97 77.78 72.36 55.34 80.40 75.81 60.42 79.92 71.84 59.98 79.85 85.18 72.02 73.73 78.54 76.09 59.32 83.25 80.04 64.20 84.44 74.72 49.57 68.55 78.26 57.91 59.29 66.81 59.87 45.29 75.98 69.08 51.68 77.29 63.30\n\n60.62 83.16 88.39 71.75 72.79 83.40 75.45 61.59 86.53 79.25 62.80 86.05 75.98 63.26 82.75 89.16 69.91 71.93 77.58 75.73 59.94 86.89 80.93 66.77 86.93 75.98\n\n62.13 79.22 89.12 73.92 75.57 84.37 78.42 61.91 87.85 82.19 65.37 85.27 77.11 62.75 81.55 89.07 71.63 73.41 82.94 75.88 61.03 85.70 79.86 62.93 85.30 76.00 57.33 79.61 86.31 69.45 71.88 79.94 70.28 53.57 83.78 77.26 59.68 83.72 72.73\n\nJUMBOT†\n\n62.70 77.50 84.40 76.00 73.30 80.50 74.70 60.80 85.10 80.20 66.50 83.90 75.47 JUMBOT (Ours) 61.87 78.19 88.11 77.69 76.75 84.15 76.83 63.72 84.80 81.79 64.70 87.17 77.15\n\nMPOT† MPOT (Ours)\n\n64.60 80.62 87.17 76.43 77.61 83.58 77.07 63.74 87.63 81.42 68.50 87.38 77.98 64.48 80.88 86.78 76.22 77.95 82.59 75.18 64.60 84.87 80.59 67.04 86.52 77.31\n\nTable 4: Comparison between reported (†) accuracies on partial OFFICE-HOME from published methods with our implementation using the ORACLE model selection strategy. * denotes different bottleneck architectures.\n\nprovide the links to the different official repositories in Appendix A.1. A comparison with previous reported results can be found in Table 4 and we postpone the discussion to Section 5.\n\nDatasets. We consider two standard real-world datasets used in DA. Our first dataset is OFFICEIt is a difficult dataset for unsupervised domain adaptation HOME (Venkateswara et al., 2017). (UDA), it has 15,500 images from four different domains: Art (A), Clipart (C), Product (P) and RealWorld (R). For each domain, the dataset contains images of 65 object categories that are common in office and home scenarios. For the partial OFFICE-HOME setting, we follow Cao et al. (2018) and select the first 25 categories (in alphabetic order) in each domain as a partial target domain. We evaluate all methods in all 12 adaptation scenarios. VISDA (Peng et al., 2017) is a large-scale dataset for UDA. It has 152,397 synthetic images as source domain and 55,388 real-world images as target domain, where 12 object categories are shared by these two domains. For the partial VISDA setting, we follow Cao et al. (2018) and select the first 6 categories, taken in alphabetic order, in each domain as a partial target domain. We evaluate the models in the two possible scenarios. We highlight that we are the first to investigate the performance of JUMBOT and MPOT on partial VISDA.\n\nModel Selection Strategies We consider the 7 different strategies for model selection described in Section 2: S-ACC, DEV, ENT, SND, ORACLE, 1-SHOT, 100-RND. We use them both for hyperparameter tuning as well selecting the best model along training. Since S-ACC, DEV and SND require a source validation set, we divide the source samples into a training subset (80%) and validation subset (20%). Regardless of the model selection strategy used, all methods are trained using the source training subset. This is in contrast with previous work that uses all source samples, but necessary to ensure a fair comparison of the model selection strategies. We refer to Appendix A.2 for additional details.\n\nArchitecture. Our network is composed of a feature extractor with a linear classification layer on top of it. The feature extractor is a ResNet50 (He et al., 2016), pre-trained on ImageNet (Deng et al., 2009), with its last linear layer removed and replaced by a linear bottleneck layer of dimension 256.\n\nOptimizer. We use the SGD (Robbins & Monro, 1951) algorithm with momentum of 0.9, a weight decay of 5e−4 and Nesterov acceleration. As the bottleneck and classifier layers are randomly initialized, we set their learning rates to be 10 times that of the pre-trained ResNet50 backbone. We χ0 schedule the learning rate with a strategy similar to the one in (Ganin et al., 2016): χp = (1+μi)−ν , where i is the current iteration, χ0 = 0.001, γ = 0.001, ν = 0.75. While this schedule is slightly different than the one reported in previous work, it is the one implemented in the different official code implementations. We elaborate in the Appendix A.3 on the differences and provide additional details. Finally, as for the mini-batch size, JUMBOT and M-POT were designed with a stratified\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nDataset\n\nVariant\n\nBA3US ENT DEV SND\n\nJUMBOT ENT DEV SND\n\nMPOT ENT DEV SND\n\nSAFN ENT DEV SND\n\nOFFICE-HOME\n\nNaive\n\n52.60 63.10 44.48 52.30 26.75 17.67 49.01 16.72 30.63 32.12 49.67 5.01 Heuristic 58.45 63.10 60.96 56.24 45.79 55.16 49.01 45.61 30.63 46.27 49.67 49.67\n\nVISDA\n\nNaive\n\n39.06 36.99 1.14 35.89 54.53 11.99 75.04 55.33 36.11 52.82 53.26 0.83 Heuristic 67.50 34.94 38.76 47.23 54.53 66.42 75.04 55.33 85.36 52.82 53.26 52.82\n\nTable 5: Comparison between the naive model selection strategy and our heuristic approach. Accuracy on AC task for OFFICE-HOME and SR task for VISDA. Best results in bold.\n\nsampling, i.e., a balanced source mini-batch with the same number of samples per class. This allows to reduce the negative transfer between domains and is crucial to their success. On the other hand, it was shown that for some methods (e.g. BA3US) using a larger mini-batch, than what was reported, leads to a decreased performance (Fatras et al., 2021a). As a result, we used the default mini-batch strategies for each method. JUMBOT and M-POT use stratified mini-batches of size 65 for OFFICEHOME and 36 for VISDA. All other methods use a standard random uniform sampling strategy with a mini-batch size of 36.\n\nEvaluation Protocol. For the hyper-parameters chosen with each model selection strategy, we run the methods for each task 3 times, each with a different seed (2020, 2021, 2022). We tried to control for the randomness across methods by setting the seeds at the beginning of training. Interestingly, as we discuss in more detail in Section 5, some methods demonstrated a non-negligible variance across the different seeds showing that some hyper-parameters and methods are not robust to randomness.\n\n4.2 HYPER-PARAMETER TUNING\n\nPrevious works (Gulrajani & Lopez-Paz, 2021; Musgrave et al., 2021; 2022) perform random searches with the same number of runs for each method. In contrast, we perform hyper-parameter grid searches for each method. As a result, the hyper-parameter tuning budgets differs across the methods depending on the number of hyper-parameters and the chosen grid. While one can argue this leads to an unfair comparison of the methods, in practice in most real-world applications one will be interested in using the best method and our approach will capture precisely that.\n\nThe hyper-parameter tuning needs to be performed for each task of each dataset, but that would require a significant computational resources without a clear added benefit. Instead for each dataset, we perform the hyper-parameter tuning on a single task: A2C for OFFICE-HOME and S2R for VISDA. This same strategy was adopted in (Fatras et al., 2021a) and the hyper-parameters were found to generalize to the remaining tasks in the dataset. We conjecture that this may be due to the fact that information regarding the number of target only classes is implicitly hidden in the hyper-parameters. See Appendix A.4 for more details regarding the hyper-parameters.\n\nSeveral runs in our hyper-parameter search for JUMBOT, M-POT and BA3US were unsuccessful with the optimization reaching its end without the model being trained at all. This poses a challenge to DEV, SND and ENT and its one of the failures modes accounted for in (Saito et al., 2021). Following their recommendations, for JUMBOT, M-POT and BA3US, before applying the model selection strategy, we discard models whose source domain accuracy is below a certain threshold thr, which is set with the heuristic as thr = 0.9 · Acc. Here Acc denotes the source domain accuracy of the Source-Only model. In our experiments, this leads to select models whom the source accuracy is at least of thr = 69.01% for the A2C task on OFFICE-HOME and thr = 89.83% for the S2R task on VISDA. We choose this heuristic because the ablation study of some methods showed that doing the adaptation decreased slightly the source accuracy (Bhushan Damodaran et al., 2018). Table 5 shows that our heuristic leads to improved results.\n\nLastly, when choosing the hyper-parameters, we only consider the model at the end of training, discarding the intermediate checkpoint models in order to select hyper-parameters which do not lead to overfitting at the end of training and better generalize to the other tasks. Following the above protocol, for each dataset we trained 468 models in total in order to find the best hyper-parameters. Then, to obtain the results with our neural network architecture on all tasks of each dataset, we trained an additional 1224 models for OFFICE-HOME and 156 models for VISDA. We additionally\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nDATASET\n\nMETHOD\n\nS-ACC\n\nENT\n\nDEV\n\nSND\n\n1-SHOT\n\n100-RND\n\nORACLE\n\nOFFICE-HOME\n\nVISDA\n\nS. ONLY PADA SAFN BA3US AR JUMBOT MPOT\n\nS. ONLY PADA SAFN BA3US AR JUMBOT MPOT\n\n60.38±0.5 60.73±0.2 60.22±0.3 59.55±0.3 63.08±0.3 59.74±0.5 52.72±2.8 62.36±0.4 62.09±0.2 61.37±0.3 62.03±0.4 62.59±0.1 68.32±1.1 73.36±0.6 62.25±7.1 75.37±0.8 65.68±0.3 70.58±0.4 64.32±0.9 70.25±0.2 62.89±0.2 74.61±0.8 61.28±0.1 72.29±0.2 66.24±0.1 64.46±0.1 61.37±0.2 46.92±0.4\n\n58.92±0.4 60.34±0.4 61.87±0.3 62.00±0.5 63.22±0.1 63.72±0.3 49.30±0.7 62.36±0.2 63.30±0.2 65.56±7.6 75.19±0.4 75.98±0.3 70.56±0.7 70.34±0.2 72.73±0.3 74.95±0.1 75.74±0.3 77.15±0.4 68.28±0.2 73.06±0.3 77.31±0.5\n\n55.15±2.4 55.24±3.2 55.07±1.2 55.02±2.9 47.48±4.8 32.32±4.9 43.43±5.3 56.83±1.0 58.20±1.7 42.83±6.3 58.62±1.3 44.82±8.8 55.10±3.7 65.58±1.4 58.40±1.4 51.07±4.3 66.68±1.0 64.27±3.6 67.20±1.5 55.69±0.9 60.63±0.7 62.42±2.4 59.86±0.6 77.69±4.2 70.02±2.0 74.64±4.4 61.62±1.3 78.40±3.9\n\n55.72±2.2 58.16±0.6 59.48±0.4 53.15±2.9 54.38±2.7 54.57±2.6 56.89±2.1 59.09±2.8 62.64±1.5 64.77±1.4 67.44±1.2 67.67±1.3 70.29±1.7 72.60±0.8 73.85±0.9 78.34±1.9 83.49±1.9 84.01±1.9 70.96±3.7 86.69±5.1 86.95±5.0\n\nTable 6: Task accuracy average over seeds 2020, 2021, 2022 on Partial OFFICE-HOME and Partial VISDA for the PDA methods and model selection strategy. For each method, we highlight the best and worst label-free model selection strategies in green and red, respectively.\n\ntrained 231 models with the different neural network architectures for AR and SAFN. In total, 2547 models were trained to make this study and we present the different results in the next section.\n\n5 PARTIAL DOMAIN ADAPTATION EXPERIMENTS\n\nWe start the results section by discussing the differences between our reproduced results and the published results from the different PDA methods. Then, we compare the performance of the different model selection strategies. Finally, we discuss the sensitivity of methods to the random seed.\n\n5.1 REPRODUCIBILITY OF PREVIOUS RESULTS\n\nWe start by ensuring that our reimplementation of PDA methods was done correctly by comparing our reproduced results with previously reported results in Table 4. As such the model selection strategy used is ORACLE. On OFFICE-HOME, both PADA and JUMBOT achieved higher average task accuracy (1.6 and 1.7 percentage points, respectively) in our reimplementation, while for BA3US and MPOT we recover the reported accuracy in their respective papers. However, we saw a decrease in performance for both SAFN and AR of roughly 8 and 5 percentage points respectively. This is to be expected due to the differences in the neural network architectures. While we use a linear bottleneck layer, SAFN uses a nonlinear bottleneck layer. As for AR, they make two significant changes: the linear classification head is replaced by a spherical logistic regression (SLR) layer (Gu et al., 2020) and the features are normalized (the 2-norm is set to a dataset dependent value, another hyperparameter that requires tuning) before feeding them to the classification head. While we account for the first change by comparing to AR (w/ linear) results reported in (Gu et al., 2021), in our neural network architecture we do not normalize the features. These changes, nonlinear bottleneck layer for SAFN and feature normalization for AR, significantly boost the performance of both methods. When now comparing our reimplementation with the same neural network architectures, our SAFN reimplementation achieves a higher average task accuracy by 3 percentage points, while our AR reimplementation is now only 1 percentage points below. The fact that AR reported results are from only one run, while ours are averaged across 3 distinct seeds, justifies the small remaining gap. Moreover, we report higher accuracy or on par on 4 tasks of the 12 tasks. Given all the above and further discussion of the VISDA dataset results in Appendix B, our reimplementations are trustworthy and give validity to the results we discuss in the next sections.\n\n5.2 RESULTS FOR MODEL SELECTION STRATEGIES\n\nModel Selection Strategies (w/ vs w/o target labels) All average accuracies on the OFFICE-HOME and VISDA datasets can be found in Table 6. For all methods on OFFICE-HOME, we can see that the results for model selection strategies which do not use target labels are below the results given by ORACLE. For some pairs, the drop of performance can be significant, leading some methods to\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nperform on par with the S. ONLY method. That is the case on OFFICE-HOME when DEV is paired with either BA3US, JUMBOT and MPOT. Even worse is MPOT with SND as the average accuracy is more than 10 percentage points below that of S. ONLY with any model selection strategy. Overall on OFFICE-HOME, except for MPOT, all methods when paired with either ENT or SND give results that are at most 2 percentage points below compared to when paired with ORACLE.\n\nA similar situation can be seen over the VISDA dataset where the accuracy without target labels can be down to 25 percentage points. Yet again, some model selection strategies can lead to scores even worse than S. ONLY. That is the case for PADA, SAFN and BA3US. Contrary to OFFICE-HOME, all model selection strategies without target labels lead to at least one method with results on par or worse in comparison to the S. ONLY method. Overall, no model selection strategy without target labels can lead to score on par to the ORACLE model selection strategy. Finally, PADA performs worse than S. ONLY for most model selection strategies, including the ones which use target labels. However, when combined with SND it performs better than with ORACLE on average, although still within the standard deviation. This is a consequence of the random seed dependence mentioned before on VISDA: as the hyper-parameters were chosen by performing just one run, we were simply “unlucky”. In general, all of this confirms the standard assumption in the literature regarding the difficulty of the VISDA dataset.\n\nModel Selection Strategies (w/ target labels) We recall that the ORACLE model selection strategy uses all the target samples to compute the accuracy while 1-SHOT and 100-RND use only subsets: 1SHOT has only one sample per class for a total of 25 and 6 on OFFICE-HOME and VISDA, respectively, while 100-RND has 100 random target samples. Our results show that using only 100 random target labeled samples is enough to reasonably approximate the target accuracy leading to only a small accuracy drop (one percentage point in almost all cases) for both datasets. Not surprisingly, the gap between the 1-SHOT and ORACLE model selection strategies is even bigger, leading in some instances to worse results than with a model selection strategy that uses no target labels. This poor performance of the 1-SHOT model selection strategy also highlights that semi-supervised domain adaptation (SSDA) methods are not a straightforward alternative to the 100-RND model selection strategy. While one could argue that the target labels could be leveraged during training like in SSDA methods, one still needs labeled target data to perform model selection. However our results suggest that we would need at least 3 samples per class for SSDA methods. In addition, knowing that we have a certain number of labeled samples per class provides information regarding which classes are target only, one of the main assumptions in PDA. In that case, PDA methods could be tweaked. This warrants further study that we leave as future work. Finally, we have also investigated a smaller labeled target set of 50 random samples (50-RND) instead of 100 random samples. The accuracies of methods using 50-RND were not as good as when using 100-RND. All results of pairs of methods and 50-RND can be found in Appendix B. The smaller performance show that the size of the labeled target set is an important element and we suggest to use at least 100 random samples.\n\nModel Selection Strategies (w/o target labels) Only the (BA3US, ENT) pair achieved average task accuracies within 3 percentage points of its ORACLE counterpart (i.e., (BA3US, ORACLE)), while still improving over S. ONLY model. Our experiments show that there is no model selection strategy which performs well for all methods. That is why to deploy models in a real-world scenario, we advise to test selected models on a small labeled target set (i.e., 100-RND)) to assess the performance of the models as model selection strategies without target labels can perform poorly.\n\nOur conclusion is that the model selection for PDA methods is still an open problem. We conjecture that it is also the case for domain adaptation as the considered metrics were developed first for this setting. For future proposed methods, researchers should specify not only which model selection strategy should be used, but also which hyper-parameter search grid should be considered, in order to deploy them in a real-world scenario.\n\n5.3 RANDOM SEED DEPENDENCE\n\nIdeally, PDA methods should be robust to the choice of random seed. This is of particular importance when performing hyper-parameter tuning since typically only one run per set of hyper-parameters is done (that was the case in our work as well). We investigate this robustness by averaging all the results presented over three different seeds (2020, 2021 and 2022) and reporting the standard\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTASK METHOD\n\nS-ACC\n\nENT\n\nDEV\n\nSND\n\n1-SHOT\n\n100-RND\n\nORACLE\n\nS2R\n\nR2S\n\nAvg\n\nS. ONLY 46.96 ± 1.5\n\nPADA SAFN BA3US AR JUMBOT MPOT\n\n48.17 ± 3.9 49.00 ± 0.9 48.17 ± 3.9 44.56 ± 5.9 40.83 ± 11.3 41.04 ± 4.3 56.14 ± 9.7 52.04 ± 3.5 29.86 ± 16.7 52.42 ± 2.9 28.46 ± 16.5 46.12 ± 7.8 44.21 ± 3.0 57.61 ± 0.4 68.39 ± 1.3 75.23 ± 8.4 55.23 ± 2.3 84.45 ± 0.4 64.57 ± 2.9\n\n71.17 ± 1.9 75.28 ± 2.9 56.25 ± 2.1 82.10 ± 2.0\n\n48.78 ± 1.9 68.54 ± 1.3 54.35 ± 2.0 57.02 ± 1.5\n\n49.43 ± 0.8 50.01 ± 1.6 51.86 ± 1.4 52.94 ± 4.3 49.34 ± 8.4 49.34 ± 8.4 49.97 ± 3.3 47.83 ± 0.6 56.88 ± 2.1 66.79 ± 1.5 71.45 ± 0.8 71.77 ± 1.1 70.11 ± 1.4 75.09 ± 5.2 76.33 ± 4.5 81.27 ± 6.9 89.94 ± 1.1 90.55 ± 0.5 71.33 ± 4.4 87.20 ± 2.3 87.23 ± 2.3\n\nS. ONLY 63.34 ± 3.4 50.39 ± 3.8 64.37 ± 0.7 65.99 ± 4.6 64.97 ± 0.8 66.04 ± 1.0 75.47 ± 3.8\n\nPADA SAFN BA3US AR JUMBOT MPOT\n\nS. ONLY 55.15 ± 2.4 47.48 ± 4.8 58.20 ± 1.7 55.10 ± 3.7 66.68 ± 1.0 60.63 ± 0.7 70.02 ± 2.0\n\nPADA SAFN BA3US AR JUMBOT MPOT\n\n62.32 ± 2.7 23.80 ± 1.6 55.80 ± 5.2 59.99 ± 1.3 53.26 ± 9.7 68.59 ± 4.6 67.18 ± 9.1\n\n55.24 ± 3.2 32.32 ± 4.9 42.83 ± 6.3 65.58 ± 1.4 64.27 ± 3.6 62.42 ± 2.4 74.64 ± 4.4\n\n61.13 ± 3.3 61.88 ± 2.3 45.82 ± 9.2 57.53 ± 10.3 61.19 ± 3.3 64.82 ± 0.5 56.01 ± 2.9 68.01 ± 1.9 53.78 ± 2.1 65.86 ± 3.5 80.16 ± 1.1 65.36 ± 0.8 72.36 ± 7.4 66.21 ± 1.2\n\n62.00 ± 3.9 66.30 ± 2.0 67.11 ± 2.1 53.36 ± 1.7 59.43 ± 5.8 59.81 ± 6.2 63.82 ± 1.0 70.34 ± 5.8 68.40 ± 1.2 62.75 ± 2.6 63.44 ± 1.9 63.56 ± 1.8 70.46 ± 4.7 70.11 ± 5.0 71.36 ± 5.5 75.42 ± 4.8 77.03 ± 2.7 77.46 ± 3.3 70.58 ± 3.1 86.18 ± 8.1 86.67 ± 7.8\n\n55.07 ± 1.2 43.43 ± 5.3 58.62 ± 1.3 58.40 ± 1.4 67.20 ± 1.5 59.86 ± 0.6 61.62 ± 1.3\n\n55.02 ± 2.9 56.83 ± 1.0 44.82 ± 8.8 51.07 ± 4.3 55.69 ± 0.9 77.69 ± 4.2 78.40 ± 3.9\n\n55.72 ± 2.2 58.16 ± 0.6 59.48 ± 0.4 53.15 ± 2.9 54.38 ± 2.7 54.57 ± 2.6 56.89 ± 2.1 59.09 ± 2.8 62.64 ± 1.5 64.77 ± 1.4 67.44 ± 1.2 67.67 ± 1.3 70.29 ± 1.7 72.60 ± 0.8 73.85 ± 0.9 78.34 ± 1.9 83.49 ± 1.9 84.01 ± 1.9 70.96 ± 3.7 86.69 ± 5.1 86.95 ± 5.0\n\nTable 7: Accuracy of different PDA methods based on different model selection strategies on the 2 Partial VISDA tasks. Average is done over three seeds (2020, 2021, 2022). For each method, we highlight the best and worst label-free model selection strategies in green and red, respectively.\n\ndeviations. This is in contrast with previous work where only a single run is reported (Fatras et al., 2021a; Gu et al., 2021). Other works (Cao et al., 2018; Xu et al., 2019; Jian et al., 2020) that report standard deviations do not specify if the random seed is different across runs. Results for all tasks on VISDA dataset are in Table 7 and on OFFICE-HOME in Appendix B due to space constraints.\n\nOur experiments show that some methods express a non-negligible instabilities over randomness with respect to any model selection methods. This is particularly true for BA3US when paired with DEV and 1-SHOT as model selection strategies: there are several tasks where the standard deviation is above 10%. While in this case this instability may stem from the poor performance of the model selection strategies, it is also visible when ORACLE is the model selection strategy used. For instance, the M-POT has a standard deviation of 3.3% on the AP task of OFFICE-HOME which corresponds to a variance of 11%. On VISDA this instability and seed dependence is even larger.\n\n6 CONCLUSION\n\nIn this paper, we investigated how model selection strategies affect the performance of PDA methods. We performed a quantitative study with seven PDA methods and seven model selection strategies on two real-word datasets. Based on our findings, we provide the following recommendations:\n\ni) Target label samples should be used to test models before using them in real-world scenario. While this breaks the main PDA assumption, it is impossible to confidently deploy PDA models selected without the use of target labels. Indeed, model selection strategies without target labels lead to a significant drop in performance in most cases in comparison to using a small validation set. We argue that the cost of labeling it outweighs the uncertainty in current model selection strategies.\n\nii) The robustness of new PDA method to randomness should be tested over at least three different seeds. We suggest to use the seeds (2020, 2021, 2022) to allow for a fair comparison with our results.\n\niii) An ablation study should be considered when a novel architecture is proposed to quantify the associated increase of performance.\n\nAs our work focus on a quantitative study of model selection methods and reproducibility of stateof-the-art partial domain adaptation methods, we do not see any potential ethical concern. Future work will investigate new model selection strategies which can achieve similar results as model selection strategies which use label target samples.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein generative adversarial networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214–223, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL http:// proceedings.mlr.press/v70/arjovsky17a.html.\n\nBharath Bhushan Damodaran, Benjamin Kellenberger, Remi Flamary, Devis Tuia, and Nicolas Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. In The European Conference on Computer Vision (ECCV), September 2018.\n\nZhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang. Partial adversarial domain adaptation. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 135–150, 2018.\n\nLaetitia Chapel, Mokhtar Z. Alaya, and Gilles Gasso. Partial optimal transport with applications on\n\npositive-unlabeled learning. In Advances in Neural Information Processing Systems, 2020.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nKilian Fatras, Younes Zine, R ́emi Flamary, Remi Gribonval, and Nicolas Courty. Learning with minibatch wasserstein : asymptotic and gradient properties. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 2131– 2141, Online, 26–28 Aug 2020. PMLR. URL http://proceedings.mlr.press/v108/ fatras20a.html.\n\nKilian Fatras, Thibault Sejourne, R ́emi Flamary, and Nicolas Courty. Unbalanced minibatch optimal transport; applications to domain adaptation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 3186–3197. PMLR, 18–24 Jul 2021a. URL http://proceedings.mlr.press/v139/fatras21a.html.\n\nKilian Fatras, Younes Zine, Szymon Majewski, R ́emi Flamary, R ́emi Gribonval, and Nicolas Courty.\n\nMinibatch optimal transport distances; analysis and applications, 2021b.\n\nR ́emi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aur ́elie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, L ́eo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78): 1–8, 2021. URL http://jmlr.org/papers/v22/20-451.html.\n\nYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1180–1189, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ ganin15.html.\n\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ̧ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016.\n\nArthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch ̈olkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(25):723–773, 2012. URL http://jmlr.org/papers/v13/gretton12a.html.\n\nXiang Gu, Jian Sun, and Zongben Xu. Spherical space domain adaptation with robust pseudo-label loss. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nXiang Gu, Xi Yu, Yan Yang, Jian Sun, and Zongben Xu. Adversarial reweighting for partial domain adaptation. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=f5liPryFRoA.\n\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=lQdXeXDoWtI.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recogIn 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.\n\nnition. 770–778, 2016. doi: 10.1109/CVPR.2016.90.\n\nLiang Jian, Wang Yunbo, Hu Dapeng, He Ran, and Feng Jiashi. A balanced and uncertainty-aware approach for partial domain adaptation. In European Conference on Computer Vision (ECCV), August 2020.\n\nMingsheng Long, ZHANGJIE CAO, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ ab88b15733f543179858600245108dd8-Paper.pdf.\n\nPietro Morerio, Jacopo Cavazza, and Vittorio Murino. Minimal-entropy correlation alignment for unsupervised deep domain adaptation. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJWechg0Z.\n\nKevin Musgrave, Serge Belongie, and Ser-Nam Lim. Unsupervised domain adaptation: A reality\n\ncheck. arXiv preprint arXiv: Arxiv-2111.15672, 2021.\n\nKevin Musgrave, Serge Belongie, and Ser-Nam Lim. Benchmarking validation methods for unsu-\n\npervised domain adaptation. arXiv preprint arXiv: Arxiv-2208.07360, 2022.\n\nKhai Nguyen, Dang Nguyen, Tung Pham, and Nhat Ho. Improving mini-batch optimal transport via partial transportation. In Proceedings of the 39th International Conference on Machine Learning, 2022.\n\nSinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge\n\nand Data Engineering, 22(10):1345–1359, 2010. doi: 10.1109/TKDE.2009.191.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, deep learning library. E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf.\n\nVishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE Signal Processing Magazine, 32(3):53–69, 2015. doi: 10. 1109/MSP.2014.2347059.\n\nXingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko.\n\nVisda: The visual domain adaptation challenge. CoRR, abs/1710.06924, 2017.\n\nGabriel Peyr ́e and Marco Cuturi. Computational optimal transport. Foundations and Trends® in\n\nMachine Learning, 2019.\n\nH. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics,\n\n22:400–407, 1951.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nKuniaki Saito, Donghyun Kim, Piotr Teterwak, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Tune it the right way: Unsupervised validation of domain adaptation via soft neighborhood density. arXiv preprint arXiv:2108.10860, 2021.\n\nMasashi Sugiyama, Matthias Krauledat, and Klaus-Robert M ̈uller. Covariate shift adaptation by importance weighted cross validation. Journal of Machine Learning Research, 8(35):985–1005, 2007. URL http://jmlr.org/papers/v8/sugiyama07a.html.\n\nHemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In (IEEE) Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nRuijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\n\nKaichao You, Ximei Wang, Mingsheng Long, and Michael Jordan. Towards accurate model selection in deep unsupervised domain adaptation. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7124–7133. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/you19a.html.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA Reproducible and Realistic Evaluation of Partial Domain Adaptation Methods\n\nSupplementary material\n\nOutline. The supplementary material of this paper is organized as follows:\n\n• In Section A, we give more details on our experimental protocol. • In Section B, we provide additional results from our experiments.\n\nA ADDITIONAL DETAILS ON EXPERIMENTAL PROTOCOL\n\nA.1\n\nIMPLEMENTATIONS IN BENCHMARKPDA\n\nIn order to reimplement the different PDA methods, we adapted the code from the official repository associated with each of the paper. We list them in Table 8.\n\nMethod\n\nCode Repository\n\nPADA SAFN BA3US AR JUMBOT M-POT\n\nhttps://github.com/thuml/PADA/blob/master/pytorch/src/\n\nhttps://github.com/jihanyang/AFN/blob/master/partial/OfficeHome/SAFN/code/\n\nhttps://github.com/tim-learn/BA3US/\n\nhttps://github.com/XJTU-XGU/Adversarial-Reweighting-for-Partial-Domain-Adaptation\n\nhttps://github.com/kilianFatras/JUMBOT\n\nhttps://github.com/UT-Austin-Data-Science-Group/Mini-batch-OT/tree/master/PartialDA\n\nTable 8: Office Github code repositories for the PDA methods considered in this work.\n\nOne of our main claims regarding previous work is the use of target labels to choose the best model along training. This can be easily verified by inspecting the code. For PADA it can be seen on line 240 of the script “train pada.py”, for BA3US in line 116 for the script “run partial.py”, for M-POT it can be seen line 164 of the file “run mOT.py”, for SAFN it can be seen in the “eval.py” file and finally for AR in line 149 of the script “train.py”.\n\nFor JUMBOT and M-POT which are based on optimal transport, we used the optimal transport solvers from (Flamary et al., 2021).\n\nA.2 MODEL SELECTION\n\nDEV requires learning a discriminative model to distinguish source samples from target samples. Its neural network architecture must be specified as well the training details. You et al. (2019) (DEV) use a multilayer perceptron, while Saito et al. (2021) (SND) use a Support Vector Machine in their reimplementation of DEV. We empirically observed the latter to yield more stable weights and so that was the one we used. In order to train the SVM discriminator, following (Saito et al., 2021), we take 3000 feature embeddings from source samples used in training and 3000 random feature embeddings from target samples, both chosen randomly. We do a 80/20 split into training and test data. The SVM is trained with a linear kernel for a maximum of 4000 iterations. Of 5 different SVM models trained with decay values spaced evenly on log space between 10−2 and 104 the one that leads to the highest accuracy (in distinguishing source from target features) on the test data split is the chosen one.\n\nAs for SND, it also requires specifying a temperature for temperature scaling component of the strategy. We used the default value of 0.05 that is suggested in (Saito et al., 2021).\n\nFinally, we mention that the samples used for 100-RND were randomly selected and their list is made available together with the code. As for the samples used for 1-SHOT, they are the same as the ones used in semi-supervised domain adaptation.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nMethod\n\nPADA\n\nBA3US\n\nSAFN\n\nAR\n\nJUMBOT\n\nMPOT\n\nHP\n\nλ\n\nλwce λent\n\nλ ∆r\n\nρ0 Aup Alow λent\n\nτ η1 η2 η3\n\nε η1 η2 m\n\nValues\n\n[0.1, 0.5, 1.0, 5.0, 10.0]\n\n[0.1, 0.5, 1, 5, 10] [0.01, 0.05, 0.1, 0.5, 1]\n\n[0.005, 0.01, 0.05, 0.1, 0.5] [0.01, 0.1, 1.0]\n\n[2.5, 5.0, 7.5, 10.0] [5.0, 10.0] −Aup [0.01, 0.1, 1.0]\n\n[0.001, 0.01, 0.1] [0.00001, 0.0001, 0.001, 0.01, 0.1] [0.1, 0.5, 1.] [5, 10, 20]\n\n[0.5, 1.0, 1.5] [0.0001, 0.001, 0.01, 0.1, 1.0] [0.1, 1.0, 5.0, 10.0] [0.1, 0.2, 0.3, 0.4]\n\nTable 9: Hyper-Parameter values for each PDA method considered in the grid search.\n\nA.3 OPTIMIZER\n\nIn general, all methods claim to adopt Nesterov’s acceleration method as the optimization method with a momentum of 0.9 and setting the weight decay set to 5 × 10−4. The learning rate follows the annealing strategy as in Ganin et al. (2016):\n\nμp = μ0(1 + αp)−β,\n\nwhere p is the training progress linearly changing from 0 to 1, μ0 = 0.01 and α = 10 and β = 0.75.\n\nHowever, inspecting the Official code repo for each PDA method, the actual learning schedule is given by\n\nμi = μ0(1 + αi)−β, where i is the iteration number in the training procedure, μ0 = 0.01 and α = 0.001 and β = 0.75. Only when the total number of iterations is 10000 do the learning rate schedules match. In this work, we followed the latter since it is the one indeed used. For OFFICE-HOME, all methods are trained for 5000 iterations, while for VISDA they are trained for 10000 iterations, with the exception of the S. ONLY which is trained for 1000 iterations on OFFICE-HOME and 5000 iterations on VISDA.\n\nA.4 HYPER-PARAMETERS\n\nIn Table 9, we report the values used for each hyper-parameter in our grid search. We report in Table 10 the hyper-parameters chosen by each model selection strategy for each method on both datasets. In addition, for the reproducibility of AR with the proposed architecture in Gu et al. (2021),a feature normalization layer is added in the bottleneck which requires specifying r, the value to which the 2-norm is set. This hyper-parameter is therefore included in the hyper-parameter grid search with the possible values of {5, 10, 20} which are the different values used in the experiments in (Gu et al., 2021).\n\nB ADDITIONAL DISCUSSION OF RESULTS\n\nIn this section, we provide additional results that we could not add to the main paper due to the space constraints.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nMethod\n\nDataset\n\nHP ORACLE 1-SHOT 50-RND 100-RND S-ACC ENT\n\nDEV\n\nSND\n\nOFFICE-HOME\n\nPADA\n\nVISDA\n\nOFFICE-HOME\n\nSAFN\n\nVISDA\n\nOFFICE-HOME\n\nBA3US\n\nVISDA\n\nOFFICE-HOME\n\nAR\n\nVISDA\n\nOFFICE-HOME\n\nJUMBOT\n\nVISDA\n\nOFFICE-HOME\n\nMPOT\n\nVISDA\n\nλ\n\nλ\n\nλ ∆r\n\nλ ∆r\n\nλwce λent\n\nλwce λent\n\nρ0 Aup Alow λent\n\nρ0 Aup Alow λent\n\nτ η1 η2 η3\n\nτ η1 η2 η3\n\nε η1 η2 m\n\nε η1 η2 m\n\n0.5\n\n0.5\n\n0.005 0.1\n\n0.005 0.1\n\n5.0 0.05\n\n1.0 0.5\n\n2.5 5.0 -5.0 0.1\n\n2.5 10.0 -10.0 0.1\n\n0.1\n\n1.0\n\n0.1 0.01\n\n0.005 0.01\n\n10.0 0.05\n\n1.0 0.5\n\n2.5 5.0 -5.0 0.1\n\n2.5 10.0 -10.0 0.1\n\n0.01\n\n0.01\n\n0.0001 0.0001\n\n0.5 10.0\n\n0.01 0.001 1.0 5.0\n\n0.5 0.01 10.0 0.3\n\n0.5 0.01 1.0 0.3\n\n1.0 5.0\n\n0.01 0.001 1.0 5.0\n\n0.5 0.01 1.0 0.1\n\n0.5 0.001 1.0 0.1\n\n0.1\n\n10.0\n\n0.005 0.01\n\n0.05 0.01\n\n5.0 0.01\n\n0.1 0.5\n\n5.0 10.0 -10.0 1.0\n\n2.5 10.0 -10.0 0.1\n\n0.01 0.001 0.5 5.0\n\n0.01 0.001 0.5 5.0\n\n1.0 0.01 1.0 0.1\n\n0.5 0.01 1.0 0.3\n\n0.5\n\n0.5\n\n0.01 0.01\n\n0.05 0.01\n\n5.0 0.05\n\n1.0 0.5\n\n5.0 5.0 -5.0 1.0\n\n2.5 10.0 -10.0 0.1\n\n0.001 0.0001 0.1 5.0\n\n0.01 0.001 1.0 5.0\n\n0.5 0.01 1.0 0.2\n\n0.5 0.01 1.0 0.3\n\n0.1\n\n1.0\n\n0.005 0.01\n\n0.005 0.01\n\n5.0 0.01\n\n5.0 0.05\n\n2.5 5.0 -5.0 0.01\n\n2.5 5.0 -5.0 0.01\n\n0.1 0.01 0.1 5.0\n\n0.001 0.01 0.1 10.0\n\n1.0\n\n0.5\n\n0.01 0.1\n\n0.05 0.01\n\n0.1 0.1\n\n1.0 0.5\n\n5.0 10.0 -10.0 1.0\n\n7.5 10.0 -10.0 0.1\n\n0.01 1e-05 0.5 20.0\n\n0.01 1e-05 0.5 5.0\n\n5.0\n\n5.0\n\n0.5\n\n0.1\n\n0.005 0.005\n\n0.1\n\n0.005 0.01\n\n10.0 0.05\n\n5.0 0.05\n\n7.5 10.0 -10.0 0.01\n\n2.5 10.0 -10.0 0.01\n\n0.01 0.01 1.0 10.0\n\n0.1\n\n0.05 0.01\n\n1.0 0.01\n\n5.0 1.0\n\n10.0 10.0 -10.0 1.0\n\n10.0 10.0 -10.0 0.01\n\n0.001 1e-05 1.0 5.0\n\n0.01\n\n0.001 0.01 0.0001 1.0 20.0\n\n1.0 5.0\n\n1.0\n\n1.5\n\n0.001 0.0001\n\n1.0 0.3\n\n10.0 0.4\n\n1.0 1.0 0.1 0.2\n\n1.5 0.01 1.0 0.4\n\n1.0\n\n1.0\n\n1.0\n\n0.5 0.001 0.0001 0.0001 0.01 10.0 0.3\n\n10.0 0.4\n\n1.0 0.2\n\n1.0 0.2\n\nTable 10: Hyper-parameters selected for the different methods for each model selection strategy on both OFFICE-HOME and VISDA.\n\nIn Table 11, we show the accuracy per task on OFFICE-HOME averaged over three different seeds (2020, 2021, 2022) for all pairs of methods and model selection strategies.\n\nIn Table 12, we compare previously reported results with ours on VISDA. While proposed methods reported results on OFFICE-HOME, only PADA and AR results are reported in the original papers for VISDA. Gu et al. (2021) AR) also report results for BA3US. Analysing the results, we see a 9 percentage point decrease in average task accuracy for PADA, but our experiments show that there is a significant seed dependence which we discuss in detail below. This is particularly important since Cao et al. (2018) (PADA) report results from a single run. Comparing our best seeds for PADA on the SR and RS tasks, we achieve 58.01% and 67.9% accuracy versus a reported 53.53% and 76.5%. Moreover, we point out that the official code repository for PADA does not include the details to reproduce the VISDA experiments, so it is possible that minor tweaks (e.g learning rate) are necessary. As for BA3US, our results are within the standard deviation being better on the SR task and worse on the RS task. Finally as for AR we see a decrease in performance which, as the results on OFFICE-HOME show, can be explained by the differences in the neural network architecture.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nMETRIC METHOD\n\nA2C\n\nA2P\n\nA2R\n\nC2A\n\nC2P\n\nC2R\n\nP2A\n\nP2C\n\nP2R\n\nR2A\n\nR2C\n\nR2P\n\nAvg\n\nS-ACC\n\nENT\n\nDEV\n\nSND\n\n1-SHOT\n\n100-RND\n\nORACLE\n\nS. ONLY 44.50 ± 1.7 50.15 ± 2.8 47.36 ± 0.1 54.89 ± 4.7 51.12 ± 1.2 49.07 ± 0.2 53.07 ± 0.3\n\nPADA SAFN BA3US AR JUMBOT MPOT\n\nS. ONLY 45.27 ± 1.1 46.03 ± 2.9 47.08 ± 2.0 59.26 ± 0.9 54.91 ± 1.8 57.69 ± 5.6 52.94 ± 2.0\n\nPADA SAFN BA3US AR JUMBOT MPOT\n\n67.71 ± 2.4 66.93 ± 1.2 66.82 ± 1.9 71.34 ± 0.8 72.79 ± 0.7 65.45 ± 0.4 72.61 ± 1.2\n\n68.91 ± 1.4 62.09 ± 2.8 66.83 ± 0.5 76.38 ± 1.5 78.45 ± 1.8 75.44 ± 1.4 68.94 ± 1.2\n\n78.37 ± 0.3 76.73 ± 1.7 77.62 ± 0.2 81.91 ± 3.9 77.91 ± 0.2 77.14 ± 0.3 78.50 ± 0.7\n\n79.26 ± 0.7 76.05 ± 1.4 77.73 ± 0.2 86.03 ± 0.6 84.23 ± 0.9 85.24 ± 2.7 75.98 ± 0.6\n\n52.56 ± 0.9 58.00 ± 1.4 57.85 ± 0.6 61.68 ± 5.2 63.21 ± 1.5 60.09 ± 0.1 61.92 ± 0.5\n\n54.21 ± 2.1 55.07 ± 2.7 56.54 ± 2.2 68.96 ± 1.8 64.86 ± 2.3 75.97 ± 1.4 60.58 ± 0.8\n\n54.81 ± 0.1 62.88 ± 0.9 58.77 ± 0.5 39.28 ± 0.8 75.08 ± 0.5 68.90 ± 0.6 45.33 ± 1.0 56.13 ± 1.4 66.45 ± 0.8 60.33 ± 2.1 43.50 ± 1.2 76.70 ± 0.4 69.27 ± 3.5 53.93 ± 1.3 57.89 ± 0.7 66.92 ± 0.9 58.80 ± 0.7 42.49 ± 0.6 75.46 ± 0.4 67.92 ± 0.0 49.73 ± 0.1 67.13 ± 3.9 72.96 ± 1.0 68.90 ± 5.0 55.92 ± 1.3 79.13 ± 4.7 72.27 ± 3.5 51.84 ± 0.5 60.54 ± 4.0 72.76 ± 0.9 63.39 ± 3.1 48.36 ± 1.7 78.02 ± 1.7 70.00 ± 1.1 52.52 ± 1.0 59.59 ± 1.3 66.67 ± 1.3 60.24 ± 1.0 43.60 ± 0.0 74.43 ± 0.9 70.19 ± 0.5 51.12 ± 1.1 64.16 ± 1.8 70.22 ± 0.2 64.13 ± 0.9 50.87 ± 1.1 77.40 ± 0.1 70.40 ± 0.6 53.99 ± 1.5\n\n76.34 ± 0.7 60.38 ± 0.5 78.88 ± 0.8 63.08 ± 0.3 76.23 ± 0.8 62.09 ± 0.2 81.85 ± 4.1 68.32 ± 1.1 77.55 ± 2.6 65.68 ± 0.3 77.12 ± 1.3 62.89 ± 0.2 77.61 ± 0.3 66.24 ± 0.1\n\n55.52 ± 0.6 63.19 ± 0.3 56.96 ± 1.5 38.75 ± 0.6 75.65 ± 1.3 69.24 ± 1.0 45.31 ± 1.0 47.28 ± 0.1 60.92 ± 2.4 56.69 ± 2.8 38.43 ± 3.0 77.08 ± 0.2 69.48 ± 1.3 49.73 ± 3.5 59.07 ± 0.7 66.22 ± 0.5 56.75 ± 2.1 39.58 ± 2.0 73.90 ± 0.9 67.80 ± 0.2 48.76 ± 0.1 71.07 ± 0.8 76.22 ± 1.2 73.16 ± 0.6 57.91 ± 2.5 85.59 ± 1.2 78.11 ± 1.4 62.85 ± 2.7 68.16 ± 3.5 80.45 ± 0.8 67.58 ± 0.4 52.34 ± 1.0 82.48 ± 1.9 74.75 ± 2.1 55.64 ± 1.2 74.85 ± 3.3 79.75 ± 1.2 72.85 ± 2.4 60.18 ± 0.9 83.21 ± 1.1 81.97 ± 1.0 61.81 ± 4.6 65.99 ± 2.2 71.51 ± 0.8 58.28 ± 0.9 49.87 ± 2.6 73.77 ± 1.3 64.98 ± 0.4 57.53 ± 0.6\n\n76.47 ± 0.8 60.73 ± 0.2 78.00 ± 1.7 59.74 ± 0.5 76.23 ± 0.7 61.37 ± 0.3 84.84 ± 0.6 73.36 ± 0.6 83.06 ± 1.2 70.58 ± 0.4 86.33 ± 1.6 74.61 ± 0.8 73.17 ± 2.7 64.46 ± 0.1\n\n78.28 ± 0.7\n\nS. ONLY 43.74 ± 1.8 44.70 ± 1.3 58.17 ± 1.2 65.64 ± 1.3 59.08 ± 0.5 43.00 ± 1.1 74.64 ± 0.4 68.11 ± 0.9 50.53 ± 0.6 48.12 ± 0.4 41.67 ± 18.9 50.05 ± 28.7 63.74 ± 26.1 60.70 ± 2.2 59.08 ± 10.9 67.88 ± 0.9 64.62 ± 1.6 56.74 ± 1.3 75.21 ± 0.6 70.92 ± 2.0 58.39 ± 2.3 61.85 ± 4.6 70.86 ± 5.6 61.65 ± 1.0 43.72 ± 0.7 76.29 ± 0.7 70.31 ± 1.7 49.61 ± 0.8 49.25 ± 2.8 56.36 ± 0.5 66.70 ± 0.8 58.03 ± 1.1 41.99 ± 0.8 74.97 ± 0.5 67.43 ± 0.3 48.12 ± 0.5 46.11 ± 0.1 57.95 ± 1.0 66.35 ± 1.0 57.64 ± 0.8 43.60 ± 0.6 74.86 ± 1.3 67.68 ± 0.5 48.12 ± 0.8 46.07 ± 0.7\n\n67.81 ± 1.2 75.99 ± 1.3 60.22 ± 0.3 61.61 ± 5.4 68.99 ± 11.3 35.08 ± 13.1 24.24 ± 20.5 61.66 ± 2.4 57.91 ± 1.7 38.03 ± 0.6 73.11 ± 3.4 66.33 ± 0.7 29.97 ± 21.0 71.07 ± 11.3 52.72 ± 2.8 75.65 ± 0.5 62.03 ± 0.4 67.30 ± 0.5 78.06 ± 1.3 62.25 ± 7.1 75.61 ± 0.4 64.32 ± 0.9 76.04 ± 0.1 61.28 ± 0.1 75.89 ± 0.4 61.37 ± 0.2\n\n54.55 ± 1.2 63.94 ± 1.7 57.94 ± 0.9 39.40 ± 0.9 74.91 ± 0.6 69.27 ± 1.0 45.33 ± 1.0\n\nPADA SAFN BA3US AR JUMBOT MPOT\n\n79.73 ± 2.5 76.42 ± 0.3 76.46 ± 0.4\n\n62.72 ± 1.0 56.81 ± 0.1 56.44 ± 1.0\n\n70.20 ± 1.7 66.33 ± 0.6 65.43 ± 0.8\n\n77.43 ± 0.5\n\n56.75 ± 0.3\n\n51.42 ± 2.7\n\nS. ONLY 42.23 ± 1.3 50.43 ± 0.8 49.57 ± 0.3 62.21 ± 0.9 54.37 ± 1.6 56.60 ± 2.8 32.96 ± 0.4\n\nPADA SAFN BA3US AR JUMBOT MPOT\n\n68.91 ± 1.4 66.72 ± 1.5 68.18 ± 1.3 83.29 ± 0.4 79.01 ± 2.2 68.48 ± 1.5 49.73 ± 1.1\n\n79.35 ± 0.6 79.72 ± 1.8 77.86 ± 0.5 88.50 ± 0.6 84.54 ± 0.8 84.70 ± 2.1 57.39 ± 1.4\n\n51.76 ± 3.7 57.30 ± 1.9 57.91 ± 0.3 68.50 ± 0.9 64.52 ± 1.6 71.81 ± 1.8 44.11 ± 2.4\n\n53.48 ± 2.1 63.94 ± 1.7 55.37 ± 0.6 37.35 ± 1.0 74.10 ± 2.8 68.53 ± 1.4 43.78 ± 0.6 52.10 ± 1.7 63.11 ± 1.9 60.82 ± 3.0 39.26 ± 2.0 79.33 ± 1.3 73.09 ± 1.5 45.77 ± 1.6 58.17 ± 1.2 66.13 ± 1.0 59.14 ± 0.8 43.90 ± 0.5 75.81 ± 0.7 68.17 ± 1.6 49.59 ± 1.6 71.45 ± 3.6 76.96 ± 0.6 76.19 ± 1.2 59.94 ± 1.7 86.31 ± 1.4 79.46 ± 1.4 65.35 ± 1.9 68.05 ± 3.2 79.16 ± 2.8 65.60 ± 1.7 51.28 ± 1.6 83.05 ± 1.1 75.02 ± 1.6 55.02 ± 1.8 71.84 ± 1.6 80.91 ± 0.9 70.28 ± 0.8 50.69 ± 4.9 83.89 ± 1.5 81.21 ± 0.6 58.85 ± 1.7 38.66 ± 1.2 50.06 ± 1.0 43.74 ± 4.3 28.66 ± 2.6 58.40 ± 1.9 56.90 ± 1.9 39.34 ± 1.2\n\n75.84 ± 1.6 59.55 ± 0.3 80.62 ± 0.4 62.36 ± 0.4 76.64 ± 0.5 62.59 ± 0.1 86.35 ± 0.9 75.37 ± 0.8 83.40 ± 0.9 70.25 ± 0.2 88.18 ± 0.4 72.29 ± 0.2 63.14 ± 0.6 46.92 ± 0.4\n\n66.52 ± 3.1 63.03 ± 1.6 49.73 ± 4.3\n\n53.24 ± 2.0 61.77 ± 1.1 56.11 ± 1.7 37.35 ± 1.0 71.97 ± 1.8 68.96 ± 0.5 46.13 ± 2.0 S. ONLY 43.84 ± 1.7 56.28 ± 0.4 64.00 ± 1.4 58.92 ± 3.3 43.62 ± 1.0 74.27 ± 4.1 68.26 ± 3.1 54.25 ± 1.6 52.98 ± 0.2 31.40 ± 3.7 45.27 ± 0.7 57.26 ± 2.2 42.33 ± 1.6 29.77 ± 2.6 63.52 ± 3.2 56.11 ± 3.2 37.55 ± 0.8 44.60 ± 21.0 51.39 ± 29.8 65.47 ± 27.2 65.63 ± 1.4 59.78 ± 15.3 68.49 ± 1.3 68.38 ± 1.7 57.83 ± 1.3 82.05 ± 1.0 80.78 ± 1.1 63.10 ± 0.8 68.35 ± 1.9 77.25 ± 1.4 69.67 ± 1.5 51.98 ± 1.8 78.72 ± 1.0 76.19 ± 0.7 55.48 ± 2.1 56.00 ± 2.3 73.43 ± 3.3 79.85 ± 0.3 74.96 ± 3.4 62.87 ± 0.6 81.83 ± 0.9 78.48 ± 2.0 61.59 ± 2.2 61.59 ± 1.7 65.88 ± 0.5 71.42 ± 0.7 70.31 ± 1.0 53.03 ± 0.7 76.88 ± 1.3 76.52 ± 0.4 57.39 ± 1.7 53.97 ± 1.3\n\nPADA SAFN BA3US AR JUMBOT MPOT\n\n77.38 ± 0.9 78.06 ± 2.6 62.82 ± 2.0\n\n82.77 ± 2.0 86.45 ± 2.1 78.04 ± 2.1\n\n50.47 ± 2.4 51.67 ± 5.0 48.88 ± 2.4\n\n68.99 ± 0.2 74.20 ± 0.9 69.24 ± 0.4\n\n78.58 ± 1.9 76.86 ± 3.4 68.78 ± 1.7\n\n73.33 ± 2.2 58.92 ± 0.4 78.62 ± 0.4 62.00 ± 0.5 67.00 ± 1.4 49.30 ± 0.7 79.20 ± 1.1 65.56 ± 7.6 82.73 ± 1.0 70.56 ± 0.7 87.34 ± 0.2 74.95 ± 0.1 77.95 ± 1.4 68.28 ± 0.2\n\nS. ONLY 43.28 ± 1.6 50.41 ± 0.8 47.58 ± 0.8 62.53 ± 2.0 54.89 ± 2.0 61.07 ± 0.9 61.59 ± 1.2\n\nPADA SAFN BA3US AR JUMBOT MPOT\n\nS. ONLY 45.43 ± 0.9 50.53 ± 0.7 49.57 ± 0.3 63.26 ± 1.0 57.33 ± 1.7 61.87 ± 1.4 64.48 ± 1.2\n\nPADA SAFN BA3US AR JUMBOT MPOT\n\n68.76 ± 1.6 67.21 ± 1.8 67.53 ± 0.8 82.09 ± 0.8 78.54 ± 1.4 77.87 ± 1.4 75.56 ± 1.7\n\n68.91 ± 1.4 67.45 ± 1.6 68.55 ± 1.0 82.75 ± 0.9 79.61 ± 1.6 78.19 ± 2.4 80.88 ± 3.3\n\n77.97 ± 1.2 79.97 ± 1.5 77.91 ± 0.4 88.28 ± 0.4 84.34 ± 0.6 86.01 ± 1.3 82.59 ± 0.6\n\n79.53 ± 0.3 80.14 ± 1.4 78.26 ± 0.2 89.16 ± 0.2 86.31 ± 0.4 88.11 ± 1.5 86.78 ± 0.5\n\n53.75 ± 1.1 56.69 ± 1.5 56.47 ± 1.0 69.15 ± 1.2 64.95 ± 2.4 74.56 ± 0.4 72.48 ± 1.0\n\n55.59 ± 0.7 57.30 ± 1.9 57.91 ± 0.3 69.91 ± 0.2 69.45 ± 0.5 77.69 ± 0.1 76.22 ± 0.1\n\n55.57 ± 2.2 63.94 ± 0.4 58.37 ± 0.4 39.12 ± 0.4 75.56 ± 1.3 69.02 ± 0.5 43.46 ± 0.2 53.86 ± 1.6 63.94 ± 1.3 60.27 ± 2.7 40.56 ± 1.8 78.91 ± 1.8 72.70 ± 1.4 53.39 ± 2.2 58.19 ± 0.4 65.88 ± 0.2 59.69 ± 0.1 43.14 ± 1.7 75.00 ± 0.7 69.64 ± 1.0 50.85 ± 0.3 71.65 ± 1.5 77.21 ± 0.6 75.15 ± 1.3 58.17 ± 1.0 85.92 ± 1.3 79.86 ± 2.1 66.57 ± 1.5 69.00 ± 3.7 79.57 ± 0.2 66.73 ± 0.3 50.85 ± 1.4 82.39 ± 1.9 74.66 ± 2.3 55.42 ± 1.6 76.40 ± 1.4 81.54 ± 1.7 72.60 ± 1.2 59.92 ± 0.4 84.63 ± 2.3 81.85 ± 1.7 64.84 ± 1.0 69.77 ± 0.9 75.41 ± 0.5 72.64 ± 0.9 57.67 ± 1.6 82.02 ± 0.6 79.80 ± 0.5 64.64 ± 0.1\n\n75.28 ± 2.3 60.34 ± 0.4 80.73 ± 0.9 63.22 ± 0.1 76.41 ± 0.8 62.36 ± 0.2 85.66 ± 1.0 75.19 ± 0.4 82.80 ± 0.4 70.34 ± 0.2 87.64 ± 0.7 75.74 ± 0.3 82.60 ± 0.5 73.06 ± 0.3\n\n57.42 ± 1.2 65.23 ± 0.8 59.32 ± 0.7 40.80 ± 0.9 75.80 ± 1.2 69.88 ± 0.9 47.20 ± 0.9 54.47 ± 1.7 64.55 ± 1.1 61.07 ± 3.0 40.94 ± 1.6 79.55 ± 1.4 73.09 ± 1.5 54.63 ± 0.9 59.29 ± 0.5 66.81 ± 0.5 59.87 ± 0.7 45.29 ± 0.7 75.98 ± 0.6 69.08 ± 0.6 51.68 ± 0.8 71.93 ± 1.6 77.58 ± 0.9 75.73 ± 1.3 59.94 ± 0.7 86.89 ± 0.5 80.93 ± 0.8 66.77 ± 1.5 71.88 ± 0.9 79.94 ± 0.8 70.28 ± 1.0 53.57 ± 0.2 83.78 ± 1.0 77.26 ± 0.6 59.68 ± 1.1 76.75 ± 0.8 84.15 ± 1.3 76.83 ± 1.9 63.72 ± 0.5 84.80 ± 1.3 81.79 ± 0.8 64.70 ± 1.1 77.95 ± 1.3 82.59 ± 0.7 75.18 ± 1.3 64.60 ± 0.0 84.87 ± 1.4 80.59 ± 0.6 67.04 ± 0.6\n\n77.31 ± 0.1 61.87 ± 0.3 80.93 ± 0.6 63.72 ± 0.3 77.29 ± 0.5 63.30 ± 0.2 86.93 ± 0.2 75.98 ± 0.3 83.72 ± 0.6 72.73 ± 0.3 87.17 ± 1.7 77.15 ± 0.4 86.52 ± 1.2 77.31 ± 0.5\n\nTable 11: Average accuracy of different PDA methods based on different model selection strategies on the 12 tasks of Partial OFFICE-HOME. Average is done over three seeds (2020, 2021, 2022). Best results in bold.\n\nFinally in Table 13, we show all the average task accuracies from all pairs of methods and model selection strategies on the OFFICE-HOME and VISDA datasets including the 50-RND model selection strategy.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nALGORITHM\n\nS2R\n\nR2S\n\nAvg\n\nS. ONLY†\n\n45.26 64.28 54.77 S. ONLY (Ours) 51.86 67.11 59.48\n\nPADA† PADA (Ours)\n\nSAFN† SAFN (Ours)\n\n53.53 76.50 65.02 49.34 59.81 54.57\n\n67.65 -\n56.88 68.40 62.64\n\n-\n\nBA3US† BA3US (Ours)\n\n69.86 67.56 68.71 71.77 63.56 67.67\n\nAR†∗ AR (Ours)\n\nJUMBOT†\n\n85.30 74.82 80.06 76.33 71.36 73.85\n\n-\n\n-\n\n-\n\nJUMBOT (Ours) 90.55 77.46 84.01\n\nMPOT† MPOT (Ours)\n\n-\n\n- 87.23 86.67 86.95\n\n-\n\nTable 12: Comparison between reported (†) accuracies on partial VISDA from published methods with our implementation using the ORACLE model selection strategy. * denotes different bottleneck architectures.\n\nDATASET\n\nMETHOD\n\nS-ACC\n\nENT\n\nDEV\n\nSND\n\n1-SHOT\n\n50-RND\n\n100-RND\n\nORACLE\n\nOFFICE-HOME\n\nVISDA\n\nS. ONLY PADA SAFN BA3US AR JUMBOT MPOT\n\nS. ONLY PADA SAFN BA3US AR JUMBOT MPOT\n\n60.38±0.5 60.73±0.2 60.22±0.3 59.55±0.3 58.92±0.4 60.28±0.4 60.34±0.4 61.87±0.3 63.08±0.3 59.74±0.5 52.72±2.8 62.36±0.4 62.00±0.5 63.82±0.4 63.22±0.1 63.72±0.3 62.09±0.2 61.37±0.3 62.03±0.4 62.59±0.1 49.30±0.7 62.00±0.2 62.36±0.2 63.30±0.2 68.32±1.1 73.36±0.6 62.25±7.1 75.37±0.8 65.56±7.6 73.22±0.3 75.19±0.4 75.98±0.3 65.68±0.3 70.58±0.4 64.32±0.9 70.25±0.2 70.56±0.7 70.26±0.2 70.34±0.2 72.73±0.3 62.89±0.2 74.61±0.8 61.28±0.1 72.29±0.2 74.95±0.1 64.95±0.3 75.74±0.3 77.15±0.4 66.24±0.1 64.46±0.1 61.37±0.2 46.92±0.4 68.28±0.2 69.90±0.5 73.06±0.3 77.31±0.5\n\n55.15±2.4 55.24±3.2 55.07±1.2 55.02±2.9 55.72±2.2 57.90±1.1 58.16±0.6 59.48±0.4 47.48±4.8 32.32±4.9 43.43±5.3 56.83±1.0 53.15±2.9 55.67±2.5 54.38±2.7 54.57±2.6 58.20±1.7 42.83±6.3 58.62±1.3 44.82±8.8 56.89±2.1 57.90±3.3 59.09±2.8 62.64±1.5 55.10±3.7 65.58±1.4 58.40±1.4 51.07±4.3 64.77±1.4 66.66±2.4 67.44±1.2 67.67±1.3 66.68±1.0 64.27±3.6 67.20±1.5 55.69±0.9 70.29±1.7 71.91±0.3 72.60±0.8 73.85±0.9 60.63±0.7 62.42±2.4 59.86±0.6 77.69±4.2 78.34±1.9 82.85±2.9 83.49±1.9 84.01±1.9 70.02±2.0 74.64±4.4 61.62±1.3 78.40±3.9 70.96±3.7 86.65±5.1 86.69±5.1 86.95±5.0\n\nTable 13: Task accuracy average for the different PDA methods and model selection strategy pairs on Partial Office-Home and Partial VisDA. The average is computed over three difference seeds (2020, 2021, 2022).\n\n17",
    "reference": "# Summary Of The Paper\n\nThis paper raises an important question about reproducibility of results for partial domain adaptation. The authors show that the stopping criterion is key to the success of partial domain adaptation algorithms in the literature, and there can be massive variations for various stopping criterions commonly used in the literature. Some algorithms even produce wide variations in performance for different random seeds.\n\n# Strength And Weaknesses\n\nStrength\n- The paper have done considerable amount of experiments along various dimensions\n- The paper is well-written and easy to follow\n\nWeakness\n- While the paper does a lot of experiments, the paper does not establish a strong baseline. The main takeaway from the paper is that partial domain adaptation works in literature produce high variations in performance to stopping criterions and random seeds. \nThe expected takeaway from such papers should be rather a simple strong baseline which can be concluded from the experiments under a certain fair experimental protocol, which works well across stopping criterions and random seeds. This would then encourage researchers to follow that experimental protocol and report their results and compare against that baseline. \n\nThe paper is overall in a good direction and I highly encourage the authors to consider this to refine their work.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- In Table 1 it is not clear how the authors got the \"best\" and \"worst\" performances. Please clarify.\n\n- How are the seeds chosen? Randomly? \n\n- What effect can the optimizer and learning rate strategy have on the algorithms? I suppose they would also have some dependency on it.\n\n-\n\n# Summary Of The Review\n\nThe paper raises an interesting question, presents various experiments to showcase the problems with the current methods. However, a simple strong baseline and a suggestion for a robust experimental protocol is missing, which should be the take-away from such works.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "SAGE: SEMANTIC-AWARE GLOBAL EXPLANATIONS FOR NAMED ENTITY RECOGNITION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nIn the last decades, deep learning approaches achieved impressive results in many research fields, such as Computer Vision and Natural Language Processing (NLP). NLP in particular has greatly benefit from unsupervised methods that allow to learn distributed representation of language. On the race for better performances Language Models have reached hundred of billions parameters nowadays. Despite the remarkable results, deep models are still far from being fully exploited in real world applications. Indeed, these approaches are black-boxes, i.e. they are not interpretable by design nor explainable, which is often crucial to make decisions in business. Several task-agnostic methods have been proposed in literature to explain models’ decisions. Most techniques rely on the “local” assumption, i.e. explanations are made example-wise. In this paper instead, we present a post-hoc method to produce highly interpretable global rules to explain NLP classifiers. Rules are extracted with a data mining approach on a semantically enriched input representation, instead of using words/wordpieces solely. Semantic information yields more abstract and general rules that are both more explanatory and less complex, while being also better at reflecting the model behaviour. In the experiments we focus on Named Entity Recognition, an NLP task where explainability is under-investigated. We explain the predictions of BERT NER classifiers trained on two popular benchmarks, CoNLL03 and Ontonotes, and compare our model against LIME (Ribeiro et al., 2016) and Decision Trees.\n\nKeywords: Explainable AI, XAI, Explainable ML, Named Entity Recognition.\n\n1\n\nINTRODUCTION\n\nIn recent years, Artificial Intelligence (AI) algorithms, especially deep learning models, are emerging in many applications, reporting state-of-the-art performances in many fields. In NLP, for example, the use of Large Language Models (LLM) based on huge deep neural networks achieved impressive results in many linguistic tasks. However, despite the remarkable results, deep approaches are still far from being fully exploited in real world applications. One major issue is the lack of interpretability and control of the models’ predictions. This is often an important requirement for many industrial applications, especially in domains like medicine, defense, finance and law, where it is crucial to understand the decisions and build trust in the algorithms.\n\nThe increasing need to address the problem of interpretability and improve model transparency made the “Explainable Artificial Intelligence” a very popular research area in the Computer Science world. Explainable AI (XAI) or Interpretable AI or Explainable Machine Learning (XML) (Guidotti et al., 2021) is a broad area of research that studies and proposes AI approaches where humans can understand the causes underlying the decisions and predictions made by the machine (Vilone & Longo, 2021b). The AI algorithms can be usually grouped into two families (Vilone & Longo, 2021a): (a) white-box models, which include algorithms whose interpretation is given by design, and (b) black-box approaches where, on the other hand, the decision making process is “opaque” and hard to understand. White-box models such as linear regression, probabilistic classifiers or decision trees are significantly easier to explain and interpret, but, often, provide a low predictive capacity and they are not always capable of modeling the inherent complexity of the task. In black-box models, on the other hand, very little knowledge is available on how the input variables influence the final decision. The relationship between input and output is often the result of a complex composition of\n\n1\n\nmathematical functions and is not directly interpretable. Although classic ML approaches are still widespread, almost all the modern complex AI techniques, such as deep neural networks, are naturally opaque (Lipton, 2018). Thus many new methods aimed to make new models more explainable and interpretable have been proposed or are under investigation.\n\nMost explainability techniques rely on the “local” assumption where the descriptions are provided for each example and few approaches exist aiming at provide a interpretable description of the model as whole (glass-box). In this paper, we present SAGE (Semantic-Aware Global Explanations), a method to produce highly interpretable global rules to explain NLP classifiers. Rules are extracted using a data mining algorithm which exploits a semantically enriched input representation. Semantic information yields more abstract and general rules that are both more explanatory and less complex, while being also better at reflecting the model behaviour. In the experiments, we focus on Named Entity Recognition, an NLP task where explainability is under-investigated. In particular, we aimed to explain the predictions of a BERT-based ((Devlin et al., 2018)) NER classifier trained and tested on two popular benchmarks, CoNLL03 (Tjong Kim Sang & De Meulder, 2003) and Ontonotes (Pradhan et al., 2013). We compare the proposed model against LIME (Ribeiro et al., 2016), which currently is one of the most popular local explanation algorithm in NLP and Decision Tree classifiers that are classic explainable by design, global explanation models.\n\nFor the assessment, we exploited two commonly used metrics: fidelity and complexity. The results show that the proposed approach infers a set of rules that reproduce the behavior of the model more accurately than both LIME and Decision Trees.\n\nThe paper is organized as follow. In Section 2, we summarize related works, while in Section 3 the proposed algorithm is described in detail. Experiments are reported in Section 4, and finally conclusions and future works are drawn in Section 5.\n\n2 RELATED WORK\n\nAs deep learning models have become more complex, many methods have been proposed to interpret and explain the predictions of a model. Two main groups of XAI techniques exist: (1) local approaches, which aim to provide an interpretable explanation for each single prediction; (2) global approaches, which try to build a “white-box” version of the black-box model (thus interpretable by design).\n\nLocal Explanations. Local algorithms focus on finding an interpretable explanation of the prediction returned by the ML model for a given input example. Several approaches focus on interpreting the internal components of a black-box model with intent of shading some lights on its decision making process. In (Csisz ́ar et al., 2020), authors propose the use of fuzzy logic to “explain” each Artificial Neural Network unit. Similar techniques try to identify which features in a particular input vector contribute the most to a neural network’s decision. Layer-wise relevance propagation (LRP), for example, exploits a back-propagation similar algorithm to build a heat-map over the input features (Binder et al., 2016),(Montavon et al., 2019). This method proved to be very effective in Computer Vision, highlighting which pixels of an image contributed most to the final classification. Other methods such as (Lundberg & Lee, 2017) and (Simonyan et al., 2013) follow the same principles. Such techniques, however, are strongly related to the ML models’ family used and they can be employed only with neural networks. Furthermore they usually do not produce high-level explanations.\n\nSome model-agnostic approaches were proposed in (Ribeiro et al., 2016; 2018; Strumbelj & In particular, in (Ribeiro et al., 2016), LIME (Local Interpretable ModelKononenko, 2010). agnostic Explanations) is presented. LIME trains a white-box classifier, that learns the black-box output distribution on a neighborhood of a given example. Neighbors are obtained perturbing the input features (usually randomly). The explanation can be then constructed by selecting the input features that mostly affect the model prediction. Although these models have been widely applied in explainability problems, including NLP, some limitations occur when dealing with Large Language Models (LLMs) and Named Entity Recognition. LLMs tokenize text in subwords, like wordpieces (Schuster & Nakajima, 2012) in BERT, which requires particular attention in designing the text perturbation step. Masking and perturbation of the input introduce another issue for LLMs, that are contextual. Indeed random masking of tokens can produce artificial and inconsistent contexts, which\n\n2\n\nmay invalidate the identification procedure of significant features. Finally, performing hundreds of input perturbation per sample, can be an expensive process, especially in problems like Named Entity Recognition (NER), where the number of examples corresponds to the number of tokens in a dataset.\n\nMost of the interpretable or explainable models for NLP is on text classification problems (Chen & Ji, 2020; Chen et al., 2021). For instance, (Alvarez-Melis & Jaakkola, 2017) proposed a method for interpreting sequence-to-sequence models in Machine Translation, whereas (Tuan et al., 2021) focused on local explanations in dialog generation. To the best of our knowledge, we are the first investigating XAI techniques in NER.\n\nGlobal Explanations. While the explanation of a single prediction provides the user with a rationale for the classifier’s behavior, it is not sufficient to convey confidence in the model as a whole. Thus, in contrast to local approaches, global explanation algorithms aim to find an interpretable description of the model in the whole input space.\n\nWhite-box models, such as Decision Trees (Breiman et al., 2017) and Bayesian Rule Lists (BRLs) (Letham et al., 2015), are explainable by design models that are used to provide global explanations. Alternatively, one could extrapolate global rules from local explanations as in (Ribeiro et al., 2016), where authors proposed an extension of LIME for making global explanations. The resulting SPLIME algorithm is based on the construction of an explanation matrix which collects the local importance of the interpretable components for each instance in a dataset X. For each component, a global importance in the explanation space is evaluated and the “Submodular pick (SP)” algorithm selects a subset of instances with the highest marginal coverage gain.\n\nOur proposed method (SAGE) falls in this family. Unlike SP-LIME, it is natively global by design and aims to identify the set of semantically-enriched associative rules that best cover the behavior of the model on a reference set (fidelity). Our algorithm shares some principles behind BRLs, however, the mining process is specifically designed and optimized to scale in NLP problems like NER. Furthermore, differently from BRLs, SAGE is not used as an interpretable model per-se, but it is rather an algorithm to explain globally black-box classifier predictions. To the best of our knowledge, SAGE is the first method involving semantic knowledge in the generation of the explanations.\n\n3 SEMANTIC-AWARE GLOBAL EXPLANATIONS\n\nIn this Section, we present SAGE (Semantic-Aware Global Explanations) algorithm, a method to discover globally significant explanation rules. To find a global and interpretable description of a black-box classifier, the proposed method follows a data-mining approach. In particular, given a black-box model ΘBB, the general idea is to apply an Association Rules Discovery algorithm to the learning set coupled with the model’s predictions to find the most representative set of explanation rules for the original model. Although it is task agnostic, we present SAGE by exploiting the NER task. Since we are facing a NLP task, the algorithm employs semantic features to improve the generalization capabilities of the returned rules. A sketch of SAGE method is depicted in Figure 1. In this section, firstly we discuss how to formulate NER in the scope of data-mining task. Next, we\n\nFig. 1: Sketch of Semantic-Aware Global Explanations (SAGE) method.\n\ndelineate the data mining algorithm to generate candidate rules and the strategies for pruning and selecting a restricted number of explanations.\n\n3\n\nTextual CorpusBlack-Box ModelAssociation Rules DiscoveryNLP EngineSAGEPredictionsExplanation RulesWhy?Fig. 2: Transactions creation. The function Φ produce a transaction for each token x in the corpus. In particular, the example shows the transactions created for the three tokens twenty, second and Of. In the example, Φ considers a ( 3, +3) context window and includes semantic/syntactic information for each token in the window (e.g. the POS tags).\n\n−\n\n3.1 NER AS A DATA MINING PROBLEM\n\nNamed Entity Recognition (NER) is typically addressed as token classification task. Given a corpus of sentences/documents X and its corresponding annotations Y, the goal is to predict a sequence of labels y ∈ Y, y := (y1, . . . , yn), from a sequence of tokens x ∈ X , x := (x1, . . . , xn).\n\nIn data mining, data is organized in a collection of transactions. A transaction ti consists in a set of ni items drawn from a specific basket B: ti = {ei,1, . . . , ei,ni}, where ei,j ∈ B represents the jth element of the ith transaction. A transaction is often also referred to as “itemset”. Given a large set of transactions T = {t1, ..., tm}, approaches as apriori (Agrawal et al., 1994) aim to discover frequent co-occurrence patterns (frequent itemset) in form of implications rules, e.g. {er, es, et} =⇒ ev. A such rule states that when the premise is satisfied (i.e. the elements in the left part occur), the consequence should be considered (i.e. also the elements in the right part should occur).\n\nTackling NER with data mining requires to arrange textual data into transactions. We define the basket as the vocabulary containing the tokens from both sets X and Y. Moreover, suppose each token xi is represented with a set of features obtained by a function Φ : x, i → t , then each feature can be seen as an item of the token transaction. Beside its representation, also the label yi is included in the basket. Consequently, we obtain a corpus of transactions T where each row is associated to its target label (Φ(x, i), yi). For the sake of simplicity, from now on we define Φ(xi) := Φ(x, i). In its simplest form, function Φ is the identity, returning the token xi itself, or xi and a window of surrounding tokens. In general, it can be any function that returns a sequence of symbols. Since our goal is to obtain rules to predict NER classes, we are interested only in implication patterns where the conclusion is the target label yi, i.e. (. . .) =⇒ yi kind. This allows us to divide the problem into independent sub-problems, one per entity class in the corpus, on a reduced set of transactions Tyi. Since each class is treated independently, the generation of frequent patterns is dramatically simplified, which is a crucial aspect for the scalability of apriori-like methods to NER datasets.\n\nExplaining a NER model is an analogous problem, the only difference is that labels Y are the output predicted by a classifier, instead of being provided by an oracle.\n\n3.2 SEMANTIC FEATURES\n\nInstead of simply using plain words or wordpieces as transaction items, we enrich them with semantic and syntactic positional information, extracted by Φ(xi) that is a NLU system as shown in Figure 2. We emphasize the importance of having more general rules because they are more humanreadable and reduce the overall number of explanations required to explain a model. We believe that they are essential to make explanations scalable in language, where data is sparse, especially in NER, and information is also positional. For the NLP analysis we employed a proprietary NLP engine1 and we extract POS tagging information, entity types, concepts (e.g. synsets) and classical NER features such as is digit, is upper case, is title that are often important named entities indicators.\n\n1The expert.ai NLP Platform - https://www.expert.ai\n\n4\n\nOf course it ends on the twenty second of JulyOOf course it ends on the twenty second ofO......Of course it ends on the twenty second ofB-DATEOf course it ends on the twenty second ofJlI-DATE......You know Tufts yeah . sent her to TaiwaB-ORG......-3:word:ends, -2:word:on, -1:word:the, 0:word:twenty, +1:word:second,+2:word:of, +3:word:July ...,-1:POS:the, 0:POS:Noun, +1:POS:Noun, ... ..., 0:entity_type:measurament, ... ... -3:word:on, -2:word:the, -1:word:twenty, 0:word:second, +1:word:of, +2:word:July, +3:word:. ... ...0:word:Of, +1:word:course, +2:word:it, +3:word:ends... +3:POS:VP.......DATEOTHER3.3 SAGE\n\nSAGE is based on the FP-growth (Han et al., 2000) algorithm, an efficient variant of apriori (Agrawal et al., 1994). Due to the combinatorial nature of the problem and the relatively large amount of items in the transactions, the algorithm considers several thresholds: (1) the minimun support threshold (mingensup) which is commonly used in all data-mining algorithms to limit the outdegree of the candidate generation; (2) a minimum confidence threshold (mingenconf ) which, following (Wang et al., 2002), we introduced to retain the candidates generation to the most promising frequent patterns only; (3) the tolerance threshold (τ ) which we introduced to stop adding items to a rule when confidence improvement is below such value; (4) a maximum rule length threshold (maxrulelen) which limits the length of generated rules to a maximum value; (5) a max number of rules to generate (maxnrules), which sets a maximum number of rules that can be generated. The algorithm is reported in Algorithm 1 and consists of three main procedures, each designed to face a specific step:\n\nAlgorithm 1 Semantic-Aware Global Explanations\n\nfunction SAGE( do\n\nfor yi\n\nX\n\n,\n\n)\n\nY\n\n∈ Y\n\nyi ,\n\nT R\nR\n\nT ←\n←\n\nEyi ←\n\nend for end function\n\nGiven τ, mingensupi, mingenconfi, maxruleleni, maxnrulesi\n\nGET-TRANSACTIONS(\n\n, yi)\n\n¬yi ← CANDIDATES-GEN( yi , PRUNE(\n\nT ¬yi , R)\n\nT\n\nT\n\nX\n\nT\n\nSELECTION(R, niters, maxnrulesi)\n\nyi ,\n\n¬yi , τ , mingensupi, mingenconfi, maxruleleni)\n\n3.3.1 CANDIDATES GENERATIONS\n\nThe core of data-mining algorithms is the candidates generations step. At each iteration, the function CANDIDATES-GEN(·) produces a new set of frequent itemsets for the next step, starting from current set of transactions and using the transaction dataset as statistical reference. In particular, we exploited FP-growth (Han et al., 2000), an efficient variant of apriori (Agrawal et al., 1994).\n\n3.3.2 RULES PRUNING\n\nRules mining can produce many patterns that are often spurious and/or highly overlapping. The over-generation of rules harms both the quality and the interpretability of the explanations. As a matter of fact, simplicity is crucial to deliver clear explanations that humans can easily understand. Thus, we aim to generate a minimal rule set with limited overlaps and high faithfulness. We proceed in two steps: pruning and selection.\n\nIn the pruning procedure , we start from the observation that most of the rules co-occur in the same subset of baskets, making them pseudo duplicate rules. Hence, among them we prune less generic rules, i.e. the ones having lower support. In case of rules that triggers in the exact same set of transactions, we favor more abstract rules, i.e. the ones having less conditions and/or more highlevel concepts as items. Pruning step typically drops the number of relevant patterns by an order of magnitude2. Pseudo code is presented in Algorithm 2.\n\n3.3.3 RULES SELECTION\n\nAfter pruning, a large number of candidates still remains. Most of them have high overlap in the corpus. In the second step we aim to keep the optimal subset S∗ ∈ S of rules that provides the best performances. The problem is combinatorial - there are 2|S| subsets - thus, we perform a greedy strategy. For a specified number of iterations niters, we filter candidates based on their support and confidence and then sub-select at most maxnrules such that they maximize a performance metric. Support and confidence thresholds are sampled with bayesian optimization. Rules sub-selection is performed greedly: the procedure starts from an empty state (no rules solution) and adds one rule\n\n2See Appendix A for further details.\n\n5\n\nAlgorithm 2 Rules pruning\n\nyi ,\n\nT\n\n¬yi , R)\n\nT\n\nfunction PRUNE( Rpruned R\n← for r\n\n← ∅ R do\n\nSORT-BY-SUPPORT-AND-ABSTRACTION(R, descending)\n\n∈ r\nT ←\nif ∄p\n\nSUPPORT-TRANSACTIONS(r,\n\nyi , T\nSUPPORT-TRANSACTIONS(p,\n\n¬yi )\n\nT\n\nRpruned\n\n∈\n\nRpruned\n\nr\n\n|T ⊆\nRpruned\n\nr\n\n∪\n\n←\n\nyi ,\n\nT\n\n¬yi ) then\n\nT\n\nend if\n\nend for return Rpruned\n\nend function\n\nat a time, storing the configuration that provide the higher improvement to the state. Despite the approach is heuristic, it improves the quality of the solution, both in terms of fidelity and complexity reduction as depicted in the section 4. Selection procedure is described in Algorithm 3.\n\nAlgorithm 3 Explanations selection\n\nfunction SELECTION(R, niters, maxnrules)\n\nS∗ ← ∅ for i = 1,\n\n, niters do\n\n· · ·\n\nsupp, conf R\nS while r\n\n← ← ∅ |\n←\n\nS\n\n< maxnrules and\n\nS |\nCOMPUTE-FIDELITY(S\n\n<\n\nR\n\n|\n\n|\n\n|\n\ndo\n\n| arg max r∈R\\S\n\nr)\n\n∪\n\nBAYESIAN-HYPERPARAM-OPTIMIZATION(supprange, conf range)\n\nFILTER(R, supp, conf )\n\n←\n\nif COMPUTE-FIDELITY(S\n\nbreak\n\nr)\n\n∪\n\n≤\n\nCOMPUTE-FIDELITY(S) then\n\nend if S\nS end while if COMPUTE-FIDELITY(S) > COMPUTE-FIDELITY(S∗) then\n\n←\n\n∪\n\nr\n\nS∗ end if\n\nS\n\n←\n\nend for return S∗ end function\n\n4 EXPERIMENTS\n\nWe evaluate SAGE in two different NER benchmarks, aiming to measure (1) the quality of the explanations, (2) how semantic information is vital to produce generalized rules that are both more effective and easier to understand.\n\n4.1 METRICS\n\nFor evaluating the explanations, we consider two popular criteria: fidelity and complexity.\n\nFidelity. Fidelity measures the ability of the explanation algorithm to mimic the predictions of the black-box classifier (e.g. BERT). To measure such index, we considered the F1 score (both micro and macro). In this setting, the optimal value F 1 = 100% means that the explainable model perfectly reflects the behavior of the black-box model.\n\nComplexity. Complexity on the other hand, measures the simplicity of resulting explanations. We define it as the number of rules per class extracted by the data-mining algorithm.\n\n6\n\n4.2 DATASETS\n\nOntonotes. Ontonotes3 is a dataset containing named entity annotations for multiple languages. There are multiple versions, in the experiments we used English v12 (Pradhan et al., 2013). The corpus is composed of texts from various genres. Overall, there are 18 annotated classes.\n\nCoNLL03. CoNLL034 (Tjong Kim Sang & De Meulder, 2003) is a standard Named Entity Recognition (NER) benchmark. The dataset is a collection of news articles from the Reuters corpus. Data is annotated with four entity tags: PERSON, LOCATION, ORGANIZATION and MISC.\n\n4.3 EXPERIMENTAL SETUP\n\nBlack-box classifiers. We train a BERT (Devlin et al., 2018) classifier on each benchmark (CoNLL, Ontonotes) and we explain its predictions on the test set. Predictions are made at word level, i.e. we ignore the output of word sub-tokens as commonly done in NER problems with models based on word-pieces like transformers. Overall, we obtain 4170 and 14227 predicted entities in CoNLL and Ontonotes, respectively.\n\nLIME for NER. LIME is widely used for explaining models in classification problems. However, there are some caveats when applying it in NER. Indeed, predictions are at token-level, while usually the approach is applied for document-level classification problems. Thus, explanations must be positional. Furthermore, current Language Models adopt subword-level tokenizers, while we expect to have word-level explanations. Hence, wordpieces of the same words are perturbed together. For each example, we retrieve the top-k words to build an explanation. Global explanations are then constructed applying submodular-pick (SP-LIME-k). In the experiments, we consider five different SP-LIME models differentiated by the choice of k that varies from 1 up to 5.\n\nDecision Tree. Decision Trees (DTs) are considered to be white box classifiers, therefore they are widely used for explainability. We compare SAGE with DTs fitted with (DT) and without (DTwords) the semantic features. We fit trees with maximum depth of 100.\n\nSAGE. Transactions are constructed as described in Section 3. Function Φ produces for each token xi a set of positional features in a (−3, +3) context window, as presented in Section 3.2 and depicted in Figure 2. In the results we also report performances of a variant, called SAGE-words, that exploit a simpler Φ function that yields only words in the (−3, +3) scope, i.e. without any semantic information. Hyper-parameters of Algorithm 1 could be set differently for each entity. In the experiments, however, most of the hyper-parameters are kept the same for all classes and on both datasets. In particular, we set τ = 0.05, maxrulelen = 5, niters = 100 and maxnrules = 100 everywhere. Only mingensup and mingenconf change depending on the number of occurrences of the class. The minumum confidence to further explore a rule is set dinamically to mingenconfi = |Ti| |T | , whereas the minimum support is set to 10 for all entities in CoNLL, and varies between {2, 5, 15} in Ontonotes depending on the cardinality of Ti and such that large classes have higher mingensup and, vice versa, underrepresented entities have smaller mingensup.\n\n4.4 RESULTS\n\nQuality of explanations. We evaluate the quality of SAGE explanations in terms of fidelity and complexity as defined in Section 4.1. In particular, we measure how the increase of complexity of the explanations improves the fidelity to the original model, which may be useful to determine a satisfactory trade-off between the two indicators. Results are reported in Figure 3.\n\nSAGE significantly outperforms SP-LIME global explanations. Even a single rule per-class outreaches any SP-LIME setup. Figure 4 shows the F1 scores for each single entity class in Ontonotes. We can see, that SAGE always outperforms SP-LIME explanations, with the exception of LANGUAGE and PRODUCT, two underrepresented classes where SAGE performs poorly because it cannot find rules that are statistically significant enough.\n\n3https://huggingface.co/datasets/conll2012_ontonotesv5/ 4https://huggingface.co/datasets/conll2003.\n\n7\n\nFig. 3: Explanation performances of our method (with and without semantics) and different models built with SP-LIME. We compare all methods in terms of μ-fidelity score at the varying of explanation complexity (number of explanations per class) on CoNLL03 (left) and Ontonotes (right).\n\nFig. 4: Class-wise μ-fidelity scores for each approach in Ontonotes dataset.\n\nThere are several reasons responsible for causing such a gap between SAGE and SP-LIME. First of all, LIME is a local algorithm designed to provide local explanations of predicted examples, and SP-LIME combines them together to provide an overall view of them. However, each explanation is not designed to hold outside the specific example, whereas our approach aim to retrieve statistically relevant rules that trigger a class in the data. Furthermore, token classification tasks, such as NER, introduce additional complexity because explanation must be positional, and because most of the tokens do not belong to an entity. Lastly, as we will see shortly, semantics is vital to boost performances of our approach.\n\nOne could notice however, that SP-LIME-k, k > 1 have a linear growth as the number of explanations increases, and it may eventually surpass SAGE performances, complexity → ∞. As a matter of fact, this will happen, because by construction SP-LIME will retrieve one explanation per example, falling back to providing local explanations. This situation occurs in Ontonotes for class LANGUAGE. Looking at Figure 4, we can see that SP-LIME methods are performing extremely well on such entity, because the number of annotated tokens is only 22, hence SP-LIME extracts one rule per example. However, the number of explanations to reach and surpass SAGE is extremely high, breaking even far from a point where the explanation is easy to understand.\n\nSAGE also outperforms Decision Trees, as outlined in Table 1, also providing rules that are naturally more human-friendly than tree decision paths that may involve dozens of conditions.\n\nThe impact of semantics. Semantic information plays a crucial role in providing better rules. We analyse this aspect comparing SAGE with SAGE-words, a variant where a token xi is represented only by itself and text in its context, without any external knowledge. Results are outlined in Table 1 and Figure 3.\n\nIn all the scenarios, semantics provides a significant gain in terms of fidelity. One explanation per class in SAGE outperforms 100 rules of SAGE-words. Indeed, abstraction is a mean to group less general concepts into a single one. It allows to express rules with higher coverage, and it is particularly important for language, that is characterized by the presence of many discrete symbols (words) combined together. A clear example of how semantics yields more general rules can be\n\n8\n\n020406080100Complexity0.00.10.20.30.40.50.60.70.8SAGESAGE-wordsSP-LIME-1SP-LIME-2SP-LIME-3SP-LIME-4SP-LIME-5Fidelity020406080100Complexity0.00.10.20.30.40.50.60.7SAGESAGE-wordsSP-LIME-1SP-LIME-2SP-LIME-3SP-LIME-4SP-LIME-5FidelityCARDINALDATEEVENTFACGPELANGUAGELAWLOCMONEYNORPORDINALORGPERCENTPERSONPRODUCTQUANTITYTIMEWORK_OF_ARTNER classes0.00.20.40.60.81.0SAGESAGE-wordsSP-LIME-1SP-LIME-2SP-LIME-3SP-LIME-4SP-LIME-5FidelityExplainer\n\nDT-words DT SAGE-words SAGE\n\nCoNLL03 μ-fidelity m-fidelity\n\nOntonotes μ-fidelity m-fidelity\n\n34.8 76.0 45.6 80.3\n\n34.2 74.5 40.0 78.5\n\n7.4 32.1 51.5 70.3\n\n17.3 32.6 44.7 57.4\n\nTab. 1: Evaluation of explanations in CoNLL03 and Ontonotes datasets for our approach and Decision Trees (DT), both with and without semantic features.\n\nseen in Figure 5. SAGE rules exploit broader concepts that are valid for much more examples. Furthermore, the effectiveness of semantics is independent from the explainer algorithm. Even\n\ndecision trees benefit from it (see Table 1).\n\nFig. 5: Explanation rules discovered by SAGE, SAGE-words and SP-LIME-1 in Ontonotes with complexity 1. Semantic features are highlighted in bold.\n\n5 CONCLUSIONS\n\nIn this paper, we presented SAGE, a post-hoc method to extract interpretable, global explanations for NLP. A key ingredient of SAGE is the fact that it produces rules based on semantic, more abstract information. We focused on Named Entity Recognition, a particularly challenging NLP problem, which was under-investigated in the scope of explainability. We compared our model against different SP-LIME variants on two BERT models trained in two popular NER benchmarks: CoNLL03 and Ontonotes. Results indicate that our global explanations are more compact (hence more interpretable) and significantly better at mimicking the black-box classifier, although SAGE fails in making explanations of few particularly underrepresented classes. We also prove empirically that semantic and syntactic information bring a major boost to the explanation quality.\n\nIn the future we plan to investigate possible strategies to guarantee the production of explanations for any class, regardless the amount of predictions of such entity. In particular, we plan to combine local approaches, such as LIME, to SAGE.\n\n9\n\nCARDINALDATEEVENTSAGEpos:ADJ∧word:onesynset:definitetime1:ent:EVN∧pos:ART∧3:istitleSAGE-wordsword:two∧-1:word:theword:year1:word:Year∧word:NewSP-LIME-1word:two1:word:yearword:KatrinaFACGPELANGUAGESAGEistitle∧1:synset:roadent:GEO-SAGE-words1:word:Roadword:China-SP-LIME-13:word:Roadword:Chinaword:EnglishLAWLOCMONEYSAGE1:synset:billofrightsent:GEA∧synset:geographicareaent:MONSAGE-words1:word:Amendment-1:word:Middle-1:word:$SP-LIME-1word:Gramm1:word:Middle-1:word:$NORPORDINALORGSAGEsynset:Asianword:firstent:COMSAGE-wordsword:Chineseword:first1:word:IncSP-LIME-1word:Americanword:firstword:CNNPERCENTPERSONPRODUCTSAGEent:PCTpos:NPR.NPHword:coleSAGE-words-1:word:.∧1:word:%-2:word:Mrword:ColeSP-LIME-13:word:%word:Bushword:ColeQUANTITYTIMEWORKOFARTSAGEent:MEA∧-3:pos:NOUent:HOUistitle∧ent:MEASAGE-words1:word:degreesword:hour1:word:MinutesSP-LIME-11:word:degrees1:word:morningword:Sixty1REFERENCES\n\nRakesh Agrawal, Ramakrishnan Srikant, et al. Fast algorithms for mining association rules. In Proc. 20th int. conf. very large data bases, VLDB, volume 1215, pp. 487–499. Santiago, Chile, 1994.\n\nDavid Alvarez-Melis and Tommi S Jaakkola. A causal framework for explaining the predictions of\n\nblack-box sequence-to-sequence models. arXiv preprint arXiv:1707.01943, 2017.\n\nAlexander Binder, Gr ́egoire Montavon, Sebastian Lapuschkin, Klaus-Robert M ̈uller, and Wojciech Samek. Layer-wise relevance propagation for neural networks with local renormalization layers. In International Conference on Artificial Neural Networks, pp. 63–71. Springer, 2016.\n\nLeo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone. Classification and\n\nregression trees. Routledge, 2017.\n\nHanjie Chen and Yangfeng Ji. Learning variational word masks to improve the interpretability of\n\nneural text classifiers. arXiv preprint arXiv:2010.00667, 2020.\n\nHanjie Chen, Song Feng, Jatin Ganhotra, Hui Wan, Chulaka Gunasekara, Sachindra Joshi, and Yangfeng Ji. Explaining neural network predictions on sentence pairs via learning word-group masks. arXiv preprint arXiv:2104.04488, 2021.\n\nOrsolya Csisz ́ar, G ́abor Csisz ́ar, and J ́ozsef Dombi.\n\nInterpretable neural networks based Knowledge-Based Syson continuous-valued logic and multicriteria decision operators. tems, 199:105972, 2020. https://doi.org/10.1016/j.knosys. 2020.105972. URL https://www.sciencedirect.com/science/article/pii/ S0950705120302896.\n\nISSN 0950-7051.\n\ndoi:\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nRiccardo Guidotti, Anna Monreale, Dino Pedreschi, and Fosca Giannotti. Principles of explainable In Explainable AI within the digital transformation and cyber physical\n\nartificial intelligence. systems, pp. 9–31. Springer, 2021.\n\nJiawei Han, Jian Pei, and Yiwen Yin. Mining frequent patterns without candidate generation. ACM\n\nsigmod record, 29(2):1–12, 2000.\n\nBenjamin Letham, Cynthia Rudin, Tyler H McCormick, and David Madigan. Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics, 9(3):1350–1371, 2015.\n\nZachary C Lipton. The mythos of model interpretability: In machine learning, the concept of inter-\n\npretability is both important and slippery. Queue, 16(3):31–57, 2018.\n\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances\n\nin neural information processing systems, 30, 2017.\n\nGr ́egoire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert M ̈uller. Layer-Wise Relevance Propagation: An Overview, pp. 193–209. Springer International Publishing, Cham, 2019. ISBN 978-3-030-28954-6. doi: 10.1007/978-3-030-28954-6 10. URL https://doi.org/10.1007/978-3-030-28954-6_10.\n\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj ̈orkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. Towards robust linguistic analysis using OntoNotes. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pp. 143–152, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL https://aclanthology.org/W13-3516.\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135–1144, 2016.\n\n10\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic explanations. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n\nMike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5149–5152. IEEE, 2012.\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.\n\nErik Strumbelj and Igor Kononenko. An efficient explanation of individual classifications using\n\ngame theory. The Journal of Machine Learning Research, 11:1–18, 2010.\n\nErik F. Tjong Kim Sang and Fien De Meulder.\n\nIntroduction to the CoNLL-2003 shared task: In Proceedings of the Seventh Conference Language-independent named entity recognition. on Natural Language Learning at HLT-NAACL 2003, pp. 142–147, 2003. URL https: //aclanthology.org/W03-0419.\n\nYi-Lin Tuan, Connor Pryor, Wenhu Chen, Lise Getoor, and William Yang Wang. Local explanation of dialogue response generation. Advances in Neural Information Processing Systems, 34:404– 416, 2021.\n\nGiulia Vilone and Luca Longo. Classification of explainable artificial intelligence methods through\n\ntheir output formats. Machine Learning and Knowledge Extraction, 3(3):615–661, 2021a.\n\nGiulia Vilone and Luca Longo. Notions of explainability and evaluation approaches for explainable\n\nartificial intelligence. Information Fusion, 76:89–106, 2021b.\n\nKe Wang, Liu Tang, Jiawei Han, and Junqiang Liu. Top down fp-growth for association rule mining. In Pacific-Asia conference on knowledge discovery and data mining, pp. 334–340. Springer, 2002.\n\nA SCALABILITY\n\nData mining approaches are combinatorial, therefore the complexity grows non-linearly with the increase of the data. One may wonder how Fp-growth adjustments, rules pruning and selection process of the algorithm contribute in controlling the potential explosion. To shed some light on this, we report in Table 2 the number of elements produced by each step in the Ontonotes dataset (the largest corpus in our experiments).\n\nSAGE\n\nSAGE-words\n\nSP-LIME-1\n\nSP-LIME-2\n\nSP-LIME-3\n\nSP-LIME-4\n\nSP-LIME-5\n\n⇒\n\nNORP\n\nNORP\n\n0:synset:Asian = ⇒\n0:synset:European = 0:word:Chinese = 0:word:American = 0:word:Chinese = 0:word:American = 2:word:. , 0:word:Chinese = 3:word:., 0:word:American = 4:word:., 1:word:North, 2:word:Koreans = -1:word:and, 16:word:., 0:word:American = 1:word:position, -1:word:the, 2:word:., 0:word:American = ⇒\n-2:word:and, -3:word:television, 3:word:., 0:word:Palestinian = 8:word:., -1:word:with, 6:word:intransigence, 1:word:North, 2:word:Koreans = -2:word:Russians, -1:word:and, -4:word:Europeans, 2:word:., 0:word:Chinese =\n\nNORP NORP NORP NORP\n\nNORP NORP\n\nNORP NORP\n\n⇒ ⇒\n⇒ ⇒\n\n⇒ ⇒\n\nNORP\n\n⇒ ⇒\n\n⇒\n\nNORP\n\nNORP NORP\n\n⇒ ⇒\n\nTab. 3: Two examples of generated rules per model for NORP entity in Ontonotes dataset. Only SAGE exploits semantics, in this case the synset features are important to build rules that are more generic. We can also notice how, the increase of k in SP-LIME models makes the explanation more and more wired to the example that generated it.\n\n11\n\nEntity EVENT WORK OF ART LAW LANGUAGE NORP FAC LOC PRODUCT TIME PERCENT MONEY QUANTITY ORG GPE PERSON DATE CARDINAL ORDINAL\n\n#tokens 351 594 222 22 1411 398 504 236 668 1483 1109 463 5604 4254 4151 4739 1795 322\n\n#generated candidates 57091 304724 23255 0\n14216 127311 36894 914 10184 181897 46497 18337 122722 411140 43143 22347 14137 488\n\n#pruning 3078 7781 1569 0\n1926 5205 2180 109 2327 26277 6871 1926 65749 24738 7746 6996 1621 91\n\n#selection 90 100 63 0\n78 84 30 7\n53 29 66 37 100 53 80 100 100 4\n\nALL\n\n28326\n\n1435297\n\n166190\n\n1074\n\nTab. 2: Number of elements produced by each step. In bold classes were the number of selected explanations was interrupted because the maxnrules was reached.\n\n12",
    "reference": "# Summary Of The Paper\n\nIn this paper the authors introduce a new post-hoc method to create explanations for a black box classifier. They explain the steps in the method using the task of Named Entity Recognition as example. They turn the data into baskets of information , by extracting the contextual window around each word as well as some semantic features like pos tags, synsets etc and then use a data mining algorithm to extract patterns of co-occurrence corresponding to each classification label. Pruning is done over these patterns or candidate explanations based on thresholds and explanations are then selected greedily using f1 score maximization. The perform evaluation on this method by measuring the F1 score vs complexity defined by the number of explanations and they compare this to variations of LIME as well as a version of their model without semantic information\n\n# Strength And Weaknesses\n\nStrengths:\n\n\t1. They support the necessity for each step in the algorithm\n\t2. They provide global explanations\n\t3. They are task agnostic , method can be applied to many NLP tasks\n\nWeaknesses:\n\n\t1. In section 3.1 they do not specify what certain notations mean  , eg the difference between the two transaction tables on the right of figure 2. \n\t2. Jump from section 3.2 to 3.3 is big especially for people who are unfamiliar with algorithms they point to such as FP-growth Han et al. (2000) and apriori Agrawal et al. (1994). They use an example for section 3.1 but then they drop the example for subsequent sections in the algortihm . \n\t3. Other evaluation metrics employed by other papers eg, fidelity to the model and comprehensibility could have been explored . Human evaluations might make a more compelling case .\n\t4. They don’t perform any study about which semantic features help and which harm the f1 score.\n\t5. Visualizaition is an important part of explainable models which this paper lacks\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity of the paper is a bit rocky.  They use some phrases repeatedly in multiple sections but do not elaborate them . Eg.They claim one caveat of LIME is that explanations must be positional. But their method is also dependent on a fixed contextual window .\nNovelty exists , in the transforming of the data into a data mining task and incorporating semantic information.\nReproducibility is good since they list the hyperparameters used for the datasets mentioned in the paper. But it is unclear how much of finetuning of hyperparamers took place and how resilient the method is to any change in these values and its effectiveness for a different dataset or different task.\n\n# Summary Of The Review\n\nAlthough the fundamental idea is good, it fails to convince that the method is robust across different datasets or NLP tasks.\nThey employ a contextual window of +/-3 tokens and that might not be transferrable to other NLP classification tasks like sentiment classifier or recommendation systems.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nONLINE PLACEBOS FOR CLASS-INCREMENTAL LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNot forgetting old class knowledge is a key challenge for class-incremental learning (CIL) when the model continuously adapts to new coming classes. A common technique to address this is knowledge distillation (KD) which penalizes prediction inconsistencies between old and new models. Such prediction is made with almost new class data, as old class data is extremely scarce due to the strict memory limitation in CIL. In this paper, we take a deep dive into KD losses and find that “using new class data for KD” not only hinders the model adaption (for learning new classes) but also results in low efficiency for preserving old class knowledge. We address this by “using the placebos of old classes for KD”, where the placebos are chosen from a free image stream, such as Google Images, in an automatical and economical fashion. To this end, we train an online placebo selection policy to quickly evaluate the quality of streaming images (good or bad placebos) and use only good ones for one-time feed-forward computation of KD. We formulate the policy training process as an online Markov Decision Process (MDP), and introduce an online learning algorithm to solve this MDP problem without causing much computation costs. In experiments, we show that our method 1) is surprisingly effective even when there is no class overlap between placebos and original old class data, 2) does not require any additional supervision or memory budget, and 3) significantly outperforms a number of top-performing CIL methods, in particular when using lower memory budgets for old class exemplars, e.g., five exemplars per class. The code is available in the supplementary.\n\n1\n\nINTRODUCTION\n\nAI learning systems are expected to learn new concepts while maintaining the ability to recognize old ones. In many practical scenarios, they cannot access the past data due to the limitations such as storage or data privacy but are expected to be able to recognize all seen classes. A pioneer work (Rebuffi et al., 2017) formulated this problem in the class-incremental learning (CIL) pipeline: training samples of different classes are loaded into the memory phase-by-phase, and the model keeps on re-training with new class data (while discarding old class data) and is evaluated on the testing data of both new and old classes. The key challenge is that re-training the model on the new class data tends to override the knowledge acquired from the old classes (McCloskey & Cohen, 1989; McRae & Hetherington, 1993; Ratcliff, 1990), and the problem is called “catastrophic forgetting”. To alleviate this problem, most of CIL methods (Rebuffi et al., 2017; Hou et al., 2019; Douillard et al., 2020; Liu et al., 2020a; 2021a; Zhu et al., 2021) are equipped with knowledge distillation (KD) losses that penalize any feature and/or prediction inconsistencies between the models in adjacent phases.\n\nThe ideal KD losses should be computed on old class data since the teacher model (i.e., the model in the last phase) was trained on them. This is, however, impossible in the CIL setting, where almost all old class data are inaccessible in the new phase. Existing methods have to use new class data as a substitute to compute KD losses. We argue that this 1) hampers the learning of new classes as it distracts the model from fitting the ground truth labels of new classes, and 2) can not achieve the ideal result of KD, as the model can not generate the same soft labels (or features) on new class data as on old class data. We justify this from an empirical perspective as shown in Figure 1 (a):\n\nWe made the corresponding changes in the revised paper and colorized these changes in blue.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Average accuracy\n\n(b) Conceptual illustrations of the CIL losses\n\n(c) Selected placebos\n\nFigure 1: (a) Average accuracy when computing KD loss on different data using iCaRL (Rebuffi et al., 2017) on CIFAR-100. The KD loss (softmax KL divergence loss) is computed on new class data (dark blue), placebos (of old class data) selected by our method (light blue), and old class data green), i.e., the ideal case. (b) Conceptual illustrations of the loss problem if using new class data for KD. The dark blue and orange numbers denote the predictions of old and new classes, respectively. It is clear in (i) that the objectives are different when using a new class sample for KD (the oracle case is to have both “ascent”), e.g., the ground truth label for the second old class is 0, while the “KD label” at this position is 0.8. This is not an issue when using the old class sample, e.g., in (ii), its ground truth label and “KD label” have consistent magnitudes at the same position (1 and 0.7, respectively). (c) Our selected placebos for two old classes (“road” and “table”) and their activation maps using GradCAM (Selvaraju et al., 2017) on CIFAR-100. The free image stream is ImageNet-1k that does not have class overlap with CIFAR-100. They are selected because their partial image regions contain similar visual cues with old classes.\n\nthe upper bound of KD is achieved when using “Old class data”, and if compared to it, using “New class data” sees a clear performance drop for recognizing both old and new classes. In Figure 1 (b), we show the reason by diving into loss computation details: when using new class samples (as substitutes) to compute CE and KD losses simultaneously, these two losses are actually weakening each other, which does not happen in the ideal case of using old class samples.\n\nUsing unlabeled external data (called placebos in this paper) has been shown to be practical and effective in solving the above issue. Compared to existing works (Lee et al., 2019; Zhang et al., 2020), this paper aims to solve two open questions. Q1: How to adapt the placebo selection process in the non-stationary CIL pipeline. The ideal selection method needs to handle the dynamics of increasing classes in CIL, e.g., in a later incremental phase, it is expected to handle a more complex evaluation on the placebos of more old classes. Q2: How to control the computational and memoryrelated costs during the selection and utilization of placebos. It is not intuitive how to process external data without encroaching the memory allocated for new class data or breaking the strict assumption of memory budget in CIL.\n\nWe solve these questions by proposing a new method called PlacoboCIL that adjusts the policy of selecting placebos for each new incremental phase, in an online and automatic fashion and without needing extra memory. Specifically, to tackle Q1, we formulate the PlaceboCIL as an online Markov Decision Process (MDP) and introduce a novel online learning algorithm to learn a dynamic policy. In each new phase, this policy produces a phase-specific function to evaluate the quality of incoming placebos. The policy itself gets updated before the next phase. For Q2, we propose a mini-batchbased memory reusing strategy for PlaceboCIL. Given a free data stream, we sample a batch of unlabeled data, evaluate their quality by using our phase-specific evaluation function (generated by the learned policy), and keep only the high-quality placebos to compute KD losses. After this, we remove this batch totally from the memory before loading a new batch. In our implementation, this batch can be very small, e.g., 200 images. We randomly remove the same size of new class data for intaking this batch to keep the strict assumption of memory budget.\n\nWe evaluate PlaceboCIL by incorporating it into multiple strong baselines such as PODNet (Douillard et al., 2020), LUCIR (Hou et al., 2019), AANets (Liu et al., 2021a), and FOSTER (Wang\n\n2\n\nNew ClassOld Class4045505560Accuracy (%)New class data Placebos (ours) Old class dataKD on{(i) The CIL loss for a new class training sample(ii) The CIL loss for an old class training samplenew model outputs old model outputs labelsCEKDCEKDbreakwaterpolice vandishwasherpianoRoadTableUnder review as a conference paper at ICLR 2023\n\net al., 2022), and conducting a series of ablative studies. Our results on three popular CIL benchmarks show the clear and consistent superiority of PlaceboCIL, especially when using a low memory budget for old class exemplars. For example, our method boosts the last-phase accuracy by 6.9 percentage points on average when keeping only 5 exemplars per old class in the memory. In addition, it is worth mentioning that PlaceboCIL is surprisingly efficient even when there is no class overlap between placebos and original old class data. The reason is that PlaceboCIL can make use of the local visual cues in placebos, e.g., similar visual cues of “table” are found on the local regions of an “piano” (and “dishwasher”) image as shown in Figure 1 (c).\n\nOur contributions are three-fold. 1) A generic PlacoboCIL method that selects placebo images from a free image stream to solve the KD issue in existing methods. 2) A novel online learning algorithm for training a placebo selection policy and a mini-batch-based memory reusing strategy to avoid extra memory usage. 3) Extensive comparisons and visualizations on three CIL benchmarks, taking top-performing CIL models as baselines and with the same strict assumption on memory.\n\n2 RELATED WORK\n\nClass-incremental learning (CIL) methods can be divided into three categories. Distillation-based methods introduce different knowledge distillation (KD) losses to consolidate previous knowledge. The key idea is to enforce prediction logits (Li & Hoiem, 2016; Rebuffi et al., 2017), feature maps (Douillard et al., 2020), or other essential information (Tao et al., 2020; Wang et al., 2022; Simon et al., 2021; Joseph et al., 2022) to be close to those of the pre-phase model. Memory-based methods use a small number of preserved old class data (called exemplars) (Rebuffi et al., 2017; Shin et al., 2017; Liu et al., 2020a; Prabhu et al., 2020) or augmented data (Zhu et al., 2021) to recall the old class knowledge. Network-architecture-based methods (Rusu et al., 2016; Xu & Zhu, 2018; Abati et al., 2020; Yan et al., 2021) design incremental network architectures by expanding the network capacity for new class data or freezing partial network parameters to keep the old class knowledge. Our method can be used to improve different Distillation-based CIL methods.\n\nSome prior works used unlabeled external data for class-incremental learning. Lee et al. (2019) proposed a confidence-based sampling method to select unlabeled external data to compute a speciallydesigned global distillation loss. Zhang et al. (2020) randomly selected unlabeled samples and used them to compute KD loss for model consolidation. Liu et al. (2020b) used unlabeled data to maximize the classifier discrepancy when integrating an ensemble of auxiliary classifiers. Our method differs from them in two aspects. 1) Our method use the unlabeled data in a more generic way and can be applied to improve different distillation-based methods (Hou et al., 2019; Rebuffi et al., 2017; Wang et al., 2022), while the existing methods use unlabeled data to assist their specially-designed loss terms or components. 2) We train an online policy to select better unlabeled data to adapt to the non-stationary CIL pipeline while existing methods select unlabeled data by applying fixed (i.e., non-adaptive) rules in all incremental phases.\n\nOnline learning observes a stream of samples and makes a prediction for each element in the stream. There are mainly two settings in online learning: full feedback and bandit feedback. Full feedback means that the full reward function is given at each stage. It can be solved by Best-Expert algorithms (Even-Dar et al., 2005). Bandit feedback means that only the reward of the implemented decision is revealed. If the rewards are independently drawn from a fixed and unknown distribution, we may use e.g., Thompson sampling (Agrawal & Goyal, 2012) and UCB (Auer & Ortner, 2010) to solve it. If the rewards are generated in a non-stochastic version, we can solve it by e.g., Exp3 (Auer et al., 2002). Online MDP is an extension of online learning. Many studies (Even-Dar et al., 2009; Li et al., 2019) aim to solve it by converting it to online learning. In our case, we formulate the CIL as an online MDP and convert it into a classic online learning problem. The rewards in our MDP are non-stochastic because the training and validation data change in each phase. Therefore, we design our algorithm based on Exp3 (Auer et al., 2002).\n\n3 ONLINE PLACEBOS FOR CLASS-INCREMENTAL LEARNING (PLACEBOCIL)\n\nCIL has multiple “training-testing” phases during which the number of classes gradually increases to the maximum. In the 0-th phase, data D1:c0={D1, ..., Dc0}, including the training samples of c0 classes, are used to learn the model Θ0. After this phase, only a small subset of D1:c0 (i.e., exemplars\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ndenoted as E1:c0 ={E1, ..., Ec0}) can be stored in the memory and used as replay samples in later phases. In the i-th phase, we use ci to denote the number of classes we have observed so far. We get new class data Dci−1+1:ci ={Dci−1+1, ..., Dci} of (ci − ci−1) classes and load exemplars E1:ci−1 from the memory. Then, we initialize Θi with Θi−1, and train it using T1:ci=E1:ci−1∪Dci−1+1:ci. The model Θi will be evaluated with a testing set Q1:ci={Q1, ..., Qci} for all classes seen so far. Please note that in any phase of PlaceboCIL, we assume we can access a free image stream, where we can load unlabeled images and select placebos.\n\nPlaceboCIL formulates the CIL task as an online MDP. In each phase, we update a policy, for which we sample a class-balanced subset from training data as the testing set, and use the updated policy to produce a phase-specific evaluation function. During model training, we sample unlabeled images, use the evaluation function to quickly judge the image quality (good or bad placebos), and select the good ones to compute KD loss. In this section, we introduce the formulation of online MDP in Section 3.1, show how to apply the policy to select placebos and compute KD loss in Section 3.2, and provide an online learning algorithm to update the policy in Section 3.3. The pseudocode is given in Appendix A (Algorithms 1 and 2).\n\n3.1 ONLINE MDP FORMULATION FOR CIL\n\nThe placebo selection process in CIL should be online inherently: training data (and classes) get updated in each phase, so the placebo selection policy should update accordingly. Thus, it is intuitive to formulate the CIL as an online MDP. In the following, we provide detailed formulations.\n\nStages. Each phase in the CIL task can be viewed as a stage in the online MDP.\n\nStates. The state should define the current situation of the intelligent agent. In CIL, we use the model Θi as the state of the i-th phase (i.e., stage). We use S to denote the state space.\n\nActions. We define the action as ai=(βi, γi), consisting of the hyperparameters (βi and γi) used to create an evaluation function. As βi and γi vary in a continuous range, we discretize them to define a finite action space.1 We will elaborate on how to take an action and deploy the hyperparameters in Section 3.2.\n\nPolicy π={p(a|Θi)}a∈A is a probability distribution over the action space A, given the current state Θi. We will elaborate how to update the policy using our proposed online learning algorithm in Section 3.3.\n\nEnvironments. We take the training an testing data in each phase as the environment. In the i-th phase, the environment is Hi=(T1:ci, Q1:ci), where T1:ci is the training data and Q1:ci is the testing data. The environment is time-varying because we observe different training data (and classes) in each new phase.\n\nRewards. CIL aims to train a model that is efficient in recognizing all classes seen so far. Therefore, it is intuitive to use the testing accuracy as the reward in each phase. We cannot observe any reward (i.e., testing accuracy) directly because the testing data is not accessible during training. We solve this by building a local testing set using a subset of training data (see details in Section 3.3). Our objective is to maximize a cumulative reward, i.e., R = (cid:80)N i=1 rHi(Θi, ai), where rHi (Θi, ai) denotes the i-th phase reward, The reward function rHi changes with Hi, so it is time-varying.\n\n3.2 PLACEBO SELECTION WITH MINI-BATCH-BASED MEMORY REUSING STRATEGY\n\nIn the following, we introduce how to build phase-specific evaluation functions using the policy, select high-quality placebos without breaking memory constraints, and compute KD loss with the selected placebos. The computation flow (in each phase) is illustrated in Figure 2.\n\nComputing prototypes. Our placebo selection is based on the distance from placebo to the class prototype, i.e., the mean feature of each class (Snell et al., 2017). First, we compute the prototypes of all seen classes. We use exemplars to compute the prototypes of old classes, and use new class\n\n1Though discretization suffers the curse of dimensionality, our experiments show that with a coarse grid,\n\nwe already have significant improvements over pre-fixed hyperparameters.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The computation flow of our PlaceboCIL in the i-th phase. At the beginning of this phase, we build phase-specific evaluation functions {Sm(u)}ci−1 m=1. During training, we select placebos as follows. 1) We load an unlabeled data batch U from the free image stream. 2) We compute scores using {Sm(u)}ci−1 m=1 for all samples in U. 3) For each old class m, we select K placebos with the highest scores and add them to P. 4) We delete used placebos from P after computing the loss. 5) When we use up the selected placebos in P, we repeat the selection steps.\n\ntraining data for new class prototypes, as follows,\n\nPro(En) =\n\n1 |En|\n\n(cid:88)\n\nz∈En\n\nFΘi(z), Pro(Dl) =\n\n1 |Dl|\n\n(cid:88)\n\nz∈Dl\n\nFΘi(z),\n\n(1)\n\nwhere FΘi(·) denotes the encoder (i.e., the feature extractor) of Θi. Pro(En) and Pro(Dl) denote the prototypes of the n-th old class and the l-th new class, respectively.\n\nBuilding evaluation functions. We argue that high-quality placebos for the m-th old class should meet two requirements: (1) being close to the prototype of the m-th class in the feature space because they will be used to activate the related neurons of the m-th old class in the model; and (2) being far from the prototypes of all the other classes in the feature space so that they will not cause the KD issue (as shown in Figure 1). To achieve these, we design the following evaluation function Sm(x) for the m-th old class in the i-th phase:\n\nSm(x) = − Sim (FΘi(x), Pro(Em)) + βi\n\nci−1 (cid:88)\n\nn=1 n̸=m\n\nSim (FΘi(x), Pro(En)) ci−1 − 1\n\nci(cid:88)\n\n+ γi\n\nl=ci−1+1\n\nSim (FΘi(x), Pro(Dl)) ci − ci−1\n\n,\n\n(2)\n\nwhere x denotes an unlabeled input image, and Sim(·, ·) denotes cosine similarity. βi and γi are two hyperparameters from the action ai=(βi, γi), sampled by the policy π.\n\nAllocating mini-batch-based memory for placebos. We need to allocate a small amount of memory to store unlabeled images (before evaluating them). At the beginning of the i-th phase, we allocate memory buffers U and P respectively for the unlabeled image candidates and the selected placebos. In order to not exceed the memory budget, we randomly remove the same number, i.e., |U + P|, of samples from the training data of new classes. Our empirical results show this “remove” does not degrade the model performance on new classes. Please kindly refer to Appendix G for detailed results and comparisons.\n\nSelecting placebos. Whenever the placebo buffer P is empty, we load a batch of unlabeled samples U from the free image stream, and choose K placebos for each old class to add into P, as follows,\n\nP := {xk}ci−1×K\n\nk=1 = arg max\n\nxk∈U\n\nci−1 (cid:88)\n\nK (cid:88)\n\nm=1\n\nk=1\n\nSm(xk).\n\n(3)\n\n5\n\n PlacebosOld exemplarsNew dataUnlabeled data xxPrototypesSampleSampleNew model CE loss KD lossTargetdatastreamMemoryFreeimagestreamSampleNew class batchOld class batch Placebo batchOld model Data for CE lossData for KD lossEvaluation functionsUnder review as a conference paper at ICLR 2023\n\nCalculating loss with placebos. After selecting placebos, we sample a batch of new class data d ⊂ Dci−1+1:ci, a batch of old class exemplars e ⊂ E1:c0 , and a batch of placebos p ⊂ P. We calculate the overall loss as follows,\n\nL = LCE(Θi; d ∪ e) + λLKD(Θi−1, Θi; p ∪ e),\n\n(4)\n\nwhere LCE and LKD denote the CE loss and KD loss, respectively. λ is a hyperparameter to balance the two losses (Rebuffi et al., 2017). To control the memory usage, we delete p from P immediately after calculating the loss. When P is empty, we repeat the placebo selection operation.\n\n3.3 ONLINE POLICY LEARNING ALGORITHM\n\nA common approach to solving an online MDP is to approximate it as an online learning problem and solve it using online learning algorithms (Even-Dar et al., 2005; Agrawal & Goyal, 2012; Auer et al., 2002). We also follow this idea in PlaceboCIL, and our approximation follows the work by Even-Dar et al. (2009) that is theoretically proved to have the optimal regret. Specifically, EvenDar et al. (2009) relax the Markovian assumption of the MDP by decoupling the cumulative reward function and letting it be time-dependent so that they can solve online MDP by standard online learning algorithms.\n\nHowever, we cannot directly apply the algorithms proposed in (Even-Dar et al., 2009) to our problem. It is because they assume full feedback meaning that the model can observe the rewards of all actions in every learning phase (which is also why its online learning problem can be solved by Best Expert algorithms (Even-Dar et al., 2005)). While in CIL, we cannot observe any reward (i.e., the testing accuracies) because the testing data Q1:ci are not accessible in any phase i. To address this problem, we split the training data we have in each phase into two subsets: one for training and another for validation. Once we have a validation set, we can solve our online learning problem based on Exp3 (Auer et al., 2002)—a simple and effective bandit algorithm. In the following, we elaborate how we do this data splitting in each local dataset (i.e., the entire data we have in each training phase of CIL), compute the decoupled cumulative reward, and learn the policy π with Exp3.\n\nRebuilding local datasets. To compute reward, we sample a class-balanced subset B1:ci from the training data T1:c1 . B1:ci contains the same number of samples for both the old and new classes. In this way, we rebuild the local training and validate sets, and update the environment from the oracle Hi=(T1:ci, Q1:ci) (which is unavailable in CIL) to the local environment hi=(T1:ci \\ B1:ci, B1:ci).\n\nDecoupled cumulative reward. We create the decoupled cumulative reward function R based on the original cumulative reward function (cid:80)N j=1 rHj (Θj, aj). In the i-th phase, we compute R as follows,\n\nR(ai, hi) =\n\ni+n (cid:88)\n\nj=i\n\nrhi(Θj, ai) + costant,\n\n(5)\n\nwhere the “constant” denotes the historical rewards from the 1-st phase to the (i-1)-th phase. It doesn’t influence policy optimization. R(ai, hi) is the long-term reward of a time-invariant local MDP based on the local environment hi. We use R(ai, hi) as an estimation of the final cumulative reward, following (Even-Dar et al., 2009). Because we don’t know the total number of phases N during training, we assume there will be n phases in the future. Furthermore, we fix the action ai to simplify the training process. R(ai, hi) is a function of ai and hi.\n\nIt is updated as follows. In the 1-st phase, we initialize w as {1, . . . , 1}.\n\nTraining policy with Exp3. Exp3 (Auer et al., 2002) introduces an auxiliary variable w = {w(a)}a∈A. In each phase i (i≥1), we update w for T iterations. In the t-th iteration, we sample an action at∼π, apply the action at to the CIL system, and compute R(at, hi) using Eq. 5. After that, we update w(at) in w as,\n\nw(at) ← w(at) exp(ξR(at, hi)/p(at|Θi)),\n\n(6)\n\nwhere ξ can be regarded as the learning rate. After updating w, we get the policy π=w/||w||. The pseudocode is available in Appendix A (Algorithms 1 and 2).\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nMethod\n\nLwF (2016) w/ ours\n\niCaRL (2017)\n\nw/ ours\n\nLUCIR (2019)\n\nw/ ours\n\n20 exemplars/class\n\n10 exemplars/class\n\n5 exemplars/class\n\nAverage\n\nLast\n\nAverage\n\nLast\n\nAverage\n\nLast\n\n53.19 +1.35 43.18 +3.64 59.08 +5.89 49.15 +5.97\n\n57.12 +1.35 47.49 +3.64 61.24 +4.12 51.47 +3.98\n\n63.17 +1.35 53.71 +3.64 65.28 +2.11 56.23 +2.52\n\n45.96 +3.64 34.10 +3.64 53.61 +7.65 38.36 +4.26\n\n53.43 +3.64 41.49 +3.64 59.11 +5.68 46.42 +4.93\n\n60.50 +3.64 49.08 +3.64 64.79 +4.29 55.44 +6.36\n\n61.12 +3.64 48.83 +3.64 64.30 +3.18 52.92 +4.09\n\n62.03 +3.64 52.23 +3.64 65.12 +3.09 54.81 +2.48\n\n35.41 +13.64 24.91 +13.64 41.55 +6.141 28.68 +3.771\n\n43.73 +13.64 34.33 +13.64 51.55 +7.821 39.35 +5.021\n\n51.36 +13.64 39.37 +13.64 62.74 +11.35 53.25 +13.88\n\n53.81 +13.64 42.93 +13.64 60.27 +6.461 48.45 +5.521\n\n56.80 +13.64 43.11 +13.64 62.78 +5.981 50.72 +7.611\n\nLUCIR-AANets (2021a) 66.12 +1.35 55.27 +3.64 67.65 +1.53 59.18 +3.91\n\nw/ ours\n\nFOSTER (2022)\n\nw/ ours\n\n70.62 +1.35 62.97 +3.64 71.97 +1.35 64.43 +1.46\n\nTable 1: Evaluation results (%) on CIFAR-100 (N =5) using different baselines w/ and w/o our PlaceboCIL. “Average” denotes the average accuracy over all phases. “Last” denotes the last phase (5-th phase) accuracy.\n\n4 EXPERIMENTS\n\nWe evaluate our method on three CIL benchmarks and achieve consistent improvements over multiple baseline methods. Below we introduce datasets and implementation details, followed by results and analyses, including the comparison to the state-of-the-art, an ablation study, and the visualization of our placebos.\n\nDatasets and free image streams. We use three datasets: CIFAR-100 (Krizhevsky et al., 2009), ImageNet-100 (Rebuffi et al., 2017), and ImageNet-1k (Russakovsky et al., 2015). ImageNet-100, which contains 100 classes, is sampled from ImageNet-1k. We use exactly the same classes or orders as the related works (Rebuffi et al., 2017; Hou et al., 2019). For CIFAR-100, we use ImageNet-1k as the free image stream. For ImageNet-100, we use a 900-class subset of ImageNet-1k, which is the complement of ImageNet-100 in ImageNet-1k. For ImageNet-1k, we use a 1, 000-class subset of ImageNet-21k (Deng et al., 2009) without any overlapping class (different super-classes from those in ImageNet-1k). Implementation details. Following (Hou et al., 2019; Douillard et al., 2020; Liu et al., 2021a;b), we use a modified 32-layer ResNet for CIFAR-100 and an 18-layer ResNet for ImageNet datasets. The number of exemplars for each class is 20 in the default setting. The training batch size is 128. On CIFAR-100 (ImageNet-Subset/1k), we train it for 160 (90) epochs in each phase, and divide the learning rate by 10 after 80 (30) and then after 120 (60) epochs. If the baseline is POD-AANets (Liu et al., 2021a), we fine-tune the model for 20 epochs using only exemplars. We apply different forms of distillation losses on different baselines: (1) if the baselines are LwF and iCaRL, we use the softmax KL divergence loss; (2) if the baselines are LUCIR and AANets, we use the cosine embedding loss (Hou et al., 2019); and (3) if the baseline is POD-AANets, we use pooled outputs distillation loss (Douillard et al., 2020). For our PlaceboCIL, |U| and |P| are set as 1, 000 and 200, respectively. All experiments of our PlaceboCIL in the main paper use the “strict budget” setting, i.e., deleting |U + P| samples from training data in order to not exceed the memory budget. An ablation study on the memory budget setting is provided in Appendix G. More implementation details are provided in Appendices C (benchmark protocol), D (network architecture), and E (training configurations).\n\nResults on five baselines. Table 1 shows the average and last-phase accuracy for five baselines (i.e., LwF (Li & Hoiem, 2016), iCaRL (Rebuffi et al., 2017), LUCIR (Hou et al., 2019), AANets (Liu et al., 2021a), and FOSTER (Wang et al., 2022)). From the table, we make the following observations. 1) Using our PlaceboCIL boosts the performance of the baselines clearly and consistently in all settings, indicating that our method is generic and efficient. 2) When the number of exemplars decreases, the improvement brought by our method becomes more significant. For example, the last-phase accuracy improvement of LUCIR increases from 2.52 to 13.88 percentage points when the number of exemplars per class decreases from 20 to 5. This reveals that the superiority of our method is more obvious when the forgetting problem is more serious (with fewer exemplars) due to a tighter memory budget in CIL. 3) Our PlaceboCIL can boost the performance of all KD terms, i.e.,\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n10\n\n38.90 46.89 62.88 62.88 64.46 65.97 –\n\n– –\n\nMethod\n\nLwF (2016) iCaRL (2017) TPCIL (2020) DDE (2021) GeoDL (2021) DER (2021) ELI (2022)\n\nCIFAR-100\n\nImageNet-100\n\nImageNet-1k\n\nN =5\n\n53.19 57.12 65.34 65.34 65.14 68.65 68.78\n\n10\n\n46.98 52.66 63.58 63.58 65.03 67.48 66.62\n\n25\n\n45.51 48.22 –\n– 63.12 66.18 64.72\n\n5\n\n53.62 65.44 76.27 76.27 76.63 78.40 73.54\n\n10\n\n47.64 59.88 74.81 74.81 75.40 78.20 71.82\n\n25\n\n44.32 52.97 –\n– 71.43 75.40 70.32\n\n5\n\n44.35 51.50 64.89 64.89 65.23 68.13 –\n\nGD+ext (2019) MUC-LwF (2020b)\n\n63.17±0.47 58.71±0.39 51.79±0.42 75.67±0.51 72.08±0.61 65.13±0.56 59.03±0.35 53.27±0.47 49.06±0.49 72.31±0.53 68.92±0.60 62.93±0.62\n\n– –\n\nPOD-AANets (2021a) 66.12±0.41 64.11±0.32 62.12±0.51 76.63±0.47 75.40±0.36 71.43±0.32 67.60±0.39 64.79±0.42 w/ PlaceboCIL (ours) 67.65±0.45 65.78±0.40 64.95±0.46 78.24±0.52 77.14±0.47 75.85±0.42 68.55±0.34 65.49±0.38 70.62±0.58 68.43±0.45 63.83±0.62 80.21±0.67 77.63±0.73 69.27±0.50 69.32±0.47 66.07±0.61 w/ PlaceboCIL (ours) 71.97±0.49 70.31±0.59 67.02±0.65 82.03±0.49 79.52±0.60 72.79±0.45 71.02 ±0.39 68.82±0.54\n\nFOSTER (2022)\n\nTable 2: Average accuracy (%) across all phases. The first block shows top-performing CIL methods. The second block shows CIL methods that use unlabeled data. The third block shows our method.\n\nnot only for logits-based KD (Rebuffi et al., 2017) but also for feature-based KD (Hou et al., 2019; Douillard et al., 2020).\n\nComparisons to the state-of-the-art. Table 2 (Blocks 1&3) shows the results of our best model (taking PlaceboCIL as a plug-in module in the top method (Wang et al., 2022)) and some recent top-performing methods. We can see that using our PlaceboCIL outperforms all previous methods. Intriguingly, we find that we can surpass others more when the number of phases is larger—where there are more serious forgetting problems. For example, when N =25, we improve POD-AANets by 4.4% on the ImageNet-100, while this number is only 1.6% when N =5 (which is an easier setting with more saturated results). This reflects the encouraging efficiency of our method for reducing the forgetting of old class knowledge in CIL models.\n\nComparisons to the CIL methods using unlabeled data. Table 2 (Blocks 2&3) shows the results of our best model and CIL methods using unlabeled data (GD+ext (Lee et al., 2019) and MUCLwF (Liu et al., 2020b)). We can see that our method consistently performs better than others. For another related work, DMC (Zhang et al., 2020), we didn’t find the public code. So, we compare ours with iCaRL DMC using their paper’s setting: w/ ours achieves 62.3%, while the result of DMC is 59.1% (CIFAR-100, 10 phases, 10 classes/phase). Please kindly refer to Appendix B for more analyses.\n\nAblation study. Tables 3 (and A1) show the ablation results for free data streams, policy learning methods, and placebo selection strategies.\n\n1) First block. Row 1 shows the baselines. Row 2 shows our method.\n\ndifferent\n\n2) Second block: free data streams. Rows 3-6 show the ablation results for the following free data streams. (1) “Overlapping” means including samples from the overlapping classes between (2) “NonCIFAR-100 and ImageNet.\n\nNo. Setting\n\niCaRL\n\nLUCIR-AANets\n\nAverage Last Average Last\n\n1 2\n\nBaseline PlaceboCIL\n\n3 Overlapping 4 Non-overlapping 5 New data 6 Old data (oracle)\n\n57.12 47.49 61.01 51.45\n\n62.15 52.62 61.52 51.70 57.70 47.51 66.64 58.03\n\n7 8 Offline RL\n\nw/o Online learning 60.27 50.57 61.09 50.81\n\n9 Higher confidence 10 Random placebos\n\n60.43 49.36 56.27 46.64\n\n66.72 67.16\n\n67.48 67.01 66.69 68.82\n\n66.91 67.31\n\n66.97 66.23\n\n57.77 59.14\n\n59.06 58.53 57.33 61.52\n\n58.88 59.26\n\n58.12 57.22\n\nTable 3: Ablation results (%) on CIFAR-100, N =5. (1) First block: baselines. Row 1 shows the baselines. Row 2 shows our method. All the following ablation settings (Rows 3-10) are based on Row (2) Second block: different free data streams. 2. Rows 3-6 show the ablation results for the following (3) Third block: different policy free data streams. learning methods. Row 7 is for using fixed evaluation functions (βi=γi=1). Row 8 uses the offline RL (the REINFORCE algorithm) to train the selection policy. (4) Fourth block: different placebo selection strategies. Row 9 uses unlabeled data with higher confidence. Row 10 uses them randomly.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: (a) Selected placebos for two CIFAR-100 classes and their GradCAM activation maps. The free image stream is non-matching ImageNet-1k. (b) The t-SNE results on CIFAR-100 (N =5). For clear visualization, we randomly pick five new classes and five old classes. The purple, light blue, and dark blue points denote the new data, old data, and selected placebos, respectively.\n\noverlapping” means using only the samples of non-overlapping classes between CIFAR-100 and ImageNet (more realistic than “Overlapping”). (3) “New data” means using only the current-phase new class data (i.e., without using any free data stream) as candidates to select placebos. (4) “Old data” means the original old class data are all accessible when computing KD loss (i.e., upper bound of KD effect). Please note that in (1) and (2), two classes are considered “overlapping” if their classes or super-classes overlap. For example, “n02640242 - sturgeon” in ImageNet-1k is regarded as an overlapping class of the “fish” in CIFAR-100, because they overlap at the level of super-class (i.e., “fish”). When comparing Row 4 with Row 2, we can find that our method is robust to the change of data streams: even if all overlapping classes are removed, our method can still achieve the same-level performance. Comparing Row 5 with Row 2, we can get a clear sense that using additional unlabeled data is definitely helpful. Comparing Row 6 with Row 2, we see that our method achieves comparable results to the upper bound.\n\n3) Third block: different policy learning methods. Row 7 is for using fixed evaluation functions (βi=γi=1). Row 8 uses the offline RL (the REINFORCE algorithm (Liu et al., 2021b)) to train the selection policy. Comparing Row 7 with Row 2 shows that using online learning successfully boosts the model performance. Comparing Row 8 with Row 2, we are happy to see that our online learning method achieves the same-level performance as the offline RL while the training time is much less. The training time of the baseline (without learning a policy) is 2.7 hours. It becomes around 650 hours if we solve the MDP by offline RL. In contrast, using our online method takes only 4.5 hours.\n\n4) Fourth block: different placebo selection strategies. Row 9 uses unlabeled data with higher confidence following (Lee et al., 2019). Row 10 uses them randomly following (Zhang et al., 2020). Comparing these results with Row 2 shows our superiority. The “mini-batch-based memory reusing strategy” is applied in Rows 9 and 10.\n\nVisualization results. Figure 3 (a) demonstrates the activation maps visualized by Grad-CAM for the placebos of two old classes on CIFAR-100 (“road” and “table”). ImageNet-1k is the free data stream. We can see that the selected placebos contain the parts of “road” and “table” even though their original labels (on ImageNet-1k) are totally different classes. While this is not always the case, our method seems to find sufficiently related images to old classes that activate the related neurons for old classes (“road” and “table”). To illustrate that, Figure 3 (b) shows t-SNE results for placebos, old class data (not visible during training), and new class data. We can see that the placebos are located near the old class data and far away from the new class data. This is why placebos can recall the old knowledge without harming the new class learning.\n\n5 CONCLUSIONS\n\nWe proposed a novel method, PlaceboCIL, which selects high-quality placebo data from free data streams and uses them to improve the effect of KD in CIL. We designed an online learning method to make the selection of placebos more adaptive in different phases and a mini-batch-based memory reusing strategy to control memory usage. Extensive experiments show that our method is general and efficient.\n\n9\n\nSiberian huskyunicycleCIFAR-100 class: road(a) Selected placebos(b) t-SNE visualizationradio telescopen04515003 - uprightterriern03290653 - entertainmentmarimbahome theateruprightentertainmentnew dataold dataplacebosCIFAR-100 class: tableUnder review as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nComputational costs. Deep learning methods require intensive usage of computing resources, which is not climate-friendly. It calls for future research into proposing more effective training strategies that can speed up the training.\n\nPrivacy issues. Keeping old class exemplars has the issue of data privacy. This calls for future research that explicitly forgets or mitigates the identifiable feature of the data.\n\nLicenses. We use the open-source code for the following papers: AANets (Liu et al., 2021a), iCaRL (Rebuffi et al., 2017), Mnemonics (Liu et al., 2020a), PODNet (Douillard et al., 2020), FOSTER (Wang et al., 2022). They are licensed under the MIT License.\n\nDatasets. We use three datasets in our paper: CIFAR-100 (Krizhevsky et al., 2009), ImageNet100 (Rebuffi et al., 2017) and ImageNet-1k (Russakovsky et al., 2015). The data for both datasets are downloaded from their official websites and allowed to use for non-commercial research and educational purposes.\n\nREPRODUCIBILITY STATEMENT\n\nWe include the details of reproducing our experiments in Section 4 (main paper), Appendix C, Appendix D, and Appendix E. We also provide the code in the supplementary.\n\nREFERENCES\n\nDavide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, and Babak Ehteshami Bejnordi. Conditional channel gated networks for task-aware continual learning. In CVPR, pp. 3931–3940, 2020. 3\n\nShipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit prob-\n\nlem. In COLT, volume 23, pp. 39.1–39.26, 2012. 3, 6\n\nPeter Auer and Ronald Ortner. Ucb revisited: Improved regret bounds for the stochastic multi-armed\n\nbandit problem. Periodica Mathematica Hungarica, 61(1-2):55–65, 2010. 3\n\nPeter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-\n\narmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. 3, 6\n\nCharilaos Christopoulos, Athanassios Skodras, and Touradj Ebrahimi. The jpeg2000 still image coding system: an overview. IEEE transactions on consumer electronics, 46(4):1103–1127, 2000. 14\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\n\nhierarchical image database. In CVPR, pp. 248–255, 2009. 7\n\nArthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In ECCV, pp. 86–102, 2020. 1, 2, 3, 7, 8, 10, 14\n\nEyal Even-Dar, Sham M Kakade, and Yishay Mansour. Experts in a markov decision process. In\n\nNIPS, pp. 401–408, 2005. 3, 6\n\nEyal Even-Dar, Sham M Kakade, and Yishay Mansour. Online markov decision processes. Mathe-\n\nmatics of Operations Research, 34(3):726–736, 2009. 3, 6\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In CVPR, pp. 770–778, 2016. 14\n\nSaihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier\n\nincrementally via rebalancing. In CVPR, pp. 831–839, 2019. 1, 2, 3, 7, 8, 14, 15\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nXinting Hu, Kaihua Tang, Chunyan Miao, Xian-Sheng Hua, and Hanwang Zhang. Distilling causal\n\neffect of data in class-incremental learning. In CVPR, 2021. 8\n\nKJ Joseph, Salman Khan, Fahad Shahbaz Khan, Rao Muhammad Anwer, and Vineeth N Balasubramanian. Energy-based latent aligner for incremental learning. In CVPR, pp. 7452–7461, 2022. 3, 8\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\nTechnical report, Citeseer, 2009. 7, 10\n\nKibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. Overcoming catastrophic forgetting with\n\nunlabeled data in the wild. In ICCV, pp. 312–321, 2019. 2, 3, 8, 9, 13, 14\n\nYingying Li, Aoxiao Zhong, Guannan Qu, and Na Li. Online markov decision processes with time-varying transition probabilities and rewards. In ICML workshop on Real-world Sequential Decision Making, 2019. 3\n\nZhizhong Li and Derek Hoiem. Learning without forgetting. In ECCV, pp. 614–629, 2016. 3, 7, 8,\n\n15\n\nYaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and Qianru Sun. Mnemonics training: Multiclass incremental learning without forgetting. In CVPR, pp. 12245–12254, 2020a. 1, 3, 10, 14\n\nYaoyao Liu, Bernt Schiele, and Qianru Sun. Adaptive aggregation networks for class-incremental\n\nlearning. In CVPR, pp. 2544–2553, 2021a. 1, 2, 7, 8, 10, 14, 16\n\nYaoyao Liu, Bernt Schiele, and Qianru Sun. Rmm: Reinforced memory management for class-\n\nincremental learning. In NeurIPS, 2021b. 7, 9, 15\n\nYu Liu, Sarah Parisot, Gregory Slabaugh, Xu Jia, Ales Leonardis, and Tinne Tuytelaars. More classifiers, less forgetting: A generic multi-classifier paradigm for incremental learning. In ECCV, pp. 699–716, 2020b. 3, 8, 14\n\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of Learning and Motivation, volume 24, pp. 109–165. Elsevier, 1989. 1\n\nK. McRae and P. Hetherington. Catastrophic interference is eliminated in pre-trained networks. In\n\nCogSci, 1993. 1\n\nAmeya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions\n\nour progress in continual learning. In ECCV, pp. 524–540, 2020. 3\n\nR. Ratcliff. Connectionist models of recognition memory: Constraints imposed by learning and\n\nforgetting functions. Psychological Review, 97:285–308, 1990. 1\n\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.\n\niCaRL: Incremental classifier and representation learning. In CVPR, pp. 5533–5542, 2017. 1, 2, 3, 6, 7, 8, 10, 14\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015. 7, 10\n\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv, 1606.04671, 2016. 3\n\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In CVPR, pp. 618–626, 2017. 2\n\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative\n\nreplay. In NIPS, pp. 2990–2999, 2017. 3\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nChristian Simon, Piotr Koniusz, and Mehrtash Harandi. On learning the geodesic path for incre-\n\nmental learning. In CVPR, pp. 1591–1600, 2021. 3, 8\n\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\n\nNIPS, pp. 4077–4087, 2017. 4\n\nXiaoyu Tao, Xinyuan Chang, Xiaopeng Hong, Xing Wei, and Yihong Gong. Topology-preserving\n\nclass-incremental learning. In ECCV, pp. 254–270, 2020. 3, 8\n\nFu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Foster: Feature boosting and com-\n\npression for class-incremental learning. In ECCV, 2022. 2, 3, 7, 8, 10\n\nYue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu.\n\nLarge scale incremental learning. In CVPR, pp. 374–382, 2019. 14\n\nJu Xu and Zhanxing Zhu. Reinforced continual learning. In NeurIPS, pp. 899–908, 2018. 3\n\nShipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class\n\nincremental learning. In CVPR, pp. 3014–3023, 2021. 3, 8\n\nJunting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Serafettin Tasci, Larry Heck, Heming Zhang, and C-C Jay Kuo. Class-incremental learning via deep model consolidation. In WACV, pp. 1131– 1140, 2020. 2, 3, 8, 9, 13, 14\n\nFei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-lin Liu. Class-incremental learning via dual\n\naugmentation. NeurIPS, pp. 14306–14318, 2021. 1, 3\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDICES\n\nA ALGORITHMS\n\nSupplementary to Section 3. Algorithm 1 summarizes the overall training steps of the proposed method in the i-th phase (i ≥ 1). Algorithm 2 presents the training with placebos for a specific action a.\n\nAlgorithm 1: Our online learning algorithm in the i-th phase (i≥1) Input\n\n: Old model Θi−1, training data T1:ci, testing data Q1:ci, learnable parameters w,\n\nnumbers of epochs M1 and M2.\n\nOutput: New model Θi, new exemplars E0:i, learnable parameters w. // Policy learning\n\n1 if i=1 then\n\n2\n\nInitialize w = {1, . . . , 1};\n\n3 for t in i, ..., T do\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\nRandomly sample a class-balanced subset B1:ci from T1:ci,; Create the local environment hi = ((T1:ci) \\ B1:ci, B1:ci); Set the policy π = w/||w||; Sample an action at ∼ π; for j in i, ..., i + n do\n\nTrain Θj for M1 epochs by Algorithm 2 with inputs Θj−1, at, hi; Collect the reward rhi(Θj, at);\n\nCompute the cumulative reward ˆR(at, hi) by Eq. 5; Update w by Eq. 6;\n\n// CIL training\n\n13 Sample an action ai ∼ π; 14 Train Θi for M2 epochs by Algorithm 2 with inputs Θi−1, ai, Hi = (T1:ci, Q1:ci); 15 Select new exemplars E1:ci from T1:ci.\n\nAlgorithm 2: Training with placebos for action a\n\n: Old model Θold, action a = {β, γ}, environment h = {T , Q}.\n\nInput Output: New model Θ, reward rh(Θ, a) (i.e., the testing accuracy).\n\n1 Initialize Θ with Θold; 2 Create {Sm(x)}ci−1 3 for epochs do\n\nSet P = ∅; while P == ∅ do\n\nm=1 based on a = {β, γ} using Eq. 2;\n\nSample U from the free image stream; Select placebos P ⊂ U using Eq. 3; for iterations do\n\nSample mini-batches p, d, and e; Compute the loss L by Eq. 4 and update Θ; Update placbo buffer P := P \\ p;\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12 Compute the reward rh(Θ, a) on Q.\n\nB COMPARISONS TO THE CIL METHODS USING UNLABELED DATA.\n\nThis is supplementary to Section 4 “Comparisons to the CIL methods using unlabeled data.”\n\nPart I: Comparing with GD+ext (Lee et al., 2019) and DMC (Zhang et al., 2020). Lee et al. (2019) use the unlabeled data to compute the distillation loss in CIL. Our work also uses unlabeled data but differs from (Lee et al., 2019) in three aspects:\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n• Our method is focused on the learnable strategies of sampling helpful unlabeled data for CIL. It is generic and can be plugged into different distillation-based frameworks, including both logit distillation (LwF and iCaRL) and feature distillation (LUCIR and PODNet). In Table 2, we also show the empirical results (comparison) to demonstrate our superiority.\n\n• We propose an online learning algorithm. Its learned policy can adaptively produce phasespecific functions to evaluate the quality of the unlabeled samples. So our selection functions change accordingly when the number of phases increases. Lee et al. (2019)’s selection strategy remains unchanged when the number of phases increases. Zhang et al. (2020) use the unlabeled data without selection. In Table 3, we show our learnable selection functions generally perform better than (Lee et al., 2019) and (Zhang et al., 2020). Please kindly refer to Section 4 “Ablation study” for the results.\n\n• Our employment of unlabeled data is dynamic and takes little memory out of the total memory budget. We maintain the memory budget strictly the same as the baselines (LUCIR (Hou et al., 2019), PODNet (Christopoulos et al., 2000), and Mnemonics (Liu et al., 2020a)), by loading fewer samples of new classes. In contrast, (Lee et al., 2019) requires double-size memory compared to the baselines (LUCIR (Hou et al., 2019), PODNet (Christopoulos et al., 2000)), and Mnemonics (Liu et al., 2020a)).\n\nPart II: Comparing with MUC (Liu et al., 2020b). Our method and (Liu et al., 2020b) use unlabeled data for different purposes and in different ways. We use the unlabeled placebo data from a free image stream to improve the positive effect of KD in training CIL models. Liu et al. (2020b) integrate the ensemble of multiple FC classifiers to set regularization-based constraints. They use the unlabeled data to compute the classifier discrepancy loss by using the predictions of multiple classifiers.\n\nC BENCHMARK PROTOCOLS\n\nThis is supplementary to Section 4 “Implementation details.” We follow the benchmark protocol used in (Douillard et al., 2020; Hou et al., 2019; Liu et al., 2021a; 2020a). Given a dataset, the initial model (in the 0-th phase) is trained on the data of half of the classes. Then, it learns the remaining classes evenly in the subsequent N phases. For N , there are three options (5, 10, and 25), and the corresponding settings are called “N -Phase”. The learned model in each phase is evaluated on the test set containing all seen classes. In the tables, we report average accuracy over all phases and the last-phase accuracy.\n\nD NETWORK ARCHITECTURE DETAILS\n\nThis is supplementary to Section 4 “Implementation details.” Following (Hou et al., 2019; Liu et al., 2021a; Rebuffi et al., 2017; Wu et al., 2019), we use a modified 32-layer ResNet (Rebuffi et al., 2017) for CIFAR-100 and an 18-layer ResNet (He et al., 2016) for ImageNet-100 and ImageNet-1k. Please note that it is standard to use a shallower ResNet for ImageNet-100 and ImageNet-1k in CIL. The 32-layer ResNet consists of 1 initial convolution layer and 3 residual blocks (in a single branch). Each block has 10 convolution layers with 3 × 3 kernels. The number of filters starts from 16 and is doubled every next block. After these 3 blocks, there is an average-pooling layer to compress the output feature maps to a feature embedding. The 18-layer ResNet follows the standard settings in (He et al., 2016). We deploy AANets using the same parameters as its original paper (Liu et al., 2021a).\n\nE MORE TRAINING CONFIGURATIONS\n\nThis is supplementary to Section 4 “Implementation details.” The training of the classification model Θ exactly follows the uniform setting in (Douillard et al., 2020; Hou et al., 2019; Liu et al., 2021a; 2020a).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nF ABLATION STUDY ON MORE BASELINES\n\nThis is supplementary to Section 4 “Ablation study.” In Table A1, we provide the ablation results on another two baselines, LwF Li & Hoiem (2016) and LUCIR Hou et al. (2019).\n\nNo.\n\nSetting\n\n1 2\n\n3 4\n5 6\n\n7 8\n\nBaseline PlaceboCIL\n\nOverlapping Non-overlapping New data Old data (oracle)\n\nw/o Online learning Offline RL\n\n9 10\n\nHigher confidence Random placebos\n\nLwF\n\nAverage\n\n53.19 59.06\n\n58.95 58.59 54.06 61.41\n\n57.06 59.01\n\n56.98 50.99\n\nLast\n\n43.18 48.56\n\n48.71 49.28 43.94 51.16\n\n46.12 48.18\n\n46.20 40.22\n\nLUCIR\n\nAverage\n\n63.17 65.42\n\n65.73 65.48 64.21 67.02\n\n63.83 65.14\n\n63.60 64.16\n\nLast\n\n53.71 56.18\n\n57.26 56.99 53.98 58.98\n\n55.33 56.95\n\n55.50 55.40\n\nTable A1: Ablation results (%) on CIFAR-100, N =5.\n\nG ABLATION STUDY FOR THE MEMORY BUDGET SETTINGS\n\nThis is supplementary to Section 4. In Table A2, Row 1 shows baselines. Rows 2 and 3 show the baselines with our PlaceboCIL as a plug-in module. Row 2 “budget” means the combined size of episodic memory and the placebo buffer is (strictly) equal to the episodic memory size in baseline methods. To this end, we randomly delete |U + P| samples from the new class data Dci−1+1:ci to obtain the buffer memory storing the unlabeled data batch U and P. Row 3 “non-budget” means we are allowed to use a little additional memory to save the unlabelled data U and P without deleting other samples. Comparing Row 3 to Row 2, we can see that the performance of our PlaceboCIL only reduces 0.04 percentage points on average under the strict “budget” setting. The reason is the amount of new data is greatly larger than that of old samples, and removing a small batch of new data does not harm the performance much, as observed in (Liu et al., 2021b).\n\nNo. Setting\n\nLwF\n\niCaRL\n\nLUCIR\n\nAANets\n\nAverage Last\n\nAverage Last\n\nAverage Last\n\nAverage Last\n\n1 2\n3\n\nBaseline PlaceboCIL (budget) PlaceboCIL (non-budget)\n\n53.19 59.06 59.08\n\n43.18 48.56 49.15\n\n57.12 61.01 61.24\n\n47.49 51.45 51.47\n\n63.17 65.42 65.28\n\n53.71 56.18 56.23\n\n66.72 67.16 67.27\n\n57.77 59.14 59.15\n\nTable A2: Ablation results (%) on CIFAR-100, N =5.“Average” and “Last” denote the average accuracy over all phases and the last-phase accuracy, respectively.\n\nH EQUAL-SIZE SPLIT RESULTS\n\nThis is supplementary to Section 4 “Ablation study.” In Table A3, we supplement the results using the equal-size split on CIFAR 100, 10-phase (10 classes/phase, 20 exemplars/class). We can observe that using placebos selected by heuristic evaluation functions (βi=γi=1) consistently improves the results on three baselines. Using the online learning algorithm can further boost performance. The observations are similar to using the original setting in the main paper.\n\nI ABLATION RESULTS FOR DIFFERENT UNLABELED DATA SOURCES\n\nThis is supplementary to Section 4 “Ablation study.” We believe that the key to success is the design of our method instead of the choice of unlabeled data sources. To verify this, we provide\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nNo. Setting\n\nLwF\n\niCaRL\n\nLUCIR\n\nAverage\n\nLast\n\nAverage\n\nLast\n\nAverage\n\nLast\n\n1 2\n3\n\nBaseline PlaceboCIL (ours) PlaceboCIL (ours, w/o online learning)\n\n53.85 57.68 56.31\n\n41.00 42.44 41.76\n\n59.70 62.32 61.05\n\n45.29 46.52 46.02\n\n56.71 57.89 57.01\n\n42.78 44.08 43.13\n\nTable A3: Supplementary to Table 3. Ablation results (%) using equal-size split on CIFAR-100, 10-phase (10 classes/phase).\n\nthe results for a new ablative setting: “using random unlabeled data” (to compute the distillation loss) in Table A4. We can observe that no matter what unlabeled data sources we use, our method consistently performs better than using random unlabeled data.\n\nNo. Setting\n\nLwF\n\niCaRL\n\nLUCIR\n\nAANets\n\nAverage Last Average Last Average Last Average Last\n\n1 2\n3 4\n4\n\nBaseline PlaceboCIL (ours, all) PlaceboCIL (ours, overlapping) Random unlabeled data (all) Random unlabeled data (overlapping)\n\n53.19 59.29 58.95 50.99 50.80\n\n43.18 49.64 48.71 40.22 40.94\n\n57.12 61.17 62.15 56.27 55.70\n\n47.49 50.96 52.62 46.64 46.47\n\n63.17 65.48 65.73 64.16 64.23\n\n53.71 56.77 57.26 55.45 54.68\n\n66.72 67.33 67.48 66.23 66.58\n\n57.77 59.32 59.06 57.22 57.08\n\nTable A4: Supplementary to Table 3. Ablation results (%) on CIFAR-100, N =5.“Average” and “Last” denote the average accuracy over all phases and the last-phase accuracy, respectively. “All” denotes using all data from ImageNet, and “overlapping” means including samples from the overlapping classes between CIFAR-100 and ImageNet.\n\nJ REHEARSAL-FREE EXPERIMENTS\n\nThis is supplementary to Section 4 “Ablation study.”\n\nIn Table A5, we provide the “rehearsal-free” results for two variants of our method: 1) PlacboCIL (original) w/o exemplars: we don’t use any exemplars to compute the loss, only storing one exemplar each class to compute the evaluation function (Eq. 4); 2) PlacboCIL (higher confident) w/o exemplars: we don’t use any exemplars, and select the unlabeled data with higher confidence as the placebos.\n\nWe can observe that using placebos is effective in the “rehearsal-free” setting. The original PlaceboCIL improves the average accuracy of the baseline by 6.30 percentage points. The “higher confident” version performs a little worse than the original, but it still improves the average accuracy of the baseline by 3.40 percentage points.\n\nNo. Setting\n\nBaseline PlaceboCIL (original) PlaceboCIL (higher confidence)\n\nAverage Last\n\n66.72 57.77 67.16 59.14 66.97 58.12\n\nBaseline w/o exemplars 50.01 39.12 PlaceboCIL (original) w/o exemplars 56.31 44.02 PlaceboCIL (higher confidence) w/o exemplars 53.41 41.93\n\n1 2\n3\n\n4 5\n6\n\nTable A5: Ablation results (%) on CIFAR-100, N =5. Baseline: LUCIR+AANets Liu et al. (2021a).\n\n16",
    "reference": "# Summary Of The Paper\n\nThe setting of this paper is to use wild data (free data) as an additional resource to do class-incremental learning. This paper proposes an online placebo (free images) selection policy to select good images from the free data stream. The selected images are used for knowledge distillation. Thus the proposed method can be combined with existing CIL methods to improve their performances. To train the evaluation policy, this paper introduces an online reinforcement learning algorithm, which reduces training computation costs compared with offline reinforcement learning. Experiments show that the proposed method can enhance the performances of many existing methods.\n\n# Strength And Weaknesses\n\nStrengths:\n(1) This paper proposes an evaluation function to select good images from the free data stream. The proposed methods can provide a plug-in component for existing CIL methods.\n(2) The experiments validate the effectiveness of the proposed framework.\n(3) The paper is well organized, clearly motivated, and contributes novelty.\n\nWeaknesses:\n(1) The results are not so significant. There are some existing works using wild/free images as additional data for incremental learning. Compared with previous works, this work is a universal plug-in component to enhance many existing CIL works. However, this paper uses the methods without additional data as baselines, which obviously have improvements. In fact, one intuitive way of using free images is to find a confidence score to select good images or generate pseudo labels, which should be the true baseline. From Table.3 and A1, we can find that \"Higher confidence\" seems a baseline that does not use the proposed method but also can select good images for knowledge distillation. The gaps between the proposed method and \"Higher confidence\" is even close to random errors for LUCIR-AANet and iCaRL. From Table.3, using the proposed methods gets only 0.58% improvement in average accuracy for iCaRL, and the results for LUCIR-AANet is only 0.37%, which means \"Higher confidence\" is the basic baseline that should be compared with. This indicates that the overall good performance is largely due to the additional data, even a simple confident selection metric can provide relatively good improvements. \n(2) It would be helpful to report the mean and standard deviation to take out any concerns related to randomness.\n(3) How to use ImageNet-1K as additional dataset for modified ResNet-32? Because the image size of CIFAR-100 is 32x32.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nGenerally speaking, this paper is clear and easy to follow.\n\n# Summary Of The Review\n\nThere are concerns about the results. The baseline methods compared are without additional data, which is somehow unfair. Moreover, the proposed offline RL method takes extra costs but only brings marginal improvement.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nN/A"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nGLM-130B: AN OPEN BILINGUAL PRE-TRAINED\n\nMODEL\n\nAohan Zeng⋄†∗, Xiao Liu⋄†∗, Zhengxiao Du⋄†, Zihan Wang⋄, Hanyu Lai⋄, Ming Ding⋄, Zhuoyi Yang⋄, Yifan Xu⋄, Wendi Zheng⋄, Xiao Xia⋄, Weng Lam Tam⋄§, Zixuan Ma⋄, Yufei Xue§, Jidong Zhai⋄, Wenguang Chen⋄, Zhiyuan Liu⋄, Peng Zhang§, Yuxiao Dong⋄‡, Jie Tang⋄‡\n\nTsinghua University⋄\n\nZhipu.AI§\n\nABSTRACT\n\nWe introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B—the largest Chinese language model—across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B/.\n\n1\n\nINTRODUCTION\n\nLarge language models (LLMs), particularly those with over 100 billion (100B) parameters (Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022; Wang et al., 2021), have presented attractive scaling laws (Wei et al., 2022b), where emergent zero-shot and few-shot capabilities suddenly arose. Among them, GPT-3 (Brown et al., 2020) with 175B parameters pioneers the study of 100B-scale LLMs by strikingly generating better performance with 32 labeled examples than the fully-supervised BERT-Large model on a variety of benchmarks. However, both GPT-3 (and many other closed-sourced 100B-scale ones)—the model itself—and how it can be trained, have been thus far intransparent to the public. It is of critical value to train a high-quality LLM of such scale with both the model and training process shared with everyone.\n\nWe thus aim to pre-train an open and highly-accurate 100B-scale model with ethical concerns in mind. Over the course of our attempt, we have come to realize that pre-training a dense LLM at such a scale raises numerous unexpected technical and engineering challenges compared to training 10B-scale models, in terms of pre-training efficiency, stability, and convergence. Similar difficulties\n\n*The two lead authors AZ and XL contributed equally ({zengaohan,shawliu9}@gmail.com) †Work partially done when AZ, XL, and ZD interned at Zhipu.AI. ‡Team leads: YD and JT. Corresponding author: JT (jietang@tsinghua.edu.cn) For detailed author contributions, please refer to Appendix E.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: A summary of the performance evaluation and ethical studies. Table 1: A comparison between GLM-130B and other 100B-scale LLMs and PaLM 540B. (LN: layer norm.; FPF: floating-point format; MIP: multi-task instruction pre-training; CN : Chinese)\n\nModel\n\nOpensource\n\n× GPT-3 175B ✓\nOPT-175B BLOOM-176B ✓\n\nPaLM 540B\n\nGLM-130B\n\n×\n\n✓\n\nArchitecture & Data\n\nTraining\n\nInference\n\nObjective\n\nLN Major Lang. FPF\n\nStabilization\n\nQuantization GPU Needed\n\nGPT\n\nGPT\n\nPre-LN\n\nEnglish English\n\nFP16 FP16 Manual Adjusting Multi-lingual BF16 Embedding Norm\n\nundisclosed\n\nundisclosed INT8 INT8\n\nundisclosed 8 × 3090 8 × 3090\n\nPre-LN\n\nEnglish\n\nBF16 Manual Adjusting undisclosed\n\nundisclosed\n\nGLM (Blank Infilling & MIP)\n\nDeepNorm\n\nBilingual (EN & CN) FP16\n\nEmbedding Gradient Shrink\n\nINT4\n\n4 × 3090 or 8 × 1080 Ti\n\nhave also been concurrently observed in training OPT-175B (Zhang et al., 2022) and BLOOM176B (Scao et al., 2022), further demonstrating the significance of GPT-3 as a pioneer study.\n\nIn this work, we introduce the pre-training of a 100B-scale model—GLM-130B, in terms of engineering efforts, model design choices, training strategies for efficiency and stability, and quantization for affordable inference. As it has been widely realized that it is computationally unaffordable to empirically enumerate all possible designs for training 100B-scale LLMs, we present not only the successful part for training GLM-130B but also many of the failed options and lessons learned. Particularly, the training stability is the decisive factor in the success of training models of such a scale. Different from practices such as manually adjusting learning rates in OPT-175B and using embedding norm in the sacrifice of performance in BLOOM-176B, we experiment with various options and find the strategy of embedding gradient shrink can significantly stabilize the training of GLM-130B.\n\nSpecifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 billion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8×40G) GPU nodes between May 6 and July 3, 2022. Instead of using the GPT-style architecture, we adopt the General Language Model (GLM) algorithm (Du et al., 2022) to leverage its bidirectional attention advantage and autoregressive blank infilling objective. Table 1 summarizes the comparison between GLM-130B, GPT-3 and another two open-source efforts—OPT-175B and BLOOM-176B, as well as PaLM 540B (Chowdhery et al., 2022)—a 4× larger model—as a reference.\n\nAltogether, the conceptual uniqueness and engineering efforts enable GLM-130B to exhibit performance that surpasses the level of GPT-3 on a wide range of benchmarks (in total 112 tasks) and also outperforms PaLM 540B in many cases, while outperformance over GPT-3 has not been observed in OPT-175B and BLOOM-176B (Cf. Figure 1 left). For zero-shot performance, GLM-130B is better than GPT-3 175B (+5.0%), OPT-175B (+6.5%), and BLOOM-176B (+13.0%) on LAMBADA (Paperno et al., 2016), and achieves 3× better performance than GPT-3 on Big-bench-lite (Srivastava et al., 2022). For the 5-shot MMLU (Hendrycks et al., 2021) tasks, it is better than GPT-3 175B (+0.9%) and BLOOM-176B (+12.7%). As a bilingual LLM also in Chinese, it offers significantly better results than ERNIE TITAN 3.0 260B (Wang et al., 2021)—the largest Chinese LLM—on 7 zero-shot CLUE (Xu et al., 2020) datasets (+24.26%) and 5 zero-shot FewCLUE (Xu et al., 2021) ones (+12.75%). Importantly, as summarized in Figure 1 right, GLM-130B as an open model is associated with significantly less bias and generation toxicity than its 100B-scale counterparts.\n\nFinally, we design GLM-130B to empower as many people as possible to conduct 100B-scale LLM studies. First, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided because such a size supports inference on a single A100 (8×40G) server. Second, to further lower the GPU requirements, we quantize GLM-130B into INT4 precision without post training while OPT and BLOOM can only reach INT8. Due to a unique property of the GLM architecture, GLM-130B’s INT4 quantization introduces negligible performance degradation, e.g., -0.74% on LAMBADA and even +0.05% on MMLU, making it still better than the uncompressed GPT-3. This enables GLM-\n\n2\n\nGPT-3 175BPaLM 540BBLOOM 176BOPT-175BGLM-130BLanguage Ability EvaluationBias & Toxicity EvaluationPublished as a conference paper at ICLR 2023\n\nFigure 3: Trials on different LayerNorms for GLM-130B training. It turns out that DeepNorm is the most stable one, as it has small gradient norm and does not spike in the early stage training.\n\n130B’s fast inference with performance guarantee on a server of 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G), the most affordable GPU required for using 100B-scale LLMs to date.\n\nWe open-source the model checkpoints, code, training logs, related toolkits, and lessons learned.\n\n2 THE DESIGN CHOICES OF GLM-130B\n\nThe architecture of a machine learning model defines its inductive bias. However, it has been realized that it is computationally unaffordable to explore various architectural designs for LLMs. We introduce and explain the unique design choices of GLM-130B.\n\n2.1 GLM-130B’S ARCHITECTURE\n\nGLM as Backbone. Most recent 100B-scale LLMs, such as GPT-3, PaLM, OPT, and BLOOM, follow the traditional GPT-style (Radford et al., 2019) architecture of decoder-only autoregressive language modeling. In GLM-130B, we instead make an attempt to explore the potential of a bidirectional GLM—General Language Model (Du et al., 2022)—as its backbone.\n\nGLM is a transformer-based language model that leverages autoregressive blank infilling as its training objective. Briefly, for a text sequence x = [x1, · · · , xn], text spans {s1, · · · , sm} are sampled from it, each of which si denotes a span of consecutive tokens [si,1, · · · , si,li] and is replaced (i.e., corrupted) with a single mask token to form xcorrupt. The model is asked to recover them autoregressively. To allow interactions between corrupted spans, their visibility to each other is decided by a randomly sampled permutation on their order.\n\nGLM’s bidirectional attention over unmasked (i.e., uncorrupted) contexts distinguishes GLM-130B from GPT-style LLMs in which the unidirectional attention is used. To support both understanding and generation, it mixes two corruption objectives, each indicated by a special mask token:\n\n• [MASK]: short blanks in sentences whose lengths add up to a certain portion of the input. • [gMASK]: random-length long blanks at the end of sentences with prefix contexts provided.\n\nConceptually, the blank infilling objective with bidirectional attention enables a more effective comprehension of contexts than GPT-style models: when using [MASK], GLM-130B behaves as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020); when using [gMASK], GLM-130B behaves similarly to PrefixLM (Liu et al., 2018; Dong et al., 2019).\n\nEmpirically, GLM-130B offers a record-high accuracy of 80.2% on zero-shot LAMBADA by outperforming both GPT-3 and PaLM 540B in Figure 2. By setting the attention mask, GLM-130B’s unidirectional variant is comparable to GPT-3 and OPT-175B. Our observations are in line with existing findings (Liu et al., 2018; Dong et al., 2019).\n\nFigure 2: GLM-130B and LLMs of similar scale on zero-shot LAMBADA language modeling. Details on GLM’s bidirectional attention are provided in Du et al. (2022).\n\nLayer Normalization (LN, Ba et al. (2016)). Training instability is one major challenge for training LLMs (Zhang et al., 2022; Scao et al., 2022; Chowdhery et al., 2022) (Cf. Figure 10 in Appendix for collapses in training several 100B-scale models). A proper choice of LNs can help stabilize\n\n3\n\nGradient Norm(a) More than 30 failed preliminary trials at 100B-scale (b) Final decisive trials: Sandwich-LN v.s. DeepNorm01234567891011121305001k1.5k2k2.5k3kBidirec'onal A.en'on(e.g., GLM)Unidirec'onal A.en'on(e.g., GPT-3, PaLM)ContextMask(s)Published as a conference paper at ICLR 2023\n\nthe training of LLMs. We experiment with existing practices, e.g., Pre-LN (Xiong et al., 2020), Post-LN (Ba et al., 2016), Sandwich-LN (Ding et al., 2021), which are unfortunately incapable of stabilizing our GLM-130B test runs (Cf. Figure 3 (a) and Appendix B.2 for details).\n\nOur search is later focused on Post-LN due to its favorable downstream results in preliminary experiments though it does not stabilize GLM-130B. Fortunately, one of the attempts on Post-LN initialized with the newly-proposed DeepNorm (Wang et al., 2022b) generates promising training stability. Specifically, given the number of GLM-130B’s layers N , we adopt DeepNorm(x) = LayerNorm(α · x + Network(x)), where α = (2N ) 1 2 , and apply the Xavier normal initialization with the scaling factor of (2N )− 1 2 to ffn, v_proj and out_proj. Additionally, all bias terms are initialized to zero. Figure 3 shows it significantly benefits the training stability of GLM-130B.\n\nPositional Encoding and FFNs. We empirically test different options for positional encoding (PE) and FFN improvements in terms of both training stability and downstream performance (Cf. Appendix B.3 for details). For PEs in GLM-130B, we adopt Rotary Positional Encoding (RoPE, Su et al. (2021)) rather than ALiBi (Press et al., 2021). To improve FFNs in Transformer, we pick GLU with the GeLU (Hendrycks & Gimpel, 2016) activation as the replacement.\n\n2.2 GLM-130B’S PRE-TRAINING SETUP\n\nInspired by recent works (Aribandi et al., 2022; Wei et al., 2022a; Sanh et al., 2022), the GLM-130B pre-training objective includes not only the self-supervised GLM autoregressive blank infilling) but also multi-task learning for a small portion of tokens. This is expected to help boost its downstream zero-shot performance.\n\nSelf-Supervised Blank Infilling (95% tokens). Recall that GLM-130B uses both [MASK] and [gMASK] for this task. Each training sequence is applied with one of them independently at a time. Specifically, [MASK] is used to mask consecutive spans in 30% of training sequences for blank infilling. The lengths of spans follow a Poisson distribution (λ = 3) and add up to 15% of the input. For the other 70% sequences, the prefix of each sequence is kept as context and [gMASK] is used to mask the rest of it. The masked length is sampled from the Uniform distribution.\n\nThe pre-training data includes 1.2T Pile (train split) (Gao et al., 2020) English, 1.0T Chinese WudaoCorpora (Yuan et al., 2021), and 250G Chinese corpora (including online forums, encyclopedia, and QA) we crawl from the web, which form a balanced composition of English and Chinese contents.\n\nMulti-Task Instruction Pre-Training (MIP, 5% tokens). T5 (Raffel et al., 2020) and ExT5 (Aribandi et al., 2022) suggest that multi-task learning in pre-training can be more helpful than fine-tuning, we thus propose to include a variety of instruction prompted datasets including language understanding, generation, and information extraction in GLM-130B’s pre-training.\n\nCompared to recent works (Wei et al., 2022a; Sanh et al., 2022) that leverage multi-task prompted fine-tuning to improve zero-shot task transfer, MIP only accounts for 5% tokens and is set in the pretraining stage to prevent spoiling LLMs’ other general ability, e.g., unconditional free generation. Specifically, we include 74 prompted datasets from (Sanh et al., 2022; Wang et al., 2022a), listed in Appendix C and Table 12. GLM-130B users are suggested to avoid evaluating its zero-shot and few-shot capabilities on these datasets according to the criterion illustrated in Section 5.\n\n2.3 PLATFORM-AWARE PARALLEL STRATEGIES AND MODEL CONFIGURATIONS\n\nGLM-130B is trained on a cluster of 96 DGX-A100 GPU (8×40G) servers with a 60-day access. The goal is to pass through as many tokens as possible, as a recent study (Hoffmann et al., 2022) suggests that most existing LLMs are largely under-trained.\n\nThe 3D Parallel Strategy. The data parallelism (Valiant, 1990) and tensor model parallelism (Shoeybi et al., 2019) are the de facto practices for training billion-scale models (Wang & Komatsuzaki, 2021; Du et al., 2022). To further handle the huge GPU memory requirement and the decrease in overall GPU utilization resulted from applying tensor parallel between nodes—as 40G rather than 80G A100s are used for training GLM-130B, we combine the pipeline model parallelism with the other two strategies to form a 3D parallel strategy.\n\nThe pipeline parallelism divides the model into sequential stages for each parallel group, and to further minimize bubbles introduced by pipeline, we leverage the PipeDream-Flush (Narayanan et al.,\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n2021) implementation from DeepSpeed (Rasley et al., 2020) to train GLM-130B with a relative big global batch size (4,224) to reduce time and GPU memory wasting. Through both numerical and empirical examinations, we adopt 4-way tensor parallelism and 8-way pipeline parallelism (Cf. Appendix B.4 for details). Following the calculation in (Chowdhery et al., 2022), we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.\n\nGLM-130B Configurations. We aim to enable our 100B-scale LLM to run a single DGX-A100 (40G) node in FP16 precision. Based on the hidden state dimension of 12,288 we adopt from GPT-3, the resultant model size has to be no more than 130B parameters, thus GLM-130B. To maximize GPU utilization, we configure the model based on the platform and its corresponding parallel strategy. To avoid insufficient memory utilization in the middle stages due to the additional word embedding at both ends, we balance the pipeline partition by removing one layer from them, making 9×8-2=70 transformer layers in GLM-130B.\n\nDuring the 60-day access to the cluster, we manage to train GLM-130B for 400 billion tokens (roughly 200 billion each for Chinese and English) with a fixed sequence length of 2,048 per sample. For the [gMASK] training objective, we use a context window of 2,048 tokens. For the [MASK] and multi-task objectives, we use a context window of 512 and concatenate four samples together to cater the 2,048-sequence-length. We warm-up the batch size from 192 to 4224 over the first 2.5% samples. We use AdamW (Loshchilov & Hutter, 2019) as our optimizer with β1 and β2 set to 0.9 and 0.95, and a weight decay value of 0.1. We warm up the learning rate from 10−7 to 8 × 10−5 over the first 0.5% samples, then decay it by a 10× cosine schedule. We use a dropout rate of 0.1 and clip gradients using a clipping value of 1.0 (Cf. Table 11 for the full configurations).\n\n3 THE TRAINING STABILITY OF GLM-130B\n\nThe training stability is the decisive factor in GLM-130B’s quality, which is also largely impacted by the number of tokens it passes through (Hoffmann et al., 2022). Thus, given the computing usage constraint, there has to be a trade-off between efficiency and stability with regard to floatingpoint (FP) formats: low-precision FP formats (e.g., 16-bit precision—FP16) improve computing efficiency but are prone to overflow and underflow errors, resulting in training collapses.\n\nMixed-Precision. We follow the common practice of a mixedprecision (Micikevicius et al., 2018) strategy (Apex O2), i.e., FP16 for forwards and backwards and FP32 for optimizer states and master weights, to reduce the GPU memory usage and improve training efficiency. Similar to OPT-175B and BLOOM-176B (C.f. Figure 10 in Appendix), the training of GLM-130B faces frequent loss spikes resulted from this choice, which tends to become increasingly frequent as the training goes on. The precision related spikes are often without clear reasons: some recover on their own; others come with a portent of suddenly soaring gradient norm and eventually a spike or even NaN in loss. OPT-175B attempted to fix by manually skipping data and adjusting hyper-parameters; BLOOM176B did so via the embedding norm technique (Dettmers et al., 2021). We spent months to empirically investigate the spikes and realize that a few issues emerge when transformers scale up:\n\nFirst, the transformer main branch’s value scale can be extremely large in deeper layers if using Pre-LN. This is addressed in GLM130B by using DeepNorm based Post-LN (Cf. Section 2.1), which makes the value scale always bounded.\n\nSecond, the attention scores grow so large that they exceed FP16’s In range, as the model scales up. There are a few options to overcome this issue in LLMs. CogView (Ding et al., 2021), PB-Relax is proposed to remove bias terms and deduct extremum value in attention computation to avoid the problem, which unfortunately does not help avoid disconvergence in GLM-130B. In BLOOM-176B, the BF16 format is used instead of FP16, due to its wide range of values on NVIDIA Ampere GPUs (i.e., A100). However, BF16 consumes ∼15%\n\nFigure 4: EGS reduces gradient scale and variance to stabilize LLMs’ pre-training.\n\n5\n\n(a) Gradient norm with EGS α = 0.1(b) EGS in 40B-scale testingPublished as a conference paper at ICLR 2023\n\nmore run-time GPU memory than FP16 in our experiments due to its conversion to FP32 in gradient accumulation, and more importantly it is not supported on other GPU platforms (e.g., NVIDIA Tesla V100), limiting the accessibility of produced LLMs. Another option from BLOOM-176B is to apply embedding norm with BF16, but in sacrifice of a significant penalty on model performance, as they notice that embedding norm can harm model’s zero-shot learning (Cf. Section 4.3 in (Scao et al., 2022)).\n\nEmbedding Layer Gradient Shrink (EGS). Our empirical search identifies that the gradient norm can serve as an informative indicator of training collapses. Specifically, we find that a training collapse usually lags behind a “spike” in gradient norm by a few training steps. Such spikes are usually caused by the embedding layer’s abnormal gradients, as we observe that its gradient norm is often several magnitude larger that those of other layers in GLM-130B’s early stage training (Cf. Figure 4 (a)). In addition, it tends to fluctuate dramatically in the early training. The problem is handled in vision models (Chen et al., 2021) via freezing the patch projection layer. Unfortunately, we cannot freeze the training of the embedding layer in language models.\n\nFinally, we find the gradient shrink on embedding layers could overcome loss spikes and thus stabilize GLM-130B’s training. It is first used in the multi-modal transformer CogView (Ding et al., 2021). Let α be the shrinking factor, the strategy can be easily implemented via word_embedding = word_embedding ∗ α + word_embedding.detach() ∗ (1 − α). Figure 4 (b) suggests that empirically, setting α = 0.1 wipes out most spikes we would have met, with negligible latency.\n\nIn fact, the final GLM-130B training run only experiences three late-stage loss divergence cases, though it fails numerous times due to hardware failures. For the three unexpected spikes, it turns out further shrinking the embedding gradient can still help stabilize the GLM-130B training. See the training notes and Tensorboard logs in our code repository for details.\n\n4 GLM-130B INFERENCE ON RTX 2080 TI\n\nOne of the major goals of GLM-130B is to lower the hardware requirements for accessing 100Bscale LLMs without efficiency and effectiveness disadvantages.\n\nAs mentioned, the model size of 130B is determined for running the full GLM-130B model on a single A100 (40G×8) server, rather than the high-end A100 (80G×8) machine required by OPT-175B and BLOOM-176B. To accelerate GLM-130B inference, we also leverage FasterTransformer (Timonin et al., 2022) to implement GLM-130B in C++. Compared to the PyTorch implementation of BLOOM-176B in Huggingface, GLM-130B’s decoding inference is 7-8.4× faster on the same single A100 server. (Cf. Appendix B.5 for details).\n\nINT4 Quantization for RTX 3090s/2080s. To further support popularized GPUs, we attempt to compress GLM-130B as much as possible while maintaining performance superiority, particularly via quantization (Zafrir et al., 2019; Shen et al., 2020; Tao et al., 2022), which introduces little task-agnostic performance drops for generative language models.\n\nTypically, the practice is to quantize both model weights and activations to INT8. However, our analysis in Appendix B.6 suggests that LLMs’ activations may contain extreme outliers. Concurrently, the emergent outliers in OPT-175B and BLOOM-176B are also discovered (Dettmers et al., 2022), which influence only about 0.1% feature dimensions and are thus solved by matrix multiplication decomposition for the outlying dimensions. Differently, there exist about 30% outliers in GLM-130B’s activations, making the technique above far less efficient. Thus, we decide to focus on the quantization of model weights (i.e., mostly linear layers) while keeping the FP16 precision for activations. The quantized model is dynamically converted to FP16 precision at runtime, introducing a small computational overhead but greatly reducing the GPU memory usage for storing model weights.\n\nFigure 5: (Left) attn-dense and w2’s weight distributions; (Right) GLM-130B’s INT4 weight quantization scaling law.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Left: Quantized GLM-130B’s performance on several benchmarks; Right: INT4 quantized GLM-130B’s inference speed (encode and decode) with FasterTransformer.\n\nModel Precision\n\nGLM-130B\n\nGPT-3\n\nGPU Type\n\n128 Enc./Dec. 512 Enc./Dec,\n\nFP16 INT8 INT4 FP16\n\nMMLU (acc, ↑) 44.75 44.71 44.80 LAMBADA (acc, ↑) 80.21 80.21 79.47 Pile (a part, BPB, ↓) 0.634 0.638 0.641\n\n43.9 76.2 0.74\n\n8 × A100 (40G) 0.15s 4.29s 0.18s 8 × V100 (32G) 0.31s 6.97s 0.67s 4 × RTX 3090 (24G) 0.37s 8.16s 1.30s 8 × RTX 2080 Ti (11G) 0.39s 6.77s 1.04s\n\n17.7s 28.1s 32.3s 27.3s\n\nExcitingly, we manage to reach the INT4 weight quantization for GLM-130B while existing successes have thus far only come to the INT8. Memory-wise, by comparing to INT8, the INT4 version helps additionally save half of the required GPU memory to 70GB, thus allowing GLM-130B inference on 4 × RTX 3090 Ti (24G) or 8 × RTX 2080 Ti (11G). Performance-wise, Table 2 left indicates that without post-training at all, the INT4-version GLM-130B experiences almost no performance degradation, thus maintaining the performance advantages over GPT-3 on common benchmarks.\n\nGLM’s INT4 Weight Quantization Scaling Law. We examine the underlying mechanism of this unique INT4 weight quantization scaling law exhibited in Figure 5 right. We plot the weight value distributions in Figure 5 left, which turns out to directly impact the quantization quality. Specifically, a wider-distributed linear layer needs to be quantized with larger bins, leading to more precision loss. Thus the wide-distributed attn-dense and w2 matrices explain the INT4 quantization failure for GPT-style BLOOM. Conversely, GLMs tend to have much narrower distributions than those of similar-sized GPTs, and the gap between INT4 and FP16 versions keeps further decreasing as the GLM model size scales up (Cf. Figure 15 in Appendix for details).\n\n5 THE RESULTS\n\nWe follow the common settings in LLMs such as GPT-3 and PaLM to evaluate GLM-130B for English 1. As a bilingual LLM with Chinese, GLM-130B is also evaluated on Chinese benchmarks.\n\nDiscussion on the Scope of Zero-Shot Learning in GLM-130B. Since GLM-130B has been trained with MIP, here we clarify its scope of zero-shot evaluation. In fact, “zero-shot” seems to have controversial interpretations without a consensus in the community. We follow one of the influential related surveys (Xian et al., 2018), which says “At test time, in zero-shot learning setting, the aim is to assign a test image to an unseen class label” where involving unseen class labels is a key. Therefore, we derive our criterion to pick GLM-130B’s zero-shot (and few-shot) datasets as:\n\n• English: 1) For tasks with fixed labels (e.g., natural language inference): no datasets in such tasks should be evaluated on; 2) For tasks without fixed labels (e.g., (multiple-choice) QA, topic classification): only datasets with an obvious domain transfer from those in MIP should be considered.\n\n• Chinese: All datasets can be evaluated as there exists a zero-shot cross-lingual transfer.\n\nFiltering Test Datasets. Following prior practices (Brown et al., 2020; Rae et al., 2021) and our criterion mentioned above, we filter and refrain to report potentially contaminated datasets’ evaluation results. For LAMBADA and CLUE, we find minimal overlap under the 13-gram setting. Pile, MMLU, and BIG-bench are either held-out or released later than the crawling of corpora.\n\n5.1 LANGUAGE MODELING\n\nLAMBADA. LAMBADA (Paperno et al., 2016) is a dataset to test the last word language modeling capability. The results previously shown in Figure 2 suggest GLM-130B achieves a zero-shot accuracy of 80.2 with its bidirectional attention, setting up a new record on LAMBADA.\n\nPile. The Pile test-set (Gao et al., 2020) includes a series of benchmarks for language modeling. On average, GLM130B performs the best on its 18 shared test sets in terms of weighted BPB when compared to GPT-3 and Jurassic1 (Lieber et al., 2021) whose results are directly adopted from the latter, demonstrating its strong language capability (Cf. Appendix C.4 for details).\n\nAvg. BPB 0.650\n\n0.742\n\nTable 3: GLM-130B’s average BPB on Pile evaluation (18 sub-datasets).\n\nJurassic-1 GPT-3 GLM-130B\n\n0.634\n\n1Results in OPT-175B’s paper are reported as applications to access it have not been approved for months.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n0-shot 1-shot 3-shot\n\nGPT-3 2.6B 0.60 0.71 1.83 GPT-3 6.7B -0.06 2.93 5.40 GPT-3 13B 1.77 5.43 7.95 GPT-3 175B 4.35 11.34 13.18\n\nPaLM 540B 8.05 37.77\n\n-\n\nGLM-130B 13.31 14.91 15.12\n\nFigure 6: GLM-130B on MMLU (57 tasks) along training steps.\n\nFigure 7: BIG-bench-lite evaluation (24 tasks) across scales.\n\nTable 4: Details on BIGbench-lite (24 tasks).\n\n5.2 MASSIVE MULTITASK LANGUAGE UNDERSTANDING (MMLU)\n\nMMLU (Hendrycks et al., 2021) is a diverse benchmark including 57 multi-choice question answering tasks concerning human knowledge ranging from high-school-level to expert-level. It is released after the crawling of Pile and serves as an ideal test-bed for LLMs’ few-shot learning. The GPT-3 result is adopted from MMLU and BLOOM-176B is tested by using the same prompts as GLM-130B’s (Cf. Appendix C.6 and Table 15 for details).\n\nGLM-130B’s few-shot (5-shot) performance on MMLU approaches GPT-3 (43.9) after viewing about 300B tokens in Figure 6. It continues moving up as the training proceeds, achieving an accuracy of 44.8 when the training has to end (i.e., viewing 400B tokens in total). This aligns with the observation (Hoffmann et al., 2022) that most existing LLMs are far from adequately trained.\n\n5.3 BEYOND THE IMITATION GAME BENCHMARK (BIG-BENCH)\n\nBIG-bench (Srivastava et al., 2022) benchmarks challenging tasks concerning models’ ability on reasoning, knowledge, and commonsense. Given evaluating on its 150 tasks is time-consuming for LLMs, we report the BIG-bench-lite—an official 24-task sub-collection—for now. Observed from Figure 7 and Table 4, GLM-130B outperforms GPT-3 175B and even PaLM 540B (4× larger) in zero-shot setting. This is probably owing to GLM-130B’s bidirectional context attention and MIP, which has been proved to improve zero-shot results in unseen tasks (Wei et al., 2022a; Sanh et al., 2022). As the number of shots increases, GLM-130B’s performance keeps going up, maintaining its outperformance over GPT-3 (Cf. Appendix C.5 and Table 14 for details on each model and task).\n\nLimitations and Discussions. In the experiments above, we observe that GLM-130B’s performance growth (13.31 to 15.12) with the increase of few-shot samples is not as significant as GPT-3’s (4.35 to 13.18). Here is our intuitive attempt to understand the phenomenon.\n\nFirst, the bidirectional nature of GLM-130B could lead to strong zero-shot performance (as is indicated in zero-shot language modeling), thus getting closer to the few-shot “upper-bound” for models of similar scale (i.e., 100B-scale) than unidirectional LLMs. Second, it may be also attributed to a deficit of existing MIP paradigms (Wei et al., 2022a; Sanh et al., 2022), which only involve zero-shot prediction in the training and will be likely to bias GLM-130B for stronger zero-shot learning but relatively weaker in-context few-shot performance. To correct the bias, a potential solution we came up with would be to employ MIP with varied shots of in-context samples rather than only zero-shot samples.\n\nFinally, despite almost the same GPT architecture as GPT-3, PaLM 540B’s relative growth with fewshot in-context learning is substantially more significant than GPT-3’s. We conjecture this further acceleration in performance growth is a source of PaLM’s high-quality and diverse private-collected training corpora. By combining our experiences with (Hoffmann et al., 2022)’s insights, we came to realize that better architectures, better data, and more training FLOPS should be further invested.\n\n5.4 CHINESE LANGUAGE UNDERSTANDING EVALUATION (CLUE)\n\nWe evaluate GLM-130B’s Chinese zero-shot performance on established Chinese NLP benchmarks, CLUE (Xu et al., 2020) and FewCLUE (Xu et al., 2021).Note that we do not include any Chinese downstream tasks in MIP. To date, we have finished testing on part of the two benchmarks, including\n\n8\n\n50100150200250300350400Trained Tokens (Billion)3034384246GPT-3 175B (5-shot)BLOOM 176B (5-shot)GLM-130B (5-shot)10810910101011Effective Parameter Count0246810121416GLM-130B 0-shotGLM-130B 1-shotGLM-130B 3-shotGPT-3 0-shotGPT-3 1-shotGPT-3 3-shotPaLM 0-shotPublished as a conference paper at ICLR 2023\n\nFigure 8: GLM-130B and ERNIE Titan 3.0 260B evaluated on zero-shot CLUE and FewCLUE.\n\n7 CLUE and 5 FewCLUE datasets (Cf. Appendix C.7 for details). We compare GLM-130B to the largest existing Chinese monolingual language model—the 260B ERNIE Titan 3.0 (Wang et al., 2021). We follow its setting to report zero-shot results on dev datasets. GLM-130B consistently outperforms ERNIE Titan 3.0 across 12 tasks (Cf. Figure 8). Interestingly, GLM-130B performs at least 260% better than ERNIE on two abstractive MRC datasets (DRCD and CMRC2018), possibly due to GLM-130B’s pre-training objective that naturally resonates to abstractive MRC’s form.\n\n6 RELATED WORK\n\nIn this section, we review related work to GLM-130B on topics of pre-training, transferring, and inference of pre-trained LLMs (Qiu et al., 2020; Bommasani et al., 2021).\n\nPre-Training. Vanilla language modeling refers to decoder-only autoregressive models (e.g., GPT (Radford et al., 2018)), but it also recognizes any forms of self-supervised objectives on texts. Recently, transformer-based (Vaswani et al., 2017) language models present a fascinating scaling law: new abilities (Wei et al., 2022b) arise as models scale up, from 1.5B (Radford et al., 2019), 10B-scale language models (Raffel et al., 2020; Shoeybi et al., 2019; Black et al., 2022), to 100Bscale GPT-3 (Brown et al., 2020). Later, despite many 100B-scale LLMs (Lieber et al., 2021; Thoppilan et al., 2022; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Wu et al., 2021; Zeng et al., 2021; Wang et al., 2021) in both English and Chinese, they are not available to public or only accessible via limited APIs. The closeness of LLMs severely stymies its development. GLM-130B’s efforts, along with recent ElutherAI, OPT-175B (Zhang et al., 2022), and BLOOM-176B (Scao et al., 2022), aim to offer high-quality open-sourced LLMs to our community.\n\nTransferring. Though fine-tuning has been a de facto way for transfer learning, the evaluation for LLMs has been focused on prompting and in-context learning due to their tremendous sizes (Brown et al., 2020; Liu et al., 2021a). Nevertheless, some recent attempts has been on parameter-efficient learning on language models (Houlsby et al., 2019) and prompt tuning (i.e., P-tuning, Li & Liang (2021); Liu et al. (2021b); Lester et al. (2021); Liu et al. (2022)). For now we do not focus on them and will leave the comprehensive testing of them on GLM-130B in future study.\n\nInference. Most public-accessible LLMs nowadays are providing their services via limited APIs.In this work, an important part of our endeavor has been on LLMs’ efficient and fast inference. Related work may include distillation (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020), quantization (Zafrir et al., 2019; Shen et al., 2020; Tao et al., 2022), and pruning (Michel et al., 2019; Fan et al., 2019). Very recent work (Dettmers et al., 2022) shows that LLMs such as OPT-175B and BLOOM-176B can be quantized to 8 bit due to special distribution of outlier dimensions. In this work, we demonstrate GLM’s scaling law for INT4 weight quantization, which allows GLM-130B to inference on as few as 4×RTX 3090 (24G) GPUs or 8×RTX 2080 Ti (11G) GPUs.\n\n7 CONCLUSION AND LESSONS\n\nWe introduce GLM-130B, a bilingual pre-trained language model that aims to facilitate open and inclusive LLM research. GLM-130B’s technical and engineering undertakings generate insight into LLMs’ architectures, pre-training objectives, training stability and efficiency, and affordable inference. Altogether, it contributes to the high quality of GLM-130B in terms of both language performance on 112 tasks and ethical results on bias and toxicity benchmarks. Our experiences of both success and failure are condensed into the lessons for training 100B-scale LLMs, attached in the Appendix B.10.\n\n9\n\nEPRSTMTOCNLI-FCBUSTMCHID-FCCLUEWSC-FCC3WSC1.1CMNLIDRCDOCNLI_50KAFQMCCMRC2018020406080Acc. or EM92.573.877.590.177.477.583.977.077.174.771.255.788.853.864.487.153.554.981.151.729.544.669.016.6GLM-130BERNIE 3.0 Titan-260BPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENT\n\nThis research was supported by Natural Science Foundation of China (NSFC) 61825602, 62276148 and Zhipu.AI. We thank all our collaborators and partners from the Knowledge Engineering Group (KEG), Parallel Architecture & Compiler technology of Mobile, Accelerated, and Networked systems Group (PACMAN), Natural Language Processing Group (THUNLP) at Tsinghua University, and Zhipu.AI.\n\nETHICS STATEMENT\n\nWe hereby acknowledge that all of the co-authors of this work are aware of the provided ICLR Code of Ethics and honor the code of conduct. This work introduces an open-source Large Language Model (LLM), which could be used to generate synthetic text for harmful applications, such as telemarketing fraud, political propaganda, and personal harassment as is discussed in (Weidinger et al., 2021; Sheng et al., 2021; Dev et al., 2021). We do not anticipate any hazardous outputs, especially towards vulnerable and historically disadvantaged groups of peoples, after using the model.\n\nAnd to better collaborate with our community to prevent and ultimately eliminate the risks technically, we make the following crucial open efforts in this work:\n\nOpen-Sourced LLMs for Ethical Risk Study. While some people think that restricting the access of LLMs can prevent such harmful applications, we argue that promoting LLM inclusivity can lead to better defense against potential harms caused by LLMs. Currently, only governments and large corporations can afford the considerable costs of pre-training LLMs. There is no guarantee that organizations having the the substantial financial resources will not do harm using a LLM. Without access to such LLMs, individuals cannot even realize the role of LLMs in the harm.\n\nConversely, releasing an open LLM can provide access and transparency to all the researchers and promote the research to reduce the potential harm of LLMs, like algorithms to identify the synthetic text Gehrmann et al. (2019). Also, it is known that LLMs can suffer from problems in fairness, bias, privacy, and truthfulness Zhang et al. (2021); Lin et al. (2022); Liang et al. (2021); Bender et al. (2021). An open LLM can reveal the model parameters and internal states corresponding to specific inputs instead of providing APIs to black-box models. In conclusion, researchers can conduct analysis of LLMs’ flaws in depth and propose improved algorithms to solve the problems.\n\nEthical Evaluation and Improvements. We also evaluate our model over a wide range of English ethical evaluation benchmarks, including bias measurement (Nadeem et al., 2021; Nangia et al., 2020), hate speech detection (Mollas et al., 2020), and toxic generation estimation (Gehman et al., 2020). Notwithstanding their deficiency (Blodgett et al., 2021; Jacobs & Wallach, 2021), these datasets serve as a meaningful initial step towards an open quantitative evaluation LLMs.\n\nOur evaluation implies that our algorithm designs, especially the bilingual pre-training of a LLM, can significantly mitigate the biases and toxicity an LLM may present while keeping its strong language performance compared to other LLMs (Brown et al., 2020; Zhang et al., 2022) trained with monolingual English corpora (Cf. Appendix A for more details).\n\nREPRODUCIBILITY\n\nCompared to mainstream closed-sourced LLMs including GPT-3 175B(Brown et al., 2020), PaLM 540B (Chowdhery et al., 2022), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), LaMDA (Thoppilan et al., 2022), FLAN (Wei et al., 2022a), and many others, GLM-130B is opensourced and devotes to promote openness and inclusivity in LLM research from the very beginning.\n\nWe have paid great effort to ensure the reproducibility of our evaluation. For pre-training section, despite the unaffordable costs it needs to reproduce at present, we still make our best efforts to disclose the code, details, and the whole process of GLM-130B’s pre-training. Our endeavor to allow GLM-130B inference on few popularized GPUs such as 3090/2080 Ti also aligns with the reproducibility undertaking, as it allows most academic researchers to reproduce GLM-130B’s results on their offline machines. We also provide free APIs for individual users to test GLM-130B’s ability.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nPre-Training. We provide the complete training notes, Tensorboard logs, and code for our pretraining in our repository (Cf. Abstract). The pre-training hyper-parameters and cluster configuration are provided in Section 2.3 and Table 11. The training corpora composition and details for Multi-task Instruction Pre-training are provided in Section 2.2 and Appendix C.1 and C.2.\n\nEvaluation. We organize all the evaluation, including language benchmarks (LAMBADA, Pile, MMLU, BIG-bench, CLUE, and FewCLUE) and ethical benchmarks (CrowS-Pairs, StereoSet, ETHOS, RealToxicPrompts), into one-command-to-run bash scripts in our code repository. Data processing details for language modeling benchmarks are provided in Section 5.1 and Appendix C.4, for MMLU are provided in Section 5.2 and Appendix C.6, for BIG-bench are provided in Section 5.3 and Appendix C.5, for CLUE and FewCLUE are provided in 5.4. For all ethical evaluation, please refer to Appendix A for details.\n\nREFERENCES\n\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3554–3565, 2021.\n\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. Ext5: Towards extreme multi-task scaling for transfer learning. In International Conference on Learning Representations, 2022.\n\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684, 2021.\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n\narXiv:1607.06450, 2016.\n\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Févry, et al. Promptsource: An integrated In Proceedings of the development environment and repository for natural language prompts. 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 93–104, 2022.\n\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In FAccT ’21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021, pp. 610–623. ACM, 2021.\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1533–1544, 2013.\n\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432–7439, 2020.\n\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. In Proceedings of BigScience Episode\\# 5–Workshop on Challenges & Perspectives in Creating Large Language Models, pp. 95–136, 2022.\n\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. Stereotyping norwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1004–1015, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nNicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 6491– 6506. Association for Computational Linguistics, 2021.\n\nXavier Carreras and Lluís Màrquez.\n\nIntroduction to the conll-2005 shared task: Semantic role\n\nlabeling. In CoNLL, pp. 152–164, 2005.\n\nThiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. The 2020 bilingual, bi-directional WebNLG+ shared In Proceedings of the 3rd Intertask: Overview and evaluation results (WebNLG+ 2020). national Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pp. 55–76, Dublin, Ireland (Virtual), 12 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.webnlg-1.7.\n\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9640–9649, 2021.\n\nKe-Li Chiu and Rohan Alexander.\n\nDetecting hate speech with gpt-3.\n\narXiv preprint\n\narXiv:2103.12407, 2021.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, 2019.\n\nTim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise\n\nquantization. arXiv preprint arXiv:2110.02861, 2021.\n\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix\n\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nSunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Arjun Subramonian, J. M. Phillips, and Kai Wei Chang. Harms of gender exclusivity and challenges in non-binary representation in language technologies. ArXiv, abs/2108.12084, 2021.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.\n\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems, 34:19822–19835, 2021.\n\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. Advances in Neural Information Processing Systems, 32, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: In Proceedings of the General language model pretraining with autoregressive blank infilling. 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320–335, 2022.\n\nOndˇrej Dušek, David M. Howcroft, and Verena Rieser. Semantic noise matters for neural natural language generation. In Proceedings of the 12th International Conference on Natural Language Generation, pp. 421–426, Tokyo, Japan, October–November 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-8652. URL https://aclanthology.org/W19 -8652.\n\nHady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. T-rex: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), 2018.\n\nMihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj Kumar Goyal, Peter Ku, and Dilek Hakkani-Tür. Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. In LREC, 2020.\n\nAngela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with\n\nstructured dropout. arXiv preprint arXiv:1909.11556, 2019.\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxicityprompts: Evaluating Neural Toxic Degeneration in Language Models. dblp://journals/dblp, 2020.\n\nSebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. GLTR: Statistical detection and visualization of generated text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 111–116, Florence, Italy, July 2019. Association for Computational Linguistics.\n\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna Clinciu, Dipanjan Das, Kaustubh D Dhole, et al. The gem benchmark: Natural language generation, its evaluation and metrics. GEM 2021, pp. 96, 2021.\n\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346–361, 2021.\n\nPeter Hase, Mona T. Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. CoRR, abs/2111.13654, 2021.\n\nRuining He, Anirudh Ravula, Bhargav Kanagal, and Joshua Ainslie. Realformer: Transformer likes residual attention. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 929–943, 2021.\n\nDan Hendrycks and Kevin Gimpel.\n\nGaussian error linear units (gelus).\n\narXiv preprint\n\narXiv:1606.08415, 2016.\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021.\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019.\n\nAbigail Z Jacobs and Hanna Wallach. Measurement and fairness. In Proceedings of the 2021 ACM\n\nconference on fairness, accountability, and transparency, pp. 375–385, 2021.\n\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4163–4174, 2020.\n\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611, 2017.\n\nPaul R Kingsbury and Martha Palmer. From treebank to propbank. Citeseer.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.\n\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the\n\ncarbon emissions of machine learning. CoRR, abs/1910.09700, 2019.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, 2021.\n\nHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597, 2021.\n\nXiangyang Li, Yu Xia, Xiang Long, Zheng Li, and Sujian Li. Exploring text-transformers in aaai\n\n2021 shared task: Covid-19 fake news detection in english. In CONSTRAINT@AAAI, 2021.\n\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. Towards understanding and mitigating social biases in language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 6565–6576. PMLR, 2021.\n\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evalua-\n\ntion. White Paper. AI21 Labs, 2021.\n\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human In Proceedings of the 60th Annual Meeting of the Association for Computational falsehoods. Linguistics (Volume 1: Long Papers), pp. 3214–3252, Dublin, Ireland, May 2022. Association for Computational Linguistics.\n\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021a.\n\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018.\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\n\nunderstands, too. arXiv preprint arXiv:2103.10385, 2021b.\n\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 61–68, 2022.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances\n\nin neural information processing systems, 32, 2019.\n\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018.\n\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2381–2391, 2018.\n\nEric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. MemoryIn International Conference on Machine Learning, ICML 2022, based model editing at scale. 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 15817–15831. PMLR, 2022.\n\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos, and Grigorios Tsoumakas. Ethos: an online\n\nhate speech detection dataset. arXiv preprint arXiv:2006.08328, 2020.\n\nMoin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5356–5371, 2021.\n\nNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel Bowman. Crows-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1953–1967, 2020.\n\nDeepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efficient pipeline-parallel dnn training. In International Conference on Machine Learning, pp. 7937–7947. PMLR, 2021.\n\nTomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. The genia corpus: An annotated research abstract\n\ncorpus in molecular biology domain. In HLT, pp. 82–86, 2002.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1525–1534, 2016.\n\nDavid A. Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. CoRR, abs/2104.10350, 2021.\n\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Björkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. Towards robust linguistic analysis using ontonotes. In CoNLL, pp. 143–152, 2013.\n\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables\n\ninput length extrapolation. In International Conference on Learning Representations, 2021.\n\nAmy Pu, Hyung Won Chung, Ankur Parikh, Sebastian Gehrmann, and Thibault Sellam. Learning compact metrics for MT. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 751–762, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.58. URL https://aclanthology.org/2021.emnlp-main.58.\n\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained models for natural language processing: A survey. Science China Technological Sciences, 63(10): 1872–1897, 2020.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\n\nstanding with unsupervised learning. 2018.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821–8831. PMLR, 2021.\n\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505–3506, 2020.\n\nSebastian Riedel, Limin Yao, and Andrew McCallum. Modeling relations and their mentions with-\n\nout labeled text. In ECML-PKDD, pp. 148–163, 2010.\n\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5418–5426, 2020.\n\nDan Roth and Wen-tau Yih. A linear programming formulation for global inference in natural\n\nlanguage tasks. In HLT-NAACL, pp. 1–8, 2004.\n\nRachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in\n\ncoreference resolution. In NAACL-HLT (2), 2018.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, et al. PhoIn Advances in torealistic text-to-image diffusion models with deep language understanding. Neural Information Processing Systems.\n\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.\n\nErik F. Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-\n\nindependent named entity recognition. In HLT-NAACL, pp. 142–147, 2003.\n\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of\n\nbert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, 2022.\n\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ́c, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\nTimo Schick, Sahana Udupa, and Hinrich Schütze. Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. Transactions of the Association for Computational Linguistics, 9:1408–1424, 2021.\n\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. In Proceedings of the 2020 Conference on MLSUM: The multilingual summarization corpus. Empirical Methods in Natural Language Processing (EMNLP), pp. 8051–8067, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.647. URL https://aclanthology.org/2020.emnlp-main.647.\n\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8815–8821, 2020.\n\nEmily Sheng, Kai-Wei Chang, P. Natarajan, and Nanyun Peng. Societal biases in language genera-\n\ntion: Progress and challenges. In ACL, 2021.\n\nSam Shleifer, Jason Weston, and Myle Ott. Normformer: Improved transformer pretraining with\n\nextra normalization. arXiv preprint arXiv:2110.09456, 2021.\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.\n\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\n\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 3645–3650. Association for Computational Linguistics, 2019.\n\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer\n\nwith rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149–4158, 2019.\n\nChaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. Compression of generative pre-trained language models via quantization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4821–4836, 2022.\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.\n\nDenis Timonin, Bo Yang Hsueh, and Vinh Nguyen. Accelerated inference for large transformer\n\nmodels using nvidia triton inference server. NVIDIA blog, 2022.\n\nLeslie G Valiant. A bridging model for parallel computation. Communications of the ACM, 33(8):\n\n103–111, 1990.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nDavid Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. Entity, relation, and event extraction with contextualized span representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5784–5789, 2019.\n\nC. Walker and Linguistic Data Consortium. ACE 2005 Multilingual Training Corpus. Linguistic\n\nData Consortium, 2005. ISBN 9781585633760.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In NeurIPS 2019, pp. 3261–3275, 2019.\n\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n\nChenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong, Jie Tang, and Dawn Song. Deepstruct: Pretraining of language models for structure prediction. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 803–823, 2022a.\n\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet:\n\nScaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555, 2022b.\n\nShuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, et al. Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2112.12731, 2021.\n\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776–5788, 2020.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-\n\naugmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022c.\n\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022a.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022b.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022c.\n\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359, 2021.\n\nShaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, et al. Yuan 1.0: Large-scale pre-trained language model in zeroshot and few-shot learning. arXiv preprint arXiv:2110.04725, 2021.\n\nYongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning—a comprehensive evaluation of the good, the bad and the ugly. IEEE transactions on pattern analysis and machine intelligence, 41(9):2251–2265, 2018.\n\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pp. 10524–10533. PMLR, 2020.\n\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, et al. Clue: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 4762–4772, 2020.\n\nLiang Xu, Xiaojing Lu, Chenyang Yuan, Xuanwei Zhang, Huilin Xu, Hu Yuan, Guoao Wei, Xiang Pan, Xin Tian, Libo Qin, et al. Fewclue: A chinese few-shot learning evaluation benchmark. arXiv preprint arXiv:2107.07498, 2021.\n\nSha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. AI Open, 2:65–68, 2021.\n\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pp. 36–39. IEEE, 2019.\n\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, et al. Pangu-\\α: Large-scale autoregressive pretrained chinese language models with auto-parallel computation. arXiv preprint arXiv:2104.12369, 2021.\n\nChiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini. Counterfactual memorization in neural language models. CoRR, abs/2112.12938, 2021.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. Position-\n\naware attention and supervised data improve slot filling. In EMNLP, pp. 35–45, 2017.\n\nBen Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. “going on a vacation” takes longer than “going for a walk”: A study of temporal commonsense understanding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3363–3369, 2019.\n\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix X. Yu, and\n\nSanjiv Kumar. Modifying memories in transformer models. CoRR, abs/2012.00363, 2020.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nPart I\n\nAppendix\n\nTable of Contents\n\nA Ethics: Evaluation on Biases and Toxicity .\nA.1 Bias Measurement: CrowS-Pairs .\nA.2 Bias Measurement: StereoSet .\n. A.3 Hate Speech Detection: ETHOS . .\nA.4 Toxic Genearation: RealToxicPrompts\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\nB Technical Details\n\n.\n\n.\n\n.\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. B.1 Tokenization . .\nB.2 Layer Normalization . .\nB.3 Positional Encoding and Feed-forward Network . .\n. .\nB.4 Pipeline Parallel Analysis . .\n. B.5 Inference Acceleration . .\n. .\n. B.6 Activation Outlier Analysis . .\n. .\nB.7 Weight Quantization . .\nB.8 Quantization settings .\n. .\nB.9 Ablation on Contribution Attribution . .\n. B.10 Lessons Learned .\n\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n\n. .\n\n. .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\nC Dataset and Evaluation Details\n\n.\n\n.\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\nC.1 Multi-task Instruction Pre-training (MIP) . .\nC.2 Data and prompts in MIP for DeepStruct .\n. .\nC.3 Result Sources for GPT-3, BLOOM-176B, and OPT-175B . .\n. .\n. C.4 Pile Test-set Evaluation . .\n. .\nC.5 BIG-bench-lite Evaluation . .\n. C.6 MMLU Evaluation .\n. .\n. C.7 Chinese Language Understanding Evaluation . .\n. .\n. C.8 Natural Language Generation . .\n. .\nC.9 Winograd-Style Tasks . .\n. .\n. .\nC.10 Closed-book Question Answering . C.11 Commonsense Reasoning . .\n. .\n. .\nC.12 Fixed Label Datasets: A Case Study in Natural Language Inference . .\nC.13 SuperGLUE . .\n. C.14 Chain-of-Thought Prompting .\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n. .\n.\n\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nD Scaling and Emergent Abilities in GLM-130B\n\nE Contributions\n\n.\n\n.\n\nE.1 Preparation .\n. E.2 Model Training . .\n. .\nE.3 Post Training E.4 Project Management . E.5 Computation Sponsor\n\n. .\n.\n\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n.\n\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n. .\n. .\n. .\n. .\n. .\n\n. .\n. .\n.\n\nF A Brief History of GLM-130B\n\nG Broader Impact\n\nG.1 Impact on AI Research . .\nG.2 Impact on Individual Developers and Small Companies .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n20\n\n21 21 21 22 22\n\n23 23 24 24 25 27 27 28 28 29 30\n\n32 32 32 39 39 40 40 40 41 43 43 44 44 44 45\n\n46\n\n52 52 52 52 52 52\n\n53\n\n55 55 55\n\nPublished as a conference paper at ICLR 2023\n\nG.3 Social Impact\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nH Environmental Impact\n\n55\n\n56\n\nA ETHICS: EVALUATION ON BIASES AND TOXICITY\n\nAlbeit LLMs’ strong abilities in language and beyond, which could bring substantial welfare to human beings, they can potentially produce toxic and illegal contents for evil use (Weidinger et al., 2021; Sheng et al., 2021; Dev et al., 2021; Bommasani et al., 2021). In GLM-130B, before granting model weight to applicants, in the model license we demand them to agree that they will not use it for any deeds that may be harmful to society and human beings.\n\nAdditionally, from a technical perspective, we argue that we must also understand LLMs’ toxic and biased behaviors and ultimately eliminate them. This aligns with our commitment to “LLM Inclusivity”, as it is necessary to include more people in the open-sourced LLM research to facilitate the process. Moreover, if an LLM is shown to be good at identifying toxic and biased content, techniques such as self-diagnoses (Schick et al., 2021) can help to reduce the harmful generation in a self-consistent post-processing procedure. Therefore, as an initial step, we evaluate GLM130B over a variety of related benchmarks to shed light on the challenging topic. Despite their limitations (Blodgett et al., 2021; Jacobs & Wallach, 2021) which should be addressed in future work, they still serve as a good start to arouse the community’s awareness of the problem.\n\nA.1 BIAS MEASUREMENT: CROWS-PAIRS\n\nCrowS-Pairs (Nangia et al., 2020), or namely Crowdsourced Stereotype Pairs benchmark, is widely used for measuring biases for masked language models. It collects 1508 examples with nine different conventional biases and adopts a probing-based approach to compare the pseudolog-likelihood of a pair of stereotypical and antistereotypical sentences. Since GLM-130B is pre-trained with autoregressive blanking infilling, CrowS-Pairs evaluation is directly applicable. We compare the GPT-3 Davinci and OPT-175B’s results on CrowS-Pairs reported in (Zhang et al., 2022) with GLM-130B.\n\nTable 5: CrowS-Pairs (Nangia et al., 2020) Bias Measurement. The lower scores the better.\n\nCategory\n\nGPT-3 OPT-175B GLM-130B\n\n62.6 Gender 73.3 Religion 64.7 Race/Color 76.2 Sexual orientation 64.4 Age 61.6 Nationality 76.7 Disability 74.6 Physical appearance Socioeconomic status 73.8\n\nOverall\n\n67.2\n\n65.7 68.6 68.6 78.6 67.8 62.9 76.7 76.2 76.2\n\n69.5\n\n55.7 73.3 58.5 60.7 63.2 64.1 71.6 74.6 70.9\n\n65.8\n\nOur results are presented in Table 5. GLM-130B shows fewer biases on almost all kinds of stereotypes except for religion and nationality. We speculate that it is because GLM-130B is a bilingual pre-trained LLM that learns the semantics for certain content from both English and Chinese corpora. Since CrowsS-Pairs’ stereotypes mainly draw from the US Equal Employment Opportunities Commission’s list2, the bias distributions in two different cultures and languages may be different and consequently reconcile social biases in GLM-130B on a benchmark originally designed for English-language society. We think this is an interesting finding, as multi-lingual pre-training may help LLMs to present less harmful biases for better fairness. Finally, we also admit that GLM130B may in turn presents some special Chinese biases which currently lack testing benchmarks and require considerable future efforts to detect and prevent.\n\nA.2 BIAS MEASUREMENT: STEREOSET\n\nAnother widely used bias and stereotype evaluation benchmark is StereoSet (Nadeem et al., 2021), which is also adopted in (Lieber et al., 2021; Artetxe et al., 2021; Zhang et al., 2022). To balance the evaluation between bias detecting and language modeling quality, StereoSet reports a series of metrics including Language Modeling Scores (LMS), Stereotype Score (SS), and Idealized Context Association Test Score (ICAT) as an overall averaged metric. For example, given the premise “She\n\n2https://www.eeoc.gov/prohibited-employment-policiespractices\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nis the twin’s mother”, StereoSet provides three candidate hypothesis: 1) “the water is deep”, 2) “she is a lazy, unkind person”, and 3) “she is a kind, caring woman”. The first option servers as a distractor to test models’ language capability and calculate LMS; the second and third statements are anti-stereotypical and stereotypical respectively and used for calculating SS. A widely-adopted technique here is to calibrate the likelihood of an option according to its length (Lieber et al., 2021; Zhang et al., 2022), as the distractor term is particularly short.\n\nFollowing (Zhang et al., 2022), we normalize scores over tokens rather than characters (Lieber et al., 2021) to yield model predictions for calculating the metrics. The results are shown in Table 6. As we observe, GLM-130B exceedingly outperforms GPT-3 Davinci and OPT-175B on all metrics. Such results accurately align with our discoveries in language modeling experiments and CrowS-Pairs bias evaluation, that GLM-130B has a high quality in both language modeling and social fairness. Table 6: StereoSet (Nadeem et al., 2021) Bias Measurement with LMS (↑), SS (↓), and ICAT (↑).\n\nCategory\n\nProfession\n\nGender\n\nReligion\n\nRace\n\nOverall\n\nLMS SS ICAT LMS SS ICAT LMS SS ICAT LMS SS ICAT LMS SS ICAT\n\nGPT-3 78.4 63.4 57.5 75.6 66.5 50.6 80.8 59.0 66.3 77.0 57.4 65.7 77.6 60.8 60.8 OPT-175B 74.1 62.6 55.4 74.0 63.6 53.8 84.0 59.0 68.9 74.9 56.8 64.8 74.8 59.9 60.0 GLM-130B 86.5 59.6 69.9 83.9 63.5 61.2 91.0 53.5 84.6 85.7 54.1 78.7 86.0 57.3 73.5\n\nA.3 HATE SPEECH DETECTION: ETHOS\n\nSocial media corpus may contain hate speeches, and to investigate to what extent LLMs know and can help to identify them is crucial. We adopt the ETHOS dataset originally proposed in (Mollas et al., 2020) to detect sexism and racism speech on zero-shot or few-shot datasets created by (Chiu & Alexander, 2021). GPT-3 Davinci (a public-accessible variant of GPT-3 175B) and OPT 175B are also tested on the benchmark (whose results are reported in (Zhang et al., 2022)). For binary classification including Zero-shot, One-shot, and Few-shot (binary) (which answers “yes” or “no”), we report binary F1; for multiclass classification (which answers “yes”, “no”, or “neither”), we report micro F1. We adopt almost the same prompts as in (Chiu & Alexander, 2021), except aligning the Few-shot (binary) prompt to the form used in One-shot and adding the word “Classification” before the colon in the original Few-shot (multiclass) prompt.\n\nResults are shown in Table 7. We find that GLM-130B outperforms two other LLMs among four different settings. On one hand, GLM130B’s pre-training over unsupervised diverse corpora from online forums and social media including sections such as “hackernews”, “stackexchange”, and “pile_cc” can endow our model with the background knowledge to identify those speeches. On the other hand, the MIP training may also improve GLM-130B’s zero-shot and few-shot capabilities.\n\nTable 7: ETHOS (Mollas et al., 2020) Hate speech detection. “(bi)” and “(mul)” denote binary and multiclass classification respectively. All scores are F1 and the higher the better.\n\nGPT-3 OPT-175B GLM-130B\n\nZero-shot One-shot Few-shot (bi) Few-shot (mul)\n\n62.8 61.6 35.4 67.2\n\n66.7 71.3 75.9 81.2\n\n68.8 79.1 79.7 85.8\n\nA.4 TOXIC GENEARATION: REALTOXICPROMPTS\n\nEvaluating the toxicity of generation by given prompts is an important part of a model’s safe deployment. We evaluate the toxic generation of GLM-130B on the RealToxicPrompts (Gehman et al., 2020) dataset. Following its settings, we use nucleus sampling (p = 0.9) to generate 25 continuations for each of the 10K random sampled prompts, limiting the maximum generated length to 128 tokens. Then we report the mean toxicity probabilities of 25 continuations evaluated by Perspective API3. In order to make a fair comparison\n\n3https://www.perspectiveapi.com/\n\n22\n\nFigure 9: RealToxicPrompts (Gehman et al., 2020) evaluation. Lower continuation toxicity probability is better.\n\n0.00.20.40.60.81.0Prompt Toxicity Probability (Binned)0.050.100.150.200.25Toxicity Probability of ContinuationGLM-130BGPT-3 DavinciPublished as a conference paper at ICLR 2023\n\nFigure 10: Handling training collapses and instability is the first priority when training LLMs.\n\nunder different tokenization methods, we only report the toxicity score of the first complete sentence of a continuation as we found that the score returned by the Perspective API seems to increase with sentence length.\n\nResults are shown in Figure 9. Generally, as the toxicity of the given prompt increases, the toxicity probability of the continuation increases accordingly in both models. Compared to GPT-3 Davinci, GLM-130B has a lower toxicity rate in all cases, indicating that GLM-130B is less prone to generating toxic content.\n\nB TECHNICAL DETAILS\n\nIn this section, we introduce additional details about the technical issues we have identified and solved throughout the GLM-130B training. Along with concurrent open-source LLM efforts, we believe that those published details could serve as great cornerstones to future LLM training.\n\nB.1 TOKENIZATION\n\nFor the tokenization of the corpus, we implement a text tokenizer based on the package icetk with several adjustments. As an image-text unified tokenizer, the vocabulary size of icetk is 150000. The first 20000 tokens are image tokens and the rest are text tokens. The text tokenizer of icetk is formulated and trained by sentencepiece4, on a 25GB bilingual corpus equally distributed with English and Chinese contents. We divide tokens recognized by the tokenizer into four categories. The common tokens are assigned from No.20000 to No.20099, consisting of punctuations, numbers and spaces free of extended definition. No.20100 to No.83822 are English tokens and No.83823 to\n\n4https://github.com/google/sentencepiece\n\n23\n\n(a) OPT 175B’s experiments(b) BLOOM 176B’s experiments(c) GLM 130B’s experiments(d) GLM 130B’s real trainingRe-load and adjust thelearning rate after collapsePublished as a conference paper at ICLR 2023\n\nNo.145653 are Chinese tokens. Tokens after No.145653 are other special tokens including concatenated punctuations and pieces from other languages, etc.\n\nDuring our implementation, We ignore the first 20000 image tokens and simply utilize the latter 130000 intended for text tokenization. we disable the ignoring of linebreak to tokenize the linebreak mark \\n into No. 20004 token <n>. On the basis of inherent tokens, we add special tokens [MASK] and [gMASK] for model prediction. We also add special tokens <sop>, <eop>, <eos> for sentence and passage separation.\n\nB.2 LAYER NORMALIZATION\n\nHere we briefly introduce the history of layer normalization in language modeling problems, and how its variants perform in recent LLMs including our experiments for them on GLM-130B.\n\nPost-LN (Vaswani et al., 2017). Post-LN is jointly proposed with the transformer architecture and is placed between the residual blocks. It is then adopted by BERT (Devlin et al., 2019) for bidirectional language model pre-training. Nevertheless, Post-LN was later accused of transformers’ slow and vulnerable converging (Xiong et al., 2020) and the Pre-LN emerged as a substitute.\n\nPre-LN (Xiong et al., 2020). On the contrary, Pre-LN is located in the residual blocks to reduce exploding gradients and becomes dominant in existing language models, including all recent LLMs. However, OPT-175B (Zhang et al., 2022), BLOOM (Scao et al., 2022), and text-to-image model CogView Ding et al. (2021) later observe that Pre-LN is still unable to handle the vulnerable training when models scale up to 100B or meet multi-modal data. This is also justified in GLM-130B’s preliminary experiments, where Pre-LN consistently crashes in its early stage training.\n\nAdditionally, another problem rooted in Pre-LN transformers is that it may harm the model performance after tuning compared to Post-LN. This is observed in (He et al., 2021).\n\nSandwich-LN (Ding et al., 2021). As a remedy, on top of Pre-LN, CogView (later in Normformer (Shleifer et al., 2021)) develops Sandwich-LN which appends extra normalization to the end of each residual branch. Accompanied with PB-Relax (Precision-Bottleneck Relaxation) techniques, they stabilize the training of a 4-billion text-to-image generation model. Despite its superiority over Pre-LN, sadly Sandwich-LN is also proved to collapse in GLM-130B training; let alone the potential consequent weaker tuning performance caused by its Pre-LN nature.\n\nB.3 POSITIONAL ENCODING AND FEED-FORWARD NETWORK\n\nPositional Encoding Vanilla transformer adopts absolute (or sinuous) position encoding, and is later evolved into relative positional encoding (Dai et al., 2019). Relative PEs can capture word relevance better than absolute positional encoding. Rotary Positional Embedding (RoPE) (Su et al., 2021) is a relative position encoding implemented in the form of absolute position encoding, and its core idea is shown in the following equation.\n\n(Rmq)⊤(Rnk) = q⊤R⊤\n\nmRnk = q⊤Rn−mk\n\n(1)\n\nThe product of q at position m and k at position n is related to their distance n − m, which reflects the relativity of the position encoding. The definition of R in the above equation is\n\nRd\n\nθ,m =\n\n\n\n \n \n \n \n\n\ncos mθ1 − sin mθ1 cos mθ1 sin mθ1 0\n0 0\n0\n\n...\n\n0 0\n\n...\n\n0 0\n\n0 0\n\n0 0\n\ncos mθ2 − sin mθ2 cos mθ2 sin mθ2 ... ...\n\n0 0\n\n0 0\n\n· · · · · · · · · · · · . . . · · · · · ·\n\n0 0\n0 0\n\n...\n\n0 0\n0 0\n\n...\n\ncos mθd/2 − sin mθd/2 cos mθd/2 sin mθd/2\n\nTo allow its value to decay as the distance increases, θ takes the value\n\n(cid:26)\n\nθ =\n\nθi = 10000\n\n−2(i−1) d\n\n,\n\n(cid:20)\n\ni ∈\n\n1, 2, · · · ,\n\n(cid:21)(cid:27)\n\nd 2\n\n24\n\n\n\n \n \n \n \n\n\n(2)\n\n(3)\n\nPublished as a conference paper at ICLR 2023\n\nA two-dimensional absolute position encoding method is proposed in vanilla GLM for modeling both intra- and inter-span position information. In GLM-130B, different from the two-dimensional positional encoding used in vanilla GLM, we turn back to conventional one-dimensional positional encoding. However, we originally thought that two-dimensional form cannot be directly applied to RoPE5. As a substitute plan, in GLM-130B we simply remove the second dimension used in the original GLM as we find that the unidirectional attention mask sub-matrices for [MASK] generation indicate the token order as well. This observation results in our transforming GLM-130B’s positional encoding into a one-dimensional one according to the following strategies:\n\n• For sequences corrupted by short spans, we discard the second-dimensional position encoding. • For sequences corrupted by a long span at the end, we change the positional ids to one-dimensional 0, 1, · · · , s − 1, and generated tokens will just prolong the first-dimensional positional encoding from the last context token s − 1.\n\nFeed-forward Network Some recent efforts to improve transformer architecture have been on the FFN, including replacing it with GLU (adopted in PaLM). Research shows that using GLU can improve model performance, which is consistent with our experimental results (Cf. Table 8). Specifically, we use GLU with the GeLU (Hendrycks & Gimpel, 2016) activation. as\n\nFFNGeGLU (x; W1, V , W2) = (GeLU(xW1) ⊗ xV ) W2\n\n(4)\n\nIn order to keep the same parameter as the vanilla FFN, the feed-forward size dffn (which is usually 4dH, where dH is the hidden dimension) is reduced to 8\n\n3 dH as the V is additionally introduced.\n\nAblation Study on PE and FFN In order to validate our PE and FFN choices, we test them in our experiments by pre-training GLMBase (110M) over a random 50G Chinese and English mixed corpus. We compare absolute PE with two recent popular relative PE variants, RoPE (Chowdhery et al., 2022) and ALiBi (Press et al., 2021). For FFN, we compare vanilla FFN with Gate Linear Unit with GeLU activations. Results from Table 8 show that both ALiBi and RoPE improve perplexity on the test set, and the improvement is more significant with RoPE while using GeGLU can further improve the model’s performance.\n\nTable 8: Ablation Study for PE and FFN on GLMBase\n\nModel\n\nTest PPL\n\nGLMBase\n\n24.58 24.14 + ALiBi + RoPE 22.95 + RoPE + GeGLU 22.31\n\nB.4 PIPELINE PARALLEL ANALYSIS\n\nIn pipeline parallelism, each stage consists of three operations (Cf. Figure 11(a)): forward (denoted as F), backward (denoted as B), and optimizer step (denoted as U). However, naive sequential pipeline implementation leads to an unbearable amount of bubbles. The improved Gpipe (Huang et al., 2019) (Cf. Figure 11(b)) strategy reduces bubbles drastically via splitting data into microbatches; the more micro-batches there are, the more stages can compute simultaneously in an iteration. The recent PipeDream-Flush (Narayanan et al., 2021) (Cf. Figure 11(c)) additionally optimizes the GPU memory usage by interweaving forward and backward from different stages to reduce forward activation’s memory occupation.\n\nWe analyze the bubble share in GLM-130B’s pre-training by assuming that the number of pipeline segments is p, the number of micro-batches is m, and the time for forward and backward per microbatch are tf and tb. In ideal case, forward and backward take tideal = m(tf +tb). But in practice, the default pipeline delivery strategy causes p − 1 forward propagation and p − 1 backward propagation bubbles, respectively, for a total time of tbubble = (p − 1)(tf + tb), so that the bubble occupancy is\n\nbubble-ratio =\n\ntbubble tideal + tbubble\n\n=\n\np − 1 m + p − 1\n\n(5)\n\nFor larger numbers of micro-batches, the bubble percentage will be reduced to an acceptable level. In particular, experiments in GPipe Huang et al. (2019) show that when m ≥ 4p, the total percentage\n\n5We later found the instructions to implement two-dimensional RoPE from its author’s blog https:\n\n//kexue.fm/archives/8397, but our training has proceeded for weeks.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\n(a) Naive pipeline implementation, which can be extremely inefficient.\n\n(b) GPipe (Huang et al., 2019) implementation.\n\n(c) Pipedream (Narayanan et al., 2021) implementation (used in GLM-130B).\n\nFigure 11: Different pipeline strategies and their conceptual comparison.\n\nof pipeline bubble time is reduced to a negligible level due to the forward recomputation technique in backpropagation that allows some overlap in computational communication, thus showing that the bubbles introduced in parallel by the pipeline model do not seriously deplete the training efficiency.\n\nIn general, in order to make full use of the hardware, it is common to place models into model parallel groups consisting of multiple nodes and try to use the full memory of each node. In this case, we can freely adjust the ratio of pipeline model parallelism and tensor model parallelism. Since data parallelism hardly affects the computation time, we assume that the scale of data parallelism is d = 1, the total number of nodes is n, the scale of tensor model parallelism is t, and the scale of pipeline model parallelism is p, and satisfies n = t × p, the bubble share in this case is\n\nbubble-ratio =\n\nn/t − 1 m + n/t − 1\n\n(6)\n\nFrom the above equation, we can see that increasing the size of tensor parallelism will further reduce the bubble ratio. However, the tensor parallelism scale cannot be increased indefinitely, which would lead to a reduction in computational granularity and greatly increase the communication cost across a certain threshold. Therefore, we can conclude that the size of tensor model parallelism should increase slowly as the model size increases, but not more than the number of graphics cards in a single machine. In the training of GLM-130B, the experiments show that the optimal tensor parallelism scale is t = 4 and does not scale up to the scale of t = 8 in the DGX-A100 system. The other parameters are m = 176, p = 8, and the bubble share is calculated to be only 3.8%, which is sufficient to demonstrate the efficiency of pipeline model parallelism.\n\n26\n\nGPU 0F0B0GPU 1GPU 2GPU 3F0F0F0B0U0B0B0U0U0U0F1B1F1F1F1B1B1B1U1U1U1U1ForwardBackwardOptimizer StepTimeBubble timeGPU 0F0GPU 1GPU 2GPU 3ForwardBackwardOptimizer StepTimeF1F2F3F4F5F6F7F0F1F2F3F4F5F6F7F0F1F2F3F4F5F6F7F1F2F3F4F5F6F7B7B0B1B2B3B4B5B6U0U0U0U0B7B0B1B2B3B4B5B6B7B0B1B2B3B4B5B6B7B0B1B2B3B4B5B6F0GPU 0F0GPU 1GPU 2GPU 3B0ForwardBackwardOptimizer StepTimeF1F2F3F0F1F2F3F0F1F2F3F0U0U0U0U0B0B2B1F1B2F2B3F3B4F4B5F5B6F6B7F7B1B3F4B4F5B5F6B6F7B7B0B1B2F4B3F5B4F6B5F7B7B6B0B1F4B2F5B3F6B4F7B7B6B6Published as a conference paper at ICLR 2023\n\nTable 9: Decoding speed in our real trials between BLOOM-176B (Scao et al., 2022) (from Huggingface Transformers) and GLM-130B’s implementation in 16-bit precision with 8 × A100 (80G).\n\nDecode Tokens\n\n128\n\n512\n\n1024\n\n2048\n\nBLOOM-176B GLM-130B\n\n36.76s 4.40s (×8.4)\n\n137.91s 18.77s (×7.3)\n\n287.93s 39.81s (×7.2)\n\n631.81s 89.88s (×7.0)\n\nFigure 12: Distribution of outliers in GLM-130B’s activations. The vertical axis denotes the hidden state dimensions (4,096 rather than 12,288 as this is a parallel segment), and the horizontal denotes tokens in a input sentence. Using a 128×128 2D histogram to get a better view of the distribution of outliers. The figure on the right swaps some of the vertical coordinates so that it can be clearly seen that the outlier occur about 30% of its dimensions.\n\nB.5\n\nINFERENCE ACCELERATION\n\nA model’s plain PyTorch implementation is easy to read and run, but it can be intolerably slow for LLMs. Based on NVIDIA’s FasterTransformer6 we spend two months implementing GLM-130B into C++ to speed up inference, including the following main optimizations:\n\n• Optimize time-costing operations such as GeGLU, Layer Normalization, and SoftMax. • Reduce the number of GPU kernel calls (e.g., fuse MultiheadAttention into one computation ker-\n\nnel).\n\n• Specify the algorithm of the best performance when calling cuBLAS. • Improve the computing efficiency by transposing the model parameters in advance. • Use half2 in FP16 computation to double the half’s access bandwidth and computing throughput.\n\nWe currently pack up the full FasterTransformer implementation for GLM-130B into a plug-andplay docker image for users’ convenience, and we are still working on adapting it to our Pytorch implementation by only changing one line of code. A comparison between our speeding up GLM130B implementation and the so far default available BLOOM-176B implementation in Huggingface Transformers7 is shown in Table 9. Our implementation for GLM-130B can be 7.0 to 8.4 times faster than BLOOM-176B’s Pytorch implementation. The exertion to accelerate LLM for tolerable response speed could be extremely crucial to its popularization.\n\nB.6 ACTIVATION OUTLIER ANALYSIS\n\nAs is described in prior sections, GLM-130B’s weight can be quantized into INT4 to drastically cut down parameter redundancy in the inference. However, we also find that GLM-130B’s activations (i.e., hidden states between layers) cannot be properly quantized, as they contain value outliers as is also suggested in concurrent literature (Dettmers et al., 2022).\n\nWhat is special in GLM-130B is that 30% of its dimensions may present value outliers (Cf. Figure 12), while other GPT-based LLMs (e.g., OPT-175B and BLOOM 176B) only has very few outlying dimensions (Dettmers et al., 2022). Therefore, the solution to decompose matrix multipli-\n\n6https://github.com/NVIDIA/FasterTransformer 7https://huggingface.co/docs/transformers/model_doc/bloom\n\n27\n\nPublished as a conference paper at ICLR 2023\n\ncation for higher-precision computation in outlying dimensions proposed in (Dettmers et al., 2022) is not applicable to GLM-130B.\n\nWe study whether these outliers can be ignored in LLM quantization, and the answer is interestingly “no”. These values can be several orders of magnitude larger than ordinary activation values (Cf. Figure 13). While most values (accounts for 99.98% dimensions in a hidden state) stay less them 6, those two outlying dimensions can reach 50 or even over 100. They are speculated to be some important clues for GLM-130B and potentially other LLMs to memorize some fixed world or language knowledge, and thus removing or omitting them in quantization can lead to significant performance degradation.\n\nB.7 WEIGHT QUANTIZATION\n\nB.7.1 PRELIMINARIES\n\nFigure 13: GLM-130B’s activation outliers’ absolute value scale.\n\nAbsmax Quantization is a symmetric quantization that a range of [−absmax(x), absmax(x)] is mapped to [−(2b − 1), 2b − 1] for x.\n\nsx =\n\nabsmax(x) 2b−1 − 1\n\nxq = round(x/sx)\n\n(7)\n\n(8)\n\nwhere sx is the scaling factor, xq is the quantization result and b is the bit width.\n\nZeropoint Quantization is an asymmetric quantization that a range of [min(x), max(x)] is mapped to [−(2b − 1), 2b − 1].\n\nsx =\n\nmax(x) − min(x) 2b − 2\n\nzx = round(min(x)/sx) + 2b−1 − 1 xq = round(x/sx) − zx\n\n(9)\n\n(10) (11)\n\nwhere zx is the zero point.\n\nCol/Row-wise Quantization Using a single scaling factor for the weight matrix often leads to more quantization errors because one single outlier leads to a decrease in the quantization precision of all other elements. A common workaround is to group the weight matrix by rows or by columns, with each group being quantized separately and having independent scaling factors.\n\nB.8 QUANTIZATION SETTINGS\n\nOur goal is to save GPU memory as much as possible without hurting model performance. In practice, we only quantize linear layers, which take up most of the transformer parameters, and leave input/output embedding, layer normalization, and bias terms unchanged. At the quantization precision of INT4, two INT4 weights are compressed into one INT8 weight for saving GPU memory usage. Absmax quantization is adopted since we found it enough to maintain model performance, and it is more computationally efficient than zeropoint quantization. During inference, only quantized weights are stored in GPU memory, the FP16 weights for linear layers will be dequantized at runtime.\n\nB.8.1 QUANTIZATION RESULTS AT SCALES\n\nGLM models at 110M to 10B scale are from GLM’s original paper(Du et al., 2022). Although the architecture of smaller scale GLMs are not the same as GLM-130B, we believe that the training objective is the key factor for quantization. Table 10 shows the performance of GLM and BLOOM family models at different scales on the LAMBADA dataset with different quantization methods. Almost all models maintain performance at INT8 precision. In general, GLM maintains better performance than BLOOM at INT4 precision as it scales.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nTable 10: Accuracy on LAMBADA dataset for GLM and BLOOM family at 100M to 176B scales across different quantization precision.\n\nBLOOM-560M BLOOM-1B1 BLOOM-3B BLOOM-7B BLOOM-176B\n\nOriginal Absmax INT8, col-wise Absmax INT4, col-wise Absmax INT4, row-wise Zeropoint INT4, col-wise Zeropoint INT4, row-wise\n\n31.40% 26.12% 9.30% 21.37% 11.51% 24.95%\n\n40.68% 40.69% 17.43% 35.80% 26.51% 33.05%\n\n48.30% 48.83% 37.88% 40.95% 41.65% 43.63%\n\n54.91% 55.33% 38.04% 46.75% 46.63% 49.41%\n\n64.37% 65.03% 34.83% NaN 48.26% NaN\n\nGLM-110M GLM-335M GLM-2B\n\nGLM-10B\n\nGLM-130B\n\nOriginal Absmax INT8, row-wise Absmax INT4, row-wise Zeropoint INT4, row-wise\n\n29.36% 29.25% 3.26% 5.45%\n\n48.51% 48.69% 38.25% 42.64%\n\n68.19% 68.12% 62.62% 64.74%\n\n72.35% 72.37% 71.03% 70.50%\n\n80.21% 80.21% 79.47% 80.63%\n\nFigure 14: Contribution attribution analysis on GLM objective and MIP training. We take GLM10B (English only) as an example in the ablation. Generally, GLM objective’s bidirectional attention accounts for 70% of the improvements, while MIP’s major contribution lies in text similarity tasks.\n\nB.8.2 WEIGHT DISTRIBUTION ANALYSIS\n\nTo achieve INT4 weight quantization, we analyze the weight value distribution of major linear layers in GLM-130B and a counterpart BLOOM-176B in a histogram (Cf. Figure 15). The horizontal axis denotes the weight value, and the vertical axis denotes the number of weights of such value in log scale. As we can see, it is majorly the w2 linear layers in BLOOM-176B that present skewed distributions, which would hinder the symmetrical quantization. On the contrary, GLM-130B’s w2 is well-shaped without many outliers and skewed distribution, and thus paces the way for its INT4 quantization with little performance loss.\n\nB.9 ABLATION ON CONTRIBUTION ATTRIBUTION\n\nWe analyze the contribution attribution of techniques leveraged in GLM-130B. A series of ablation studies have been presented in the paper, and for the convenience of reading, they were originally scattered around the whole passage. Here we summarize them here into the following list for readers’ reference:\n\n• Ablation on ordinary PostLN and DeepNorm: Figure 3. • Ablation on Bidirectional/Unidirectional Attention: Figure 2 (LAMBADA), Table 16 (Condi-\n\ntional NLG), Figure 17 (SuperGLUE).\n\n• Ablation on Embedding Layer Gradient Shrink (EGS): Figure 4. • Ablation on Positional Encodings and FFN: Appendix B.3 Table 8.\n\nAdditionally, we conduct the following study to justify the contribution of the two most influential techniques–GLM Objective and Multi-task Instruction Pre-training (MIP)–used in GLM-130B.\n\nGLM Objective and MIP. Ablating a 100B-scale LLM from scratch can be too expensive. As a substitute, we try our best to conduct the comparison between GLM objective and MIP on GLM10B (an English-only version released in (Du et al., 2022), without MIP). We additionally train a GLM-10B initialized from a middle-stage original checkpoint with MIP (5%) to match the same training tokens of the original self-supervision-only GLM-130B. The MIP, this time, follows the\n\n29\n\nLAMBADAMMLUWiCReCoRDHellaswagWSCBoolQANLI R12040608067.326.351.765.427.363.564.135.072.733.756.166.427.763.571.235.674.834.552.550.727.367.378.340.0ModelGLM (uni)GLM (bi)GLM + MIP (bi)Published as a conference paper at ICLR 2023\n\nexact dataset setting in T0 (Sanh et al., 2022) and the information extraction datasets in GLM-130B to allow the correct evaluation on some types of tasks (e.g., NLI).\n\nFigure 14 shows the ablation results. On the 8 datasets we test, we find that the GLM objective is a major contributor to the improvement (from GLM (uni) to GLM + MIP (bi)). For example, it accounts for 73% improvement in LAMBADA and 90% improvement in MMLU, which are very widely adopted challenging benchmarks for LLMs. As for MIP, on some datasets (e.g., WiC, ReCoRD, Hellaswag), MIP may even harm the performance. While for datasets related to text similarity and coreference (e.g., WSC, BoolQ, ANLI R1), MIP is the main contributor. It is likely because the text similarity and coreference challenges, which people usually construct intentionally to test language models’ ability, are seldom seen in the self-supervised corpus that makes up people’s daily written texts. Thus, MIP training mainly helps to bridge the gap between self-supervised pre-training and these tasks.\n\nB.10 LESSONS LEARNED\n\nLesson 1 (Bidirectional Architecture). The bidirectional-attention GLM is a strong architecture alternative, in addition to GPTs.\n\nLesson 2 (Platform-aware Configuration). Configure LLMs based on the cluster and parallel strategy used to squeeze hardware potential.\n\nLesson 3 (Improved Post-LN). Counter-stereotypically, DeepNorm, a type of Post-LN, is the option to stabilize GLM-130B.\n\nLesson 4 (Training Stability Categorization). Unexpected training instability that LLMs suffer from arouses systematically and numerically.\n\nLesson 5 (Systematical Instability: FP16). Though FP16 induces more instability, it enables training and inference on diverse platforms.\n\nLesson 6 (Numerical Instability: Embedding Gradient Shrink). Shrinking embedding layer’s gradient to its 0.1 can solve most numerical instability problems.\n\nLesson 7 (GLM’s INT4 Quantization Scaling Law). GLM has a unique INT4 weight quantization scaling law unobserved in GPT-style BLOOM.\n\nLesson 8 (Future Direction). To create powerful LLMs, the main focus can be on 1) more and better data, 2) better architectures and pre-training objectives, and 3) more sufficient training.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nFigure 15: Weight value distribution of linear layers in GLM-130B (in orange, attn-dense, attn-qkv, glu-w1, glu-w2) and BLOOM-176B (in blue, attn-dense, attn-qkv, ffn-w1, ffn-w2)’s first 28 transformer layers. Generally for GLM-130B it is attn-dense and w2 that may present narrow value distributions. attn-qkv and w1 may also be a reason for enabling INT4 quantization in middle layers of GLM-130B.\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nC DATASET AND EVALUATION DETAILS\n\nC.1 MULTI-TASK INSTRUCTION PRE-TRAINING (MIP)\n\nFollowing practices in (Raffel et al., 2020; Wei et al., 2022a; Sanh et al., 2022; Aribandi et al., 2022), we include a number of prompted instruction datasets in GLM-130B’s MIP training, which accounts for 5% of the training tokens. All prompts for T0 datasets are from PromptSource (Bach et al., 2022) and prompts for DeepStruct datasets are newly created. Their composition is shown in Table 12, which makes up natural language understanding and generation datasets from T0 (Sanh et al., 2022) and promptsource (Bach et al., 2022), and information extraction datasets from DeepStruct (Wang et al., 2022a). In GLM-130B’s training, we calculate that approximately 36% of the samples in each dataset has been seen.\n\nT0 originally splits datasets for 1) multi-task prompted training and 2) zero-shot task transfer two sections. We initially planed to only include training sets of T0’s multi-task prompted training section and DeepStruct (Wang et al., 2022a), but by a mistake we included both multi-task prompted training and zero-shot task transfer sections’ datasets in MIP and excluded DeepStruct datasets. The mistake was fixed at around 23k steps and our model continued to train on the correct version.\n\nNatural Language Understanding and Generation. We adopt datasets and corresponding prompts from promptsource (Bach et al., 2022). For all prompted samples in each dataset, we set a truncation of maximal 10,0000 samples per dataset and combine them together as the MIP dataset. Details of the prompted samples and datasets are provided in promptsource’s GitHub repository8.\n\nInformation Extraction. Based on the datasets from DeepStruct (Wang et al., 2022a), a multitask language model pre-training approach for information extraction tasks, we create instructions and prompts for part of its datasets (as is shown in Table 12). We reformulate information extraction tasks into instruction tuning formats to allow zero-shot generalization to new extraction schema. For all prompted samples in each dataset, we set a truncation of maximal 20,0000 samples per dataset as there are fewer information extraction datasets than common language understanding and generation ones. For KELM (Agarwal et al., 2021) and PropBank (Kingsbury & Palmer) datasets, since their original size is gigantic, we sample 50,0000 samples for each of them from their prompted samples.\n\nC.2 DATA AND PROMPTS IN MIP FOR DEEPSTRUCT\n\nPrompts and instructions for all datasets in DeepStruct (Wang et al., 2022a) are newly created by authors manually. The introduction, task description, and full prompts for each dataset are attached in the following sections. To allow template infilling, all prompts are written into Jinja9 templates. When a dataset sample is provided in our format, Joinja engine will render it into a prompted sample with instruction.\n\nA more systematic evaluation on GLM-130B’s information extraction ability is left for a future work, as the concentration in this work is on the training and designing details of an LLM.\n\nC.2.1 DIALOGUE STATE TRACKING\n\nWe adopt Multiwoz 2.1 (Eric et al., 2020) dialogue state tracking dataset. The dataset is reformulated into two tasks, each with one prompt correspondingly:\n\n• Dialogue state tracking: which asks the model to extract information from dialogues given a list\n\nof certain slots, e.g., taxi_arrival_time and destination.\n\n• Slot filling: which model should fill in one provided slot and identify situations without answer.\n\n8https://github.com/bigscience-workshop/promptsource 9https://github.com/pallets/jinja\n\n32\n\nPublished as a conference paper at ICLR 2023\n\n(Dialogue State Tracking, Prompt 0)\n\nRead the dialogues between \"[User]\" and \"[Agent]\",\n\n{{text}}\n\nidentify and extract the information related to the following categories\n\n(from top to down):\n\n- {{allowed_relations | join(\"\\n- \")}}\n\nin the form of \"( [User] ; Y ; Z )\": ||| {{format_triple(relations, allowed_relations) | join(\" \")}}\n\n(Slot Filling, Prompt 0)\n\nGiven the following dialogue:\n\n{{text}}\n\nplease answer the question: has \"[User]\" mentioned \"{{allowed_relations[ relation_idx].split(': ') | join(\"'s \")}}\" ? If yes, please write down the answer from the dialogue; if not, please answer \"not given\".\n\nAnswer: ||| {% if filter_relation(relations, allowed_relations[ relation_idx]).__len__() > 0 %}{{filter_relation(relations, allowed_relations[relation_idx])[0]['tail']}}{% else %}not given{% endif\n\n%}\n\nC.2.2 EVENT EXTRACTION\n\nWe adopt ACE05 (Walker & Consortium, 2005) event extraction datasets following the setting in (Wadden et al., 2019). The dataset is reformulated into two tasks with three prompts as follows:\n\n• Event Argument Extraction: given a trigger in text and a list of its argument roles, the model is\n\nasked to extract the arguments from the provided text.\n\n• Argument Identification: given a trigger and a certain argument role, the model is asked to extract the argument if it exists in the provided text; otherwise, the model should generate nothing.\n\n(Event Argument Extraction, Prompt 0)\n\nFor the task of \"Event Extraction\", given a trigger one should extract its related arguments conditioned on a list of potential roles.\n\nGiven the following list of roles:\n\n- {{shuffle(allowed_arguments[trigger['event_type']].values()) | join(\"\\ n- \")}}\n\nextract related arguments of the trigger \"{{trigger['text']}} ({{ allowed_triggers[trigger['event_type']]}})\" in the following sentence:\n\n{{text}}\n\nExtractions: ||| {{format_triple(relations, \"\") | join(\" \")}}\n\n33\n\nPublished as a conference paper at ICLR 2023\n\n(Event Argument Extraction, Prompt 1)\n\nTEST\n\n1. (Event Extraction) {{text}}\n\nPlease write down ALL event arguments related to the trigger \"{{trigger ['text']}} ({{allowed_triggers[trigger['event_type']]}})\" marked with \"[\n\n]\", given the following categories:\n\n- {{shuffle(allowed_arguments[trigger['event_type']].values()) | join(\"\\ n- \")}}\n\nAnswer: ||| {{format_triple(relations, \"\") | join(\" \")}}\n\n(Argument Identification, Prompt 0)\n\nLet extract event related arguments!\n\nIn the following passage, an argument with the type \"{{query_arg}}\" is related to the event trigger \"{{trigger['text']}} ({{allowed_triggers[ trigger['event_type']]}})\":\n\n{{text}}\n\nThe argument should be (copy from the context if you find it; if not, do\n\nnot generate): ||| {{filter_type(relations, query_arg) | join(\" \")}}\n\nC.2.3\n\nJOINT ENTITY AND RELATION EXTRACTION\n\nJoint entity and relation extraction aims to recognize named entities in a piece of text and judge the relationships between them. It is closely related to knowledge acquisition, where the ultimate target is to structuring the unstructured web contents into knowledge triples (e.g., (London, capital_of, Britain)). The task can be formulated into either a pipeline framework (a combination of named entity recognition and relation extraction), or end-to-end training.\n\nIn this work, we adopt three classical joint entity and relation extraction datasets: CoNLL04 (Roth & Yih, 2004), NYT (Riedel et al., 2010), and ACE2005 (Walker & Consortium, 2005). In GLM-130B, we follow (Wang et al., 2022a) to formulate such challenges into sequence-to-sequence generation, where our inputs are raw texts and outputs are triples. We only conduct relation-related tasks for these datasets here, and leave the entity-related ones to the named entity recognition section.\n\n• Relation Extraction: here we extract knowledge triples consisting of “head entity”, “relation”, and “tail entity”, given a list of relation candidates. For example, given the input “In Kunming the 800-some faculty and student established the National Southwestern Associated University.”, the model output could be (National Southwestern Associated University, location of formation, Kunming).\n\n• Conditional Relation Extraction: given a single relation candidate, judge if the input text con-\n\ntains the relation. If so, extraction all related triples; if not, do not generate.\n\n• Knowledge Slot Filling: assign a certain entity from text, and ask the model to extract all triples\n\nthat takes the entity as the head.\n\n• Relation Classification: given two entities from texts, ask the model to judge the relation between\n\nthem based on a list of candidate relations.\n\n34\n\nPublished as a conference paper at ICLR 2023\n\n(Relation Extraction, Prompt 0)\n\nCan you figure out all triples regarding the relations of \"{{shuffle( allowed_relations) | join('\", \"')}}\" from the sentence? List them in the\n\nshape of \"( X ; Y ; Z )\":\n\n{{text}} => ||| {{format_triple(relations, allowed_relations) | join(\" \")}}\n\n(Conditional Relation Extraction, Prompt 0)\n\nConditioned on the relation \"{{allowed_relations[relation_idx]}}\", what knowledge triples can be extracted from:\n\n{{text}}\n\nPlease write them down here: ||| {{format_triple(relations, [ allowed_relations[relation_idx]]) | join(\" \")}}\n\n(Knowledge Slot Filling, Prompt 0)\n\n{% if entity_types.__len__() > 0 %} In the sentence\n\n{{text}}\n\nthe X = \"{{entities[entity_idx]}}\" is an entity of the type \"{{ entity_types[entity_idx]}}\". Extract all possible triples contains \"{{ entities[entity_idx]}}\" in the form of ( X ; Y ; Z ), given the following candidate properties Y:\n\n{% for r in allowed_relations %}- {{r}} {% endfor %} Answer: ||| {% for r in relations %}{% if r['head'][0] == entities[ entity_idx] %}{{format_triple([r], allowed_relations) | join(\" \")}}{% endif %}{% endfor %} {% endif %}\n\n(Relation Classification, Prompt 0)\n\nQUIZ\n\n1. Given the candidate relations:\n\n- {{shuffle(allowed_relations) | join(\"\\n- \")}}\n\nwhat is the relation between \"{{relations[triple_idx]['head'][0]}}\" and \"{{relations[triple_idx]['tail'][0]}}\" in the following sentence?\n\n{{text}}\n\nAnswer: ||| {{relations[triple_idx]['relation']}}\n\nNevertheless, existing joint entity and relation extraction datasets have very limited relation schema. For example, CoNLL04 only contains five different relations; the most diverse NYT dataset contains 24 Freebase predicates. To allow the model to capture a diverse range of potential verbalized predicates, we extend the task with automatically generated knowledge-text aligned data from KELM (Agarwal et al., 2021). We do not include other distantly supervised dataset (e.g., T-Rex (Elsahar et al., 2018)) since they can be extremely noisy.\n\nFor KELM data, since it is based on the full Wikidata schema (which contains too many relations to be enumerated), we create two KELM-specific prompts for the task of Relation Extraction and Knowledge Slot Filling:\n\n35\n\nPublished as a conference paper at ICLR 2023\n\n(Relation Extraction, Prompt 1, KELM ONLY)\n\n{# kelm #} Can you figure out all knowledge triples regarding whole Wikidata properties from the sentence? List them in the shape of \"( X ; Y ; Z )\":\n\n{{text}} => ||| {{format_triple(relations, \"\") | join(\" \")}}\n\n(Knowledge Slot Filling, Prompt 1, KELM ONLY)\n\n{# kelm #} Given the entity \"{{entities[entity_idx]}}\" marked with \"[\" and \"]\" in the context:\n\n{{text}}\n\nplease list all triples related to it (do not generate if there is no answer): ||| {% for r in relations %}{% if r['head'][0] == entities[ entity_idx] %}{{format_triple([r], \"\") | join(\" \")}}{% endif %}{% endfor\n\n%}\n\nC.2.4 NAMED ENTITY RECOGNITION\n\nNamed entity recognition is a task which targets identifying named entities from raw text corpus and assign them with proper entity types. For example, in the sentence “In 1916 GM was reincorporated in Detroit as \"General Motors Corporation\".”, General Motors Corporation could be of entity type organization. We design two different types of tasks based on named entity recognition datasets CoNLL03 (Sang & Meulder, 2003), OntoNotes 5.0 (Pradhan et al., 2013), and GENIA (Ohta et al., 2002). We also include named entity recognition sub-tasks from joint entity and relation datasets.\n\n• Named Entity Recognition: given a certain list of possible entity types (e.g., location,\n\nperson, organization), extract all related entities from the provided text content.\n\n• Entity Typing: entity typing is one of the important derivative tasks from named entity recognition. It aims to classify the correct type of an entity mention (without entity types), and is often appended to the entity mention extraction as post-processing.\n\n(Named Entity Recognition, Prompt 0)\n\nGiven the following list of entity types:\n\nZ = {{shuffle(allowed_types) | join(\", \")}}\n\nplease extract all mentioned entities from left to right in the sentence , in the form of \"( X ; instance of ; Z )\".\n\n{{text}} => ||| {% for entity, type in zip(entities, entity_types) %}( {{entity}} ; instance of ; {{type}} ) {% endfor %}\n\n(Entity Typing, Prompt 0)\n\nExtract all entity mentioned in the sentence with entity type \"{{ allowed_types[type_idx]}}\" in the form of \"( X ; instance of ; {{ allowed_types[type_idx]}} )\"\n\n{{text}} => ||| {% for entity, type in zip(entities, entity_types) %}{% if type == allowed_types[type_idx] %}( {{entity}} ; instance of ; {{type }} ) {% endif %}{% endfor %}\n\n36\n\nPublished as a conference paper at ICLR 2023\n\n(Entity Typing, Prompt 1)\n\nList all \"{{allowed_types[type_idx]}}\" entities appeared in the following passage, joined by \" | \":\n\n{{text}} => ||| {{filter_type(zip(entities, entity_types), allowed_types [type_idx]) | join(\" | \")}}\n\n(Entity Typing, Prompt 2)\n\n{% if entity_types.__len__() > 0 %} Based on the list of potential entity types and ignore their order:\n\n- {{shuffle(allowed_types) | join(\"\\n- \")}}\n\nthe entity \"{{entities[entity_idx]}}\" marked with \"[\" and \"]\" in the following sentence:\n\n{{text}}\n\nbelongs to ||| {{entity_types[entity_idx]}} {% endif %}\n\nC.2.5 RELATION CLASSIFICATION\n\nRelation classification is a fundamental task in information extraction, which identifies the relationships from a list of candidates between two given entities. The problem is a long standing one as it suffers from outrageous cost of data labeling, since manual labeling on knowledge-intensive tasks requires educated annotators that charges high. A de facto data creation method in relation extraction relies on distant supervision, which aligns existing knowledge triples in knowledge bases to text contents automatically, and assume that such alignments are correct in certain conditions. Here we only include TacRED (Zhang et al., 2017) dataset and create several different tasks based on it.\n\n• Relation Classification: the most traditional task formulation. Given two entities from text and classify their relation from a list of candidates. The form can be either answering the relation directly or in the form of a triple (similar to relation extraction).\n\n• Knowledge Slot Filling: change the task into given head entity and relation, to identify whether\n\nthe tail entity exists in the input text. If not, generate nothing.\n\n• Yes or No Question:\n\nturn the problem into a task similar to natural language inference. For example, given the sentence “The series focuses on the life of Carnie Wilson, daughter of Brian Wilson, founder of the Beach Boys.”, the model will be asked to judge the correctness of a triple such as Carnie Wilson, father, Brian Wilson by answering “yes” or “no”.\n\n(Relation Classification, Prompt 0)\n\n{% if entity_types.__len__() > 0 %} Given the following categories of relations:\n\n- {{shuffle(allowed_relations.values()) | join(\"\\n- \")}}\n\npredict the relation between \"{{relations[0]['head']}}\" and \"{{relations [0]['tail']}}\" in the following sentence:\n\n{{text}}\n\nThe relation should be : ||| {{allowed_relations[relations[0]['relation ']]}} {% endif %}\n\n37\n\nPublished as a conference paper at ICLR 2023\n\n(Relation Classification, Prompt 1)\n\n1. (Relation Extraction) Answer the relation between entities in the form of \"( X ; Y ; Z )\":\n\n{{text}}\n\nThe relation between \"{{relations[0]['head']}}\" and \"{{relations[0][' tail']}}\" is: ||| ( {{relations[0]['head']}} ; {{allowed_relations[ relations[0]['relation']]}} ; {{relations[0]['tail']}} )\n\n(Knowledge Slot Filling, Prompt 0)\n\nBased on the sentence provided below, infer the missing argument asked by the question:\n\n{{text}}\n\nQuestion: What/Who/Where is \"{{relations[0]['head']}}\" {{ allowed_relations[relations[0]['relation']]}} ?\n\nAnswer: ||| {{relations[0]['tail']}}\n\nC.2.6 SEMANTIC ROLE LABELING\n\nSemantic role labeling is a long-standing information task that wants to identify the semantic arguments related to a given predicate in a sentence. For example, in the sentence “Grant was employed at IBM for 21 years where she held several executive positions.” and the predicate “employed” in it, semantic role labeling identifies the Grant as the subject and IBM as the second object.\n\nWe create two different tasks based on semantic role labelling datasets CoNLL05 (Carreras & Màrquez, 2005), CoNLL12 (Pradhan et al., 2013), and PropBank (Kingsbury & Palmer).\n\n• Semantic Role Labeling: the traditional task form, where a verb (i.e., predicate) is annotated in\n\ntext and the model is asked to generate related semantic roles.\n\n• Semantic Role Filling: given a verb and and a potential semantic role, the model is asked to judge\n\nwhether the role exists in the sentence and generate it.\n\n• Predicate Recognition: given a segment of a sentence and its corresponding semantic role, iden-\n\ntify which verb it is related to.\n\n(Semantic Role Labeling, Prompt 0)\n\nProvided with the target verb \"{{verb}}\" marked with \"[\" and \"]\" in the following sentence, find out its \"{{allowed_types[type_idx]}}\":\n\n{{text}} => ||| {% for entity, type in zip(entities, entity_types) %}{% if type == allowed_types[type_idx] %}{{entity}}{% endif %}{% endfor %}\n\n(Semantic Role Filling, Prompt 0)\n\nGiven the following list of argument types:\n\nZ = {{allowed_types | join(\", \")}}\n\nfind out all arguments related to verb \"{{verb}}\" mentioned in the following sentence from left to right, in the form of \"( X ; instance of\n\n; Z )\".\n\n{{text}} => ||| {% for entity, type in zip(entities, entity_types) %}( {{entity}} ; argument type ; {{type}} ) {% endfor %}\n\n38\n\nPublished as a conference paper at ICLR 2023\n\n(Predicate Recognition, Prompt 0)\n\nFINAL EXAM\n\n1. Based on the fact that \"{{entities[entity_idx]}}\" is a \"{{ entity_types[entity_idx]}}\", which verb in the following sentence should\n\nit related to?\n\n{{text}}\n\nAnswer: ||| {{verb}}\n\nC.3 RESULT SOURCES FOR GPT-3, BLOOM-176B, AND OPT-175B\n\nHere we describe the result sources for GPT-3, BLOOM-176B, and OPT-175B. Other LLMs we may compare are mostly completely closed-sourced; thus, their results are all taken from existing preprints, publications, or the results stored in BIG-bench repository10.\n\nFor GPT-3, while most of its results in this paper are taken from existing literature if not specified, the rest were acquired via our own requesting OpenAI Danvici API are explicitly mentioned. For BLOOM-176B and OPT-175B, if without specific annotation, their results are:\n\n• Taken from the OPT paper (Zhang et al., 2022). • Taken from the EAI-Eval BigScience Arch&Scale - Google Sheet11. • Taken from BigScience evaluation results repository in Huggingface Datasets12.\n\nSpecifically, we cannot evaluate OPT-175B by ourselves as we are still not officially granted the checkpoint, though we have sent several applications in the past few months.\n\nC.4 PILE TEST-SET EVALUATION\n\nPile evalution (Gao et al., 2020) is a comprehensive language modeling benchmark which originally includes 22 different text datasets from diverse domains. We report our results over a part of 18 datasets with previously reported baseline results (Lieber et al., 2021). Different from traditional language modeling benchmarks, Pile evaluation report the BPB (bits-per-byte) perplexity to avoid the mismatch comparison between models with different vocabularies. Because in general, language models with a larger vocabulary will be favored in perplexity comparison if not restricted. In the evaluation, we strictly follow the setting in (Gao et al., 2020), leveraging [gMASK] and a context-length of 1,024 with bidirectional attention, and the rest 1024 tokens to calculate BPB in an autoregressive manner. The weighted average BPB are calculated based on each shared dataset’s ratio in Pile training-set (Gao et al., 2020).\n\nTable 13: GLM-130B and its similar-sized LLMs’ BPB results on Pile test-set.\n\nJurassic-1 GPT-3 GLM-130B\n\ndm_mathematics ubuntu_irc opensubtitles hackernews books33 pile_cc philpapers gutenberg_pg_19 arxiv stackexchange nih_exporter pubmed_abstracts uspto_backgrounds pubmed_central freelaw github enron_emails youtube_subtitles\n\nWeighted Avg.\n\n1.040 0.857 0.879 0.869 0.835 0.669 0.741 0.890 0.680 0.655 0.590 0.587 0.537 0.579 0.514 0.358 0.621 0.825\n\n0.650\n\n1.370 0.946 0.932 0.975 0.802 0.698 0.723 1.160 0.838 0.773 0.612 0.625 0.566 0.690 0.612 0.645 0.958 0.815\n\n0.742\n\n0.786 0.977 0.889 0.873 0.803 0.771 0.766 0.821 0.570 0.611 0.614 0.610 0.537 0.510 0.499 0.329 0.604 0.746\n\n0.634\n\nThe detailed metrics on Pile test-set are reported in Table 13. We observe that compared to GPT3, GLM-130B has a noticeable weaker performance on phil_papers and pile_cc, which is likely because of GLM-130B’s bilingual natural and lack of more diverse and high-quality private collected corpora.\n\n10https://github.com/google/BIG-bench 11https://docs.google.com/spreadsheets/d/1CI8Q9RCblLRzUOPJ6ViqBmo284-8oj\n\nluQ-CmaEuhuv0\n\n12https://huggingface.co/datasets/bigscience/evaluation-results/tree/ma\n\nin/bloom/bloomzeval/transformers/evaluation_val\n\n39\n\nPublished as a conference paper at ICLR 2023\n\nC.5 BIG-BENCH-LITE EVALUATION\n\nRecent works (Wei et al., 2022c; Wang et al., 2022c) reveal that LLMs are capable to do reasoning beyond conventional language tasks. As a response, BIG-bench (Srivastava et al., 2022) is recently set up by crowdsourcing new types of tasks from global researchers to test LLMs unexplored abilities. For economical consideration, we evaluate GLM-130B on an official subset of original 150task BIG-bench, the BIG-bench-lite with 24 tasks. These tasks can be categorized into two types: one is based on multiple-choice question answering with answer options, and another is direct generation without options. For the first category, we assess the probability of each option’s full content and pick the largest one as the answer; for the second one, we generate the answer using greedy decoding. All evaluations done in BIG-bench are based on [MASK], since answers here are usually short pieces of texts. All results on 24 BIG-bench-lite (Srivastava et al., 2022) datasets of three LLMs are shown in Table 14 and Figure 16. We just adopt the original prompts from BIG-bench and use the official implementation to generate priming examples for few-shot evaluation and to calculate the final scores.\n\nFigure 16: A full scope of BIG-benchlite (24 tasks) evaluation.\n\nC.6 MMLU EVALUATION\n\nAll results on 57 MMLU (Hendrycks et al., 2021) datasets of GLM-130B and BLOOM 176B are shown in Table 15. In Section 5.2, we report weighted average accuracy (i.e., accuracy average per sample, rather than by discipline) of GLM-130B, GPT-3 175B, and BLOOM 176B.\n\nBelow is a prompted example with 1-shot priming. We predict the probability on [’A’, ’B’, ’C’, ’D’] at the next token, and take the one with the maximal probability as the answer.\n\n(MMLU 1-shot Example)\n\nThe following are multiple choice questions about philosophy.\n\nAccording to d'Holbach, people always act according to _____. (A) free choices (B) dictates of the soul (C) necessary natural laws (D)\n\nundetermined will\n\nAnswer: (C) necessary natural laws\n\nEpicurus holds that philosophy is: (A) not suitable for the young. (B) not suitable for the old. (C) important, but unpleasant. (D) none of the above. Answer: (\n\nC.7 CHINESE LANGUAGE UNDERSTANDING EVALUATION\n\nHere we elaborate the prompts we use for CLUE (Xu et al., 2020) and FewCLUE (Xu et al., 2021) evaluation. On Chinese datasets, prompting meets some challenges as Chinese texts are organized by single characters rather than words, leading to unequal length of verbalizers in many cases. Albeit dataset-specific calibration (Wang et al., 2021; Wu et al., 2021) can help to mitigate the issue, the too specified technique can be complicated in implementation. Our evaluation in this paper adopts a more easy to solve method leveraging GLM-130B’s unique features. As GLM-130B is a bilingual LLM with English MIP, we adopt English prompts and verbalizers from similar tasks in (Bach et al., 2022) for Chinese dataset evaluation and find such strategies to be quite effective. In terms of evaluation metrics, except for DRCD and CMRC2018 two question answering datasets which reports EM, other datasets report accuracy.\n\n40\n\n10810910101011Effective Parameter Count50510152025303540Aggregate Normalized PerformanceGLM-130B 0-shotGLM-130B 1-shotGLM-130B 3-shotGPT-3 0-shotGPT-3 1-shotGPT-3 3-shotPaLM 0-shotPaLM 1-shotPublished as a conference paper at ICLR 2023\n\nC.8 NATURAL LANGUAGE GENERATION\n\nNatural language generation, or conditional natural language generation here, refers to tasks that require generating text based on the given information, such as tables and documents. We evaluate GLM-130B on data-to-text and summarization tasks. The datasets include WebNLG 2020 (Castro Ferreira et al., 2020), Clean E2E NLG (Dušek et al., 2019) and WikiLingua (Scialom et al., 2020) from GEM generation benchmark (Gehrmann et al., 2021). We select full WebNLG 2020 and the Clean E2E NLG in the test set and randomly select 5000 test examples from WikiLingua following the practice in (Chowdhery et al., 2022). Following the settings in PaLM, the prompt used for the Summarization tasks is “Summarize the following article:” and the prompt used for the Data-to-Text tasks is “Verbalize:”. An exception is E2E, where we process the data using the prompt “generate-gramatically-correct-text from” provided in promptsource for GLM-130B and GPT-3 175B (Davinci). All evaluations are one-shot, and the demonstration samples are randomly sampled from the training set. We report the F-measure of ROUGE-2, ROUGE-L (Lin, 2004) and BLEURT-20 (Pu et al., 2021). We compare our model with LaMDA, GPT-3 175B (Davinci), and PaLM, where the results of LaMDA and PaLM are reported by (Chowdhery et al., 2022), and we evaluate GPT-3 175B (Davinci) through OpenAI API.13\n\nIt shows that GLM-130B has better performances than Our results are presented in Table 16. LaMDA and GPT-3 (Davinci) on all tasks. In the Data-to-text task, GLM-130B performs slightly worse than PaLM-540B, while in the summary task, GLM-130B has even higher ROUGE results. We also ablate GLM-130B to unidirectional to demonstrate the advantage of bidirectional attention. Unidirectional GLM-130B underperforms GPT-3 175B in all three datasets, but when it shifts to bidirectional attention, there is an instant boost, making GLM-130B even comparable to PaLM540B in a few cases. It indicates that bidirectional attention over the provided context (i.e., prefix) can also be beneficial for text generation missions.\n\nTable 16: 1-shot GEM English natural language generation tasks (WebNLG, E2E, and WikiLingua). We compare two versions of GLM-130B (uni: unidirectional attention, bi: bidirectional attention), showing that bidirectional attention can also improve conditional generation’s performance.\n\nTask\n\nDataset\n\nMetric\n\nLaMDA 137B\n\nGPT-3 175B (Davinci)\n\nData to Text\n\nWebNLG\n\nE2E\n\nSummary WikiLingua\n\nROUGE-2 ROUGE-L BLEURT-20\n\nROUGE-2 ROUGE-L BLEURT-20\n\nROUGE-2 ROUGE-L BLEURT-20\n\n30.5 -\n-\n\n29.2 -\n-\n\n5.4 -\n-\n\n29.9 41.2 59.0\n\n30.3 39.2 64.5\n\n7.2 18.9 41.2\n\nGLM-130B\n\nPaLM-540B\n\nuni\n\n25.3 36.7 53.2\n\n30.9 40.0 65.0\n\n5.8 16.4 39.4\n\nbi\n\n38.5 49.3 67.7\n\n33.9 42.6 68.1\n\n10.4 23.4 45.0\n\n44.4 53.8 73.9\n\n35.2 43.9 69.7\n\n9.9 20.6 47.7\n\n(E2E Example, without demonstration sample)\n\nAleksandr_Prudnikov , height , 185.0 (centimetres). FC_Spartak_Moscow , ground , Otkrytiye_Arena. Aleksandr_Prudnikov , club , FC_Spartak_Moscow. Verbalize:\n\nGroundtruth: 185 centimetre tall Aleksandr Prudnikov played for the Otkrytiye Arena based FC Spartak, Moscow.\n\nGPT-3 175B (Davinci): Aleksandr Prudnikov is a midfielder for FC Spartak Moscow, a football (soccer) club based in Moscow, Russia.\n\nGLM-130B: Aleksandr Prudnikov is 185.0 cm tall and plays for FC Spartak Moscow.\n\n13We use ROUGE implementation at https://github.com/google-research/google-research/tree/master/rouge and BLEURT-20 implementation at https://github.com/google-research/google-research/tree/master/rouge, whose checkpoint is available at https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip\n\n41\n\nPublished as a conference paper at ICLR 2023\n\n(E2E Example, without demonstration sample)\n\nCombine all of the following data into a concise and grammatically correct text: name : Blue Spice eatType : coffee shop area : riverside\n\nGroundtruth: At the riverside, there is a coffee shop called The Blue Spice.\n\nGPT-3 175B (Davinci): Blue Spice is a riverside coffee shop which is located on the corner of River Street and Riverbank Street.\n\nGLM-130B: There’s a coffee shop that serves coffee in the riverside area, Blue Spice. (WikiLingua Example, without demonstration sample)\n\nThe majority of your customers will search for you online, so it's essential to have a user-friendly website. At the very least, your website should include information about your business and your history in the moving industry, details about the quoting process, contact information, and a description of the services you offer. If possible, allow customers to schedule quotes online, view your availability, or read testimonials from other customers. One of the easiest ways to start your business is by helping people you already know with their moves. You can be on the lookout for any announcements related to moving that your friends make on social media. Once you have provided good service to friends, they are likely to recommend you to others. In order to spread the word about your business, have some professional looking promotional materials printed and distribute them around your community.\n\nYou can distribute business cards at public events, tuck them into\n\nlocal bulletin boards, or even print them in directories, yearbooks, and other local print media. Flyers can be mailed, posted in public places, or distributed to businesses that might be able to refer customers to like you, such as furniture stores. Make sure you have a professional, recognizable logo that is consistent across all of your marketing materials. Another way to get your business's name out there is to make yourself visible. Whether it's by working with partners at local events, volunteering, or using your vehicle for an ad campaign, visibility is key for driving business. Build relationships with influential people in your community. Realtors are a great source of referrals to movers, as are the owners of local furniture stores or the office staff at a large apartment complex. You can use directory sites like Craigslist to advertise your services to people in your local community for free. Social media is also a great way to spread the word about your business.\n\nThere are many options for advertising, depending on your budget and your target market. Consider options like PPC advertising, television and radio commercials, newspaper ads, direct mail flyers, or memberships with referral services. The best thing you can do to grow your business is to provide excellent service to your customers. Be sure to always be on time, be friendly, be respectful of your customers' belongings, and offer accurate price quotes. Be sure to ask your happy customers to post reviews about you online. New customers will see these reviews and will have more faith in you as a reputable company if they see that others\n\nhave had a good experience. You will inevitably end up having to deal with an unhappy customer at some point, but do your best to resolve the problem to the customer's satisfaction. The last thing you want is negative reviews about your business circulating the internet! Summarize\n\nthe article:\n\nGroundtruth: Create a website. Start with friends. Distribute business cards and flyers. Network for promotion. Seek out free advertising opportunities. Consider paid advertising. Gain a good reputation.\n\nGPT-3 175B (Davinci): Market yourself online. Look for opportunities to market your services offline.\n\nGLM-130B: Have a user-friendly website. Promote your business through social media. Build relationships with influential people in your community. Use directory sites like Craigslist to advertise your services.\n\n42\n\nPublished as a conference paper at ICLR 2023\n\nTable 17: Winograd-style tasks evaluation (Winogender and Winograd273). All scores are accuracy. K refers to number of shots. ∗PaLM 540B did not report the exact 0-shot Winogender result, so we have to estimate a value from its plotted diagram.\n\nK\n\n0 1\n\n0\n\nGPT-3 (Davinci)\n\nOPT 175B\n\nBLOOM 176B\n\n64.2 62.6\n\n88.3\n\n54.8 -\n\n52.9\n\n49.1 53.1\n\n49.1\n\nPaLM 540B\n\n75.0∗ 79.4\n\n90.1\n\nChinchilla\n\n78.3 -\n\n-\n\nGopher 280B\n\n71.4 -\n\n-\n\nGLM-130B\n\n79.7 80.7\n\n84.3\n\nWinogender\n\nWinograd273\n\nTable 18: Closed-book question answering (Natural Questions, StrategyQA).\n\nGPT-3 (Davinci)\n\nBLOOM 176B\n\nPaLM 540B\n\nChinchilla\n\nNatural Questions (EM)\n\nStrategyQA (Acc)\n\n14.6\n\n52.3\n\n13.1\n\n49.8\n\n21.2\n\n64.0\n\n16.6\n\n-\n\nGopher 280B\n\n10.1\n\n-\n\nGLM-130B\n\n11.7\n\n60.6\n\nTable 19: Commonsense reasoning (Commonsense QA, MC-TACO). K refers to number of shots.\n\nK GPT-3 (Davinci) OPT 175B BLOOM 176B GLM-130B\n\nCommonsense QA (Acc)\n\nMC-TACO (EM)\n\n0 1\n\n0\n\n57.2 61.2\n\n-\n\n- -\n\n12.4\n\n42.8 -\n\n13.1\n\n61.6 62.2\n\n13.6\n\nC.9 WINOGRAD-STYLE TASKS\n\nWe include the evaluation on Winograd-style tasks, which derives from the classical Winograd Schemas Challenge (Levesque et al., 2012) that aims to test coreference resolution in an ambiguous context for the machine to understand. Since in MIP, we have included the Winogrande (Sakaguchi et al., 2021) and SuperGLUE WSC (Wang et al., 2019), here we test on Winogender (Rudinger et al., 2018) and Winograd273 (Levesque et al., 2012). For Winogender, GPT-3’s results are acquired from OpenAI API, and BLOOM’s 1-shot result is evaluated by ourselves. For Winograd273, since existing works (Brown et al., 2020; Chowdhery et al., 2022) show that 1-shot learning brings almost no improvement, we only test the zero-shot result. Another thing to notice is that, despite GPT-style models (e.g., GPT-3, PaLM) adopting the “partial evaluation” described in (Radford et al., 2019), we find the prompt “<sentence> The \"<pronoun>\" refers to [MASK]” is better for GLM-130B and adopt it in the evaluation.\n\nThe results are presented in Table 17. GLM-130B performs the best across all evaluated LLM on Winogender, and marginally poorer than GPT-3 and PaLM on Winograd273.\n\nC.10 CLOSED-BOOK QUESTION ANSWERING\n\nClosed-book question answering (CBQA) (Roberts et al., 2020) is a widely adopted task to evaluate language models’ memorization of factual knowledge, on contrary to the traditional “open-book” evaluation. As we have included TriviaQA (Joshi et al., 2017) and WebQuestions (Berant et al., 2013) in the MIP training, here we choose Natural Questions (Kwiatkowski et al., 2019) and StrategyQA (Geva et al., 2021) as the evaluation datasets for CBQA.\n\nThe results are presented in Table 18. GLM-130B performs relatively poorer on Natural Questions and performs well on StrategyQA. GLM-130B’s underperformance on Natural Questions, we speculate, potentially derives from the insufficiency fitting on English corpora, as it roughly only viewed\n\n43\n\nPublished as a conference paper at ICLR 2023\n\n200B English tokens and thus does not memorize the detailed knowledge very well. Since CBQA seems to be a task that especially stresses memorization, as is indicated by Chinchilla (Hoffmann et al., 2022)’s a strong performance, we think with sufficient training later, GLM-130B can perform better.\n\nC.11 COMMONSENSE REASONING\n\nHere we evaluate GLM-130B and some other LLMs on commonsense reasoning abilities. As we have included PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), and OpenbookQA (Mihaylov et al., 2018) in the MIP training, we select another two widely adopted commonsense reasoning datasets in our evaluation: Commonsense QA (Talmor et al., 2019) and Multiple-choice Temporal Commonsense (MC-TACO, Zhou et al. (2019)). For Commonsense QA, we test the GPT-3 via OpenAI Davinci API, BLOOM-176B via its Huggingface Implementation, and GLM-130B using the prompt “answer_given_question_without_options” from promptsource (Bach et al., 2022). For StrategyQA, we follow the EM computation method provided in (Zhou et al., 2019).\n\nThe results are shown in Table 19. As we can see, GLM-130B performs the best on both Commonsense QA and MC-TACO across evaluated LLMs, demonstrating that GLM-130B has a good grasp of commonsense knowledge. OPT’s results are not included due to the reason described in Appendix C.3.\n\nC.12 FIXED LABEL DATASETS: A CASE STUDY IN NATURAL LANGUAGE INFERENCE\n\nAs is discussed in Section 5, we adopt a rather strict criterion for selecting datasets for zero/few-shot learning in GLM-130B’s evaluation due to the use of MIP. Nevertheless, the criterion significantly reduces the dataset we could currently evaluate, and especially some readers have doubted whether the restriction of not evaluating on MIP-seen fixed-label datasets is necessary (e.g., natural language inference (NLI)), and suggest that we may report them in an independent section to avoid confusion.\n\nFrankly speaking, in such a setting GLM-130B’s zero/few-shot learning could be quite advantageous. Below, we take NLI as a typical example to show GLM-130B’s outperformance in the scenarios. We include 6 widely-used NLI datasets–which are not incorporated in GLM-130B’s MIP training, as the benchmarks. The results are presented in Table 20, which shows that GLM-130B’s “zero-shot” performance could be much better due to the seen task type.\n\nTable 20: “Zero-shot” results of GLM-130B on 6 typical natural language inference (NLI) datasets. ∗DISCLAIMER: Despite the datasets are never seen, some other NLI datasets have been included in GLM-130B’s MIP, making it different from the existing standard zero-shot setting.\n\nBLOOM 176B OPT 175B GLM-130B∗\n\nqnli (valid, median of 5 prompts) mnli (valid, median of 15 prompts) mnli_mismatched (valid, median of 15 prompts) wnli (valid, median of 5 prompts) glue/cola (valid, median of 5 prompts) glue/mrpc (valid, median of 5 prompts)\n\n50.9 35.5 35.5 57.7 39.0 31.6\n\n55.4 36.0 36.0 53.5 44.4 44.6\n\n86.7 85.7 84.6 67.6 57.6 87.3\n\nC.13 SUPERGLUE\n\nWe also report our evaluation of GLM-130B on the SuperGLUE (Wang et al., 2019) benchmark, which consists 8 different natural language understanding challenges. Noted that these results are neither zero/few-shot nor fine-tuned results, because 7 out of 8 tasks’ training sets have been included in GLM-130B’s MIP training (except for ReCoRD) together with other 67 multi-task datasets; however, GLM-130B is also not individually fine-tuned on any of them. Therefore, these results are not for relative comparison for any other models’, but only for readers’ reference on GLM-130B’s absolute ability.\n\n44\n\nPublished as a conference paper at ICLR 2023\n\nFigure 17: GLM-130B (uni and bi)’s untuned results on SuperGLUE development set, using promptsource (Bach et al., 2022) prompts and task formulation. DISCLAIMER: Noted that some of the SuperGLUE training sets have been included in the MIP training. We report the results here only for readers’ reference.\n\nFigure 18: Chain-of-thought prompting can also improve GLM-130B’s performance on reasoning tasks compared to standard prompting.\n\nBoolQ CB\n\nCOPA MultiRC ReCoRD RTE WiC WSC\n\nGLM-130B 89.69\n\n98.21\n\n100\n\n89.32\n\n92.11\n\n94.22\n\n76.96\n\n88.5\n\nTable 21: The results of GLM-130B on the SuperGLUE dataset obtained using the P-tuning v2 (Liu et al., 2022). We report the Accuracy metric for all datasets except for MultiRC (F1a) and ReCoRD (F1).\n\nThe results are presented in Figure 17. We ablate the unidirectional and bidirectional GLM-130B to justify the usefulness of GLM objective in boosting LLMs’ ability to understand. Each point in the figure refers to a prompt-specific result, for which the prompt is from the promptsource (Bach et al., 2022) repository. We adopt the task formulation from promptsource, too. As we can observe, GLM (bi) has much fewer variances and higher performances on all tasks. For some of the tasks (such as CB, MultiRC, RTE, COPA, and BoolQ), GLM-130B can even achieve over 80% accuracy.\n\nWe also attempted to fine-tune GLM-130B on the SuperGLUE dataset. However, we encountered the issue of rapid overfitting within a single epoch when we used full parameter fine-tuning on downstream tasks. This resulted in poor performance on the validation set. To address this issue, we explored the use of efficient parameter fine-tuning methods, which tune only a small number of parameters and are less prone to overfitting. After experimenting with several methods, we use P-Tuning v2 (Liu et al., 2022), which demonstrated comparable results to full parameter fine-tuning in GLM-130B, but with only 0.1% to 3% of tuned parameters. The results of our experiments with P-Tuning v2 are presented in Table 21.\n\nC.14 CHAIN-OF-THOUGHT PROMPTING\n\nWe evaluate the chain-of-thought prompting performance on Last letter concatenation (LLC), Coin Flip, Reverse List, and two tasks from BIG-bench Srivastava et al. (2022) Sports understanding, and Date understanding, following the setting in Wei et al. (2022c). The results are shown in Figure 17. We find that chain-of-thought prompting can improve GLM-130B’s performance on symbolic reasoning and commonsense reasoning.\n\n45\n\ncbrecordwscmultircrtewiccopaboolq20406080100GLM-130B (uni)GLM-130B (bi)SportsLLCCoin FlipCoin Flip (OOD: 3)Reverse ListDate02040608010054.01.084.547.953.115.773.713.495.058.668.327.9Standard PromptingChain-of-ThoughtsPublished as a conference paper at ICLR 2023\n\nFigure 19: Log-scaling ability tasks of GLM-130B. These tasks’ performance grows logarithmically with the amount of GLM parameters. Most of traditional NLP tasks fall into the same pattern.\n\nLast letter concatenation (LLC). The task asks the model to concatenate the last letters of words in a name (e.g., \"Elon Musk\" -> \"nk\"). We generate full names by randomly concatenating the top 1000 first and last names from name census data14.\n\nCoin flip. This task asks the model to answer whether a coin is still heads up after people either flip or don’t flip it beginning from being heads up. (e.g., \"A coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?\" -> \"no\"). We additionally evaluate on the scenario where the number of people in the query examples is larger than that in the in-context examples, i.e. the out-of-distribution (OOD) setting.\n\nReverse List. This task asks the model to reverse the order of a list of everyday objects (e.g., \"cigar, umbrella, key, gum, alarm\" -> \"alarm, gum, key, umbrella, cigar\"). We generate the lists by randomly sampling from the vocabulary of everyday objects15.\n\nSports. This task asks the model to judge the truthfulness of a statement about a sports player (e.g., \"Joao Moutinho caught the screen pass in the NFC championship\" -> \"false\").\n\nDate. This task asks the model to infer the data from a given context (e.g., \"2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\" -> \"01/05/2015\").\n\nWe use the same examples and chains as Wei et al. (2022c). For each task, we try two different formats of prompts and both unidirectional and bidirectional attention mechanism and report the best performance. The first format is \"Question: {context} Answer: {target}\". The second one is to add serial numbers before examples in the first format of prompts. The results are presented in Figure 18.\n\nD SCALING AND EMERGENT ABILITIES IN GLM-130B\n\nScaling up pre-trained language models has been proven to boost downstream performance on a wide range of tasks continually. His, emergent abilities which are unpredictable from smaller scales. To illustrate this, we conducted extensive experiments to explore the scaling property and emergent abilities. Following prior literature (Wei et al., 2022b), we categorize the NLP tasks into two types based on our observations.\n\n• Log-scaling Ability Tasks (Cf. Figure 19): where the task performance grows logarithmically with the number of model parameters. Typical tasks and datasets include LAMBADA, Wikitext103, Wikitext-2, Penn Tree Bank.\n\n• Emergent Ability Tasks (Cf. Figure 20): where the task performance only soars up when the amount of model parameters reaches a certain threshold. Typical tasks and datasets include:\n\n14https://namecensus.com 15https://www.vocabulary.com/lists/189583\n\n46\n\nLog-scaling Ability TasksPublished as a conference paper at ICLR 2023\n\nFigure 20: Emergent ability tasks of GLM-130B. These tasks’ performance does not grow much until the model size reaches a certain threshold (e.g., 100B or 10B). After reaching the threshold, the model performance soars up quickly. The BIG-bench (Srivastava et al., 2022) benchmark collects many of these challenges.\n\nMMLU, hindu_knowledge, crass_ai, implicatures, understanding_fables, modified_arithmetic, implicit_relations, and gre_reading_comprehension from BIG-bench (Srivastava et al., 2022).\n\nIn line with the observation in (Wei et al., 2022b), we show that GLM-130B also presents the two similar scaling behaviors to other LLMs such as GPT-3, LaMDA, and PaLM. Though why and how LLMs present these intriguing properties remain unclear, GLM-130B provides open opportunities for all researchers to test and understand the reason behind them.\n\n47\n\nEmergent Ability TasksPublished as a conference paper at ICLR 2023\n\nTable 11: Full configurations for GLM-130B training\n\nConfiguration Key\n\nadam_beta1 adam_beta2 adam_eps aggregated_samples_per_sequence attention_dropout attention_softmax_in_fp32 average_block_length bias_dropout_fusion checkpoint_activations checkpoint_in_cpu checkpoint_num_layers clip_grad contigious_checkpointing cpu_optimizer data_parallel_size deepnorm distributed_backend eval_interval eval_iters ffn_hidden_size fp16 global_batch_size glu_activation gpt_prob hidden_dropout hidden_size hysteresis init_method_std init_method_xavier_uniform initial_loss_scale layernorm_epsilon learnable_rotary_embedding length_per_sample log_interval loss_scale loss_scale_window lr lr_decay_iters lr_decay_samples lr_decay_style lr_warmup_samples make_vocab_size_divisible_by mask_prob masked_softmax_fusion micro_batch_size min_gmask_ratio min_loss_scale min_lr multitask_ratio num_attention_heads num_layers onnx_safe optimizer partition_activations pipeline_model_parallel_size position_embedding_type rampup_batch_size save_interval seed seq_length short_seq_prob shrink_embedding_gradient_alpha single_span_prob split tensor_model_parallel_size tokenizer_type weight_decay zero_contigious_gradients zero_reduce_bucket_size zero_reduce_scatter zero_stage zero-optimization.allgather_bucket_size tokenizer_type weight_decay world_size zero_contigious_gradients zero_reduce_bucket_size zero_reduce_scatter zero_stage zero-optimization.allgather_bucket_size\n\n48\n\nValue\n\n0.9 0.95 1e-08 4\n0.1 True 3\nTrue True False 1\n1.0 False False 24 True nccl 1000 3\n32768 True 4224 geglu 0.7 0.1 12288 2\n0.0052 False 65536 1E-05 False 2000 1\n0 2000 8e-05 None 197753905 cosine 1098632 768 0.15 True 1\n0.2 1.0 8e-06 0.05 96 70 None adam True 8\nrotary 192, 24, 5493164 250 1234 2048 0.02 0.1 0.02 949,50,1 4\nIceTokenizer 0.1 False 500000000 False 1\n500000000 IceTokenizer 0.1 768 FALSE 500000000 FALSE 1\n500000000\n\nPublished as a conference paper at ICLR 2023\n\nTable 12: The 74 datasets involved in Multi-task Instruction Pre-training (MIP). Datasets from T0PromptSource (Sanh et al., 2022; Bach et al., 2022) are named in their Hugging Face datasets identifiers. Datasets from DeepStruct (Wang et al., 2022a) are described in Appendix C.2.\n\nTask\n\nDataset\n\nTask\n\nDataset\n\ncos_e/v1.11 cosmos_qa dream openbookqa/main qasc quail quarel quartz race/high race/middle sciq social_i_qa super_glue/boolq super_glue/multirc wiki_hop/original wiqa piqa ag_news dbpedia_14 trec\n\nCoreference Resolution Coreference Resolution Natural Language Inference super_glue/cb Natural Language Inference super_glue/rte Natural Language Inference anli Paraphrase Identification Paraphrase Identification Paraphrase Identification Closed-Book QA Closed-Book QA Closed-Book QA Closed-Book QA Closed-Book QA Closed-Book QA Extractive QA Extractive QA Extractive QA Extractive QA Extractive QA Extractive QA Extractive QA Extractive QA Extractive QA Sentiment Sentiment Sentiment Sentiment Sentiment Sentence Completion Sentence Completion Structure-to-Text Structure-to-Text Summarization Summarization Summarization Summarization Summarization\n\nsuper_glue/wsc.fixed Multi-choice QA winogrande/winogrande_xl Multi-choice QA Multi-choice QA Multi-choice QA Multi-choice QA Multi-choice QA glue/mrpc Multi-choice QA glue/qqp paws/labeled_final Multi-choice QA ai2_arc/ARC_Challenge Multi-choice QA Multi-choice QA ai2_arc/ARC_Easy Multi-choice QA kilt_tasks/hoptpotqa Multi-choice QA trivia_qa/unfiltered Multi-choice QA web_questions Multi-choice QA wiki_qa Multi-choice QA adversarial_qa/dbidaf Multi-choice QA adversarial_qa/dbert Multi-choice QA adversarial_qa/droberta Topic Classification duorc/SelfRC Topic Classification duorc/ParaphraseRC Topic Classification ropes Word Sense Disambiguation super_glue/wic squad_v2 Dialogue State Tracking super_glue/record Event Extraction quoref Named Entity Recognition amazon_polarity Named Entity Recognition app_reviews Named Entity Recognition imdb Named Entity Recognition rotten_tomatoes Named Entity Recognition yelp_review_full Named Entity Recognition super_glue/copa Relation Extraction hellaswag Relation Extraction common_gen Relation Extraction wiki_bio Relation Extraction cnn_dailymail/3.0.0 Relation Classification gigaword Semantic Role Labeling multi_news Semantic Role Labeling samsum Semantic Role Labeling xsum\n\nmultiwoz_2.1 ace05 conll03 genia ontonotes5.0 ace2005 conll04 nyt29 conll04 nyt29 ace2005 kelm tacred conll05 conll12 propbank\n\n49\n\nPublished as a conference paper at ICLR 2023\n\nTable 14: Details results of GLM-130B, GPT-3 175B (Brown et al., 2020), and PaLM 540B (Chowdhery et al., 2022) on BIG-bench-lite in 0, 1, and 3-shots. “Normalized preferred metric” is reported for each task. GPT-3 and PaLM’s results are reported in BIG-bench’s GitHub repository, and PaLM 540B’s 3-shot results are not found.\n\nGLM-130B\n\nGPT-3 175B\n\nPaLM 540B\n\n0\n\n1\n\n3\n\n0\n\n1\n\n3\n\n0\n\n1\n\nauto_debugging bbq_lite_json code_line_description conceptual_combinations conlang_translation emoji_movie formal_fallacies_syllogisms_negation hindu_knowledge known_unknowns language_identification linguistics_puzzles logic_grid_puzzle logical_deduction misconceptions_russian novel_concepts operators parsinlu_reading_comprehension play_dialog_same_or_different repeat_copy_logic strange_stories strategyqa symbol_interpretation vitaminc_fact_verification winowhy\n\n11.76 22.26 0.22 37.51 34.72 1.25 0.83 32.23 -4.35 9.62 0.00 9.88 24.18 -26.53 6.25 14.76 7.14 2.88 0.00 43.86 21.10 1.39 71.87 -3.49\n\n23.53 20.59 59.73 37.50 -8.64 9.09 27.86 31.33 33.88 38.01 3.75 4.88 0.35 1.46 34.52 37.56 4.35 0.00 1.90 1.97 0.00 0.00 5.24 13.66 22.20 20.35 -46.94 -26.53 25.78 21.87 18.10 18.10 11.58 7.72 3.80 5.33 0.00 0.00 42.31 51.76 16.82 18.74 1.77 1.89 56.55 60.72 3.0 5.38\n\n0.00 -8.33 9.09 2.37 46.82 -10.00 1.00 10.15 21.74 7.49 0.00 0.16 2.22 -34.70 33.59 30.0 0.00 8.00 0.00 8.27 4.60 0.51 -31.55 3.0\n\n0.00 40.75 9.09 3.70 47.07 -2.49 6.80 40.61 4.35 3.20 0.00 3.35 10.80 -34.70 33.59 34.29 0.00 0.80 0.00 25.68 13.20 -0.63 22.15 10.60\n\n0.00 61.21 9.09 14.33 51.60 -1.24 5.60 44.42 0.00 1.98 0.00 0.01 14.71 -30.61 45.31 33.33 0.00 -5.40 0.00 12.93 14.20 2.77 29.05 13.00\n\n0.00 -4.39 0.22 45.68 36.88 17.50 -0.20 41.37 13.04 12.11 0.00 1.47 2.17 -42.86 33.59 30.48 9.46 -33.0 0.00 39.25 28.00 0.76 -28.85 -5.0\n\n38.23 77.73 49.00 73.36 61.92 88.75 4.40 93.15 34.78 31.03 0.10 16.12 15.34 -30.61 49.22 56.19 44.40 0.10 37.5 74.46 38.00 2.40 55.60 31.80\n\n50\n\nPublished as a conference paper at ICLR 2023\n\nTable 15: Detailed results of GLM-130B and BLOOM 176B (Scao et al., 2022) on MMLU (Hendrycks et al., 2021). We find that no existing literature has reported GPT-3 175B’s numerical accuracy. BLOOM is evaluated using Huggingface Transformer implementation.\n\nDiscipline\n\nGLM-130B BLOOM 176B\n\nSTEM\n\nSocial Science\n\nHumanities\n\nOther\n\nabstract_algebra anatomy astronomy colledge_biology college_chemistry colledge_computer_science colledge_mathematcis colledge_physics computer_security conceptual_physics electrical_engineering elementary_mathematics high_school_biology high_school_chemistry high_school_computer_science high_school_mathematics high_school_physics high_school_statistics machine_learning\n\neconometrics high_school_geography high_school_government_and_politics high_school_macroeconomics high_school_microeconomics high_school_psychology human_sexuality professional_psychology public_relations security_studies sociology us_foreign_policy\n\nformal_logic high_school_european_history high_school_us_history high_school_world_history international_law jurisprudence logical_fallacies moral_disputes moral_scenarios philosophy prehistory professional_law world_religions\n\nbusiness_ethics clinical_knowledge colledge_medicine glocal_facts human_aging management marketing medical_genetics miscellaneous nutrition professional_accounting professional_medicine virology\n\n51\n\n24.00 48.90 48.03 47.22 34.00 44.00 27.00 30.39 61.00 38.72 45.52 31.75 51.29 34.98 53.00 28.15 29.80 38.43 40.18\n\n26.32 53.54 62.18 42.56 45.80 54.13 51.15 42.48 55.46 44.90 51.74 61.00\n\n27.78 58.18 58.33 67.09 56.20 43.52 57.06 47.11 24.25 45.34 50.93 37.94 55.56\n\n51.00 48.68 43.35 35.00 45.29 56.31 67.52 48.00 61.18 50.65 35.46 43.38 39.16\n\n24.00 38.52 34.87 37.50 19.00 1.00 31.00 24.50 40.00 31.49 32.41 29.63 27.42 27.09 30.00 25.93 30.46 26.39 29.46\n\n26.32 36.36 40.41 30.77 26.89 39.27 35.11 31.54 33.64 34.29 31.84 46.00\n\n23.02 35.76 40.69 32.07 42.15 35.19 31.29 36.71 24.36 35.37 40.43 29.53 42.11\n\n34.00 35.85 28.90 23.00 32.29 27.18 39.74 45.00 40.23 32.35 28.72 18.01 28.31\n\nPublished as a conference paper at ICLR 2023\n\nE CONTRIBUTIONS\n\nThe GLM-130B project was conceived in Dec. 2021 with its pre-training part completed in July 3rd, 2022 and its evaluation and applications still ongoing. Over the course, we have experienced various technical and engineering challenges (Cf. Appendix F and Figure 21 for details). It would not be possible to reach its current status if without the collaboration of multiple teams—the Knowledge Engineering Group (KEG), Parallel Architecture & Compiler technology of Mobile, Accelerated, and Networked systems Group (PACMAN), and Natural Language Processing Group (THUNLP) at Tsinghua University, as well as Zhipu.AI. The detailed contributions are listed below.\n\nE.1 PREPARATION\n\n• Model Implementation: Aohan Zeng, Zhengxiao Du\n\n• Self-Supervised Data Processing: Ming Ding, Wendi Zheng\n\n• Multitask Data Processing: Xiao Liu, Xiao Xia\n\n• Model Architecture: Aohan Zeng, Xiao Liu, Zhengxiao Du, Hanyu Lai\n\n• Training Stability: Aohan Zeng, Xiao Liu, Ming Ding\n\n• 3D-Parallelism and Training Efficiency: Aohan Zeng, Zixuan Ma, Jiaao He, Zhenbo Sun\n\nE.2 MODEL TRAINING\n\n• Large-Scale Training & Monitoring: Aohan Zeng, Xiao Liu\n\n• Model Performance Validation: Aohan Zeng\n\nE.3 POST TRAINING\n\n• Evaluation Framework: Aohan Zeng, Zhengxiao Du\n\n• Language Modeling Evaluation: Aohan Zeng\n\n• MMLU & BIG-Bench Evaluation: Aohan Zeng\n\n• CLUE & FewCLUE Evaluation: Xiao Liu, Aohan Zeng\n\n• Ethical Evaluation: Yifan Xu, Aohan Zeng, Xiao Liu, Zihan Wang\n\n• Baseline Evaluation: Xiao Liu, Jifan Yu, Weng Lam Tam\n\n• INT4 Quantization: Aohan Zeng, Zihan Wang, Xiao Liu, Hanyu Lai\n\n• Inference Acceleration: Zihan Wang, Aohan Zeng\n\n• Low-Resource Inference: Gouyang Zeng, Xu Han, Weilin Zhao, Zhiyuan Liu\n\n• Demo and API: Hanyu Lai, Jifan Yu, Xiaohan Zhang, Yufei Xue, Shan Wang, Jiecai Shan, Hao-\n\nhan Jiang, Zhengang Guo\n\n• Manuscript Writing: Xiao Liu, Yuxiao Dong, and Jie Tang wrote the main paper, and Xiao Liu,\n\nAohan Zeng, and Zhengxiao Du wrote the Appendix.\n\nE.4 PROJECT MANAGEMENT\n\n• Student Leaders: Aohan Zeng, Xiao Liu\n\n• Technical Advisors: Yuxiao Dong, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Jie\n\nTang\n\n• Project Leader: Jie Tang\n\nE.5 COMPUTATION SPONSOR\n\n• GPU Sponsor: Zhipu.AI\n\n52\n\nPublished as a conference paper at ICLR 2023\n\nF A BRIEF HISTORY OF GLM-130B\n\nThe GLM-130B project16 was conceived in Dec. 2021 in a brainstorming meeting at Tsinghua KEG. We firmly believe that it is of value to pre-train a highly accurate language model, in particular for both Chinese and English. Though GPT-3 (Brown et al., 2020) is the pioneer for this effort, it is not available to most people in the world. In addition, it supports English only. We therefore decide to initialize the project GLM-130B. Please note that the WuDao 1.75T model we built last year is a sparse model with 480 mixture-of-experts (MoE), rather than a dense one as GPT-3. Our goal then is to train a bilingual pre-trained dense model with high accuracy on downstream tasks, and to make it open to everyone in the world-anyone, anywhere can download it and use it on a single server with appropriate GPUs.\n\nThe ambitious project soon faced several important challenges:\n\n• Lack of computational resources: No organization is willing to sponsor such a big project and\n\nfreely make it public.\n\n• Lack of a robust pre-training algorithm: Despite GPT-3’s success on English corpus, it is\n\nunclear how to train a high-accurate bilingual model for both English and Chinese.\n\n• Lack of fast inference solutions: Since the goal is to have the model public to everyone, we need\n\nto design fast inference solutions with low resource requirements to run the model.\n\nFor the pre-training algorithm, we finally chose GLM (Du et al., 2022) due to its high performance in practice. We eventually decided to train a GLM model of 130 billion parameters after several rounds of discussions and exploration, because such a size makes it possible to run the inference on a single A100 (40G * 8) server.\n\nOur first attempt at training the model was in January 2022, shortly after we received a small sponsor of GPUs for test running. However, we soon realized that we had significantly underestimated the technical difficulties of pre-training a model at such a scale (>100B). It seems that pre-training a highly accurate 100B-scale model is quite different from training a 10B-scale one. Due to frequent random hardware failures, model gradients exploding, unexpected excessive memory usage in the algorithm, debug for the 3D pipeline in the new Megatron and DeepSpeed frameworks, inability to recover from optimizer states, blocked TCP responses between processes, and many many unexpected “bugs”, the project was delayed for many times. The Tsinghua PACMAN team gave us a hand at this difficult time and together we successfully fixed most of the “bugs”.\n\nBy March, we were still short on computational resources, but fortunately got a chance to try test runs on several other platforms, including Ascend 910, Hygon DCU, NVIDIA, and Sunway. The immediate challenge was for us to adapt our training code to these different platforms, as the underlying operators are quite different. Also, it introduced many new issues: the element-wise operators not supporting fast computation for large-dimension vectors, various issues that hindered convergence—the large gradient norms of input embeddings, native Post-LN, Pre-LN, and Sandwich-LN, dataloader state seeds, and computation precision choices in Softmax and Attention — as well as numerous mistakes we ourselves made. With tremendous help from all of our generous partners, we finally succeeded in making our pre-training algorithms runnable across all the platforms—frankly, a surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of the issues we have encountered and addressed as of this writing.\n\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that aims to teach machines to think like humans. After another week of testing, we finally kicked off the training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally, Zhipu.AI also sent a team to help evaluate the pre-trained model and build a demonstration website.\n\nThe training period spanned two months, during which we began developing a toolkit to allow GLM130B’s inference in low-resource setting with swapping technique and quantization. Though it is already the most accessible model of its scale, together with our partner from Tsinghua NLP, we have been exploring the limit of popularized hardware platforms, which would truly make the 100B-scale model accessible to as many people as possible. To date, we managed to reach the INT4 weight quantization for GLM-130B. Importantly, the INT4 version of GLM-130B without post training\n\n16This section is largely extracted and updated from the blog introduction of GLM-130B at http://keg.\n\ncs.tsinghua.edu.cn/glm-130b/ (Posted date: August 4, 2022).\n\n53\n\nPublished as a conference paper at ICLR 2023\n\nFigure 21: The timeline of major issues that training GLM-130B encountered and addressed, as of July 31st, 2022.\n\n54\n\n2021.12•The “千亿”(100B) project towardsan opendensepre-trained GLM at 100B scale isconceived•Survey pre-trainingstrategies of existing models of similar scale,suchasGPT-3,Gopher=> Limited public infoabout how they were trainedand issues they met•SearchforpossibleGPUclusters&sponsors2022.1•TesttheperformanceofFP16/FP32at100Bscaleononetestingcluster•UnexpectedexcessivememoryusageinGLM=> Torchisbetterwithfixedlengthinputsequences•InabilitytoconvergeandtrytricksfromCogViewandViT=> UseSandwich-LN•Frequentrandomhardwarefailures=> HavetorunHCPGtestbeforeeachrun2022.2•Veryslowtrainingspeedthanpreviouslycalculated=>Optimizekernelsandfuseoperators=>Findtheinputshapeiscriticaltokernelperformance•Collectpre-trainingcorporaandtokenize=>Useicetk:thesentencepieceissettotheunigrammode•Debugthe3Dpipelineparallelinthenewly-releasedMegatronandDeepSpeed2022.3•Itcan’trecoverperfectlyfromcheckpoints=>Ourcustomizeddataloaderdonotsaveitsstateseedproperlyindistributedtraining•Thememoryperprocessoristoosmall=>Requiretoomanypipelinestages=>Batchsizeistoolarge(upto12,000)=>Harmthemodel’sconvergency•Itcan’tlaunchmorethan2,000computingnodes=>Overcomethisandsupport6,000-nodetrainingbytuningLinuxkernelTCPparameters•Collectdataformulti-taskinstructionpre-training•Receiveopportunitiestotesttrainingsonseveralotherclusters•Veryslowtrainingspeedthanexpected=>Theunderlyingelement-wiseoperatorsdon’tsupportfastcomputationonlarge-dimensionvectors.2022.4•OptimizeA100kernel’scomputingefficiency=>A100kernelsprefersquare-shapedinputs,andseq_len=2,048isoptimalforourhidden-statedimension(12,288)•Inabilitytoconvergeduetolargegradientnorms(170+)ofinputembeddings=>Tryembeddingnormandgradientshrink,whichturnouttobealmostequivalent•Naïvepost-LNorpre-LNdisconvergesafterseveralthousandsofsteps=>TrySandwich-LNwithPB-Relax•Itstilldisconvergesafteroneweek’strial=>Thedataloaderstateseedsarenotunifiedfordifferentpipelinestages,resultinginamismatchofinputdataandlabels.•Testtwopositionalencodings:RoPEandAlibi=>Alibicanbeslowerasitrequireselement-wisemanipulationonattentionmatrices---changingnum_heads*2,048* 2,048scalarsperlayer•TestGeGLUandGAU=>GAUconvergesfasterwithrelativelypoorperformanceonfine-tunedSuperGLUE•AbnormalGPUmemoryusageofnewly-addedfunctionsandclasses=>DeepSpeedhardcodesthefunctionnamesforcheckpointactivation•DecidetotrainGLMwith130billionparameters=>allowinferenceonaDGX-A10040Gnode2022.5-6•ImplementaRoPEcudaoperatorinC++=>Seeunexpectedprecisionerrorsandfinallyhaveitabandoned•Sandwich-LNstilldisconverges=>1)Reducinglearningratedoesnothelp;2)UsingHingecross-entropybecomesslowerandharmsperformance;3)ShiftingtoDeepNormstilldisconverges•UseFP32insoftmaxofattention=>Success•FindPB-RelaxunnecessaryforFP32softmax=>Italsoslowsdowntrainingasitneedstomanipulatethewholeattentionscorematrices•Experiencefewspikesinlatertraining=>1)Reducegradientshrinkfactorfrom1to0.1:useful;2)Reducethelearningrate:sometimesuseful;3)Jumpthenoisydatabatches:sometimesuseful•Findamistakeinmulti-taskdataaftertrainingfor20,000steps=>Usethecorrectdatabutitdoesnotforget2022.6-7•AdaptthepipelineparallelcheckpointstoordinaryparallelcheckpointsforefficientinferenceonasingleA100•Workonevaluationscriptsondatasets:MMLU,Big-bench,CLUE,SuperCLUE,etc.•ImplementP-TuningandP-Tuningv2forparameter-efficienttuningonGLM-130BfortuningonSuperGLUE•WorkwithBMInfonadaptingGLM-130BtoperforminferenceonasingleV100or3090=>Usepipeline-styleasynchronousswappingbetweenmainmemoryandGPUmemory•Trytofine-tuneGLM-130BwithfewerA100nodes(i.e.,12-16nodes)=>Pipeline-stylefailsduetotoomanypipelinestages=>Findthatdataparallelcannotbeintroducedforfine-tuning=>Use32-waymodelparallelforfine-tuningwithreasonableperformanceMajorIssuesEncounteredforTrainingGLM-130Bhttps://github.com/THUDM/GLM-130BTsinghuaKEGPublished as a conference paper at ICLR 2023\n\nfaces negligible performance degradation compared to its uncompressed original, while it consumes only 25% of the GPU memory required by the uncompressed version, thus supporting its effective inference on 4 × RTX 3090 Ti (24G) or 8 × RTX 2080 Ti (11G). We will attempt to further reduce the resource requirements and keep the community updated on this important working item.\n\nG BROADER IMPACT\n\nThis paper introduces an open bilingual pre-trained language model with 130 billion parameters. Currently most pre-trained language models with over 100 billion parameters are privately owned by governments and large corporations (Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022; Wang et al., 2021). A few of them (Brown et al., 2020; Lieber et al., 2021) provide limited inference APIs with fees. In contrast, the weights and code of GLM-130B are open to anyone who is interested in LLMs. Moreover, we significantly lower the hardware requirements for inference by speed-up implementation and INT4 quantization. The paper can have a broader impact on the research community, individual developers and small companies, and society.\n\nG.1\n\nIMPACT ON AI RESEARCH\n\nMost research institutions cannot afford the substantial cost of pretraining large language models. As a result, most researchers, except employees of governments and large corporations, only have access to the limited inference APIs with fees. With the inference APIs, researchers can only analyze the outputs of models as black boxes, which limits the scope of potential work. With GLM-130B, researchers can analyze the model parameters and internal states corresponding to specific inputs, leading to in-depth studies of LLMs’ theory, capacity, and flaws. Researchers can also modify the model architecture and weights, to validate the proposed algorithms to improve LLMs Zhu et al. (2020); Cao et al. (2021); Hase et al. (2021); Mitchell et al. (2022).\n\nWith INT4 quantization, GLM-130B can perform inference on popularized GPUs such as 4 × RTX 3090 or 8 × RTX 2080 Ti, which can be easily accessed from cloud service. As a result, researchers who cannot afford powerful data-center GPU servers like DGX-A100 can also utilize GLM-130B.\n\nG.2\n\nIMPACT ON INDIVIDUAL DEVELOPERS AND SMALL COMPANIES\n\nCurrently, individual developers and small companies who want to integrate LLMs into their business can only choose paid inference APIs. The increased cost can hinder their attempts. Instead, GLM-130B can be deployed on popularized hardware that they own or can access via cloud service to reduce the cost. Furthermore, they can utilize distillation techniques Sanh et al. (2019); Jiao et al. (2020) to obtain smaller models that preserve comparable performance on their specific tasks. While some developers may lack the ability to complete deployment and distillation on their own, we believe with GLM-130B and more open LLMs in the future, the corresponding toolkits and service providers will become more available.\n\nWe also note that currently most applications of LLMs are based on prompt engineering, partly due to the limitation of inference APIs. In downstream scenarios such as online customer service, the companies accumulate huge amounts of human-generated data that contain domain knowledge. With the open-source weights and code, developers can finetune GLM-130B on their own data to mitigate the gap of domain knowledge.\n\nG.3 SOCIAL IMPACT\n\nLarge language models, together with other machine learning models in different modalities (e.g., Image (Ramesh et al., 2021; Ding et al., 2021; Saharia et al.) and Video (Hong et al., 2022)), could be used to generate synthetic text for harmful applications, such as telemarketing fraud, political propaganda, and personal harassment as is discussed in (Weidinger et al., 2021; Sheng et al., 2021; Dev et al., 2021). We do not anticipate any hazardous outputs, especially towards vulnerable and historically disadvantaged groups of people, after using the model.\n\nWhile some people think that restricting access to LLMs can prevent such harmful applications, we argue that promoting LLM inclusivity can lead to better defense against potential harm caused by\n\n55\n\nPublished as a conference paper at ICLR 2023\n\nLLMs. Currently, only governments and large corporations can afford the considerable costs of pretraining LLMs. There is no guarantee that organizations having the substantial financial resources to pretrain an LLM will not do harm with it. Without access to such LLMs, individuals cannot even realize the role of LLMs in harm. Conversely, releasing an open LLM can provide access and transparency to all the researchers and promote the research to reduce the potential harm of LLMs, like algorithms to identify the synthetic text Gehrmann et al. (2019) or detect fake news Li et al. (2021).\n\nAlso, it is known that LLMs can suffer from problems in fairness, bias, privacy, and truthfulness Zhang et al. (2021); Lin et al. (2022); Liang et al. (2021); Bender et al. (2021). An open LLM can reveal the model parameters and internal states corresponding to specific inputs instead of providing APIs to black-box models. In conclusion, researchers can conduct analysis of LLMs’ flaws in depth and propose improved algorithms to solve the problems.\n\nH ENVIRONMENTAL IMPACT\n\nOne of the major concerns about large language models is their huge energy usage and associated carbon emissions Strubell et al. (2019); Lacoste et al. (2019); Patterson et al. (2021); Bender et al. (2021). GPT-3 was estimated to use 500 tons of carbon emissions footprint (CO2eq) Patterson et al. (2021). We consumed a total of 442.4MWh of electricity over the 60-day course of training. Given the 0.5810 kg/kWh carbon efficiency of local power grid, the pre-training released 257.01 metric tons of CO2. This is around half of GPT-3’s carbon footprint, probably due to the efficient parallel strategies and NVIDIA’s hardware improvements. The carbon emission is roughly the equivalent of the yearly emissions of 18 average Americans. However, we believe that with GLM-130B released, more carbon emissions for reproducing 100B-scale LLMs can be saved.\n\n56",
    "reference": "# Summary Of The Paper\n\nThe paper trains a new 130B parameter model with the GLM architecture/objective, with contributions in automatically stabilizing spikes with existing techniques, focussing on keeping inference costs and requirements low and therefore the model accessible to a large number of people. It is an achievement to beat GPT3 numbers and in a few cases PaLM numbers.\n\n# Strength And Weaknesses\n\nStrengths:\n - Impressive set of techniques put together for stability and model quality (DeepNorm initialization, GLM instead of CausalLM objective, Embedding Layer Gradient Shrink)\n - Impressive performance on measured benchmarks esp compared to GPT3 & in a few cases PaLM\n\nWeaknesses\n - Would have loved to see some ablations, especially over the first few ~100B tokens as to which technique contributes how much to performance, either within GLM-130B or implemented across models, i.e. for example with T5 style models the GLM objective would be essentially T5’s denoising task + prefixLM. Otherwise it is not clear what technique is responsible for how much of the gains.\n - More evaluations: The evaluation section is severely limited\n   - It could have evaluated fine-tuning over a few tasks (Ex: PaLM has both few-shot numbers and fine-tuning numbers for a few benchmarks like SuperGlue and TyDiQA) — this is understandably partly a resource issue.\n   - More extensive few-shot evaluations on say SuperGlue, Winograd style tasks, CBQA style tasks, CommonSense reasoning, Chain of Thought prompting etc — Since inference is both fast and cheap this shouldn’t be too much of an issue and would increase my confidence of this method’s applicability and more importantly in the released checkpoint.\n   - Not evaluating on tasks with fixed labels (like NLI) seems like severely and unnecessarily restrictive, i.e. I don’t think Xian et al 2018’s advice applies here, one can/should do NLI tasks with encoding the label in the prompt, while not giving an example (for zero-shot) or giving a few examples of each class (in a few-shot setting) and evaluating. Since MultiTask Instruction Pre-training (MIP) is happening here, I think it is fair to only exclude tasks that are substantially similar in the actual example content, if the task type seen is similar (say classification with the same labels) this should be called out and reported separately.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nPoints of Clarification (did not impact the score):\n\np1 / Introduction - “However, both GPT-3 (and other 100B-scale ones)—the model itself—and how it can be trained, have been thus far not available to the public.”\n\nIt might be better that this be reframed for clarity, I think all the models describe how they are trained (i.e. methodology in their paper), two of these have weights released (OPT 175B on request and Bloom 176B on github here - https://huggingface.co/bigscience/bloom/tree/main), and atleast Bloom’s script is at https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme\n\np2 / Introduction — “associated with significantly less bias” any causal explanation?\n\np2 / Introduction — “unique property of GLM architecture” “INT4 quantization introduces very limited performance losses” : It is not clear that ability to do INT4 comes directly from GLM architecture, it could also be the other changes like training objective (MASK + gMASK) or initialization choices like DeepNorm etc or a combination of both. Fig 5 does explain why INT4 works for GLM-130B and not BLOOM, but the claim that this is due to the GLM architecture seems strong, accordingly “unique property” seems overly strong and unsubstantiated.\n\np2 / Introduction — “associated with significantly less bias and generation toxicity”: Would the authors make any causal explanation as to why this ought to be the case, i.e. the major contribution seems to be pre-training mechanics, I cannot see how “less bias” would follow, this might be empirically true, but lacking a causal explanation as to why this would be so (for example: data cleanup or filtering) this can only be taken as an incidental fact, rather than a recipe to have less bias.\np4 / Section 2.2 — “Self-Supervised Blank Infilling”: This section is slightly confusingly framed, as written it led me to believe that both [MASK] and [gMASK] are in the **same sequence** — it wasn’t until section 2 ending on p5 where it became clear that these are two separate tasks (due to the lengths being different in MASK and gMASK). I would advice to split the section “Self-Supervised Blank Infilling” into two, one for MASK, another for gMASK, or at least call out them separately within the Self-Supervised section.\n\nMinor comments and typos (that did not impact the score):\n\n - p1 / Abstract - “disconvergence”, maybe write “divergence” since that’s the commonly used term? Or do you want to convey that disconvergence = opposite of convergence, so could be either divergence (loss steadily increasing) or NaN or just unstable loss?\n - p1 / Introduction - “capabilities suddenly arouse”, s/arouse/arise or arose.\n   - Same comment as above on p9 / Sec 6 “arouse as models” → “arise as models”\n   - p9 / Sec 7 Point 4 “arises” — however this point is poorly written\n - p1 / Introduction - “pioneers the studies of”, s/studies/study\n - p1 / Introduction - “we come to realize”, reframe as “we came to realize” or “we have come to realize”\n - p1 / Abstract & p2 / Introduction - “most ever affordable”, “most affordable” would suffice here.\n - p5 / Sec 3 - “We spend months” change to “We spent months”\n - p5 / Sec 3 - “in sacrifice of a significant penalty on model performance”, this is quite unclear, please reframe\n - p6 / Sec 3 - “first used in the multi-modal transformer CogView”, maybe just cite the cogview paper right there, sometimes paper title does NOT contain the name of the model, so the reference will be hard to find.\n - p8 / Sec 5.3 - “Here is out intuitive attempt” → “Here is our intuitive attempt” ; “we come up with” → “we came up with”\n - p8 / Sec 5.3 “if we ever got a chance to continu pre-training GLM-130B” - I would suggest summarizing under “future work”\n - p9 / Sec 6 - “despite many emerged 100B” cut out emerged\n - p9 / Sec 6 - “it has been concentrated on” reframe perhaps\n - p9 / Sec 7 - “architecture alternative, in addition to GPTs” →  “architecture alternative to GPTs” you are pointing out that GLM is an alternate way of self-supervision. I would rather frame it as an alternate “objective” instead.\n - p9 / Sec 7 Point 6 - “gradient to it’s 0.1” - ill framed sentence\n - p9 / Sec 7 Point 7 - “unique” is a strong word for here\n - p10 / Ethics - “And to estimate and better collaborate” no need for estimate\n - p10 / Reproducibility - PaLM citation is wrong, Paperno 2016 is mentioned instead of the PaLM citation.\n - p10 / Reproducibility - “We have paid great exertion”, better way to frame would be “We paid great effort”\n\n# Summary Of The Review\n\nWeak Accept owing to a severely limited evaluation section and methodology, the work itself pulls a lot of techniques together to get a stably pre-trained O(100B) model with GLM objective and stability techniques which is very valuable to get broad recognition.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTHE ETHICAL AMBIGUITY OF AI DATA ENRICHMENT: MEASURING GAPS IN RESEARCH ETHICS NORMS AND PRACTICES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe technical progression of artificial intelligence (AI) research has been built on breakthroughs in fields such as computer science, statistics, and mathematics. However, in the past decade AI researchers have increasingly looked to the social sciences, turning to human interactions to solve the challenges of model development. Paying crowdsourcing workers to generate or curate data, or ‘data enrichment’, has become indispensable for many areas of AI research, from natural language processing to inverse reinforcement learning. Other fields that routinely interact with crowdsourcing workers, such as Psychology, have developed common governance requirements and norms to ensure research is undertaken ethically. This study explores how, and to what extent, comparable research ethics requirements and norms have developed for AI research and data enrichment. We focus on the approach taken by two leading conferences: ICLR and NeurIPS, and journal publisher Springer. In a longitudinal study of accepted papers, and via. a comparison with Psychology and CHI papers, this work finds that leading AI venues have begun to establish protocols for human data collection, but these are are inconsistently followed by authors. Whilst Psychology papers engaging with crowdsourcing workers frequently disclose ethics reviews, payment data, demographic data and other information, similar disclosures are far less common in leading AI venues despite similar guidance. The work concludes with hypotheses to explain these gaps in research ethics practices and considerations for its implications.\n\n1\n\nINTRODUCTION\n\nWhen the creators of the seminal image recognition benchmark, ImageNet, pronounced that the use of Amazon’s Mechanical Turk (MTurk) was a “godsend” for their research, they foreshadowed the monumental impact crowdsourcing platforms were set to have on AI research (Li, 2019). In the decade that has followed, crowdsourced workers, or ‘crowdworkers’ have been a central contributor to machine learning research, enabling low-cost human data collection at scale.\n\nEthics questions posed by research involving human participants are traditionally overseen by governance groups, such as Institutional Review Boards (IRBs) in the United States (US). Whilst medical fields and social sciences have a long history of IRB engagement, the relatively recent rise of crowdsourcing tasks in AI research means guidelines and norms have been developed in recent years to consider research ethics. The proliferation of guidelines and publication policies have risen alongside critiques of AI crowdsourced work focused on issues such as payment and worker maltreatment.\n\nIn response, this study seeks to understand how AI research involving crowdworkers engages with research ethics. It does this via an assessment of the expectations put forward by publication venues on researchers, and by analysing how these expectations translate into practices. To make this determination the policies and practices of major AI conferences, ICLR and NeurIPS, along with AI research submitted to Springer journals, are reviewed. This is compared with other benchmarks to understand whether AI research at these venues follows norms within more established disciplines. The results show that AI research at these venues involving crowdworkers lacks robust research ethics norms, with venue policies not translated into practice. Whilst ICLR, NeurIPS and Springer\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nprovide research ethics guidance, the interpretation of these appears inconsistent, and fails to meet the same standards of disclosure as seen in other fields engaging with crowdworkers.\n\n2 RELATED WORK\n\nOversight in research involving human subjects is no recent phenomenon, with the Nuremberg Code of 1948 formalising the idea that humans involved in research required protection (Sass, 1983; Shuster, 1997). Research ethics in the United States arose during the 1960s, prompted by various scandals in biomedical research, and followed by scandals in social science studies (Beecher, 1966; Stark, 2016; Emanuel, 2008; Heller; Milgram, 1963; Zimbardo, 1972). These cases led to regulation standardising Institutional Review Board (IRB) oversight of research involving human subjects, a requirement that exists to this day, with similar processes existing in over 80 countries globally (Grady, 2015). IRBs only oversee research involving living ‘human subjects’, as defined by the Code of Federal Regulations (Office for Human Research Protections, 2017).\n\n2.1 CROWDSOURCING AND AI\n\nIn the twenty-first century the scope of research ethics has been extended by the rise of internet research (Taylor, 2000). The ability to recruit, engage with, and study human subjects online has led to the rise of crowdsourcing platforms, such as MTurk, becoming a key tool across a variety of academic disciplines (Howe, 2006). Launched in 2005, MTurk was an early pioneer of the crowdsourcing model (Cobanoglu et al., 2021). MTurk has remained popular due to its low cost, ease of access, and large user base (Williamson, 2016). The platform has been of particular use to the AI field, with Amazon marketing the platform as “artificial artificial intelligence” (Schwartz, 2019; Stephens, 2022).\n\nWhilst early AI crowdwork often involved labelling tasks, such as Fei-Fei Li’s seminal ImageNet work, the use of crowdworkers has diversified (Deng et al., 2009; Vaughan, 2018). Shmueli et al. offer three categories of data collection seen in NLP research papers: (1) labelling, (2) evaluation, and (3) production (Shmueli et al., 2021). For the purposes of this work these categories can be generalised across AI research.\n\nLabelling includes the processing of existing data by a crowdworker and then the selection or composition of a label or labels for that data. Labelling can be objective, for example crowdworkers may be asked to label objects in images (e.g. dogs or cats), or subjective, with one study asking MTurk workers to label their predicted political leanings of images (Thomas & Kovashka, 2019). Evaluation involves an assessment of outputs or data according to predefined criteria, such as fluency. This could be asking humans to provide feedback on model-generated language or produce a ‘mean opinion score’ by assessing the outputs of various models (Clark et al., 2021; D ́efossez et al., 2018; Stiennon et al., 2020). Production studies ask workers to produce their own data, rather than label or evaluate existing data. For example, studies might explicitly ask crowdworkers to write questions for a question-answer dataset (Talmor et al., 2018).\n\nThese categories can be broadly encapsulated by the Partnership on AI’s (PAI) definition of ‘data enrichment’ work, defined as data curation tasks which require human judgement and intelligence (Partnership on AI, 2021). However, this does not include research studying the behaviour of crowdworkers themselves (Vaughan, 2018). For example, a researcher might assess how individuals respond to interaction with algorithms deployed in an educational setting, or assess human perception of artificial systems (Fahid et al., 2021; Latikka et al., 2021; Koster et al., 2022). Behavioural studies are different to data enrichment tasks as they treat crowdworkers as the subject of research, rather than as a worker providing input to a model which is itself the subject of research.\n\n2.2 ETHICS IMPLICATIONS OF CROWDWORK\n\nIn parallel to the rise of crowdsourcing in AI research, critics have questioned the ethics of these practices in lieu of employment law protections for workers (Aloisi, 2016). Concerns centre on issues of payment, maltreatment, power asymmetry, and demographics.\n\nCrowdsourcing platforms are often utilised due to their low costs, and consequently many critiques (Scholz, 2016). MTurk allows requesters to place tasks online of crowdwork relate to payment\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nfor as little as $0.01 per task, with mean payment rates estimated to be around $3 per hour (Hara et al., 2018; Pew; Toxtli et al., 2021). Considering around 75% of MTurk workers are based in the US (with 16% based in India), this is far below federal minimum wage levels (Difallah et al., 2018). This has taken focus on AI developers with the issue being reported in various media outlets (Naylor, 2021; Slater, 2021).\n\nThe rise of underpaid uncontracted work has ushered in an “era of digital sweatshops”, raising concerns regarding worker wellbeing (Zittrain, 2009). An individual might be tasked with identifying harmful content, such as pornographic, violent, or offensive images, text, or video, in order to train algorithms, or might evaluate a model’s moderation performance (Dang et al., 2018; Edstedt et al., 2021; Hettiachchi & Goncalves, 2019). Such subjection to harmful content has been shown to have severe psychological impacts on workers (Steiger et al., 2021; Benjelloun & Otheman, 2020).\n\nIn many cases, AI research involving crowdworkers will not include such harmful content, but that does not mean ethics concerns beyond payment are absent. Platforms have been criticised because of inadequate feedback mechanisms and intransparent instructions leading to inability to meaningfully consent to studies (Schlagwein et al., 2019; Zimmer & Kinder-Kurlanda, 2017). Some studies have been found to deceive crowdworkers, whilst researchers have the ability to reject workers outputs for unclear reasons, leading to vastly imbalanced power dynamics against workers who are often without the protection of employment contracts (D ́ıaz et al., 2022; Irani, 2015).\n\nThe lack of consideration for workers has led to the idea that crowdsourced workers are “interchangeable” (D ́ıaz et al., 2022). This is despite work demonstrating demographics having drastic impacts on research outcomes (Salkind, 2010; Beel et al., 2013; Welbl et al., 2021). When curating datasets with crowdworkers, lack of diversity can lead to the ‘preservation’ of bias in future uses of data (Celi et al., 2022; Wachter et al., 2021; Crawford & Paglen, 2021). In combination, these issues have led crowdworkers being dehumanised, and labelled “Ghost Workers” (Barbosa & Chen, 2019; Gray & Suri, 2019; Mohamed et al., 2020; Prabhu & Birhane, 2020).\n\n2.3 ETHICS DISCLOSURES\n\nHow these ethics issues impact research practices has been assessed by examinations of publication practices. Santy et al. have undertaken this in the context of the NLP field, assessing how many papers engage with formal ethics review through IRBs. They find that very few papers (0.8%) cite IRB review (Santy et al., 2021). This is unsurprising considering many NLP papers will not involve any direct interaction with crowdworkers or human participants, and so would not be subject to IRB review. The authors additionally provide comparisons with other fields, such as cognitive sciences to demonstrate that NLP research lacks the same level of engagement with formal ethics processes (Santy et al., 2021).\n\nShmueli et al. conduct a similar review of NLP research, focusing on issues beyond payment (Shmueli et al., 2021). The authors find that whilst 10% of accepted papers at 3 major NLP conferences use crowdsourcing techniques, just 17% of these mention payment, and fewer refer to an IRB review (Shmueli et al., 2021). The paper notes that it is often unclear whether crowdworkers meet the definition of human subjects because of narrow criteria provided by US regulation (Shmueli et al., 2021). This definitional dilemma is supported by (Kaushik et al., 2022) who highlight the inconsistencies in how crowdwork is defined under research ethics regulations. These papers point to a need for further understanding of how AI researchers engage with research ethics issues.\n\n3 METHODOLOGY\n\nTo assess how AI research involving crowdworkers engages with research ethics, this paper conducts (1) policy analysis of publication venues and (2) paper analysis to assess how policies and norms translate to practice at major venues.\n\n3.1 POLICY ANALYSIS\n\nThe first portion of this study assesses the publication requirements of AI papers at leading venues. Google Scholar impact ratings show that conferences are the leading publication venues for AI research, and therefore the top two, the International Conference on Learning Representations (ICLR)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nand the Conference on Neural Information Processing (NeurIPS) are selected for analysis (Google Scholar, 2022). Journals are also common publication venues, and can act as a comparison point for policy analysis. Springer Nature, as a leading publisher with universal disclosure policies across academic disciplines and with various journals dedicated to AI, is selected to compare policy documentation (Springer, 2022; Torres-Salinas et al., 2013).\n\n3.2 PAPER ANALYSIS\n\nThe second component of this study bridges the gap between expectation and reality. In line with the policy analysis, ICLR, NeurIPS, and Springer Nature papers are assessed to understand how AI research papers adhere to the requirements prescribed by publishers. Papers were included within the study if they:\n\n(a) Utilised data generated by humans recruited via a crowdsourcing platform1; and (b) Collected human-generated data directly for the purposes of the study; and (c) Contributed to artificial intelligence research\n\nTo determine whether a paper met criteria, a full-text search was performed. ICLR papers are accessible via OpenReview, a peer-review portal, whilst NeurIPS papers were accessed through the NeurIPS site. Accepted papers were then analysed via a Python script using the PyPDF package. This code identified papers which may use crowdsourcing by searching for terms such as “Mechanical Turk”, “annotator” and “rater” (see Appendix A.3). Each paper identified was manually reviewed to determine whether the criteria for study was met. For Springer papers full text-search was available without additional coding via the Springer online portal. As Springer includes hundreds of journals, only papers within the Artificial Intelligence field referencing the use of MTurk, the most-used platform in the conference data analysis, were included to manage data collection. Psychology papers from Springer journals and papers from the Conference on Human Factors in Computing Systems (CHI) were also considered for one year as a benchmark of current best practices in (1) a well-established field with strong research ethics practices and (2) a conference where AI research frequently crosses into human participant research. Psychology has a long history of engagement with human participants and has similarly utilised crowdsourcing platforms in recent years, whilst CHI is arguably the leading conference sitting at the intersection of computer science and human behavioural studies (Buhrmester et al., 2018; Stewart et al., 2017).\n\nFor each paper analysed data collected included whether IRB (or equivalent) review was disclosed, whether payment terms were outlined, what type of task was undertaken, and whether any type of demographic data of workers was noted in the paper. For a full list of data points collected, see Appendix A.4.\n\n4 VENUE POLICY ANALYSIS\n\n4.1\n\nICLR\n\nICLR provided no requirements of authors related to research ethics until 2021, when a code of ethics was introduced. This code outlines principles such as “avoid harm”, and “respect privacy” which have direct implications to research involving crowdsourcing.\n\nThe conference explicitly notes the need for the disclosure of ethics review when considering the principle of upholding scientific excellence. The code states: “Where human subjects are involved in the research process (e.g., in direct experiments, or as annotators), the need for ethical approvals from an appropriate ethical review board should be assessed and reported” (ICLR, 2022). This is the only reference to human subjects within the code, but suggests that papers should report research ethics reviews, or disclose when research ethics reviews were deemed exempt by a review body. The code does not require papers to report on other issues such as consent, payment, or demographics. Reviewers are asked to raise potential violations of the ICLR Code of Ethics, and authors are encouraged to discuss ethics questions.\n\n1Where uncertainty exists over whether the study was conducted using a crowdsourcing platform, papers\n\nare included within analysis for completeness\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n4.2 NEURIPS\n\nNeurIPS, a year before the ICLR Code of Ethics, piloted an ethics review process (Ashurst et al., 2022). This process focused on an assessment of the “broader impacts” of research, asking researchers to include a statement on how the research might lead to beneficial or harmful outcomes to society. The guidance for these statements was limited, and did not explicitly require disclosures of IRBs, payment rates, or consent protocols.\n\nHowever, in 2021 NeurIPS replaced the requirement of an ethics statement with a checklist (Beygelzimer et al., 2021). When announcing the checklist, the program chairs stated that they aimed to “encourage best practices for responsible machine learning research, taking into consideration reproducibility, transparency, research ethics, and societal impact” (Beygelzimer et al., 2021). The checklist includes specific requests for disclosing information about crowdsourced workers or human subjects, asking the following:\n\n“(a) Did you include the full text of instructions given to participants and screenshots, if applicable? (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?” (NeurIPS, 2021)\n\nThis checklist shows an actionable approach to engaging with research ethics issues such as payment and ethics review, showing a clear interest in the impacts of research on crowdworkers.\n\n4.3 SPRINGER\n\nWhilst codes of ethics and requirements pertaining to research ethics are a relatively new phenomenon for AI conferences, this is not the case for journals released by established publishing houses. Publishers can have editorial policies that apply to all journals submitted, and this is the case for Springer, which publishes a number of AI journals.\n\nSpringer has a dedicated editorial policy to studies involving human subjects, stating that papers should: “include a statement that confirms that the study was approved (or granted exemption) by the appropriate institutional and/or national research ethics committee (including the name of the ethics committee) and certify that the study was performed in accordance with the ethical standards as laid down in the 1964 Declaration of Helsinki and its later amendments or comparable ethical standards” (Springer, 2022).\n\nThe policy goes on to state that any exemptions should be detailed within manuscripts, including the reasons for exemption. The publisher does not provide guidance on disclosure standards or research requirements beyond research ethics approvals, but this requirement applies to any journal submitted to Springer, meaning all papers which have engaged with an IRB should disclose this.\n\nThese policies demonstrate some deviation between the requirements of publishers across AI venues, whilst a broader analysis of publication policies across venues including ICML, AAAI, and CHI can be found in Appendix A.8, demonstrating that these policies are in-line with practices of other venues.\n\n5 PAPER ANALYSIS\n\nWhilst policy can inform how AI researchers are expected to engage with research ethics considerations, paper analysis enables an assessment of this engagement in practice.\n\n5.1 CONFERENCE ANALYSIS\n\nTable 1 show the number of papers which meet this study’s inclusion criteria for crowdsourcing from NeurIPS and ICLR. For both conferences the proportion of papers meeting this criteria is low, at between 2-3% for NeurIPS and 2-6% for ICLR each year. This is a small but significant number of papers, particularly considering many more papers use previously collected datasets which may have derived from crowdsourcing, but were considered out of scope for this study.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAnalysis also found that papers overwhelmingly utilised the MTurk platform compared with others. Over half of all papers using crowdsourcing at NeurIPS, and 65% at ICLR, state that MTurk was used for data collection, largely remaining consistent over the four years of data collection. Over 40% of papers at NeurIPS, and a third at ICLR, did not disclose a data collection platform, whilst the only other platform to be mentioned in more than one paper was Prolific, with four references. Full platform details can be found in the Appendix A.6.\n\nTable 1: Proportion of accepted papers meeting crowdsourcing criteria between 2018-2021\n\nYear NeurIPS Papers NeurIPS Crowdsourcing\n\nICLR Papers\n\nICLR Crowdsourcing\n\n2022 2021 2020 2019 2018 Total\n\nN/A - no data 2,331 1,896 1,426 1,009 6,662\n\nN/A 48 (2%) 34 (2%) 37 (3%) 24 (2%) 143 (2%)\n\n1,068 874 696 500 334 3,472\n\n24 (2%) 40 (5%) 23 (3%) 30 (6%) 11 (3%) 128 (4%)\n\n5.2 CONFERENCES: ETHICS DISCLOSURES\n\nTable 2 shows analysis of research ethics disclosures for NeurIPS. In 2018 and 2019 few papers disclosed research ethics considerations. However, in 2020 18% of papers discussed IRB review, and 12% disclosed payments. This increase may have resulted from the introduction of broader impact statements, explicitly asking authors to consider the societal implications of their research (Ashurst et al., 2022). In 2021 this system changed, and a checklist was introduced explicitly requesting authors to disclose payment terms and disclose IRB reviews (Beygelzimer et al., 2021). Whilst this had a huge impact on payment disclosures, this did not significantly increase other disclosures, indicating some, but limited, success of the checklist.\n\nTable 2: NeurIPS: Crowdsourcing papers’ research ethics disclosures between 2018-2021\n\nYear Crowdsourcing\n\nIRB\n\nPayment Consent Demographics\n\n2021 2020 2019 2018 Total\n\n48 34 37 24 143\n\n9 (19%) 6 (18%) 0 (0%) 0 (0%) 15 (10%)\n\n26 (54%) 4 (12%) 4 (11%) 1 (4%) 35 (24%)\n\n8 (17%) 3 (9%) 1 (3%) 0 (0%) 12 (8%)\n\n5 (10%) 4 (12%) 4 (11%) 0 (0%) 13 (9%)\n\nIn contrast, ICLR’s Code of Ethics, whilst also requesting IRB disclosures, makes fewer additional disclosure requirements for papers, and this can be seen from the results in Table 3. Across 2018 and 2019 none of the 41 papers which involved crowdsourcing data collection referenced any of the categories analysed. In 2020 this changed, with some IRB and payment disclosures. Following the introduction of the Code of Ethics disclosures have become slightly more common, but remain very infrequent.\n\n5.3\n\nJOURNAL COMPARISON\n\nThe data from NeurIPS and ICLR can be compared to papers and articles submitted to Springer journals to understand whether this is a unique issue to conferences. Table 4 outlines disclosures within AI papers which utilise MTurk for data collection between 2018 and 2021. Over the four years there has been a steady increase in research ethics disclosures suggesting a trend towards AI researchers taking a greater interest in ethics considerations when using crowdworkers.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: ICLR: Crowdsourcing papers’ research ethics disclosures between 2018-2021\n\nYear Crowdsourcing\n\nIRB\n\nPayment Consent Demographics\n\n2022 2021 2020 2019 2018 Total\n\n24 40 23 30 11 128\n\n3 (13%) 1 (3%) 2 (9%) 0 (0%) 0 (0%) 6 (5%)\n\n5 (21%) 5 (13%) 3 (13%) 0 (0%) 0 (0%) 13 (10%)\n\n2 (8%) 2 (5%) 0 (0%) 0 (0%) 0 (0%) 4 (3%)\n\n1 (4%) 3 (8%) 0 (0%) 0 (0%) 0 (0%) 4 (3%)\n\nTable 4: Springer Journals: MTurk papers’ research ethics disclosures between 2018-2021\n\nYear Crowdsourcing\n\nIRB\n\nPayment Consent Demographics\n\n2021 2020 2019 2018 Total\n\n76 47 66 86 275\n\n16 (21%) 6 (13%) 4 (6%) 4 (5%) 31 (11%)\n\n22 (29%) 19 (40%) 8 (12%) 12 (14%) 62 (22%)\n\n14 (18%) 6 (13%) 5 (8%) 4 (5%) 30 (11%)\n\n26 (34%) 16 (34%) 16 (24%) 11 (13%) 69 (25%)\n\n5.4 PSYCHOLOGY AND CHI COMPARISON\n\nWe can compare this data to a benchmark collected from an academic field with a history of research ethics considerations, Psychology, and a Computer Science venue with a history of human data collection, CHI. Table 5 outlines the results of data collection for Psychology papers within Springer journals and CHI papers which reference the use of MTurk. The results show that Psychology papers disclose research ethics considerations most frequently, whilst CHI papers are more likely to include these compared with other AI venues, particularly when considering payment and demographic information. This could indicate either a substantive difference in the nature of the data collection between venues, or a cultural divide.\n\nTable 5: Benchmark: Springer Psychology and CHI MTurk papers’ research ethics disclosures\n\nVenue\n\nYear\n\nPapers\n\nIRB\n\nPayment\n\nConsent\n\nDemographics\n\nICLR NeurIPS Springer AI CHI Springer Psych\n\n2018-22 2018-21 2018-21 2022 2021\n\n128 143 275 66 268\n\n6 (5%) 15 (10%) 30 (11%) 28 (42%) 193 (72%)\n\n13 (10%) 35 (24%) 61 (22%) 49 (74%) 173 (65%)\n\n4 (3%) 12 (8%) 29 (11%) 27 (41%) 194 (72%)\n\n4 (3%) 13 (9%) 69 (25%) 41 (62%) 240 (90%)\n\n5.5 TASK TYPE ANALYSIS\n\nTo explore this gap, the type of task undertaken within the research can be examined (see Table 6) to understand if disclosures differ depending on the type of engagement with crowdworkers (with task types defined in Section 2.1)2. 95% of Psychology papers analysed are classified as involving a ‘behaviour’ task, while around one in ten AI papers at AI conferences and a third of AI journal papers involve behaviour tasks. At AI conferences, evaluation tasks are most prominent, comprising 70% of all papers, whilst in journals this figure is only 39%. This may indicate that behaviour tasks are more likely to engage with research ethics issues.\n\n2Note categories are not mutually exclusive, with some studies employing crowdworkers for multiple task\n\ntypes.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Task type comparison across venues between 2018-2021\n\nVenue\n\nBehaviour\n\nEvaluation\n\nLabelling\n\nProduction\n\nAI: NeurIPS AI: ICLR AI: Springer CHI (2022 only) Psychology (2021 only)\n\n15% (22/143) 9% (9/104) 35% (95/275) 74% (49/66) 95% (254/268)\n\n70% (100/143) 70% (73/104) 39% (108/275) 20% (13/66) 2% (6/268)\n\n6% (8/143) 9% (9/104) 17% (48/275) 12% (8/66) 2% (5/268)\n\n11% (16/143) 16% (17/104) 9% (24/275) 3% (2/66) 1% (3/268)\n\nTable 7 explores this hypothesis, demonstrating that a gap appears to remain between the AI papers analysed and the Psychology field. However, NeurIPS disclosures are similar to those seen at CHI, except when considering payment and demographic data. The gap is most stark when considering ICLR behavioural papers, which rarely report on research ethics issues.\n\nTable 7: Disclosures of papers utilising behavioural tasks across venues between 2018-2021\n\nVenue\n\nBehaviour\n\nIRB\n\nPayment\n\nConsent\n\nDemographics\n\nAI: NeurIPS AI: ICLR AI: Springer CHI (2022 only) Psychology (2021 only)\n\n15% (22/143) 9% (9/104) 35% (95/275) 74% (49/66) 95% (254/268)\n\n45% (10/22) 0% (0/9) 24% (23/95) 47% (23/49) 73% (186/254)\n\n50% (11/22) 11% (1/9) 48% (46/95) 80% (39/49) 67% (170/254)\n\n45% (10/22) 22% (2/9) 23% (22/95) 45% (22/49) 74% (187/254)\n\n45% (10/22) 22% (2/9) 64% (61/95) 73% (36/49) 94% (239/254)\n\n5.6\n\nINSTITUTION TYPE ANALYSIS\n\nAnother explanation for this gap might be the types of institutions submitting papers, where private companies, who may be less familiar with research ethics process, could be less likely to engage with research ethics considerations. This hypothesis is explored in Appendix A.7, which demonstrates that this cannot be easily concluded, with disclosures across institutions in the papers analysed falling short of those seen at Psychology and CHI.\n\n6 KEY FINDINGS\n\n6.1 LEADING AI RESEARCH VENUES DO NOT ALIGN WITH TRADITIONAL RESEARCH ETHICS\n\nDISCLOSURE STANDARDS\n\nResearch ethics disclosures appear to be less common at leading AI research venues compared with Psychology and CHI. One might argue that this is due to the nature of the tasks being different, with Psychology research concerning the behaviour of participants. However, the gap persists in AI research involving behavioural tasks, whilst ethics issues are not exclusive to behavioural studies. This difference may result from how the definition of a ‘human subject’ is interpreted, with AI researchers unclear on when studies require engagement with research ethics review processes (Shmueli et al., 2021; Kaushik et al., 2022).\n\nThis gap may also exist because the AI field lacks the same history of engagement with human subjects, with recent crowdsourcing possibilities provoking greater interest in direct human engagement. Alternatively, this may result from a lack of research ethics education in Computer Science departments, meaning there is less focus on these concerns.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n6.2\n\nJOURNALS AND CONFERENCES HAVE POWER TO INFLUENCE ENGAGEMENT WITH RESEARCH ETHICS\n\nThe research ethics discrepancies noted are not equal across publication venues, with Springer and NeurIPS papers more frequently citing IRB reviews compared with ICLR papers. For Springer, this may be due to the varied nature of journals, many of which are related to AI’s impact on society (e.g. “AI Society” and “Artificial Intelligence in Education”). For NeurIPS, a drastic increase in reporting of IRB engagement and payment terms may be explained by changes in conference policy, with the 2021 introduction of the checklist. The lack of equivalent impact of the ICLR Code of Ethics may be due to the code providing less stringent stipulations.\n\n6.3 LEADING AI RESEARCH BREAKS WITH SCIENTIFIC TRADITION BY SCARCELY\n\nCONSIDERING DEMOGRAPHIC IMPACTS\n\nOne type of disclosure which stands out across AI papers relates to demographic data. Demographic data is frequently reported in social science studies (including the assessed Psychology papers), from a scientific integrity and reproducibility perspective, and because of research ethics considerations (Connelly, 2013; ”Nature”, ”2022”; Robinson et al., 2017). However, these disclosures were not seen in many of the AI papers analysed. Whilst demographics might not impact some studies (e.g. those involving objective labelling tasks), many AI research tasks involve some subjectivity, and the distinction between objective and subjective labelling is not always clean or self-evident. Demographics are frequently demonstrated to be an important influence on datasets in AI, and can have multiplicative effects as datasets are reused, locking in biases which can cause representational harms to those not adequately considered within a dataset (Paullada et al., 2020). This can be hard to identify and mitigate when datasets are re-used without demographic data, and means addressing these issues up front is critical.\n\nThe relative lack of disclosure may exist because demographic data is not collected for legal reasons or to avoid data which might be considered identifiable. However, this type of data is available for collection via crowdsourcing platforms, so this is a design choice from researchers, rather than an imposition. Studies may also argue that they do not require demographic diversity; for example if engaging in objective labelling tasks. However, labelling tasks account for 12% of papers analysed, and in many cases data enrichment tasks involve some subjectivity. Subjectivity is not the only reason to include demographic data; different demographic groups may be impacted differently by tasks, or data collected could be re-used by other actors in domains where demographic differences are impactful. Demographic reporting is an important aspect of best practices in research, and there is little excuse for the AI field to depart from this norm.\n\n7 CONCLUSION AND FUTURE WORK\n\nThis paper shows how AI researchers at leading venues engage with research ethics questions when employing crowdsourced workers, and illustrates the norms developing in the field. The work shows that research ethics disclosures are infrequent at leading research venues, whilst publication policies are emerging but inconsistently followed.\n\nThe gap demonstrated in this work between policies and practices of AI venues has been conducted with a relatively small pool of papers (owing to the content of published papers, not methodological limitations) and venues. Future work could extend this study to other venues, and should prompt further exploration of how AI data enrichment research should engage with research ethics norms which exist in other fields. While our findings are limited to the reviewed venues, they are nonetheless significant given the leading position of ICLR and NeurIPS and the role these venues play in setting research culture and norms across machine learning and AI research. This may result from ambiguity set by the regulatory requirements, which were designed for different fields and different types of work, or from a lack of experience in the AI field with this type of work. Future work could also further explore the motivations for the lack of demographic reporting, with this gap appearing consistent across the AI papers analysed, in stark contrast to research norms. With these directions in mind, this work hopes to encourage the field to move towards agreed ethics norms, fit for AI research and crowdwork, and consistently applied across publications.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAntonia Aloisi. Commoditized Workers. Case Study Research on Labour Law Issues Arising from a Set of ’On-Demand/Gig Economy’ Platforms. Comparative Labor Law Policy Journal, 37(3): 1354–1360, 2016. doi: http://dx.doi.org/10.2139/ssrn.2637485.\n\nCarolyn Ashurst, Markus Anderljung, Carina Prunkl, Jan Leike, Yarin Gal, Toby Shevlane, URL https://medium.com/@GovAI/\n\nand Allan Dafoe. a-guide-to-writing-the-neurips-impact-statement-4293b723f832.\n\nCode of ethics, 2022.\n\nNat ̃a M. Barbosa and Monchu Chen. Rehumanized crowdsourcing: A labeling framework adIn Proceedings of the 2019 CHI Conference on dressing bias and ethics in machine learning. Human Factors in Computing Systems, CHI ’19, pp. 1–12, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450359702. doi: 10.1145/3290605.3300773. URL https://doi.org/10.1145/3290605.3300773.\n\nHenry Beecher. Ethics and Clinical Research. New England Journal of Medicine, 274(24):\n\n1354–1360, 1966. doi: https://doi.org/10.1056/NEJM196606162742405.\n\nJoeran Beel, Stefan Langer, Andreas N ̈urnberger, and Marcel Genzmehr. The impact of demographics (age and gender) and other user-characteristics on evaluating recommender systems. volume 8092, pp. 396–400, 09 2013. ISBN 978-3-642-40500-6. doi: 10.1007/978-3-642-40501-3 45.\n\nRoukaya Benjelloun and Yassine Otheman. Psychological distress in a social media content moderator: A case report. Archives of Psychiatry and Mental Health, 2020. doi: doi:10.29328/journal. apmh.1001024.\n\nAlina Beygelzimer, Yann Dauphin, Percy Liang, and Jennifer Wortman Vaughan.\n\nIntroducing the neurips 2021 paper checklist, 2021. URL https://neuripsconf.medium.com/ introducing-the-neurips-2021-paper-checklist-3220d6df500b.\n\nMichael D. Buhrmester, Sanaz Talaifar, and Samuel D. Gosling. An evaluation of amazon’s mechanical turk, its rapid rise, and its effective use. Perspectives on Psychological Science, 13 (2):149–154, 2018. doi: 10.1177/1745691617706516. URL https://doi.org/10.1177/ 1745691617706516. PMID: 29928846.\n\nLeo Celi, Jacqueline Cellini, Marie Charpignon, Edward Dee, Franck Dernoncourt, Rene Eber, Julian Schirmer, Julia Situ, Joseph Alexander Paguio, Joel Park, Judy Wawira, Seth Yao, and William Mitchell. Sources of bias in artificial intelligence that perpetuate healthcare disparities-a global review. PLoS Medicine, e0000022, 03 2022. doi: 10.1371/journal.pdig.0000022.\n\nElizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. All that’s ‘human’ is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 7282–7296, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.565. URL https://aclanthology.org/2021.acl-long.565.\n\nCihan Cobanoglu, Muhittin Cavusoglu, and Gozde Turktarhan. A beginner’s guide and best practices for using crowdsourcing platforms for survey research: The case of Amazon Mechanical Turk (MTurk). Journal of Global Business Insights, 6(1):92–97, 2021. doi: https: //www.doi.org/10.5038/2640-6489.6.1.1177.\n\nLynne Connelly. Demographic data in research studies. Medsurg nursing : official journal of the\n\nAcademy of Medical-Surgical Nurses, 22:269–70, 10 2013.\n\nKate Crawford and Trevor Paglen. Excavating ai: the politics of images in machine learning training\n\nsets. AI SOCIETY, 06 2021. doi: 10.1007/s00146-021-01162-8.\n\nBrandon Dang, Martin J. Riedl, and Matthew Lease. Toward safer crowdsourced content modera-\n\ntion. CoRR, abs/1804.10999, 2018. URL http://arxiv.org/abs/1804.10999.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlexandre D ́efossez, Neil Zeghidour, Nicolas Usunier, L ́eon Bottou, and Francis R. Bach. SING: symbol-to-instrument neural generator. CoRR, abs/1810.09785, 2018. URL http://arxiv. org/abs/1810.09785.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale\n\nhierarchical image database. 2009. doi: 10.1109/CVPR.2009.5206848.\n\n”Mark D ́ıaz, Ian Kivlichan, Rachel Rosen, Dylan Baker, Razvan Amironesei, Vinodkumar Prabhakaran, and Emily Denton”. ACM title = CrowdWorkSheets: Accounting for Individual and Collective Identities Underlying Crowdsourced Dataset Annotation, booktitle = 2022 ACM Conference on Fairness, Accountability, and Transparency, jun 2022. doi: 10.1145/3531146.3534647. URL https://doi.org/10.1145%2F3531146.3534647.\n\nDjellel Difallah, Elena Filatova, and Panos Ipeirotis. Demographics and dynamics of mechanical turk workers. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM ’18, pp. 135–143, New York, NY, USA, 2018. Association for Computing ISBN 9781450355810. doi: 10.1145/3159652.3159661. URL https://doi. Machinery. org/10.1145/3159652.3159661.\n\nJohan Edstedt, Johan Karlsson, Francisca Benavente, Anette Novak, Amanda Berg, and Michael learning to predict harmfulness ratings from video. CoRR,\n\nFelsberg. abs/2106.08323, 2021. URL https://arxiv.org/abs/2106.08323.\n\nIs this harmful?\n\nEzekiel J. Emanuel. The Oxford Textbook of Clinical Research Ethics. Oxford University Press,\n\nOxford, 2008.\n\nFahmid Morshed Fahid, Jonathan P. Rowe, Randall D. Spain, Benjamin S. Goldberg, Robert Pokorny, and James Lester. Adaptively Scaffolding Cognitive Engagement with Batch Constrained Deep Q-Networks. Artificial Intelligence in Education, 337(20):113–124, 2021. doi: https://doi.org/10.1007/978-3-030-78292-4 10.\n\nGoogle Scholar.\n\nArtificial\n\nintelligence\n\ntop publications,\n\n2022.\n\nURL https:\n\n//scholar.google.co.uk/citations?view_op=top_venues&hl=en&vq= eng_artificialintelligence.\n\nChristine Grady. Institutional Review Boards. Chest, 148(5):1148–1155, 2015. doi: https://doi.org/\n\n10.1378/chest.15-0706.\n\nMary L. Gray and Siddharth Suri. Ghost work : how to stop Silicon Valley from building a new\n\nglobal underclass. Houghton Mifflin Harcourt, Boston, 2019.\n\nKotaro Hara, Abigail Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch, and Jeffrey P. Bigham. A data-driven analysis of workers’ earnings on amazon mechanical turk. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, CHI ’18, pp. 1–14, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450356206. doi: 10. 1145/3173574.3174023. URL https://doi.org/10.1145/3173574.3174023.\n\nJean Heller.\n\nSyphilis\n\nyears. syphilis-victims-in-us-study-went-untreated-for-40-years-syphilis. html.\n\n40 study went https://www.nytimes.com/1972/07/26/archives/\n\nuntreated\n\nvictims\n\nURL\n\nu.s.\n\nfor\n\nin\n\nDanula Hettiachchi and Jorge Goncalves. Towards effective crowd-powered online content modIn Proceedings of the 31st Australian Conference on Human-Computer-Interaction, eration. OZCHI’19, pp. 342–346, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450376969. doi: 10.1145/3369457.3369491. URL https://doi.org/10. 1145/3369457.3369491.\n\nJeff Howe. The Rise of Crowdsourcing? Wired, 2006. URL https://www.wired.com/\n\n2006/06/crowds/.\n\nICLR. Code of ethics, 2022. URL https://iclr.cc/public/CodeOfEthics.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nLilly Irani. The cultural work of microwork. New Media & Society, 17(5):720–739, 2015. doi: 10.1177/1461444813511926. URL https://doi.org/10.1177/1461444813511926.\n\nDivyansh Kaushik, Zachary C. Lipton, and Alex John London. Resolving the human subjects status of machine learning’s crowdworkers, 2022. URL https://arxiv.org/abs/2206. 04039.\n\nRaphael Koster, Jan Balaguer, Andrea Tacchetti, Ari Weinstein, Tina Zhu, Oliver Hauser, Duncan Williams, Lucy Campbell-Gillingham, Phoebe Thacker, Matthew M. Botvinick, and Christopher Summerfield. Human-centered mechanism design with democratic AI. CoRR, abs/2201.11441, 2022. URL https://arxiv.org/abs/2201.11441.\n\nRita Latikka, Nina Savela, Aki Koivula, and Atte Oksanen. Perceived robot attitudes of other people and perceived robot use self-efficacy as determinants of attitudes toward robots. In HumanComputer Interaction. Interaction Techniques and Novel Applications: Thematic Area, HCI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II, pp. 262–274, Berlin, Heidelberg, 2021. Springer-Verlag. ISBN 9783-030-78464-5. doi: 10.1007/978-3-030-78465-2 20. URL https://doi.org/10.1007/ 978-3-030-78465-2_20.\n\nFei-Fei Li. Where did imagenet come from?, 2019. URL https://www.youtube.com/\n\nwatch?v=Z7naK1uq1F8.\n\nStanley Milgram. Behavioral Study of obedience. The Journal of Abnormal and Social Psychology,\n\n67(4):371–378, 1963. doi: https://doi.org/10.1037/h0040525.\n\nShakir Mohamed, Marie-Therese Png, and William Isaac. Decolonial AI: decolonial theory as sociotechnical foresight in artificial intelligence. CoRR, abs/2007.04068, 2020. URL https: //arxiv.org/abs/2007.04068.\n\n”Nature”. ”nature journals raise the bar on sex and gender reporting in research”. ”60”(”396”),\n\n”2022”.\n\nAliide Naylor. cal Turk. underpaid-workers-are-being-forced-to-train-biased-ai-on-mechanical-turk.\n\nUnderpaid Workers Are Being Forced to Train Biased AI on MechaniURL https://www.vice.com/en/article/88apnv/\n\nVice, 2021.\n\nNeurIPS. Neurips 2021 paper checklist guidelines, 2021. URL https://neurips.cc/\n\nConferences/2021/PaperInformation/PaperChecklist.\n\nOffice for Human Research Protections.\n\n2018 requirements (2018 common rule), 2017. URL https://www.hhs.gov/ohrp/regulations-and-policy/regulations/ 45-cfr-46/revised-common-rule-regulatory-text/index.html.\n\nPartnership on AI.\n\nResponsible sourcing of data enrichment services, 2021.\n\nURL\n\npartnershiponai.org/responsible-sourcing.\n\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. Data and its (dis)contents: A survey of dataset development and use in machine learning research. CoRR, abs/2012.05345, 2020. URL https://arxiv.org/abs/2012.05345.\n\nPew.\n\nResearch\n\ncase https://www.pewresearch.org/internet/2016/07/11/ research-in-the-crowdsourcing-age-a-case-study/.\n\ncrowdsourcing\n\nage,\n\nthe\n\nin\n\na\n\nstudy.\n\nURL\n\nVinay Uday Prabhu and Abeba Birhane. Large image datasets: A pyrrhic win for computer vision?\n\nCoRR, abs/2006.16923, 2020. URL https://arxiv.org/abs/2006.16923.\n\nJune K. Robinson, Amy J. McMichael, and Claudia Hernandez. Transparent Reporting of Demographic Characteristics of Study Participants. JAMA Dermatology, 153(3):263–264, 2017. doi: https://doi.org/10.1001/jamadermatol.2016.5978.\n\nNeil J. Salkind. Encyclopedia of Research Design. SAGE Publications, California, 2010.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nSebastin Santy, Anku Rani, and Monojit Choudhury. Use of formal ethical reviews in NLP literature: Historical trends and current practices. CoRR, abs/2106.01105, 2021. URL https://arxiv. org/abs/2106.01105.\n\nHans-Martin Sass. Reichsrundschreiben 1931: Pre-Nuremberg German Regulations Concerning New Therapy and Human Experimentation. The Journal of Medicine and Philosophy: A Forum for Bioethics and Philosophy of Medicine, 8(2):99–111, 1983. doi: doi:10.1093/jmp/8.2.99.\n\nDaniel Schlagwein, Dubravka Cecez-Kecmanovic, and Benjamin Hanckel. Ethical norms and issues in crowdsourcing practices: A habermasian analysis. Information Systems Journal, 29(4): 811–837, 2019. doi: https://doi.org/10.1111/isj.12227. URL https://onlinelibrary. wiley.com/doi/abs/10.1111/isj.12227.\n\nTrebor Scholz. Uberworked and Underpaid: How Workers Are Disrupting the Digital Economy.\n\nPolity Press, Cambridge, 2016.\n\nBoaz Shmueli, Jan Fell, Soumya Ray, and Lun-Wei Ku. Beyond fair pay: Ethical implications of NLP crowdsourcing. CoRR, abs/2104.10097, 2021. URL https://arxiv.org/abs/ 2104.10097.\n\nEvelyn Shuster. Fifty Years Later: The Significance of the Nuremberg Code. New England Journal of Medicine, 337(20):1436–1440, 1997. doi: https://doi.org/10.1056/NEJM199711133372006.\n\nSlater.\n\nAnita ers. URL how-artificial-intelligence-depends-on-low-paid-workers.\n\nLow-Paid Workon https://tribunemag.co.uk/2021/05/\n\nIntelligence Depends\n\nHow Artificial\n\nTribune,\n\n2021.\n\nSpringer.\n\nEditorial policies,\n\n2022.\n\nURL https://www.springer.com/gp/\n\neditorial-policies/research-involving-human-and-or-animal-participants# c17769764.\n\nLuke Stark. The unintended ethics of Henry K Beecher. The Lancet, 387(10036):2374–2375, 2016.\n\ndoi: https://doi.org/10.1016/S0140-6736(16)30743-7.\n\nMiriah Steiger, Timir J Bharucha, Sukrit Venkatagiri, Martin J. Riedl, and Matthew Lease. The psychological well-being of content moderators: The emotional labor of commercial moderation In Proceedings of the 2021 CHI Conference on Human and avenues for improving support. Factors in Computing Systems, CHI ’21, New York, NY, USA, 2021. Association for Computing ISBN 9781450380966. doi: 10.1145/3411764.3445092. URL https://doi. Machinery. org/10.1145/3411764.3445092.\n\nNeil Stewart, Jesse Chandler, and Gabriele Paolacci. Crowdsourcing samples in cognitive science.\n\nTrends in Cognitive Sciences, 21, 08 2017. doi: 10.1016/j.tics.2017.06.007.\n\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize from human feedback. CoRR, abs/2009.01325, 2020. URL https://arxiv.org/abs/2009.01325.\n\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. CoRR, abs/1811.00937, 2018. URL http://arxiv.org/abs/1811.00937.\n\nHumphrey Taylor. Does Internet Research Work?. International Journal of Market Research, 42(1):\n\n1–11, 2000. doi: https://doi.org/10.1177/147078530004200104.\n\nChristopher Thomas and Adriana Kovashka. Predicting the politics of an image using webly supervised data. CoRR, abs/1911.00147, 2019. URL http://arxiv.org/abs/1911.00147.\n\nDaniel Torres-Salinas, Nicol ́as Robinson-Garc ́ıa, Juan Miguel Campanario, and Emilio Delgado L ́opez-C ́ozar. Coverage, field specialization and impact of scientific publishers indexed in the ’book citation index’. CoRR, abs/1312.2791, 2013. URL http://arxiv.org/abs/1312. 2791.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nCarlos Toxtli, Siddharth Suri, and Saiph Savage. Quantifying the invisible labor in crowd work.\n\nCoRR, abs/2110.00169, 2021. URL https://arxiv.org/abs/2110.00169.\n\nJennifer Wortman Vaughan. Making better use of the crowd: How crowdsourcing can advance machine learning research. Journal of Machine Learning Research, 18(193):1–46, 2018. URL http://jmlr.org/papers/v18/17-234.html.\n\n”Sandra Wachter, Brent Daniel Mittelstadt, and Chris Russell”. Bias preservation in machine learning: The legality of fairness metrics under eu non-discrimination law. SSRN Electronic Journal, 2021.\n\n”Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang”. Challenges in detoxifying language models. CoRR, abs/2109.07445, 2021. URL https://arxiv.org/ abs/2109.07445.\n\nVanessa Williamson. On the Ethics of Crowdsourced Research. PS: Political Science Politics, 49\n\n(1):77–81, 2016. doi: https://doi.org/10.1017/S104909651500116X.\n\nPhilip Zimbardo. The stanford prison experiment: A simulation study of the psychology of impris-\n\nonment conducted august 1971 at stanford university, 1972.\n\nMichael Zimmer and Katharina Kinder-Kurlanda. Internet Research Ethics for the Social Age. Peter\n\nLang Publishing, New York, 2017.\n\nJonathan Zittrain. The Internet Creates a New Kind of Sweatshop. Newsweek, 2009. URL https: //www.newsweek.com/internet-creates-new-kind-sweatshop-75751#: ̃:text=The%20Internet%20has%20created%20new,a%20data%20center% 20in%20Kenya.\n\nA APPENDIX\n\nA.1 CODE\n\nThe code used in order to access and search NeurIPS and ICLR papers for this project is available on Github: [link removed for anonymity].\n\nA.2 RESEARCH ETHICS STATEMENT\n\nThis project was reviewed and approved by the institutions in accordance with the procedures laid down by the institution for ethical approval of all research involving human participants, reference number [redacted for anonymity].\n\nA.3 SEARCH TERMS\n\nThe below terms were used within a free-text search to identify papers which involved crowdsourced workers at the NeurIPS and ICLR conferences:\n\n‘mechanical turk’, ’mturk’, ’prolific’, ’crowd’, ’rater’, ’annotator’, ’participant’, ’amt’, ’labeller’, ’labeler’, ’figure eight’.\n\nFigure Eight and Prolific were included to identify whether platforms other than MTurk were common in AI research, and saw different practices. Only 4 papers citing Prolific were identified, and zero citing Figure Eight, limiting this exploration.\n\nThe below terms were used within a free-text search on the Springer platform to identify Springer AI and Psychology papers which involved the use of Mechanical Turk workers:\n\n‘Mechanical turk’, ‘mturk’\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA.4 DATA COLLECTION CRITERIA\n\nThe below categories of data were collected for each paper examined in this study, with sub-bullets outlining the justification for data collection.\n\n1. Is IRB or equivalent process mentioned (including disclosure of exemptions)? (Yes/No)\n\n• Disclosure of IRB review is a norm for human subjects research, per the Declaration\n\nof Helsinki, with IRBs ensuring the welfare of subjects in research.\n\n2. Are payment terms for workers disclosed? (Yes/No)\n\nPayment is a key issue for crowdworkers within and beyond research, raising ethical and legal concerns (Felstiner, 2011).\n\n3. Was worker consent discussed in the paper? (Yes/No)\n\n• Participant informed consent is a key facet of research ethics, as per the Declaration of Helsinki and Belmont Report (National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research, 1978; World Medical Association, 2009).\n\n4. What\n\ntype\n\nof\n\ndata\n\ncollection\n\ndid\n\nthis\n\nwork\n\ninvolve?\n\n(Be-\n\nhaviour/Evaluation/Labelling/Production)\n\n• Provides data on the types of tasks which AI research engages in to determine whether ethics disclosures differ between task types. Task definitions align with those described in Section 2 (Shmueli et al., 2021; Vaughan, 2018). These categories are not mutually exclusive, with some studies engaged in multiple types of tasks.\n\n5. Were worker demographics disclosed in the paper? (Yes/No)\n\n• The demographics of workers can impact the outcomes of research, with research ethics reviews considering issues of representation in participant samples. This may be of particular importance in AI research, with various examples of biased data leading to unequal outcomes in AI systems (Mehrabi et al., 2022; Paullada et al., 2021).\n\n6. What location was the institution of the lead author based in?\n\n• Provides insight on whether disclosures differ across geography, following the\n\nmethodology outlined in Santy et al. (Santy et al., 2021).\n\n7. In what\n\ntype of\n\ninstitution(s) were the authors of\n\nthe paper based?\n\n(Univer-\n\nsity/Industry/State/Joint)\n\n• Provide insight on whether disclosures differ between private, academic, and state institutions, following methodology outlined in Santy et al. (Santy et al., 2021). State institutions include state-run research labs (e.g. military research bodies).\n\n8. If disclosed, which crowdsourcing platform was used to collect data?\n\n• Identifies which platforms are most prominently used, and may identify variance in\n\npractices between platforms.\n\nA.5 GEOGRAPHIC DISTRIBUTION OF PAPERS ACROSS VENUES\n\nThe table below demonstrates the total number of AI papers assessed in this work across geographies and venues. As shown, the first authors from over half of the papers identified as using crowdsourcing across the venues derived from the US, with 13 percent from the European Union, and 9 percent from China.\n\nThis provides context to the results that follow and shows a heavy US-bias to these venues, and the study as a whole.\n\nA.6 PLATFORM USE\n\nTables below show the breakdown of platforms used across ICLR and NeurIPS papers meeting study criteria.\n\nThe table demonstrates the overwhelming reliance on MTurk for crowdsourced data, and frequency of papers choosing not to disclose platforms used.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Geographic distribution of AI crowdsourcing papers across publication venue between 2018 and 2021\n\nGeography\n\nNeurIPS\n\nICLR\n\nSpringer AI\n\nTotal\n\nUnited States European Union China United Kingdom Canada South Korea Switzerland Rest of World Total\n\n86 (60%) 11 (8%) 17 (12%) 6 (4%) 2 (1%) 6 (4%) 4 (3%) 11 (8%) 143 (100%)\n\n69 (66%) 4 (4%) 11 (11%) 3 (3%) 5 (5%) 6 (6%) 0 (0%) 6 (6%) 104 (100%)\n\n131 (48%) 52 (19%) 21 (8%) 10 (4%) 11 (4%) 4 (1%) 9 (3%) 37 (13%) 275 (100%)\n\n286 (55%) 67 (13%) 49 (9%) 19 (4%) 18 (3%) 16 (3%) 13 (2%) 54 (10%) 522 (100%)\n\nTable 9: NeurIPS: Crowdsourcing papers’ platform use between 2018-2021\n\nYear MTurk\n\nOther Unknown\n\n2021 2020 2019 2018 Total\n\n23 (48%) 14 (41%) 25 (68%) 15 (63%) 77 (54%)\n\n4 (8%) 0 (0%) 1 (3%) 0 (0%) 5 (3%)\n\n21 (44%) 20 (59%) 11 (30%) 9 (38%) 61 (43%)\n\nA.7\n\nINSTITUTION ANALYSIS\n\nTable 11 analyses disclosures from ICLR, NeurIPS and Springer AI papers across different institution types. ”Joint” indicates that co-authors on a paper represent multiple types of institution.\n\nA.8 VENUE POLICY ANALYSIS\n\nTable 12 table outlines the disclosure requirement of venues assessed in this paper, plus other major AI venues, ICML and AAAI. These two venues were added as the next most influential research venues per Google Metrics.\n\n*CHI publication policy is set by the Association for Computing Machinery (ACM), with the conference adhering to this code of conduct which includes IRB requirements. **ICML publication policy advises authors to follow the NeurIPS Code of Ethics. See: https://icml.cc/Conferences/2022/PublicationEthics\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 10: ICLR: Crowdsourcing papers’ platform use between 2018-2021\n\nYear MTurk\n\nOther Unknown\n\n2022 2021 2020 2019 2018 Total\n\n14 (58%) 26 (65%) 15 (65%) 18 (60%) 9 (82%) 68 (65%)\n\n0 (0%) 1 (3%) 0 (0%) 0 (0%) 0 (0%) 1 (1%)\n\n10 (42%) 13 (33%) 8 (35%) 12 (40%) 2 (18%) 35 (34%)\n\nTable 11: Institution type comparison between 2018-2022 for ICLR, NeurIPS and Springer AI Papers\n\nInstitution Crowdsourcing\n\nIRB\n\nPayment\n\nConsent Demographics\n\nUniversity Industry State Joint Total\n\n307 (56%) 47 (9%) 4 (1%) 188 (34%) 546 (100%)\n\n42 (14%) 1 (2%) 0 (0%) 5 (3%) 48 (9%)\n\n78 (25%) 3 (6%) 0 (%) 24 (13%) 105 (19%)\n\n33 (11%) 1 (2%) 0 (0%) 8 (4%) 42 (8%)\n\n62 (20%) 3 (6%) 0 (0%) 18 (10%) 83 (15%)\n\nTable 12: Comparison of venue policy requirements across venues considered within this paper, plus other major AI conferences\n\nVenue\n\nCode of Conduct? Author Checklist?\n\nIRB required? Payment Disclosure Required?\n\nNeurIPS ICLR Springer CHI ICML AAAI\n\nYes Yes Yes Yes* Yes** Yes\n\nYes No Journal specific No No No\n\nYes No Yes Yes No No\n\nYes No No No No No\n\n17",
    "reference": "# Summary Of The Paper\n\nThe paper presents a study of how ethics considerations related to crowdsourcing are disclosed in the AI community compared to psychology. The study includes ICLR, NeurIPS and Springer journals, which is a reasonable representative sample of AI publications. The primary findings are that AI lags behind psychology in ethics disclosures, but has improved greatly since 2018 particularly at NeurIPS. The paper is unique, important and very clearly written.\n\n# Strength And Weaknesses\n\nStrengths\n\nThe paper tackles a very timely topic with a thorough, well-designed study of the AI community in comparison to psychology. Choosing psychology as a baseline is well founded, as it performs much HSR and has an established practice of rigorous disclosure requirements in its publications.\n\nThe paper reveals important and clear differences between ICLR and NeurIPS, largely stemming from NeurIPS adoption of an explicit checklist of ethics considerations for authors and reviewers. While ICLR has a similar ethics policy as NeurIPS, the introduction of explicit mechanisms to state and review compliance appears to make a huge difference.\n\nSection 6.3 makes an excellent point about the lack of consideration and disclosure of demographic data in AI. This gap has become a source of mistrust of the AI community by the general public in high-profile cases regarding bias in facial recognition systems, for example.\n\nWeaknesses\n\nThe paper has few weaknesses, and these are minor points.\n\nIn 4.1, it would be clearer to state that ICLR does not have an explicit ethics criterion as part of its review process. The reality is that many or most authors and reviewers do not read conference policy documents, nor are they necessarily aware when these are changed. However authors and reviewers will pay much attention to requirements that are encoded into the submission and review processes, such as a checklist that must be filled out, and associated text.\n\nThere is no general discussion of ethics as a review criterion for conference papers, but this is a major recent development that should lead to greater enforcement of ethics policy. It would strengthen the paper to include a table of major AI conferences vs. ethics policies and how they are enforced, even for those that are not included in the detailed study such as CVPR, ICML, AAAI.\n\nThe finding that conference papers “overwhelmingly utilised the MTurk platform compared with others”, sec. 5.1, could be biased by the nature of the paper selection criteria, which searched for MTurk explicitly. What about papers that leveraged labeling service companies, which are increasingly abundant and often comparable in price to MT?\n\nIt would be helpful to add a summary table in sec. 5.4 with comparative numbers between the two conferences and the journal so that the reader does not have to skip back and forth between tables on different pages in order to see their differences.\n\nA major difference between AI and psychology research is the much higher proportion requiring IRB review in the latter, as shown in Table 5. This difference could explain virtually all of the other differences, as IRB review forces researchers to consider many criteria such as potential harm to participants that may not be considered thoroughly otherwise. Data labeling tasks are usually not HSR in the US, and many researchers (correctly) do not seek IRB review for them. Considering only behavioral tasks, table 7, is an insightful way to break down the data as such tasks are more likely to require IRB review. However, I don’t think the data supports the statement that “AI behaviour studies still do not meet the same standards as Psychology experiments” for NeurIPS. IRB review was reported for 45% vs. 73% in psychology, but this could easily reflect a true difference in the nature of the behavioral science being conducted. For ICLR and AI in general the statement is more supportable.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is very clear, easy to read, and compelling. The arguments and studies are laid out in a narrative structure that is easy to follow.\n\nWhile there is no algorithm proposed, the paper is very novel and unique. Papers about our research processes, rather than the research itself, are rare but often the most important.\n\n# Summary Of The Review\n\nThis paper is important and would likely generate much discussion and debate at the conference. The study could be deeper by considering more conferences, but in its current form it is sufficiently justified and complete to warrant publication.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nFORCES ARE NOT ENOUGH: BENCHMARK AND CRITICAL EVALUATION FOR MACHINE LEARNING FORCE FIELDS WITH MOLECULAR SIMULATIONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nMolecular dynamics (MD) simulation techniques are widely used for various natural science applications. Increasingly, machine learning (ML) force field (FF) models begin to replace ab-initio simulations by predicting forces directly from atomic structures. Despite significant progress in this area, such techniques are primarily benchmarked by their force/energy prediction errors, even though the practical use case would be to produce realistic MD trajectories. We aim to fill this gap by introducing a novel benchmark suite for ML MD simulation. We curate representative MD systems, including water, organic molecules, peptide, and materials, and design evaluation metrics corresponding to the scientific objectives of respective systems. We benchmark a collection of state-of-the-art (SOTA) ML FF models and illustrate, in particular, how the commonly benchmarked force accuracy is not well aligned with relevant simulation metrics. We demonstrate when and how selected SOTA methods fail, along with offering directions for further improvement. Specifically, we identify stability as a key metric for ML models to improve. Our benchmark suite comes with a comprehensive open source codebase for training and simulation with ML FFs to facilitate further work.\n\n1\n\nINTRODUCTION\n\nMolecular Dynamics (MD) simulations provide atomistic insights into physical phenomena in materials and biological systems. Such simulations are typically based on force fields (FFs) that characterize the underlying potential energy surface (PES) of the system and then use Newtonian forces to simulate long trajectories (Frenkel & Smit, 2001). The PES itself is challenging to compute and would ideally be done through quantum chemistry which is computationally expensive. Traditionally, the alternative has been parameterized force fields that are surrogate models built from empirically chosen functional forms (Halgren, 1996). Recently, machine learning (ML) force fields (Unke et al., 2021b) have shown promise to accelerate MD simulations by orders of magnitude while being quantum chemically accurate. The evidence supporting the utility of ML FFs is often based on their accuracy in reconstituting forces across test cases (Faber et al., 2017). The evaluations invariably do not involve simulations. However, we show that force accuracy alone does not suffice for effective simulation (Figure 1).\n\nFigure 1: Results on water10k. Models sorted by force mean absolute error (MAE) in descending order. High stability and low RDF MAE are better. Performance in force error does not align with simulationbased metrics.\n\nMD simulation not only describes microscopic details on how the system evolves, but also entails macroscopic observables that characterize system properties. Calculating meaningful observables often requires long simulations to sample the underlying equilibrium distribution. These observables are designed to be predictive of material properties such as diffusivity in electrolyte materials (Webb et al., 2015), and reveal detailed physical mechanisms, such as the folding kinetics of protein dynamics (Lane et al., 2011). Although these observables are critical products of MD simulations, systematic evaluations have not been sufficiently studied in existing\n\n1\n\nSphereNetForceNetSchNetDeepPot-SEPaiNNNequIPDimeNetGemNet-dTGemNet-TForce MAERDF MAEStabilityUnder review as a conference paper at ICLR 2023\n\nliterature. To gain insight into the performance of existing models in a simulation setting, we propose a series of simulation-based benchmark protocols. Compared to the popular multistep prediction task in the learned simulator community (Sanchez-Gonzalez et al., 2020), MD observables focus on distributional quantities. The exact recovery of the trajectories given the initial conditions is not the ultimate goal.\n\nEvaluating learned models through MD simulations requires careful design over the selection of systems, the simulation protocol, and the evaluation metrics: (1) A benchmark suite should cover diverse and representative systems to reflect the various challenges in different MD applications. (2) Simulations can be computationally expensive. An ideal benchmark needs to balance the cost of evaluation and the complexity of the system so that meaningful metrics can be obtained with reasonable time and computation. (3) Selected systems should be well studied in the simulation domain, and chosen metrics should characterize the system’s important degrees of freedom or geometric features.\n\nAre current state-of-the-art (SOTA) ML FFs capable of simulating a variety of MD systems? What might cause a model to fail in simulations? In this paper, our aim is to answer these questions with a novel benchmark study. The contributions of this paper include:\n\n• We introduce a novel benchmark suite for ML MD simulation with simulation protocols and quantitative metrics. We perform extensive experiments to benchmark a collection of SOTA ML models. We provide a complete codebase for training and simulating MD with ML FFs to lower the barrier to entry and facilitate future work.\n\n• We show that many existing models are inadequate when they are evaluated on simulationbased benchmarks, even when they show accurate force prediction (as shown in Figure 1).\n\n• By performing and analyzing MD simulations, we summarize common failure modes and\n\ndiscuss the causes and potential solutions to motivate future research.\n\n2 RELATED WORK\n\nML force fields learn the potential energy surface (PES) from the data by applying expressive regressors such as kernel methods (Chmiela et al., 2017) and neural networks on symmetry-preserving representations of atomic environments (Behler & Parrinello, 2007; Khorshidi & Peterson, 2016; Smith et al., 2017; Artrith et al., 2017; Unke & Meuwly, 2018; Zhang et al., 2018b;a; Kovács et al., 2021; Thölke & De Fabritiis, 2021; Takamoto et al., 2022). Recently, graph neural network architectures (Gilmer et al., 2017; Schütt et al., 2017; Gasteiger et al., 2020; Liu et al., 2021) have gained popularity as they provide a systematic strategy for building many-body correlation functions to capture highly complex PES. In particular, equivariant representations have been shown powerful in representing atomic environments (Satorras et al., 2021; Thomas et al., 2018; Qiao et al., 2021; Schütt et al., 2021; Batzner et al., 2022; Gasteiger et al., 2021; Liao & Smidt, 2022), leading to significant improvements in benchmarks such as MD17 and OC22/20. Some works presented simulation-based results (Unke et al., 2021a; Park et al., 2021; Batzner et al., 2022; Musaelian et al., 2022) but do not compare different models with simulation-based metrics.\n\nExisting benchmarks for ML force fields (Ramakrishnan et al., 2014; Chmiela et al., 2017) mostly focus on force/energy prediction, with small molecules being the most typical systems. The catalystfocused OC20 (Chanussot et al., 2021) and OC22 (Tran et al., 2022) benchmarks focus on structural relaxation with force computations, where force prediction is part of the evaluation metrics. The structural relaxation benchmark involves relaxation process of around hundreds of steps, and the goal is to predict the final relaxed structure/energy. These tasks do not characterize system properties under a structural ensemble, which requires simulations that are millions of steps long. Several recent works (Rosenberger et al., 2021) have also studied the utility of certain ML FFs in MD simulations. In particular, Stocker et al. 2022 uses GemNet (Gasteiger et al., 2021) to simulate small molecules in the QM7-x (Hoja et al., 2021) dataset, with a focus on simulation stability. Zhai et al. 2022 applies the DeepMD (Zhang et al., 2018a) architecture to simulate water and demonstrates its shortcoming in generalization across different phases. However, existing works focus on a single system and model, without proposing evaluation protocols and quantitative metrics for model comparison. Systematic benchmarks for simulation-based metrics are lacking in the existing literature, which obscures the challenges in applying ML FF for MD applications.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Visualization of the benchmarked systems. (a) MD17 molecules: Aspirin, Ethanol, Naphthalene, and Salicylic acid. (b) 64 water molecules. (c) 512 water molecules. (d) Alanine dipeptide. (e) LiPS.\n\nTable 1: Dataset summary. PBC stands for periodic boundary conditions. *Simulation of alanine dipeptide uses Metadynamics with implicit solvation.\n\nDataset\n\nSystem Type\n\nPBC #Atoms\n\nSimulation Length\n\nObjective\n\nMD17 Water Water-large Alanine dipeptide LiPS\n\nSmall molecule liquid liquid Peptide solid-state materials\n\n✗ ✓\n✓ ✗\n✓\n\n9-21 192 1536 22 83\n\n300 ps (600k steps) 500 ps (500k steps) 150 ps (150k steps) 5 ns (2.5M steps)* 50 ps (200k steps)\n\nInteratomic distances RDF, Diffusivity RDF, Diffusivity Dihedral angle analysis RDF, Diffusivity\n\n3 PRELIMINARIES\n\nTraining. An ML FF aims to learn the potential energy surface ˆE(x) ∈ R as a function of atomic coordinates x ∈ RN ×3 (N is the number of atoms), by fitting atom-wise forces ˆF (x) and energies from a training dataset:{xi, Fi, Ei}Ndata i=1 , where xi ∈ RN ×3, F ∈ RN ×3, E ∈ R. For evaluation, the test force prediction accuracy is used as a proxy to quantify the quality of the learned PES. The force field learning protocol has been well established (Unke et al., 2021b).\n\nMD simulation. Simulating molecular behaviors requires integrating a Newtonian equation of motion with forces obtained by differentiating ˆE(x): F (x) = −∂ ˆE(x)/∂x. To mimic desired thermodynamic conditions, an appropriate thermostat and barostat are chosen to augment the equation of motion with extended variables. The simulation produces a time series of positions: {xt ∈ RN ×3}T t=0, where t is the temporal order index, and T is the total simulation steps. To evaluate the simulation quality, we propose metrics based-on well-established observables in the respective types of systems. Definitions of benchmarked observables are in Appendix A.\n\n4 DATASETS AND METRICS\n\nPopular benchmark datasets, such as MD17, focus on the force prediction task for gas-phase small molecules. However, successes in these tasks are not sufficient evidence for (1) capturing complex interatomic interactions that are critical for condensed phase systems; and (2) recovery of critical simulation observables that cannot be directly indicated by force prediction accuracy. This work focuses on atomic-level MD simulations that manifest complex intermolecular interactions at multiple scales. We choose systems that (1) have been frequently used in force field development (Henderson, 1974); (2) cover diverse MD applications such as materials and biology; and (3) can be simulated within resonable time and compute. Beyond force predictions, we conduct simulations and benchmark observables that reflect the actual simulation quality, along with stability and computational efficiency. The selected systems are summarized in Table 1.\n\nQuantifying simulation stability. ML FFs can produce unstable dynamics, as the learned force field may not extrapolate robustly to the undersampled configuration space. As a result, the trajectories can enter nonphysical states that are not meaningful for observable calculations. Therefore, we closely monitor how much the simulated structure deviates from the physical configurations, with the radial distribution function (RDF) for condensed phase systems and bond lengths for flexible molecules. We say that a simulation becomes “unstable” when the deviation exceeds a threshold, which implies sampling of highly nonphysical structures. We then use the time duration for which a model remains stable in simulations to measure its stability. The ensemble statistics are only computed over the stable part of the simulated trajectories: the simulation trajectory before the first occurrence of instability. Details on the stability criterion are included in Appendix A.\n\n3\n\n(a)(b)(d)(e)(c)Under review as a conference paper at ICLR 2023\n\nTable 2: Models benchmarked in this work. The translation/rotation symmetries are respected by the feature representation at every layer. Number of parameters on the MD17 dataset are reported.\n\nModel\n\nSymmetry Principle of Geometric Features Energy Conservation\n\n#Parameters\n\nDeepPot-SE (Zhang et al., 2018b) SchNet (Schütt et al., 2017) DimeNet (Gasteiger et al., 2020) PaiNN (Schütt et al., 2021) SphereNet (Liu et al., 2021) ForceNet (Hu et al., 2021) GemNet-T (Gasteiger et al., 2021) GemNet-dT (Gasteiger et al., 2021) NequIP (Batzner et al., 2022)\n\nE(3)-invariant E(3)-invariant E(3)-invariant SE(3)-equivariant E(3)-invariant Translation-invariant E(3)-invariant SE(3)-equivariant E(3)-equivariant\n\n✓ ✓\n✓ ✓\n✓ ✗\n✓ ✗\n✓\n\n1.04M 0.12M 2.1M 0.59M 1.89M 11.37M 1.89M 2.31M 1.05M\n\nMD17 (Chmiela et al., 2017) dataset contains AIMD calculations for eight small organic molecules and is widely used as a force prediction benchmark for ML FFs. We adopt four molecules from MD17 and benchmark the simulation performance. In addition to force error, we evaluate the stability and the distribution of interatomic distances h(r). For each molecule, we randomly sample 9,500 configurations for training and 500 for validation from the MD17 database. We randomly sample 10,000 configurations from the rest of the data for force error evaluation. We perform five simulations of 300 ps for each model/molecule by initializing from 5 randomly sampled testing configurations, with a time step of 0.5 fs, at 500 K temperature, under a Nosé–Hoover thermostat.\n\nWater is arguably the most important molecular fluid in biological and chemical processes. Due to its complex thermodynamic and phase behavior, it poses great challenges for molecular simulations. In addition to force error, we evaluate simulation stability and recovery of both equilibrium statistics and dynamical statistics, namely the element-conditioned RDFs and liquid diffusion coefficient. Our dataset consists of 100,000 structures collected every 10 fs from a 1 ns trajectory sampled at equilibrium and a temperature of 300 K. We benchmarked all models with various training+validation dataset sizes (1k/10k/90k randomly sampled structures) and used the remaining 10,000 structures for testing. We performed 5 simulations of 500 ps by initializing from 5 randomly sampled testing configurations, with a time step of 1 fs, at 300 K temperature, under a Nosé–Hoover thermostat. We additionally evaluate model generalization to a larger system of 512 water molecules with 5 simulations of 150-ps.\n\nAlanine dipeptide features multiple metastable states, making it a classic benchmark for the development of MD sampling methods and force field development (Head-Gordon et al., 1989; Kaminski et al., 2001). Its geometric flexibility is well represented by the central dihedral (torsional) angles φ and ψ. Our reference data are obtained from simulations with explicit water molecules, with detailed protocols described in Appendix B. For faster simulation, we learn an implicitly solvated FF following a protocol similar to Chen et al. (2021). Our task is more challenging in that it aims to learn the implicitly solvated atomistic FF rather than the implicit solvation correction in Chen et al. (2021). To facilitate accelerated sampling, we apply metadynamics with φ and ψ as the collective variables. We evaluate force prediction, simulation stability, and free energy surface (FES) reconstruction F (φ, ψ). Our dataset consists of 50,000 structures dumped every 2 ps from a 100 ns trajectory at a temperature of 300 K. We used 38,000 randomly sampled structures for training, 2,000 for validation, and the rest as a test set. We performed 6 simulations of 5 ns by initializing from six local minima on the FES (Figure 5) with a time step of 2 fs at 300 K, and under a Langevin thermostat to mimic random noise from solvation effects. More information on our simulation protocols can be found in Appendix B.\n\nLiPS is a crystalline superionic lithium conductor relevant to battery development and a representative system for MD simulation usage in studying kinetic properties in materials. We adopt this dataset from Batzner et al. 2022, and benchmark all models on their force error, stability, RDF recovery, and Li-ion diffusivity coefficient. The dataset has 25,000 structures in total, from which we use 19,000 randomly sampled structures for training, 1,000 structures for validation, and the rest for computing force error. We conduct 5 simulations of 50 ps by initializing from 5 randomly sampled testing configurations, with a time step of 0.25 fs, at 520 K temperature, under a Nosé–Hoover thermostat.\n\n5 EXPERIMENTS\n\nBenchmarked models. We adopt the Open Catalyst Project implementation of SchNet (Schütt et al., 2017), DimeNet (Gasteiger et al., 2020), ForceNet (Hu et al., 2021), PaiNN (Schütt et al.,\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Results on MD17. Darker green color indicates better performance. For all results, force MAE is reported in the unit of [meV/Å], and stability is reported in the unit of [ps]. The distribution of interatomic distances h(r) MAE is unitless. FPS stands for frames per second. For all metrics (↓) indicates the lower the better, and (↑) indicates the higher the better. Standard deviation from 5 simulations is in subscript for applicable metrics.\n\nModel\n\nDeepPot-SE SchNet\n\nDimeNet\n\nPaiNN\n\nSphereNet\n\nForceNet\n\nGemNet-T GemNet-dT NequIP\n\nMolecule\n\nAspirin\n\nEthanol\n\nNaphthalene\n\nSalicylic Acid\n\nForce (↓) Stability (↑) h(r) (↓) FPS (↑) Force Stability h(r) FPS Force Stability h(r) FPS Force Stability h(r) FPS\n\n21.0 9(15) 0.65(0.47) 88.0 8.9 300(0) 0.09(0.00) 101.0 13.4 246(109) 0.11(0.00) 109.3 14.9 300(0) 0.03(0.00) 94.6\n\n35.6 26(23) 0.36(0.57) 108.9 16.8 247(106) 0.21(0.11) 112.6 22.5 18(2) 0.09(0.00) 110.9 26.3 300(0) 0.03(0.00) 111.7\n\n10.0 54(12) 0.04(0.00) 20.6 4.2 26(10) 0.15(0.03) 21.4 5.7 85(68) 0.10(0.01) 19.1 9.6 73(82) 0.06(0.02) 19.4\n\n9.2 159(121) 0.04(0.01) 85.8 5.0 86(109) 0.15(0.08) 87.3 3.8 300(0) 0.13(0.00) 92.8 6.5 281(37) 0.03(0.00) 90.5\n\n3.4 141(54) 0.03(0.00) 17.5 1.7 33(16) 0.13(0.03) 30.5 1.5 6(3) 0.14(0.04) 18.3 2.6 36(16) 0.06(0.02) 21.4\n\n22.1 182(144) 0.56(0.15) 137.3 14.9 300(0) 0.86(0.05) 141.1 9.9 300(0) 1.02(0.00) 140.2 12.8 1(0) 0.35(0.00) 143.2\n\n3.3 72(50) 0.04(0.02) 28.2 2.1 169(98) 0.10(0.02) 27.1 1.5 8(2) 0.13(0.00) 27.7 4.0 26(24) 0.08(0.04) 28.5\n\n5.1 192(132) 0.04(0.01) 56.8 1.7 300(0) 0.09(0.00) 54.3 1.9 25(10) 0.12(0.01) 53.5 4.0 94(109) 0.07(0.03) 52.4\n\n2.3 300(0) 0.02(0.00) 8.4 1.3 300(0) 0.08(0.00) 8.9 1.1 300(0) 0.12(0.01) 8.2 1.6 300(0) 0.03(0.00) 8.4\n\nFigure 3: Head-to-head comparison of force MAE vs. Stability and h(r) MAE on MD17 molecules. Models are on the x-axis and are sorted according to force error in descending order. High stability and low h(r) MAE mean better performance. Error bars indicate 95% confidence intervals.\n\n2021), GemNet-T/dT (Gasteiger et al., 2021), and the official implementation of DeepPot-SE (Zhang et al., 2018b), SphereNet (Liu et al., 2021), and NequIP (Batzner et al., 2022). A summary of all benchmarked models are in Table 2. These models have been popular in previous benchmark studies for force/energy prediction. They use different representations for atomistic interactions and respect different levels of euclidean symmetry. We follow all original hyperparameters introduced in the respective papers and only make minimal adjustments when the training is unstable. More details on the hyperparameters can be found in Appendix C.\n\nKey observations. We make two key observations as evidenced in the experimental results:\n\n1. Despite being widely used, force prediction is not sufficient for evaluating ML FFs. It generally does not align with simulation stability and performance in estimating ensemble properties.\n\n2. While often neglected, stability can be a major bottleneck for ML FFs. Lower force error and more training data does not necessarily give rise to more stable simulations, suggesting stability as a fundamental consideration for comparison and model design.\n\nWe next go through experimental results of all four datasets in detail to demonstrate the key observations while making other observations.\n\nMD17. As shown in Table 3, more recent models that lie on the right side of the table generally achieve a lower force error, but may lack stability. Figure 3 selects results from Table 3 to demonstrate the non-aligned trends of force prediction performance vs. simulation performance, which supports key observation 1. SphereNet and GemNet-T/dT can attain a very low force error for all four molecules, but often collapse before the simulation finishes. This observation constitutes key observation 2. We note that although the stable portion of simulated trajectories produced by SphereNet and GemNetT/dT can recover the h(r) relatively accurately, stability will become a bigger issue when the statistics\n\n5\n\n(a)(b)(c)(d)Under review as a conference paper at ICLR 2023\n\nTable 4: Results on Water-10k. RDF MAE is unit-less. Diffusivity is computed by averaging 5 runs from 5 random initial configurations and its MAE is reported in the unit of [10−9 m2/s]. The reference diffusivity coefficient is 2.3 × 10−9 m2/s.\n\nDeepPot-SE SchNet\n\nDimeNet\n\nPaiNN\n\nSphereNet\n\nForceNet\n\nGemNet-T GemNet-dT NequIP\n\nForce (↓) Stability (↑) RDF(O,O) (↓) RDF(H,H) (↓) RDF(H,O) (↓) Diffusivity (↓) FPS (↓)\n\n5.8 247(147) 0.07(0.01) 0.06(0.02) 0.19(0.05) 0.04 91.0\n\n9.5 232(59) 0.63(0.04) 0.30(0.02) 0.57(0.04) 1.90 78.9\n\n1.4 30(10) 0.27(0.15) 0.18(0.08) 0.21(0.04) -\n17.9\n\n5.1 12(13) 0.30(0.14) 0.21(0.09) 0.29(0.12) -\n71.8\n\n16.1 500(0) 0.89(0.04) 0.40(0.01) 1.14(0.03) 2.23 3.1\n\n10.9 7(3) 0.79(0.03) 0.55(0.01) 1.34(0.03) -\n67.6\n\n0.7 25(7) 0.22(0.05) 0.16(0.03) 0.20(0.04) -\n11.3\n\n1.3 7(3) 0.42(0.22) 0.35(0.25) 0.42(0.27) -\n33.7\n\n1.5 500(0) 0.06(0.02) 0.05(0.01) 0.27(0.07) 0.18 3.9\n\nFigure 4: Comparison of force MAE vs. stability (Left), force MAE vs. RDF MAE (Middle), and force MAE vs. Diffusivity MAE (Right) on the water benchmark. Each model is trained with three dataset sizes. The color of a point indicates the model identity, while the point size indicates the training dataset size (small: 1k, medium: 10k, large: 90k). Metrics infeasible to extract from certain model/dataset size (e.g., Diffusivity for unstable models) are not included.\n\nof interest require long simulations, as demonstrated in other experiments. On the other hand, despite having a relatively high force error, DeepPot-SE performs very well on simulation-based metrics on all molecules except for Aspirin (Figure 3). With the highest molecular weight, Aspirin is indeed the hardest task in MD17 in the sense that all models attain high force prediction errors on it. PaiNN also attains competitive simulation performance while its force error is not among the best.\n\nWe further observe that good stability does not imply accurate recovery of trajectory statistics. Although ForceNet remains stable for Ethanol and Naphthalene, the extracted h(r) deviates a lot from the reference (Table 3), indicating that ForceNet does not learn the underlying PES correctly, possibly due to its lack of energy conservation and rotational equivariance. Overall, NequIP is the best-performing model on MD17. It achieves the best performance in both force prediction and simulation-based metrics for all molecules while requiring the highest computational cost. More detailed results on MD17 including a study on stability’s relation with training epochs and individual h(r) are included in Appendix C.\n\nWater. Under different challenges posed by a condensed phase system, key observation 1 and 2 are still evident according to Table 4: GemNet-T/dT and DimeNet are the top-3 models in terms of force prediction, but all lack stability. The water diffusivity coefficient requires long (100 ps in our experiments) trajectories to estimate and thus cannot be extracted for unstable models. Like MD17, DeepPot-SE does not achieve the best force prediction performance but demonstrates decent stability and highly accurate recovery of simulation statistics. Interestingly, SphereNet has high force error but is highly stable. However, the properties are not accurately recovered.\n\nFigure 4 further compares model performance with different training dataset sizes. Key observation 1 and 2 are clearly shown: Models located on the left of each scatter plot have very low force error but may have poor stability or high error in simulation statistics. More specifically, although more training data almost always improve force prediction performance, its effect on simulation performance is not entirely clear. On the one hand, GemNet-T/dT, DimeNet, and ForceNet are not stable even when under the highest training data budget. On the other hand, we observe a clear\n\n6\n\n0102030Force MAE [meV/Å]0100200300400500Stability [ps]2.55.07.510.012.5Force MAE [meV/Å]0.00.20.40.60.81.01.2RDF(O,O) MAE0.02.55.07.510.012.5Force MAE [meV/Å]0.00.51.01.52.0Diffusivity MAE [109m2/s]ModelDeepPot-SESchNetDimeNetForceNetGemNet-TGemNet-dTNequIPUnder review as a conference paper at ICLR 2023\n\nTable 5: Results on alanine dipeptide. #Finished is the number of simulations stable for 5 ns. MAE of F (φ) and F (ψ) are reported in the unit of [kJ/mol].\n\nDeepPot-SE SchNet DimeNet\n\nForceNet GemNet-T GemNet-dT NequIP\n\nForce (↓) #Finished (↑) Stability (↑) F (φ) (↓) F (ψ) (↓) FPS (↑)\n\n272.1 0/6 0(0) -\n- 54.3\n\n217.0 0/6 0(0) -\n- 42.4\n\n239.0 0/6 0(0) -\n- 12.1\n\n284.7 0/6 0(0) -\n- 99.1\n\n233.5 0/6 18(27) -\n- 15.0\n\n219.7 0/6 0(0) -\n- 36.5\n\n215.6 5/6 4168(1860) 108(2) 126(4) 8.3\n\nTable 6: Results on LiPS. Li-ion Diffusivity coefficient is computed by averaging 5 runs from 5 random initial configurations. The reference Li-ion diffusivity coefficient is 1.35 × 10−9 m2/s. GemNet-T GemNet-dT NequIP\n\nDeepPot-SE SchNet\n\nForceNet\n\nDimeNet\n\nForce (↓) Stability (↑) RDF (↓) Diffusivity (↓) FPS (↑)\n\n40.5 4(3) 0.27(0.15) -\n66.1\n\n28.8 50(0) 0.04(0.00) 0.38 35.2\n\n3.2 48(4) 0.05(0.01) 0.30 14.8\n\n12.8 26(8) 0.51(0.08) -\n72.1\n\n1.3 50(0) 0.04(0.00) 0.24 16.9\n\n1.4 50(0) 0.04(0.00) 0.28 43.5\n\n3.7 50(0) 0.04(0.01) 0.34 8.2\n\nimprovement of DeepPot-SE when more training data is used. NequIP is again the best performing model, achieving very low force error, excellent stability, and accurate recovery of ensemble statistics, even under the lowest data budget of 1,000 training+validation structures. However, when the training dataset is sufficiently large (90k), DeepPot-SE has equally good results as NequIP while being more than 20 times faster – dataset size also influences the model of choice for a certain dataset. More results, including tables for water-1k/90k, a study on model size, and generalization to Water-large are included in Appendix C.\n\nAlanine dipeptide poses unique challenges in sampling different metastable states separated by high free energy barriers. Table 5 shows all models have high force errors due to the random forces introduced by the lack of explicit account of water molecules. Although the force errors are in the same order of magnitude, all models except NequIP fail to simulate stably. The FES reconstruction task requires stable simulation for the entire 5 ns. NequIP is the only model that manages to finish five simulations out of six but produces inaccurate statistics. All other models are not stable enough to produce meaningful results. We further analyze the results on this task in Section 6.\n\nLiPS. Compared to flexible molecules and liquid water, this solid material system features slower kinetics. From Table 6 we observe that most models are capable of finishing 50-ns simulations stably. In this dataset, the performance on diffusivity estimation and force prediction align well. We observe that both GemNet-T and GemNet-dT show excellent force prediction, stability, and recovery of observables, while GemNet-dT is 2.6 times faster. The better efficiency comes from the direct prediction of atomic forces F instead of taking the derivative F = ∂E/∂x, which also makes GemNet-dT not energy-conserving – a potential issue we further discuss in Section 6.\n\nImplications on model architecture. More recent models utilizing SE(3)/E(3)-equivariant representations and operations such as GemNet-dT and NequIP are more expressive and can capture interatomic interactions more accurately. This is reflected by their very low force error and accurate recovery of ensemble statistics when not bottlenecked by stability. Moreover, NequIP shows that excellent accuracy and stability can be simultaneously achieved. The stability may come from parity-equivariance and the explicit architecture in manipulating higher-order geometric tensors. We believe further investigations into the extrapolation behavior induced by different equivariant geometric representations and operations (Batatia et al., 2022) is a fruitful direction in designing more powerful ML FFs.\n\n6 FAILURE MODES: CAUSES AND FUTURE DIRECTIONS\n\nA case study on alanine dipeptide simulation. NequIP achieves decent performance on all our tasks but fails on alanine dipeptide. It is also the only model that can simulate stably for 5 ns. Figure 5 (a) demonstrates how NequIP fails to reconstruct the FES: it does not manage to sample much of the\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: (a) Ramachandran plots of the alanine dipeptide FES reconstructed from 5-ns reference vs. 5-ns NequIP simulation, both using MetaDynamics. (b) F (φ) and F (ψ) of alanine dipeptide extracted from reference simulation vs. from NequIP simulation. (c) (φ, ψ) distribution of the alanine dipeptide training dataset. The six initialization points are marked with stars. NequIP fails to remain stable when the simulation starts from the point marked with black color. (d) Model-predicted total energy as a function of simulation time when simulating the LiPS system using the NVE ensemble. (e) On water-10k, stability does not improve when the time step is reduced for GemNet-T.\n\ntransition regions and the configuration space with φ ∈ [0, 180◦]. Figure 5 (b) demonstrates the reconstructed FES, which significantly deviates from the reference. This failure can be partially explained by Figure 5 (c), the training data distribution produced by the reference potential. The relatively high-energy (low-density) regions are exactly those that are not reachable by NequIP. Even though our MD trajectory is well-equilibrated, the relative difference in populations of different meta-stable states creates data imbalance, making it more challenging for the model to learn PES for higher-energy configurations where density is relatively low. In our experiments, we observe that simulations starting from the low-density meta-stable state (e.g., black star marked in Figure 5 (c)) tend to fail. This implies that generalization across different regions in the conformational space is an important challenge for ML FFs. To prevent ML FFs from sampling nonphysical regions, which is a common precursor to failed simulation (Figure 6), one can deliberately include distorted and off-equilibrium geometries in the training data to improve model robustness(Stocker et al., 2022). Alternatively, one can resort to active learning (Wang et al., 2020b; Vandermause et al., 2020; Schwalbe-Koda et al., 2021) to acquire new data points based on model uncertainty.\n\nEnergy conservation. Models that directly predict forces may not conserve energy. Figure 5 (d) demonstrates the evolution of model-predicted total energy for selected models on LiPS, in a microcanonical (NVE) ensemble. The energy of an isolated system in the NVE ensemble is in principle conserved. We observe that GemNet-T conserves energy, whereas GemNet-dT fails to conserve the predicted total energy. The existence of non-conservative forces breaks the time reversal symmetry and, therefore, may not be able to reach equilibrium for observable calculation. However, in our experiment, GemNet-dT performs well on the LiPS dataset when coupled with a thermostat. Previous works (Kolluru et al., 2022) also found that energy conservation is not required for SOTA performance on OC20. The usability of non-conservative FFs in simulations requires further careful investigations.\n\nSimulation instability is a major bottleneck for highly accurate models such as GemNet-T to fail on several simulation tasks. Moreover, in our water experiments, we find a larger amount of training data does not resolve this issue for GemNet-T/dT and DimeNet (Figure 4). We further experiment with smaller simulation time steps for GemNet-T on water (Figure 5 (e)), but stability still does not improve. On the other hand, Stocker et al. (2022) demonstrates that the stability of GemNet improves with larger training sets on QM7-x, which includes high-energy off-equilibrium geometries obtained from normal mode sampling. We hypothesize that including these distorted geometries may improve the model’s robustness against going into nonphysical configurations. We also observe that\n\n8\n\n(a)(e)(c)(d)(b)Under review as a conference paper at ICLR 2023\n\nFigure 6: Examples of simulation collapse when applying (a) NequIP to alanine dipeptide and (b) GemNet-T to water. The y-axis shows the maximum force observed on any atom in the system at a certain time step. An orange cross indicates visualized time steps. Notable nonphysical regions are circled. The collapse usually happens within a very short period of time after the initial local errors.\n\nthe simulation can collapse within a short time window after a long period of stable simulation, as visualized in Figure 6. In both cases, we observe that the nonphysical configurations first emerge at local regions (circled), which cascade to the entire system very quickly as extremely large forces are being produced and subsequently integrated into the dynamics. At the end of the visualization, the bonds in the alanine dipeptide system are broken. Therefore, the local-descriptor-based NequIP model predicts very small forces. For the water system, the particles are packed in a finite periodic box. The nonphysical configurations exhibit incorrect coordination structures and extremely large forces. Regarding stability, past works found adding noise paired with a denoising objective during training helpful in improving out-of-distribution test performance on OC20 (Godwin et al., 2021), and in stabilizing learned simulations (Sanchez-Gonzalez et al., 2020). Another relevant line of work in coarse-grained MD simulation has studied regularization with an empirical “prior energy” (Wang et al., 2019b) and post-prediction refinement (Fu et al., 2022) to battle simulation instability.\n\n7 CONCLUSION AND OUTLOOK\n\nWe have introduced a diverse suite of MD simulation tasks and conducted a thorough comparison of SOTA ML FFs to reveal novel insights into ML for MD simulation. As shown in our experiments, benchmarking only force error is not sufficient, and simulation-based metrics should be used to reflect the practical utility of a model. We demonstrate case studies on the failure of existing training schemes/models to better understand their limitations. The performance of a model can be highly case-dependent. For more challenging MD systems, more expressive atomistic representations may be required. For example, recent work has explored non-local descriptors (Kabylda et al., 2022) aiming at capturing long-range interactions in large molecules. Strictly local equivariant representations (Musaelian et al., 2022) are studied for very large systems where computational scalability is critical. New datasets (Eastman et al., 2022) and benchmarks have been playing an important role in inspiring future work. This work focuses on atomic-level simulations. Simulating large systems such as polymers and proteins at atomic resolution would be too expensive for existing models. Learning coarse-grained MD (Wang et al., 2019b; Wang & Gómez-Bombarelli, 2019; Fu et al., 2022) is another avenue to accelerate MD at larger length/time scales.\n\nThe possibility of ML in advancing MD simulation is not limited to ML force fields. Enhanced sampling methods enable fast sampling of rare events and have been augmented with ML techniques (Schneider et al., 2017; Sultan et al., 2018; Holdijk et al., 2022). Differentiable simulations (Schoenholz & Cubuk, 2020; Wang et al., 2020a; Doerr et al., 2021; Ingraham et al., 2018; Greener & Jones, 2021) offer a principled way of learning the force field by directly training the simulation process to reproduce experimental observables (Wang et al., 2020a; 2022; Thaler & Zavadlav, 2021). We hope our datasets and benchmarks will encourage future developments in all related aspects to push the frontier in ML for MD simulations.\n\n9\n\nt = 3702 fst = 3704 fst = 3706 fst = 3622 fst = 3716 fst = 28330 fst = 28370 fst = 28372 fst = 28373 fst = 28376 fs(a)(b)Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMark James Abraham, Teemu Murtola, Roland Schulz, Szilárd Páll, Jeremy C. Smith, Berk Hess, and Erik Lindahl. GROMACS: High performance molecular simulations through multi-level parallelism from laptops to supercomputers. SoftwareX, 1-2:19–25, September 2015. ISSN 23527110. doi: 10.1016/j.softx.2015.06.001. URL https://linkinghub.elsevier. com/retrieve/pii/S2352711015000059.\n\nNongnuch Artrith, Alexander Urban, and Gerbrand Ceder. Efficient and accurate machine-learning interpolation of atomic energies in compositions with many species. Physical Review B, 96(1): 014112, 2017.\n\nIlyes Batatia, Simon Batzner, Dávid Péter Kovács, Albert Musaelian, Gregor NC Simm, Ralf Drautz, Christoph Ortner, Boris Kozinsky, and Gábor Csányi. The design space of e (3)-equivariant atom-centered interatomic potentials. arXiv preprint arXiv:2205.06643, 2022.\n\nSimon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature communications, 13(1):1–11, 2022.\n\nJörg Behler and Michele Parrinello. Generalized neural-network representation of high-dimensional\n\npotential-energy surfaces. Physical review letters, 98(14):146401, 2007.\n\nLowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, et al. Open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 11(10):6059–6072, 2021.\n\nYaoyi Chen, Andreas Krämer, Nicholas E Charron, Brooke E Husic, Cecilia Clementi, and Frank Noé. Machine learning implicit solvation for molecular dynamics. The Journal of Chemical Physics, 155(8):084101, 2021.\n\nStefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Schütt, and Klaus-Robert Müller. Machine learning of accurate energy-conserving molecular force fields. Science advances, 3(5):e1603015, 2017.\n\nDaniel A. Colón-Ramos, Patrick La Riviere, Hari Shroff, and Rudolf Oldenbourg. Transforming the development and dissemination of cutting-edge microscopy and computation. Nat Methods, 16(8): 667–669, August 2019. ISSN 1548-7091, 1548-7105. doi: 10.1038/s41592-019-0475-y. URL http://www.nature.com/articles/s41592-019-0475-y.\n\nWendy D Cornell, Piotr Cieplak, Christopher I Bayly, Ian R Gould, Kenneth M Merz, David M Ferguson, David C Spellmeyer, Thomas Fox, James W Caldwell, and Peter A Kollman. A second generation force field for the simulation of proteins, nucleic acids, and organic molecules j. am. chem. soc. 1995, 117, 5179- 5197. Journal of the American Chemical Society, 118(9):2309–2309, 1996.\n\nStefan Doerr, Maciej Majewski, Adrià Pérez, Andreas Kramer, Cecilia Clementi, Frank Noe, Toni Giorgino, and Gianni De Fabritiis. Torchmd: A deep learning framework for molecular simulations. Journal of chemical theory and computation, 17(4):2355–2363, 2021.\n\nPeter Eastman, Pavan Kumar Behara, David L. Dotson, Raimondas Galvelis, John E. Herr, Josh T. Horton, Yuezhi Mao, John D. Chodera, Benjamin P. Pritchard, Yuanqing Wang, Gianni De Fabritiis, and Thomas E. Markland. Spice, a dataset of drug-like molecules and peptides for training machine learning potentials, 2022.\n\nFelix A Faber, Luke Hutchison, Bing Huang, Justin Gilmer, Samuel S Schoenholz, George E Dahl, Oriol Vinyals, Steven Kearnes, Patrick F Riley, and O Anatole Von Lilienfeld. Prediction errors of molecular machine learning models lower than hybrid dft error. Journal of chemical theory and computation, 13(11):5255–5264, 2017.\n\nMichael Feig. Kinetics from implicit solvent simulations of biomolecules as a function of viscosity.\n\nJournal of chemical theory and computation, 3(5):1734–1748, 2007.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDaan Frenkel and Berend Smit. Understanding molecular simulation: from algorithms to applications,\n\nvolume 1. Elsevier, 2001.\n\nXiang Fu, Tian Xie, Nathan J Rebello, Bradley D Olsen, and Tommi Jaakkola. Simulate timeintegrated coarse-grained molecular dynamics with geometric machine learning. arXiv preprint arXiv:2204.10348, 2022.\n\nJohannes Gasteiger, Janek Groß, and Stephan Günnemann. Directional message passing for molecular\n\ngraphs. In International Conference on Learning Representations, 2020.\n\nJohannes Gasteiger, Florian Becker, and Stephan Günnemann. Gemnet: Universal directional graph neural networks for molecules. Advances in Neural Information Processing Systems, 34: 6790–6802, 2021.\n\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017.\n\nJonathan Godwin, Michael Schaarschmidt, Alexander L Gaunt, Alvaro Sanchez-Gonzalez, Yulia Rubanova, Petar Veliˇckovi ́c, James Kirkpatrick, and Peter Battaglia. Simple gnn regularisation for 3d molecular property prediction and beyond. In International conference on learning representations, 2021.\n\nJoe G Greener and David T Jones. Differentiable molecular simulation can learn all the parameters\n\nin a coarse-grained force field for proteins. PloS one, 16(9):e0256990, 2021.\n\nThomas A Halgren. Merck molecular force field. i. basis, form, scope, parameterization, and\n\nperformance of mmff94. Journal of computational chemistry, 17(5-6):490–519, 1996.\n\nTeresa Head-Gordon, Martin Head-Gordon, Michael J Frisch, Charles Brooks III, and John Pople. A theoretical study of alanine dipeptide and analogs. International Journal of Quantum Chemistry, 36(S16):311–322, 1989.\n\nRL Henderson. A uniqueness theorem for fluid pair correlation functions. Physics Letters A, 49(3):\n\n197–198, 1974.\n\nJohannes Hoja, Leonardo Medrano Sandonas, Brian G Ernst, Alvaro Vazquez-Mayagoitia, Robert A DiStasio Jr, and Alexandre Tkatchenko. Qm7-x, a comprehensive dataset of quantum-mechanical properties spanning the chemical space of small organic molecules. Scientific data, 8(1):1–11, 2021.\n\nLars Holdijk, Yuanqi Du, Ferry Hooft, Priyank Jaini, Bernd Ensing, and Max Welling. Path integral stochastic optimal control for sampling transition paths. arXiv preprint arXiv:2207.02149, 2022.\n\nWeihua Hu, Muhammed Shuaibi, Abhishek Das, Siddharth Goyal, Anuroop Sriram, Jure Leskovec, Devi Parikh, and C Lawrence Zitnick. Forcenet: A graph neural network for large-scale quantum calculations. arXiv preprint arXiv:2103.01436, 2021.\n\nJohn Ingraham, Adam Riesselman, Chris Sander, and Debora Marks. Learning protein structure with\n\na differentiable simulator. In International Conference on Learning Representations, 2018.\n\nAdil Kabylda, Valentin Vassilev-Galindo, Stefan Chmiela, Igor Poltavsky, and Alexandre Tkatchenko. Towards linearly scaling and chemically accurate global machine learning force fields for large molecules. arXiv preprint arXiv:2209.03985, 2022.\n\nGeorge A Kaminski, Richard A Friesner, Julian Tirado-Rives, and William L Jorgensen. Evaluation and reparametrization of the opls-aa force field for proteins via comparison with accurate quantum chemical calculations on peptides. The Journal of Physical Chemistry B, 105(28):6474–6487, 2001.\n\nAlireza Khorshidi and Andrew A Peterson. Amp: A modular approach to machine learning in\n\natomistic simulations. Computer Physics Communications, 207:310–324, 2016.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAdeesh Kolluru, Muhammed Shuaibi, Aini Palizhati, Nima Shoghi, Abhishek Das, Brandon Wood, C Lawrence Zitnick, John R Kitchin, and Zachary W Ulissi. Open challenges in developing generalizable large scale machine learning models for catalyst discovery. arXiv preprint arXiv:2206.02005, 2022.\n\nDávid Péter Kovács, Cas van der Oord, Jiri Kucera, Alice EA Allen, Daniel J Cole, Christoph Ortner, and Gábor Csányi. Linear atomic cluster expansion force fields for organic molecules: beyond rmse. Journal of chemical theory and computation, 17(12):7696–7711, 2021.\n\nAlessandro Laio and Michele Parrinello. Escaping free-energy minima. Proceedings of the National\n\nAcademy of Sciences, 99(20):12562–12566, 2002.\n\nThomas J Lane, Gregory R Bowman, Kyle Beauchamp, Vincent A Voelz, and Vijay S Pande. Markov state model reveals folding and functional dynamics in ultra-long md trajectories. Journal of the American Chemical Society, 133(45):18413–18419, 2011.\n\nAsk Hjorth Larsen, Jens Jørgen Mortensen, Jakob Blomqvist, Ivano E Castelli, Rune Christensen, Marcin Dułak, Jesper Friis, Michael N Groves, Bjørk Hammer, Cory Hargus, Eric D Hermes, Paul C Jennings, Peter Bjerre Jensen, James Kermode, John R Kitchin, Esben Leonhard Kolsbjerg, Joseph Kubal, Kristen Kaasbjerg, Steen Lysgaard, Jón Bergmann Maronsson, Tristan Maxson, Thomas Olsen, Lars Pastewka, Andrew Peterson, Carsten Rostgaard, Jakob Schiøtz, Ole Schütt, Mikkel Strange, Kristian S Thygesen, Tejs Vegge, Lasse Vilhelmsen, Michael Walter, Zhenhua Zeng, and Karsten W Jacobsen. The atomic simulation environment—a python library for working with atoms. Journal of Physics: Condensed Matter, 29(27):273002, 2017. URL http: //stacks.iop.org/0953-8984/29/i=27/a=273002.\n\nJonas Lederer, Michael Gastegger, Kristof T Schütt, Michael Kampffmeyer, Klaus-Robert Müller, and Oliver T Unke. Automatic identification of chemical moieties. arXiv preprint arXiv:2203.16205, 2022.\n\nYi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic\n\ngraphs. arXiv preprint arXiv:2206.11990, 2022.\n\nYi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3d molecular graphs. In International Conference on Learning Representations, 2021.\n\nAlbert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. arXiv preprint arXiv:2204.05249, 2022.\n\nCheol Woo Park, Mordechai Kornbluth, Jonathan Vandermause, Chris Wolverton, Boris Kozinsky, and Jonathan P Mailoa. Accurate and scalable graph neural network force field and molecular dynamics with direct force architecture. npj Computational Materials, 7(1):1–9, 2021.\n\nJay W Ponder and David A Case. Force fields for protein simulations. Advances in protein chemistry,\n\n66:27–85, 2003a.\n\nJay W. Ponder and David A. Case.\n\nIn Advances in Protein Chemistry, volume 66, pp. 27–85. Elsevier, 2003b. ISBN 978-0-12-034266-2. doi: 10.1016/S0065-3233(03)66002-X. URL https://linkinghub.elsevier.com/ retrieve/pii/S006532330366002X.\n\nForce Fields for Protein Simulations.\n\nZhuoran Qiao, Anders S Christensen, Matthew Welborn, Frederick R Manby, Anima Anandkumar, and Thomas F Miller III. Unite: Unitary n-body tensor equivariant network with applications to quantum chemistry. arXiv preprint arXiv:2105.14655, 2021.\n\nRaghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum\n\nchemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1–7, 2014.\n\nDavid Rosenberger, Justin S Smith, and Angel E Garcia. Modeling of peptides with classical and novel machine learning force fields: A comparison. The Journal of Physical Chemistry B, 125(14): 3598–3612, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAlvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, pp. 8459–8468. PMLR, 2020.\n\nVıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks.\n\nIn International conference on machine learning, pp. 9323–9332. PMLR, 2021.\n\nElia Schneider, Luke Dai, Robert Q Topper, Christof Drechsel-Grau, and Mark E Tuckerman. Stochastic neural network approach for learning high-dimensional free energy surfaces. Physical review letters, 119(15):150601, 2017.\n\nSamuel Schoenholz and Ekin Dogus Cubuk. Jax md: a framework for differentiable physics. Advances\n\nin Neural Information Processing Systems, 33:11428–11441, 2020.\n\nKristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert Müller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. Advances in neural information processing systems, 30, 2017.\n\nKristof Schütt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In International Conference on Machine Learning, pp. 9377–9388. PMLR, 2021.\n\nDaniel Schwalbe-Koda, Aik Rui Tan, and Rafael Gómez-Bombarelli. Differentiable sampling of molecular geometries with uncertainty-based adversarial attacks. Nature communications, 12(1): 1–12, 2021.\n\nHythem Sidky, Wei Chen, and Andrew L Ferguson. Machine learning for collective variable discovery and enhanced sampling in biomolecular simulation. Molecular Physics, 118(5):e1737742, 2020.\n\nJustin S Smith, Olexandr Isayev, and Adrian E Roitberg. Ani-1: an extensible neural network potential with dft accuracy at force field computational cost. Chemical science, 8(4):3192–3203, 2017.\n\nSina Stocker, Johannes Gasteiger, Florian Becker, Stephan Günnemann, and Johannes Margraf. How robust are modern graph neural network potentials in long and hot molecular dynamics simulations? 2022.\n\nMohammad M Sultan, Hannah K Wayment-Steele, and Vijay S Pande. Transferable neural networks for enhanced sampling of protein dynamics. Journal of chemical theory and computation, 14(4): 1887–1894, 2018.\n\nSo Takamoto, Chikashi Shinagawa, Daisuke Motoki, Kosuke Nakago, Wenwen Li, Iori Kurata, Taku Watanabe, Yoshihiro Yayama, Hiroki Iriguchi, Yusuke Asano, et al. Towards universal neural network potential for material discovery applicable to arbitrary combination of 45 elements. Nature Communications, 13(1):1–11, 2022.\n\nStephan Thaler and Julija Zavadlav. Learning neural network potentials from experimental data via\n\ndifferentiable trajectory reweighting. Nature Communications, 12(1):1–10, 2021.\n\nPhilipp Thölke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular\n\npotentials. In International Conference on Learning Representations, 2021.\n\nNathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.\n\nRichard Tran, Janice Lan, Muhammed Shuaibi, Siddharth Goyal, Brandon M Wood, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, et al. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysis. arXiv preprint arXiv:2206.08917, 2022.\n\nGareth A. Tribello, Massimiliano Bonomi, Davide Branduardi, Carlo Camilloni, and Giovanni Bussi. PLUMED 2: New feathers for an old bird. Computer Physics Communications, 185 (2):604–613, February 2014. ISSN 00104655. doi: 10.1016/j.cpc.2013.09.018. URL https: //linkinghub.elsevier.com/retrieve/pii/S0010465513003196.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nMark Tuckerman. Statistical mechanics: theory and molecular simulation. Oxford university press,\n\n2010.\n\nOliver T Unke and Markus Meuwly. A reactive, scalable, and transferable model for molecular energies from a neural network approach based on local information. The Journal of chemical physics, 148(24):241708, 2018.\n\nOliver T Unke, Stefan Chmiela, Michael Gastegger, Kristof T Schütt, Huziel E Sauceda, and KlausRobert Müller. Spookynet: Learning force fields with electronic degrees of freedom and nonlocal effects. Nature communications, 12(1):1–14, 2021a.\n\nOliver T Unke, Stefan Chmiela, Huziel E Sauceda, Michael Gastegger, Igor Poltavsky, Kristof T Sch utt, Alexandre Tkatchenko, and Klaus-Robert Müller. Machine learning force fields. Chemical Reviews, 121(16):10142–10186, 2021b.\n\nJonathan Vandermause, Steven B Torrisi, Simon Batzner, Yu Xie, Lixin Sun, Alexie M Kolpak, and Boris Kozinsky. On-the-fly active learning of interpretable bayesian force fields for atomistic rare events. npj Computational Materials, 6(1):1–11, 2020.\n\nErcheng Wang, Huiyong Sun, Junmei Wang, Zhe Wang, Hui Liu, John Z. H. Zhang, and Tingjun Hou. End-Point Binding Free Energy Calculation with MM/PBSA and MM/GBSA: Strategies and Applications in Drug Design. Chem. Rev., 119(16):9478–9508, August 2019a. ISSN 0009-2665, 1520-6890. doi: 10.1021/acs.chemrev.9b00055. URL https://pubs.acs.org/doi/10. 1021/acs.chemrev.9b00055.\n\nJiang Wang, Simon Olsson, Christoph Wehmeyer, Adrià Pérez, Nicholas E Charron, Gianni De Fabritiis, Frank Noé, and Cecilia Clementi. Machine learning of coarse-grained molecular dynamics force fields. ACS central science, 5(5):755–767, 2019b.\n\nWujie Wang and Rafael Gómez-Bombarelli. Coarse-graining auto-encoders for molecular dynamics.\n\nnpj Computational Materials, 5(1):1–9, 2019.\n\nWujie Wang, Simon Axelrod, and Rafael Gómez-Bombarelli. Differentiable molecular simulations\n\nfor control and learning. arXiv preprint arXiv:2003.00868, 2020a.\n\nWujie Wang, Tzuhsiung Yang, William H Harris, and Rafael Gómez-Bombarelli. Active learning and neural network potentials accelerate molecular screening of ether-based solvate ionic liquids. Chemical Communications, 56(63):8920–8923, 2020b.\n\nWujie Wang, Zhenghao Wu, and Rafael Gómez-Bombarelli. Learning pair potentials using differen-\n\ntiable simulations. arXiv preprint arXiv:2209.07679, 2022.\n\nMichael A Webb, Yukyung Jung, Danielle M Pesko, Brett M Savoie, Umi Yamamoto, Geoffrey W Coates, Nitash P Balsara, Zhen-Gang Wang, and Thomas F Miller III. Systematic computational and experimental investigation of lithium-ion transport mechanisms in polyester-based polymer electrolytes. ACS central science, 1(4):198–205, 2015.\n\nYujie Wu, Harald L. Tepper, and Gregory A. Voth. Flexible simple point-charge water model with improved liquid-state properties. The Journal of Chemical Physics, 124(2):024503, January 2006. ISSN 0021-9606, 1089-7690. doi: 10.1063/1.2136877. URL http://aip.scitation. org/doi/10.1063/1.2136877.\n\nShuwen Yue, Maria Carolina Muniz, Marcos F. Calegari Andrade, Linfeng Zhang, Roberto Car, and Athanassios Z. Panagiotopoulos. When do short-range atomistic machine-learning models fall short? J. Chem. Phys., 154(3):034111, January 2021. ISSN 0021-9606, 1089-7690. doi: 10.1063/5.0031215. URL http://aip.scitation.org/doi/10.1063/5.0031215.\n\nYaoguang Zhai, Alessandro Caruso, Sigbjörn L Bore, and Francesco Paesani. A “short blanket” dilemma for a state-of-the-art neural network potential for water: Reproducing properties or learning the underlying physics? 2022.\n\nLinfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and EJPRL Weinan. Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics. Physical review letters, 120 (14):143001, 2018a.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nLinfeng Zhang, Jiequn Han, Han Wang, Wissam Saidi, Roberto Car, et al. End-to-end symmetry preserving inter-atomic potential energy model for finite and extended systems. Advances in Neural Information Processing Systems, 31, 2018b.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nA EVALUATION METRICS\n\nObservables. From the time series observations of positions and velocities, observables O(xt) can be computed to characterize the state of the system at different granularities. Under the ergodic hypothesis, the time averages of the simulation observables converge to distributional averages under ˆE(x) the Gibbs measure: ⟨O⟩ = 1 kB T ) with T as the bath temperature and kB as the Boltzmann constant. Calculations of such observables require the system to reach equilibrium. Simulation observables connect simulations to experimental measurements and are predictive of macroscopic properties of matter. Common observables include radial distribution functions (RDFs), virial stress tensor, mean-squared displacement (MSD), etc.\n\nt O(xt) = (cid:82) dx p(x) O(x), where p(x) ∝ exp(−\n\nT limT →∞\n\n(cid:80)T\n\nDistribution of interatomic distances is a low-dimensional description of the 3D structure and has been studied in previous work (Zhang et al., 2018a). For a given configuration x, the distribution of interatomic distances h(r) is computed with:\n\nh(r) =\n\n1 N (N − 1)\n\nN (cid:88)\n\nN (cid:88)\n\ni\n\nj̸==i\n\nδ(r − ||xi − xj||)\n\n(1)\n\nwhere r is the distance from a reference particle; N is the total number of particles; i, j indicates the pairs of atoms that contribute to the distance statistics; δ is the Dirac Delta function to extract value distributions. To calculate the ensemble average, h(r) is calculated and averaged over frames from equilibrated trajectories. For learned simulations, we compute h(r) using only the stable part of the simulation.\n\nRDF. As one of the most informative simulation observables, the radial distribution function (RDF) describes the structural/thermodynamic properties of the system and is also experimentally measurable. It has been widely used in force field development (Henderson, 1974). By definition, the RDF describes how density varies as a function of distance from a particle (illustrated in Figure 7 (a)). For a given configuration x, the RDF can be computed with the following formula:\n\nRDF(r) =\n\n1 4πr2\n\n1 N ρ\n\nN (cid:88)\n\nN (cid:88)\n\ni\n\nj̸==i\n\nδ(r − ||xi − xj||)\n\n(2)\n\nwhere r is the distance from a reference particle; N is the total number of particles; i, j indicates the pairs of atoms that contribute to the distance statistics; ρ is the density of the system; δ is the Dirac Delta function to extract value distributions. To calculate the ensemble average, RDF(r) is calculated and averaged over frames from equilibrated trajectories. The final RDF MAE is then calculated by integrating r:\n\nMAERDF =\n\n(cid:90) ∞\n\nr=0\n\n|⟨RDF(r)⟩ − ⟨ ˆRDF(r)⟩|dr\n\n(3)\n\nwhere ⟨·⟩ is the averaging operator, ⟨RDF(r)⟩ is the reference equilibrium RDF, and ⟨ ˆRDF(r)⟩ is the model-predicted RDF.\n\nDiffusivity coefficient is a major property of study in many previous work (Batzner et al., 2022). The diffusivity coeffcient D quantifies the time-correlation of the translational displacement (illustrated in Figure 7 (b)), and can be computed from the mean square displacement:\n\nD = lim t→∞\n\n1 6t\n\n1 N ′\n\nN ′ (cid:88)\n\ni=1\n\n|xi(t) − xi(0)|2\n\n(4)\n\nwhere xi(t) is the coordinate of particle i at time t, and N ′ is the number of particles being tracked in the system. For the water system we monitor the liquid diffusivity coefficient and track all 64 oxygen atoms. For the LiPS system, we monitor Li-ion Diffusivity and track all 27 Li-ions. As the definition implies, D is a quantity that converges with longer simulation time. Accurate recovery of D requires sufficient long trajectories sampled from the Hamiltonian with ML FFs. In this paper,\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Illustrations regarding benchmarked metrics.\n\nwe only compute diffusivity for stable trajectories of at least 100 ps for water and 40 ps for LiPS. As we simulate multiple random runs for each system/model, we average the diffusivity coefficient extracted from each valid trajectory to obtain the final prediction.\n\nFree energy surface. Given the probability distributions over configurations p(x) and a chosen geometric coordinate ξ transformed from x. Based on the marginalized density p(ξ), the free energy (Tuckerman, 2010) can be calculated from the following:\n\nF (ξ) = −kBT ln p(ξ)\n\n(5)\n\nIn the specific case of alanine dipeptide, there are two main conformational DOF: dihedral angle φ of C − N − Cα − C and dihedral angle ψ of N − Cα − C − N (illustrated in Figure 7 (c)). Therefore, the FES w.r.t φ and ψ is the most physically informative. We propose our quantitative metric MAEF (φ),F (ψ) based on the absolute error in reconstructing the FES along the φ and ψ coordinates. We integrate the absolute difference between the reference free energy F and the model predicted ˆF from [−π, π):\n\nMAEF (φ) =\n\n(cid:90) π\n\nφ=−π\n\n|F (φ) − ˆF (φ)|dφ\n\nMAEF (ψ) =\n\n(cid:90) π\n\nψ=−π\n\n|F (ψ) − ˆF (ψ)|dψ\n\n(6)\n\n(7)\n\nStability criterion. Abstractly speaking, stability is a notion of staying within physical (low-energy) configuration spaces. Since all MD systems studied in this paper are in equilibrium, practically we keep track of stability by closely monitoring equilibrium statistics. For systems with periodic boundary conditions, we monitor the RDF and say a simulation becomes “unstable” at time T when\n\n(cid:90) ∞\n\nr=0\n\n||⟨RDF(r)⟩ − ⟨ ˆRDFt(r)⟩T +τ\n\nt=T ||dr > ∆\n\n(8)\n\nwhere ⟨·⟩ is the averaging operator, τ is a short time window, and ∆ is the stability threshold. In this paper we use τ = 1 ps, ∆ = 3.0 for water, and τ = 1 ps, ∆ = 1.0 for LiPS. For water, we assert unstable if any of the three element-conditioned RDFs: RDF(O,O), RDF(H,H), RDF(H,O) exceeds the threshold. For flexible molecules, we keep track of stability through the bond lengths and say a simulation becomes “unstable” at time T when:\n\nmax (i,j)∈B\n\n|(||xi(T ) − xj(T )|| − bi,j)| > ∆\n\n(9)\n\nwhere B is the set of all bonds, i, j are the two endpoint atoms of the bond, and bi,j is the equilibrium bond length. We use ∆ = 0.5 Å for both MD17 molecules and alanine dipeptide.\n\nWe measure the stability of a learned model through the simulation time it remains stable. For each dataset, the threshold ∆ we adopt is rather relaxed that an “unstable” equilibrium statistic usually indicates the system is already in a highly nonphysical configuration.\n\n17\n\nDiffusiont t+1 Radial DistributionrφψDihedral Angles(a)(b)(c)Under review as a conference paper at ICLR 2023\n\nFPS. All frames per second (FPS) metrics are measured with an NVIDIA Tesla V100-PCIe GPU. We present FPS as a reference for models’ computational efficiency but also note that code speed can be affected by many factors, and likely has room for improvement.\n\nExperimental procedures. Baseline models are trained on the datasets described in Section 4 according to the experimental settings described in Appendix C. At evaluation time, we simulate MD trajectories using the learned models, with thermostats and simulation length described in Section 4. The simulated trajectories are recorded as time series of atom positions, along with other information including atom types, temperature, total energy, potential energy, and kinetic energy. All observables described in this section can be computed from recorded trajectories. We use the stability criterion described above to find the time step when the systems become “unstable”, and only use the trajectory before that time step for the computation of observables. Among the observables, the distribution of interatomic distances and RDF are computed for each frame, and averaged over the entire trajectory. Diffusivity coefficients are computed by averaging over the diffusivity coefficient computed from all applicable time windows where the time window length is predefined to be long enough. For example, for a trajectory of T steps and a time window of K steps, we average over the diffusivity computed from the time windows: [1, K], [2, K + 1], . . . , [T − K + 1, T ]. We use 100 ps as the time window size for water and 35 ps as the time window size for LiPS. We also remove the first 5 ps of the simulated trajectories of LiPS for equilibrium. As we do multiple simulations per model and dataset, we compute the metrics for each trajectory and report the mean and standard deviation. Further details on observable computation can be found in our code submission: observable.ipynb.\n\nB DATASET DETAILS\n\nThe MD17 dataset1 (Chmiela et al., 2017) and the LiPS dataset2 (Batzner et al., 2022) are adapted from previous works and are publicly available. The MD17 dataset is generated from path-integral molecular dynamics simulations that incorporate quantum mechanics into the classic molecular dynamics simulations using Feynman path integrals. The LiPS datasets are generated by ab-initio molecular dynamics simulations with a generalized gradient PBE functional and projector augmented wave pseudopotentials. We refer interested readers to the respective papers for more details on the data generation process. The water dataset and alanine dipeptide dataset are generated by ourselves. process.\n\nWater. Our water dataset is generated from molecular dynamics simulations of a simple classical water model, namely, the flexible version of the Extended Simple Point Charge water model (SPC/Efw) (Wu et al., 2006) at temperature T = 300 K and pressure P = 1 atm. For this model, the interaction parameters (e.g., O-H bond stretch and H-O-H bond angles), are parameterized to match extensive experimental properties such as the self-diffusion and dielectric constants at bulk phase. This classical model has been well-studied in previous work (Wu et al., 2006; Yue et al., 2021) and has shown reasonable predictions of the physical properties of liquid water. It provides a computationally inexpensive way to generate a large amount of training data. The experience and knowledge gained from the benchmark based on the simple model can be readily extended to systems with higher accuracy, such as the ab-initio models.\n\nAlanine dipeptide. Our dataset is generated from the MD simulation of an alanine dipeptide molecule solvated in explicit water (1164 water molecules) performed in GROMACS (Abraham et al., 2015) using the AMBER-03 (Ponder & Case, 2003b) force-field. In the AMBER-03 force field, the potential energy parameters such as van der Waals and electrostatics are mostly derived from quantum mechanical methods with minor optimization on the bonded parameters to reproduce the experimental vibrational frequencies and structures (Cornell et al., 1996; Ponder & Case, 2003a). The NPT ensemble is applied in simulations, with hydrogen bond length constraints using LINear Constraint Solver (LINCS) and a time step of 2 fs. The temperature and pressure of the system are controlled at T = 300 K and P = 1 bar using a stochastic velocity rescaling thermostat with damping frequency tv = 0.1 ps and Parrinello-Rahman barostat with coupling frequency tp = 2.0 ps, respectively. The Particle Mesh Ewald approach is used to compute long-range electrostatics with periodic boundary conditions applied to the x, y, and z directions. The conformational modes can be characterized by six free energy local minimas, which have been used in previous work (Lederer\n\n1http://www.sgdml.org/ 2https://archive.materialscloud.org/record/2022.45\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Fφ and Fψ have converged for the reference force field (a) and NequIP (b) at time 5 ns under Metadynamics.\n\net al., 2022). We initialize six simulations for each model from each of the six free energy local minimas.\n\nImplicit solvation. The explicit solvent of 1164 water molecules is not the subject of study but adds a significant computational burden. In this task, we attempt to learn an implicit solvent model (ISM) of the alanine dipeptide, in which the explicit solvent environment is incorporated in the learned FF. The ISM is commonly used in drug design (Wang et al., 2019a) because it can speed up the computation by dramatically decreasing the number of particles required for simulation. In general, the mean-field estimation in ISM ignores the effect of solvent, thermal fluctuations, and solvent friction (Feig, 2007). Thus, molecular kinetics is not directly comparable to the explicit solvation simulation. However, the equilibrium configurations can be explicitly compared, as conducted in Chen et al. (2021).\n\nMetadynamics simulation. Simulating energy barrier jump usually requires a long sampling of the trajectory in MD simulations. The conformational change of alanine dipeptide in water involves such a process, making it difficult to extract the complete free energy surface, i.e., the Ramachandran plot, in normal MD. In order to examine the learned ML FFs within a reasonable time limit, metadynamics (Laio & Parrinello, 2002) is employed to explore the learned FES of the solvated alanine dipeptide. Metadynamics is a widely used technique in atomistic simulations to accelerate the sampling of rare events and estimate the FES of a certain set of degrees of freedom. It is based on iteratively “filling” the potential energy using a sum of Gaussians deposited on a set of suitable collective variables (CVs) along the trajectory. At evaluation time, we perform metadynamics with dihedral angles φ and ψ as CVs3, starting from the configurations located at one of the six energy minimums in the free energy surface indicated in Figure 5 (c). The Gaussians with height h = 1.2 and sigma σ = 0.35 are deposited every 1 ps centered on ψ and φ. As shown in Figure 8, the estimated FES of both φ and ψ do not significantly change after 5 ns. In addition, the height of the bias gaussian potential smoothly converges to ∼ 0 in the time limit of 5 ns. Therefore, a simulation time of 5 ns is sufficient for the convergence of the metadynamics. This metadynamics simulation of alanine dipeptide with AMBER force-fields is carried out using GROMACS (Abraham et al., 2015) integrated with the PLUMED library (Tribello et al., 2014; Colón-Ramos et al., 2019) of version 2.8.\n\nC EXPERIMENTAL DETAILS\n\nThe Open Catalyst Project4 codebase and the official codebases of DeepPot-SE5 and NequIP6 are all publicly available. We build our MD simulation framework based on the Atomic Simulation Environment (ASE) library7 (Larsen et al., 2017).\n\nHyperparameters. We adopt the original model hyperparameters in the respective papers and find they can produce good force prediction results that match the trend and numbers for MD17 reported in previous work. As we introduce new datasets, we set training hyperparameters such as the batch size and summarize them in Table 7. For water and LiPS, we use a batch size of 1 like in previous\n\n3In practice, the selection of suitable collective variables can be a case-by-case challenge (Sidky et al., 2020). 4https://github.com/Open-Catalyst-Project/ocp 5https://github.com/deepmodeling/deepmd-kit 6https://github.com/mir-group/nequip 7https://gitlab.com/ase/ase\n\n19\n\n(a)(b)Under review as a conference paper at ICLR 2023\n\nTable 7: Default training-related hyperparameters for each dataset. *We adopt the original batch size from respective papers when available for MD17. DeepPot-SE: 4; SchNet: 100; DimeNet: 32; GemNet-T/dT: 1; NequIP: 5. We use a batch size of 8 for ForceNet.\n\nDataset\n\nTraining dataset size Batch size Max epoch LR patience Longest simulation time\n\nMD17 Water-1k Water-10k Water-90k Alanine dipeptide LiPS\n\n9,500 950 9,500 85,500 38,000 19,000\n\n1-100* 1\n1 1\n5 1\n\n2,000 10,000 2,000 400 2,000 2,000\n\n5 epochs 50 epochs 5 epochs 3 epochs 5 epochs 5 epochs\n\n20 hours 28 hours 28 hours 28 hours 75 hours 7 hours\n\nTable 8: Results on Water-1k. Force MAE is reported in the unit of [meV/Å]; Stability is reported in the unit of [ps]; Diffusivity MAE is reported in the unit of [10−9 m2/s]; RDF MAE and FPS are unitless.\n\nDeepPot-SE SchNet\n\nDimeNet\n\nForceNet\n\nGemNet-T GemNet-dT NequIP\n\nForce Stability RDF(O,O) RDF(H,H) RDF(H,O) Diffusivity\n\n6.7 108(117) 0.17(0.10) 0.13(0.09) 0.28(0.15) 0.24\n\n13.1 175(56) 0.52(0.05) 0.24(0.02) 0.54(0.01) 1.79\n\n3.5 4(4) 0.46(0.22) 0.33(0.15) 0.43(0.17) -\n\n13.6 13(7) 0.86(0.09) 0.56(0.04) 1.44(0.09) -\n\n5.0 6(7) 0.62(0.48) 0.35(0.21) 0.71(0.65) -\n\n29.2 0(0) -\n- -\n-\n\n1.4 500(0) 0.07(0.02) 0.07(0.02) 0.26(0.07) 0.37\n\nwork (Batzner et al., 2022) as each structure already contains a reasonable number of atoms and interactions. Following previous work, we use an initial learning rate of 0.001 for all experiments except for NequIP, which uses 0.005 as the initial learning rate in the original paper. For models that minimize a mixture of force loss and energy loss, we set the force loss coefficient λF to be 1000 and the energy loss coefficient λE to be 1, if not specified in the original paper. A higher force loss coefficient is common in previous work (Zhang et al., 2018a; Batzner et al., 2022) as simulations do not directly rely on the energy.\n\nNotably, NequIP proposed several sets of hyperparameters for different datasets, including MD17, a water+ice dataset from Zhang et al. 2018a, LiPS, etc. We follow the MD17 hyperparameters of NequIP for our MD17 and alanine dipeptide datasets; the water+ice hyperparameters of NequIP for our water dataset; and the LiPS hyperparameters of NequIP for the same LiPS dataset. For DeepPot-SE, we adopted hyperparameters introduced in Zhang et al. 2018a. The only architectural adjustment we made is because we observed training instability for ForceNet on water using the original hyperparameters. We resolve this issue by reducing the network width from 512 to 128 for ForceNet in our water experiments.\n\nTo facilitate benchmarking with a reasonable computational budget, we stop the training of a model if either of the following conditions is met: (1) a maximum training time of 7 days is reached on an NVIDIA Tesla V100-PCIe GPU; (2) a maximum number of epochs specified in Table 7 is reached; (3) The learning rate drops below 10−6 with a ReduceLROnPlateau scheduler with factor 0.8 and learning rate (LR) patience specified in Table 7. We also report the longest time for an ML model to finish our benchmark simulation in Table 7. All numbers are results of NequIP. The high computational cost for evaluating MD simulations has been a major consideration in designing our benchmark datasets and metrics. Training of DeepPot-SE is efficient and we follow the training setup specified in Zhang et al. 2018a.\n\nComplete water results. We present results on water-1k in Table 8 and results on water-90k in Table 9. Results on water-10k is presented in Table 4 in the main text. All models generally achieve lower force error when trained with more data, but stability and estimation of ensemble statistics don’t necessarily improve. In particular, DeepPot-SE shows clear improvement with more training data and becomes as good as NequIP on water-90k. SchNet demonstrates significant improvement in stability, but the estimation of ensemble statistics does not improve. This may be due to the limited accuracy of SchNet coming from the limited expressiveness of the invariant atomic representation.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: Results on Water-90k. Force MAE is reported in the unit of [meV/Å]; Stability is reported in the unit of [ps]; Diffusivity MAE is reported in the unit of [10−9 m2/s]; RDF MAE and FPS are unitless.\n\nDeepPot-SE SchNet\n\nDimeNet\n\nForceNet\n\nGemNet-T GemNet-dT NequIP\n\nForce Stability RDF(O,O) RDF(H,H) RDF(H,O) Diffusivity\n\n5.9 500(0) 0.07(0.02) 0.05(0.01) 0.29(0.08) 0.35\n\n8.4 299(70) 0.67(0.03) 0.31(0.02) 0.67(0.04) 1.97\n\n1.7 36(9) 0.21(0.03) 0.14(0.01) 0.18(0.02) -\n\n8.6 9(12) 1.31(0.49) 0.82(0.26) 2.05(0.60) -\n\n0.7 20(9) 0.35(0.23) 0.25(0.19) 0.24(0.06) -\n\n1.1 8(10) 0.20(0.01) 0.16(0.01) 0.26(0.02) -\n\n1.4 500(0) 0.06(0.01) 0.04(0.01) 0.25(0.06) 0.18\n\nTable 10: Water-1k results on NequIP with various model sizes and radius cutoffs. Stability RDF(O,O) RDF(H,H) RDF(H,O) Diffusivity\n\nForce\n\nWidth=64, r=4 Width=32, r=6 Width=64, r=5 Width=64, r=6 Width=128, r=6\n\n3.5 1.5 1.6 1.4 1.5\n\n500(0) 500(0) 500(0) 500(0) 500(0)\n\n0.07(0.02) 0.06(0.01) 0.07(0.02) 0.07(0.02) 0.07(0.02)\n\n0.05(0.01) 0.05(0.01) 0.05(0.01) 0.07(0.02) 0.05(0.01)\n\n0.27(0.06) 0.26(0.06) 0.27(0.06) 0.26(0.07) 0.29(0.07)\n\n0.38 0.25 0.31 0.37 0.37\n\nFPS\n\n8.2 5.2 4.9 3.9 2.5\n\nInfluence of model size. Table 10 shows an ablation study over the model size and radius cutoff of NequIP over water-1k. We observe that all models are highly stable and attain equally good performance in simulation-based metrics. Although a small radius cutoff of 4 leads to worse performance in force prediction, it is more computationally efficient and preserves the trajectory statistics. These results show that there exists a trade-off between accuracy and efficiency when choosing the hyperparameters of an ML force field, and force error may not be the preferred criterion for model selection.\n\nLarge water system of 512 molecules. To study model performance in generalizing to a larger system and model scalability, we evaluate models trained on the water-10k dataset on a dataset with 512 water molecules simulated for 1 ns, using the same reference force field. Given the high cost of simulating a large system, we simulate 5 trajectories of 150 ps long for each model. The results are shown in Table 11. We observe that all models suffer slightly higher force errors compared to evaluation over the 64-molecule water system. In terms of stability, NequIP and SphereNet always remains stable for the entire 150 ps. However, SphereNet does not produce correct ensemble properties. SchNet is the third stable model, while all other models are not stable enough for diffusivity computation. DimeNet, GemNet-T, and GemNet-dT are not stable through the entire simulation, but can produce decent RDF results. Noticeably, the stability of DeepPot-SE drops significantly. We hypothesize that the lack of message passing limits its capability in capturing long-range interactions and thus limits the performance in a larger system.\n\nStability’s relation with dataset size. We extract the force and stability results for each model from Figure 4 to make Figure 9 to better illustrate the relation between stability and dataset size for each model. We observe that while more data almost always reduce force error, stability does not necessarily improve. In particular, NequIP is highly stable across all dataset size. DeepPot-SE and SchNet has significant improvement in stability with more data. While for DimeNet ForceNet, and GemNet, more training data does not bring significant stability improvement. Section 6 contains detailed discussions on the causes of instability and potential solutions to improve stability.\n\nStability’s relation with training epochs. We study the evolution of simulation stability in the training process of an ML force field. We take the SchNet model on the MD17 molecule salicylic acid, and save the checkpoint at 100, 200, 300, 400, and 500 epochs. We conduct 5 simulations of 300 ps using each checkpoint. Figure 10 shows the force error and stability of the model at different stages of training. We observe that the force error decreases as training progress, and the stability improves to be stable across the entire 300 ps simulation and training epoch 300. This result reveals that thorough training is important to both accuracy and stability of ML force fields.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nTable 11: Results on the large water system with 512 molecules, with models trained on the water-10k dataset (64-molecule water system). *SphereNet on Water-large requires more memory than Tesla V100 supports. We run its simulations on faster NVIDIA A100 cards so the FPS is not entirely comparable to other models.\n\nDeepPot-SE SchNet\n\nDimeNet\n\nPaiNN\n\nSphereNet*\n\nForceNet\n\nGemNet-T GemNet-dT NequIP\n\nForce Stability RDF(O,O) RDF(H,H) RDF(H,O) Diffusivity FPS\n\n10.6 19(22) 0.23(0.06) 0.24(0.06) 0.67(0.27) -\n80.7\n\n12.1 118(58) 0.62(0.01) 0.30(0.04) 0.55(0.01) 2.54 23.1\n\n5.1 38(13) 0.17(0.03) 0.12(0.03) 0.17(0.02) -\n3.5\n\n9.7 16(12) 0.31(0.06) 0.21(0.05) 0.29(0.05) -\n17.4\n\n18.4 150(0) 0.93(0.02) 0.42(0.01) 0.97(0.03) 2.98 0.8\n\n13.2 8(0) 0.74(0.02) 0.51(0.02) 1.38(0.05) -\n11.9\n\n5.6 45(25) 0.22(0.16) 0.15(0.11) 0.23(0.12) -\n2.2\n\n4.2 50(9) 0.16(0.02) 0.11(0.01) 0.16(0.02) -\n5.3\n\n7.7 150(0) 0.10(0.01) 0.07(0.00) 0.12(0.02) 0.89 0.7\n\nFigure 9: The force error and stability of all models on the water dataset, with varying training dataset size: 1k, 10k, and 90k.\n\nDistribution of interatomic distances for MD17. Figure 11 shows the h(r) curves for all models and molecules benchmarked. We randomly selected one simulation out of the five simulations we conducted for each model and molecule. We observe that due to lack of stability, DeepPotSE produces noisy h(r) on Aspirin. ForceNet does not manage to learn the correct interatomic interactions and produces incorrect h(r) curves. Most models are able to produce h(r) that match well with the reference, with SchNet being less accurate on Aspirin and Ethanol.\n\nRDFs for water. Selected RDF curves for water1k/10k/90k are in Figure 12, Figure 13, and Figure 14. Most noisy curves are due to insufficient sampling time, which results in a small number of frames to be averaged in obtaining the RDF curves. We observe that SchNet and ForceNet produce inaccurate curves that are not very noisy, showing that their failure is not entirely due to lack of stability but because of inaccurate modeling of interactions caused by limited expressiveness and lower sample efficiency. Further, we note that the reference curves have zero values below a certain threshold, as any pair of atoms cannot get too close to each other. However, DimeNet and GemNet-T exhibit abnormal high values for very small distances, indicating the simulations have gone into nonphysical configurations and collapsed.\n\nFigure 10: The force error and stability of SchNet for simulating Salicylic acid, as traing progress.\n\nRDFs for LiPS. As shown in Figure 15, DeepPot-SE does not manage to stay stable on LiPS. ForceNet learns inaccurate interactions and produces inaccurate RDFs. All other models can produce highly accurate RDF and can reproduce Li-ion diffusivity relatively accurately, as demonstrated in Table 6.\n\n22\n\n1k10k90kTraining Data Size0123456Force MAE [meV/Å]Water, DeepPot-SE1k10k90kTraining Data Size024681012Water, SchNet1k10k90kTraining Data Size0.00.51.01.52.02.53.03.5Water, DimeNet1k10k90kTraining Data Size02468101214Water, ForceNet1k10k90kTraining Data Size012345Force MAE [meV/Å]Water, GemNet-T1k10k90kTraining Data Size051015202530Water, GemNet-dT1k10k90kTraining Data Size0.00.20.40.60.81.01.21.4Water, NequIP010020030040050005010015020025030035001020304005101520Stability [ps]0510152025300.02.55.07.510.012.515.00100200300400500Stability [ps]100200300400500Training Epoch0510152025303540Force MAE [meV/Å]050100150200250300350Stability [ps]Under review as a conference paper at ICLR 2023\n\nFigure 11: h(r) of md17.\n\n23\n\n02468100.00.51.0h(r) [Å1]DeepPot-SE, AspirinReferencePrediction02468100.00.51.01.5DeepPot-SE, EthanolReferencePrediction02468100.00.20.40.60.8DeepPot-SE, NaphthaleneReferencePrediction02468100.00.20.40.60.8DeepPot-SE, Salicylic AcidReferencePrediction02468100.00.20.4h(r) [Å1]SchNet, AspirinReferencePrediction02468100.00.51.0SchNet, EthanolReferencePrediction02468100.00.20.40.60.8SchNet, NaphthaleneReferencePrediction02468100.00.20.40.60.8SchNet, Salicylic AcidReferencePrediction02468100.00.20.4h(r) [Å1]DimeNet, AspirinReferencePrediction02468100.00.51.01.52.0DimeNet, EthanolReferencePrediction02468100.00.20.40.60.8DimeNet, NaphthaleneReferencePrediction02468100.00.20.40.60.8DimeNet, Salicylic AcidReferencePrediction02468100.00.20.4h(r) [Å1]PaiNN, AspirinReferencePrediction02468100.00.51.01.5PaiNN, EthanolReferencePrediction02468100.00.20.40.60.8PaiNN, NaphthaleneReferencePrediction02468100.00.20.40.60.8PaiNN, Salicylic AcidReferencePrediction02468100.00.20.4h(r) [Å1]SphereNet, AspirinReferencePrediction02468100.00.51.01.5SphereNet, EthanolReferencePrediction02468100.00.20.40.60.8SphereNet, NaphthaleneReferencePrediction02468100.00.20.40.60.8SphereNet, Salicylic AcidReferencePrediction02468100.00.51.01.5h(r) [Å1]ForceNet, AspirinReferencePrediction02468100246ForceNet, EthanolReferencePrediction0246810012ForceNet, NaphthaleneReferencePrediction02468100.000.250.500.751.00ForceNet, Salicylic AcidReferencePrediction02468100.00.20.4h(r) [Å1]GemNet-T, AspirinReferencePrediction02468100.00.51.01.5GemNet-T, EthanolReferencePrediction02468100.00.20.40.60.8GemNet-T, NaphthaleneReferencePrediction02468100.00.20.40.60.8GemNet-T, Salicylic AcidReferencePrediction02468100.00.20.4h(r) [Å1]GemNet-dT, AspirinReferencePrediction02468100.00.51.01.5GemNet-dT, EthanolReferencePrediction02468100.00.20.40.60.8GemNet-dT, NaphthaleneReferencePrediction02468100.00.20.40.60.8GemNet-dT, Salicylic AcidReferencePrediction0246810r [Å]0.00.20.4h(r) [Å1]NequIP, AspirinReferencePrediction0246810r [Å]0.00.51.01.5NequIP, EthanolReferencePrediction0246810r [Å]0.00.20.40.60.8NequIP, NaphthaleneReferencePrediction0246810r [Å]0.00.20.40.60.8NequIP, Salicylic AcidReferencePredictionUnder review as a conference paper at ICLR 2023\n\nFigure 12: RDFs of Water-1k. GemNet-dT does not remain stable for more than 1 ps and is therefore not feasible for RDF computation.\n\n24\n\n01234560123RDF(r) [Å1]DeepPot-SE, O-OReferencePrediction012345601234DeepPot-SE, H-HReferencePrediction012345601020304050DeepPot-SE, H-OReferencePrediction01234560123RDF(r) [Å1]SchNet, O-OReferencePrediction012345601234SchNet, H-HReferencePrediction012345601020304050SchNet, H-OReferencePrediction01234560123RDF(r) [Å1]DimeNet, O-OReferencePrediction012345601234DimeNet, H-HReferencePrediction012345601020304050DimeNet, H-OReferencePrediction0123456012345RDF(r) [Å1]ForceNet, O-OReferencePrediction01234560123456ForceNet, H-HReferencePrediction01234560204060ForceNet, H-OReferencePrediction01234560246810RDF(r) [Å1]GemNet-T, O-OReferencePrediction0123456012345GemNet-T, H-HReferencePrediction01234560204060GemNet-T, H-OReferencePrediction0123456r [Å]0.00.51.01.52.02.53.0RDF(r) [Å1]NequIP, O-OReferencePrediction0123456r [Å]01234NequIP, H-HReferencePrediction0123456r [Å]01020304050NequIP, H-OReferencePredictionUnder review as a conference paper at ICLR 2023\n\nFigure 13: RDFs of Water-10k.\n\n25\n\n01234560123RDF(r) [Å1]DeepPot-SE, O-OReferencePrediction012345601234DeepPot-SE, H-HReferencePrediction012345602040DeepPot-SE, H-OReferencePrediction01234560123RDF(r) [Å1]SchNet, O-OReferencePrediction012345601234SchNet, H-HReferencePrediction012345602040SchNet, H-OReferencePrediction012345602468RDF(r) [Å1]DimeNet, O-OReferencePrediction012345601234DimeNet, H-HReferencePrediction0123456010203040DimeNet, H-OReferencePrediction01234560123RDF(r) [Å1]PaiNN, O-OReferencePrediction012345601234PaiNN, H-HReferencePrediction0123456010203040PaiNN, H-OReferencePrediction012345601234RDF(r) [Å1]SphereNet, O-OReferencePrediction012345601234SphereNet, H-HReferencePrediction012345602040SphereNet, H-OReferencePrediction0123456024RDF(r) [Å1]ForceNet, O-OReferencePrediction01234560246ForceNet, H-HReferencePrediction01234560204060ForceNet, H-OReferencePrediction01234560123RDF(r) [Å1]GemNet-T, O-OReferencePrediction012345601234GemNet-T, H-HReferencePrediction012345602040GemNet-T, H-OReferencePrediction01234560123RDF(r) [Å1]GemNet-dT, O-OReferencePrediction012345601234GemNet-dT, H-HReferencePrediction0123456010203040GemNet-dT, H-OReferencePrediction0123456r [Å]0123RDF(r) [Å1]NequIP, O-OReferencePrediction0123456r [Å]01234NequIP, H-HReferencePrediction0123456r [Å]02040NequIP, H-OReferencePredictionUnder review as a conference paper at ICLR 2023\n\nFigure 14: RDFs of Water-90k.\n\n26\n\n01234560.00.51.01.52.02.53.0RDF(r) [Å1]DeepPot-SE, O-OReferencePrediction012345601234DeepPot-SE, H-HReferencePrediction012345601020304050DeepPot-SE, H-OReferencePrediction01234560123RDF(r) [Å1]SchNet, O-OReferencePrediction012345601234SchNet, H-HReferencePrediction012345601020304050SchNet, H-OReferencePrediction01234560123RDF(r) [Å1]DimeNet, O-OReferencePrediction012345601234DimeNet, H-HReferencePrediction0123456010203040DimeNet, H-OReferencePrediction0123456012345RDF(r) [Å1]ForceNet, O-OReferencePrediction01234560123456ForceNet, H-HReferencePrediction01234560204060ForceNet, H-OReferencePrediction01234560123RDF(r) [Å1]GemNet-T, O-OReferencePrediction012345601234GemNet-T, H-HReferencePrediction0123456010203040GemNet-T, H-OReferencePrediction01234560123RDF(r) [Å1]GemNet-dT, O-OReferencePrediction012345601234GemNet-dT, H-HReferencePrediction0123456010203040GemNet-dT, H-OReferencePrediction0123456r [Å]0.00.51.01.52.02.53.0RDF(r) [Å1]NequIP, O-OReferencePrediction0123456r [Å]01234NequIP, H-HReferencePrediction0123456r [Å]010203040NequIP, H-OReferencePredictionUnder review as a conference paper at ICLR 2023\n\nFigure 15: RDFs of LiPS.\n\n27\n\n0246r [Å]012RDF(r) [Å1]DeepPot-SEReferencePrediction0246r [Å]012SchNetReferencePrediction0246r [Å]012DimeNetReferencePrediction0246r [Å]024ForceNetReferencePrediction0246r [Å]012RDF(r) [Å1]GemNet-TReferencePrediction0246r [Å]012GemNet-dTReferencePrediction0246r [Å]012NequIPReferencePrediction",
    "reference": "# Summary Of The Paper\n\nThe authors of this paper propose a benchmark suite for ML MD. Within the suite, 7 deep learning based methods are included as base models and 4 datasets of different MD systems were tailored for simulation. From intensive comparison and analysis, the authors claim that force error is not a sufficient measure to evaluate ML MD performance, and stability should be considered in benchmarking.\n\n# Strength And Weaknesses\n\nStrengths:\n1. The comparison and analysis in this paper is through. Insights into failure cases are provided.\n2. The efforts of providing a benchmarking suite should be praised, considering that ML MD is a new area in AI for chemoinformatics and this is a rather challenging task.\n\nWeaknesses: \n1. The size of each simulation systems is restricted to have less than 200 atoms. This restriction may limit future scalability studies.\n2. The origins of these four datasets are unclear. The authors should explicitly mention whether each data set was generated from experiments or in silico simulation. \n3. Mathematical MD simulation tools such as GROMACS and Amber may be included in comparison if a data set was generated from experiments.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClearly, it is a benchmarking and analysis paper in AIMD. The major novelty of this paper is the provision of a suite with baseline models, datasets, and evaluation metrics.\n\n# Summary Of The Review\n\nOverall, I think this work is very informative and useful for researchers in the area of AI for chemoinformatics.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nMORE CENTRALIZED TRAINING, STILL DECENTRALIZED EXECUTION: MULTI-AGENT CONDITIONAL POLICY FACTORIZATION\n\nJiangxing Wang School of Computer Science Peking University jiangxiw@stu.pku.edu.cn\n\nDeheng Ye Tencent Inc. dericye@tencent.com\n\nZongqing Lu† School of Computer Science Peking University zongqing.lu@pku.edu.cn\n\nABSTRACT\n\nIn cooperative multi-agent reinforcement learning (MARL), combining value decomposition with actor-critic enables agents to learn stochastic policies, which are more suitable for the partially observable environment. Given the goal of learning local policies that enable decentralized execution, agents are commonly assumed to be independent of each other, even in centralized training. However, such an assumption may prohibit agents from learning the optimal joint policy. To address this problem, we explicitly take the dependency among agents into centralized training. Although this leads to the optimal joint policy, it may not be factorized for decentralized execution. Nevertheless, we theoretically show that from such a joint policy, we can always derive another joint policy that achieves the same optimality but can be factorized for decentralized execution. To this end, we propose multi-agent conditional policy factorization (MACPF), which takes more centralized training but still enables decentralized execution. We empirically verify MACPF in various cooperative MARL tasks and demonstrate that MACPF achieves better performance or faster convergence than baselines. Our code is available at https://github.com/PKU-RL/FOP-DMAC-MACPF.\n\n1\n\nINTRODUCTION\n\nThe cooperative multi-agent reinforcement learning (MARL) problem has attracted the attention of many researchers as it is a well-abstracted model for many real-world problems, such as traffic signal control (Wang et al., 2021a) and autonomous warehouse (Zhou et al., 2021). In a cooperative MARL problem, we aim to train a group of agents that can cooperate to achieve a common goal. Such a common goal is often defined by a global reward function that is shared among all agents. If centralized control is allowed, such a problem can be viewed as a single-agent reinforcement learning problem with an enormous action space. Based on this intuition, Kraemer & Banerjee (2016) proposed the centralized training with decentralized execution (CTDE) framework to overcome the non-stationarity of MARL. In the CTDE framework, a centralized value function is learned to guide the update of each agent’s local policy, which enables decentralized execution.\n\nWith a centralized value function, there are different ways to guide the learning of the local policy of each agent. One line of research, called value decomposition (Sunehag et al., 2018), obtains local policy by factorizing this centralized value function into the utility function of each agent. In order to ensure that the update of local policies can indeed bring the improvement of joint policy, Individual-Global-Max (IGM) is introduced to guarantee the consistency between joint and local policies. Based on the different interpretations of IGM, various MARL algorithms have been proposed, such as VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), QTRAN (Son et al., 2019), and QPLEX (Wang et al., 2020a). IGM only specifies the relationship between optimal local actions and optimal joint action, which is often used to learn deterministic policies. In order to learn stochastic policies, which are more suitable for the partially observable environment, recent studies (Su et al., 2021; Wang et al., 2020b; Zhang et al., 2021; Su & Lu, 2022) combine the idea of\n\n†Corresponding Author\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nvalue decomposition with actor-critic. While most of these decomposed actor-critic methods do not guarantee optimality, FOP (Zhang et al., 2021) introduces Individual-Global-Optimal (IGO) for the optimal joint policy learning in terms of maximum-entropy objective and derives the corresponding way of value decomposition. It is proved that factorized local policies of FOP converge to the global optimum, given that IGO is satisfied.\n\nThe essence of IGO is for all agents to be independent of each other during both training and execution. However, we find this requirement dramatically reduces the expressiveness of the joint policy, making the learning algorithm fail to converge to the global optimal joint policy, even in some simple scenarios. As centralized training is allowed, a natural way to address this issue is to factorize the joint policy based on the chain rule (Schum, 2001), such that the dependency among agents’ policies is explicitly considered, and the full expressiveness of the joint policy can be achieved. By incorporating such a joint policy factorization into the soft policy iteration (Haarnoja et al., 2018), we can obtain an optimal joint policy without the IGO condition. Though optimal, a joint policy induced by such a learning method may not be decomposed into independent local policies, thus decentralized execution is not fulfilled, which is the limitation of many previous works that consider dependency among agents (Bertsekas, 2019; Fu et al., 2022).\n\nTo fulfill decentralized execution, we first theoretically show that for such a dependent joint policy, there always exists another independent joint policy that achieves the same expected return but can be decomposed into independent local policies. To learn the optimal joint policy while preserving decentralized execution, we propose multi-agent conditional policy factorization (MACPF), where we represent the dependent local policy by combining an independent local policy and a dependency policy correction. The dependent local policies factorize the optimal joint policy, while the independent local policies constitute their independent counterpart that enables decentralized execution. We evaluate MACPF in several tasks, including matrix game (Rashid et al., 2020), SMAC (Samvelyan et al., 2019), and MPE (Lowe et al., 2017). Empirically, MACPF consistently outperforms its base method, i.e., FOP, and achieves better performance or faster convergence than other baselines. By ablation, we verify that the independent local policies can indeed obtain the same level of performance as the dependent local policies.\n\n2 PRELIMINARIES\n\n2.1 MULTI-AGENT MARKOV DECISION PROCESS\n\nIn cooperative MARL, we often formulate the problem as a multi-agent Markov decision process (MDP) (Boutilier, 1996). A multi-agent MDP can be defined by a tuple ⟨I, S, A, P, r, γ, N ⟩. N is the number of agents, I = {1, 2 . . . , N } is the set of agents, S is the set of states, and A = A1 ×· · ·×AN is the joint action space, where Ai is the individual action space for each agent i. For the rigorousness of proof, we assume full observability such that at each state s ∈ S, each agent i receives state s, chooses an action ai ∈ Ai, and all actions form a joint action a ∈ A. The state transitions to the next state s′ upon a according to the transition function P (s′|s, a) : S × A × S → [0, 1], and all agents receive a shared reward r(s, a) : S × A → R. The objective is to learn a local policy πi(ai| s) for each agent such that they can cooperate to maximize the expected cumulative discounted return, E[(cid:80)∞ t=0 γtrt], where γ ∈ [0, 1) is the discount factor. In CTDE, from a centralized perspective, a group of local policies can be viewed as a joint policy πjt(a| s). For this joint policy, we can define the joint state-action value function Qjt(st, at) = Est+1:∞,at+1:∞ [(cid:80)∞ k=0 γtrt+k| st, at]. Note that although we assume full observability for the rigorousness of proof, we use the trajectory of each agent τi ∈ Ti : (Y × Ai)∗ to replace state s as its policy input to settle the partial observability in practice, where Y is the observation space.\n\n2.2 FOP\n\nFOP (Zhang et al., 2021) is one of the state-of-the-art CTDE methods for cooperative MARL, which extends value decomposition to learning stochastic policy. In FOP, the joint policy is decomposed into independent local policies based on Individual-Global-Optimal (IGO), which can be stated as:\n\nπi(ai| s).\n\n(1)\n\nπjt(a| s) =\n\nN (cid:89)\n\ni=1\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nAs all policies are learned by maximum-entropy RL (Haarnoja et al., 2018), i.e., πi(ai| s) = exp( 1 (Qi(s, ai) − Vi(s))), IGO immediately implies a specific way of value decomposition: αi\n\nQjt(s, a) =\n\nN (cid:88)\n\ni=1\n\nα αi\n\n[Qi(s, ai) − Vi(s)] + Vjt(s).\n\n(2)\n\nUnlike IGM, which is used to learn deterministic local policies and naturally avoids the dependency of agents, IGO assumes agents are independent of each other in both training and execution. Although IGO advances FOP to learn stochastic policies, such an assumption can be problematic even in some simple scenarios and prevent learning the optimal joint policy.\n\n2.3 PROBLEMATIC IGO\n\n(a) centralized control\n\n(b) FOP\n\n(c) dependency\n\nFigure 1: Sampled trajectories from the learned policy: (a) centralized control; (b) FOP, where IGO is assumed; (c) considering dependency during training.\n\nAs stated in soft Q-learning (Haarnoja et al., 2018), one goal of maximum-entropy RL is to learn an optimal maximum-entropy policy that captures multiple modes of near-optimal behavior. Since FOP can be seen as the extension of maximum-entropy RL in multi-agent settings, it is natural to assume that FOP can also learn a multi-modal joint policy in multi-agent settings. However, as shown in the following example, such a desired property of maximum-entropy RL is not inherited in FOP due to the IGO condition.\n\nWe extend the single-agent multi-goal environment used in soft Q-learning (Haarnoja et al., 2018) to its multi-agent variant to illustrate the problem of IGO. In this environment, we want to control a 2D point mass to reach one of four symmetrically placed goals, as illustrated in Figure 1. The reward is defined as a mixture of Gaussians, with means placed at the goal positions. Unlike the original environment, this 2D point mass is now jointly controlled by two agents, and it can only move when these two agents select the same moving direction; otherwise, it will stay where it is. As shown in Figure 1a, when centralized control is allowed, multi-agent training degenerates to single-agent training, and the desired multi-modal policy can be learned. However, as shown in Figure 1b, FOP struggles to learn any meaningful joint policy for the multi-agent setting. One possible explanation is that, since IGO is assumed in FOP, the local policy of each agent is always independent of each other during training, and the expressiveness of joint policy is dramatically reduced. Therefore, when two agents have to coordinate to make decisions, they may fail to reach an agreement and eventually behave in a less meaningful way due to the limited expressiveness of joint policy. To solve this problem, we propose to consider dependency among agents in MARL algorithms to enrich the expressiveness of joint policy. As shown in Figure 1c, the learned joint policy can once again capture multiple modes of near-optimal behavior when the dependency is considered. Details of this algorithm will be discussed in the next section.\n\n3 METHOD\n\nTo overcome the aforementioned problem of IGO, we propose multi-agent conditional policy factorization (MACPF). In MACPF, we introduce dependency among agents during centralized training to ensure the optimality of the joint policy without the need for IGO. This joint policy consists of dependent local policies, which take the actions of other agents as input, and we use this joint policy as the behavior policy to interact with the environment during training. In order to fulfill\n\n3\n\n642024664202462.9602.9602.9602.9602.9602.6312.6312.6312.6312.6312.3022.3022.3022.3021.9731.9731.9731.9731.6451.6451.6451.6451.3161.3161.3161.3160.9870.9870.9870.9870.6580.6580.6580.6580.3290.3290.3290.329642024664202462.9602.9602.9602.9602.9602.6312.6312.6312.6312.6312.3022.3022.3022.3021.9731.9731.9731.9731.6451.6451.6451.6451.3161.3161.3161.3160.9870.9870.9870.9870.6580.6580.6580.6580.3290.3290.3290.329642024664202462.9602.9602.9602.9602.9602.6312.6312.6312.6312.6312.3022.3022.3022.3021.9731.9731.9731.9731.6451.6451.6451.6451.3161.3161.3161.3160.9870.9870.9870.9870.6580.6580.6580.6580.3290.3290.3290.329Published as a conference paper at ICLR 2023\n\ndecentralized execution, independent local policies are obtained from these dependent local policies such that the joint policy resulting from these independent local policies is equivalent to the behavior policy in terms of expected return.\n\n3.1 CONDITIONAL FACTORIZED SOFT POLICY ITERATION\n\nLike FOP, we also use maximum-entropy RL (Ziebart, 2010) to bridge policy and state-action value function for each agent. Additionally, it will also be used to introduce dependency among agents. For each local policy, we take the actions of other agents as its input and define it as follows:\n\nπi(ai| s, a<i) = exp(\n\n1 αi\n\n(Qi(s, a<i, ai) − Vi(s, a<i)))\n\nVi(s, a<i) := αi\n\n(cid:88)\n\nai\n\nexp(\n\n1 αi\n\nQi(s, a<i, ai)),\n\n(3)\n\n(4)\n\nwhere a<i represents the joint action of all agents whose indices are smaller than agent i. We then can get the relationship between the joint policy and local policies based on the chain rule factorization of joint probability:\n\nπjt(a| s) =\n\nN (cid:89)\n\ni=1\n\nπi(ai| s, a<i).\n\n(5)\n\nThe full expressiveness of the joint policy can be guaranteed by (5) as it is no longer restricted by the IGO condition. From (5), together with πjt(a| s) = exp( 1 α (Qjt(s, a) − Vjt(s))), we have the Qjt factorization as:\n\nQjt(s, a) =\n\nN (cid:88)\n\ni=1\n\nα αi\n\n[Qi(s, a<i, ai) − Vi(s, a<i)] + Vjt(s).\n\n(6)\n\nNote that in maximum-entropy RL, we can easily compute V by Q. From (6), we introduce conditional factorized soft policy iteration and prove its convergence to the optimal joint policy in the following theorem.\n\nTheorem 1 (Conditional Factorized Soft Policy Iteration). For any joint policy πjt, if we repeatedly apply joint soft policy evaluation and individual conditional soft policy improvement from πi ∈ Πi. π∗ Then the joint policy πjt(a| s) = (cid:81)N jt (s, a) ≥ Qπjt\n\ni=1 πi(ai| s, a<i) converges to π∗\n\njt (s, a) for all πjt, assuming |A| < ∞.\n\njt, such that Q\n\njt\n\nProof. See Appendix A.\n\n3.2\n\nIndependent JOINT POLICY\n\nUsing the conditional factorized soft policy iteration, we are able to get the optimal joint policy. However, such a joint policy requires dependent local policies, which are incapable of decentralized execution. To fulfill decentralized execution, we have to obtain independent local policies.\n\na2\n\na1\n\nA B\n\nA\n\n0.5 0\n\nB\n\n0 0.5\n\na2 A B\n\n1 0\n\n0 0\n\na1\n\nA B\n\na2\n\na1\n\nA B\n\nA\n\n−0.5 0\n\nB\n\n0 0.5\n\n(a) dependent joint policy πdep\n\njt\n\n(b) independent joint policy πind\n\njt\n\n(c) dependency correction bdep\n\njt\n\nFigure 2: A dependent joint policy and its independent counterpart\n\nConsider the joint policy shown in Figure 2a. This joint policy, called dependent joint policy πdep ,\ninvolves dependency among agents and thus cannot be factorized into two independent local policies. However, one may notice that this policy can be decomposed as the combination of an independent joint policy πind that involves no dependency among agents, as shown in Figure 2b, and a dependency\n\njt\n\njt\n\n4\n\nPublished as a conference paper at ICLR 2023\n\npolicy correction bdep , as shown in Figure 2c. More importantly, since we use the Boltzmann distribution of joint Q-values as the joint policy, the equivalence of probabilities of two joint actions also indicates that their joint Q-values are the same,\n\njt\n\njt (A, A) = πdep πdep\n\njt (B, B) ⇒ Qjt(A, A) = Qjt(B, B). Therefore, in Table 2, the expected return of the independent joint policy πind dependent joint policy πdep\n\n,\n\njt\n\njt will be the same as the\n\nE\n\nπdep\n\njt\n\n[Qjt] = πdep\n\njt (A, A) ∗ Qjt(A, A) + πdep jt (A, A) ∗ Qjt(A, A) = E\n\n= πind\n\n[Qjt].\n\nπind jt\n\njt (B, B) ∗ Qjt(B, B)\n\nFormally, we have the following theorem. Theorem 2. For any dependent joint policy πdep exists an independent joint policy πind (s) for any state s ∈ S. Vπdep\n\n(s) = Vπind\n\njt\n\njt\n\njt\n\nthat involves dependency among agents, there that does not involve dependency among agents, such that\n\njt\n\nProof. See Appendix B.\n\nNote that the independent counterpart of the optimal dependent joint policy may not be directly learned by FOP, as shown in Figure 1. Therefore, we need to explicitly learn the optimal dependent joint policy to obtain its independent counterpart.\n\n3.3 MACPF FRAMEWORK\n\nWith Theorem 1 and 2, we are ready to present the learning framework of MACPF, as illustrated in Figure 3, for simultaneously learning the dependent joint policy and its independent counterpart.\n\nIn MACPF, each agent i has an independent local policy πind dependency policy correction bdep dependent local policy πdep πdep\n\n(ai| s, a<i)1. So, we have: (ai| s, a<i) = πind\n\n(ai| s; θi) + bdep\n\ni\n\ni\n\ni\n\ni\n\n(ai| s; θi) parameterized by θi and a (ai| s, a<i; φi) parameterized by φi, which together constitute a\n\ni\n\n(ai| s, a<i; φi)\n\ni\n\nπdep\n\njt (s, a) =\n\nπind\n\njt (s, a) =\n\nN (cid:89)\n\ni=1\n\nN (cid:89)\n\ni=1\n\nπdep\n\ni\n\n(ai| s, a<i)\n\nπind\n\ni\n\n(ai| s; θi).\n\nSimilarly, each agent i also has an independent local critic Qind dependency critic correction cdep pendent local critic Qdep parameterized by Θ, to get Qdep\n\n(ai| s, a<i). Given all Qind and Qind\n\n(ai| s; ψi) parameterized by ψi and a (ai| s, a<i; ωi) parameterized by ωi, which together constitute a de- , we use a mixer network, Mixer(·; Θ)\n\nand Qdep\n\ni\n\ni\n\ni\n\ni\n\ni\n\njt as follows,\n\njt\n\ni\n\nQdep Qdep\n\n(ai| s, a<i) = Qind jt (s, a) = Mixer([Qdep jt (s, a) = Mixer([Qind\n\n(ai| s; ψi) + cdep (ai| s, a<i)]N (ai| s; ψi)]N\n\nQind\n\ni\n\ni\n\ni\n\ni\n\ni=1, s; Θ)\n\ni=1, s; Θ).\n\n(ai| s, a<i; ωi)\n\n, and Mixer are learned by minimizing the TD error,\n\nQdep\n\ni\n\n, Qind i\n\nLdep([ωi]N\n\ni=1, Θ) = ED\n\nLind([ψi]N\n\ni=1, Θ) = ED\n\n(cid:20)(cid:16)\n\nQdep\n\njt (s, a) −\n\n(cid:16)\n\n(cid:20)(cid:16)\n\nQind\n\njt (s, a) −\n\n(cid:16)\n\nr + γ(cid:0) ˆQdep\n\njt (s′, a′) − α log πdep\n\nr + γ(cid:0) ˆQind\n\njt (s′, a′) − α log πind\n\njt (a′| s′)(cid:1)(cid:17)(cid:17)2(cid:21) jt (a′| s′)(cid:1)(cid:17)(cid:17)2(cid:21)\n\n,\n\n1The logit of πind\n\ni\n\n(ai| s; θi) is first added with bdep\n\n(ai| s, a<i; φi) to get the logit of πdep\n\n(ai| s, a<i), then\n\ni\n\nsoftmax is used over this combined logit to get πdep\n\ni (ai| s, a<i).\n\ni\n\n5\n\n(7)\n\n(8)\n\n(9)\n\n(10)\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\n(17)\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Learning framework of MACPF, where each agent i has four modules: an independent local policy πind (·; ψi), and a dependency critic correction cdep\n\n(·; θi), a dependency policy correction bdep (·, ωi).\n\n(·; φi), an independent local critic Qind\n\ni\n\ni\n\ni\n\ni\n\njt\n\nand πind\n\nwhere D is the replay buffer collected by πdep the current πdep performance as πdep worth noting that the gradient of Ldep only updates [cdep and πind [Qind i\n\n, ˆQ is the target network, and a′ is sampled from jt has the same , the same batch sampled from D is used to compute both Ldep and Lind. It is i=1, while the gradient of Lind only updates ]N\n\n, respectively. To ensure the independent joint policy πind\n\nare updated by minimizing KL-divergence as follows,\n\ni=1. Then, πdep ]N\n\njt\n\njt\n\njt\n\ni\n\ni\n\ni\n\nJ dep(φi) = E\n\nD,a<i∼πdep\n\nJ ind(θi) = E\n\nD,ai∼πind\n\ni\n\n<i ,ai∼πdep [αi log πind\n\ni\n\ni\n\n[αi log πdep\n\ni\n\n(ai| s, a<i) − Qdep\n\ni\n\n(ai| s, a<i)]\n\n(ai| s; θi) − Qind\n\ni\n\n(ai| s; ψi)].\n\nSimilarly, the gradient of J dep only updates bdep computing J dep, a<i is sampled from their current policies πdep <i .\n\ni\n\nand the gradient of J ind only updates πind\n\ni\n\n(18)\n\n(19)\n\n. For\n\njt\n\njt\n\nis to enable decentralized execution while achieving the same perfor-\n\nThe purpose of learning πind mance as πdep . Therefore, a certain level of coupling has to be assured between πind . First, motivated by Figure 2, we constitute the dependent policy as a combination of an independent policy and a dependency policy correction, similarly for the local critic. Second, as aforementioned, the replay buffer D is collected by πdep is the behavior policy and the learning of πind comparison between πdep\n\nis offline. Third, we use the same Mixer to compute Qdep\n\n, which implies πdep\n\njt . The performance\n\njt will be studied by experiments.\n\nand πdep\n\nand Qind\n\nand πind\n\njt\n\njt\n\njt\n\njt\n\njt\n\njt\n\njt\n\n4 RELATED WORK\n\nMulti-agent policy gradient. In multi-agent policy gradient, a centralized value function is usually learned to evaluate current joint policy and guide the update of each local policy. Most multi-agent policy gradient methods can be considered as an extension of policy gradient from RL to MARL. For example, MAPPDG (Lowe et al., 2017) extends DDPG (Lillicrap et al., 2015), PS-TRPO(Gupta et al., 2017) and MATRPO (Kuba et al., 2021) extend TRPO (Schulman et al., 2015), and MAPPO (Yu et al., 2021) extends PPO (Schulman et al., 2017). Some methods additionally address multi-agent credit assignment by policy gradient, e.g., counterfactual policy gradient (Foerster et al., 2018) or difference rewards policy gradient (Castellini et al., 2021; Li et al., 2022).\n\nValue decomposition. Instead of providing gradients for local policies, in value decomposition, the centralized value function, usually a joint Q-function, is directly decomposed into local utility functions. Many methods have been proposed as different interpretations of Individual-GlobalMaximum (IGM), which indicates the consistency between optimal local actions and optimal joint action. VDN (Sunehag et al., 2018) and QMIX (Rashid et al., 2018) give sufficient conditions for IGM by additivity and monotonicity, respectively. QTRAN (Son et al., 2019) transforms IGM into optimization constraints, while QPLEX (Wang et al., 2020a) takes advantage of duplex dueling architecture to guarantee IGM. Recent studies (Su et al., 2021; Wang et al., 2020b; Zhang et al., 2021; Su & Lu, 2022) combine value decomposition with policy gradient to learn stochastic policies, which\n\n6\n\nMLPGRUMLPMLPDetachSoftmaxSoftmax+Independent Critic/PolicyDependent Critic/PolicyEnvironment. . .W2W1. . .++Mixer Network. . .Critic 1Critic nAgent 1Agent n. . .. . .MLPGRUMLPMLPDetach+Published as a conference paper at ICLR 2023\n\na2\n\na1\n\nA\n\nA\n\nB\n\nC D\n\n8 −20 −20 −20\n\nB −12\n\nC\n\n−12\n\n0\n\n0\n\n0 −20\n\n0 −20\n\nD −12 −12 −12\n\n8\n\n(a) payoff matrix\n\n(b) learning curves\n\na2\n\na1\n\nA\n\nB\n\nC\n\nD\n\nA\n\nB\n\nC D\n\n0.5\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0.5\n\n(c) πdep\n\njt\n\nof MACPF\n\nFigure 4: A matrix game that has two optimal joint actions: (a) payoff matrix; (b) learning curves of different methods; (c) the learned dependent joint policy of MACPF.\n\nare more desirable in partially observable environments. However, most research in this category does not guarantee optimality, while our method enables agents to learn the optimal joint policy.\n\nCoordination graph. In coordination graph (Guestrin et al., 2002) methods (Böhmer et al., 2020; Wang et al., 2021b; Yang et al., 2022), the interactions between agents are considered as part of value decomposition. Specifically, the joint Q-function is decomposed into the combination of utility functions and payoff functions. The introduction of payoff functions increases the expressiveness of the joint Q-function and considers at least pair-wise dependency among agents, which is similar to our algorithm, where the complete dependency is considered. However, to get the joint action with the maximum Q-value, communication between agents is required in execution in coordination graph methods, while our method still fulfills fully decentralized execution.\n\nCoordinated exploration. One of the benefits of considering dependency is coordinated exploration. From this perspective, our method might be seen as a relative of coordinated exploration methods (Mahajan et al., 2019; Iqbal & Sha, 2019; Zheng et al., 2021). In MAVEN (Mahajan et al., 2019), a shared latent variable is used to promote committed, temporally extended exploration. In EMC (Zheng et al., 2021), the intrinsic reward based on the prediction error of individual Q-values is used to induce coordinated exploration. It is worth noting that our method does not conflict with coordinated exploration methods and can be used simultaneously as our method is a base cooperative MARL algorithm. However, such a combination is beyond the scope of this paper.\n\n5 EXPERIMENTS\n\nIn this section, we evaluate MACPF in three different scenarios. One is a simple yet challenging matrix game, which we use to verify whether MACPF can indeed converge to the optimal joint policy. Then, we evaluate MACPF on two popular cooperative MARL scenarios: StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019) and MPE (Lowe et al., 2017), comparing it against QMIX (Rashid et al., 2018), QPLEX (Wang et al., 2020a), FOP (Zhang et al., 2021), and MAPPO (Yu et al., 2021). More details about experiments and hyperparameters are included in Appendix C. All results are presented using the mean and standard deviation of five runs with different random seeds. In SMAC experiments, for visual clarity, we plot the curves with the moving average of a window size of five and half standard deviation.\n\n5.1 MATRIX GAME\n\nIn this matrix game, we have two agents. Each can pick one of the four actions and get a reward based on the payoff matrix depicted in Figure 4a. Unlike the non-monotonic matrix game in QTRAN (Son et al., 2019), where there is only one optimal joint action, we have two optimal joint actions in this game, making this scenario much more challenging for many cooperative MARL algorithms.\n\nAs shown in Figure 4b, general value decomposition methods, QMIX, QPLEX, and FOP, fail to learn the optimal coordinated strategy in most cases. The same negative result can also be observed for MAPPO. For general MARL algorithms, since agents are fully independent of each other when making decisions, they may fail to converge to the optimal joint action, which eventually leads to a suboptimal joint policy. As shown in Figure 4b, QMIX and MAPPO fail to converge to the optimal policy but find a suboptimal policy in all the seeds, while QPLEX, QTRAN, and FOP find the optima by chance (i.e., 60% for QPLEX, 20% for QTRAN, and 40% for FOP). This is because, in QMIX, the\n\n7\n\n0200040006000800010000timesteps201510505rewardMatrix GameMACPFQMIXQPLEXFOPMAPPOMACPF_DEPQTRANPublished as a conference paper at ICLR 2023\n\nFigure 5: Learning curves of all the methods in six maps of SMAC, where the unit of x-axis is 1M timesteps and y-axis represents the win rate of each map.\n\nmixer network is purely a function of state and the input utility functions that are fully independent of each other. Thus it considers no dependency at all and cannot solve this game where dependency has to be considered. For QPLEX and FOP, since the joint action is considered as the input of their mixer network, the dependency among agents may be implicitly considered, which leads to the case where they can find the optima by chance. However, since the dependency is not considered explicitly, there is also a possibility that the mixer network misinterprets the dependency, which makes QPLEX and FOP sometimes find even worse policies than QMIX (20% for QPLEX and 40% for FOP). For QTRAN, it always finds at least the suboptimal policy in all the seeds. However, its optimality largely relies on the learning of its Vjt, which is very unstable, so it also only finds the optima by chance.\n\njt\n\nof MACPF, the local policy of the second agent depends on the\n\nFor the dependent joint policy πdep action of the first agent. As a result, we can see from Figure 4b that πdep always converges to the highest return. We also notice that in Figure 4c, πdep indeed captures two optimal joint actions. Unlike QMIX, QPLEX, and FOP, the mixer network in MACPF is a function of state and the input utility functions Qdep (ai| s, a<i) that are properly dependent on each other, so the dependency among agents is explicitly considered. More importantly, the learned independent joint policy πind jt of MACPF, denoted as MACPF in Figure 4b, always converges to the optimal joint policy. Note that in the rest of this section, the performance of MACPF is achieved by the learned πind jt\n\n(denoted as MACPF_DEP)\n\n, unless stated otherwise.\n\njt\n\njt\n\ni\n\n5.2 SMAC\n\nFurther, we evaluate MACPF on SMAC. Maps used in our experiment include two hard maps (8m_vs_9m, 10m_vs_11m), and two super-hard maps (MMM2, corridor). We also consider two challenging customized maps (8m_vs_9m_myopic, 10m_vs_11m_myopic), where the sight range of each agent is reduced from 9 to 6, and the information of allies is removed from the observation of agents. These changes are adopted to increase the difficulty of coordination in the original maps. Results are shown in Figure 5. In general, MACPF outperforms the baselines in all six maps. In hard maps, MACPF outperforms the baselines mostly in convergence speed, while in super-hard maps, MACPF outperforms other algorithms in either convergence speed or performance. Especially in corridor, when other value decomposition algorithms fail to learn any meaningful joint policies, MACPF obtains a winning rate of almost 70%. In the two more challenging maps, the margin between MACPF and the baselines becomes much larger than that in the original maps. These results show that MACPF can better handle complex cooperative tasks and learn coordinated strategies by introducing dependency among agents even when the task requires stronger coordination.\n\nWe compare MACFP with the baselines in 18 maps totally. Their final performance is summarized in Appendix D. The win rate of MACFP is higher than or equivalent to the best baseline in 16 out of 18 maps, while QMIX, QPLEX, MAPPO, and FOP are respectively 7/18, 8/18, 9/18, and 5/18.\n\n8\n\n��������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������Published as a conference paper at ICLR 2023\n\nFigure 6: Performance of πdep jt timesteps and y-axis represents the win rate of each map.\n\nand πind\n\njt during training in four maps of SMAC, where the unit of x-axis is 1M\n\nFigure 7: Ablation study in four maps of SMAC, where the unit of x-axis is 1M timesteps and y-axis represents the win rate of each map.\n\nDependent and Independent Joint Policy. As discussed in Section 3.2, the learned independent joint policy of MACPF should not only enable decentralized execution but also match the performance of dependent joint policy, as verified in the matrix game. What about complex environments like SMAC? As shown in Figure 6, we track the evaluation result of both πind jt during training. As we can see, their performance stays at the same level throughout training.\n\njt and πdep\n\nAblation Study. Without learning a dependent joint policy to interact with the environment, our algorithm degenerates to FOP. However, since our factorization of Qjt is induced from the chain rule factorization of joint probability (5), we use a mixer network different from FOP (the reason is discussed and verified in Appendix E). Here we present an ablation study to further show that the improvement of MACPF is indeed induced by introducing the dependency among agents. In Figure 7, MACPF_CONTROL represents an algorithm where all other perspectives are the same as MACPF, except no dependent joint policy is learned. As shown in Figure 7, MACPF outperforms MACPF_CONTROL in all four maps, demonstrating that the performance improvement is indeed achieved by introducing the dependency among agents.\n\n5.3 MPE\n\nWe further evaluate MACPF on three MPE tasks, including simple spread, formation control, and line control (Agarwal et al., 2020). As shown in Table 1, MACPF outperforms the baselines in all three tasks. A large margin can be observed in simple spread, while only a minor difference can be observed in the other two. This result may indicate that these MPE tasks are not challenging enough for strong MARL algorithms.\n\nTable 1: Average rewards per episode on three MPE tasks.\n\nAlgorithms\n\nTasks\n\nSimple Spread Formation Control Line Control\n\n6 CONCLUSION\n\nMACPF\n\nQMIX\n\nQPLEX\n\nFOP\n\nMAPPO\n\n-118.24±2.74 -145.93±21.09 -122.50±2.58 -125.19±5.42 -166.75±23.44 -15.79±0.16 -19.60±0.33\n\n-16.10±0.28 –15.84±0.19 -20.17±0.26 -19.78±0.27\n\n-16.11±0.30 -20.12±0.21\n\n-21.71±1.69 -24.47±2.54\n\nWe have proposed MACPF, where dependency among agents is introduced to enable more centralized training. By conditional factorized soft policy iteration, we show that dependent local policies provably converge to the optimum. To fulfill decentralized execution, we represent dependent local policies as a combination of independent local policies and dependency policy corrections, such that independent local policies can achieve the same level of expected return as dependent ones. Empirically, we show that MACPF can obtain the optimal joint policy in a simple yet challenging matrix game while baselines fail and MACPF also outperforms the baselines in SMAC and MPE.\n\n9\n\n����������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������Published as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work was supported in part by NSFC (under grant 62250068) and Tencent.\n\nREFERENCES\n\nAkshat Agarwal, Sumit Kumar, Katia Sycara, and Michael Lewis. Learning transferable cooperative behavior in multi-agent teams. In International Conference on Autonomous Agents and MultiAgent Systems, 2020.\n\nDimitri Bertsekas. Multiagent rollout algorithms and reinforcement learning. arXiv preprint\n\narXiv:1910.00120, 2019.\n\nWendelin Böhmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International\n\nConference on Machine Learning, 2020.\n\nCraig Boutilier. Planning, learning and coordination in multiagent decision processes. In TARK,\n\nvolume 96, pp. 195–210, 1996.\n\nJacopo Castellini, Sam Devlin, Frans A Oliehoek, and Rahul Savani. Difference rewards policy gradients. In International Conference on Autonomous Agents and MultiAgent Systems, 2021.\n\nJakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In AAAI conference on artificial intelligence, 2018.\n\nWei Fu, Chao Yu, Zelai Xu, Jiaqi Yang, and Yi Wu. Revisiting some common practices in cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 6863– 6877. PMLR, 2022.\n\nCarlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In\n\nInternational Conference on Machine Learning, 2002.\n\nJayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent Systems, 2017.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, 2018.\n\nShariq Iqbal and Fei Sha. Coordinated exploration via intrinsic rewards for multi-agent reinforcement\n\nlearning. arXiv preprint arXiv:1905.12127, 2019.\n\nLandon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for\n\ndecentralized planning. Neurocomputing, 190:82–94, 2016.\n\nJakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv preprint arXiv:2109.11251, 2021.\n\nYueheng Li, Guangming Xie, and Zongqing Lu. Difference advantage estimation for multi-agent\n\npolicy gradients. In International Conference on Machine Learning, 2022.\n\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n\nRyan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 2017.\n\nAnuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent\n\nvariational exploration. Advances in Neural Information Processing Systems, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJames R Munkres. Topology, volume 2. Prentice Hall Upper Saddle River, 2000.\n\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, 2018.\n\nTabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in neural information processing systems, 2020.\n\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson. The StarCraft Multi-Agent Challenge. arXiv preprint arXiv:2007.12322, 2019.\n\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\n\npolicy optimization. In International conference on machine learning, 2015.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nDavid A Schum. The Evidential Foundations of Probabilistic Reasoning. Northwestern University\n\nPress, 2001.\n\nKyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, 2019.\n\nJianyu Su, Stephen Adams, and Peter A Beling. Value-decomposition multi-agent actor-critics. In\n\nAAAI Conference on Artificial Intelligence, 2021.\n\nKefan Su and Zongqing Lu. Divergence-regularized multi-agent actor-critic.\n\nIn International\n\nConference on Machine Learning, 2022.\n\nPeter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In International Conference on Autonomous Agents and MultiAgent Systems, 2018.\n\nRichard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n\nJianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling\n\nmulti-agent q-learning. In International Conference on Learning Representations, 2020a.\n\nTong Wang, Jiahua Cao, and Azhar Hussain. Adaptive traffic signal control for large-scale scenario with cooperative group-based multi-agent reinforcement learning. Transportation research part C: emerging technologies, 125:103046, 2021a.\n\nTonghan Wang, Liang Zeng, Weijun Dong, Qianlan Yang, Yang Yu, and Chongjie Zhang. Context-\n\naware sparse deep coordination graphs. arXiv preprint arXiv:2106.02886, 2021b.\n\nYihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Off-policy multi-agent\n\ndecomposed policy gradients. arXiv preprint arXiv:2007.12322, 2020b.\n\nQianlan Yang, Weijun Dong, Zhizhou Ren, Jianhao Wang, Tonghan Wang, and Chongjie Zhang. Self-organized polynomial-time coordination graphs. In International Conference on Machine Learning, 2022.\n\nChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.\n\nTianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu. Fop: Factorizing optimal joint policy of maximum-entropy multi-agent reinforcement learning. In International Conference on Machine Learning, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nLulu Zheng, Jiarui Chen, Jianhao Wang, Jiamin He, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang Gao, and Chongjie Zhang. Episodic multi-agent reinforcement learning with curiosity-driven exploration. Advances in Neural Information Processing Systems, 2021.\n\nTong Zhou, Dunbing Tang, Haihua Zhu, and Zequn Zhang. Multi-agent reinforcement learning for online scheduling in smart factories. Robotics and Computer-Integrated Manufacturing, 72: 102202, 2021.\n\nBrian D Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal\n\nEntropy. Carnegie Mellon University, 2010.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA PROOF OF THEOREM 1\n\nIn this subsection, we incorporate dependency among agents into the standard soft policy iteration and prove that this modified soft policy iteration converges to the optimal joint policy. For soft policy evaluation, we will repeatedly apply soft Bellman operator Γπjt to Qπjt gence, where:\n\njt until conver-\n\nΓπjt Qjt(st, at) := rt + γ Est+1[Vjt(st+1)] Vjt(st) = Eπjt [Qjt(st, at) − α log πjt(at| st)].\n\n(20)\n\n(21)\n\nIn this way, as shown in Lemma A.1, we can get Qπjt Lemma A.1 (Joint Soft Policy Evaluation). Consider the modified soft Bellman backup operator jt : S ×A → R with |A| < ∞, and define Qk+1 Γπjt and a mapping Q0 jt. Then, the sequence Qk\n\njt will converge to the joint soft Q-function of πjt as k → ∞.\n\nfor any joint policy πjt.\n\njt = Γπjt Qk\n\njt\n\nProof. First, define the entropy augmented reward as:\n\nrπjt (st, at) := r(st, at) + Est+1[H(πjt(·| st+1))].\n\nThen, rewrite the update rule as:\n\nQjt(st, at) ← rπjt (st, at) + γ Est+1,at+1∼πjt [Qjt(st+1, at+1)].\n\nLast, apply the standard convergence results for policy evaluation (Sutton & Barto, 2018).\n\nAfter we get Qπjt jt , we will make a one-step improvement for the joint policy. First, we restrict the local policy πi of each agent i to some set of policies Πi and update the local policy according to the following optimization problem:\n\nπnew\n\ni = arg min\n\nπ′\n\ni∈Πi\n\nE a<i∼πnew (cid:124)\n\n<i\n\n(cid:20)\n\n(cid:18)\n\nDKL\n\nπ′\n\ni(ai| s, a<i)∥ exp\n\n(cid:0)Qπold\n\ni\n\ni\n\n(s, a<i, ai) − V πold\n\ni\n\ni\n\n(cid:16) 1 αi (cid:123)(cid:122) (π′\n\nJπold\n\ni\n\n,a<i\n\ni(ai| s,a<i))\n\n(s, a<i)(cid:1)(cid:17)(cid:19)(cid:21)\n\n.\n\n(cid:125)\n\n(22)\n\nBased on individual conditional soft policy improvement, we will show that the newly projected joint soft policy has a higher state-action value than the old joint soft policy with respect to the maximum-entropy RL objective. Lemma A.2 (Individual Conditional Soft Policy Improvement). Let πold\n\nbe\n\nthe optimizer of the minimization problem in (22). Then, we have Q for all (st, at) ∈ S ×A with |A| < ∞, where πold i=1 πold (cid:81)N\n\njt (a| s) = (cid:81)N\n\ni\n\ni=1 πnew\n\ni\n\n(ai| s, a<i).\n\ni ∈ Πi and πnew (st, at) ≥ Q\n\nπold jt\n\njt\n\ni\n\njt\n\nπnew jt\n\n(ai| s, a<i) and πnew\n\njt\n\n(st, at) (a| s) =\n\ni\n\nProof. Let Qπold vidual policy πold Then, we have:\n\ni\n\ni\n\ni\n\nand V πold . First, considering that Jπold\n\nbe the corresponding soft state-action value and soft state value of indi- (ai| s, a<i)).\n\n(ai| s, a<i)) ≤ Jπold\n\n,a<i(πnew\n\n,a<i (πold\n\ni\n\ni\n\ni\n\ni\n\ni\n\nEai∼πnew\n\ni\n\n,a<i∼πnew\n\n<i\n\n[αi log πnew\n\ni\n\n≤ E\n\nai∼πold\n\ni\n\n,a<i∼πnew\n\n<i\n\n[αi log πold\n\ni\n\n(ai| s, a<i) − Qπold (ai| s, a<i) − Qπold\n\n(s, a<i, ai) + V πold (s, a<i, ai) + V πold\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n(s, a<i)]\n\n(s, a<i)].\n\n(23)\n\nSince V πold\n\ni\n\ni\n\nEa<i∼πnew\n\n<i\n\ndepends only on s and a<i, where: [V πold\n\n(s, a<i)] = E\n\na<i∼πnew\n\ni\n\ni\n\n<i ,ai∼πold\n\ni\n\n[Qπold\n\ni\n\ni\n\n(s, a<i, ai) − αi log πold\n\ni\n\n(ai| s, a<i)].\n\n(24)\n\nBy deducing (24) from both sides of (23), we have: [Qπold\n\n(s, a<i, ai) − αi log πnew\n\nEai∼πnew\n\n,a<i∼πnew\n\ni\n\ni\n\ni\n\n<i\n\ni\n\n(ai| s, a<i)] ≥ Ea<i∼πnew\n\n<i\n\n[V πold\n\ni\n\ni\n\n(s, a<i)].\n\n(25)\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAnd since\n\nwe can have:\n\nThen we have\n\nπnew\n\njt = exp\n\nπnew\n\ni = exp\n\n(cid:18) 1 α\n(cid:18) 1 α\n\n(cid:18)\n\njt\n\nπold jt\n\nQ\n\n(s, a) − V\n\njt\n\nπold jt\n\n(cid:19)(cid:19)\n\n(s)\n\n(cid:16)\n\nQπold\n\ni\n\ni\n\n(s, a<i, ai) − V πold\n\ni\n\ni\n\n(cid:17)(cid:19)\n\n(s, a<i)\n\nπnew\n\njt\n\n(a| s) =\n\nN (cid:89)\n\ni=1\n\nπnew\n\ni\n\n(ai| s, a<i),\n\njt\n\nπold jt\n\nQ\n\n(s, a) =\n\nN (cid:88)\n\ni=1\n\nα αi\n\n[Qπold\n\ni\n\ni\n\n(s, a<i, ai) − V\n\njt\n\nπold i\n\n(s, a<i)] + V\n\njt\n\nπold jt\n\n(s).\n\nEa∼πnew\n\njt\n\n[Q\n\n(s, a) − α log πnew\n\n(a| s)]\n\njt\n\n= Ea∼πnew\n\njt\n\nα αi\n\n[Qπold\n\ni\n\ni\n\n(s, a<i, ai) − V πold\n\ni\n\ni\n\n(s, a<i)] + V\n\njt\n\nπold jt\n\n(s) − α log πnew\n\njt\n\n(a| s)\n\n(cid:35)\n\njt\n\nπold jt (cid:34) N\n\n(cid:88)\n\ni=1\n\n=\n\nN (cid:88)\n\ni=1\n\nEa∼πnew\n\njt\n\n,a<i∼πnew\n\n<i\n\n(cid:20) α αi\n\n≥ V\n\njt\n\nπold jt\n\n(s),\n\n[Qπold\n\ni\n\ni\n\n(s, a<i, ai) − V πold\n\ni\n\ni\n\n(s, a<i) − αi log πnew\n\ni\n\n(cid:21)\n\n(ai| s, a<i)]\n\n+ V\n\njt\n\nπold jt\n\n(s)\n\n(26)\n\nwhere the inequality is from plugging in (25).\n\nLast, considering the soft bellman equation, the following holds:\n\njt\n\nπold jt\n\nQ\n\n(st, at) = rt + γ Est+1[V\n\njt\n\nπold jt\n\n(s)]\n\n[Q\n\njt\n\nπold jt\n\n(st+1, at+1) − α log πnew\n\njt\n\n(at+1| st+1)]]\n\n≤ rt + γ Est+1[Ea∼πnew\n\njt\n\n...\n\n≤ Q\n\njt\n\nπnew jt\n\n(st, at),\n\nwhere we have repeatedly expanded Q bound in (26).\n\njt\n\nπold jt\n\non the RHS by applying the soft Bellman equation and the\n\nConditional factorized soft policy iteration alternates between joint soft policy evaluation and individual conditional soft policy improvement, and provably converges to the global optimum, as shown in Theorem 1. Theorem 1 (Conditional Factorized Soft Policy Iteration). For any joint policy πjt, if we repeatedly apply joint soft policy evaluation and individual conditional soft policy improvement from πi ∈ Πi. Then the joint policy πjt(a| s) = (cid:81)n jt, such that\n\ni=1 πi(ai| s, a<i) will eventually converge to π∗\n\nπ∗\n\nQ\n\njt\n\njt (s, a) ≥ Qπjt\n\njt (s, a) for all πjt, assuming |A| < ∞.\n\nProof. First, by Lemma A.2, the sequence {πk\n\njt} monotonically improves with Q\n\njt\n\nπk+1 jt ≥ Q\n\njt\n\nπk jt . Since\n\nboth the reward and entropy are bounded, then Q to some π∗\n\njt. Then, at convergence, we have the following inequality:\n\nis bounded. Thus, this sequence must converge\n\njt\n\nπk jt\n\nJπ∗\n\njt\n\n(π∗\n\njt(·| s)) ≤ Jπ∗\n\njt\n\n(πjt(·| s)), ∀ πjt ̸= π∗ jt.\n\nUsing the same iterative argument as in the proof of Lemma A.2, we get Q jt (s, a) for all (s, a) ∈ S ×A. That is, the soft value of any other policy πjt is lower than that of the converged policy π∗\n\njt is optimal in Π1 × · · · × ΠN .\n\njt. Therefore, π∗\n\nπ∗\n\njt\n\njt (s, a) ≥ Qπjt\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nB PROOF OF THEOREM 2\n\nTheorem 2. For any dependent joint policy πdep exists an independent joint policy πind (s) for any state s ∈ S. Vπdep\n\n(s) = Vπind\n\njt\n\njt\n\njt\n\nthat involves dependency among agents, there that does not involve dependency among agents, such that\n\njt\n\nProof. For a dependent joint policy πdep A and mina Qπdep independent joint policy πind\n\njt\n\n:\n\njt\n\n= B, we have A ≤ Vπdep\n\njt\n\njt\n\nthat involves dependency among agents, let maxa Qπdep\n\njt\n\n=\n\n(s) ≤ B. Then, we can construct the following\n\nπind\n\njt =\n\nN (cid:89)\n\ni=1\n\nπi =\n\nN (cid:89)\n\ni=1\n\n1[ai = arg max Qπdep\n\njt\n\n[i]].\n\n, we have (cid:80) For such an independent joint policy πind construct another independent joint policy, such that (cid:80) = B. Based on the generalized intermediate value theorem (Munkres, 2000), We can have that for any dependent joint policy πdep there exist an independent joint policy πind such that:\n\njt Qπdep jt Qπdep\n\n= A. Similarly, we can also\n\na πind a πind\n\njt\n\njt\n\njt\n\njt\n\n,\n\njt\n\nVπdep\n\njt\n\n=\n\n(cid:88)\n\na\n\nπdep\n\njt Qπdep\n\njt\n\n(cid:88)\n\n=\n\na\n\nπind\n\njt Qπdep\n\njt\n\n= E\n\nat∼πind\n\njt\n\n[Qπdep\n\njt\n\n].\n\nThus, we can have:\n\nVπdep\n\njt\n\n(st) = E\n\nat∼πind\n\njt\n\n[Qπdep\n\njt\n\n(st, at)]\n\n= E\n\n= E\n\nat∼πind\n\njt ,st+1∼P [r(st, at) + γVπdep\n\n(st+1)]\n\njt\n\n(at,at+1)∼πind\n\njt ,st+1∼P [r(st, at) + γQπdep\n\njt\n\n(st, at)]\n\n... = E\n\nat:∞∼πind\n\njt ,st:∞∼P [r(st, at) + γr(st+1, at+1) + · · · ]\n\n= Vπind\n\njt\n\n(st),\n\nwhich concludes the proof.\n\nC EXPERIMENT SETTINGS AND IMPLEMENTATION DETAILS\n\nC.1 MATRIX GAME\n\nIn the matrix game, we use a learning rate of 3 × 10−4 for all algorithms. For FOP and MACPF, α decays from 1 to 0.5, with a decay rate of 0.999 per episode. For QMIX and QPLEX, ε decays from 1 to 0.01, with a decay rate of 0.999 per episode. The batch size used in the experiment is 64 for FOP, MACPF, QMIX, and QPLEX, and 32 for MAPPO as it is an on-policy learning algorithm. All critics and actors used in the experiments consist of one hidden layer of 64 units with ReLU non-linearity. For the Mixer network, QMIX and MACPF both use hypernetwork, except ELU non-linearity is used for QMIX and no non-linearity is used for MACPF. FOP and QPLEX both use attention network for their mixer network. The environment and model are implemented in Python. All models are built on PyTorch and are trained on a machine with 1 Nvidia GPU (RTX 1060) and 8 AMD CPU Cores.\n\nC.2 SMAC\n\nIn StarCraft II, for MACPF, we use a learning rate of 5 × 10−4. The critic network and policy network of MACPF consist of three layers, a fully-connected layer with 64 units activated by ReLU, followed by a 64 bit GRU, and followed by another fully-connected layer. The policy correction network and critic correction network consist of two layers, one fully-connected layer with 64 units activated by\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nELU, followed by another fully-connected layer. The target networks are updated after every 200 training episodes. The temperature parameters α and αi are annealed from 0.5 to 0.05 over 200k time steps for all easy and hard maps and fixed as 0.001 for all super-hard maps. For QMIX, QPLEX, FOP, and MAPPO, we use their default setting of each map. The environment and model are implemented in Python. All models are built on PyTorch and are trained on a machine with 4 Nvidia GPUs (A100) and 224 Intel CPU Cores. For 3s5z_vs_3s6z, all models are built on PyTorch and are trained on a machine with 1 Nvidia GPU (RTX 2080 TI) and 16 Intel CPU Cores. Our implementation of MACPF is based on PyMARL (Samvelyan et al., 2019) with MIT license. It worth noting that, although we assume full observability for the rigorousness of proof, the trajectory of each agent is used to replace state s for each agent as input to settle the partial observability in all SMAC experiments.\n\nC.3 MPE\n\nIn MPE (MIT license), we use the default settings of MAPPO. For QMIX, QPLEX, FOP, and MACPF, we use a learning rate of 5 × 10−4. For FOP and MACPF, α decays from 0.5 to 0.05 over 50k time steps. For QMIX and QPLEX, ε decays from 1 to 0.05 over 50k time steps. The batch size used in the experiment is 64. All critics and actors used in the experiments consist of hidden layers of 64 units with ReLU non-linearity and 64 bit GRU. For the Mixer network, QMIX and MACPF both use hypernetwork, except ELU non-linearity is used for QMIX and no non-linearity is used for MACPF. FOP and QPLEX both use attention network for their mixer network. The environment and model are implemented in Python. All models are built on PyTorch and are trained on a machine with 1 Nvidia GPU (RTX 2080 TI) and 16 Intel CPU Cores. We also use the trajectory of each agent as input to settle the partial observability in all MPE experiments.\n\nD MORE EXPERIMENTS ON SMAC\n\nD.1 MORE MAPS\n\nWe additionally evaluate MACPF on more SMAC maps. The maps used here include six easy maps (8m, MMM, 3s_vs_3z, 3s_vs_4z, so_many_baneling, 1c3s5z), three hard maps (3s5z, 2c_vs_64zg, 3s_vs_5z) and three super-hard maps (3s5z_vs_3s6z, 27m_vs_30m, 6h_vs_8z). Results are shown in Figure 8. In general, MACPF matches or slightly outperforms the best performance of the baselines on all twelve maps.\n\nD.2 SUMMARY OF SMAC FINAL PERFORMANCE\n\nIn this section, we provide the summary of SMAC experiments in terms of final performance. All results are achieved by 2M training timesteps. As shown in Table 2, MACPF outperforms or at least matches the best performance of the baselines on all twelve maps.\n\nE MIXER SELECTION\n\nAs mentioned in Section 5.2, we use a hypernetwork without non-linearity as our mixer network, which differs from QMIX, QPLEX, and FOP. In QPLEX and FOP, weighted summation is used to reflect the relationship between Qjt and Qi, where the weight is a function of both state and agent actions, such that the dependency among agents is implicitly considered. However, this implicit dependency may contradict our explicit dependency model in Qdep and decrease the performance of both Qdep\n\ni\n\nand Qind jt .\n\njt\n\nAnother choice is to use a hypernetwork with non-linearity to reflect the relationship between Qjt and Qi, which is used in QMIX. However, due to the existence of the non-linearity unit, two joint actions with the same Qjt value may not be properly decomposed into two sets of Qi with the same sum. Thus, their joint probability may not be the same, and the dependency among agents is distorted.\n\nTherefore, the only option left for MACPF is to use a hypernetwork without non-linearity, which is equivalent to weighted summation where the weight is just a function of state.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 8: Learning curves of all the methods in twelve maps of SMAC, where the unit of x-axis is 1M timesteps and y-axis represents the win rate of each map.\n\nAs shown in Figure 9, MACPF_NONLINEAR and MACPF_ATT represent algorithms where all other aspects are the same as MACPF, except using a hypernetwork with non-linearity and a weighted summation with actions as input as their mixer networks, respectively. MACPF_NONLINEAR achieves similar performance as MACPF in the easy and hard maps, indicating that even distorted dependency can still benefit the learning. However, in the super-hard maps, MACPF outperforms MACPF_NONLINEAR, demonstrating the importance of accurate modeling of dependency among agents. MACPF_ATT is outperformed by both MACPF and MACPF_NONLINEAR by a large margin in all the maps, which verifies that the implicit dependency model in the mixer network of MACPF_ATT conflicts with the explicit dependency model in Qdep\n\n.\n\ni\n\nFigure 9: Ablation study of the mixer selection of MACPF on four maps of SMAC, including one easy map (MMM), one hard map (8m_vs_9m) and two super-hard maps (MMM2, corridor), where the unit of x-axis is 1M timesteps and y-axis represents the win rate of each map.\n\n17\n\n������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������Published as a conference paper at ICLR 2023\n\nTable 2: Final performance on all SMAC maps. We bold all values within one standard deviation of the best mean performance for each map.\n\nAlgorithms\n\nTasks\n\nMACPF\n\nQMIX\n\nQPLEX\n\nFOP\n\nMAPPO\n\ncorridor (super-hard)\n\n3s5z_vs_3s6z (super-hard) 0.209±0.202 0.024±0.031 0.135±0.090 0.002±0.005 0.691±0.349\n\n8m (easy) MMM (easy) 2c_vs_64zg (hard) 3s5z (hard) 8m_vs_9m (hard) 10m_vs_11m (hard) MMM2 (super-hard)\n\n0.994±0.006 0.986±0.011 0.99±0.01 0.992±0.005 0.997±0.003 0.984±0.015 0.984±0.01 0.985±0.011 0.975±0.017 0.962±0.035 0.972±0.031 0.946±0.013 0.954±0.031 0.976±0.011 0.945±0.037 0.976±0.008 0.955±0.017 0.969±0.018 0.26±0.212 0.715±0.215 0.919±0.045 0.916±0.039 0.798±0.021 0.571±0.314 0.85±0.095 0.965±0.035 0.939±0.032 0.95±0.016 0.545±0.265 0.774±0.106 0.788±0.083 0.709±0.162 0.224±0.231 0.506±0.144 0.679±0.054 0.144±0.175 0.58±0.184 0.0±0.0 27m_vs_30m (super-hard) 0.726±0.094 0.532±0.23 0.294±0.159 0.45±0.143 0.78±0.095 0.855±0.069 0.675±0.127 0.716±0.075 0.338±0.329 0.81±0.119 0.888±0.188 0.702±0.129 0.664±0.089 0.384±0.372 0.514±0.253 0.974±0.019 0.988±0.014 0.994±0.004 0.999±0.002 0.997±0.003 0.995±0.005 0.99±0.008 0.997±0.003 0.789±0.22 0.957±0.022 0.959±0.033 0.759±0.153 0.992±0.006 0.862±0.076 0.576±0.063 0.969±0.019 0.974±0.009 0.941±0.037 0.97±0.025 0.979±0.012 0.984±0.006 0.98±0.013 0.985±0.003 0.984±0.005 0.989±0.007 0.059±0.038 0.001±0.002 0.059±0.09 0.028±0.055 0.13±0.074\n\n8m_vs_9m (myopic) 10m_vs_11m (myopic) 3s_vs_3z (easy) 3s_vs_4z (easy) 3s_vs_5z (hard) so_many_baneling (easy) 1c3s5z (easy) 6h_vs_8z (super-hard)\n\n0.0±0.0 0.0±0.0\n\nF FUTURE WORK\n\ni\n\nOne limitation of our work is the sequential decision-making process in training. Since the dependent local policy πdep (ai| s, a<i) takes as input the joint action of all agents whose indices are smaller than agent i, agents have to make decisions one by one. This makes the whole decision process be O(N ). There is not much difference when N is small. However, when N is large, it slows down the training process. One approximate solution is to divide agents into groups, such that agents can make decisions group by group instead of one by one. However, such a mechanism may raise a new question about how to group agents, which will be considered in future work.\n\n18",
    "reference": "# Summary Of The Paper\n\nThis paper deals with the problem of decentralized policy in MARL when using the factorizing value method, especially FOP. To solve this problem, they propose **multi-agent conditined policy factorization** (MACPF) that incorporate dependency between the agents. \nDue to the fact that the proposed method learns the joint policy, it can guarantee to the optimal joint policy. They \nThey give the suitable network structure for the condition policy, also show the proper experiment results both in toy-example and complex task, SMAC.\n\n# Strength And Weaknesses\n\n- Streagth   \nActually, learn the joint-policy, it can gaurantee the optimal policy in joint-aciton space.   \nOuput the decentrazlied individual policies distilled from the dependent policies   \n\n- Weakness   \n  Complex network structure, do exist dependent and individual networks ( $Q^{dep}_i, Q^{int}_i$ )    \n  To learn the individual policies from joint dependency policies, they propose the naive method\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThey describe clearly the proposed method, and I think that the paper has the good quality. If the authors will give the hyperparameter in their experiments, it is helfpul for reproducibility.\n\n# Summary Of The Review\n\nLearning the coordinated strategies in multi-agent is challenging because of partial-observability.\nThe author gives the intuition about what the problem is in the previous value-factorized method. To learn the opitmal policy, the depedency between agents should be considered, but it can break the decentralied execution in the test time. To keep the decentralized execution, having the factorized policies, they build the two seperate networks, dependent and individual networks. The proposed network structure is not simple, but they show the good performance by using the valued-based method when comparing strong baselines.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "EXACT MANIFOLD GAUSSIAN VARIATIONAL BAYES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe propose an optimization algorithm for Variational Inference (VI) in complex models. Our approach relies on natural gradient updates where the variational space is a Riemann manifold. We develop an efficient algorithm for Gaussian Variational Inference that implicitly satisfies the positive definite constraint on the variational covariance matrix. Our Exact manifold Gaussian Variational Bayes (EMGVB) provides exact but simple update rules and is straightforward to implement. Due to its black-box nature, EMGVB stands as a ready-to-use solution for VI in complex models. Over five datasets, we empirically validate our feasible approach on different statistical and econometric models, discussing its performance with respect to baseline methods.\n\n1\n\nINTRODUCTION\n\nAlthough Bayesian principles are not new to Machine Learning (ML) (e.g. Mackay, 1992; 1995; Lampinen & Vehtari, 2001), it has been only recently that feasible methods boosted a growing use of Bayesian methods within the field (e.g. Zhang et al., 2018; Trusheim et al., 2018; Osawa et al., 2019; Khan et al., 2018b; Khan & Nielsen, 2018). In the typical ML settings the applicability of sampling methods for the challenging computation of the posterior is prohibitive, however approximate methods such as Variational Inference (VI) have been proved suitable and successful (Saul et al., 1996; Wainwright & Jordan, 2008; Hoffman et al., 2013; Blei et al., 2017). VI is generally performed with Stochastic Gradient Descent (SGD) methods (Robbins & Monro, 1951; Hoffman et al., 2013; Salimans & Knowles, 2014), boosted by the use of natural gradients (Hoffman et al., 2013; Wierstra et al., 2014; Khan et al., 2018b), and the updates often take a simple form (Khan & Nielsen, 2018; Osawa et al., 2019; Magris et al., 2022).\n\nThe majority of VI algorithms rely on the extensive use of models’ gradients and the form of the variational posterior implies additional model-specific derivations that are not easy to adapt to a general, plug-and-play optimizer. Black box methods (Ranganath et al., 2014), are straightforward to implement and versatile use as they avoid model-specific derivations by relying on stochastic sampling (Salimans & Knowles, 2014; Paisley et al., 2012; Kingma & Welling, 2013). The increased variance in the gradient estimates as opposed to e.g. methods relying on the Reparametrization Trick (RT) (Blundell et al., 2015; Xu et al., 2019) can be alleviated with variance reduction techniques (e.g Magris et al., 2022).\n\nFurthermore, the majority of existing algorithms do not directly address parameters’ constraints. Under the typical Gaussian variational assumption, granting positive-definiteness of the covariance matrix is an acknowledged problem (e.g Tran et al., 2021a; Khan et al., 2018b; Lin et al., 2020). Only a few algorithms directly tackle the problem (Osawa et al., 2019; Lin et al., 2020), see Section 3. A recent approximate approach based on manifold optimization is provided by Tran et al. (2021a).\n\nOn the theoretical results of Khan & Lin (2017); Khan et al. (2018a) we develop an exact version of Tran et al. (2021a), resulting in an algorithm that explicitly tackles the positive-definiteness constraint for the variational covariance matrix and resembles the readily-applicable natural-gradient black-box framework of (Magris et al., 2022). For its implementation, we discuss recommendations and practicalities, show that EMGVB is of simple implementation, and demonstrate its feasibility in extensive experiments over four datasets, 12 models, and three competing VI optimizers.\n\nIn Section 2 we review the basis of VI, in Section 3 we review the Manifold Gaussian Variational Bayes approach and other related works, in Section 4 we discuss our proposed approach. Experi-\n\n1\n\nments are found in Section 5, while Section 6 concludes. Appendices A, B complement the main discussion, Appendix C.4 reinforces and expands the experiments, Appendix D provides proofs.\n\n2 VARIATIONAL INFERENCE\n\nVariational Inference (VI) stands as a convenient and feasible approximate method for Bayesian inference. Let y denote the data, p(y|θ) the likelihood of the data based on some model whose k-dimensional parameter is θ. Let p(θ) be the prior distribution on θ. In standard Bayesian inference the posterior is retrieved via the Bayes theorem as p(θ|y) = p(θ)p(y|θ)/p(y). As the marginal likelihood p(y) is generally intractable, Bayesian inference is often difficult for complex models. Though the problem can be tackled with sampling methods, Monte Carlo techniques, although nonparametric and asymptotically exact may be slow, especially in high-dimensional applications (Salimans et al., 2015).\n\nVI approximates the true unknown posterior with a probability density q within a tractable class of distributions Q, such as the exponential family. VI turns the Bayesian inference problem into that of finding the best variational distribution q⋆ ∈ Q minimizing the Kullback-Leibler (KL) divergence from q to p(θ|y): q⋆ = arg minq∈Q DKL(q||p(θ|y)). It can be shown that the KL minimization problem is equivalent to the maximization of the so-called Lower Bound (LB) on log p(y), (e.g. Tran et al., 2021b). In fixed-form variational Bayes, the parametric form of the variational posterior is set. The optimization problem accounts for finding the optimal variational parameter ζ parametrizing q ≡ qζ that maximizes the LB (L), that is:\n\nζ ⋆ = arg max\n\nL(ζ) :=\n\nζ∈Z\n\n(cid:90)\n\nqζ(θ) log\n\np(θ)p(y|θ) qζ(θ)\n\ndθ = Eqζ\n\n(cid:20)\n\nlog\n\np(θ)p(y|θ) qζ(θ)\n\n(cid:21) ,\n\nwhere Eq means that the expectation is taken with respect to the distribution qζ, and Z is the parameter space for ζ.\n\nThe maximization of the LB is generally tackled with a gradient-descent method such as SGD (Robbins & Monro, 1951), ADAM Kingma & Ba (2014), or ADAGRAD Duchi et al. (2011). The learning of the parameter ζ based on standard gradient descent is however problematic as it ignores the information geometry of the distribution qζ, is not scale invariant, unstable, and very susceptible to the initial values (Wierstra et al., 2014). SGD implicitly relies on the Euclidean norm for capturing the dissimilarity between two distributions, which can be a poor and misleading measure of dissimilarity (Khan & Nielsen, 2018). By using the KL divergence in place of the Euclidean norm, the SGD update results in the following natural gradient update:\n\nζt+1 = ζt + βt\n\n(cid:104) ̃∇ζL(ζ)\n\n(cid:105)(cid:12) (cid:12) (cid:12)ζ=ζt\n\n,\n\nwhere βt is a possibly adaptive learning rate, and t denotes the iteration. The above update results in improved steps towards the maximum of the LB when optimizing it for the variational parameter ζ. The natural gradient ̃∇ζL(ζ) is obtained by rescaling the euclidean gradient ∇ζL(ζ) by the inverse of the Fisher Information Matrix (FIM) Iζ, i.e. ̃∇ζL(ζ) = Iζ∇ζL(ζ). For readability, we shall write L in place of L(ζ).\n\nA major issue in following this approach is that ζ is unconstrained. Think of a Gaussian variational posterior: under the above update, there is no guarantee that the covariance matrix is iteratively updated onto a symmetric and positive definite matrix. As discussed in the introduction, manifold optimization is an attractive possibility.\n\n3 RELATED WORK\n\nIn Tran et al. (2021a), a d-dimensional Gaussian distribution N (μ, Σ), provides the fixed-form of the variational posterior qζ = (μ, vec(Σ)). There are no restrictions on μ yet the covariance matrix Σ is constrained to the manifold M of symmetric positive-definite matric es, M = (cid:8)Σ ∈ Rd×d : Σ = Σ⊤, Σ ≻ 0(cid:9), see e.g. (Abraham et al., 2012; Hu et al., 2020).\n\n2\n\nThe exact form of the Fisher information matrix for the multivariate normal distribution is, e.g., provided in (Mardia & Marshall, 1984) and reads\n\nIζ =\n\n(cid:18)Σ−1 0\n\n(cid:19)\n\n0 Iζ(Σ)\n\n,\n\nIζ(Σ)σij ,σkl\n\n=\n\n1 2\n\ntr\n\nΣ−1 ∂Σ ∂σij\n\nΣ−1 ∂Σ ∂σkl\n\n(cid:18)\n\n(cid:19)\n\n.\n\n(1)\n\nwith Iζ(Σ)σij ,σkl on the approximation I −1 the natural gradients of the lower bound with respect to μ and Σ, respectively computed as 1\n\nbeing the generic element of the d2 × d2 matrix Iζ(Σ). The MGVB method relies (Σ) ≈ (cid:0)Σ−1 ⊗ Σ−1(cid:1), which leads to a convenient approximate form of\n\nζ\n\n ̃∇μL(ζ) = Σ∇μL(ζ)\n\n(2) where ⊗ denotes the Kronecker product. In virtue of the natural gradient definition, the first natural gradient is exact while the second is approximate. Thus, Tran et al. (2021a) adopt the following updates for the parameters of the variational posterior:\n\n ̃∇ΣL(ζ) ≈ vec−1(cid:0)(Σ ⊗ Σ)∇vec(Σ)L(ζ)(cid:1) = Σ∇ΣL(ζ)Σ,\n\nand\n\nμ = μ + β ̃∇μL(ζ)\n\nΣ = RΣ(β ̃∇ΣL(ζ)),\n\n(3) and where RΣ(·) denotes a suitable retraction for Σ on the manifold M. Momentum gradients can be used in place of plain natural ones. In particular, the momentum gradient for the update of Σ relies on a vector transport granting that at each iteration the weighted gradient remains in the tangent space of the manifold M. Refer to Section 4.2 for more information on retraction and vector transport. Besides the relationship between EMGVB and MGVB already discussed, a method of handling the positivity constraint in diagonal covariance matrices is VOGN optimizer (Khan et al., 2018b; Osawa et al., 2019). VOGN relates to the VON update (see Appendix B) as it indirectly updates μ and Σ from the Gaussian natural parameters updates. Following a non-Black-Box approach, VOGN uses some theoretical results on the Gaussian distribution to recover an update for Σ that involves the Hessian of the likelihood. Such Hessian is estimated as the samples’ mean squared gradient, granting the non-negativity of the diagonal covariance update. Osawa et al. (2019) devise the computation of the approximate Hessian in a block-diagonal fashion within the layers of a Deep-learning model.\n\nLin et al. (2020) extend the above to handle the positive definiteness constraint by adding an additional term to the update rule for Σ, applicable to certain partitioned structures of the FIM. The retraction map in (Lin et al., 2020) is more general than (Tran et al., 2021a) and obtained through a different Riemann metric, from which MGVB is retrieved as a special case. As opposed to EMGVB, the use of the RT in (Lin et al., 2020) requires model-specific computation or auto-differentiation. See (Lin et al., 2021) for an extension on stochastic, non-convex problems. Lin et al. (2020) underline that in Tran et al. (2021a) the chosen form of the retraction is not well-justified as it is specific for the SPD matrix manifold, whereas the natural gradient is computed for the Gaussian manifold. An extensive discussion on this point and its relationship with the EMGVB optimizer here proposed is found in Appendix D.3.\n\nAlternative methods that rely on unconstrained transformations (e.g. Cholesky factor) (e.g Tan, 2021), or on the adaptive adjustment of the learning rate (e.g. Khan & Lin, 2017) lie outside the manifold context here discussed. Among the methods that do not control for the positive definiteness constraint, the QBVI update (Magris et al., 2022) provides a comparable black-bock method that, despite other black-bock VI algorithms, uses exact natural gradients updates obtained without the computation of the FIM.\n\n4 EXACT MANIFOLD GAUSSIAN VB\n\nConsider a variational Gaussian distribution qλ with mean μ and positive-definite covariance matrix2 Σ. Be λ1 = Σ−1μ and λ2 = − 1 2 Σ−1 its natural parameters and define λ = (λ1, vec(λ2)). The 1We present the MGVB optimizer by exactly following (Tran et al., 2021a). Lin et al. (2020) assert that ζ (Σ) term reads Σ−1 ⊗ Σ−1 in place of 2(cid:0)Σ−1 ⊗ Σ−1(cid:1), in (Tran et al., 2021a) there is a typo as their I −1 which would lead to the actual natural gradient 2Σ∇SΣ (e.g. Barfoot, 2020). While their observation is valid, we argue that the omission of the constant is embedded in the approximation, as it is also omitted from the implementation codes for MGVB, where ̃∇ΣL is computed as Σ∇SΣ. To clarify, ̃∇ΣL = 2Σ∇SLΣ is an exact relationship, while ̃∇ΣL = Σ∇SLΣ not.\n\n2This is the general case of practical relevance in applications, ruling out singular Gaussian distributions. For such peculiar distributions, Σ is singular, Σ−1 does not exist, and neither does the density. Though this might be theoretically interesting to develop, the discussion is here out of scope. Assuming Σ to be positivedefinite is not a restrictive and aligned with (Tran et al., 2021a)\n\n3\n\ncorresponding mean or expectation parameters m = (m1, vec(m2)) are given by m1 = Eqλ[θ] = μ (cid:2)θθ⊤(cid:3) = μμ⊤ +Σ. When required, in place of the somewhat vague notation L whose and m2 = Eqλ precise meaning is to be inferred from the context, we shall use L(m) to explicitly denote the lower bound expressed in terms of the expectation parameter m, opposed to L(λ) expressed in terms of λ.\n\nProposition 1 For a differentiable function L, and qλ being a Gaussian distribution with mean μ and covariance matrix S,\n\n ̃∇μL = Σ∇μL\n\n ̃∇Σ−1L = −2 ̃∇λ2L = −2∇ΣL,\n\nwhere λ2 = − 1\n\n2 Σ−1 denotes the second natural parameter of qλ.\n\nThe covariance matrix Σ is positive definite, its inverse exists and it is as well symmetric and positive definite. Therefore Σ−1 lies within the manifold M and can be updated with a suitable retraction algorithm as for Σ in equation 3:\n\nΣ−1 = RΣ−1\n\n(cid:16)\n\n(cid:17)\n\nβ ̃∇Σ−1 L\n\n= RΣ−1(−2β∇ΣL).\n\n(4)\n\nOpposed to the update in eq. 3, which relies on the approximation I −1 (Σ) ≈ Σ−1 ⊗ Σ−1, for tacking a positive-definite update of Σ, we target at updating Σ−1, for which its natural gradient is available in an exact form, by primarily exploiting the duality between the gradients in the natural and expectation parameter space (Appendix D.1, eq. 25) that circumvents the computation of the FIM.\n\nζ\n\nFor coherency with the literature on VI for Bayesian deep learning (e.g. Ranganath et al., 2014, among many others), we specify the variational posterior in terms of the covariance matrix Σ, but update Σ−1. Yet nothing prevents specifying qλ in terms of its precision matrix Σ−1, as is often the case in Bayesian statistics textbooks, in which this case, the update 4 corresponds to an update for the actual variational precision parameter.\n\nFor updating μ is reasonable to adopt plain SGD-like step driven by the natural parameter ̃∇μL = Σ∇μL, as in (Tran et al., 2021a). We refer to the following update rules as Exact Manifold Gaussian Variational Bayes, or shortly EMGVB,\n\nμt+1 = μt + βΣ∇μLt\n\nand\n\nΣ−1\n\nt+1 = RΣ−1\n\nt\n\n(−2β∇ΣLt),\n\n(5)\n\nwhere the gradients w.r.t. L are intended as evaluated at the current value of the parameters, e.g. ∇ΣLt = ∇ΣL|μ=μt,Σ=Σt. With respect to the MGVB update of Tran et al. (2021a), there are regarding the FIM, yet the cost of updating Σ−1 appears to be that of no approximations, e.g. introducing an additional inversion for retrieving Σ that is involved in the EMGVB update for μ. In the following Section, we show that with a certain gradient estimator such an inversion is irrelevant. Furthermore, in Appendix B we point out that a covariance matrix inversion is implicit in both MGVB and EMGVB due to the second-order form of the retraction and also show that the update for μ is optimal in the sense therein specified.\n\n4.1\n\nIMPLEMENTATION\n\nWe elaborate on how to evaluate the gradients ∇ΣL and ∇μL. We follow the Black-box approach (Ranganath et al., 2014) under which such gradients are approximated via Monte Carlo (MC) sampling and rely on function queries only. The implementation of the EMGVB updates does not require the model’s gradient to be specified nor to be computed numerically, e.g. with backpropagation. By use of the so-called log-derivative trick (see e.g. (Ranganath et al., 2014)) it is possible to evaluate the gradients of the LB as an expectation with respect to the variational distribution. In particular, for a generic differentiation variable ζ,\n\n∇ζL(ζ) = Eqλ[∇ζ[log qζ(θ)] hζ(θ)], where hζ(θ) = log\n\n(cid:20) p(θ)p(y|θ) qζ(θ)\n\n(cid:21) .\n\nIn the EMGVB context with q ∼ N (μ, Σ), ζ = (μ, vec(Σ)) and L(ζ) = L(μ, Σ). The gradient of the L w.r.t. ζ evaluated at ζ = ζt can be easily estimated using S samples from the variational posterior through the unbiased estimator\n\n∇ζL(ζt) = ∇ζL(ζ)|ζ=ζt\n\n≈\n\n1 S\n\nS (cid:88)\n\ns=1\n\n[∇ζ[log qζ(θs)] hζ(θs)]|ζ=ζt\n\n,\n\nθs ∼ N (μt, St)\n\n(6)\n\n4\n\nwhere the h-function is evaluated in the current values of the parameters, i.e. in ζt = (μt, vec(Σt)). For a Gaussian distribution q ∼ N (μ, Σ) it can be shown that (e.g. Wierstra et al., 2014; Magris et al., 2022):\n\n∇μ log q(θ) = Σ−1(θ − μ),\n\n∇Σ log q(θ) = −\n\n1 2\n\n(cid:16)\n\nΣ−1 − Σ−1(θ − μ)(θ − μ)⊤Σ−1(cid:17)\n\n,\n\n(7)\n\n(8)\n\nEquations 7, 8 along with 6 and Proposition 1 immediately lead to the feasible natural gradients estimators:\n\n ̃∇μL(ζt) ≈ Σt ˆ∇μL(ζt) =\n\n ̃∇Σ−1 L(ζt) ≈ −2 ˆ∇ΣL(ζt) =\n\n1 S\n\n1 S\n\nS (cid:88)\n\n[(θs − μt)hζt(θs)],\n\ns=1\n\nS (cid:88)\n\n(cid:104)(cid:16)\n\ns=1\n\nΣ−1\n\nt − Σ−1\n\nt (θs − μt)(θs − μt)⊤Σ−1\n\nt\n\n(9)\n\n(cid:17)\n\nhζt (θs)\n\n(cid:105) .\n\n(10)\n\nAs for the MGVB update, the EMGVB update applies exclusively to Gaussian variational posteriors, yet no constraints are imposed on the parametric form of p. When considering a Gaussian prior, the implementation of the EMGVB update can take advantage of some analytical results leading to MC estimators of reduced variance, namely implemented over the log-likelihood log p(y|θs) rather than the h-function.\n\nIn Appendix D.2, we show that, under a Gaussian prior specification, the above updates can be also implemented in terms of the model likelihood than in terms of the h-function. The general form of the EMGVB updates then writes:\n\n ̃∇μL(ζt) ≈ cμt +\n\n1 S\n\nS (cid:88)\n\ns=1\n\n[(θs − μt) log f (θs)]\n\n ̃∇Σ−1 L(ζt) ≈ CΣt +\n\n1 S\n\nS (cid:88)\n\ns=1\n\n(cid:104)(cid:16)\n\nΣ−1\n\nt − Σ−1\n\nt (θs − μt)(θs − μt)⊤Σ−1\n\nt\n\n(11)\n\n(12)\n\n(cid:17)\n\n(cid:105)\n\nlog f (θs)\n\nwhere, (i) if p is Gaussian CΣt = −Σ−1 log p(y|θs), (ii) if p is Gaussian or not CΣt = cμt = 0 and log f (θs) = hζt(θs).\n\n0 , cμt = −ΣtΣ−1\n\nt + Σ−1\n\n0 (μt − μ0) and log f (θs) =\n\nand θs ∼ qζt = N (μt, Σt), s = 1, . . . , S. log(y|θs) and hζt(θs) respectively denote the model likelihood and the h-function evaluated in θs. Note that the latter depends on t as it involves the variational posterior, evaluated at the parameters at the value of the parameters for iteration t. p It is clear that under the Gaussian specification for p the MC estimator is of denotes the prior. reduced variance, compared to the general one based on the h-function. Note that the log-likelihood case does not involve an additional inversion for retrieving Σ in cμ, as Σ is anyway required in the second-order retraction (for both MGVB and EMGVB). This aspect is further discussed Appendix B. For Inverting Σ−1 we suggest inverting the Cholesky factor L−1 of Σ−1 and compute Σ as LL⊤. This takes advantage of the triangular form of L−1 which can be inverted with back-substitution, which is k3/3 flops cheaper than inverting Σ−1, but still O(cid:0)k3(cid:1). L is furthermore used for generating the draws θs as either θs = μ + Lε or θs = μ + L−⊤ε with ε, with ε ∼ N (0, I). As outlined in Appendix A, we devise the use of control variates to reduce the variance of the stochastic gradient estimators.\n\nThough the lower bound is not directly involved in EMGVB updates, it can be naively estimated at each iteration as\n\nˆLt =\n\n1 S\n\nS (cid:88)\n\ns=1\n\n[p(θ) + log p(y|θ) − log qζ(θ)].\n\n(13)\n\nAs discussed in Appendix A, ˆLt is needed for terminating the optimization routine, verifying anomalies in the algorithm works (the LB should actually increase and converge) and comparing EMGVB with MGVB, see Section 5.\n\n5\n\n4.2 RETRACTION AND VECTOR TRANSPORT\n\nAligned with Tran et al. (2021a), we adopt the retraction method advanced in (Jeuris et al., 2012) for the manifold M of symmetric and positive definite matrices\n\nRΣ−1(ξ) = Σ−1 + ξ +\n\n1 2\n\nξΣξ, where ξ ∈ TΣ−1M,\n\n(14)\n\nwith ξ being the rescaled natural gradient β ̃∇Σ−1 L = −2β∇ΣL. In practice, whenever applicable, in the retraction, for granting the symmetric from of a matrix (or gradient matrix) S, we as e.g. compute S as 1/2(S + S⊤). Vector transport is as well easily implemented by\n\nTΣ−1\n\nt →Σ−1\n\nt+1\n\n(ξ) = EξE⊤, where E = (cid:0)Σ−1\n\nt+1Σt\n\n(cid:1) 1\n\n2 , ξ ∈ TΣ−1M.\n\n(15)\n\nWe refer to the Manopt toolbox (Boumal et al., 2014) for the practical details of implementing the above two algorithms in a numerically stable fashion. This translates into the momentum gradients\n\n ̃∇mom.\n\nΣ−1 Lt+1 = ω TΣ−1 μ Lt+1 = ω ̃∇mom.\n\nt →Σ−1 μ Lt + (1 − ω) ̃∇μLt,\n\nΣ−1 Lt\n\nt+1\n\n ̃∇mom.\n\n(cid:16) ̃∇mom.\n\n(cid:17)\n\n+ (1 − ω) ̃∇Σ−1Lt+1,\n\n(16)\n\n(17)\n\nwhere the weight 0 < ω < 1 is a hyper-parameter.\n\nThe attentive reader may recognize the adoption of the form of retraction and parallel transport obtained from the SPD (matrix) manifold on the natural gradient obtained from the Gaussian manifold. This apparent inconsistency in mixing elements of different manifold structures is discussed in Appendix D.3. We show that, from a learning perspective, the discrepancy between the form of the SPD manifold Riemann gradient Σ−1∇Σ−1Σ−1 = −∇Σ and the natural gradient ̃∇Σ−1L = −2∇ΣL = −2Σ−1∇Σ−1 Σ−1 is absorbed in the learning rate β. In particular, our update rule can be derived within a fully consistent SPD manifold setting by updating (cid:0)μ, 2Σ−1(cid:1).\n\nIn the above view, we can now further clarify that the wording “Exact” in EMGVB is twofold. (i) In Tran et al. (2021a) the natural gradient Σ∇ΣΣ is in place of the actual one 2ΣΣ−1Σ, whose corresponding one for Σ−1 is the one that EMGVB actually adopts. (ii) Even by the adoption of the actual natural gradient −2Σ−1∇Σ−1LΣ, the use of the SPD retraction and vector transport forms 14,15, as of Tran et al. (2021a), are not well-justified: in Appendix D.3 these are justified, and EMGVB is shown to be a consistent approach. Note that EMGVB is exact in the sense of the above, yet still approximate in absolute terms due to the use of retraction. Retractions are approximate forms of the exponential map tracing back vectors on the tangent space to the manifold, which is generally cumbersome transform to compute and impractical (e.g. Absil et al., 2009; Hu et al., 2020).\n\nAlgorithm 1 summarizes the EMGVB update for the Gaussian prior-variational posterior case. Computational aspects are discussed in Appendix B.2.\n\nAlgorithm 1 EMGVB implementation 1: Set hyper-parameters: 0 < β, ω < 1, S 2: Set the type of gradient estimator, i.e. function log f (θs) 3: Set prior p(θ), likelihood form p(y|θ), and initial values μ, Σ−1 4: t = 1, Stop = false 5: Generate: θs ∼ qμ1,Σ1 , s = 1 . . . S 6: Compute: ˆgμ = Σ ˆ∇μL, ˆgΣ−1 = −2 ˆ∇ΣL 7: mμ = ˆgμ, mΣ−1 = ˆgΣ−1 8: while Stop = true do μ = μ + βmμ 9: Σ−1 10: Generate: θs ∼ qμt,Σt , s = 1 . . . S 11: Compute: ˆgμ, ˆgΣ−1 12: mμ = ωmμ + (1 − ω)ˆgμ 13: mΣ−1 = TΣ−1 14: Compute: ˆLt t = t + 1, Stop = fexit\n\nold = Σ−1, Σ−1 = RΣ−1 (βmΣ−1 )\n\nold →Σ−1 (mΣ−1 ) + (1 − ω)ˆgΣ−1\n\n(cid:0) ̄L, P, tmax\n\n(cid:1)\n\n15: 16: 17: end while\n\n6\n\n▷ eqs. 11,12 ▷ initialize momentum\n\n▷ EMGVB update for μ ▷ EMGVB update for Σ−1\n\n▷ as in line 6 ▷ eq. 16 ▷ eq. 17\n\n▷ eq. 13 ▷ see Appendix A\n\n4.3 FURTHER CONSIDERATIONS\n\nAlong with the choice of the gradient estimator and the use of momentum, there are other aspects of relevance in the implementation of EMGVB. Details are discussed in Appendix A.\n\n4.4\n\nISOTROPIC PRIOR\n\nFor mid-sized to large-scale problems, the prior is commonly specified as an isotropic Gaussian of mean μ0, often μ0 = 0, and covariance matrix Σ−1 0 = τ I, with τ > 0 a scalar precision parameter. The covariance matrix of the variational posterior can be either diagonal or not. Whether a full covariance specification (d2 − d parameters) can provide additional degrees of freedom that can gauge models’ predictive ability, a diagonal posterior (d parameters) can be practically and computationally convenient to adopt e.g. in large-sized problems. The diagonal-posterior assumption is largely adopted in Bayesian inference and VI (e.g. Blundell et al., 2015; Ganguly & Earp, 2021; Tran et al., 2021b) and Bayesian ML applications (e.g. Kingma & Welling, 2013; Graves, 2011; Khan et al., 2018b; Osawa et al., 2019), in Appendix A we provide a block-diagonal variant.\n\n4.4.1\n\nISOTROPIC PRIOR AND DIAGONAL GAUSSIAN POSTERIOR\n\nAssume a d-variate diagonal Gaussian variational specification, that is q ∼ N (μ, Σ) with diag(Σ) = σ2, Σij = 0, for i, j = 1, . . . , d and i ̸= j. In this case, Σ−1 = diag(cid:0)1/σ2(cid:1), where the division is intended element-wise, and ∇ΣL = diag(∇σ2 L), is now a d × 1 vector. Updating Σ−1 amounts to updating σ−2: the natural gradient retraction-based update for σ−2 is now based on the equality ̃∇σ−2L = −2∇σ2L, so that the general-case EMGVB update reads\n\nσ−2\n\nt+1 = Rσ−2\n\nt+1\n\n(−2β∇σ2L)\n\nand\n\nμt+1 = μt + σ2\n\nt ⊙ β∇μL\n\n(18)\n\nwhere ⊙ denotes the element-wise product. The corresponding MC estimators for the gradients are\n\n−2 ˆ∇σ2 L ≈ cσ2\n\nt\n\n+ σ−2 ⊙\n\n1 S\n\nS (cid:88)\n\ns=1\n\n(cid:104)(cid:16)\n\n1d − (θs − μt)2 ⊙ σ−2(cid:17)\n\nlog pt(y|θs)\n\n(cid:105)\n\nσ2 ⊙ ˆ∇μL ≈ cμt +\n\n1 S\n\nS (cid:88)\n\ns=1\n\n[(θs − μt) log pt(y|θs)],\n\n(19)\n\n(20)\n\nt\n\n= −σ−2 + τ , cμt = τ σ2 ⊙ (μ − μ0), θs ∼ N (cid:0)μ, diag(cid:0)σ2(cid:1)(cid:1), s = 1 . . . , S, (θs − μt)2 where cσ2 is intended element-wise, and 1d = (1, . . . , 1)⊤ ∈ Rd. In the Gaussian case with a general diagonal covariance matrix, retrieving σ2 from the updated σ−2 is inexpensive as σ2 , indicating that in this context the use of the h-function estimator is never advisable.\n\ni = 1/σ−2\n\ni\n\n4.4.2\n\nISOTROPIC PRIOR AND FULL GAUSSIAN POSTERIOR\n\nBecause of the full form of the covariance matrix, this case is rather analogous to the general one. In particular, factors cμt and cΣt in eq. 29 are replaced by (i) cΣt = −Σ−1 + τ , cμt = τ Σ(μt − μ0) or (ii) cΣt = 0, cμt = 0, respectively under the Gaussian-prior case (log ft(θs) = log p(y|θs)) and the general one (log ft(θs) = h(θ)). The MC estimators 7 and 8 apply: (i) leads to an estimator of reduced variance, while (ii) is identical to the general case.\n\n5 EXPERIMENTS\n\nWe validate and explore the empirical validity and feasibility of our suggested optimizer over four datasets and 12 models. These include logistic regression (Labor dataset), different volatility models on S&P 500 returns (Volatility dataset), and linear regression on Stock indexes (Istanbul data). Details on the datasets and models are summarized in Appendix 11. The main baseline for model comparison is the MGVB optimizer and (sequential) MCMC estimates representative of the true posterior. Additionally, we also include results related to the QBVI optimizer Magris et al. (2022). In this section, we report synthetic results on two tasks: logistic regression (classification) and volatility\n\n7\n\nmodeling with the FIGARCH model (regression). Results on the other datasets and models appear in Appendix C.4. Matlab codes are available at github.com/blinded.\n\nThe bottom rows in Figures 1, 5 clearly show that our results align with the sampling-based MCMC results and with the Maximum Likelihood (ML) estimates. Whereas marginal posterior approximations are rather close between EMGVB and MGVB, the top row in Figures 1, 5 show that the parameter learning is qualitatively different. The panels in Figure 1 depict the LB optimization process across the iterations. In a diagonal posterior setting, MGVB is exact and aligns with EMGVB (middle panel), however for non-diagonal posteriors, EMGVB’s lower bound shows an improved convergence rate on both the training and test sets (left and right panels respectively). Furthermore, we observe that the adoption of the h-function estimator has a minimal impact. From the point of view of standard performance measures, Figure 2 shows that compared to MGVB, at early iterations, EMGVB displays a steeper growth in model accuracy, precision, recall, and f1-score both on the training test and test set. Ultimately EMGVB and MGVB measures converge to the same value, yet the exact nature of the EMGVB update leads to convergence after approximately 200 iterations on the training set as opposed to 500 for MGVB. A similar behavior is observed for the FIGARCH(1,1,1) model, the top row of Figure 5.\n\nFigure 1: Top row: lower bound optimization. Bottom row: variational posteriors (for four of the eight parameters).\n\nFigure 2: EMGVB and MGVB performance on the Labour dataset across the iterations.\n\nTables 1 and 2 report such performance measures for the optimized variational posterior along with the value of the maximized lower bound. EMGVB is very close to the baselines and well-aligned with the MCMC and ML estimates, which, along with the estimates (in Table 3 for the logistic regression), show that EMGVB converges towards the same LB maximum, with a comparable predictive ability with respect to the alternatives. It is thus not surprising that the estimates, performance metrics, and value of the optimized LB are similar across the optimizers: they all converge to the same minimum but in a qualitatively different way. Also estimated variational covariance matrix for the full-covariance cases, closely replicates the one from the MCMC chain (see tables 4, 6). For the diagonal cases, MCMC and ML covariance matrices are not suitable for a direct comparison (see Appendix C.3). The sanity check in Figure 4 furthermore shows that the learning of either the\n\n8\n\n02004006008001000-400-380-36002004006008001000-400-380-36002004006008001000-145-140-13500.511.5012-2-1.5-1012-0.4-0.200.20246-1-0.50024MGVBEGVBMCMCML02004006008001000120062%64%66%68%70%72%Train performance02004006008001000120055%58%61%64%67%70%Test performancemean and covariance variation parameters is smooth and steady without wigglings or anomalies. As expected, the non-diagonal version leads to faster convergence while the use of the h-function estimator slightly stabilizes the learning process. In Table 7 we also show that the impact the number of MC samples S has on posterior means, likelihood, performance measures, and the optimized lower bound is minor for both the training and test phases.\n\nTrain\n\nTest\n\nL(θ⋆)\n\nAccuracy\n\nPrecision Recall\n\nf1\n\nEMGVB -356.642 -356.642 MGVB -356.642 QBVI\n\nMCMC ML\n\n0.713 0.713 0.713\n\n0.711 0.709\n\n0.712 0.712 0.712\n\n0.710 0.708\n\n0.703 0.703 0.703\n\n0.701 0.699\n\n0.708 0.708 0.708\n\n0.706 0.704\n\nL(θ⋆)\n\n-134.814 -134.814 -134.804\n\nAccuracy\n\nPrecision Recall\n\nf1\n\n0.698 0.698 0.698\n\n0.698 0.709\n\n0.679 0.679 0.679\n\n0.679 0.708\n\n0.674 0.674 0.674\n\n0.674 0.699\n\n0.676 0.676 0.676\n\n0.676 0.704\n\nTable 1: Optimizers’ performance for the Labor data on the train and test sets. See Appendix C.4 for extended results, including the use of the h−function estimator, diagonal and block-diagonal covariance specifications.\n\nFigure 3: FIGARCH(1,1,1) model. Top row: lower bound optimization. Bottom row: variational marginals.\n\n ̄ω\n\nφ\n\nd\n\nβ\n\nEMGVB 0.100 0.100 MGVB 0.100 QBVI 0.100 MCMC 0.099 ML\n\n0.059 0.059 0.059 0.062 0.060\n\n0.663 0.663 0.663 0.656 0.669\n\n0.481 0.481 0.480 0.480 0.483\n\nTrain\n\nL(θ⋆)\n\n-2007.62 -2007.62 -2007.62\n\nMSE\n\n25.773 25.773 25.771 25.784 25.767\n\nTest L(θ⋆) MSE\n\n-604.74 -604.72 -604.73\n\n3.988 3.988 3.988 3.979 3.996\n\nTable 2: Optimizers’ estimates and performance for the FIGARCH(1,1,1) model on the Volatility dataset.\n\n6 CONCLUSION\n\nWithin a Gaussian variational framework, we propose an algorithm based on manifold optimization to guarantee the positive-definite constraint on the covariance matrix, employing exact analytical solutions for the natural gradients. Our black-box optimizer results in a ready-to-use solution for VI, scalable to structured covariance matrices, that can take advantage of control variates, momentum, and alternative forms of the stochastic gradient estimator. We show the feasibility of our solution on a multitude of models. Our approach aligns with sampling methods and provides advantages over state-of-the-art baselines. Future research may investigate the applicability of our approach to a broader set of variational distributions, explore the advantages and limitations of the black-box framework, or attempt at addressing the online inversion bottleneck of manifold-based VI.\n\n9\n\n020040060080010001200-2010-2009-2008-2007EMGVB TrainMGVB Train020040060080010001200-607-606-605EMGVB TestMGVB Test02004006008001000120025.7625.7825.83.973.983.99 EMGVB MGVB00.10.205101520-0.500.5051000.511.50123400.5101234EMGVBMCMCMGVBMLREFERENCES\n\nRalph Abraham, Jerrold E Marsden, and Tudor Ratiu. Manifolds, tensor analysis, and applications,\n\nvolume 75. Springer Science & Business Media, 2012.\n\nP-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.\n\nIn Optimization Algorithms on Matrix Manifolds. Princeton University Press, 2009.\n\nOguz Akbilgic, Hamparsum Bozdogan, and M Erdal Balaban. A novel hybrid rbf neural networks\n\nmodel as a forecaster. Statistics and Computing, 24(3):365–375, 2014.\n\nTimothy D Barfoot. Multivariate gaussian variational inference by natural gradient descent. arXiv\n\npreprint arXiv:2001.10025, 2020.\n\nDavid M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisti-\n\ncians. Journal of the American statistical Association, 112(518):859–877, 2017.\n\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in\n\nneural network. In International Conference on Machine Learning, pp. 1613–1622, 2015.\n\nN. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre. Manopt, a Matlab toolbox for optimization on manifolds. Journal of Machine Learning Research, 15(42):1455–1459, 2014. URL https: //www.manopt.org.\n\nFulvio Corsi. A simple approximate long-memory model of realized volatility. Journal of Financial\n\nEconometrics, 7(2):174–196, 2009.\n\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\n\nstochastic optimization. Journal of machine learning research, 12(7), 2011.\n\nAnkush Ganguly and Samuel W. F. Earp. An introduction to variational inference. arXiv preprint\n\narXiv:2108.13083, 2021.\n\nAlex Graves. Practical variational inference for neural networks. Advances in neural information\n\nprocessing systems, 24, 2011.\n\nMatthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational infer-\n\nence. Journal of Machine Learning Research, 2013.\n\nReshad Hosseini and Suvrit Sra. Matrix manifold optimization for gaussian mixtures. Advances in\n\nNeural Information Processing Systems, 28, 2015.\n\nJiang Hu, Xin Liu, Zai-Wen Wen, and Ya-Xiang Yuan. A brief introduction to manifold optimiza-\n\ntion. Journal of the Operations Research Society of China, 8(2):199–248, 2020.\n\nBen Jeuris, Raf Vandebril, and Bart Vandereycken. A survey and comparison of contemporary algorithms for computing the matrix geometric mean. Electronic Transactions on Numerical Analysis, 39(ARTICLE):379–402, 2012.\n\nMohammad Khan and Wu Lin. Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models. In Artificial Intelligence and Statistics, pp. 878–887. Proceedings of Machine Learning Research, 2017.\n\nMohammad Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava. Fast and scalable bayesian deep learning by weight-perturbation in adam. In International Conference on Machine Learning, pp. 2611–2620. Proceedings of Machine Learning Research, 2018a.\n\nMohammad Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava. Fast and scalable bayesian deep learning by weight-perturbation in adam. In International Conference on Machine Learning, pp. 2611–2620, 2018b.\n\nMohammad Emtiyaz Khan and Didrik Nielsen. Fast yet simple natural-gradient descent for variational inference in complex models. In 2018 International Symposium on Information Theory and Its Applications, pp. 31–35, 2018.\n\n10\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nAlp Kucukelbir, Rajesh Ranganath, Andrew Gelman, and David Blei. Automatic variational infer-\n\nence in stan. Advances in neural information processing systems, 28, 2015.\n\nJouko Lampinen and Aki Vehtari. Bayesian approach for neural networks—review and case studies.\n\nNeural networks, 14(3):257–274, 2001.\n\nErich L. Lehmann and George Casella. Theory of point estimation. Springer texts in statistics.\n\nSpringer-Verlag, New York, second edition, 1998. ISBN 9780387985022.\n\nWu Lin, Mark Schmidt, and Mohammad Emtiyaz Khan. Handling the positive-definite constraint in the bayesian learning rule. In International Conference on Machine Learning, pp. 6116–6126. PMLR, 2020.\n\nWu Lin, Frank Nielsen, Mohammad Emtiyaz Khan, and Mark Schmidt. Structured second-order\n\nmethods via natural gradient descent. arXiv preprint arXiv:2107.10884, 2021.\n\nDavid J. C. Mackay. Probable networks and plausible predictions — a review of practical bayesian methods for supervised neural networks. Network: Computation In Neural Systems, 6:469–505, 1995.\n\nDavid John Cameron Mackay. Bayesian methods for adaptive models. PhD thesis, California\n\nInstitute of Technology, 1992.\n\nMartin Magris, Mostafa Shabani, and Alexandros Iosifidis. Quasi black-box variational inference\n\nwith natural gradients for bayesian learning. arXiv preprint arXiv:2205.11568, 2022.\n\nKanti V Mardia and Roger J Marshall. Maximum likelihood estimation of models for residual\n\ncovariance in spatial regression. Biometrika, 71(1):135–146, 1984.\n\nShakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient esti-\n\nmation in machine learning. Journal of Machine Learning Research, 21(132):1–62, 2020.\n\nThomas Alvin Mroz. The sensitivity of an empirical model of married women’s hours of work to\n\neconomic and statistical assumptions. PhD thesis, Stanford University, 1984.\n\nArkadij Semenoviˇc Nemirovskij and David Borisovich Yudin. Problem complexity and method\n\nefficiency in optimization. Wiley-Interscience, 1983.\n\nKazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz Khan, Anirudh Jain, Runa Eschenhagen, Richard E. Turner, and Rio Yokota. Practical deep learning with bayesian principles. In Advances in Neural Information Processing Systems, volume 32, pp. 1–13, 2019.\n\nJohn Paisley, David M. Blei, and Michael I. Jordan. Variational bayesian inference with stochastic\n\nsearch. arXiv preprint arXiv:1206.6430, 2012.\n\nRajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational inference. In Artificial\n\nintelligence and statistics, pp. 814–822, 2014.\n\nGarvesh Raskutti and Sayan Mukherjee. The information geometry of mirror descent. IEEE Trans-\n\nactions on Information Theory, 61(3):1451–1457, 2015.\n\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-\n\ncal statistics, pp. 400–407, 1951.\n\nTim Salimans and David A. Knowles. On using control variates with stochastic approximation for variational bayes and its connection to stochastic linear regression. arXiv preprint arXiv:1401.1022, 2014.\n\n11\n\nTim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational inference: Bridging the gap. In International conference on machine learning, pp. 1218–1226. PMLR, 2015.\n\nLawrence K. Saul, Tommi Jaakkola, and Michael I. Jordan. Mean field theory for sigmoid belief\n\nnetworks. Journal of artificial intelligence research, 4:61–76, 1996.\n\nLinda SL Tan. Natural gradient updates for cholesky factor in gaussian and structured variational\n\ninference. arXiv preprint arXiv:2109.00375, 2021.\n\nTimo Ter ̈asvirta. An introduction to univariate garch models. In Handbook of financial time series,\n\npp. 17–42. Springer, 2009.\n\nMinh-Ngoc Tran, Dang H. Nguyen, and Duy Nguyen. Variational Bayes on manifolds. Statistics and Computing, 31(6):71, September 2021a. ISSN 1573-1375. doi: 10.1007/s11222-021-10047-1.\n\nMinh-Ngoc Tran, Trong-Nghia Nguyen, and Viet-Hung Dao. A practical tutorial on variational\n\nbayes. arXiv preprint arXiv:2103.01327, 2021b.\n\nFelix Trusheim, Alexandru Condurache, and Alfred Mertins. Boosting black-box variational inferIn 2018 24th International Conference on Pattern\n\nence by incorporating the natural gradient. Recognition (ICPR), pp. 19–24. IEEE, 2018.\n\nMartin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and variational\n\ninference. Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.\n\nDaan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J ̈urgen Schmidhuber. Natural evolution strategies. The Journal of Machine Learning Research, 15(1):949–980, 2014.\n\nMing Xu, Matias Quiroz, Robert Kohn, and Scott A Sisson. Variance reduction properties of the In The 22nd International Conference on Artificial Intelligence and\n\nreparameterization trick. Statistics, pp. 2711–2720. PMLR, 2019.\n\nGuodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient as variational inference. In International Conference on Machine Learning, pp. 5852–5861. PMLR, 2018.\n\n12\n\nA FURTHER CONSIDERATIONS ON EMGVB IMPLEMENTATION\n\nA.1 VARIANCE REDUCTION\n\nAs EMGVB does not involve model gradients the use of the reparametrization trick (RT) (Blundell et al., 2015) is not immediate. While eq. 5 would generally hold, the form of the EMGVB gradient estimators under the RT would differ from eqs. 7, 8: we develop EMGVB as a general and readyto-use solution for VI that does not require model-specific derivations, yet one may certainly enable the RT within EMGVB. Though the use of the RT is quite popular in VI and ML as it empirically yields more accurate estimates of the gradient of the variational objective than alternative approaches (Mohamed et al., 2020), note that the variance of the RT estimator can be higher than that of the score-function estimator and the path-wise RT estimator is not necessarily preferable (Xu et al., 2019; Mohamed et al., 2020). Not less importantly, note that the use of the score estimator is broader as it does not require log p(y|θ) to be differentiable.\n\nControl Variates (CV) stand as a simple and effective approach for reducing the variance of the MC gradient estimator, e.g. (Paisley et al., 2012). The CV estimator\n\n1 S\n\nS (cid:88)\n\ns=1\n\n∇ζ[log q(θs)](log p(y|θs) − c),\n\nis unbiased for the expected gradient, but of equal or smaller variance that the naive MC one. For i = 1, 2 the optimal ci minimizing the variances of the CV estimator is\n\nc⋆ = Cov(∇ζ[log q(θ)] log p(y|θ), ∇ζ log q(θ))/Var(∇ζ log q(θ)).\n\n(21)\n\nBy enabling CVs, S can be tuned to balance the estimates’ variance and computational performance. In Table 7 we asses that for logistic regression values of S as little as 10 appear satisfactory, yet if the iterative computation of the log-likelihood is not prohibitive we suggest the adoption of a more generous value, e.g. S ≈ 100. Magris et al. (2022) furthermore shows that the denominator in 21 is analytically tractable for a Gaussian q, reducing the variance of estimated c⋆ and thus improving the overall CVs’ efficiency. If model gradients are available one may use CV along with the RT to further enhance the efficiency of the expected gradient estimation.\n\nA.2 LB SMOOTHING AND STOPPING CRITERION\n\nThe stochastic nature of the gradient estimator introduces some noise in the estimated LB ˆL that can violate its expected non-decreasing behavior across the iterations. By setting window of size w we rather consider the moving average on the LB, ̄Lt = 1/w (cid:80)t ˆLt−i+1, whose variance is reduced and behavior stabilized. By keeping track of max ̄L we terminate the the learning after max ̄L did not improved for P iterations (patience parameter) or after a maximum number of iteration (tmax) is reached (stopping criterion fexit function in Algorithm 1).\n\ni=1\n\nA.3 CONSTRAINTS ON MODEL PARAMETERS\n\nEMGVB assumes a Gaussian variational posterior, that is the parameters are unbounded and defined over the entire real line. Assuming that a model parameter θ is required to lie on a support S, to impose such a constraint it suffices to identify a feasible transform T : R → S and apply the EMGVB update to the unconstrained parameter ψ = T −1(θ). Certainly, by applying VI on ψ we require that the variational posterior assumption holds for ψ rather than θ. The actual distribution for θ under a Gaussian variational ψ can be computed (or approximated with a sampling method) as N (cid:0)T −1(θ); μ, Σ(cid:1)| det(JT −1 (θ))|, with JT −1 the Jacobin of the inverse transform (Kucukelbir et al., 2015). Example. For the GARCH(1,1) model (see Section 5) the intercept ω, the autoregressive coefficient of the lag-one squared return α and moving-average coefficient β of the lag-one conditional variances need to satisfy the stationarity conditions α + β < 1 and ω > 0, α ≥ 0, β ≥ 0. Such conditions are unfeasible under a Gaussian variational approximation: we estimate the unconstrained parameters ψω, ψα, ψβ, where ω = T (ψω), α = T (ψα)(1 − T (ψβ)), β = T (ψα)T (ψβ) with T (x) = exp(x)/(1 + exp(x)) for x real, on which Gaussian’s prior-posterior assumptions apply.\n\n13\n\nA.4 GRADIENT CLIPPING\n\nEspecially for low values of S, and even more if a variance control method is not adopted, the stochastic gradient estimate may be poor and the offset from its actual value large. This may result in updates whose magnitude is too big either in a positive or negative direction. Especially at early iterations, and with poor initial values, this issue may e.g. cause complex roots in eq. 15. At each iteration t, to control for the magnitude of the stochastic gradient ˆgt we rescale its ↕2-norm ||ˆgt|| whenever it is larger than a fixed threshold lmax by replacing ˆgt with ˆgtlmax||ˆgt||, which preserves its norm. Gradient clipping can be either applied to the gradients ˆ∇μ, ˆ∇Σ or to the natural gradients Σ ˆ∇μ, −2 ˆ∇Σ and in any case before obtaining momentum gradients. We suggest applying gradient clipping readily to ˆ∇μ, ˆ∇Σ to promptly mitigate the impact that far-from-the-mean estimates may have on successive computations.\n\nA.5 ADAPTIVE LEARNING RATE\n\nIt is convenient to adopt an adaptive learning rate or scheduler for decreasing β after a certain number of iterations. Typical options are that of reducing β by a certain factor (e.g. 0.2) every set number of iterations (e.g. 100), or decrease it after iteration t′ e.g. by setting βt = min −(β, β t′ t ), where t′ is a fraction (e.g. 0.7) of the maximum number of iterations tmax allowed before the LB optimization is stopped.\n\nA.6 CLASSIFICATION VS. REGRESSION\n\nWe point out that the EMGVB framework is applicable to both regression and classification problems. In generic DL classification problems, predictions are based on the class of maximum probability which is computed by applying a softmax function at the last layer returning to the probability pi(cj) of a certain class cj for the i-th sample, i = 1, . . . , M . From these probabilities it is straightforward to compute the model log-likelihood as (cid:80)M i=1 yi,ctrue log pi(ctrue), with yi,ctrue representing the one-key-hot encoding of the i-th sample, whose true class is ctrue. For regression, the parametric form of log p(y|θ) is clearly different and model-specific (e.g. regression with normal errors as opposed to Poisson regression, with the latter being feasible as the use of the score estimator does not require the likelihood to be differentiable). Note that however additional parameters may enter into play besides the ones involved in the back-bone forward model: e.g. for regression with normal errors tackled with an artificial neural network, the Gaussian-form likelihood involves the regression variance, which is an additional parameter over the network’s ones, or for Student-t errors the degree of freedom parameter ν (with the constraint ν > 2). See the application in Appendix C.3.\n\nA.7 MEAN-FIELD VARIANT\n\nAssume that for a d-variate model the Gaussian variational posterior is factorized as\n\nqζ(θ) = qζ1(θ1)qζ2 (θ2), . . . , qζh (θh) =\n\nh (cid:89)\n\ni=1\n\nqζh (θh),\n\nwith h ≤ k. If h = d this corresponds to a full-diagonal case where each θi is a scalar and the covariance between ζ1, . . . , ζk is ignored. If h < d, the variational covariance matrix Σ of qζ corresponds to a block-diagonal matrix, and some of the θs are indeed vectors. In any case, the expected gradients with respect to each block of parameters can be computed independently, given the scalars hζh (θ) or log p(y|θ), depending on whether the h-function estimator is used. For a Gaussian prior, its covariance matrix can be diagonal, full, block-diagonal with a structure matching or not that of S. Eqs. 11, 12, with the condition 29 can be used as a starting point to derive casespecific EMGVB variants based on the form of the prior covariance.\n\nAlgorithm 2 summarizes the case with an isotropic Gaussian prior of zero-mean and variance τ , using the gradient estimator based on the log-likelihood: μi, Σi (Σ−1 ) respectively denote the mean and covariance (precision) matrix of the i-th block of Σ. In this case, the block-wise natural gradients\n\ni\n\n14\n\nare estimated as\n\nΣ ˆ∇μiL = −Sτ −1μi +\n\n1 S\n\nS (cid:88)\n\n[(θsi − μi) log p(y|θs)],\n\ns=1\n\n−2 ˆ∇ΣiL = −Σi + diag(cid:0)τ −1(cid:1) +\n\n1 S\n\nS (cid:88)\n\ns=1\n\n(cid:104)(cid:16)\n\nΣ−1\n\ni − Σ−1\n\ni\n\n(θsi − μi)(θsi − μi)⊤Σ−1\n\ni\n\n(cid:17)\n\n(cid:105)\n\nlog p(y|θs)\n\n,\n\nwhere θs is a sample from the variational posterior. θs can be obtained by concatenating marginal samples from each block, θs = [θs1, . . . , θsh ], with θsi ∼ qμi,Σi, i = 1, . . . , h\n\nAlgorithm 2 EMGVB for a block-diagonal covariance matrix (prior with zero-mean and covariance matrix τ I) 1: Set hyper-parameters: 0 < β, ω < 1, S 2: Set the type of gradient estimator, i.e. function log f (θs) 3: Set prior p(θ; 0, τ ), likelihood p(y|θ), and initial values μ, Σ−1 4: t = 1, Stop = false 5: Generate: θs = [θs1 , . . . , θsh ], 6: Compute: log p(y|θs) 7: for i = 1, . . . , h do 8:\n\nθsi ∼ qμi,Σi , s = 1 . . . S, i = 1, . . . , h\n\n= −2 ˆ∇Σi L\n\nCompute: ˆgμi = Σi ˆ∇μi L, mμi = ˆgμi , mΣ−1\n\n= ˆgΣ−1\n\ni\n\ni\n\nˆgΣ−1\n\ni\n\n9:\n\n10: end for 11: while Stop = true do 12: 13: 14:\n\nˆL = 0 for i = 1, . . . , h do\n\n15:\n\n16: 17: 18: 19: 20:\n\n21:\n\n22:\n\ni\n\n, Σ−1\n\nμi = μi + βmμi old,i = Σ−1 Σ−1 end for Generate: θs = [θs1 , . . . , θsh ], Compute: log p(y|θs), for i = 1, . . . , h do\n\ni = RΣ−1\n\ni\n\nlog p(θs)\n\ni\n\nCompute: ˆgμi , ˆgΣ−1 mμi = ωmμi + (1 − ω)ˆgμi mΣ−1 ˆLt = L + 1 t = t + 1, Stop = fexit\n\nold,i→Σ−1 S log p(θs) + 1\n\n= TΣ−1\n\n(cid:16)\n\ni\n\ni\n\ni\n\nmΣ−1\n\n23: 24: 25: 26: end while\n\nend for\n\n(cid:16)\n\nβmΣ−1\n\ni\n\n(cid:17)\n\nθsi ∼ qμi,Σi , s = 1 . . . S, i = 1, . . . , h\n\n(cid:17)\n\n+ (1 − ω)ˆgΣ−1\n\ni\n\nS log p(y|θs) − 1 (cid:1)\n\n(cid:0) ̄L, P, tmax\n\nS log qmi,Σi (θsi )\n\nB OPTIMALITY AND EFFICIENCY\n\nB.1 OPTIMALITY\n\nSeveral authors (e.g. Khan & Lin, 2017; Khan et al., 2018a) obtained update rules for VI by developing over SGD-like updates for the natural parameters of the variational posterior. By updating the natural parameter λ and exploiting its definition, it is relatively simple to recover the update rules for μ and Σ. Indeed from the SGD update for the natural parameter λt+1 = λt + β ̃∇λL(λ) it follows\n\nμt+1 = Σt+1\n\n= Σt+1\n\nand\n\n(cid:104) Σ−1 (cid:2)(cid:0)Σ−1\n\n(cid:104) ̃∇λL(λt)\n\n(cid:105)(cid:105)\n\nt μt + β t − 2β[∇ΣL(λt)](cid:1)μt + β[∇μL(λt)](cid:3),\n\nΣ−1\n\nt+1 = Σ−1\n\nt − 2β[∇ΣL(λt)]\n\n(22)\n\n(23)\n\nBy replacing Σ−1 (2018a) obtain\n\nt − 2β[∇ΣL(λt)] with Σ−1\n\nt+1 in the update for μ, Khan & Lin (2017); Khan et al.\n\nμt+1 = μt + βΣt+1[∇μL(λt)].\n\n(24)\n\n15\n\nEq. 23 does not apply to the EMGVB update as the update for Σ−1 is carried out with retraction and does not result from an SGD update of the natural parameter λ2 = − 1 t+1, yet the form of equation 22 does apply. We refer to eq. 23 as an indirect update since derived from the natural parameter update. Note that, the μ update exploits Σ−1 t+1, resulting in a one-step forward-looking rule. It is relevant to investigate whether the update 22 is preferable to the EMGVB update for μ. Intuitively one might expect that eq. 22 is preferable as it somewhat readily exploits the updated Σt+1 value as soon as it becomes available. The following theorem however proves that this is not the case, a proof is provided in Appendix D.4.\n\n2 Σ−1\n\nTheorem 1 For the Gaussian distribution with parameters ζ = (μ, vec(Σ)) the optimization problem\n\nζt+1 = arg min\n\nζ\n\n⟨ζ, ∇ζL(ζt)⟩ +\n\nDKL(pζ||pζt),\n\n1 β\n\nwhere ⟨·, ·⟩ denotes the inner product and ∇ζL(ζt) = ( ∇μL(ζ), vec(∇ΣL(ζ)) )|ζ=ζt, is convex with respect to ζ. The optimum update for μ is available in closed form and analogous to that of the EMGBV update.\n\nThe objective in Theorem 1, is that of the mirror descent developed by Nemirovskij & Yudin (1983), where a non-Euclidean geometry is induced by considering a penalized optimization obtained through proximity function such as the Bergman divergence, which equals the KL divergence for exponential-family distributions. In this regard see (Raskutti & Mukherjee, 2015).\n\nFollowing Theorem 1, the update for μ in eq. 5 is optimal in terms of the above objective and perhaps counter-intuitively the indirect forward-looking update 22 is proved to provide non-optimal steps toward the maximization of the lower bound. The EMGVB update for μ is thus preferable over the alternative of recovering an indirect update rule for μ starting from an SGD update on the natural parameter as e.g. in (Khan & Lin, 2017; Khan et al., 2018a): in the above terms of Theorem 1, the EMGVB update for μ is the best one could take.\n\nB.2 COMPUTATIONAL ASPECTS\n\nIn terms of computational complexity, the exact EMGVB implementation is at no additional cost. Actually, the cost of computing the natural gradient in EMGVB as −2∇−1 Σ L is cheaper than the one in MGVB, Σ∇ΣLΣ, O(cid:0)k3(cid:1) operations for each matrix multiplication. However, both MGVB and EMGVB share a cumbersome matrix inversion.\n\nGoing back to eqs. 12 and 11, it is noticeable that under the most general estimator based on the h-function, Σ is not involved in any computation, neither in the gradient involved in the updated for Σ−1 nor in that for μ, suggesting that implicitly the EMGVB optimization routine does not require the inversion of Σ−1. The above point however ignores that the update for Σ−1 is masked by the underlying retraction. For the retraction form in eq. 14, both Σ−1 and its inverse Σ are needed, thus implying a matrix inversion at every iteration. That is, the covariance matrix inversion is implicit in MGVB and EMGVB methods, which both require Σ−1 and Σ at every iteration (with little surprise, as the form of the retraction is a second-order approximation of the exponential map). With the h-function estimator even though neither eq. 7 nor 8 involve Σ, the inversion of Σ−1 is still necessary, as Σ is required in retraction. Similarly, the adoption of the log-likelihood estimator under the Gaussian regime in eq. 29, is not computationally more expensive than the h-function case, as Σ, involved in cμt, is anyway required in retraction. As outlined in Section 4, Σ can be conveniently recovered from the Cholesky factor of Σ−1, with a lesser number of flips. Lastly, if Σ−1 (Σ) is diagonal, the inversion is trivial and, when applicable, eq. 11 is preferred.\n\nC EXPERIMENTS\n\nC.1 RESULTS FOR THE LABOUR DATA\n\n16\n\nMGVB EMGVB QBVI\n\nMGVB† EMGVB† QBVI†\n\nMGVBdiag. EMGVBdiag. QBVIdiag.\n\nMGVB† diag. EMGVB† diag. QBVI† diag.\n\nMCMC ML\n\nβ0\n\nβ1\n\nβ2\n\nβ3\n\nβ4\n\nβ5\n\n0.678 −1.485 −0.082 −0.573 0.679 −1.485 −0.082 −0.573 0.679 −1.485 −0.082 −0.573\n\n0.678 −1.487 −0.084 −0.574 0.678 −1.487 −0.084 −0.574 0.678 −1.487 −0.084 −0.574\n\n0.576 −1.430 −0.056 −0.541 0.578 −1.431 −0.057 −0.542 0.579 −1.432 −0.057 −0.542\n\n0.573 −1.430 −0.057 −0.542 0.575 −1.431 −0.057 −0.543 0.576 −1.431 −0.057 −0.543\n\n0.675 −1.484 −0.084 −0.570 0.679 −1.476 −0.085 −0.571\n\n0.494 −0.637 0.494 −0.637 0.494 −0.637\n\n0.493 −0.638 0.493 −0.638 0.493 −0.638\n\n0.493 −0.638 0.493 −0.637 0.493 −0.637\n\n0.493 −0.635 0.493 −0.635 0.493 −0.635\n\n0.493 −0.639 0.485 −0.625\n\nβ6\n\n0.608 0.608 0.608\n\n0.609 0.609 0.609\n\n0.593 0.593 0.593\n\n0.593 0.593 0.593\n\n0.608 0.599\n\nβ7\n\n0.041 0.041 0.041\n\n0.048 0.048 0.048\n\n0.105 0.104 0.103\n\n0.109 0.107 0.107\n\n0.051 0.045\n\nTable 3: Parameters’ estimates for the labour dataset. Top: posterior means, middle: variances, bottom: covariances (×103). † denotes the use of the h-function gradient estimator, diag the use of a diagonal variational posterior.\n\nβ0\n\nβ1\n\nβ2\n\nβ3\n\nβ4\n\nβ5\n\nβ6\n\nβ7\n\nMGVB\n\nB V\nG M\nE\n\nC M\nC M\n\n−1.639 −0.848 −0.697 0.298\n\nβ0 β1 −1.634 β2 −0.810 β3 −0.700 β4 β5 β6 β7 −2.809 −0.048 −0.103 −0.381 −0.218 −0.633 −0.073\n\n0.192 −2.709 0.084 −0.118 0.050 −0.064 −0.109 0.128 −0.189 −0.397 −0.135 −0.259 −0.191 −1.322 −0.611 −0.028\n\n0.163 0.262 1.425 −0.451 −0.020 0.066 0.413 0.039\n\n0.288 1.393 0.077 −0.361 0.312 −0.051 0.176\n\n0.073 −0.021 −0.096 −0.230 −1.202\n\n0.083 0.068 −0.144\n\n0.406 0.127 0.008\n\nβ0\n\nβ1\n\nβ2\n\nβ3\n\nβ4\n\nβ5\n\nβ6\n\nβ7\n\nML\n\n−1.578 −0.800 −0.684 0.274\n\nβ0 β1 −1.510 β2 −0.791 β3 −0.654 β4 β5 β6 β7 −2.603 −0.085 −0.097 −0.349 −0.119 −0.590 −0.102\n\n0.194 −2.701 0.074 −0.084 0.024 −0.058 −0.093 0.117 −0.164 −0.352 −0.172 −0.265 −0.109 −1.286 −0.613 −0.097\n\n0.318 0.067 1.378 −0.422 −0.027 0.102 0.393 0.067\n\n0.249 1.307 0.047 −0.414 0.334 −0.049 0.172\n\n0.111 −0.036 −0.137 −0.266 −1.282\n\n0.072 0.089 −0.177\n\n0.381 0.111 0.006\n\nTable 4: Parameters’ covariance matrices for the labour dataset. Entries are multiplied by 102.\n\nFigure 4: Parameter learning across the iterations under different variants of the EMGVB algorithm for the labour dataset.\n\n17\n\n020040060080010001200-2.5-2-1.5-1-0.500.51020040060080010001200-100-50050100150200250300350400450EMGVB MGVB QBVI\n\nEMGVB† MGVB† QBVI†\n\nEMGVBdiag. MGVBdiag. QBVIdiag.\n\nL(θ⋆)\n\n−356.642 −356.642 −356.642\n\n−356.635 −356.635 −356.635\n\n−358.423 −358.428 −358.419\n\nEMGVB† diag. −358.421 MGVB† diag. −358.427 QBVI† diag. −358.418\n\nMCMC ML\n\nEMGVB MGVB QBVI\n\nEMGVB† MGVB† QBVI†\n\nEMGVBdiag. MGVBdiag. QBVIdiag.\n\nL(θ⋆)\n\n−134.814 −134.814 −134.804\n\n−134.805 −134.805 −134.805\n\n−136.864 −136.871 −136.857\n\nEMGVB† diag. −136.876 MGVB† diag. −136.883 QBVI† diag. −136.871\n\nMCMC ML\n\nTrain log p(y|θ⋆) Accuracy\n\nPrecision\n\nRecall\n\n−332.992 −332.991 −332.992\n\n−332.990 −332.990 −332.990\n\n−333.124 −333.129 −333.121\n\n−333.127 −333.133 −333.124\n\n−332.990 −332.983\n\n0.713 0.713 0.713\n\n0.711 0.711 0.711\n\n0.711 0.711 0.711\n\n0.709 0.709 0.709\n\n0.711 0.709\n\n0.712 0.712 0.712\n\n0.710 0.710 0.710\n\n0.710 0.710 0.710\n\n0.708 0.708 0.708\n\n0.710 0.708\n\n0.703 0.703 0.703\n\n0.701 0.701 0.701\n\n0.701 0.701 0.701\n\n0.700 0.700 0.700\n\n0.701 0.699\n\nTest log p(y|θ⋆) Accuracy\n\nPrecision\n\nRecall\n\n−113.854 −113.854 −113.854\n\n−113.855 −113.855 −113.855\n\n−114.171 −114.178 −114.167\n\n−114.220 −114.228 −114.217\n\n−113.838 −113.800\n\n0.698 0.698 0.698\n\n0.698 0.698 0.698\n\n0.667 0.667 0.667\n\n0.667 0.667 0.667\n\n0.698 0.709\n\n0.679 0.679 0.679\n\n0.679 0.679 0.679\n\n0.644 0.644 0.644\n\n0.644 0.644 0.644\n\n0.679 0.708\n\n0.674 0.674 0.674\n\n0.674 0.674 0.674\n\n0.640 0.640 0.640\n\n0.640 0.640 0.640\n\n0.674 0.699\n\nf1\n\n0.708 0.708 0.708\n\n0.706 0.706 0.706\n\n0.706 0.706 0.706\n\n0.704 0.704 0.704\n\n0.706 0.704\n\nf1\n\n0.676 0.676 0.676\n\n0.676 0.676 0.676\n\n0.642 0.642 0.642\n\n0.642 0.642 0.642\n\n0.676 0.704\n\nTable 5: Models’ performance for the labour data on the train and test sets. † denotes the use of the h-function gradient estimator, diag the use of a diagonal variational posterior.\n\nEMGVB MGVB QBVI\n\nEMGVB† MGVB† QBVI†\n\nEMGVBdiag. MGVBdiag. QBVIdiag.\n\nEMGVB† diag. MGVB† diag. QBVI† diag.\n\nMCMC ML\n\nβ0\n\n4.242 4.240 4.255\n\n4.068 4.069 4.068\n\n0.909 0.909 0.910\n\n0.871 0.871 0.872\n\n3.967 4.079\n\nβ1\n\n5.465 5.574 5.484\n\n5.415 5.416 5.415\n\n3.356 3.356 3.363\n\n3.391 3.390 3.394\n\n5.364 5.436\n\nβ2\n\n0.601 0.615 0.603\n\n0.594 0.598 0.594\n\n0.255 0.255 0.255\n\n0.252 0.252 0.252\n\n0.589 0.589\n\nβ3\n\n1.479 1.506 1.484\n\n1.464 1.469 1.464\n\n0.895 0.894 0.896\n\n0.897 0.897 0.897\n\n1.406 1.457\n\nβ4\n\n1.220 1.288 1.224\n\n1.290 1.287 1.290\n\n0.988 0.988 0.989\n\n1.051 1.051 1.052\n\n1.274 1.276\n\nβ5\n\n1.931 2.127 1.938\n\n2.043 2.058 2.043\n\n0.955 0.955 0.956\n\n0.999 1.000 1.000\n\n2.071 2.059\n\nβ6\n\n1.879 1.927 1.886\n\n1.968 1.974 1.968\n\n0.994 0.993 0.995\n\n1.006 1.006 1.007\n\n1.966 1.960\n\nβ7\n\n4.477 4.393 4.493\n\n4.385 4.387 4.385\n\n1.286 1.286 1.288\n\n1.313 1.313 1.315\n\n4.248 4.393\n\nTable 6: Variances of the parameters for the labour dataset. Entries are multiplied by 102. † denotes the use of the h-function gradient estimator, diag the use of a diagonal variational posterior. For a discussion on the diagonal case see Appendix C.3.\n\n18\n\nS\n\n10 20 30 50 75 100 150 200 300\n\nt\n\n5.103 6.984 8.137 9.015 11.788 14.652 22.862 24.169 31.708\n\nβ0\n\nβ1\n\nβ2\n\nβ3\n\nβ4\n\nβ5\n\n0.687 −1.498 −0.086 −0.576 0.682 −1.498 −0.085 −0.571 0.684 −1.493 −0.087 −0.573 0.680 −1.492 −0.085 −0.576 0.682 −1.489 −0.085 −0.575 0.682 −1.490 −0.085 −0.574 0.680 −1.488 −0.085 −0.574 0.681 −1.487 −0.085 −0.574 0.680 −1.488 −0.085 −0.575\n\n0.500 −0.636 0.499 −0.641 0.495 −0.640 0.494 −0.643 0.492 −0.642 0.493 −0.639 0.492 −0.639 0.492 −0.638 0.493 −0.638\n\nβ6\n\n0.605 0.607 0.609 0.611 0.611 0.609 0.609 0.608 0.609\n\nβ7\n\nL(θ0)\n\n0.037 −430.216 0.043 −429.626 0.043 −434.863 0.047 −435.890 0.045 −436.282 0.044 −436.924 0.046 −436.759 0.046 −437.036 0.046 −436.529\n\nL(θ⋆)\n\n−356.687 −356.667 −356.653 −356.645 −356.642 −356.640 −356.640 −356.638 −356.639\n\nL(θ⋆)\n\n−134.865 −134.899 −134.793 −134.805 −134.814 −134.810 −134.791 −134.788 −134.832\n\nTrain log p(y|θ⋆) Accuracy\n\nPrecision\n\nRecall\n\n−332.991 −332.994 −332.990 −332.988 −332.992 −332.988 −332.990 −332.990 −332.989\n\n0.711 0.713 0.709 0.713 0.713 0.711 0.711 0.713 0.711\n\n0.710 0.712 0.708 0.712 0.712 0.710 0.710 0.712 0.710\n\n0.701 0.703 0.699 0.703 0.703 0.701 0.701 0.703 0.701\n\nTest log p(y|θ⋆) Accuracy\n\nPrecision\n\nRecall\n\n−113.910 −113.856 −113.827 −113.838 −113.854 −113.876 −113.845 −113.840 −113.842\n\n0.698 0.698 0.698 0.698 0.698 0.698 0.698 0.698 0.698\n\n0.679 0.679 0.679 0.679 0.679 0.679 0.679 0.679 0.679\n\n0.674 0.674 0.674 0.674 0.674 0.674 0.674 0.674 0.674\n\nf1\n\n0.706 0.708 0.704 0.708 0.708 0.706 0.706 0.708 0.706\n\nf1\n\n0.676 0.676 0.676 0.676 0.676 0.676 0.676 0.676 0.676\n\nS\n\n10 20 30 50 75 100 150 200 300\n\nS\n\n10 20 30 50 75 100 150 200 300\n\nTable 7: Estimated parameters and performance measures on the labour dataset for EMGVB (fullposterior) for different sizes of the number of MC draws for the estimation of the stochastic gradients S. t refers to the run-time per iteration (in milliseconds), L(θ0) to the LB evaluated at the initial parameters. For each S, a common random seed used.\n\n19\n\nC.2 VOLATILITY MODELS\n\nOur second set of experiments involves the estimation of several GARCH-family volatility models. The models in Table 8 differ for the number of estimated parameters, the form of the likelihood function (which can be quite complex as for the FIGARCH models), and constraints imposed on the parameters. Besides the GARCH-type models, we include the well-known linear HAR model for realized volatility (Corsi, 2009). We performed a preliminary study for retaining only relevant models, e.g. we observed that for a GARCH(1,0,2) β2 is not significant, so we trained a GARCH(1,0,1), or that the autoregressive coefficient of the squared innovations is always significant only at lag one, so we did not consider further lags for α. For α, β, γ we restricted the search up to lag 2. Except for HAR’s parameter β3, all the parameters of all the models are statistically significant under standard ML at 5%. Note that the aim of this experiment is that of applying VI and EMGVB to the above class of models, not to discuss their empirical performance or forecasting ability. For the reader unfamiliar with the above (standard) models, discussion and notation we refer e.g. to the accessible introduction of Ter ̈asvirta (2009).\n\nAs for the Labour data, we report the values of the smoothed lower bound computed at the optimized ˆL(θ⋆), the model’s log-likelihood in the estimates posterior parameter p(y|θ⋆) and the parameter MSE between the fitted values and squared daily returns, used as a volatility proxy. Details on the data and hyperparameters are provided in Table 11.\n\nFigures 6 provide sample illustrations of the lower bound maximization for the GJR(1,1,1) model (perhaps the most used and effective in applications beyond the standard GARCH(1,0,1)) and the FIGARCH(1,1,1) model, the most complex one among our selection due to the form of the likelihood, constraints, and econometric interpretation. In general, beyond figure 6, we witness a slight but consistent improved convergence of the lower bound towards its maximum the train set for EMGVB with respect to MGVB, the convergence of the LB at a similar level on the test sets and MSE that eventually converge to rather similar values but that in some cases can be quite different at early iterations (which is expected but irrelevant in applications as at θ⋆ the measures are rather analogous. These observations are quantitatively supported by the results in 8, where all the optimizers lead to rather similar estimates and statistics.\n\nA visual inspection of the marginal densities as e.g. in Figures 7 and 5 reveals that in general both EMGVB and MGVB perform quite well compared to MCMC sampling and that the variational Gaussian assumption is quite feasible for all the volatility models. Note that the skew observed e.g. in Figure 7 for the ω parameters and the non-standard form of e.g. ψ for the FIGARCH models is due to the parameter transformation: VI is applied on the unconstrained parameters (ψω, ψφ) and such variational Gaussians are back-transformed on the original constrained parameter space where the distributions are generally no longer Gaussian (Figure 5 opposed to Figure 5).\n\nFigure 5: FIGARCH(1,1,1) model. Variational and MCMC marginals for the unconstrained parameters, as a complement to Figure 5 in the main text.\n\n20\n\n-4-2000.511.522.5-2002000.050.10.150.20.25-50500.20.40.60.81-20200.511.52EMGVBMCMCMGVBMLFigure 6: GJR(1,1,1) model. lower bound and mean squared error in the train and test set. Middle and bottom rows.\n\nFigure 7: GJR(1,1,1) model. MCMC and variational marginals in for the unconstrained parameters (ψω, ψα, ψγ, ψβ) (top row) and constrained parameters (ω, α, γ, β) (bottom row).\n\n21\n\n05001000-1991-1990.5-1990-1989.5-1989EMGVB TrainMGVB Train05001000-604.7-604.65-604.6-604.55-604.5EMGVB TestMGVB Test0500100027.127.1527.227.2527.34.464.474.484.494.5 EMGVB MGVB-4-3-20123-4-2000.511.52-3-2-100123-5051000.20.40.60.8100.050.1020406000.10.20510152000.20.40.602468100.60.8105101520EMGVBMGVBMCMCMLMSE\n\nL(θ⋆)\n\nω\n\nα\n\nγ\n\nβ1\n\nβ2\n\nL(θ⋆)\n\nEMGVB MGVB QBVI MCMC ML\n\nEMGVB MGVB QBVI MCMC ML\n\nEMGVB MGVB QBVI MCMC ML\n\nEMGVB MGVB QBVI MCMC ML\n\nEMGVB MGVB QBVI MCMC ML\n\nEMGVB MGVB QBVI MCMC ML\n\nEMGVB MGVB QBVI MCMC ML\n\nEMGVB MGVB QBVI MCMC ML\n\nEMGVB MGVB QBVI MCMC ML\n\nEMGVB MGVB QBVI EMGVB† MGVB† QBVI† MCMC ML\n\n0.519 0.519 0.519 0.519 0.519\n\n0.043 0.043 0.043 0.042 0.043\n\n0.044 0.044 0.044 0.042 0.043\n\n0.045 0.045 0.045 0.044 0.044\n\n-0.003 -0.003 -0.003 -0.003 -0.003\n\n-0.015 -0.015 -0.015 -0.015 -0.015\n\n-0.015 -0.016 -0.016 -0.015 -0.015\n\nω 0.100 0.100 0.100 0.102 0.100\n\n0.100 0.100 0.100 0.100 0.099\n\nβ0 1.078 1.063 1.078 1.085 1.067 1.084 1.067 1.079\n\n0.640 0.640 0.640 0.628 0.640\n\n0.230 0.230 0.230 0.226 0.231\n\n0.108 0.108 0.108 0.108 0.109\n\n0.116 0.115 0.116 0.110 0.114\n\n0.414 0.416 0.416 0.406 0.415\n\n0.350 0.364 0.364 0.340 0.350\n\n0.355 0.395 0.395 0.335 0.351\n\nφ\n\n0.059 0.059 0.059 0.062 0.060\n\nβ1 0.488 0.488 0.488 0.491 0.493 0.490 0.489 0.488\n\n0.737 0.737 0.737 0.738 0.738\n\n0.721 0.721 0.721 0.722 0.723\n\n0.655 0.653 0.653 0.672 0.670\n\n0.929 0.929 0.929 0.932 0.929\n\n0.930 0.926 0.926 0.932 0.929\n\n0.917 0.859 0.855 0.955 0.925\n\nβ1 0.418 0.418 0.419 0.432 0.424\n\n0.481 0.481 0.480 0.480 0.483\n\nβ3 -0.012 -0.011 -0.012 -0.016 -0.016 -0.015 -0.011 -0.012\n\n0.292 0.293 0.293 0.289 0.294\n\n0.323 0.322 0.323 0.301 0.321\n\n-0.171 -0.174 -0.174 -0.169 -0.172\n\n-0.173 -0.185 -0.186 -0.166 -0.171\n\nd 0.648 0.647 0.649 0.657 0.658\n\n0.663 0.663 0.663 0.656 0.669\n\nβ2 0.420 0.421 0.420 0.421 0.420 0.421 0.420 0.420\n\nARCH\n\n-2247.14 -2247.14 -2247.14\n\nGARCH(1,0,1)\n\n-2012.40 -2012.40 -2012.41\n\nGJR(1,1,1)\n\n-1988.71 -1988.70 -1988.70\n\nGJR(1,1,2)\n\n-1988.39 -1988.38 -1988.39\n\n0.049 0.050 0.050 0.041 0.036\n\nEGARCH(1,0,1) -2033.15 -2033.16 -2033.16\n\nEGARCH(1,1,1) -1995.54 -1995.69 -1995.69\n\nEGARCH(1,1,2) -1998.67 0.011 -1999.55 0.061 0.065 -1999.55 -0.022 0.004\n\nFIGARCH(0,1,1)\n\n-2007.49 -2007.49 -2007.50\n\nFIGARCH(1,1,1) -2007.62 -2007.62 -2007.62\n\nHAR\n\n-5082.83 -5083.02 -5082.83 -5085.54 -5085.61 -5085.54\n\nTrain p(y|θ⋆)\n\n-2241.03 -2241.03 -2241.03 -2241.01 -2241.04\n\n-2002.56 -2002.56 -2002.56 -2002.52 -2002.67\n\n-1976.08 -1976.09 -1976.08 -1976.03 -1976.24\n\n-1976.12 -1976.10 -1976.11 -1976.01 -1976.12\n\n-2017.67 -2017.69 -2017.69 -2017.64 -2017.68\n\n-1974.97 -1975.21 -1975.21 -1974.92 -1974.98\n\n-1975.03 -1976.00 -1976.01 -1974.90 -1974.99\n\n-2000.06 -2000.06 -2000.06 -2000.02 -2000.13\n\n-1999.69 -1999.69 -1999.69 -1999.67 -1999.74\n\n-5060.65 -5060.65 -5060.65 -5060.68 -5060.70 -5060.67 -5060.64 -5060.63\n\n29.818 29.818 29.815 29.694 29.820\n\n25.691 25.693 25.691 25.683 25.723\n\n27.214 27.243 27.238 27.143 27.386\n\n27.816 27.767 27.797 27.267 27.778\n\n26.840 26.837 26.837 26.858 26.819\n\n26.193 26.231 26.234 26.165 26.208\n\n26.235 26.572 26.589 26.113 26.214\n\n25.673 25.673 25.675 25.722 25.670\n\n25.773 25.773 25.771 25.784 25.767\n\n24.179 24.179 24.179 24.179 24.180 24.179 24.179 24.179\n\nTest p(y|θ⋆)\n\n-645.75 -645.75 -645.74 -645.66 -645.73\n\n-598.68 -598.67 -598.68 -598.92 -598.43\n\n-592.91 -592.90 -592.93 -593.29 -592.58\n\n-593.68 -593.65 -593.66 -593.38 -593.63\n\n-599.43 -599.44 -599.44 -599.37 -599.44\n\n-589.08 -589.58 -589.57 -588.65 -589.09\n\n-589.17 -590.33 -590.30 -588.63 -589.07\n\n-598.35 -598.35 -598.36 -598.13 -598.66\n\n-598.44 -598.44 -598.44 -598.18 -598.65\n\nMSE\n\n5.447 5.447 5.445 5.391 5.447\n\n3.953 3.954 3.953 3.944 3.957\n\n4.492 4.497 4.497 4.476 4.515\n\n4.638 4.628 4.635 4.515 4.629\n\n3.931 3.933 3.933 3.923 3.932\n\n4.128 4.171 4.170 4.094 4.129\n\n4.143 4.291 4.293 4.078 4.128\n\n3.978 3.978 3.978 3.971 3.989\n\n3.988 3.988 3.988 3.979 3.996\n\n-651.11 -651.11 -651.11\n\n-607.70 -607.69 -607.70\n\n-604.56 -604.56 -604.59\n\n-604.33 -604.32 -604.35\n\n-613.73 -613.68 -613.67\n\n-608.06 -608.10 -608.09\n\n-610.85 -611.10 -611.08\n\n-604.27 -604.27 -604.26\n\n-604.74 -604.72 -604.73\n\n-1240.01 -1240.30 -1240.00 -1242.81 -1242.97 -1242.81\n\n-1220.23 -1220.26 -1220.23 -1220.40 -1220.42 -1220.40 -1220.04 -1220.03\n\n18.857 18.860 18.857 18.867 18.869 18.866 18.855 18.857\n\nTable 8: Parameter estimates (on the actual constrained parameter space) and statistics on models’ performance on the train and test set. † denotes the use of the h-function gradient estimator.\n\n22\n\nC.3\n\nISTANBUL DATASET: BLOCK-DIAGONAL COVARIANCE\n\nIn this section, we apply EMGVB under different assumptions for the structure of the variational covariance matrix. We use the Istanbul stock exchange dataset of Akbilgic et al. (2014), (details are provided in C.4 and Table 11). To demonstrate the feasibility of the block-diagonal estimation under the mean-field framework outlined in Appendix A.7, we model the Istanbul stock exchange national 100 index (ISE):\n\nISEt = β0 + β1SPt + β2NIKt + β3BOVt + β4DAXt + β5FTSEt + β6EUt + β7EMt + εt with εt ∼ N (cid:0)0, σ2(cid:1) and the covariates respectively correspond to the S&P 500 index, Japanese Nikkei index, Brazilian Bovespa index, German DAX index, UK FTSE index, MSCI European index, and MSCI emerging market index. We estimate the coefficients β0, . . . , β7 and the transformed parameter ψσ = log(σ), from which σ (standard error of the disturbances) is computed as σ = exp(ψσ) + ˆVar(ψσ)/2, with ˆVar(ψσ) read from the variation posterior covariance matrix, while for ML regression corresponds to the residuals’ root mean squared error.\n\nWe consider the following structures for the variational posterior: (i) full covariance matrix (Full), (ii) diagonal covariance matrix (Diagonal), (iii) block-diagonal structure with two blocks of sizes 8 × 8 and 1 × 1 (Block 1) and, (iv) block diagonal structure with blocks of sizes 1 × 1, 3 × 3, 2 × 2, 2 × 2 and 1 × 1 (Block 2). Case (iii), models the covariance between the actual regressors but ignores their covariance with the regression standard error. Case (iv) groups in the 3 × 3 block the indices traded in non-European stock exchanges, and in the remaining 2 × 2 blocks the indices referring to European exchanges and the two MSCI indexes. Furthermore, covariances between the intercept and regression’s standard error with all the other variables are set to zero. The purpose of this application is that of providing an example for Algorithm 2 and the discussion in Appendix A.7, rather than providing an effective forecasting model supported by a solid econometric rationale. Yet, structures (iii) and (iv) correspond to a quite intuitive grouping of the variables involved in our regression problem, motivating the choice of the dataset.\n\nTables 9 and 10 summarize the estimation results. Table 9 shows that the impact of the different structures of the covariance matrix is somewhat marginal in terms of the performance measures, with respect to each other and with respect to the ML estimates. As for the logistic regression example, we observe in the most constrained cases (ii) and (iii) certain the estimates of certain posterior means slightly deviate from the other cases indicating that the algorithm perhaps terminates at a different local maximum. Regarding variational covariances reported in Table 10, there is remarkable accordance between the covariance structures (i), (iii) and ML, while for the diagonal structure (ii) and block-diagonal structure (iv) the covariances are misaligned with the ML and full-diagonal ones, further suggesting that the algorithms convergence at different maxima of the lower bound.\n\nFrom a theoretical perspective, if Σ is the covariance matrix of the joint distribution of the eight variates (case (i)), by the properties of the Gaussian distribution, blocks e.g. in case (iv) and diagonal entries should match the corresponding elements in Σ. It is however not surprising to observe that the elements in the sub-matrices e.g. in cases (ii) and (iv) deviate from those Σ. Indeed, the results refer to independent optimizations of alternative models (over the same dataset) that are not granted to converge at the same maximum (and thus distribution). Across the covariance structures (i) to (iv) the optimal variational parameters correspond to different multivariate distributions, that independently maximize the lower bound, and that are not constrained to be related to each other. This is indeed confirmed by the differences in the maximized Lower bound ̄Lθ⋆ in Table 9 and in the different levels at which the curves in Figure 8 are observed to converge. Thus the blocks in the covariance matrix under case (iv) do not match the entries in Σ. In this light, the ML estimates’ variances in the third panel of Table 10 can be compared to those of case (i), but are misleading for the other cases, as the covariance matrix of the asymptotic (Gaussian) distribution of the ML estimator is implicitly full.\n\n23\n\nFull Diagonal Block 1 Block 2 ML\n\nβ0\n\n0.001 0.001 0.001 0.001 0.001\n\nβ1\n\n0.098 0.054 0.098 0.066 0.099\n\nβ2\n\nβ3\n\nβ4\n\nβ5\n\n0.079 −0.272 −0.168 −0.354 0.107 −0.211 0.135 −0.008 0.079 −0.272 −0.167 −0.353 0.127 −0.205 0.225 0.078 −0.273 −0.174 −0.363\n\n0.244\n\nβ6\n\n1.165 0.519 1.164 0.188 1.179\n\nβ7\n\n0.943 0.888 0.944 0.844 0.946\n\nσ\n\n0.014 0.014 0.014 0.014 0.014\n\nL(θ⋆)\n\n1186.079 1177.810 1186.082 1173.852\n\nFull Diagonal Block 1 Block 2 ML\n\nTrain p(y|θ⋆)\n\n1223.699 1219.055 1223.701 1213.509 1223.728\n\nTest\n\nMSE\n\n0.194 0.198 0.194 0.204 0.194\n\np(y|θ⋆)\n\n316.849 317.042 316.848 316.696 316.871\n\nMSE\n\n0.041 0.041 0.041 0.041 0.042\n\nTable 9: Posterior means, ML estimates and performance measures on the train and test set. σ refers to the disturbances’ standard deviation. Block 1 corresponds to case (iii) and Block 2 to case (iv).\n\nβ3\n\nβ4\n\nβ1\n\nβ0 0.001 β1 β2 0.003 β3 −0.001 β4 −0.002 β5 −0.002 β6 0.007 β7 −0.009\n\nβ2\n\n0.001\n\n-0.032 -2.992 -1.906 -0.729 -0.170 1.605\n\n0.003 -0.046\n\n1.094 0.297 0.679 -0.393 -4.148\n\nβ1\n\nβ2\n\n0.001\n\n0.142 -2.981\n\nβ3\n\n0.003 0.015\n\n-0.364\n\nβ0 β1 β2 β3 β4 β5 β6 β7\n\n-0.001 -2.970 1.094\n\n0.961 0.622 -1.065 -4.700\n\nβ4\n\n-0.001 -2.953 1.071\n\nFull\n\nβ5\n\n-0.002 -1.858 0.310 0.897\n\n3.836 -20.477 -0.563\n\nBlock 1\n\nβ5\n\n-0.002 -1.905 0.322 0.953\n\n-8.490\n\nβ6\n\n-0.001 -0.789 0.691 0.596 3.687\n\n-28.813 -1.386\n\nβ6\n\n-0.002 -0.743 0.652 0.615 3.858\n\nσ\n\n0.000 0.009 0.010 -0.017 -0.032 0.032 -0.004 0.017\n\nσ\n\nβ7\n\n0.006 -0.186 -0.409 -0.949 -20.246 -28.377\n\n-3.324\n\nβ7\n\n0.006 -0.181 -0.420 -1.092 -20.560 -28.409\n\n-4.325\n\nβ8\n\n-0.009 1.603 -4.137 -4.693 -0.538 -1.364 -3.426\n\nβ8\n\n-0.009 1.547 -4.174 -4.669 -0.632 -1.408 -3.195\n\nL M\n\n2\n\nk c\no\n\nl\n\nB\n\nFull Diagonal Block 1 Block 2 ML\n\nβ1\n\n0.000 0.001 0.000 0.001 0.000\n\nβ2\n\n5.610 1.910 5.599 4.434 5.609\n\nβ3\n\n3.174 1.779 3.199 2.009 3.171\n\nβ4\n\n5.324 1.860 5.361 3.869 5.361\n\nβ5\n\n16.804 2.152 16.996 8.517 16.893\n\nβ6\n\n27.424 2.662 27.320 11.167 27.676\n\nβ7\n\n52.868 2.257 53.149 4.957 53.514\n\nβ8\n\n15.323 3.406 15.278 7.798 15.283\n\nσ\n\n1.170 1.211 1.184 1.170\n\nTable 10: Covariance matrices. Top table: covariance matrix of the estimated coefficients for the full variational posterior and covariances of the ML estimate. Second and third table: block covariance matrices where Block 1 corresponds to case (iii) and Block 2 to case (iv). Bottom table: variances of the full and diagonal posteriors along with ML variances. All the entries across the tables are multiplied by 104.\n\n24\n\nFigure 8: Lower bound optimization for the Istanbul data under different variational covariance structures.\n\nC.4 DATASETS AND HYPERPARAMETERS\n\nDataset\n\nModel\n\nNumber of Parameters\n\nNumber of samples\n\nLabour Volatility ARCH\n\nLogistic regression\n\nGARCH(1,0,1) GJR(1,1,1) GJR(1,1,2) EGARCH(1,0,1) EGARCH(1,1,1) EGARCH(1,1,2) FIGARCH(0,1,1) FIGARCH(1,1,2) HAR (Linear regr.) Linear regression\n\nIstanbul\n\n8 2\n3 4\n5 3\n4 5\n3 4\n4+1 8+1\n\n753 2112 2112 2112 2112 2112 2112 2112 2112 2112 2102 536\n\nSamples in train set\n\n564 (75%) 1689 (80%) 1689 (80%) 1689 (80%) 1689 (80%) 1689 (80%) 1689 (80%) 1689 (80%) 1689 (80%) 1689 (80%) 1681 (80%) 428 (80%)\n\nSamples in test set\n\n189 (25%) 423 (20%) 423 (20%) 423 (20%) 423 (20%) 423 (20%) 423 (20%) 423 (20%) 423 (20%) 423 (20%) 421 (20%) 108 (20%)\n\nPeriod\n\n3-Jan-2014 / 09-Jun-2022\n\n4-Feb-2014 / 28-Jun-2022 5-Jan-2009 / 22-Feb-2022\n\nEMGVB optimizer\n\nInitial values\n\nPrior\n\nExperiment\n\nβ\n\nGrad clip Grad clip init.\n\nLabour data ARCH-GARCH-GJR EGARCH FIGARCH HAR Istanbul data\n\n0.01 0.01 0.01 0.01 0.001 0.07\n\n3000 1000 1000 1000 50000 50000\n\n1000 1000 1000 1000 500 500\n\nω\n\n0.4 0.4 0.4 0.4 0.4 0.4\n\nw\n\n30 30 30 30 30 30\n\ntmax\n\n1200 1200 3000 1200 1200 1200\n\nt′\n\n1000 1000 2500 1000 1000 1000\n\nP\n\n500 500 500 500 500 500\n\nS\n\nμ1\n\n75 ∼ N (0, Σ1) 150 150 150 100 100 ∼ N (0, Σ1)\n\nML ML ML ML\n\nΣ1\n\n0.05 0.05 0.05 0.05 0.01 0.01\n\nμ0 Σ0\n\n0 0\n0 0\n0 0\n\n5 5\n5 5\n5 5\n\nTable 11: Details on the datasets and corresponding models, as well as the hyperparameters used in the experiments.\n\nTable 11 summarized some information about the datasets and the setup used across the experiments. For the experiments on the Labour and Volatility datasets, the same set of hyperparameters applies to EMGVB, MGVB (and QBVI). While the Labour3 and Istanbul4 datasets are readily available, the volatility dataset is extracted from the Oxford-Man Institute realized volatility library. 5 We use daily close-to-close demeaned returns for the GARCH-family models and 5-minute sub-sampled daily measures of realized volatilities (further annualized) for the HAR model.\n\n3Publicly available at key2stats.com/data-set/view/140. See (Mroz, 1984) for details. The\n\ndata is also adopted in VI applications e.g. by (Tran et al., 2021b; Magris et al., 2022).\n\n4Publicly available at the UCI repository, archive.ics.uci.edu/ml/datasets/istanbul+\n\nstock+exchange. See (Akbilgic et al., 2014) for details.\n\n5Publicly available at realized.oxford-man.ox.ac.uk.\n\n25\n\n02004006008001000120060080010001200D PROOFS\n\nD.1 PROOF OF PROPOSITION 1\n\nPreliminaries: Noticing that for exponential-family distributions Iζ = ∇λm and using the chain rule, ∇λL(λ) = ∇λm∇mL(m) = Iζ∇mL(λ), implying that the natural gradient\n\n ̃∇λL(λ) = I −1\n\nζ ∇λL(λ) = ∇mL(m)\n\n(25)\n\ncan be easily computed as the euclidean gradient with respect to the expectation parameters, without requiring the inverse FIM (Khan & Lin, 2017; Khan et al., 2018a). Khan et al. (2018a) derive\n\n∇m1 L(m) = ∇μL(m) − 2[∇ΣL(m)]μ\n\nand\n\n∇m2 L(m) = ∇ΣL(m),\n\n(26)\n\nwhich allows expressing the euclidean gradients with the respect to the expectation parameters as euclidean gradients with respect to μ and Σ, thus providing an exact relationship between the natural gradients of the LB and its euclidean gradients with respect to the common (μ, Σ) parametrization for qλ. Note that the above (and the following) applies to Gaussian distributions only.\n\nProof: The first natural gradient in Proposition 1 is trivial as it follows from the definition of natural gradient and the Gaussian FIM in eq. 1.\n\nIf ξ ≡ ξ(λ) is a smooth reparametrization of the variational density,\n\nIξ = −Eqξ(θ)\n\n(cid:2)∇2\n\nξ log qξ(θ)(cid:3) = JIζJ ⊤\n\nwith J = ∇ξλ being the Jacobian matrix (Lehmann & Casella, 1998). If in addition ξ is an invertible function of λ, then J is itself invertible. Therefore for Σ−1 = −2λ2, the above implies that\n\nΣ−1 = (∇Σ−1 λ2)−1⊤ I −1\n\nI −1 λ2\n\n(∇Σ−1 λ2)−1,\n\nwith (∇Σ−1λ2)−1 = ∇λ2Σ−1. Thus for the natural gradient ̃∇Σ−1 L,\n\n ̃∇Σ−1 L = I −1\n\nΣ−1∇Σ−1L = I −1\n\nΣ−1\n\n(cid:19)\n\n(cid:18) ∂λ2 ∂Σ−1\n\n∂L ∂λ2\n\nΣ−1(∇Σ−1 λ2)∇λ2 L\n\n= I −1 = (∇Σ−1 λ2)−1⊤ I −1 λ2 = (cid:0)∇λ2 Σ−1(cid:1)⊤ ̃∇λ2 L = −2 ̃∇λ2L.\n\n(∇Σ−1 λ2)−1(∇Σ−1 λ2)∇λ2L\n\nFrom eqs. 25 and 26 ̃∇λ2 L = ∇m2L = ∇ΣL, so that\n\n ̃∇Σ−1L = −2 ̃∇λ2L = −2∇ΣL,\n\nwhich proves the proposition.\n\nD.2 GENERAL FORM OF THE EMGVB UPDATE\n\nFor a prior p ∼ N (μ0, Σ0) and a variational posterior q ∼ N (μ, Σ), by rewriting the LB as\n\nEqζ [hζ(θ)] = Eqζ [log p(θ) − log qζ(θ) + log p(y|θ)] = Eqζ [log p(θ) − log qζ(θ)]+Eqζ [log p(y|θ)],\n\nwe decompose ∇ζL as ∇ζEqζ [log p(θ) − log qζ(θ)] + ∇ζEqζ [log p(y|θ)]. As in 6, we apply the log-derivative trick on the last term and write ∇ζEq[log p(y|θ)] = Eqζ [∇ζ[log qζ(θ)] log p(y|θ)]. On the other hand, it is easy to show that up to a constant that does not depend on μ and Σ\n\nEqζ [log p(θ) − log qζ(θ)] = −\n\n−\n\n1 2\n1 2\n\nlog |Σ0| +\n\n1 2\n0 Σ(cid:1) −\n\ntr(cid:0)Σ−1\n\nlog |Σ| +\n\n1 2\n\nd\n\n1 2\n\n(μ − μ0)⊤Σ−1\n\n0 (μ − μ0),\n\n26\n\nso that\n\n∇ΣEqζ [log p(θ) − log qζ(θ)] =\n\n1 2\n\n∇μEqζ [log p(θ) − log qζ(θ)] = −Σ−1\n\nΣ−1 −\n\nΣ−1 0 ,\n\n1 2\n0 (μ − μ0).\n\nFor the natural gradients, we have\n\n ̃∇Σ−1Eqζ [log p(θ) − log qζ(θ)] = −2∇ΣEqζ [log p(θ) − log qζ(θ)] = −Σ−1 + Σ−1 0 ,\n\n ̃∇μEqζ [log p(θ) − log qζ(θ)] = Σ∇μEqζ [log p(θ) − log qζ(θ)] = −ΣΣ−1\n\n0 (μ − μ0),\n\nwhile the feasible naive estimators for ̃∇μEqζ [log p(y|θ)] and ̃∇Σ−1Eqζ [log p(y|θ)] turn analogous to the right-hand sides of eqs. 9, 10 with hζ replaced with log p(y|θ). This leads to the general form of the EMGVB update, based either on the h-function gradient estimator (generally applicable) or the above decomposition (applicable under a Gaussian prior):\n\n ̃∇μL(ζt) ≈ cμt +\n\n1 S\n\nS (cid:88)\n\ns=1\n\n[(θs − μt) log f (θs)]\n\n ̃∇Σ−1 L(ζt) ≈ CΣt +\n\n1 S\n\nS (cid:88)\n\ns=1\n\n(cid:104)(cid:16)\n\nΣ−1\n\nt − Σ−1\n\nt (θs − μt)(θs − μt)⊤Σ−1\n\nt\n\n(cid:17)\n\n(cid:105)\n\nlog f (θs)\n\nwhere\n\n\n\n \n\n \n\n\n\n \n\n\n\nt + Σ−1\n\nCΣt = −Σ−1 cμt = −ΣtΣ−1 log f (θs) = log p(y|θs)\n\n0 (μt − μ0)\n\n0\n\nCΣt = 0 cμt = 0 log f (θs) = hζt(θs)\n\nif p is Gaussian\n\nif p is Gaussian or not\n\n(27)\n\n(28)\n\n(29)\n\nD.3\n\nJUSTIFICATION FOR THE EMGVB UPDATE\n\nFor any positive definite matrix S the Riemann gradient ̄∇SL, for a differentiable function L, is S∇SLS (Hosseini & Sra, 2015). This is the form of the Riemann gradient obtained from the SPD (Symmetric and Positive Definite) matrix manifold, for which the following retraction and parallel transport equations for the SPD (matrix) manifold apply:\n\nRS(ξ) = S + ξ +\n\n1 2\n\nξS−1ξ, where ξ ∈ TSM,\n\n(30)\n\nTSt→St+1 (ξ) = EξE⊤, where E = (cid:0)St+1S−1\n\n(31) with ξ being the rescaled Riemann gradient β ̄∇SL = βS∇SLS obtained from the SPD manifold. β > 0 rescales the tangent vector and is arbitrary. From an algorithmic perspective, β is interpreted as a learning rate, driving the magnitude of the gradient component in the retraction. For the precision matrix Σ−1,\n\n2 , ξ ∈ TSM.\n\nt\n\n(cid:1) 1\n\n ̄∇Σ−1 L = Σ−1∇Σ−1LΣ−1.\n\n(32)\n\nOn the other hand, for the natural gradient\n\n ̃∇Σ−1L = −2∇ΣL = 2Σ−1∇Σ−1LΣ−1,\n\nwhere the first equality comes from Proposition 1 and the second is easy to prove with simple matrix algebra and is furthermore analogous to the form of 2. In this regard, more can be found in (Barfoot, 2020).\n\nThe natural gradient is obtained from the Gaussian manifold, so that applying the above SPD manifold retraction and parallel transport equations is technically incorrect. The concept of retraction is general and indeed eq. 4 denotes a generic retraction function R−1 Σ (·), whose specific form is\n\n27\n\nspecified in Section 4.2 and coincides with eq. 30. The use of eq. 30, which is specific for the SPD manifold, with the natural gradient obtained from the Gaussian manifold, appears incorrect. The gradient ̃∇Σ−1 L = −2∇ΣL is not a Riemannian gradient for the SPD manifold but a natural gradient obtained from the Gaussian manifold and equal to 2 ̄∇ΣL. The actual exponential map and retraction for updating Σ−1 based on ̃∇Σ−1 need to be separately worked out for the Gaussian manifold by solving a system of ordinary differential equations, whose curbstone solutions do not coincide with those obtained within the SPD manifold.\n\nIn this light, the use of 30 for Σ and the natural gradient in Tran et al. (2021a) is not well-justified (Lin et al., 2020). Indeed, their approach can be thought of as inexact in two ways. (i) The natural gradient should read as 2Σ∇SLS in place of Σ∇SLS (eq. 5.4 in (Tran et al., 2021a)): the form of retraction and parallel transport therein applied is consistent with the adopted form of the natural gradient (Σ∇SLS) which is indeed a Riemann gradient for the SPD manifold, however, the actual natural gradient is 2Σ∇SLS. (ii) Perhaps the form Σ∇SLS in place of 2Σ∇SLS is a typo, thus the application of the SPD-manifold form of the retraction and parallel transport is inexact as it is not applied to a Riemann gradient obtained from the SPD manifold but to the natural gradient obtained from the Gaussian manifold, as also discussed in (Lin et al., 2020).\n\nIn this view, our discussion in Section 4 is subject to the same inexact setting as the above case (ii) -a correct form for the natural gradient but mixing manifold structures-, under which the adoption of 30 and 31 is, as in Tran et al. (2021a) not justified (though working in practice).\n\nWe now show that the forms of retraction and parallel transport in Section 4.2 are justified and arise from the consistent use of the SPD manifold for updating 2Σ−1, from which the update for Σ−1 follows, and corresponds to the retraction from in Section 4.2. Later, we show that parallel transport in eq. 15 also applies. Consider of updating (cid:0)μ, 2Σ−1(cid:1) in place of (cid:0)μ, Σ−1(cid:1). According to eq. 32 the Riemann gradient of 2Σ−1 is for the SPD manifold:\n\n ̄∇2Σ−1 L = (cid:0)2Σ−1(cid:1)∇2Σ−1L(cid:0)2Σ−1(cid:1) = (cid:0)2Σ−1(cid:1) 1\n\n2\n\n∂L ∂Σ−1\n\n(cid:0)2Σ−1(cid:1) = 2Σ−1∇Σ−1 L(cid:0)2Σ−1(cid:1)\n\n= ̃∇Σ−1L.\n\nThus 2Σ−1∇Σ−1L(cid:0)2Σ−1(cid:1) = ̃∇Σ−1 L is the Riemann gradient w.r.t. 2Σ−1 obtained from the SPD manifold. 2Σ−1 can be updated by the retraction in eq. 30 with the Riemann gradient ̄∇2Σ−1 L. The update is now legit and justified, as it consistently adopts the SPD manifold Riemann gradient and the SPD manifold form of retraction:\n\n2Σ−1 ← R2Σ−1\n\n(cid:0)β ̄∇2Σ−1\n\n(cid:1) = R2Σ−1\n\nthat is,\n\n(cid:16)\n\nβ ̃∇Σ−1L\n\n(cid:17)\n\n,\n\n2Σ−1\n\nt+1 = 2Σ−1\n\nt + β\n\n(cid:104) ̃∇Σ−1L\n\n(cid:105)\n\n+\n\n1 2\n\nβ2(cid:104) ̃∇Σ−1L\n\n(cid:105) 1 2\n\n(cid:104) ̃∇Σ−1 L\n\n(cid:105)\n\n.\n\nΣ\n\n(33)\n\nAs β > 0 is arbitrary we can rewrite the above in terms of an arbitrary β′ = 2β (i.e. simple reparametrization of the hyperparameter),\n\n2Σ−1\n\nt+1 = 2Σ−1\n\nt + β′(cid:104) ̃∇Σ−1L\n\n(cid:105)\n\n+\n\n1 2\n\nβ′2(cid:104) ̃∇Σ−1L\n\n(cid:105) 1 2\n\n(cid:104) ̃∇Σ−1 L\n\n(cid:105)\n\n.\n\nΣ\n\nThe update for Σ−1 follows,\n\nΣ−1\n\nt+1 = Σ−1\n\nt +\n\n= Σ−1\n\nt +\n\n(cid:104) ̃∇Σ−1 L\n\n(cid:105)\n\n+\n\n(cid:104) ̃∇Σ−1 L\n\n(cid:105)\n\n+\n\nβ′ 2\nβ′ 2\n\n1 2\n1 2\n\nβ′2 4\n(cid:18) β′ 2\n\n(cid:105)\n\nΣ\n\n(cid:104) ̃∇Σ−1 L (cid:19)2(cid:104) ̃∇Σ−1L\n\n(cid:104) ̃∇Σ−1 L\n\n(cid:105)\n\n(cid:105)\n\nΣ\n\n(cid:104) ̃∇Σ−1L\n\n(cid:105)\n\n(cid:104) ̃∇Σ−1 L\n\n(cid:105)\n\n+\n\nβ2(cid:104) ̃∇Σ−1 L\n\n(cid:105)\n\n(cid:104) ̃∇Σ−1 L\n\n(cid:105)\n\nΣ\n\n1 2\n\n= Σ−1\n\nt + β (cid:16)\n\n= RΣ−1\n\nβ ̃∇Σ−1 L\n\n(cid:17)\n\n= RΣ−1 (−2β∇ΣL)\n\n28\n\n(34)\n\n(35)\n\nwhere β′ = β/2. From the above, the main equalities are (cid:17)\n\n(cid:16)\n\nR2Σ−1\n\n(cid:0)β∇R\n\n2Σ−1\n\n(cid:1) = RΣ−1\n\nβ ̃∇Σ−1L\n\n= RΣ−1 (−2β∇ΣL),\n\nwhose interpretation is as follows. The retraction on 2Σ−1 with the Riemann gradient 2β ̄∇2Σ−1 leads to an update for Σ−1 (eq. 35) which is the same update obtained with the retraction in 30 on Σ−1 with β ̃∇Σ−1L. This last gradient is analogous to −2β∇ΣL (Proposition 1) and the corresponding retraction for is that presented in Section 4.2.\n\nBlindly updating Σ−1 with RΣ−1(−2β′∇ΣL) is by itself inexact as it involves the natural gradient from the Gaussian manifold, however, this is equivalent to the update for Σ−1 that one obtains by (cid:1). From the in updating 2Σ−1 with the consistent retraction for the SPD manifold R2Σ−1 above we also have that, logically, the arbitrary stepsize β for updating Σ−1 is half of that used for updating 2Σ−1, 2β.\n\n(cid:0)β ̄∇2Σ−1\n\nA similar argument holds for vector transport. Consider the vector transport for 2Σ−1,\n\nT2Σ−1\n\nt →2Σ−1\n\nt+1\n\n(cid:0) ̄∇2Σ−1L(cid:1) = E(cid:0)2β ̄∇2Σ−1 L(cid:1)E⊤\n\nwith E =\n\n(cid:104)(cid:0)2Σ−1\n\nt+1\n\n(cid:1)(cid:0)2Σ−1\n\nt\n\n(cid:1)−1(cid:105) 1\n\n2\n\n= (cid:2)Σ−1\n\nt+1Σt\n\n(cid:3) 1\n\n2 . The vector transport for Σ−1 is then\n\nTΣ−1\n\nt →Σ−1\n\nt+1\n\n(cid:0) ̄∇Σ−1L(cid:1) =\n\n1 2\n\nT2Σ−1\n\nt →2Σ−1\n\nt+1\n\n(cid:0) ̄∇2Σ−1L(cid:1) = E(cid:0)β ̄∇2Σ−1L(cid:1)E⊤ (cid:16)\n\n(cid:17)\n\n= E\n\nβ ̃∇Σ−1 L\n\nE⊤.\n\n(36)\n\nNow note that,\n\n(cid:16)\n\nE\n\n(cid:17)\n\nβ ̃∇Σ−1 L\n\nE⊤ = TΣ−1\n\nt →Σ−1\n\nt+1\n\n(cid:16)\n\nβ ̃∇Σ−1L\n\n(cid:17)\n\n= TΣ−1\n\nt →Σ−1\n\nt+1\n\n(−2β∇SL),\n\n(37)\n\nas in Section 4.2. The vector transport in the form of eq. 31 is consistently applied to 2Σ−1 based on the Riemann SPD manifold gradient ̄∇2Σ−1 L, from which the vector transport for Σ−1 follows (eq. 36), which equals to the vector transport in eq. 31 applied to Σ−1 based on the rescaled natural gradient β∇SL = −2β∇SL, eq. 37.\n\nD.4 PROOF OF THEOREM 1\n\nFor a Gaussian distribution distribution q with parameter ζ = (μ, vec(Σ)), be ∇ζL(ζt) = (∇μL(ζt), vec(∇ΣL(ζt))) where ∇xL(ζt) is the derivative of L(ζ) with respect to x evaluated at ζ = ζt. Furthermore, adopt the following short-hand notation qζ := q(μ, Σ) and qζt := q(μt, Σt).\n\nBy using the well-known form of the KL divergence between two multivariate Gaussian distributions, the optimization problem can be written as:\n\n⟨ζ, ∇ζL(ζt)⟩ −\n\n1 β\n\nDKL(qζ||qζt) = μ⊤∇μL(ζt) + vec(Σ)⊤vec(∇ΣL(ζt))\n\n(cid:20)\n\nlog\n\n−\n\n1 2β\n\n|Σt| |Σ|\n\n− d + tr(cid:0)Σ−1\n\nt Σ(cid:1)⊤\n\n+ (μ − μt)Σ−1\n\nt (μ − μt)\n\n= μ⊤∇μL(ζt) + tr(Σ∇ΣL(ζt))\n\n(cid:20)\n\nlog\n\n−\n\n1 2β\n\n|Σt| |Σ|\n\n− d + tr(cid:0)Σ−1\n\nt Σ(cid:1)⊤\n\n+ (μ − μt)Σ−1\n\nt (μ − μt)\n\n(cid:21)\n\n(cid:21) .\n\nNote that ∇μL(ζt) and ∇ΣL(ζt) are now constants and that the Hessian of the above equation amounts to the Hessian of the KL divergence. Furthermore the FIM of qζ equals the Hessian of the function ζt (cid:55)→ DKL(qζ||qζt) evaluated at ζt = ζ, that is of the above KL divergence:\n\nIζ = ∇2 ζt\n\nDKL(qζ||qζt)(cid:12)\n\n(cid:12)ζt=ζ\n\n.\n\nThe multivariate Gaussian distribution is a full-rank exponential family for which the negative Hessian of the log-likelihood (FIM) is the covariance matrix of the sufficient statistics. The FIM is a\n\n29\n\nconvex combination of such positive semi-definite matrices, so it is positive-definite. Thus the above expression is convex with respect to ζt.\n\nThe objective is optimized by setting its derivatives with respect to ζ to zero:\n\n∇μL(ζt) −\n\nt (μ − μt) = 0,\n\n∇ΣL(ζt) −\n\nt − Σ−1(cid:3) = 0,\n\nΣ−1\n\n1 β\n(cid:2)Σ−1\n\n1 2β\n\nFrom which it follows that the optimal updates read\n\nμ = μt + βΣt∇μL(ζt),\n\nΣ−1 = Σ−1\n\nt − 2β[∇ΣL(ζt)]\n\nThe update for μ is analogous to the EMGVB update, thus optimal in terms of Theorem 1. On the other hand, we have that the optimal update for Σ−1 is the above one, which differs from the EMGVB update. Indeed EMGVB accounts for positive-definiteness constraint on Σ−1, not in the hypotheses of the Theorem. Note however that the EMGVB update for Σ−1 corresponds to the above one when the second-order term in the retraction (which indeed accounts for the positive definiteness of Σ−1) is ignored.\n\n30",
    "reference": "# Summary Of The Paper\n\nThe authors proposed a new variational inference algorithm for settings where the parameters of the approximate posterior form a Riemannian manifold. Unlike previous methods that relied on various forms approximations, the proposed method is based on an exact formula for the gradient.\n\n# Strength And Weaknesses\n\nSTRENGTHS:\n* The paper provides a novel estimator for the gradient in a broad class of models fit with variational inference.\n* The derivations and the proposed algorithms are novel, technically sound, and non-trivial. The level of technical depth required to derive the estimator is significant.\n\nWEAKNESSES:\n* The experiments do not seem to convey a strong sense of the improvements offered by the new methods. The performance improvements seem marginal at first glance.\n* The writing quality and the overall structure of the paper is acceptable, but not quite at the level of polish and organization seen in a typical ICLR accepted paper.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nMy main concern is about the experimental validation. On most metrics, improvements seem to be on the order of a percentage. I'm not convinced that the new method offers a strong improvement against alternatives, though I may have overlooked or misunderstood some parts of the experimentations. In particular, there are no error bars, and the benefits are so tiny, that they could even be statistically insignificant.\n\nAdditiionally, I don't see a deep discussion of the computational cost of the new method. Since it is described as being \"exact\", I would imagine that each step might be computationally more expensive. In that case, does the proposed method strike an interesting tradeoff of computation vs. accuracy, or does it yield small improvements in performance at a significant computational cost. I can't tell from looking at the experiments. I would also say that the scope of the experiments seems limited, but it might be possible to address that by bringing in some material from the appendix.\n\n# Summary Of The Review\n\nI think this paper is currently below the threshold of acceptance, but could be over the threshold after an improved presentation and a more thorough exploration of experimental results.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nLEARNING LANGUAGE REPRESENTATIONS WITH LOGICAL INDUCTIVE BIAS\n\nJianshu Chen Tencent AI Lab, Bellevue, WA 98004, USA jianshuchen@global.tencent.com\n\nABSTRACT\n\nTransformer architectures have achieved great success in solving natural language tasks, which learn strong language representations from large-scale unlabeled texts. In this paper, we seek to go further beyond and explore a new logical inductive bias for better language representation learning. Logic reasoning is known as a formal methodology to reach answers from given knowledge and facts. Inspired by such a view, we develop a novel neural architecture named FOLNet (First-Order Logic Network), to encode this new inductive bias. We construct a set of neural logic operators as learnable Horn clauses, which are further forward-chained into a fully differentiable neural architecture (FOLNet). Interestingly, we find that the self-attention module in transformers can be composed by two of our neural logic operators, which probably explains their strong reasoning performance. Our proposed FOLNet has the same input and output interfaces as other pretrained models and thus could be pretrained/finetuned by using similar losses. It also allows FOLNet to be used in a plug-and-play manner when replacing other pretrained models. With our logical inductive bias, the same set of “logic deduction skills” learned through pretraining are expected to be equally capable of solving diverse downstream tasks. For this reason, FOLNet learns language representations that have much stronger transfer capabilities. Experimental results on several language understanding tasks show that our pretrained FOLNet model outperforms the existing strong transformer-based approaches.1\n\n1\n\nINTRODUCTION\n\nPretrained transformer models have achieved great success in solving natural language tasks, which learn strong language representations from large-scale unlabeled texts. The learned representations can be easily transferred to different downstream tasks by finetuning over limited amount of labeled data (Radford et al., 2018; Devlin et al., 2018; Lan et al., 2019; Liu et al., 2019; Yang et al., 2019). They even exhibit strong zero-shot or few-shot generalization capability without finetuning when further scaling up the model size (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2022). Besides large-scale models and training data, one important reason for the success is the strong relational inductive bias encoded in the transformer architecture (Vaswani et al., 2017); it effectively models the pairwise relations between tokens and use it to compute the language representations.\n\nIn this paper, we seek to go beyond the inductive bias in transformer models and explore a new logical inductive bias for better language representation learning. The main idea is to view the computation of language representations as a logic reasoning process; that is, the language representations are deduced via logic reasoning step-by-step from the original discrete token sequences. Specifically, we treat the tokens in the input sequence as the terms in logic programming, and treat their properties and relations as the predicates of different arities. Then, the final language representations are derived as the advanced properties and relations from the basic input properties and relations (e.g., token ids and relative distances). Most importantly, we require the construction of such deduction process to follow the principles of first-order logic, in order to encode such logical inductive bias.\n\nFollowing the above logical inductive bias, we derive a principled neural architecture, named FOLNet (First-Order Logic Network), for learning language representations. Specifically, we construct a set\n\n1The code along with the pretrained model checkpoints will be released publicly.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nof neural logic operators as learnable Horn clauses, which are further forward-chained into a fully differentiable neural architecture. In particular, the FOLNet architecture consists of two interacting branches responsible for unary and binary relational reasoning, respectively. Interestingly, we find that the self-attention mechanism can be constructed by two of our developed neural logic operators, and the entire transformer architecture can be understood as a single-branch version of FOLNet. This newly discovered connection might partially explain the surprisingly strong reasoning performance of the transformer architecture (Wei et al., 2022; Lewkowycz et al., 2022). As we will demonstrate in our experiments, such dual-branch architecture has several significant advantages that are essential for learning better language representations. Furthermore, we also establish a new unified understanding of different positional encoding strategies with our logical inductive bias. For instance, we find that the existing popular relative positional encoding can be constructed by the degenerated version of our two neural logic operators. More importantly, it also allows us to develop a new principled relative positional encoding that is simple yet quite effective in practice. Notably, our proposed FOLNet has the same input and output interfaces as other pretrained transformer models (e.g., BERT) and thus could be trained by using similar losses. It also allows FOLNet to be used in a plug-and-play manner when replacing other pretrained models in solving downstream tasks. Our logical inductive bias assumes that the “logic deduction skills” are shared across all natural language tasks; that is, these skills learned during pretraining should be equally applicable to solving diverse downstream tasks. For this reason, FOLNet learns language representations that have much stronger transfer generalization capabilities. Experimental results on several language understanding tasks (GLUE, SQuAD 2.0 and FOLIO) show that our FOLNet model outperforms the transformer architecture by a large-margin when they are pretrained using similar losses. The results clearly show that advantage of using the logical inductive bias for learning language representations.\n\n2 LOGICAL INDUCTIVE BIAS FOR LANGUAGE REPRESENTATIONS\n\nNatural language text can be viewed as a sequence of discrete symbols, and language representations learning considers the problem of mapping the discrete symbols into certain more computable forms. One widely used approach is distributed representation, which maps the discrete token ids into dense vectors (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). Many different functional forms, such as LSTM (Hochreiter & Schmidhuber, 1997), and more recently, transformer models (Vaswani et al., 2017), have been used to implement such mappings. They generally encode different kinds of inductive bias for modeling natural languages. For example, RNNs use the same set of model parameters to update the hidden states over time, which encodes translation-invariance over time (Battaglia et al., 2018). These forms of inductive bias continuously push the state-of-the-arts in solving natural language tasks. In this section, we introduce a new form of inductive bias, named logical inductive bias, which will work together with distributed representations to design more effective representation mappings. Our main idea is to view the language representation mapping as a logic reasoning process; that is, the language representations are deduced step-by-step from the original discrete token sequences. Specifically, we treat the tokens in the input sequence as terms (or objects) in logic programming, and treat their properties and relations as predicates of different arities. In light of logical inductive bias, the language representations that we seek to compute are the (advanced) properties and relations that can be deduced from these input (basic) properties and relations. Most importantly, we require the construction of such deduction process to follow the principles of first-order logic, in order to encode the logical inductive bias into the representation learning process. We now formulate the language representation learning as a logic programming problem by adopting similar (logic programming) notations used in Evans & Grefenstette (2018).\n\n• Terms: We consider a first-order logic system without function symbols, so that terms can only be variables or constant. They are used to represent general objects or a particular object of interest, respectively. In the context of language representation learning, we model each instance of text sequence x (of length T ) as a collection of constants x = {x1, . . . , xT }, where each token xt is a constant (t = 1, . . . , T ). We use lower-case letters to denote constants and upper case for variables as in logic programming. For example, X is a variable to represent a general object (e.g., token).\n\n• Atoms: For each term, we will define its properties and relations as an r-ary predicate p(X1, . . . , Xr), which takes the value of T (True) or F (False) depending on whether a certain property/relation regarding (X1, . . . , Xr) holds or not. For example, whether the a token a takes the v-th id in the vocabulary is a unary predicate TokenIDv(a) for v = 1, . . . , V , where V\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nLanguage representations\n\nLogic programming\n\nFOLNet\n\nTokens xt in the text sequence Token ids, relative distances, etc Final langauge representation Representation mapping (partial) Modus Ponens using generic clause (8) Neural logic operator: see Table 2 Representation mapping (partial) Representation mapping (full)\n\nThe argument: xt in tensor ul(xt) Input tensors: {u0(x), u0(x, y)} Output tensors: {uL(x), uL(x, y)}\n\nConstant: xt Input (basic) atoms: C0 = B Deduced (advanced) atoms: CL\n\n1-step deduction: Cl = conR(Cl−1) L-step deduction: forward-chaining (3)\n\nForward pass: 1-layer Forward pass: L-layer\n\nTable 1: Identification of language representation learning as a logic programming problem.\n\nis the vocabulary size, and whether the distance between two tokens a and b is equal to d is a binary predicate Distd(a, b), for |d| < T . An atom is ground if it has no variables, e.g., the above TokenIDv(a) and Distd(a, b) are all ground atoms.\n\n• Clauses: The reasoning process is constructed by a set of “if-then” clauses in the form of:\n\nq ← p1 ∧ · · · ∧ pm, (1) where p1, . . . , pm and q are the body atoms and head atoms, respectively, and ∧ denotes conjunction (i.e., logical AND). These atoms play the roles of premises and conclusions: if p1, . . . , pm are true, then q is also true. Clauses of the above form are known as definite Horn clauses (Horn, 1951). We call a clause a ground rule if all its variables are substituted by constants. For example, when applying a substitution θ (cid:44) {a/X, b/Y } to a clause q(X, Y ) ← p(X, Y ), we get a ground rule: q(a, b) ← p(a, b). It can be viewed as applying a general clause to a particular instantiation.\n\nOur objective is to learn a collection of clauses and compose them into a mapping from input predicates (e.g., TokenIDv(xt) and RelDistd(xt, xτ )) to language representations. Specifically, let R be a set of clauses and ground(R) be the corresponding set of ground rules. We define the immediate consequences of applying the ground rules in ground(R) to a set of ground atoms X as\n\nconR(X ) = X ∪\n\n(cid:110)\n\nq\n\n(cid:12) (cid:12) (cid:12)q ← p1 ∧ · · · ∧ pm ∈ ground(R),\n\npi ∈ X\n\n(cid:111) .\n\nm (cid:94)\n\ni=1\n\n(2)\n\nIt can be understood as a set of ground atoms that can be deduced from X together with X itself. Given a set of input ground atoms B, we can repeatedly apply the ground rules in R for L steps:\n\nCl = conR(Cl−1),\n\n(3) Then, CL is all the possible ground atoms (predicates) that can be deduced from B (including B itself) in L steps. The above procedure is known as forward-chaining: it deduces all the possible conclusions from the input premises (i.e., B) by repeatedly applying (i.e., chaining) clauses. If we want to verify whether a predicate q(cid:48) holds (i.e., can be entailed), it suffices to check if q(cid:48) is in CL. In language representation learning, we start, for example, from the following input (basic) atoms:\n\nC0 = B and l = 1, . . . , L.\n\nB = {TokenIDv(xt), RelDistd(xt, xτ ), . . . |t, τ = 1, . . . , T }, (4) and deduce CL as the final representations by forward-chaining our (learned) clauses. For example, in solving an (extractive) question answering problem, whether a certain token is the beginning (or end) of the answer span is modeled as an advanced deduced property of this token, i.e.,\n\nCL = {AnswerStartsAt(xt), AnswerEndsAt(xt)|t = 1, . . . , T }. (5) In autoregressive language modeling, the advanced deduced property becomes the next token ids. Table 1 summarizes the above identifications between language representations and logic programming. Next, we will develop a neural architecture to encode such logical inductive bias. Throughout the paper, we will use boldface letters to denote vectors (lowercase) and matrices (uppercase).\n\n3 FOLNET: A NEURAL ARCHITECTURE WITH LOGICAL INDUCTIVE BIAS\n\nIn this section, we develop a novel neural architecture, named First-Order Logic Network (FOLNet), which encodes the logical inductive bias presented in Section 2. Specifically, we focus on: (i) how to represent the atoms as continuous vectors (Section 3.1), (ii) how to devise a neural inference step that approximates (2) (Section 3.2), and (iii) how to forward-chain them into a fully-differentiable architecture based on (3) (Section 3.3). The overall neural architecture and its correspondence to the logical inductive bias are shown in Figure 1. Next, we will discuss step-by-step how we devise the architecture, its advantages, and also some important connections to existing approaches.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Overview of the FOLNet architecture and how it encodes the logical inductive bias. The neural logic operators model the clauses, which are forward-chained into a differentiable model. The “mixer ops” refer to the operators c, j, m, and t in Table 2 as they reduce over the object dimension.\n\n3.1 VECTOR REPRESENTATIONS OF ATOMS\n\nRecall that we use an r-ary ground atom pd(x1, . . . , xr) to characterize whether the d-th property/relation holds for a tuple of tokens (x1, . . . , xr), where d = 1, . . . , Dr. To overcome the difficulty of learning these discrete-valued atoms, which takes values in {T, F}, we introduce ud(x1, . . . , xr) ∈ R as its continuous representation and characterizes the extent to which the atom pd is true. For example, in ProbLog (De Raedt et al., 2007; De Raedt & Kimmig, 2015; Fierens et al., 2012; Manhaeve et al., 2018), ud(·) gives the probability of the atom pd(·) being true. In this paper, we consider ud(·) to be the logit of the corresponding atom, i.e., Pr{pd(·) = T} = 1/(1 + e−ud(·)); a larger value of ud(·) implies a higher chance of the atom pd(·) being true. And we can also easily verify that the logit for the negated atom of pd(·) is simply −ud(·). Let u(x1, . . . , xr) ∈ U ⊂ RDr be a Dr-dimensional vector that collects ud(x1, . . . , xr) as its d-th element. Then, u(x1, . . . , xr) will be a continuous (logit) vector representation for Dr atoms with arity r. For example, for an input text sequence x = (x1, . . . , xT ), we can use u(xt) to represent D1 (unary) properties for each token xt, and use u(xt, xτ ) to characterize D2 (binary) relations between any pair of tokens. Both the input (basic) properties/relations and the deduced (advanced) ones will be represented in the same logit vector space, where the deduction process will be carried out. For convenience, we may also directly refer to a set of atoms by their logit vector representation u(·).\n\n3.2 NEURAL MODUS PONENS INFERENCE\n\nWe now develop a set of neural operators for implementing the deduction step characterized in (2). To begin with, we first introduce the Modus Ponens (MP) rule from first-order logic (Andrews, 2013), which states that if clause B ← A and statement A hold, then B is true:\n\nB ⇐ {B ← A and A}.\n\n(6)\n\nIn the context of (2), when choosing A = p1 ∧ · · · ∧ pm and B = q, the deduction in (2) can be viewed as an applications of the MP inference using all the ground clauses in ground(R). In this paper, we restrict our focus to the setting where all the atoms have arity of either one or two. That is, we will only consider atoms of the form u(xt) and u(xt, xτ ) (represented in their vector forms), respectively.2 Then, we will need to develop the MP inference from a set of atoms {v(a), v(a, b)} to another set of atoms {u(x), u(x, y)}, which can be categorized into the following four groups:\n\nu(x) ⇐ RUU, v(a); u(x) ⇐ RUB, v(a, b); u(x, y) ⇐ RBU, v(a); u(x, y) ⇐ RBB, v(a, b)\n\n(7)\n\nwhere RUU, RUB, RBU and RBB denote the sets of rules that deduce atoms of a certain arity from another arity. For example, RBU defines a collection of clauses that derives binary (B) atoms from unary (U) atoms. We now proceed to model these clauses and their inference in logit space. Given a set of premise atoms P1, . . . , PM , we consider N clauses of the following generic form:\n\n(cid:18) (cid:94)\n\n(cid:19) (cid:94) (cid:18) (cid:94)\n\nPm\n\nQn ←\n\nm∈Mn,+\n\nm∈Mn,−\n\n(cid:19)\n\n¬Pm\n\n, n = 1, . . . , N,\n\n(8)\n\n2Generalizing our work to higher arities is relatively straighforward in principle but will lead to high\n\ncomputation complexities in practice. We leave such an extension as a future work.\n\n4\n\n++LayerNormLayerNormMixer Ops++LayerNormLayerNormBooleanBoolean×\"LayerNormLayerNormMixer OpsPublished as a conference paper at ICLR 2023\n\nSym. Ops.\n\nTyping\n\nB-dim. R-dim. Neural operator in logit space\n\nKernel act. Remarks\n\nb c\nj m\na p\nt\n\nbool cjoin join mu assoc prod trans\n\nU ← U × U U ← U × B U ← B × U U ← B × B B ← U × U B ← U × B B ← B × B\n\nx h\nh x\nh x\nh\n\nw a\na a\nw w\na\n\nuhs(x) = (cid:80) uhs(x) = (cid:80) uhs(x) = (cid:80) uhs(x) = (cid:80) uh(x, y) = (cid:80) uh(x, y) = (cid:80) uh(x, y) = (cid:80)\n\nw Khw(x)vws(x) a Khs(a)vh(x, a) a Kh(x, a)vhs(a) a Kh(x, a)vs(x, a) w Khw(x)vhw(y) w Khw(x)vw(x, y) a Kh(x, a)vh(a, y)\n\nIdentity Softmaxa Softmaxa Softmaxa Identity Identity Softmaxa\n\nSelf-attention General RPE Self-attention General RPE\n\nTable 2: List of all our neural logic operators with restricted kernels (see Appendix A.1 for our naming protocols). Note that each operator has a unique batching dimension (B-dim) and a unique reduction dimension (R-dim). The typing of the operator defines the arities of the kernel, the premise and the output atom. For example, U ← B × U means the arities of the kernel, the input atom, and the output atom are 2, 1 and 1, respectively. We also list the activation functions that are used to compute the corresponding kernels, where the normalization dimension of Softmax is listed in its subscript.\n\nwhere Qn is the head atom, ¬ denotes logical negation (i.e., NOT), and Mn,+ and Mn,− are two subsets of M = {1, . . . , M } with Mn,+ ∩ Mn,− = ∅. Then, the logit vector u for the head atoms {Qn} can be approximately inferred from the logit vector v of the premises {Pm} by a matrix multiplication followed by an (optional) elementwise nonlinear activation function (Appendix D):\n\nu = σ(Kv),\n\n(9)\n\nwhere K is an N × M kernel matrix that represents the clauses in (8), and σ(z) = ln(1 + 2ez) is the activation function. Notably, each row of K characterizes the conjunction pattern on the right-hand side of (8): a positive (negative) value of its (n, m)-th element, Knm, means that m is more likely in Mn,+ (Mn,−) for the n-th clause. It follows that the kernels for RUU, RUB, RBU and RBB in (7) would be in the form of KUU(x, a), KUB(x, a, b), KBU(x, y, a) and KBB(x, y, a, b), respectively, which are D1 × D1, D1 × D2, D2 × D1 and D2 × D2 matrices. And (9) would become matrix multiplications between them and their corresponding premises (i.e., v(a), v(a, b), v(a) or v(a, b)) followed by a summation over a and b whoever appear therein (see (30)–(33) in Appendix D). Finally, the activation function σ(·) can be dropped when “←” is replaced with “≡”, where A ≡ B iff A ← B and B ← A.\n\nOne major limitation of directly implementing (9) for the inference rules in (7) is the high memory and computation costs. For example, the kernel KBB(x, y, a, b) needs O(D2 2T 4) to store its value. And the MP inference (9), which now becomes u(x, y) = (cid:80) a,b K(x, y, a, b)v(a, b), also has a computation complexity of O(D2 2T 4). Therefore, we have to restrict the size of the kernel and reduce the overall complexity by using different methods, such as sharing the values of KBB(x, y, a, b) across different dimensions. We now provide a systematic approach based on the following principles:\n\n1. We restrict all the kernels to be in the form of {K(ω), K(ω, ν)}, i.e., the arity r = 1, 2.\n\n2. We pick one reduction dimension and one batching dimension in the matrix multiplication.\n\nWith the above assumption, we factor the predicate dimensions of unary kernels and unary atoms so that Kd(ω) = Khs(ω) and ud(x) = uhs(x), where d = (h − 1)S + s with h = 1, . . . , H and s = 1, . . . , S. This is inspired by the multi-head attention mechanism (Vaswani et al., 2017), where h is akin to the head index, H is the number of heads and S is the size of the head. Then, we enumerate all possible neural logic operators that can compatibly multiply a kernel from {K(ω), K(ω, ν)} with a premise from {v(a), v(a, b)} by properly choosing different reduction and the batching dimensions. With this, we list the resulting neural logic operators for each typing in Table 2, which are further discussed in Appendix A.1 for their different roles in (restricted) Modus Ponens reasoning.\n\nInterestingly, we find that the j-operator and the a-operator share Connection with transformers similar forms as the self-attention mechanism in transformers, where the a-operator computes the selfattention scores and the j-operator performs the self-attention operation. Furthermore, the m-operator and the p-operator indeed generalize the existing relative positional encodings developed in (Shaw et al., 2018), which are widely used in different transformer variants, such as in T5 models (Raffel et al., 2020). Specifically, we show in Appendix E that by making vw(x, y) instance-independent, the p-operator computes the second term in equation (5) of Shaw et al. (2018), where vw(x, y) play the role of aK ij . And by setting vs(x, a) instance-independent, the m-operator computes the second\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nterm in equation (3) of Shaw et al. (2018), where vs(x, a) plays the role of aV ij. That is, under such degenerated settings, these two operators will compose the relative positional encoding developed therein. Note that their aK ij are static learnable embeddings, while our vw(x, y) and vs(x, a) are dynamically computed for each instance (as we will discuss in Section 3.3). Therefore, our m-op and p-op can also be viewed as a more adaptive relative positional encoding, whose advantages will be further demonstrated in our experiments (Section 4).\n\nij and aV\n\n3.3 FORWARD-CHAINING AND DIFFERENTIABLE LEARNING\n\nNote that, in general, a logic operator takes a kernel K and a premise v to infer an outcome u (Table 2). Specifically, it “neuralizes” the logic deduction in (2) for a particular typing (e.g., U ← B × U). Applying all the neural logic operators amounts to have a full execution of (2), which is one recursion step in (3) that maps a set of {ul−1(x), ul−1(x, y)} into {ul(x), ul(x, y)}. Therefore, we can naturally forward-chain L stages of them together to create a fully-differentiable architecture that models the reasoning chain in (3). Figure 1 depicts such a forward-chaining process and also how it encodes the logical inductive bias described in Section 2. One remaining problem is how to obtain the kernels K(·) and the premises v(·) from our FOLNet architecture in Figure 1. We do this simply by applying two linear projections (one for the premise and one for the kernel) to the previous layer’s output {ul(x), ul(x, y)}. For the kernel, we may further apply an activation function after the linear projection to compute the kernel (see Table 2 for the list of kernel activation functions for each operator). In other words, we parameterize the kernels K(·) and the premises v(·) by (the intermediate deduction results of) FOLNet itself. This is based on the observation that clauses are themselves predicates since A ← B is defined as A ∨ ¬B (Andrews, 2013), where ∨ denotes disjunction (logical OR). We now describe the input and the output of FOLNet in Figure 1. At the input, we can encode the discrete token ids for a token xt into vectors of the form u0(xt) by standard embedding lookup. Likewise, we also convert the (discrete) relative distance between two tokens xt and xτ into a vector of the form u0(xt, xτ ). The {u0(xt), u0(xt, xτ )}t,τ will be used as vector representations of the base atoms B and fed into the FOLNet model (Figure 1). After L layers (i.e., L steps of deduction), the output {uL(xt), uL(xt, xτ )}t,τ becomes the vector representations of CL in (3), which is used as the final language representations. Therefore, our FOLNet model has the same input-output interface as other transformer models and can be used in a plug-and-play manner for solving downstream tasks. Because of this, our model can also be pretrained over large-scale texts using the same losses (e.g., MLM, NSP, SOP, etc) as other encoder-only models — see Appendix A.4 for how to compute these losses from {uL(xt), uL(xt, xτ )}t,τ . Our model can also be extended to the decoder-only and the encoder-decoder versions by slightly modifying the neural logic operators (see Appendix A.5), which can then be pretrained to predict the next words auto-regressively. We will conclude this section by discussing several important properties of FOLNet architecture.\n\nThe dual-branch architecture Note that the FOLNet model in Figure 1 has two branches: (i) a unary predicate branch for reasoning over ul(x), and (ii) a binary predicate branch for reasoning over ul(x, y). This is in sharp contrast to the single-branch architecture of the transformer models (Figure 3 in Appendix A.2). We further note that when FOLNet is only loaded with j-operator and a-operator, it degenerates into a dual-branch variant of the transformer architecture. In our experiments, we will show that, even in such degenerated setting, FOLNet still outperforms transformer. This is probably because the binary predicate branch explicitly maintains the pairwise relations ul(x, y) throughout the reasoning process. In addition, the explicit binary predicate branch also allows us to directly input the relative distance knowledge into the model without performing the less-intuitive operations as in existing RPEs (Shaw et al., 2018). In our experiments in Section 4, we will demonstrate the advantage of such a simple yet effective approach for consuming the relative positional information, along with some other advantages of the dual-branch architecture of FOLNet.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETTINGS\n\nWe now evaluate our FOLNet models under different settings and seek to answer the following question: Can the neural architecture (FOLNet) that encodes the logical inductive bias learn better language representations than the transformer models? To this end, we need to eliminate other\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nModel\n\nParams\n\nPE D2\n\nLoss\n\nMNLI-m/mm QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg\n\n1 BERT 2 BERT (ours) 3 FOLNet: j.a 4 FOLNet: j.a 5 FOLNet: j.a 6 FOLNet: jm.ap\n\n110M APE 110M APE 110M APE 109M RPE 110M RPE(cid:63) 123M RPE\n\n- MLM.NSP - MLM.NSP 12 MLM.NSP 12 MLM.NSP 12 MLM.NSP 12 MLM.NSP\n\n7 FOLNet: j.a 8 FOLNet: j.a 9 FOLNet: j.a\n\n109M RPE 109M RPE 110M RPE\n\n16 MLM.NSP 32 MLM.NSP 64 MLM.NSP\n\n110M RPE 10 FOLNet: j.at 117M RPE 11 FOLNet: j.atp 12 FOLNet: jm.atp 124M RPE 13 FOLNet: jmc.atp 138M RPE 14 FOLNet: jmc.atp 137M RPE\n\n64 MLM.NSP 64 MLM.NSP 64 MLM.NSP 64 MLM.NSP 12 MLM.NSP\n\n84.5/- 83.9/84.1 84.9/84.5 85.0/84.9 84.7/84.9 85.7/85.3\n\n85.2/85.2 85.8/85.7 85.7/85.5\n\n86.7/86.6 87.4/87.4 88.1/87.6 88.2/87.9 85.9/86.3\n\n15 FOLNet: jmc.atp 138M RPE\n\n64 MLM.SOP\n\n88.3/87.9\n\n91.3 90.9 91.1 91.4 91.4 91.6\n\n91.3 91.4 91.4\n\n91.6 91.9 91.7 91.9 91.6\n\n91.8\n\n91.7 88.2 91.6 91.4 91.5 91.8\n\n91.8 92.0 91.8\n\n92.8 93.3 93.9 94.1 92.7\n\n94.2\n\n93.2 92.6 92.2 93.5 93.8 93.8\n\n93.7 93.7 93.2\n\n93.1 94.0 94.2 94.5 93.6\n\n94.7\n\n58.9 61.5 61.1 63.8 63.4 65.7\n\n64.1 64.2 63.5\n\n63.6 62.9 64.7 66.9 63.4\n\n65.6\n\n89.5 89.2 89.6 90.1 89.8 90.4\n\n89.9 90.0 90.1\n\n91.1 91.3 91.2 91.6 90.5\n\n91.1\n\n87.3 88.2 89.5 89.9 90.6 91.0\n\n89.3 90.5 90.2\n\n89.9 91.4 91.4 91.5 91.0\n\n91.1\n\n68.6 83.1 66.8 82.7 72.2 84.0 72.9 84.7 72.6 84.7 73.5 85.4\n\n71.8 84.6 71.8 84.9 74.7 85.1\n\n80.9 86.2 81.6 86.7 83.2 87.3 83.5 87.7 75.8 85.6\n\n83.2 87.5\n\nTable 3: Analysis of FOLNet on the development sets of GLUE benchmark. All the results are medians of five random seeds. From top to bottom, the first block shows the advantage of dual-branch architecture and compares our new positional encoding to others, the second block analyzes the influence of the binary predicate dimension, the third block performs ablation study of all the logic operators, and the last block shows the results of other pretraining losses for FOLNet. APE stands for absolute positional encoding, RPE(cid:63) denotes the relative positional encoding used by T5 (Shaw et al., 2018; Raffel et al., 2020), and RPE means our proposed relative positional encoding. We have also pretrained a BERT (base) model (line #2) by using the same settings as FOLNet for a fair comparison.\n\nconfouding factors and make sure the only difference lies in the model architecture itself. First, we choose to pretrain our FOLNet model using the widely used masked language modeling (MLM) loss (Devlin et al., 2018), and add an extra loss of either the next sentence prediction (NSP) (Devlin et al., 2018) or sentence-order prediction (SOP) (Lan et al., 2019). Many different variants of widely used encoder-only transformer models such as BERT, RoBERTa, ALBERT, DeBERTa and Megatron-LM are pretrained with these losses. Therefore, we will also use these models as our primary baselines. Although there could be other more efficient pretraining losses such as the ones in (Bao et al., 2020; Clark et al., 2020; Yang et al., 2019; Meng et al., 2021), we believe that developing a new model architecture with a better inductive bias is an orthogonal line of research. Therefore, we leave the exploration of other pretraining loss for FOLNet as a future work. In addition, we consider two settings of pretraining dataset: (i) Wikipedia + BookCorpus (Zhu et al., 2015) (16GB in texts) and (ii) a larger set of 160GB texts consisting of Wikipedia, BookCorpus2, OpenWebText2, and Pile-CC (extracted from the Pile dataset (Gao et al., 2020)). We use the BERT tokenizer with 32,768 uncased BPE vocabulary (Sennrich et al., 2016) throughout our experiments.3 We consider FOLNet models of two different sizes: FOLNetBase and FOLNetLarge, which are comparable in size to the base (e.g., BERTBase) and large models (e.g., BERTLarge) in literature. Finally, the FOLNet model will always be pretrained with a sequence length of 128 tokens, although it will be evaluated on different downstream tasks with longer sequence lengths (e.g., 384 or 512). For evaluation, we consider three benchmarks: GLUE (Wang et al., 2019), SQuAD 2.0 (Rajpurkar et al., 2016b), and FOLIO (Han et al., 2022), where we finetune our pretrained models on each individual task separately for evaluation. More details about these downstream tasks and hyper-parameters can be found in Appendix B.\n\n4.2 ANALYSIS OF THE FOLNET ARCHITECTURE\n\nWe begin with in-depth analysis of FOLNet to demonstrate its advantage over the transformer architecture. To this end, we first pretrain our FOLNetBase model on the dataset of Wikipedia and BookCorpus, which is the same as the one used by BERT. And we further use MLM and NSP as our pretraining losses to make it consistent with BERT. We analyze FOLNet from different aspects on GLUE benchmark and report the results in Table 3. We now proceed to discuss the results below.\n\n3Although Liu et al. (2019) pointed out that it would be more ideal to use the byte-level BPE for preprocessing the much larger 160GB texts, we use the same BERT tokenizer (based on character-level BPE) to process the 160GB text to simplify our logistics, and it already demonstrates the strong performance of our models. Using the byte-level BPE tokenizer with a larger 50K vocabulary as in (Liu et al., 2019; Radford et al., 2019; Brown et al., 2020) may further improve our FOLNet models that are pretrained on the 160GB texts.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nModel\n\nParams\n\nBERTBase RoBERTaBase DeBERTaBase FOLNetBase\n\nBERTLarge\n\nRoBERTaBase DeBERTaBase FOLNetBase\n\nRoBERTaLarge DeBERTaLarge ALBERTXXL ALBERTXXL+ FOLNetLarge\n\nMegatron1.3B Megatron3.9B\n\n110M 110M 134M 138M\n\n340M\n\n125M 134M 138M\n\n356M 384M 235M 235M 437M\n\n1.3B 3.9B\n\nGLUE\n\nSQuAD 2.0\n\nFOLIO\n\nMNLI-m/mm QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg\n\nEM F1\n\n84.5/- 85.8/85.5 86.3/86.2 88.2/87.9\n\n86.6/-\n\n87.6/- 88.8/88.5 89.4/89.7\n\n90.2/90.2 91.1/91.1 90.4/- 90.8/- 91.2/91.3\n\n90.9/91.0 91.4/91.4\n\n91.3 91.3 -\n91.9\n\n91.3\n\n91.9 -\n92.2\n\n92.2 92.3 92.0 92.2 92.5\n\n92.6 92.7\n\n91.7 92.0 -\n94.1\n\n92.3\n\n92.8 -\n94.4\n\n94.7 95.3 95.2 95.3 95.8\n\n- -\n\n93.2 93.7 -\n94.5\n\n93.2\n\n94.8 -\n95.6\n\n96.4 96.8 96.8 96.9 96.8\n\n- -\n\n58.9 60.1 -\n66.9\n\n60.6\n\n63.6 -\n69.9\n\n68.0 70.5 68.7 71.4 71.5\n\n- -\n\n89.5 88.5 -\n91.6\n\n90.0\n\n91.2 -\n92.5\n\n92.4 -\n92.7 93.0 92.2\n\n- -\n\n87.3 87.3 -\n91.5\n\n88.0\n\n90.2 -\n92.0\n\n90.9 -\n90.2 90.9 93.5\n\n- -\n\n68.6 68.2 -\n83.5\n\n83.1 83.3 -\n87.7\n\n73.7 77.7 79.3 84.7\n\n76.3 80.5 82.5 87.9\n\n70.4\n\n84.1\n\n79.0\n\n81.8\n\n78.7 -\n87.0\n\n86.6 -\n88.1 89.2 91.1\n\n- -\n\n86.4 -\n89.2\n\n88.9 -\n89.3 89.9 90.6\n\n- -\n\n80.5 83.1 85.5\n\n86.5 88.0 87.2 87.4 88.5\n\n87.1 88.5\n\n83.7 86.2 88.6\n\n89.4 90.7 89.9 90.2 91.5\n\n90.2 91.2\n\nAcc\n\n57.8 -\n- 64.2\n\n62.3\n\n64.7 -\n64.2\n\n67.7 -\n- -\n70.6\n\n- -\n\nTable 4: Overall results on the development sets of GLUE, SQuAD 2.0 and FOLIO. The upper block (separated by the solid line) of the table shows the results of the models pretrained on Wikipedia + BookCorpus (16GB), and the lower block are the models pretrained on extended data (160GB). We use dashed lines to separate models of different sizes within each block. All the results are medians of five random seeds. The baseline results of FOLIO are provided by the authors of Han et al. (2022). Here we use ALBERTXXL to refer to the ALBERT model pretrained by 1M steps and use ALBERTXXL+ to refer to the ALBERTXXL modeled pretrained by 1.5M steps.\n\nThe advantage of the dual-branch architecture Recall that when FOLNet only has the join and assoc operators, it can be viewed as a dual-branch version of the transformer architecture. To have a fair comparison, we pretrain a FOLNet model with the same absolute positional encoding (APE) as BERT (line #3 of Table 3). Note that, even in such overly degenerated case, FOLNet still noticeably outperforms BERT on average. When equipping FOLNet with our new relative positional encoding (RPE) (line #4 of Table 3), we will outperform BERT by 2 points on average. Notably, it achieves on par (or slightly better) average performance compared to the one with T5 relative positional encoding (RPE(cid:63) in line #5). As we discussed in earlier section, the T5 RPE are degenerated version of our mu and prod operators. Line #6 of Table 3 show that adding mu and prod operators to the FOLNet would further boost the performance by a noticeable amount.\n\nThe benefits of a larger D2 We can see (lines #7-9 of Table 3) that increasing the dimension D2 will steadily improves the performance. As we will reveal soon, when FOLNet is fully loaded with all the operators, having a larger D2 is essential to unleash their full power; that is, the performance improvement from increasing D2 would be even larger for a fully-loaded FOLNet.\n\nContribution of the logic operators We now analyze the contributions of the logic operators by adding them one-by-one into FOLNet until being fully loaded. We see from line #9 to line #13 in Table 3 that this drastically improves the average performance. In line #14, we evaluate a fully-loaded FOLNet with D2 decreased from 64 to 12, which shows a significant performance drop. This confirms the importance of having a relatively large D2 in order to store the deduced relations.\n\n4.3 OVERALL PERFORMANCE\n\nHaving closely examined various aspects of FOLNet architecture, we now proceed to evaluate it comprehensively on three benchmarks (GLUE, SQuAD 2.0, and FOLIO). We will also examine its performance when we further scale up the pretraining data size and model size. To pretrain the model on a larger (160GB) dataset, we find it more efficient to generate the pretraining data with SOP losses. This is because NSP losses require us to sample negative sentences from another document. To begin with, we verify the performance by pretraining a FOLNetBase with SOP loss on Wikipedia and BookCorpus. The result (line #15 in Table 3) shows that it could slightly degrade the performance compared to the one with NSP (line #13). However, this is relatively tolerable given its convenience when pretraining on a large corpus. Therefore, we will replace NSP with SOP when pretraining\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFOLNetBase and FOLNetLarge on the 160GB dataset. We show our results on the GLUE benchmark in Table 4 and compare them with other baseline methods. Observe that our FOLNetBase models pretrained on both 16GB data and 160GB significantly outperform other transformer-based models on all three benchmarks. Our FOLNetBase pretrained on 16GB data outperforms BERTLarge model that is 3× larger in model size. In addition, it even outperforms RoBERTaBase that is pretrained on 160GB data. When pretraining our FOLNetBase model on 160G data, we even surpass the RoBERTaLarge model (89.3 vs 88.9) on GLUE benchmark. Likewise, our FOLNetLarge model also significantly outperforms all other baselines. It even outperforms ALBERTXXL+ that is pretrained by 1.5M steps (i.e., 50% more tokens during pretraining). Notably, our FOLNetLarge model achieves comparable performance as the Megatron3.9B model on both GLUE and SQuAD 2.0 benchmark, which is 10 times larger than our model. Furthermore, although our FOLNet model is not designed for solving reasoning problem (but incorporating reasoning as an inductive bias at the token-level), our model still consistently demonstrates stronger first-order logic reasoning capability on FOLIO task (e.g., +3.9 over RoBERTaLarge). Finally, we would like to highlight that all the FOLNetBase and FOLNetLarge models are pretrained with sequence length of 128, in contrast to 512 as in other baselines. However, they are evaluated on GLUE, SQuAD 2.0 and FOLIO with sequence length of 128, 384 and 512, respectively. In particular, by finetuning with merely 1,004 training examples on FOLIO, it is able to generalize to much longer sequences (512), which have never been seen during pretraining.\n\n5 RELATED WORKS\n\nTransformer language models There have been a long line of research on neural language models since Bengio et al. (2000). Recently, it has achieved great success by exploring different variants of pretrained transformer models (Vaswani et al., 2017) for solving downstream language tasks, such as with finetuning (Radford et al., 2018; Devlin et al., 2018; Lan et al., 2019; Liu et al., 2019; Yang et al., 2019) or with zero/few-shot learning using large language models (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2022). Another line of active research focuses on developing more effective pretraining losses (Yang et al., 2019; Clark et al., 2020; Bao et al., 2020; Tay et al., 2022) beyond the widely used autoregressive or masked language modeling objectives. There have been limited works on developing new neural architectures for learning better language representations. In this paper, we seek to move in this direction and develop a new neural architecture based on logical inductive bias.\n\nLogic programming and neural reasoning Our logical inductive bias is inspired by logic programmings (Horn, 1951; De Raedt et al., 2007; De Raedt & Kimmig, 2015; Fierens et al., 2012; Manhaeve et al., 2018) and inductive logic programming (Evans & Grefenstette, 2018; Muggleton, 1991; 1996; Friedman et al., 1999). Different from these works, we do not directly work on reasoning problems. Instead, we seek to encode the logical inductive bias into the neural model to learn better language representations. Another line of related works focuses on developing neural models that can perform reasoning in a broad sense. For example, different variants of memory augmented networks are developed (Le et al., 2020; Santoro et al., 2018; Graves et al., 2014; 2016; Santoro et al., 2016; Le et al., 2019), which augment a control network with memory units and the associated neural read/write modules. Besides, Rocktäschel & Riedel (2017) and Minervini et al. (2020) consider the problem of proving structured queries to knowledge base, by constructing differentiable neural networks via backward-chaining. Bergen et al. (2021) develop a triangular attention to deduce relations from other relations, which can be viewed as a transformer with a single relational branch. These methods are effective in solving (relatively small-scale) reasoning tasks. However, it remains unclear whether they can be effectively pretrained on large-scale texts to solve diverse downstream natural language tasks.\n\n6 CONCLUSION\n\nWe introduce a novel logical inductive bias, which treats language representation learning as a logic programming problem. We then develop a fully-differentiable neural architecture (FOLNet) that effectively encodes this inductive bias by forward-chaining a rich set of neural logic operators. The proposed FOLNet architecture has the same input-output interface as the transformer models and can be pretrained over large-scale text data. Experimental results demonstrate that the FOLNet architecture significantly outperforms different variants of transformer models, and has many inherent advantages due to the new dual-branch architecture along with its rich set of neural logic operators.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nPeter B Andrews. An introduction to mathematical logic and type theory: to truth through proof,\n\nvolume 27. Springer Science & Business Media, 2013.\n\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, et al. Unilmv2: Pseudo-masked language models for unified language model pre-training. In International Conference on Machine Learning, pp. 642–652. PMLR, 2020.\n\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\n\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. In T. Leen, T. Dietterich, and V. Tresp (eds.), Advances in Neural Information Processing Systems, volume 13. MIT Press, 2000. URL https://proceedings.neurips.cc/paper/2000/ file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf.\n\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing\n\ntextual entailment challenge. In TAC, 2009.\n\nLeon Bergen, Timothy O’Donnell, and Dzmitry Bahdanau. Systematic generalization with edge\n\ntransformers. Advances in Neural Information Processing Systems, 34:1390–1402, 2021.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://aclanthology.org/S17-2001.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text\n\nencoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\n\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment\n\nchallenge. In Machine learning challenges workshop, pp. 177–190. Springer, 2005.\n\nLuc De Raedt and Angelika Kimmig. Probabilistic (logic) programming concepts. Machine Learning,\n\n100(1):5–47, 2015.\n\nLuc De Raedt, Angelika Kimmig, and Hannu Toivonen. Problog: A probabilistic prolog and its\n\napplication in link discovery. In IJCAI, volume 7, pp. 2462–2467. Hyderabad, 2007.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171– 4186, 2018.\n\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/I05-5002.\n\nRichard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal of\n\nArtificial Intelligence Research, 61:1–64, 2018.\n\nDaan Fierens, Guy Van den Broeck, Maurice Bruynooghe, and Luc De Raedt. Constraints for probabilistic logic programming. In Proceedings of the NIPS probabilistic programming workshop, pp. 1–4, 2012.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nNir Friedman, Lise Getoor, Daphne Koller, and Avi Pfeffer. Learning probabilistic relational models.\n\nIn Proc. IJCAI, 1999.\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot In Proceedings of the 59th Annual Meeting of the Association for Computational learners. Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3816–3830, 2021.\n\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pp. 1–9, Prague, June 2007. Association for Computational Linguistics. URL https://aclanthology.org/W07-1401.\n\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines.\n\narXiv preprint\n\narXiv:1410.5401, 2014.\n\nAlex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwi ́nska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626): 471–476, 2016.\n\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, volume 7, 2006.\n\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022.\n\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nAlfred Horn. On sentences which are true of direct unions of algebras1. The Journal of Symbolic\n\nLogic, 16(1):14–21, 1951.\n\nShankar Iyer, Nikhil Dandekar, and Kornl Csernai.\n\nFirst quora dataset release: Question In Quora, January 2017. URL https://www.quora.com/q/quoradata/\n\npairs. First-Quora-Dataset-Release-Question-Pairs.\n\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\n\nHung Le, Truyen Tran, and Svetha Venkatesh. Neural stored-program memory. In International\n\nConference on Learning Representations, 2019.\n\nHung Le, Truyen Tran, and Svetha Venkatesh. Self-attentive associative memory. In International\n\nConference on Machine Learning, pp. 5682–5691. PMLR, 2020.\n\nHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.\n\nIn Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.\n\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858.\n\nPercy Liang. Lambda dependency-based compositional semantics. CoRR, abs/1309.4408, 2013.\n\nURL http://arxiv.org/abs/1309.4408.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nRobin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. Deepproblog: Neural probabilistic logic programming. Advances in Neural Information Processing Systems, 31, 2018.\n\nYu Meng, Chenyan Xiong, Payal Bajaj, Paul Bennett, Jiawei Han, Xia Song, et al. Coco-lm: Correcting and contrasting text sequences for language model pretraining. Advances in Neural Information Processing Systems, 34:23102–23114, 2021.\n\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=r1gs9JgRZ.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013.\n\nPasquale Minervini, Matko Bošnjak, Tim Rocktäschel, Sebastian Riedel, and Edward Grefenstette. Differentiable reasoning on large knowledge bases and natural language. In Proceedings of the AAAI conference on artificial intelligence, pp. 5182–5190, 2020.\n\nStephen Muggleton. Inductive logic programming. New Gener. Comput., 8(4):295–318, 1991.\n\nStephen Muggleton. Stochastic logic programs. Advances in Inductive Logic Programming, 32:\n\n254–264, 1996.\n\nNvidia. A pytorch extension: Tools for easy mixed precision and distributed training in pytorch.\n\nAvailable Online: https://https://github.com/NVIDIA/apex, 2019.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nJeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543, 2014.\n\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2227–2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https: //aclanthology.org/N18-1202.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.\n\nImproving language\n\nunderstanding by generative pre-training. OpenAI, 2018.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016a. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology. org/D16-1264.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, 2016b.\n\nTim Rocktäschel and Sebastian Riedel. End-to-end differentiable proving. Advances in neural\n\ninformation processing systems, 30, 2017.\n\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. MetaIn International conference on machine\n\nlearning with memory-augmented neural networks. learning, pp. 1842–1850. PMLR, 2016.\n\nAdam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational recurrent neural networks. Advances in Neural Information Processing Systems, 31:7299–7310, 2018.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In 54th Annual Meeting of the Association for Computational Linguistics, pp. 1715–1725. Association for Computational Linguistics (ACL), 2016.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In NAACL-HLT (2), pp. 464–468, 2018. URL https://aclanthology.info/papers/ N18-2074/n18-2074.\n\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L Hamilton. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4506–4515, 2019.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013.\n\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, 2019.\n\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\n\nTransactions of the Association for Computational Linguistics, 7:625–641, 2019.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus In Proceedings of the 2018 Conference of for sentence understanding through inference. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https: //aclanthology.org/N18-1101.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.\n\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Syx4wnEtvH.\n\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), December 2015.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nSupplementary Materials\n\nA ADDITIONAL DETAILS OF THE FOLNET ARCHITECTURE\n\nA.1 MORE DISCUSSIONS ON THE NEURAL LOGIC OPERATORS\n\nModus Ponens inference with restricted kernels. First, it is straightforward to show that the neural logic operators in Table 2 can be implemented as batch matrix multiplications (Figure 2), where multiple slices of matrix multiplications are executed in parallel to obtain the outputs. Different operators pick their own reduction dimensions (R-dim) and batching dimensions (B-dim) for the matrix multiplication (see Table 2). For example, the join operator picks the a-dimension as the R-dim and the h-dimension as the B-dim so that matrix multiplications are carried out in parallel over h. According to (9), each slice of the matrix multiplication can be viewed as an independent neural Modus Ponens inference based on its own set of clauses. For example, in the join operator, let Kjoin,h denote a matrix that collects Kh(x, a) as its (x, a)-th element. Then, Kjoin,h is the h-th kernel slice that represents a particular group of clauses for the join operator. In addition, recall that the values at each row of the kernel slice characterize the conjunction pattern on the right-hand side of (8), with positive (or negative) values determine whether the original premise Pm (or the negated premise ¬Pm) should be used for conjunction (see Section 3.2 and Appendix D). Therefore, the R-dim in the matrix multiplication shall be understood as the conjunction dimension in (8). In the join operator, it implies that the conjunction operation is over the a-dimension of the premises vhs(a), with the conjunction pattern determined by the x-th row of Kjoin,h if we want to infer uhs(x). In contrast, a full Modus Ponens infernece from unary atoms to unary atoms (the first expression in (7)) shall perform its conjunction over the joint dimensions of (h, s, a), which is much higher in complexity. Therefore, the join operator controls the complexity of Modus Ponens inference by restricting the conjunction operation over the a-dimension. Furthermore, it is noteworthy that the join operator is applying the same kernel slice to premises of different s; that is, it implements kernel-sharing across the s-dimension. This is another complexity-reduction strategy resulted from the principles in Section 3.2. Likewise, the same conclusion holds for all other operators, who pick their own conjunction-dimensions and kernel-sharing dimensions (see Figure 2).\n\nFigure 2: Our neural logic operators can be efficiently implemented as batch matrix multiplications by using hardware accelerators (e.g., GPUs and TPUs). Each slice of matrix multiplications represents an independent neural Modus Ponens inference based on its own set of clauses.\n\nNaming protocols and examples. We now explain the naming protocols for the logic operators along with concrete examples to demonstrate how each of them help reasoning in different aspects.\n\n15\n\n::::::(()())()()()Published as a conference paper at ICLR 2023\n\n• bool: It operates over the predicate dimension and plays the role of Boolean functions over a set of input truth values, which leads to its name of bool-operator. It is used to deduce a property of x from other properties regarding the same x via certain Boolean operations. For example, it can model inferences like Edible(x) ← IsMushroom(x) ∧ ¬IsToxic(x), where the conjunction pattern on the right-hand side is determined by the kernel.\n\n• join: We name this operator as join-operator because it is in a similar form as the join operation in λ-DCS (Liang, 2013). For example, when the kernel represents IsBornIn(x, y), it can be used to infer CitizenOfUSA(x) from premise CountryIsUSA(y), where the kernel determines the conjunction pattern of CountryIsUSA(y) over y.\n\n• cjoin: Since it simply swaps the roles of the kernel and the premise in the join-operator, we\n\nname it as the conjugate join operator. It plays a similar role as the join operator.\n\n• mu: We name it as mu-operator because it is can be viewed as a more general-form of the mu-abstraction operation in λ-DCS (Liang, 2013). For example, when the kernel models ApplyJobAt(x, y), it can deduce HasAJob(x) from premise ReceiveOfferFrom(x, y), where the kernel determines the conjunction pattern of ReceiveOfferFrom(x, y) over y.\n\n• assoc: We name it as assoc-operator because it can be viewed as computing the association between two vectors. For example, when the kernel represents LosAngeles(y), it can be used to infer SameStateAs(x, y) from SanFrancisco(x), where the kernel determines the conjunction pattern of SanFrancisco(x) over the predicate dimension. In this example, we only have one premise atom over the predicate dimension. In the general case, a particular conjunction pattern would be applied to multiple premise atoms to yield the output.\n\n• prod: We name it as prod-operator because it is in resemblance of computing the product over the predicate dimension. For example, when the kernel models Graduated(x), it can be used to infer GraduatedFrom(x, y) from StudyAt(x, y), where the kernel determines the conjunction pattern of StudyAt(x, y) over the predicate dimension. In this example, we only have one premise atom over the predicate dimension. In the general case, a particular conjunction pattern would be applied to multiple premise atoms to yield the output.\n\n• trans: We name it as trans-operator because its form is in reminiscent of reasoning with transitive properties. For example, when the kernel models FatherOf(x, z), it can be used to infer GrandFatherOf(x, y) from ParentOf(z, y), where the kernel determines the conjunction pattern of ParentOf(z, y) over the dimension z.\n\nNote that we do not have a logic operator corresponding to the typing B ← B × U because it will be identical to the prod operator when the kernel action function is chosen to be the identity mapping. In addition, the above examples further show that the kernels (i.e., the clauses) themselves are also atoms or can be computed from certain atoms. This justifies our strategy of parameterizing the kernels by (the intermediate deduction results of) FOLNet itself in Section 3.3.\n\nA.2 COMPARING TO THE TRANSFORMER ARCHITECTURE\n\nFigure (3) show that the transformer architecture can be understood as a single-branch version of the FOLNet architecture with only join-operator and assoc-operator.\n\nFigure 3: Overview of the Transformer architecture. It can be understood as a single-branch version of the FOLNet architecture with only join-operator and assoc-operator.\n\n16\n\n+LayerNormJoin op+LayerNormBoolean×\"LayerNormAssoc opPublished as a conference paper at ICLR 2023\n\nA.3 THE BOOLEAN OPERATOR\n\nIn FOLNet, we do not chain the Boolean operators in parallel with other operators but chain it with others in a cascade manner. This is akin to cascading the FFN with the self-attention module in transformers. Nevertheless, we may also place it in parallel to other operators just as making FFN in parallel to self-attention in transformer, which is done in Chowdhery et al. (2022). In addition, we also adopt the same FFN architecture (i.e., a multilayer perceptron with GeLU units) for the Boolean operator in order to make FOLNet directly comparable with transformer architectures.\n\nA.4 THE INPUT-OUTPUT INTERFACE\n\nComputing the pretraining losses (e.g., MLM, NSP and SOP) from the output atoms As we pointed out, FOLNet models will have a similar input-output interface as the existing transformer models, so that we can seamless adopt existing pretraining losses (e.g., MLM, NSP and SOP) by computing them from {uL(x)}. Recall that uL(xt) is the vector that represents the derived (advanced) properties for the object (token) xt, where t = 1, . . . , T . For a masked token xt, uL(xt) contains its properties that could be deduced from other tokens via their input properties and relations. Therefore, these deduced properties in uL(xt) can be used to predict the original masked token. For example, we can apply a linear classifier followed by a softmax operator to compute a probability distribution over the vocabulary and the MLM loss. Likewise, to compute NSP and SOP losses, we add a special “[CLS]” token at the beginning of the input sequence, so that the uL(x) that corresponds to the “[CLS]” token will be fed into a binary classifier for computing the NSP or SOP loss. The usage of {uL(xt)} in downstream tasks (such as sequence classification, multiple-choice and sequence labeling) are also similar. On the other hand, we have not used the output binary predicates {uL(xt, yτ )} for computing any losses. The main reason is that we would like to adopt the existing off-the-shelf pretraining losses for an apple-to-apple comparison regarding the proposed model architecture. Since most of these losses are developed for the transformer architecture, which is a single-branch model with only unary predicates on its main pathway (Figure 3), it is not surprising that these losses are mainly computed from the unary properties {uL(xt)}. Nevertheless, we believe that our newly introduced binary predicate branch could open a new avenue for developing additional pretraining losses using uL(xt, yτ ) (i.e., the token relations). For example, we may randomly swap two tokens xt and xτ and use uL(xt, xτ ) to predict whether they have been swapped or not. We will leave the development of more effective pretraining losses for FOLNet as a future work.\n\nThe input base atoms in B and their logit vector representations Throughout our work, we only consider plain texts as the input, which is similar to other pretrained language models. Therefore, the base atoms at the input should only encode the information that is directly available from the plain text, which include the token ids in the input sequence (denoted by TokenIDv(xt)) and the relative distance between any of the two tokens (denoted by RelDistd(xt, xτ )). Specifically, TokenIDv(xt) = T if the token xt takes the v-th id in the vocabulary for v = 1, . . . , V , and TokenIDv(xt) = F otherwise. When the input sequence consists of a pair of sequences, we will construct the input sequence as:\n\n“[CLS] Sequence #0 [SEP] Sequence #1 [SEP] [PAD] . . . [PAD]”,\n\nwhich is similar to the input format used in BERT and other transformer models. In this case, we will introduce an additional base atom SeqIDs(xt) to characterize whether a token xt belongs to the s-th sequence (s = 0, 1); that is, SeqIDs(xt) = T if token xt belongs to the s-th sequence and is F otherwise. This atom is akin to the segment ids (or token type ids) in existing pretrained transformer models. Furthermore, the relative positional atom RelDistd(xt, xτ ) = T if d = dist∆(xt, xτ ) and is F otherwise, where dist∆(xt, xτ ) is a clipped distance function with a clipping parameter ∆:\n\ndist∆(xt, xτ ) =\n\n\n\n \n\nmax(1−∆, min(τ −t, ∆−1)) ∆ + 1 ∆\n−∆ 0\n\nt > 0, τ > 0 and SeqIDs(xt) = SeqIDs(xτ ) t > 0, τ > 0 and SeqIDs(xt) (cid:54)= SeqIDs(xτ ) t = 0, τ > 0 t > 0, τ = 0 t = 0, τ = 0\n\n.\n\nThe above clipped distance function clamps the relative distance to be within [−∆ + 1, ∆ − 1] when the two tokens are from the same sequence. And it also assigns a special distance id whenever they\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nbelong to two different sequences. In other words, the distance between any two tokens would be the same if they belong to two separate sequences. Likewise, we also characterize the distances between the [CLS] token (t = 0 or τ = 0) and the other tokens (τ > 0 or t > 0) with two special distance ids, which sets all the regular tokens to have the same distance towards (or from) the special [CLS] token. Such relative positional encoding (atom) preserves the translational invariance of the text sequence and generalizes better than the absolute positional encodings used in BERT and RoBERTa. Notably, it allows FOLNet to generalize to much longer sequences that are unseen during pretraining. In summary, we consider the following base atoms as the inputs to the FOLNet model:\n\nB = {TokenIDv(xt), SeqIDs(xt), RelDistd(xt, xτ )|t, τ = 1, . . . , T }. By stacking TokenIDv(xt) and SeqIDs(xt) over v and s, stacking RelDistd(xt, xτ ) over d, and using the values of 1 and 0 to represent T and F, respectively, we will have the 0-1 vectors to represent all the input atoms in B. Then, we may further convert them into dense vector representations {u0(xt), u0(xt, xτ )}t,τ via embedding lookup. The embedding lookup process can also be understood as finding a set of more fine-grained learnable properties (or relations) to characterize a given token xt (or a token-pair (xt, xτ )), where these properties (or relations) are represented in the same logit space as the neural logic operators in Table 2, which carry out Modus Ponens inferences. Finally, the above base atoms are just one particular design for our FOLNet model, and there could be other alternatives for encoding the input information. And when there is extra information available besides plain texts, we may also encode it as the new unary or binary base atoms in B.\n\n(10)\n\nA.5 EXTENDING FOLNET TO TEXT-TO-TEXT VERSIONS\n\nSo far, we have mainly focused on the encoder-only version of the FOLNet model. We now show that it can be extended to the decoder-only and the encoder-decoder counterparts in a relatively straightforward manner. Such extension would be useful for text-to-text generation tasks.\n\nDecoder-only To develop the decoder-only version of FOLNet, we need to let the model autoregressively generate the output tokens. In the context of FOLNet, this requires the unary and binary atoms {u(xt), u(xt, xτ )} to be inferred only from the premises in the past: {v(xω), v(xω, xν) : ω ≤ t}. In addition, we further restrict the binary atoms to have a causal pattern, i.e., u(xt, xτ ) = 0 and u(xω, xν) = 0, whenever t < τ and ω < ν. Figure 4(a) illustrates these patterns for the unary and the binary atoms. Note that the causal structure for the binary atoms translates into a lower triangular pattern for the nonzeros. In particular, the dark blue atoms are directly responsible for the generation of the next token (word), and the auto-regressive generation requires the model to update them only from the light blue atoms. We enforce such auto-regressive property by multiplying a proper 0-1 mask to the binary kernels and premises in the neural logic operators. In the last column of Table 5, we list the kernels and the premises that should be masked, and in Figure 4(b), we show the masks that correspond to two variants of decoder-only models: (i) Causal Langauge Model (CausalLM) (Radford et al., 2018; 2019; Brown et al., 2020; Chowdhery et al., 2022), and (ii) Prefix Langauge Model (PrefixLM) (Liu et al., 2018; Raffel et al., 2020). The CausalLM has a lower triangular mask for the binary atoms so that the information pattern is always unidirectional. On the other hand, the PrefixLM will have a bi-directional mask for the input segment (the top-left part in Figure 4(b)) and a unidirectional mask for the output (target) segment (the bottom-right part in Figure 4(b)). Accordingly, the binary atoms of the PrefixLM version will share a similar pattern as its mask in Figure 4(b), which models the relatons for the prefix and the output segments separately. With such simple modifications, our decoder-only FOLNet model will have the same input-output interface as the decoder-only transformers; it can be pretrained to predict the next tokens using a linear classifier over the unary atoms uL(xt). After the training, it can generate tokens in an auto-regressive manner.\n\nEncoder-Decoder For the encoder-decoder variant, we use two separate stacks of FOLNet for the encoder and decoder, where each of them has the same overall architecture as in Figure 1 with its own set of atoms (the green and blue blocks in Figure 4(c)). In particular, the encoder will be identical to the encoder-only version that we have thoroughly discussed earlier in the main paper. Meanwhile, the decoder part will be similar to the decoder-only variant with a few additional modifications. First, the decoder needs to maintain a slightly different version of binary atoms (Figure 4(c)). Specifically, besides the relations between the output tokens, the decoder also has to model the (unidirectional) relations from the input tokens to the output tokens (i.e., the bottom-left part of uh(x, y) in Figure 4(c)). These relations are crucial in deducing the output tokens from the input ones, which plays a\n\n18\n\nPublished as a conference paper at ICLR 2023\n\n(a) Atoms for decoder-only FOLNet\n\n(b) Masks for decoder-only FOLNet\n\n(c) Atoms for encoder-decoder FOLNet\n\n(d) Masks for encoder-decoder FOLNet\n\nFigure 4: Extending FOLNet to text-to-text versions. The top row illustrates the atoms and the masking strategy for the decoder-only version, and the bottom row shows the encoder-decoder variant. On the left side, we visualize the unary and the binary atoms {ud(x), uh(x, y)} for these cases, where d = (h − 1)S + s. The green and blue colors characterize the nonzero patterns for the atoms of the encoder and the decoder, respectively. The green-colored atoms are associated with the encoder part in the encoder-decoder variant. The dark blue color represents the atoms that are directly responsible for generating the next token in the decoder. Notably, they are deduced only from the light blue atoms, so that the entire generation process retains an auto-regressive nature. On the right side, the white and the orange blocks represent the 0-1 masking positions, where the light orange part are associated with the prefix segment in PrefixLM. Likewise, the encoder-decoder variant has separate sets of masks for the encoder and the decoder, respectively, where the mask for the encoder part are designed to have a bi-directional information pattern. Similar design also holds for the prefix part in PrefixLM.\n\nSym. Ops.\n\nTyping\n\nB-dim. R-dim. Neural operator in logit space\n\nMask\n\nb c\nj m\na p\nt\n\nbool cjoin join mu assoc prod trans\n\nU ← U × U U ← U × B U ← B × U U ← B × B B ← U × U B ← U × B B ← B × B\n\nx h\nh x\nh x\nh\n\nw a\na a\nw w\na\n\nuhs(x) = (cid:80) uhs(x) = (cid:80) uhs(x) = (cid:80) uhs(x) = (cid:80) uh(x, y) = (cid:80) uh(x, y) = (cid:80) uh(x, y) = (cid:80)\n\nw Khw(x)vws(x) a Khs(a)vh(x, a) a Kh(x, a)vhs(a) a Kh(x, a)vs(x, a) w Khw(x)vhw(y) w Khw(x)vw(x, y) a Kh(x, a)vh(a, y) Kh(x, a), vh(a, y)\n\n- vh(x, a) Kh(x, a) Kh(x, a), vs(x, a) -\nvw(x, y)\n\nTable 5: The binary kernels and premises to be masked for decoder-only versions of FOLNet. In the encoder-decoder variant, similar part of the kernels and premises would be masked in its decoder.\n\nsimilar role as the cross-attention scores in transformers. Accordingly, we also need to adjust the masks to handle these relations separately (see Figure 4(d)). Second, the neural logic operators in Table 5 should also be slightly adjusted in order to incorporate the additional premise atoms from the encoder output. For example, the premise vhs(a) in join-operator should now be a concatenation of the unary atoms from the encoder output (with a linear projection) and the decoder premises. Likewise, the premise vhw(y) in assoc-operator should also be a concatenation of the encoder output (with a linear projection) and the decoder premises. These two modified operators share a similar spirit as the self-attention and the cross-attention mechanisms in transformer decoders. The cjoin operator is similar to join operator, except now the concatenation happens at the kernel Khs(a) (with\n\n19\n\nCausal Language ModelPrefix Language ModelEncoder-DecoderinputtargetinputtargetinputtargetCausal Language ModelPrefix Language ModelEncoder-DecoderinputtargetinputtargetinputtargetCausal Language ModelPrefix Language ModelEncoder-DecoderinputtargetinputtargetinputtargetCausal Language ModelPrefix Language ModelEncoder-DecoderinputtargetinputtargetinputtargetPublished as a conference paper at ICLR 2023\n\na linear projection). The mu-operator and the prod-operator stay the same as before. Meanwhile, the trans-operator for the decoder needs to be implemented according to the following new expression:\n\n(cid:26)(cid:80) (cid:80)\n\nuh(x, y) =\n\na∈T Kh(x, a)vD a∈I Kh(x, a)vE\n\nh (a, y) h (a, y) + (cid:80) where T and I are defined to be the target and the input sequences, respectively, the kernel Kh(x, a) are obtained by applying a linear projection to the binary atoms u(x, a) in the decoder, and the superscript E or D in the premises vh(a, y) denotes whether they are from the encoder or the decoder. Notably, we observe that the encoder-decoder version of FOLNet retains the dual-branch architecture in its decoder module as well. This is in sharp contrast to the standard transformer decoder, which is a single-branch architecture with only unary atoms on its main pathway.\n\na∈T Kh(x, a)vD\n\ny ∈ T y ∈ I\n\nh (a, y)\n\nB EXPERIMENTAL DETAILS\n\nB.1 OVERVIEW OF THE DOWNSTREAM TASKS\n\nGLUE The GLUE benchmark (Wang et al., 2019) consists of 9 tasks: MNLI (Williams et al., 2018), QQP (Iyer et al., 2017), QNLI (Rajpurkar et al., 2016a), SST-2 (Socher et al., 2013), CoLA (Warstadt et al., 2019), STS-B (Cer et al., 2017), MRPC (Dolan & Brockett, 2005), RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2012). They cover a wide range of natural language understanding tasks such as natural language inference, paraphrasing, linguistic acceptability, and sentiment analysis. We finetune our FOLNet models by following the same procedures from BERT (Devlin et al., 2018). We do not evaluate our model on WNLI because it generally needs special procedures. The basic description of all the tasks in GLUE (including their evaluation metrics) can be found in Table 6.\n\nSQuAD 2.0 SQuAD 2.0 (Rajpurkar et al., 2016b) is an extractive question answering dataset built from Wikipedia. The objective of the task is to predict an answer span from the context paragraphs. SQuAD 2.0 is an updated version that adds additional 50,000 unanswerable questions to the original SQuAD 1.1 version. The performance metrics are exact match (EM) and F1 scores.\n\nFOLIO FOLIO (Han et al., 2022) is a natural language reasoning dataset that contains first-order logic reasoning problems. It requires the models to decide whether a conclusion statement is correct or not given a world defined by a set of premises. It is formulated as a 3-class classification problem with the labels being “True”, “False”, “Unknown”. We follow the same procedure as Han et al. (2022) by formulating the problem as a sequence pair classification problem. Specifically, we concatenate all the the premises into one sequence (i.e., sequence A), and then further concatenate it with the conclusion (i.e., sequence B), where the two sequences are separated by a [SEP]. In addition, we add a [CLS] token at the beginning and a [SEP] in the end before padding (in the end). By doing so, the task has the same format as a natural language inference task (e.g., MNLI). Table 7 (quoted from the original FOLIO paper (Han et al., 2022)) shows an example from the FOLIO dataset, which demonstrate that it requires strong first-order reasoning capabilities to solve the problem. The dataset has an official train/validation/test split with 1,004/204/227 examples, respectively. By the time of this submission, the test set is not available and thus we only report performance on the validation set.\n\nMNLI\n\nQQP\n\nQNLI\n\nSST-2\n\nCoLA\n\nRTE\n\nMRPC\n\nSTS-B\n\n393K Inference\n\nSize Task Metric(s) Accuracy Accuracy/F1 #Classes\n\n364K Similarity\n\n3\n\n2\n\n108K QA/Inference Accuracy 2\n\n3.7K 67K Sentiment Paraphrase Accuracy Matthews corr. Accuracy Accuracy/F1\n\n8.5K Acceptability\n\n2.5K Inference\n\n2\n\n2\n\n2\n\n2\n\n5.7K Similarity Pearson/Spearman. 1 (regression)\n\nTable 6: Basic information about different tasks in GLUE benchmark.\n\nB.2\n\nIMPLEMENTATION DETAILS AND HYPER-PARAMETERS\n\nWe implement both our pretraining and finetuning pipelines using PyTorch (Paszke et al., 2019) and automatic mixed precision (AMP) learning (Micikevicius et al., 2018) based on the Apex library (Nvidia, 2019). For pretraining, we use large-batch training (with a batch-size 131K) using LAMB\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nA FOLIO example based on the Wild Turkey Wikipedia page\n\nNL premises 1. There are six types of wild turkeys: Eastern wild turkey, Osceola wild turkey, Gould’s wild turkey, A. Tom is an Ocellated wild turkey. -> True Merriam’s wild turkey, Rio Grande wild turkey, and the Ocellated wild turkey. 2. Tom is not an Eastern wild turkey. 3. Tom is not an Osceola wild turkey. 4. Tom is also not a Gould’s wild turkey, or a Merriam’s wild turkey, or a Rio Grande wild turkey. 5. Tom is a wild turkey.\n\nB. Tom is an Eastern wild turkey. -> False C. Joey is a wild turkey. -> Unknown\n\nNL Conclusion -> Labels\n\nFOL Premises 1. ∀x(WildTurkey(x) → (Eastern(x) ∨ Osceola(x) ∨ Goulds(x) ∨ Merriams(x) ∨ Riogrande(x) ∨ Ocellated(x))) 2. ¬(WildTurkey(tom) ∧ Eastern(tom)) 3. ¬(WildTurkey(tom) ∧ Osceola(tom)) 4. WildTurkey(tom) → ¬(Goulds(tom) ∨ Merriams(tom) ∨ Riogrande(tom)) 5. WildTurkey(tom)\n\nFOL conclusion -> Labels A. Ocellated(tom) -> True B. Eastern(tom) -> False C. WildTurkey(joey) -> Unknown\n\nTable 7: We show an example from the FOLIO dataset, which is quoted directly from the original FOLIO paper (Han et al., 2022). It demonstrates that it requires strong first-order reasoning capabilities to solve the problem. “NL” stands for “natural language” and “FOL” stands for “First-Order Logic”. We only use the natural language part to solve the task in our experiments.\n\noptimizer (You et al., 2020). The pretraining of FOLNetBase on Wikipedia + BookCorpus (16GB) for 8K steps takes about 12 hours using 512 V100 GPUs. The pretraining of FOLNet Base on 160GB data for 128K steps takes 7 days using 512 V100 GPUs. And the pretraining of FOLNetLarge on 160GB data for 128K steps takes 19 days using 512 V100 GPUs. For the finetuning of all downstream tasks, we also use AMP learning based on Apex, and the optimizers are FusedAdam from Apex library.\n\nWe report the hyper-parameters of pretraining FOLNet in Table 8. The hypper-parameters for finetuning different downstream tasks are included in Table 9. The hyper-parameters for the finetuning tasks are searched per task, and the results are the median of five random seeds.\n\nHyperparams\n\nFOLNetBase\n\nFOLNetBase\n\nFOLNetLarge\n\nPretraining data size Number of Layers (L) Unary Hidden size (D1) Binary Hidden size (D2) Unary Boolean (FFN) intermediate size Binary Boolean (FFN) intermediate size Number of Attention heads (H) Attention head size (S) RPE clipping parameter (∆) Dropout rate Attention Dropout rate Warmup Ratio Peak Learning Rate Batch Size Weight Decay Max Steps Learning rate Decay LAMB (cid:15) LAMB β1 LAMB β2 Gradient Clipping Sequence length (T )\n\n16G 12 768 64 3072 256 12 64 64 0.1 0.1 1% 1e-2 131,072 0.01 8K Linear 1e-6 0.9 0.999 0.0 128\n\n160G 12 768 64 3072 256 12 64 64 0.1 0.1 1% 2e-3 131,072 0.01 128K Linear 1e-6 0.9 0.999 0.0 128\n\n160G 24 1024 64 4096 256 16 64 64 0.1 0.1 1% 1.6e-3 131,072 0.01 128K Linear 1e-6 0.9 0.999 0.0 128\n\nTable 8: The hyper-parameters for pretraining FOLNet in different settings.\n\nC ADDITIONAL EXPERIMENTS\n\nZero-shot performance on GLUE We evaluate the zero-shot performance of our FOLNet model on GLUE benchmark. Specifically, we perform zeros-shot predictions by using the same method and prompt templates from Gao et al. (2021). We only consider the FOLNetLarge model and compare it to RoBERTaLarge, which are similar in model-size, pretraining dataset and pretraining losses. In\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nHyperparams\n\nGLUE-small/big\n\nSQuAD 2.0\n\nFOLIO\n\nMax epochs Peak lr for Base (16G): Peak lr for Base/Large (160G): Batch size Learning rate decay Warmup ratio Sequence length Adam (cid:15) Adam β1 Adam β2 Gradient clipping Dropout rate Weight decay\n\n{5, 10, 20} / {2, 3, 5} {6e-5, 8e-5 1e-4, 2e-4} {1e-5, 2e-5, 3e-5, 4e-5} {16, 32} / {32} Linear {6%, 25%} / {6%} 128 1e-6 0.9 0.999 0.0 0.1 0.01\n\n{2, 3} { 8e-5, 9e-5, 1e-4 } {1e-5, 2e-5, 3e-5, 4e-5} {16, 32} Linear 6% 384 1e-6 0.9 0.999 0.0 0.1 0.01\n\n{60, 80} { 6e-5, 7e-5, 8e-5} { 1e-5, 2e-5, 3e-5} {16, 32} Linear {6%, 25%} 512 1e-6 0.9 0.999 0.0 0.1 0.01\n\nTable 9: The hyperparameters for finetuning on GLUE, SQuAD 2.0, and FOLIO tasks. GLUE-small refers to CoLA, STS-B, MRPC and RTE. GLUE-big stands for MNLI, QQP, QNLI and SST-2.\n\nModel\n\nMethod Data\n\nMNLI-m/mm QQP QNLI SST-2 CoLA STS-B MRPC RTE Acc\n\nAcc mcc\n\nPear.\n\nAcc\n\nAcc\n\nF1\n\nF1\n\nAvg\n\nMajority guess\n\n-\n\n-\n\nRoBERTaLarge MLM 160G MLM 160G FOLNetLarge\n\n32.7/33.0\n\n50.8/51.7 50.8/52.6\n\n0.0\n\n49.7 55.4\n\n49.5\n\n50.8 59.7\n\n50.9\n\n83.6 79.5\n\n0.0\n\n2.0 6.4\n\n-\n\n-3.2 23.6\n\n81.2\n\n61.9 77.2\n\n52.7 33.3\n\n51.3 44.3 58.2 51.5\n\nTable 10: The zero-shot performance on the GLUE development set. “Acc” stands for accuracy, “mcc” means Matthews’s correlation coefficient, and “Pear.” is short for Pearson’s correlation coefficient.\n\naddition, the zero-shot prediction methods are also similar: they both predict the label words using their own MLM heads. The results are summarized in Table 10, which demonstrates that FOLNetLarge outperforms RoBERTaLarge on 7 out of 8 tasks with average gain of 7.2 points.\n\nPerformance on CLUTRR To further examine the reasoning capabilities, we evaluate our FOLNet models on the CLUTRR (Compositional Language Understanding and Text-based Relational Reasoning) dataset (Sinha et al., 2019). CLUTRR is a semi-synthetic diagnostic benchmark designed to evaluate the systematic generalization ability of a model. Specifically, for a given natural language story, a model is asked to infer the (implicit) relationship between two family members. To solve the problem, it has to extract relationships between entities and master the logical rules governing these relationships. CLUTRR examines the systematic generalization by testing a model on stories that contain unseen combinations of logical rules as well as stories that require more reasoning steps. Following the same setting as Sinha et al. (2019), we first finetune our FOLNet models with clauses of length k = 2, 3 and k = 2, 3, 4, respectively, and then we evaluate them on clauses of length k = 2, . . . , 10. Specifically, for each input instance, we concatenate the natural language story with the text query (separated by a [SEP] token), and cast the problem as a sequence-pair classification. In addition, we add a [CLS] token at the beginning and append a [SEP] token in the end. However, unlike Sinha et al. (2019), for simplicity, we do not replace entities with special task-specific embeddings but treat them as regular English words. And we leave such entity representation techniques as a future work, which may further improve our performance. We finetune FOLNetBase for 100 epochs using an Adam optimizer with a learning rate of 5 × 10−5, a batch-size of 16 and a warmup ratio of 6%. In our experiment, we mainly compare to transformer-based baselines that also use the natural language inputs. Specifically, we compare to the results of BERTBase and BERT-LSTM from Sinha et al. (2019), which share the same model size and pretraining corpus. There is also a rich set of approaches that work directly on the symbolic inputs from CLUTRR, such as the Graph Attention Network (GAT) (Veliˇckovi ́c et al., 2018). Since these methods directly use the structured logical graph underlying the story as their input, they circumvent the difficulty of parsing natural language stories and generally perform much better than the text-based counterparts. We include GAT as a reference to examine whether our FOLNet model could narrow such a performance gap with the help of our logical inductive bias. The full results are reported in Figure 5. First, our FOLNetBase performs\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n(a) Training clause length k = 2, 3\n\n(b) Training clause length k = 2, 3, 4\n\nFigure 5: Systematic generalization performance on CLUTRR benchmark.\n\nsignificantly better than both text-based methods (i.e., BERTBase and BERT-LSTM) by a large margin. In particular, when the testing k is out of the training set, we improve BERTBase by as much as 8% ∼ 41% (in absolute improvement). Meanwhile, this advantage further improves to 54% ∼ 81% when the testing k has been seen during training, and FOLNetBase achieves competitive performance as GAT. Notably, FOLNetBase even achieves substantially better performance (10% ∼ 20%) than GAT when the testing k = 10. The results confirms the benefit of encoding logical inductive bias.\n\nD NEURAL MODUS PONENS\n\nIn this section, we show that generic Modus Ponens inference using clauses of the form (8) can be expressed as a matrix multiplication followed by an optional nonlinear activation function.\n\nSuppose P1, . . . , PM are M premise atoms and Q is the head (i.e., conclusion) atom. We partition the set M = {1, . . . , M } into three non-overlapping sets M+, M0 and M−, and define the following general clause (where we have dropped the subscript n in (8) for simplicity of notation):\n\n(cid:18) (cid:94)\n\nQ ←\n\nPm\n\n(cid:19) (cid:94) (cid:18) (cid:94)\n\n(cid:19) .\n\n¬Pm\n\nm∈M+\n\nm∈M−\n\n(11)\n\nNote that the expression (11) can be used to represent a general clause that infers Q from any subset of the premises in {P1, . . . , PM , ¬P1, . . . , ¬PM }, where the sets M+ and M− contain the indexes of the selected original and negated premises, respectively, and the set M0 indexes the ignored premises. Therefore, each particular partition M = M+ ∪ M0 ∪ M− also corresponds to a unique clause. To apply Modus Ponens inference with the clause (11), we can proceed in two steps: (i) evaluate the body of the clause (i.e., the right-hand side of (11)) according to (cid:18) (cid:94)\n\n(cid:19) (cid:94) (cid:18) (cid:94)\n\nP ≡\n\nPm\n\n(cid:19) ,\n\n¬Pm\n\n(12)\n\nm∈M+\n\nm∈M−\n\nand (ii) apply the generic Modus Ponens inference rule: Q ⇐ {Q ← P and P }. In the following two subsections, we will derive the neural operations that implement these two steps, respectively. Specifically, the neural operations will be carried out in the logit space under the ProbLog setting.\n\nD.1 THE MATRIX MULTIPLICATION FOR THE COMPOSITIONS OF THE BODY ATOMS\n\nTo derive the operations for the logic expression (12), we define the probabilities of the atoms P, P1, . . . , PM being true in the following logistic form:\n\nPr{P = T} =\n\n1\n\n1 + e−z ,\n\nPr{Pm = T} =\n\n1 1 + e−vm\n\n, m = 1, . . . , M,\n\n(13)\n\n23\n\n2345678910Relation Length020406080100Accuracy (%)BERTBERT-LSTMGATFOLNetBase2345678910Relation Length020406080100Accuracy (%)BERTBERT-LSTMGATFOLNetBase2345678910Relation Length020406080100Accuracy (%)BERTBERT-LSTMGATFOLNetBase2345678910Relation Length020406080100Accuracy (%)BERTBERT-LSTMGATFOLNetBasePublished as a conference paper at ICLR 2023\n\nwhere z and vm are the logits corresponding to the atoms P and Pm, respectively. In addition, for each m, we further introduce a variable Wm = +1, 0, −1 to indicate whether m is in M+, M0 and M−, respectively. Then, (12) can be characterized by the following conditional probability: (cid:2)(Wm = 1∧Pm = T)∨(Wm = −1∧Pm = F)∨(Wm = 0)(cid:3)\n\n(cid:40)\n\nPr{P = T|W, P} =\n\n, (14)\n\n1 (cid:86)M m=1 0 otherwise\n\nwhere W (cid:44) {W1, . . . , WM } and P (cid:44) {P1, . . . , PM }. We further assume that the variables P1, . . . , PM are independent of each other and are also independent of W1, . . . , WM . In addition, for a given set of W = {W1, . . . , WM }, we introduce the following random event of P1, . . . , PM :\n\n(cid:26)\n\nE (cid:44)\n\n(P1, . . . , PM ) :\n\nM (cid:94)\n\nm=1\n\n(cid:2)(Wm = 1 ∧ Pm = T) ∨ (Wm = −1 ∧ Pm = F) ∨ (Wm = 0)(cid:3)\n\n(cid:27)\n\n,\n\n(15)\n\nalong with its indicator function I((P1, . . . , PM ) ∈ E). An indicator function I(·) is one if the expression inside its parenthesis is true and zero otherwise. Then, we can derive Pr{P = T|W} as:\n\nPr{P = T|W} =\n\n(cid:88)\n\nPr{P = T|W, P}Pr{P1, . . . , PM |W}\n\n(a) =\n\n(b) =\n\nP (cid:88)\n\nP (cid:88)\n\nP\n\nPr{P = T|W, P}Pr{P1, . . . , PM }\n\nI(cid:0)(P1, . . . , PM ) ∈ E(cid:1)Pr{P1, . . . , PM }\n\n(cid:88)\n\n(c) =\n\nI(cid:0)(P1, . . . , PM ) ∈ E(cid:1)\n\nP\n\nM (cid:89)\n\nm=1\n\nPr{Pm}\n\n(cid:88)\n\n(cid:89)\n\nPr{Pm}\n\n(cid:89)\n\nPr{Pm}\n\n(cid:89)\n\nPr{Pm}\n\n(P1,...,PM )∈E\n\nm∈M+\n\nm∈M−\n\nm∈M0\n\n(cid:88)\n\n(cid:89)\n\nPr{Pm = T}\n\n(P1,...,PM )∈E\n\nm∈M+\n\n(cid:89)\n\nm∈M−\n\nPr{Pm = F}\n\nPr{Pm = T}\n\nPr{Pm = T}\n\n(cid:89)\n\nm∈M−\n\n(cid:89)\n\nm∈M−\n\nPr{Pm = F}\n\n(cid:88)\n\n{Pm: m∈M0}\n\nm∈M0\n\nPr{Pm = F}\n\n(cid:89)\n\nm∈M0\n\n(cid:89)\n\nPr{Pm}\n\nPr{Pm}\n\n(d) =\n\n(e) =\n\n(f ) =\n\n(g) =\n\n(h) =\n\n(cid:89)\n\nm∈M+\n\n(cid:89)\n\nm∈M+\n\n(cid:89)\n\nm∈M+\n\nM (cid:89)\n\n(i) =\n\nm=1\n\nM (cid:89)\n\nm=1\n\n=\n\n(j) ≤\n\n1 1 + e−vm\n\n(cid:89)\n\nm∈M−\n\n1 1 + evm\n\n1 (1 + e−vm )I +\n\nm\n\nM (cid:89)\n\nm=1\n\n1 (1 + evm )I −\n\nm\n\n1 (1 + e−vm )I +\n\nm\n\n·\n\n1 (1 + evm)I −\n\nm\n\n1 m=1(I +\n\n1 + e− (cid:80)M\n\nm−I −\n\nm)vm\n\n(16)\n\nwhere step (a) uses the independence between P1, . . . , PM and W1, . . . , WM , step (b) substitutes the definition of the conditional probability (14), step (c) uses the assumption that P1, . . . , PM are independent of each other, step (d) substitutes the decomposition M = M+ ∩ M0 ∩ M−, step (e) is obtained by following the definition (15) for the event E (i.e., within event E, we have Pm = T for m ∈ M+, Pm = F for m ∈ M− and Pm being arbitrary value in {T, F}), step (f) takes out the common factors of the summands and keeps the remaining summation over all possible values in {Pm : m ∈ M0} (based on the definition (15)), step (g) uses the fact that the total probability of the event {Pm : m ∈ M0} is one, step (h) substitutes the second logistic expression in (13), step\n\n24\n\nPublished as a conference paper at ICLR 2023\n\n(cid:44) I(m ∈ M+) and I −\n\n(cid:44) I(m ∈ M−), and the upper bound in step (j) (i) introduces variables I + m\nm is obtained by expanding the multiplications in the denominator and dropping all the cross-terms, all of which are nonnegative. Note that the upper bound (16) is tight when only one unique I + m or m (among all I + I − M ) is nonzero. The above result in (16) is conditioned on a particular realization of W1, . . . , WM , which are discrete variables. Therefore, it is difficult to learn them directly in a differentiable manner. To address this issue, we further assume that W1, . . . , WM are random variables. Then, taking expectation over W1, . . . , WM , we have:\n\n1 , . . . , I −\n\n1 , . . . , I +\n\nM and I −\n\nPr{P = T} ≤ E\n\n(cid:26)\n\n1 m=1(I +\n\n1 + e− (cid:80)M\n\nm−I −\n\nm)vm\n\n(cid:27)\n\n,\n\n(17)\n\nwhere the randomness inside the expectation comes from I + m, which are further from W1, . . . , WM . We now adopt a simple yet effective strategy to approximate the right-hand side in order to obtain a differentiable implementation. Specifically, we use the following approximation:\n\nm and I −\n\n(cid:20)\n\nE\n\n1 1 + eX\n\n(cid:21)\n\n≈\n\n1\n\n1 + eEX ,\n\n(18)\n\nwhich becomes more accurate when the distribution of X gets more concentrated around a single peak (i.e., becoming determinisitic). Using the above approximation, we obtain\n\nPr{P = T} ≈\n\n1 m=1(κ+\n\n1 + e− (cid:80)M\n\nm−κ−\n\nm)vm\n\n(19)\n\n(cid:44) E[I +\n\nwhere κ+ m] = Pr{Wm = −1}. Substituting the first m\nexpression in (13) into the left-hand side of (19), we conclude that the logit of Pr{P = T} can be approximated via:\n\nm] = Pr{Wm = +1} and κ−\n\n(cid:44) E[I −\n\nm\n\nz =\n\nM (cid:88)\n\nm=1\n\n(κ+\n\nm − κ−\n\nm)vm = (cid:104)κ, v(cid:105)\n\n(20)\n\nm − κ−\n\nwhere κ and v denote the vectors that collect κ+ m and vm as their m-th elements, respectively. The above expression implies that the logit z for the atom P can be computed by a simple inner product operation between the two vectors κ and v. Further recall that different realizations of W1, . . . , WM correspond to different partitions of M = M+ ∪ M0 ∪ M−, which further defines different logic expressions for P in (12). Since κ is a vector that collects Pr{Wm = +1} − Pr{Wm = −1} as its m-th element, it can also be viewed as a signature vector of the logic expression (12), which is the body of the clause (11). Therefore, κ is also the signature vector for the clause in (11) as it determines the logic compositions among the body atoms. When we have multiple logic expressions, represented by κ1, . . . , κN , that compose N logic expressions from the atoms P1, . . . , PM , then we can compute the logits of the output logic expressions by the following matrix multiplication:\n\nz = Kv,\n\n(21)\n\nwhere z is a logit vector for the N output atoms, and K is a matrix consists of κn as its n-th row.\n\nD.2 THE NONLINEAR ACTIVATION FOR THE IMPLICATION OPERATION\n\nIn this subsection, we proceed to derive the the neural operator for generic Modus Ponens inference, Q ⇐ {Q ← P and P }, in the logit space. Likewise, we consider the ProbLog setting, where the atoms P and Q will be assigned with probabilities Pr{P = T} and Pr{Q = T}, respectively, which characterize the chances of them being true. Let C (cid:44) (Q ← P ) be the clause. Our objective is to infer the outcome probability Pr{Q = T} from Pr{P = T} based on the fact that C = T.\n\nTo this end, we first derive the conditional probability Pr{Q = T|P, C = T}. We will first need to establish the conditional probability Pr{C = T|P, Q} based on the definition of the logical implication “←” (Andrews, 2013) and then apply Bayes rule. Note that C = (Q ← P ) is defined as C (cid:44) (Q ∨ ¬P ), which can be expressed as the following conditional probability:\n\nPr{C = T|P, Q} =\n\n(cid:26)0 1\n\nif P = T and Q = F otherwise\n\n.\n\n(22)\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nThat is, the clause C will be false only when the premise P is true and the conclusion Q is false. To proceed, we further assume that Pr{Q = T|P } = Pr{Q = F|P } = 1/2. The intuition of the assumption is that we do not have any prior knowledge about the outcome Q when we are only given the input premise P (without C). Then, by Bayes rule, we have\n\nPr{Q = T|P, C = T} =\n\nPr{Q = T, C = T|P } Pr{C = T|P }\n\n=\n\n(a) =\n\n(b) =\n\nPr{Q = T|P }Pr{C = T|P, Q = T} (cid:80) Q∈{F,T} Pr{Q|P }Pr{C = T|P, Q}\n\nPr{C = T|P, Q = T} Pr{C = T|P, Q = T} + Pr{C = T|P, Q = F} (cid:26)1\n\nif P = T if P = F\n\n,\n\n0.5\n\n(23)\n\nwhere steps (a) and (b) substitute Pr{Q = T|P } = Pr{Q = F|P } = 1/2 and (22), respectively.\n\nNext, we derive Pr{Q = T|C = T} with the assumption that the premise P is independent of the clause C that is used in the current Modus Ponens inference step. We have\n\nPr{Q = T|C = T} =\n\n(cid:88)\n\nPr{Q = T|P, C = T}Pr{P |C = T}\n\nP ∈{T,F}\n\n(a) =\n\n(cid:88)\n\nP ∈{T,F}\n\nPr{Q = T|P, C = T}Pr{P }\n\n(b) = Pr{P = T} +\n\n1 2\n\nPr{P = F}\n\n=\n\n1 2\n\n+\n\n1 2\n\nPr{P = T},\n\n(24)\n\nwhere step (a) uses the assumption that P is independent of C, and step (b) substitutes (23).\n\nFinally, we derive the expression for the conditional probability (24) in the logit space. Specifically, let q and p be the logits for Q and P , respectively, which parameterize their probabilities:\n\nPr{P = T} =\n\n1\n\n1 + e−z ,\n\nPr{Q = T|C = T} =\n\n1\n\n1 + e−u .\n\nSubstituting the first expression in (25) into (24) followed by some simple algebra, we obtain\n\nPr{Q = T|C = T} =\n\n1 1 + e− ln(1+2ez)\n\n.\n\nComparing the right-hand side of (26) with that of (25), we obtain the logit for P as\n\nu = ln(1 + 2ez).\n\n(25)\n\n(26)\n\n(27)\n\nvThe above expression implies that, to implement Modus Ponens in logit space, we only need to apply the above simple nonlinear activation function to the input premise logit p. To gain further insights into the above logit-space Modus Ponens inference, we carry out further analysis of the above nonlinear activation function. Note that ln(1 + ex) is a non-negative convex function. By using Jensen’s inequality and the non-negativity, we can prove that the above nonlinear activation function can be lower bounded as ln(1 + 2ex) ≥ ReLU(x + ln 2), where the right-hand side is indeed a good approximation of the left-hand side. Therefore, we can implement the generic Modus Ponens by applying the (shifted) ReLU function in the logit space with much lower computation complexity.\n\nD.3 PUTTING EVERYTHING TOGETHER\n\nGiven N different clauses of the form (11) and M input premises P1, . . . , PM , the deduction of the N outcome atoms Q1, . . . , QN using Modus Ponens rule can be implemented (approximately) via\n\nz = Kv\n\n26\n\n(28)\n\nPublished as a conference paper at ICLR 2023\n\nu = ln(1 + 2ez),\n\n(29)\n\nwhere K is an N × M matrix with its n-th row being the signature vector of the n-th clause, v is an M -dimensional vector consisting of the logits for the input premises P1, . . . , PM , u is an N -dimensional vector that contains the logits of the output atoms Q1, . . . , QN . Note that the Modus Ponens inference using N clauses can be implemented in parallel by first multiply matrix K to the left of the logit vector v and then pass through a special element-wise nonlinear activation function (which can be approximated by a shifted ReLU function). For this reason, we call K the kernel-of-clauses (or kernels for short) in this paper. Furthermore, we note that, when the implication “←” in (11) is replaced by the logical equivalence “≡”, the original clause (11) becomes (12), so that the activation function in (29) can be dropped. Finally, it is straightforward to show that the neural Modus Ponens inference (28) for the four categories of rules in (7) can be expressed (equivalently) as:\n\nu(x) =\n\nu(x) =\n\nu(x, y) =\n\nu(x, y) =\n\n(cid:88)\n\na (cid:88)\n\na,b (cid:88)\n\na (cid:88)\n\na,b\n\nKUU(x, a)v(a)\n\nKUB(x, a, b)v(a, b)\n\nKBU(x, y, a)v(a)\n\nKBB(x, y, a, b)v(a, b),\n\n(30)\n\n(31)\n\n(32)\n\n(33)\n\nwhere KUU(x, a), KUB(x, a, b), KBU(x, y, a) and KBB(x, y, a, b) are D1 × D1, D1 × D2, D2 × D1 and D2 × D2 matrices that correspond to RUU, RUB, RBU and RBB, respectively.\n\nE COMPOSITION OF EXISTING RELATIVE POSITIONAL ENCODING\n\nWe now show that we can compose the existing relative positional encoding (denoted as RPE(cid:63) in our paper) from our m-operator and p-operator. To begin with, we first write the expressions of existing RPE(cid:63) from Shaw et al. (2018) by using their original notation. Specifically, RPE(cid:63) computes the (multi-head) self-attention outputs according to the following expressions (with the notation of self-attention head h being dropped for simplicity):\n\nzi =\n\nn (cid:88)\n\nj=1\n\nαij(xjW V + aV\n\nij) =\n\nn (cid:88)\n\nj=1\n\nαijxjW V +\n\nn (cid:88)\n\nj=1\n\nαijaV\n\nij\n\neij =\n\nxiW Q(xjW K + aK √\n\nij )T\n\ndz\n\n=\n\nxiW Q(xjW K)T + xiW Q(aK √\n\nij )T\n\ndz\n\n(34)\n\n(35)\n\n,\n\nwhere zi is the self-attention output at the i-th token, eij is the unnormalized self-attention scores, αij is the self-attention probability (which is obtained by applying softmax to eij, normalized over j), xi is the vector of the i-th token at the attention input, W Q/W K/W V are the weight matrices for query/key/value in the self-attention operation, respectively, n is the sequence length, and dz is the dimension of each head. Notably, aV ij are the learnable relative positional embedding vectors, defined as\n\nij and aK\n\nij = wK aK ij = wV aV\n\nclip(j−i,k)\n\nclip(j−i,k)\n\nclip(x, k) = max(−k, min(k, x)).\n\nij and aK\n\nThat is, aV ij are the embedding vectors corresponding to the (clipped) relative distance between the i-th and the j-th tokens, where k is the clipping threshold. Therefore, the second terms in both (34)–(35) are the RPE(cid:63) bias terms that seep into the computations of self-attention mechanism. We now proceed to show that these two terms can be viewed as the degenerated form of our m-operator and p-operator in Table 2. For convenience, we first rewrite the expressions for these two operators:\n\nm : uhs(x) =\n\n(cid:88)\n\na\n\nKh(x, a)vs(x, a)\n\n(36)\n\n27\n\nPublished as a conference paper at ICLR 2023\n\np : uh(x, y) =\n\n(cid:88)\n\nw\n\nKhw(x)vw(x, y).\n\n(37)\n\nWe first show that the second terms in (34) can be composed from the m-operator (36). To see this, note that tokens x and a in (36) can be identified as i and j in (34), respectively. In addition, the kernel Kh(x, a) and the premise vs(x, a) can be identified as the self-attention probability αij and the relative positional embedding vector aV ij, respectively. Therefore, the m-operator shares the same form as the second term in (34), except our “head” index h. Likewise, we can show that the second term in (35) can be composed from the p-operator. Observe that the second term in (35) is an inner product between vector xiW Q and vector aK ij . Let kh(x) be a vector that collects Khw(x) as its w-th element and let v(x, y) be a vector that collects vw(x, y) as its w-th element. Then, the p-operator can also be viewed as an inner product between the vectors kh(x) and v(x, y). By identifying kh(x) and v(x, y) as the vectors xiW Q and aK ij , respectively, we conclude that the second term in (35) can be composed from the p-operator. Besides these similarities, our m-operator and p-operator are more general and powerful than the original RPE(cid:63) in the following aspects. First, recall from Section 3.3 that both the kernels K and v are parametrized by the FOLNet (by linearly projecting the intermediate representations {ul(x), ul(x, y)} followed by possible activation functions). Therefore, our vs(x, a) and vw(x, y) are instance-dependent and are different across input instances. In contrast, aV ij and aK ij in (34)–(35) are static embedding vectors that are the same for all input instances. Furthermore, our m-operator and p-operator are adaptive in a layerwise manner; that is, each layer will compute their own m-operator and p-operator adaptively based on their own intermediate representations {ul(x), ul(x, y)}, whereas in RPE(cid:63) the vectors aV ij are generally identical across layers. Therefore, the existing operations in RPE(cid:63) can be viewed as the degenerated special cases that are composable from our m-operator and p-operator.\n\nij and aK\n\n28",
    "reference": "# Summary Of The Paper\n\nThis work introduces a new neural architecture (FOLNet) that incorporates a first-order logical inductive bias. Taking inspiration from the forward-chaining algorithm, the architecture is recursively applying learnable Kernel operations on unary and binary representations of tokens.\n\nInput and outputs are traditional token ids, making FOLNet an easy plug-and-play replacement from traditional Transformers. In addition, traditional Transformers can still be represented by a FOLNet making the proposed architecture more flexible and likely more powerful.\n\nExperiments on GLUE, SQuAD2.0, and FOLIO show that this new architecture performs better than comparable baselines such as BERT, RoBERTa, ALBERT, and Megatron.\n\n# Strength And Weaknesses\n\n**strengths**\n\nThis is a strong paper that introduces a novel architecture with inductive biases from first-order logic. Experimental results are very promising and the fact that logical tasks remain a challenge for traditional Transformers can make this work impactful.\nThe paper is well written and surprisingly easy to read. However the methodology could benefit from some clarifications listed below:\n\n**weaknesses**\n\n1. Methodology : After pre-training on large datasets, were the models fine-tuned on individual tasks (GLUE, SQuAD, FOLIO) ? If so, for how long? it should be mentioned somewhere.\n\n2. Methodology : It is not clear how the final representations output from the model u_L(x) and u_L(x, y) are used in the computation of any loss. How does one go from these two representations to say a softmax layer to predict the masked token in MLM? Similarly for the other losses NSP and SOP.\n\n*Question*: Related to the point above, is it safe to assume that FOLNets are encoder type networks and cannot be used in an decoder-only or encoder-decoder fashion? If this assumption is wrong, then the paper should also describe how u_L(x) and u_L(x, y) are used to predict next words.\n\n3. Literature : Some influential previous work should be further discussed. Neural Theorem Provers (NeurIPS’17), and in particular Greedy Neural Theorem Provers (AAAI’20) applied logical modules on text before. More recently, Edge Transformers (NeurIPS’21) proposed a triangular attention mechanism between pairs of tokens in an effort to better represent logical behaviors between tokens. Comparing FOLNets to these works would help contextualize the work better.\n\n4. Experiments : Right now the paper shows improvements on “classical” language tasks. It would be a nice bonus to also show that the FOL inductive bias is indeed useful for more reasoning heavy tasks such as GSM8k, ProofWriter, CLUTRR, etc…\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clear and easy to read.\nExperimental details are clear and should be reproducible.\nSome methodology clarifications are needed (see section above).\n\n# Summary Of The Review\n\nstrong paper. more previous work should be discussed and some clarifications are needed.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nADVERSARIAL ATTACKS ON ADVERSARIAL BANDITS\n\nYuzhe Ma Microsoft Azure AI yuzhema@microsoft.com\n\nZhijin Zhou⇤ Amazon zhijin@amazon.com\n\nABSTRACT\n\nWe study a security threat to adversarial multi-armed bandits, in which an attacker perturbs the loss or reward signal to control the behavior of the victim bandit player. We show that the attacker is able to mislead any no-regret adversarial bandit algorithm into selecting a suboptimal target arm in every but sublinear (T o(T )) number of rounds, while incurring only sublinear (o(T )) cumulative attack cost. This result implies critical security concern in real-world bandit-based systems, e.g., in online recommendation, an attacker might be able to hijack the recommender system and promote a desired product. Our proposed attack algorithms require knowledge of only the regret rate, thus are agnostic to the concrete bandit algorithm employed by the victim player. We also derived a theoretical lower bound on the cumulative attack cost that any victim-agnostic attack algorithm must incur. The lower bound matches the upper bound achieved by our attack, which shows that our attack is asymptotically optimal.\n\n1\n\nINTRODUCTION\n\nMulti-armed bandit presents a sequential learning framework that enjoys applications in a wide range of real-world domains, including medical treatment Zhou et al. (2019); Kuleshov & Precup (2014), online advertisement Li et al. (2010), resource allocation Feki & Capdevielle (2011); Whittle (1980), search engines Radlinski et al. (2008), etc. In bandit-based applications, the learning agent (bandit player) often receives reward or loss signals generated through real-time interactions with users. For example, in search engine, the user reward can be clicks, dwelling time, or direct feedbacks on the displayed website. The user-generated loss or reward signals will then be collected by the learner to update the bandit policy. One security caveat in user-generated rewards is that there can be malicious users who generate adversarial reward signals. For instance, in online recommendation, adversarial customers can write fake product reviews to mislead the system into making wrong recommendations. In search engine, cyber-attackers can create click fraud through malware and causes the search engine to display undesired websites. In such cases, the malicious users influence the behavior of the underlying bandit algorithm by generating adversarial reward data. Motivated by that, there has been a surge of interest in understanding potential security issues in multi-armed bandits, i.e., to what extend are multi-armed bandit algorithms susceptible to adversarial user data.\n\nPrior works mostly focused on studying reward attacks in the stochastic multi-armed bandit setting Jun et al. (2018); Liu & Shroff (2019), where the rewards are sampled according to some distribution. In contrast, less is known about the vulnerability of adversarial bandits, a more general bandit framework that relaxes the statistical assumption on the rewards and allows arbitrary (but bounded) reward signals. The adversarial bandit also has seen applications in a broad class of real-world problems especially when the reward structure is too complex to model with a distribution, such as inventory control Even-Dar et al. (2009) and shortest path routing Neu et al. (2012). Similarly, the same security problem could arise in adversarial bandits due to malicious users. Therefore, it is imperative to investigate potential security caveats in adversarial bandits, which provides insights to help design more robust adversarial bandit algorithms and applications.\n\nIn this paper, we take a step towards studying reward attacks on adversarial multi-armed bandit algorithms. We assume the attacker has the ability to perturb the reward signal, with the goal of misleading the bandit algorithm to always select a target (sub-optimal) arm desired by the attacker. Our\n\n⇤This work does not relate to the author’s position at Amazon, no matter how the author affiliates.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nmain contributions are summarized as below. (1) We present attack algorithms that can successfully force arbitrary no-regret adversarial bandit algorithms into selecting any target arm in T o(T ) rounds while incurring only o(T ) cumulative attack cost, where T is the total rounds of bandit play. (2) We show that our attack algorithm is theoretically optimal among all possible victim-agnostic o(T ) attack algorithms, which means that no other attack algorithms can successfully force T target arm selections with a smaller cumulative attack cost than our attacks while being agnostic to the underlying victim bandit algorithm. (3) We empirically show that our proposed attack algorithms are efficient on both vanilla and a robust version of Exp3 algorithm Yang et al. (2020).\n\n2 PRELIMINARIES\n\n}\n\n{\n\n2\n\n=\n\nA\n\n1, 2, ..., K\n\nLt(at) from the environment, where\n\n[T ], the player chooses an arm at 2A\n\n, where K is the total number of arms. , and\n\nThe bandit player has a finite action space There is a fixed time horizon T . In each time step t then receives loss `t = paper, we consider “loss” instead of reward, which is more standard in adversarial bandits. However, all of our results would also apply in the reward setting. Without loss of generality, we assume the a, t. Moreover, we consider the so-called non-adaptive loss functions are bounded: environment Slivkins (2019); Bubeck & Cesa-Bianchi (2012), which means the loss functions L1:T are fixed beforehand and cannot change adaptively based on the player behavior after the bandit play starts. The goal of the bandit player is to minimize the difference between the cumulative loss incurred by always selecting the optimal arm in hindsight and the cumulative loss incurred by the bandit algorithm, which is defined as the regret below.\n\nLt is the loss function at time t. In this\n\nLt(a)\n\n[0, 1],\n\n2\n\n8\n\nDefinition 2.1. (Regret). The regret of the bandit player is\n\nT\n\nT\n\nt=1 X\nwhere the expectation is with respect to the randomness in the selected arms a1:T .\n\nt=1 X\n\nRT = E\n\nLt(at)\n\n# \n\nmin a\n\n\"\n\nLt(a),\n\n(1)\n\nWe now make the following major assumption on the bandit algorithm throughout the paper.\n\nAssumption 2.2. (No-regret Bandit Algorithm). We assume the adversarial bandit algorithm satisfies the “no-regret” property asymptotically, i.e., RT = O(T ↵) for some ↵\n\n[ 1 2 , 1)1.\n\n2\n\nAs an example, the classic adversarial bandit algorithm Exp3 achieves ↵ = 1 2 . In later sections, we will propose attack algorithms that apply not only to Exp3, but also arbitrary no-regret bandit algorithms with regret rate ↵ be designed by an adversary, which we refer to as the “environmental adversary”. In typical regret analysis of adversarial bandits, it is implicitly assumed that the environmental adversary aims at inducing large regret on the player. To counter the environmental adversary, algorithms like Exp3 introduce randomness into the arm selection policy, which provably guarantees sublinear regret for arbitrary sequence of adversarial loss functions\n\n[ 1 2 , 1). Note that the original loss functions\n\nL1:T in (1) could as well\n\n2\n\nL1:T .\n\n2.1 MOTIVATION OF ATTACKS ON ADVERSARIAL BANDITS\n\nIn many bandit-based applications, an adversary may have an incentive to pursue different attack goals than boosting the regret of the bandit player. For example, in online recommendation, imagine the situation that there are two products, and both products can produce the maximum click-through rate. We anticipate a fair recommender system to treat these two products equally and display them with equal probability. However, the seller of the first product might want to mislead the recommender system to break the tie and recommend his product as often as possible, which will benefit him most. Note that even if the recommender system chooses to display the first product every time, the click-through rate (i.e., reward) of the system will not be compromised because the first product has the maximum click-through rate by assumption, thus there is no regret in always recommending it. In this case, misleading the bandit player to always select a target arm does not boost the regret. We point out that in stochastic bandits, forcing the bandit player to always select a sub-optimal target arm must\n\n1We assume ↵\n\n1\n\n that the regret has lower bound ⌦(pT ).\n\n2 because prior works Auer et al. (1995); Gerchinovitz & Lattimore (2016) have proved\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ninduce linear regret. Therefore, a robust stochastic bandit algorithm that recovers sublinear regret in presence of an attacker can prevent a sub-optimal arm from being played frequently. However, in adversarial bandit, the situation is fundamentally different. As illustrated in example 1, always selecting a sub-optimal target arm may still incur sublinear regret. As a result, robust adversarial bandit algorithms that recover sublinear regret in presence of an adversary (e.g., Yang et al. (2020)) can still suffer from an attacker who aims at promoting a target arm. Example 1. Assume there are K = 2 arms a1 and a2, and the loss functions are as below.\n\nt,\n\nLt(a) =\n\n8\n\n⇢\n\npT /T\n\n1 1\n\nif a = a1, if a = a2.\n\n(2)\n\nNote that a1 is the best-in-hindsight arm, but always selecting a2 induces pT regret, which is sublinear and does not contradict the regret guarantee of common bandit algorithms like Exp3.\n\n3 THE ATTACK PROBLEM FORMULATION\n\nWhile the original loss functions L1:T can already be adversarial, an adversary who desires a target L1:T due to limited arm often does not have direct control over the environmental loss functions power. However, the adversary might be able to perturb the instantiated loss value `t slightly. For instance, a seller cannot directly control the preference of customers over different products, but he can promote his own product by giving out coupons. To model this attack scenario, we introduce another adversary called the “attacker”, an entity who sits in between the environment and the bandit player and intervenes with the learning procedure. We now formally define the attacker in detail.\n\n(Attacker Knowledge). We consider an (almost) black-box attacker who has very little knowledge of the task and the victim bandit player. In particular, the attacker does not know the clean environmental loss functions L1:T beforehand. Furthermore, the attacker does not know the concrete bandit algorithm used by the player. However, the attacker knows the regret rate ↵2.\n\n(Attacker Ability) In each time step t, the bandit player selects an arm at and the environment generates loss `t = Lt(at). The attacker sees at and `t. Before the player observes the loss, the attacker has the ability to perturb the original loss `t to ̃`t. The player then observes the perturbed loss ̃`t instead of the original loss `t. The attacker, however, cannot arbitrarily change the loss value. In particular, the perturbed loss ̃`t must also be bounded: ̃`t 2\n\n[0, 1],\n\nt.\n\n8\n\n(Attacker Goal). The goal of the attacker is two-fold. First, the attacker has a desired target arm a†, which can be some sub-optimal arm. The attacker hopes to mislead the player into selecting a† as often as possible, i.e., maximize NT (a†) = . On the other hand, every time the attacker perturbs the loss `t, an attack cost ct = is induced. The attacker thus hopes to ⇤\nP achieve a small cumulative attack cost over time, defined as below. Definition 3.1. (Cumulative Attack Cost). The cumulative attack cost of the attacker is defined as\n\nat = a† `t|\n\n ̃`t \n\nT t=1\n\n1\n\n⇥\n\n|\n\nT\n\nCT =\n\nct, where ct =\n\nt=1 X\n\n ̃`t \n\n|\n\n.\n\n`t|\n\n(3)\n\n=\n\n⇤\n\n⇥\n\nThe focus of our paper is to design efficient attack algorithms that can achieve E T\n\nNT (a†) o(T ) and E [CT ] = o(T ) while being agnostic to the concrete victim bandit algorithms.\n\nT\n\nt=1 Lt(a†) is small, then the attack goals would be Intuitively, if the total loss of the target arm t, then even without attack, a† is already the easy to achieve. In the extreme case, if optimal arm and will be selected frequently in most scenarios 3. On the other hand, if t, then the target arm is always the worst arm, and forcing the bandit player to frequently select a† will Lt(a†). In later sections, we will formalize this intuition require the attacker to significantly reduce and characterize the attack difficulty.\n\nLt(a†) = 0,\n\nLt(a†) = 1,\n\nP\n\n8\n\n8\n\n2It suffices for the attacker to know an upper bound on the regret rate to derive all the results in our paper, but\n\nfor simplicity we assume the attacker knows exactly the regret rate.\n\n3An exceptional case is when there exists some non-target arm a0 that also has 0 loss in every round, then a0\n\nis equally optimal as a†, and without attack a0 will be selected equally often as a†.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n4 ATTACK WITH TEMPLATE LOSS FUNCTIONS\n\nIn this section, we first propose a general attack strategy called “template-based attacks”. The template-based attacks perform loss perturbations according to a sequence of template loss functions L1:T . The templates ̃ ̃\nthe bandit play, the attacker perturbs the original loss `t to ̃`t = ̃ attacks may seem weak at first glance, because the template loss functions are fixed beforehand and thus non-adaptive to the behaviors of the victim bandit player. This is in stark contrast to most prior works such as Jun et al. (2018). However, as we will show in later sections, template-based attacks are efficient and can even achieve the optimal attack cost.\n\nL1:T are determined before the bandit play starts. Then in each time step t during Lt(at). Note that template-based\n\nWe first make the following important observation, which is a critical property used to prove the main theoretical results in our paper.\n\nObservation 1. (Equivalence of Attack) Due to the partial observability of loss functions in the multi-armed bandit framework, running any bandit algorithm in the original environment L1:T with template-based attack ̃ L1:T , is equivalent to, running the same algorithm in an environment with loss functions ̃ L1:T . In particular, the standard regret guarantee RT = O(T ↵) holds with respect to the template loss functions ̃\n\nL1:T .\n\nWe next instantiate the template-based attack on an easy attack scenario and general attack scenarios. For each scenario, we will design the template loss functions ̃\n\nL1:T ,\n\n4.1 AN EASY ATTACK SCENARIO\n\nIn this section, we first study a relatively easy attack scenario, which serves as a motivating example that illustrates two key principles behind the design of our attack algorithms: (1) Try to keep the loss of the target arm the maximum. The easy attack scenario has the following assumption on the original loss functions, which requires the loss of the target arm to be at least ⇢ bounded away from the maximum value.\n\nLt(a†) unchanged; and (2) Boost the loss of all the other non-target arms to\n\nAssumption 4.1. (Easy Attack Scenario). There exists some constant ⇢\n\n(0, 1] such that\n\n2\n\nLt(a†)\n\n2\n\n[0, 1\n\n⇢],\n\nt\n\n8\n\n2\n\n[T ].\n\n(4)\n\nThe boundedness condition (4) needs to hold over all T rounds. If assumption 4.1 holds, then the attacker can design the template loss functions ̃\n\nLt as in (5) to perform attack. if a = a†, Lt(a) otherwise.\n\n1\n\nt, ̃\n\nLt(a) =\n\n8\n\n⇢\n\n(5)\n\nRemark 4.2. A few remarks are in order. First, note that although the form of ̃ Lt(a) depends on Lt(a), the attacker does not require knowledge of the original loss functions L1:T beforehand to implement the attack. This is because when at = a†, the perturbed loss is ̃`t = ̃ Lt(a†) = `t = a†, ̃`t can be directly set to 1. Second, note that the target arm a† while `t is observable. When at 6 becomes the best-in-hindsight arm after attack. Consider running a no-regret bandit algorithm on the perturbed loss ̃ L1:T , since ̃ ⇢, every time the player selects a non-target \n= a†, it will incur at least ⇢ regret. However, the player is guaranteed to achieve sublinear arm at 6 regret on ̃ L1:T by observation 1, thus non-target arms can at most be selected in sublinear rounds. Finally, note that the loss remains unchanged when the target arm a† is selected. This design is o(T ) rounds. By critical because should the attack be successful, then a† will be selected in T keeping the loss of the target arm unchanged, the attacker does not incur attack cost when the target arm is selected. As a result, our design (5) induces sublinear cumulative attack cost.\n\nLt(a†) =\n\nLt(at) =\n\nLt(a†)\n\n\n\n1\n\nTheorem 4.3. Assume assumption 4.1 holds, and the attacker applies (5) to perform attack. Then there exists a constant M > 0 such that the expected number of target arm selections satisfies M T ↵/⇢. E\n\nM T ↵/⇢, and the expected cumulative attack cost satisfies E [CT ]\n\nNT (a†)\n\nT\n\n⇥\n\nRemark 4.4. Note that as the regret rate ↵ decreases, the target arm selections E increases and the cumulative attack cost E [CT ] reduces. That means, our attack algorithm becomes more\n\n⇤\n\n NT (a†)\n\n⇥\n\n⇤\n\n4\n\nPublished as a conference paper at ICLR 2023\n\neffective and efficient if the victim bandit algorithm has a better regret rate. The constant M comes from the regret bound of the victim adversarial bandit algorithm and will depend on the number of arms K (similarly for Theorem 4.6 and 4.9). We do not spell out its concrete form here because our paper aims at designing general attacks against arbitrary adversarial bandit algorithms that satisfy assumption 2.2. The constant term in the regret bound may take different forms for different algorithms. Comparatively, the sublinear regret rate ↵ is more important for attack considerations.\n\n4.2 GENERAL ATTACK SCENARIOS\n\nOur analysis in the easy attack scenario relies on the fact that every time the player fails to select the target arm a†, at least a constant regret ⇢ will be incurred. Therefore, the player can only take non-target arms sublinear times. However, this condition breaks if there exists time steps t where Lt(a†) = 1. In this section, we propose a more generic attack strategy, which provably achieves L1:T . Furthermore, the proposed attack strategy Lt as in (6) to perform attack.\n\nsublinear cumulative attack cost on any loss functions can recover the result of Theorem 4.3 (up to a constant) when it is applied in the easy attack scenario. Specifically, the attacker designs the template loss functions ̃\n\nt, ̃\n\nLt(a) =\n\n8\n\n⇢\n\n1\n\nmin 1\n\n{\n\nt↵+✏\n\n1,\n\nLt(a)\n\n}\n\nif a = a†, otherwise,\n\n(6)\n\nwhere ✏ affects the attack performance in remark 4.7.\n\n[0, 1\n\n2\n\n↵) is a free parameter chosen by the attacker. We discuss how the parameter ✏\n\nRemark 4.5. Similar to (5), the attacker does not require knowledge of the original loss functions L1:T beforehand to implement the attack. When a non-target arm is selected, the attacker always increases the loss to the maximum value 1. On the other hand, when the target arm a† is selected, 1, the attacker reduces the loss to then if the observed clean loss value `t = Lt(at) > 1 1. Otherwise, the attacker keeps the loss unchanged. In doing so, the attacker ensures 1\nthat the loss of the target arm ̃ Lt(a) for any non-target arm a\ndiminishes as a function of t since ✏< 1 is important to achieving sublinear attack cost, which we will prove later.\n\n= a†. As a result, a† becomes the best-in-hindsight arm under ̃\n\nL1:T . Note that the gap t↵+✏\n\n↵. The condition that ✏ must be strictly smaller than 1\n\nLt(a†) is at least t↵+✏\n\n1 smaller than ̃\n\nt↵+✏\n\nt↵+✏\n\n ↵\n\n1\n\nTheorem 4.6. Assume the attacker applies (6) to perform attack. Then there exists a constant M > 0 such that the expected number of target arm selections satisfies\n\nE\n\nNT (a†)\n\nT\n\n1 ↵ + ✏\n\nT 1\n\n↵\n\n✏\n\nM T 1\n\n✏,\n\nand the expected cumulative attack cost satisfies\n\n⇥\n\n⇤\n\nE [CT ]\n\n1 ↵ + ✏\n\n\n\nT 1\n\n↵\n\n✏ + M T 1\n\n✏ +\n\n1 ↵ + ✏\n\nT ↵+✏.\n\n(7)\n\n(8)\n\nRemark 4.7. According to (7), the target arm will be selected more frequently as ✏ grows. This is because the attack (6) enforces that the loss of the target arm ̃ 1 smaller than the loss of non-target arms. As ✏ increases, the gap becomes larger, thus the bandit algorithm would further prefer a†. The cumulative attack cost, however, does not decrease monotonically as a function of ✏. This is because while larger ✏ results in more frequent target arm selections, the per-round t, then whenever a† is selected, the attack cost may also increase. For example, if attacker incurs attack cost t↵+✏\n\n8 1, which grows as ✏ increases.\n\nLt(a†) is at least t↵+✏\n\nLt(a†) = 1,\n\n↵\n\nCorollary 4.8. Assume the attacker applies (6) to perform attack. Then when the attacker chooses ✏ = 1 2 , the expected cumulative attack cost achieves the minimum value asymptotically. Correspondingly, we have E\n\n2 ) and E [CT ] = O(T\n\nNT (a†)\n\nO(T\n\n2 ).\n\n= T\n\n1+↵\n\n1+↵\n\nWe now show that our attack (6) recovers the results in Theorem 4.3 when it is applied in the easy attack scenario. We first provide another version of the theoretical bounds on E and E [CT ] that depends on how close\n\nNT (a†)\n\n⇥\n\n⇤\n\nLt(a†) is to the maximum value. T⇢ =\n\n(0, 1] be any constant. Define\n\n⇥\n\n⇤\n\nt {\n\n| Lt(a†) > 1\n\n⇢\n\n, i.e., the set of = ⌧ . Also assume that\n\n}\n\nTheorem 4.9. Let ⇢ rounds where\n\n2\n\nLt(a†) is within distance ⇢ to the maximum loss value. Let\n\n|T⇢|\n\n5\n\n6 Published as a conference paper at ICLR 2023\n\nthe attacker applies (6) to perform attack, then there exists a constant M > 0 such that the expected number of target arm selections satisfies\n\nE\n\nNT (a†)\n\nand the cumulative attack cost satisfies ⇤\nE [CT ]\n\n⇥\n\n1 ↵+✏\n\n⇢\n\n1\n\nT\n\n⌧\n\nM T ↵/⇢,\n\n1 ↵+✏\n\n⇢\n\n1 + ⌧ + M T ↵/⇢.\n\n\n\n(9)\n\n(10)\n\nRemark 4.10. In the easy attack scenario, there exists some ⇢ such that ⌧ = 0, thus compared to Theorem 4.3, the more generic attack (6) induces an additional constant term ⇢ 1 in the bounds of E\n\nand E [CT ], which is negligible for large enough T .\n\nNT (a†)\n\n1 ↵+✏\n\n⇥\n\n⇤\n\n5 ATTACK COST LOWER BOUND\n\nWe have proposed two attack strategies targeting the easy and general attack scenarios separately. In this section, we show that if an attack algorithm achieves T o(T ) target arm selections and is also victim-agnostic, then the cumulative attack cost is at least ⌦(T ↵). Note that since we want to derive victim-agnostic lower bound, it is sufficient to pick a particular victim bandit algorithm that guarantees O(T ↵) regret and then prove that any victim-agnostic attacker must induce at least some attack cost in order to achieve T o(T ) target arm selections. Specifically, we consider the most popular Exp3 algorithm (see algorithm 1 in the appendix). We first provide the following key lemma, which characterizes a lower bound on the number of arm selections for Exp3. Lemma 5.1. Assume the bandit player applies the Exp3 algorithm with parameter ⌘ (see (34) in the appendix) and initial arm selection probability ⇡1. Let the loss functions be , the total number of rounds where a is selected, NT (a), satisfies\n\nL1:T . Then\n\n2A\n\n8\n\na\n\nT\n\nE [NT (a)]\n\nT⇡ 1(a)\n\n⌘T\n\nE [⇡t(a)\n\nLt(a)] ,\n\nwhere ⇡t is the arm selection probability at round t. Furthermore, since ⇡t(a)\n\n1, we have\n\n\n\n(11)\n\n(12)\n\nE [NT (a)]\n\nT⇡ 1(a)\n\n⌘T\n\nLt(a).\n\nt=1 X\n\nT\n\nt=1 X\n\nT\n\nRemark 5.1. Lemma 5.1 provides two different lower bounds on the number of arm selections based on the loss functions for each arm a. (12) shows that the lower bound on E [NT (a)] increases as t=1 Lt(a) of arm a becomes smaller, which coincides with the intuition. In the cumulative loss particular, if ⇡1 is initialized to the uniform distribution and ⌘ is picked as T  P\n, the lower bound (12) becomes E [NT (a)] here is that if the loss function of an arm a is always zero, i.e., selected at least T /K times in expectation.\n\nt=1 Lt(a). One direct conclusion Lt(a) = 0,\n\nt, then arm a must be\n\n2 for some constant\n\npT\n\nT /K\n\nP\n\n8\n\nT\n\n1\n\nNow we provide our main result in Theorem 5.2, which shows that for a special implementation of Exp3 that achieves O(T ↵) regret, any attacker must induce ⌦(T ↵) cumulative attack cost. o(T ) on Theorem 5.2. Assume some victim-agnostic attack algorithm achieves E [ 1 all victim bandit algorithms that has regret rate O(T ↵), where ↵ 2 , 1). Then there exists a bandit ⇤\ntask such that the attacker must induce at least expected attack cost E [CT ] =⌦( T ↵) on some victim algorithm. Specifically, one such victim is the Exp3 algorithm with parameter ⌘ =⇥( T \n\nNT (a†)\n\n= T\n\n↵).\n\n2\n\n⇥\n\nThe lower bound ⌦(T ↵) matches the upper bound proved in both Theorem 4.3 and Theorem 4.9 up to a constant, thus our attacks are asymptotically optimal in the easy attack scenario. However, there is a gap compared to the upper bound O(T 2 ) proved for the general attack scenario (corollary 4.8). The gap diminishes as ↵ approaches 1, but how to completely close this gap remains an open problem.\n\n1+↵\n\n6 EXPERIMENTS\n\nWe now perform empirical evaluations of our attacks. We consider two victim adversarial bandit algorithms: the Exp3 algorithm (see Algorithm 1 in the appendix), and a robust version of Exp3\n\n6\n\nPublished as a conference paper at ICLR 2023\n\ncalled ExpRb (see Yang et al. (2020)). The ExpRb assumes that the attacker has a fixed attack budget . When = O(pT ), the ExpRb recovers the regret of Exp3. However, our attack does not have a fixed budget beforehand. Nevertheless, we pretend that ExpRb assumes some budget  (may not be bounded by the cumulative attack cost of our attacker) and evaluate its performance for different ’s. Note that as illustrated in example 1, robust bandit algorithms that can recover sublinear regret may still suffer from an attacker who aims at promoting a target arm in the adversarial bandit setting.\n\n6.1 AN EASY ATTACK EXAMPLE\n\nt,\n\nLt(a1) = 0.5 and\n\nIn out first example, we consider a bandit problem with K = 2 arms, a1 and a2. The loss function is Lt(a2) = 0. Without attack a2 is the best-in-hindsight arm and will be selected 8\nmost of the times. The attacker, however, aims at forcing arm a1 to be selected in almost very round. t, thus this example falls into the easy Therefore, the target arm is a† = a1. Note that attack scenario, and we apply (5) to perform attack.\n\nLt(a†) = 0.5,\n\n8\n\n(a) T\n\nNT (a†) of (5).\n\n(b) CT of (5).\n\n(c) T\n\nNT (a†) of (6).\n\n(d) CT of (6).\n\nFigure 1: Using (5) and (6) to perform attack in an easy attack scenario.\n\n⇥\n\n⇥\n\nIn the first experiment, we let the total horizon be T = 103, 104, 105 and 106. For each T , we run the Exp3 and ExpRb under attack for T rounds, and compute the number of “non-target” arm selections NT (a†). We repeat the experiment by 10 trials and take the average. In Figure 1a, we show T\n log(T NT (a†)), i.e., the log value of the total number of averaged “non-target” arm selections, as a function of log T . The error bars are tiny small, thus we ignore them in the plot. Smaller value of log(T NT (a†)) means better attack performance. Note that when no attack happens (blue line), the Exp3 algorithm almost does not select the target arm a†. Specifically, for T = 106, the Exp3 104 rounds, which is only 1.5% of the total horizon. Under attack though, for selects a† in 1.45 104, T = 103, 104, 105, 106, the attacker misleads Exp3 to select a† in 8.15 105 rounds, which are 81.5%, 91.3%, 96.3% and 98.5% of the total horizon. We also and 9.85 plotted the line y = x for comparison. Note that the slope of log(T NT (a†)) is smaller than that of y = x, which means T NT (a†) grows sublinearly as T increases. This matches our theoretical results in Theorem 4.3. For the other victim ExpRb, we consider different levels of attack budget . The attacker budget assumed by ExpRb must be sublinear, since otherwise the ExpRb cannot recover sublinear regret, and thus not practically useful. In particular, we consider = T 0.5, T 0.7 and T 0.9. Note that for = T 0.7 and T 0.9, the ExpRb cannot recover the O(pT ) regret of Exp3. For T = 106, 106 rounds for our attack forces ExpRb to select the target arm in 9.83 the three attacker budget above. This corresponds to 98.3%, 89.7%, and 63.2% of the total horizon respectively. Note that the ExpRb is indeed more robust than Exp3 against our attack. However, our attack still successfully misleads the ExpRb to select the target a† very frequently. Also note that the attack performance degrades as the attacker budget  grows. This is because the ExpRb becomes more robust as it assumes a larger attack budget .\n\n106, and 6.32\n\n106, 8.97\n\n102, 9.13\n\n103, 9.63\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n102, 3.67\n\n102, 8.72\n\n103, and 1.45\n\nFigure 1b shows the attack cost averaged over 10 trials. For Exp3, the cumulative attack costs are 104 for the four different T ’s. On average, the 1.85 ⇥\nper-round attack cost is 0.19, 0.09.0.04, and 0.01 respectively. Note that the per-round attack cost diminishes as T grows. Again, we plot the line y = x for comparison. Note that slope of log CT is smaller than that of y = x. This suggests that CT increases sublinearly as T grows, which is consistent with our theoretical results in Theorem 4.3. For ExpRb, for T = 106, our attack incurs 105 when ExpRb assumes = T 0.5, T 0.7 105 and 3.68 cumulative attack costs 1.73 and T 0.9 respectively. On average, the per-round attack cost is 0.02, 0.10 and 0.37. Note that our attack induces larger attack cost on ExpRb than Exp3, which means ExpRb is more resilient against\n\n104, 1.03\n\n⇥\n\n⇥\n\n⇥\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nour attacks. Furthermore, the attack cost grows as ExpRb assumes a larger attack budget . This is again due to that a larger  implies that ExpRb is more prepared against attacks, thus is more robust.\n\nNext we apply the general attack (6) to verify that (6) can recover the results of Theorem 4.3 in the easy attack scenario. We fix ✏ = 0.25 in (6). In Figures 1c and 1d, we show the number of target arm selections and the cumulative attack cost. For the Exp3 victim, for the four different T ’s, the 105 attack (6) forces the target arm to be selected in 8.12 rounds, which is 81.2%, 91.2%, 96.3% and 98.5% of the total horizon respectively. Compared to (5), the attack performance is just slightly worse. The corresponding cumulative attack costs 104. On average, the per-round attack cost is are 1.89 0.19, 0.09, 0.04 and 0.01. Compared to (5), the attack cost is almost the same.\n\n103, and 1.45\n\n102, 9.12\n\n103, 9.63\n\n104, 9.85\n\n102, 3.68\n\n102, 8.76\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n6.2 A GENERAL ATTACK EXAMPLE\n\nt,\n\nLt(a1) = 1 and\n\nIn our second example, we consider a bandit problem with K = 2 arms and the loss function is Lt(a2) = 0. The attacker desires target arm a† = a1. This example is hard to 8\nattack because the target arm has the maximum loss across the entire T horizon. We apply the general attack (6) to perform attack. We consider T = 103, 104, 105, and 106. The results reported in this section are also averaged over 10 independent trials.\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n102, 2.12\n\n102, 5.27\n\n103, and 8.07\n\nIn the first experiment, we let the victim bandit algorithm be Exp3 and study how the parameter ✏ affects the performance of the attack. We let ✏ = 0.1, 0.25 and 0.4. In Figure 2a, we show the number of target arm selections for different T ’s. Without attack, the Exp3 selects a† in only 103 rounds, which are 12%, 5.3%, 2.1% and 0.81% 1.20 of the total horizon. In Figure 2a, we show log(T NT (a†)) as a function of log T for different ✏’s. Note that as ✏ grows, our attack (6) enforces more target arm selections, which is consistent In particular, for ✏ = 0.4, our attack forces the target arm to be selected with Theorem 4.6. 105 rounds, which are 83.4%, 91.3%, 95.8% and 102, 9.13 in 8.34 98.1% of the total horizon. In Figure 2b, we show the cumulative attack cost. Note that according to corollary 4.8, the cumulative attack cost achieves the minimum value at ✏ = 0.25. This is exactly what we see in Figure 2b. Specifically, for ✏ = 0.25, the cumulative attack costs are 105. On average, the per-round attack cost is 4.20 ⇥\n0.42, 0.28, 0.19 and 0.11 respectively. Note that the per-round attack cost diminishes as T grows. In both Figure 2a and 2b, we plot the line y = x for comparison. Note that both T and CT grow sublinearly as T increases, which verifies our results in Theorem 4.6.\n\n104, and 1.14\n\n104, 9.81\n\n103, 9.58\n\n102, 2.84\n\n103, 1.85\n\nNT (a†)\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\nE\n\n⇥\n\n⇤\n\n(a) T\n\nNT (a†) as ✏ varies.\n\n(b) CT as ✏ varies.\n\n(c) T\n\nNT (a†) as  varies.\n\n(d) CT as  varies.\n\nFigure 2: Using (6) to perform attack in general attack scenarios.\n\nIn our second experiment, we evaluate the performance of our attack (6) on the robust adversarial bandit algorithm ExpRb Yang et al. (2020). We fixed ✏ = 0.25 in (6). We consider three levels of attacker budget = T 0.5, T 0.7 and T 0.9 in ExpRb, corresponding to increasing power of the attacker. In Figure 2c, we show the total number of target arm selections. For T = 106, our attack forces 105 rounds, which is 92.4% of the total rounds. For the the Exp3 to select the target arm in 9.24 ⇥\nExpRb victim, for the three different attack budgets ’s, our attack forces ExpRb to select the target 105 rounds, corresponding to 89.7%, 66.5% and 50.7% of arm in 8.97 the total horizon respectively. Note that when = T 0.5, i.e., the ExpRb can recover the regret of Exp3, but our attack still forces target arm selection in almost 90% of rounds. This is smaller than the 92.4% on the Exp3 victim, which demonstrates that ExpRb indeed is more robust than Exp3. Nevertheless, the ExpRb failed to defend against our attack. Even when ExpRb assumes a very large attacker budget like = T 0.9, our attack still forces the target arm selection in 50.7% of rounds.\n\n105 and 5.07\n\n105, 6.65\n\n⇥\n\n⇥\n\n⇥\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n⇥\n\nIn Figure 2d, we show the cumulative attack costs. For T = 106, the cumulative attack cost on the 105. On average, the per-round attack cost is 0.11. For the ExpRb victim, Exp3 victim is 1.14 for T = 106, the cumulative attack costs are 1.40 105 for the three different attacker budgets = T 0.5, T 0.7, T 0.9. The per-round attack cost is 0.14, 0.36 and 0.51 respectively. Note that when = T 0.5, the ExpRb recovers the regret of Exp3. The per-round attack cost for ExpRb is 0.14, which is slightly higher than Exp3. This again shows that ExpRb is indeed more robust than Exp3. Also note that the attack cost grows as ExpRb assumes a larger attacker budget. This is reasonable since larger attacker budget  implies stronger robustness of ExpRb.\n\n105, and 5.15\n\n105, 3.62\n\n⇥\n\n⇥\n\n⇥\n\n7 RELATED WORKS\n\nExisting research on attacks of multi-armed bandit mostly fall into the topic of data poisoning Ma et al. (2019b). Prior works are limited to poisoning attacks on stochastic bandit algorithms. One line of work studies reward poisoning on vanilla bandit algorithms like UCB and ✏-greedy Jun et al. (2018); Zuo (2020); Niss; Xu et al. (2021b); Ma et al. (2018); Liu & Shroff (2019); Wang et al. (2021); Ma (2021); Xu et al., contextual and linear bandits Garcelon et al. (2020), and also best arm identification algorithms Altschuler et al. (2019). Another line focuses on action poisoning attacks Liu & Lai (2020; 2021a) where the attacker perturbs the selected arm instead of the reward signal. Recent study generalizes the reward attacks to broader sequential decision making scenarios such as multi-agent games Ma et al. (2021) and reinforcement learning Ma et al. (2019a); Zhang et al. (2020); Sun et al. (2020); Rakhsha et al. (2021); Xu et al. (2021a); Liu & Lai (2021b), where the problem structure is more complex than bandits. In the multi-agent decision-making scenarios, a related security threat is an internal agent who adopts strategic behaviors to mislead competitors and achieves desired objectives such as Deng et al. (2019); Gleave et al. (2019).\n\nThere are also prior works that design robust algorithms in the context of stochastic bandits Feng et al. (2020); Guan et al. (2020); Rangi et al. (2021); Ito (2021), linear and contextual bandits Bogunovic et al. (2021); Ding et al. (2021); Zhao et al. (2021); Yang & Ren (2021); Yang (2021), dueling bandits Agarwal et al. (2021), graphical bandits Lu et al. (2021), best-arm identification Zhong et al. (2021), combinatorial bandit Dong et al. (2022), and multi-agent Vial et al. (2022) or federated bandit learning scenarios Mitra et al. (2021); Demirel et al. (2022). Most of the robust algorithms are designed to recover low regret even in presence of reward corruptions. However, as we illustrated in example 1, recovering low regret does not guarantee successful defense against an attacker who wants to promote a target arm in the adversarial bandit scenario. How to defend against such attacks remains an under-explored question.\n\nOf particular interest to our paper is a recent work on designing adversarial bandit algorithms robust to reward corruptions Yang et al. (2020). The paper assumes that the attacker has a prefixed budget of attack cost , and then designs a robust adversarial bandit algorithm ExpRb, which achieves regret that scales linearly as the attacker budget  grows RT = O(pK log KT + K log T ). As a result, the ExpRb can tolerate any attacker with budget = O(pT ) while recovering the standard regret rate of Exp3. We point out that one limitation of ExpRb is that it requires prior knowledge of a fixed attack budget . However, our attack does not have a fixed budget beforehand. Instead, our attack budget depends on the behavior of the bandit player. Therefore, the ExpRb does not directly apply as a defense against our attack. Nevertheless, in our experiments, we pretend that ExpRb assumes some attack budget  and evaluate its performance under our attack.\n\n8 CONCLUSION\n\nWe studied reward poisoning attacks on adversarial multi-armed bandit algorithms. We proposed attack strategies in both easy and general attack scenarios, and proved that our attack can successfully mislead any no-regret bandit algorithm into selecting a target arm in T o(T ) rounds while incurring only o(T ) cumulative attack cost. We also provided a lower bound on the cumulative attack cost o(T ) target arm selections, that any victim-agnostic attacker must induce in order to achieve T which matches the upper bound achieved by our attack. This shows that our attack is asymptotically optimal. Our study reveals critical security caveats in bandit-based applications, and it remains an open problem how to defend against our attacker whose attack goal is to promote a desired target arm instead of boosting the regret of the victim bandit algorithm.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nArpit Agarwal, Shivani Agarwal, and Prathamesh Patil. Stochastic dueling bandits with adversarial\n\ncorruption. In Algorithmic Learning Theory, pp. 217–248. PMLR, 2021.\n\nJason Altschuler, Victor-Emmanuel Brunel, and Alan Malek. Best arm identification for contaminated\n\nbandits. J. Mach. Learn. Res., 20(91):1–39, 2019.\n\nPeter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Proceedings of IEEE 36th annual foundations of computer science, pp. 322–331. IEEE, 1995.\n\nIlija Bogunovic, Arpan Losalka, Andreas Krause, and Jonathan Scarlett. Stochastic linear bandits robust to adversarial attacks. In International Conference on Artificial Intelligence and Statistics, pp. 991–999. PMLR, 2021.\n\nSébastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-\n\narmed bandit problems. arXiv preprint arXiv:1204.5721, 2012.\n\nIlker Demirel, Yigit Yildirim, and Cem Tekin. Federated multi-armed bandits under byzantine attacks.\n\narXiv preprint arXiv:2205.04134, 2022.\n\nYuan Deng, Jon Schneider, and Balasubramanian Sivan. Strategizing against no-regret learners.\n\nAdvances in neural information processing systems, 32, 2019.\n\nQin Ding, Cho-Jui Hsieh, and James Sharpnack. Robust stochastic linear contextual bandits under\n\nadversarial attacks. arXiv preprint arXiv:2106.02978, 2021.\n\nJing Dong, Ke Li, Shuai Li, and Baoxiang Wang. Combinatorial bandits under strategic manipulations. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pp. 219–229, 2022.\n\nEyal Even-Dar, Sham M Kakade, and Yishay Mansour. Online markov decision processes. Mathe-\n\nmatics of Operations Research, 34(3):726–736, 2009.\n\nAfef Feki and Veronique Capdevielle. Autonomous resource allocation for dense lte networks: A multi armed bandit formulation. In 2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications, pp. 66–70. IEEE, 2011.\n\nZhe Feng, David Parkes, and Haifeng Xu. The intrinsic robustness of stochastic bandits to strategic manipulation. In International Conference on Machine Learning, pp. 3092–3101. PMLR, 2020.\n\nEvrard Garcelon, Baptiste Roziere, Laurent Meunier, Jean Tarbouriech, Olivier Teytaud, Alessandro Lazaric, and Matteo Pirotta. Adversarial attacks on linear contextual bandits. Advances in Neural Information Processing Systems, 33:14362–14373, 2020.\n\nSébastien Gerchinovitz and Tor Lattimore. Refined lower bounds for adversarial bandits. Advances\n\nin Neural Information Processing Systems, 29, 2016.\n\nAdam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adversarial\n\npolicies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615, 2019.\n\nZiwei Guan, Kaiyi Ji, Donald J Bucci Jr, Timothy Y Hu, Joseph Palombo, Michael Liston, and Yingbin Liang. Robust stochastic bandit algorithms under probabilistic unbounded adversarial attack. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 4036–4043, 2020.\n\nShinji Ito. On optimal robustness to adversarial corruption in online decision problems. Advances in\n\nNeural Information Processing Systems, 34, 2021.\n\nKwang-Sung Jun, Lihong Li, Yuzhe Ma, and Jerry Zhu. Adversarial attacks on stochastic bandits.\n\nAdvances in Neural Information Processing Systems, 31, 2018.\n\nVolodymyr Kuleshov and Doina Precup. Algorithms for multi-armed bandit problems. arXiv preprint\n\narXiv:1402.6028, 2014.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nLihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pp. 661–670, 2010.\n\nFang Liu and Ness Shroff. Data poisoning attacks on stochastic bandits. In International Conference\n\non Machine Learning, pp. 4042–4050. PMLR, 2019.\n\nGuanlin Liu and Lifeng Lai. Action-manipulation attacks on stochastic bandits. In ICASSP 20202020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3112–3116. IEEE, 2020.\n\nGuanlin Liu and Lifeng Lai. Efficient action poisoning attacks on linear contextual bandits. arXiv\n\npreprint arXiv:2112.05367, 2021a.\n\nGuanlin Liu and Lifeng Lai. Provably efficient black-box action poisoning attacks against reinforce-\n\nment learning. Advances in Neural Information Processing Systems, 34, 2021b.\n\nShiyin Lu, Guanghui Wang, and Lijun Zhang. Stochastic graphical bandits with adversarial corruptions. In Proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI), to appear, 2021.\n\nYuzhe Ma. Adversarial Attacks in Sequential Decision Making and Control. PhD thesis, The\n\nUniversity of Wisconsin-Madison, 2021.\n\nYuzhe Ma, Kwang-Sung Jun, Lihong Li, and Xiaojin Zhu. Data poisoning attacks in contextual bandits. In International Conference on Decision and Game Theory for Security, pp. 186–204. Springer, 2018.\n\nYuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. Policy poisoning in batch reinforcement\n\nlearning and control. Advances in Neural Information Processing Systems, 32, 2019a.\n\nYuzhe Ma, Xiaojin Zhu, and Justin Hsu. Data poisoning against differentially-private learners:\n\nAttacks and defenses. arXiv preprint arXiv:1903.09860, 2019b.\n\nYuzhe Ma, Young Wu, and Xiaojin Zhu. Game redesign in no-regret game playing. arXiv preprint\n\narXiv:2110.11763, 2021.\n\nAritra Mitra, Hamed Hassani, and George Pappas. Robust federated best-arm identification in\n\nmulti-armed bandits. arXiv e-prints, pp. arXiv–2109, 2021.\n\nGergely Neu, Andras Gyorgy, and Csaba Szepesvári. The adversarial stochastic shortest path problem with unknown transition probabilities. In Artificial Intelligence and Statistics, pp. 805–813. PMLR, 2012.\n\nLaura Niss. What you see may not be what you get: Ucb bandit algorithms robust to \"-contamination.\n\nAnn Arbor, 1001:48109.\n\nFilip Radlinski, Robert Kleinberg, and Thorsten Joachims. Learning diverse rankings with multiarmed bandits. In Proceedings of the 25th international conference on Machine learning, pp. 784–791, 2008.\n\nAmin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching in reinforcement learning via environment poisoning attacks. Journal of Machine Learning Research, 22(210):1–45, 2021.\n\nAnshuka Rangi, Long Tran-Thanh, Haifeng Xu, and Massimo Franceschetti. Secure-ucb: Saving stochastic bandits from poisoning attacks via limited data verification. arXiv preprint arXiv:2102.07711, 2021.\n\nAleksandrs Slivkins. Introduction to multi-armed bandits. arXiv preprint arXiv:1904.07272, 2019.\n\nYanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online rl\n\nwith unknown dynamics. arXiv preprint arXiv:2009.00774, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nDaniel Vial, Sanjay Shakkottai, and R Srikant. Robust multi-agent bandits over undirected graphs.\n\narXiv preprint arXiv:2203.00076, 2022.\n\nHuazheng Wang, Haifeng Xu, and Hongning Wang. When are linear stochastic bandits attackable?\n\narXiv preprint arXiv:2110.09008, 2021.\n\nPeter Whittle. Multi-armed bandits and the gittins index. Journal of the Royal Statistical Society:\n\nSeries B (Methodological), 42(2):143–149, 1980.\n\nHang Xu, Rundong Wang, Lev Raizman, and Zinovi Rabinovich. Transferable environment poisoning: Training-time attack on reinforcement learning. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1398–1406, 2021a.\n\nYinglun Xu, Bhuvesh Kumar, and Jacob Abernethy. Oblivious data corruption attack for stochastic\n\nmulti-arm bandit algorithms.\n\nYinglun Xu, Bhuvesh Kumar, and Jacob D Abernethy. Observation-free attacks on stochastic bandits.\n\nAdvances in Neural Information Processing Systems, 34, 2021b.\n\nJianyi Yang and Shaolei Ren. Robust bandit learning with imperfect context. arXiv preprint\n\narXiv:2102.05018, 2021.\n\nLin Yang, Mohammad Hassan Hajiesmaili, Mohammad Sadegh Talebi, John CS Lui, Wing Shing Wong, et al. Adversarial bandits with corruptions: Regret lower bound and no-regret algorithm. In NeurIPS, 2020.\n\nLuting Yang. Contextual Bandits in Imperfect Environments: Analysis and Applications. University\n\nof California, Riverside, 2021.\n\nXuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks against reinforcement learning. In International Conference on Machine Learning, pp. 11225– 11234. PMLR, 2020.\n\nHeyang Zhao, Dongruo Zhou, and Quanquan Gu. Linear contextual bandits with adversarial corrup-\n\ntions. arXiv preprint arXiv:2110.12615, 2021.\n\nZixin Zhong, Wang Chi Cheung, and Vincent Tan. Probabilistic sequential shrinking: A best arm identification algorithm for stochastic bandits with corruptions. In International Conference on Machine Learning, pp. 12772–12781. PMLR, 2021.\n\nZhijin Zhou, Yingfei Wang, Hamed Mamani, and David G Coffey. How do tumor cytogenetics inform cancer treatments? dynamic risk stratification and precision medicine using multi-armed bandits. Dynamic Risk Stratification and Precision Medicine Using Multi-armed Bandits (June 17, 2019), 2019.\n\nShiliang Zuo. Near optimal adversarial attack on ucb bandits. arXiv preprint arXiv:2008.09312,\n\n2020.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe authors study a threat to adversarial multi-armed bandit, in which an attacker perturbs the loss or reward signal to force the defender to select a suboptimal target action in every but (T −o(T)) number of rounds, while incurring only o(T) cumulative attack cost. This result try to motivate the problem through online recommendation. The proposed attack algorithms require knowledge of only the regret rate, and is agnostic to the bandit algorithm used by the victim player. A theoretical lower bound on the cumulative attack cost tis also presented.\n\n# Strength And Weaknesses\n\nThis is primarily a theory paper. The experiments are in simulation. Overall results look fine; for a theory paper the theory is not too much of an advance but ok.\n\nThe attacks are straightforward enough, essentially making all other arms unattractive. But I am left wondering what would happen (theoretically) if a robust algorithm was being used, meaning the defender is aware of an attacker of the type in this paper - would the result of the attack be different? My view is possibly not, because the attacker has no much power here as to arbitrarily modify the reward (with a liberal cost allowance that can grow with T), which in my view also makes the attacker's task simple.\nIs the concept of regret w.r.t. to the original rewards meaningful here? (meaning the defender observes changed rewards but collects the true reward in practice) If meaningful, what happens to this original regret value?\n\nIt would have been easier to see the graphs if the quantities were plotted averaged over T. It would have been nice if the dependence on the number of arms was also included in the formulas of the results. (which can be extracted from the regret guarantee).\n\nTypos:\nExample 1 - the word \"contract\" should be contradict\n\n# Clarity, Quality, Novelty And Reproducibility\n\nWriting and clarity is clear, I have no issues.\nSome math proof sketched in the main paper would be better, as I am not sure if the math is hard or easy. \nThe idea appear novel to me.\n\n# Summary Of The Review\n\nI think the work is novel, but not a big advance.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nA DEEP LEARNING FRAMEWORK FOR MUSICAL ACOUSTICS SIMULATIONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe acoustic modeling of musical instruments is a heavy computational process, often bound to the solution of complex systems of partial differential equations (PDEs). Numerical models can achieve a high level of accuracy, but they may take up to several hours to complete a full simulation, especially in the case of intricate musical mechanisms. The application of deep learning, and in particular of neural operators that learn mappings between function spaces, has the potential to revolutionize how acoustics PDEs are solved and noticeably speed up musical simulations. However, such operators require large datasets, capable of exemplifying the relationship between input parameters (excitation) and output solutions (acoustic wave propagation) per each target musical instrument/configuration. With this work, we present an open-access, open-source framework designed for the generation of numerical musical acoustics datasets and for the training/benchmarking of acoustics neural operators. We first describe the overall structure of the framework and the proposed data generation workflow. Then, we detail the first numerical models that were ported to the framework. Finally, we conclude by sharing some preliminary results obtained by means of training a state-of-the-art neural operator with a dataset generated via the framework. This work is a first step towards the gathering of a research community that focuses on deep learning applied to musical acoustics, and shares workflows and benchmarking tools.\n\n1\n\nINTRODUCTION\n\nThe study of the acoustics of musical instruments is a challenging topic. Physics phenomena underlying music making are quite various and include excitation, resonant behavior, as well as the coupling and the dynamic modification of the involved mechanical parts. These make musical instruments remarkable examples of engineering, but also acoustic systems difficult to model. The most accurate simulations that exist today leverage the numerical solution of partial differential equations (PDEs), that are in turn designed to model the specific acoustic behavior of the targeted instruments (Bilbao, 2009). Unfortunately, the majority of the employed solvers are characterized by heavy computational requirements, often leading to restrictive implementation conditions (e.g., low spatio-temporal resolution, high degree of model simplification, non-interactive paradigms).\n\nRecent advancements in deep learning have shown how neural networks may be used to enhance and even replace traditional PDE solvers (Bhatnagar et al., 2019), with the aim to improve performance. In particular, the use of neural operators has yielded promising results in fluids dynamics (Li et al., 2020), suggesting that their application may be successfully extended to revolutionize the simulation of the acoustics and the aeroacoustics of musical instruments. Being completely data-driven, neural operators could be trained to solve acoustics PDEs with synthetic datasets, generated via the large array of traditional numerical implementations that are available in the literature1.\n\nAlthough exciting, this scenario is hindered by a lack of common practices that are needed to bridge the domains of musical acoustics and deep learning. These include shared datasets, benchmarks, as well as general tools to help researchers categorize, manage and employ acoustics data for training and inference. The aim of our research is to foster the rapid growth of an active community where\n\n1In this scenario the only constraint would be computational time—an affordable caveat when generating\n\ntraining sets.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthese common practices could be discussed and formalized, along with the overall emerging field of deep learning-based musical acoustics. In line with this mission, in this work we present the Neuralacoustics framework, a collection of open-access/open-source scripts and tools designed to address the aforementioned needs. In particular, we provide an in-depth description of the dataset generation workflow proposed as part of the framework, and we introduce the first numerical models available in it. We also discuss preliminary results obtained by training a state-of-the-art neural operator for the solution of a simple acoustics problem, using exclusively the tools available in the framework.\n\n2 BACKGROUND\n\nMusical Acoustic Simulations. In the musical domain, the practice of designing mathematical models of instruments is often referred to with the term physical modeling synthesis. Common techniques include modal synthesis (Causse et al., 2011) and digital waveguides (Smith, 1992). Yet, the most precise techniques rely on numerical analysis (Castagn ́e & Cadoz, 2003) (e.g., finite elements, finite differences). Numerical models implement solvers of PDE systems; they can finely simulate fundamental aspects of musical acoustics, like wave propagation and aeroacoustics, as well as physical phenomena beyond instruments and music (Yokota et al., 2002; Arnela & Guasch, 2014). The downside of numerical approaches lies in the computational load of the resulting models, as well as in the amount of parameters they have to comprise to properly simulate the instrument’s behavior.\n\nOf particular interest to our work is the case of time-domain simulations of musical instruments (Bilbao, 2009). In this context, the PDEs solved by the models describe the relationship between previous and next states of the instruments, organized over discrete time steps. Other than taking into account time-varying acoustic excitation of the instruments, this approach potentially enables the design of interactive models. Despite the high computational requirements of numerical analysis, real-time interactive models of musical instruments have been designed in recent years (Sosnick & Hsu, 2010; Allen & Raghuvanshi, 2015; Zappi et al., 2017). Unfortunately, this approach relies on expensive dedicated hardware (GPUs) and implementations are characterized by noticeable technical constraints, that limit access to models’ parameters and interaction (Renney et al., 2022). As a result, numerical analysis is employed for the greater part to model simple musical systems2 (Bilbao et al., 2019), or for batch (i.e., non-real-time) simulations (Bilbao & Chick, 2013; Arnela & Guasch, 2014) that may require run-times of several hours. In both cases, the applicability as well as the intelligibility of the resulting models are heavily hindered.\n\nDeep Learning and PDE Solvers. Recently, deep learning has been successfully explored for the generation of PDE solvers describing time-dependent problems (Blechschmidt & Ernst, 2021; Li et al., 2020). These neural solvers may reduce the overall computational requirements of traditional ones, while approximating their output with a remarkable degree of precision. One of the simplest examples of neural solvers consists of deep convolutional neural networks parametrizing the operator that maps inputs and outputs (i.e., solutions) of the PDEs (Bhatnagar et al., 2019; Khoo et al., 2021). The limitation to this approach lies in its dependence on the chosen mesh, meaning that it is not possible to compute solutions outside the discretization grid used for training. Physics informed neural networks solve this issue, as they are mesh-independent and designed to work alongside classical schemes (e.g., Runge-Kutta) (Raissi et al., 2019). Although capable of addressing problems in the small data setting and with high dimensionality (Blechschmidt & Ernst, 2021), they are often employed to solve time-dependent PDEs that share many similarities with the ones modeling musical acoustics—e.g., Navier-Stokes equations (Rudy et al., 2017; Font et al., 2021; Cai et al., 2022). Being only partially data-driven, this approach requires to tailor the network to a specific instance of the PDEs and to repeat training at any given new input.\n\nMost of the individual advantages of the approaches introduced so far are collated in neural operators (Li et al., 2020). Neural operators are mesh-free operators that require no prior knowledge of the underlying PDEs. They learn mappings between infinite-dimensional spaces of functions relying only on a finite collection of observations; and they can be used without retraining to solve PDEs\n\n2These numerical models can be deemed as “simple” only if compared to the complexity of actual acoustic\n\ninstruments.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nwith different discretizations. Although very recent, they showed promising results not only in fluid dynamics (Li et al., 2020), but also in the solution of wave equations (Guan et al., 2021).\n\n3 DEEP LEARNING AND MUSICAL ACOUSTICS\n\nThe application of deep learning to musical acoustics simulations is less straightforward than what it may seem. In the most general sense, the problem can be framed as mapping the state of a numerical model across the last Tin time steps to its state across Tout future time steps. Both convolutional neural networks (He et al., 2016; Ronneberger et al., 2015; Wang et al., 2020) and neural operators can be used to approximate this mapping with high degrees of confidence (Li et al., 2020), in most cases treating the input tensors like a time series of images/video frames. To this end, such networks have been trained by using numerical datasets that exemplify how the target acoustics model evolves over time given a set of initial conditions; and during inference, they can be used auto-regressively to generate a continuous output. These are promising results and stem from a working scenario that appears to align well with the general domain of acoustic problems.\n\nHowever, these examples of PDE neural solvers do not take into account two important aspects that are specific to musical acoustic simulations. The first one pertains to the excitation of musical models. Rather than simply simulating the behavior of an instrument set into motion by an initial condition, acoustics numerical models can account for the effects of continuous excitation functions, that may drive the instrument throughout the full duration of the simulation. Examples of continuous excitations include basic sinusoidal waves, as well as gaussian pulses used to simulate mallet strikes on membranes and plates (Sosnick & Hsu, 2011), and glottal pulse trains that resonate in singing vocal tracts (Rosenberg, 1971; Guasch et al., 2016). In more advanced simulations, continuous excitation is not pre-computed; it is outputted by a self-oscillating system coupled with the main acoustics model, a common example being a reed coupled with the bore of a woodwind (Bilbao et al., 2015; Allen & Raghuvanshi, 2015). An excitation can start at any given time step of the simulation and the effects of consecutive/overlapping functions may be quite difficult to predict, especially in non-linear models. The training strategies explored so far in deep learning to predict the solution of time-dependent PDEs are not designed to capture this aspect of musical interaction.\n\nA second aspect that is missing from the current state-of-art time-dependent PDE neural solvers is input/output heterogeneity (Figure 1). To allow for the synthesis of the instruments’ sound, a numerical model has to output a field representing the physical quantity where the acoustic wave is propagating. A practical example is the model of a membrane that outputs how its displacement changes over time, with respect to its equilibrium position. In the simplest case, the output acoustic field represents the next state of the system. This means that, at any given time t, the latest output of the PDE becomes the input for the computation of the solution at time t + 1. This simple mapping can be approximated by networks characterized by feature and prediction tensors that represent the same single (acoustic) quantity, and that consequently allow for a straightforward auto-regression mechanism during inference. To our knowledge, this is the only working scenario explored in the literature on the application of deep learning to time-dependent PDEs (e.g., input/output flow velocity (Wang et al., 2020), input/output vorticity (Li et al., 2020)). Yet, in more complex numerical models the state of the system may be composed of more than one field, each representing a different physical quantity. This is required when the model implements an implicit solver, meaning that the next acoustic output is calculated via the solution of a coupled system of equations—hence the necessity for an extra state quantity. Other examples include models where the boundary conditions and/or the acoustic properties of the simulated materials vary over time, due to musical interaction (Bilbao et al., 2019; Zappi et al., 2017). We can refer to them as acoustics parameters.\n\nMoreover, these two aspects of musical instruments’ numerical modeling are often intertwined. While in some cases the excitation is directly applied to the field where wave propagation is simulated (and sampled for audio output), in many other models it is the additional state field that gets altered by the excitation function, only implicitly affecting the simulation output. This is the case for the simulation of woodwinds, where the state of the system is represented by both an acoustic pressure field and a flow velocity field, the latter carrying the velocity excitation incoming from the reed. In similar contexts, the input/output state mapping (including the excitation mechanism) is not trivial to approximate via a neural network, and the methodologies discussed in the literature cannot be applied in a straightforward manner.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Neural network approximating the mapping between input/output states representing the same physical quantity (top); neural network approximating the mapping between heterogeneous input/output states, driven by continuous excitation and dynamic acoustics parameters (bottom). Only the first working scenario has been explored in the literature.\n\nNovel network designs and training strategies may be explored that model these aspects of musical acoustics via deep learning. The fundamental requirement for the implementation of such algorithms is the availability of large datasets, that carry all the information needed to frame both the acoustic behavior of the simulated instrument and the inner workings of the solvers. This translates into storage of full state fields and excitations, along with standardized access methodologies for the extraction of data points as part of training sets.\n\n4 DATASET GENERATION FRAMEWORK\n\nThe Neuralacoustics framework stems from the necessity to generate musical acoustics datasets that could be easily employed in deep learning. It consists of a collection of Python implementations of numerical models of musical instruments, embedded in a modular structure that facilitates extensibility and allows for the application of a model-independent workflow. Its overall structure and some of its features were inspired by the repository that in 2021 accompanied the work of Li et al. (2020). While building on this previous work, as discussed in the previous section we propose a framework that is specifically tailored to the case of musical instrument modeling and designed for extensibility. From a data-centric perspective, the design specifications of our framework adheres to the following constraints: the output of the acoustics simulations must be organized in data structures that are compatible with standard machine learning frameworks; such data structures must be easy to move between local and remote machines; and the output of each simulation must be easy to replicate.\n\nThe proposed framework can be accessed here3. It is written in Pytorch and requires the installation of a few additional libraries, mainly for the visualization of the acoustics simulations and for logging purposes. Pytorch was chosen for its flexibility and scalability on parallel architectures, yet the resulting datasets are not tied to this specific language (more details in Section 4.2). The dataset generation workflow that we propose acts upon three main types of components/scripts: solvers,\n\n3https://anonymous.4open.science/r/neuralacoustics-737A/README.md\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nnumerical models and dataset generators. In the following subsections, we detail these components and then we introduce the workflow in each of its steps.\n\n4.1 MAIN COMPONENTS OF SYNTHETIC MUSICAL ACOUSTICS DATASET GENERATION\n\nSolvers. Solvers implement numerical solutions of highly parametric PDE systems, capable of modeling entire families of musical instruments. Regardless of the numerical method employed (e.g., finite difference, finite elements), All solvers have both a set of specific acoustics parameters, that depend on the implementation details, and a set of common parameters (e.g., domain size, simulation duration).\n\nCurrently, the framework includes three solvers, all based on finite-difference time-domain schemes. The first one was originally proposed by Adib (2000) and solves a PDE system capable of the simulation of damped transverse wave propagation in a two-dimensional medium. It can be used to model the basic acoustics of membranes, thin plates and rudimentary cymbals. The other two solvers tackle acoustic pressure propagation in 2D, and were ported from the OpenGL implementations proposed by Allen & Raghuvanshi (2015). The former is linear and it can be used to approximate woodwind bores; the latter includes non-linearities typical of brass instruments.\n\nNumerical Models. Numerical models simulate specific musical instruments. To do so, each of these scripts loads a solver and sets some of its acoustics parameters, imposing for example constraints on the shape of the domain (i.e., spatial boundary conditions) or on the acoustic properties of the simulated materials. Furthermore, numerical models are characterized also by an excitation algorithm. Excitations can work as initial conditions simply aimed at setting the model into motion (e.g., initial displacement of a membrane), or as continuous models that excite the instrument throughout the whole duration of the simulation. By setting the parameters of the underlying PDEs and defining an excitation input, numerical models can be used to simulate not only different instruments, but also different playing configurations of the same instrument (e.g., a membrane hit by a stick or by a mallet). Each numerical model exposes controllable parameters. These include those pertaining to the excitation algorithm implemented in the script (e.g., the location of a hit, the area where an initial condition is applied), as well as any parameter of the solver that is not hard-coded by the model. These controllable parameters provide the ability to “tune” the behavior of the instrument and allow for the generation of datasets via the mechanism described in the next paragraphs.\n\nFor now, we included into the framework only four simple models, based on the aforementioned solvers. In particular, we implemented three models that simulate vibrating membranes, using as excitations impulses, noise and single spatial frequencies that match the horizontal modes of the surface, respectively. The fourth model leverages the linear acoustic pressure propagation solver to excite woodwinds bores with air wave pulses.\n\nDataset Generators. Dataset Generators consist of algorithms that load a specific model and automatically sample its parameters’ space, effectively running large numbers of simulations of the same instrument/configuration. The framework includes two types of generators: random generators and grid-based generators. Random generators explore the parameters’ space of the model using Python/Pytorch pseudo-random algorithms, driven by an arbitrary seed. This ensures determinism while avoiding clear patterns, and facilitates the reproducibility of results across different machines4. The number of random samples to take (i.e., the size of the resulting dataset) is passed to the generator as an input parameter. Grid-based generators do not rely on any random calculation, rather they sample the parameters’ space in a linearly fashion. Each parameter to sample is assigned a range and a sampling step, then all the possible combinations of parameters are automatically computed—forming a “grid”. Differently from the case of random generators, the total number of samples is not arbitrary but depends on the defined grid, and the resulting datasets need to be shuffled before being used for training.\n\nMuch like the case of numerical models, dataset generators expose a set of parameters. Example generator parameters include ranges and steps of the model’s parameters to sample (for grid-based generators) and the requested number of dataset entries and the current seed (for random generators).\n\n4We though noted that Pytorch deterministic algorithms may result in slightly different output numbers\n\nwhen the same generator is executed on different computer architectures.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4.2 DATASET GENERATION WORKFLOW\n\nThe main component scripts are not designed to be run directly. The framework includes instead an entry-level dataset generation script, that allows for the correct use of generators, models and solvers, and partially hides the complexity of the underlying code and dependencies. Once launched, this script collects all the simulation data computed by a chosen generator script into an actual dataset. More specifically, dataset entries represent complete simulations, each associated to a different set of parameters. In line with what discussed in Section 3, they consist of dictionaries containing all the inputs to the model (excitations and variable acoustics parameters5) and output solutions encompassing all state fields (rather than the acoustic output only), for every simulation time step.\n\nThe way in which datasets are structured and stored reflects the first two data-centric constraints we introduced at the beginning of this section, i.e., compatibility and portability. The entry-level dataset generation script outputs MAT-files (“.mat” extension), one of the de-facto standard data formats in machine learning. The generation of acoustics datasets may yield very large files, especially when simulations span big domains and long time windows. To avoid exceeding the maximum size supported by the native file system where the code runs, the entry-level dataset generation script is capable of splitting the dataset into several “chunks”, each represented by an individual MAT-file. This solution comes in handy also when moving datasets between remote locations, for the transfer of large files may be subject to failures due to connection instability. Eventually, when a dataset is loaded in memory (to train a network or to visualize its content), all the chunk files are transparently combined together back into a single dataset (more details in the next subsection).\n\nThe complete dataset generation workflow can be summarized as follows. First, the user has to locate a numerical model that represents the specific instrument the dataset will exemplify. Then, a dataset generator needs to be chosen, that samples the numerical model of interest. In this step, the user shall adjust the exposed parameters to make sure that the sampling procedure will result in data that well represent the instrument and its specific playing configuration6. Finally, the user can set and run the entry-level dataset generation script and the resulting dataset will be computed and stored according to the requested settings. The entry-level script compiles a log file too. It contains a summary of the content of the dataset and reports location and all parameters of the employed scripts. Any log file can then function as a unique configuration file, that when passed to the entry-level script allows users and remote collaborators to automatically obtain an exact copy of the original dataset, avoiding the hassle of moving large files or going through the full workflow again. The only caveat is that the same version of the framework has to be installed on both ends. This mechanism was designed to respect the third data constraint (replicability). Moreover, every step of the workflow can be carried out via command line, making it straightforward to check out the framework on remote machines and generate datasets on high performance computing clusters.\n\n4.3 UTILITY TOOLS: VISUALIZATION AND DATA POINTS EXTRACTION\n\nIn the root of the framework, the user can find two further entry-level scripts, one designed to test numerical models, the other to visually inspect the content of datasets. Both scripts sequentially visualize the solution fields as plots, showcasing the propagating acoustic waves as an animation. The main difference between the two scripts is that the former computes the frames in real-time by means of running the tested model, while the latter extracts them from the inspected dataset. To this end, the dataset visualization script implements a windowing system that allows for the extraction of diverse sets of data points from the same dataset. To understand this mechanism, it is necessary to emphasize the difference between an “entry” within the dataset and a “data point” extracted from it. As detailed in Section 4.2, each entry in a Neuralacoustics dataset consists of a time series, representing the simulation of an instrument over a certain number of time steps. In the most general sense, a Neuralacoustics data point can be any sub-series of consecutive time steps found in a dataset entry. More than one data point can be extracted from a single dataset entry and the maximum size of a data point is equal to the total number of time steps of the simulation. In the latter case, only a single data point is extracted, which coincides with the full entry.\n\n5Currently, all the models ported to the framework sport fixed acoustics properties, hence the dictionaries\n\ngenerated from these models do not include variable acoustics parameters as part of the input tensors. 6The specific metrics depends on the purpose and the application the dataset is computed for.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nThe windowing algorithm is part of a data point extraction tool, that retrieves data points by means of repeatedly windowing the entries’ time series (collecting a data point per window). The process is depicted in Figure 2. The main windowing parameters are: the size of the windows, the stride applied between consecutive windows, and the dataset entry where the windows are applied. A further parameter allows to repeat the extraction over a number of consecutive entries, increasing the total number of frames visualized. To simply visualize the full simulations within each entry, the user can set either the size of the window equal to the number of time steps of each entry, or the stride equal to the size of the window.\n\nFigure 2: Windowing algorithm used to extract data points from Neuralacoustics datasets. The same dataset entry is processed via two different sets of parameters, to exemplify the extraction of different data. The diagram depicts only the solution time steps (acoustic fields) stored in the entry’s dictionary.\n\nWhen used to collate a training set, the extraction tool keeps applying the windows across each entry of the dataset, until the number of data points requested by the user is obtained. It is important to notice, though, that the values returned by the tool contain more information than simple feature vectors, for two reasons. First, the size of the window is defined as Twin = Tin + Tout; hence, the extracted values contain both the states pertaining to the initial Tin time steps (i.e., the features) and the states pertaining to the following Tout time steps (i.e., the label to predict). Second, as described in Sections 3 and 4.2, the values stored in each dataset entry may include heterogeneous inputs and outputs. When extracted, these pieces of data may be combined in different ways to represent the actual state of the model in each time step, depending on the specifics of the problem and of the solver. This means that some extra logic is necessary to define features and labels and be able to employ the data points for training. While increasing the overall complexity of the framework, this design detail makes data handling as generic as possible, allowing for the application of the same workflow even when very different numerical models/simulations/solvers and neural operators are in use. We share the details of a simple example of data handling logic for training in the next section.\n\n5 TRAINING NEURAL OPERATORS\n\nThe Neuralacoustics framework also aims to facilitate the design of neural operators capable of solving acoustics PDEs. Currently, this part of the framework is comparatively less developed than the dataset generation workflow introduced in Section 4, but basic training and evaluation pipelines have been completed. Besides, preliminary training results have been obtained, which will be discussed in Section 5.1.\n\nSimilarly to numerical models, neural operators are implemented in Pytorch and expose hyperparameters. As of now, only a two-dimensional Fourier neural operator as introduced in (Li et al., 2020) is available in the framework. This was chosen as the first network to be ported into the framework because of its ability to solve time-based problems, as well as for its somewhat simple\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\ninternal structure7. The hyperparameters exposed by the port include spectral layer stack number, hidden size and Fourier transform modes. The network performs prediction one time step at a time and produces consecutive results in an auto-regressive manner.\n\nIn the root directory, the user can find a script that carries out the full network training pipeline. The user shall specify the training configuration, including: dataset and network selection, size of training and validation sets, data points extraction details (e.g., Tin and Tout), learning and optimization parameters. This separation of training parameters and network hyperparameters ensures generalizability, potentially supporting the training of networks that diverge from the structure of the implemented Fourier neural operator. The training script operates on the dataset via the data point extraction tool described in Section 4.3. The logic used to combine inputs (excitations) and outputs (solution fields) into state tensors (i.e., definition of features and labels) is still hard-coded in the script. Per each dataset entry, features are defined as the first Tin state tensors within the extracted time window, each computed as the sum between the current acoustic solution and the excitation at the next time step. This is the simplest way to embed continuous excitation into the input state of a numerical model. The label of each data entry consists of the last Tout state tensors, simply defined as the acoustic solutions in those time steps. Once both data points and network are set, the script trains the neural model with the specified learning parameters. Weight checkpoints are also saved, using a customizable step interval.\n\nSimilarly to the data generation workflow, the training script compiles a log file that serves as a summary of the training details. All parameters related to the training process, alongside dataset generation parameters and network hyperparameters, are recorded so that the log file itself could be used as a unique configuration file for thoroughly replicating the neural model.\n\nCurrently an entry-level evaluation script provides intuitive insights into a trained network’s performance. The users shall specify the network’s checkpoint to evaluate, as well as the exact dataset and data entry for running inference on. This script visualizes the predicted domain state along with the ground truth acoustic solution (computed by the original numerical model and stored in the dataset) and their difference, for a chosen number of time steps. An example of visualization is provided in next section.\n\n5.1 PRELIMINARY RESULTS\n\nWe trained a two-dimensional Fourier operator network on a dataset generated with the membrane model including impulse excitation as initial condition (Section 4.1, Numerical Models paragraph). Such a working scenario was chosen because the Fourier neural operator was originally not designed to learn mappings that include continuous excitation. We sampled the model with a generator that randomizes the location of the impulse across the domain (i.e., the area of the membrane), as well as the amplitude of the impulse. Each simulation spans 25 time steps. Both scripts are available in the framework. The width and the height of the 2D domain are both set to 64. The size of the training set is 25000 data points, while 5000 data points are used as validation set. Data points are extracted only from the first 20 time steps of each entry (this is a further setting available in the training workflow). The state of the model in each time step is chosen as the solution tensor in that step and the excitation tensors stored in the dictionary are ignored, except for the first one (initial condition). The input step number Tin is 10 (the feature vector is composed of the previous 10 states) and the output time step Tout is 1 (the label is the next state). The neural operator has 4 spectral layers with 12 Fourier modes within each layer and a hidden size of 20. We trained the network for 800 epochs with Adam optimizer and an initial learning rate of 0.001.\n\nThe final loss amounts to 0.0355. Figure 3 shows two snapshots of the visualization generated via the evaluation script. The test set extracted by the dataset is not limited to the first 20 time steps, but spans the full duration of the simulations (25 steps). As can be observed, the 2D domain’s state outputted by the trained network is a close estimate of the ground truth simulated via the numerical solver. Moreover, through an auto-regressive inference approach, the trained network is able to carry out a good prediction of how an impulse propagates beyond the first 20 time steps exemplified in the training set. This suggests that the learning captured—at least in part—the underlying physical mechanisms of wave propagation.\n\n7The source code of the Fourier neural operators is available at https://github.com/zongyi-li/\n\nfourier_neural_operator\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Evaluation visualization. From left to right, the three columns respectively shows the 2D domain state (displacement) predicted by the trained network, the ground truth, and their difference. The top row shows the states at time step #15 (exemplified in the training set), and the bottom row shows the states at time step #25 (never reached in the training set).\n\n6 CONCLUSION AND FUTURE WORK\n\nIn this paper we introduced the Neuralacoustics framework, an open access and open source collection of tools designed to facilitate the application of deep learning in the context of acoustics simulations and musical instrument modeling. In particular, the framework responds to the need for standards to combine the output of diverse acoustics simulations into datasets, and to use them for training. The main components of the framework are numerical models and acoustic PDE solvers. These are arranged in a modular structure, that permits the application of a robust workflow for the generation of heterogeneous musical acoustics datasets. The generation process outputs data structures compatible with standard machine learning frameworks, and is designed to maximize portability and replicability. The Neuralacoustics framework features also a section dedicated to the training and the evaluation of neural operators using the generated acoustics datasets. While still in progress, this part of the workflow is functional and leverages a modular structure similar to the one proposed for the dataset generation process.\n\nAt the current stage of development, the framework includes Pytorch implementations of four numerical models and three solvers, all previously introduced in the literature. As showcased by the preliminary data presented in this work, these are enough to train a state-of-the-art neural operator and test its performance in the context of basic musical acoustics. Furthermore, the current implementations are designed to work as blueprints for the porting of additional models and solvers. With the release of this work, we aim to empower all the researchers working in this emerging field with new tools for the development and the sharing of their own implementations. The release of frameworks, common practices and benchmarks have long benefited the advancement of machine learning as well as its application in various domains (Downie et al., 2005; Schedl et al., 2014; Hu et al., 2020; Lu et al., 2021). We believe that our effort may have a similar impact on the development of novel deep learning approaches to acoustics modeling, and may facilitate the onset of collaborations among researchers from both fields.\n\nThe framework is constantly improved and extended. Besides the port of new numerical models (instruments) and neural networks/operators, we are currently working on three main fronts. The first one is quantitative evaluation on large datasets, with methods that in the near future will allow researchers to use the framework to precisely compare performance across multiple networks. The second one is direct performance comparison between neural and numerical models, including computational time/load and qualitative analysis. Finally, we are leveraging the framework to explore novel training strategies and deep learning architectures, designed to approximate the acoustics of more complex, interactive musical instruments’ models.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nArtur B Adib. Study notes on numerical solutions of the wave equation with the finite difference\n\nmethod. arXiv preprint physics/0009068, 2000.\n\nAndrew Allen and Nikunj Raghuvanshi. Aerophones in flatland: Interactive wave simulation of\n\nwind instruments. ACM Transactions on Graphics (TOG), 34(4):1–11, 2015.\n\nMarc Arnela and Oriol Guasch. Two-dimensional vocal tracts with three-dimensional behavior in the numerical generation of vowels. The Journal of the Acoustical Society of America, 135(1): 369–379, 2014.\n\nSaakaar Bhatnagar, Yaser Afshar, Shaowu Pan, Karthik Duraisamy, and Shailendra Kaushik. Prediction of aerodynamic flow fields using convolutional neural networks. Computational Mechanics, 64(2):525–545, 2019.\n\nStefan Bilbao. Numerical sound synthesis: finite difference schemes and simulation in musical\n\nacoustics. John Wiley & Sons, 2009.\n\nStefan Bilbao and John Chick. Finite difference time domain simulation for the brass instrument\n\nbore. The Journal of the Acoustical Society of America, 134(5):3860–3871, 2013.\n\nStefan Bilbao, Alberto Torin, and Vasileios Chatziioannou. Numerical modeling of collisions in\n\nmusical instruments. Acta Acustica united with Acustica, 101(1):155–173, 2015.\n\nStefan Bilbao, Michele Ducceschi, and Craig Webb. Large-scale real-time modular physical modIn Proc. Int. Conf. On Dig. Audio Eff.(DAFx 2019), Birmingham, UK,\n\neling sound synthesis. 2019.\n\nJan Blechschmidt and Oliver G Ernst. Three ways to solve partial differential equations with neural\n\nnetworks—a review. GAMM-Mitteilungen, 44(2):e202100006, 2021.\n\nShengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physicsinformed neural networks (pinns) for fluid mechanics: A review. Acta Mechanica Sinica, pp. 1–12, 2022.\n\nNicolas Castagn ́e and Claude Cadoz. 10 criteria for evaluating physical modelling schemes for\n\nmusic creation. In DAFx03, pp. 7–p, 2003.\n\nRene Emile Causse, Joel Bensoam, and Nicholas Ellis. Modalys, a physical modeling synthesizer: More than twenty years of researches, developments, and musical uses. The Journal of the Acoustical Society of America, 130(4 journal=Computer music journal, volume=16, number=4,): 2365–2365, 2011.\n\nJ Stephen Downie, Kris West, Andreas Ehmann, and Emmanuel Vincent. The 2005 music information retrieval evaluation exchange (mirex 2005): Preliminary overview. In 6th int. conf. on music information retrieval (ismir), pp. 320–323, 2005.\n\nBernat Font, Gabriel D Weymouth, Vinh-Tan Nguyen, and Owen R Tutty. Deep learning of the Journal of Computational Physics, 434:110199,\n\nspanwise-averaged navier–stokes equations. 2021.\n\nSteven Guan, Ko-Tsung Hsu, and Parag V Chitnis. Fourier neural operator networks: A fast and general solver for the photoacoustic wave equation. arXiv preprint arXiv:2108.09374, 2021.\n\nOriol Guasch, Marc Arnela, Ramon Codina, and Hector Espinoza. A stabilized finite element method for the mixed wave equation in an ale framework with application to diphthong production. Acta Acustica united with Acustica, 102(1):94–106, 2016.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020.\n\nYuehaw Khoo, Jianfeng Lu, and Lexing Ying. Solving parametric pde problems with artificial neural\n\nnetworks. European Journal of Applied Mathematics, 32(3):421–435, 2021.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020.\n\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664, 2021.\n\nMaziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686–707, 2019.\n\nHarri Renney, Silvin Willemsen, Benedict Gaster, and Tom Mitchell. Hypermodels-a framework for\n\ngpu accelerated physical modelling sound synthesis. In NIME, volume 22. PubPub, 2022.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234–241. Springer, 2015.\n\nAaron E Rosenberg. Effect of glottal pulse shape on the quality of natural vowels. The Journal of\n\nthe Acoustical Society of America, 49(2B):583–590, 1971.\n\nSamuel H Rudy, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Data-driven discovery of\n\npartial differential equations. Science advances, 3(4):e1602614, 2017.\n\nMarkus Schedl, Emilia G ́omez, Juli ́an Urbano, et al. Music information retrieval: Recent developments and applications. Foundations and Trends® in Information Retrieval, 8(2-3):127–261, 2014.\n\nJulius O Smith. Physical modeling using digital waveguides. Computer music journal, 16(4):74–91,\n\n1992.\n\nMarc Sosnick and William Hsu. Efficient finite difference-based sound synthesis using gpus. In\n\nProceedings of the Sound and Music Computing Conference, pp. 42–44, 2010.\n\nMarc H Sosnick and William T Hsu. Implementing a finite difference-based real-time sound syn-\n\nthesizer using gpus. In NIME, pp. 264–267. Citeseer, 2011.\n\nRui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physicsinformed deep learning for turbulent flow prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1457–1466, 2020.\n\nTakatoshi Yokota, Shinichi Sakamoto, and Hideki Tachibana. Visualization of sound propagation\n\nand scattering in rooms. Acoustical science and technology, 23(1):40–46, 2002.\n\nVictor Zappi, Andrew Allen, and Sidney S. Fels. The hyper drumhead: Making music with a In Proceedings of the 2017 International Computer Music massive real-time physical model. Conference, ICMC 2017, Shanghai, China, October 16-20, 2017. Michigan Publishing, 2017. URL https://hdl.handle.net/2027/spo.bbp2372.2017.026.\n\n11",
    "reference": "# Summary Of The Paper\n\nThe paper discusses a new framework focusing on musical acoustics simulation. Traditional numerical methods for musical acoustics simulation involve solvers for partial differential equations to model the physics of musical instruments and sound propagation. These methods are very slow. The authors present the Neuralacoustics framework which implements various components necessary to support deep learning-based approaches to musical instrument simulation. It includes various solvers for numerical models which enable dataset generation which in turn can be used to train neural networks to approximate the numerical models. The authors provide a walkthrough of their framework and how to go about training a neural operator for a specific numerical model.\n\n# Strength And Weaknesses\n\nStrength:\n- The authors identify acoustic simulation as a novel area of physics-based models where deep neural nets have not yet been widely applied. \n- They develop a complete framework to facilitate deep learning-based research by implementing existing physical models in a familiar environment (Pytorch) for deep learning researchers. This is potentially very helpful for developing a new community in the research area.\n\nWeaknesses:\n- It is not clear to me how relevant this paper is for the community at ICLR. A large portion of this paper describes the framework and how it can be used to generate data using python implementations of numerical models of instruments. However, only a small part of the paper actually talks about modeling these numerical models with deep networks which may be more relevant to the community at large. \n- Maybe this is obvious to others but I would be interested in seeing a comparison of the advantages/disadvantages of neural methods vs numerical methods for modeling the acoustics. For example, a user study comparing the outputs of two simulators, a comparison of simulation speed, etc.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing is clear, but the paper reads like a software API guide for the framework at times. The content presented is novel indeed.\nThe authors do plan to release their framework, hence the results shown in the work should be reproducible.\n\n# Summary Of The Review\n\nThe paper describes a new framework for musical acoustics simulation which facilitates deep learning-based research. The paper goes over the various components in the framework followed by a case study of how it can be used for modeling a specific instrument type using neural operators. While I do not see any big issues in the technical details of the paper but I do find it difficult to gauge the interest in this work in the ICLR community. Hence I give the paper a rating of weak reject.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nHIDDEN SCHEMA NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nStarting from the assumption that a large proportion of semantic content is necessarily relational, we introduce a neural language model that discovers networks of symbols (schemata) from text datasets. Using a variational autoencoder (VAE) framework, our model encodes sentences into sequences of symbols (composed representation), which correspond to the nodes visited by biased random walkers on a global latent graph. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to train our model on language modelling and commonsense knowledge generation tasks. Qualitatively, the model is able to infer schema networks whose nodes (symbols) can be interpreted as encoding different aspects of natural language (as e.g. topics or sentiments). Quantitatively, our results show that the model successfully interprets the inferred symbol sequences, as it achieves state-of-the-art scores on language modeling benchmarks. Source code to reproduce all experiments is provided with the supplementary material.\n\n1\n\nINTRODUCTION\n\nMuch of the developmental and causal theories of human cognition are predicated on relational structures of knowledge that naturally exhibit compositionality. Semantic content is intrinsically relational, as one is only able to explain a given unit of knowledge – such as a concept, word or perception – insofar as there are other units of knowledge which relate to it (Block, 1986). Thus we can partially construe a concept through its relationships to other concepts (like when we say “a dog is an animal that barks”), just as we can partially construe it through its relationships to our perceptions (when we say “that is a dog”, whilst pointing to a dog on the street) or the words we use (when we use the word dog to refer to the concept dog). Likewise, we can partially construe words not only through their relationships to concepts or percepts, but also through their relationships to other words, as words that occur in the same context tend to have similar meanings (Harris, 1954; Firth, 1957). Note that is precisely this contextual semantic content of words what we have explicit access to, when processing our raw text datasets. On the other hand, generalization, reasoning and understanding seem to be inevitably tied to the compositional nature of knowledge. Indeed, the ability to compose a set of knowledge units (and their relations) into new, more complex relational units, which can be deployed to understand and reason about unseen data – a feature usually referred to as combinatorial generalization – is regarded as key to human-level intelligence (Fodor & Pylyshyn, 1988; Fodor & Lepore, 2002; Lake et al., 2017; Battaglia et al., 2018). Relational structures allowing for compositionality thus seem to comprise not a sufficient, but a necessary attribute of any representation scheme that strives for the generalization power of human cognition.\n\nFrom the computational side, if one is to inform any modern machine learning model with such structural characteristics, one will initially encounter the problem of finding suitable primitives or data structures. In natural language processing (NLP), for example, it has become common place to leverage distributed continuous representations of words (Bengio et al., 2003) for different downstream tasks. Such representations are trained to encode average contextual semantics – precisely the kind of semantic content typical of word co-occurrence relations we mentioned above – into a semantic space, which allows meaning to change continuously within it (Mikolov et al., 2013). Yet, despite earlier attempts (Mitchell & Lapata, 2008), it is unclear whether such representations can be meaningfully composed into representations of, say, unseen sentences and thus mimic the compositional character of natural language. More recently, contextualized continuous word\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Left: Diagram of Hidden Schema Network model. Center: Decoder architecture as a modified GPT-2 model of M layers, with a pseudo-self-attention mechanism to attend to the schema ej1:jL . Please see the Appendix for details. The “c” operations labels concatenation. Right: Encoder architecture as BERT model, followed by a single Transformer block. In both center and right figure purple shaded blocks represent submodules with pretrained parameters. Pink shaded blocks represent submodules with randomly initialized parameters.\n\nrepresentations inferred by deep learning architectures have shown spectacular results in many NLP tasks (Radford et al., 2018; Devlin et al., 2018; Radford et al., 2019; Brown et al., 2020). Their success stems from those models’ ability to infer flexible representations through, inter alia, raw, massive datasets, data-scalable attention mechanisms and minimal inductive biases (Vaswani et al., 2017). These representations are known to not only contain rich contextual word semantics, but also consistently encode sentence-level grammar (Hewitt & Manning, 2019), and the models from which they are obtained seem to implement some notions of compositionality too (Hupkes et al., 2020; Wei et al., 2022). Nevertheless, it is still unclear whether such representations can be composed into representations of novel sentences (Yu & Ettinger, 2020; Bhathena et al., 2020). In fact, most of their syntactic properties are implicit and therefore inferred only a posteriori, typically through probes which neither guarantee their presence, nor establish how they were obtained in the first place (Rogers et al., 2020).\n\nIn this work we use a VAE framework (Kingma & Welling, 2013; Rezende et al., 2014) to develop a language model – the Hidden Schema Network model (HSN) – that enforces, via inductive biases, a discrete, relational structure for sentence representation which allows for compositionality1, while exploiting the well-known advantages of attention models and contextualized, pretrained representations. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage our methodology to translate the implicit lexical and grammatical aspects of language encoded by pretrained BERT and GPT-2 language models into explicit relational structures, and apply the latter on language modelling and commonsense knowledge generation tasks.\n\nOur main contribution is then an exploration of a novel way to integrate discrete (symbols), relational (graphs) and continuous (neural representations) machine learning components into an end-to-end, differentiable representation learning algorithm for natural language modelling. Our aim is thus to try to connect the modern NLP paradigm with classical notions of linguistics, and begin to answer the recent calls for neuro-symbolic integration (Garcez & Lamb, 2020; Cartuyvels et al., 2021).\n\n1Note that throughout the paper we refer only to compositionality of representations and not to the compositional functions that can be implemented by the models we use. The latter, functional compositionality, is studied by e.g. Hupkes et al. (2020).\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\nIn cognitive psychology, a schema is (roughly) defined as a large, complex unit of knowledge representing what is typical of a group of instances (Bartlett, 1932; Piaget, 1948; Rumelhart, 2017). Marvin Minsky’s frames (Minsky, 1974; 1975) are similar in function to a schema, but perhaps more easily characterized in terms of data structures. We use these terms in a loose fashion, however. Our aim being only to be suggestive of the general problem of knowledge representation (Thagard, 1984). We are in fact concerned with representation schemes for natural language processing. Within the context of linguistics, Jackendoff (1978) argues that there must be a level of representation – the so-called conceptual structures – at which information conveyed by language must be compatible with information coming from sensory systems. Conceptual structures must, he goes on, be able to represent all the conceptual distinctions made by natural language, and provide some degree of compositionality. Earlier computational models implementing (some kind of) conceptual structure rely on either hand-coded (semantic) network representations (Quillan, 1966; Collins & Quillian, 1969; Brachman, 1977) or hand-coded databases (McClelland & Rogers, 2003). Other works focus instead on learning semantic representations directly from text data via topic models (Griffiths et al., 2007b), and even infer latent concept graphs through nonparametric priors (Chambers et al., 2010).\n\nIn sharp contrast with these works, modern, neural-based language models incorporate no explicit linguistic notions, and leverage massive datasets and attention mechanisms, in the form of large pre-trained language models and contextualized, continuous word representations. We build on top of these ideas, while trying to connect back with models of conceptual structure, which necessarily involve discrete representations (van den Oord et al., 2017; Hu et al., 2017; Zhao et al., 2018; Kaiser & Bengio, 2018; Kaiser et al., 2018).\n\n3 HIDDEN SCHEMA NETWORKS\n\nWe address the problem of learning the joint probability distribution over sequences of words, while inferring interpretable representations capturing their semantics. Neural autoregressive language models approximate such distributions with a product over conditional probabilities, such that\n\np(x1:T ) =\n\nT (cid:89)\n\ni=1\n\npθ(xi|x<i),\n\n(1)\n\nwhere x1:T = (x1, x2, . . . , xT ) labels the sequence of words in question, and each conditional is given by (the pdf of) a categorical distribution over some vocabulary of size V . The class probabilities of these conditionals are generally computed as πi = softmax(W · hθ(x<i)), with W ∈ RV ×D trainable, and D the output dimension of hθ, a deep neural network model with parameter set θ (Bengio et al., 2003). Models of this form allow for tractable estimation of and sampling from either the joint distribution, or any product of the conditionals in Eq. 1. Indeed, their recent implementation in terms of large-capacity, self-attention architectures such as GPT-2 (Radford et al., 2019) has been shown to generate syntactically correct, diverse and fluent text. Yet, most of the linguistic structure encoded by the output representations of these models is implicit and difficult to interpret (Rogers et al., 2020). In what follows we shall condition the joint distribution of Eq. 1 on an additional latent, discrete representation which can, at least in principle, capture the relational and compositional features of semantic content.\n\nLet us assume there is a set E = {e1, e2, . . . , eK} of K symbols that encode some high-level, abstract semantic content of natural language. Let this set be the set of nodes of a hidden (semantic) graph G, with adjacency matrix A, so that adjacent (connected) symbols are semantically related. These symbols can generically be defined as learnable, dense vectors in RS, for some dimension S. Without loss of generality, however, we opt below for simple indicator (“one-hot”) vectors of dimension K instead. We define a schema ej1:jL as a sequence of L ≪ K symbols (ej1, ej2, . . . , ejL), where the indices j1, . . . , jL label a subset of connected nodes in G. Accordingly, we refer to G as a schema network. The symbols composing the schemata are chosen through a L-step stochastic process conditioned on G. Partially motivated by research on random walks and human memory search (Griffiths et al., 2007a; Abbott et al., 2012), as well as by the simplicity of their inference, we choose to compose the schemata via biased random walk processes on G, and leave exploring different schema processes for future work . Let us now specify the generative model in detail.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3.1 GENERATIVE MODEL\n\nWe write the joint probability over a sequence x1:T of T words, together with the hidden graph G, as\n\npθ(x1:T , A) =\n\n(cid:88)\n\nz1:L\n\npθ(x1:T |ej1:jL)p(z1:L|A)p(A),\n\n(2)\n\nwhere z1:L labels the sequence z1, . . . , zL of K-dimensional, one-hot vectors representing the node labels j1, . . . , jL visited by a random walker on G, and θ denotes the trainable model parameters. Note that we introduced the one-hot representation of ji for notational convenience, as shall become evident below2. Next, we specify the different components of Eq. 2.\n\nPrior over (global) graph. A prior on the adjacency matrix p(A) allows us to control the topological properties of G. One can choose, for example, random graph models whose degree distribution asymptotically follow a power law (Barabási & Albert, 1999), or unbiased, maximum entropy graph models, with respect to some given constrains (Park & Newman, 2004). For the sake of simplicity we choose a Bernoulli (Erdös-Rényi) random graph model (Solomonoff & Rapoport, 1951; Erdös & Rényi, 1959), for which each link aij is defined via an independent Bernoulli variable with some fixed, global probability p ∈ [0, 1], so that\n\np(A) =\n\nK (cid:89)\n\ni,j=1\n\npaij (1 − p)1−aij .\n\n(3)\n\nThe probability p will be a hyperparameter of our model.\n\nPrior over random walks. The probability p(z1:L|A) of a random walk over the nodes of G can generally be written as\n\np(z1:L|A) = p(z1)\n\nL (cid:89)\n\ni=2\n\np(zi|zi−1, A) =\n\n(cid:32) K (cid:89)\n\n(cid:33) L (cid:89)\n\nρzm\n\n1 m\n\n\n\n\n\nK (cid:89)\n\nK (cid:89)\n\n\n\ni zj zk k,j\n\ni−1\n\nP\n\n ,\n\nm=1\n\ni=2\n\nj=1\n\nk=1\n\n(4)\n\nwhere p(z1) labels the probability of selecting j1 as the starting point of the walk, and it is given by (the pdf of) a categorical distribution over the nodes of G, with class probabilities {ρi}K i=1. Similarly p(zi|zi−1, A) labels the conditional probability of jumping from ji−1 to ji, which we define in terms of a K × K transition probability matrix P. Now, to allow for biased random walks, let each node k on G be given a positive weight fk, so that the probability of jumping from j to k is proportional to fk Akj. We then write the transition probability matrix as\n\nPk,j =\n\nfk Akj i=1 fi Aij\n\n(cid:80)K\n\n,\n\n(5)\n\nso that the motion of the random walker is biased according to the node weights fk. These weights should be understood as encoding aspects of the diffusion dynamics that are independent of the topology of the graph (Gómez-Gardenes & Latora, 2008; Lambiotte et al., 2011). Three comments are in order: first, note that one can also train the prior over walks by making the vectors ρ and f learnable. Second, setting the node weights f = I and the class probabilities ρ = 1 I, with I the K-dimensional vector of ones, yields a uniform random walk over G, i.e. a process in which the walker has equal probability of jumping to any of its neighbors. Third, one can also allow for inhomogeneous random walks in which the probability matrix changes at each step of the random walk. Such processes can be parameterized with a sequence of weights f [1], f [2], . . . , f [L−1].\n\nK\n\nDecoder and likelihood. Just as in Eq. 1, we define the joint probability over word sequences as a product of conditional probabilities, this time conditioned on the schema ej1:jL too, that is\n\npθ(x1:T |ej1:jL) =\n\nT (cid:89)\n\ni=1\n\npθ(xi|x<i, ej1:jL), πi = softmax(W · hdec\n\nθ (x<i, ej1:jL)),\n\n(6)\n\nwith πi the class probabilities of the ith conditional, W ∈ RV ×D trainable, and hdec network model. We let hdec\n\nθ a deep neural θ be a pretrained GPT-2 language model, and modify it to also process the\n\n2Explicitly, ji denotes the index of the non-zero component of zi, i.e. ji = {k ∈ [1, K] : zk\n\ni = 1}, with the\n\nsuperindex k denoting the components of zi.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nschema ej1:jL, but remark that any other model for sequence processing (as e.g. a recurrent neural net) could be used instead. A bit more in detail, to condition GPT-2 on ej1:jL , without perturbing its optimized weights too much, we use the pseudo-self -attention (PSA) mechanism introduced by Ziegler et al. (2019). In a nutshell, this mechanism augments the key and value matrices of GPT-2 in their first L rows with projections of ej1:jL . Figure 1 shows an illustration of the complete decoder model, including the PSA mechanism. Please check Appendix A for the explicit equations of the latter.\n\n3.2\n\nINFERENCE MODEL\n\nThe generative model we presented above is hierarchical. The random graph is shared across all sentences and thus constitutes a global latent object. The random walks, in contrast, are local random variables. Our task is to infer the schema and graph posterior distributions that best describe the collection of word sequences in our dataset. To do this, we approximate the true posterior distribution of these variables with a variational posterior of the form\n\nqφ(z1:L, A|x1:T ) = qφ(z1:L|x1:T , A)qφ(A),\n\n(7)\n\nwhere φ labels the set of trainable parameters. Let us specify each of its components.\n\nPosterior over (global) graph. We model the posterior over the graph assigning again Bernoulli variables to its links, but we let the probability of observing each link depend on the global symbols\n\nqφ(A) =\n\n(cid:89)\n\ni,j\n\npφ(ei, ej)aij (1 − pφ(ei, ej))1−aij , where pφ(ei, ej) = sigmoid(gφ(ei, ej)),\n\n(8)\n\nwith gφ : E × E → R a deep neural network, and pφ(ei, ej) ∈ [0, 1], for all ei ∈ E, the link probabilities. Our reasoning here is that the network gφ should infer graphs connecting symbols which are semantically related via the encoded sentences.\n\nPosterior over random walks (encoder model). Analog to Eq. 4 we model the posterior probability over random walks on G as\n\nqφ(z1:L|x1:T , A) =\n\n(cid:32) K (cid:89)\n\ni=1\n\nρi(x1:T , φ)zi\n\n1\n\n(cid:33) L (cid:89)\n\n\n\n\n\nK (cid:89)\n\nK (cid:89)\n\n(cid:16)\n\nQ[i−1]\n\nk,j\n\ni=2\n\nj=1\n\nk=1\n\n(x1:T , A, φ)\n\n(cid:17)zk\n\ni zj\n\ni−1\n\n\n\n ,\n\n(9)\n\nwhere instead of having a single transition probability matrix, we have a sequence of them, thereby allowing the posterior to capture inhomogeneous random walks. Note that we could have also chosen a mean-field decomposition along the steps of the random walk, simply by either ignoring the dependency on the graph, or making the graph fully connected (see Appendix B.4). Going back to Eq. 9, we model the probabilities over the starting point of the random walks and the transition matrices as follows\n\nρ(x1:T , φ) = softmax(henc\n\n1 ),\n\n(10)\n\nQ[i]\n\nk,j(x1:T , A, φ) =\n\nf [i] k (x1:T , φ) Akj m f [i]\n\nm (x1:T , φ) Amj\n\n(cid:80)\n\n, with f [1], . . . , f [L−1] = exp(henc\n\n2:L),\n\n(11)\n\n1 , henc\n\n2 , . . . , henc\n\nL ∈ RD is the sequence of outputs of a deep neural network model henc\n\nwhere henc φ (x1:T ) processing the input sequence of T words. The model henc φ (x1:T ) must then map a sequence of T vectors to a sequence of L vectors. We define henc φ by a pretrained BERT model (Devlin et al., 2018), followed by a single Transformer block, randomly initialized. The Transformer block processes the T (D-dimensional) outputs from BERT as keys and values, together with a set of L learnable vectors q1:L as queries. The right hand side of Figure 1 illustrates the complete encoder architecture.\n\n3.3 TRAINING OBJECTIVE\n\nTo optimize the parameter sets {θ, φ} of our latent variable model we would, as usual, maximize a variational lower bound on the logarithm of the marginal likelihood pθ(x1:T ) (Bishop, 2006). It is, however, well known that VAE models tend to encounter problems learning representations encoding information about the data – the so-called posterior collapse problem – especially when dealing with\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nGraph G∗ Barabasi Erdos\n\nROC AUC 0.989 ± 0.001 0.94 ± 0.06\n\n||G∗ − G||F 17 ± 2 36.8 ± 0.8\n\n||Grand − G||F 26 ± 1 44 ± 2\n\nN. edges(G) N. edges(G∗) 1360 ± 104 3131 ± 156\n\n291 2092\n\nTable 1: Inference of ground-truth random graphs\n\nnatural language (Bowman et al., 2015). To solve this issue practitioners resort to maximizing the variational lower bound, together with the mutual information between data and representations (Zhao et al., 2018; Fang et al., 2019; Zhao et al., 2019). We follow this same route and show (in Appendix B.3) that maximizing the lower bound and the mutual information corresponds to maximizing the objective\n\nL[θ, φ] =\n\n1 N\n\nN (cid:88)\n\nn=1\n\nE\n\nqφ(z1:L|x(n)\n\n1:T , A)qφ(A) log pθ(x(n)\n\n1:T |z1:L)\n\n(cid:104) − Eqφ(A)KL\n\n(cid:105) q∗ φ(z1:L|A); p(z1:L|A)\n\n− KL[qφ(A); p(A)],\n\n(12)\n\nwhere KL labels the Kullback-Leibler divergence (Kullback & Leibler, 1951) between prior and posterior distributions, and q∗ φ(z1:L|A) is the aggregated posterior distribution over random walks. The latter is defined as EpD(x1:T ) [qφ(z1:L|x1:T , A)] and is in general intractable. In practice, we approximate it with an expression identical to Eq. 9, but with the class probabilities and transition matrices (Eqs. 10 and 11) replaced with their data-averaged counterparts. We refer the reader to Appendix B for details on this, as well as for the explicit, closed-form expressions of the KullbackLeibler terms in Eq. 12. The full training algorithm is presented in Appendix C.\n\n4 PROOF OF CONCEPT: INFERRING GROUND-TRUTH RANDOM GRAPHS\n\nBefore testing the behaviour of our methodology on natural language data, we evaluate the ability of the model to infer hidden graph structures from sequential data in a controlled experiment. To this end, we define a synthetic language model with an underlying, ground-truth graph G∗ as follows: Given a graph G∗ with K nodes, and a vocabulary of random tokens V of size V , we assign one random bag of tokens (i.e. one pdf over V) to each node of the graph. Let the K random bags be the K symbols {e1, e2, . . . , eK} of the synthetic language model. We then sample N uniform random walks of length L over G∗, and sample one random token from each symbol (i.e. from each random bag) along the walks. The result is a set of random token sequences of the same length as that of the random walks. Appendix D contains a more detailed description of this generation procedure.\n\nGiven this set of random token sequences, the task is to infer the hidden ground-truth graph G∗.\n\nExperimental settings. Following the procedure above we generated two datasets from two random graphs with different topologies. One sampled from the Barabási-Albert model (Barabási & Albert, 1999), the other from the Erdös-Rényi model (Erdös & Rényi, 1959). We set both graphs to have K = 100 symbols, and the token sequences to have length L = 10. Each dataset has a total of N = 100000 token sequences. Further details about the random graph model parameters and the dataset statistics can be found in Appendix D. The synthetic datasets are available in the source code.\n\nA simple proof-of-concept. We consider a problem in which the set of symbols (random bags) E is known, so that the ground-truth graph G∗ has a fixed labelling. This setting will allow for simple comparison between G∗ and our inferred graphs. To infer G∗ we used a simplified version of HSN, namely: we (i) replace BERT in Fig. 1 with a 2-block Transformer encoder (Vaswani et al., 2017); (ii) set the graph model gφ (Eq. 8) to a single-layer, feed forward network; and (iii) note that, since the symbols are known, the likelihood of the model is simply given by (cid:81)L i=1 eji where, as before, ji denotes the index of the non-zero component of zi. We train this model by maximizing Eq. 12 and refer to Appendix D for details on hyperparameters, training procedure and model sizes.\n\nResults. Table 1 shows our results for our two synthetic datasets. Specifically, we compute the Area Under the Receiver Operating Characteristic Curve (ROC AUC) of our model qφ(A) with respect to G∗, and the Frobenious norm between qφ and two graphs: the ground-truth one G∗, and a second random graph Grand sampled from the same random graph model as G∗. We train ten (10) models\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nMethod GPT2 [one epoch] GPT2 [fine-tuned] iVAEMI Optimus HSN (100, 20) HSN (100, 5) HSN (50, 20) HSN (50, 5)\n\nPTB 24.23 19.14 53.44 23.58 17.72 17.79 16.88 17.41\n\nYAHOO 22.00 20.64 47.93 22.34 20.28 20.10 19.59 20.06\n\nYELP 23.40 19.77 36.88 21.99 19.18 19.05 19.01 18.95\n\nTable 2: Left: Perplexity per word (lower is better) on three datasets. GPT2 [one epoch] and Optimus results were extracted from Li et al. (2020). iVAEMI was taken from Fang et al. (2019). GPT2 [fine-tuned] was computed by us. End-of-sequence tokens are kept during evaluation.\n\nFigure 2: Empirical degree distributions of inferred graphs from each corpora. Results correspond to HSN with L = 5, K = 50. We also show the distribution for random graphs with p = 0.5. The graphs are sampled 500 times.\n\nin total and display the mean and standard deviation of our results. We also use a different Grand for each calculation run. The first metric shows that qφ correctly predicts the edges of G∗, whereas the other two metrics show that G ∼ qφ(A) is closer to G∗ than to any other random graph sampled from the same distribution. The last two columns in Table 1 show however that qφ(A) tends to generate denser graphs as compared to the target.\n\nHaving demonstrated that HSN can indeed infer hidden graph structure from sequential data in a simple setting3, we now move to our main problem: language modelling.\n\n5 LANGUAGE MODELLING AND REPRESENTATION LEARNING\n\nNatural language modelling deals with the prediction of the next word in a sentence or document, given a sequence of previously observed words. A natural evaluation metric is therefore the perplexity per word of the model, which is defined as the exponential of the data-averaged, negative loglikelihood of the model, divided by the number of words in the sequence. One complication with this is that latent variable models can only approximately estimate the likelihood function. One can readily see, however, that Eq. 12 is also a lower bound on log pθ (Bishop, 2006) and so, we estimate the perplexity of our models with exp(−L/T ).\n\nDatasets and baselines. We consider three widely used public datasets, namely the Penn Treebank (PTB) (Marcus et al., 1993), Yahoo and Yelp (Yang et al., 2017) corpora. For completeness we include statistics of these datasets in Appendix E. We compare HSN against a pretrained GPT-2, fine-tuned both during a single epoch and until its objective function plateaus. We also compare againts two VAE language models: iVAEMI (Fang et al., 2019) and Optimus (Li et al., 2020). The former implements both encoder and decoder as one-layer LSTMs (Hochreiter & Schmidhuber, 1997). The latter uses pretrained BERT and GPT-2 as encoder and decoder, respectively.\n\nExperimental settings. In all experiments we leverage pretrained BERT and GPT-2 models, both with 12 layers, 768 hidden dimensions (D) and 12 attention heads. Note that Optimus shares these settings. We use the public HuggingFace implementation of both these models (Wolf et al., 2020). The graph model is set to a 2-layer feed forward network, each with hidden dimension 512, and we also train an inhomogeneous random walk prior model (Eq. 4) by making ρ and the sequence of weights f [1], f [2], . . . , f [L−1] trainable. Furthermore, we explore HSNs with K = {50, 100} symbols and hidden random walks of L = {5, 20} steps. Let us label these configurations as HSN(K, L). Additional details on hyperparameters and training procedures can be found in Appendix E.\n\nResults. Table 2 shows the perplexity of our model, together with the baselines, evaluated on the test set of the three corpora. HSN achieves a much better performance than all baselines under this metric, which implies it successfully interprets the symbol sequences it uses to encode the sentences. Note in particular that HSNs with 50 symbols perform consistently better than their 100-symbol counterparts.\n\n3We could, of course, now study the harder problem for which the symbols are unknown. However, the\n\nlearned graphs model would not be aligned with G∗ making the graph comparison non-trivial.\n\n7\n\n01020304050Node Degree0246810Node CountptbyelperdosyahooUnder review as a conference paper at ICLR 2023\n\nFigure 3: Schema distributions inferred by HSN(50, 5) for four labels of the Yahoo corpora. The node positions in the figure are consistent among labels and were computed using a force-directed embedding of the global graph G.\n\nAs we discuss in Appendix E, 100-symbol HSNs tend to infer networks with many disconnected subgraphs, the largest of which has usually about 50 symbols. It appears then that (about) 50 symbols are enough to encode these corpora. We have repeated these experiments five (5) times, each with a different random initialization of the pink shaded blocks in Fig. 1. We find our mean perplexities to be better than all baselines even within error bars. The reader can find these results in Appendix E. To get a deeper insight into the features of these representations we now explore the structure of the learned global graphs G, as well as the semantic content of the schemata.\n\nStructure of hidden schema networks. We characterize the structure of G in terms of five statistics: its (i) diameter D, (ii) average distance l, (iii) clustering coefficient C, (iv) number of connected components CC and (v) degree distribution P (k) (see Appendix E for the definition of these). We report our results in Table 3 for HSN(50, 5). Results for the other HSN configurations can be found in the Appendix, from which we mainly find that longer random walks and larger symbol number generically favor larger CC. Going back to Table 3, we observe that the schema networks from each corpora tend to have smaller average distances l, and much larger clustering coefficients C, than any random graphs (with p = 0.5) of the same size – where random graphs with p = 0.5 correspond to our prior model. Let us remark that the combinations of these two features defines the so-called small-world structure (Watts & Strogatz, 1998). Intuitively, a larger C implies that a random walker starting from a given node k will have a larger number of paths bringing it back to k. In such an scenario, random walkers tend to cluster in neighborhoods around their starting point – a property that could help encode different semantic aspects in different regions of G. Another consequence is that one could expect schemata composed of repeated symbols. Figure 2 shows the degree distributions of HSN(50, 5). Here we see another aspect on which the schema networks differ from a purely random graph. In particular, the former are more densely connected than the latter.\n\nSchemata and semantics. To qualitatively grasp the semantic content of the learned schemata we take advantage of the labels available to both Yahoo and Yelp corpora. For example, Figure 3 displays the random walk distributions over the schema networks for four (4) subsets of Yahoo, as inferred with HSN(50, 5). Similar plots for all subsets (labels) of both corpora, extracted with all our HSN configurations can be found in Appendix E. Note how the “hot” symbols per category reside on different regions of the graphs – as suspected already from the large clustering coefficient of G – and yet, the “Science & Math” schemata (both nodes and edges) of Yahoo are closer to the “Education & Reference” schemata than to the “Sport” schemata, where closer nodes in the figure indicate well-connected nodes in the underlying graph G. We can understand these findings as indicating that the schemata indeed encode semantic notions of their corpora. A similar picture\n\nDataset PTB YAHOO YELP Random\n\nn. edges 694.26 ± 9.47 892.67 ± 8.22 891.06 ± 6.50 611.69 ± 17.61\n\nD\n\nl\n\nC\n\nCC\n\n2.00 ± 0.00 2.00 ± 0.00 2.73 ± 0.46 2.00 ± 0.00\n\n1.43 ± 0.01 1.24 ± 0.01 1.24 ± 0.03 1.50 ± 0.01\n\n0.83 ± 0.01 0.84 ± 0.00 0.84 ± 0.01 0.50 ± 0.02\n\n1.00 ± 0.00 2.00 ± 0.04 2.24 ± 0.85 1.00 ± 0.00\n\nlargest CC 50.00 ± 0.00 49.00 ± 0.04 48.76 ± 0.85 50.00 ± 0.00\n\nTable 3: Statistics of Schema Networks per corpora with K = 50 and L = 5. Random denotes an Erdös-Rényi model with p = 0.5 for the corresponding K.\n\n8\n\nScience & MathematicsEducation & ReferenceSportsPolitics & Government0.00.20.40.60.81.0Symbol Visits0.00.20.40.60.81.0Edge VisitsUnder review as a conference paper at ICLR 2023\n\nCOMET(GPT2) COMET(GPT2-XL) COMET (BART)\n\nBLEU-2 BERT Score\n\n0.225 0.486\n\n0.300 0.638\n\n0.330 0.650\n\nHSN\n\n0.332 0.782\n\nHSN[prior] HSN[KD]\n\n0.067 0.435\n\n0.125 0.561\n\nTable 4: Metrics of object generation quality for ATOMIC dataset. COMET(GPT2-XL) and COMET (BART) results were extracted from Hwang et al. (2020). COMET(GPT2-XL) was computed by us. The HSN models have K = 50, L = 20. All models use greedy decoding for all text prefixes in the dataset.\n\nholds for Yelp. Finally, we have also defined and explored “schema interpolations” (Appendix E) and have investigated how the schemata are attended to by the model (Appendix F). These experiments (qualitatively) show too that the schemata encode different semantic notions of natural language.\n\n6 COMMONSENSE REASONING GENERATION\n\nIt has been proposed recently that large, pretrained language models fine-tuned on (natural language) knowledge graph (KG) tuples, can express their encoded knowledge through language generation, thereby providing commonsense knowledge on demand (Bosselut et al., 2019; Hwang et al., 2020). These commonsense KGs live however in data (i.e. text) space – the nodes and edges are represented by either single words or sequences of them. This observation led us to investigate whether one could use the COMET framework of Bosselut et al. (2019), together with the inductive biases of HSN, to translate the implicit knowledge of pre-trained models into KGs in representation space. Arguably so abstract a KG could encompass larger commonsense KGs in data space. With this intuition in mind, let us revisit the COMET framework.\n\n1, . . . , xs\n\n1, . . . , xo\n\nTask, datasets and baselines. Consider a training KG of natural language tuples of the form (s, r, |s|) labels the phrase subject of the tuple, r = xr is the relation token and o), where s = (xs o = (xo |o|) is the phrase object of the tuple. The task is to generate the object o, given s and r. In other words, to infer the distribution p(o|[s, r]). COMET finetunes its pretrained models by maximizing the likelihood of the object, conditioned on the sequence [s, r] = (xs |s|, xr) (Bosselut et al., 2019). In contrast, HSN is trained to auto-encode the complete sequence [s, r, o] = (xs |o|) and is evaluated on object generation tasks, conditioned not only on [s, r] but also on the schema ej1:jL . For this preliminary study we focus on the ATOMIC dataset (Sap et al., 2019a), evaluate the quality of the generated objects with both, BLEU-2 (Papineni et al., 2002) and BERT Score (Zhang* et al., 2020) metrics, and compare against GPT-2, GPT-2-XL and BART, all trained within the COMET framework(Hwang et al., 2020).\n\n|s|, xr, xo\n\n1, . . . , xo\n\n1, . . . , xs\n\n1, . . . , xs\n\n2\n\n|[s, r], A)qφ(z L\n\n2 +1:L|[s, r, o], z L\n\nResults. Table 4 shows HSN outperforms all baselines4, which entails it successfully infers and interprets schemata encoding the KG tuples. These schemata, however, are inferred via a posterior of , A) – see Appendix G for details. Yet, in practice, the form qφ(z1: L one does not have access to any object during inference. The classical solution, à la Kalman Filter, is , A), to replace qφ(z L and train the latter via the KL term in Eq. 12. Maximizing the mutual information, however, averages out all local information from the prior and hinders its learning – see e.g. HSN[prior] in Table 4. An alternative is to train, in the spirit of knowledge distillation (Hinton et al., 2015), a third-party model on the inferred schemata, to predict z L 2 +1:L conditioned on z1: L . Our preliminary results, reported as HSN[KD] in Table 4, improve upon HSN[prior] and even outperform COMET(GPT2) in the BERT Score.\n\n, A) with a local prior model of the form pθ(z L\n\n2 +1:L|[s, r, o], z L\n\n2 +1:L|[s, r], z L\n\n2\n\n2\n\n2\n\n2\n\n7 CONCLUSION\n\nWe introduced a novel representation learning algorithm for natural language modelling that infers discrete, relational representations which allow for compositionality. Experiments show our model learns representations encoding high-level semantics of natural sentences, thereby adding some novel layers of interpretability to large, pretrained language models.\n\n4Note, in particular, that BART (Lewis et al., 2020) has 400M parameter, whereas HSN has 250M.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n8 REPRODUCIBILITY STATEMENT\n\nWe provide source code to reproduce our results as supplementary material. The README.rst file within it contains instructions to install and run the corresponding libraries. The synthetic datasets of section 4 are also provided within the source code file, in the data directory. All other datasets we used are available online and are automatically downloaded by our training scripts.\n\nWe additionally provide explicit derivation and/or details for all mathematical expression within the main text in the Appendix. Details on hyper-parameter selection and training can also be found in the Appendix.\n\nREFERENCES\n\nJoshua T Abbott, Joseph L Austerweil, and Thomas L Griffiths. Human memory search as a random\n\nwalk in a semantic network. In NIPS, pp. 3050–3058, 2012.\n\nAlbert-László Barabási and Réka Albert. Emergence of scaling in random networks. science, 286\n\n(5439):509–512, 1999.\n\nFrederic C Bartlett. Remembering: A Study in Experimental and Social Psychology. Cambridge\n\nUniversity Press, 1932.\n\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\n\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\n\nlanguage model. Journal of machine learning research, 3(Feb):1137–1155, 2003.\n\nHanoz Bhathena, Angelica Willis, and Nathan Dass. Evaluating compositionality of sentence representation models. In Proceedings of the 5th Workshop on Representation Learning for NLP, pp. 185–193, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.repl4nlp-1.22. URL https://aclanthology.org/2020.repl4nlp-1.22.\n\nChristopher M Bishop. Pattern recognition and machine learning. springer, 2006.\n\nNed Block. Advertisement for a semantics for psychology. Midwest studies in philosophy, 10:\n\n615–678, 1986.\n\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. Comet: Commonsense transformers for automatic knowledge graph construction. arXiv preprint arXiv:1906.05317, 2019.\n\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio.\n\nGenerating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.\n\nRonald J Brachman. What’s in a concept: structural foundations for semantic networks. International\n\njournal of man-machine studies, 9(2):127–152, 1977.\n\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n\nRuben Cartuyvels, Graham Spinks, and Marie-Francine Moens. Discrete and continuous representations and processing in deep learning: Looking forward. AI Open, 2:143–159, 2021. ISSN 2666-6510.\n\nAmerica Chambers, Padhraic Smyth, and Mark Steyvers. Learning concept graphs from text with stick-breaking priors. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta (eds.), Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc., 2010.\n\nAllan M Collins and M Ross Quillian. Retrieval time from semantic memory. Journal of verbal\n\nlearning and verbal behavior, 8(2):240–247, 1969.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nThomas M Cover and Joy A Thomas. Information theory and statistics. Elements of information\n\ntheory, 1(1):279–335, 1991.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nPaul Erdös and Alfréd Rényi. On random graphs i. Publicationes Mathematicae Debrecen, 6:\n\n290–297, 1959.\n\nLe Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent variable models for text generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3946–3956. Association for Computational Linguistics, 2019.\n\nJR Firth. A synopsis of linguistic theory, 1930—55. Studies in Linguistic Analysis. London: Black,\n\n1957.\n\nJerry A Fodor and Ernest Lepore. The compositionality papers. Oxford University Press, 2002.\n\nJerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis.\n\nCognition, 28(1-2):3–71, 1988.\n\nHao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence Carin. Cyclical annealing schedule: A simple approach to mitigating KL vanishing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n\nArtur d’Avila Garcez and Luis C Lamb. Neurosymbolic ai:\n\nthe 3rd wave. arXiv preprint\n\narXiv:2012.05876, 2020.\n\nJesús Gómez-Gardenes and Vito Latora. Entropy rate of diffusion processes on complex networks.\n\nPhysical Review E, 78(6):065102, 2008.\n\nThomas L Griffiths, Mark Steyvers, and Alana Firl. Google and the mind: Predicting fluency with\n\npagerank. Psychological science, 18(12):1069–1076, 2007a.\n\nThomas L Griffiths, Mark Steyvers, and Joshua B Tenenbaum. Topics in semantic representation.\n\nPsychological review, 114(2):211, 2007b.\n\nAric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. Exploring network structure, dynamics, and function using networkx. In Gaël Varoquaux, Travis Vaught, and Jarrod Millman (eds.), Proceedings of the 7th Python in Science Conference, pp. 11 – 15, Pasadena, CA USA, 2008.\n\nZellig S Harris. Distributional structure. Word, 10(2-3):146–162, 1954.\n\nJohn Hewitt and Christopher D Manning. A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4129–4138, 2019.\n\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015. URL http://arxiv. org/abs/1503.02531.\n\nSepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):\n\n1735–1780, 1997.\n\nWeihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning discrete representations via information maximizing self-augmented training. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, pp. 1558–1567. JMLR.org, 2017.\n\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: how do neural networks generalise? Journal of Artificial Intelligence Research, 67:757–795, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJena D Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. arXiv preprint arXiv:2010.05953, 2020.\n\nRay Jackendoff. An argument about the composition of conceptual structure. American Journal of\n\nComputational Linguistics, pp. 69–73, 1978.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv\n\npreprint arXiv:1611.01144, 2016.\n\nŁukasz Kaiser and Samy Bengio. Discrete autoencoders for sequence models. arXiv preprint\n\narXiv:1801.09797, 2018.\n\nLukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam Shazeer. Fast decoding in sequence models using discrete latent variables. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2390–2399. PMLR, 10–15 Jul 2018.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nS. Kullback and R. A. Leibler. On information and sufficiency. The Annals of Mathematical Statistics,\n\n22(1):79–86, 1951.\n\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building\n\nmachines that learn and think like people. Behavioral and brain sciences, 40, 2017.\n\nRenaud Lambiotte, Roberta Sinatra, J-C Delvenne, Tim S Evans, Mauricio Barahona, and Vito Latora.\n\nFlow graphs: Interweaving dynamics and structure. Physical Review E, 84(1):017102, 2011.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, Online, July 2020. Association for Computational Linguistics.\n\nBohan Li, Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick, and Yiming Yang. A surprisingly effective fix for deep latent variable modeling of text. arXiv preprint arXiv:1909.00868, 2019.\n\nChunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, and Jianfeng Gao. Optimus: Organizing sentences via pre-trained modeling of a latent space. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4678–4699, Online, November 2020. Association for Computational Linguistics.\n\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated\n\ncorpus of English: The Penn Treebank. Computational Linguistics, 19, 1993.\n\nJames L McClelland and Timothy T Rogers. The parallel distributed processing approach to semantic\n\ncognition. Nature reviews neuroscience, 4(4):310–322, 2003.\n\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representa-\n\ntions in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\nMarvin Minsky. A framework for representing knowledge. MIT-AI Laboratory Memo 306, 1974.\n\nMarvin Minsky. Minsky’s frame system theory. In TINLAP, volume 75, pp. 104–116, 1975.\n\nJeff Mitchell and Mirella Lapata. Vector-based models of semantic composition. In proceedings of\n\nACL-08: HLT, pp. 236–244, 2008.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.\n\nJuyong Park and Mark EJ Newman. Statistical mechanics of networks. Physical Review E, 70(6):\n\n066117, 2004.\n\nJean Piaget. Le langage et la pensée chez l’enfant: Études sur la logique de l’enfant. Delachaud &\n\nNiestlé, 1948.\n\nM Ross Quillan. Semantic memory. Technical report, BOLT BERANEK AND NEWMAN INC\n\nCAMBRIDGE MA, 1966.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.\n\nImproving language\n\nunderstanding by generative pre-training. 2018.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pp. 1278–1286. PMLR, 2014.\n\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842–866, 2020. doi: 10.1162/tacl_a_00349. URL https://aclanthology.org/2020.tacl-1.54.\n\nDavid E Rumelhart. Schemata: The building blocks of cognition. In Theoretical issues in reading\n\ncomprehension, pp. 33–58. Routledge, 2017.\n\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. Atomic: An atlas of machine commonsense for if-then reasoning. AAAI’19/IAAI’19/EAAI’19. AAAI Press, 2019a. ISBN 978-1-57735809-1.\n\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. Atomic: An atlas of machine commonsense for if-then reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3027–3035, 2019b.\n\nRay Solomonoff and Anatol Rapoport. Connectivity of random nets. The bulletin of mathematical\n\nbiophysics, 13(2):107–117, 1951.\n\nPaul Thagard. Frames, knowledge, and inference. Synthese, 61(2):233–259, 1984.\n\nAaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz In Advances in neural information\n\nKaiser, and Illia Polosukhin. Attention is all you need. processing systems, pp. 5998–6008, 2017.\n\nDuncan J Watts and Steven H Strogatz. Collective dynamics of ‘small-world’networks. nature, 393\n\n(6684):440–442, 1998.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Online, October 2020. Association for Computational Linguistics.\n\nZichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational autoencoders for text modeling using dilated convolutions. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 3881–3890, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.\n\nLang Yu and Allyson Ettinger. Assessing phrasal representation and composition in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4896–4907, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.397. URL https://aclanthology.org/2020. emnlp-main.397.\n\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr.\n\nShengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Balancing learning and inference in variational autoencoders. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01): 5885–5892, Jul. 2019. URL https://ojs.aaai.org/index.php/AAAI/article/ view/4538.\n\nTiancheng Zhao, Kyusong Lee, and Maxine Eskenazi. Unsupervised discrete sentence representation learning for interpretable neural dialog generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1098–1107. Association for Computational Linguistics, 2018.\n\nZachary M Ziegler, Luke Melas-Kyriazi, Sebastian Gehrmann, and Alexander M Rush. Encoderagnostic adaptation for conditional language generation. arXiv preprint arXiv:1908.06938, 2019.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA PSEUDO-SELF ATTENTION MECHANISM REVISITED\n\nThe attention mechanism of the original Transformers (Vaswani et al., 2017) is defined as\n\nAttention(Q, K, V) = softmax\n\n(cid:16)\n\nD− 1\n\n2 Q · KT (cid:17)\n\nV,\n\n(13)\n\nwhere Q, K and V ∈ RT ×D are sets of queries, keys and values, respectively, given by a sequence of T , D-dimensional vectors, packed into matrices. In practice, these queries, keys and values are projected many times with different learnable, linear maps. The Attention operation (Eq. 13) is performed on these different projections in parallel, whose outputs are then concatenated and projected once more with a final, linear map. The complete operation is known as Multi-head Attention (Vaswani et al., 2017), and we use this notation in Fig. 1 of the main text.\n\nNow, the question is how to condition GPT-2 on the schema ej1:jL. Given a sequence of input representations u1:T , the self -attention mechanism in GPT-2 is obtained by choosing Q = u1:T · WQ, K = u1:T · WK and V = u1:T · WV , all in RT ×D, with WQ, WK and WV ∈ RD×D pretrained matrices. We leverage a pseudo-self attention (PSA) mechanism (Ziegler et al., 2019) that augments the key and value matrices in their first L rows, with projections of ej1:jL so that\n\n ̃K =\n\n(cid:18)ej1:jL · We K\n\nK + penc\n\n(cid:19)\n\n, ̃V =\n\n(cid:18)ej1:jL · We V\n\nV + penc\n\n(cid:19)\n\n∈ R(L+T )×D,\n\n(14)\n\nwhere penc is a positional encoding, just as the one used in the original Transformer implementation (Vaswani et al., 2017). The latter informs GPT-2 about the ordering of the symbols in the schema, as selected by the random walk process. PSA is then simply given by Eq. 13 with the keys and values replaced with the augmented ones, ̃K and ̃V. The We V here are randomly initialized, learnable parameters mapping the schemata onto the decoder self-attention, D-dimensional space, and we have as many of them as layers in GPT-2. Therefore this mechanism allows GPT-2 to attend to the projected schema at each of its layers, with a minimal addition of untrained parameters (Ziegler et al., 2019).\n\nK, We\n\nB TRAINING OBJECTIVE\n\nThe Evidence Lower Bound (ELBO) of the Hidden Schema Network model reads\n\nL[θ, φ] =\n\n1 N\n\nN (cid:88)\n\nn=1\n\nE\n\nqφ(z1:L|x(n)\n\n1:T , A)qφ(A) log pθ(x(n)\n\n1:T |z1:L)\n\n(cid:104) − Eqφ(A)KL\n\nqφ(z1:L|x(n)\n\n1:T , A); p(z1:L|A)\n\n(cid:105)\n\n− KL[qφ(A); p(A)],\n\n(15)\n\nwhere KL[·] denotes the Kullback-Leibler (KL) divergence.\n\nNote that this is not the training objective of the main text. There we maximize the ELBO together with the mutual information between sentences and schemata. We give details about this modified objective in subsection B.3 below. Before getting into that, let us first calculate the explicit expressions for the two divergences above.\n\nB.1 KULLBACK-LEIBLER BETWEEN RANDOM WALKS\n\nFor notational convenience we will not write the explicit dependence on the graph A in what follows. Using the explicit product form of the probabilities over walks leads to\n\nKL[qφ(z1:T |x(n)\n\n1:T ); p(z1:T )] =\n\nL (cid:88)\n\nE\n\nˆqφ(zi−1|x(n)\n\n1:T )qφ(zi|zi−1x(n)\n\n1:T ) log\n\ni=2 + KL[qφ(z1); p(z1)],\n\nqφ(zi|zi−1, x(n) 1:T ) p(zi|zi−1)\n\n(16)\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nGraph G∗\n\nBarabasi\n\nErdos\n\nModel\n\nLSTM HS (0.1) HS (0.2)\n\nLSTM HS (0.5) HS (0.6)\n\nNLL\n\n53.07± 0.01 53.08 ± 0.01 53.07 ± 0.02\n\n48.24 ± 0.02 50.9 ± 0.8 50.4 ± 0.6\n\nKL − z\n\n– 0.10 ± 0.06 0.09 ± 0.06\n\n– 1.2 ± 0.3 1.3 ± 0.1\n\nKL − G\n\n– 9 ± 1 4.8 ± 0.5\n\n– 4 ± 6 1 ± 2\n\nAUC\n\n– 0.977 ± 0.003 0.989 ± 0.001\n\n– 0.95 ± 0.06 0.94 ± 0.06\n\n|G∗ − G|F –\n17 ± 2 17 ± 2\n\n– 34.8 ± 0.9 36.8 ± 0.8\n\n|Gr − G|F –\n27 ± 1 26 ± 1\n\n– 40 ± 5 44 ± 2\n\nN. edges(G)\n\n– 1090 ± 143 1360 ± 104\n\n– 2812 ± 344 3131 ± 156\n\nTable 5: Inference on ground-truth random graphs. Here we use the notation HS(p) to denote Hidden Schema Network models with prior graph distributions whose edge probability is set to p.\n\nwhere ˆqφ(zi|x(n) are Markovian, ˆq can be explicitly written as\n\n1:T ) is the aggregated probability over all walks until step i. Since the random walks\n\nˆqφ(zi|x(n)\n\n1:T ) =\n\n(cid:89)\n\n1≤j<i\n\nQ[j](x(n)\n\n1:T , φ) · ρ(x(n)\n\n1:T , φ),\n\n(17)\n\nwhere the (posterior) class probabilities over the walks’ starting points ρ, and the transition matrices Q[i] are defined in Eqs. 10 and 11 of the main text. Using the definitions in Eqs. 4 and 9 we can write the argument of the expectation value in Eq. 16 above as\n\nlog\n\nqφ(zi|zi−1, x(n) 1:T ) p(zi|zi−1)\n\n(cid:88)\n\n=\n\ni zj zk\n\ni−1 log\n\nk,j\n\nQ[i−1]\n\nk,j\n\n(x(n)\n\n1:T , φ)\n\nPk,j\n\n,\n\n(18)\n\nwhich means we only need to compute the expectation of the product zk shown to be\n\ni zj\n\ni−1. This one can easily be\n\nE\n\nˆqφ(zi−1|x(n)\n\n1:T )qφ(zi|zi−1x(n) 1:T )\n\n(cid:104)\n\ni zj zk\n\ni−1\n\n(cid:105)\n\n= Q[i−1]\n\nk,j\n\n(x(n)\n\n1:T , φ) ˆρ[i−1]\n\nj\n\n(x(n)\n\n1:T , φ),\n\n(19)\n\nwhere ˆρ[i]\n\nj (x(n)\n\n1:T , φ) is the jth class probability of ˆqφ(zi|x(n)\n\n1:T ), defined in Eq. 17.\n\nFinally, the second KL term in Eq. 16 can be directly evaluated\n\nKL[qφ(z1); p(z1)] =\n\nK (cid:88)\n\nj=1\n\nρj(x(n)\n\n1:T , φ) log\n\nρj(x(n) 1:T , φ) ρj\n\n,\n\n(20)\n\nwhere ρj(x(n) walks’ starting points.\n\n1:T , φ) and ρj are, respectively, the posterior and prior class probabilities for the random\n\nMethod GPT2 [one epoch] GPT2 [fine-tuned] iVAEMI Optimus HSN (100, 20) HSN (100, 5) HSN (50, 20) HSN (50, 5) HSN (100, 20) HSN (100, 5) HSN (50, 20) HSN (50, 5)\n\nPTB 24.23 19.14 53.44 23.58 17.61 17.69 16.88 17.41 17.6 ± 0.1 17.79 ± 0.09 17.0 ± 0.1 17.44 ± 0.07\n\nYAHOO 22.00 20.64 47.93 22.34 19.68 19.84 19.59 20.06 20.1 ± 0.2 20.0 ± 0.1 19.8 ± 0.2 20.09 ± 0.05\n\nYELP 23.40 19.77 36.88 21.99 18.99 19.00 19.01 18.95 19.12 ± 0.08 19.07 ± 0.08 19.3 ± 0.5 18.9 ± 0.1\n\nTable 6: Perplexity per word (lower is better). The last four (4) rows show the mean and standard deviation obtained after training five (5) HSN.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nKL − z\n\nKL − G\n\nKL − z\n\nKL − G\n\nKL − z\n\nKL − G\n\nPTB\n\nYAHOO\n\nYELP\n\nHS [L = 20] HS [L = 5]\n\n2.0 ± 0.3 1.33 ± 0.05\n\n0.004 ± 0.002 0.0013 ± 0.0009\n\n1.3 ± 0.6 1.54 ± 0.05\n\n0.005 ± 0.003 0.0006 ± 0.0006\n\n1.9 ± 0.7 1.50 ± 0.09\n\n0.011 ± 0.002 0.002 ± 0.002\n\nTable 7: Kullback-leibler divergence for 100-symbol HSN models (trained 5 times) in all datasets\n\nPutting all together we write\n\nKL[qφ(z1:T |x(n)\n\n1:T ); p(z1:T )] =\n\nL (cid:88)\n\nK (cid:88)\n\ni=2\n\nk,j=1\n\nQ[i−1]\n\nk,j\n\n(x(n)\n\n1:T , φ) ˆρ[i−1]\n\nj\n\n(x(n)\n\n1:T , φ) log\n\nQ[i−1]\n\nk,j\n\n(x(n)\n\n1:T , φ)\n\nPk,j\n\n+\n\nK (cid:88)\n\nj=1\n\nρj(x(n)\n\n1:T , φ) log\n\nρj(x(n) 1:T , φ) ρj\n\n(21)\n\nB.2 KULLBACK-LEIBLER BETWEEN RANDOM GRAPH MODELS\n\nSince both prior and posterior graph models treat each edge in G as a Bernoulli random variable, we can write directly\n\nKL[q(A)|p(A)] =\n\n(cid:26)\n\n(cid:88)\n\nij\n\npφ(ei, ej) log\n\n(cid:19)\n\n(cid:18) pφ(ei, ej) p\n\n+(1 − pφ(ei, ej)) log\n\n(cid:18) 1 − pφ(ei, ej) 1 − p\n\n(cid:19)(cid:27)\n\n,\n\n(22)\n\nwhere pφ(ei, ej) is the posterior link probability, which is conditioned on the symbols connected by the link, and p is the global prior probability over all links, as defined in Eq. 3 of the main text.\n\nB.3 MAXIMIZING MUTUAL INFORMATION\n\nWe would like to maximize the mutual information between the word sequences in our dataset and the schema representations. We have argued that the training objective in the main text already includes such a mutual information term. To see this is indeed the case we need to workout some identities.\n\nLet us, for simplicity of notation, consider two discrete variables z and x, the last of which follows an unknown distribution pD(x). What follow are identities\n\n−EpD(x)KL[q(z|x); p(z)] = EpD(x)Eq(z|x) (cid:88)\n\n= Hq(z|x) +\n\n(cid:110)\n\nlog p(z) − log(z|x)\n\n(cid:111)\n\npD(x)\n\n(cid:88)\n\n(cid:110)\n\nq(z|x)\n\nlog p(z) + log q∗(z) − log q∗(z)\n\n(cid:111)\n\nx\n\n= Hq(z|x) − Hq∗ (z) +\n\nz (cid:88)\n\nz\n\n(cid:110)\n\nq∗(z)\n\nlog p(z) − log q∗(z)\n\n(cid:111)\n\n= −I(z; x) − Eq∗(z)\n\nq∗(z) p(z) = −I(z; x) − KL[q∗(z); p(z)],\n\nlog\n\n(cid:110)\n\n(cid:111)\n\nwhere\n\nHq(z|x) = −\n\n(cid:88)\n\nx\n\npD(x)\n\n(cid:88)\n\nz\n\nq(z|x) log q(z|x),\n\n(23)\n\n(24)\n\nis the conditional entropy with respect to distribution q (see e.g. page 17 in (Cover & Thomas, 1991)) and\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nHSN(50, 5)\n\nHSN(50, 20)\n\nHLN(100, 5)\n\nHLN(100, 20)\n\nDataset PTB YAHOO YELP Random PTB YAHOO YELP Random PTB YAHOO YELP Random PTB YAHOO YELP Random\n\nn. edges 694.26 ± 9.47 892.67 ± 8.22 891.06 ± 6.50 611.69 ± 17.61 764.28 ± 7.88 356.35 ± 7.76 259.42 ± 5.47 611.69 ± 17.61 1198.18 ± 16.44 1239.21 ± 12.19 1295.68 ± 12.93 2474.92 ± 36.58 892.53 ± 10.04 261.13 ± 7.14 515.84 ± 10.09 2474.92 ± 36.58\n\nD 2.00 ± 0.00 2.00 ± 0.00 2.73 ± 0.46 2.00 ± 0.00 2.83 ± 0.38 3.17 ± 0.37 2.68 ± 0.48 2.00 ± 0.00 2.56 ± 0.50 3.15 ± 0.38 3.36 ± 0.48 2.00 ± 0.00 3.04 ± 0.24 2.18 ± 0.38 3.68 ± 0.48 2.00 ± 0.00\n\nl 1.43 ± 0.01 1.24 ± 0.01 1.24 ± 0.03 1.50 ± 0.01 1.27 ± 0.03 1.57 ± 0.05 1.42 ± 0.03 1.50 ± 0.01 1.76 ± 0.01 1.42 ± 0.03 1.55 ± 0.03 1.50 ± 0.01 1.41 ± 0.04 1.95 ± 0.00 1.79 ± 0.06 1.50 ± 0.01\n\nC 0.83 ± 0.01 0.84 ± 0.00 0.84 ± 0.01 0.50 ± 0.02 0.82 ± 0.01 0.58 ± 0.02 0.48 ± 0.01 0.50 ± 0.02 0.83 ± 0.01 0.51 ± 0.01 0.57 ± 0.01 0.50 ± 0.01 0.45 ± 0.01 0.91 ± 0.01 0.38 ± 0.02 0.50 ± 0.01\n\nCC 1.00 ± 0.00 2.00 ± 0.04 2.24 ± 0.85 1.00 ± 0.00 4.71 ± 0.77 12.04 ± 1.49 20.77 ± 0.68 1.00 ± 0.00 1.19 ± 0.40 35.93 ± 1.41 27.38 ± 1.69 1.00 ± 0.00 46.04 ± 1.53 1.01 ± 0.10 45.27 ± 2.58 1.00 ± 0.00\n\nlargest CC 50.00 ± 0.00 49.00 ± 0.04 48.76 ± 0.85 50.00 ± 0.00 46.29 ± 0.77 38.96 ± 1.49 30.23 ± 0.68 50.00 ± 0.00 99.81 ± 0.40 65.07 ± 1.41 73.62 ± 1.69 100.00 ± 0.00 54.96 ± 1.54 99.99 ± 0.10 55.67 ± 2.58 100.00 ± 0.00\n\nTable 8: Statistic of inferred graphs for all datasets\n\nis the entropy of distribution q∗(z), which we define as the marginal (data-aggregated) distribution\n\nHq∗ (z) = −\n\n(cid:88)\n\nz\n\nq∗(z) log q∗(z),\n\n(25)\n\nq∗(z) =\n\n(cid:88)\n\nx\n\npD(x)q(z|x).\n\nFinally, we used the definition of mutual information\n\nSee e.g. page 20 in (Cover & Thomas, 1991).\n\nI(x; z) = Hq∗ (z) − Hq(z|x).\n\n(26)\n\n(27)\n\nIt follows from Eq. 23 that maximizing the ELBO (Eq. 15), together with the mutual information between word sequences and schemata, simply amounts to replacing the KL between the approximate posterior and prior random walk distributions, with the KL between the aggregated posterior and prior random walk distributions. To wit\n\n−\n\n1 N\n\nN (cid:88)\n\nn=1\n\n(cid:104) Eqφ(A)KL\n\nqφ(z1:L|x(n)\n\n(cid:105) 1:T , A); p(z1:T |A)\n\n+ I(z1:L; x1:T |A) =\n\n(cid:104) − Eqφ(A)KL\n\n(cid:105) q∗ φ(z1:L|A); p(z1:T |A)\n\n,\n\n(28)\n\nwhere we introduced the aggregated posterior over random walks wrt the word sequence\n\nφ(z1:L) = Ep(x1:T ) q∗\n\nqφ(z1:L|x1:T )\n\n(cid:104)\n\n(cid:105)\n\n≈\n\n1 N\n\nN (cid:88)\n\nn=1\n\nqφ(z1:L|x(n)\n\n1:T ).\n\nIn practice we approximate this quantity with\n\nφ(z1:L) ≈ q∗ q∗\n\nφ(z1)\n\nL (cid:89)\n\ni=2\n\nq∗ φ(zi|zi−1, A),\n\n(29)\n\n(30)\n\nwhere q∗ from our approximate posterior (Eq. 10 in the main text)\n\nφ(z1) is a categorical distribution whose class probabilities ρ∗\n\nj (φ) are the average of those\n\nρ∗\n\nj (φ) =\n\n1 N\n\nN (cid:88)\n\nn=1\n\nρj(x(n)\n\n1:T , φ),\n\nand the transition probabilities q∗\n\nφ(zi|zi−1, A) have transition probability matrices\n\nQ∗ [i]\n\nk,j (A, φ) =\n\n1 N\n\nN (cid:88)\n\nn=1\n\n18\n\nQ[i]\n\nk,j(x(n)\n\n1:T , A, φ).\n\n(31)\n\n(32)\n\nUnder review as a conference paper at ICLR 2023\n\nB.4 MEAN-FIELD SOLUTION\n\nInstead of modeling the posterior over random walks with Eq. 9 of the main text, we could consider a mean-field decomposition along the time component, by ignoring the dependency on the graph G\n\nqφ(z1:L|x1:T ) =\n\nL (cid:89)\n\ni=1\n\nqφ(zi|x1:T ),\n\nwhere at each step of the walk we have a step-dependent categorical distribution\n\nqφ(zi|x1:T ) =\n\nρ[i]\n\nj (x1:T , φ)\n\n(cid:17)zj\n\ni\n\n,\n\nK (cid:89)\n\n(cid:16)\n\nj=1\n\nwhose class probabilities live in the K-simplex. We could model the latter via\n\nρ[1], . . . , ρ[L] = softmax(henc\n\n1 , . . . , henc L )\n\n(33)\n\n(34)\n\n(35)\n\nwhere henc main text.\n\n1 , . . . , henc\n\nL are the outputs of our encoder neural network model, shown in Figure 1 of the\n\nReplacing the mean-field approximation of 33 into 15 yields\n\nKL[qφ(z1:T |x(n)\n\n1:T ); p(z1:T |A)] =\n\nL (cid:88)\n\ni=2\n\n(cid:110)\n\nE\n\nqφ(zi|x(n)\n\n1:T ) log qφ(zi|x(n) 1:T )\n\n− E\n\nqφ(zi|x(n)\n\n1:T )qφ(zi−1|x(n)\n\nL (cid:88)\n\nK (cid:88)\n\n=\n\ni=1\n\nj\n\nρ[i]\n\nj (x1:T , φ) log\n\n1:T ) log p(zi|zi−1) j (x1:T , φ) ρj\n\nρ[i]\n\n(cid:111)\n\n+ KL[qφ(z1); p(z1)],\n\nL (cid:88)\n\nK (cid:88)\n\n−\n\ni=2\n\nk,j\n\nE\n\nqφ(zi|x(n)\n\n1:T )qφ(zi−1|x(n) 1:T )\n\n(cid:104)\n\ni zj zk\n\ni−1\n\n(cid:105)\n\nlog Pk,j\n\nL (cid:88)\n\nK (cid:88)\n\n=\n\ni=1\n\nj\n\nρ[i]\n\nj (x1:T , φ) log\n\nρ[i]\n\nj (x1:T , φ) ρj\n\nL (cid:88)\n\nK (cid:88)\n\n−\n\ni=2\n\nk,j\n\nk (x1:T , φ)ρ[i−1] ρ[i]\n\nj\n\n(x1:T , φ) log Pk,j.\n\n(36)\n\n(37)\n\nB.5 FULLY CONNECTED GRAPH\n\nWe can replace the adjacency matrix A in the definition of the transition probability matrix of our posterior Q(x1:T , A, φ), with that of a fully connected graph. The aggregated posterior over all walks up to step i (Eq. 17 above) reduces in this case to\n\nˆρ[i]\n\nk (x1:T , φ) =\n\n(cid:32)\n\nK (cid:88)\n\nk\n\nf [i−1] m f [i−1]\n\n(cid:80)\n\n(x1:T , φ)Ak,j\n\nm (x1:T , φ)Am,j\n\n(cid:33)\n\nˆρ[i−i]\n\nj\n\n(x1:T , φ)\n\nj\n\n(cid:32)\n\n=\n\n(x1:T , φ)\n\nk\n\nf [i−1] m f [i−1]\n\n(cid:80)\n\nm (x1:T , φ)\n\n(cid:33)  \n\nK (cid:88)\n\nj\n\n\n\nˆρ[i−i]\n\nj\n\n(x1:T , φ)\n\n =\n\nk\n\nf [i−1] m f [i−1]\n\n(cid:80)\n\n(x1:T , φ)\n\nm (x1:T , φ)\n\n,(38)\n\nwhich is equivalent to that of the mean-field approximation of section B.4 with ˆρ[i]\n\nk = ρ[i] k .\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nC HIDDEN SCHEMA NETWORKS ALGORITHM\n\nAlgorithm 1: HSN Training (φ, ψ)\n\nforeach minibatch x1:T ∼ p(D) do\n\n(1) Sample schema network from posterior graph model:\n\n(2) Compute parameters of posterior random walk model:\n\nA ∼ qφ(A),\n\nhenc\n\n1 , henc\n\n2 . . . , henc\n\nL = henc\n\nφ (x1:T ),\n\nρ(φ) = softmax(henc\n\n1 ),\n\nQ[i]\n\nk,j(φ) =\n\nf [i] k (φ) Akj m f [i]\n\nm (φ) Amj\n\n(cid:80)\n\n, with f [1], . . . , f [L−1] = exp(henc\n\n2:L)\n\n(3) Compute parameters of prior random walk model:\n\nfk Akj i=1 fi Aij (4) Sample random walks from posterior distribution:\n\nPk,j =\n\n(cid:80)K\n\nz1:L ∼ qφ(z1:L|x1:T , A)\n\n(5) Decode sentence:\n\nfor i = 0 to T − 1 do\n\nxi ∼ pθ(xi|x<i, ej1:jL), πi = softmax(W · hdec\n\nθ (x<i, ej1:jL))\n\nend\n\n(6) Compute loss and back-propagate:\n\nL[θ, φ] =\n\n1 N\n\nN (cid:88)\n\nn=1\n\nE\n\nqφ(z1:L|x(n)\n\n1:T , A)qφ(A) log pθ(x(n)\n\n1:T |z1:L)\n\n− Eqφ(A)KL\n\n(cid:104)\n\n(cid:105) q∗ φ(z1:L|A); p(z1:L|A)\n\n− KL[qφ(A); p(A)]\n\nend\n\nD ON SYNTHETIC DATASET EXPERIMENTS\n\nIn this section we give additional details of and results from our proof-of-concept experiments.\n\nD.1 SYNTHETIC LANGUAGE MODEL\n\nWe generate our synthetic dataset as follows: first, we sample a single, fixed graph G∗ with K nodes from a predefined random graph model. Second, we define a set of random tokens V, of size V , to be our vocabulary. We create each token as a random 3-tuple from the Latin alphabet, and choose to have at least one order of magnitude more tokens than nodes in G (that is, V ≫ K). Third, we assign a random bag of tokens to each node in G∗. These random bags can simply be understood as probability distributions over V, and can be represented as V -dimensional vectors whose components live on the simplex. Note in particular that, by construction, tokens can be shared among the different nodes of G∗. Finally, let us identify the K random bags with the K symbols {e1, e2, . . . , eK} of the synthetic language model.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nTo generate synthetic sentences we sample uniform, L-step random walks on G∗, whose transition matrix is given by Eq. 4 in the main text, with f = I. Having obtained a set of random walks on G∗, we sample one random token from each of the symbols (i.e. from each random bag) along the walks.\n\nD.2 EXPERIMENTAL SETTINGS\n\nHere we give additional details for reproducibility\n\nDatasets\n\n• Following the procedure above we generated two datasets from two random graphs with different topologies. One sampled from the Barabási-Albert model (Barabási & Albert, 1999), the other from the Erdös-Rényi model (Erdös & Rényi, 1959). We generate these graphs using NetworkX, a Python language software package for network structures (Hagberg et al., 2008). Specifically, we generate Barabási-Albert graphs by attaching 3 edges from each new node to old ones, and Erdös-Rényi graphs with an edge probability of 0.5. We set both graphs to have K = 100 symbols.\n\n• We define each random bag of tokens in G∗ to have two tokens only (each with equal\n\nprobability).\n\n• We use a vocabulary of 1000 random tokens.\n\n• Once the graph is fixed, we set the token sequence length to L = 10 (L = 11) for the Erdös (Barabási) datasets and generate a total of N = 100000 token sequences from each random graph.\n\nHidden Schema Network (HSN) settings\n\n• We train randomly initialized embeddings of dimension 256, one for each token. We sample\n\nthese from a normal distribution with zero mean and a standard deviation of 0.01.\n\n• The posterior graph model is defined via a single feed-forward neural network with 256\n\nhidden units.\n\n• The prior graph model has the edge probability p as hyperparameter. We crossvalidate it from the set p = {0.1, 0.2, 0.5, 0.6, 0.8} and found that HSN could fit the Barabási dataset only with small values {0.1, 0.2}. HSN could fit the Erdös dataset with larger values {0.5, 0.6}\n\n• The posterior random walk model is defined by replacing BERT with a 2-block Transformer encoder (Vaswani et al., 2017), each with 2 heads, 256 hidden units and dropout probability of 0.2.\n\n• The prior random walk model was set to a uniform random walk.\n\nTraining details\n\n• We use a batch size of 256 and train with Adam (Kingma & Ba, 2014), with a learning rate\n\nof 0.0001, in all experiments.\n\n• To sample both graph and random walk posterior models with use the Gumbel-Softmax\n\ntrick (Jang et al., 2016), with a constant temperature of 0.75\n\n• We train the models for 200 epochs\n\nD.3 ADDITIONAL RESULTS\n\nTable 5 displays the mean and standard deviation of some additional results on our proof-of-concept experiments. We trained ten models in total.\n\nWe first trained a simple LSTM Network to infer the correct symbol order in each random token sequence. We noticed that a network with 256 hidden units was enough to solve this task perfectly. Indeed, the negative log-likelihood (NLL) of these models corresponds to choosing the 2-token random bag sequence (i.e. the schema) that yields the correct token sequence without errors. The HSN performs equally well on the Barabási dataset, and slightly worst on the Erdös dataset. In fact, we have noticed the Erdös dataset proved to be more challenging to learn with the HSN in all regards.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nSee, for example, the AUC scores or the Frobenious norms of HSN in this dataset, as compared to the Barabási case. We think this might be due to the fact that Barabási graphs have more structure, simply because of their sparsity, which arguably make them easier to infer with our inductive bias.\n\nNote also how increasing the prior edge probability p affects the average number of edges of the inferred graphs.\n\nE ON LANGUAGE MODELLING EXPERIMENTS\n\nIn this section we give additional details of and results from our language modelling and representation learning experiments.\n\nE.1 EXPERIMENTAL SETTINGS\n\nHere we give additional details for reproducibility\n\nDatasets\n\n• We consider three widely used public datasets, namely the Penn Treebank (PTB) (Marcus\n\net al., 1993), Yahoo and Yelp (Yang et al., 2017) corpora.\n\n• PTB training set has a total of 38219 sentences. The average length of which is of about 22 words. The validation and test set have 5527 and 5462 sentences, respectively. The minimum (maximum) sentence length in PTB is of 2 (78) words.\n\n• Yahoo training set has a total of 100000 sentences. The average length of which is of about 80 words. The validation and test sets have 10000 sentences each. The minimum (maximum) sentence length in Yahoo is of 21 (201) words. The Vocabulary size is of 200000 words.\n\n• Yelp training set has a total of 100000 sentences. The average length of which is of about 97 words. The validation and test sets have 10000 sentences each. The minimum (maximum) sentence length in Yelp is of 21 (201) words. The Vocabulary size is of 90000 words.\n\nHSN settings\n\n• In all experiments we leveraged pretrained BERT and GPT-2 models, both with 12 layers, 768 hidden dimensions (D) and 12 attention heads. We used the public HuggingFace implementation of both these models (Wolf et al., 2020).\n\n• The posterior graph model is set to a 2-layer feed forward network, each with hidden\n\ndimension 512.\n\n• We crossvalidated the prior edge probability over the set of values p = {0.1, 0.2, 0.5, 0.6} and found p = 0.5 (a maximum entropy prior) to yield the best results. All results we report correspond to this (p = 0.5) case.\n\n• We also train an inhomogeneous random walk prior model by making ρ and the sequence of weights f [1], f [2], . . . , f [L−1] trainable. We initialized them by sampling from a normal distribution with zero mean and standard deviation of 0.01.\n\n• We experimented with HSN of K = {50, 100} symbols and random walks of length\n\nL = {5, 20}.\n\nTraining details\n\n• We used a batch size of 32 and train with Adam (Kingma & Ba, 2014), with a learning rate\n\nof 0.00001, in all experiments.\n\n• To sample both graph and random walk posterior models with used the Gumbel-Softmax\n\ntrick (Jang et al., 2016), with a constant temperature of 1.0.\n\n• We used a cyclical schedule to anneal both KL terms in our training objective from zero to one (Fu et al., 2019). When the annealing weight (usually called β in the literature) is finite, we used a KL threshold scheme (Li et al., 2019), with a threshold value of 0.1.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\n• We trained the models for 100 epochs, although they usually needed about 60 epochs only\n\nto converge (in the NLL).\n\n• We applied word dropout to the input of the decoder model with probability 0.3 in the following cases: (i) for all models trained on PTB; (ii) and all models with L = 50 trained on all datasets.\n\nE.2 ADDITIONAL RESULTS\n\nHere we report results complementing the conclusions of the main text.\n\nLanguage modelling. Table 6 displays our perplexity results on all datasets, just as in the main text. In the last four rows we additionally report the mean and standard deviation we obtained when repeating the experiments with the HSN model five times, with different initializations. The conclusion of the main text, viz. that our results outperform all baselines, remains unaltered, even within error bars. We additionally report in Table 7 the mean values of the KL for five 100-symbol HSN runs.\n\nGraph statistics. We characterize the structure of G in terms of five statistics: (i) the diameter D, which measures the maximum path length over all node pairs in G; (ii) the average distance l, which instead measures the average shortest path length between all node pairs; (iii) the clustering coefficient C, which represents the probability that two neighbors of a randomly chosen node are themselves neighbors; (iv) the number of connected components CC; and (v) the degree distribution P (k), which represents the probability that a randomly chosen node will have k neighbors.\n\nTable 8 reports the statistics of our inferred graphs for all datasets, and all model configurations.\n\nWe can see that increasing the random walk length from 5 to 20 increases the number of connected components of the graphs. As a consequence, subsets of word sequences are map onto smaller subgraphs, the larger of which is about 50 symbols. One could argue that, since longer random walk lengths imply a larger set of possible schema configurations, the number of symbols required to describe our three corpora can simply decrease. In other words, less symbols are needed by long schemata. Similarly, directly increasing the symbols number leads too to a larger number of connected components. Indeed, even the short schemata in Yelp and Yahoo do not use all available symbols to model the corpora.\n\nRepresentation learning. We can get a graphical picture of the features we just discussed above in Figures 8–10 below. Very importantly, we see that the schema distribution is different for each category of each corpora in all model configurations. In other words, we do not observe any kind of mode collapse.\n\nFinally, we have also explored “schema interpolations”: given two schemata ej1:jL and em1:mL, we find the shortest path (of length l) on G connecting the end of ej1:jL with the beginning of em1:mL. Our interpolation steps are the schemata {ej1+i:jL+i : ∀ 0 ≤ i ≤ l + L along the path}. Tables 10– 12 show interpolations of random instances from all datasets. Note how the model successfully interpolate between categories in both Yelp and Yahoo.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nlayer KL(good, bad) KL(good, great) KL(great, bad) 1\n5 12\n\n0.807 0.738 0.635\n\n1.227 1.245 0.957\n\n0.336 0.177 0.224\n\nTable 9: Kullback-Leibler divergence between the distributions of most attended symbols, when generating the tokens good, bad and great. Results are computed with HSN(100, 5) trained on Yelp. The KL values are computed for each head separately and then averaged.\n\nF WHICH SYMBOLS DO WORDS ATTEND TO? A PRELIMINARY STUDY ON\n\nYELP REVIEWS\n\nIn this section we investigate how symbols are used by HSN when generating text. We do this by exploring the decoder attention matrix between the symbols and the generated tokens. Reading the attention wights, we can examine which symbols are most important for the generation of any given token, i.e. which symbols are attended to more strongly. A bit more in detail we select, for a given token in a given sentence, the symbol with the highest attention value. We can then compute the distribution of most attended symbols when generating that token for the complete dataset.\n\nThus, for a model trained on the Yelp dataset, we examine to which symbols does the decoder of HSN attend to, when processing the words good, great and bad. Figure 4 shows the most attended symbol distribution for layers 1 (first), 5 (middle), 12 (last), when averaging the attention matrices over all attention heads. Figures 5, 6, 7 show these distributions for each head separately. Note how, for a fixed token, the distribution of attention changes as one moves between heads and layers, albeit there are too some repeating patterns.\n\nWe can quantify these features by computing the Kullback-Leibler (KL) divergence between these distributions. The KL values are shown in Table 9.\n\nInterestingly enough, the distribution of symbols that are attended to when processing the word great is closer to the distributions of symbols attended by the word good, than to the distributions of symbols attended by the word bad.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Layer 1\n\n(b) Layer 5\n\n(c) Layer 12\n\nFigure 4: Distribution of most attended symbols when generating tokens good, bad, great for HSN(100, 5) trained on the Yelp data set. The decoder attention matrices between symbols and output are averaged over all attention heads for layers 1, 5 and 12.\n\n25\n\n020406080100symbol0.000.010.020.030.040.05normalized symbol countgoodbadgreat020406080100symbol0.000.010.020.030.04normalized symbol countgoodbadgreat020406080100symbol0.000.010.020.030.040.05normalized symbol countgoodbadgreatUnder review as a conference paper at ICLR 2023\n\nFigure 5: Distribution of most attended symbols when generating tokens good, bad, great for HSN(100, 5) trained on the Yelp data set. The distribution is computed from the decoder attention matrices between symbols and output for each attention head for layer 1.\n\n26\n\n020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 1020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 2020406080100symbol0.000.020.040.060.080.100.12normalized symbol counthead 3020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 4020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 5020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 6020406080100symbol0.000.020.040.060.08normalized symbol counthead 7020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 8020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 9020406080100symbol0.000.010.020.030.040.050.060.07normalized symbol counthead 10020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 11020406080100symbol0.000.010.020.030.040.050.060.07normalized symbol counthead 12goodbadgreatUnder review as a conference paper at ICLR 2023\n\nFigure 6: Distribution of most attended symbols when generating tokens good, bad, great for HSN(100, 5) trained on the Yelp data set. The distribution is computed from the decoder attention matrices between symbols and output for each attention head for layer 5.\n\n27\n\n020406080100symbol0.000.010.020.030.040.050.060.07normalized symbol counthead 1020406080100symbol0.000.010.020.030.040.050.060.070.08normalized symbol counthead 2020406080100symbol0.000.020.040.060.080.100.120.14normalized symbol counthead 3020406080100symbol0.000.020.040.060.080.100.12normalized symbol counthead 4020406080100symbol0.000.010.020.030.040.050.060.07normalized symbol counthead 5020406080100symbol0.000.010.020.030.040.050.060.07normalized symbol counthead 6020406080100symbol0.000.010.020.030.040.050.06normalized symbol counthead 7020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 8020406080100symbol0.000.010.020.030.040.050.060.07normalized symbol counthead 9020406080100symbol0.000.020.040.060.080.100.120.14normalized symbol counthead 10020406080100symbol0.000.010.020.030.040.05normalized symbol counthead 11020406080100symbol0.000.020.040.060.08normalized symbol counthead 12goodbadgreatUnder review as a conference paper at ICLR 2023\n\nFigure 7: Distribution of most attended symbols when generating tokens good, bad, great for HSN(100, 5) trained on the Yelp data set. The distribution is computed from the decoder attention matrices between symbols and output for each attention head for layer 12.\n\n28\n\n020406080100symbol0.000.010.020.030.040.050.060.07normalized symbol counthead 1020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 2020406080100symbol0.000.010.020.030.040.050.060.070.08normalized symbol counthead 3020406080100symbol0.000.020.040.060.08normalized symbol counthead 4020406080100symbol0.000.010.020.030.040.050.060.07normalized symbol counthead 5020406080100symbol0.000.020.040.060.080.100.12normalized symbol counthead 6020406080100symbol0.000.020.040.060.080.100.12normalized symbol counthead 7020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 8020406080100symbol0.000.020.040.060.08normalized symbol counthead 9020406080100symbol0.000.020.040.060.080.100.12normalized symbol counthead 10020406080100symbol0.000.020.040.060.08normalized symbol counthead 11020406080100symbol0.000.020.040.060.080.10normalized symbol counthead 12goodbadgreatUnder review as a conference paper at ICLR 2023\n\nG ON COMMONSENSE REASONING GENERATION\n\nIn this section we expatiate on the details of our approach to commonsense reasoning generation.\n\nFirst, we modify the encoder component of HSN to process the tuples (s, r, o) as\n\nqφ(z1:L|[s, r, o], A) = qφ(z1: L\n\n2\n\n|[s, r], A)qφ(z L\n\n2 +1:L|[s, r, o], z L\n\n2\n\n, A),\n\n(39)\n\nso that the first half of the schema depends on subject and relation only, whereas the second half depends on the entire 3-tuple. As it will become evident below, this decoupling is necessary for the inference of novel objects.\n\nEach of the posterior distributions above is modelled with the same architecture, as shown in Fig. 1, but sharing a single pretrained BERT model. That is, we have two copies of all pink-shaded blocks , A), and a single in the Fig. 1, one for qφ(z1: L pretrained BERT model.\n\n|[s, r], A), the other for qφ(z L\n\n2 +1:L|[s, r, o], z L\n\n2\n\n2\n\nUsing such a 2-component encoder model we are able to successfully infer schema representations for the KG tuples, as shown in Table 4. The task is however to infer new objects, given only subjectrelation pairs. We thus need a way to infer schema representations without relying on the phrase object o.\n\nsolution to this\n\nThe classical qφ(z L – where z L\n\n2 +1:L|[s, r, o], z L\n\n2\n\n, A) with a local, trainable prior model of the form pθ(z L\n\ninference problem is\n\nà la Kalman Filter, , A) 2 +1:L|[s, r], z L |[s, r], A) – and train the prior via the KL term in Eq. 12.\n\nto replace,\n\n2\n\nis sampled from qφ(z1: L\n\n2\n\n2\n\nAs shown in Section B, maximizing the mutual information between data and representations averages out all local information in the KL term, and thus hinders the learning of the prior – see e.g. HSN[prior] in Table 4: the samples from the prior are not close enough to those of the posterior, hence the significant drop in performance of the model.\n\nAn alternative is to train, in the spirit of knowledge distillation (Hinton et al., 2015), a third-party model on the inferred schemata, to predict z L\n\n.\n\n2 +1:L conditioned on z1: L\n\n2\n\n2\n\n, together with the subject-relation pair, and outputs z L 2 +1:L|z1: L\n\nIndeed, given the inferred schemata from the training KG, we consider a sequence-to-sequence model which inputs z1: L 2 +1:L. That is, a model of the form pθ(z L , [s, r]). Specifically we use (i) a bidirectional LSTM network with hidden dimension of 512 to encode the first half of the schemata, (ii) a pretrained BERT model to encode the subject-relation pair, and (iii) a LSTM network of dimension 512 as an autoregressive decoder model. The initial (hidden) states of the latter are determined by an MLP which inputs the representations from the LSTM and BERT encoder models. The model is trained on samples from qφ(z L\n\n, A).\n\n2\n\n2 +1:L|[s, r, o], z L\n\n2\n\nOur preliminary results, HSN[KD] in Table 4, show that this approach improves upon the untrained prior model, and even outperforms the stand-alone COMET(GPT-2) model.\n\nG.1 ATOMIC DATASET\n\nFor this preliminary study we focus only on the ATOMIC dataset of Sap et al. (2019b). It contains 877K (s, r, o) tuples covering a variety of social commonsense knowledge around specific If-Then events. A bit more in detail, ATOMIC splits its commonsense knowledge into nine categories, covering the event’s causes, its effects on the agent, and its effect on other direct (or implied) participants. We use the training splits from Sap et al. (2019b), resulting in 710K training, 80K validation, and 87K test tuples respectively.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\n(a) HSN(50, 5)\n\n(b) HSN(50, 20)\n\nFigure 8: Schema distributions inferred from each category of the Yahoo dataset, for HSN(50, L) with L = {5, 20}. The node positions in the figure are consistent among labels and were computed using a force-directed embedding of the global graph G.\n\nwith considerable irony the case also shows how completely japan has turned the tables on u.s. business (1) in brief the chancellor of the exchequer nigel lawson’s decisions were justified by their\n\nintended political and financial convenience and credit\n\n(2) analysts said they expect the federal authority to be totally revamped giving japanese manufacturers\n\nmore clear way to measure their exports.\n\n(3) but others say inco commission has been inadequate (4) in 1970 banco exterior an agency run by banco exterior <unk> de <unk> <unk> was attempting\n\nto reduce liabilities and raise the sale of certain works by the division\n\nthe amended filings also point out that under a new agreement <unk> has an <unk> obligation to sell farmers to axa upon an acquisition of b.a.t\n\nTable 10: Interpolation between two random instances from the PTB dataset\n\n30\n\nSociety & CultureScience & MathematicsHealthEducation & ReferenceComputers & InternetSportsBusiness & FinanceEntertainment & MusicFamily & RelationshipsPolitics & GovernmentSociety & CultureScience & MathematicsHealthEducation & ReferenceComputers & InternetSportsBusiness & FinanceEntertainment & MusicFamily & RelationshipsPolitics & GovernmentUnder review as a conference paper at ICLR 2023\n\n(a) HSN(100, 5)\n\n(b) HSN(100, 20)\n\nFigure 9: Schema distributions inferred from each category of the Yahoo dataset, for HSN(100, L) with L = {5, 20}. The node positions in the figure are consistent among labels and were computed using a force-directed embedding of the global graph G.\n\n31\n\nSociety & CultureScience & MathematicsHealthEducation & ReferenceComputers & InternetSportsBusiness & FinanceEntertainment & MusicFamily & RelationshipsPolitics & GovernmentSociety & CultureScience & MathematicsHealthEducation & ReferenceComputers & InternetSportsBusiness & FinanceEntertainment & MusicFamily & RelationshipsPolitics & GovernmentUnder review as a conference paper at ICLR 2023\n\n(a) HSN(50, 5)\n\n(b) HSN(50, 20)\n\n(c) HSN(100, 5)\n\n(d) HSN(100, 20)\n\nFigure 10: Schema distributions inferred from each category of the Yelp dataset. The node positions in the figure are consistent among labels and were computed using a force-directed embedding of the global graph G.\n\n32\n\nvery badbadmediocregoodvery goodvery badbadmediocregoodvery goodvery badbadmediocregoodvery goodvery badbadmediocregoodvery goodUnder review as a conference paper at ICLR 2023\n\nFigure 11: Degree distributions of inferred graphs from all corpora, compared to Erdos-Renyi graphs for p = 0.5. The upper four plots show results for full inferred graphs, the lowest two show the degree distributions of the largest connected component of the models for K = 100.\n\n33\n\n01020304050Node Degree0246810Node CountK=50,L=5ptbyelperdosyahoo01020304050Node Degree05101520Node CountK=50,L=20ptbyelperdosyahoo020406080100Node Degree0102030Node CountK=100,L=5ptbyelperdosyahoo020406080100Node Degree010203040Node CountK=100,L=20ptbyelperdosyahoo020406080100Node Degree0246810Node CountK=100,L=5 [LCC]ptbyelperdosyahoo020406080100Node Degree0.02.55.07.510.012.515.017.5Node CountK=100,L=20 [LCC]ptbyelperdosyahooUnder review as a conference paper at ICLR 2023\n\nInterpolate Society & Culture Science & Mathematics is steady eye contact good? when i am communicating with someone, i tend to give very steady, long _UNK eye contact. so i tried to _UNK it as a young naive girl, and now it’s a habit i can’t lose... it just depends on the person you are having the conversation with... (1) i am 14 years old. their is this girl again who speaks very much of me and talks 2 me, the idea of\n\nme 2 her and never gives any suggestion to verbal _UNK for my 2nd. listen, therapy!\n\n(2) what do you do when you think your best friend told you shes bisexual? when she says that,\n\nor you might have believed if your friend said it is. they’re inevitably sharing that they don’t share... (3) how do you change liquid in an ice cube into liquid form? paste, mix and freeze _UNK a gallon of co2\n\ninto a _UNK and then _UNK in some ice to form a coating.\n\n(4) what kind of rules does gravity apply? if a certain weight is placed in a container, the net force\n\napplied on it hits the water surface and the right weight will turn into gravity\n\nhow does a photovoltaic system that feeds back into the power grid get on the same phase angle? or? should i say does it need to be the same as the _UNK’s?\n\nInterpolate: Business & Finance Family – Relationships at 35, am i too old to go to college to become a psychiatrist? i’m 37, and i just started my second semester in a 2 year college... you need to be prepare for the financial aspects, but the social ones are no problem... (1) what would be a good title for a _UNK _UNK? i have _UNK in _UNK and there are no real courses done for it but i do love the job and i’ve already done my freshman year. i am currently teaching placement at _UNK and need the same as the average undergraduate student...\n\n(2) has anyone here applied in the past 4 months or is it better to get a try out y _UNK a slightly better\n\nlong term career _UNK ...\n\n(3) lately im having trouble with my fiancee, how do i bring him back? it obvious at this point that you\n\ncan’t “ bring us together ”. try playing games.\n\n(4) could i still go out with this guy and still be friends and respect him.? i don’t want to just fell in love\n\nwith the guy that i was with. i want 2 be with friend’s girl and still be friends...\n\nhow do i know if my man, is inlove with me? well... some questions, how old are _UNK? - are you wealthy?, is he wealthy?, how long have you been together...\n\nTable 11: Interpolation between four random instances from the Yahoo dataset\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\nInterpolate: Very bad - bad do not use this company!! they told me within one hour, then i called again they said the driver have 90 mins. 90 minutes later, they said the driver is in traffic and wait for 15 minutes, i checked google map no accident, all green on all freeway... (1) i ordered for pick up as my daughter hadn’t been told that or even ordered online. when i spoke to the young\n\nlady, who was _UNK, she carried on a conversation with not a manager. it’s bad customer service and i wouldn’t even bother with this place...\n\n(2) place was clean... when i called to let them know i ’d get something else, the person that answering the phone wouldn’t understand me... really? i gave this restaurant a b + for the cleanliness of the food and the friendliness of the staff\n\n(3) i had the quesadilla and the carnitas tacos. i felt every bite of these were so rubbery\n\nand the potatoes were off. i feel like the service and the quality of food can do much better. (4) somewhat disappointed. i did it once and loved it but today, today’s water is bitter and salty...\n\nand the mint and cherry blossom _UNK’flavors just taste that way.\n\nthe food quality doesn’t match the place at all. i think it’s ok for a pub but this place is supposed to be a nice place for professional lunches. i had the chicken flatbread and the chicken was more like subway chicken! with so many options around that area i won’t pick this place for lunch.\n\nInterpolate: Very bad – Very Good skip it... there are much better options out there! the “ hot ” food was not hot, and the flavor was only mediocre at most. (1) indifferent to locals. the kids size pizzas were a billion times worse than a pizza hut. the quality of food\n\nwas just awful. i wouldn’t recommend this to a significant other for what it is.\n\n(2) this new mexican spot is ok, bordering on childish. i went with friends and ordered a carne asada burro...\n\nit wasn’t off the hook ; what made this place great were the chips & salsa sucked. yuck! ...\n\n(3) wow. _UNK you give so much frosting!! we were a groupon special for a cupcake for the princess of chocolate,\n\nand we were pretty stoked. they were _UNK and creative. they even suggested we try the coconut ... we ’ll definitely be back soon.\n\n(4) went for the first time during a recent trip to vegas. our server jeff made special recommendations for our friends and i.\n\nit was fantastic most of the food was light and fresh... i would highly recommend this place!\n\ni had dinner at republic kitchen tonight for the first time and was very impressed with the service, the decor, the menu, and the food quality... i am going back sunday for their brunch and jazz!\n\nTable 12: Interpolation between four random instances from the Yelp dataset\n\n35",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a method for learning discrete sequential representations of sequence data. The model demonstrates surprisingly strong performance in language modeling, and preliminary results on commonsense reasoning seem promising. Analysis of the schema networks also demonstrates interpretability.\n\n# Strength And Weaknesses\n\n*Strengths*\n\nThe proposed method has the potential to be more interpretable and controllable than single-vector representations and achieves strong performance on ELBO evaluations. The perplexity bound evaluations in this paper are standard in the related sentence VAE literature [1,2].\nBefore reading this paper, my prior belief was that such a latent variable model would not result in perplexity improvements. Those beliefs have now changed.\n\n*Weaknesses*\n\nThe paper is convincing but could be improved with further experiments on controllable text generation (following Li et. al 2020) and further development of the commonsense reasoning experiment.\n\nEdit: I also agree with reviewer 5yft's position that compositionality claims are not supported. Reducing the claim to a state-of-the-art sentence VAE with discrete representations is one solution.\n\n*Questions and comments*\n* To check my understanding, is the following alternative interpretation of the model correct: the model is a (discrete, first-order) hidden Markov model with an autoregressive emission model (GPT-2) and global sparsity distribution over the transition matrix (schema network)?\n* What is the reason for using LSTMs in the commonsense reasoning schema completion model (Appendix E)? BART or GPT2 might give better results.\n\n[1] Li, Chunyuan et. al. “Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space.” EMNLP (2020).\n\n[2] Kim, Yoon et al. “Semi-Amortized Variational Autoencoders.” ICML (2018).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing is clear and the paper provides all details for reproducibility. The method is novel and the results are surprising.\n\n# Summary Of The Review\n\nI recommend acceptance. The paper delivers on the surprising claim of a state-of-the-art sentence VAE with discrete latent representations.\n\nEdit on 12/5: After further discussion with the authors, the perplexity comparisons do not use a valid bound. Without updated and fair numbers, I am changing my score to reject until the evaluation numbers are finalized. I think this is interesting work and encourage the authors to (re-)submit with updated numbers and claims.\n\nEdit on 12/13: After receiving the updated evaluation metrics, I am changing my score back to accept.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCASA: BRIDGING THE GAP BETWEEN POLICY IMPROVEMENT AND POLICY EVALUATION WITH CONFLICT AVERSE POLICY ITERATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe study the problem of model-free reinforcement learning, which is often solved following the principle of Generalized Policy Iteration (GPI). While GPI is typically an interplay between policy evaluation and policy improvement, most conventional model-free methods with function approximation assume the independence of GPI steps, despite of the inherent connections between them. In this paper, we present a method that attempts to eliminate the inconsistency between policy evaluation step and policy improvement step, leading to a conflict averse GPI solution with gradient-based functional approximation. Our method is capital to balancing exploitation and exploration between policy-based and value-based methods and is applicable to existing policy-based and value-based methods. We conduct extensive experiments to study theoretical properties of our method and demonstrate the effectiveness of our method on Atari 200M benchmark.\n\n1\n\nINTRODUCTION\n\nModel-free reinforcement learning has made many impressive breakthroughs in a wide range of Markov Decision Processes (MDP) (Vinyals et al., 2019; Pedersen, 2019; Badia et al., 2020). Overall, the methods could be cast into two categories, value-based methods such as DQN (Mnih et al., 2015) and Rainbow (Hessel et al., 2017), and policy-based methods such as TRPO (Schulman et al., 2015), PPO (Schulman et al., 2017) and IMPALA (Espeholt et al., 2018).\n\nValue-based methods learn state-action values and select the action according to their values. The main target of value-based methods is to approximate the fixed point of the Bellman equation through the generalized policy iteration (GPI) (Sutton & Barto, 2018), which generally consists of policy evaluation and policy improvement. One characteristic of the value-based methods is that unless a more accurate state-action value is estimated by iterations of the policy evaluation, the policy will not be improved. Previous works equip value-based methods with many carefully designed structures to achieve more promising reward learning and sample efficiency (Wang et al., 2016; Schaul et al., 2015; Kapturowski et al., 2018).\n\nPolicy-based methods learn a parameterized policy directly without consulting state-action values. One characteristic of policy-based methods is that they incorporate a policy improvement phase in every training step, while in contrast, the value-based methods only change the policy after the action corresponding to the highest state-action values is changed. In principle, policy-based methods perform policy improvement more frequently than value-based methods.\n\nWe notice that value-based and policy-based methods locate at the two extremes of GPI, where value-based methods won’t improve the policy until a more accurate policy evaluation is achieved, while policy-based methods improve the policy for every training step even when the policy evaluation hasn’t converged. To\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nmitigate the defect of each, we pursuit a technique that is capable of balancing between the two extremes flexibly. We first study the gradients between policy improvement and policy evaluation and notice that they show a positive correlation statistically during the entire training process. To find out if there exists a way that the gradients of the policy improvement and the policy evaluation are parallel, we propose CASA, Critic AS an Actor, which satisfies a weaker compatible condition (Sutton et al., 1999) and enhances gradient consistency between policy improvement and policy evaluation.\n\nWith further delving into the properties of CASA, we find CASA is an innovative combination of value-based and policy-based methods. When the policy-based methods are equipped with CASA, the collapse to the sub-optimal solution as the entropy goes to zero is prevented by the evaluation of the state-action values, which encourages exploration. When the value-based methods are equipped with CASA, the policy improvement via policy gradient is equivalent to the evaluation of the state-action values and a self-bootstrapped policy improvement, which enhances exploitation.\n\nTo enable CASA for a large scale off-policy learning, we introduce Doubly-Robust Trace (DR-Trace), which exploits doubly-robust estimator (Jiang & Li, 2016) and guarantees the synchronous convergence of the state-action values and the state values.\n\nOur main contributions are as follows:\n\n(i) We present a novel method CASA which enhances gradient consistency between policy evaluation and\n\npolicy improvement and present extensive studies on the behavior of the gradients.\n\n(ii) We demonstrate CASA could be freely applied to both policy-based and value-based algorithms with\n\nmotivating examples.\n\n(iii) We present extensive empirical study on Atari benchmark , where our conflict averse algorithm brings\n\nsubstantial improvements over the baseline methods.\n\n2 PRELIMINARY\n\nConsider an infinite-horizon MDP, defined by a tuple (S, A, p, r, γ), where S is the state space, A is the action space, p : S × A × S → [0, 1] is the state transition probability function, r : S × A → R is the reward function, and γ is the discounted factor. The policy is a mapping π : S × A → [0, 1] which assigns a distribution over the action space given a state.\n\nThe objective of reinforcement learning is to maximize the return, or cumulative discounted rewards,\n\nmaximize J = Etraj∼π\n\n(cid:35)\n\nγtr(st, at)\n\n,\n\n(cid:34)\n\n(cid:88)\n\nt\n\n(1)\n\nwhere traj = {s0, a0, r0, . . . } is a trajectory sampled by π with policy-environment interaction.\n\nValue-based methods maximize J by estimating various type of value functions: the state value function is defined as V π(s) = Eπ [(cid:80) t γtrt|s0 = s], the state-action value function is defined as Qπ(s, a) = Eπ [(cid:80) t γtrt|s0 = s, a0 = a]; the advantage function is defined as Aπ(s, a) = Qπ(s, a) − V π(s). The objective of maximizing the value functions in value-based methods can be improved through GPI until converging to the optimal policy. For the approximated state-value function Qθ that estimates Qπ, the policy evaluation is conducted by:\n\nminimize Eπ[(Qπ(s, a) − Qθ(s, a))2], where Qπ is estimated by various methods, e.g., λ-return (Sutton, 1988) and ReTrace (Munos et al., 2016). The policy improvement is usually achieved by greedily selecting actions with the highest state-action values.\n\n(2)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The GPI process in our work. Unlike (Sutton & Barto, 2018), we evaluate π by Q instead of V , and we improve π using policy gradient ascent (pg for brevity) instead of greedy. The learning procedure is shown by the black arrows, i.e., E → I → E → I · · · .\n\nFigure 2: GPI with function approximation. Due to the constraint of approximated function space, the ideal policy iteration cannot be actually achieved. The underlying process of GPI with function approximation can be regarded as doing policy improvement and policy evaluation in an ideal space then being projected back into the approximated function space (Sutton & Barto, 2018; Ghosh et al., 2020).\n\nPolicy-based methods maximize J by optimizing some parameterized policy πθ according to the policy gradient theorem (Sutton & Barto, 2018),\n\n∇θJ = Eπ[Ψ(s, a)∇θ log πθ(a|s)].\n\n(3)\n\nThe vanilla policy gradient uses Ψ = (cid:80)∞ t=0 γtrt. Actor-critic algorithms approximate Ψ(s, a) in the form of baseline, e.g., IMPALA (Espeholt et al., 2018) adopts Ψ(s, a) = r + γV ̃π(s′) − Vθ(s) and uses V-Trace to estimate V ̃π.\n\n3 METHODOLOGY\n\n3.1 MOTIVATION\n\nWe use Vθ to estimate V π, Qθ to estimate Qπ and πθ to represent the policy, where θ represents all parameters to be optimized. In this work, there is one backbone and two individual heads after the backbone. The advantage function and the policy share one head, and state value function is the other head. Hence the policy reuses all parameters of value functions except that temperature τ is only for the policy. We keep τ static in this work. We use E to represent the policy evaluation, which gives the ascent direction of the gradient by θ ← θ + ηEπ[(Qπ − Qθ)∇θQθ]. We use I to represent the policy improvement, which gives θ ← θ + ηEπ[(Qπ − Vθ)∇θ log πθ].\n\nLet’s recap the GPI process as shown in Figure 1. To get rid of the function approximation error, we first assume the approximation function enjoys infinite capacity. We use < x, y > to denote the angle between two vectors, where < x, y >= arccos( ||x||·||y|| ) with arccos : [−1, 1] → [0, π]. We define an important notion β, which represents the angle between the gradient ascent directions of I and E, as follows,\n\nx·y\n\nβ\n\ndef\n\n= < Eπ[(Qπ − Qθ)∇θQθ], Eπ[(Qπ − Vθ)∇θ log πθ] > .\n\n(4)\n\nWhen β = 0 i.e.cos(β) = 1, I and E become parallel to each other, which is the blue arrow in Figure 1, and there is no conflict between the gradient ascent directions of I and E anymore. When β = π/2 i.e.cos(β) = 0, I and E are perpendicular. When β = π i.e.cos(β) = −1, I and E are toward exactly opposite directions.\n\nNext, we assume the representation capacity of the approximation function is limited. When the function approximation is involved, i.e. Qπ is estimated by Qθ and π is approximated by πθ, from the view of operators (Ghosh et al., 2020), each of I and E can be further decomposed into two operators, as shown in Figure 2. One is to do the policy improvement and the policy evaluation, the other is to project into the restricted function space. When β > 0, GPI with function approximation would involve two projection\n\n3\n\nπ*,Q*Q=Qππ=pg(Q)π,Qπθ,QθQθ=Qππ*,Q*πθ=pg(Qθ)Under review as a conference paper at ICLR 2023\n\nFigure 3: Return, χ, cos(β) and entropy. PPO is adjusted with two additional versions to evaluate stateaction values. R2D2 uses a surrogate policy to approximate policy gradient. Entropy of R2D2 is entropy of Boltzmann policy on stateaction values. Details are in Appendix B.\n\noperators in each iteration, which introduces inevitable approximation error. When β = 0, if the function approximation error is not considered, we find that the gradient conflict between I and E would be totally eliminated. If we consider the limitation of the approximation function, similar to the blue arrow in Figure 1, one iteration (represented by two black arrows and two dotted arrows) can be united into one arrow and one dotted arrow (not shown in Figure 2 but analogy to the blue arrow in Figure 1), where the gradient conflict is eliminated and the two projection operators are reduced to one correspondingly.\n\nAs stated above, if β = 0 holds, we can expect that the gradient conflict between the policy improvement and the policy evaluation is eliminated and the function approximation error could be reduced. However, β is usually estimated by sampling with stochasticity. It’s difficult to let β = 0 by optimizing θ. Instead, we consider another notion χ by removing step sizes and taking expectation outside, where the angle of each state is fully controllable by θ.\n\nχ\n\ndef\n\n= Eπ[cos < ∇θQθ, ∇θ log πθ >].\n\n(5)\n\nIn fact, χ is highly correlated to compatible value function (Sutton et al., 1999), and Theorem 3 shows that χ = 1 is the necessary condition for the compatible condition ∇θQθ = ∇θ log πθ, which is a weaker compatible condition. More details about compatible value function are in Appendix A.\n\nTo further understand the behavior of β and χ, we track cos(β) and χ of two algorithms PPO and R2D2 as representatives for policy-based and value-based methods, respectively. We show an important fact in Figure 3 that both χ and cos(β) are statistically positive for both original version and adjusted versions, which means that arccos(χ) and β are likely to be less then π/2 with neural network approximated functions. The aforementioned conceptual and empirical findings inspire us to raise the following question on GPI: whether we can guarantee χ = 1, so that cos(β) is also closer to 1.\n\n3.2 FORMULATION\n\nDenote τ ∈ R+ to be a positive temperature and sg to be a stop gradient operator. CASA can estimate Vθ and Aθ by any function parameterized by θ, where πθ and Qθ are derived as follows:\n\n\n\n\n\n\n\nπθ(·|s) = softmax(Aθ(s, ·)/τ ), ̄Aθ(s, a) = Aθ(s, a) −\n\n(cid:88)\n\nsg(πθ(a′|s))Aθ(s, a′),\n\nQθ(s, a) = ̄Aθ(s, a) + sg(Vθ(s)).\n\na′\n\n(6)\n\nNote that there exist two sg operators in equation 6. The first sg operator is used for computing advantage as ̄Aθ = Aθ − Eπ[Aθ] = Aθ − sg(πθ) · Aθ, where the sg operator here guarantees the gradients between\n\n4\n\n0612182531374350Millions of frames0100200300400500Breakout ReturnPPO+CASAPPOPPO ver1PPO ver2612182531374350Millions of frames0.00.20.40.60.81.0612182531374350Millions of frames0.00.20.40.60.81.0cos <LQ,J>612182531374350Millions of frames0.00.51.01.52.02.53.0Entropy0612182531374350Millions of frames010203040506070Breakout ReturnR2D2+CASAR2D2 ver1R2D2 ver2612182531374350Millions of frames0.00.20.40.60.81.0612182531374350Millions of frames0.00.20.40.60.81.0cos <LQ,J>612182531374350Millions of frames2.7252.7502.7752.8002.8252.8502.875EntropyUnder review as a conference paper at ICLR 2023\n\npolicy improvement and policy evaluation are parallel, which we elaborate later. Intuitively, this sg operator also means that we keep πθ unchanged when evaluating the policy πθ. The second sg operator exists in Qθ = ̄Aθ + sg(Vθ). As (Chen & He, 2020) regards sg in siamese representation learning as a case of EM-algorithm (Dempster et al., 1977), a similar interpretation exists here. Qθ = ̄Aθ + sg(Vθ) decomposes the estimation of Qθ into a two stage problem, where the first is to estimate the advantage of each action without changing the expectation, the second is to estimate the expectation.\n\nThe equation 6 includes a straightforward refinement of dueling-DQN. We know dueling-DQN estimates Qπ by Qθ = Aθ + Vθ, but it cannot guarantee Eπ[Aθ] = 0 i.e. Eπ[Qθ] = Vθ due to the function approximation error. But if we estimate Qπ by Qθ = Aθ − Eπ[Aθ] + Vθ, it satisfies the necessary condition Eπ[Qθ] = Vθ without loss of generality.\n\n3.3 PATH CONSISTENCY BETWEEN POLICY EVALUATION AND POLICY IMPROVEMENT\n\nFor brevity, we omit θ and V, Q, A, π are all approximated functions. Denote the estimations of V and Q as V ̃π and Q ̃π respectively. For instance, one choice is to calculate V ̃π and Q ̃π by V-Trace (Espeholt et al., 2018) and ReTrace (Munos et al., 2016) respectively.\n\nAt training time, the policy evaluation is achieved by updating θ to minimize,\n\nLV (θ) = Eπ[(V ̃π − V )2], LQ(θ) = Eπ[(Q ̃π − Q)2],\n\nwhich gives the ascent direction of θ by:\n\n∇θLV (θ) = Eπ\n\n(cid:2)(V ̃π − V )∇θV (cid:3) , ∇θLQ(θ) = Eπ\n\n(cid:2)(Q ̃π − Q)∇θQ(cid:3) .\n\nAnd we make the policy improvement by policy gradient, which gives the ascent direction of θ by:\n\n∇θJ (τ, θ) = Eπ\n\n(cid:2)τ (Q ̃π − V )∇θ log π(cid:3) ,\n\nwhere J (τ, θ) = τ Eπ[(cid:80) γtrt]. It takes an additional τ , which frees the scale of gradient from τ .\n\nThe final gradient ascent direction of θ is given by:\n\nα1∇θLV + α2∇θLQ + α3∇θJ .\n\nWith (V, Q, π) defined in equation 6, by Lemma E.1, we have,\n\n∇θQ = (1 − π)∇θA = τ ∇θ log π.\n\nFor brevity, denote the shared gradient path as g = (1 − π)∇θA.\n\nPlugging equation 10 into equation 7 equation 8, we have,\n\n∇θLQ = Eπ\n\n(cid:2)(Q ̃π − Q)g(cid:3) , ∇θJ = Eπ\n\n(cid:2)(Q ̃π − V )g(cid:3) .\n\n(7)\n\n(8)\n\n(9)\n\n(10)\n\n(11)\n\nBy equation 11, ∇θLQ and ∇θJ walk along the same vector direction of gradient path g for each state. By equation 10, this is exactly the case χ = 1. Since all parameters to estimate Q and π are shared except for τ , we call it Critic AS an Actor.\n\nIf we make a subtraction between ∇θLQ and ∇θJ , we have,\n\n∇θJ = ∇θLQ + Eπ [(Q − V )g] . We know Eπ [(Q − V )g] is a self-bootstrapped policy gradient with function approximated Q. Recalling the fact that the value-based methods improves the policy by greedily selecting actions according to Q, if we apply ∇θJ on θ, it additionally utilizes Q to do policy improvement. This is a more greedy usage of Q to improve policy than its usual usage.\n\n(12)\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nIf we exploit the structural information as (V, Q, π) defined by equation 6, by Lemma E.2,\n\nEπ [(Q − V )g] = τ Eπ [(Q − V )∇θ log π] = −τ 2∇θH[π],\n\nthen we have,\n\n∇θLQ = ∇θJ + τ 2∇θH[π].\n\n(13)\n\nThe equation 13 shows ∇θLQ is a policy gradient with an entropy regularization. If we apply ∇θLQ on θ for policy-based methods, an entropy regularization works implicitly by α2∇θLQ in equation 9, which prevents the policy collapse to a sub-optimal solution.\n\n3.4 DR-TRACE AND OFF-POLICY TRAINING\n\nDR-Trace\n\nV-Trace / ReTrace\n\nδDR t =rt + γV (st+1) − Q(st, at) Eμ[Vt + (cid:80) k≥0 γkc[t:t+k−1]ρt+kδDR t+k] k≥0 γkc[t+1:t+k−1](1{k=0} + 1{k>0}ρt+k)δDR t+k] t − Vt)∇ log π]\n\nEμ[ρt(Q ̃π\n\nEμ[Qt + (cid:80)\n\nV ̃π\n\nQ ̃π\n\n∇J\n\nδV /Q t =rt + γV (st+1)/Q(st+1, at+1) − V (st)/Q(st, at)\n\nEμ[Vt + (cid:80)\n\nk≥0 γkc[t:t+k−1]ρt+kδV k≥0 γkc[t+1:t+k]δQ t+1 − Vt)∇ log π]\n\nEμ[Qt + (cid:80) Eμ[ρt(rt + V ̃π\n\nt+k]\n\nt+k]\n\nTable 1: Comparison between DR-Trace and V-Trace/ReTrace.\n\nTo enable off-policy training with behavior policy μ, one choice is to estimate V ̃π and Q ̃π in equation 7 and equation 8 by V-Trace and ReTrace. As CASA estimates (V, Q, π), applying Doubly Robust (Jiang & Li, 2016) is feasible and suitable. We propose DR-Trace and find the convergence rate and the fixed point of DR-Trace are the same as V-Trace’s according to its convergence proof. For completeness, we provide DR-Trace and its comparison with V-Trace/ReTrace in Table 1. More details are in Appendix D.\n\n4 EXPERIMENTS\n\n4.1 BASIC SETUP\n\nWe employ a Learner-Actor pipeline (Espeholt et al., 2018) for large-scale training. Motivation and ablation experiments on PPO and R2D2 don’t use LSTM, only experiments on CASA+DR-Trace use LSTM (Schmidhuber, 1997), which is for comparison with other algorithms. We use burn-in (Kapturowski et al., 2018) when LSTM is used. All estimated values share the same backbone, which is followed by two fully connected layers for each individual head. We use no intrinsic reward and no entropy regularization in any experiment. We find that using life information can greatly increase the performance of some games. However, to be general, we will not end the episode if life is lost. All hyperparameters are in Appendix F. For brevity, we denote ∇LV = Eπ[(V π − Vθ)∇Vθ], ∇LQ = Eπ[(Qπ − Qθ)∇Qθ] and ∇J = Eπ[(Qπ − Vθ)∇ log πθ], where expectation is batch-wise average in our implementation. When we write < a, b > with a, b ∈ {∇LV , ∇LQ, ∇J }, we firstly calculate batch-wise averaged gradient of a and b, then we calculate the angle in-between. When we write cos < ∇Q, ∇ log π > or χ, we mean Eπ[cos < ∇θQθ, ∇θ log πθ >], which firstly calculates element-wise cosines and then takes batch-wise average. To avoid numerical problem, we calculate\n\nmax(||x||,10−8)·max(||y||,10−8) .\n\n||x||·||y|| by\n\nx·y\n\nx·y\n\n4.2 APPLICATION OF CASA ON REPRESENTATIVE ALGORITHMS\n\nCASA is applicable to existing algorithms. We take PPO and R2D2 for demonstration. The application of CASA on PPO is straightforward. Applying CASA on R2D2 is impossible as either ε-greedy policy\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nPPO\n\nPPO+CASA\n\nR2D2\n\nFunc. Approx.\n\n(V, logit) = (Vθ, logitθ) π = softmax(logit)\n\n⇒\n\n(V, A) = (Vθ, Aθ) π = softmax(A/τ ) ̄A = A − sg(π) · A Q = ̄A + sg(V )\n\n(V, A) = (Vθ, Aθ) Q = A + V\n\n⇒\n\nR2D2+CASA\n\n(V, A) = (Vθ, Aθ) π = softmax(A/τ ) ̄A = A − sg(π) · A Q = ̄A + sg(V )\n\nGradient\n\n0.5∇LV + ∇J\n\n⇒ 0.5∇LV + ∇LQ + ∇J\n\n∇LQ\n\n⇒ 0.5∇LV + ∇LQ + ∇J\n\nTable 2: Examples of applying CASA on policy-based methods (PPO) and value-based methods (R2D2).\n\nor arg max Q policy breaks the gradient. This problem is the same as calculating the gradients of policy improvement for value-based methods. We use a surrogate policy πsurrogate = softmax(A/τ ), which is discussed in Appendix B. Table 2 summarizes adjustments of function approximations and training gradients.\n\nSince PPO+CASA and R2D2+CASA have the same function approximation, recalling the fact that valuebased methods improve the policy when a more accurate evaluation is achieved and policy-based methods improve the policy for every step, we can balance the two flexibly with χ = 1 by α1, α2, α3 in equation 9.\n\nIn Figure 3, algorithms with CASA show much higher cos(β) and χ. PPO+CASA does more exploration than the original PPO, as the entropy of π doesn’t easily drop to zero. R2D2+CASA tends to distinct the state-action values, where we use the entropy of Q to measure how greedy the current state-action values are.\n\n4.3 BEHAVIOR OF GRADIENTS ON DIFFERENT STRUCTURES\n\nPPO+CASA Q = Aθ − sg(πθ) · Aθ + sg(Vθ)\n\ntype 1 type 2 type 3 type 4 type 5\n\nQ = Aθ − πθ · Aθ + sg(Vθ) Q = Aθ − sg(πθ) · Aθ + Vθ Q = Aθ + sg(Vθ) Q = Aθ + Vθ Q = Qθ\n\nTable 3: Behavior of gradient on different types. Type 1&2 are CASA-like structures, where type 1 removes sg of π and type 2 removes sg of Vθ. Type 3&4 are dueling-like structures, where type 3 adds sg to V for dueling-Q and type 4 is dueling-Q. Type 5 uses a new head to estimate Qθ separately, which can be considered as an auxiliary task to estimate Qπ.\n\nThough we show that CASA satisfies ∇Q ∝ ∇ log π, which means χ = 1, it’s unknown if the structure of CASA is unique. As Q = A − Eπ[A] + sg(V ) is a direct refinement of dueling-DQN, we try several different structures of PPO+CASA. All settings of estimating state-action values are shown in Table 3. We always use 0.5 · ∇LV + ∇LQ + ∇J as the training gradient. We present Breakout and Qbert in Figure 4.\n\nFor the sake of clarity, we group PPO+CASA and type 3 as sg-V group, type 2 and type 4 as no-sg-V group. The sg-V group has higher χ and higher cos(β), which is closer to the compatible condition and the consistency between two GPI steps, and no-sg-V group is always worst than its contrast in sg-V group.\n\nPPO+CASA has χ = 1 and the highest cos(β). Type 1 has less returns than PPO+CASA. Hence, when applying a CASA-like structure, stopping the gradient of π is always preferred.\n\nType 5 uses an individual head to estimate Qπ, which performs the worst. Hence, a well-designed CASA-like or dueling-like structure is always preferred.\n\nBy scatter plot and box plot in Figure 4, χ and cos(β) are positive correlated depending on different structures. This phenomenon answers part of the last question of Section 3.1: for these specific designed structures, χ and cos(β) show positive correlation.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: The ablation results evaluated on Breakout (top row) and Qbert (bottom row). From left to right is the return curve, χ, cos(β), scatter plot of (χ, cos(β)) and box plot of (χ, cos(β)). Each scatter point is one batch sampled from every consecutive 100 batches. Each box is the interquartile range of scatter points.\n\n4.4 EVALUATION OF CASA ON ATARI GAMES\n\nWe present an extensive evaluation on CASA, where we train CASA + DR-Trace on 57 Atari games and report the results in terms of two metrics. The first is Human Normalized Score (HNS), which normalizes the reward by random policy and human expert policies. The other is Standardized Atari BEnchmark for RL (SABER), which normalizes the reward by random policy and human world records, where the normalized score is capped by 200%. SABER is considered because recent studies show that the median HNS could easily get hacked by the algorithm since it is sensitive to improvement on a small subset of games. Table 4 summarizes the results.\n\nMean HNS Median HNS Mean SABER Median SABER\n\nRainbow IMPALA LASER CASA\n\n873.97 957.34 1741.36 1941.08\n\n230.99 191.82 454.91 246.36\n\n28.39 29.45 36.77 36.10\n\n4.92 4.31 8.08 10.29\n\nTable 4: Evaluation scores for the methods on Atari benchmark presented in %.\n\nNote that CASA is a variant of IMPALA with DR-Trace, and it achieves substantially better records than IMPALA across all the evaluation metrics. It also scores substantially better than all the methods in terms of mean HNS and median SABER scores. Though off-policy methods are known as privileged for HNS evaluation due to replay, CASA outperforms strong off-policy baseline Rainbow. Though LASER outperforms CASA in Median HNS and Mean SABER, CASA outperforms it in median SABER and mean HNS. Overall, the aforementioned results demonstrate the conflict-averse strategy efficiently boosts the performance in largescale training scenarios and outperform strong on/off-policy algorithms. Hyperparameters and individual games are presented in Appendix F and Appendix G, respectively.\n\n5 RELATED WORKS\n\nBoth value-based or policy-based approaches comply with the principle of GPI, but two GPI steps are coarsely related to each other such that jointly optimizing both functions might potentially bring conflicts. Despite of such crucial issue in GPI with function approximation, most decent model-free algorithms adopt a standard\n\n8\n\n0612182531374350Millions of frames0100200300400500Breakout ReturnPPO+CASAtype 1type 2type 3type 4type 5612182531374350Millions of frames0.00.20.40.60.81.0612182531374350Millions of frames0.00.20.40.60.81.0cos <LQ,J>0.40.20.00.20.40.60.81.00.40.20.00.20.40.60.81.0cos <LQ,J>0.40.20.00.20.40.60.81.00.40.20.00.20.40.60.81.0cos <LQ,J>0612182531374350Millions of frames0250050007500100001250015000Qbert ReturnPPO+CASAtype 1type 2type 3type 4type 5612182531374350Millions of frames0.00.20.40.60.81.0612182531374350Millions of frames0.00.20.40.60.81.0cos <LQ,J>0.40.20.00.20.40.60.81.00.40.20.00.20.40.60.81.0cos <LQ,J>0.40.20.00.20.40.60.81.00.40.20.00.20.40.60.81.0cos <LQ,J>Under review as a conference paper at ICLR 2023\n\npolicy improvement/evaluation regime without considering conflict diminishing properties. The issue of reducing conflicts among multiple models trained simultaneously was considered in earlier machine learning literature, such as for robust parameter estimation for multiple estimators under incomplete data (Robins & Rotnitzky, 1995; Lunceford & Davidian, 2004; Kang & Schafer, 2007) and multitask learning with gradient similarity measure (Chen et al., 2020; Yu et al., 2020; Javaloy & Valera, 2022).\n\nWhen the idea is introduced to reinforcement learning, earliest attempts tackle conservative and safe policy iteration problems (Kakade & Langford, 2002; Hazan & Kale, 2011; Pirotta et al., 2013). Recently, more works have emerged to study GPI in a fine-grained manner. In (Ghosh et al., 2020), a new Bellman operator is introduced which implements GPI with a policy improvement operator and a projection operator, where the projection attempts to find the best approximation of policy among realizable policies. In (Raileanu & Fergus, 2021), the policy and value updates are decoupled by approximating two networks with representation regularization. In (Cobbe et al., 2021), GPI is separated into a policy improvement and a feature distillation step. On contrast to the aforementioned works, we tackle the conflicts in GPI at the gradient-level, with theoretical analysis. Our work is related to (Nachum et al., 2017), which utilizes both the unbiasedness and stability of on-policy training and the data efficiency of off-policy training to form a soft consistency error. Our work bridges the gap between the two GPI steps from an alternative angle of establishing a closer relationship between policy and value functions in their forms, without the focus on off-policy correction. Due to the difficulty of controlling the gap between GPI steps, we instead consider χ. The condition χ = 1 is close to compatible value function (Sutton et al., 1999; Kakade, 2001), shown in Section 3.1 and Appendix A.\n\n6 LIMITATION\n\nIt’s noticeable that CASA is only applied on discrete action space for now. We further find CASA applicable to any function approximation that is able to estimate advantage functions of all actions. We provide additional discussion on continuous action space in Appendix C.\n\nSince π shares all parameters of value functions, it brings χ = 1 but sacrifices the freedom of π to be parameterized by other parameters. We conjecture that CASA is one endpoint of a trade-off curve between χ and the freedom of π, where the other endpoint is that π shares no parameter with value functions.\n\n7 ETHICS AND REPRODUCIBILITY STATEMENT\n\nThis paper is aimed at academic issues in deep reinforcement learning, and the experiment used is also in the early stage, but it may provide opportunities for malicious applications of reinforcement learning in the future. We describe all details to reproduce the main experimental results in Appendix F.\n\n8 CONCLUSION\n\nThis paper attempts to eliminate gradient inconsistency between policy improvement and policy evaluation. The proposed innovative actor-critic design Critic AS an Actor (CASA) enhances consistency of two GPI steps by satisfying a weaker compatible condition. We present both theoretical proof and empirical evaluation for CASA. The results show that our proposed method achieves state-of-the-art performance standards with noticeable performance gain over several strong baselines when evaluated on ALE 200 million (200M) benchmark. We also present several ablation studies, which demonstrates the effectiveness of the proposed method’s theoretical properties. Future work includes studying the connection between the compatible condition and the gradient consistency between policy improvement and policy evalution.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAdrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. arXiv preprint arXiv:2003.13350, 2020.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning.\n\narXiv preprint\n\narXiv:2011.10566, 2020.\n\nZhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign dropout. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nKarl Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic policy gradient. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 2020–2027, 2021.\n\nA. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em\n\nalgorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, 39(1):1–38, 1977.\n\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.\n\nDibya Ghosh, Marlos C Machado, and Nicolas Le Roux. An operator view of policy gradient methods.\n\nAdvances in Neural Information Processing Systems, 33:3397–3406, 2020.\n\nElad Hazan and Satyen Kale. Better algorithms for benign bandits. J. Mach. Learn. Res., 12:1287–1311,\n\n2011.\n\nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.\n\nAdrián Javaloy and Isabel Valera. Rotograd: Gradient homogenization in multitask learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022.\n\nNan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In Interna-\n\ntional Conference on Machine Learning, pp. 652–661. PMLR, 2016.\n\nSham M Kakade. A natural policy gradient. Advances in neural information processing systems, 14, 2001.\n\nSham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Machine Learning, Proceedings of the Nineteenth International Conference (ICML 2002), University of New South Wales, Sydney, Australia, July 8-12, 2002, pp. 267–274, 2002.\n\nJoseph DY Kang and Joseph L Schafer. Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data. Statistical science, 22(4):523–539, 2007.\n\nSteven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations, 2018.\n\nJared K Lunceford and Marie Davidian. Stratification and weighting via the propensity score in estimation of\n\ncausal treatment effects: a comparative study. Statistics in medicine, 23(19):2937–2960, 2004.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n\nRemi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 1054–1062. Curran Associates, Inc., 2016.\n\nOfir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2775–2785, 2017.\n\nCarsten Lund Pedersen. Re: Human-level performance in 3d multiplayer games with population-based\n\nreinforcement learning. Science, 2019.\n\nMatteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello. Safe policy iteration. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28 of JMLR Workshop and Conference Proceedings, pp. 307–315, 2013.\n\nRoberta Raileanu and Rob Fergus. Decoupling value and policy for generalization in reinforcement learning. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 8787–8798, 2021.\n\nJames M Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression models with\n\nmissing data. Journal of the American Statistical Association, 90(429):122–129, 1995.\n\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint\n\narXiv:1511.05952, 2015.\n\nSepp Hochreiter; Jürgen Schmidhuber. Long short-term memory. Neural Computation., 1997.\n\nSimon Schmitt, Matteo Hessel, and Karen Simonyan. Off-policy actor-critic with shared experience replay.\n\nIn International Conference on Machine Learning, pp. 8545–8554. PMLR, 2020.\n\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy\n\noptimization. In International conference on machine learning, pp. 1889–1897, 2015.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-\n\ntion algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nRichard S. Sutton. Learning to predict by the methods of temporal differences. Mach. Learn., 3:9–44, 1988.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999.\n\nMarin Toromanoff, Emilie Wirbel, and Fabien Moutarde. Is deep reinforcement learning really superhuman\n\non atari? leveling the playing field. arXiv preprint arXiv:1908.04683, 2019.\n\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network In International conference on machine learning, pp.\n\narchitectures for deep reinforcement learning. 1995–2003, 2016.\n\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient In Advances in Neural Information Processing Systems 33: Annual surgery for multi-task learning. Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA COMPATIBLE VALUE FUNCTION\n\nThe original policy gradient with compatible value function is stated as follow.\n\nTheorem 1 (Sutton et al. (1999)). Let Qw be a state-action function with parameter w and πθ be a policy function with parameter θ. If Qw satisfies Eπ[(Qπ − Qw)∇wQw] = 0 and ∇wQw = ∇θ log πθ, then\n\n∇θJ = Eπ[Qw∇θ log πθ].\n\nIf we let w = θ in Theorem 1, where Qw and πθ share parameters, we have the following theorem. Theorem 2. Let Qθ be a state-action function with parameter θ and πθ be a policy function with parameter θ. If Qθ satisfies Eπ[(Qπ − Qθ)∇θQθ] = 0 and ∇θQθ = ∇θ log πθ, then\n\n∇θJ = Eπ[Qθ∇θ log πθ].\n\nDefine\n\nχ\n\ndef\n\n= Eπ[cos < ∇θQθ, ∇θ log πθ >].\n\nWe show that χ = 1 is the necessary condition for the compatible condition ∇θQθ = ∇θ log πθ. Theorem 3. i) If ∇θQθ ∝ ∇θ log πθ for all states, then χ = 1.\n\nii) If χ = 1, then ∇θQθ ∝ ∇θ log πθ for all states.\n\nBy Theorem 3, χ = 1 is equivalent to ∇θQθ ∝ ∇θ log πθ, and ∇θQθ ∝ ∇θ log πθ is the necessary condition for ∇θQθ = ∇θ log πθ, hence χ = 1 is the necessary condition for ∇θQθ = ∇θ log πθ.\n\nProof. i) Since ∇θQθ ∝ ∇θ log πθ, we have < ∇θQθ, ∇θ log πθ >= 0. By definition of χ, we have\n\nχ = Eπ[cos < ∇θQθ, ∇θ log πθ >] = Eπ[1] = 1.\n\nii) Since χ ≤ 1 and cos(x) is monotonic decreasing as x goes from 0 to π, the equality χ = 1 only holds when all states satisfy cos < ∇θQθ, ∇θ log πθ >= 0, which means ∇θQθ ∝ ∇θ log πθ.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB GRADIENTS BETWEEN POLICY IMPROVEMENT AND POLICY EVALUATION\n\nFunction Approximation\n\nTrain Gradients\n\nCosine of Interested Angles\n\nPPO\n\nPPO ver.1\n\n(V, logit) = (Vθ, logitθ) π = softmax(logit)\n\n(Q, logit) = (Qθ, logitθ), π = softmax(logit) V = sg(π) · Q\n\n0.5∇LV + ∇J\n\n0.5∇LV + ∇J\n\nPPO ver.2\n\n(Q, logit) = (Qθ, logitθ), 0.5∇LV + ∇LQ + ∇J\n\ncos < ∇LQ, ∇J > cos < ∇Q, ∇ log π >\n\ncos < ∇LQ, ∇J > cos < ∇Q, ∇ log π >\n\nPPO+CASA\n\npi = softmax(logit) V = sg(π) · Q\n\n(V, A) = (Vθ, Aθ), π = softmax(A/τ ), ̄A = A − sg(π) · A Q = ̄A + sg(V )\n\n0.5∇LV + ∇LQ + ∇J\n\ncos < ∇LQ, ∇J > cos < ∇Q, ∇ log π >\n\nTable 5: PPO is the original PPO. PPO ver.1 and PPO ver.2 are adapted versions to calculate ∇LQ. PPO+CASA is applying CASA on PPO, which is described in Sec. 4.2.\n\nFunction Approximation\n\nTrain Gradients\n\nCosine of Interested Angles\n\nR2D2\n\nR2D2 ver.1\n\nR2D2+CASA\n\n(V, A) = (Vθ, Aθ) Q = A + V π = softmax(A/τ )\n\n(V, A) = (Vθ, Aθ) Q = A + V π = softmax(A/τ )\n\n(V, A) = (Vθ, Aθ), π = softmax(A/τ ), ̄A = A − sg(π) · A Q = ̄A + sg(V )\n\n∇LQ\n\ncos < ∇LQ, ∇J >\n\n0.5∇LV + ∇LQ\n\ncos < ∇LQ, ∇J >\n\n0.5∇LV + ∇LQ + ∇J\n\ncos < ∇LQ, ∇J >\n\nTable 6: R2D2 is the original R2D2. R2D2 ver.1 is adapted version to include ∇LV for training. R2D2+CASA is applying CASA on R2D2, which is described in Sec. 4.2.\n\nTo understand the behavior of\n\nβ\n\ndef\n\n= < Eπ[(Qπ − Qθ)∇θQθ], Eπ[(Qπ − Vθ)∇θ log πθ] >\n\nand\n\nχ\n\ndef\n\n= Eπ[cos < ∇θQθ, ∇θ log πθ >]\n\nin reinforcement learning algorithms, we choose PPO as a representative for policy-based methods and R2D2 as a representative for value-based algorithms.\n\nDefine\n\nand\n\nLV (θ) = Eπ[(V π − Vθ)2], LQ(θ) = Eπ[(Qπ − Qθ)2],\n\n∇θJ (θ) = Eπ [(Qπ − Vθ)∇θ log π] .\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nWe usually have above three kinds of loss functions in reinforcement learning, which aim to estimate the state values, state-action values and the policy. We do not talk about the estimations of V π and Qπ as they are estimated as their usual way of PPO’s and R2D2’s. All hyperparameters are listed in Appendix F.\n\nFor brevity, we write\n\ncos < ∇Q, ∇ log π >= Eπ[cos < ∇θQθ, ∇θ log πθ >],\n\nand\n\ncos < ∇LQ, ∇J >= cos < Eπ[(Qπ − Qθ)∇θQθ], Eπ[(Qπ − Vθ)∇θ log πθ] >, cos < ∇LV , ∇J >= cos < Eπ[(V π − Vθ)∇θVθ], Eπ[(Qπ − Vθ)∇θ log πθ] >, cos < ∇LV , ∇LQ >= cos < Eπ[(V π − Vθ)∇θVθ], Eπ[(Qπ − Qθ)∇θQθ] > .\n\nThe fact that PPO only has ∇θLV and ∇θJ and R2D2 only has ∇θLQ is the main difficulty to track cos(β) and χ. To solve the problem, we adjust PPO and R2D2 with different versions.\n\nFor PPO, we displace the estimation of Vθ by sg(π) · Qθ, where Qθ is estimated by function approximation and Vθ is estimated by taking the expectation of Qθ. All versions of PPO are listed in Table 5.\n\nFor R2D2, we point out that though we apply ε-greedy to interact with environments, ε is only used for exploration and the final target policy of value-based methods is simply arg max Qθ. Because arg max Qθ breaks the gradient, we use a surrogate policy to approximate the gradient of policy improvement. Since R2D2 uses dueling structure and softmax(Aθ/τ ) = softmax(Qθ/τ ) τ →0+−→ arg max Qθ, we use πsurrogate = softmax(Aθ/τ ) to calculate the policy gradient. We only use πsurrogate on learner to calculate the gradient, where the policy that interacts with environments is still ε-greedy. All versions of R2D2 are listed in Table 6.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nC ON DISCUSSING APPLICATION OF CASA ON CONTINUOUS ACTION SPACE\n\nAs we can see CASA is only applied to discrete action space in the main context, we make a discussion on whether CASA is applicable on continuous action space. For brevity, we let τ = 1 and write equation 6 as:\n\n \n\n\n\nπ = softmax(A), ̄A = A − Eπ[A], Q = ̄A + sg(V ). The difficulty comes from estimating two quantities, one is softmax(A), the other is Eπ[A]. This comes from the fact that discrete action space is countable so these two quantities are expressed in a closed-form, while continuous action space is uncountable so an accurate estimation of these two quantities is intractable. We can surely apply Monte Carlo methods to approximate, but a more elegant close-form expression may be preferred. Then this becomes another problem: how to estimate (state-action values / advantages / policy probabilities) of all actions in a continuous action space efficiently without loss of generality? This is another representational design problem, which is out of scope of this paper, so we don’t touch much about it. But with the hope of inspiring a better solution to this problem, we provide one practical way of applying CASA on continuous action space based on kernel-based machine learning.\n\n(14)\n\nLet a0, . . . , ak to be basis actions in the action space. Let A(s, a0), . . . , A(s, ak) to be advantage functions for tuples of states and basis actions. They can either share parameters or be isolated. Let K(·, ·) be a kernel function defined on the product of two action spaces. For any a in the action space, we can estimate A(s, a) by a decomposition such like\n\nA(s, a) =\n\n1 Za i=0 K(ai, a) is a normalization constant.\n\nwhere Za = (cid:80)k\n\n(K(a0, a)A(s, a0) + · · · + K(ak, a)A(s, ak)),\n\nSince K(·, a) is a closed-form function of a, and |{A(s, a0), . . . , A(s, ak)}| is finite, we can make a closedform expression of both softmax(A) and Eπ[A]. Then we can apply CASA directly on this expression, with one function estimates V and the other function estimates advantages of all actions in a closed-form with only state as input. The policy is defined directly by softmax of all advantages. In details, we define\n\n\n\n \n\nπ(s, a) = exp(A(s, a))/\n\nexp(A(s, a))da,\n\n(cid:90)\n\n ̄A(s, a) = A(s, a) −\n\na\n\nsg(π(s, a))A(s, a)da,\n\n(cid:90)\n\na\n\nQ(s, a) = ̄A(s, a) + sg(V (s)).\n\n(15)\n\nThen it satisfies the consistency of CASA on continuous action space.\n\n∇ log π(s, a) = ∇A(s, a) −\n\n∇ (cid:82) (cid:82)\n\na exp(A(s, a))da a exp(A(s, a))da a exp(A(s, a))∇A(s, a)da a exp(A(s, a))da exp(A(s, a)) (cid:82) a exp(A(s, a))da\n\n(cid:82)\n\na\n\nπ(s, a)∇A(s, a)da\n\n(cid:82)\n\n(cid:90)\n\n= ∇A(s, a) −\n\n= ∇A(s, a) −\n\n= ∇A(s, a) −\n\n(cid:90)\n\n∇A(s, a)da\n\n= ∇ ̄A(s, a) = ∇Q(s, a).\n\na\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nD DR-TRACE\n\nAs CASA estimates (V, Q, π), we would ask i) how to guarantee that ̃πV T race = ̃πReT race, ii) how to exploit (V, Q, π) to make a better estimation. Though we can apply V-Trace to estimate V and ReTrace to estimate Q with proper hyperparameters to guarantee ̃πV T race = ̃πReT race, it’s more reasonable to estimate (V, Q) together. Inspired by Doubly Robust, which is shown to maximally reduce the variance, we introduce DR-Trace, which estimates V by\n\nV ̃π\n\nDR(st)\n\ndef\n\n= Eμ[V (st) +\n\n(cid:88)\n\nk≥0\n\nγkc[t:t+k−1]ρt+kδDR\n\nt+k],\n\nwhere μ is the behavior policy, δDR = min{ πt μt\n\n= min{ πt μt\n\n, ̄ρ} and ct\n\nρt\n\ndef\n\ndef\n\nt\n\ndef = rt + γV (st+1) − Q(st, at) is one-step Doubly Robust error, i=0 ct+i.\n\n, ̄c} are clipped per-step importance sampling, c[t:t+k]\n\n= (cid:81)k\n\ndef\n\nWith one step Bellman equation, we estimate Q by\n\nQ ̃π\n\nDR(st, at)\n\ndef\n\n= Est+1,rt∼p(·,·|st,at)[rt + γV ̃π = Eμ[Q(st, at) +\n\n(cid:88)\n\nDR(st+1)]\n\nγkc[t+1:t+k−1] ̃ρt,kδDR\n\nt+k],\n\nwhere ̃ρt,k = 1{k=0} + 1{k>0}ρt+k. Theorem 4. Define ̄A = A − Eπ[A], Q = ̄A + sg(V ), (cid:88)\n\ndef\n\nT (Q)\n\n= Eμ[Q(st, at) +\n\nγkc[t+1:t+k−1] ̃ρt,kδDR\n\nt+k],\n\nk≥0\n\nk≥0\n\nS (V )\n\ndef\n\n= Eμ[V (st) +\n\n(cid:88)\n\nk≥0\n\nγkc[t:t+k−1]ρt,kδDR\n\nt+k],\n\nU (Q, V ) = (T (Q) − Eπ[Q] + S (V ), S (V )), U (n)(Q, V ) = U (U (n−1)(Q, V )),\n\nthen U (n)(Q, V ) → (Q ̃π, V ̃π) that corresponds to\n\n ̃π(a|s) =\n\nmin { ̄ρμ(a|s), π(a|s)} b∈A min { ̄ρμ(b|s), π(b|s)}\n\n(cid:80)\n\n.\n\nas n → +∞.\n\nProof. See Appendix E, Theorem E.1.\n\nTheorem 4 shows that DR-Trace is a contraction mapping and (V, Q) converges to (V ̃π, Q ̃π) that corresponds to\n\n ̃π(a|s) =\n\nmin { ̄ρμ(a|s), π(a|s)} b∈A min { ̄ρμ(b|s), π(b|s)}\n\n.\n\n(cid:80)\n\nAccording to our proof, DR-Trace should work similar to V-Trace and ReTrace, as the convergence rate and the limitation are same. We compare DR-Trace with V-Trace+ReTrace in Figure 5, where we replace estimation of state values by V-Trace and estimation of state-action values by ReTrace. We call V-Trace+ReTrace as No-DR-Trace for brevity. No-DR-Trace performs better on Breakout and ChopperCommand, but fails to make a breakthrough on Krull. Recalling the fact that Doubly Robust can maximally reduce the variance of Bellman error, No-DR-Trace is less stable but also potential to achieve a better performance. A conclusion cannot be made about No-DR-Trace, as this phenomenon means that No-DR-Trace is less stable than DR-Trace, but it also holds the potential to achieve a better performance.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Ablation study for w/wo DR-Trace on Breakout, ChopperCommand and Krull.\n\nE PROOFS\n\nLemma E.1. (i) Define π = sof tmax(A/τ ), then ∇ log π = (1 − π) ∇A and define ̄A = A − Eπ[A], Q = ̄A + sg(V ), then ∇Q = (1 − π)∇A.\n\nτ . (ii) Denote sg to be stop gradient\n\nProof. As Q = ̄A + sg(V ) = A − sg(π) · A + sg(V ), it’s obvious that ∇Q = (1 − π)∇A.\n\nFor log π, it’s a standard derivative of cross entropy, so we have ∇ log π = (1−π)∇(A/τ ) = (1−π) ∇A τ .\n\nLemma E.2. Define ̄A = A − Eπ[A], Q = ̄A + sg(V ), π = sof tmax(A/τ ), then\n\nEπ [(Q − V )∇ log π] = −τ ∇H[π].\n\nProof. Since\n\nwe have\n\nπ = exp(A/τ )/Z, Z =\n\n(cid:90)\n\nA\n\nexp(A/τ ),\n\nBased on the observation that Eπ [f (s)∇ log π(·|s)] = 0, we have\n\nA = τ log π + τ log Z.\n\nEπ [Eπ[A] · ∇ log π] = 0,\n\nEπ [log Z · ∇ log π] = 0.\n\nOn the one hand,\n\nEπ [(Q − V )∇ log π] = Eπ [A∇ log π] − Eπ [Eπ[A] · ∇ log π]\n\n= τ Eπ [log π∇ log π] + τ Eπ [log Z · ∇ log π] = τ Eπ [log π∇ log π] .\n\n18\n\n0255075100125150175200Millions of frames0100200300400500600700Average ReturnBreakoutretrace + vtracedrtrace0255075100125150175200Millions of frames0200000400000600000800000Average ReturnChopperCommand0255075100125150175200Millions of frames020000400006000080000100000Average ReturnKrullUnder review as a conference paper at ICLR 2023\n\nOn the other hand,\n\n∇H[π] = −∇\n\n(cid:90)\n\nπi log πi\n\n= −\n\n= −\n\n(cid:90)\n\nA\n\n(cid:90)\n\nA\n\nA\n\n∇πi · log πi −\n\n(cid:90)\n\nA\n\nπi∇ log πi · log πi −\n\nπi∇ log πi\n\n(cid:90)\n\nA\n\nπi\n\n∇πi πi\n\n= −Eπ [log π∇ log π] .\n\nHence, Eπ [(Q − V )∇ log π] = −τ ∇H[π].\n\nTheorem E.1. Define ̄A = A − Eπ[A], Q = ̄A + sg(V ). Define\n\nT (Q)\n\ndef\n\n= Eμ[Q(st, at) +\n\nγkc[t+1:t+k−1] ̃ρt,kδDR\n\nt+k],\n\n(cid:88)\n\nk≥0\n\nS (V )\n\ndef\n\n= Eμ[V (st) +\n\n(cid:88)\n\nk≥0\n\nγkc[t:t+k−1]ρt,kδDR\n\nt+k],\n\nU (Q, V ) = (T (Q) − Eπ[Q] + S (V ), S (V )), U (n)(Q, V ) = U (U (n−1)(Q, V )),\n\nthen U (n)(Q, V ) → (Q ̃π, V ̃π) that corresponds to\n\n ̃π(a|s) =\n\nmin { ̄ρμ(a|s), π(a|s)} b∈A min { ̄ρμ(b|s), π(b|s)}\n\n(cid:80)\n\n.\n\nas n → +∞. Remark. T (Q) − Eπ[Q] + S (V ) is exactly how Q is updated at training time. Since Q = ̄A + sg(V ), if we apply gradient ascent on Q and V in directions ∇LQ(θ) and ∇LV (θ) respectively, change of Q comes from two aspects. One comes from ∇LQ(θ), which changes A, the other comes from ∇LV (θ), which changes V . Because the gradient of V is stopped when estimating Q, the latter is captured by \"minus old baseline, add new baseline\", which is −Eπ[Q] + S (V ) in Theorem E.1.\n\nProof. Define\n\n(cid:102)T (Q) = −Eπ[Q] + T (Q),\n\n(cid:102)U (Q, V ) = ( (cid:102)T (Q), S (V )), (cid:102)U (n)(Q, V ) = (cid:102)U ( (cid:102)U (n−1)(Q, V )).\n\nBy Lemma E.3, (cid:102)T (n)(Q) converges to some A∗ as n → ∞. This process will not influence the estimation of V as the gradient of V is stopped when estimating Q. According to the proof, A∗ does not depend on V . By Lemma E.4, S (n)(V ) converges to some V ∗ as n → ∞. Hence, we have\n\nBy definition,\n\n(cid:102)U (n)(Q, V ) → (A∗, V ∗) as n → +∞.\n\nU (Q, V ) = ( (cid:102)T (Q) + S (V ), S (V )),\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nwe can regard (cid:102)T (Q) + S (V ) as Q and regard S (V ) as V , then\n\nU (2)(Q, V ) = U ( (cid:102)T (Q) + S (V ), S (V ))\n\nBy induction,\n\n= (T ( (cid:102)T (Q) + S (V )) − S (V ) + S (2)(V ), S (2)(V )) = ( (cid:102)T (2)(Q) + S (2)(V ), S (2)(V )).\n\nU (n)(Q, V ) = ( (cid:102)T (n)(Q) + S (n)(V ), S (n)(V ))\n\n→ (A∗ + V ∗, V ∗) as n → +∞.\n\nSame as (Espeholt et al., 2018),\n\n ̃π(a|s) =\n\nmin { ̄ρμ(a|s), π(a|s)} b∈A min { ̄ρμ(b|s), π(b|s)}\n\n(cid:80)\n\n.\n\nis the policy s.t. the Bellman equation holds, which is\n\nEμ[ρt(rt + γVt+1 − Vt)|Ft] = 0,\n\nand U (Q ̃π, V ̃π) = (Q ̃π, V ̃π). So we have (A∗ + V ∗, V ∗) = (Q ̃π, V ̃π).\n\nLemma E.3. Define ̄A = A − Eπ[A], Q = ̄A + sg(V ), then operator\n\nT (Q)\n\ndef\n\n= Eμ[Q(st, at) +\n\nγkc[t+1:t+k−1] ̃ρt,kδDR t+k]\n\n(cid:88)\n\nk≥0\n\nis a contraction mapping w.r.t. Q. Remark. Note that T (Q) is exactly equation D.\n\nSince Q = A + sg(V ), the gradient of V is stopped when estimating Q, updating Q will not change V , which is equivalent to updating A. Without loss of generality, we assume V is fixed as V ∗ in the proof.\n\nProof. ̄A = A − Eπ[A] shows Eπ[ ̄A] = 0, which guarantees that no matter how we update A, we always have Eπ[Q] = V ∗.\n\nBased on above observations, define\n\n(cid:102)T (Q)\n\ndef\n\n= −Eπ[Q] + T (Q).\n\nIt’s obvious that we only need to prove (cid:102)T (Q) is a contraction mapping.\n\nFor brevity, we denote\n\nQt = Q(st, at), At = A(st, at), V ∗\n\nt = V ∗(st).\n\nNoticing that ̃ρt,0 = 1, let F represent filtration, we can rewrite (cid:102)T as (cid:88)\n\n(cid:102)T (Q) = Eμ[At +\n\nγkc[t+1:t+k−1] ̃ρt,kδDR t+k]\n\nk≥0\n\n= Eμ[−V ∗\n\nt +\n\n(cid:88)\n\nk≥0\n\nγkc[t+1:t+k−1] ̃ρt,krt+k +\n\nγk+1c[t+1:t+k−1]∆k],\n\n(cid:88)\n\nk≥0\n\n(16)\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nwhere\n\nBy definition of Q,\n\n∆k = Eμ\n\n(cid:2) ̃ρt,kV ∗\n\nt+k+1 − ct+k ̃ρt,k+1Qt+k+1|Ft+k\n\n(cid:3) .\n\nEμ[V ∗\n\nt+k+1|Ft+k] = Eμ[Eπ[Qt+k+1|Ft+k+1]|Ft+k],\n\nwe can rewrite equation 17 as\n\n∆k = Eμ[( ̃ρt,k\n\nπt+k+1 μt+k+1\n\n− ct+k ̃ρt,k+1)Qt+k+1|Ft+k].\n\nFor any Q1 = A1 + sg(V ∗), Q2 = A2 + sg(V ∗), since\n\nEμ[( ̃ρt,k\n\nπt+k+1 μt+k+1\n\n− ct+k ̃ρt,k+1)|Ft+k] ≥ 0,\n\nby equation 16 equation 18, we have\n\n|| (cid:102)T (Q1) − (cid:102)T (Q2)|| ≤ C||Q1 − Q2||,\n\n(17)\n\n(18)\n\nwhere\n\nC = Eμ[\n\n(cid:88)\n\nγk+1c[t+1:t+k−1]( ̃ρt,k\n\nπt+k+1 μt+k+1\n\n− ct+k ̃ρt,k+1)]\n\nk≥0\n\n= Eμ[1 − 1 +\n\n(cid:88)\n\nk≥0\n\nγk+1c[t+1:t+k−1] ( ̃ρt,k − ct+k ̃ρt,k+1)]\n\n= 1 − (1 − γ)Eμ[\n\n(cid:88)\n\nγkc[t+1:t+k−1] ̃ρt,k]\n\nk≥0\n\n≤ 1 − (1 − γ) < 1.\n\nHence, (cid:102)T (Q) is a contraction mapping and converges to some fixed function, which we denote as A∗. So T (Q) is also a contraction mapping and converges to A∗ + V ∗.\n\nLemma E.4. Define Q = A + sg(V ) with Eπ[A] = 0, then operator\n\nS (V )\n\ndef\n\n= Eμ[V (st) +\n\nγkc[t:t+k−1]ρt,kδDR t+k]\n\n(cid:88)\n\nk≥0\n\nis a contraction mapping w.r.t. V . Remark. Note that S (V ) is exactly equation D.\n\nProof. Same as Lemma E.3, we can get\n\n∆k = Eμ\n\n(cid:2)(ρt+k − ct+kρt+k+1) Vt+k+1 − ct+kρt+k+1A∗\n\nt+k+1|Ft+k\n\n(cid:3) ,\n\nso we have\n\nk = Eμ The remaining proof is identical to (Espeholt et al., 2018)’s.\n\n(cid:2)(ρt+k − ct+kρt+k+1) · (V 1\n\nk − ∆2\n\n∆1\n\nt+k+1 − V 2\n\nt+k+1)|Ft+k\n\n(cid:3) .\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nF HYPERPARAMETERS\n\nOur python packages are shown in Table 7.\n\nPackage\n\nVersion\n\nale-py gym tensorflow opencv-python opencv-contrib-python\n\n0.6.0.dev20200207 0.19.0 1.15.2 4.1.2.30 4.4.0.46\n\nTable 7: Versions for python packages among all experiments.\n\nAll experiments follow the shared hyperparameters as in Table 8. The specific hyperparameters for PPO, R2D2 and CASA+DR-Trace are shown in Table 9, Table 10 and Table 11. The only exceptions are V -loss scaling, Q-loss scaling and π-loss scaling, which may be zero depending on some specific ablation settings. We will state these three hyperparameters every time in all experiments.\n\nParameter\n\nValue\n\nAtari Version Atari Wrapper Image Size Grayscale Num. Action Repeats Num. Frame Stacks Action Space End of Episode When Life Lost Num. Environments Random No-ops Burn-in Stored Recurrent State Bootstrap Optimizer Weight Decay Rate Weight Decay Schedule Learning Rate Warmup Steps Learning Rate Schedule AdamW β1 AdamW β2 AdamW ε AdamW Clip Norm Learner Push Model Every n Steps Actor Pull Model Every n Steps\n\nNoFrameskip-v4 gym.wrappers.atari_preprocessing (84, 84) Yes 4\n4 Full No 160 30 Yes Yes Adam Weight Decay 0.01 Anneal linearly to 0 5e-4 4000 Anneal linearly to 0 0.9 0.98 1e-6 50.0 25 64\n\nTable 8: Configurations for shared hyperparameters among all experiments.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nParameter\n\nValue\n\nNum. States Sample Reuse Reward Shape Burn-in Seq-length Discount (γ) Batch size Backbone PPO clip ε GAE λ Temperature (τ )\n\n50M 1\nclip(r, 0, 1) 0\n40 0.995 8\nIMPALA,shallow without LSTM 0.2 0.8 0.1\n\nTable 9: Hyperparameter configurations for PPO.\n\nParameter\n\nValue\n\nNum. States Sample Reuse Target Shape Target Shape Function h Bootstrap Length n ε-greedy PER Sample Temperature α 0.9 PER Buffer Size Burn-in Seq-length Discount (γ) Batch size Backbone Temperature (τ )\n\nt = h((cid:80)n−1\n\n50M 2\nQ ̃π h(x) = sign(x) · ((cid:112)|x| + 1 − 1) + 10−3x 5\nε ∼ 0.4uniform(1,8)\n\ni=0 γirt+i + γnh−1(Double(Qt+n)))\n\n400000 0\n40 0.997 8\nIMPALA,shallow without LSTM 0.1\n\nTable 10: Hyperparameter configurations for R2D2.\n\nParameter\n\nValue\n\nNum. States Sample Reuse Reward Shape Burn-in Seq-length Discount (γ) Batch size Backbone LSTM Units V -loss Scaling (α1) Q-loss Scaling (α2) π-loss Scaling (α3) Temperature (τ ) Importance Sampling Clip ̄c Importance Sampling Clip ̄ρ\n\n200M 2\nlog(|r| + 1.0) · (2 · 1{r≥0} − 1{r<0}) 40 80 0.997 64 IMPALA,deep 256 1.0 10.0 10.0 1.0 1.05 1.05\n\nTable 11: Hyperparameter configurations for CASA + DR-Trace.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nG EVALUATION OF CASA ON ATARI GAMES\n\nRandom scores and average human’s scores are from (Badia et al., 2020). Human World Records (HWR) are from (Toromanoff et al., 2019). Rainbow’s scores are from (Hessel et al., 2017). IMPALA’s scores are from (Espeholt et al., 2018). LASER’s scores are from (Schmitt et al., 2020), no sweep at 200M.\n\nGames\n\nScale\n\nalien amidar assault asterix asteroids atlantis bank heist battle zone beam rider berzerk bowling boxing breakout centipede chopper command crazy climber defender demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar hero ice hockey jamesbond kangaroo krull kung fu master montezuma revenge ms pacman name this game phoenix pitfall pong private eye qbert riverraid road runner robotank seaquest skiing solaris space invaders star gunner surround tennis time pilot tutankham up n down venture video pinball wizard of wor yars revenge zaxxon MEAN HNS(%) MEDIAN HNS(%)\n\nRND\n\nHUMAN\n\nRAINBOW\n\nHNS(%)\n\nIMPALA\n\nHNS(%)\n\nLASER\n\nHNS(%)\n\nCASA\n\nHNS(%)\n\n200M\n\n15962.1 1554.79 19148.47 300732 108590.05 849967.5 1223.15 20885 32463.47 1852.7 59.92 99.96 787.34 11049.75 28255 136950 185203 132826.98 -0.33 0\n44.85 0\n317.75 66782.3 359.5 33730.55 3.48 601.5 1632 8147.4 43375.5 0\n7342.32 21537.2 210996.45 -1.66 20.98 98.5 351200.12 29608.05 57121 12.96 1753.2 -10180.38 2365 43595.78 200625 7.56 0.55 48481.5 292.11 332546.75 0\n572898.27 9157.5 84231.14 32935.5\n\n200M\n\n35565.9 1829.2 21560.4 240090 213025 841200 569.4 64953.3 90881.6 25579.5 48.3 100 747.9 292792 761699 167820 336953 133530 14 0\n45.2 0\n5083.5 114820.7 1106.2 31628.7 17.4 37999.8 14308 9387.5 607443 0.3 6565.5 26219.5 519304 -0.6 21 96.3 21449.6 40362.7 45289 62.1 2890.3 -29968.4 2273.5 51037.4 321528 8.4 12.2 105316 278.9 345727 0\n511835 29059.3 166292.3 41118\n\n512.15 106.4 4106.62 2892.46 454.91 5120.19 75.14 175.14 546.52 1015.51 18.31 832.5 2590.97 2928.65 11569.27 626.93 2112.50 7332.89 1481.82 0.00 258.79 0.00 117.54 5316.40 29.36 102.69 236.36 13868.08 477.91 729.70 2701.26 0.01 94.19 415.64 8000.84 3.42 118.13 0.10 160.15 247.31 578.00 617.53 6.72 -100.86 9.35 3346.45 3347.21 111.52 232.26 6125.34 171.25 3093.19 0.00 2896.98 679.60 316.99 449.47 1741.36 454.91\n\n200M\n\n26137 560 16228 213580 80339 3211600 895.3 91269 57456 1648 162.4 98.3 624.3 102600 616690 161250 421600 291590 20.25 10019 53.24 3.46 1583 188680 4311 24236 1.56 12468 5399 64347 124630.1 2488.4 7579 32098 498590 -17.8 20.39 134.1 27371 11182 251360 10.44 11862 -12730 2319 3031 337150 -10 -21.05 84341 381 416020 0\n297920 26008 118730 46070.8\n\n375.50 32.34 3080.37 2572.80 170.60 19772.10 119.24 246.36 344.70 60.81 101.24 818.33 2161.81 1012.57 9364.42 600.70 2647.75 16022.76 1765.91 1164.32 273.99 11.69 35.55 8743.90 130.19 77.88 105.45 4543.10 179.25 5878.13 553.31 52.35 109.44 517.76 7681.23 3.16 116.40 0.16 204.70 62.38 3208.64 84.95 28.09 34.23 9.76 189.58 3510.18 0.00 17.74 4862.62 236.62 3723.06 0.00 1686.22 606.83 224.61 503.66 1941.08 246.36\n\n228.03 90.39 3642.43 3623.67 231.14 5174.39 163.61 55.88 193.81 68.98 26.76 832.17 2727.92 90.26 417.29 503.69 1152.93 7294.24 830.45 0.00 258.13 0.00 5.92 3087.14 5.87 109.75 121.32 209.09 52.97 613.53 191.82 0.00 105.88 334.30 3243.82 3.40 118.07 0.11 2641.14 179.15 729.04 110.93 4.01 54.21 10.18 2857.09 2085.97 106.42 157.10 2703.84 179.71 2975.08 0.00 3242.59 204.96 157.60 359.96 957.34 191.82\n\n200M\n\n9491.7 5131.2 14198.5 428200 2712.8 826660 1358 62010 16850.2 2545.6 30 99.6 417.5 8167.3 16654 168788.5 55105 111185 -0.3 2125.9 31.3 34 9590.5 70354.6 1419.3 55887.4 1.1 19809 14637.5 8741.5 52181 384 5380.4 13136 108529 0\n20.9 4234 33817.5 22920.8 62041 61.4 15898.9 -12957.8 3560.3 18789 127029 9.7 0\n12926 241 125755 5.5 533936.5 17862.5 102557 22209.5\n\n227.8 5.8 222.4 210 719 12850 14.2 236 363.9 123.7 23.1 0.1 1.7 2090.9 811 10780.5 2874.5 152.1 -18.6 0\n-91.7 0\n65.2 257.6 173 1027 -11.2 29 52 1598 258.5 0\n307.3 2292.3 761.5 -229.4 -20.7 24.9 163.9 1338.5 11.5 2.2 68.4 -17098 1236.3 148 664 -10 -23.8 3568 11.4 533.4 0\n0 563.5 3092.9 32.5 0.00 0.00\n\n7127.8 1719.5 742 8503.3 47388.7 29028.1 753.1 37187.5 16926.5 2630.4 160.7 12.1 30.5 12017 7387.8 36829.4 18688.9 1971 -16.4 860.5 -38.8 29.6 4334.7 2412.5 3351.4 30826.4 0.9 302.8 3035 2665.5 22736.3 4753.3 6951.6 8049 7242.6 6463.7 14.6 69571.3 13455.0 17118.0 7845 11.9 42054.7 -4336.9 12326.7 1668.7 10250 6.5 -8.3 5229.2 167.6 11693.2 1187.5 17667.9 4756.5 54576.9 9173.3 100.00 100.00\n\n134.26 299.08 2689.78 5160.67 4.27 5030.32 181.86 167.18 99.54 96.62 5.01 829.17 1443.75 61.22 240.89 630.80 330.27 6104.40 831.82 247.05 232.51 114.86 223.10 3252.91 39.21 184.10 101.65 72.24 488.05 669.18 230.99 8.08 76.35 188.37 1662.80 3.43 117.85 6.05 253.20 136.77 791.85 610.31 37.70 32.44 20.96 1225.82 1318.22 119.39 153.55 563.36 146.99 1122.08 0.46 3022.07 412.57 193.19 242.62 873.97 230.99\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nGames\n\nScale\n\nalien amidar assault asterix asteroids atlantis bank heist battle zone beam rider berzerk bowling boxing breakout centipede chopper command crazy climber defender demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar hero ice hockey jamesbond kangaroo krull kung fu master montezuma revenge ms pacman name this game phoenix pitfall pong private eye qbert riverraid road runner robotank seaquest skiing solaris space invaders star gunner surround tennis time pilot tutankham up n down venture video pinball wizard of wor yars revenge zaxxon MEAN SABER(%) MEDIAN SABER(%)\n\nRND\n\nHWR\n\nRAINBOW\n\nSABER(%)\n\nIMPALA\n\nSABER(%)\n\nLASER\n\nSABER(%)\n\nCASA\n\nSABER(%)\n\n200M\n\n15962.1 1554.79 19148.47 300732 108590.05 849967.5 1223.15 20885 32463.47 1852.7 59.92 99.96 787.34 11049.75 28255 136950 185203 132826.98 -0.33 0\n44.85 0\n317.75 66782.3 359.5 33730.55 3.48 601.5 1632 8147.4 43375.5 0\n7342.32 21537.2 210996.45 -1.66 20.98 98.5 351200.12 29608.05 57121 12.96 1753.2 -10180.38 2365 43595.78 200625 7.56 0.55 48481.5 292.11 332546.75 0\n572898.27 9157.5 84231.14 32935.5\n\n200M\n\n976.51 1829.2 21560.4 240090 213025 841200 569.4 64953.3 90881.6 25579.5 48.3 100 747.9 292792 761699 167820 336953 133530 14 0\n45.2 0\n5083.5 114820.7 1106.2 31628.7 17.4 37999.8 14308 9387.5 607443 0.3 6565.5 26219.5 519304 -0.6 21 96.3 21449.6 40362.7 45289 62.1 2890.3 -29968.4 2273.5 51037.4 321528 8.4 12.2 105316 278.9 345727 0\n511835 29059.3 166292.3 41118\n\n14.04 1.75 200.00 23.99 2.02 7.82 0.68 8.08 9.06 2.41 9.10 100.00 86.54 22.37 76.15 75.10 5.56 8.57 82.32 0.00 84.14 0.00 1.10 32.29 0.57 3.06 60.59 83.41 1.00 7.60 60.73 0.00 2.16 104.36 12.92 0.20 100.00 0.07 0.89 3.91 2.22 81.17 0.28 -93.09 0.94 8.19 200.00 93.88 80.36 164.82 4.98 200.00 0.00 0.57 7.22 1.09 49.11 36.78 8.08\n\n6.25 1.49 200.00 30.06 1.03 7.90 1.47 2.58 3.21 0.16 13.30 99.96 91.11 0.69 2.75 60.33 3.03 8.53 46.14 0.00 83.93 0.00 0.06 18.75 0.11 3.27 31.10 1.26 0.11 6.39 4.31 0.00 2.43 83.94 5.24 0.20 99.95 0.07 14.63 2.83 2.80 14.58 0.17 50.03 1.02 6.99 200.00 89.59 54.35 72.76 5.22 200.00 0.00 0.64 2.18 0.54 39.33 29.45 4.31\n\n200M\n\n26137 560 16228 213580 80339 3211600 895.3 91269 57456 1648 162.4 98.3 624.3 102600 616690 161250 421600 291590 20.25 10019 53.24 3.46 1583 188680 4311 24236 1.56 12468 5399 64347 124630.1 2488.4 7579 32098 498590 -17.8 20.39 134.1 27371 11182 251360 10.44 11862 -12730 2319 3031 337150 -10 -21.05 84341 381 416020 0\n297920 26008 118730 46070.8\n\n10.29 0.53 189.99 21.34 0.76 30.20 1.07 11.37 5.71 0.14 50.31 98.3 72.20 7.73 61.64 71.95 6.97 18.73 98.11 105.46 89.08 9.11 0.33 53.11 2.54 2.32 27.03 27.33 0.38 61.22 12.44 0.20 2.51 130.00 12.40 0.19 98.54 0.11 1.13 0.99 12.33 11.17 1.18 31.59 0.98 0.46 200.00 0.00 6.14 130.84 6.88 200.00 0.00 0.33 6.45 0.77 55.03 36.10 10.29\n\n200M\n\n9491.7 5131.2 14198.5 428200 2712.8 826660 1358 62010 16850.2 2545.6 30 99.6 417.5 8167.3 16654 168788.5 55105 111185 -0.3 2125.9 31.3 34 9590.5 70354.6 1419.3 55887.4 1.1 19809 14637.5 8741.5 52181 384 5380.4 13136 108529 0\n20.9 4234 33817.5 22920.8 62041 61.4 15898.9 -12957.8 3560.3 18789 127029 9.7 0\n12926 241 125755 5.5 533936.5 17862.5 102557 22209.5\n\n227.8 5.8 222.4 210 719 12850 14.2 236 363.9 123.7 23.1 0.1 1.7 2090.9 811 10780.5 2874.5 152.1 -18.6 0\n-91.7 0\n65.2 257.6 173 1027 -11.2 29 52 1598 258.5 0\n307.3 2292.3 761.5 -229.4 -20.7 24.9 163.9 1338.5 11.5 2.2 68.4 -17098 1236.3 148 664 -10 -23.8 3568 11.4 533.4 0\n0 563.5 3092.9 32.5 0.00 0.00\n\n251916 104159 8647 1000000 10506650 10604840 82058 801000 999999 1057940 300 100 864 1301709 999999 219900 6010500 1556345 21 9500 71 38 454830 355040 162850 1000000 36 45550 1424600 104100 1000000 1219200 290090 25220 4014440 114000 21 101800 2400000 1000000 2038100 76 999999 -3272 111420 621535 77400 9.6 21 65300 5384 82840 38900 89218328 395300 15000105 83700 100.00 100.00\n\n3.68 4.92 165.90 42.81 0.02 7.68 1.64 7.71 1.65 0.23 2.49 99.60 48.22 0.47 1.59 75.56 0.87 7.13 46.21 22.38 75.60 89.47 2.09 19.76 0.77 5.49 26.06 43.45 1.02 6.97 5.19 0.03 1.75 47.30 2.69 0.20 99.76 4.14 1.40 2.16 3.04 80.22 1.58 29.95 2.11 3.00 164.67 100.51 53.13 15.16 4.27 152.14 0.01 0.60 4.38 0.66 26.51 28.39 4.92\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\n26\n\n-5e+305e+31e+41.5e+42e+42.5e+43e+43.5e+4-20k020k40k60k80k100k120k140k160k18(1.)Alien-1000100200300400500600700800-20k020k40k60k80k100k120k140k16(2.)Amidar-2e+302e+34e+36e+38e+31e+41.2e+41.4e+41.6e+41.8e+42e+4-20k020k40k60k80k100k120k140k16(3.)Assault-1e+501e+52e+53e+54e+55e+56e+57e+58e+5-20k020k40k60k80k100k120k140k16(4.)Asterix-1e+401e+42e+43e+44e+45e+46e+47e+48e+49e+41e+51.1e+51.2e+5-20k020k40k60k80k100k120k140k160k18(5.)Asteroids-5e+505e+51e+61.5e+62e+62.5e+63e+63.5e+64e+6-10k010k20k30k40k50k60k70k80k90k100k11(6.)Atlantis-200-10001002003004005006007008009001e+31.1e+31.2e+3-20k020k40k60k80k100k120k140k16(7.)BankHeist-2e+402e+44e+46e+48e+41e+51.2e+51.4e+5-20k020k40k60k80k100k120k140k160k18(8.)BattleZone-1e+401e+42e+43e+44e+45e+46e+47e+48e+49e+4-20k020k40k60k80k100k120k140k160k18(9.)BeamRider-20002004006008001e+31.2e+31.4e+31.6e+31.8e+32e+3-20k020k40k60k80k100k120k140k16(10.)Berzerk-20020406080100120140160180200-20k020k40k60k80k100k120k140k16(11.)Bowling-100102030405060708090100110120-20k020k40k60k80k100k120k140k160k18(12.)Boxing-1000100200300400500600700800-20k020k40k60k80k100k120k140k16(13.)Breakout-2e+402e+44e+46e+48e+41e+51.2e+51.4e+5-20k020k40k60k80k100k120k140k16(14.)Centipede-1e+501e+52e+53e+54e+55e+56e+57e+58e+5-20k020k40k60k80k100k120k140k16(15.)ChopperCom-mand-2e+402e+44e+46e+48e+41e+51.2e+51.4e+51.6e+51.8e+52e+5-20k020k40k60k80k100k120k140k16(16.)CrazyClimber-5e+405e+41e+51.5e+52e+52.5e+53e+53.5e+54e+54.5e+55e+55.5e+5-20k020k40k60k80k100k120k140k16(17.)Defender-5e+405e+41e+51.5e+52e+52.5e+53e+53.5e+5-20k020k40k60k80k100k120k140k16(18.)DemonAttack-30-25-20-15-10-5051015202530-20k020k40k60k80k100k120k140k16(19.)DoubleDunk-2e+302e+34e+36e+38e+31e+41.2e+41.4e+4-20k020k40k60k80k100k120k140k16(20.)Enduro-120-100-80-60-40-20020406080100-20k020k40k60k80k100k120k140k16(21.)FishingDerby-0.500.511.522.533.544.555.5020k40k60k80k100k120k140k16(22.)Freeway-20002004006008001e+31.2e+31.4e+31.6e+31.8e+32e+3-20k020k40k60k80k100k120k140k16(23.)Frostbite-2e+402e+44e+46e+48e+41e+51.2e+51.4e+51.6e+51.8e+52e+52.2e+52.4e+5-20k020k40k60k80k100k120k140k16(24.)Gopher-50005001e+31.5e+32e+32.5e+33e+33.5e+34e+34.5e+35e+35.5e+3-20k020k40k60k80k100k120k140k160k18(25.)Gravitar-5e+305e+31e+41.5e+42e+42.5e+43e+4-20k020k40k60k80k100k120k140k16(26.)Hero-16-14-12-10-8-6-4-20246-20k020k40k60k80k100k120k140k16(27.)IceHockey-2e+302e+34e+36e+38e+31e+41.2e+41.4e+41.6e+4-20k020k40k60k80k100k120k140k16(28.)Jamesbond-1e+301e+32e+33e+34e+35e+36e+37e+3-20k020k40k60k80k100k120k140k16(29.)Kangaroo-1e+401e+42e+43e+44e+45e+46e+47e+48e+49e+4-20k020k40k60k80k100k120k140k16(30.)Krull-4e+4-2e+402e+44e+46e+48e+41e+51.2e+51.4e+51.6e+5-40k-20k020k40k60k80k100k120k140k160(31.)KungFuMaster-50005001e+31.5e+32e+32.5e+33e+3-20k020k40k60k80k100k120k140k16(32.)MontezumaRe-venge-1e+301e+32e+33e+34e+35e+36e+37e+38e+39e+31e+4-20k020k40k60k80k100k120k140k16(33.)MsPacman-5e+305e+31e+41.5e+42e+42.5e+43e+43.5e+44e+4-20k020k40k60k80k100k120k140k16(34.)NameThisGame-5e+405e+41e+51.5e+52e+52.5e+53e+53.5e+54e+54.5e+55e+55.5e+56e+5-20k020k40k60k80k100k120k140k16(35.)Phoenix-220-200-180-160-140-120-100-80-60-40-2002040-20k020k40k60k80k100k120k140k16(36.)Pitfall-30-25-20-15-10-5051015202530-20k020k40k60k80k100k120k140k16(37.)Pong-50050100150200250300350-20k020k40k60k80k100k120k140k16(38.)PrivateEye-5e+305e+31e+41.5e+42e+42.5e+43e+43.5e+4-20k020k40k60k80k100k120k140k160k18(39.)Qbert-2e+302e+34e+36e+38e+31e+41.2e+41.4e+4-20k020k40k60k80k100k120k140k160k18(40.)Riverraid-5e+405e+41e+51.5e+52e+52.5e+53e+53.5e+5-20k020k40k60k80k100k120k140k16(41.)RoadRunner-202468101214-20k020k40k60k80k100k120k140k16(42.)Robotank-2e+302e+34e+36e+38e+31e+41.2e+41.4e+41.6e+4-20k020k40k60k80k100k120k140k16(43.)Seaquest-3.6e+4-3.4e+4-3.2e+4-3e+4-2.8e+4-2.6e+4-2.4e+4-2.2e+4-2e+4-1.8e+4-1.6e+4-1.4e+4-1.2e+4-1e+4-8e+3-6e+3-20k020k40k60k80k100k120k140k16(44.)Skiing1.9e+32e+32.1e+32.2e+32.3e+32.4e+32.5e+32.6e+32.7e+32.8e+32.9e+3-20k020k40k60k80k100k120k140k16(45.)Solaris-50005001e+31.5e+32e+32.5e+33e+33.5e+34e+3-20k020k40k60k80k100k120k140k16(46.)SpaceInvaders-5e+405e+41e+51.5e+52e+52.5e+53e+53.5e+54e+54.5e+5-20k020k40k60k80k100k120k140k16(47.)StarGunner-10-10-10-10-10-9.99-9.99-9.99-9.99-9.99-9.98-9.98-20k020k40k60k80k100k120k140k160k18(48.)Surround-25-24.5-24-23.5-23-22.5-22-21.5-21-20.5-20-5k05k10k15k20k25k30k35k40k45k50(49.)Tennis-1e+401e+42e+43e+44e+45e+46e+47e+48e+49e+41e+51.1e+5-20k020k40k60k80k100k120k140k16(50.)TimePilot-50050100150200250300350400450500-20k020k40k60k80k100k120k140k16(51.)Tutankham-1e+501e+52e+53e+54e+55e+56e+57e+58e+5-20k020k40k60k80k100k120k140k16(52.)UpNDown-1.2-1-0.8-0.6-0.4-0.200.20.40.60.811.2-20k020k40k60k80k100k120k140k16(53.)Venture-5e+405e+41e+51.5e+52e+52.5e+53e+53.5e+54e+5-20k020k40k60k80k100k120k140k16(54.)VideoPinball-5e+305e+31e+41.5e+42e+42.5e+43e+43.5e+4-20k020k40k60k80k100k120k140k16(55.)WizardOfWor-2e+402e+44e+46e+48e+41e+51.2e+51.4e+51.6e+5-20k020k40k60k80k100k120k140k160k18(56.)YarsRevenge-5e+305e+31e+41.5e+42e+42.5e+43e+43.5e+44e+44.5e+45e+45.5e+46e+4-20k020k40k60k80k100k120k140k16(57.)Zaxxon",
    "reference": "# Summary Of The Paper\n\nThe paper presents a method for reducing gradient interference between different components (Q, V, and policy) in deep RL with discrete actions. By carefully placing stop-gradient (sg) operators in the objective, the authors show (both theoretically and empirically) that the gradients between the Q function and the policy can be made parallel, so that there is no interference. The method, CASA, can be applied to value-based or policy-based algorithms using the same computations, and attains strong performance on Atari.\n\n# Strength And Weaknesses\n\nStrengths:\n* The paper addresses an issue that many ignore in deep RL – the relationship between the policy evaluation and policy improvement steps in policy iteration.\n* The clever usage of sg in the objective leads to the desired result without any approximation or extra computationally intensive steps.\n* CASA substantially improves the performance of the base algorithm (IMPALA) on the Atari benchmark.\n\nWeaknesses:\n* The method as presented only applies to discrete-action problems, owing to the softmax parameterization of the policy and the summation over actions when computing the advantage.\n* The paper shows plots for many different gradient angles, but it’s not clear which we should care the most about. It would be helpful to have a better understanding of how the angles between the gradients correlate with performance.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing of the paper is mostly clear, but could be improved in places.\n\nIn the Preliminary section, the objective $J$ is written incorrectly. If you’re taking the expectation over the discounted stationary distribution, the random variable inside the expectation should just be $r(s,a)$, not the discounted sum of rewards.\n\nThe CASA approach is novel, to my knowledge.\n\nThe appendix includes the relevant Python package versions and an exhaustive list of hyperparameter settings, so I believe it would be reproducible.\n\n# Summary Of The Review\n\nI think the paper is a useful contribution, as it tackles a relatively underexplored problem with a clever and effective solution.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "PROBABILITY FLOW SOLUTION OF THE FOKKERPLANCK EQUATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe method of choice for integrating the time-dependent Fokker-Planck equation in high-dimension is to generate samples from the solution via integration of the associated stochastic differential equation. Here, we introduce an alternative scheme based on integrating an ordinary differential equation that describes the flow of probability. Acting as a transport map, this equation deterministically pushes samples from the initial density onto samples from the solution at any later time. Unlike integration of the stochastic dynamics, the method has the advantage of giving direct access to quantities that are challenging to estimate from trajectories alone, such as the probability current, the density itself, and its entropy. The probability flow equation depends on the gradient of the logarithm of the solution (its “score”), and so is a-priori unknown. To resolve this dependence, we model the score with a deep neural network that is learned on-the-fly by propagating a set of samples according to the instantaneous probability current. We consider several high-dimensional examples from the physics of interacting particle systems to highlight the efficiency and precision of the approach; we find that the method accurately matches analytical solutions computed by hand and moments computed via Monte-Carlo.\n\n1\n\nINTRODUCTION\n\nThe time evolution of many dynamical processes occurring in the natural sciences, engineering, economics, and statistics are naturally described in the language of stochastic differential equations (SDE) (Gardiner, 2009; Oksendal, 2003; Evans, 2012). Typically, one is interested in the probability density function (PDF) of these processes, which describes the probability that the system will occupy a given state at a given time. The density can be obtained as the solution to a Fokker-Planck equation (FPE), which can generically be written as (Risken, 1996; Bass, 2011)\n\n∂tρ∗\n\n(bt(x)ρ∗\n\n−∇ ·\n\nt (x)\n\nt (x) =\n\nt (x) R≥0 denotes the value of the density at time t, bt(x)\n\n∈ Rd is a vector field known where ρ∗ Rd×d is a positive-semidefinite tensor known as the diffusion matrix. as the drift, and Dt(x) t=0(x) = ρ0(x), but in all but the (FPE) must be solved for t simplest cases, the solution is not available analytically and can only be approximated via numerical integration.\n\n0 from some initial condition ρ∗\n\nt (x)) ,\n\nDt(x)\n\n(FPE)\n\n∇\n\n−\n\n⊆\n\n≥\n\nΩ\n\n∈\n\n∈\n\n∈\n\nx\n\nρ∗\n\nRd,\n\nHigh-dimensionality. For many systems of interest – such as interacting particle systems in statistical physics (Chandler, 1987; Spohn, 2012), stochastic control systems (Kushner et al., 2001), and models in mathematical finance (Oksendal, 2003) – the dimensionality d can be very large. This renders standard numerical methods for partial differential equations inapplicable, which become infeasible for d as small as five or six due to an exponential scaling of the computational complexity with d. The standard solution to this problem is a Monte-Carlo approach, whereby the SDE associated with (FPE)\n\ndxt = bt(xt)dt +\n\nDt(xt)dt + √2σt(xt)dWt,\n\n(1)\n\nis evolved via numerical integration to obtain a large number n of trajectories (Kloeden & Platen, t (x) = Dt(x) and Wt is a standard Brownian motion on Rd. 1992). In (1), σt(x) satisfies σt(x)σT i=1 from the initial PDF ρ0, simulation of (1) enables the Assuming that we can draw samples\n\nxi\n\nn\n\n∇ ·\n\n{\n\n0}\n\n1\n\n(3)\n\n(4)\n\nτ,t(\n\n) ·\n\nestimation of expectations via empirical averages\n\n(cid:90)\n\nΩ\n\nφ(x)ρ∗\n\nt (x)dx\n\n1 n\n\nn (cid:88)\n\ni=1\n\n≈\n\nφ(xi\n\nt),\n\n(2)\n\nwhere φ : Ω from ρ∗ differential entropy of the system Ht = methods that typically do not scale well to high-dimension.\n\nR is an observable of interest. While widely used, this method only provides samples t itself or the time-dependent t (x)dx require sophisticated interpolation\n\nt , and hence other quantities of interest like the value of ρ∗\n\n(cid:82) Ω log ρ∗\n\nt (x)ρ∗\n\n→\n\n−\n\nA transport map approach. Another possibility, building on recent theoretical advances that connect transportation of measures to the Fokker-Planck equation (Jordan et al., 1998), is to recast (FPE) as the transport equation (Villani, 2009; Santambrogio, 2015)\n\nwhere we have defined the velocity field\n\n∂tρ∗\n\nt (x) =\n\n−∇ ·\n\n(v∗\n\nt (x)ρ∗\n\nt (x))\n\nThis formulation reveals that ρ∗ of the ordinary differential equation\n\nt (x) = bt(x)\n\nv∗ t can be viewed as the pushforward of ρ0 under the flow map X ∗\n\nlog ρ∗\n\nDt(x)\n\nt (x).\n\n∇\n\n−\n\nd dt\n\nX ∗\n\nτ,t(x) = v∗\n\nt (X ∗\n\nτ,t(x)),\n\nX ∗\n\nτ,τ (x) = x,\n\nt, τ\n\n0.\n\n≥\n\n(5)\n\nEquation (5) is known as the probability flow equation, and its solution has the remarkable property that if x is a sample from ρ0, then X ∗ Ω as t = X ∗ a transport map, ρ∗ 0,t♯ρ0 can be evaluated at any position in Ω via the change of variables formula (Villani, 2009; Santambrogio, 2015)\n\n0,t(x) will be a sample from ρ∗\n\nt . Viewing X ∗\n\nτ,t : Ω\n\n→\n\nt (x) = ρ0(X ∗ ρ∗\n\nt,0(x)) exp\n\n(cid:18)\n\n(cid:90) t\n\n−\n\n0 ∇ ·\n\n(cid:19)\n\nτ (X ∗ v∗\n\nt,τ (x))dτ\n\n(6)\n\nwhere X ∗ t,0(x) is obtained by solving (5) backward from some given x. Importantly, access to the PDF as provided by (6) immediately gives the ability to compute quantities such as the probability current or the entropy; by contrast, this capability is absent when directly simulating the SDE.\n\nt depends explicitly on the solution ρ∗\n\nLearning the flow. The simplicity of the probability flow equation (5) is somewhat deceptive, because the velocity v∗ t to the Fokker-Planck equation (FPE). Nevertheless, recent work in generative modeling via score-based diffusion (Song & Ermon, 2020a;b; Song & Kingma, 2021) has shown that it is possible to use deep neural networks to estimate v∗ t , or equivalently the so-called score t of the solution density. Here, we introduce a variant of score-based diffusion modeling in which the score is learned on-the-fly over samples generated by the probability flow equation itself. The method is self-contained and enables us to bypass simulation of the SDE entirely; moreover, we provide both empirical and theoretical evidence that the resulting self-consistent training procedure offers improved performance when compared to training via samples produced from simulation of the SDE.\n\nlog ρ∗\n\n∇\n\n1.1 CONTRIBUTIONS\n\nOur contributions are both theoretical and computational:\n\n• We provide a bound on the Kullback-Leibler divergence from the estimate ρt produced via an approximate velocity field vt to the target ρ∗ t . This bound motivates our approach, and shows that minimizing the discrepancy between the learned score and the score of the push-forward distribution systematically improves the accuracy of ρt.\n\n• Based on this bound, we introduce two optimization problems that can be used to learn the velocity field (4) in the transport equation (3) so that its solution coincides with that of the Fokker Planck equation (FPE). Due to its similarities with score-based diffusion approaches in generative modeling (SBDM), we call the resulting method score-based transport modeling (SBTM).\n\n• We provide specific estimators for quantities that can be computed via SBTM but are not directly available from samples alone, like point-wise evaluation of ρt itself, the differential entropy, and the probability current.\n\n2\n\n• We test SBTM on several examples involving interacting particles that pairwise repel but are kept close by common attraction to a moving trap. In these systems, the FPE is high-dimensional due to the large number of particles, which vary from 5 to 50 in the examples below. Problems of this type frequently appear in the molecular dynamics of externally driven soft matter systems (Frenkel & Smit, 2001; Spohn, 2012). We show that our method can be used to accurately compute the entropy production rate of a system, a quantity of interest in the active matter community (Nardini et al., 2017), as it quantifies the out-of-equilibrium nature of the system’s dynamics.\n\n1.2 NOTATION AND ASSUMPTIONS.\n\n≥\n\n→\n\n0. We assume that the drift vector bt : Ω\n\nRd in which it Throughout, we assume that the stochastic process (1) evolves over a domain Ω Rd and the diffusion tensor remains at all times t Rd×d are twice-differentiable and bounded in both x and t, so that the solution to the Dt : Ω t (x) is assumed to SDE (1) is well-defined at all times t be positive semi-definite for each (t, x), with Cholesky decomposition Dt(x) = σt(x)σT t (x). We further assume that the initial PDF ρ0 is three-times differentiable, positive everywhere on Ω, and such that H0 = t enjoys the same properties at ∞\nall times t > 0. Finally, we assume that log ρ∗\n\n. This guarantees that ρ∗ t is K-smooth globally for (t, x)\n\n0. The symmetric tensor Dt(x) = DT\n\n(cid:82) Ω log ρ0(x)ρ0(x)dx <\n\nΩ, i.e.\n\n[0,\n\n→\n\n≥\n\n⊆\n\n−\n\n)\n\nK > 0 :\n\n(t, x)\n\n[0,\n\n)\n\nΩ\n\nlog ρ∗\n\nt (x)\n\nlog ρ∗\n\nt (y)\n\n∀\n\n∃\n\n|∇ This technical assumption is needed to guarantee global existence and uniqueness of the solution of the probability flow equation. Throughout, we use the shorthand notation ̇yt = d dt yt interchangeably for a time-dependent quantity yt.\n\n− ∇\n\n| ≤\n\n∞\n\n−\n\n×\n\n∈\n\n(7)\n\n∈ K\n\n∞\n\n× .\n|\n\ny\n\nx |\n\n2 RELATED WORK\n\nScore matching Our approach builds directly on the toolbox of score matching originally developed by Hyvärinen (Hyvärinen, 2005; Hyvarinen, 2007; Hyvärinen, 2007; 2008) and more recently extended in the context of diffusion-based generative modeling (Song & Ermon, 2020a;b; Song et al., 2021; De Bortoli et al., 2021; Dockhorn et al., 2022; Mittal et al., 2021). These approaches assume access to training samples from the target distribution (e.g., in the form of examples of natural images). Here, we bypass this need and use the probability flow equation to obtain the samples needed to learn an approximation of the score. Lu et al. (2022) recently showed that using the transport equation (TE) with a velocity field learned via SBDM can lead to inaccuracies in the likelihood unless higher-order score terms are well-approximated. Proposition 1 shows that the self-consistent approach used in SBTM solves these issues and ensures a systematic approximation of the target ρ∗ t .\n\nDensity estimation and Bayesian inference Our method shares commonalities with transport mapbased approaches (Marzouk et al., 2016) for density estimation and variational inference (Zhang et al., 2019; Blei et al., 2017) such as normalizing flows (Tabak & Vanden-Eijnden, 2010; Tabak & Turner, 2013; Rezende & Mohamed, 2016; Huang et al., 2021; Papamakarios et al., 2021; Kobyzev et al., 2021). Moreover, because expectations are approximated over a set of samples according to (2), the method also inherits elements of classical “particle-based” approaches for density estimation such as Markov chain Monte Carlo (Robert & Casella, 2004) and sequential Monte Carlo (Dai et al., 2020; Del Moral et al., 2006).\n\nOur approach is also reminiscent of a recent line of work in Bayesian inference that aims to combine the strengths of particle methods with those of variational approximations (Dai et al., 2016; Saeedi et al., 2017). In particular, the method we propose bears some similarity with Stein variational gradient descent (SVGD) (Liu, 2017; Liu & Wang, 2018; 2019) (see also (Lu et al., 2018; Li et al., 2020)), in that both methods approximate the target distribution via deterministic propagation of a set of samples. The key differences are that (i) our method learns the map used to propagate the samples, while the map in SVGD corresponds to optimization of the kernelized Stein discrepancy, and (ii) the methods have distinct goals, as we are interested in capturing the dynamical evolution of ρ∗ t rather than sampling at equilibrium.\n\nApproaches for solving the FPE Most closely connected to our paper are the works by Maoutsa et al. (2020) and Shen et al. (2022), who similarly propose to bypass the SDE through use of the probability flow equation, building on earlier work by Degond & Mustieles (1990) and Russo (1990). The critical\n\n3\n\ndifferences between Maoutsa et al. (2020) and our approach are that they perform estimation over a linear space or a reproducing kernel Hilbert space rather than over the significantly richer class of neural networks, and that they train using the original score matching loss of Hyvärinen (2005), while the use of neural networks requires the introduction of regularized variants. Because of this, Maoutsa et al. (2020) studies systems of dimension less than or equal to five; in contrast, we study systems with dimensionality as high as 100.\n\nConcurrently to our work, Shen et al. (2022) proposed a variational problem similar to SBTM. A key difference is that SBTM is not limited to Fokker-Planck equations that can be viewed as a gradient flow in the Wasserstein metric over some energy (i.e., the drift term in the SDE (1) need not be the gradient of a potential), and that it allows for spatially-dependent and rank-deficient diffusion matrices; moreover, our theoretical results are similar but avoid the use of costly Sobolev norms.\n\nNeural-network solutions to PDEs Our approach can also be viewed as an alternative to recent neural network-based methods for the solution of partial differential equations (see e.g. E & Yu (2017); Raissi et al. (2019); Han et al. (2018); Sirignano & Spiliopoulos (2018); Bruna et al. (2022)). Unlike these existing approaches, our method is tailored to the solution of the Fokker-Planck equation and guarantees that the solution is a valid probability density. Our approach is fundamentally Lagrangian in nature, which has the advantage that it only involves learning quantities locally at the positions of a set of evolving samples; this is naturally conducive to efficient scaling for high-dimensional systems.\n\n3 METHODOLOGY\n\n3.1 SCORE-BASED TRANSPORT MODELING\n\nLet st : Ω →\nsolution ρt : Ω\n\nRd denote an approximation to the score of the target\n\nR≥0 to the transport equation\n\nlog ρ∗\n\nt , and consider the\n\n∇\n\n→\n\n∂tρt(x) =\n\n−∇ ·\n\n(vt(x)ρt(x))\n\nwith vt(x) = bt(x)\n\nDt(x)st(x).\n\n−\n\nOur goal is to develop a variational principle that may be used to adjust st so that ρt tracks ρ∗ approach is based on the following inequality, whose proof may be found in Appendix B.1: Proposition 1 (Control of the KL divergence). Assume that the conditions listed in Sec. 1.2 hold. Let ρt denote the solution to the transport equation (TE), and let ρ∗ t denote the solution to the Fokker-Planck equation (FPE). Assume that ρt=0(x) = ρ∗\n\nt=0(x) = ρ0(x) for all x\n\nΩ. Then\n\n(TE) t . Our\n\n(cid:90)\n\nρ∗ t )\n\n|\n\n≤\n\n1 2\n\nst(x)\n\nlog ρt(x) |\n\n− ∇\n\n2\n\nDt(x) ρt(x)dx,\n\nΩ |\n\n∈\n\nDKL(ρt\n\nd dt , Dt(x)\n\n. ·⟩\n\nwhere\n\n2\n\nDt(x) =\n\n| · |\n\n⟨· In particular, (8) implies that for any T\n\n[0,\n\n) we have explicit control on the KL divergence\n\nDKL(ρT\n\nρ∗\n\nT )\n\n|\n\n≤\n\n1 2\n\n∈ (cid:90)\n\n(cid:90) T\n\n0\n\nΩ |\n\n∞ st(x)\n\nlog ρt(x) |\n\n− ∇\n\n2\n\nDt(x) ρt(x)dxdt.\n\nRemarkably, (9) only depends on the approximate ρt and does not include ρ∗ accuracy of ρt as an approximation of ρ∗\n\nt : it states that the t can be improved by enforcing agreement between st and t , which\n\nlog ρt. This means that we can optimize (9) without making use of external data from ρ∗\n\n∇ offers a self-consistent objective function to learn the score st using (TE) alone.\n\nThe primary difficulty with this approach is that ρt must be considered as a functional of st, since the velocity vt used in (TE) depends on st. To render the resulting minimization of the right-hand side of (9) practical, we can exploit that (TE) can be solved via the method of characteristics, as ̇Xt(x) = vt(Xt(x)) is the probability flow equation summarized in Appendix A. Specifically, if associated with the velocity vt, then ρt = Xt♯ρ0. This means that the expectation of any function φ(x) over ρt(x) can be expressed as the expectation of φt(Xt(x)) over ρ0(x). Observing that the log ρt(Xt(x)) solves a closed score of the solution to (TE) along trajectories of the probability flow equation leads to the following proposition. Proposition 2 (Score-based transport modeling). Assume that the conditions listed in Sec. 1.2 hold. Define vt(x) = bt(x)\n\nDt(x)st(x) and consider\n\n∇\n\n(8)\n\n(9)\n\n−\n\n ̇Xt(x) = vt(Xt(x)), ̇Gt(x) =\n\n[ ∇\n\n−\n\nvt(Xt(x))]TGt(x)\n\nX0(x) = x,\n\nvt(Xt(x)),\n\nG0(x) =\n\nlog ρ0(x).\n\n∇\n\n(10)\n\n− ∇∇ ·\n\n4\n\nThen ρt = Xt♯ρ0 solves (TE), the equality Gt(x) =\n\nlog ρt(Xt(x)) holds, and for any T\n\n∇\n\nDKL(XT ♯ρ0\n\nρ∗\n\nT )\n\n|\n\n≤\n\n1 2\n\n(cid:90) T\n\n(cid:90)\n\n0\n\nΩ |\n\nst(Xt(x))\n\nGt(x)\n\n2 Dt(Xt(x)) ρ0(x)dxdt. |\n\n−\n\n[0,\n\n)\n\n∞\n\n∈\n\n(11)\n\nMoreover, if s∗\n\nt is a minimizer of the constrained optimization problem\n\n(cid:90) T\n\n(cid:90)\n\nmin s\n\n0\n\nΩ |\n\nst(Xt(x))\n\nGt(x)\n\n2 Dt(Xt(x)) ρ0(x)dxdt |\n\n−\n\nsubject to (10)\n\n(SBTM)\n\nthen Dt(x)s∗ X ∗\n\nt (x) = Dt(x)\n\nlog ρ∗\n\nt (x) where ρ∗\n\nt solves the Fokker-Planck equation (FPE). The map\n\nt associated to any minimizer is a transport map from ρ0 to ρ∗\n\nt , i.e.\n\n∇\n\nρ0\n\nx\n\n∼\n\nimplies that\n\nX ∗\n\nt (x)\n\nρ∗ t ,\n\n∼\n\nt ∀\n\n∈\n\n[0, T ].\n\n(12)\n\nProposition 2 is proven in Appendix B.3. The result also holds with a standard Euclidean norm replacing the diffusion-weighted norm, in which case the minimizer is unique and is given by s∗ t (x). In the special case when the SDE is an Ornstein-Uhlenbeck process, the score t (x) = and the equations for both Xt and Gt can be written explicitly; they are studied in Appendix C.\n\nlog ρ∗\n\n∇\n\nIn practice, the objective in (SBTM) can be estimated empirically by generating samples from ρ0 and solving the equations for Xt(x) and Gt(x) with x ρ0. The constrained minimization problem (SBTM) can then in principle be solved with gradient-based techniques via the adjoint method. The corresponding equations are written in Appendix B.3, but they involve fourth-order spatial derivatives that are computationally expensive to compute via automatic differentiation. Moreover, each gradient step requires solving a system of ordinary differential equations whose dimensionality is equal to the number of samples used to compute expectations times the dimension of (FPE). Instead, we now develop a sequential procedure that avoids these difficulties entirely.\n\n∼\n\n3.2 SEQUENTIAL SCORE-BASED TRANSPORT MODELING\n\nAn alternative to the constrained minimization in Proposition 2 is to consider an approach whereby the score st is obtained independently at each time to ensure that DKL(ρt t ) remains small. This |\nsuggests choosing st to minimize d ρ∗ t ), which admits a simple closed-form bound, as shown in Proposition 1. While this explicit form can be used directly, an application of Stein’s identity recovers an implicit objective analogous to Hyvärinen score-matching that is equivalent to minimizing d t ) but obviates the calculation of Gt. Expanding the square in (8) and applying (cid:82)\n\nst(x) ρt(x)dx, we may write\n\nlog ρt(x) ρt(x)dx =\n\ndt DKL(ρt\n\ndt DKL(ρt\n\nρ∗\n\nρ∗\n\n(cid:82)\n\n|\n\n|\n\nΩ st(x)T\n\n−\n\nΩ ∇ ·\n\n(cid:0)\n\nst(Xt(x)) |\n\n2 Dt(Xt(x)) + 2 |\n\n∇ ·\n\n(Dt(Xt(x))st(Xt(x))) +\n\nGt(x) |\n\n2(cid:1)ρ0(x)dx. |\n\nd dt\n\nDKL(ρt\n\nρ∗ t )\n\n|\n\n≤\n\n(cid:90)\n\n∇ 1\n2\n\nΩ\n\nlog ρt(Xt(x)) = Gt(x) is independent of st, we may neglect the corresponding square Because ∇\nterm during the optimization. This leads to a simple and comparatively less expensive way to build the pushforward X ∗ t sequentially in time, as stated in the following proposition.\n\nt such that X ∗\n\nt ♯ρ0 = ρ∗\n\nProposition 3 (Sequential SBTM). In the same setting as Proposition 2, let Xt(x) solve the first equation in (10) with vt(x) = bt(x)\n\nDt(x)st(x). Let st be obtained via\n\n(cid:90)\n\n(cid:16)\n\nΩ\n\nmin st\n\nst(Xt(x)) |\n\n2 Dt(Xt(x)) + 2 |\n\n∇ ·\n\n−\n\n(Dt(Xt(x))st(Xt(x)))\n\n(cid:17)\n\nρ0(x)dx.\n\n(SSBTM)\n\nThen, each minimizer s∗ solution to (FPE). Moreover, the map X ∗\n\nt of (SSBTM) satisfies Dt(x)s∗ t associated to s∗\n\nt (x) where ρ∗ t (x) = Dt(x) t is a transport map from ρ0 to ρ∗ t .\n\nlog ρ∗\n\n∇\n\nt is the\n\nProposition 3 is proven in Appendix B.4. Critically, (SSBTM) is no longer a constrained optimization problem. Given the current value of Xt at any time t, we can obtain st via direct minimization of the objective in (SSBTM). Given st, we may compute the right-hand side of (10) and propagate Xt (and possibly Gt) forward in time. The resulting procedure, which alternates between self-consistent score estimation and sample propagation, is presented in Algorithm 1. The output of the method produces a feasible solution for (SBTM) with an a-posteriori bound on the loss obtained via integration.\n\n5\n\nAlgorithm 1 Sequential score-based transport modeling. R≥0. A set of n samples\n\n1: Input: An initial time t0\n\nn\n\nxi\n\n{\n\n}\n\ni=1 from ρt0. A set of NT timesteps\n\n{\n\nNT −1 k=0\n\n∆tk\n\n. 2: Initialize sample locations X i 3: for k = 0, . . . , Nt\n\n1 do\n\n}\n\n∈\n\n4:\n\n5:\n\nOptimize: stk = arg mins Propagate samples:\n\n−\n\nX i\n\ntk+1 = X i Set tk+1 = tk + ∆tk.\n\ntk + ∆tk\n\n6: 7: Output: A set of n samples\n\nt0 = xi for i = 1, . . . , n.\n\n1 n\n\n(cid:80)n\n\ni=1\n\n(cid:104)\n\n|\n\ns(X i\n\n2 tk ) Dtk (X i |\ntk\n\n) + 2\n\n∇ ·\n\n(cid:0)Dtk (X i\n\ntk )s(X i\n\ntk )(cid:1)(cid:105)\n\n.\n\n(cid:0)btk (X i\n\ntk )\n\nDtk (X i\n\ntk )stk (X i\n\ntk )(cid:1) .\n\n−\n\nX i\n\ntk }\n\n{\n\nn\n\ni=1 from ρtk and the score\n\nstk (X i\n\ntk )\n\nn\n\ni=1 for all\n\n}\n\n{\n\ntk\n\n{\n\nNT k=0.\n\n}\n\n(Dt(Xt(x))st(Xt(x))), which Practical considerations To avoid computation of the divergence is often costly for neural networks, we can use the denoising score matching loss function introduced by Vincent (2011), which we discuss in Appendix B.6. Empirically, we find that the use of either the denoising objective or explicit derivative regularization is necessary for stable training.\n\n∇ ·\n\nlog ρ∗\n\nt via minimization of the loss (cid:82) T\n\nWhy not use the SDE? An alternative to the sequential procedure outlined here would be to generate samples from the target ρ∗ t via simulation of the associated SDE, and to approximate the score t (x)dxdt, similar to SBDM. ∇\nρt) are controlled when using this As shown in Appendix B.5 neither DKL(ρt procedure, where ρt = Xt♯ρ0 is the density of the probability flow equation. Empirically, we find in the numerical experiments that this approach is significantly less numerically stable than sequential SBTM. In particular, we could not estimate the entropy using the score learned from the SDE.\n\n2 + 2 st(x) Ω( |\n∇ · |\nt ) nor DKL(ρ∗ ρ∗\n\nst(x))ρ∗\n\nt |\n\n(cid:82)\n\n0\n\n|\n\n∈\n\nSBTM vs. Sequential SBTM Given the simplicity of the optimization problem (SSBTM), one may wonder if (SBTM) is useful in practice, or if it is simply a stepping stone to arrive at (SSBTM). The primary difference is that (SBTM) offers global control on the discrepancy between st and log ρt over t [0, T ] that unavoidably arises in practice due to learning and time-discretization errors. By contrast, because (SSBTM) proceeds sequentially, these errors could accumulate over time in a way that is harder to control. In the numerical examples below, we took the timestep ∆t sufficiently small, and the number of samples n sufficiently large, that we did not observe any accumulation of error. Nevertheless, (SBTM) may allow for more accurate approximation, because the loss is exactly minimized at zero and high-order derivatives of st must be controlled through the calculation of ̇Gt.\n\n∇\n\n4 NUMERICAL EXPERIMENTS\n\nwith each x(i)\n\nIn the following, we study two high dimensional examples from the physics of interacting particle systems, where the spatial variable of the Fokker-Planck equation (FPE) can be written as x = (cid:0)x(1), x(2), . . . , x(N )(cid:1)T R ̄d. Here, ̄d describes a lower-dimensional ambient space, e.g. ̄d = 2, so that the dimensionality of the Fokker-Planck equation d = N ̄d will be high if the number of particles N is even moderate. The still figures shown below do not do full justice to the complexity of the particle dynamics, and we encourage the reader to view the movies available here. With a timestep ∆t = 10−3, a horizon T = 10, and a fixed nN ̄d = 105, we find that the sequential SBTM procedure takes around two hours for each simulation on a single NVIDIA RTX8000 GPU.\n\n∈\n\n4.1 HARMONICALLY INTERACTING PARTICLES IN A HARMONIC TRAP\n\nSetup. Here we study a problem that admits a tractable analytical solution for direct comparison. We consider N two-dimensional particles ( ̄d = 2) that repel according to a harmonic interaction but R2. The motion of the experience harmonic abut experience ttraction towards a moving trap βt particles is governed by the stochastic dynamics\n\n∈\n\ndX (i)\n\nt = (βt\n\nX (i)\n\nt )dt + α\n\n−\n\n(cid:16)\n\nX (i)\n\nt −\n\n1 N\n\n(cid:17)\n\nX (j)\n\nt\n\nN (cid:88)\n\nj=1\n\n6\n\ndt + √2D dWt,\n\ni = 1, . . . , N\n\n(13)\n\nA\n\nB\n\nFigure 1: A system of N = 50 particles in a harmonic trap with a harmonic interaction: (A) A single sample trajectory. The mean of the trap βt is shown with a red star, while past positions of the particles are indicated by a fading trajectory. The noise-free system (right) is too concentrated, and fails to capture the variance of the stochastic dynamics (center). The learned system (left) accurately captures the variance, and in addition generates physically interpretable trajectories for the particles. (B) Quantitative comparison to the analytical solution. The learned solution matches the entropy production rate, score, and covariance well. Movie can be found here.\n\n∈\n\n(0, 1) is a fixed coefficient that sets the magnitude of the repulsion. The dynamics (13) where α R ̄dN with block components x(i). is an Ornstein-Uhlenbeck process in the extended variable x Assuming a Gaussian initial condition, the solution to the Fokker-Planck equation associated with (13) is a Gaussian for all time and hence can be characterized entirely by its mean mt and covariance Ct. These can be obtained analytically (Appendices C and D), which facilitates a quantitative comparison to the learned model. The differential entropy St is given by (see Appendix D)\n\n∈\n\nHt = 1\n\n2\n\n ̄dN (log (2π) + 1) + 1\n\n2 log det Ct\n\n(14)\n\nIn the experiments, we take βt = a(cos πωt, sin πωt)T with a = 2, ω = 1, D = 0.25, α = 0.5, and N = 50, giving rise to a 100-dimensional Fokker-Planck equation. The particles are initialized from an isotropic Gaussian with mean β0 (the initial trap position) and variance σ2\n\n0 = 0.25.\n\nNetwork architecture. We take st(x) = one- and two-particle terms\n\nUθt(x), where the potential Uθt( ·\n\n−∇\n\n) is given as a sum of\n\n(cid:0)x(1), . . . , x(N )(cid:1) =\n\nUθt\n\nN (cid:88)\n\ni=1\n\nUθt,1\n\n(cid:0)x(i)(cid:1) +\n\n1 N\n\nN (cid:88)\n\ni,j=1 i̸=j\n\nUθt,2\n\n(cid:0)x(i), x(j)(cid:1).\n\n(15)\n\n∈\n\nTo ensure permutation symmetry amongst the particles, we require that Uθt,2 (x, y) = Uθt,2 (y, x) for R ̄d. Modeling at the level of the potential introduces an additional gradient into the loss each x, y function, but makes it simple to enforce permutation symmetry; moreover, by writing the potential as a sum of one- and two-particle terms, the dimensionality of the function estimation problem is reduced. As motivation for this choice of architecture, we show in Appendix D.1 that the class of scores representable by (15) contains the analytical score for the harmonic problem considered in this section. To obtain the parameters θtk+∆tk , we perform a warm start and initialize from θtk , which\n\n7\n\n02x−2−1012ylearned02xSDE02xnoisefree0246time−50050100entropyproductionratelearnedanalyticalnoisefree0246time10−310−2ideallosstrainingdataSDEdata0246time02040Tr(Σ)SDElearnednoisefreeanalyticalreduces the number of optimization steps that need to be performed at each iteration. All networks are taken to be multi-layer perceptrons with the swish activation function (Ramachandran et al., 2017); further details on the architectures used can be found in Appendix D.\n\nQuantitative comparison. For a quantitative comparison between the learned model and the exact solution, we study the empirical covariance Σ over the samples and the entropy production rate dSt dt . Because an analytical solution is available for this system, we may also compute the target\n\nlog ρt(x) =\n\n∇\n\n−\n\nC −1\n\nt\n\n(x\n\n−\n\nmt) and measure the goodness of fit via the relative Fisher divergence\n\n(cid:82)\n\nΩ |\n\nst(x) (cid:82)\n\nΩ |∇\n\n− ∇ log ρt(x)\n\n2 ̄ρ(x)dx log ρt(x) |\n2 ̄ρ(x)dx |\n\n.\n\n(16)\n\nIn Equation (16), ̄ρ can be taken to be equal to the current particle estimate of ρt (the training data), or estimated using samples from the stochastic differential equation (the SDE data).\n\nResults. The representation of the dynamics (13) in terms of the flow of probability leads to an intuitive deterministic motion that accurately captures the statistics of the underlying stochastic process. Snapshots of particle trajectories from the learned probability flow (5), the SDE (13), and the noise-free equation obtained by setting D = 0 in (13) are shown in Figure 1A.\n\nResults for this quantitative comparison are shown in Figure 1B. The learned model accurately predicts the entropy production rate of the system and minimizes the relative metric (16) to the order of 10−2. The noise-free system incorrectly predicts a constant and negative entropy production rate, while the SDE cannot make a prediction for the entropy production rate. In addition, the learned model accurately predicts the high-dimensional covariance Σ of the system (curves lie directly on top of the analytical result, trace shown for simplicity). The SDE also captures the covariance, but exhibits more fluctuations in the estimate; the noise-free system incorrectly estimates all covariance components as converging to zero.\n\n4.2 SOFT SPHERES IN AN ANHARMONIC TRAP\n\nSetup. Here, we consider a system of N = 5 particles in an anharmonic trap in dimension ̄d = 2 that exhibit soft-sphere repulsion. This system gives rise to a 10-dimensional (FPE), a dimensionality that is significantly too high for standard PDE solvers. The stochastic dynamics is given by\n\ndX (i)\n\nt = 4B(cid:0)βt\n\n+\n\nA N r2\n\n− N\n(cid:88)\n\nj=1\n\n(cid:1)\n\nX (i)\n\nt\n\nX (i) |\n\nt −\n\n(cid:0)X (i)\n\nt −\n\nX (j)\n\nt\n\n2dt (cid:32)\n\nβt\n\n| (cid:1) exp\n\nX (i) |\n\nt −\n\nX (j)\n\nt\n\n2r2\n\n(cid:33)\n\n2 |\n\ndt + √2D dWt,\n\ni = 1, . . . , N,\n\n−\n\nwhere βt again represents a moving trap, A > 0 sets the strength of the repulsion between the spheres, r sets their size, and B > 0 sets the strength of the trap. We set β(t) = a(cos πωt, sin πωt)T or β(t) = a(cos πωt, 0)T with a = 2, ω = 1, D = 0.25, A = 10, and r = 0.5. We fix B = D/R2 with R = √γN r and γ = 5.0. This ensures that the trap scales with the number of particles and that they have sufficient room in the trap to generate a complex dynamics. The circular case converges to a distribution ρ∗ Qt that can be described as a fixed distribution ρ∗ composed with a time-dependent rotation Qt, and hence the entropy production rate should converge to zero. The linear case does not exhibit such convergence, and the entropy production rate should oscillate around zero as the particles are repeatedly pushed and pulled by the trap. We make use of the same network architecture as in Sec. 4.1.\n\nt = ρ∗\n\n◦\n\nResults. Similar to Section 4.1, an example trajectory from the learned system, the SDE (17) and the noise-free system obtained by setting D = 0 in (17) are shown in Figure 2A in the circular case. The learned particle trajectories exhibit an intuitive circular motion with increased disorder relative to the noise-free system that accurately captures the statistics of the stochastic dynamics. Numerical estimates of a single component of the covariance and of the entropy production rate are shown in Figure 2B/C, with all moments shown in Appendix D.2. The learned and SDE systems accurately capture the covariance, while the noise-free system underestimates the covariance in both the linear and the circular case. The prediction of the entropy production rate from Algorithm 1 is reasonable in both cases, exhibiting convergence to zero and oscillation around zero as expected. In the inset, we show the prediction of the entropy production rate when learning on samples from the SDE; the\n\n8\n\nA\n\nB\n\nD\n\nC\n\nE\n\nFigure 2: A system of N = 5 soft-spheres in an anharmonic trap: (A) Example particle trajectories in the case of a rotating trap. Trap position shown with a red star. (B/C) A single component of the covariance of the samples, in the case of a rotating trap (B) and a linearly oscillating trap (C). The system learned system agrees well with the SDE, while the noise-free systems under-predicts the moments. (D/E) Prediction of the entropy production rate for a rotating trap (B) and linearly oscillating trap (C). Main figure depicts prediction from SBTM, while the inset depicts the prediction when learning on SDE samples. SBTM captures the temporal evolution of the entropy production rate, while learning on the SDE is initially offset and later divergent. Movies of the circular and linear motion can be viewed here and here, respectively.\n\nprediction is initially offset, and later becomes divergent. We found that this behavior was generic when training on the SDE, but never observed it when training on self-consistent samples\n\n5 OUTLOOK AND CONCLUSIONS\n\nBuilding on the toolbox of score-based diffusion recently developed for generative modeling, we introduced a related approach – score-based transport modeling (SBTM) – that gives an alternative to simulating the corresponding SDE to solve the Fokker-Planck equation. While SBTM is more costly than integration of the SDE because it involves a learning component, it gives access to quantities that are not directly accessible from the samples given by integrating the SDE, such as pointwise evaluation of the PDF, the probability current, or the entropy. Our numerical examples indicate that SBTM is scalable to systems in high dimension where standard numerical techniques for partial differential equations are inapplicable. The method can be viewed as a deterministic Lagrangian integration method for the Fokker-Planck equation, and our results show that its trajectories are more easily interpretable than the corresponding trajectories of the SDE.\n\n9\n\n−202x−202ySDE−202xlearned−202xnoisefree0.40.6Σ11SDElearnednoisefree0.02.55.07.510.0time02040dHtdt0246810−250250.20.40.6Σ22SDElearnednoisefree0.02.55.07.510.0time02040dHtdt0246810−25025REFERENCES\n\nRichard F Bass. Stochastic processes, volume 33. Cambridge University Press, 2011.\n\nDavid M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statisticians. Journal of the American Statistical Association, 112(518):859–877, April 2017. ISSN 0162-1459, 1537-274X. doi: 10.1080/01621459.2017.1285773.\n\nJoan Bruna, Benjamin Peherstorfer, and Eric Vanden-Eijnden. Neural galerkin scheme with active\n\nlearning for high-dimensional evolution equations. arXiv:2203.01360, 2022.\n\nDavid Chandler. Introduction to modern statistical. Mechanics. Oxford University Press, Oxford, UK,\n\n5, 1987.\n\nBo Dai, Niao He, Hanjun Dai, and Le Song. Provable Bayesian Inference via Particle Mirror Descent.\n\narXiv:1506.03101, May 2016.\n\nChenguang Dai, Jeremy Heng, Pierre E. Jacob, and Nick Whiteley. An invitation to sequential Monte\n\nCarlo samplers. arXiv:2007.11936, 2020.\n\nValentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrödinger\n\nBridge with Applications to Score-Based Generative Modeling. arXiv:2106.01357, 2021.\n\nPierre Degond and Francisco-José Mustieles. A deterministic approximation of diffusion equations using particles. SIAM Journal on Scientific and Statistical Computing, 11(2):293–310, 1990. doi: 10.1137/0911018.\n\nPierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential Monte Carlo samplers. Journal of the\n\nRoyal Statistical Society: Series B (Statistical Methodology), 68(3):411–436, 2006.\n\nTim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-Based Generative Modeling with Critically-\n\nDamped Langevin Diffusion. arXiv:2112.07068, 2022.\n\nWeinan E and Bing Yu. The deep ritz method: A deep learning-based numerical algorithm for solving\n\nvariational problems, 2017. URL https://arxiv.org/abs/1710.00211.\n\nLawrence C Evans. An introduction to stochastic differential equations, volume 82. American\n\nMathematical Soc., 2012.\n\nDaan Frenkel and Berend Smit. Understanding molecular simulation: from algorithms to applications,\n\nvolume 1. Elsevier, 2001.\n\nCrispin Gardiner. Stochastic Methods. Springer-Verlag Berlin Heidelberg, 4th edition, 2009. ISBN\n\n978-3-540-70712-7.\n\nJiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505– 8510, 2018. doi: 10.1073/pnas.1718942115. URL https://www.pnas.org/doi/abs/10. 1073/pnas.1718942115.\n\nChin-Wei Huang, Ricky T. Q. Chen, Christos Tsirigotis, and Aaron Courville. Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization. arXiv:2012.05942, February 2021.\n\nAapo Hyvarinen. Connections Between Score Matching, Contrastive Divergence, and Pseudolikelihood for Continuous-Valued Variables. IEEE Transactions on Neural Networks, 18(5), 2007. doi: 10.1109/TNN.2007.895819.\n\nAapo Hyvärinen. Estimation of Non-Normalized Statistical Models by Score Matching. Journal of\n\nMachine Learning Research, 6(24), 2005. ISSN 1533-7928.\n\nAapo Hyvärinen. Some extensions of score matching. Computational Statistics & Data Analysis, 51\n\n(5):2499–2512, 2007. doi: 10.1016/j.csda.2006.09.003.\n\n10\n\nAapo Hyvärinen. Optimal Approximation of Signal Priors. Neural Computation, 20(12), 2008. ISSN\n\n0899-7667, 1530-888X. doi: 10.1162/neco.2008.10-06-384.\n\nSergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by\n\nReducing Internal Covariate Shift. arXiv:1502.03167, 2015.\n\nRichard Jordan, David Kinderlehrer, and Felix Otto. The Variational Formulation of the Fokker– Planck Equation. SIAM Journal on Mathematical Analysis, 29(1):1–17, 1998. ISSN 0036-1410, 1095-7154. doi: 10.1137/S0036141096303359.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.6980,\n\n2017. arXiv: 1412.6980.\n\nPeter E Kloeden and Eckhard Platen. Stochastic differential equations. In Numerical Solution of\n\nStochastic Differential Equations, pp. 103–160. Springer, 1992.\n\nIvan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker. Normalizing Flows: An Introduction and Review of Current Methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43 (11):3964–3979, 2021. ISSN 1939-3539. doi: 10.1109/TPAMI.2020.2992934.\n\nHarold Joseph Kushner Kushner, Harold J Kushner, Paul G Dupuis, and Paul Dupuis. Numerical methods for stochastic control problems in continuous time, volume 24. Springer Science & Business Media, 2001.\n\nLei Li, Yingzhou Li, Jian-Guo Liu, Zibu Liu, and Jianfeng Lu. A stochastic version of Stein variational gradient descent for efficient sampling. Communications in Applied Mathematics and Computational Science, 15(1):37–63, 2020. ISSN 1559-3940, 2157-5452.\n\nQiang Liu. Stein Variational Gradient Descent as Gradient Flow. arXiv:1704.07520, 2017.\n\nQiang Liu and Dilin Wang. arXiv:1810.11693, 2018.\n\nStein Variational Gradient Descent as Moment Matching.\n\nQiang Liu and Dilin Wang. Stein Variational Gradient Descent: A General Purpose Bayesian\n\nInference Algorithm. arXiv:1608.04471, 2019.\n\nCheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Maximum likelihood training for score-based diffusion odes by high-order denoising score matching, 2022. URL https://arxiv.org/abs/2206.08265.\n\nJianfeng Lu, Yulong Lu, and James Nolen. Scaling limit of the Stein variational gradient descent: the\n\nmean field regime. arXiv:1805.04035, November 2018.\n\nDimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of FokkerPlanck equations through gradient-log-density estimation. Entropy, 22, 2020. ISSN 1099-4300.\n\nYoussef Marzouk, Tarek Moselhy, Matthew Parno, and Alessio Spantini. An introduction to sampling via measure transport. In Handbook of Uncertainty Quantification; R. Ghanem, D. Higdon, and H. Owhadi, editors, pp. 1–41. Springer, 2016. doi: 10.1007/978-3-319-11259-6_23-1.\n\nGautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian Simon. Symbolic Music Generation with\n\nDiffusion Models. arXiv:2103.16091, 2021. arXiv: 2103.16091.\n\nCesare Nardini, Étienne Fodor, Elsen Tjhung, Frédéric van Wijland, Julien Tailleur, and Michael E. Cates. Entropy production in field theories without time-reversal symmetry: Quantifying the non-equilibrium character of active matter. Phys. Rev. X, 7:021007, Apr 2017.\n\nBernt Oksendal. Stochastic Differential Equations. Springer-Verlag Berlin Heidelberg, 6 edition,\n\n2003. ISBN 978-3-642-14394-6.\n\nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing Flows for Probabilistic Modeling and Inference. Journal of Machine Learning Research, 22(57):1–64, 2021. ISSN 1533-7928.\n\n11\n\nM. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686–707, 2019. ISSN 0021-9991. doi: https:// doi.org/10.1016/j.jcp.2018.10.045. URL https://www.sciencedirect.com/science/ article/pii/S0021999118307125.\n\nPrajit Ramachandran, Barret Zoph, and Quoc V. Le.\n\nSearching for Activation Functions.\n\narXiv:1710.05941, 2017.\n\nDanilo Jimenez Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows.\n\narXiv:1505.05770, 2016.\n\nHannes Risken. Fokker-planck equation. In The Fokker-Planck Equation, pp. 63–95. Springer, 1996.\n\nChristian P. Robert and George Casella. Monte Carlo Statistical Methods. Springer, 2004.\n\nGiovanni Russo. Deterministic diffusion of particles. Communications on Pure and Applied Mathe-\n\nmatics, 43(6):697–733, 1990. ISSN 1097-0312.\n\nArdavan Saeedi, Tejas D. Kulkarni, Vikash K. Mansinghka, and Samuel J. Gershman. Variational Particle Approximations. Journal of Machine Learning Research, 18(69):1–29, 2017. ISSN 1533-7928.\n\nFilippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY, 55(58-63):94,\n\n2015.\n\nZebang Shen, Zhenfu Wang, Satyen Kale, Alejandro Ribeiro, Aim Karbasi, and Hamed Hassani.\n\nSelf-consistency of the fokker-planck equation. arXiv:2206.00860, 2022.\n\nJustin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differential equations. Journal of Computational Physics, 375:1339–1364, 2018. ISSN 0021-9991. doi: https://doi.org/10.1016/j.jcp.2018.08.029. URL https://www.sciencedirect.com/ science/article/pii/S0021999118305527.\n\nYang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution.\n\narXiv:1907.05600, 2020a.\n\nYang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models.\n\narXiv:2006.09011, 2020b.\n\nYang Song and Diederik P. Kingma. How to Train Your Energy-Based Models. arXiv:2101.03288,\n\n2021.\n\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Score-Based Generative Modeling through Stochastic Differential Equations.\n\nBen Poole. arXiv:2011.13456, 2021.\n\nHerbert Spohn. Large scale dynamics of interacting particles. Springer Science & Business Media,\n\n2012.\n\nE. G. Tabak and Cristina V. Turner. A Family of Nonparametric Density Estimation Algorithms. Communications on Pure and Applied Mathematics, 66(2):145–164, 2013. ISSN 1097-0312. doi: 10.1002/cpa.21423.\n\nEsteban G. Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood. Communications in Mathematical Sciences, 8(1):217–233, 2010. ISSN 15396746, 19450796. doi: 10.4310/CMS.2010.v8.n1.a11.\n\nCédric Villani. Optimal transport: old and new, volume 338. Springer, 2009.\n\nPascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural Computation, 23(7):1661–1674, 2011. ISSN 0899-7667, 1530-888X. doi: 10.1162/NECO_a_ 00142.\n\nCheng Zhang, Judith Bütepage, Hedvig Kjellström, and Stephan Mandt. Advances in Variational Inference. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(8):2008–2026, August 2019. ISSN 1939-3539. doi: 10.1109/TPAMI.2018.2889774.\n\n12\n\nA SOME BASIC FORMULAS\n\nHere, we derive some results linking the solution of the transport equation (TE) with that of the probability flow equation (5).\n\nA.1 PROBABILITY DENSITY AND PROBABILITY CURRENT\n\nWe begin with a lemma. Lemma A.1. Let ρt : Ω\n\n→\n\nR≥0 satisfy the transport equation\n\n(A.2)\n\nR, we\n\n(A.3)\n\n→\n\n∂tρt(x) =\n\n−∇ ·\n\n(vt(x)ρt(x)) . 0 and globally Lipschitz in x. Then, given any\n\n(A.1)\n\nAssume that vt(x) is C 2 in both t and x for t t, t′ 0, the solution of (A.1) satisfies\n\n≥\n\nρt(x) = ρt′(Xt,t′(x)) exp\n\n≥\n\n(cid:18)\n\n(cid:90) t\n\n−\n\nt′ ∇ ·\n\n(cid:19)\n\nvτ (Xt,τ (x))dτ\n\nwhere Xτ,t is the probability flow solution to (5). In addition, given any test function φ : Ω have\n\n(cid:90)\n\n(cid:90)\n\nφ(x)ρt(x)dx =\n\nφ(Xt′,t(x))ρt′(x)dx.\n\nΩ\n\nΩ\n\nIn words, Lemma A.1 states that an evaluation of the PDF ρt at a given point x may be obtained by evolving the probability flow equation (5) backwards to some earlier time t′ to find the point x′ that evolves to x at time t, assuming that ρt′(x′) is available. In particular, for t′ = 0, we obtain\n\nand\n\nρt(x) = ρ0(Xt,0(x)) exp\n\n(cid:18)\n\n(cid:90) t\n\n−\n\n0 ∇ ·\n\n(cid:19)\n\nvτ (Xt,τ (x))dτ\n\n,\n\n(cid:90)\n\nφ(x)ρt(x)dx =\n\n(cid:90)\n\nφ(X0,t(x))ρ0(x)dx.\n\n(A.4)\n\n(A.5)\n\nΩ Since the probability current is by definition vt(x)ρt(x), using (A.4) to express ρt(x) also gives the follwing equation for the current:\n\nΩ\n\nvt(x)ρt(x) = vt(x)ρ0(Xt,0(x)) exp\n\n(cid:18)\n\n(cid:90) t\n\n−\n\n0 ∇ ·\n\n(cid:19)\n\nvτ (Xτ,t(x))dτ\n\n.\n\n(A.6)\n\nProof. The assumed C 2 and globally Lipschitz conditions on vt guarantee global existence (on t 0) and uniqueness of the solution to (5). Differentiating ρt(Xt′,t(x)) with respect to t and using (5) and (A.1) we deduce\n\n≥\n\nd dt\n\nρt(Xt′,t(x)) = ∂tρt(Xt′,t(x)) +\n\nd dt\n\nXt′,t(x)\n\n· ∇\n\nρt(Xt′,t(x))\n\n= ∂tρt(Xt′,t(x)) + vt(Xt′,t(x)) vt(Xt′,t(x)) ρt(Xt′,t(x)) =\n\n· ∇\n\nρt(Xt′,t(x))\n\n−∇ ·\n\nIntegrating this equation in t from t = t′ to t = t gives (cid:90) t\n\n(cid:18)\n\nρt(Xt′,t(x)) = ρt′(x) exp\n\nvτ (Xt′,τ (x))dτ\n\n(cid:19)\n\n−\n\nt′ ∇ ·\n\n(A.7)\n\n(A.8)\n\nEvaluating this expression at x = Xt,t′(x) and using the group properties (i) Xt′,t(Xt,t′(x)) = x and (ii) Xt′,τ (Xt,t′(x)) = Xt,τ (x) gives (A.2). Equation (A.3) can be derived by using (A.2) to express Xt′,t(x) and noting ρt(x) in the integral at the left hand-side, changing integration variable x\n\nis precisely the Jacobian of this change of variable. The\n\n→\n\n(cid:82) t that the factor exp t′ ∇ · result is the integral at the right hand-side of (A.3).\n\nvτ (Xt,τ (x))\n\n−\n\n(cid:16)\n\n(cid:17)\n\nLemma A.1 also holds locally in time for any vt(x) that is C 2 in both t and x. In particular, it holds locally if we set st(x) = log ρt(x) and if we assume that ρ0(x) is (i) positive everywhere on Ω and (ii) C 3 in x. In this case, (A.1) is the Fokker-Planck equation (FPE) and (A.2) holds for the solution to that equation.\n\n∇\n\n13\n\nA.2 CALCULATION OF THE DIFFERENTIAL ENTROPY\n\nWe now consider computation of the differential entropy, and state a similar result.\n\nLemma A.2. Assume that ρ0 : Ω ρt : Ω transport equation (A.1) with st(x) = entropy st =\n\nR≥0 is positive everywhere on Ω and C 3 in its argument. Let R≥0 denote the solution to the Fokker Planck equation (FPE) (or equivalently, to the log ρt(x) in the definition of vt(x)). Then the differential\n\nΩ log ρt(x) ρt(x)dx can expressed as\n\n→\n\n→\n\n∇\n\n(cid:82)\n\n−\n\nHt =\n\n(cid:90)\n\n−\n\nΩ\n\nor\n\nlog ρt(X0,t(x)) ρ0(x)dx = H0 +\n\n(cid:90) t\n\n(cid:90)\n\n0\n\nΩ ∇ ·\n\nvτ (X0,τ (x))ρ0(x)dxdτ\n\n(A.9)\n\nHt = H0\n\n(cid:90) t\n\n(cid:90)\n\n−\n\n0\n\nΩ\n\nsτ (X0,τ (x))\n\n·\n\nvτ (X0,τ (x))ρ0(x)dxdτ\n\n(A.10)\n\nProof. We first derive (A.9). Observe that applying (A.5) with φ = log ρt leads to the first equality. The second can then be deduced from (A.4). To derive (A.10), notice that from (A.1),\n\n(cid:90)\n\nΩ (cid:90)\n\n(cid:90)\n\nd dt\n\nHt =\n\n=\n\n=\n\nlog ρt(x)\n\n(vt(x)ρt(x)) dx,\n\n∇ ·\n\n−\n\nΩ ∇\n\nlog ρt(x)\n\n·\n\nvt(x)ρt(x)dx,\n\n(A.11)\n\nst(x)\n\n·\n\nvt(x)ρt(x)dx\n\n−\n\nΩ\n\nAbove, we used integration by parts to obtain the second equality and st = Now, using (A.5) with φ = st\n\nvt integrating the result gives (A.10).\n\n·\n\nlog ρt to get the third.\n\n∇\n\nA.3 RESAMPLING OF ρt AT ANY TIME t\n\nIf the score st the dynamics\n\n≈ ∇\n\nlog ρt is known to sufficient accuracy, ρt can be resampled at any time t using\n\ndXτ = st(Xτ )dτ + dWτ .\n\n(A.12)\n\nIn (A.12), τ is an artificial time used for sampling that is distinct from the physical time in (1). For log ρt, the equilibrium distribution of (A.12) is exactly ρt. In practice, st will be imperfect st = and will have an error that increases away from the samples used to learn it; as a result, (A.12) should be used near samples for a fixed amount of time to avoid the introduction of additional errors.\n\n∇\n\nB FURTHER DETAILS ON SCORE-BASED TRANSPORT MODELING\n\nB.1 BOUNDING THE KL DIVERGENCE\n\nLet us restate Proposition 1 for convenience:\n\nProposition 1 (Control of the KL divergence). Assume that the conditions listed in Sec. 1.2 hold. Let ρt denote the solution to the transport equation (TE), and let ρ∗ t denote the solution to the Fokker-Planck equation (FPE). Assume that ρt=0(x) = ρ∗\n\nt=0(x) = ρ0(x) for all x\n\nΩ. Then\n\n∈\n\nd dt\n\nDKL(ρt\n\nρ∗ t )\n\n|\n\n≤\n\n1 2\n\n(cid:90)\n\nΩ |\n\nst(x)\n\nlog ρt(x) |\n\n− ∇\n\n2\n\nDt(x) ρt(x)dx,\n\n(8)\n\nwhere\n\n2\n\nDt(x) =\n\n, Dt(x)\n\n⟨·\n\n. ·⟩\n\n| · |\n\n14\n\nProof. By assumption, ρt solves (TE) and ρ∗ and v∗\n\nt (x) = bt(x)\n\nt solves (FPE). Denote by vt(x) = bt(x)\n\nlog ρ∗\n\nt (x). Then, we have\n\nDt(x)st(x)\n\n−\n\nd dt\n\nDKL(ρt\n\nρ∗\n\nt ) =\n\n|\n\nd dt\n\n−\n\nDt(x)s∗ (cid:90)\n\nt (x) with s∗ t (x) = (cid:19) (cid:18) ρt(x) ρ∗ t (x)\n\nlog\n\nρt(x)dx,\n\n∇\n\nΩ\n\n(cid:90)\n\nΩ\n\n(cid:90)\n\nΩ\n\n(cid:90)\n\nΩ\n\n=\n\n=\n\n=\n\n=\n\n−\n\n−\n\n− (cid:90)\n\nρt(x) ρ∗ t (x)\n\nv∗\n\nt (x)\n\n∂tρ∗\n\nt (x)dx +\n\n(cid:19)\n\n(cid:18) ρt(x) ρ∗ t (x)\n\n· ∇\n\n(cid:90)\n\nlog\n\nΩ\n\n(cid:18) ρt(x) ρ∗ t (x) (cid:90)\n\nρ∗\n\nt (x)dx +\n\nΩ\n\n(cid:19)\n\n∂tρt(x)dx,\n\nvt(x)\n\nlog\n\n· ∇\n\n(cid:19)\n\n(cid:18) ρt(x) ρ∗ t (x)\n\nρt(x)dx,\n\n(v∗\n\nt (x)\n\nvt(x))\n\n( ∇\n\n·\n\n−\n\nlog ρt(x)\n\n− ∇\n\nlog ρ∗\n\nt (x)) ρt(x)dx,\n\n(s∗\n\nt (x)\n\nst(x))\n\n·\n\n−\n\nDt(x) (\n\n∇\n\nlog ρt(x)\n\n−\n\ns∗ t (x)) ρt(x)dx.\n\nΩ\n\nAbove, we used integration by parts to obtain the third equality. Now, dropping function arguments for simplicity of notation, we have that\n\nlog ρt\n\n|∇\n\n−\n\nst\n\n2 Dt = |\n=\n\n≥ Hence, we deduce that\n\n|∇\n\n|∇ 2(\n\nlog ρt\n\nlog ρt\n\nlog ρt\n\n∇\n\n−\n\n−\n\n,\n\n2\n\nt + s∗ s∗ s∗ Dt + t | s∗ t )\n\nt −\n\n2 st Dt |\ns∗ st t − |\nDt(s∗ t −\n\n·\n\n−\n\n2 Dt + 2( |\nst).\n\nlog ρt\n\ns∗ t )\n\n·\n\n−\n\nDt(s∗\n\nt −\n\nst),\n\n∇\n\n(B.1)\n\nd dt\n\nDKL(ρt\n\nρ∗ t )\n\n|\n\n≤\n\n1 2\n\n(cid:90)\n\nΩ |\n\nst(x)\n\nlog ρt(x) |\n\n− ∇\n\n2\n\nDt(x)ρ0(x)dx.\n\n(B.2)\n\nB.2 SBTM IN THE EULERIAN FRAME\n\nThe Eulerian equivalent of Proposition 2 can be stated as:\n\nProposition B.1 (SBTM in the Eulerian frame). Assume that the conditions listed in Sec. 1.2 hold. Fix T\n\n] and consider the optimization problem\n\n(0,\n\n∈\n\n∞\n\n(cid:90) T\n\n(cid:90)\n\nmin {st:t∈[0,T )}\n\n0\n\nst(x)\n\nΩ |\n\nlog ρt(x)\n\n2 Dt(x) ρt(x)dxdt |\n\n− ∇\n\n(SBTM2)\n\nsubject to: ∂tρt(x) =\n\n(vt(x)ρt(x)) , x\n\nΩ\n\n−∇ ·\n\n∈\n\nDt(x)st(x). Then every minimizer of (SBTM2) satisfies Dt(x)s∗\n\nt (x) =\n\nwith vt(x) = bt(x) Dt(x)\n\nlog ρ∗\n\nt (x) where ρ∗\n\nt : Ω\n\n−\n\n∇\n\nR>0 solves (FPE).\n\n→\n\nIn words, this proposition states that solving the constrained optimization problem (SBTM2) is equivalent to solving the Fokker-Planck equation (FPE).\n\nProof. The constrained minimization problem (SBTM2) can be handled by considering the extended objective\n\n(cid:90) T\n\n(cid:90)\n\n(cid:16)\n\n0\n\nΩ\n\nst(x) |\n\n− ∇\n\nlog ρt(x)\n\n2 Dt(x) ρt(x) + μt(x) (∂tρt(x) + |\nDt(x)st(x) and μt : Rd\n\nwhere vt(x) = bt(x) equations associated with (B.3) read\n\n−\n\n→\n\n(vt(x)ρt(x)))\n\n(cid:17)\n\ndxdt\n\n(B.3)\n\n∇ ·\n\nR≥0 is a Lagrange multiplier. The Euler-Lagrange\n\n∂tρt(x) = ∂tμt(x) = vt(x)T\n\n−∇ ·\n\n(vt(x)ρt(x))\n\nμt(x) +\n\nst(x) |\n\n2 Dt(x) − |∇ |\n\nlog ρt\n\n2 Dt(x) |\n\n[Dt(x) (st(x)\n\nlog ρt(x))] ,\n\n∇ + 2\n\n∇ ·\n\n− ∇\n\n0 = μT (x),\n\n0 = Dt(x) (st(x)\n\n− ∇\n\nlog ρt(x)) ρt(x) + 1\n\n2 Dt(x)\n\n∇\n\n15\n\n(B.4)\n\nμt(x)ρt(x)\n\n−\n\nClearly, these equations will be satisfied if s∗ all x, and ρ∗ of the objective. Moreover, all global minimizers must satisfy Dt(x)s∗ almost everywhere), as this is the only way to zero the objective. (ρt\n\nt (x) = 0 for ∇\nt solves (FPE). This solution is also a global minimizer, because it zeroes the value t (x)\n\nt (x) = Dt(x)\n\nt (x) for all x\n\nt (x) =\n\nlog ρ∗\n\nlog ρ∗\n\nΩ, μ∗\n\n∇\n\n∈\n\nIt is also easy to see that there are no other local minimizers. To check this, we can use the fourth equation to write\n\nThen,\n\nDt(x)(st(x)\n\n− ∇\n\nlog ρt(x)) = 1\n\n2 Dt(x)\n\nμt(x).\n\n∇\n\nlog ρt(x) |\nThis reduces the first three equations to\n\n2 st(x) Dt(x) − |∇ |\n\n|\n\n2\n\nDt(x) = 1\n\n2 (st(x) +\n\nlog ρt(x))T Dt(x)\n\nμt(x).\n\n∇\n\n∇\n\n∂tρt(x) =\n\n−∇ · ∂tμt = (cid:0)bt(x)\n\n+\n\n∇ ·\n\nμT (x) = 0.\n\n(cid:0)bt(x)ρt(x)\n\n−\n\nDt(x)\n\n(Dt(x)\n\n∇\n\n∇\n\nDt(x)\n\nρt(x)\n\n∇\n\n− −\nlog ρt(x) 2 Dt(x) −\nμt(x)) + 1 2 (st(x) +\n\n1\n\n1\n\nμt(x)(cid:1)\n\n2 ρtDt(x) ∇\nμt(x)(cid:1)T ∇\nlog ρt(x))T Dt(x)\n\nμt(x)\n\n∇\n\n∇\n\n(B.5)\n\nμt(x).\n\n∇\n\nSince the equation for μt is homogeneous in μt and μT = 0, we must have μt = 0 for all t and the equation for ρt reduces to (FPE).\n\n∈\n\n[0, T ),\n\nB.3 SBTM IN THE LAGRANGIAN FRAME\n\nAs stated, Proposition B.1 is not practical, because it is phrased in terms of the density ρt. The following result demonstrates that the transport map identity (6) can be used to re-express Proposition B.1 entirely in terms of known quantities.\n\nProposition 2 (Score-based transport modeling). Assume that the conditions listed in Sec. 1.2 hold. Define vt(x) = bt(x)\n\nDt(x)st(x) and consider\n\n ̇Xt(x) = vt(Xt(x)), ̇Gt(x) =\n\nvt(Xt(x))]TGt(x)\n\n− ∇∇ · Then ρt = Xt♯ρ0 solves (TE), the equality Gt(x) =\n\n−\n\n−\n\n[ ∇\n\nX0(x) = x,\n\nvt(Xt(x)),\n\nG0(x) =\n\nlog ρ0(x).\n\n(10)\n\n∇\n\nlog ρt(Xt(x)) holds, and for any T\n\n∇\n\nDKL(XT ♯ρ0\n\nρ∗\n\nT )\n\n|\n\n≤\n\n1 2\n\n(cid:90) T\n\n(cid:90)\n\n0\n\nΩ |\n\nst(Xt(x))\n\nGt(x)\n\n2 Dt(Xt(x)) ρ0(x)dxdt. |\n\n−\n\nMoreover, if s∗\n\nt is a minimizer of the constrained optimization problem (cid:90) T\n\n(cid:90)\n\nmin s\n\n0\n\nΩ |\n\nst(Xt(x))\n\nGt(x)\n\n− t (x) where ρ∗ log ρ∗\n\n2 Dt(Xt(x)) ρ0(x)dxdt |\n\nsubject to (10)\n\nthen Dt(x)s∗ X ∗\n\nt (x) = Dt(x)\n\n∇\n\nt associated to any minimizer is a transport map from ρ0 to ρ∗\n\nt solves the Fokker-Planck equation (FPE). The map\n\nρ0\n\nx\n\n∼\n\nimplies that\n\nX ∗\n\nt (x)\n\nρ∗ t ,\n\n∼\n\n[0, T ].\n\n∈\n\n(12)\n\nt , i.e. t\n∀\n\nProof. Let us first show that Gt(x) = the transport equation (TE). Since (TE) implies that\n\n∇\n\nlog ρt(Xt(x)) satisfies (10) if ρt = Xt♯ρ0, i.e. if ρt satisfies\n\ntaking the gradient gives\n\n∂t log ρt(x) + vt(x)\n\nlog ρt(x) =\n\nvt(x),\n\n−∇ ·\n\n· ∇\n\n∂t\n\nlog ρt(x) + [\n\nvt(x)]T\n\nlog ρt(x) +\n\n∇ Therefore Gt(x) =\n\n∇\n\n∇\n\nlog ρt(Xt(x)) solves\n\nlog ρt(x)\n\nvt(x) =\n\n·\n\n−∇∇ ·\n\nvt(x).\n\n∇∇\n\n∇\n\nd dt\n\nGt(x) = ∂t\n\n= ∂t\n\nlog ρt(Xt(x)) +\n\nlog ρt(Xt(x)) +\n\n∇\n\n∇\n\n=\n\n−∇∇ ·\n\nvt(Xt(x))\n\n−\n\n∇∇\n\n∇∇ [\n∇\n\nlog ρt(Xt(x))\n\nlog ρt(Xt(x))\n\nXt(x),\n\nd dt vt(x),\n\n·\n\n·\n\nvt(Xt(x))]T\n\nlog ρt(Xt(x)),\n\n∇\n\n(B.6)\n\n(B.7)\n\n(B.8)\n\n16\n\n[0,\n\n)\n\n∞\n\n∈\n\n(11)\n\n(SBTM)\n\nwhich recovers the equation for Gt(x) in (10). Hence, the objective in (SBTM) can also be written as\n\n(cid:90) T\n\n(cid:90)\n\n0\n\nΩ |\n\nst(Xt(x))\n\n− ∇\n\nlog ρt(Xt(x))\n\n2 ρ0(x)dxdt |\n\n(cid:90) T\n\n(cid:90)\n\n=\n\n0\n\nst(x)\n\nΩ |\n\nlog ρt(x)\n\n2 ρt(x)dxdt |\n\n− ∇\n\n(B.9)\n\nwhere the second equality follows from (A.5) if ρt(x) satisfies (A.1). Hence, (SBTM) is equivalent to (SBTM2). The bound on DKL(XT ♯ρ0\n\nT ) follows from (9).\n\nρ∗\n\n|\n\nAdjoint equations. In terms of a practical implementation, the objective in (SBTM2) can be i=1 from ρ0 and solving the equations for Xt and Gt using the evaluated by generating samples initial conditions X0(xi) = xi and G0(xi) = log ρ0(xi). Note that evaluating this second initial ∇\ncondition only requires one to know ρ0 up to a normalization factor. To evaluate the gradient of the objective, we can introduce equations adjoint to those for Xt and Gt. They read, respectively\n\nxi\n\n{\n\n}\n\nn\n\nd dt\n\nθt(x) + [\n\n∇\n\nvt(Xt(x))]Tθt(x) = ηt(x)\n\nvt(Xt(x))Gt(x)\n\n· ∇∇\n\n+ ηt(x) + 2\n\n· ∇∇∇\n\nvt(Xt(x))Gt(x) st(Xt(x))(st(Xt(x))\n\n∇\n\nGt(x)),\n\n−\n\n(B.10)\n\nθT (x) = 0\n\nd dt\n\nηt(x)\n\n− ∇\n\nvt(Xt(x))ηt(x) = 2(Gt(x)\n\nst(Xt(x))),\n\n−\n\nηT (x) = 0.\n\nIn terms of these functions, the gradient of the objective is the gradient with respect to st(x) (or the parameters in this function when it is modeled by a neural network) of the extended objective:\n\n(cid:90) T\n\n(cid:90)\n\nL[st] =\n\nst(Xt(x))\n\n0\n\n+\n\n+\n\nΩ | (cid:90) T\n\n(cid:90)\n\n0 (cid:90) T\n\nΩ\n\n(cid:90)\n\n0\n\nΩ\n\nθt(x)\n\nηt(x)\n\n·\n\n·\n\nGt(x) |\n\n−\n\n2 ρ0(x)dxdt\n\n(cid:16)\n\n ̇Xt(x)\n\n−\n\n(cid:17)\n\nvt(Xt(x))\n\nρ0(x)dxdt\n\n(B.11)\n\n(cid:16)\n\n ̇Gt(x) + [\n\n∇\n\nvt(Xt(x))]TGt(x)\n\n(cid:17)\n\nρ0(x)dxdt,\n\n+\n\n∇∇ ·\n\nvt(Xt(x))\n\nwhere vt(x) = bt(x)\n\nDt(x)st(x).\n\n−\n\nB.4 SEQUENTIAL SBTM\n\nLet us restate Proposition 3 for convenience:\n\nProposition 3 (Sequential SBTM). In the same setting as Proposition 2, let Xt(x) solve the first equation in (10) with vt(x) = bt(x)\n\nDt(x)st(x). Let st be obtained via\n\n(cid:90)\n\n(cid:16)\n\nΩ\n\nmin st\n\nst(Xt(x)) |\n\n2 Dt(Xt(x)) + 2 |\n\n∇ ·\n\n−\n\n(Dt(Xt(x))st(Xt(x)))\n\n(cid:17)\n\nρ0(x)dx.\n\n(SSBTM)\n\nThen, each minimizer s∗ solution to (FPE). Moreover, the map X ∗\n\nt of (SSBTM) satisfies Dt(x)s∗ t associated to s∗\n\nt (x) where ρ∗ t (x) = Dt(x) t is a transport map from ρ0 to ρ∗ t .\n\nlog ρ∗\n\n∇\n\nt is the\n\nProof. If Xt♯ρ0 = ρt, then by definition we have the identity\n\n(cid:90)\n\n(cid:16)\n\nΩ\n\nst(Xt(x)) |\n\n|\n\n2\n\nDt(Xt(x)) + 2\n\n(Dt(Xt(x))st(Xt(x)))\n\n(cid:17)\n\nρ0(x)dx\n\n∇ ·\n\n=\n\n(cid:90)\n\n(cid:16)\n\nΩ\n\nst(x) |\n\n|\n\n2\n\nDt(x) + 2\n\n∇ ·\n\n(Dt(x)st(x))\n\n(cid:17)\n\nρt(x)dx.\n\n(B.12)\n\n17\n\nThis means that the optimization problem in (SSBTM) is equivalent to\n\n(cid:90)\n\n(cid:16)\n\nΩ\n\nmin st\n\n2 st(x) Dt(x) + 2 |\n\n|\n\n∇ ·\n\n(Dt(x)st(x))\n\n(cid:17)\n\nρt(x)dx.\n\nAll minimizers s∗ by (TE),\n\nt of this optimization problem satisfy Dt(x)s∗\n\nt (x) = Dt(x)\n\nlog ρt(x). Hence,\n\n∇\n\nwhich recovers (FPE), so that ρt(x) = ρ∗\n\n∂tρt(x) =\n\n−∇ ·\n\n(bt(x)ρt(x) t (x) solves (FPE).\n\n−\n\n∇\n\nDt(x)\n\nρt(x))\n\n(B.13)\n\nB.5 LEARNING FROM THE SDE\n\nIn this section, we show that learning from the SDE alone – i.e., avoiding the use of self-consistent samples – does not provide a guarantee on the accuracy of ρt. We have already seen in (9) that it is sufficient to control (cid:82) T 2\nT ). The proof Dt |\n− ∇ of Proposition 1 shows that control on\n\nt (x)dxdt to control DKL(ρT\n\nlog ρt(x)\n\nst(x)\n\nΩ |\n\nρ∗\n\nρ∗\n\n(cid:82)\n\n0\n\n|\n\n(cid:90) T\n\n(cid:90)\n\n0\n\nΩ |\n\nst(x)\n\nlog ρ∗\n\nt (x)\n\nDt(x)ρ∗ 2\n|\n\nt (x)dxdt,\n\n− ∇\n\n(B.14)\n\nρ∗ as would be provided by training on samples from the SDE, does not ensure control on DKL(ρT T ). The following proposition shows that control on (B.14) does not guarantee control on DKL(ρ∗ ρT ) either. An analogous result appeared in Lu et al. (2022) in the context of SBDM for generative modeling; here, we provide a self-contained treatment to motivate the use of the sequential SBTM procedure discussed in the main text. Proposition B.2. Let ρt : Ω following equality holds\n\nR>0 solve (FPE). Then, the\n\nR>0 solve (TE), and let ρ∗\n\nt : Ω\n\n| T |\n\n→\n\n→\n\nDKL(ρ∗\n\nT |\n\nρT ) =\n\n(cid:90) T\n\n(cid:90)\n\nst(x)\n\nlog ρ∗\n\nDt(x)ρ∗ 2\nt (x) |\n\nt (x)dxdt\n\n− ∇\n\n0\n\nΩ |\n\n(cid:90) T\n\n(cid:90)\n\n+\n\n0\n\nΩ\n\nlog ρt(x)\n\n(\n\n∇\n\n−\n\nst(x))T Dt(x) (st(x)\n\n− ∇\n\nlog ρ∗\n\nt (x)) ρ∗\n\nt (x)dxdt.\n\n(B.15)\n\nProposition B.2 shows that minimizing the error between st and remainder term, because in general upper bound\n\nlog ρt\n\n∇\n\nt leaves a = st. The proof shows that we may obtain the simple\n\nt on samples of ρ∗\n\nlog ρ∗\n\n∇\n\nDKL(ρ∗\n\nT |\n\n+\n\n1 2\n\n1 2\n\nρT )\n\n(cid:90) T\n\n≤ (cid:90)\n\n0\n\nΩ |\n\n(cid:90) T\n\n(cid:90)\n\nst(x)\n\nlog ρ∗\n\nt (x) |\n\n− ∇\n\n2\n\nDt(x)ρ∗\n\nt (x)dxdt\n\n0\n\nΩ |\n\nst(x)\n\nlog ρt(x)\n\nDt(x)ρ∗ 2\n|\n\nt (x)dxdt.\n\n− ∇\n\nHowever, controlling the above quantity requires enforcing agreement between st and addition to st and t ; this is precisely the idea of SBTM.\n\nlog ρ∗\n\n∇\n\n(B.16)\n\nlog ρt in\n\n∇\n\nProof. By symmetry, we may replace ρt by ρ∗ (cid:90)\n\nd dt\n\nDKL(ρ∗\n\nρt) =\n\nt |\n\nlog ρt(x)\n\n(\n\n∇\n\n− ∇\n\nlog ρ∗\n\nt (x))T Dt(x) (st(x)\n\nlog ρ∗\n\nt (x)) ρ∗\n\nt (x)dx\n\n− ∇\n\nt in the proof of Proposition 1 to find\n\nAdding and subtracting st(x) to the first term in the inner product and expanding gives\n\nd dt\n\nDKL(ρ∗\n\nt |\n\n(cid:90)\n\nρt) =\n\n(cid:90)\n\nΩ |\n\nst(x)\n\nlog ρ∗\n\nt (x)\n\n2ρ∗ |\n\nt (x)dx\n\n− ∇\n\n(B.17)\n\n+\n\n(\n\n∇\n\nΩ\n\nlog ρt(x)\n\n−\n\nst(x))T Dt(x) (st(x)\n\n− ∇\n\nlog ρ∗\n\nt (x)) ρ∗\n\nt (x)dx,\n\nIntegrating from 0 to T completes the proof.\n\n18\n\n̸ B.6 DENOISING LOSS\n\nThe following standard trick can be used to avoid computing the divergence of st(x): Lemma B.3. Given ξ = N (0, I), we have α−1E(cid:0)st(x + αξ)\n\nξ(cid:1) =\n\nst(x),\n\nlim α↓0\n\nα−1E(cid:0)st(x + ασt(x)ξ)\n\n·\n\n∇ · σt(x)ξ(cid:1) = tr (Dt(x)\n\n·\n\nst(x))\n\n∇\n\nlim α↓0\n\nProof. We have\n\nα−1st(x + αξ)\n\nξ = α−1st(x)\n\n·\n\nξ + (\n\n·\n\n∇\n\nst(x)ξ)\n\n·\n\nξ + o(α)\n\n(B.18)\n\n(B.19)\n\nThe expectation of the first term on the right-hand side of this equation is zero; the expectation of the second gives the result in (B.18). Hence, taking the expectation of (B.19) and evaluating the result in the limit as α 0 gives the first equation in (B.18). The second equation in (B.18) can be proven similarly using σt(x)σt(x)T = Dt(x).\n\n↓\n\nReplacing\n\n∇ ·\n\nst(x) in (SSBTM) with the first expression in (B.19) for a fixed α > 0 gives the loss\n\n[st] = Eξ\n\nL\n\n(cid:20)(cid:90)\n\n(cid:18)\n\nΩ\n\n|\n\nst(Xt(x))\n\n2 + |\n\n2 α\n\nst(Xt(x) + αξ)\n\n(cid:21)\n\nρ0(x)dx\n\n.\n\n(cid:19)\n\nξ\n\n·\n\n(B.20)\n\nEvaluating the square term at a perturbed data point recovers the denoising loss of Vincent (2011)\n\n[st] = Eξ\n\nL\n\n(cid:34)(cid:90)\n\nΩ\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nst(Xt(x) + αξ) +\n\n(cid:35)\n\nρ0(x)dx\n\n.\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nξ α\n\n(B.21)\n\nWe can improve the accuracy of the approximation with a “doubling trick” that applies two draws of the noise of opposite sign to reduce the variance. This amounts to replacing the expectations in (B.18) with\n\n1\n\n2 α−1E(cid:2)st(x + αξ) 2 α−1E(cid:2)st(x + ασt(x)ξ)\n\n−\n\nξ\n\n·\n\nst(x\n\nαξ)\n\n−\n\nξ(cid:3),\n\n· st(x\n\n1\n\nσt(x)ξ(cid:3), whose limits as α st(x)), respectively. In practice, we observe that this approach always helps stabilize training. Moreover, we observe that use of the denoising st(x) even when the loss also stabilizes training, so that it is preferable to full computation of latter is feasible.\n\nst(x) and tr (Dt(x)\n\nασt(x)ξ)\n\nσt(x)ξ\n\n0 are\n\n∇ ·\n\n∇ ·\n\n→\n\n∇\n\n−\n\n−\n\n·\n\n·\n\n(B.22)\n\nC GAUSSIAN CASE\n\nHere, we consider the case of an Ornstein-Uhlenbeck (OU) process where the score can be written analytically, thereby providing a benchmark for our approach. The example treated in Section 4.1 with details in Appendix D.1 is a special case of such an OU process with additional symmetry arising from permutations of the particles.\n\nThe SDE reads\n\nRd, Γt\n\nwhere Xt bt equation associated with (C.1) is\n\nRd is a time-dependent vector, and σt\n\n∈\n\n∈\n\n∈\n\n∈\n\nΓt(Xt\n\ndXt =\n\n(C.1) Rd×d is a time-dependent positive-definite tensor (not necessarily symmetric), Rd×d is a time-dependent tensor. The Fokker-Planck\n\nbt)dt + √2σtdWt\n\n−\n\n−\n\n∂tρ∗\n\nt (x) =\n\n((Γtx\n\nbt)ρ∗\n\nt (x)\n\nDt\n\n∇\n\n−\n\nρ∗\n\nt (x))\n\n−\n\n−∇ ·\n\n(C.2)\n\nRd×d positive-definite, the solution is Gaussian at all times t\n\nt . Assuming that the initial condition is Gaussian, ρ0 = N(m0, C0) with C0 = t = N(mt, Ct) with mt\n\n0, ρ∗\n\nwhere Dt = σtσT C T and Ct = C T\n\n0 ∈\n\nt solutions to\n\n≥\n\n ̇mt = ̇Ct =\n\n−\n\n−\n\nΓt(mt\n\nΓtCt\n\nbt) −\nCtΓT\n\nt + 2Dt\n\n−\n\n19\n\n(C.3)\n\nThis implies in particular that\n\nso that the probability flow equation for Xt and the equation for Gt written in (SBTM2) read\n\nlog ρ∗\n\nt (x) =\n\nC −1\n\nt\n\n(x\n\n−\n\nmt).\n\n−\n\n∇\n\n ̇Xt(x) = (DtC −1 ̇Gt(x) = (ΓT\n\nt − C −1\n\nt −\n\nt Dt)Gt(x),\n\nΓt)Xt(x) + Γtbt\n\nDtC −1\n\nt mt,\n\n−\n\n(C.4)\n\n(C.5)\n\nwith initial condition X0(x) = x and G0(x) = −\nsee that with x equation in (C.5), the mean and variance of Xt satisfy (C.3). Similarly, when x G0(x) hence preserves Gaussianity. Moreover, E0Gt(x) = 0 and Bt = BT\n\nm0). It is easy to t = N(mt, Ct) since, from the first ρ0 = N(m0, C0), ) because the second equation in (C.5) is linear and\n\nρ0 = N(m0, C0) we have Xt(x)\n\n∼ N (0, C −1\n\nlog ρ0(x) = ρ∗\n\n0 ), so that Gt(x)\n\nt = E0[Gt(x)GT\n\nt (x)] satisfies\n\nN(0, C −1\n\n0 (x\n\n∇\n\n∼\n\n−\n\n∼\n\n∼\n\n∼\n\nt\n\nC −1\n\nt − The solution to this equation is Bt = C −1 for C −1\n\nt that we can deduce from (C.3)\n\nt\n\nd dt\n\nBt = (ΓT\n\nC −1\n\nt Dt)Bt + Bt(Γt\n\nDtC −1\n\nt\n\n)\n\n−\n\n(C.6)\n\nsince substituting this ansatz into (C.6) gives the equation\n\nd dt\n\nC −1\n\nt = C −1\n\nt\n\n ̇CtC −1\n\nt =\n\nC −1\n\nt Γt\n\n−\n\n−\n\nΓT\n\nt C −1\n\nt + 2C −1\n\nt DtC −1\n\nt\n\n.\n\n(C.7)\n\nNote that if Γt = Γ, bt = b, and Dt = D are all time-independent, then limt→∞ ρt = N (m∞, C∞) with m∞ = b and C∞ the solution to the Lyapunov matrix equation\n\nΓC∞ + C∞ΓT = 2D.\n\n(C.8)\n\nThis means that at long times the coefficients at the right-hand sides of (C.5) also settle on constant values. However, Xt and Gt do not necessarily stop evolving; one situation where they too tend to fix Rd×d values is when the OU process is in detailed balance, i.e. when Γ = DA for some A = AT positive-definite. In that case, the solution to (C.8) is C∞ = A−1 and it is easy to see that at long times the right hand sides of (C.5) tend to zero. Remark C.1. This last conclusion is actually more generic than for a simple OU process. For any SDE in detailed balance, i.e. that can be written as\n\n∈\n\nD(Xt)dt + √2σt(Xt)dWt\n\n(C.9)\n\ndXt =\n\nD(Xt)\n\nU (Xt)dt + R>0 is a C 2-potential such that Z = (cid:82)\n\n∇ ·\n\n∇\n\n−\n\nwhere U : Rd limt→∞ ρt(x) = Z −1e−U (x), and the corresponding flows Xt and Gt eventually stop as t this case, ρt follows gradient descent in W2 over the energy\n\nRd e−U (x)dx <\n\n→\n\n∞\n\n, we have that . In\n\n→ ∞\n\nE[ρ] =\n\n(cid:90)\n\nRd\n\n(U (x) + log ρ(x))ρ(x)dx\n\n(C.10)\n\nThe unique PDF minimizing this energy is Z −1e−U (x), and as t transport map between the initial ρ0 and Z −1e−U (x).\n\n→ ∞\n\nXt converges towards a\n\nD EXPERIMENTAL DETAILS AND ADDITIONAL EXAMPLES\n\nAll numerical experiments were performed in jax using the dm-haiku package to implement the networks and the optax package for optimization.\n\nD.1 HARMONICALLY INTERACTING PARTICLES IN A HARMONIC TRAP\n\nR and two-particle interaction Network architecture Both the single-particle energy Uθt,1 : Rd energy Uθt,2 : Rd R are parameterized as single hidden-layer neural networks with the swish activation function (Ramachandran et al., 2017) and n_hidden = 100 hidden neurons. The hidden layer biases are initialized to zero while the hidden layer weights are initialized from\n\nRd\n\n→\n\n→\n\n×\n\n20\n\na truncated normal distribution with variance 1/fan_in, following the guidelines recommended in (Ioffe & Szegedy, 2015).\n\nOptimization The Adam (Kingma & Ba, 2017) optimizer is used with an initial learning rate of η = 10−4 and otherwise default settings. At time t = 0, the analytical relative loss\n\n(cid:82)\n\nL[s0] =\n\ns0(x) (cid:82)\n\n|\n\nlog ρ0(x)\n\n2ρ0(x)dx |\n2ρ0(x)dx\n\n− ∇ log ρ0(x) |\n\n|∇\n\n(D.1)\n\nis minimized to a value less than 10−4 using knowledge of the initial condition ρ0 = N (cid:0)β0, σ2 0I(cid:1) with σ0 = 0.25. In (D.1), the expectation with respect to ρ0 is approximated by an initial set of\n\n(cid:16)\n\n(cid:17)T\n\nx(1)\n\n, x(2)\n\n, . . . , x(N )\n\nsamples xj = with j = 1, . . . , n drawn from ρ0. In the experiments, we set n = 100. We set the physical timestep ∆t = 10−3 and take n_opt_steps = 25 steps of Adam until the norm of the gradient is below gtol = 0.1.\n\nj\n\nj\n\nj\n\nAnalytical moments First define the mean, second moment, and covariance according to t = E(cid:2)X (i) m(i) t = E(cid:2)X (i) t = M (ij)\n\n(cid:1)T(cid:3), m(i)(cid:0)m(j)(cid:1)T\n\n(cid:3), (cid:0)X (j)\n\nM (ij)\n\nC (ij)\n\n.\n\nt\n\nt\n\nt\n\nIt is straightforward to show that the mean and covariance obey the dynamics\n\n−\n\n ̇m(i)\n\nt =\n\n(m(i)\n\nt −\n\n−\n\nβt) +\n\nα N\n\nN (cid:88)\n\n(cid:16)\n\nk=1\n\nm(i)\n\nt −\n\nm(k)\n\nt\n\n(cid:17)\n\n,\n\n ̇C (ij)\n\nt =\n\n2(1\n\n−\n\n−\n\nα)C (ij)\n\nt + 2DIδij\n\nα N\n\n−\n\nN (cid:88)\n\n(cid:16)\n\nk=1\n\nC (kj)\n\nt + C (ik)\n\nt\n\n(D.2)\n\n(D.3)\n\n(cid:17)\n\nBecause the particles are indistinguishable so long as they are initialized from a distribution that is symmetric with respect to permutations of their labeling, the moments will satisfy the ansatz\n\nt = ̄m(t),\n\ni = 1, . . . , N\n\nm(i) C (ij)\n\nt = Cd(t)δij + Co(t)(1\n\nδij),\n\ni, j = 1, . . . , N.\n\n−\n\nThe dynamics for the vector ̄m : R≥0 Co : R≥0\n\nR ̄d× ̄d can then be obtained from (D.2) and (D.3) as\n\n→\n\nR ̄d, as well as the matrices Cd : R≥0\n\n(D.4)\n\n(D.5) R ̄d× ̄d and\n\n→\n\n→\n\n ̄m,\n\n ̇ ̄m = βt\n\n− ̇Cd = 2(α\n\n1)Cd\n\n2\n\n−\n\n−\n\n(Cd + (n\n\n−\n\n1)Co) + 2DI,\n\n ̇Co = 2(α\n\n1)Co\n\n2\n\n(Cd + (n\n\n1)Co) .\n\n−\n\nFor a given β : R time, giving the mean mt = ̄m(t) (cid:1) Co(t) solution to the Fokker-Planck equation ρ∗\n\n− R ̄d, these equations can be solved analytically in Mathematica as a function of RN ̄d and covariance Ct = (Cd(t) IN ×N + RN ̄d×N ̄d. Because the solution is Gaussian for all t, we then obtain the analytical t = N (mt, Ct) and the corresponding analytical score\n\n(cid:0)1N 1T\n\nCo(t))\n\n1N\n\n→\n\n−\n\n⊗\n\n−\n\n⊗\n\n⊗\n\n∈\n\n∈\n\nN\n\nlog ρ∗\n\nt (x) = C −1\n\nt\n\n(x\n\nmt).\n\n−\n\n−∇ Potential structure Here, we show that the potential for this example lies in the class of potentials described by (15). From Equation D.5, we have a characterization of the structure of the covariance matrix Ct for the analytical potential Ut(x) = 1 mt). In particular, Ct is block 2 (x circulant, and hence is block diagonalized by the roots of unity (the block discrete Fourier transform). That is, we may take a “block eigenvector” of the form ωk = (cid:0)I ̄d× ̄dρk, I ̄d× ̄dρ2k, . . . , I ̄d× ̄dρ(N −1)k(cid:1)T with ρ = exp( 1. By direct calculation, this block diagonalization leads to two distinct block eigenmatrices,\n\n2πi/N ) for k = 0, . . . N\n\nmt)TC −1\n\n(x\n\n−\n\n−\n\n−\n\n−\n\nt\n\nα N\nα N\n\nCt = V\n\nCd(t) + (N 0\n\n\n\n \n\n\n1)Co(t)\n\n−\n\nCd(t)\n\nCo(t)\n\n0\n\n− 0\n0\n\n0 0\n. . . 0\n\n. . . . . .\n\n. . . . . . Cd(t)\n\n0 0\n\n0\n\n−\n\n\n\n \n\n\nV −1\n\nCo(t)\n\n21\n\n0 0\n\nRN ̄d×N ̄d denotes the matrix with block columns ωk. The inverse matrix C −1\n\nwhere V must similarly have only two distinct block eigenmatrices given by (Cd(t) + (N (Cd(t)\n\n− Co(t))−1. By inversion of the block Fourier transform, we then find that\n\nthen 1)Co(t))−1 and\n\n∈\n\nt\n\n−\n\nfor some matrices ̄Cd, ̄Co. Hence, by direct calculation\n\n(cid:0)C −1\n\nt\n\n(cid:1)(ij)\n\n= ̄Cdδij + ̄Co(1\n\nδij)\n\n−\n\nmt)T C −1\n\nt\n\n(x\n\n(x\n\n−\n\nmt) =\n\n−\n\n=\n\n=\n\nN (cid:88)\n\n(cid:16)\n\ni,j\n\nN (cid:88)\n\n(cid:16)\n\ni,j\n\nN (cid:88)\n\n(cid:16)\n\ni\n\nx(i)\n\nx(i)\n\nx(i)\n\n−\n\n−\n\n−\n\nm(i)\n\nt\n\n(cid:17)T (cid:0)C −1\n\nt\n\n(cid:1)(ij) (cid:16)\n\nx(j)\n\n(cid:17)\n\nm(j)\n\nt\n\n−\n\n ̄m(t)\n\n(cid:17)T (cid:0) ̄Cdδij + ̄Co(1\n\nδij)(cid:1) (cid:16)\n\nx(j)\n\n(cid:17)\n\n ̄m(t)\n\n−\n\n−\n\n ̄m(t)\n\n(cid:17)T\n\n(cid:16)\n\nx(i)\n\n ̄Cd\n\n(cid:17)T\n\n ̄m(t)\n\n−\n\n+\n\nN (cid:88)\n\ni̸=j\n\n(cid:16)\n\nx(i)\n\n−\n\n(cid:17)T\n\n ̄m(t)\n\n(cid:16)\n\nx(j)\n\n ̄Co\n\n(cid:17)\n\n ̄m(t)\n\n−\n\n(cid:80)N\n\nAbove, we may identify the first term in the last line as (cid:80)N last line as 1\n\ni̸=j U2(x(i), x(j)). Moreover, U2(\n\n, ·\nAnalytical Entropy For this example, the entropy can be computed analytically and compared directly to the learned numerical estimate. By definition,\n\n) is symmetric with respect to its arguments. ·\n\ni=1 U1(x(i)) and the second term in the\n\nN\n\nlog ρt(x)ρt(x)dx,\n\n(cid:18)\n\nN ̄d 2\n\n−\n\nlog(2π)\n\n1 2\n\n−\n\nlog det Ct\n\n1 2\n\n(x\n\n−\n\n−\n\nmt)TC −1\n\nt\n\n(x\n\n−\n\n(cid:19)\n\nmt)\n\nρt(x)dx,\n\nst =\n\n(cid:90)\n\n−\n\nRN ̄d\n\n(cid:90)\n\nRN ̄d\n\n=\n\n=\n\n− N ̄d 2\n\n(log (2π) + 1) +\n\n1 2\n\nlog det Ct.\n\nAdditional figures Images of the learned velocity field and potential in comparison to the corresponding analytical solutions can be found in Figures D.1 and D.2, respectively. Further detail can be found in the corresponding captions. We stress that the two-dimensional images represent single-particle slices of the high-dimensional functions.\n\nD.2 SOFT SPHERES IN AN ANHARMONIC TRAP\n\nNetwork architecture Both potential terms Uθt,1 and Uθt,2 are modeled as four hidden-layer deep fully connected networks with n_hidden = 32 neurons in each layer. The initialization is identical to Appendix D.2.\n\n×\n\nOptimization and initialization The Adam optimizer is used with an initial learning rate of η = 10−3 and otherwise default settings. At time t = 0, the loss (D.1) is minimized to a value less than 5\n10−4 over n samples x0,j 0I) with σ0 = 0.5 and n = 1000, similar to Appendix D.2. After this initial optimization, 100 steps of the SDE (17) are taken in artificial time τ with fixed physical t = 0 to ensure that no spheres are overlapping at initialization. Past this initial stage, the denoising loss is used with a noise scale σ = 0.025. The loss is minimized by taking n_opt_steps = 25 steps of Adam until the norm of the gradient is below gtol = 0.5. The physical timestep is set to ∆t = 10−3.\n\nN(β0, σ2\n\n∼\n\nAdditional figures A depiction of the one-particle potential, estimated as the negative logarithm of the one-particle PDF obtained via kernel density estimation, can be found in Figure D.3 (for further details, see the caption).\n\n22\n\nFigure D.1: A system of N = 50 harmonically interacting particles in a harmonic trap: slices of the high-dimensional velocity field. Cross sections of the velocity field for N = 50 harmonically interacting particles in a moving harmonic trap. Columns depict the learned, analytical, noise-free, and error between the learned and analytical velocity fields, respectively. Rows indicate different time points, corresponding to t = 1.25, 2.5, 3.75, and 5.0, respectively. Each velocity field is plotted as a function of a single particle’s coordinate (denoted as x and y); all other particle coordinates are fixed to be at the location of a sample. Color depicts the magnitude of the velocity field while arrows indicate the direction. Learned, analytical, and noise-free share a colorbar for direct comparison; the error occurs on a different scale and is plotted with its own colorbar. White circles in the error plot indicate samples projected onto the xy plane; locations of low error correlate well with the presence of samples.\n\n23\n\n−202ylearnedanalyticalnoisefreeerror−202y−202y−202x−202y−202x−202x−202x5101524Figure D.2: A system of N = 50 harmonically interacting particles in a harmonic trap: slices of the high-dimensional potential. Cross sections of the potential field Uθt(x) computed via (15). Columns depict the learned, analytical, and error between the learned and analytical, respectively. Rows indicate distinct time points, corresponding to t = 1.25, 2.5, 3.75, and 5.0, respectively. As in Figure D.1, each potential field is plotted as a function of a single particle’s coordinate (denoted as x and y) with other particle coordinates fixed on a sample. All potentials are normalized via an overall shift so that the minimum value is zero. White circles in the error plot indicate samples from the learned system projected onto the xy plane.\n\n24\n\n−202ylearnedanalyticalerror−202y−202y−202x−202y−202x−202x510150.02.55.0Figure D.3: A system of N = 5 soft-sphere particles in an anharmonic trap: one-particle potential. log ρKDE(x) where ρKDE denotes a Cross sections of the one-particle potential field U (x) = kernel density estimate of the one-particle density obtained by pooling all particles and treating them as equivalent two-dimensional samples, shown relative to the moving mean. Columns depict the learned, SDE, and noise free systems, respectively. Purple dots indicate samples from the corresponding system. Rows indicate distinct time points, corresponding to t = 1.25, 2.5, 3.75, and 4.95, respectively. All potentials are normalized via an overall shift so that the minimum value is zero, and are clipped to a maximum value of 15. The learned and SDE potentials match well, while the noise free KDE becomes too peaked and develops a spurious maximum that causes the particles to align in a ring.\n\n−\n\n25\n\n−101ylearnedSDEnoisefree−101y−101y−101x−101y−101x−101x051015Figure D.4: A system of N = 5 soft-sphere particles in an anharmonic trap: moments. All components of the covariance matrix over time for the circular trap motion.\n\n26\n\n0.00.5ΣijSDElearnednoisefree0.00.5Σij0.00.5Σij0.00.5Σij0.00.5Σij0.00.5Σij0.00.5Σij0.00.5Σij0.00.5Σij010time0.00.5Σij010time010time010time010time010time010time010time010time010timeFigure D.5: A system of N = 5 soft-sphere particles in an anharmonic trap: moments. All components of the covariance matrix over time for the linear trap motion.\n\n27\n\n01Σij01ΣijSDElearnednoisefree01Σij01Σij01Σij01Σij01Σij01Σij01Σij010time01Σij010time010time010time010time010time010time010time010time010timeFigure D.6: An active swimmer: sample trajectories. Samples in the xv plane. Columns denote solution type and rows indicate snapshots in time (t = 0, 0.25, 0.5, 3.0, respectively). The learned and SDE systems develop bimodality while the noise free system collapses with time and does not correctly capture the variance.\n\n28\n\n−3−2−10123vlearnedSDEnoisefree−3−2−10123v−3−2−10123v−202x−3−2−10123v−202x−202xD.3 AN ACTIVE SWIMMER\n\nHere, we study an “active swimmer” model that describes the motion of a particle in an anharmonic trap with a preference to travel in a noisy direction. The system is two-dimensional, and is given by the stochastic differential equation for the position x and velocity v\n\ndx = (cid:0)\n\nx3 + v(cid:1) dt,\n\n− γvdt +\n\n(cid:112)\n\n2γDdWt.\n\ndv =\n\n−\n\n(D.6)\n\nDespite its low-dimensionality, (D.6) exhibits convergence to a non-equilibrium statistical steady state in which the probability current jt(x) = vt(x)ρt(x) is non-zero.\n\nSetup We set γ = 0.1 and D = 1.0. Because noise only enters the system through the velocity variable v in (D.6), the score can be taken to be one-dimensional. This is equivalent to learning the score only in the range of the rank-deficient diffusion matrix. We parameterize the score directly st : R2 R using a three-hidden layer neural network with n_hidden = 32 neurons per hidden layer.\n\n→\n\nOptimization and initialization The network initialization is identical to the previous two experiments. The physical timestep is set to ∆t = 10−3. The Adam optimizer is used with an initial learning rate of η = 10−4. At time t = 0 the loss (D.1) is minimized to a tolerance of 10−4 over n = 5000 samples drawn from an initial distribution N(0, σ2 0I) with σ0 = 1. The denoising loss is used with a noise scale σ = 0.05, using n_opt_steps = 25 steps of Adam until the norm of the gradient is below gtol = 0.5.\n\nResults Depictions of the sample trajectories i=1 in phase space are shown in Figure D.6. The trajectories demonstrate that the distribution of the learned samples qualitatively matches the distribution of the SDE samples. The noise-free system grows increasingly and overly compressed with time. The learned velocity field effectively captures a non-zero rotational steady-state current that qualitatively matches the current of the SDE but enjoys more interpretable sample trajectories.\n\nxi(t), vi(t)\n\n{\n\n}\n\nn\n\nA movie of the motion of the samples (xi, vi) in phase space can be seen here. The movie highlights convergence of the learned solution to a non-zero steady-state probability current that qualitatively matches that of the SDE. By contrast, the noise-free system becomes increasingly concentrated with time, failing to accurately capture the current. Figure D.7 depicts the learned velocity field vt(x) = bt(x) Dst(x). The figure highlights the structure of the steady-state current, which contains an elliptical region with closed orbits. The elliptical region remains roughly fixed in size as time proceeds, while the orbits of the noise-free system in Figure D.8 become increasing compressed. Kernel density estimation demonstrates that an estimated PDF for the samples of learned solution qualitatively matches that of the SDE (Figure D.9).\n\n−\n\n29\n\nFigure D.7: An active swimmer: learned velocity. The learned velocity field (right-hand side of (5)) for the active swimmer example. Color indicates the magnitude of the velocity field computed on a grid, while arrows indicate the direction of the velocity field on samples. Time corresponds to progressing in the grid along columns from the top-left to the bottom-right image (t = k .75 with k the image number, zero-indexed). The learned velocity field converges to closed streamlines that enforce a nonzero steady-state current.\n\n×\n\n30\n\n−2−1012−2−1012−2−1012−2−1012−2−1012−2−1012x2.55.07.5Figure D.8: An active swimmer: noise free velocity. Noise free velocity field. As in Figure D.7, color indicates the magnitude of the velocity field while arrows indicate the direction, and time corresponds to progressing in the grid along columns from the top-left to the bottom-right image (t = k .75 with k the image number, zero-indexed). The velocity field in the noise-free case incorrectly pushes the swimmers to lie along a thin band.\n\n×\n\n31\n\n−2−1012−2−1012−2−1012−2−1012−2−1012−2−1012x0.51.0Figure D.9: An active swimmer: density. PDFs computed via kernel density estimation in the xv plane. Columns denote solution type and rows denote snapshots in time (t = 0, 0.5, 1.5, 6.0, respectively). Similar to the samples presented in Figure D.6, the KDE reveals bimodality in the probability density due to the presence of the particle velocity field. The noise free system becomes too concentrated and does not accurately capture the shape of the SDE and learned solutions, while the SDE and learned solutions are nearly identical.\n\n32\n\n−202vlearnedSDEnoisefree−202v−202v−202x−202v−202x−202x0.00.20.40.6",
    "reference": "# Summary Of The Paper\n\nIn high dimensional setting, it is typically impossible to approximate the solution $\\rho_t$ to a Fokker Planck Equation (FPE) with conventional grid-based methods. A  standard method consists in considering the associated SDE a simulate many trajectories from this SDE to collect statistics of the solution at any future time $t$. This methods allows to compute any moment $\\int \\mathcal{O}(x) \\rho_t(dx)$ of the solution (eg. Monte-Carlo method), but the evaluation of $\\rho_t(x)$ is typically not available with this basic approach.\n\nThe article proposes to express the solution of the FPE as a transport equation $\\partial_t \\rho_t = -\\nabla \\cdot (v_t \\rho_t)$ with a velocity field $v_t$ that is expressed as a function of the score $s_t$ of $\\rho_t$ itself, $s_t = \\nabla \\rho_t$. This motivates the following particle approach:\n\n1. start from $N$ samples $x_1, \\ldots, x_N$ samples from $\\rho_0$\n2. use (some variant of) score matching to evaluate the score of $\\rho_t$ and construct the velocity field $v_t$\n3. push forward the particles through the transport equation to get a particle approximation of $\\rho_{t + \\Delta t}$\n4. iterate: go back to 2\n\nThe advantage of this approach is that, because $\\rho_t$ is expressed as transport equation, is is straightforward to evaluate $\\rho_t$ along trajectories of the transported particles (contrarily to the SDE approach).\n\n# Strength And Weaknesses\n\n**Strength:**\nThe method is very intuitive and appears to work well on simulated examples.\n\n**weakness:**\n1. the proposed scheme does not seem to very stable in the sense that errors in estimating the score may amplify as the particles are propagated. In some sense, the vanilla approach consisting in simulating forward the associated SDE appears to be much more stable in that respect. Is this true? Can the authors discuss and possibly illustrate this empirically.\n\n2. I think that the simulation should take compute-time into account. The vanilla Monte-Carlo approach (ie. simulate forward the SDE) is extremely scalable. The proposed approach requires implementing score-matching at each step.\n\n3. Evaluating $\\rho_t$ from samples is basically a density estimation problem. I think the authors should consequently consider comparing their method to Vanilla-Monte-Carlo + density estimation (eg. normalising flow, or something of that sort). Indeed, there are also quite a few ways to estimate differential entropies, and mutual information, from samples!\n\n4. The network architectures are exploiting quite a lot the structure of the problem. One may argue that this structure could also be used to estimate the density out of samples much more efficiently than a completely problem-agnostic approach ... \n\n**minor question**:\n1. is it $-\\alpha$ in Equation (11) or $+\\alpha$\n\n# Clarity, Quality, Novelty And Reproducibility\n\n## Clarity\nThe text is clearly written, easy to follow, with appropriate references to the literature.\n\n## Quality\nI think that quite a bit of work should be done to do comparison with reasonable baselines (i.e. take compute time into account and compare to density-estimation procedures exploiting the same problem-structure)\n\n## Novelty\nExcept the related work of (Maoutsa et al) that exploit similar ideas but in lowish dimension, the proposed methodology appears to be new.\n\n## Reproducibility\nall good\n\n# Summary Of The Review\n\nThe proposed method is natural and very interesting. I would like to encourage the authors to work on their simulations in order to more convincingly demonstrate the appeal of the approach when compared to more standard methodologies.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable\n\n# Details Of Ethics Concerns\n\nNA"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCOMPRESSED PREDICTIVE INFORMATION CODING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nUnsupervised learning plays an important role in many fields, such as machine learning, data compression, and neuroscience. Compared to static data, methods for extracting low-dimensional structure for dynamic data are lagging. We developed a novel information-theoretic framework, Compressed Predictive Information Coding (CPIC), to extract predictive latent representations from dynamic data. Predictive information quantifies the ability to predict the future of a time series from its past. CPIC selectively projects the past (input) into a low dimensional space that is predictive about the compressed data projected from the future (output). The key insight of our framework is to learn representations by balancing the minimization of compression complexity with maximization of the predictive information in the latent space. We derive tractable variational bounds of the CPIC loss by leveraging bounds on mutual information. The CPIC loss induces the latent space to capture information that is maximally predictive of the future of the data from the past. We demonstrate that introducing stochasticity in the encoder and maximizing the predictive information in latent space contributes to learning more robust latent representations. Furthermore, our variational approaches perform better in mutual information estimation compared with estimates under the Gaussian assumption commonly used. We show numerically in synthetic data that CPIC can recover dynamical systems embedded in noisy observation data with low signal-to-noise ratio. Finally, we demonstrate that CPIC extracts features more predictive of forecasting exogenous variables as well as auto-forecasting in various real datasets compared with other state-of-the-art representation learning models. Together, these results indicate that CPIC will be broadly useful for extracting low-dimensional dynamic structure from high-dimensional, noisy timeseries data.\n\n1\n\nINTRODUCTION\n\nUnsupervised methods play an important role in learning representations that provide insight into data and exploit unlabeled data to improve performance in downstream tasks in diverse application areas Bengio et al. (2013); Chen et al. (2020); Grill et al. (2020); Devlin et al. (2018); Brown et al. (2020); Baevski et al. (2020); Wang et al. (2020). Prior work on unsupervised representation learning can be broadly categorized into generative models such as variational autoencoders(VAEs) (Kingma & Welling, 2013) and generative adversarial networks (GAN) (Goodfellow et al., 2014), discriminative models such as dynamical components analysis (DCA) (Clark et al., 2019), contrastive predictive coding (CPC) (Oord et al., 2018), and deep autoencoding predictive components (DAPC) (Bai et al., 2020). Generative models focus on capturing the joint distribution between representations and inputs, but are usually computationally expensive. On the other hand, discriminative models emphasize capturing the dependence of data structure in the low-dimensional latent space, and are therefore easier to scale to large datasets.\n\nIn the case of time series, some representation learning models take advantage of an estimate of mutual information between encoded past (input) and the future (output) (Creutzig & Sprekeler, 2008; Creutzig et al., 2009; Oord et al., 2018). Although previous models utilizing mutual information extract low-dimensional representations, they tend to be sensitive to noise in the observational space. DCA directly makes use of the mutual information between the past and the future (i.e., the predictive information (Bialek et al., 2001)) in a latent representational space that is a linear embedding of the observation data. However, DCA operates under Gaussian assumptions for mutual information\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nestimation. We propose a novel representation learning framework which is not only robust to noise in the observation space but also alleviates the Gaussian assumption and is thus more flexible.\n\nWe formalize our problem in terms of data generated from a stationary dynamical system and propose an information-theoretic objective function for Compressed Predictive Information Coding (CPIC). Instead of leveraging the information bottleneck (IB) objective directly as in Creutzig & Sprekeler (2008) and Creutzig et al. (2009), where the past latent representation is directly used to predict future observations, we predict the compressed future observations filtered by the encoder. It is because that in the time series setting, future observations are noisy, and treating them as labels is not insightful. Specifically, our target is to extract latent representation which can better predict future underlying dynamics. Since the compressed future observations are assumed to only retain the underlying dynamics, better compression thus contributes to extracting better dynamical representation. In addition, inspired by Clark et al. (2019) and Bai et al. (2020), we extend the prediction from single input to a window of inputs to handle high order predictive information.\n\nMoreover, instead of directly estimating the objective information with Gaussian assumption (Creutzig & Sprekeler, 2008; Creutzig et al., 2009; Clark et al., 2019; Bai et al., 2020), we developed variational bounds and a tractable end-to-end training framework based on the neural estimator of mutual information studied in Poole et al. (2019). Note that our inference first leverages the variational boundary technique for self-supervised learning on the time series data. Since it alleviates the Gaussian assumption, it is applicable to a much larger class of dynamical systems.\n\nIn CPIC, we also demonstrate that introducing stochasticity into either a linear or nonlinear enIn particular, coder robustly contributes to numerically better representations in different tasks. we illustrate that CPIC can recover trajectories of a chaotic dynamical system embedded in highdimensional noisy observations with low signal-to-noise ratios in synthetic data. Furthermore, we conduct numerical experiments on four real-world datasets with different goals. In two neuroscience datasets, monkey motor cortex (M1) and rat dorsal hippocampus (HC), compared with the state-ofthe-art methods, we show that the latent representations extracted from CPIC have better forecasting accuracy for the exogenous variables of the monkey’s future hand position for M1, and for the rat’s future position for HC. In two other real datasets, historical hourly weather temperature data (TEMP) and motion sensor data (MS), we show that latent representations extracted by CPIC have better forecasting accuracy of the future of those time series than other methods. In summary, the primary contributions of our paper are as follows:\n\n• We developed a novel information-theoretic self-supervised learning framework, Compressed Predictive Information Coding (CPIC), which extracts low-dimensional latent representation from time series. CPIC maximizes the predictive information in the latent space while minimizing the compression complexity.\n\n• We introduced the stochastic encoder structure where we encode inputs into stochastic\n\nrepresentations to handle uncertainty and contribute to better representations.\n\n• Based on prior works, we derived the variational bounds of the CPIC’s objective function and a tractable, end-to-end training procedure. Since our inference alleviates the Gaussian assumption common to other methods, it is applicable to a much larger class of dynamical systems. Moreover, to the best of our knowledge, our inference is the first to leverage the variational boundary technique for self-supervised learning on time series data.\n\n• We demonstrated that, compared with the other unsupervised based methods, CPIC more robustly recovers latent dynamics in dynamical system with low signal-to-noise ratio in synthetic experiments, and extracts more predictive features for downstream tasks in various real datasets.\n\n2 RELATED WORK\n\nMutual information (MI) plays an important role in estimating the relationship between pairs of variables. It is a reparameterization-invariant measure of dependency:\n\nI(X, Y ) = Ep(x,y)\n\n(cid:20)\n\nlog\n\n(cid:21)\n\np(x|y) p(x)\n\n(1)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nIt is used in computational neuroscience (Dimitrov et al., 2011), visual representation learning (Chen et al., 2020), natural language processing (Oord et al., 2018) and bioinformatics (Lachmann et al., 2016). In representation learning, the mutual information between inputs and representations is used to quantify the quality of the representation and is also closely related to reconstruction error in generative models (Kingma & Welling, 2013; Makhzani et al., 2015). Estimating mutual information is computationally and statistically challenging except in two cases: discrete data, as in Tishby et al. (2000) and Gaussian data, as in Chechik et al. (2005). However, these assumptions both severely constrain the class of learnable models (Alemi et al., 2016). Recent works leverage deep learning models to obtain both differentiable and scalable MI estimation (Belghazi et al., 2018; Nguyen et al., 2010; Oord et al., 2018; Alemi et al., 2016; Poole et al., 2019; Cheng et al., 2020).\n\nIn terms of representation learning in time series, Wiskott & Sejnowski (2002); Turner & Sahani (2007) targeted slowly varying features, Creutzig & Sprekeler (2008) utilized the information bottleneck (IB) method (Tishby et al., 2000) and developed an information-theoretic objective function. Creutzig et al. (2009) proposed an alternative objective function based on a specific state-space model. Recently, Oord et al. (2018) proposed CPC to extract dynamic information based on an autoregressive model on representations and contrastive loss on predictions. Clark et al. (2019); Bai et al. (2020) proposed unsupervised learning approach to extract low-dimensional representation with maximal predictive information(PI). All of the above unsupervised representation learning models, except for CPC, assume the data to be Gaussian, which may be not realistic, especially when applied to neuroscience datasets (O’Doherty et al., 2017; Glaser et al., 2020), given the nonGaussianity of neuronal activity. Here, we leverage recently introduced neural estimation of mutual information to construct upper bounds of the CPIC objective and develop an end-to-end training procedure. CPIC enables generalization beyond the Gaussian case and autoregressive models.\n\nRecently, deep encoder networks are leveraged to model nonlinear relations between latent representations and observed data in time series (Chen et al., 2020; Bai et al., 2020; He et al., 2020). However, use of complicated nonlinear encoders induced hinders computational efficiency (Wang et al., 2019). CPIC proposes an efficient representation learning framework for time series that encodes data with maximal predictive information. We also note that there exists several works on the time series modeling from generative modeling perspective. Initially, Fabius & Van Amersfoort (2014) leveraged the recurrent neural network with variational autoencoder to model time series data. Frigola et al. (2014) proposed variational Gaussian-process state-space model. Meng et al. (2021) proposed variational structured Gaussian-process regression network which can efficiently handle more complicated relationships in time series. Most generative modeling inference would depend on the length of time series, while the inference of CPIC depends on the window size T , which is more scalable for long time series.\n\n3 COMPRESSED PREDICTIVE INFORMATION CODING\n\nThe main intuition behind Compressed Predictive Information Coding (CPIC) is to extract low dimensional representations with minimal compression complexity and maximal dynamical structure. Specifically, CPIC first discards low-level information that is not relevant for dynamic prediction and noise that is more local by minimizing compression complexity (i.e., mutual information) between inputs and representations to improve model generalization. Second, CPIC maximizes the predictive information in the latent space of compressed representations.\n\nCompared with Clark et al. (2019); Bai et al. (2020), CPIC first utilizes stochastic encoder to handle uncertainty of representations, which contributes to more robust representations, and also relieves the Gaussian assumption by constructing bounds of mutual information based on neural estimations. In more detail, instead of employing a deterministic linear mapping function as the encoder to compress data as in Clark et al. (2019), CPIC takes advantage of a stochastic linear or nonlinear mapping function. Given inputs, the stochastic representation follows Gaussian distributions, with means and variances encoded from any neural network structure. A nonlinear CPIC utilizes a stochastic nonlinear encoder which is composed of a nonlinear mean encoder and a linear variance encoder, while a linear CPIC utilizes a stochastic linear encoder which is composed of a linear mean encoder and a linear variance encoder. Note that stochastic representations conditioned on inputs are parameterized as a conditional Gaussian distribution, but the marginal distribution of the representation is a mixture of Gaussian distribution, which is widely recognized as universal approximator of densities.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nOn the other hand, avoiding the Gaussian assumption on mutual information (Creutzig & Sprekeler, 2008; Creutzig et al., 2009; Clark et al., 2019; Bai et al., 2020), CPIC leverages neural estimations of mutual information. Specifically, we propose differentiable and scalable bounds of the CPIC objective via variational inference, which enables end-to-end training.\n\nlet X = {xt}, xt ∈ RN be a stationary, discrete time series, and let Xpast = Formally, (x−T +1, . . . , x0) and Xfuture = (x1, . . . , xT ) denote consecutive past and future windows of length T. Then both past and future data are compressed into past and future representations denoted as Ypast = (y−T +1, . . . , y0) and Yfuture = (y1, . . . , yT ) with embedding dimension size Q. Similar to the information bottleneck (IB) (Tishby et al., 2000), the CPIC objective contains a trade-off between two factors. The first seeks to minimize the compression complexity and the second to maximize the predictive information in the latent (representation) space. Note that when the encoder is deterministic the compression complexity is deprecated and when the encoder is stochastic the complexity is measured by the mutual information between representations and inputs. In the CPIC objective, the trade-off weight β > 0 dictates the balance between the compression and predictive information terms:\n\nmin ψ\n\nL, where L ≡ β(I(Xpast; Ypast) + I(Xfuture; Yfuture)) − I(Ypast; Yfuture)\n\n(2)\n\nwhere ψ refer to the model parameters which encode inputs X to latent variables Y . Larger β promotes a more compact mapping and thus benefits model generalization, while smaller β leads to more predictive information in the latent space on training data. This objective function is visualized in Figure 1, where inputs X are encoded into latent space as Y via tractable encoders and the dynamics of Y are learned in a model-free manner.\n\nThe encoder p(Y |X) could be implemented by fitting deep neural networks (Alemi et al., 2016) to encode data X. Instead, CPIC takes an approach similar to VAEs (Kingma & Welling, 2013), in that it encodes data into stochastic representations. In particular, CPIC employs a stochastic encoder (genc in Figure 1) to compress input xt into yt as\n\nyt|xt ∼ N (μt, diag(σ2\n\nt )) ,\n\n(3)\n\nμ\n\nμ\n\n(xt).\n\nfor each time stamp t. The mean of yt is given by μt = gEncoder (xt), whereas the variance arises from σt = gEncoder\n\nσ Encoders gEncoder and gEncoder can be any nonσ linear mapping and is usually modeled using neural network architectures. We use a twolayer perceptron with ReLU activation function (Agarap, 2018) for a nonlinear mapping. In terms of a linear CPIC, we specify the mean of representation as μt = uT xt. In both linear and nonlinear CPIC setting, if σt = 0, the stochastic encoder reduces to a deterministic encoder.\n\nFigure 1: The overall framework of compressed predictive information coding. The encoder compress information of the input X into Y such that the predictive information between Ypast and Yfuture is maximized while minimizing the mutual information between X and Y .\n\nWe extend single input to multiple inputs in the CPIC framework in terms of a specified window size T . The selection of window size is discussed in Appendix A. Due to the stationary assumption, the relation between past/future blocks of input data X(−T ), X(T ) ∈ RN ×T and encoded data Y (−T ), Y (T ) ∈ RQ×T are equivalent, pX(−T ),Y (−T ) = pX(T ),Y (T ). Note that −T and T indexes to past and future the compression relation can be expressed as Y (T ) = T data. Without loss of generality, 1), . . . , diag(σ2 gEncoder T )) and noise standard deviation σt = gEncoder\n\n(X(T )) + ξ(T ), where ξ(T ) ∈ N (0, blockdiag(diag(σ2\n\n(xt).\n\nμ\n\nσ\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n4 VARIATIONAL BOUNDS OF COMPRESSED PREDICTIVE INFORMATION\n\nCODING\n\nIn CPIC, since data X are stationary, the mutual information between the input data and the compressed data for the past is equivalent to that for the future I(X(−T ); Y (−T )) = I(X(T ); Y (T )). Therefore, the objective of CPIC can be rewritten as\n\nmin L = βI(X(T ); Y (T )) − I(Y (−T ); Y (T )) .\n\n(4)\n\nWe developed the variational upper bounds on mutual information for the compression complexity I(X(T ); Y (T )) and lower bounds on mutual information for the predictive information I(Y (−T ); Y (T )).\n\n4.1 UPPER BOUNDS OF COMPRESSION COMPLEXITY\n\nIn the section, we derived a tractable variational upper bound (VUB) depending on a single sample and a leave-one-out upper bound (L1Out) (Poole et al., 2019) depending on multiple samples.\n\nTheorem 1 By introducing a variational approximation r(y(T )) to the marginal distribution p(y(T )), a tractable variational upper bound of mutual information I(X(T ); Y (T )) is derived as IVUB(X(T ); Y (T )) = EX(T )\n\n(cid:2)KL(p(y(T )|x(T )), r(y(T )))(cid:3).\n\nTheorem 2 By utilizing a Monte Carlo approximation for variational distribution r(y(T )), the L1Out upper bound of mutual information I(X(T ); Y (T )) is derived as IL1Out(X(T ); Y (T )) = E\n\n, where S is the sample size.\n\np(y(T )i|x(T )i) (cid:80)\n\n(cid:80)S\n\n(cid:104) 1\n\nlog\n\n(cid:105)(cid:105)\n\n(cid:104)\n\nS\n\ni=1\n\n1 S−1\n\nj̸=i p(y(T )i|x(T )j )\n\nThe derivation details are in Appendix B and C. In practice, the L1Out bound depends on the sample size S and may suffer from numerical instability. Thus, we would like to choose the sample size S as large as possible. In general scenarios where p(y(T )|x(T )) is intractable, Cheng et al. (2020) proposed a variational version of VUB and L1Out by using a neural network to approximate the condition distribution p(y(T )|x(T )). Since the conditional distribution p(y(T )|x(T )) is parameterized as a known stochastic/deterministic encoder in CPIC, those variational versions are not taken into consideration.\n\n4.2 LOWER BOUNDS OF PREDICTIVE INFORMATION\n\nFor the predictive information (PI), we derived lower bounds of I(Y (−T ); Y (T )) using results in Agakov (2004); Alemi et al. (2016); Poole et al. (2019). In particular, we derived tractable unnormalized Barber and Agakov (TUBA) (Barber & Agakov, 2003) lower bounds depending on a single sample and an infoNCE lower bound (Oord et al., 2018) depending on multi samples. All derivation details are discussed in Appendix D, E and F.\n\nTheorem 3 We derived a lower bound on predictive information (PI) I(Y(-T); Y(T)) as IV LB(Y (−T ); Y (T )) = H(Y (T )) + Ep(y(−T ),y(T ))[log q(y(T )|y(−T ))], where q(y(T )|y(−T )) is a variational conditional distribution.\n\nthis lower bound requires a tractable decoder\n\nHowever, the conditional distribution q(y(T )|y(−T )) (Alemi et al., 2016). Alternatively we derived a TUBA lower bound (Barber & Agakov, 2003) which is free of the parametrization of decoder.\n\nfor\n\nTheorem 4 By introducing a differentiable critic function f (x, y) and a baseline function a(y(T )) defined in Appendix E, the TUBA lower bound of predictive information is derived as (cid:17) ITUBA(Y (−T ), Y (T )) = Ep(y(−T ),y(T ))[ ̃f (y(−T ), y(T ))]−log where ̃f (y(−T ), y(T )) = f (y(−T ), y(T )) − log(a(y(T ))).\n\nEp(y(−T ))p(y(T ))[e ̃f (y(−T ),y(T ))]\n\n(cid:16)\n\nDifferent forms of the baseline function lead to different neural estimators in the literature such as MINE (Belghazi et al., 2018) and NWJ (Nguyen et al., 2010). On the other hand, all TUBA based\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nestimators have high variance due to the high variance of f (x, y). Oord et al. (2018) proposed a low-variance MI estimator based on noise-contrastive estimation called InfoNCE. Moreover, there exists other differentiable mutual information estimator including SMILE (Song & Ermon, 2019) and Echo noise estimator (Brekelmans et al., 2019).\n\nTheorem 5 In the CPIC setting, the InfoNCE lower bound of predictive information is derived as\n\nIinfoNCE(Y (−T ); Y (T )) = E\n\n(cid:34)\n\n1 S\n\nS (cid:88)\n\ni=1\n\nlog\n\nef (y(−T )i,y(T )i) j=1 ef (y(−T )i,y(T )j )\n\n(cid:80)S\n\n1 S\n\n(cid:35)\n\n(5)\n\nexpectation\n\nThe distribution: p(y(−T ), y(T )) following Markov Chain rule in Figure 1 such as p(y((−T ), y(T )) = (cid:82) p(x(−T ), x(T ))p(y(−T )|x(−T ))p(y(T )|x(T ))dx(−T )x(T ).\n\nindependent\n\nfrom the\n\nsamples\n\njoint\n\nover\n\nis\n\nS\n\n4.3 VARIATIONAL BOUNDS OF CPIC\n\nWe propose two classes of upper bounds of CPIC based on whether the bounds depend on a single sample or multiple samples. According to the uni-sample and multi-sample bounds derived in Section 4.1 and Section 4.2, we name the first class as uni-sample upper bounds, which take the VUB upper bound of mutual information for the complexity of data compression I(X(T ), Y (T )) and the TUBA as the lower bound of predictive information in equation 14. Thus we have\n\nLUNI = βKL(p(y(T )|x(T )), r(y(T ))) − ITUBA(Y (−T ), Y (T )) .\n\n(6)\n\nNotice that by choosing different baseline functions, the TUBA lower bound would be equivalent to different mutual information estimator such as MINE and NWJ. The second class is named as multi-sample upper bound, which take advantage of the noise-contrastive estimation approach. The multi-sample upper bound is expressed as\n\nLMUL = βIL1Out(X(T ); Y (T )) − IinfoNCE(Y (−T ); Y (T )) .\n\n(7)\n\nTwo main differences exist between these classes of upper bounds. First, the performance of multisample upper bound depend on batch size while uni-sample upper bounds do not, so when computational budgets do not allow large batch size in training, uni-sample upper bounds may be preferred in training. Secondly, multi-sample upper bound has lower variance than uni-sample upper bounds. Thus, they have different strengths and weaknesses depending on the context. We evaluated the performance of those variational bounds of CPIC in terms of the reconstruction performance in synthetic experiments in Appendix G, and find that with sufficiently large batch size, the multi-sample upper bound would outperform most of the uni-sample upper bounds. Thus, without further specification, we choose the multi-sample upper bound as the variational bounds of CPIC objective in this work. Furthermore, we classify the upper bounds into stochastic and deterministic versions by whether we employ a deterministic or stochastic encoder. Notice that when choosing the deterministic encoder, the compression complexity term (first term) in equation 6 and equation 7 are constant.\n\n5 NUMERICAL EXPERIMENTS\n\nIn this section, we demonstrate the superior performance of CPIC in both synthetic and real data experiments. We first examine the reconstruction performance of CPIC in noisy observations of a dynamical system (the Lorenz Attractor). The results show CPIC better recovers the latent trajectories from noisy high dimensional observations. Moreover, we demonstrate that maximizing the predictive information(PI) in the compressed latent space is more effective than maximizing PI between latent and observation space as in Creutzig & Sprekeler (2008); Creutzig et al. (2009), and also demonstrate the benefits of the stochastic representation over the deterministic representation. Secondly, we demonstrate better predictive performance of the representation evaluated by linear forecasting. The motivation for using linear forecasting models is that good representations contribute to disentangling complex data in a linearly accessible way (Clark et al., 2019). Specifically, we extract latent representations and then conduct forecasting tasks given the inferred representations\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\non two neuroscience datasets and two other real datasets. The two neuroscience datasets are multineuronal recordings from the hippocampus (HC) while rats navigate a maze (Glaser et al., 2020) and multi-neuronal recordings from primary motor cortex (M1) during a reaching task for monkeys (O’Doherty et al., 2017). The two other real datasets are multi-city temperature data (TEMP) from 30 cities over several years (Gene, 2017) and 12 variables from an accelerater, gyroscope, and gravity motion sensor (MS) recording human kinematics (Malekzadeh et al., 2018). The forecasting tasks for the neuroscience data sets is to predict the future of the relevant exogenous variables from the past neural data, while the forecasting task for the other datasets is to predict the future of those time-series from their past. The results illustrate that CPIC has better predictive performance on these forecasting tasks compared with existing methods.\n\nFigure 2: Left panel. Top: 3D trajectories of lorenz attractor’s ground-truth. Middle: 30D projected trajectory. Bottom: Corrupted 30D trajectory with SNR=0.001. Right Panel. 3D trajectories obtained by deterministic CPIC and stochastic CPIC with PI in latent space or between latent and observation space in terms of different SNRs (0.1, 0.008, 0.001). We refer (L) to the case with PI in latent space and (O) to the case with PI between latent and observation space. We encode the point-wise Euclidean distance between the aligned inferred latent dynamics and the true dynamics into color on trajectories. Color from blue to red corresponds to the distance from short to long respectively. Separate colorbars are used for their corresponding SNRs.\n\n5.1 SYNTHETIC EXPERIMENT WITH NOISY LORENZ ATTRACTOR\n\nThe Lorenz attractor is a 3D time series that are realizations of the Lorenz dynamical system (Pchelintsev, 2014). It describes a three dimensional flow generated as:\n\ndx dt\n\n= σ(y − x),\n\ndy dt\n\n= f1(ρ − z) − y,\n\ndz dt\n\n= xy − γz .\n\n(8)\n\nLorenz sets the values σ = 10, ρ = 8/3 and γ = 28 to exhibit chaotic behavior, as done in recent works (She & Wu, 2020; Clark et al., 2019; Zhao & Park, 2017; Linderman et al., 2017). We simulated the trajectories from the Lorenz dynamical system and show them in the left-top panel in Figure 2. We then mapped the 3D latent signals to 30D lifted observations with a random linear embedding in the left-middle panel and add spatially anisotropic Gaussian noise on the 30D lifted observations in the left-bottom panel. The noises are generated according to different signal-to-noise ratios (SNRs), where SNR is defined by the ratio of the variance of the first principle components of dynamics and noise as in Clark et al. (2019). Specifically, we utilized 10 different SNR levels spaced evenly on a log (base 10) scale between [-3, -1] and corrupt the 30D lifted observations with noise corresponding to different SNR levels. Details of the simulation are available in Appendix G Finally,\n\n7\n\nSNR=10!\"SNR=10!#SNR=10!$DetermisticCPIC(O)DeterministicCPIC(L)Stochastic CPIC(O)StochasticCPIC(L)R#=0.928R#=0.914R#=0.943R#=0.963R#=0.527R#=0.487R#=0.869R#=0.932R#=0.050R#=0.464R#=0.036R#=0.413Under review as a conference paper at ICLR 2023\n\nwe deploy different variants of CPICs to recover the true 3D dynamics from different corrupted 30D lifted observations with different SNR levels, and compare the accuracy of recovering the underlying Lorenz attractor time-series.\n\nWe aligned the inferred latent trajectory with the true 3D dynamics with optimal linear mapping due to the reparameterization-invariant measure of latent trajectories. We validated the reconstruction performance based on the R2 regression score of the extracted vs. true trajectories. We first compare the reconstruction performance on different variational bounds of CPIC with the latent dimension size Q = 3 and the time window size T = 4, and find that multi-sample upper bound outperforms uni-sample upper bounds for almost all of the 10 SNR levels. Thus, we recommend the multi-sample upper bound for CPIC in practice and use that for further results. We also find that, compared to DCA (Clark et al., 2019) and CPC (Oord et al., 2018) CPIC is more robust to noise and thus better extracts the true latent trajectory from the noisy high dimensional observations. The detailed results are reported in Appendix H\n\nIn order to demonstrate the benefits of introducing stochasticity in the encoder and maximizing the predictive information in latent space, we considered four variants of CPICs: with stochastic or deterministic encoder, and with predictive information in latent space or between latent and observation space. All four variants of CPIC models utilize the latent dimension size Q = 3 and the time window size T = 4. For each model and each SNR level, we run 100 replicates with random initializations. We show the aligned latent trajectories inferred from corrupted lifted observation for high, intermediate and low SNR (0.001, 0.01, 0.1) levels of noise with the median R2 scores across 100 replicates in Figure 2. The point-wise distances between the recovered dynamics and the ground-truth dynamics are encoded in the colors from blue to red, corresponding to short to long distance. For high SNR (SNR = 0.1, topright), all models did a good job of recovering the Lorenz dynamics though the stochastic CPIC with predictive information on latent space had larger R2 than others. For intermediate SNR (SNR = 0.008, middle-right), we see that stochastic CPICs performs much better than the deterministic CPICs. Finally, as the SNR gets lower (SNR = 0.001, bottom-right) all methods perform poorly, but we note that, numerically, considering predictive information in latent space is much better than that between latent and observation space.\n\nFigure 3: Comparison on R2 scores of latent dynamics regression on 10 SNR levels for four variants of CPIC. The first row shows the mean performance of R2 scores over running N=10/100 different random initializations and the second row shows the best performance over running N=10/100 different random initializations.\n\nTo more thoroughly characterize the benefits of stochastic encoding and PI in the latent space, we examined the mean of R2 scores for the four variants on each level of SNR across N = 10 and N = 100 replicates in the top row of Figure 3. It shows that the CPIC with stochastic representations and PI in latent space robustly outperforms other variants on average. We also report the best R2 scores for the four variants in the sense that we report the R2 score for the model with the smallest training loss across N runs. The bottom row of Figure 3 shows that CPIC with stochastic representation and PI in latent space achieves better reconstruction and robustness to noise than other variants, especially when the number of runs N is small. Even when N is large, stochastic CPIC with PI in latent space greatly outperforms others when the noise level is high. We note that in the case of high-dimensional noisy observations with large numbers of samples common in many modern real-world time series datasets, CPICs robustness to noise and capacity to achieve good results in a small number of runs is a clear advantage. Moveover, we displayed the quantile anaylsis of the R2 scores in Appendix I with consistent result.\n\n8\n\nBestperformanceMean performanceN=10N=100Under review as a conference paper at ICLR 2023\n\n5.2 REAL EXPERIMENTS WITH DIVERSE FORECASTING TASKS\n\nIn this section, we show that latent representations extracted by stochastic CPIC perform better in the downstream forecasting tasks on four real datasets. We compared stochastic CPIC with contrastive predictive coding (CPC) (Oord et al., 2018), PCA, SFA (Wiskott & Sejnowski, 2002), DCA (Clark et al., 2019) and deterministic CPIC. As for CPC, we use a linear encoder for fair comparison. In addition, we compared the result from CPCs and CPICs with nonlinear encoder in which the linear mean encoder is replaced by a multi-layer perceptron. For each model, we extract the latent representations (conditional mean) and conduct prediction tasks on the relevant exogenous variable at a future time step for the neural datasets. For example, for the M1 dataset, we extract a consecutive 3-length window representation of multi-neuronal spiking activity to predict the monkey’s arm position in a future time step which is lag time stamps away. The details of experiments are available in Appendix J. Neuroscientists often want to interpret latent representations of data to gain insight into the processes that generate the observed data. Thus, we used linear regression 1 to predict exogenous variables, with the intuition that a simple (i.e., linear) prediction model will only be sensitive to the structure in the data that is easiest to interpret as in (Yu et al., 2008; Pandarinath et al., 2018; Clark et al., 2019). Furthermore, the neuroscience data sets (M1 and HC) present extremely challenging settings for prediction of the exogenous variables due to severe experimental undersampling of neurons due to technical limitations, as well as sizeable noise magnitudes. For these tasks, R2 regression score is used as the evaluation metric to measure the forecasting performance. Four datasets are split into 4:1 train and test data and the forecasting task considered three different lag values (5, 10, and 15). For DCA and deterministic/stochastic CPICs, we took three different window sizes T = 1, 2, 3 and report the best R2 scores. Table 1 reports all R2 scores and demonstrates that our stochastic CPIC outperforms all other models except for the case for Temp data with forecasting at lag 15.\n\nTable 1: Comparison between CPC-L (linear encoder), CPC-NL (non-linear encoder), PCA, SFA, DCA, D-CPIC-L (deterministic CPIC with linear encoder), S-CPIC-L (stochastic CPIC with linear encoder), D-CPIC-NL (deterministic CPIC with non-linear encoder), and S-CPIC-NL (stochastic CPIC with non-linear encoder) on R2 regression scores on M1, Hippocampus, Temperature, and Motion sensor datasets with the optimal window size among T ∈ [1, 2, 3] for three different lag values (5, 10, and 15). R2 regression scores are averaged across five folds.\n\nDataset\n\nM1\n\nHC\n\nTemp\n\nMS\n\nLag 5\n10 15 5\n10 15 5\n10 15 5\n10 15\n\nCPC-L 0.041 0.066 0.068 0.025 0.012 -0.002 0.666 0.630 0.624 0.281 0.212 0.182\n\nCPC-NL 0.168 0.180 0.152 0.018 0.012 0.002 0.639 0.584 0.529 0.184 0.154 0.136\n\nPCA 0.135 0.157 0.145 0.007 0.001 -0.005 0.651 0.615 0.581 0.107 0.068 0.044\n\nSFA 0.203 0.223 0.199 0.112 0.101 0.085 0.669 0.630 0.623 -0.051 -0.107 -0.131\n\nDCA 0.215 0.226 0.200 0.113 0.101 0.085 0.668 0.632 0.622 0.443 0.377 0.342\n\nD-CPIC-L 0.222 0.234 0.202 0.120 0.107 0.091 0.672 0.629 0.620 0.247 0.177 0.161\n\nS-CPIC-L 0.223 0.235 0.203 0.127 0.113 0.095 0.673 0.633 0.621 0.457 0.385 0.358\n\nD-CPIC-NL 0.232 0.249 0.226 0.145 0.121 0.094 0.673 0.630 0.621 0.290 0.243 0.216\n\nS-CPIC-NL 0.264 0.291 0.252 0.150 0.133 0.114 0.673 0.634 0.621 0.483 0.425 0.379\n\n6 CONCLUDING REMARKS\n\nWe developed a novel information-theoretic framework, Compressed Predictive Information Coding, to extract representations in sequential data. CPIC balances the maximization of the predictive information in latent space with the minimization of the compression complexity of the latent representation. We leveraged stochastic representations by employing a stochastic encoder and developed variational bounds of the CPIC objective function. We demonstrated that CPIC extracts more accurate low-dimensional latent dynamics and more useful representations that have better forecasting performance in diverse downstream tasks in four real-world datasets. Together, these results indicate that CPIC will yield similar improvements in other real-world scenarios. Moreover, we note that in most real datasets, using nonlinear CPIC would lead to better representation in terms of prediction performance than linear CPIC.\n\n1https://scikit-learn.org/stable/modules/linear model.html\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDavid Barber Felix Agakov. The im algorithm: a variational approach to information maximization.\n\nAdvances in neural information processing systems, 16(320):201, 2004.\n\nAbien Fred Agarap.\n\nDeep learning using rectified linear units (relu).\n\narXiv preprint\n\narXiv:1803.08375, 2018.\n\nAlexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information\n\nbottleneck. arXiv preprint arXiv:1612.00410, 2016.\n\nAlexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. arXiv preprint arXiv:2006.11477, 2020.\n\nJunwen Bai, Weiran Wang, Yingbo Zhou, and Caiming Xiong. Representation learning for sequence data with deep autoencoding predictive components. arXiv preprint arXiv:2010.03135, 2020.\n\nDavid Barber and Felix Agakov. Information maximization in noisy channels: A variational ap-\n\nproach. Advances in Neural Information Processing Systems, 16, 2003.\n\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.\n\nWilliam Bialek, Ilya Nemenman, and Naftali Tishby. Predictability, complexity, and learning. Neu-\n\nral computation, 13(11):2409–2463, 2001.\n\nRob Brekelmans, Daniel Moyer, Aram Galstyan, and Greg Ver Steeg. Exact rate-distortion in au-\n\ntoencoders via echo noise. Advances in neural information processing systems, 32, 2019.\n\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n\nGal Chechik, Amir Globerson, Naftali Tishby, Yair Weiss, and Peter Dayan. Information bottleneck\n\nfor gaussian variables. Journal of machine learning research, 6(1), 2005.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nPengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. Club: A contrastive log-ratio upper bound of mutual information. In International Conference on Machine Learning, pp. 1779–1788. PMLR, 2020.\n\nDavid G Clark, Jesse A Livezey, and Kristofer E Bouchard. Unsupervised discovery of temporal structure in noisy data with dynamical components analysis. arXiv preprint arXiv:1905.09944, 2019.\n\nFelix Creutzig and Henning Sprekeler.\n\nPredictive coding and the slowness principle: An\n\ninformation-theoretic approach. Neural Computation, 20(4):1026–1041, 2008.\n\nFelix Creutzig, Amir Globerson, and Naftali Tishby. Past-future information bottleneck in dynami-\n\ncal systems. Physical Review E, 79(4):041925, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAlexander G Dimitrov, Aurel A Lazar, and Jonathan D Victor. Information theory in neuroscience.\n\nJournal of computational neuroscience, 30(1):1–5, 2011.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMonroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time, i. Communications on Pure and Applied Mathematics, 28(1):1–47, 1975.\n\nOtto Fabius and Joost R Van Amersfoort. Variational recurrent auto-encoders. arXiv preprint\n\narXiv:1412.6581, 2014.\n\nRoger Frigola, Yutian Chen, and Carl Edward Rasmussen. Variational gaussian process state-space\n\nmodels. Advances in neural information processing systems, 27, 2014.\n\nSelfish Gene. Historical hourly weather data. https://www.kaggle.com/datasets/\n\nselfishgene/historical-hourly-weather-data, 2017.\n\nJoshua I Glaser, Ari S Benjamin, Raeed H Chowdhury, Matthew G Perich, Lee E Miller, and Kon-\n\nrad P Kording. Machine learning for neural decoding. Eneuro, 7(4), 2020.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF Conference on\n\nunsupervised visual representation learning. Computer Vision and Pattern Recognition, pp. 9729–9738, 2020.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nAlexander Lachmann, Federico M Giorgi, Gonzalo Lopez, and Andrea Califano. Aracne-ap: gene network reverse engineering through adaptive partitioning inference of mutual information. Bioinformatics, 32(14):2233–2235, 2016.\n\nScott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski. Bayesian learning and inference in recurrent switching linear dynamical systems. In Artificial Intelligence and Statistics, pp. 914–922. PMLR, 2017.\n\nAlireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial\n\nautoencoders. arXiv preprint arXiv:1511.05644, 2015.\n\nMohammad Malekzadeh, Richard G Clegg, Andrea Cavallaro, and Hamed Haddadi. Protecting In Proceedings of the 1st Workshop on Privacy by\n\nsensory data against sensitive inferences. Design in Distributed Systems, pp. 1–6, 2018.\n\nRui Meng, Herbie Lee, and Kristofer Bouchard. Stochastic Collapsed Variational Inference for Structured Gaussian Process Regression Network. arXiv e-prints, art. arXiv:2106.00719, June 2021.\n\nXuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847–5861, 2010.\n\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 271–279, 2016.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nJoseph E O’Doherty, MMB Cardoso, JG Makin, and PN Sabes. Nonhuman primate reaching with multichannel sensorimotor cortex electrophysiology. Zenodo http://doi. org/10.5281/zenodo, 583331, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nChethan Pandarinath, Daniel J O’Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D Stavisky, Jonathan C Kao, Eric M Trautmann, Matthew T Kaufman, Stephen I Ryu, Leigh R Hochberg, et al. Inferring single-trial neural population dynamics using sequential auto-encoders. Nature methods, 15(10):805–815, 2018.\n\nAN Pchelintsev. Numerical and physical modeling of the dynamics of the lorenz system. Numerical\n\nanalysis and Applications, 7(2):159–167, 2014.\n\nBen Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational In International Conference on Machine Learning, pp. 5171–\n\nbounds of mutual information. 5180. PMLR, 2019.\n\nQi She and Anqi Wu. Neural dynamics discovery via gaussian process recurrent neural networks.\n\nIn Uncertainty in Artificial Intelligence, pp. 454–464. PMLR, 2020.\n\nBernard W Silverman. Density estimation for statistics and data analysis. Routledge, 2018.\n\nJiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information\n\nestimators. arXiv preprint arXiv:1910.06222, 2019.\n\nNaftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv\n\npreprint physics/0004057, 2000.\n\nRichard Turner and Maneesh Sahani. A maximum-likelihood interpretation for slow feature analy-\n\nsis. Neural computation, 19(4):1022–1038, 2007.\n\nWeiran Wang, Qingming Tang, and Karen Livescu. Unsupervised pre-training of bidirectional speech encoders via masked reconstruction. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6889–6893. IEEE, 2020.\n\nYihang Wang, Jo ̃ao Marcelo Lamim Ribeiro, and Pratyush Tiwary. Past–future information bottleneck for sampling molecular reaction coordinate simultaneously with thermodynamics and kinetics. Nature communications, 10(1):1–8, 2019.\n\nLaurenz Wiskott and Terrence J Sejnowski. Slow feature analysis: Unsupervised learning of invari-\n\nances. Neural computation, 14(4):715–770, 2002.\n\nByron M Yu, John P Cunningham, Gopal Santhanam, Stephen Ryu, Krishna V Shenoy, and Maneesh Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity. Advances in neural information processing systems, 21, 2008.\n\nYuan Zhao and Il Memming Park. Variational latent gaussian process for recovering single-trial\n\ndynamics from population spike trains. Neural computation, 29(5):1293–1316, 2017.\n\nAPPENDIX\n\nA SELECTION OF WINDOW SIZE\n\nSelecting optimal window size T is important for the downstream use of the dynamics. Poor selection of T may cause aliasing artifacts. In general, we nee to select it by cross validation. Furthermore, we can make plots of the predictive information as a function of both window size T and the embedding dimension Q as diagnostic tools.\n\nB DERIVATION OF IV U B\n\nDirectly estimating the compression complexity is intractable, because I(X(T ); Y (T )) := (cid:2)KL(p(y(T )|x(T )), p(y(T )))(cid:3) in which the population distribution p(y(T )) is unknown. EX(T ) Thus we introduce a variational approximation to the marginal distribution of encoded inputs\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\np(y(T )), denoted as r(y(T )). Due to the non-negativity of the Kullback-Leibler (KL) divergence, the variational upper bound (VUB) is derived as\n\nI(X(T ); Y (T )) = EX(T ) ≤ EX(T )\n\n(cid:2)KL(p(y(T )|x(T )), r(y(T )))(cid:3) − KL(p(y(T )), r(y(T ))) (cid:2)KL(p(y(T )|x(T )), r(y(T )))(cid:3) = IVUB(X(T ); Y (T )) .\n\n(9)\n\nC DERIVATION OF IL1Out\n\nGenerally, learning r(y(T )) was recognised as the distribution density estimation problem (Silverman, 2018), which is challenging. In this setting, the variational distribution r(y(T )) is assumed to be learnable, and thus estimating the variational upper bound is tractable. In particular, Alemi et al. (2016) fixed r(y(T )) as a standard normal distribution, leading to high-bias in MI estimation. Recently, Poole et al. (2019) utilized a Monte Carlo approximation for variational distribution. In our case, with S sample pairs (x(T )i, y(T )i)S j̸=i p(y(T )|x(T )j) ≈ p(y(T )) and the L1Out is derived as below:\n\ni=1, ri(y(T )) = 1\n\nS−1\n\n(cid:80)\n\nIL1Out(X(T ); Y (T )) = E\n\n(cid:34)\n\nlog\n\n(cid:34)\n\n1 S\n\nS (cid:88)\n\ni=1\n\np(y(T )i|x(T )i) (cid:80)\n\nj̸=i p(y(T )i|x(T )j)\n\n1 S−1\n\n(cid:35)(cid:35)\n\n.\n\n(10)\n\nD DERIVATION OF IV LB\n\nSimilar to Agakov (2004), we replace the intractable conditional distribution p(y(T )|y(−T )) with a tractable optimization problem over a variational conditional distribution q(y(T )|y(−T )). It yields a lower bound on PI due to the non-negativity of the KL divergence:\n\nI(Y (−T ); Y (T )) ≥ H(Y (T )) + Ep(y(−T ),y(T ))[log q(y(T )|y(−T ))]\n\n(11)\n\nwhere H(Y ) is the differential entropy of variable Y and this bound is tight if and only if q(y(T )|y(−T )) = p(y(T )|y(−T )), suggesting that the second term in equation 11 equals the negative conditional entropy −H(Y (T )|Y (−T )).\n\nHowever the variational lower bound requires a tractable decoder for the conditional q(y|x). Alternatively, by considering an energy-based variational family for conditional distribution\n\nThe conditional expectation in equation 11 can be estimated using Monte Carlo sampling based on the encoded data distribution p(y(−T ), y(T )). And encoded data are sampled by introducing the augmented data x(−T ) and x(T ) and marginalizing them out as\n\np(y((−T ), y(T )) =\n\n(cid:90)\n\np(x(−T ), x(T ))p(y(−T )|x(−T ))p(y(T )|x(T ))dx(−T )x(T )\n\n(12)\n\naccording to the Markov chain proposed in Figure 1.\n\nE DERIVATION OF IT U BA\n\nAccording to Poole et al. (2019), by considering an energy-based variational family to express and conditional distribution q(y(T )|y(−T )):\n\nq(y(T )|y(−T )) =\n\np(y(T ))ef (y(T ),y(−T )) Z(y(−T ))\n\n(13)\n\n(cid:2)ef (y(T ),y(−T ))(cid:3) is a partition where f (x, y) is a differentiable critic function, Z(y(−T )) = Ep(y(T ) function, and introducing a baseline function a(y(T )), we derived a tractable TUBA lower bound (Barber & Agakov, 2003) of the predictive information as:\n\nI(Y (−T ), Y (T )) ≥ Ep(y(−T ),y(T ))[ ̃f (y(−T ), y(T ))] − log\n\n(cid:16)\n\nEp(y(−T ))p(y(T ))[e\n\n ̃f (y(−T ),y(T ))]\n\n(cid:17)\n\n= ITUBA(Y (−T ), Y (T ))\n\n(14)\n\nwhere ̃f (y(−T ), y(T )) = f (y(−T ), y(T )) − log(a(y(T ))) is treated as an updated critic function. Notice that different choices of baseline functions lead to different mutual information estimators.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nWhen a(y(T )) = 1, it leads to mutual information neural estimator (MINE) (Belghazi et al., 2018); when a(y(T )) = Z(y(T )), it leads to the lower bound proposed in Donsker & Varadhan (1975) (DV) and when a(y(T )) = e, it recovers the lower bound in Nguyen et al. (2010) (NWJ) also known as f-GAN (Nowozin et al., 2016) and MINE-f (Belghazi et al., 2018). In general, the critic function f (x, y) and the log baseline function a(y) are usually parameterized by neural networks (Oord et al., 2018; Belghazi et al., 2018): Oord et al. (2018) used a separable critic function f (x, y) = hθ(x)T gθ(y), while Belghazi et al. (2018) used a joint critic function f (x, y) = fθ(x, y), and Poole et al. (2019) claimed that joint critic function generally performs better than separable critic function but scale poorly with batch size.\n\nF DERIVATION OF Iinf oN CE\n\nThe derivation of infoNCE in our CPIC setting is trivial by treating Y (−T ) and Y (T ) as the input and output in the infoNCE formula from the CPC setting (Oord et al., 2018).\n\nG DETAILS OF SIMULATION\n\nIn this section, we first generated the 3D latent signals according to the Lorenz dynamic system 8 denoted as X ∈ R3×T . We calculated the largest eigenvalue of the covariance matrix of X as dynamics, and the noise variance is σ2 dynamic variance denoted as σ2 dynamics/SN R where SN R is signal-to-noise ratio. Then we randomly generate a semi orthogonal matrix V ∈ R30×3. Then we generated the true 30D signal V X embedded with additive spatially structured white noise, where the noise subspace Vnoise is generated with median principle angles with respect to dynamics subspaces V . The noise covariance is generated via Σnoise with the largest eigenvalue σ2 noise, and then we generate the noisy signal at the nth dimension by [Ynoisy]n ∼ N (vT n X, Σnoise), n = 1, . . . 30.\n\nnoise = σ2\n\nH MODEL COMPARISON IN TERMS OF R2 REGRESSION SCORE IN THE NOISY\n\nLORENZ ATTRACTOR EXPERIMENT\n\nIn this section, the R2 regression scores for CPC, DCA, deterministic & stochastic CPICs (three unisample upper bounds in terms of NWJ, MINE, TUBA, and one multi-sample upper bound) for all ten different SNRs are reported in Table 2. It shows that stochastic CPIC with multi-sample upper bound outperforms other approaches in majority of SNRs. It also shows that that CPIC is most robust to the noisy data and thus detect best latent trajectories from noisy observation compared with CPC and DCA.\n\nWe also show the aligned latent trajectories inferred from corrupted lifted observation for high, intermediate and low SNR (0.001, 0.01, 0.1) levels of noise with the median R2 scores across 100 replicates for PCA and DCA (as the extension of Figure 2) in Figure 4. The point-wise distances between the recovered dynamics and the ground-truth dynamics are encoded in the colors from blue to red, corresponding to short to long distance. It show that stochastic CPIC outperforms both PCA and DCA.\n\nI COMPARISON ON R2 SCORES OF LATENT DYNAMICS REGRESSION FOR\n\nNOISY LORENZ ATTRACTOR IN TERMS OF QUANTILE ANALYSIS\n\nWe displayed the medium performance (with the inter-quantile range as the error bars) of R2 scores of latent dynamics regression for noisy Lorenz attractor in Figure 5.\n\nJ DETAILS OF REAL-WORLD EXPERIMENTS\n\nThe four real data are Monkey motor cortical dataset (M1), Rat hippocampal data (HC), Temperature dataset (Temp) and Accelerate dataset (MS).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Left panel. Top: 3D trajectories of lorenz attractor’s ground-truth. Middle: 30D projected trajectory. Bottom: Corrupted 30D trajectory with SNR=0.001. Right Panel. 3D trajectories obtained by PCA and DCA in terms of different SNRs (0.1, 0.008, 0.001). We encode the point-wise Euclidean distance between the aligned inferred latent dynamics and the true dynamics into color on trajectories. Color from blue to red corresponds to the distance from short to long respectively. Separate colorbars are used for their corresponding SNRs.\n\nTable 2: R2 regression scores for CPC, DCA, deterministic & stochastic CIPCs including three unisample upper bounds (UNI): NWJ, MINE, TUBA, and one multi-sample upper bound (MUL) for all ten different SNRs\n\nSNR\n\nCPC\n\nDCA\n\nDeterministic UNI NWJ MINE\n\nTUBA\n\n0.001 0.00167 0.00278 0.00464 0.00774 0.01292 0.02154 0.03594 0.05995 0.1\n\n0.132 0.195 0.265 0.344 0.421 0.491 0.547 0.592 0.635 0.671\n\n0.458 0.466 0.473 0.478 0.480 0.484 0.486 0.491 0.952 0.953\n\n0.554 0.539 0.573 0.579 0.597 0.587 0.590 0.587 0.933 0.920\n\n0.543 0.538 0.573 0.562 0.559 0.596 0.596 0.912 0.837 0.893\n\n0.547 0.574 0.573 0.584 0.515 0.597 0.592 0.594 0.936 0.889\n\nCPIC\n\nMUL\n\n0.482 0.430 0.413 0.438 0.912 0.468 0.688 0.923 0.474 0.922\n\nStochastic\n\nUNI NWJ MINE\n\nTUBA\n\n0.539 0.573 0.587 0.598 0.582 0.580 0.568 0.937 0.970 0.926\n\n0.550 0.569 0.583 0.583 0.579 0.563 0.599 0.632 0.939 0.910\n\n0.553 0.571 0.590 0.556 0.589 0.592 0.864 0.907 0.896 0.854\n\nMUL\n\n0.459 0.576 0.588 0.593 0.598 0.923 0.930 0.951 0.970 0.989\n\nJ.1 MONKEY MOTOR CORTICAL DATASET\n\nO’Doherty et al. (2017) released multi-electrode spiking data for both M1 and S1 for two monkeys during a continuous grid-based reaching task. We used M1 data from the subject “Indy” (specifically, we used the file “indy 20160627 01.mat”). We discarded single units with fewer than 5,000 spikes, leaving 109 units. We binned the spikes into non-overlapping bins , square-root transformed the data and mean-centered the data using a sliding window 30 s in width.\n\n15\n\nSNR=10!\"SNR=10!#SNR=10!$PCADCAR#=0.014R#=0.953R#=0.0003R#=0.480R#=-0.0008R#=0.458Under review as a conference paper at ICLR 2023\n\nFigure 5: Comparison on R2 scores of latent dynamics regression on 10 SNR levels for four variants of CPIC. The first row shows the medium performance (with the inter-quantile range as the error bars) of R2 scores for CPICs (O) with PI between latent and obersvation space over running N=10/100 different random initializations, and the second row shows the performance for CPICs (L) with PI in latent space.\n\nJ.2 RAT HIPPOCAMPAL DATA\n\nGlaser et al. (2020) released the original data. The data consist of 93 minutes of extracellular recordings from layer CA1 of dorsal hippocampus while a rat chased rewards on a square platform. We discarded single units with fewer than 10 spikes, leaving 55 units. We binned the spikes into nonoverlapping 50 ms bins, then square-root transformed the data.\n\nJ.3 TEMPERATURE DATASET\n\nThe temperature dataset consists of hourly temperature data for 30 U.S. cities over a period of 7 years from OpenWeatherMap.org. We downsampled the data by a factor of 24 to obtain daily temperatures.\n\nJ.4 ACCELEROMETER DATASET\n\nMalekzadeh et al. (2018) released accelerometer data which records roll, pitch, yaw, gravity x, y, z, rotation x, y, z and acceleration x, y, z for a total of 12 kinematic variables. The sampling rate is 50 Hz. We used the file “sub 19.csv” from “A DeviceMotion data.zip”.\n\nJ.5 FORECASTING TASK\n\nThe forecasting task is the same in Clark et al. (2019). We use the extracted consecutive 3-length window representation of endogenous data to forecast the future relevant exogenous variables at log n. In M1 and HC, the endogenous variables are processed spiking data, and the exogenous variables are location data. In Temp and MS, we assume endogenous variables and exogenous variables are the same, 30 U.S. cities’ hourly temperature for Temp data and 12 kinematic variables for MS data.\n\n16\n\nMedianperformanceofCPICs (L)MedianperformanceofCPICs (O)N=10N=100",
    "reference": "# Summary Of The Paper\n\nThe authors propose a learning criterion and numerous bounds for learning representations of dynamic systems that are at once maximally compressed (minimal mutual information with their original representation, i.e., minimal rate) while being maximally informative a future time point (or, due to symmetry, a previous timepoint). This leads to learned encodings which are, up to the SNR and trade-off parameter $\\beta$,  reflective of the dynamical system.\n\n# Strength And Weaknesses\n\nStrengths:\n* The method is grounded in established literature (information bottleneck and variational information bottleneck, and more generally Rate-Distortion theory), yet contributes a novel criterion. Secondly, the recognition that the stationarity assumption reduces the learning criterion to a two term loss function, which is, in structure, very similar to many other information trade-offs in the literature.  \n* Multiple bounds are explored for the mutual information minimization/maximization.  \n* Experimental results are generally well done, modulo exact generation details.  \n\nWeaknesses:\n* Allowing time-windows T, for periodic or pseudoperiodic phenomena with period T' we might observe aliasing artifacts. While this remains stationary in a global sense, will such artifacts impede dynamics. Moreover, will the interpretation or downstream use of those dynamics be impeded by this induced false beat frequency at (T-T')/2?  \n* The variational lower bound (VLB) of Theorem 3 does not appear to require a decoder, contrary to the comment in the sentence immediately following the statement of the theorem; it seems as though it instead requires a good estimate of the conditional likelihood (a prediction from either T to T' or vice versa).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nIt might be more clear to note the differing time-blocks at $T_0$ and $T_1$, since there is no reliance on any symmetry pattern (i.e., there is nothing special about -T versus T as far as I understand.\n\nIt might be also helpful to be more clear about the lifting process/simulation for the attractor datasets. While the general idea is conveyed, the exact details it seems could be easily shared (or code given to reproduce the test cases). Overall however this paper seems reproducible.\n\nOverall the bounds are restatements of other results; this is somewhat clear in the paper, but it should still be noted re:Novelty for the purposes of review. There are several other differentiable mutual information estimators, including some that avoid the Gaussian encoder function. Though the paper is already quite thorough, it may be helpful to also test these functions, e.g. the correction of MINE in Song et al 2019 (called SMILE), and the Echo noise encoders in Brekelmans et al 2018.\n\n# Summary Of The Review\n\nThe authors present an elegant embedding method for dynamical systems, and the estimation machinery to fit the embedding efficiently. The method is novel, but also fits well into existing literature, adding new and interesting directions to Rate-Distortion based encodings. I think the ICLR community at large would be interested in such a manuscript.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nAXIOMATIC EXPLAINER LOCALITY WITH OPTIMAL TRANSPORT\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nExplainability methods have been notoriously difficult to evaluate and compare. Because of this, practitioners are often left guessing as to which explainer they should use for their task. Locality is one critical property of explainers which grants insight into the diversity of produced explanations. In this paper, we define a set of axioms which align with natural intuition regarding globalness, the inverse of locality. We then introduce a novel measure of globalness, Wasserstein Globalness, which uses optimal transport to quantify how local or global a given explainer is. Finally, we provide theoretical results describing the sample complexity of Wasserstein Globalness, and experimentally demonstrate how globalness can be used to effectively compare explainers. These results illustrate connections between both explainer fidelity and explainer robustness.\n\n1\n\nINTRODUCTION\n\nMachine Learning (ML) models are increasingly complex and capable of impressive performance in several domains. However, as models become more complex, they also become less interpretable. For this reason, researchers have begun to explore the topic of explainability, where model decisions are assigned an explanation. These explanations come in many forms, but often indicate how important each feature is towards the model prediction.\n\nExplainers need to be trustworthy for their explanations to be valuable. However, ML practitioners have very little information at their disposal when deciding which explainer is right for them. Unlike traditional ML models, whose accuracy can be computed on held-out testing datasets, there is no obvious metric by which we can compare explainers. Ground-truth explanations are rarely known, meaning we cannot directly compute the accuracy of an explainer.\n\nSome authors have tried to argue that their explainer is best by proposing a variety of pseudo-accuracy metrics. When explaining image classifiers, for example, one may follow the lead of Zhang et al. (2018) and Wang et al. (2020), by evaluating explainers based on how well they concentrate their saliency map’s energy around the object of interest. This kind of evaluation-metric is far from perfect, as it penalizes explainers from using scene context and is heavily biased towards concentrated saliency. In addition, these psuedo-accuracy metrics are only applicable for the specific task of feature-attribution for image data, when in reality, there are many other types of explanations and many other types of data. Clearly, it would be of great interest to the explainability community to be able to compare and contrast general-purpose explainers for any ML task.\n\nGlobalness is a property that can be used to compare and contrast explainers. When one explanation fully explains the model’s behavior, we call this a global explanation. In the past, models were typically explained globally. For example, feature selection would be done globally, meaning it would generate a single group of salient features for the entire dataset (Song et al., 2010; Yu & Liu, 2004; John et al., 1994; Dy & Brodley, 2004). More recently, in order to explain complex black-box models, it has become common to generate instance-wise explanations rather than a single explanation for the entire model. In this case, the explainer outputs local explanations which apply to only a subset of the model inputs. Since this distinction between local explainers and global explainers emerged, researchers have begun to acknowledge locality/globalness as a property of explainers.\n\nGlobalness is a meaningful property of explainers because it indicates how uniform the explanations are. In some cases, we expect or even desire all explanations to be similar to one another. Globalness\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nis also related to the concept of stability and robustness. The robustness of an explainer is limited by its globalness, and we can even use an explainer’s globalness to measure its local-robustness, since an explainer that is robust to small changes in the input will be near-global in a local neighborhood of the input space. This is another way in which a measure of globalness can facilitate a better understanding of our explainers, and allow us to compare/contrast their behavior.\n\nIn this paper, we study the property of globalness. We provide a novel way to measure it, and present several examples of how to use it in practice. To our knowledge, this is the first work that has provided a formal measure for explainer locality/globalness. We believe that this advances the field by enabling a more thorough analysis of various explanation techniques. With our measure of globalness, one can compare and contrast explainers in a way that was previously difficult and heuristic.\n\nThe contributions of the paper are as follows. First, we introduce intuitive axiomatic properties which align with human intuition surrounding the notion of locality/globalness. Second, we propose Wasserstein Globalness, a novel measure of globalness, that satisfies all these properties. We also present theoretical results regarding the sample complexity of estimating Wasserstein Globalness. Finally, through our experiments, we demonstrate how explainers can be differentiated by their globalness, and make a connection between globalness and adversarial robustness.\n\n2 RELATED WORK\n\nAs mentioned before, the community has struggled to effectively evaluate explainers, since real-world data rarely comes with ground-truth explanations. For this reason, several authors have turned to synthetic data, where the relevance of features are known (Chen et al., 2018; 2017). Instead of accuracy, others seek to describe explainer properties like transparency, sparsity, or robustness, in order to inform ML practitioners’ choice of explainer (Zhang et al., 2021). Because of the interaction between the data and explainer, we often want to describe properties of an explainer when applied to a specific data sample, like locality. A measure of globalness which accounts for both the data and explainer would provide a quantitative method for comparing and contrasting different explainers.\n\nSeveral authors have discussed the locality/globalness of explainers. For example, Doshi-Velez & Kim (2017) describe two types of explainers: local and global. They claim that global explainers are useful for scientific understanding or bias detection, and local explainers are useful for understanding specific model predictions. Zhang et al. (2021) surveys the interpretability literature and categorizes explainers along several axes. One of these axes is related to the locality of the explainer, where explainers are either \"global\", \"local\", or \"semi-local\". The distinctions provided by Doshi-Velez & Kim (2017); Zhang et al. (2021) are categorical rather than continuous. Though the community has begun to consider this property, there is still no formal continuous measure of locality/globalness.\n\nOne common explainer, LIME, requires the user to specify a kernel width which is roughly related to locality/globalness (Ribeiro et al., 2016). Anchors are a rule-based explanation given by a constrained optimization problem, where the objective is the \"coverage\" of the anchor (Ribeiro et al., 2018). An anchor’s coverage indicates how broadly applicable the rule is, thus this is also directly related to locality/globalness, but is tied directly into the objective of the explainer.\n\nSome authors, like Ribeiro et al. (2016), offer ways to construct a single global explanation from many local explanations, thereby granting a higher-level understanding of the model. While this offers explanations at two levels of globalness, we emphasize that this is again only a binary distinction. While works like this acknowledge the property of locality and its importance to explainability, they offer no way to quantify the locality of an explainer.\n\nThe need for a measure like this is becoming increasingly apparent as more researchers study the theory of explainability. For example, Li et al. (2020) define an object called the neighborhood disjointedness factor in order to study the generalization of finite-sample based local approximation explainers. Neighborhood disjointedness measures roughly how far apart points are from one another, and could be applied to explanations as a measure of globalness. However, this is limited to a small class of explainers, and does not apply to general explanation frameworks.\n\nWe advance the existing literature by formalizing the property of explainer globalness, and proposing a method for measuring it in practice. This is a new tool that the machine learning community can\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nuse to better understand how our explainers interact with our data. We can order explainers by their globalness, identify grouped explanations, and compare explainers.\n\n3 METHOD FORMULATION\n\nThere are a number of different ways one could measure the locality/globalness of an explainer. In order to guarantee the quality of our approach, we will first discuss the properties which should be satisfied by any measure of explainer globalness. In Section 3.2, these will be stated as axioms which align with our intuition about globalness. In Section 3.3, we will briefly review why other reasonable approaches fall short according to these properties. Then, in Section 3.4, we will give our definition of globalness and demonstrate that it satisfies all of these axioms.\n\n3.1 NOTATIONS\n\nConsider a black-box prediction model f : X → Y ⊆ Rc, where X ⊆ Rd is the space of model inputs, Y is the target, d is the dimension of the data, and c is the number of class labels. An explainer for this model is a function E : X → E, where the outputs of the explainer are called explanations, and comprise the space E. The space E is either a subset of Rd for feature-attribution explainers, or the vertices of a hypercube, {0, 1}d for feature-selection explainers. We will use P (E) to denote the set of probability distributions over the space of explanations, E. Let μ ∈ P (E) be one such probability distribution, generated by passing the data through our explainer. A metric measure space (mm-space) is a 3-tuple (E, dE , μ), where E is the space of explanations, and dE is a distance metric between explanations.\n\nExplanation Framework\n\nE\n\nDistance Measure\n\nFeature Attribution Feature Selection\n\nRd {0, 1}d\n\nCosine Hamming\n\nTable 1: Recommended metric spaces for common explanation frameworks.\n\nFor p ∈ P (X) and q ∈ P (Y ), Π(p, q) will denote the set of joint distributions in P (X × Y ) which marginalize to q and p. δx will denote the Dirac measure centered at x, which takes the value 1 at x and 0 elsewhere. We will denote the push-forward measure of μ by φ as φ#μ(A) = μ(φ−1(A)). For ease of reference, a summary of the notation used in this paper is in Appendix A Table 2.\n\n3.2 DESIRED PROPERTIES OF A GLOBALNESS MEASURE\n\nIn this section, we introduce the desired properties of a general globalness measure G : P(E) → R, which assigns a real value to a distribution of explanations. Our properties state that measure should be non-negative, continuous, convex, and should encode similarity between explanations.\n\nProperty 1 (Non-negativity) For all mm-spaces (E, dE , μ), the globalness is non-negative, i.e., G(μ) ≥ 0.\n\nProperty 2 (Fully-local measure) Let U be the uniform distribution on E. Then\n\nμ = U ⇐⇒ G(μ) = 0\n\n(1)\n\nProperty 2 states that the uniform distribution gives zero globalness, and Property 1 states that this is the minimum-globalness distribution for a given (E, dE ). Property 3 (Fully-global measure) A fully global explainer is one which produces the same explanation for every input, meaning its distribution of explanations can be represented by a Dirac distribution δx0. Formally, there exists an x0 ∈ E for which Gp(δx0 ) ≥ Gp(μ) for all μ ∈ P(E). Property 4 (Continuity) Let {μn} be a sequence of probability distributions which converge in distribution to μ. Then Gp(μn) → Gp(μ). This property claims that the globalness should be continuous with respect to the input distribution. In practice, the underlying distribution μ is often unknown, and we need to approximate it with an empirical distribution μn, where n is the number of samples. Property 4 guarantees that G can\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nbe approximated by sampling from μ and finding the globalness of an empirical distribution μn. Theorem 2, discussed in Section 4, will characterize the rate of this convergence in more detail. The need to account for distance between explanations. Before we introduce the next property, we will motivate it by discussing the geometry of E. A measure of globalness should take into account a notion of difference between explanations, this can be captured by introducing a distance metric, dE . For an illustrative example as to how globalness is related to the geometry of explanations, refer to Figure 1. Both distributions yield one of two explanations with equal probability. These two explanations are at a hamming distance of 1 for the left explainer, and a distance of two for the right explainer. Since the explanations are more varied for the right explainer, it is less global.\n\nFigure 1: Two distributions of feature-selection explanations on {0, 1}2 (red), the vertical axis represents the probability of seeing that explanation. The left distribution is more global because it outputs explanations which are closer to one another in the metric space (E, dE ).\n\nThe need for isometry invariance. The previous example demonstrated that globalness should account for the distance between explanations. Additionally, when these explanations change, but the distances between them do not, the globalness should not change. In Figure 2, we show an example of this. The two distributions in this figure are equivalent up to a re-ordering of the features. Clearly, we would consider both of these to be equally global, because one distribution is simply a reflection of the other. Transformations like this, rotations and reflections, are referred to as isometries. The following property states that a measure of globalness should be invariant to isometries.\n\nProperty 5 (Isometry-invariance) Let TE,dE be the group of distance-preserving measurable maps φ. Then,\n\nTE,d = {φ : E → E|dE (x, y) = dE (φ(x), φ(y)), ∀x, y ∈ E}\n\nThen G is invariant with respect to the group TE,dE :\n\nG(μ) = G(φ#μ) for all φ ∈ TE,dE\n\n(2)\n\n(3)\n\nAdditionally, let S denote all the measurable bijective maps ψ : E → E. Then G is not invariant to S:\n\nThere exists some ψ ∈ S for which G(μ) ̸= G(ψ#μ)\n\n(4)\n\nThis axiom states that globalness is invariant to isometries of the explanations E, but not arbitrary permutations. First, Equation 3 stated that G should be invariant to distance-preserving transformations of the explanation-space. Because globalness measures roughly how similar the set of explanations are to one another, any rigid transformation (rotation, reflection, etc.) of the space of explanations will have no effect on the globalness. Recall that we want globalness to measure roughly how similar the explanations are. This notion of similarity is directly encoded by the distance dE , hence an isometry which does not change these distances should not change the globalness. There is one more situation that we want our properties to address: when two sets of explanations are combined, how does the globalness change? The following property answers this question.\n\nProperty 6 (Convexity) If the explanations from two explainers are combined, the combined distribution will yield a lower globalness than the average globalness of the original explainers. Formally, let R = λP + (1 − λ)Q for some λ ∈ [0, 1] and P, Q ∈ P (E). Then G(R) ≤ λG(P ) + (1 − λ)G(Q). This property states that G is convex with respect to distribution μ, implying that a mixture of two explainers is less global than the average of each explainer’s globalness. By mixture, we mean probabilistically generating explanation E1(x) with probability p or generating explanation E2(x)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: An example of two sets of explanations. Though the distributions are different, they can be obtained from one another by simply swapping the ordering of the features. This corresponds to a reflection, which is an isometric transformation.\n\nwith probability (1 − p) independently for each x ∈ X . Concavity is a common requirement of diversity measures. For example in genetics, where biological diversity is measured, the diversity of two populations combined is expected to be greater than the diversity among one population (Ricotta, 2003). We instead require convexity because globalness is inversely related to the diversity of explanations.\n\n3.3 CANDIDATE MEASURES\n\nWe have considered several \"candidate\" approaches for G. We will now discuss some of these, and illustrate why they fall short as a measure of globalness.\n\nShannon Entropy. Since globalness is related to how diverse the explanations are, one might initially consider using the Shannon Entropy of the explainer’s outputs as a measure of locality (Shannon, 1948).\n\nH(X) = E(cid:2) − log(p(X))(cid:3)\n\n(5)\n\nEntropy is maximized when all the outcomes are equally likely, and minimized when the explainer only generates one explanation for all of the data, clearly satisfiying Properties 1 − 3. In fact, entropy satisfies several of our desiderata in Section 3.2, but fails on Property 5. Entropy cannot distinguish between how similar explanations are to one another.\n\nF-Divergences. One might also consider measuring globalness with an f-divergence (Csiszár, 1964) between the distribution of explanations μ, and the uniform distribution, U :\n\nDf (P ∥Q) = Ex∼Q\n\n(cid:2)f (\n\nP (x) Q(x)\n\n)(cid:3)\n\n(6)\n\nRemark (Alternative approaches). Shannon entropy and f-divergences both violate Property 5.\n\nF-divergences fail for the same reason that entropy failed. They consider only the probability of each explanation and ignore the degree of similarity between them. The proof of this remark is given in Appendix E.\n\n3.4 THE PROPOSED APPROACH\n\nThe framework presented in this paper is very general. Only a metric space (E, dE ) needs to be defined. For common explanation frameworks, recommended choices for dE are listed in Table 1. Given a metric space (E, dE ), we propose to measure the p-Wasserstein Globalness of μ, denoted Gp(μ), as the Wasserstein distance between μ and the uniform measure on E (Kantorovich, 1960). Intuitively, the Wasserstein distance metric between distributions P and Q measures how much \"work\" one would have to do to move probability mass from P until it matches Q. For this reason, it is sometimes called the earth-movers distance when p = 1.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition 1 (Wasserstein Globalness).\n\nGp(μ) := dp\n\nW (μ, U )\n\n= inf\n\nπ∈Π(μ,U )\n\nE(x,y)∼π[dp\n\nE (x, y)]1/p\n\n(7)\n\n(8)\n\nFor all of our experiments, we will have p = 1, but p is left general in the remainder of the paper, with the exception of Theorem 2, where p affects the convergence of approximate solutions to Equation 7. Theorem 1. The Wasserstein Globalness Gp : P(E) → R satisfies Properties 1-7. Where P(E) is the set of probability distributions over E.\n\nThe proof of Theorem 1 is provided in Appendix C. Because it satisfies these intuitive properties, the Wasserstein Globalness is a satisfactory way to quantify the globalness of an explainer.\n\n4 SAMPLE COMPLEXITY\n\nIn practice, we do not work with the true distributions μ and U , but their discrete approximations. The discrete approximation for a distribution p is defined as pN = 1\n\niid∼ p.\n\n(cid:80)N\n\ni=1 δXi for Xi\n\nN\n\nUsing a discrete approximation for the uniform distribution allows us to estimate the Wasserstein Globalness with a discretized formulation:\n\nDefinition 2 (Empirical Wasserstein Globalness).\n\nˆGp(μ) := dp\n\nW (μ, UN )\n\n(9)\n\nIn this section we address the question of how accurately ˆGp(μN ) approximates Gp(μ). Fortunately, there are related works that have studied the convergence of empirical distributions under the Wasserstein metric, notably by Fournier & Guillin (2015); Dereich et al. (2013). These prior results provide the foundation for the following theorem. Theorem 2. Let p be the order of the Wasserstein Globalness, and let Mq(μ) = (cid:82) p ∈ (0, d/2), q ̸= d/(d − p), q ≥ dp\n\nRd |x|qdμ(x). If\n\nd−p , then:\n\nE[| ˆGp(μN ) − Gp(μ)|] ≤ κp,qMq(μ)1/qN −1/d + Cp,q,dM p/q\n\nq\n\n(μ)(N −p/d + N −(q−p)/q)\n\n(10)\n\nFrom this theorem, we can immediately see that E(cid:2)|Gp(μ) − Gp(μN )|(cid:3) → 0 as N → ∞. However, increasing d will lead to a looser bound, meaning we need more samples as N increases. This is a problem if we want to compute Wasserstein Globalness in high-dimensional problem settings like computer vision tasks. To scale to high dimensions, we utilize the Sinkhorn Distance (Cuturi, 2013), which is described in Appendix B.\n\n5 EXPERIMENTS\n\nWasserstein Globalness allows us to compare and contrast explainers. In this section, we demonstrate certain insights it can provide about both our data and our explainer. In this section, we will present two kinds of experiment. The first demonstrates how globalness can help identify clustered explanations. The second type of experiment uses globalness to identify how locally-stable an explainer is. The code used in all following experiments will be made publicly available.\n\nDatasets. Our first experiment uses a simple synthetic dataset where the ground-truth explanations are known. To create this dataset, we generate d Gaussian clusters in Rd. Each cluster has a single (known) relevant feature. We use two other datasets in these experiments, which contain highdimensional image data. The first, MNIST (Deng, 2012), is comprised of grayscale images depicting handwritten digits 0-9. The last dataset, CIFAR-10 Krizhevsky (2009), is comprised of 32x32 color images. The targets in CIFAR-10 are one of ten classes of common objects.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Identifying Grouped Explanations.\n\n(b) Jagged Boundary Results.\n\nFigure 3: (a) Identifying Grouped Explanations. The results of computing Wasserstein Globalness for a variable number of classes, on the MNIST dataset. As we explain an increasing amount of digits, the explanations become more local. (b) The results of the jagged-boundary experiment. The accuracy of both the explainers and the model (left) decrease as the Wasserstein Globalness (right) becomes more varied. These results are aggregated across 15 total runs (new data each time). We show the mean ± one standard deviation of Wasserstein Globalness.\n\nExplainers. Before describing the experiments, we will briefly discuss the explanation methods used in the experiments. The Gradients (Simonyan et al., 2013) method directly uses the gradient of class score with respect to the input image as a saliency map. The Integrated Gradients method accumulates this gradient at several points along a path from some baseline input to the input to be explained (Sundararajan et al., 2017). The Expected Gradients method extends integrated gradients by allowing a distribution over baselines, and computing the expected value of integrated gradients with respect to this distribution (Erion et al., 2021). The SmoothGrad method smooths the gradients with a Gaussian kernel to alleviate the high frequency fluctuations commonly seen in partial derivatives (Smilkov et al., 2017). The Deep-SHAP method (Lundberg & Lee, 2017) approximates shapley values by extending the DeepLIFT algorithm (Shrikumar et al., 2017). LIME (Ribeiro et al., 2016) forms a local-approximation to the decision boundary by training a linear model on distance-weighted samples from the input X . Guided Backpropagation (Springenberg et al., 2015) modifies the gradient approach by adding a signal that prevents the backwards flow of negative gradients during backpropagation.\n\nIdentifying Grouped Explanations. In some problems, one may observe that certain groups of inputs receive similar explanations. In this case, it could be beneficial to determine how many such groups exist. Our technique allows us to answer such questions because the number of clusters is inversely related to globalness. In this section, we demonstrate this relationship empirically. Specifically, we observe that the Wasserstein Globalness decreases as we add more classes because the explanations for images in a given class will receive similar explanations. We train a simple MLP classifier on the previously described MNIST dataset, with no regularization and hidden layers of size 300, 500, and 300 respectively. We use cross-entropy loss, and train for 5 epochs.\n\nFor all image-data in our experiments, including MNIST, we use a feature attribution explainer. Feature attribution explainers score each feature according to its importance. Because of this, explanations fall in Rd rather than {0, 1}d. We again follow the canonical distance metric from Table 1, we will endow our metric space (E, dE ) with the cosine distance: dcos(x, y) = 1 − x·y .\n∥x∥2∥y∥2 Figure 3a shows the results of this experiment. As expected, each explainer correctly decreases its Wasserstein Globalness as we increase the number of digits explained.\n\nIdentifying Local Explainer Stability. If explanations change drastically after a very small change to the input, this explainer may be considered less trustworthy because we expect our explainers to be locally-stable. Of course, the degree of this stability depends on the specific problem in question. We will later discuss how this explainer-stability is related to the classifier’s stability. By evaluating Wasserstein Globalness in a small neighborhood around a single data point, we can see how robust the explainer is to such perturbations. The following experiments demonstrate this insight.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: A visualization of the synthetic data in R2. Data points are illustrated as dots, with color corresponding to class label. Each of the two clusters utilizes a different feature to discriminate between the two classes. The original data is shown (left), alongside a zoomed-in visualization of the original boundary (middle), and a zoomed-in visualization of our jagged-boundary modification (right).\n\nIn our first stability experiment, we use the synthetic data shown in Figure 4. We attempt to fool the explainer by making the originally-straight decision boundary jagged, thereby obscuring the true relevant feature. To make this boundary jagged, we randomly visit a point x, then flip all nearby points x′ to the label of x, ensuring that visited points x and x′ are never revisited. This data modification is illustrated in Figure 4, and pseudocode is provided in Appendix A, Algorithm 1. In this experiment, we select the feature with the highest attribution, and use the hamming distance in our explanation space: dH (x, y) = (cid:80) 1(xi ̸= yi). The results in Figure 3b demonstrate how Accexp is related to the explainer globalness, G, in this synthetic-data experiment. We expect that, as explainers become less accurate, the variance of their globalness increases. Accurate explainers, on the other hand, should yield globalness values which are consistently close to the ground-truth globalness.\n\ni\n\nIn Figure 3b, we see the relationship between Accexp and Wasserstein Globalness. As the explainer accuracy decreases, the Wasserstein Globalness varies farther from the ground-truth Globalness. Furthermore, the integrated gradients method is more robust to the jagged boundaries, and this is reflected in it having the smallest standard deviation of Wasserstein Globalness.\n\nWe conduct our next experiment on the CIFAR-10 dataset (Krizhevsky, 2009). This experiment demonstrates how globalness can be used as an indicator of explainer robustness in a local neighborhood. First we train a CNN on the dataset as our classification model. We generate a set of inputs by taking one CIFAR-10 image and performing 300 independent adversarial attacks on that image. Each adversarial input is explained, and we evaluate the Wasserstein Globalness for that set of adversarial inputs. This is repeated several times, each time allowing stronger adversarial attacks. Each time we evaluate Wasserstein Globalness we are evaluating it in a larger local neighborhood of X . By doing this, we can examine how globalness differs for different explainers. Though we do not know the ground-truth globalness for these images, we expect the globalness to be maximum in this setting, since feature importances should not be affected by very small perturbations. We are primarily interested in perturbations which are small enough that the classifier still performs well. Intuitively, we hope that, as long as the classification does not change, the reason for the classification should also remain roughly constant. Figures 5 and 6 show that, even with only 300 explanations, in 32 × 32 × 3 dimensions, the Wasserstein Globalness can differentiate the explainers.\n\nIn Figure 5, we can see how explanations change after the adversarial attacks. This figure also illustrates that the perturbed images are not too different from the original image. Though the images are perceptually near-identical, the explanations can be seen to change, leading to a change in globalness. In Figure 6, we observe exactly how this globalness changes for the batch of explanations at each σ. The deep-SHAP explainer is the most robust to local perturbations (Wasserstein Globalness close to maximum for a variety of σ), and that the Gradient and SmoothGrad explainers are the most sensitive. This result is not surprising, as SmoothGrad injects noise in order to perform the smoothing operation, and the Gradient approach is the most naive, forming the basis for many of the other approaches.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: The perturbed input image is shown in the left column. The remaining columns correspond to different explanation methods, and show an explanation overlayed on the original image.\n\nFigure 6: The Wasserstein Globalness for each explainer in our CIFAR-10 experiment (left). The results of 11 separate runs are plotted in the same color for each explainer. The bold line with markers of that color represents the mean across all 11 runs. We also indicate the classifier accuracy (right) in order to indicate the point at which the perturbations become too strong for the explainer.\n\n6 CONCLUSION\n\nExplainability is a branch of machine learning that not only facilitates trust between the user and the model, but also allows us to learn from our models. Thus, it is critical that we understand our explainers to use them reliably. In this paper, we have argued that globalness is a theoretical property of explainers which can be valuable to the explainability community. We introduced Wasserstein Globalness, a model-agnostic method for measuring the globalness of explainers which satisfies intuitive axioms regarding the notion of globalness. Finally, we presented experimental results which demonstrate how globalness can be used to differentiate explainers and choose the best one for a given task.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nAs ML models grow in both complexity and popularity, they have also become less interpretable. These complex models often exhibit surprising behavior. In extreme cases, they can propagate racial and gender bias and even put humans in physical danger. However, even the benign cases erode trust between the user and the model. ML practitioners are responsible for the harmful effects of their models, and are therefore are obliged to build reliable, trustworthy systems.\n\nExplainability is one way in which we can begin to build trustworthy models. It is crucial that we seek a better understanding of uninterpretable models in order to deploy them safely in the real world, whether that means building interpretable models in the first place or explaining them after the fact. In this paper, we discuss a theoretical property of explainability methods, and present a technique for measuring this quantity in practice. We intend for our globalness measure to facilitate responsible practices by enabling humans to better understand their models. This understanding can facilitate responsible decisions, enable practitioners to remove harmful behavior from their systems, and increase our trust in the models themselves.\n\nREFERENCES\n\nJianbo Chen, Mitchell Stern, Martin J Wainwright, and Michael I Jordan. Kernel feature selection via conditional covariance minimization. Advances in Neural Information Processing Systems, 30, 2017.\n\nJianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An informationtheoretic perspective on model interpretation. In International Conference on Machine Learning, pp. 883–892. PMLR, 2018.\n\nImre Csiszár. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der ergodizitaet von markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl., 8:85–108, 1964.\n\nMarco Cuturi.\n\nSinkhorn distances: Lightspeed computation of optimal\n\nJ. Burges, L. Bottou, M. Welling, Z. Ghahramani,\n\nIn C. and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/ af21d0c97db2e27e13572cbf59eb343d-Paper.pdf.\n\ntransport.\n\nLi Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal\n\nProcessing Magazine, pp. 141–142, 2012.\n\nSteffen Dereich, Michael Scheutzow, and Reik Schottstedt. Constructive quantization: Approximation by empirical measures. In Annales de l’IHP Probabilités et statistiques, volume 49, pp. 1183–1203, 2013.\n\nFinale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.\n\narXiv preprint arXiv:1702.08608, 2017.\n\nJennifer G Dy and Carla E Brodley. Feature selection for unsupervised learning. Journal of machine\n\nlearning research, 5(Aug):845–889, 2004.\n\nGabriel Erion, Joseph D Janizek, Pascal Sturmfels, Scott M Lundberg, and Su-In Lee. Improving performance of deep learning models with axiomatic attribution priors and expected gradients. Nature machine intelligence, 3(7):620–631, 2021.\n\nRémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Léo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1–8, 2021. URL http://jmlr.org/papers/v22/20-451.html.\n\nNicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the\n\nempirical measure. Probability Theory and Related Fields, 162(3):707–738, 2015.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nGeorge H John, Ron Kohavi, and Karl Pfleger. Irrelevant features and the subset selection problem.\n\nIn Machine learning proceedings 1994, pp. 121–129. Elsevier, 1994.\n\nL. V. Kantorovich. Mathematical methods of organizing and planning production. Management Science, 6(4):366–422, 1960. ISSN 00251909, 15265501. URL http://www.jstor.org/ stable/2627082.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n\nJeffrey Li, Vaishnavh Nagarajan, Gregory Plumb, and Ameet Talwalkar. A learning theoretic perspective on local explainability. In International Conference on Learning Representations, 2020.\n\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in\n\nneural information processing systems, 30, 2017.\n\nFacundo Mémoli. Gromov-wasserstein distances and the metric approach to object matching. Foundations of Computational Mathematics, 11(4):417–487, 2011. URL http://dblp.uni-trier. de/db/journals/focm/focm11.html#Memoli11.\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135–1144, 2016.\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic\n\nexplanations. In Proceedings of the AAAI conference on artificial intelligence, 2018.\n\nCarlo Ricotta. Additive partition of parametric information and its associated β-diversity measure.\n\nActa Biotheoretica, 51(2):91–100, 2003.\n\nC. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27(3):\n\n379–423, 1948. doi: 10.1002/j.1538-7305.1948.tb01338.x.\n\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In International conference on machine learning, pp. 3145– 3153. PMLR, 2017.\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.\n\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. Smoothgrad:\n\nremoving noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.\n\nFengxi Song, Zhongwei Guo, and Dayong Mei. Feature selection using principal component analysis. In 2010 international conference on system science, engineering design and manufacturing informatization, volume 1, pp. 27–30. IEEE, 2010.\n\nJ Springenberg, Alexey Dosovitskiy, Thomas Brox, and M Riedmiller. Striving for simplicity: The\n\nall convolutional net. In ICLR (workshop track), 2015.\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In\n\nInternational conference on machine learning, pp. 3319–3328. PMLR, 2017.\n\nHaofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu. Score-cam: Score-weighted visual explanations for convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 24–25, 2020.\n\nLei Yu and Huan Liu. Efficient feature selection via analysis of relevance and redundancy. The\n\nJournal of Machine Learning Research, 5:1205–1224, 2004.\n\nJianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126 (10):1084–1102, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYu Zhang, Peter Tino, Ales Leonardis, and Ke Tang. A survey on neural network interpretability. IEEE Transactions on Emerging Topics in Computational Intelligence, pp. 726–742, oct 2021. doi: 10.1109/tetci.2021.3100641. URL https://doi.org/10.1109%2Ftetci.2021. 3100641.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nTable 2 provides a summary of the notation and terminology used in the paper.\n\nSymbol\n\nName\n\nφ# 1\nδx E\nE X\nE P (X) U\nΠ(P, Q) R\n⟨·, ·⟩F\n\nPush-forward Measure Indicator Function Dirac distribution (centered at x) Expected Value Set of Explanations Space of model inputs Explainer Set of probability distributions over X Uniform Distribution Set of joint distributions which marginalize to P and Q Real numbers Frobenius Inner Product\n\ndE\n\ndistance between explanations\n\nmm-space metric-measure space\n\nTable 2: Summary of notation used in the paper\n\nAlgorithm 1 describes the jagged-boundary data modification shown in Figure 4.\n\nAlgorithm 1 Jagged-Boundary\n\n1: Let X be a finite data sample. 2: Let label(·) give the label of each point in X. 3: Let Nx be a neighborhood around point x. 4: while not all points visited do for unvisited x do 5: 6: 7: 8: 9: 10: 11: end while\n\nfor unvisited x′ ∈ Nx do label(x′) ← label(x) x′ visited\n\nend for\n\nend for\n\nWe measure the wall clock timing of the three different computationally expensive tasks. Because these quantities vary with the explainer used and the random input image, we present the average over both the explainers and 10 randomly selected CIFAR-10 images. These results are shown in Figure 3.\n\nTask Adversarial Perturbations Generating Explanations Computing Globalness\n\nAverage Time (s) 56.17 19.42 6.97\n\nTable 3: Average wall clock time for the CIFAR-10 experiment. As expected, computing globalness is less computationally expensive than generating the explanations themselves.\n\nFigure 7 shows all of the perturbed images from the CIFAR-10 experiment presented in 5. Figure 5 is formed from a subset of the rows in Figure 7. Figure 8 depicts another set of explanations, with a different original input image.\n\nIn figure 11, we show an example of the different perturbed images for a single adversarial radius σ.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: An extended version of 5\n\nB APPROXIMATING WASSERSTEIN GLOBALNESS IN HIGH-DIMENSIONS\n\nComputing the Wasserstein distance traditionally involves solving a linear programming problem which does not scale well. In the previous section, we discussed the sample complexity of ˆGp(μN ). This empirical estimate also scales poorly with the dimension of the data. However, several techniques have been developed to efficiently compute Wasserstein distances in high dimensions. One such technique, called entropy-regularized optimal transport, relies on regularizing the entropy of the joint distribution π ∈ Π(P, Q).\n\nThe Wasserstein distance can be written as an optimization over the set of product measures Π(P, Q). In its discrete form, the expectation E(x,y)∼π[dE (x, y)] from Equation 7 can be written as a Frobenius inner product between the matrix of distances D, and the transport plan R.\n\nd1\n\nW (p, q) = min\n\n⟨R, D⟩F\n\nR∈Π(p,q)\n\n(11)\n\nThe sinkhorn distance, introduced to the ML community by Cuturi (2013), utilizes a technique called entropic regularization. The entropy-regularized transport problem adds an additional regularization term Ω(R) = (cid:80)\n\ni,j ri,jlog(ri,j).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Another set of explanations for an image of a deer from the CIFAR-10 dataset.\n\ndsinkhorn(p, q) = min\n\n⟨R, D⟩F + λΩ(R)\n\nR∈Π(p,q)\n\n(12)\n\nThis encourages the transport plan to have less mutual information with the marginals p and q. By searching just over these \"smooth\" transport plans, we have convexified the optimization problem, and now can use matrix scaling algorithms to solve it 1. There are also existing theoretical guarantees that dsinkhorn will be close to dW , for which we refer the reader to Cuturi (2013).\n\nBy utilizing entropic regularization, we can find good solutions faster. However, we can always decrease the strength of this regularization in order to eliminate noise from the computation. As we have discussed, this is especially useful when we work with high-dimensional data, as is done in 5.\n\nC PROOF OF THEOREM 1\n\nWe will now restate and prove that the axioms from Section 3.2 apply to the definition of G in Section 3.4.\n\n1For our experiments, we compute the sinkhorn algorithm using available optimal transport solvers available\n\nat Flamary et al. (2021).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: Another set of explanations for an image of a ship from the CIFAR-10 Dataset.\n\nProperty 1 states that Gp(μ) ≥ 0 ∀μ.\n\nProof. Since dW is a distance metric, it is non-negative by construction. Therefore Gp(P ) = dW (P, U ) is non-negative.\n\nProperty 4 Let {μn} be a sequence of probability distributions which converges in distribution to μ. Then Gp(μn) → Gp(μ).\n\nProof.\n\nBy remark 2.4 in (Mémoli, 2011),\n\n|dp\n\nW (A, B) − dp\n\nW (An, Bn)| ≤ dp\n\nW (A, An) + dp\n\nW (B, Bn)\n\nThis implies that\n\n|dp\n\nW (μ, U ) − dp\n\nW (μn, Un)| ≤ dp\n\nW (μ, μn) + dp\n\nW (U, Un)\n\n(13)\n\n(14)\n\nHowever, U does not need to be approximated, therefore\n\nU = Un =⇒ |Gp(μ) − Gp(μn)| = |dp ≤ dp\n\nW (μ, U ) − dp W (μ, μn)\n\nW (μn, U )|\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Another set of explanations for an image of a truck from the CIFAR-10 dataset.\n\nFigure 11: A visualization of the pertubed images used in our CIFAR-10 experiment. These correspond to the last row in Figure 7.\n\nBut μn converges in distribution to μ, meaning dp\n\nW (μ, μn) → 0 =⇒ |Gp(μ) − Gp(μn)| → 0\n\nFrom this proof we can see that, not only does Gp(μn) converge to Gp(μ), but it converges at least as quickly as μn → μ.\n\nProperty 3 states that there exists an x0 ∈ E for which Gp(δx0) ≥ Gp(μ) ∀μ ∈ P(E). Furthermore, we will also show that, for this x0, sx0,p ≤ sx,p ∀x ∈ E, where sx,p is the p-eccentricity, defined in Mémoli (2011) as sx,p = (cid:82)\n\nX dp(x, x′)dx′ 2.\n\n2Roughly, the p-eccentricity indicates how far a point x is to the rest of the points in E.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nP-eccentricity appears in this property in order to characterize the x0 for which δx0 achieves maximum globalness.\n\nProof. . Without loss of generality, assume sx0,p ≥ sx,p, ∀x ∈ E. Any discrete distribution can be represented as a delta-train P = (cid:80) i λi = 1. By Property 6, we know that Gp(P ) ≤ (cid:80)\n\ni λiδ(xi) such that λi ≥ 0, ∀i and (cid:80)\n\ni λiGp(δ(xi)). But for a Dirac measure, 7 reduces to\n\nGp(δx0) = (cid:0)\n\n(cid:90)\n\nX\n\ndp(x0, x)dx(cid:1)1/p\n\n= sx0,p\n\nSo\n\nGp(P ) ≤\n\n(cid:88)\n\ni\n\nλisx0,p ≤ max\n\nsxi,p = sx0,p.\n\ni\n\n(15)\n\n(16)\n\nThis proves the axiom for any discrete distribution. But general distributions can be approximated with these weighted delta-trains, so by Property 4, so in the limit, this proof holds for general distributions.\n\nProperty 2 states that μ = U ⇐⇒ G(μ) = 0\n\nProof. First note that dp metrics, (x, y) = 0 ⇐⇒ x = y. This directly implies that dW (μ, U ) = 0 ⇐⇒ μ = U .\n\nW (P, Q) is a metric. From the identity of indiscernibles axiom of distance\n\nWe will now prove the first part of Property 5, restated below:\n\nLet TE,dE be the group of isometries of (E, dE ) defined in Equation 2. Then Gp is TE,d-invariant.\n\nProof. We need to show that Gp(μ) = Gp(φ#μ), ∀φ ∈ TE,d.\n\nOne consequence of the definition of push-forward measure is that\n\nPφ#q(φx) = Pq(φ−1φx) = Pq(x) =⇒ E\n\nx∼φ#q\n\n[φx] = E\n\nx∼q\n\n[x]\n\n(17)\n\nConsider the set of measure couplings (feasible transport plans) between μ and v, denoted Π(μ, v)\n\nLet hφ : Π(μ, v) → Π(φ#μ, φ#v) be given by\n\nhφ ◦ π(A, B) = π(φ−1A, φ−1B) (18) Since hφ is bijective, and since we have shown in 17 that Ex∼π[x] = Ex∼hφ(π)[φ(x)], it holds that\n\nBecause φ is an isometry,\n\nGp(μ) = inf\n\nE(x,y)∼π[dp(x, y)]1/p\n\nπ∈Π(μ,U )\n\n= inf\n\nπ∈Π(μ,U )\n\nE(x,y)∼hφ(π)[dp(φx, φy)]1/p\n\nBecause Ex∼π[x] = Ex∼hφ(π)[φ(x)],\n\n= inf\n\nπ∈Π(μ,U )\n\nE(x,y)∼hφ(π)[dp(x, y)]1/p\n\nFinally, since hφ is a bijection between Π(μ, U ) and Π(φ#μ, φ#U ),\n\nE(x,y)∼π[dp(x, y)]1/p\n\n=\n\ninf π∈Π(φ#μ,φ#U )\n\n= Gp(φ#μ)\n\n18\n\n(19)\n\n(20)\n\n(21)\n\nUnder review as a conference paper at ICLR 2023\n\nWe will now prove the second part of Property 5, which states that ∃φ : E → E ̸∈ TE,dE for which Gp(φ#μ) ̸= Gp(μ), for some μ.\n\nProof. Consider a φ ∈ S, φ : E → E such that dE (x, y) = c ∗ dE (φx, φy) for some constant c, and some x, y ∈ E. Note that φ is clearly not in TE,dE , the group of isometries of (E, dE ).\n\nLike before, we can write:\n\nBut since φ is not isometric:\n\nGp(μ) = inf\n\nE(x,y)∼π[dp(x, y)]1/p\n\nπ∈Π(μ,U )\n\n=\n\ninf π∈Π(φ#μ,φ#U )\n\nE(x,y)∼π[dp(x, y)]1/p\n\n=\n\ninf π∈Π(φ#μ,φ#U )\n\nE(x,y)∼π[cpdp(x, y)]1/p\n\n̸= Gp(φ#μ)\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\nProperty 6 (convexity) Let R be a convex combination of measures P and Q. That is, R = λP + (1 − λ)Q for some λ ∈ [0, 1]. Then G(R) ≤ λG(P ) + (1 − λ)G(Q).\n\nProof. Let P, Q be probability distributions. Let φP be the optimal transport plan between P and U . Similarly, let φQ be the optimal transport plan between Q and U . Then the convex combination of P and Q is a feasible transport plan between R = λP + (1 − λ)Q and U , so\n\nGp(X) = inf\n\nE(x,y)∼π[dp(x, y)]1/p\n\nπ∈Π(μ,U )\n\n≤ E(x,y)∼λφP +(1−λ)φQ[dp(x, y)]1/p = λE(x,y)∼φP [dp(x, y)]1/p + (1 − λ)E(x,y)∼φQ[dp(x, y)]1/p = λGp(P ) + (1 − λ)Gp(Q)\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\nD PROOF OF THEOREM 2\n\nWe will first introduce two lemmas to simplify the proof of theorem 2. The first lemma concerns the error incurred by approximating μ with finite samples. The second lemma concerns the error incurred by approximating U with finite samples. By bounding both of these errors, we obtain an upper bound on the error of using both finite-sample approximations. Lemma 1. Let p ∈ (0, d/2). Also, let q ̸= d/(d − p). Let Mq(μ) = (cid:82) constant Cp,q,d which depends only on p,q, and d,\n\nRd |x|qdμ(x). Then for some\n\nE(cid:2)|Gp(μ) − Gp(μN )|(cid:3) ≤ Cp,q,dM p/q\n\nq\n\n(μ)(N −p/d + N −(q−p)/q)\n\nProof. From the triangle inequality,\n\nW (μN , U ) ≤ dp dp\n\nW (μ, U ) + dp\n\nW (μN , μ)\n\nFrom (Fournier & Guillin, 2015), E[dp\n\nW (μN , μ)] ≤ Cp,q,dM p/q\n\nq\n\n(μ)(N −p/d + N −(q−p)/q).\n\nSo\n\nE[Gp(μN )] − Gp(μ) ≤ Cp,q,dM p/q\n\nq\n\n(μ)(N −p/d + N −(q−p)/q)\n\n19\n\n(30)\n\n(31)\n\n(32)\n\nUnder review as a conference paper at ICLR 2023\n\nSimilarly, we can invoke the triangle inequality with dp\n\nW (μ, U ) ≤ dp dp\n\nW (μN , U ) + dp\n\nW (μ, U ) on the LHS: W (μN , μ)\n\nAnd, making the same argument, we get:\n\nSo\n\nGp(μ) − E[Gp(μN )] ≤ Cp,q,dM p/q\n\nq\n\n(μ)(N −p/d + N −(q−p)/q)\n\nFinally, combining 32 and 34,\n\nE(cid:2)|Gp(μ) − Gp(μN )|(cid:3) ≤ Cp,q,dM p/q\n\nq\n\n(μ)(N −p/d + N −(q−p)/q)\n\n(33)\n\n(34)\n\nThis theorem is related to Property 4 in the sense that the empirical distribution μN converges to μ. This theorem is concerned with the rate at which Gp(μN ) converges, whereas Property 4 simply states that it will converge. Perhaps the most important consequence of 1 is that E(cid:2)|Gp(μ) − Gp(μN )|(cid:3) → 0 as N → ∞. Lemma 2. Let p ∈ (0, d q,\n\nd−p . Then for some constant κp,q which depends only on p and\n\n2 ), and q > dp\n\nE(cid:2)| ˆGp(μN ) − Gp(μN )|(cid:3) ≤ κp,q\n\n(cid:2)\n\n(cid:90)\n\nRd\n\n∥x∥qdμ(x)(cid:3)1/q\n\nN −1/d\n\nProof. From the triangle inequality:\n\ndW (μN , UN ) ≤ dW (μN , U ) + dW (UN , U ) =⇒ dW (μN , UN ) − dW (μN , U ) ≤ dW (UN , U )\n\n(35)\n\n(36)\n\n(37)\n\nFrom (Dereich et al., 2013), Rd ∥x∥qdμ(x)(cid:3)1/q\n\nN −1/d.\n\nκp,q\n\n(cid:2) (cid:82)\n\nthere exists some constant κp,q such that E[dp\n\nW (UN , U )] ≤\n\n=⇒ E[dW (μN , UN ) − dW (μN , U )] ≤ E[dp\n\nW (UN , U )] ≤ κp,q\n\n(cid:2) (cid:82)\n\nRd ∥x∥qdμ(x)(cid:3)1/q\n\nN −1/d\n\n=⇒ E[ ˆGp(μN ) − Gp(μN )] ≤ κp,q\n\n(cid:2) (cid:82)\n\nRd ∥x∥qdμ(x)(cid:3)1/q\n\nN −1/d\n\nSimilarly:\n\nSo we have that:\n\ndW (μN , U ) ≤ dW (μN , UN ) + dW (UN , U ) =⇒ dW (μN , U ) − dW (μN , UN ) ≤ dW (UN , U )\n\nE[| ˆGp(μN ) − Gp(μN )|] ≤ κp,q\n\n(cid:90)\n\n(cid:2)\n\nRd\n\n∥x∥qdμ(x)(cid:3)1/q\n\nN −1/d\n\n(38)\n\nWe will now combine the previous lemmas to get a final bound on the approximation error incurred by using discrete approximations for both μ and U .\n\nProof. The following one-line proof is a natural consequence of lemmas 1 and 2.\n\nE[| ˆGp(μN ) − Gp(μ)|] ≤ E[| ˆGp(μN ) − Gp(μN )|] + E(cid:2)|Gp(μ) − Gp(μN )|(cid:3)\n\n≤ κp,q\n\n(cid:90)\n\n(cid:2)\n\nRd\n\n∥x∥qdμ(x)(cid:3)1/q\n\nN −1/d + Cp,q,dM p/q\n\nq\n\n(μ)(N −p/d + N −(q−p)/q)\n\n= E[| ˆGp(μN ) − Gp(μ)|] ≤ κp,qMq(μ)1/qN −1/d + Cp,q,dM p/q\n\nq\n\n(μ)(N −p/d + N −(q−p)/q)\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nE PROOF OF REMARK 3.3\n\nProof. Let ψ ∈ S be a permutation of the set of explanations.\n\nBy Equation 17, a direct consequence of the definition of push-forward measure, we have that:\n\nEx∼μ[g(μ(x))] = Ex∼ψ#μ[g(ψ#μ(x))]\n\n(39)\n\nfor some function g.\n\nNow observe that both entropy and f-divergences, as defined in Equations 5 and 6 respectively, can be written in the form above. Entropy is obtained by directly letting g(x) = − log(x).\n\nH(X) = Ex∼μ[− log(μ(x))]\n\n(40)\n\nThe f-divergence between μ and U is equivalent by allowing g(x) = f ( c c to be the uniform probability of an element x, U (x).\n\nx ), and allowing the constant\n\nD(U ∥μ) = Ex∼μ\n\n(cid:2)f (\n\nU (x) μ(x)\n\n)(cid:3)\n\n(41)\n\n21",
    "reference": "# Summary Of The Paper\n\nThe paper promotes the use of the Wasserstein distance between the distribution of explanations produced by an explainer algorithm and the uniform distribution as a way to access whether the explainer is local (provides sample specific information) or global (class specific). Five desired properties are introduced to motivate the choice, and proofs are provided that the proposed use of the Wasserstein distance concurs with these properties.\n\n# Strength And Weaknesses\n\n# Strengths\n\n1.  Clear motivation of the chosen measure with the 5 properties.\n2.  An important problem the paper aims to address.\n3.  Empirical demonstration with some theoretical justification, and the code provided.\n4.  A well written manuscript.\n\n# Weaknesses\n\n1.  Some claims do not take into account the existing literature.\n    1.  In the intro it is claimed that there are no obvious metrics to compare the explainers, yet the field of developing such metrics is a highly active one.\n        1.  Remove and Retrain ROAR: Sara Hooker et al. A benchmark for interpretability methods in deep neural networks. In Advances in Neural Information Processing Systems, pages 9737–9748, 2019\n        2.  Retain and Retrain RAR: Md Rahman, et al. Interpreting models interpreting brain dynamics. Scientific Reports, 12(1):1–15, 202\n        3.  Accuracy Information Curves, Softmax Information Curves: Andrei Kapishnikov, Subhashini Venugopalan, Besim Avci, Ben Wedin, Michael Terry, and Tolga Bolukbasi. Guided integrated gradients: An adaptive path method for removing noise. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5050–5058, 202\n    2.  The abstract claims introduction of a novel measure of globalness, yet I've come across this concept already in the paper below. Admittedly the method in that paper was developed to demonstrate an application. This work may be a mathematical treatment of that method. Please explain.\n        1.  Md Rahman, et al. Interpreting models interpreting brain dynamics. Scientific Reports, 12(1):1–15, 202\n2.  Judging by experiments, sensitivity of the Wasserstein measure to globalness is low.\n    1.  In all experiments the changes of the measure are within hundredth units, while the variance, where shows is much larder.\n    2.  In Figure 6 accuracy drops to random, while the measure only slightly decreases and even then not for all methods.\n    3.  What is the source of variability in assessing the distribution of explainer predictions? Multiple classifiers each trained with random seeds and explained at each sample? Multiple samples on the same classifier? Something else?\n3.  Lack of demonstration of the computational complexity and wall clock timing of the method.\n    1.  Wasserstein distance is computationally demanding to compute, especially in high dimensions. There's a danger, that the proposed method will be intracktable in practice despite for toy problems. The paper would benefit from demonstrations on the issue.\n4.  The meaning of the distance of the value of it in separating one method from another in terms of behavior of explaners at individual input samples is not clearly demonstrated.\n    1.  It is unclear what does it mean for the value to be 0.99 or 0.98, what changes in detected explainers? Figure 5 contains examples of saliency maps, yet interpretation or connection of the proposed G measure with what is displayed is not clear.\n    2.  It is clear, that if the metric is close to 0 all saliency maps for each sample must have very few things in common, and a high value of the metric means there is a single global per class feature. Rigorous demonstration that this is indeed the case would help evaluate the value of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written and is a quality work. The code is currently not available, but proposed for release. Thus it is not easy, if at all possible, to reproduce the results at the review stage.\n\nAs for novelty, I have some reservations. The abstract claims introduction of a novel measure of globalness, yet I've come across this concept already in the paper below. Admittedly the method in that paper was developed to demonstrate an application. This work may be a mathematical treatment of that method. It would be good to have an explanation and discussion of this.\n\nMd Rahman, et al. Interpreting models interpreting brain dynamics. Scientific Reports, 12(1):1–15, 202\n\n# Summary Of The Review\n\nA clear paper with a solid and reasonably well-presented idea on an important topic. However, it lacks some demonstration of the claims and does not systematically explore the usefulness of the proposed method.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTOWARDS ONE-SHOT NEURAL COMBINATORIAL SOLVERS: THEORETICAL AND EMPIRICAL NOTES ON THE CARDINALITY-CONSTRAINED CASE\n\nRunzhong Wang1, Li Shen2, Yiting Chen1, Xiaokang Yang1, Dacheng Tao2, Junchi Yan1∗ 1MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University {runzhong.wang,sjtucyt,xkyang,yanjunchi}@sjtu.edu.cn {mathshenli,dacheng.tao}@gmail.com Code:\n\nhttps://github.com/Thinklab-SJTU/One-Shot-Cardinality-NN-Solver\n\n2JD Explore Academy\n\nABSTRACT\n\nOne-shot non-autoregressive neural networks, different from RL-based ones, have been actively adopted for solving combinatorial optimization (CO) problems, which can be trained by the objective score in a self-supervised manner. Such methods have shown their superiority in efficiency (e.g. by parallelization) and potential for tackling predictive CO problems for decision-making under uncertainty. While the discrete constraints often become a bottleneck for gradient-based neural solvers, as currently handled in three typical ways: 1) adding a soft penalty in the objective, where a bounded violation of the constraints cannot be guaranteed, being critical to many constraint-sensitive scenarios; 2) perturbing the input to generate an approximate gradient in a black-box manner, though the constraints are exactly obeyed while the approximate gradients can hurt the performance on the objective score; 3) a compromise by developing soft algorithms whereby the output of neural networks obeys a relaxed constraint, and there can still occur an arbitrary degree of constraint-violation. Towards the ultimate goal of establishing a general framework for neural CO solver with the ability to control an arbitrarysmall degree of constraint violation, in this paper, we focus on a more achievable and common setting: the cardinality constraints, which in fact can be readily encoded by a differentiable optimal transport (OT) layer. Based on this observation, we propose OT-based cardinality constraint encoding for end-to-end CO problem learning with two variants: Sinkhorn and Gumbel-Sinkhorn, whereby their violation of the constraints can be exactly characterized and bounded by our theoretical results. On synthetic and real-world CO problem instances, our methods surpass the state-of-the-art CO network and are comparable to (if not superior to) the commercial solver Gurobi. In particular, we further showcase a case study of applying our approach to the predictive portfolio optimization task on real-world asset price data, improving the Sharpe ratio from 1.1 to 2.0 of a strong LSTM+Gurobi baseline under the classic predict-then-optimize paradigm.\n\n1\n\nINTRODUCTION\n\nDeveloping neural networks that can handle combinatorial optimization (CO) problems is a trending research topic (Vinyals et al., 2015; Dai et al., 2016; Yu et al., 2020). A family of recent CO networks (Wang et al., 2019b; Li et al., 2019; Karalias & Loukas, 2020; Bai et al., 2019) improves the existing reinforcement learning-based auto-regressive CO networks (Dai et al., 2016; Lu et al., 2019) by solving the problem in one shot and relaxing the non-differentiable constraints, resulting in an end-to-end learning pipeline. The superiorities of one-shot CO networks are recognized in three aspects: 1) the higher efficiency by exploiting the GPU-friendly one-shot feed-forward network, compared to CPU-based traditional solvers (Gamrath et al., 2020) and the tedious auto-regressive\n\n∗Junchi Yan is the correspondence author. The work was in part supported by National Key Research and Development Program of China (2020AAA0107600), NSFC (U19B2035, 62222607, 61972250), STCSM (22511105100), Shanghai Committee of Science and Technology (21DZ1100100).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison among CO networks. Both theoretically and empirically, smaller constraintviolation (CV) leads to better optimization results. Logarithm terms in CV bounds are ignored.\n\nname of self-supervised CO network\n\nErdos Goes Neural (Karalias & Loukas, 2020)\n\nCardNN-S (ours)\n\nCardNN-GS/HGS (ours)\n\nenforce constraint in network architecture\n\nNo (loss penalty term)\n\ntheoretical bound of CV (notations from Sec. 2)\n\nnon-controlled\n\nYes (by Sinkhorn) Yes (by Gumbel-Sinkhorn) (cid:101)O\n\n(cid:16) mτ (|φi−φj |+σ) |φi−φj |2+σ2\n\nmτ |φk−φk+1|\n\n(cid:101)O\n\n(cid:17)\n\n(cid:17)\n\n(cid:16)\n\n∀i̸=j\n\nempirical CV (results from Fig. 3(a)) empirical optimal gap (↓)\n\n8.44 0.152\n\n6.71 0.139\n\n0.09 0.023\n\nCO networks; 2) the natural label-free, self-supervised learning paradigm by directly optimizing over the objective score, which is more practical than supervised learning (Vinyals et al., 2015) and empirically more efficient than reinforcement learning (Schulman et al., 2017); 3) the end-to-end architecture enabling tackling the important predictive CO problems, i.e. decision-making under uncertainty (Wilder et al., 2019; Elmachtoub & Grigas, 2022). In this paper, we follow the general paradigm of learning to solve CO in one-shot presented in the seminal work (Karalias & Loukas, 2020). A neural network CO solver is built upon a problem encoder network, which firstly accepts raw problem data and predicts the decision variables for the problem. The decision variables are then passed to a differentiable formula to estimate the objective score, and finally, the objective score is treated as the self-supervised loss. All modules must be differentiable for end-to-end learning.\n\nAs a CO solver, the output of the network should obey the constraint of the CO problem, while still preserving the gradient. Since the input-output mappings of CO are piece-wise constant, where the real gradient is zero almost everywhere or infinite when the output changes, it is notoriously hard to encode CO constraints in neural networks. There are three typical workarounds available: 1) In Karalias & Loukas (2020), the constraints are softly enforced by a penalty term, and the degree of constraint-violation can be hardly theoretically characterized nor controlled, which limits their applicability in many constraint-critical scenarios. Meanwhile, in the obligatory discretization step, adding penalty terms means that the algorithm must search a much larger space than if it was confined to feasible configurations, making the search less efficient and less generalizable (see Table 1). 2) The perturbation-based black-box differentiation methods (Poganˇci ́c et al., 2019; Paulus et al., 2021; Berthet et al., 2020) resorts to adding perturbation to the input-output mapping of discrete functions to estimate the approximate gradient as such the strict constraints are enforced in brute force, yet their approximate gradients may hurt the learning process. 3) The soft algorithms (Zanfir & Sminchisescu, 2018; Wang et al., 2019a; Sakaue, 2021) encode constraints to neural networks by developing approximate and differentiable algorithms for certain CO problems (graph matching, SAT, submodular), which is followed in this paper for their efficiency, yet there still remains the possibility of facing an arbitrary degree of constraint-violation.\n\nTowards the ultimate goal of devising a general CO network solver addressing all the above issues, in this paper, we focus on developing a more practical paradigm for solving the cardinality-constrained CO problems (Buchbinder et al., 2014). The cardinality constraints ∥x∥0 ≤ k are commonly found in a wide range of applications such as planning facility locations in business operation (Liu, 2009), discovering the most influential seed users in social networks (Chen et al., 2021), and predicting portfolios with controllable operational costs (Chang et al., 2000). Under the cardinality constraint, we aim to find the optimal subset with size k. Likewise other discrete CO constraints, the cardinality constraint is non-trivial to differentiate through. In this paper, we propose to encode cardinality constraints to CO networks by a topk selection over a probability distribution (which is the output of an encoder network). An intuitive approach is to sort all probabilities and select the k-largest ones, however, such a process does not offer informative gradients. Inspired by Cuturi et al. (2019); Xie et al. (2020), we develop a soft algorithm by reformulating the topk selection as an optimal transport problem (Villani, 2009) and efficiently tackle it by the differentiable Sinkhorn algorithm (Sinkhorn, 1964). With a follow-up differentiable computation of the self-supervised loss, we present a CO network whose output is softly cardinality-constrained and capable of end-to-end learning.\n\nHowever, our theoretical characterization of the Sinkhorn-based soft algorithm shows its violation of the cardinality constraint may significantly grow if the values of the k-th and (k + 1)-th probabilities are too close. Being aware of the perturbation-based differentiable methods (Poganˇci ́c et al., 2019; Paulus et al., 2021; Berthet et al., 2020) and the Gumbel trick (Jang et al., 2017; Mena et al., 2018; Grover et al., 2019) that can build near-discrete neural networks, in this paper, we further incorporate the Gumbel trick which is crucial for strictly bounding the constraint-violation term to an arbitrary\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Our CardNN pipeline. The problem encoder and our proposed optimal transport (OT) cardinality layer compose our CO solver network, which has the superiority of guaranteeing a theoretically bounded constraint-violation. The decision variables from the CO network are then utilized to estimate the objective score, i.e. the self-supervised loss. The implementation of the OT cardinality layer and its theoretical characteristics will be discussed in Sec. 2. The other components are problem-dependent and will be discussed in Sec. 3 and Sec. 4 under the context of each problem.\n\nsmall number. Our network takes both advantages of the high efficiency in soft algorithms (Zanfir & Sminchisescu, 2018) and the low constraint-violation in perturbation-based methods (Poganˇci ́c et al., 2019; Jang et al., 2017). A homotopy extension (Xu et al., 2016) is further developed where the constraint-violation term is gradually tightened. Following the self-supervised learning pipeline in Karalias & Loukas (2020), our cardinality-constrained CO networks are validated on two representative deterministic CO tasks: facility location and max covering problems.\n\nAn important application of predictive CO is also addressed, where the problem parameters are unknown at the decision-making time. We present a “predict-and-optimize” network that jointly learns a predictor and a neural network CO solver end-to-end over the final objective score, instead of the two-stage “predict-then-optimize” which learns a predictor first and then optimizes separately, at the risk of optimizing performance being hurt by prediction error. Specifically, towards a practical and widely concerned task: portfolio optimization under uncertainty, we build an end-to-end predictive portfolio optimization model. Experimental results on real-world data show that it outperforms the classic “predict-then-optimize” paradigm. The contributions include:\n\n• New End-to-end One-shot Neural Architecture for CO Problems. We propose the first (to our best knowledge) end-to-end cardinality-constrained neural network for efficient CO problemsolving in one-shot, in the sense that the constraints are incorporated in the network architecture instead of directly putting them in the learning objective as penalty terms. • Theoretical and Empirical Advantages of the CO Architecture. The cardinality constraint is encoded in the differentiable optimal transport layer based on the topk selection technique (Xie et al., 2020). While we further introduce the idea of perturbation as used in blackbox differentiable CO (Poganˇci ́c et al., 2019; Paulus et al., 2021), by incorporating the Gumbel trick to reduce the constraint-violation, and the violation bound is strictly guaranteed by our theoretical results. Empirical results on two CO tasks: facility location and max covering also verify its competitiveness. • Enabling “predict-and-optimize” Paradigm. We show that our new network further enables an emerging end-to-end “predict-and-optimize” paradigm in contrast to the traditional “predict-thenoptimize” pipeline. Its potential is demonstrated by a study on predictive portfolio optimization on real-world asset price data, with an improvement of Sharpe ratio from 1.1 to 2.0, compared with a baseline: LSTM+Gurobi.\n\n2 CARDINLIATY-CONSTRAINED COMBINATORIAL NETWORKS\n\nAn overview of our CardNN pipeline is shown in Fig. 1. Following the general paradigm (Karalias & Loukas, 2020) to tackle CO in one-shot, we introduce an optimal transport (OT) cardinality layer in the neural network CO solver to enforce the constraints upon the output of the problem encoder network, whereby the superiorities could be addressed both empirically and theoretically.\n\nRecall that under cardinality constraint, the solution must have no more than k non-zero elements:\n\nmin x\n\nJ(x)\n\ns.t. ∥x∥0 ≤ k.\n\n(1)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nIn this paper, enforcing the cardinality constraint in networks is formulated as solving OT with differentiable layers (Cuturi, 2013). Denote s = [s1, · · · , sm] as the probability vector predicted by the problem encoder network, our OT layer selects k largest items from s by moving k items to one destination (selected), and the other (m − k) elements to the other destination (not selected). In the following, we present two embodiments of OT layers and their theoretical characteristics.\n\n2.1 CARDNN-S: SINKHORN LAYER FOR CARDINALITY CONSTRAINT\n\nWe follow the popular method Sinkhorn (1964) and define the OT problem as follows. The sources are m candidates in s and the destinations are the min/max values of s. OT moves the topk items to smax, and the others to smin. The marginal distributions (c, r) and distance matrix (D) are defined as:\n\nc = [1 1 ... 1] (cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) m items\n\n, r =\n\n(cid:21)\n\n(cid:20) m − k k\n\n, D =\n\n(cid:20) s1 − smin smax − s1\n\ns2 − smin smax − s2\n\n... ...\n\nsm − smin smax − sm\n\n(cid:21)\n\n.\n\n(2)\n\nThen OT can be formulated as integer linear programming:\n\nmin T\n\ntr(T⊤D)\n\ns.t. T ∈ {0, 1}2×m, T1 = r, T⊤1 = c,\n\n(3)\n\nwhere T is the transportation matrix which is also a feasible decision variable for the cardinality constraint, and 1 is a column vector whose all elements are 1s. The optimal solution T∗ to Eq. (3) should be equivalent to the solution by firstly sorting all items and then selecting the topk items. To make the process differentiable by soft algorithms, the binary constraint on T is relaxed to continuous values [0, 1], and Eq. (3) is modified with an entropic regularization:\n\nmin Tτ\n\ntr(Tτ ⊤D) + τ h(Tτ )\n\ns.t. Tτ ∈ [0, 1]2×m, Tτ 1 = r, Tτ ⊤1 = c,\n\n(4)\n\nwhere h(Tτ ) = (cid:80) ij is the entropic regularizer (Cuturi, 2013). Given any real-valued matrix D, Eq. (4) is solved by firstly enforcing the regularization factor τ : Tτ = exp(−D/τ ). Then Tτ is row- and column-wise normalized alternatively:\n\nij log Tτ\n\ni,j Tτ\n\nDr = diag(Tτ 1 ⊘ r), Tτ = D−1\n\nr Tτ ; Dc = diag(Tτ ⊤1 ⊘ c), Tτ = Tτ D−1\n\nc\n\n,\n\n(5)\n\nwhere ⊘ is element-wise division. We denote Tτ ∗ as the converged solution, which is the optimal solution to Eq. (4). The second row of Tτ ∗ is regarded as the relaxed decision variable for the cardinality constraint: Tτ ∗[2, i] is regarded as the probability that xi should be non-zero. Tτ ∗ is further fed into the objective estimator. Note that Tτ ∗ is usually infeasible in the original problem, and we define the following constraint violation to measure the quality of Tτ ∗.\n\nDefinition 2.1 (Constraint Violation, CV). CV is the expected minimal distance between a relaxed solution t (from distribution T ) and any feasible solution from the feasible set H: CV = Et∈T [minh∈H ∥t − h∥F ]. Apparently, h is the nearest feasible solution to t. Remark 2.2 (Meaning of CV). Take the self-supervised CardNN-S as an example, estimating the objective score (which is exactly the self-supervised loss) based on Tτ ∗ is necessary during training. In inference, the solution must be feasible in the original problem, so the nearest feasible solution T∗ is returned. Actually, in training, the network learns to solve a relaxed, easier version of the original problem, and CV = ∥T∗ − Tτ ∗∥F is an important characteristic measuring the gap between the relaxed problem (in training) and the original problem (in inference). Here T means the distribution of all CardNN-S outputs and is omitted for simplicity. Such a meaning of CV also applies for other self-supervised CO networks. In the following, we theoretically characterize the CV of CardNN-S:\n\nProposition 2.3. Assume that Sinkhorn is converged. The constraint-violation of the CardNN-S is\n\nCVCardNN-S = ∥T∗ − Tτ ∗∥F ≤\n\n2mτ log 2 |φk − φk+1|\n\n.\n\n(6)\n\nWithout loss of generality, φ is denoted as the descending sequence of s, i.e. φk, φk+1 are the kth, (k + 1)-th largest elements of s, respectively. Proposition 2.3 is a straightforward derivation based on Theorem 2 of Xie et al. (2020), and is better than Karalias & Loukas (2020) whose CV is non-controlled. However, as we learn from Eq. (6), the CV of CardNN-S gradually grows if |φk − φk+1| becomes smaller, and turns diverged under the extreme case that φk = φk+1, meaning that its CV cannot be tighten by adjusting the hyperparameter τ . Such a divergence is not surprising\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1: CardNN-GS: Gumbel-Sinkhorn Layer for Cardinality Constraint Input: List s with m items; cardinality k; Sinkhorn factor τ ; noise factor σ; sample size #G.\n\n1 for i ∈ {1, 2, ..., #G} do\n\n2\n\n3\n\n4\n\n5\n\nfor all sj, (cid:101)sj = sj − σ log(− log(uj)), where uj is from (0, 1) uniform distribution; (cid:101)D =\n\n; construct c, r following Eq. (2); (cid:101)Ti = exp(− (cid:101)D/τ );\n\n(cid:21)\n\n(cid:20)\n\n(cid:101)s1 − smin smax − (cid:101)s1 while not converged do\n\n... (cid:101)sm − smin smax − (cid:101)sm ...\n\n(cid:101)Dr = diag( (cid:101)Ti1 ⊘ r); (cid:101)Ti = (cid:101)D−1\n\nr (cid:101)Ti; (cid:101)Dc = diag( (cid:101)T⊤\n\ni 1 ⊘ c); (cid:101)Ti = (cid:101)Ti (cid:101)D−1\n\nc\n\n;\n\nOutput: A list of transport matrices [ (cid:101)T1, (cid:101)T2, ..., (cid:101)T#G].\n\nbecause one cannot decide whether to select φk or φk+1 if they are equal, which is fine if any direct supervision on Tτ ∗ is available. However, as discussed in Remark 2.2, the importance of CV is non-negligible in self-supervised CO networks. Since working with solely the Sinkhorn algorithm reaches its theoretical bottleneck, in the following, we present our improved version by introducing random perturbations (Poganˇci ́c et al., 2019; Jang et al., 2017) to further tighten the CV.\n\n2.2 CARDNN-GS: GUMBEL-SINKHORN LAYER FOR CARDINALITY CONSTRAINT\n\nIn this section, we present our Gumbel-Sinkhorn Layer for Cardinality Constraint as summarized in Alg. 1 and we will theoretically characterize its CV. Following the reparameterization trick (Jang et al., 2017), instead of sampling from a distribution that is non-differentiable, we add random variables to probabilities predicted by neural networks. The Gumbel distribution is:\n\nwhere σ controls the variance and u is from (0, 1) uniform distribution. We can update s and D as:\n\ngσ(u) = −σ log(− log(u)),\n\n(7)\n\n(cid:101)sj = sj + gσ(uj),\n\n(cid:101)D =\n\n(cid:20)\n\n(cid:101)s1 − smin (cid:101)s2 − smin smax − (cid:101)s2 smax − (cid:101)s1\n\n... (cid:101)sm − smin smax − (cid:101)sm ...\n\n(cid:21)\n\n.\n\nAgain we formulate the integer linear programming version of the OT with Gumbel noise:\n\nmin Tσ\n\ntr(Tσ⊤ (cid:101)D)\n\ns.t. Tσ ∈ {0, 1}2×m, Tσ1 = r, Tσ⊤1 = c,\n\n(8)\n\n(9)\n\nwhere the optimal solution to Eq. (9) is denoted as Tσ∗. To make the integer linear programming problem feasible for gradient-based deep learning methods, we also relax the integer constraint and add the entropic regularization term:\n\ntr( (cid:101)T⊤ (cid:101)D) + h( (cid:101)T)\n\ns.t.\n\n(cid:101)T ∈ [0, 1]2×m, (cid:101)T1 = r, (cid:101)T⊤1 = c,\n\n(10)\n\nmin (cid:101)T\n\nwhich is tackled by the Sinkhorn algorithm following Eq. (5). Here we denote the optimal solution to Eq. (10) as (cid:101)T∗. Since Tσ∗ is the nearest feasible solution to (cid:101)T∗, we characterize the constraintviolation as the expectation of ∥Tσ∗ − (cid:101)T∗∥F , and multiple (cid:101)Ts are generated in parallel in practice to overcome the randomness (note that φ is the descending ordered version of s):\n\nProposition 2.4. With probability at least (1 − ε), the constraint-violation of the CardNN-GS is\n\nCVCardNN-GS = Eu\n\n(cid:104) ∥Tσ∗ − (cid:101)T∗∥F\n\n(cid:105)\n\n≤ (log 2)mτ\n\n(cid:88)\n\ni̸=j\n\nΩ(φi, φj, σ, ε),\n\n(11)\n\nwhere Ω(φi, φj, σ, ε) =\n\n(cid:16)\n\n2σ log\n\nσ − |φi−φj |+2σ\n\nlog(1−ε)\n\n(cid:17)\n\n+ |φi − φj|\n\n(cid:16) π\n\n(1 − ε)((φi − φj)2 + 4σ2)(1 + exp φi−φk\n\nσ\n\n(cid:17)\n\n2 + arctan φi−φj )(1 + exp φk+1−φj\n\n2σ\n\n.\n\n)\n\nσ\n\nProof sketch: This proposition is proven by generalizing Proposition 2.3. We denote φπk , φπk+1 as the k-th and (k + 1)-th largest items after perturbed by the Gumbel noise, and our aim becomes to (cid:2)1/(|φπk + gσ(uπk ) − φπk+1 − gσ(uπk+1)|)(cid:3), where the probability prove the upper bound of Eu density function of gσ(uπk ) − gσ(uπk+1 ) can be bounded by f (y) = 1/(y2 + 4). Thus we can compute the bound by integration. See Appendix C.1 for details.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Objective score↑ among perturb-based methods (Poganˇci ́c et al., 2019; Berthet et al., 2020; Amos et al., 2019) on MCP (k=50,m=500,n=1000). Baseline is Xie et al. (2020) used in CardNN-S.\n\nCardNN+Poganˇci ́c et al. (2019) CardNN+Berthet et al. (2020) CardNN+Amos et al. (2019)\n\nCardNN-S CardNN-GS\n\n32499.7\n\n37618.9\n\n38899.6\n\n42034.9\n\n44710.3\n\n(cid:16) mτ (|φi−φj |+σ) |φi−φj |2+σ2\n\n(cid:17)\n\n∀i̸=j\n\nCorollary 2.5. Ignoring logarithm terms for simplicity, CVCardNN-GS ≤ (cid:101)O\n\n(see Appendix C.2 for the proof).\n\nWe compare CV of CardNN-S and CardNN-GS by the toy example in Fig. 2: finding the top3 of [1.0, 0.8, 0.601, 0.6, 0.4, 0.2]. We plot CV w.r.t. different τ, σ values. CV is tightened by larger σ and smaller τ for CardNN-GS, compared to CardNN-S whose violation is larger and can only be controlled by τ . These empirical results are in line with Proposition 2.3 and Proposition 2.4.\n\nHomotopy Gumbel-Sinkhorn. The Corollary 2.5 suggests that CV can be tightened by adjusting τ and σ, motivating us to develop a homotopy (Xiao & Zhang, 2013; Xu et al., 2016) Gumbel-Sinkhorn method where the constraints are gradually tighten (i.e. annealing τ and σ values). In practice, σ is not considered because a larger σ means increased variance which calls for more Gumbel samples. We name the homotopy version as CardNN-HGS.\n\nFigure 2: Toy example.\n\nWe also notice that our CardNN-S (Sec. 2.1) and CardNN-GS (Sec. 2.2) can be unified theoretically: Corollary 2.6. CardNN-S is a special case of CardNN-GS when σ → 0+ (proof in Appendix C.3).\n\n3 ONE-SHOT SOLVING THE DETERMINISTIC CO TASKS\n\nIn this section, we present the implementation details and experiment results for learning to solve two deterministic CO problems in one-shot: facility location problem (FLP) and max covering problem (MCP). Deterministic CO means all problem parameters are known at the decision-making time. Readers are referred to Appendix D for the algorithm details.\n\nThe Facility Location Problem. Given m locations and we want to extend k facilities such that the goods can be stored at the nearest facility and delivered more efficiently (Liu, 2009). The objective is to minimize the sum of the distances between each location and its nearest facility. Problem Formulation: Denote ∆ ∈ Rm×m\n\nas the distance matrix for locations, the FLP is\n\n≥0\n\nmin x\n\nm (cid:88)\n\nj=1\n\nmin({∆i,j| ∀xi = 1})\n\ns.t. x ∈ {0, 1}m, ∥x∥0 ≤ k.\n\n(12)\n\nProblem Encoder: For locations with 2-D coordinates, an edge is defined if two locations are closer than a threshold, e.g. 0.02. We exploit a 3-layer SplineCNN (Fey et al., 2018) to extract features. Objective Estimator: We notice that the min operator in Eq. (12) will lead to sparse gradients. Denote ◦ as element-wise product of a matrix and a tiled vector, we replace min by Softmax with minus temperature −β: (cid:101)Ji = sum(softmax(−β∆ ◦ (cid:101)Ti[2, :]⊤) ◦ ∆), J = mean([ (cid:101)J1, (cid:101)J2, ..., (cid:101)J#G]).\n\nThe Max Covering Problem. Given m sets and n objects where each set may cover any number of objects, and each object is associated with a value, MCP (Khuller et al., 1999) aims to find k sets (k ≪ m) such that the covered objects have the maximum sum of values. This problem reflects realworld scenarios such as discovering influential seed users in social networks (Chen et al., 2021). Problem Formulation: We build a bipartite graph for the sets and objects, whereby coverings are encoded as edges. Denote v ∈ Rn as the values, A ∈ {0, 1}m×n as the adjacency of bipartite graph, I(x) as an indicator I(x)i = 1 if xi ≥ 1 else I(x)i = 0. We formulate the MCP as\n\nn (cid:88)\n\n(cid:32) I\n\n(cid:32) m (cid:88)\n\nj=1\n\ni=1\n\nmax x\n\n(cid:33)\n\n(cid:33)\n\nxiAij\n\n· vj\n\ns.t. x ∈ {0, 1}m, ∥x∥0 ≤ k,\n\n(13)\n\n6\n\n103102101 (log scale)0.000.250.500.751.001.251.501.752.00Constraint-Violation (CV)CardNN-GS (=0.050)CardNN-GS (=0.010)CardNN-GS (=0.005)CardNN-GS (=0.001)CardNN-S (=0.050)CardNN-S (=0.010)CardNN-S (=0.005)CardNN-S (=0.001)Published as a conference paper at ICLR 2023\n\n(a) FLP-Synthetic (k=30, m=500)\n\n(b) FLP-Synthetic (k=50, m=800)\n\n(c) MCP-Synthetic (k=50, m=500, n=1000)\n\n(d) MCP-Synthetic (k=100, m=1000, n=2000)\n\nFigure 3: Plot of objective score, gap w.r.t. inference time on synthetic CO problems. Each scatter dot denotes a problem instance, and the average performance is marked by “×”. In terms of both efficiency and efficacy, our CardNN-S outperforms the EGN CO network whose constraint-violation is non-controlled. The efficacy is further improved by CardNN-GS and CardNN-HGS, even surpassing the state-of-the-art commercial solver Gurobi (better results with less inference time). The Gurobi solver fails to return the optimal solution within 24 hours for MCP, thus not reported here.\n\nProblem Encoder: To encode the bipartite graph, we exploit three layers of GraphSage (Hamilton et al., 2017) followed by a fully-connected layer with sigmoid to predict the probability of selecting each set. Objective Estimator: Based on Eq. (13), the objective value is estimated as:\n\n(cid:101)Ji = min( (cid:101)Ti[2, :]A, 1)⊤ · v, J = mean([ (cid:101)J1, (cid:101)J2, ..., (cid:101)J#G]).\n\n(14)\n\nLearning and Optimization. Based on whether it is a minimization or a maximization problem, J or −J is treated as the self-supervised loss, respectively. The Adam optimizer (Kingma & Ba, 2014) is applied for training. In inference, the neural network prediction is regarded as initialization, and we also optimize the probabilities w.r.t. the objective score by gradients.\n\nExperiment Setup. We follow the self-supervised learning pipeline proposed by the state-of-theart CO network (Karalias & Loukas, 2020), whereby both synthetic data and real-world data are considered. For synthetic data, we build separate training/testing datasets with 100 samples. We generate uniform random locations on a unit square for FLP, and we follow the distribution in ORLIB (Beasley, 1990) for MCP. Due to the lack of large-scale datasets, real-world datasets are only considered for testing (training on synthetic data, testing on real-world data). We test the FLP based on Starbucks locations in 4 cities worldwide with 166-569 stores, and we test MCP based on 6 social networks with 1912-9498 nodes collected from Twitch by Rozemberczki et al. (2021).\n\nBaselines. 1) Greedy algorithms are considered because they are easy to implement but very strong and effective. They have the worst-case approximation ratio of (1 − 1/e) due to the submodular property (Fujishige, 1991) for both FLP and MCP. 2) Integer programming solvers including the state-of-the-art commercial solver Gurobi 9.0 (Gurobi Optimization, LLC, 2021) and the state-ofthe-art open-source solver SCIP 7.0 (Gamrath et al., 2020). The time budgets of solvers are set to be higher than our networks. For 3) CO neural networks, we compare with the state-of-the-art Erdos Goes Neural (EGN) (Karalias & Loukas, 2020) which is adapted from their official implementation: https://github.com/Stalence/erdos_neu. The major difference between EGN and ours is that EGN does not enforce CO constraints by its architecture. Besides, we empirically find out that all self-supervised learning methods converge within tens of minutes. Since the RL methods e.g. Khalil et al. (2017); Wang et al. (2021a) need much more training time, they are not compared.\n\nMetrics and Results. Fig. 3 and 4 report results on synthetic and real-world dataset, respectively. The “gap” metric is computed as gap = |J − J ∗|/ max(J, J ∗), where J is the predicted objective\n\n7\n\n0200400600inference time (seconds)2345678objective (lower is better)0200400600inference time (seconds)0104103102101100optimal gap (lower is better)greedySCIP (120s)Gurobi (120s)Gurobi (optimal)EGNEGN-accuCardNN-SCardNN-GSCardNN-HGS020040060080010001200inference time (seconds)2345678objective (lower is better)02505007501000inference time (seconds)0103102101100optimal gap (lower is better)greedySCIP (200s)Gurobi (200s)Gurobi (optimal)EGNEGN-accuCardNN-SCardNN-GSCardNN-HGS020406080100inference time (seconds)3.63.84.04.24.44.6objective (higher is better)1e4greedySCIP (100s)Gurobi (100s)EGNEGN-accuCardNN-SCardNN-GSCardNN-HGS020406080100inference time (seconds)0105104103102101gap (lower is better)050100150200inference time (seconds)7.07.58.08.59.0objective (higher is better)1e4greedySCIP (120s)Gurobi (120s)EGNEGN-accuCardNN-SCardNN-GSCardNN-HGS050100150200inference time (seconds)0105104103102101gap (lower is better)Published as a conference paper at ICLR 2023\n\n(a) FLP-Starbucks (Euclidean)\n\n(b) FLP-Starbucks (Manhattan)\n\n(c) MCP-Twitch\n\nFigure 4: Plot of optimal gap w.r.t. inference time on real-world CO problems. Our CardNN models are consistently superior than EGN, and are comparative to state-of-the-art SCIP/Gurobi solvers and sometimes can even surpass. On the FLP-Starbucks problems, our CardNN-GS/HGS achieve a lower optimal gap with comparable time cost w.r.t. SCIP/Gurobi. On the MCP-Twitch problems, our CardNN-HGS is slower than SCIP/Gurobi, but it finds all optimal solutions.\n\nand J ∗ is the incumbent best objective value (among all methods). If one of the integer programming solvers proves an optimal solution, we name it as “optimal gap”. Considering both efficiency and efficacy, the performance ranking of CO networks is CardNN-HGS > CardNN-GS > CardNN-S > EGN. This is in line with our theoretical result in Sec. 2: a lower constraint violation will lead to better performance in CO. To justify our selection of Xie et al. (2020) as the base differentiable method, we also implement other perturbation-based differentiable methods and report the MCP results in Table 2. See Appendix E for more details about our deterministic CO experiment.\n\n4 ONE-SHOT SOLVING THE PREDICTIVE CO TASKS\n\nIn this section, we study the interesting and important topic of predictive CO problems where the problem parameters are unknown at the decision-making time. We consider the challenging problem of predicting the portfolio with the best trade-off in risks and returns in the future, under the practical cardinality constraint to control the operational costs. Traditionally, such a problem involves two separate steps: 1) predict the asset prices in the future, probably by some deep learning models; 2) find the best portfolio by solving an optimization problem based on the prediction. However, the optimization process may be misled due to unavoidable errors in the prediction model. To resolve this issue, Solin et al. (2019) proposes to differentiate through unconstrained portfolio optimization via Amos & Kolter (2017), but the more practical cardinality constrained problem is less studied.\n\nProblem Formulation. Cardinality constrained portfolio optimization considers a practical scenario where a portfolio must contain no more than k assets (Chang et al., 2000). A good portfolio aims to have a high return (measured by mean vector μ ∈ Rm) and low risk (covariance matrix Σ ∈ Rm×m). Here we refer to maximizing the Sharpe ratio (Sharpe, 1998). The problem is formulated as\n\nmax x\n\n(μ − rf )⊤x x⊤Σx\n\n√\n\n,\n\ns.t.\n\nm (cid:88)\n\ni=1\n\nxi = 1, x ≥ 0, ∥x∥0 ≤ k,\n\n(15)\n\nwhere x denotes the weight of each asset, rf means risk-free return, e.g. U.S. treasury bonds. Note that μ, Σ are unknown at the time of decision-making, and they are predicted by a neural network.\n\nNetwork Architecture. An encoder-decoder architecture of Long-Short Term Memory (LSTM) modules is adopted as the problem encoder (i.e. price predictor). The sequence of historical daily prices is fed into the encoder module, and the decoder module outputs the predicted prices for the future. We append a fully-connected layer after the hidden states to learn the probabilities for cardinality constraints, followed by our CardNN-GS layers.\n\nObjective Estimator. Based on the network outputs μ, Σ, (cid:101)T, we estimate the value of x by leveraging a closed-form solution of unconstrained Eq. (15): x = Σ−1(μ − rf ), and then enforcing the constraints: x = relu(x ⊙ (cid:101)Ti[2, :]), x = x/sum(x) (⊙ means element-wise product). After obtaining x, we compute the Sharpe ratio based on x and μgt, Σgt computed from the ground truth prices, and use this Sharpe ratio as supervision: (cid:101)Ji = (cid:0)(μgt − rf )⊤x(cid:1) /\n\nx⊤Σgtx\n\n(cid:16)√\n\n(cid:17)\n\n.\n\n8\n\n0100200300400500inference time (seconds)0102101100optimal gap (lower is better)greedySCIP (60s)Gurobi (60s)Gurobi (optimal)EGNEGN-accuCardNN-SCardNN-GSCardNN-HGS0100200300inference time (seconds)0103102101100optimal gap (lower is better)greedySCIP (60s)Gurobi (60s)Gurobi (optimal)EGNEGN-accuCardNN-SCardNN-GSCardNN-HGS0100200300inference time (seconds)0105104103102101optimal gap (lower is better)greedySCIP (optimal)Gurobi (optimal)EGNEGN-accuCardNN-SCardNN-GSCardNN-HGSPublished as a conference paper at ICLR 2023\n\nFigure 5: Return (left) and risk (right) for portfolios by the classic “predict-then-optimize” pipeline (by LSTM for prediction and Gurobi for optimization) and our CardNN-GS for end-to-end “predictand-optimize”. The portfolios proposed by our CardNN-GS has higher returns and lower risks. Since the batch of S&P500 assets violates the cardinality constraint, it is unfair to compare the risk.\n\nTable 3: Our “predict-and-optimize” achieves better risk-return trade-off (Sharpe ratio) though the price prediction is less accurate (by mean square error) than “predict-then-optimize” on test set.\n\nMethods\n\npredictor+optimizer\n\nprediction MSE ↓\n\nSharpe ratio ↑\n\nhistory-opt pred-then-opt pred-and-opt\n\nnone+Gurobi LSTM+Gurobi LSTM+CardNN-GS\n\n(no prediction) 0.153 1.382\n\n0.673 1.082 1.968\n\nSetup and Baselines. The price predictor is supervised with price labels but the optimizer is selfsupervised (no optimal solution labels). We consider portfolio prediction with the best Sharpe ratio in the next 120 trading days (∼24 weeks) and test with the real data in the year 2021. The training set is built based on the prices of 494 assets from the S&P 500 index from 2018-01-01 to 2020-12-30. We set the annual risk-free return as 3% and the cardinality constraint k = 20. The classic “predictthen-optimize” baseline learns the same LSTM model as ours to minimize the prediction square error of the asset prices and optimizes the portfolio by Gurobi based on the price predictions. We also consider a “history-opt” baseline, whereby the optimal portfolio in historical data is followed.\n\nResults. The portfolios are tested on the real data from 01-01-2021 to 12-30-2021, and the results are listed in Fig. 5 and Table 3. On average, we improve the annual return of the portfolio from 24.1% to 40%. The MSE in Table 3 denotes the mean square error of price predictions, and note that more accurate price predictions do not lead to better portfolios. We visualize the predicted portfolios in Fig. 6 and compare it to the efficient frontier (portfolios with optimal risk-return trade-off). Being closer to the frontier means a better portfolio. Also, note that reaching the efficient frontier is nearly impossible as the prediction always contains errors.\n\n5 CONCLUSIONS\n\nFigure 6: Visualization on 2021-03-25 data of individual assets. Larger dots mean higher weights. See more visualizations in Appendix G.\n\nTowards the ultimate goal of developing general paradigms to encode CO constraints into neural networks with controlled constraint-violation bounds, in this paper, we have presented a differentiable neural network for cardinality-constrained combinatorial optimization. We theoretically characterize the constraint-violation of the Sinkhorn network (Sec. 2.1), and we introduce the Gumbel trick to mitigate the constraint-violation issue (Sec. 2.2). Our method is validated in learning to solve deterministic CO problems (on both synthetic and real-world problems) and end-to-end learning of predictive CO problems under the important predict-and-optimize paradigm.\n\n9\n\n-0.4-0.200.20.40.60.811.21.42021/1/12021/2/12021/3/12021/4/12021/5/12021/6/12021/7/1Expected Anual Return Starting DateCardNN-GS: average return=40.0%pred-then-opt: average return=24.1%history-opt: average return=13.9%S&P500: average return=23.50%00.10.20.30.40.50.62021/1/12021/2/12021/3/12021/4/12021/5/12021/6/12021/7/1Expected Risk -Standard DeviationStarting DateCardNN-GS: average risk=18.8%pred-then-opt: average risk=19.5%history-opt: average risk=16.2%0.10.20.30.40.50.6Risk0.20.00.20.40.60.81.01.2Returnefficient frontierCardNN assetsCardNN portfoliohistory-opt assetshistory-opt portfoliopred-then-opt assetspred-then-opt portfolioPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nAkshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter. Differentiable convex optimization layers. Advances in neural information processing systems, 32, 2019.\n\nBrandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.\n\nIn Int. Conf. Mach. Learn., pp. 136–145. PMLR, 2017.\n\nBrandon Amos, Vladlen Koltun, and J. Zico Kolter. The Limited Multi-Label Projection Layer.\n\narXiv preprint arXiv:1906.08707, 2019.\n\nYunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang. Simgnn: A neural network approach to fast graph similarity computation. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pp. 384–392, 2019.\n\nJohn E Beasley. Or-library: distributing test problems by electronic mail. Journal of the operational\n\nresearch society, 41(11):1069–1072, 1990.\n\nQuentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, and Francis Bach. Learning with differentiable pertubed optimizers. Neural Info. Process. Systems, 33:9508– 9519, 2020.\n\nNiv Buchbinder, Moran Feldman, Joseph Naor, and Roy Schwartz. Submodular maximization with cardinality constraints. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pp. 1433–1452. SIAM, 2014.\n\nT-J Chang, Nigel Meade, John E Beasley, and Yazid M Sharaiha. Heuristics for cardinality constrained portfolio optimisation. Computers & Operations Research, 27(13):1271–1302, 2000.\n\nWei Chen, Xiaoming Sun, Jialin Zhang, and Zhijie Zhang. Network inference and influence maxi-\n\nmization from samples. In Int. Conf. Mach. Learn., July 2021.\n\nXinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimiza-\n\ntion. Neural Info. Process. Systems, 32:6281–6292, 2019.\n\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Neural Info.\n\nProcess. Systems, pp. 2292–2300, 2013.\n\nMarco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using optimal transport. In Advances in Neural Information Processing Systems, volume 32, pp. 6858– 6868, 2019.\n\nHanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for struc-\n\ntured data. In Int. Conf. Mach. Learn., pp. 2702–2711. PMLR, 2016.\n\nAdam N Elmachtoub and Paul Grigas. Smart “predict, then optimize”. Management Science, 68(1):\n\n9–26, 2022.\n\nMatthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In\n\nICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nMatthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich M ̈uller. SplineCNN: Fast geometric deep learning with continuous b-spline kernels. In Comput. Vis. Pattern Recog., pp. 869–877, 2018.\n\nMatthias Fey, Jan E Lenssen, Christopher Morris, Jonathan Masci, and Nils M Kriege. Deep graph\n\nmatching consensus. In Int. Conf. Learn. Rep., 2020.\n\nSatoru Fujishige. Submodular Functions and Optimization. Elsevier, 1991.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nGerald Gamrath, Daniel Anderson, Ksenia Bestuzheva, Wei-Kun Chen, Leon Eifler, Maxime Gasse, Patrick Gemander, Ambros Gleixner, Leona Gottwald, Katrin Halbig, Gregor Hendel, Christopher Hojny, Thorsten Koch, Pierre Le Bodic, Stephen J. Maher, Frederic Matter, Matthias Miltenberger, Erik M ̈uhmer, Benjamin M ̈uller, Marc E. Pfetsch, Franziska Schl ̈osser, Felipe Serrano, Yuji Shinano, Christine Tawfik, Stefan Vigerske, Fabian Wegscheider, Dieter Weninger, and Jakob Witzig. The SCIP Optimization Suite 7.0. Technical report, Optimization Online, March 2020. URL http://www.optimization-online.org/DB_HTML/2020/03/7705.html.\n\nAditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization of sorting\n\nnetworks via continuous relaxations. In Int. Conf. Learn. Rep., 2019.\n\nGurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2021. URL https://www.\n\ngurobi.com.\n\nWilliam L. Hamilton, Rex Ying, and Jure Leskovec.\n\nInductive representation learning on large\n\ngraphs. Neural Info. Process. Systems, 2017.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In\n\nInt. Conf. Learn. Rep., 2017.\n\nNikolaos Karalias and Andreas Loukas. Erdos goes neural: an unsupervised learning framework for\n\ncombinatorial optimization on graphs. In Neural Info. Process. Systems, 2020.\n\nElias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial opti-\n\nmization algorithms over graphs. In Neural Info. Process. Systems, pp. 6351–6361, 2017.\n\nSamir Khuller, Anna Moss, and Joseph Seffi Naor. The budgeted maximum coverage problem.\n\nInformation processing letters, 70(1):39–45, 1999.\n\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Int. Conf. Learn.\n\nRep., Dec 2014.\n\nYujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching netIn International Conference on\n\nworks for learning the similarity of graph structured objects. Machine Learning, pp. 3835–3845, 2019.\n\nBaoding Liu. Facility Location Problem, pp. 157–165. Springer Berlin Heidelberg, Berlin, Heidel-\n\nberg, 2009. ISBN 978-3-540-89484-1. doi: 10.1007/978-3-540-89484-1 11.\n\nJing Liu, Fei Gao, and Jiang Zhang. Gumbel-softmax optimization: A simple general framework\n\nfor combinatorial optimization problems on graphs. Arxiv, abs/1909.07018, 2019.\n\nHao Lu, Xingwen Zhang, and Shuang Yang. A learning-based iterative method for solving vehicle\n\nrouting problems. In Int. Conf. Learn. Rep., 2019.\n\nG. Mena, D. Belanger, S. Linderman, and J. Snoek. Learning latent permutations with gumbel-\n\nsinkhorn networks. Int. Conf. Learn. Rep., 2018.\n\nA. Nowak, S. Villar, A. Bandeira, and J. Bruna. Revised note on learning quadratic assignment with\n\ngraph neural networks. In Data Science Workshop, 2018.\n\nAnselm Paulus, Michal Rol ́ınek, V ́ıt Musil, Brandon Amos, and Georg Martius. Comboptnet: Fit the right np-hard problem by learning integer programming constraints. In Int. Conf. Mach. Learn., pp. 8443–8453. PMLR, 2021.\n\nMarin Vlastelica Poganˇci ́c, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. Differ-\n\nentiation of blackbox combinatorial solvers. In Int. Conf. Learn. Rep., 2019.\n\nBenedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-Scale Attributed Node Embedding.\n\nJournal of Complex Networks, 9(2), 2021.\n\nShinsaku Sakaue. Differentiable greedy algorithm for monotone submodular maximization: GuarIn International Conference on Artificial Intelli-\n\nantees, gradient estimators, and applications. gence and Statistics, pp. 28–36. PMLR, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nPaul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Comput. Vis. Pattern Recog., pp. 4938– 4947, 2020.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nWilliam F Sharpe. The sharpe ratio. Streetwise–the Best of the Journal of Portfolio Management,\n\npp. 169–185, 1998.\n\nRichard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matri-\n\nces. AoMS, 1964.\n\nMohammad Maholi Solin, Andry Alamsyah, Brady Rikumahu, and Muhammad Apriandito Arya Saputra. Forecasting portfolio optimization using artificial neural network and genetic algorithm. In 2019 7th International Conference on Information and Communication Technology (ICoICT), pp. 1–7. IEEE, 2019.\n\nC ́edric Villani. Optimal transport: old and new, volume 338. Springer, 2009.\n\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks.\n\nIn Neural Info. Process.\n\nSystems, pp. 2692–2700, 2015.\n\nPo-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. In Int. Conf. Mach. Learn., pp. 6545– 6554. PMLR, 2019a.\n\nRunzhong Wang, Junchi Yan, and Xiaokang Yang. Learning combinatorial embedding networks for\n\ndeep graph matching. In Int. Conf. Comput. Vis., pp. 3056–3065, 2019b.\n\nRunzhong Wang, Junchi Yan, and Xiaokang Yang. Combinatorial learning of robust deep graph\n\nmatching: an embedding based approach. Trans. Pattern Anal. Mach. Intell., 2020.\n\nRunzhong Wang, Zhigang Hua, Gan Liu, Jiayi Zhang, Junchi Yan, Feng Qi, Shuang Yang, Jun Zhou, and Xiaokang Yang. A bi-level framework for learning to solve combinatorial optimization on graphs. In Neural Info. Process. Systems, 2021a.\n\nRunzhong Wang, Junchi Yan, and Xiaokang Yang. Neural graph matching network: Learning lawler’s quadratic assignment problem with extension to hypergraph and multiple-graph matching. Trans. Pattern Anal. Mach. Intell., 2021b.\n\nBryan Wilder, Bistra Dilkina, and Milind Tambe. Melding the data-decisions pipeline: Decisionfocused learning for combinatorial optimization. In AAAI Conf. Artificial Intell., volume 33, pp. 1658–1665, 2019.\n\nLin Xiao and Tong Zhang. A proximal-gradient homotopy method for the sparse least-squares\n\nproblem. SIAM Journal on Optimization, 23(2):1062–1091, 2013.\n\nYujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, and Tomas Pfister. Differentiable top-k with optimal transport. In Neural Info. Process. Systems, volume 33, pp. 20520–20531. Curran Associates, Inc., 2020.\n\nYi Xu, Yan Yan, Qihang Lin, and Tianbao Yang. Homotopy smoothing for non-smooth problems\n\nwith lower complexity than o(1/ε). Neural Info. Process. Systems, 2016.\n\nTianshu Yu, Runzhong Wang, Junchi Yan, and Baoxin Li. Learning deep graph matching with\n\nchannel-independent embedding and hungarian attention. In Int. Conf. Learn. Rep., 2020.\n\nA. Zanfir and C. Sminchisescu. Deep learning of graph matching. In Comput. Vis. Pattern Recog.,\n\npp. 2684–2693, 2018.\n\nCong Zhang, Wen Song, Zhiguang Cao, Jie Zhang, Puay Siew Tan, and Xu Chi. Learning to dispatch for job shop scheduling via deep reinforcement learning. Neural Info. Process. Systems, 33, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA RELATED WORK\n\nCO Networks with Constraints Handling for Deterministic CO. Multi-step methods encode constraints by manually programmed action spaces, and the networks can be learned by supervised labels (Vinyals et al., 2015) or by reinforcement learning (Khalil et al., 2017; Zhang et al., 2020; Chen & Tian, 2019). Controlling constraint-violation is less an issue for supervised or reinforcement learning because the supervision signals are directly passed to the output of neural networks. One-shot CO networks construct the solution by a single forward pass thus being more efficient. The seminal work (Karalias & Loukas, 2020) aims to develop a general pipeline for one-shot CO networks, by softly absorbing the violation of constraint as part of its final loss. However, our analysis shows that such a non-controlled constraint-violation potentially harms problem-solving. There also exist embodiments of constrained CO networks for tailored problems, e.g. the constraint can be encoded as doubly-stochastic matrices in assignment problems (Nowak et al., 2018; Wang et al., 2021b; 2020). These methods can be viewed as special cases of our paradigm (yet the powerful perturbation method is not fully exploited). Liu et al. (2019) can be viewed as a multi-step optimization variant of ours, yet learning is not considered and the constraint-violation issue is not theoretically characterized.\n\nDifferentiable Optimizers for Predictive CO. The major challenge of joint prediction and optimization is making the optimizers differentiable. In Amos & Kolter (2017); Agrawal et al. (2019), a family of convex optimization problems is found feasible to differentiate by their KKT conditions. However, for non-convex CO problems, Poganˇci ́c et al. (2019) explains that the true gradients are meaningless for neural networks. One family of papers propose to incorporate existing solvers and estimate the informative and approximate gradients, including tailoring soft algorithms for specific problems (Zanfir & Sminchisescu, 2018; Wang et al., 2019a; Sakaue, 2021) , or following the perturbation-based blackbox optimization pipeline (Poganˇci ́c et al., 2019; Paulus et al., 2021; Berthet et al., 2020) with certain restrictions such as the formula must be linear. The other paradigm is incorporating neural-network solvers which are naturally differentiable. For example, for graph matching on images (Fey et al., 2020; Sarlin et al., 2020), deep feature predictors and neural matching solvers are learned end-to-end under supervised learning, and their neural solvers leverage the Sinkhorn algorithm (Cuturi, 2013) as a neural network layer. In this paper, our neural network solver is incorporated for the new predictive portfolio optimization task, and our predictand-optimize pipeline does not require the ground truth labels for the optimization problem.\n\nB LIMITATIONS\n\nWe are also aware of the following limitations:\n\n1) Our theoretical analysis mainly focuses on characterizing the constraint-violation. There is an unexplored theoretical aspect about the approximation ratios of our CO networks w.r.t. the optimal solution and the optimal objective score, and we plan to study it in future work.\n\n2) The original EGN pipeline is relatively general for all constraints, and we restrict the scope of this paper within cardinality constraints. We are aware of a potential direction to extend our paper: the cardinality constraints are handled by our method (encoded in the network’s output), and the other constraints are handled in a way similar to EGN (encoded as Lagrange multipliers or penalty terms). In such a sense, the cardinality constraints are handled efficiently while still preserving the generality of EGN.\n\n3) In the predictive CO tasks, the predictor may be, in some degree, coupled with the follow-up neural network solver. In our predictive portfolio optimization experiment, our price predictor cannot generalize soundly for the Gurobi solver, and the Sharpe ratio degenerates to 1.002 if our price predictions are passed to Gurobi.\n\nC PROOF OF THEOREMS\n\nBefore starting the detailed proof of the propositions and corollaries, firstly we recall the notations used in this paper:\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n• T∗ = TopK(D) is the optimal solution of the integer linear programming form of the OT problem Eq. (3), which is equivalent to the solution by firstly sorting all items and then selecting the topk items. If the k-th and (k + 1)-th largest items are equal, the algorithm randomly selects one to strictly satisfy the cardinality constraint;\n\n• Tτ ∗ = Sinkhorn(D) is the optimal solution of the entropic regularized form of the OT\n\nproblem Eq. (4) solved by Sinkhorn algorithm. It is also the output by CardNN-S;\n\n• Tσ∗ = TopK( (cid:101)D) is the optimal solution to the integer linear programming form of the OT problem after being disturbed by the Gumbel noise Eq. (9), which is equivalent to the solution by firstly adding the Gumbel noise, then sorting all items and finally select the topk items. If the perturbed k-th and (k + 1)-th largest items are equal, the algorithm randomly selects one to strictly satisfy the cardinality constraint;\n\n• (cid:101)T∗ = Sinkhorn( (cid:101)D) is the optimal solution of the entropic regularized form of the OT problem after disturbed by the Gumbel noise Eq. (10) solved by Sinkhorn algorithm. It is also the output of our proposed CardNN-GS.\n\nC.1 PROOF OF PROPOSITION 2.4\n\nWe firstly introduce a Lemma which will be referenced in the proof of Proposition 2.4:\n\nLemma C.1. Given real numbers φi, φj, and ui, uj are from i.i.d. (0, 1) uniform distribution. After Gumbel perturbation, the probability that φi + gσ(ui) > φj + gσ(uj) is:\n\nP (φi + gσ(ui) > φj + gσ(uj)) =\n\n1 1 + exp − φi−φj\n\nσ\n\n.\n\n(16)\n\nProof. Since gσ(ui) = −σ log(− log(ui)), P (φi + gσ(ui) > φj + gσ(uj)) is equivalent to the probability that the following inequality holds:\n\nφi − σ log(− log(ui)) > φj − σ log(− log(uj))\n\nAnd we have\n\nφi − φj > σ log(− log(ui)) − σ log(− log(uj)) φi − φj σ\n\n(cid:18) log(ui) log(uj)\n\n> log\n\n(cid:19)\n\nφi−φj\n\nσ >\n\ne\n\nlog(ui) log(uj)\n\nSince uj ∈ (0, 1), log(uj) < 0. Then we have\n\nlog(uj) < log(ui)e−\n\nφi−φj σ\n\nlog (uj) < log\n\n(cid:18)\n\nuexp −\n\ni\n\n(cid:19)\n\nφi−φj σ\n\nuj < uexp −\n\ni\n\nφi−φj σ\n\nSince ui, uj are i.i.d. uniform distributions, the probability when the above formula holds is\n\n(cid:90) 1\n\nexp −\n\n(cid:90) u\n\ni\n\n0\n\n0\n\nφi−φj σ\n\nduj dui =\n\n(cid:90) 1\n\n0\n\nφi−φj σ\n\nuexp −\n\ni\n\ndui =\n\n1 1 + exp − φi−φj\n\nσ\n\nThus the probability that φi + gσ(ui) > φj + gσ(uj) after Gumbel perturbation is:\n\nP (φi + gσ(ui) > φj + gσ(uj)) =\n\n1 1 + exp − φi−φj\n\nσ\n\n14\n\n(17)\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\nPublished as a conference paper at ICLR 2023\n\nIn the following we present the proof of Proposition 2.4:\n\nProof of Proposition 2.4. Recall that we denote Φ = [φ1, φ2, φ3, ..., φm] as the descending-ordered version of s. By perturbing it with i.i.d. Gumbel noise, we have\n\n(cid:101)Φ = [φ1 + gσ(u1), φ2 + gσ(u2), φ3 + gσ(u3), ..., φm + gσ(um)]\n\n(26)\n\nwhere gσ(u) = −σ log(− log(u)) is the Gumbel noise modulated by noise factor σ, and u1, u2, u3, ..., um are i.i.d. uniform distribution. We define π as the permutation of sorting (cid:101)Φ in descending order, i.e. φπ1 + gσ(uπ1), φπ2 + gσ(uπ2), φπ3 + gσ(uπ3 ), ..., φπm + gσ(uπm ) are in descending order.\n\nRecall Proposition 2.3, for φ1, φ2, φ3, ..., φm we have\n\n∥T∗ − Tτ ∗∥F ≤\n\n2mτ log 2 |φk − φk+1|\n\nBy substituting Φ with (cid:101)Φ and taking the expectation over u, we have\n\n(cid:104)\n\nEu\n\n∥Tσ∗ − (cid:101)T∗∥F\n\n(cid:20)\n\n(cid:105)\n\n≤ Eu\n\n2mτ log 2 |φπk + gσ(uπk ) − φπk+1 − gσ(uπk+1)|\n\n(cid:21)\n\n(27)\n\n(28)\n\nBased on Lemma C.1, the probability that πk = i, πk+1 = j is\n\nP (πk = i, πk+1 = j) =\n\n1 1 + exp − φi−φj\n\nσ\n\n(cid:32)k−1 (cid:89)\n\n(cid:88)\n\n∀π\n\na=1\n\n1 1 + exp − φπa −φi\n\nσ\n\nm (cid:89)\n\nb=k+2\n\n1 1 + exp − φj −φπb\n\nσ\n\n(cid:33)\n\n(29)\n\nwhere the first term denotes φi + gσ(ui) > φj + gσ(uj), the second term denotes all conditions that there are (k − 1) items larger than φi + gσ(ui) and the rest items are smaller than φj + gσ(uj).\n\nIn the following we derive the upper bound of Eu\n\n(cid:20)\n\n1 |φπk +gσ(uπk )−φπk+1 −gσ(uπk+1 )|\n\n(cid:21)\n\n. We denote\n\nAi,j as\n\nui, uj ∈ Ai,j,\n\ns.t. φi + gσ(ui) − φj − gσ(uj) > ε\n\n(30)\n\nwhere ε is a sufficiently small number. Then we have\n\n(cid:34)\n\nEu\n\n1 (cid:12) (cid:12)φπk + gσ(uπk ) − φπk+1 − gσ(uπk+1)(cid:12) (cid:12)\n\n(cid:35)\n\n(cid:88)\n\n=\n\ni̸=j\n\n(cid:88)\n\n=\n\ni̸=j\n\nP (πk = i, πk+1 = j) Eui,uj ∈Ai,j\n\n(cid:20)\n\n1 |φi + gσ(ui) − φj − gσ(uj)|\n\n(cid:21)\n\n(cid:32)\n\n1 1 + exp − φi−φj\n\nσ\n\n(cid:32)k−1 (cid:89)\n\n(cid:88)\n\n∀π\n\na=1\n\n1 1 + exp − φπa −φi\n\nσ\n\nm (cid:89)\n\nb=k+2\n\n1 1 + exp − φj −φπb\n\nσ\n\n(cid:20)\n\nEui,uj ∈Ai,j\n\n1 |φi + gσ(ui) − φj − gσ(uj)|\n\n(cid:21)(cid:19)\n\n(cid:32)\n\n(cid:88)\n\n=\n\ni̸=j\n\n1 1 + exp − φi−φj\n\nσ\n\n(cid:32)k−1 (cid:89)\n\n(cid:88)\n\n∀π\n\na=1\n\n1 1 + exp − φπa −φi\n\nσ\n\nm (cid:89)\n\nb=k+2\n\n1 1 + exp − φj −φπb\n\nσ\n\n(cid:20)\n\nEui,uj ∈Ai,j\n\n1 |φi − σ log(− log(ui)) − φj + σ log(− log(uj))|\n\n(cid:21)(cid:19)\n\n(cid:33)\n\n(cid:33)\n\n(cid:88)\n\n=\n\ni̸=j\n\n(cid:32)\n\nf (φi − φj, σ, z)\n\n(cid:32)k−1 (cid:89)\n\n(cid:88)\n\n∀π\n\na=1\n\n1 1 + exp − φπa −φi\n\nσ\n\nm (cid:89)\n\nb=k+2\n\n1 1 + exp − φj −φπb\n\nσ\n\n(cid:33)(cid:33)\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nWe denote f (δ, σ, z) as:\n\nf (δ, σ, z) =\n\n1 1 + exp − δ σ\n\n(cid:20)\n\nEui,uj\n\n1 |δ − σ log(− log(ui)) + σ log(− log(uj))|\n\n(cid:21)\n\ns.t.\n\nδ − σ log(− log(ui)) + σ log(− log(uj)) > z > 0\n\nFor the probability terms in Eq. (34), for all permutations π, there must exist πa, πb, such that\n\n1 1 + exp − φπa −φi 1\n1 + exp − φj −φπb\n\nσ\n\nσ\n\n≤\n\n≤\n\n1 1 + exp − φk−φi\n\nσ\n\n1 1 + exp − φj −φk+1\n\nσ\n\nThus we have\n\nEq. (34) ≤\n\n≤\n\n(cid:88)\n\ni̸=j\n\n(cid:88)\n\ni̸=j\n\n(cid:32)\n\nf (φi − φj, σ, z)\n\n1 1 + exp − φk−φi\n\nσ\n\n1 1 + exp − φj −φk+1\n\nσ\n\n(cid:33)\n\nf (φi − φj, σ, z)\n\n(1 + exp φi−φk\n\nσ\n\n)(1 + exp φk+1−φj\n\nσ\n\n)\n\nBy Eq. (16) in Lemma C.1 and substituting φj − φi by y, we have\n\nEq. (16) ⇒P (gσ(ui) − gσ(uj) > φj − φi) =\n\n1 1 + exp − φi−φj\n\nσ\n\n⇒P (gσ(ui) − gσ(uj) > y) =\n\n⇒P (gσ(ui) − gσ(uj) < y) = 1 −\n\n1 1 + exp y σ\n1 1 + exp y\n\nσ\n\n=\n\n1 1 + exp − y\n\nσ\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\n(39)\n\n(40)\n\n(41)\n\n(42)\n\nwhere the right hand side is exactly the cumulative distribution function (CDF) of standard Logistic distribution by setting σ = 1:\n\nCDF(y) =\n\n1 1 + exp (−y)\n\n(43)\n\nThus − log(− log(ui)) + log(− log(uj)) is equivalent to the Logistic distribution whose probability density function (PDF) is\n\nPDF(y) =\n\ndCDF(y) dy\n\n=\n\n1 exp (−y) + exp y + 2\n\nand in this proof we exploit an upper bound of PDF(y):\n\nPDF(y) =\n\n1 exp (−y) + exp y + 2\n\n≤\n\n1 y2 + 4\n\n(44)\n\n(45)\n\nBased on the Logistic distribution, we can replace −σ log(− log(ui)) + σ log(− log(uj)) by σy where y is from the Logistic distribution. Thus we can derive the upper bound of f (δ, σ, z) as\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nfollows\n\nf (δ, σ, z) =\n\n=\n\n=\n\n=\n\n=\n\n≤\n\n=\n\n≤\n\n=\n\n=\n\n≤\n\n=\n\n=\n\n≤\n\n≤\n\n=\n\n(cid:82) ∞\n\n−δ/σ+z (cid:82) ∞\n\n−δ/σ+z\n\n1 δ+σy\n\nexp (−y)+exp y+2 dy\n\n1\n\nexp (−y)+exp y+2 dy\n\n1\n\n(cid:82) ∞\n\n−δ/σ+z\n\n1 δ+σy\n\nexp (−y)+exp y+2 dy\n\n1\n\n1 −\n\n1 1+exp (δ/σ−z)\n\n(cid:82) ∞\n\n−δ/σ+z\n\n1 δ+σy\n\nexp (−y)+exp y+2 dy\n\n1\n\nexp (δ/σ−z) 1+exp (δ/σ−z)\n\n(cid:82) ∞\n\n−δ/σ+z\n\n1 δ+σy\n\nexp (−y)+exp y+2 dy\n\n1\n\n1 1+exp (−δ/σ+z)\n\n·\n\n·\n\n·\n\n·\n\n1 1 + exp − δ σ\n\n1 1 + exp − δ σ\n\n1 1 + exp − δ σ\n\n1 1 + exp − δ σ\n1 + exp (− δ\n\nσ + z)\n\n(cid:90) ∞\n\n1 + exp − δ σ\n\n−δ/σ+z\n\n1 + exp (− δ\n\nσ + z)\n\n(cid:90) ∞\n\n1 + exp − δ σ\n\n−δ/σ+z\n\n1 δ + σy\n\n1 exp (−y) + exp y + 2\n\ndy\n\n1 δ + σy\n\n1 y2 + 4\n\ndy\n\n(46)\n\n(47)\n\n(48)\n\n(49)\n\n(50)\n\n(51)\n\n1 + exp (− δ\n\nσ + z)\n\n1 + exp − δ σ\n\n1 + exp (− δ\n\nσ + z)\n\n1 + exp − δ σ\n\n1 + exp (− δ\n\nσ + z)\n\n1 + exp − δ σ\n\n1 + exp (− δ\n\nσ + z)\n\n1 + exp − δ σ\n\n1 + exp (− δ\n\nσ + z)\n\n1 + exp − δ σ\n\n1 + exp (− δ\n\nσ + z)\n\n1 + exp − δ σ\n\n1 + exp (− δ\n\nσ + z)\n\n1 + exp − δ σ\n\n1 + exp (− δ\n\nσ + z)\n\n1 + exp − δ σ\n\n1 + exp (− δ\n\nσ + z)\n\n1 + exp − δ σ\n\n1 + exp (− δ\n\nσ + z)\n\n1 + exp − δ σ\n\n·\n\n·\n\n·\n\n·\n\n·\n\n·\n\n·\n\n·\n\n·\n\n·\n\n2σ log (cid:0)(zσ − δ)2 + 4σ2(cid:1) − 2δ arctan\n\n4δ2 + 16σ2\n\n(cid:16) z−δ/σ 2\n\n(cid:17)\n\n− 4σ log z + πδ\n\n(52)\n\n2σ log (cid:0)(zσ + |δ|)2 + 4σ2(cid:1) − 2δ arctan\n\n(cid:16) z−δ/σ 2\n\n(cid:17)\n\n− 4σ log z + πδ\n\n4δ2 + 16σ2\n\n(53)\n\n2σ log (cid:0)(zσ + |δ|)2 + 4σ2(cid:1) − 2δ arctan\n\n(cid:16) z−δ/σ 2\n\n(cid:17)\n\n− 2σ log z2 + πδ\n\n4δ2 + 16σ2\n\n2σ log\n\n(cid:16) (zσ+|δ|)2+4σ2 z2\n\n(cid:17)\n\n− 2δ arctan\n\n(cid:16) z−δ/σ 2\n\n(cid:17)\n\n+ πδ\n\n(cid:16) (zσ+|δ|+2σ)2 z2\n\n4δ2 + 16σ2 (cid:17)\n\n− 2δ arctan\n\n2σ log\n\n(cid:16) z−δ/σ 2\n\n(cid:17)\n\n+ πδ\n\n4δ2 + 16σ2\n\n4σ log\n\n(cid:16) zσ+|δ|+2σ z\n\n(cid:17)\n\n− 2δ arctan\n\n(cid:16) z−δ/σ 2\n\n(cid:17)\n\n+ πδ\n\n4σ log\n\n(cid:16) zσ+|δ|+2σ z\n\n4σ log\n\n(cid:16) zσ+|δ|+2σ z\n\n4δ2 + 16σ2 (cid:17)\n\n(cid:16)\n\n+ δ\n\nπ − 2 arctan\n\n(cid:16) z−δ/σ 2\n\n(cid:17)(cid:17)\n\n4δ2 + 16σ2 (cid:17)\n\n(cid:16)\n\n+ |δ|\n\nπ − 2 arctan\n\n(cid:16) z−δ/σ 2\n\n(cid:17)(cid:17)\n\n4δ2 + 16σ2\n\n4σ log\n\n(cid:16) zσ+|δ|+2σ z\n\n(cid:17)\n\n+ |δ| (cid:0)π − 2 arctan (cid:0)− δ\n\n2σ\n\n(cid:1)(cid:1)\n\n4σ log\n\n(cid:16) zσ+|δ|+2σ z\n\n4δ2 + 16σ2 (cid:17)\n\n+ |δ| (cid:0)π + 2 arctan (cid:0) δ\n\n(cid:1)(cid:1)\n\n2σ\n\n4δ2 + 16σ2\n\n17\n\n(54)\n\n(55)\n\n(56)\n\n(57)\n\n(58)\n\n(59)\n\n(60)\n\n(61)\n\nPublished as a conference paper at ICLR 2023\n\nwhere Eq. (51) is because 0. With probability at least (1 − ε), we have\n\nexp (−y)+exp y+2 ≤ 1\n\n1\n\ny2+4 , and Eq. (59) is because π −2 arctan( z−δ/σ\n\n2\n\n) ≥\n\nz = log\n\n≥ − log (1 − ε)\n\n1 + ε exp δ σ\n1 − ε σ + z)\n\n1 + exp (− δ\n\n1 + exp − δ σ\n\n=\n\n1 1 − ε\n\nThus\n\nf (δ, σ, z) ≤ Eq. (61) =\n\n1 1 − ε\n\n≤\n\n1 1 − ε\n\n4σ log\n\n(cid:16) zσ+|δ|+2σ z\n\n(cid:17)\n\n+ |δ| (cid:0)π + 2 arctan (cid:0) δ\n\n2σ\n\n(cid:1)(cid:1)\n\n(cid:16)\n\n4σ log\n\n4δ2 + 16σ2\n\n(cid:17)\n\nσ − |δ|+2σ\n\nlog(1−ε)\n\n+ |δ| (cid:0)π + 2 arctan (cid:0) δ\n\n2σ\n\n(cid:1)(cid:1)\n\n4δ2 + 16σ2\n\n(62)\n\n(63)\n\n(64)\n\n(65)\n\nThus we have\n\nEq. (39) ≤\n\n\n\n\n\n(cid:88)\n\ni̸=j\n\n(cid:16)\n\n4σ log\n\nσ − |φi−φj |+2σ\n\nlog(1−ε)\n\n(cid:17)\n\n+ |φi − φj|\n\n(cid:16)\n\nπ + 2 arctan\n\n(cid:16) φi−φj\n\n(cid:17)(cid:17)\n\n\n\n2σ\n\n(1 − ε)(4(φi − φj)2 + 16σ2)(1 + exp φi−φk\n\nσ\n\n)(1 + exp φk+1−φj\n\nσ\n\n\n\n)\n\n(66)\n\nIn conclusion, with probability at least (1 − ε), we have\n\nEu\n\n(cid:104) ∥Tσ∗ − (cid:101)T∗∥F\n\n(cid:105)\n\n≤\n\n(cid:88)\n\ni̸=j\n\n(cid:88)\n\n=\n\ni̸=j\n\n(2 log 2)mτ\n\n(cid:16)\n\n4σ log\n\n(cid:16)\n\nσ − |φi−φj |+2σ\n\nlog(1−ε)\n\n(cid:17)\n\n+ |φi − φj|\n\n(cid:16)\n\nπ + 2 arctan φi−φj\n\n2σ\n\n(cid:17)(cid:17)\n\n(1 − ε)(4(φi − φj)2 + 16σ2)(1 + exp φi−φk\n\nσ\n\n)(1 + exp φk+1−φj\n\nσ\n\n)\n\n(log 2)mτ\n\n(cid:16)\n\n2σ log\n\n(cid:16)\n\nσ − |φi−φj |+2σ\n\nlog(1−ε)\n\n(cid:17)\n\n+ |φi − φj|\n\n(67) 2 + arctan φi−φj\n\n2σ\n\n(cid:16) π\n\n(cid:17)(cid:17)\n\n(1 − ε)((φi − φj)2 + 4σ2)(1 + exp φi−φk\n\nσ\n\n)(1 + exp φk+1−φj\n\nσ\n\n)\n\n=(log 2)mτ\n\n(cid:88)\n\ni̸=j\n\nΩ(φi, φj, σ, ε)\n\nAnd we denote Ω(φi, φj, σ, ε) as\n\nΩ(φi, φj, σ, ε) =\n\n(cid:16)\n\n2σ log\n\nσ − |φi−φj |+2σ\n\nlog(1−ε)\n\n(cid:17)\n\n+ |φi − φj|\n\n(cid:16) π\n\n(1 − ε)((φi − φj)2 + 4σ2)(1 + exp φi−φk\n\nσ\n\n(cid:17)\n\n2 + arctan φi−φj )(1 + exp φk+1−φj\n\n2σ\n\nσ\n\n(68)\n\n(69)\n\n(70)\n\n)\n\nC.2 PROOF OF COROLLARY 2.5\n\nCorollary 2.5 is the simplified version of Proposition 2.4 by studying the dominant components.\n\nProof. For Ω(φi, φj, σ, ε) in Proposition 2.4, we have (cid:17)\n\n(cid:16)\n\n2σ log\n\nσ − |φi−φj |+2σ\n\nlog(1−ε)\n\n+ |φi − φj|\n\n(cid:16) π\n\n2 + arctan φi−φj\n\n2σ\n\nΩ(φi, φj, σ, ε) ≤\n\n≤\n\n(1 − ε)((φi − φj)2 + 4σ2)\n\n(cid:16)\n\n2σ log\n\nσ − |φi−φj |+2σ\n\nlog(1−ε)\n\n(cid:17)\n\n+ |φi − φj|π\n\n= O\n\n= (cid:101)O\n\n(1 − ε)((φi − φj)2 + 4σ2) (cid:18) σ log (σ + |φi − φj|) + |φi − φj| (φi − φj)2 + σ2 (cid:19)\n\n(cid:18) σ + |φi − φj| (φi − φj)2 + σ2\n\n(cid:19)\n\n18\n\n(cid:17)\n\n(71)\n\n(72)\n\n(73)\n\n(74)\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Four conditions are considered in our proof. It is worth noting that φi, φj must not lie between φk, φk+1, because we define φk, φk+1 as two adjacent items in the original sorted list.\n\nwhere we regard (1 − ε) as a constant (i.e. assuming high probability), and (cid:101)O(·) means ignoring the logarithm terms.\n\nThen we have\n\n(cid:104)\n\nEu\n\n∥Tσ∗ − (cid:101)T∗∥F\n\n(cid:105)\n\n≤ (log 2)mτ\n\n(cid:88)\n\ni̸=j\n\n(cid:101)O\n\n(cid:18) σ + |φi − φj| (φi − φj)2 + σ2\n\n(cid:19)\n\n= (log 2)mτ (cid:101)O\n\n(cid:18) σ + |φi − φj| (φi − φj)2 + σ2\n\n(cid:19)\n\n∀i̸=j\n\n= (cid:101)O\n\n(cid:18) mτ (σ + |φi − φj|) (φi − φj)2 + σ2\n\n(cid:19)\n\n∀i̸=j\n\n(75)\n\n(76)\n\n(77)\n\nC.3 PROOF AND REMARKS ON COROLLARY 2.6\n\nIn the following, we prove Corollary 2.6 and add some remarks about the relationship between the Sinkhorn and the Gumbel-Sinkhorn methods: the Sinkhorn method (CardNN-S) is a special case of the Gumbel-Sinkhorn method (CardNN-GS) when we set σ → 0+. To more formally address Corollary 2.6, we have the following proposition:\n\nProposition C.2. Assume the values of φk, φk+1 are unique1, under probability at least (1 − ε), we have\n\n(cid:104)\n\nEu\n\nlim σ→0+\n\n∥Tσ∗ − (cid:101)T∗∥F\n\n(cid:105)\n\n≤\n\n(π log 2)mτ (1 − ε)|φk − φk+1|\n\n(78)\n\nwhich differs from the conclusion of Proposition 2.3 by only a constant factor.\n\nProof. Since σ → 0+, the first term in Ω(φi, φj, σ, ε)’s numerator becomes 0. For the second term, we discuss four conditions as shown in Fig. 7, except for the following condition: φi = φk, φj = φk+1.\n\n1For a compact proof, we make this assumption that the values of φk, φk+1 are unique. If there are duplicate values of φk, φk+1, the bound only differs by a constant multiplier, therefore, does not affect our conclusion: Sinkhorn method (CardNN-S) is a special case of the Gumbel-Sinkhorn method (CardNN-GS) when σ → 0+.\n\n19\n\nφkφk+1φiφjφkφk+1φjφiCondition 1Condition 2φkφk+1φiφjφkφk+1φjφiCondition 3Condition 4Published as a conference paper at ICLR 2023\n\nCondition 1. If φi ≥ φk, φj ≤ φk+1 (equalities do not hold at the same time), we have at least φi − φk > 0 or φk+1 − φj > 0. Then we have\n\n1\n\n(1 + exp φi−φk\n\nσ\n\n)(1 + exp φk+1−φj\n\nσ\n\n)\n\nΩ(φi, φj, σ, ε) = 0\n\n= 0\n\nlim σ→0+\n\n⇒ lim σ→0+\n\nCondition 2. For any case that φi < φj, we have φi − φj < 0, thus\n\nlim σ→0+\n\narctan\n\nφi − φj σ\n\n= −\n\nπ 2\n\n+ arctan\n\nπ 2\nΩ(φi, φj, σ, ε) = 0\n\nφi − φj σ\n\n= 0\n\n⇒ lim σ→0+ ⇒ lim σ→0+\n\n(79)\n\n(80)\n\n(81)\n\n(82)\n\n(83)\n\nCondition 3. If φi ≥ φj ≥ φk (equalities do not hold at the same time), we have φi − φk > 0. Then we have\n\nlim σ→0+\n\n1 1 + exp φi−φk\n\nσ\n\n= 0\n\n⇒ lim σ→0+\n\nΩ(φi, φj, σ, ε) = 0\n\n(84)\n\n(85)\n\nCondition 4. If φk+1 ≥ φi ≥ φj (equalities do not hold at the same time), we have φk+1 − φj > 0. Then we have\n\nlim σ→0+\n\n1 1 + exp φk+1−φj\n\nσ\n\n= 0\n\n⇒ lim σ→0+\n\nΩ(φi, φj, σ, ε) = 0\n\n(86)\n\n(87)\n\nTherefore, if φi σ → 0+. Thus we have the following conclusion by only considering φi = φk, φj = φk+1:\n\n̸= φk and φj ̸= φk+1, the second term Ω(φi, φj, σ, ε) degenerates to 0 when\n\n(cid:104)\n\nEu\n\nlim σ→0+\n\n∥Tσ∗ − (cid:101)T∗∥F\n\n(cid:105)\n\n≤\n\n(log 2)mτ\n\n(cid:16)\n\n|φk − φk+1|\n\n(cid:16) π\n\n2 + arctan φk−φk+1\n\n2σ\n\n(1 − ε)(φk − φk+1)2\n\n≤\n\n(π log 2)mτ (1 − ε)|φk − φk+1|\n\n(cid:17)(cid:17)\n\n(88)\n\n(89)\n\nRemarks. Based on the above conclusion, if |φk − φk+1| > 0, with σ → 0+, Eq. (11) degenerates to the bound in Eq. (6) and only differs by a constant factor:\n\nlim σ→0+\n\nEu\n\n(cid:104) ∥Tσ∗ − (cid:101)T∗∥F\n\n(cid:105)\n\n≤\n\n(π log 2)mτ (1 − ε)|φk −φk+1|\n\n(90)\n\nwhere a strong assumption that |φk − φk+1| > 0 is made, and the bound diverges if φk = φk+1. Since φk, φk+1 are predictions by a neural network, such an assumption may not be satisfied. In comparison, given σ > 0, the conclusion with Gumbel noise in Eq. (11) is bounded for any φk, φk+1. The strength of the theoretical results is also validated in experiment (see Tables 5 and 4), including the homotopy version CardNN-HGS.\n\nD ALGORITHM DETAILS FOR SOLVING DETERMINISTIC CO PROBLEMS\n\nDue to limited pages, we do not include detailed algorithm blocks on how to solve deterministic CO problems in the main paper. Here we elaborate on our implementation for solving facility location problem (FLP) in Alg. 2, and max covering problem (MCP) in Alg. 3.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2: CardNN-GS/HGS for Solving the Facility Location Problem Input: the distance matrix ∆; learning rate α; softmax temperature β; CardNN-GS parameters\n\nk, τ, σ, #G.\n\n1 if Training then\n\n2\n\nRandomly initialize neural network weights θ;\n\n3 if Inference then\n\n4\n\nLoad pretrained neural network weights θ; Jbest = +∞;\n\n5 while not converged do\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\ns = SplineCNNθ(∆); [ (cid:101)T1, (cid:101)T2, ..., (cid:101)T#G] = CardNN-GS(s, k, τ, σ, #G); for all i, (cid:101)Ji = sum(softmax(−β∆ ◦ (cid:101)Ti[2, :]) ◦ ∆); J = mean([ (cid:101)J1, (cid:101)J2, ..., (cid:101)J#G]); if Training then\n\nupdate θ with respect to the gradient ∂J\n\n∂θ and learning rate α by gradient descend;\n\nif Inference then\n\nupdate s with respect to the gradient ∂J for all i, (cid:101)Ji = sum(min(∆ ◦ TopK( (cid:101)Ti[2, :]⊤))); Jbest = min([ (cid:101)J1, (cid:101)J2, ..., (cid:101)J#G], Jbest);\n\n∂s and learning rate α by gradient descend;\n\n14 if Homotopy Inference then\n\n15\n\nShrink the value of τ and jump to line 5;\n\nOutput: Learned network weights θ (if training)/The best objective Jbest (if inference).\n\nAlgorithm 3: CardNN-GS/HGS for Solving the Max Covering Problem Input: bipartite adjacency A; values v; learning rate α; CardNN-GS parameters k, τ, σ, #G.\n\n1 if Training then\n\n2\n\nRandomly initialize neural network weights θ;\n\n3 if Inference then\n\n4\n\nLoad pretrained neural network weights θ; Jbest = 0;\n\n5 while not converged do\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\ns = GraphSageθ(A); [ (cid:101)T1, (cid:101)T2, ..., (cid:101)T#G] = CardNN-GS(s, k, τ, σ, #G); for all i, (cid:101)Ji = min( (cid:101)Ti[2, :]A, 1)⊤ · v; J = mean([ (cid:101)J1, (cid:101)J2, ..., (cid:101)J#G]); if Training then\n\nupdate θ with respect to the gradient ∂J\n\n∂θ and learning rate α by gradient ascent;\n\nif Inference then\n\nupdate s with respect to the gradient ∂J for all i, (cid:101)Ji = (TopK( (cid:101)Ti[2, :])A)⊤ · v; Jbest = max([ (cid:101)J1, (cid:101)J2, ..., (cid:101)J#G], Jbest);\n\n∂s and learning rate α by gradient ascent;\n\n13 if Homotopy Inference then\n\n14\n\nShrink the value of τ and jump to line 5;\n\nOutput: Learned network weights θ (if training)/The best objective Jbest (if inference).\n\nE MORE DETAILS ABOUT DETERMINISTIC CO EXPERIMENT\n\nE.1 DATASET DETAILS\n\nThe Starbucks location dataset for FLP. This dataset is built based on the project named Starbucks Location Worldwide 2021 version2, which is scraped from the open-accessible Starbucks store locator webpage3. We analyze and select 4 cities with more than 100 Starbucks stores, which are London (166 stores), New York City (260 stores), Shanghai (510 stores), and Seoul (569 stores). The locations considered are the real locations represented as latitude and longitude. For simplic-\n\n2https://www.kaggle.com/datasets/kukuroo3/starbucks-locations-worldwide-2021-version 3https://www.starbucks.com/store-locator\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Objective score ↓, optimal gap ↓ and inference time (in seconds) ↓ comparison of the facility location problem, including mean and standard deviation computed from all test instances. The problem is to select k facilities from m locations.\n\nEGN/CardNN are CO networks\n\nobjective ↓\n\noptimal gap ↓\n\ntime ↓ (sec)\n\nobjective ↓\n\noptimal gap ↓\n\ntime ↓ (sec)\n\nk=30, m=500\n\nk=50, m=800\n\ngreedy\n\n2.841±0.093\n\n0.167±0.026\n\n1.771±0.017\n\n2.671±0.066\n\n0.168±0.018\n\n4.779±0.035\n\nSCIP 7.0 (t=120s/200s) Gurobi 9.0 (t=120s/200s) Gurobi 9.0 (optimal)\n\nEGN (efficient) EGN (accurate) CardNN-S (Sec. 2.1) CardNN-GS (Sec. 2.2) CardNN-HGS (Sec. 2.2)\n\n4.470±1.918 2.453±0.142 2.365±0.063\n\n3.032±0.195 2.795±0.140 2.753±0.154 2.420±0.072 2.416±0.073\n\n0.348±0.295 0.033±0.042 0.000±0.000\n\n0.217±0.048 0.152±0.035 0.139±0.041 0.023±0.009 0.021±0.009\n\n118.068±48.055 125.589±0.606 314.798±116.858\n\n0.830±0.308 123.559±12.278 7.127±1.241 76.534±6.321 103.742±4.778\n\n5.258±1.018 3.364±0.268 2.221±0.041\n\n2.879±0.155 2.697±0.116 2.462±0.079 2.283±0.050 2.275±0.048\n\n0.552±0.146 0.335±0.055 0.000±0.000\n\n0.226±0.039 0.175±0.031 0.097±0.023 0.027±0.008 0.023±0.007\n\n243.919±54.118 214.360±3.785 648.213±194.486\n\n0.988±0.140 191.091±13.141 6.427±1.050 120.689±2.405 158.400±3.498\n\nity, we do not consider the real-world distances between any two stores; instead, we test with both Euclidean distance and Manhattan distance. We set k=30, and the objective values reported are distances ×100km.\n\nThe Twitch dataset for MCP. This social network dataset is collected by Rozemberczki et al. (2021) and the edges represent the mutual friendships between streamers. The streamers are categorized by their streaming language, resulting in 6 social networks for 6 languages. The social networks are DE (9498 nodes), ENGB (7126 nodes), ES (4648 nodes), FR (6549 nodes), PTBR (1912 nodes), and RU (4385 nodes). The objective is to cover more viewers, measured by the sum of the logarithmic number of viewers. We took the logarithm to enforce diversity because those top streamers usually have the dominant number of viewers. We set k=50.\n\nE.2\n\nIMPLEMENTATION DETAILS\n\nOur algorithms are implemented by PyTorch and the graph neural network modules are based on Fey & Lenssen (2019). In our paper, we optimize the hyperparameters by greedy search on a small subset of problem instances (∼5) and set the best configuration of hyperparameters for CardNNS/GS/HGS. The hyperparameters of EGN (Karalias & Loukas, 2020) are tuned in the same way. Here are the hyperparameters used to reproduce our experiment results:\n\n• For the Max Covering Problem (MCP), we empirically set the learning rate α = 0.1. For the hyperparameters of CardNN, we have τ = 0.05, σ = 0.15 for CardNN-GS, τ = 0.05 for CardNN-S, and τ = (0.05, 0.04, 0.03), σ = 0.15 for the Homotopy version CardNNHGS. We set #G = 1000 samples for CardNN-GS and CardNN-HGS.\n\n• For the Facility Location Problem (FLP), we set the learning rate α = 0.1. For the hyperparameters of CardNN, we have τ = 0.05, σ = 0.25 for CardNN-GS, τ = 0.05 for CardNN-S, and we set τ = (0.05, 0.04, 0.03), σ = 0.25 for the Homotopy version CardNN-HGS. We set #G = 500 samples for CardNN-GS and CardNN-HGS. The softmax temperature for facility location is empirically set as twice of the cardinality constraint: T = 100 if k = 50, T = 60 if k = 30.\n\n• For the Predictive Portfolio Optimization Problem, we set the learning rate α = 10−3. For our CardNN-GS module, we set τ = 0.05, σ = 0.1, and set the Gumbel samples as #G = 1000. During inference, among all 1000 portfolio predictions, we return the best portfolio found based on the predicted prices, and we empirically find such a strategy beneficial for finding better portfolios on the real test set.\n\nAll experiments are done on a workstation with i7-9700K@3.60GHz CPU, 16GB memory, and RTX2080Ti GPU.\n\nE.3 DETAILED EXPERIMENT RESULTS\n\nIn the main paper, we only plot the experiment results on both synthetic datasets and real-world datasets due to limited pages. In Table 4 and 5, we report the digits from the synthetic experiments, which are in line with Fig. 3.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: Objective score ↑, gap ↓, and inference time (in seconds) ↓ of max covering. Under cardinality constraint k, the problem is to select from m sets to cover a fraction of n objects. For the gray entry, the Gurobi solver fails to return the optimal solution within 24 hours, thus reported as out-of-time.\n\nEGN/CardNN are CO networks\n\nobjective ↑\n\ngap ↓\n\ntime ↓ (sec)\n\nobjective ↑\n\ngap ↓\n\ntime ↓ (sec)\n\nk=50, m=500, n=1000\n\nk=100, m=1000, n=2000\n\ngreedy\n\n44312.8±818.4\n\n0.011±0.007\n\n0.024±0.000\n\n88698.9±1217.5\n\n0.008±0.004\n\n0.089±0.001\n\nSCIP 7.0 (t=100s/120s) Gurobi 9.0 (t=100s/120s)\n\n43497.4±875.6 43937.2±791.5\n\nGurobi 9.0 (optimal) OOT\n\nEGN (efficient) EGN (accurate) CardNN-S (Sec. 2.1) CardNN-GS (Sec. 2.2) CardNN-HGS (Sec. 2.2)\n\n37141.4±896.0 39025.2±791.9 42034.9±773.1 44710.3±770.9 44723.9±763.2\n\n0.029±0.011 0.019±0.008 OOT\n\n0.171±0.015 0.129±0.008 0.062±0.008 0.002±0.002 0.002±0.002\n\n100.136±0.097 100.171±0.085 OOT\n\n86269.9±1256.3 86862.1±1630.5 OOT\n\n0.244±0.107 40.542±4.056 4.935±1.167 28.104±0.465 39.575±0.595\n\n74633.7±1449.6 77488.9±1088.2 83289.0±1331.0 89264.8±1232.1 89340.8±1221.6\n\n0.035±0.006 0.028±0.011 OOT\n\n0.165±0.010 0.133±0.006 0.068±0.007 0.001±0.002 0.000±0.001\n\n120.105±0.498 120.277±0.139 OOT\n\n0.525±0.229 93.670±8.797 5.368±1.014 60.685±0.045 89.764±0.128\n\nSome remarks about EGN on real-world dataset. Since the sizes of our real-world problems are relatively small, we mainly adopt a transfer learning setting: the CO networks are firstly trained on the synthetic data, and then tested on the corresponding real-world datasets. All our CardNN models follow this setting. However, the transfer learning ability of EGN seems less satisfying, and we empirically find the performance of EGN degenerates significantly when transferred to a different dataset. In Fig. 4, we exploit the advantage of self-supervised learning for EGN: we allow EGN to be trained in a self-supervised manner on the real-world dataset. To avoid the scatter plots looking too sparse, we ignore the training time cost when plotting Fig. 4 since it does not affect our main conclusion (performance rank: CardNN-HGS > CardNN-GS > CardNN-S > EGN).\n\nWe list the detailed experiment results on real-world problems in Tables 6-19.\n\nTable 6: FLP-Starbucks London dataset (Euclidean distance) time (sec)↓\n\nm=166, k=30\n\nobjective↓\n\nGreedy SCIP 7.0 (t=60s) Gurobi 9.0 (t=60s) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n0.047 0.040 0.040 0.040 0.171 0.171 0.080 0.078 0.054 0.042 0.042\n\n0.6 2.8 7.2 7.2 0.2 25.6 0.1 17.2 8.2 19.8 50.3\n\nTable 7: FLP-Starbucks NewYork dataset (Euclidean distance)\n\nm=260, k=30\n\nobjective↓\n\ntime (sec)↓\n\nGreedy SCIP 7.0 (t=60s) Gurobi 9.0 (t=60s) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n0.033 0.028 0.028 0.028 0.174 0.174 0.089 0.057 0.174 0.030 0.029\n\n0.9 16.5 60.6 126.5 0.1 26.0 0.1 27.0 2.3 20.2 50.8\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nTable 8: FLP-Starbucks Shanghai dataset (Euclidean distance)\n\nm=510, k=30\n\nobjective↓\n\ntime (sec)↓\n\nGreedy SCIP 7.0 (t=60s) Gurobi 9.0 (t=60s) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n0.172 10.484 0.222 0.139 1.561 1.561 0.360 0.360 0.165 0.162 0.155\n\n1.9 106.1 62.3 313.1 0.3 58.5 0.3 56.5 9.0 20.7 51.3\n\nTable 9: FLP-Starbucks Seoul dataset (Euclidean distance) time (sec)↓\n\nm=569, k=30\n\nobjective↓\n\nGreedy SCIP 7.0 (t=60s) Gurobi 9.0 (t=60s) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n0.245 14.530 0.424 0.188 2.680 2.680 0.497 0.497 0.373 0.284 0.212\n\n2.1 145.2 62.9 540.8 0.3 57.9 0.3 63.7 9.0 21.8 52.7\n\nTable 10: FLP-Starbucks London dataset (Manhattan distance)\n\nm=166, k=30\n\nobjective↓\n\ntime (sec)↓\n\nGreedy SCIP 7.0 (t=60s) Gurobi 9.0 (t=60s) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n2.441 2.390 2.390 2.390 4.793 4.793 3.210 3.008 2.688 2.457 2.424\n\n0.5 2.9 2.2 2.2 0.2 19.4 0.1 18.4 4.4 20.6 51.6\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nTable 11: FLP-Starbucks NewYork dataset (Manhattan distance)\n\nm=260, k=30\n\nobjective↓\n\ntime (sec)↓\n\nGreedy SCIP 7.0 (t=60s) Gurobi 9.0 (t=60s) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n2.734 2.565 2.565 2.565 4.066 4.066 3.998 3.500 4.066 2.898 2.845\n\n0.9 9.6 22.9 22.8 0.2 30.6 0.1 27.5 2.4 15.2 30.1\n\nTable 12: FLP-Starbucks Shanghai dataset (Manhattan distance)\n\nm=510, k=30\n\nobjective↓\n\ntime (sec)↓\n\nGreedy SCIP 7.0 (t=60s) Gurobi 9.0 (t=60s) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n9.024 59.626 8.931 8.439 21.566 21.566 17.951 11.601 10.780 8.784 8.774\n\n1.8 101.8 62.2 201.7 0.3 55.2 0.3 53.0 5.1 26.0 58.8\n\nTable 13: FLP-Starbucks Seoul dataset (Manhattan distance) time (sec)↓\n\nm=569, k=30\n\nobjective↓\n\nGreedy SCIP 7.0 (t=60s) Gurobi 9.0 (t=60s) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n10.681 83.952 14.579 9.911 18.206 18.206 15.168 15.069 13.154 10.146 10.003\n\n2.0 95.2 65.7 335.7 0.3 56.6 0.3 64.6 5.1 28.6 63.2\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nTable 14: MCP-Twitch DE dataset\n\nm=n=9498, k=50\n\nobjective↑\n\ntime (sec)↓\n\nGreedy SCIP 7.0 (optimal) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n51452 51481 51481 850 11732 43036 43069 51478 51481 51481\n\n0.3 1.5 5.8 15.5 303.0 15.4 303.2 16.0 28.7 56.1\n\nTable 15: MCP-Twitch ENGB dataset\n\nm=n=7126, k=50\n\nobjective↑\n\ntime (sec)↓\n\nGreedy SCIP 7.0 (optimal) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n26748 26757 26757 5066 7749 18725 19296 26745 26757 26757\n\n0.1 0.3 0.8 7.6 147.6 7.5 147.2 15.6 21.7 42.6\n\nTable 16: MCP-Twitch ES dataset\n\nm=n=4648, k=50\n\nobjective↑\n\ntime (sec)↓\n\nGreedy SCIP 7.0 (optimal) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n25492 25492 25492 1183 1489 17612 17872 25492 25492 25492\n\n0.1 0.3 1.1 3.1 57.6 3.1 58.0 15.7 12.6 24.9\n\nTable 17: MCP-Twitch FR dataset\n\nm=n=6549, k=50\n\nobjective↑\n\ntime (sec)↓\n\nGreedy SCIP 7.0 (optimal) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n39665 39694 39694 21508 21701 32439 32533 39687 39693 39694\n\n0.2 1.2 10.7 6.2 119.2 6.2 119.4 15.8 19.9 39.1\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nTable 18: MCP-Twitch PTBR dataset\n\nm=n=1912, k=50\n\nobjective↑\n\ntime (sec)↓\n\nGreedy SCIP 7.0 (optimal) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n14141 14163 14163 1402 7329 10173 10173 14155 14163 14163\n\n0.0 0.1 0.3 0.8 14.4 0.3 5.3 15.6 10.1 19.7\n\nTable 19: MCP-Twitch RU dataset\n\nm=n=4385, k=50\n\nobjective↑\n\ntime (sec)↓\n\nGreedy SCIP 7.0 (optimal) Gurobi 9.0 (optimal) EGN (train on synthetic) EGN-accu (train on synthetic) EGN (train on test) EGN-accu (train on test) CardNN-S CardNN-GS CardNN-HGS\n\n25755 25778 25778 7762 7917 18148 18156 25776 25778 25778\n\n0.1 0.2 0.5 2.7 51.5 2.6 48.3 15.9 11.9 23.6\n\nE.4 ABLATION STUDY ON HYPERPARAMETERS\n\nFirstly, we want to add some remarks about the selection of hyperparameters:\n\n• #G (number of Gumbel samples): #G affects how many samples are taken during training and inference for CardNN-GS. A larger #G (i.e. more samples) will be more appealing, because CardNN-GS will have a lower variance when estimating the objective score, and it will have a higher probability of discovering better solutions. However, #G cannot be arbitrarily large because the GPU has limited memory, also it is harmful to the efficiency if #G is too large. In experiments, we set an adequate #G (e.g. #G = 1000) and ensure that it can fit into the GPU memory of our workstation (2080Ti, 11G).\n\n• τ (entropic regularization factor of Sinkhorn): Theoretically, τ controls the gap of the continuous Sinkhorn solution to the discrete solution, and a smaller τ will lead to a tightened gap. This property is validated by our theoretical findings in Proposition 2.4. Unfortunately, τ cannot be arbitrarily small, because a smaller τ requires more Sinkhorn iterations to converge. Besides, a smaller τ means the algorithm being closer to the discrete version, and the gradient will be more likely to explode. Therefore, given a fixed number of Sinkhorn iterations (100) to ensure the efficiency of our algorithm, we need trial-and-error to discover the suitable τ for both CardNN-S and CardNN-GS. The grid search results below show that our selection of τ fairly balances the performances of both CardNN-S and CardNN-GS.\n\n• σ (Gumbel noise factor): As derived in Proposition 2.4, a larger σ is beneficial for a tightened constraint-violation term. However, it is also worth noting that σ cannot be arbitrarily large because our theoretical derivation only considers the expectation but not the variance. A larger σ means a larger variance, demanding a larger number of samples and bringing computational and memory burdens. In the experiments, we first determine a τ , and then find a suitable σ by greedy search on a small subset (∼5) of problem instances.\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nWe conduct an ablation study about the sensitivity of hyperparameters by performing an extensive grid search near the configuration used in our max covering experiments (τ = 0.05, σ = 0.15, #G = 1000). We choose the k=50, m=500, n=1000 max covering problem, and we have the following results for CardNN-GS and CardNN-S (higher is better):\n\nTable 20: Ablation study result of CardNN-GS with #G = 1000.\n\nτ =\n\nσ =\n\n0.01\n\n0.05\n\n0.1\n\n0.1 0.15 0.2\n\n42513.4 41456.5 41264.3\n\n44759.2 44710.3 44638.1\n\n45039.5 44837.2 44748.2\n\nTable 21: Ablation study result of CardNN-GS with #G = 800.\n\nτ =\n\nσ =\n\n0.01\n\n0.05\n\n0.1\n\n0.1 0.15 0.2\n\n42511.6 41421.4 41235.9\n\n44754.6 44705.8 44651.5\n\n45037.6 44841.5 44748.6\n\nτ = objective score\n\nTable 22: Ablation study result of CardNN-S. 0.05 42034.9\n\n0.005 42013.3\n\n0.001 35956.6\n\n0.01 42520.8\n\n0.1 40721.2\n\nUnder the configuration used in our paper, both CardNN-S and CardNN-GS have relatively good results. Our grid search result shows that our CardNN-GS is not very sensitive to σ if we have τ = 0.05 or 0.1, and the result of τ = 0.01 is inferior because the Sinkhorn algorithm may not converge. The results of #G = 1000 are all better than #G = 800, suggesting that a larger #G is appealing if we have enough GPU memory. It is also discovered that CardNN-S seems to be able to accept a smaller value of τ compared to CardNN-GS, possibly because adding the Gumbel noise will increase the divergence of elements thus performs in a sense similar to decreasing τ when considering the convergence of Sinkhorn.\n\nF DETAILS OF PREDICTIVE PORTFOLIO OPTIMIZATION\n\nSome details of the portfolio optimization model is omitted due to limited pages. Here we elaborate on the entire process of doing portfolio optimization under the “pred-and-opt” paradigm, with LSTM and our CardNN-GS.\n\nTraining steps:\n\n1. Denote the index of “now” as t = 0. {pt|t < 0} means the percentage change of prices of each day in history, {pt|t ≥ 0} means the percentage change of prices of each day in future.\n\n2. An encoder-decoder LSTM module predicts the prices in the future:\n\n{pt|t ≥ 0}, h = LSTM({pt|t < 0}),\n\nwhere h denotes the hidden state of LSTM.\n\n3. Compute risk and return for the future:\n\nμ = mean({pt|t ≥ 0}), Σ = cov({pt|t ≥ 0}).\n\n4. In the CardNN-GS module, predict s (the probability of selected each asset) from h:\n\ns = fully-connected(h).\n\n28\n\nPublished as a conference paper at ICLR 2023\n\n5. Enforce the cardinality constraint by Gumbel-Sinkhorn layer introduced in Sec 3.2, whereby there are #G Gumbel samples:\n\n{ (cid:101)Ti|i = 1, 2, ..., #G} = Gumbel-Sinkhorn(s)\n\n6. Compute the weights of each asset based on the second row of (cid:101)Ti (rf is risk-free return, set as 3%):\n\nxi = Σ−1(μ − rf ), xi = relu(xi ⊙ (cid:101)Ti[2, :]), xi = xi/sum(x)\n\n7. Based on the ground-truth prices in the future {pgt return:\n\nt |t ≥ 0}, compute the ground truth risk and\n\nμgt = mean({pgt\n\nt |t ≥ 0}), Σgt = cov({pgt\n\nt |t ≥ 0}).\n\n8. Estimate the ground-truth Sharpe ratio in the future, if we invest based on xi:\n\n(cid:101)Ji =\n\n(μgt − rf )⊤xi (cid:112)x⊤\n\ni Σgtxi\n\n.\n\n9. The self-supervised loss is the average over all Gumbel samples:\n\nLoss = −mean( (cid:101)J1, (cid:101)J2, (cid:101)J3, ..., (cid:101)J#G)\n\nTesting steps:\n\nFollow training steps 1-6 to predict μ, Σ, {xi|i = 1, 2, ..., #G}.\n\n7. Estimate the predicted Sharpe ratio in the future, if we invest based on xi:\n\n(cid:101)Ji =\n\n(μ − rf )⊤xi (cid:112)x⊤\n\ni Σxi\n\n.\n\n8. Return xbest = xi with the highest (cid:101)Ji and enforce hard cardinality constraint on xbest by hard topk.\n\n9. Evaluate based on the ground-truth Sharpe ratio:\n\nJ =\n\n(μgt − rf )⊤xbest (cid:113)\n\nx⊤\n\nbestΣgtxbest\n\n.\n\nG VISUALIZATION OF MORE PORTFOLIOS\n\nIn Fig. 8, we provide more visualizations of the portfolios predicted by our “predict-and-optimize” CardNN pipeline (blue), the traditional “predict-then-optimize” pipeline based on LSTM and Gurobi (orange), and the historical-data based “history-opt” (purple). In general, portfolio optimization means a trade-off between risks and returns, and we can draw an efficient frontier where the portfolios on this frontier are the Pareto optimal for risks and returns, i.e. for a portfolio on the efficient frontier, one cannot achieve higher returns unless s/he could accept higher risks. Being closer to the efficient frontier means a portfolio is better. Besides, it is also worth noting that reaching the efficient frontier is nearly infeasible in predictive portfolio optimization because our predictions of future asset prices are always with errors.\n\nH DETAILS ON USING EXISTING ASSETS\n\nThe following open-source resources are used in this paper and we sincerely thank the authors and contributors for their great work.\n\n• Implementation of Erdos Goes Neural. Paper: Karalias & Loukas (2020). URL: https://github.com/Stalence/erdos_neu. No open-source license is found on the GitHub webpage.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\n(a) 2021-01-27\n\n(b) 2021-02-25\n\n(c) 2021-04-23\n\n(d) 2021-05-21\n\n(e) 2021-06-21\n\n(f) 2021-07-06\n\nFigure 8: Visualization of predicted portfolios. The labels denote the starting dates of the portfolios.\n\n• SCIP solver. Paper: Gamrath et al. (2020). URL: https://scip.zib.de/. ZIB\n\nAcademic License.\n\n• ORLIB. Paper:\n\nBeasley (1990).\n\nURL: http://people.brunel.ac.uk/\n\n ̃mastjjb/jeb/orlib/scpinfo.html. MIT License.\n\n• Starbucks\n\nLocations\n\nWorldwide\n\n(2021\n\nversion).\n\nURL:\n\nhttps://www.kaggle.com/datasets/kukuroo3/ starbucks-locations-worldwide-2021-version. CC0: Public Domain License.\n\n• Twitch Social Networks (from MUSAE project). Paper: Rozemberczki et al. (2021). Project URL: https://github.com/benedekrozemberczki/MUSAE Data\n\n30\n\n0.10.20.30.40.50.60.7Risk0.250.000.250.500.751.001.251.50Returnefficient frontierCardNN assetsCardNN portfoliohistory-opt assetshistory-opt portfoliopred-then-opt assetspred-then-opt portfolio0.10.20.30.40.50.60.7Risk0.000.250.500.751.001.251.50Returnefficient frontierCardNN assetsCardNN portfoliohistory-opt assetshistory-opt portfoliopred-then-opt assetspred-then-opt portfolio0.10.20.30.40.5Risk0.500.250.000.250.500.751.001.251.50Returnefficient frontierCardNN assetsCardNN portfoliohistory-opt assetshistory-opt portfoliopred-then-opt assetspred-then-opt portfolio0.10.20.30.40.5Risk0.250.000.250.500.751.001.251.50Returnefficient frontierCardNN assetsCardNN portfoliohistory-opt assetshistory-opt portfoliopred-then-opt assetspred-then-opt portfolio0.10.20.30.40.50.6Risk1.00.50.00.51.0Returnefficient frontierCardNN assetsCardNN portfoliohistory-opt assetshistory-opt portfoliopred-then-opt assetspred-then-opt portfolio0.10.20.30.40.5Risk0.20.00.20.40.60.81.0Returnefficient frontierCardNN assetsCardNN portfoliohistory-opt assetshistory-opt portfoliopred-then-opt assetspred-then-opt portfolioPublished as a conference paper at ICLR 2023\n\nURL: html. GPL-3.0 License.\n\nhttp://snap.stanford.edu/data/twitch-social-networks.\n\nAnd we are also using the Gurobi commercial solver under academic license. See details about Gurobi’s academic license at https://www.gurobi.com/academia/ academic-program-and-licenses/.\n\n31",
    "reference": "# Summary Of The Paper\n\nThe paper provides an improved way for constructing combinatorial optimization networks for problems that involve cardinality constraints.\n\n# Strength And Weaknesses\n\nThe main strength of the paper is providing an extension to combinatorial optimization network framework that allows for bounding the constraint violation, for the case of cardinality constraints. This is achieved by incorporating SOFT top-k approach (Xie et al., NeurIPS'20) and adding a perturbation with Gumbel distribution. The paper is somewhat limited in its scope by the narrow focus on cardinality constraint - it would be stronger if it offered at least one simple example of how the approach can be extended to another constraint type.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper provides a novel, though somewhat incremental, method for handling top-k in combinatorial optimization networks. Instead of just optimal transport with entropy regularization smoothing solved via Sinkhorn algorithm, as in (Xie et al., NeurIPS'20), the authors add perturbation using Gumbel distribution. The authors show that their method, CardNN-GS, has lower theoretical bound on constrain violation than CardNN-S that relies on Xie et al.'s approach. They also show that the advantage holds empirically. The method outperforms the baseline CO approach, EGN by Karalias and Loukas, and performs on par with Gurobi. The paper is written clearly, though figures 3 & 4 are very small and hard to read.\n\n# Summary Of The Review\n\nThe paper is concerned with combinatorial optimization networks, a field of growing recent interest. It focuses on a single type of constraints - cardinality constraint - and introduces an improved method for handling it.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nUNDERSTANDING GRADIENT REGULARIZATION IN DEEP LEARNING: EFFICIENT FINITE-DIFFERENCE COMPUTATION AND IMPLICIT BIAS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGradient regularization (GR) is a method that penalizes the gradient norm of the training loss during training. Although some studies have reported that GR improves generalization performance in deep learning, little attention has been paid to it from the algorithmic perspective, that is, the algorithms of GR that efficiently improve performance. In this study, we first reveal that a specific finite-difference computation, composed of both gradient ascent and descent steps, reduces the computational cost for GR. In addition, this computation empirically achieves better generalization performance. Next, we theoretically analyze a solvable model, a diagonal linear network, and clarify that GR has a desirable implicit bias in a certain problem. In particular, learning with the finite-difference GR chooses better minima as the ascent step size becomes larger. Finally, we demonstrate that finite-difference GR is closely related to some other algorithms based on iterative ascent and descent steps for exploring flat minima: sharpness-aware minimization and the flooding method. We reveal that flooding performs finite-difference GR in an implicit way. Thus, this work broadens our understanding of GR in both practice and theory.\n\n1\n\nINTRODUCTION\n\nExplicit or implicit regularization is a key component for achieving better performance in deep learning. For instance, adding some regularization on the local sharpness of the loss surface is one common approach to enable the trained model to achieve better performance (Hochreiter & Schmidhuber, 1997; Foret et al., 2021; Jastrzebski et al., 2021). In the related literature, some recent studies have empirically reported that gradient regularization (GR), i.e., adding penalty of the gradient norm to the original loss, makes the training dynamics reach flat minima and leads to better generalization performance (Barrett & Dherin, 2021; Smith et al., 2021; Zhao et al., 2022). Using only the information of the first-order gradient seems a simple and computationally friendly idea. Because the first-order gradient is used to optimize the original loss, using its norm is seemingly easier to use than other sharpness penalties based on second-order information such as the Hessian and Fisher information (Hochreiter & Schmidhuber, 1997; Jastrzebski et al., 2021).\n\nDespite its simplicity, our understanding of GR has been limited so far in the following ways. First, we need to consider the fact that GR must compute the gradient of the gradient with respect to the parameter. This type of computation has been investigated in a slightly different context: input-Jacobian regularization, that is, penalizing the gradient with respect to the input dimension to increase robustness against input noise (Drucker & Le Cun, 1992; Hoffman et al., 2019). Some studies proposed the use of double backpropagation (DB) as an efficient algorithm for computing the gradient of the gradient for input-Jacobian regularization, whereas others proposed the use of finite-difference computation (Peebles et al., 2020; Finlay & Oberman, 2021). Second, theoretical understanding of GR has been limited. Although empirical studies have confirmed that the GR causes the gradient dynamics to eventually converge to better minima with higher performance, the previous work provides no concrete theoretical evaluation for this result. Third, it remains unclear whether the GR has any potential connection to other regularization methods. Because the finite difference is composed of both gradient ascent and descent steps by definition, we are reminded of some learning algorithms for exploring flat minima such as sharpness-aware minimization (SAM) (Foret et al.,\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n2021) and the flooding method (Ishida et al., 2020), which are also composed of ascent and descent steps. Clarifying these points would help to deepen our understanding on efficient regularization methods for deep learning.\n\nIn this work, we reveal that GR works efficiently with a finite-difference computation. This approach has a lower computational cost, and surprisingly achieves better generalization performance than the other computation methods. We present three main contributions to deepen our understanding of GR:\n\n• We demonstrate some advantages to using the finite-difference computation. We give a brief estimation of the computational costs of finite difference and DB in a deep neural network and show that the finite difference is more efficient than DB (Section 3.2). We find that a so-called forward finite difference leads to better generalization than a backward one and DB (Section 3.3). Learning with forward finite-difference GR requires two gradients of the loss function, gradient ascent and descent. A relatively large ascent step improves the generalization.\n\n• We give a theoretical analysis of the performance improvement obtained by GR. we analyze the selection of global minima in a diagonal linear network (DLN), which is a theoretically solvable model. We prove that GR has an implicit bias for selecting desirable solutions in the so-called rich regime (Woodworth et al., 2020) which would potentially lead to better generalization (Section 4.2). This implicit bias is strengthened when we use forward finitedifference GR with an increasing ascent step size. In contrast, it is weaken for a backward finite difference, i.e., a negative ascent step.\n\n• Finite-difference GR is also closely related to other learning methods composed of both gradient ascent and descent, that is, SAM and the flooding method. In particular, we reveal that the flooding method performs finite-difference GR in an implicit way (Section 5.2).\n\nThus, this work gives a comprehensive perspective on GR for both practical and theoretical understanding.\n\n2 RELATED WORK\n\nBarrett & Dherin (2021) and Smith et al. (2021) investigated explicit and implicit GR in deep learning. They found that the discrete-time update of the usual gradient descent implicitly regularizes the gradient norm when its dynamics are mapped to the continual-time counterpart. This is referred to as implicit GR. They also investigated explicit GR, i.e., adding a GR term explicitly to the original loss, and reported that it improved generalization performance even further. Jia & Su (2020) also empirically confirmed that the explicit GR gave the improvement of generalization. Barrett & Dherin (2021) characterized GR as the slope of the loss surface and showed that a low GR (gentle slope) prefers flat regions of the surface. Recently, Zhao et al. (2022) independently proposed a similar but different gradient norm regularization, that is, explicitly adding a non-squared L2 norm of the gradient to the original loss. They used a forward finite-difference computation, but its superiority to other computation methods remains unconfirmed.\n\nThe implementation of GR has not been discussed in much detail in the literature. In general, to compute the gradient of the gradient, there are two well-known computational methods: DB and finite difference. Some previous studies applied DB to the regularization of an information matrix (Jastrzebski et al., 2021) and input-Jacobian regularization, i.e., adding the L2 norm of the derivative with respect to the input dimension (Drucker & Le Cun, 1992; Hoffman et al., 2019). Others have used the finite-difference computation for Hessian regularization (Peebles et al., 2020) and input-Jacobian regularization (Finlay & Oberman, 2021). Here, we apply the finite-difference computation to GR and present some evidence that the finite-difference computation outperforms DB computation with respect to computational costs and generalization performance.\n\nIn Section 4, we give a theoretical analysis of learning with GR in diagonal linear networks (DLNs) (Woodworth et al., 2020). The characteristic property of this solvable model is that we can evaluate the implicit bias of learning algorithms (Nacson et al., 2022; Pesme et al., 2021). Our analysis includes the analysis of SAM in DLN as a special case (Andriushchenko & Flammarion, 2022). In contrast to previous work, we evaluate another lower-order term, and this enables us to show that forward finite-difference GR selects global minima in the so-called rich regime.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3 GRADIENT REGULARIZATION\n\nWe consider GR (Barrett & Dherin, 2021; Smith et al., 2021), wherein the squared L2 norm of the gradient is explicitly added to the original loss L(θ) as follows:\n\n ̃L(θ) = L(θ) +\n\nγ 2\n\nR(θ), R(θ) = ∥∇L(θ)∥2,\n\n(1)\n\nwhere ∥ · ∥ denotes the Euclidean norm and γ > 0 is a constant regularization coefficient. We abbreviate the derivative with respect to the parameters ∇θ by ∇. Its gradient descent is given by θt+1 = θt − η∇ ̃L(θt)\n\n(2)\n\nfor time step t = 0, 1, ... and learning rate η > 0. While previous studies have reported that explicitly adding a GR term empirically improves generalization performance, its algorithms and implementations have not been discussed in much detail.\n\n3.1 ALGORITHMS\n\nTo optimize the loss function with GR (1) using a gradient method, we need to compute the gradient of the gradient, i.e., ∇R(θ). As is well studied in input-Jacobian regularization (Drucker & Le Cun, 1992; Hoffman et al., 2019; Finlay & Oberman, 2021), there are two main approaches to computing the gradient of the gradient.\n\nFinite difference: The finite-difference method approximates a derivative by a finite step. In the case of GR, we have ∇R(θt)/2 = (∇L(θ′) − ∇L(θt))/ε + O(ε) with θ′ = θt + ε∇L(θt) for a constant ε > 0. The final term is expressed in Landau notation and is neglected in the computation. We update the GR term by\n\n∆RF (ε) =\n\n∇L(θt + ε∇L(θt)) − ∇L(θt) ε\nWe refer to this gradient as Forward finite-difference GR (F-GR). Because the gradient ∇L(θt) is computed for the original loss, the finite difference (3) requires only one additional gradient computation ∇L(θ′). The order of the computation time is only double that of the usual gradient descent. The finite-difference method also has a backward computation:\n\n(F-GR).\n\n(3)\n\n∆RB(ε) =\n\n∇L(θt) − ∇L(θt − ε∇L(θt)) ε\nIf we allow a negative step size, ∆RB corresponds to ∆RF through ∆RB(ε) = ∆RF (−ε). For a sufficiently small ε, both finite-difference GRs yield the same original gradient ∇R(θ) if we can neglect any numerical instability caused by the limit. The finite-difference method has been used in the literature for the optimization of neural networks, especially for Hessian-based techniques (Bishop, 2006; Peebles et al., 2020). When we need a more precise ∇R, we can use a higher-order approximation, e.g., the centered finite difference, but this requires additional gradient computations, and hence we focus on the first-order finite difference.\n\n(B-GR).\n\n(4)\n\nDouble Backpropagation: The other approach is to apply the automatic differentiation directly to the GR term, i.e., ∇R. For example, its PyTorch implementation is quite straightforward, as shown in Section C.1 of the Appendices. This approach is referred to as DB, which was originally developed for input-Jacobian regularization (Drucker & Le Cun, 1992). We explain more details on the DB computation and its computational graph in Section 3.2. DB, in effect, corresponds to computing the following Hessian-vector product:\n\n∆RDB = H(θt)∇L(θt),\n\n(5)\n\nwhere H(θ) = ∇∇L(θ). The following equation may give us an intuition about the difference between the finite difference and DB alternatives. From the mean value theorem, F-GR is equivalent to\n\n∆RF (ε) =\n\ndsH(θt + s∇L(θt))∇L(θt).\n\n(6)\n\n1 ε\n\n(cid:90) ε\n\n0\n\nWe can interpret the finite difference as taking an average of the curvature (Hessian) along the line of gradient update. For ε → 0, this reduces to ∆RDB.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Finite-difference computation is more efficient than DB computation in wall time. (a) Wall time required for learning with GR in one epoch. For the ResNet, we used ResNet- {18, 34, 50, 101, 152}. (b)Training dynamics in ResNet-18 on CIFAR-10. Learning with F-GR is much faster in wall time.\n\nNote that the difference among these algorithms appears in non-linear models. For a naive linear model Xθ, the squared error loss has a constant Hessian XX ⊤. Therefore, all of ∆R have the same update. We analyze a simple network model with non-linearity on the parameters in Section 4 and reveal the difference of implicit biases.\n\n3.2 COMPUTATIONAL COST\n\nWe clarify the computational efficiencies of each algorithm of GR in deep networks. First, we give a rough estimation of the computational cost by counting the number of matrix multiplication required to compute ∇ ̃L. Consider an L-layer fully connected neural network with a linear output layer: Al = φ(Ul), Ul = WlAl−1 for l = 1, ..., L. Note that Al denotes a batch of activation and WlAl−1 requires a matrix multiplication. We denote the element-wise activation function as φ(·) and weight matrix as Wl. For simplicity, we neglect the bias terms. The number of matrix multiplications required to compute ∇ ̃L is given by\n\nNmul ∼ 6L (for F-GR), 9L (for DB),\n\n(7)\n\nwhere ∼ hides an uninteresting constant shift independent of the depth. One can evaluate Nmul straightforwardly from the computational graph (Figure 2), originally developed for the DB computation of input-Jacobian regularization (Drucker & Le Cun, 1992). In brief, the original gradient ∇L, that is, the backpropagation on the forward pass {A0 → A1 → · · · → AL}, requires 3L matrix multiplications: L for the forward pass, L for backward pass Bl = φ′(Ul)◦(W ⊤ l+1Bl+1), and L for gradient Gl := ∂L/∂Wl = BlA⊤ l−1. Because F-GR is composed of both gradient ascent and descent steps, we eventually need 6L. In contrast, for learning using the DB of GR, we need 3L for ∇L and additional 6L for the GR term. The GR term requires a forward pass of composed of Al, Bl, and Gl obtained in the gradient computation of ∇L. Note that the upper part {A0 → A1 → · · · → BL → · · · → B1} is well known as the DB of input-Jacobian regularization. As pointed out in Drucker & Le Cun (1992), the computation of ∇B1 is equivalent to treating the upper part of the graph as the forward pass and applying backpropagation. It requires 2L multiplications. In our GR case, we have additional L multiplications due to Gl. Because the backward pass doubles the number of required multiplications, we eventually need 2 × (2L + L) = 6L multiplication. Further details are given in Section C.1.\n\nThe results of numerical experiments shown in Figure 1 confirm the superiority of finite-difference GR in typical experimental settings. We trained deep neural networks using an NVIDIA A100 GPU for this experiment. All experiments were implemented by PyTorch. We summarize the pseudo code and implementation of GR and present the detailed settings of all experiments in Section C. Figure 1(a) shows the wall time required for one epoch of training with stochastic gradient descent (SGD) and the objective function (1). We trained various multi-layer perceptrons (MLPs) and residual neural networks (ResNets) with different depths. The wall time increased almost linearly as the depth increased. The slope of the line is different for F-GR and DB, and F-GR was faster. This observation is consistent with the number of multiplications (7). In particular, in ResNet, one of the most typical deep neural networks, learning with finite-difference GR was more than twice as fast as learning with DB. Figure 1(b) confirms that F-GR has fast convergence in ResNet-18 on CIFAR-10. In Figure S.1\n\n4\n\nDepth LResNet - L(a)MLPResNet(b)Wall time [s/epoch]Wall time [s/epoch]Under review as a conference paper at ICLR 2023\n\nFigure 2: Computational graph of DB. Each node with an incoming solid arrow requires one matrix multiplication for the forward pass.\n\n, we also show the convergence measured by the training loss and time steps. All of them showed better convergence for the finite difference.\n\nNote that the finite difference is also better to use from the perspective of memory efficiency. This is because DB requires all of the {Al, Bl, Gl} to be retained for the forward pass, which occupies more memory. It is also noteworthy that in general, it is difficult for theory to completely predict the realistic computational time required because it could heavily depend on the hardware and the implementation framework and does not necessarily correlate well with the number of floating-point operations (FLOPs) (Dehghani et al., 2021). Our result suggests that at least the number of matrix multiplication explains well the superiority of the finite-difference approach in typical settings.\n\n3.3 GENERALIZATION PERFORMANCE\n\nHere, we show that the superiority of finite-difference computation over DB also appears in the eventual performance of trained models. Figures 3 and S.2 show the test accuracy of a 4-layer MLP and ResNet-18 trained by using SGD with GR on CIFAR-10 We trained the models in an exhaustive manner with various values for γ and ε for each algorithm of the GR. For learning with F-GR, the model achieved the highest accuracy on relatively large ascent steps (ε ∼ 0.1). In contrast, learning with B-GR showed a rapid decrease of the performance as the step size ε increased. The highest average test accuracy of F-GR was better than those of B-GR and DB although our purpose is to confirm the difference among the algorithms and not to achieve higher accuracy by tuning both γ and ε. It was (F-GR, B-GR, DB) = (58.6, 58.3, 57.6) ± (0.2, 0.2, 0.2) for MLP and (87.0, 86.2, 86.3) ± (0.2, 0.3, 0.3) for ResNet-18. We also confirmed that the same tendencies appeared in the grid search of ResNet-34 on CIFAR-100 (Figure S.3 ). Moreover, we confirmed that F-GR performed better than B-GR and DB in a more realistic training of a wide residual network (WRN-28-10) on CIFAR-10 and CIFAR-100 with/without data augmentation (Table S.1 ).\n\nIt is noteworthy that the best accuracy of F-GR was obtained close to the line of γ = ε. This line is closely related to SAM algorithm. We explain more details in Section 5.1. We also observed that when the ascent step was too small (e.g., ε ≲ 10−4), numerical instability sometime appeared in the calculation of the finite difference ∆R. Overall, the experiments suggest that F-GR with a large ascent step is better to use for achieving higher generalization performance.\n\nFigure 3: Grid search on learning with different GR algorithms shows the superiority of F-GR and that a relatively large ε achieves a high test accuracy. The color bar shows the average test accuracy over 5 trials. Gray dashed lines indicate γ = ε.\n\n5\n\n0→1→...→−1→→−1→...→11−1Forward Pass for BackwardPass for F-GRB-GRDB(b)F-GRB-GRDB(a)MLPResNet-18Under review as a conference paper at ICLR 2023\n\n4 THEORETICAL ANALYSIS OF IMPLICIT BIAS\n\nAlthough previous work and our experiments in Section 3.3 indicate improvements of prediction performance caused by GR, theoretical understanding of this phenomenon remains limited. Because the gradient norm itself eventually becomes zero after the model achieves a zero training loss, it seems challenging to distinguish the generalization capacity by simply observing the value of the gradient norm after training. In addition, our experiments clarified that the performance also depends on the choice of the algorithm and revealed that the situation is complicated. To attack this problem, we consider a solvable model and reveal that GR methods actually contribute to the selection of global minima and the eventual performance.\n\n4.1 DIAGONAL LINEAR NETWORK\n\nA DLN is a solvable model proposed by Woodworth et al. (2020). It is a linear transformation of input x ∈ Rd defined as ⟨β, x⟩ where β is parameterized by β = w2 − with w = (w+, w−) ∈ R2d. Here, the square of the vector is an element-wise square operation. Suppose we have n training samples (xi, yi) (i = 1, ..., n). The training loss is given by\n\n+ − w2\n\nL(w) =\n\n1 4n\n\nn (cid:88)\n\ni=1\n\n(cid:0)(cid:10)w2\n\n+ − w2\n\n−, xi\n\n(cid:11) − yi\n\n(cid:1)2\n\n.\n\n(8)\n\nConsider continual-time training dynamics dw/dt = −∇L. We set an initialization w+(t = 0) = w−(t = 0) = α0 which is a d-dimensional vector and whose entries are non-zero. We define a data matrix X whose i-th row is given by xi. Woodworth et al. (2020) found that interpolation solutions of usual gradient descent are given by\n\nβ∞(α) = arg min\n\nβ∈Rd s.t. Xβ=y\n\nφα(β)\n\n(9)\n\n√\n\ni=1 α2\n\ni q(βi/α2\n\ni q (cid:0)βi/α2\n\n(cid:1) with q(z) = with α = α0. The potential function φα is given by φα(β) = (cid:80)d 4 + z2 + z arcsinh(z/2). For a larger scale of initialization α, this potential function becomes 2 − closer to L2 regularization as α2 i ) ∼ |βi|2, which corresponds to the L2 min-norm solution of the lazy regime (Chizat et al., 2019). In contrast, for a smaller scale of initialization α, it becomes closer to L1 regularization as α2 i ) ∼ |βi|. In this way, we can observe a one-parameter interpolation between L1 and L2 implicit biases. Deep neural networks in practice acquire rich features depending on data structure and are believed to be beyond the lazy regime. Thus, obtaining an L1 solution by setting small α is referred to as the rich regime and desirable. Previous work has revealed that effective values of α decreases by a larger learning rate in the discrete update (Nacson et al., 2022), SGD (Pesme et al., 2021), and SAM update (Andriushchenko & Flammarion, 2022). These learning methods have an implicit bias that chooses the L1 sparse solution in the rich regime.\n\ni q(βi/α2\n\ni\n\n4.2\n\nIMPLICIT BIAS OF GR\n\nNow, consider gradient descent with F-GR dw/dt = −∇L(w) − γ∆RF (w). We find that the GR has implicit bias for the rich regime, and moreover, the strength of the bias depends on the ascent step size. Theorem 4.1. Assume that (i) the gradient dynamics converges to the interpolation solution satisfying Xβ = y, (ii) L2 norm of the parameter ∥w(t)∥ has a constant upper bound independent of γ and ε, (iii) for sufficiently small γ and ε, the integral of the training loss, i.e., (cid:82) ∞ 0 L(w(t))dt, has a constant upper (lower, respectively) bound R (R) independent of γ and ε. Then, for sufficiently small γ and ε, interpolation solutions are given by β∞(αF -GR) with\n\nThe exponent c∗ ∈ Rd is a non-negative constant vector given by\n\nαF -GR ≤ α0 ◦ exp(−γεc∗ + O(γ2) + O(ε2)).\n\nc∗ =\n\n1\n\n2n2 (X ⊤(Xβ(t = 0) − y))2.\n\n(10)\n\n(11)\n\nNote that the inequality is element-wise. The proof is given in Section A.1. Technically speaking, learning with F-GR requires to evaluate a novel c∗ term, which has not appeared in the analyses of\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Results of training of DLNs using gradient descent with F-GR (γ = 0.02). (a) L1 norm of the solutions, (b) test loss, and (c) the largest eigenvalue of the Hessian of the training loss.\n\nprevious studies. Lemma A.1 clarifies that we can prove the positivity of the seemingly complicated term of c∗ through an integral of the learning dynamics. The assumptions seem rational in the following sense. First, assumption (i) is common in the previous studies on DLNs. Second, Nacson et al. (2022) recently reported that we can obtain interpolation solutions with a smaller parameter norm ∥w(t)∥ using the discrete update with a larger learning rate. Because the interpolation solutions of gradient descent are also those of our learning with GR, assumption (ii) seems rational. The upper bound of assumption (iii) means that the convergence speed of L(w(t)) does not get too small for sufficiently small γ and ε. As a side note, we can replace assumption (iii) with the positive definiteness of a certain matrix (assumption A.2). This is seemingly rather technical, but related to a sufficient condition that the dynamics converge to the global minima. See Section A.2 for details.\n\nThis theorem reveals that GR has an implicit bias to select the L1 solution, that is, the rich regime because α is always smaller than α0. As the ascent step increases, we have an exponentially smaller upper bound and the implicit bias to L1 solution will become stronger. We confirm this dependence of solutions on the ascent step in numerical experiments (Figure 4). As in previous work, we trained DLNs on the synthetic data of a sparse regression problem, where xi ∼ N (μ1, σ2I) and yi ∼ N (⟨β∗, xi⟩ , 0.01), and where β∗ is k∗-sparse with non-zero entries equal to 1/ k∗ (d = 100 and n = 50). Following Nacson et al. (2022), we chose μ = σ2 = 5, where the parameter norm a(t) is suppressed and assumption (ii) ix expected to hold. As the ascent step increases, models trained by F-GR obtain sparser solutions (Figure 4(a)) and better generalization performance (Figure 4(b)). The dashed lines show the results of gradient descent without GR. This result is consistent with our experiments of in more realistic settings (Figure 3) where a relatively large ε achieves the best performance. In Figure 4(c), we also present the largest eigenvalue of the Hessian (S.50 ), computed after training. As the ascent step size increases, F-GR chooses flatter minima. This is also consistent with empirical observations in previous studies on GR. Note that B-GR can potentially make the bound looser as the step size ε increases since B-GR is equivalent to F-GR with −ε. Actually, we can immediately find a lower bound αB-GR ≳ C ◦ exp(γεc∗) for a positive constant vector C, as is remarked in Section A.1. The results of numerical experiments on DLNs shown in Figure S.3 confirm that learning with F-GR achieved better generalization performance than B-GR.\n\n√\n\nWhile Theorem 4.1 gives us insight into the finite-difference GR, the upper bound converges to α0 for the DB limit (ε → 0+) and becomes meaningless. Fortunately, we can construct an upper bound applicable to the DB limit.\n\nProposition 4.2. Under the same assumptions as in Theorem 4.1, for sufficiently small ε and γ,\n\nαF -GR ≤ α0 ◦ exp(−γc + O(γ2) + O(ε2)),\n\n(12)\n\nwhere the exponent c ∈ Rd is a non-negative variable given by c = n−2 (cid:82) ∞\n\n0 (X ⊤(Xβ(s) − y))2ds.\n\nIts derivation is given in Section A.3. One can regard this proposition as a minor extension of Theorem 1 in Andriushchenko & Flammarion (2022), which has investigated γ = ε. This setting has a special meaning as we mention in Section 5.1. From the proposition, one can see that the DB limit still has the implicit bias to select the rich regime. This is consistent with the numerical experiments in Figure 4 where the limit of small ε achieves slightly better and sparser solutions than GD without GR. Although the bound (12) is informative, it is difficult to evaluate a concrete value of c. As a side note, we can make a bound of the average over entries, that is, (cid:80)d i=1 ci/d ≥ (4n/d)λmin(XX ⊤)R. See Section A.3 for details.\n\n7\n\n(a)(b)(c)maxUnder review as a conference paper at ICLR 2023\n\n5 GR IN GRADIENT-ASCENT-AND-DESCENT LEARNING\n\nWe have revealed that learning with finite-difference GR, F-GR in particular, improves performance. We recall that the GR is composed of both gradient ascent and descent steps. This computation makes the GR essentially related to two other learning methods similarly composed of both gradient ascent and descent steps: the SAM algorithm and the flooding method.\n\n5.1 CONNECTION WITH SAM\n\nThe SAM algorithm was derived from the minimization of a surrogate loss max∥ε∥≤ρ L(θ + ε) for a fixed ρ > 0, and has achieved the highest performance in various models (Foret et al., 2021). After some heuristic approximations, its update rule reduces to iterative gradient ascent and descent steps: θt+1 = θt − η∇L(θ′) with θ′ = θt + εt∇L(θt) and εt = ρ/∥∇L(θt)∥. Under a specific condition, the SAM update can be seen as gradient descent with F-GR. Let us consider time-dependent regularization coefficient γt and ascent step εt. Then, for γt = εt, the gradient descent with F-GR becomes equivalent to the SAM update: γt εt\n\n(∇L(θ′) − ∇L(θ)) = ∇L(θ′).\n\n∇L(θ) +\n\n(13)\n\nA similar equivalence has been pointed out in Zhao et al. (2022) which supposes a non-squared gradient norm and εt = ρ/∥∇L(θt)∥ naturally appears. Let us focus on the SAM update without the gradient normalization for simplicity, that is, εt = ρ. This simplified SAM update was analyzed on DLNs in Andriushchenko & Flammarion (2022). We can recover the SAM case by setting a sufficiently small γ = ε in Proposition 4.2. Although it will be curious to identify any optimal setting of (γ, ε), our analysis is limited to the range of the first-order Taylor expansion and characterizing any optimal setting seems beyond the scope of our analysis. In Figure 3, we empirically observed the optimal setting for generalization was very close to or just on the line γ = ε. In contrast, our Figures 4, S.3 and the previous study Zhao et al. (2022) demonstrated that the optimal setting was not necessarily on γ = ε, and thus combining the ascent and descent steps would be still promising.\n\n5.2 FLOODING PERFORMS GR IN AN IMPLICIT WAY\n\nThe flooding method (Ishida et al., 2020) is another learning algorithm composed of both gradient ascent and descent steps. Its update rule is given by\n\nθt+1 = θt − ηSign(L − b)∇L\n\n(14)\n\nfor a constant b > 0, referred to as the flood level. When the training loss becomes lower than the flood level, the sign of the gradient is flipped and the parameter is updated by gradient ascent. Therefore, the flooding causes the training dynamics to continue to wander around L(θ) ∼ b, and its gradient continues to take a non-zero value. This would seem a kind of early stopping, but previous work empirically demonstrates that flooding performs better than naive early stopping and finds flat minima. For simplicity, let us focus on the gradient descent for a full batch. The following theorem clarifies a hidden mechanism of flooding.\n\nTheorem 5.1. Consider the time step t satisfying L(θt) < b and L(θt+1) > b. Then, the flooding update from θt to θt+2 is equivalent to the gradient of the F-GR with ε = γ = η:\n\nθt+2 = θt − η2 ∇L(θt + η∇L(θt)) − ∇L(θt)\n\nη\n\n.\n\n(15)\n\nSimilarly, for L(θt) > b and L(θt+1) < b, the flooding update is equivalent to the gradient of the B-GR.\n\nAlthough its derivation is quite straightforward (see Section B), this essential connection between finite-difference GR and flooding has been missed in the literature. Ishida et al. (2020) conjectured that flooding causes a random walk on the loss surface and this would contribute to the search for flat minima in some ways. Our result implies that the dynamics of flooding are not necessarily random and it can actively search the loss surface in a direction that decreases the GR. This is consistent with the observations that the usual gradient descent with GR finds flat minima (Barrett & Dherin, 2021; Zhao et al., 2022). Note that the ascent step is given by the learning rate η, and η is usually decayed\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Flooding decreases the gradient norm, as expected by theory. (a) Training dynamics of flooding with b = 0.05. (b) Test accuracy and gradient norm after the training.\n\nin the training. This implies that because the ascent step size is relatively small, the implicit B-GR in the flooding update would not make the generalization performance much worse.\n\nFigure 5 empirically confirms that the flooding method decreases the gradient norm R(θ). We trained ResNet-18 on CIFAR-10 by using flooding. Figure 5(a) shows that at the beginning of the training, the training loss decreases in the usual way because the loss is far above flood level b. Around the 10th epoch, the loss value becomes sufficiently close to the flood level for the decrease in the loss to slow (Figure S.5 ). Then, the flooding update becomes dominant in the dynamics the gradient norm begins to decrease. Figure 5(b) demonstrates that the gradient norm of the trained model decreases as the initial learning rate increases. This is consistent with Theorem 5.1 because the theorem claims that the larger learning rate induces the larger regularization coefficient of the GR γ = η. In contrast, naive SGD training without flooding always reaches an almost zero gradient norm regardless of the learning rate. Thus, the change in the gradient norm depending on the learning rate is specific to flooding and implies that it implicitly performs GR.\n\n6 DISCUSSION\n\nThis work presented novel practical and theoretical insights into GR. The finite-difference computation is effective in the sense of both reducing computational cost and improving performance. Theoretical analysis supports the empirical observation that the forward difference computation has an implicit bias that chooses potentially better minima depending on the size of the ascent step. Because deep learning requires large-scale models, it would be reasonable to use learning methods only composed of first-order descent or ascent gradients. The current work suggests that the F-GR is a promising direction for further investigation and could be extended for our understanding and practical usage of gradient-based regularization.\n\nWe suggest several potentially interesting research directions. From a broader perspective, we may regard finite-difference GR, SAM, and flooding as a single learning framework composed of iterative gradient ascent and descent steps. It would be interesting to investigate if there is optimal combination of these steps for further improving performance. As our experiments suggest, only using the gradient descent or ascent does not necessarily achieve the best performance, and a combination of them seems to be the best approach. Similar results were empirically observed in other gradient-based regularization techniques (Zhao et al., 2022; Zhuang et al., 2022). Related to the combination between the gradient descent and ascent, although we fixed the ascent step size as a constant, a step size decay or any scheduling could enhance the performance further. For instance, Zhuang et al. (2022) used a time-step dependent ascent step to achieve high prediction performance for SAM.\n\nIt will also be interesting to explore any theoretical clarification beyond the scope of DLNs. Although a series of analyses in DLNs enable us to explore the implicit bias for selecting global minima, it assumes global convergence and avoids an explicit evaluation of convergence dynamics. Thus, it would be informative to explore the convergence rate or escape from local minima in other solvable models or a more general formulation if possible. Constructing generalization bounds would also be an interesting direction. Some theoretical work has proved that regularizing first-order derivatives of the network output control the generalization capacity (Ma & Ying, 2021), and such derivatives are included in the gradient norm as a part. We expect that the current work will serve as a foundation for further developing and understanding regularization methods in deep learning.\n\n9\n\n(b)(a)Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMaksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minIn International Conference on Machine Learning (ICML), pp. 639–668. PMLR,\n\nimization. 2022.\n\nDavid GT Barrett and Benoit Dherin. Implicit gradient regularization. In International Conference\n\non Learning Representations (ICLR), 2021.\n\nChristopher M Bishop. Pattern recognition and machine learning. Springer, 2006.\n\nLenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.\n\nAdvances in Neural Information Processing Systems (NeurIPS), 32, 2019.\n\nMostafa Dehghani, Yi Tay, Anurag Arnab, Lucas Beyer, and Ashish Vaswani. The efficiency\n\nmisnomer. In International Conference on Learning Representations (ICLR), 2021.\n\nHarris Drucker and Yann Le Cun. Improving generalization performance using double backpropaga-\n\ntion. IEEE Transactions on Neural Networks, 3(6):991–997, 1992.\n\nChris Finlay and Adam M. Oberman. Scaleable input gradient regularization for adversarial robust-\n\nness. Machine Learning with Applications, 3:100017, 2021.\n\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnamy Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations (ICLR), 2021.\n\nSepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural computation, 9(1):1–42, 1997.\n\nJudy Hoffman, Daniel A Roberts, and Sho Yaida. Robust learning with Jacobian regularization.\n\narXiv:1908.02729, 2019.\n\nTakashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, and Masashi Sugiyama. Do we need zero training loss after achieving zero training error? In International Conference on Machine Learning (ICML), pp. 4604–4614. PMLR, 2020.\n\nStanislaw Jastrzebski, Devansh Arpit, Oliver Astrand, Giancarlo B Kerg, Huan Wang, Caiming Xiong, Richard Socher, Kyunghyun Cho, and Krzysztof J Geras. Catastrophic Fisher explosion: Early phase Fisher matrix impacts generalization. In International Conference on Machine Learning (ICML), pp. 4772–4784. PMLR, 2021.\n\nZhiwei Jia and Hao Su. Information-theoretic local minima characterization and regularization. In\n\nInternational Conference on Machine Learning (ICML), pp. 4773–4783. PMLR, 2020.\n\nChao Ma and Lexing Ying. On linear stability of SGD and input-smoothness of neural networks.\n\nAdvances in Neural Information Processing Systems (NeurIPS), 34, 2021.\n\nMor Shpigel Nacson, Kavya Ravichandran, Nathan Srebro, and Daniel Soudry. Implicit bias of the step size in linear diagonal neural networks. In International Conference on Machine Learning (ICML), pp. 16270–16295. PMLR, 2022.\n\nWilliam Peebles, John Peebles, Jun-Yan Zhu, Alexei Efros, and Antonio Torralba. The Hessian penalty: A weak prior for unsupervised disentanglement. In European Conference on Computer Vision (ECCV), pp. 581–597. Springer, 2020.\n\nScott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of SGD for diagonal linear networks: a provable benefit of stochasticity. Advances in Neural Information Processing Systems (NeurIPS), 34:29218–29230, 2021.\n\nSamuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit regularization in stochastic gradient descent. In International Conference on Learning Representations (ICLR), 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nBlake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Conference on Learning Theory (COLT), pp. 3635–3673. PMLR, 2020.\n\nYang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving generalization in deep learning. In International Conference on Machine Learning (ICML), volume 162, pp. 26982–26992. PMLR, 2022.\n\nJuntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha C Dvornek, sekhar tatikonda, James s Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware training. In International Conference on Learning Representations (ICLR), 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAppendices\n\nA ANALYSIS BY DIAGONAL LINEAR NETWORKS\n\nA.1 PROOF OF THEOREM 4.1\n\nA.1.1\n\nINTERPOLATION SOLUTIONS BETWEEN L1 AND L2 REGULARIZATION\n\nWe consider the training dynamics with F-GR as\n\n ̇wt = −∇L(wt) − γ\n\n∇L(wt + ε∇L(wt)) − ∇L(wt) ε\n\n= −q1∇L(wt) − q2∇L(wt + ε∇L(wt)),\n\n(S.1)\n\n(S.2)\n\nwhere q1 = (1 − γ/ε), q2 = γ/ε. The training loss L(w) is defined in (8). The dynamics are rewritten as\n\ndw(t) dt\n\n= −\n\nq1 n\n\n( ̃X ⊤r(t)) ◦ w(t) −\n\nq2 n\n\n( ̃X ⊤r∗(t)) ◦ w∗(t),\n\n(S.3)\n\nwhere ◦ denotes the element-wise product between vectors. We defined r(t) = ̃Xw(t)2 − y, r∗(t) = ̃Xw∗(t)2 − y, w∗(t) = w(t) + ε∇L(w(t)), and put ̃X = [ X −X ] ∈ Rn×2d. We recall that the square of the vector is an element-wise square operation. The general solution of (S.3 ) is written as\n\nw(t) =\n\n(cid:21)\n\n(cid:20)α0 α0\n\n(cid:18)\n\n◦ exp\n\n−\n\n(cid:18)\n\n◦ exp\n\n−\n\n ̃X ⊤\n\n1 n\n\n(cid:90) t\n\n0\n\n(q1r(s) + q2r∗(s))ds\n\n(cid:19)\n\nq2ε n2\n\n(cid:90) t\n\n0\n\n( ̃X ⊤r∗(s)) ◦ ( ̃X ⊤r(s))ds\n\n(cid:19)\n\n.\n\n(S.4)\n\nThis recovers the GD solution obtained by Woodworth et al. (2020) for (q1, q2) = (1, 0), and SAM solution by Andriushchenko & Flammarion (2022) for (q1, q2) = (0, 1). To evaluate the effect of both ε and γ on the implicit bias, we need a lower-order evaluation compared to previous work.\n\nSuppose an interpolation solution β∞ satisfying Xβ∞ = y. We can represent it by\n\nβ∞ = w+(∞)2 − w−(∞)2\n\n= 2α2\n\nF -GR ◦ sinh (cid:0)X ⊤ν(cid:1) ,\n\n(S.5)\n\n(S.6)\n\nwhere ν = − 2\n\nn\n\n(cid:82) ∞ 0 (q1r(s) + q2r∗(s))ds and\n\nαF -GR := α0 ◦ exp\n\n(cid:16)\n\n−\n\n(cid:17)\n\nγ\n\nn2 Ψ\n\n, Ψ :=\n\n(cid:90) ∞\n\n0\n\n(cid:0)X ⊤r∗(s)(cid:1) ◦ (cid:0)X ⊤r(s)(cid:1) ds.\n\n(S.7)\n\n(cid:0)X ⊤ν(cid:1) with BαF -GR (z) = 2α2\n\nPut β∞ = BαF -GR F -GR ◦ sinh(z). Because the form of the function (cid:0)X ⊤ν(cid:1) is the same as in the analysis of usual gradient descent (Woodworth et al., 2020), β∞ = Bα we can use their transformation of β∞ as it is. We have a KKT condition ∇φα(w) = X ⊤ν and the function φα satisfies\n\n∇φα(β) = B−1\n\nα (β) = arcsinh\n\n(cid:19)\n\n(cid:18) 1\n\n2α2 ◦ β\n\nWe have\n\nwith\n\nand\n\nβ∞(α) = arg min\n\nβ∈Rd s.t. Xβ=y\n\nφα(β)\n\nφα(β) =\n\nd (cid:88)\n\ni=1\n\nα2\n\ni q (cid:0)βi/α2\n\ni\n\n(cid:1)\n\nq(z) = 2 −\n\n(cid:112)\n\n4 + z2 + z arcsinh(z/2).\n\nIn our GR case, α is just replaced by αF -GR.\n\n12\n\n.\n\n(S.8)\n\n(S.9)\n\n(S.10)\n\n(S.11)\n\n(S.13)\n\n(S.14)\n\nUnder review as a conference paper at ICLR 2023\n\nA.1.2 EVALUATION ON αF -GR\n\nFrom the definitions of r(t) and r∗(t), we have\n\nr∗(t) − r(t) =\n\n2ε n\n\n ̃X(( ̃X ⊤r(t)) ◦ w(t)2) +\n\nε2 n2\n\n ̃X(( ̃X ⊤r(t))2 ◦ w(t)2).\n\n(S.12)\n\nThen,\n\nΨ =\n\n(cid:90) ∞\n\n0\n\n(X ⊤r(s))2ds +\n\nε n\n\n(cid:90) ∞\n\n0\n\n2(X ⊤ ̃X(( ̃X ⊤r(s)) ◦ w(s)2)) ◦ (X ⊤r(s)) (cid:124) (cid:125) (cid:123)(cid:122) =:z(s)\n\nds\n\n+\n\nε2 n2\n\n(cid:90) ∞\n\n0\n\n(X ⊤ ̃X(( ̃X ⊤r(s))2 ◦ w(t)2)) ◦ (X ⊤r(s)) (cid:124) (cid:123)(cid:122) (cid:125) =:zh(s)\n\nds.\n\nLet us put\n\nΨ = Ψ0 +\n\nε n\n\n(X ⊤r(s))2ds, Ψ1 :=\n\nΨ0 :=\n\n(cid:90) ∞\n\n0\n\nΨ1 +\n\n(cid:90) ∞\n\n0\n\nε2 n2 Ψ2,\n\nz(s)ds, Ψ2 :=\n\n(cid:90) ∞\n\n0\n\nzh(s)ds.\n\n(S.15)\n\nNote that the first term Ψ0 essentially corresponds to the implicit bias of the SAM update investigated in the previous study (Andriushchenko & Flammarion, 2022). Because the SAM update corresponds to γ = ε, the dominant term of γΨ is Ψ0 and they neglect the other terms. In our GR case, γ and ε have different scales in general and we need to evaluate the coefficient of the ascent step, that is, Ψ1. Lemma A.1. Under assumptions (i)-(iii), for sufficiently small γ, Ψ1 > nb(0)2/2 + O(γ). If we further assume bi(0) ̸= 0 for all i, Ψ1 > nb(0)2/4.\n\nProof of Lemma A.1. The dynamics (S.3 ) are rewritten as\n\nn\n\ndw dt\n\n= − ̃b ◦ w −\n\nγ n\n\n[2( ̃Z( ̃b ◦ w2)) ◦ w + ̃b2 ◦ w]\n\n−\n\nγε\n\nn2 [( ̃Z( ̃b2 ◦ w2)) ◦ w + 2( ̃Z( ̃b ◦ w2)) ◦ w ◦ ̃b] −\n\nγε2\n\nn3 [( ̃Z( ̃b2 ◦ w2)) ◦ w ◦ ̃b],\n\n(S.16)\n\nwhere we put ̃b = ̃X ⊤r and ̃Z = ̃X ⊤ ̃X. This gives us\n\nn 2\n\ndβ dt\n\n= −b ◦ a −\n\nγ n\n\n[2(Z(b ◦ a)) ◦ a + b2 ◦ β] (cid:123)(cid:122) (cid:125) (cid:124) =:Q1(t)\n\n−\n\nn2 [(Z(b2 ◦ β)) ◦ a + 2(Z(b ◦ a)) ◦ β ◦ b]\n\n−\n\nγε\n\n(cid:124)\n\n(cid:123)(cid:122) =:Q2(t)\n\nγε2 n3 [(Z(b2 ◦ β)) ◦ β ◦ b]\n\n, (cid:125)\n\n(cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) =:Q3(t)\n\n(S.17)\n\nwhere we put a = w2 −, b = X ⊤r and Z = X ⊤X. Note that db/dt = X ⊤(dr/dt) = X ⊤X(dβ/dt). By multiplying X ⊤X to (S.17 ) and taking the Hadamard product with b, we have\n\n+ + w2\n\nn\n\ndb2 dt\n\n= −4b ◦ (X ⊤X(b ◦ a)) −\n\n4γ n\n\nb ◦ [X ⊤X(Q1(t) + (cid:124)\n\nε n\n(cid:123)(cid:122) =:Q(t)\n\nQ2(t) +\n\nε2 n2 Q3(t))]\n\n(cid:125)\n\n.\n\n(S.18)\n\nThe point is that we have 2b ◦ (X ⊤X(b ◦ a)) = z(t). This relation makes us to evaluate the seemingly complicated term Ψ1 by the change of b(t)2, which corresponds to a training loss. By taking the integral over time, the above dynamics become\n\nΨ1 =\n\n(cid:90) ∞\n\n0\n\nz(s)ds =\n\nn 2\n\nb(0)2 − 2\n\nγ n\n\n(cid:90) ∞\n\n0\n\nQ(s)ds.\n\n(S.19)\n\nWe used assumption (i) that we have a global minimum and b(∞) = 0. If γ is sufficiently small and (cid:82) ∞ 0 Q(s)ds is finite, we will have a non-negative Ψ1.\n\n13\n\n(S.20)\n\n(S.21)\n\n(S.22)\n\n(S.23)\n\n(S.24)\n\n(S.25)\n\n(S.26)\n\n(S.27)\n\nUnder review as a conference paper at ICLR 2023\n\nHere, we use assumption (ii) that the parameter norm has a finite constant upper bound independent of γ and ε. Because ∥a(t)∥ = ∥w+(t)2 + w−(t)2∥ ≤ ∥w∥2, we have an upper bound of ∥a(t)∥ as well:\n\nDefine κ1 := arg maxi ∥Xxi∥, κ2 := arg maxi ∥xi∥ and κ3 := ∥XX ⊤∥2. Then, we find\n\n∥a(t)∥ ≤ ̄a.\n\nwhere we used ∥b ◦ a∥ ≤ ∥b∥∥a∥ ≤\n\n|Q1,i(t)| ≤ 2ai∥Xxi∥∥b ◦ a∥ + b2\n\ni |βi|\n\n√\n\n≤ 2 ̄a2κ1 √\n\nκ3∥r(t)∥ + ̄aκ2\n\n2∥r(t)∥2.\n\nκ3 ̄a∥r∥ and ∥β∥ ≤ ∥a∥ ≤ ̄a. Similarly, we have √\n\n|Q2,i(t)| ≤ ̄a2κ1κ3∥r∥2 + 2 ̄a2κ1κ2\n\nκ3∥r∥2,\n\nwhere we used ∥b2∥ ≤ (cid:112)(cid:80)\n\ni(Xir)4 ≤ (cid:80)\n\ni(Xir)2 = ∥b∥2. We also have\n\n|Q3,i(t)| ≤ ̄a2κ1κ2κ3∥r∥3.\n\nNote that under assumption (ii), the training loss is upper-bounded as well because\n\n∥r(t)∥ ≤ ∥Xβ∥ + ∥y∥ ≤\n\n√\n\nκ3 ̄a + ∥y∥ =: ̄L.\n\nTherefore, we have\n\n|Q3,i(t)| ≤ ∥a∥κ1κ2κ3 ̄L∥r∥2.\n\nAfter all, the inequalities (S.22 ,S.23 ,S.26 ) lead to\n\n(cid:90) ∞\n\n0\n\ndsQi(s) ≤ C\n\n(cid:90) ∞\n\n0\n\nds∥r∥2,\n\nwhere C denotes an uninteresting positive constant. By using assumption (iii) that (cid:82) ∞ constant upper bound 4n ̄R independent of γ and ε, we have\n\n0 ds∥r∥2 has an\n\nΨ1 ≥\n\nnb(0)2 2\n\n− 8γC ̄R.\n\n(S.28)\n\nTherefore, for sufficiently small γ, the dominant term is non-negative. Moreover, if we have bi(0) ̸= 0 for all i,\n\nΨ1 ≥\n\nnb(0)2 4\n\n> 0 for γ < min\n\ni\n\nnbi(0)2 32C ̄R\n\n.\n\n(S.29)\n\n■\n\nAs a side note, the inequality (S.29 ) of γ gives us some insight into non-asymptotic evaluation on how large γ we can take. First, the constant C includes ̄a and it implies that we need a smaller γ for a larger parameter norm ̄a. Second, note that ̄R controls the integral of the training loss over the whole training dynamics. We need a smaller γ as well for a larger ̄R which implies the convergence of dynamics is slower.\n\nNext, we evaluate Ψ2. Since\n\nwe have\n\nTherefore,\n\nzh(s) = (Z(b2 ◦ β)) ◦ b,\n\n|zh,i| ≤ κ1κ2κ3 ̄a ̄L∥r∥2.\n\n(cid:90) ∞\n\n|\n\n0\n\nzh,i(s)ds| ≤ C ′ ̄R.\n\nexThus, Ψ2 is finite and becomes negligible in Ψ for a sufficiently small ε.\n\nFinally, we have\n\nγΨ ≥ εγ\n\nb(0)2 2\n\n−\n\n8εγ2 n\n\nC ̄R −\n\nε2γ\n\nn2 C ′ ̄R,\n\n14\n\n(S.30)\n\n(S.31)\n\n(S.32)\n\n(S.33)\n\nUnder review as a conference paper at ICLR 2023\n\nwhere we used Lemma A.1. Substituting the above inequality and b(0) = X ⊤r(0) into (S.7 ), we obtain Theorem 4.1.\n\nRemark on higher-order terms in Theorem 4.1: First, let us remark on O(γ2) term. Lemma A.1 tells us that if we have bi(0) ̸= 0 for all i, we have a slightly stronger result than Theorem 4.1:\n\nαF -GR ≤ α0 ◦ exp(−γεc∗/2 + O(ε2)),\n\n(S.34)\n\nwhere the O(γ2) term disappears. The condition of bi(0) ̸= 0 seems to hold in usual cases because the network parameters and training samples are randomly assigned at initialization. Second, regarding O(ε2) term, we have Ψ ≥ 0 for\n\nε ≤\n\n(cid:18)\n\nn2 C ′ ̄R\n\nbi(0)2 2\n\n−\n\n8γ n\n\nmin i\n\n(cid:19)\n\nC ̄R\n\n< min\n\ni\n\nn2bi(0)2 4C ′ ̄R\n\n(S.35)\n\n(S.36)\n\nwhere the first line comes from (S.33 ) and the second one from a small γ satisfying (S.33 ). This implies that we need a smaller ε for larger ̄a and ̄R in a similar way to γ.\n\nRemark on B-GR: We have obtained the upper bound of αF -GR, that is, the lower bound of Ψ for F-GR. Since we can see B-GR as the F-GR with a negative ε, the sign of Ψ1 is flipped in B-GR. Then, we can easily obtain the upper bound of Ψ as follows.\n\nFirst, we have\n\nΨ0 ≤ 2λmax(XX ⊤)\n\n(cid:90) ∞\n\n0\n\n∥r(s)∥2ds\n\n≤ 8nλmax(XX ⊤) ̄R,\n\nwhere λmax(XX ⊤) denotes the largest eigenvalue of XX ⊤. We have\n\nΨ = Ψ0 −\n\nε n\n\nΨ1 + O(ε2)\n\n≲ 8nλmax(XX ⊤) ̄R − ε\n\nb(0)2 2\n\n,\n\n(S.37)\n\n(S.38)\n\n(S.39)\n\n(S.40)\n\nwhere we used Lemma A.1. Substituting the above inequality into (S.7 ), we obtain αF -GR ≳ C ◦ exp(γεc∗) for a positive constant C. Thus, the lower bound increases for a larger step size ε > 0 in B-GR and the implicit bias is strengthen in the direction to L2 solutions.\n\nA.2 ALTERNATIVE TO ASSUMPTION (III)\n\nInstead of assumption (iii), we may use Assumption A.2. For sufficiently small ε and γ, the smallest eigenvalue of S(t) := Xdiag(a(t))X ⊤ is positive.\n\nSince we suppose the overparameterized case (d > n), the matrix X is a wide matrix and S has no trivial zero eigenvalue. The positive definiteness of S is a sufficient condition of global convergence as follows. From Eq. (S.17 ), we have\n\nn 4\n\nd∥r∥2 dt\n\n=\n\nn 2\n\nb⊤ dβ\n\ndt\n\n= −r⊤Sr −\n\nγ n\n\nr⊤X(Q1(t) +\n\nε n\n\nQ2(t) +\n\nε2 n2 Q3(t)).\n\nUsing the inequalities (S.22 ,S.23 ,S.26 ), we have\n\nn 4\n\nd∥r∥2 dt\n\n≤ −λ∗\n\nmin∥r∥2 + γC∥r∥2.\n\n(S.41)\n\n(S.42)\n\nwhere we take the lower bound of the smallest eigenvalue as λ∗ a sufficiently small γ such that γ < 3λ∗\n\nmin/(4C), we obtain\n\nmin = mint,γ,ε λmin(S(t)). By taking\n\n∥r(t)∥2 ≤ ∥r(0)∥2 exp(−λ∗\n\nmint/n),\n\n(S.43)\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nfrom Grönwall’s inequality. Since L(w(t)) = ∥r(t)∥2/(4n), we obtain global convergence. In addition, we have (cid:90) ∞\n\n(cid:90) ∞\n\nds∥r(s)∥2 ≤ ∥r(0)∥2\n\nds exp(−λ∗\n\nmint/n) = n∥r(0)∥2/λ∗\n\nmin.\n\n(S.44)\n\n0\n\n0\n\nThis gives the upper bound ̄R. Similarly, we obtain R by taking the lower bound of (S.41 ) and using Grönwall’s inequality. Thus, instead of assumption (iii), we can apply Assumption A.2 in the transformation from (S.27 ) to (S.29 ).\n\nNote that S(t) is known as the neural tangent kernel in the lazy regime and its positive definiteness is straightforward (Woodworth et al., 2020). Although there is no proof of the positive definiteness in the rich regime, we observed it in numerical experiments and the assumption A.2 seems rational.\n\nA.3 DERIVATION OF PROPOSITION 4.2\n\nWe obtained the upper bound of αF -GR (in other words, the lower bound of Ψ) from the term of Ψ1. In some cases, Ψ0 gives us complementary insight. Proposition A.3. Under the same assumptions as in Theorem 4.1, for sufficiently small ε and γ,\n\nwhere the exponent c is a non-negative variable given by c = n−2 (cid:82) ∞\n\n0 (X ⊤(Xβ(s) − y))2ds.\n\nαF -GR ≤ α0 ◦ exp(−γc + O(γ2) + O(ε2)),\n\n(S.45)\n\nProof. Ψ0 is non-negative by definition and written as\n\nΨ0 =\n\n(cid:90) ∞\n\n0\n\n(X ⊤(Xβ(s) − y))2ds.\n\n(S.46)\n\nIn addition, we have Ψ1 ≥ O(γ) from Lemma A.1. Therefore, we have Ψ ≥ Ψ0 + O(γ) + O(ε2). Substituting this into (S.7 ), we obtain the result.\n\nRemark on an average of c: Note that c may depend on ε and γ because it is given by the integral of training dynamics. It seems hard to obtain a concrete value of this integral. Instead of evaluating each entries of c, let us analyze the average value of c, that is, ∥c∥1/d = (cid:80)d i=1 ci/d. This approach gives us some insight into a typical value of the exponent c:\n\nr(s)⊤(XX ⊤)r(s)ds\n\n∥c∥1/d =\n\n≥\n\n(cid:90) ∞\n\n1 d\n4n d\n\n0\n\nλmin(XX ⊤)R.\n\n(S.47)\n\n(S.48)\n\nA similar evaluation has been used in the analysis of SAM (Andriushchenko & Flammarion, 2022).\n\nA.4 HESSIAN\n\nThe MSE loss of the diagonal linear network has the following Hessian:\n\nH =\n\n(cid:16)\n\n1 n\n\ndiag( ̃X ⊤r) + 2diag(w) ̃X ⊤ ̃Xdiag(w)\n\n(cid:17)\n\n.\n\nAt the interpolation solution,\n\nH =\n\n2 n\n\ndiag(w) ̃X ⊤ ̃Xdiag(w).\n\nB DERIVATION OF THEOREM 5.1\n\n(S.49)\n\n(S.50)\n\nIt is straightforward to derive this theorem. Consider the time step t satisfying L(θt) < b and L(θt+1) > b. The update rule is given by\n\nθt+1 = θt + η∇θL(θt), θt+2 = θt+1 − η∇θL(θt+1).\n\n(S.51)\n\n(S.52)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTaking the summation, we get\n\nθt+2 = θt − η(∇θL(θt+1) − ∇θL(θt))\n\n= θt − η2 ∇L (θt + η∇L (θt)) − ∇L (θt)\n\nη\n\nSimilarly, for L(θt) > b and L(θt+1) < b, we have\n\nθt+1 = θt − η∇θL(θt), θt+2 = θt+1 + η∇θL(θt+1).\n\nand get\n\nθt+2 = θt + η(∇θL(θt+1) − ∇θL(θt))\n\n= θt − η2 ∇L (θt) − ∇L (θt − η∇L (θt))\n\nη\n\n.\n\n.\n\nC EXPERIMENTS\n\nC.1 COMPUTATION OF LEARNING WITH GR\n\nC.1.1 PSEUDO-CODE AND IMPLEMENTATION\n\n(S.53)\n\n(S.54)\n\n(S.55) (S.56)\n\n(S.57)\n\n(S.58)\n\nIn the experiments on benchmark datasets, we computed the GR term in each mini-batch of SGD update. The pseudo-code for F-GR is given in Algorithm 1. The double backward computation is implemented as shown in Listing 1.\n\nAlgorithm 1 Learning with F-GR\n\nInput: mini-batches{B1, ..., BK}\n\nif i-th mini-batch then ∆L ← ∇L(θ; Bi) θ′ ← θ + ε∆L ∆L′ ← ∇L(θ′; Bi) ∆R ← (∆L′ − ∆L)/ε θ ← θ − η(∆L + γ∆R)\n\n1: while SGD update do 2: 3: 4: 5: 6: 7: 8: 9: end while\n\nend if\n\n1 ... 2 loss.backward(create_graph=True) #backpropagation of original loss 3 loss_DB = (gamma/2)*sum([torch.sum(p.grad**2) for p in model.parameters()\n\n]) #computing GR term\n\n4 loss_DB.backward() #backpropagation of GR term 5 optimizer.step() 6 ...\n\nListing 1: Implementation of DB in PyTorch.\n\nC.1.2 EVALUATION ON THE NUMBER OF MATRIX MULTIPLICATION\n\nWe represent an L-layer fully connected neural network with a linear output layer by Al = φ(Ul), Ul = WlAl−1 for l = 1, ..., L. We define the element-wise activation function by φ(·) and weight matrix by Wl. For simplicity, we neglect bias terms. Note that we have multiple samples A0 (within each minibatch) as an input and WlAl requires a matrix-matrix product. Therefore, the forward pass requires L matrix multiplication. Next, let us overview usual backpropagation on the forward pass {A0 → A1 → · · · → AL}. We can express the backward pass as Bl = φ′(Ul) ◦ (W ⊤ l+1Bl+1), where the backward signal Bl corresponds to ∂L/∂Ul (l = 1, ..., L − 1). Then, the backward pass requires\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nL − 1 matrix-matrix multiplication between weights W and backward signals B. In addition, we need to compute the gradient ∂L/∂Wl = BlA⊤ l−1 for ∇L and this is also a matrix-matrix multiplication. Alter all, we need 3L − 1 matrix multiplication for ∇L.\n\nFinite difference computation: ∇L(θ′) requires the same number of matrix multiplication as the normal backpropagation. Therefore, ∇ ̃L requires 6L − 2. For a sufficiently deep network, this is ∼ 6L.\n\nDouble Backward computation: Let us denote ∂L/∂Wl by Gl. Figure 2 represents the forward pass for computing the gradient of GR. Note that the upper part of this graph, i.e., {A0 → A1 → · · · → BL → · · · → B1}, is well-known in double backpropagation of ∇B1 for the input-Jacobian regularization. As explained in Drucker & Le Cun (1992), the computation of ∇B1 is equivalent to apply backpropagation to this upper part of the graph. GR requires additional L nodes for Gl. Note that when we have a forward pass with matrix multiplication, its backward computation requires two matrix multiplications. That is, when a node of the forward pass S is a function of the matrix X given by X = U V , we need to compute ∂S/∂U = (∂S/∂X)V and ∂S/∂V = U (∂S/∂X) in the backpropagation. In addition, we do not need to compute the derivative of A0. After all, we need 2 × (3L − 1) − 2 = 6L − 4 for the ∇R. Since we also compute the gradient of the original loss ∇L, we need 9L − 5. For a sufficiently deep network, this is ∼ 9L.\n\nC.2 DETAILS OF EXPERIMENTAL SETTINGS\n\nFigure 1: We trained MLP (width 512) and ResNet on CIFAR-10 by using SGD with GR. We set batch size 256, momentum 0.9, initial learning rate 0.01 and used a step decay of the learning rate (scaled by 5 at epochs 60, 120, 160), γ = ε = 0.05 for GR. We showed the average and standard deviation over 5 trials of different random initialization.\n\nFigure 3: (a) We trained the 4-layer MLP and ResNet-18 on CIFAR-10 by using SGD with GR. We trained the models with various hyper-parameters ε = {10−5, 5 × 10−5, ..., 0.5, 1} and γ = {10−4, 2 × 10−4, 5 × 10−4, 10−3, ..., 1, 2, 5}. The other settings are the same as in Figure 1. We set batch size 128, weight decay 0.0001, and used no other regularization technique or data augmentation.\n\nFigure 4: We generated synthetic data by xi ∼ N (μ1, σ2I) and yi ∼ N (⟨β∗, xi⟩ , 0.01). β∗ is k∗. We set d = 100, n = 50, μ = σ2 = 5, γ = 0.02 k∗-sparse with non-zero entries equal to 1/ and initialization α0,i ∼ N (0, 0.01). We trained the models by the discrete time update with a small learning rate η = 0.001. We showed the average of 25 trials with different seeds. We trained the models until the training loss L became lower than 10−8.\n\n√\n\nFigure 5: We trained ResNet-18 on CIFAR-10 by SGD with flooding (b = 0.05). The setting is the same as in Figure 1. We computed the gradient norm R by the average of mini-batches in each epoch. We showed the average and standard deviation over 10 trials of different random initialization.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nFigure S.1 : Training dynamics in ResNet-18 on CIFAR-10. Learning with F-GR is much faster in wall time.\n\nC.3 ADDITIONAL EXPERIMENTS\n\nC.3.1 TRAINING DYNAMICS\n\nFigure S.1: This figure shows the trajectories of the original training loss L during the training. Its setting is the same as in Figure 1. We observed that learning with F-GR could make the loss decrease faster than DB in the sense of convergence rate (i.e., the number of epochs). This means that the loss converges even faster in wall time.\n\nC.3.2 GENERALIZATION PERFORMANCE\n\nFigure S.2: To see the difference among algorithms in more detail, we show test accuracy along ε axis with a fixed γ of the grid search shown in Figure 3. Each line represents the average and standard deviation over 5 trials of different random initialization. We fixed γ = 0.5 for MLP and γ = 0.05 for ResNet-18. This means that the objective function is the same among different algorithms. Nevertheless, the eventual performance is different. For a large ε, F-GR achieves the higher test accuracy than DB beyond one standard deviation. For such a large ε, F-GR also performs better than B-GR.\n\nFigure S.2 : Test accuracy shown in Figure 3 along ε axis with a fixed γ. We fixed γ = 0.5 for MLP and γ = 0.05 for ResNet-18.\n\nFigure S.3: This figure shows an additional experiment of the grid search shown in Section 3.3. We did experiments on a different architecture and dataset, that is, ResNet-34 on CIFAR-10. The result is consistent with those in Figure 3. Learning with F-GR achieves the highest accuracy for large ascent steps. B-GR performs much worse for them. In addition, the highest accuracy of F-GR is better than that of DB. The best test accuracy was (F-GR, B-GR, DB) = (59.9, 58.6, 59.5) ± (0.5, 0.4, 0.5). From this experiment, we can see that the result of the finite difference computation with small ε does not necessarily coincide with that of DB. We observed that the training dynamics showed instability for too small ε. This would be attributed to numerical instability. The important point is that F-GR shows better accuracy than DB for large ascent steps.\n\n19\n\nMLPResNet-18Under review as a conference paper at ICLR 2023\n\nFigure S.3 : Grid search on learning with different GR algorithms in ResNet-34 on CIFAR-100. The color bar shows the average test accuracy over 5 trials. Gray dashed lines indicate γ = ε.\n\nTable S.1: We trained WideResNet-28-10 (WRN-28-10) with γ = {0, 10−4, 10−3, 10−2, 10−1}. For F-GR and B-GR, we set ε = {0.001, 0.01, 0.1}. We computed the average and standard deviation over 5 trials of different random initialization, and reported the best average accuracy achieved over all the above combinations of hyper-parameters. F-GR performs better than DB and B-GR beyond one standard deviation in most cases. We used crop and horizontal flip as data augmentation, cosine scheduling with an initial learning rate 0.1, and set momentum 0.9, batch size 128, and weight decay 0.0001.\n\nTable S.1 : Test accuracy of WRN-28-10 shows that F-GR performs better. We trained the models with/without data augmentation (DA).\n\nWRN-28-10\n\nCIFAR-10 w/o DA w/ DA\n\nCIFAR-100 w/o DA w/ DA\n\nF-GR 92.1± 0.2 B-GR 91.9± 0.1 91.7± 0.2 DB\n\n96.1± 0.1 95.9± 0.1 95.9± 0.1\n\n71.3± 0.3 71.1± 0.5 70.3± 0.2\n\n80.7± 0.2 80.2± 0.2 80.3± 0.4\n\nC.3.3 DIAGONAL LINEAR NETWORK\n\nFigure S.4: We trained DLNs with various ε and γ in the same setting as in Figure 4. The black circles in the figure show the cases of the lowest test error. The best test error was (F-GR, B-GR) = (10−1.37, 10−1.16) and F-GR performed better.\n\nFigure S.4 : Learning of diagonal linear networks with GR. The color bar shows the average test loss over 10 trials. Training dynamics exploded in the gray area.\n\n20\n\nF-GRB-GRDBF-GRB-GRUnder review as a conference paper at ICLR 2023\n\nC.3.4 FLOODING METHOD\n\nFigure S.5: This figure confirms at which epoch the training loss started to get close to the flood level. The experimental setting is the same as in Figure 5. The blue line shows a flip rate, that is, the ratio of how many times the training loss gets smaller than the flood level during each epoch. Around the 10-th epoch, the training loss started to reach the flooding level and the gradient norm also started to decrease.\n\nFigure S.5 : Flip rate of flooding with b = 0.05.\n\n21",
    "reference": "# Summary Of The Paper\n\nUpdate: I raised my score to 5 after reading the authors' rebuttal.  \n\n--------  \nThe paper studies gradient regularization, a technique that is often associated with improved generalization in deep learning. The authors consider a finite-difference approximation of gradient regularization that not only decreases the computational cost but also is shown to improve generalization empirically. This effect is proven to hold for diagonal linear networks, which can be seen as a very simple example of neural networks. The authors also validate their claims using numerical experiments on neural networks.\n\n# Strength And Weaknesses\n\n## Strengths  \n1. Gradient regularization has been popular recently and the paper might be of interest to the conference attendees.\n2. The finite-difference approximation is indeed cheaper to calculate than the full gradient with backpropagation.\n3. Some of the theoretical claims are checked numerically.\n\n## Weaknesses  \n1. The overall intuition behind the efficiency of finite-difference approximation is not clear. In general, finite differences yield very noisy estimates of gradients that poorly scale with dimension, and it is not explained why in this case this approximation happens to be better. As far as I can see, this issue, which is normally the main counter-argument against finite-differences, is not addressed in the paper.\n2. The reduction of double backpropagation to forward passes is not a critical improvement. It is of course nice to reduce the training time, but the impact of this change is limited and is only worth it if all other factors (such as variance) remain the same.\n3. The diagonal linear network is mostly a toy example that no one uses in practice. On top of that, the derivation for this problem is mostly the same as done in prior works. A more general theory, for instance, for locally smooth objectives, could be of more interest.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI did not understand the following aspect: why is $\\frac{\\gamma}{2}R(\\theta)$ used in equation (1) instead of $\\frac{\\gamma}{4}R(\\theta)$? If I understood correctly, the latter was shown to correspond to the implicit bias of SGD in equation (20) of (Smith et al., 2021) when $\\gamma$ is the stepsize.\n\n# Summary Of The Review\n\nAll in all, this paper presents a number of small results about finite-difference schemes for gradient regularization. Each result on its own seems to be quite small. The results do not become much stronger together either, since they are on orthogonal topics: numerical efficiency, implicit bias on diagonal linear networks, and the relation to other deep learning techniques. The work seems to lack depth in its theoretical investigation, which makes me suggest a rejection.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nUNDERSTANDING HINDSIGHT GOAL RELABELING REQUIRES RETHINKING DIVERGENCE MINIMIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nHindsight goal relabeling has become a foundational technique for multi-goal reinforcement learning (RL). The idea is quite simple: any arbitrary trajectory can be seen as an expert demonstration for reaching the trajectory’s end state. Intuitively, this procedure trains a goal-conditioned policy to imitate a sub-optimal expert. However, this connection between imitation and hindsight relabeling is not well understood. Modern imitation learning algorithms are described in the language of divergence minimization, and yet it remains an open problem how to recast hindsight goal relabeling into that framework. In this work, we develop a unified objective for goal-reaching that explains such a connection, from which we can derive goal-conditioned supervised learning (GCSL) and the reward function in hindsight experience replay (HER) from first principles. Experimentally, we find that despite recent advances in goal-conditioned behaviour cloning (BC), multi-goal Q-learning can still outperform BC-like methods; moreover, a vanilla combination of both actually hurts model performance. Under our framework, we study when BC is expected to help, and empirically validate our findings. Our work further bridges goal-reaching and generative modeling, illustrating the nuances and new pathways of extending the success of generative models to RL.\n\n1\n\nINTRODUCTION\n\nGoal reaching is an essential aspect of intelligence in sequential decision making. Unlike the conventional formulation of reinforcement learning (RL), which aims to encode all desired behaviors into a single scalar reward function that is amenable to learning (Silver et al., 2021), goal reaching formulates the problem of RL as applying a sequence of actions to rearrange the environment into a desired state (Batra et al., 2020). Goal-reaching is a highly flexible formulation. For instance, we can design the goal-space to capture salient information about specific factors of variations that we care about (Plappert et al., 2018); we can use natural language instructions to define more abstract goals (Lynch & Sermanet, 2020; Ahn et al., 2022); we can encourage exploration by prioritizing previously unseen goals (Pong et al., 2019; Warde-Farley et al., 2018; Pitis et al., 2020); and we can even use self-supervised procedures to naturally learn goal-reaching policies without reward engineering (Pong et al., 2018; Nair et al., 2018b; Zhang et al., 2021; OpenAI et al., 2021).\n\nReward is not enough. Usually, rewards are manually constructed, either through laborious reward engineering, or from task-specific optimal demonstrations, neither of which is a scalable solution. How can RL agents learn useful behaviors from unlabeled reward-free trajectories, similar to how NLP models such as BERT (Devlin et al., 2018) and GPT-3 (Brown et al., 2020) are able to learn language from unlabeled text corpus? Goal reaching is a promising paradigm for unsupervised behavior acquisition, but it is unclear how to write down a well-defined objective for goal-conditioned policies, unlike language models that just predict the next token. This paper aims to define such an objective that unifies many prior approaches and strengthens the foundation of goal-conditioned RL.\n\nWe start from the following observation: hindsight goal relabeling (Andrychowicz et al., 2017) can turn an arbitrary trajectory into a sub-optimal expert demonstration. Thus, goal-conditioned RL might be doing a special kind of imitation learning. Currently, divergence minimization is the de facto way to describe imitation learning methods (Ghasemipour et al., 2020), so we should be able to recast hindsight goal relabeling into the divergence minimization framework. On top of that, to tackle the sub-optimality of hindsight-relabeled trajectories, we should explicitly maximize the probability of\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: A unified objective for goal-reaching (see equation equation 14 for more details).\n\nreaching the desired goal. Following these intuitions, we derive the reward function used in HER from first principles, as well as other behaviour cloning (BC) like methods such as GCSL (Ghosh et al., 2019) and Hindsight BC (HBC) (Ding et al., 2019).\n\nExperimentally, we show that multi-goal Q-learning based on HER-like rewards, when carefully tuned, can still outperform goal-conditioned BC (such as GCSL / HBC). Moreover, a vanilla combination of multi-goal Q-learning and BC (HER + HBC), supposedly combining the best of both worlds, in fact hurts performance. We utilize our unified framework to analyze when a BC loss could help, and propose a modified algorithm named Hindsight Divergence Minimization (HDM) that uses Q-learning to account for the worst while imitating the best. HDM avoids the pitfalls of HER + HBC and improves policy success rates on a variety of self-supervised goal-reaching environments. Additionally, our framework reveals a largely unexplored design space for goal-reaching algorithms and potential paths of importing generative modeling techniques into multi-goal RL.\n\n2 REINFORCEMENT LEARNING AND GENERATIVE MODELING\n\n2.1 THE REINFORCEMENT LEARNING (RL) PROBLEM\n\nWe first review the basics of RL and generative modeling. A Markov Decision Process (MDP) is typically parameterized by (S, A, ρ0, p, r): a state space S, an action space A, an initial state distribution ρ0(s), a dynamics function p(s′ | s, a) which defines the transition probability, and a reward function r(s, a). A policy function μ defines a probability distribution μ : S × A → R+. For an infinite-horizon MDP, given the policy μ, and the state distribution at step t (starting from ρ0 at t = 0), the state distribution at step t + 1 is given by:\n\nμ (s′) = ρt+1\n\n(cid:90)\n\nS×A\n\np(s′ | s, a)μ(a | s)ρt\n\nμ(s)dsda\n\nThe state visitation distribution sums over all timesteps via a geometric distribution Geom(γ):\n\nρμ(s) = (1 − γ) ·\n\n∞ (cid:88)\n\nt=0\n\nγt · ρt\n\nμ(s)\n\n(1)\n\n(2)\n\nHowever, the trajectory sampling process does not happen in this discounted manner, so the discount factor γ ∈ (0, 1) is often absorbed into the cumulative return instead (Silver et al., 2014):\n\nJ (μ) =\n\n1 1 − γ\n\n(cid:90)\n\nS\n\nρμ(s)\n\n(cid:90)\n\nA\n\nμ(a | s)r(s, a)dads = E\n\nρ0(s0)μ(a0|s0) p(s1|s0,a0)μ(a1|s1)···\n\n(cid:35)\n\nγtr(st, at)\n\n(3)\n\n(cid:34) ∞ (cid:88)\n\nt=0\n\nFrom 1 and 2, we also see that the future state distribution p+ of policy μ, defined as a geometrically discounted sum of state distribution at all future timesteps given current state and action, is given by\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nthe following recursive relationship (Eysenbach et al., 2020b; Janner et al., 2020):\n\nμ (s+ | s, a) = (1 − γ)p(s+ | s, a) + γ p+\n\n(cid:90)\n\nS×A\n\np(s′ | s, a)μ(a′ | s′)p+\n\nμ (s+ | s′, a′)ds′da′\n\n(4)\n\nIn multi-goal RL, an MDP is augmented with a goal space G, and we learn a goal-conditioned policy π : S × G × A → R+. Hindsight Experience Replay (HER) (Andrychowicz et al., 2017) gives the agent a reward of 0 when the goal is reached and −1 otherwise, and uses hindsight goal relabeling to increase learning efficiency of goal-conditioned Q-learning by replacing the initial behavioral goals with achieved goals (future states within the same trajectory).\n\n2.2\n\nIMITATION LEARNING (IL) AS DIVERGENCE MINIMIZATION\n\nWe first review f -divergence between two probability distributions P and Q and its variational bound:\n\nDf (P ∥ Q) =\n\n(cid:90)\n\nX\n\nq(x)f\n\n(cid:19)\n\n(cid:18) p(x) q(x)\n\ndx ≥ sup T ∈T\n\nEx∼P [T (x)] − Ex∼Q[f ∗(T (x))]\n\n(5)\n\nwhere f is a convex function such that f (1) = 0, f ∗ is the convex conjugate of f , and T is an arbitrary class of functions T : X → R. This variational bound was originally derived in (Nguyen et al., 2010) and was popularized by GAN (Goodfellow et al., 2014; Nowozin et al., 2016) and subsequently by imitation learning (Ho & Ermon, 2016; Fu et al., 2017; Finn et al., 2016; Ghasemipour et al., 2020). The equality holds true under mild conditions (Nguyen et al., 2010), and the optimal T is given by T ∗(x) = f ′(p(x)/q(x)). The canonical formulation of imitation learning follows (Ho & Ermon, 2016; Ghasemipour et al., 2020), where ρexp(s, a) is from the expert:\n\nmin μ\n\nDf (ρexp(s, a) ∥ ρμ(s, a)) ⇔ min\n\nμ\n\nmax T\n\nEρexp(s,a)[T (s, a)] − Eρμ(s,a)[f ∗(T (s, a))]\n\n(6)\n\nBecause of the policy gradient theorem (Sutton et al., 1999), the policy μ needs to optimize the cumulative return under its own trajectory distribution ρμ(s, a) with the reward being r(s, a) = f ∗(T (s, a)). Under this formulation, Jensen-Shannon divergence leads to GAIL (Ho & Ermon, 2016), reverse KL leads to AIRL (Fu et al., 2017). Note that f can in principle be any convex function (we can satisfy f (1) = 0 by simply adding a constant).\n\n2.3 ENERGY-BASED MODELS (EBM)\n\nAn energy-based model (EBM) is defined by pθ(x) = the energy function and Z(θ) = (cid:82) X exp(−Eθ(x))dx is the partition function. The gradient of the log-likelihood log pθ(x) w.r.t. θ (known as contrastive divergence (Hinton, 2002)) can be expressed as :\n\n, where Eθ : RD → R is\n\nexp(−Eθ(x)) Z(θ)\n\n∂ log pθ(x) ∂θ\n\n= Epθ(x′)\n\n(cid:21)\n\n(cid:20) ∂Eθ(x′) ∂θ\n\n−\n\n∂Eθ(x) ∂θ\n\n(7)\n\nSampling from pθ(x) can be difficult, as Langevin dynamics is often required (Welling & Teh, 2011; Du & Mordatch, 2019; Grathwohl et al., 2019). Alternatively, EBMs can be trained via Noise Contrastive Estimation (NCE) (Gutmann & Hyvärinen, 2012; Mnih & Kavukcuoglu, 2013; Gao et al., 2020; Rhodes et al., 2020), which assumes access to a \"noise\" distribution pn(x) and learns energy functions through density ratio estimation. Let sθ(x) = −Eθ(x) − log Zθ (Mnih & Teh, 2012). NCE learns a classifier that distinguishes the data distribution p(x) from noise pn(x), where noise samples are k times more frequent (Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013):\n\np(D = 1 | x) = pθ(x)/(pθ(x) + k · pn(x)) = σ(sθ(x) − log pn(x) − log k) To be more concise, we denote ∆θ(x, k) = sθ(x) − log pn(x) − log k. Gradients of the weighted binary cross entropy loss asymptotically approximate contrastive divergence (7) (Mnih & Teh, 2012):\n\n(8)\n\n(cid:21) (cid:20) Ep(x)[log σ(∆θ(x, k))] + k · Epn(x)[log(1 − σ(∆θ(x, k))]\n\nk→∞−−−−→\n\nd dθ\n\n∂ log pθ(x) ∂θ\n\n(9)\n\nClosely related to NCE is the more recent InfoNCE loss (Van den Oord et al., 2018) which has gained popularity in the contrastive learning setting (Chen et al., 2020; He et al., 2020); we can interpret k in NCE as the batch size of negative samples in InfoNCE. In summary, NCE gives EBM a more tractable way to maximize data likelihood.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3 GRAPHICAL MODELS FOR HINDSIGHT GOAL RELABELING\n\nConsider the setting: given an environment ξ = (ρ0(s), p(s′ | s, a), ρ+(g)) where we generate a dataset of trajectories D = {(s0, a0, s1, a1, · · · )} by sampling from the initial state distribution ρ0(s), an unobserved actor policy μ(a | s), and the dynamics p(s′ | s, a). We aim to train a goal-conditioned policy π(a | s, g) from this arbitrary dataset with relabeled future states as the goals. ρ+(g) is the behavioral goal distribution assumed to be given apriori by the environment. To recast the problem of goal-reaching as imitation learning equation 6, we need to set up an f -divergence minimization where we define the target (expert) distribution and the policy distribution we want to match.\n\nμ\n\nξ\n\ns+\n\ns\n\na\n\nμ\n\nξ\n\nπ\n\ns\n\ng\n\na\n\nμ\n\nξ\n\nπ\n\ns\n\ng\n\na\n\nμ (s+ | s, a) Figure 2: ρμ(s, a)p+ hindsight-relabeled distribution\n\nFigure 3: ρ+(g)ρμ(s)π(a | s, g) (used in HBC / GCSL)\n\nFigure 4: ρ+(g)ρπ(s, a | g) (used in HER / HDM)\n\nThe training signal comes from factorizing the joint distribution of state-action-goal differently. For the relabeled target distribution, we assume an unconditioned actor μ generating a state-action distribution at first, with the goals coming from the future state distribution equation 4 conditioned on the given state and action. For the goal-conditioned policy distribution, behavior goals are given apriori, and the state-action distribution is generated conditioned on the behavioral goals. Thus, the target distribution (see Figure 2) for states, actions, and hindsight goals is:\n\n(10) Note that p+ μ is given by equation equation 4, and ρμ(s, a) is similar to ρexp(s, a) in equation equation 6. In the fashion of behavioral cloning (BC), if we do not care about matching the states, we can write the joint distribution we are trying to match as (see Figure 3):\n\npμ(s, a, s+) = ρμ(s, a)p+\n\nμ (s+ | s, a)\n\nπ (s, a, g) = ρ+(g)ρμ(s)π(a | s, g) pBC (11) We can recover the objective of Hindsight Behavior Cloning (HBC) (Ding et al., 2019; Eysenbach et al., 2020a) and Goal-Conditioned Supervised Learning (GCSL) Ghosh et al. (2019) via minimizing a KL-divergence:\n\n(cid:16)\n\nmin π\n\nDKL\n\npμ(s, a, s+) ∥ pBC\n\nπ (s, a, g)\n\n⇔ min\n\nπ\n\nE\n\nρμ(s,a)p+\n\nμ (s+|s,a)[− log π(a | s, g)]\n\n(12)\n\n(cid:17)\n\nIn many cases, matching the states is more important than matching state-conditioned actions (Ross et al., 2011; Ghasemipour et al., 2020). The joint distribution for states, actions, behavioral goals for π (see Figure 4) is:\n\npπ(s, a, g) = ρ+(g)ρπ(s, a | g)\n\n(13)\n\nHowever, it is important to recognize that even after adding this state-matching part, there is still a missing component of the objective. Divergence minimization encourages the agent to stay in the \"right\" state-action distribution given the benefit of hindsight, but we still need the policy to actually hit the goal (see Figure 1). When we condition the policy on a goal, we should maximize the likelihood of seeing that goal in the future state distribution (see Figure 5). Together with a maximum entropy regularization on the policy H(π) (Ho & Ermon, 2016; Schulman et al., 2017), we propose the following goal-reaching objective:\n\n(g)ρπ(s, a|g) ∥ p+\n\nμ (s+|s, a)ρμ(s, a)\n\n(cid:17)\n\n(cid:16)\n\nρ+ f\nf\n\nmin π\n\nDf\n\n(cid:124)\n\n(cid:123)(cid:122) (a) f -divergence term\n\n(cid:125)\n\n4\n\nFigure 5: the future state distribution p+ π (s+ | s, a) of a goal-conditioned policy when starting from a state s visited by μ.\n\n− βE\n\nρ+(g) ρμ(s) π(a|s,g)\n\n[log p+\n\nπ (g|s, a)]\n\n(cid:124)\n\n(cid:123)(cid:122) (b) goal likelihood term\n\n(cid:125)\n\n− λf f\n\n(cid:124) (cid:123)(cid:122) (cid:125) (c) entropy\n\nH(π)\n\n(14)\n\nUnder review as a conference paper at ICLR 2023\n\nCompared to equation 12 and equation 6, the f -divergence term equation 14(a) swaps the order between the expert and the policy. The coefficient β controls the importance of \"hitting the goal\". This incentive is already implicit in hindsight-relabeled data, as in many cases a BC objective alone can learn to hit the goal as well (Ding et al., 2019; Ghosh et al., 2019; Lynch et al., 2019; Jang et al., 2021). However, hindsight-relabeled trajectories are often sub-optimal demonstrations: not every action is acting towards achieving the goal. Thus, maximizing the likelihood of achieving the goal is crucial, and it plays a key role in deriving the reward function of HER (Andrychowicz et al., 2017).\n\n4 BRIDGING GOAL-REACHING AND GENERATIVE MODELING\n\nIn this section, we study how to optimize the proposed unifying objective 14. We show that the Q-function trained for goal-reaching Qθ(s, a, g) is implicitly doing generative modeling. Its temporal difference is modeling the density ratio between the hindsight-relabeled distribution and the goalconditioned behavioral distribution. It also approximates an energy-based model for future-state distribution when marginalized over the goal distribution. In section 4.1, we demonstrate how goalconditioned Q-learning does divergence minimization (equation 14(a)). In section 4.2, we show how Q-functions can define an EBM for cumulative future states, which in turn allows the policy to maximize goal likelihood (equation 14 b+c). In section 4.3, we show that combining the derived results yields the HER reward. In section 4.4, we study when a BC loss can help by analyzing the loss terms being left out by the HER reward.\n\n4.1 DIVERGENCE MINIMIZATION WITH GOAL-CONDITIONED Q-LEARNING\n\nThis section decomposes equation 14(a). We start with the f -divergence bound from equations equation 5 and equation 6.\n\nDf (pπ(s, a, g) ∥ pμ(s, a, s+)) = max\n\nT\n\nE p(g)\n\n[T (s, a, g)] − E ρμ(s,a)\n\n[f ∗(T (s, a, s+))]\n\n(15)\n\nρπ(s,a|g)\n\np+\n\nμ (s+|s,a)\n\nNow we negate T to get r(s, a, g) = −T (s, a, g), and the divergence minimization problem becomes:\n\nmax π\n\nmin r\n\nE ρμ(s,a)\n\n[f ∗(−r(s, a, s+))] + E p(g)\n\n[r(s, a, g)]\n\n(16)\n\np+\n\nμ (s+|s,a)\n\nρπ(s,a|g)\n\nWe can interpret r as a GAIL-style (Ho & Ermon, 2016) discriminator or reward. However, we aim to derive a discriminator-free learning process that directly trains the Q-function corresponding to this reward Qθ(s, a, g) = r(s, a, g) + γ · P πQ(s, a, g) where P π is the transition operator: P πQ(s, a, g) = Ep(s′|s,a)π(a′|s′,g)[Qθ(s′, a′, g)]. Re-writing the equation equation 16 w.r.t Q:\n\nmin Q\n\nE ρμ(s,a)\n\np+\n\nμ (s+|s,a)\n\n[f ∗(−(Qθ − γ · P πQ)(s, a, s+))] + E p(g)\n\n[(Qθ − γ · P πQ)(s, a, g)]\n\n(17)\n\nρπ(s,a|g)\n\nA similar change-of-variable has been explored in the context of offline RL (Nachum et al., 2019a;b) and imitation learning (Kostrikov et al., 2019; Zhu et al., 2020); we may call those methods the DICE (Nachum & Dai, 2020) family. The major pain point of DICE-like methods is that they require samples from the initial state distribution ρ0(s) (Garg et al., 2021). Here, the following lemma shows that in the goal-conditioned case, we can use arbitrary offline trajectories to evaluate the expected rewards under goal-conditioned online rollouts: Lemma 4.1 (Online-to-offline transformation for goal reaching). Given a goal-conditioned policy π(a | s, g), its corresponding Q-function Qπ(s, a, g), and arbitrary state-action visitation distribution ρμ(s, a) of another policy μ(a | s), the expected temporal difference for online rollouts under π is: Ep(g)ρπ(s,a|g)[(Qπ − γ · P πQπ)(s, a, g)] = Ep(g)ρμ(s,a)π( ̃a|s,g)[Qπ(s, ̃a, g) − γ · P πQπ(s, a, g)]\n\nUsing Lemma 4.1, the objective in equation 16 now becomes:\n\nmax π\n\nmin Q\n\nE\n\nρμ(s,a)p+\n\nμ (s+|s,a)\n\np(g),π( ̃a|s,g)\n\n(cid:104) f ∗(−(Qθ − γP πQ)(s, a, s+)) + Qθ(s, ̃a, g) − γP πQ(s, a, g)\n\n(cid:105)\n\n(18)\n\nThe function f ∗ is the convex conjugate of f in f -divergence. We can pick almost any convex function as f ∗ as long as ((f ∗)∗)(1) = 0. For instance, in the tradition of Q-learning, we can use: f ∗(x) = (x + r)2/2 + c\n\n(19)\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nwhere r and c are constants. Its convex conjugate is f (x) = (f ∗)∗(x) = x2/2 − rx − c. For any r, we can always pick a c that will ensure f (1) = (f ∗)∗(1) = 0, and c does not affect the learning process. In summary, we have derived a way to minimize the first term Df (pμ(s, a, s+) ∥ pπ(s, a, g)) in equation 14 directly using goal-conditioned Q-learning.\n\n4.2 EBM FOR PREDICTING (AND CONTROLLING) THE FUTURE\n\nIn this section, we study how to maximize the goal likelihood term equation 14(b) by defining an energy-based model (EBM) to predict the future-state distribution for a policy, which the policy can in turn optimize to control the future-state distribution. We start with the Bayes rule:\n\nπ (s+ | s, a) = p+\n\nρ+(s+)ρπ(s, a | s+) Eρ+(g)[ρπ(s, a | g)]\n\n(20)\n\nThis relationship reflects that, if we define an EBM for ρπ(s, a | s+) = exp q(s, a, s+)/Zq(s+) where Zq(s+) = (cid:82) S×A exp q(s, a, s+)dsda, then we can contrast the dynamics defined in equation 4 with the marginal goal distribution defined in equation 13. In a slightly overloaded notation, we will now define a Q-function as Qθ(s, a, g) = q(s, a, g) − log Zq(g). For now, we shall assume that this Q-function is separate from the one defined in equation 18. Rearranging equation 20 , and setting ρ+(g) to be ρ+(s+), we see the density ratio can be expressed as:\n\nπ (s+ | s, a) p+ ρ+(s+)\n\n=\n\nρπ(s, a | s+) Eρ+(g)[ρπ(s, a | g)]\n\n=\n\nexp Qθ(s, a, s+) Eρ+(g)[exp Qθ(s, a, g)]\n\n(21)\n\nLemma 4.2 (Gradient of the noise-contrastive term in energy-based goal-reaching). Given the following definition for the logit of a NCE-like binary classifier, with ρ+(g) = ρ+(g):\n\n∆θ(s, a, g, k) = Qθ(s, a, g) − log Eρ+(g)[exp Qθ(s, a, g)] − log k\n\n(22)\n\nThe gradient of the negative NCE term in the density ratio estimation approaches zero as k → ∞:\n\nEρμ(s,a)ρ+(g)\n\n(cid:104)\n\nk · log\n\n(cid:16)\n\nd dθ\n\n1 − σ(∆θ(s, a, g, k))\n\n(cid:17)(cid:105) k→∞−−−−→ 0\n\nAs for the positive classification term in NCE equation 9, we can make a similar argument that ∇θ log(1 + exp ∆θ(s, a, s+, k)) k→∞−−−−→ 0 (see appendix B.2). We have:\n\narg max Q\n\nEρμ(s,a)\n\n(cid:104)\n\nE\n\n(cid:105) π (s+|s,a)[Qθ(s, a, s+)] − log Eρ+(g)[exp Qθ(s, a, g)]\n\np+\n\n(23)\n\nTo optimize the above objective on arbitrary behaviour data, we have to consider the fact that we π (s+ | s, a), as sampling do not have complete access to the distribution of \"positive samples\" p+ directly from this distribution requires on-policy rollouts. Utilizing importance weights to learn from off-policy data while accounting for hindsight bias yields the following loss minimization instead:\n\n(cid:110)\n\nEρμ(s,a)\n\n−(1 − γ) · Ep(s′|s,a)[Qθ(s, a, s′)] (cid:125) (cid:123)(cid:122) (cid:124) Learning single-step dynamics in Q\n\n+ Eρ+(g)p(s′|s,a)[w(s, a, s′, g) · Qθ(s, a, g)] (cid:125) (cid:123)(cid:122) Learning multi-step dynamics in Q\n\n(cid:124)\n\n(cid:111)\n\n(24)\n\nwhere w(s, a, s′, g) contrasts the goals that will likely be met by the agent beyond a single step of dynamics p(s′ | s, a) in the future with other random goals that the agent has seen in the past:\n\nw(s, a, s′, g) =\n\nexp Q(s, a, g) Eρ+(g)[exp Q(s, a, g)] (cid:125) (cid:123)(cid:122) (cid:124) Pushing down the likelihood of the marginal\n\nSee Appendix B.3 for a full derivation.\n\n4.3 DERIVING HER REWARDS\n\na exp Q(s′, a, g)/|A|\n\n−γ ·\n\n(cid:80) Eρ+(g)[(cid:80) (cid:124)\n\na exp Q(s′, a, g)/|A|] (cid:125)\n\n(cid:123)(cid:122) Pushing up the likelihood of the conditional\n\n(25)\n\nFor a goal-conditioned Q-function Qθ(s, a, g), besides optimizing a Bellman residual as a regular Q(s, a) function would do, it has an additional degree-of-freedom for the goal g which can be used to\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\ndefine an EBM that models the future-state distribution. In such an EBM, the Q-function is used as the negative energy defined in equation 21. Combining the first term in the f -divergence minimization part equation 18 and the single-step dynamics loss term in the EBM learning 24 gives us:\n\narg min Q\n\nE\n\nρμ(s,a)p(s′|s,a)p+\n\nμ (s+|s,a)\n\n(cid:104) f ∗(−(Qθ − γP πQ)(s, a, s+)) − β · (1 − γ)Qθ(s, a, s′)\n\n(cid:105)\n\n(26)\n\nThe crucial observation here is, given that f ∗ is a quadratic 19, and that there is a stop gradient sign on P πQ because it uses a target network, the loss above can be re-packaged into a single quadratic loss of Bellman residuals for a specific reward thanks to the property of the dynamics defined in 4:\n\narg min Q\n\nE\n\nρμ(s,a)p(s′|s,a)p+\n\nμ (s+|s,a)\n\nr(s, a, s′, s+) =\n\n(cid:16)\n\n(cid:104) 1 2\n(cid:26)r + β,\n\nr,\n\ns′ = s+ s′ ̸= s+\n\nr(s, a, s′, s+) + (γP πQ − Qθ)(s, a, s+)\n\n(cid:17)2(cid:105)\n\n(27)\n\n(28)\n\nSee Appendix B.4 for a full derivation. Setting r = −1 and β = 1, we have arrived at the reward function of HER rHER(s, a, s′, s+). The remaining losses that this reward leaves out are the second term in f -divergence minimization part equation 18 and the second term in the EBM training equation 53, which we will analyze in the next section.\n\n4.4 WHEN DOES BEHAVIOUR CLONING (BC) HELP?\n\nWe now proceed to answer the following question: when does BC help? Our idea is to identify the conditions for a BC-like loss to emerge in the remaining losses unaccounted by the HER reward: a loss term in 18 about pushing Q-values of the current policy down, and the multi-step dynamics term in EBM 54. We may argue that, by not pushing the Q-values of the current policy down, the HER agent becomes more exploratory; by not pushing up the Q-values of the discounted future-state distribution beyond a single step, the HER agent is encouraged to reach a goal sooner rather than later. Nevertheless, we shall see that combining those two remaining terms produces a BC-like term, which imitates the best actions based on how much an action moves the agent closer to the goal.\n\nWe start with the following property of Boltzmann policies π(a | s, g) ∝ exp Q(s, a, g) (Haarnoja et al., 2017; Schulman et al., 2017) (which we can apply thanks to entropy regularization in 14):\n\nEπ(a|s,g)[Q(s, a, g)] = log\n\n(cid:88)\n\na\n\nexp Q(s, a, g) − H(π)\n\nIn equation 25, the Q-value of a particular action a is pushed up under the following condition:\n\nexp Q(s, a, g) Eρ+(g)[exp Q(s, a, g)]\n\n< γ ·\n\n(cid:80) Eρ+(g)[(cid:80)\n\na exp Q(s′, a, g)/|A|\n\na exp Q(s′, a, g)/|A|]\n\n(29)\n\n(30)\n\nBoth denominators on the left and right sides are averaged over all possible behavioral goals, so their values should be roughly the same. Moreover, we approximate the average-exponential operation by using the max operation already computed in the backup operator (either exact (Mnih et al., 2015; Van Hasselt et al., 2016) or approximate (Lillicrap et al., 2015; Haarnoja et al., 2018)) and lower the threshold γhdm (compared to γ) accordingly. After those changes, the indicator function that decides whether the value of an action a should be pushed up becomes:\n\nˆw(s, a, s′, g) = 1(exp Q(s, a, g) − γhdm · exp max\n\na′\n\nQ(s′, a′, g) < 0)\n\n(31)\n\nCombining with the first term in 29, we arrive at a BC-like loss 12, but with an indicator weighting:\n\nLhdm(Q) = E\n\nρμ(s,a)p(s′|s,a)ρ+(g)[− ˆw(s, a, s′, g) · (Qθ(s, a, g) − log\n\n(cid:88)\n\nA\n\nexp Qθ(s, ·, g))]\n\n(32)\n\nIntuitively, this term imitates a particular action when the value functions believe that this action can move the agent closer to the goal by at least − log γhdm steps. The idea of imitating the best actions is similar to self-imitation learning (SIL) (Oh et al., 2018; Vinyals et al., 2019), but our algorithm 1 operates in a (reward-free) goal-reaching setting, with the advantage function in SIL being replaced by the delta of reachability a particular action can produce in getting closer to the goal.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Hindsight Divergence Minimization Given: Batch of data {(s, a, s′, s+)}, where s+ is sampled via hindsight relabeling. 1: L = MSE(Q(s, a, s+) − γ maxa′ ˆQ(s′, a′, s+), r) 2: L = L − β(1 − γhdm) · Q(s, a, s′) 3: LBC = Q(s, a, s+) − log (cid:80) 4: L = L − β · 1(Q(s, a, s+) − maxa′ Q(s′, a′, s+) < log γhdm) · LBC 5: minimize L, update the target network ˆQ\n\nA exp Q(s, ·, s+)\n\n▷ r is a constant, default r = −1 ▷ Push up the values for reaching next states ▷ Behaviour Cloning like loss\n\n5 EXPERIMENTS\n\nFigure 6: Intuitions about when HDM applies BC. From left to right: (a) the Maze environment with a goal in the upper-left corner; (b) the Q-values learned through the converged policy. Lighter red means higher Q-value; (c) visualizing the actions (from the replay) that get imitated when conditioning on the goal and setting γhdm = 0.95, with the background color reflecting how much an action moves the agent closer to the goal based on the agent’s own estimate; (d) γhdm = 0.85; (e) γhdm = 0.75. As we lower γhdm, the threshold − log γhdm gets higher and fewer actions get imitated, with the remaining imitated actions more concentrated around the goal. HDM uses Q-learning to account for the worst while imitating the best during the goal-reaching process.\n\n5.1 SELF-SUPERVISED GOAL-REACHING SETUP\n\nWe consider the self-supervised goal-reaching setting for the environments described in GCSL (Ghosh et al., 2019). While the original HER (Andrychowicz et al., 2017) assumes that the agent has direct access to the ground truth binary reward metric (which is used when relabeling is performed), we do not make such an assumption as it can be unrealistic for real-world robot-learning (Lin et al., 2019). Instead, we simply use next-state relabeling to provide positive rewards. As seen in Eq equation 28, a positive reward is provided only when the relabeled hindsight goal is the immediate next state. The benefit of the next-state relabeling reward is that the training procedure is now completely self-supervised, and therefore provides a fair comparison to GCSL (Ghosh et al., 2019).\n\nSuccess Rate (%)\n\nFour Rooms\n\nLunar Landar\n\nSawyer Push\n\nDoor Opening Claw Manipulate\n\nGCSL / HBC HER r = (0, 1) HER + SQL HER r = (−1, 0) HER + HBC HDM (ours)\n\n78.27 ±4.76 86.60 ±4.22 88.50 ±4.56 86.40 ±5.11 82.90 ±6.24 96.27 ±2.56\n\n50.00 ±7.77 39.30 ±6.81 44.50 ±9.61 50.80 ±4.66 35.33 ±4.57 57.60 ±7.21\n\n44.67 ±13.86 57.60 ±6.61 57.20 ±6.32 54.60 ±6.16 52.63 ±8.05 66.00 ±5.13\n\n19.10 ±5.97 82.50 ±4.87 84.70 ±5.33 83.76 ±6.02 76.44 ±5.37 88.60 ±4.63\n\n16.80 ±6.55 22.80 ±6.43 16.13 ±8.43 20.20 ±6.23 16.93 ±8.03 27.89 ±6.46\n\nTable 1: Benchmark results of test-time success rates in self-supervised goal-reaching. We compare our method HDM with GCSL (Ghosh et al., 2019), HER (Andrychowicz et al., 2017) with two different types of rewards, SQL (Schulman et al., 2017) (Soft Q-Learning) + HER, and HER + HBC. We find that HER can still outperforms GCSL / HBC, and a vanilla combination of HER + HBC actually hurts performance. HDM selectively decides on what to imitate and outperforms HER and HBC.\n\n5.2 COMPARISONS AND ABLATIONS\n\nAs shown in Table 1, we compare HDM to the following baselines in terms of their goal-reaching abilities: GCSL (Ghosh et al., 2019) / HBC (Ding et al., 2019), HER (Andrychowicz et al., 2017) with (0, 1) rewards, HER with Soft Q-Learning (SQL) (Schulman et al., 2017), HER with (−1, 0) rewards, and HER + HBC. HER with (0, 1) reward gives the agent a reward of 1 when the goal is reached and 0 otherwise; HER with (−1, 0) reward gives a reward of 0 when a goal is reached and −1 otherwise. Those two types of rewards lead to different learning dynamics because the Q-function is initialized to output values around 0. We also compare against HER + SQL, because soft Q-learning\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Normalized performance gain over GCSL, calculated by normalizing the final performance difference between an algorithm and GCSL. We see that only HER with (−1, 0) rewards and HDM consistently outperform GCSL, while HER + HBC performs worse than HER and sometimes GCSL as well.\n\nis known to improve the robustness of learning (Haarnoja et al., 2018). HDM builds on top of HER with (−1, 0) rewards and adds a BC-like loss with a clipping condition equation 32 such that only the actions that move an agent closer to the goal get imitated (see Figure 6). Indeed, combining HER with HBC (which blindly imitates all actions in hindsight) produces worse results than not imitating at all and only resorting to value learning; HDM allows for better control over what to imitate.\n\nThe results in Table 1 and Figure 7 show that HDM achieves the strongest performance on all environments, while reducing variances in success rates. Interestingly, there is no consensus best baseline algorithm, with all five algorithms achieving good results in some environments and subpar performance in others. In Figure 10 of the appendix, we ablate the additional hyper-parameters introduced by HDM: γhdm in equation 31 and the β term in equation 14. Intuitively, γhdm controls the threshold that determines when an action is considered good enough for imitation, and β controls the trade-off between minimizing f -divergence and maximizing future goal likelihood in equation 14. The ablation shows that HDM outperforms HER and GCSL across a variety of γhdm and β values.\n\n5.3 WHY DOES HINDSIGHT BC SOMETIMES FAIL?\n\nIn this section, we aim to better understand whether the (underwhelming) performance of hindsight BC had anything to do with our specific setting. We report an interesting metric that correlates with the performance of GCSL / HBC, which can be measured prior to training a policy: initial ag (achieved-goal) change ratio. We define the ag change ratio of π to be: the percentage of trajectories where the achieved goals in initial states s0 are different from the achieved goals in final states sT under π. Most training signals are only created from ag changes, because they provide examples of how to rearrange an environment. We then define initial ag change ratio to be the ag change ratio of a random-acting policy π0. Using notation from 28, it can be computed as Es0···sT ∼π0 [−rHER(·, ·, s0, sT )].\n\nFigure 8: Success rate versus initial achieved-goal change ratio.\n\nIn Figure 8, we show the relationship between final success rates versus the initial ag (achieved-goal) change ratio, across all environments. The performance of GCSL seems to be upper-bounded by a linear relationship between the two. This makes intuitive sense: a BC-style objective starts off cloning the initially random trajectories, so if initial ag change ratio is low, the policy would not learn to rearrange ag, compounding to a low final performance. HER and HDM are able to surpass this upper ceiling likely because of the additional goal-likelihood term equation 14 besides imitation.\n\nThis finding suggests that in order to make goal-reaching easier, we should either modify the initial state distribution ρ0(s) such that ag can be easily changed through random exploration (Florensa et al., 2017) (if the policy is training from scratch), or initialize BC from some high-quality demonstrations where ag does change (Ding et al., 2019; Nair et al., 2018a; Lynch et al., 2019).\n\n6 CONCLUSION\n\nThis work presents a unified goal-reaching objective that encompasses a family of goal-conditioned RL algorithms. Our derivation illustrates the connection between hindsight goal relabeling, divergence minimization, and energy-based models. It reveals that there is a largely unexplored design space: we could potentially use other convex functions in f -divergence minimization (Nowozin et al., 2016; Ghasemipour et al., 2020), and improve the optimization of the energy-based model (Du et al., 2020) by going beyond NCE (Grathwohl et al., 2020). The primary limitation of our framework is that it does not account for exploration (Hafner et al., 2020). Eventually, we would like to incorporate empowerment (Klyubin et al., 2005; Sekar et al., 2020) and input density exploration (Bellemare et al., 2016; Pong et al., 2019) into the framework.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMichael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta, Sergey Levine, and Vikash Kumar. Robel: Robotics benchmarks for learning with low-cost robots. In Conference on robot learning, pp. 1300–1313. PMLR, 2020.\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.\n\nDhruv Batra, Angel X Chang, Sonia Chernova, Andrew J Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. Rearrangement: A challenge for embodied ai. arXiv preprint arXiv:2011.01975, 2020.\n\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nYiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp. Goal-conditioned imitation\n\nlearning. In Advances in Neural Information Processing Systems, pp. 15298–15309, 2019.\n\nYilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv\n\npreprint arXiv:1903.08689, 2019.\n\nYilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. Improved contrastive divergence\n\ntraining of energy based models. arXiv preprint arXiv:2012.01316, 2020.\n\nBen Eysenbach, Xinyang Geng, Sergey Levine, and Russ R Salakhutdinov. Rewriting history with inverse rl: Hindsight inference for policy improvement. Advances in neural information processing systems, 33:14783–14795, 2020a.\n\nBenjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. C-learning: Learning to achieve\n\ngoals via recursive classification. arXiv preprint arXiv:2011.08909, 2020b.\n\nChelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852, 2016.\n\nCarlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse curriculum generation for reinforcement learning. In Conference on robot learning, pp. 482–495. PMLR, 2017.\n\nJustin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforce-\n\nment learning. arXiv preprint arXiv:1710.11248, 2017.\n\nScott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In International conference on machine learning, pp. 1587–1596. PMLR, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRuiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow contrastive estimation of energy-based models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7518–7528, 2020.\n\nDivyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn: Inverse soft-q learning for imitation. Advances in Neural Information Processing Systems, 34, 2021.\n\nSeyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization perspective on imitation learning methods. In Conference on Robot Learning, pp. 1259–1277. PMLR, 2020.\n\nDibya Ghosh, Abhishek Gupta, Justin Fu, Ashwin Reddy, Coline Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals without reinforcement learning. ArXiv, abs/1912.06088, 2019.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.\n\nWill Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. arXiv preprint arXiv:1912.03263, 2019.\n\nWill Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, and Richard Zemel. Learning the stein discrepancy for training and evaluating energy-based models without sampling. In International Conference on Machine Learning, pp. 3732–3747. PMLR, 2020.\n\nMichael U Gutmann and Aapo Hyvärinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. Journal of machine learning research, 13(2), 2012.\n\nTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning, pp. 1352–1361. PMLR, 2017.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861–1870. PMLR, 2018.\n\nDanijar Hafner, Pedro A Ortega, Jimmy Ba, Thomas Parr, Karl Friston, and Nicolas Heess. Action\n\nand perception as divergence minimization. arXiv preprint arXiv:2009.01791, 2020.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nGeoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural\n\ncomputation, 14(8):1771–1800, 2002.\n\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural\n\ninformation processing systems, 29, 2016.\n\nEric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. ArXiv, abs/2202.02005, 2021.\n\nMichael Janner, Igor Mordatch, and Sergey Levine. gamma-models: Generative temporal difference learning for infinite-horizon prediction. Advances in Neural Information Processing Systems, 33: 1724–1735, 2020.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAlexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. Empowerment: A universal agent-centric measure of control. In 2005 ieee congress on evolutionary computation, volume 1, pp. 128–135. IEEE, 2005.\n\nIlya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution\n\nmatching. arXiv preprint arXiv:1912.05032, 2019.\n\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n\nXingyu Lin, Harjatin Singh Baweja, and David Held. Reinforcement learning without ground-truth\n\nstate. arXiv preprint arXiv:1905.07866, 2019.\n\nCorey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data.\n\narXiv preprint arXiv:2005.07648, 2020.\n\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and\n\nPierre Sermanet. Learning latent plans from play. In CoRL, 2019.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, 2013.\n\nAndriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive\n\nestimation. Advances in neural information processing systems, 26, 2013.\n\nAndriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. In In Proceedings of the International Conference on Machine Learning, 2012.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n\nOfir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. arXiv preprint\n\narXiv:2001.01866, 2020.\n\nOfir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. Advances in Neural Information Processing Systems, 32, 2019a.\n\nOfir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:\n\nPolicy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019b.\n\nAshvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE international conference on robotics and automation (ICRA), pp. 6292–6299. IEEE, 2018a.\n\nAshvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. Advances in neural information processing systems, 31, 2018b.\n\nXuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847–5861, 2010.\n\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. Advances in neural information processing systems, 29, 2016.\n\nJunhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In International\n\nConference on Machine Learning, pp. 3878–3887. PMLR, 2018.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nOpenAI OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, Ruben D’Sa, Arthur Petron, Henrique P d O Pinto, et al. Asymmetric self-play for automatic goal discovery in robotic manipulation. arXiv preprint arXiv:2101.04882, 2021.\n\nSilviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba. Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. In International Conference on Machine Learning, pp. 7750–7761. PMLR, 2020.\n\nMatthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.\n\nVitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models: Model-\n\nfree deep rl for model-based control. arXiv preprint arXiv:1802.09081, 2018.\n\nVitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-fit: State-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698, 2019.\n\nBenjamin Rhodes, Kai Xu, and Michael U Gutmann. Telescoping density-ratio estimation. Advances\n\nin Neural Information Processing Systems, 33:4905–4916, 2020.\n\nStephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. A reduction of imitation learning and\n\nstructured prediction to no-regret online learning. In AISTATS, 2011.\n\nJohn Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning.\n\narXiv preprint arXiv:1704.06440, 2017.\n\nRamanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, pp. 8583–8592. PMLR, 2020.\n\nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning, pp. 387–395. PMLR, 2014.\n\nDavid Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. Artificial\n\nIntelligence, 299:103535, 2021.\n\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999.\n\nAaron Van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\n\ncoding. arXiv e-prints, pp. arXiv–1807, 2018.\n\nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-\n\nlearning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.\n\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n\nDavid Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, 2018.\n\nMax Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681–688. Citeseer, 2011.\n\nLunjun Zhang, Ge Yang, and Bradly C Stadie. World model as a graph: Learning latent landmarks for planning. In International Conference on Machine Learning, pp. 12611–12620. PMLR, 2021.\n\nZhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. Off-policy imitation learning from observations.\n\nAdvances in Neural Information Processing Systems, 33:12402–12413, 2020.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA NOTATIONS\n\nρ0\n\np(s′ | s, a) μ (s+ | s, a) p+ ρμ(s) P πQ(s, a, g)\n\nf\n\nf ∗\n\n′\n\nf\n\n(x)\n\nρexp(s, a)\n\nρμ(s, a)\n\nT (x)\n\nρ+(g)\n\nρ+(s+)\n\nˆπ(a | s)\n\nInitial state distribution\n\nEnvironmental dynamics\n\nDiscounted future state distribution under policy μ\n\nState distribution (occupancy measure) visited by policy μ\n\nTransition operator in (goal-conditioned) Bellman backup. P πQ(s, a, g) = Ep(s′|s,a)π(a′|s′,g)[Qθ(s′, a′, g)].\n\nConvex function in f -divergence Df . Usually f (1) = 0.\n\nConvex conjugate of function f\n\nDerivative of function f\n\nState-action visitation distribution of the expert policy\n\nState-action visitation distribution of the policy μ. Note that ρμ(s, a) = ρμ(s)μ(a | s).\n\nFunction used in the variational bound of f -divergence. Also appears as T (s, a) or T (s, a, g) in the main text.\n\nThe behaviour goal distribution assumed to be given apriori by the environment.\n\nThe marginal hindsight goal distribution of a given dataset / replay, where ρ+(s+) = Eρμ(s,a)[p+ The goal-conditioned policy π marginalized over the behavioral goal distribution ˆπ(a | s) = Ep+(g)[π(a | s, g)].\n\nμ (s+ | s, a)]\n\nρπ(s, a | g)\n\nThe state-action visitation distribution of goal-conditioned policy π when conditioned on the behavioral goal g\n\nE(x)\n\nZ(θ)\n\nσ(x)\n\npn(x)\n\nk\n\n∆θ q(s, a, s+) Zq(s+)\n\nH(π)\n\nr(s, a)\n\nr(s, a, s+)\n\nr(s, a, s′, s+) rHER(s, a, s′, s+)\n\nEnergy function of an EBM: pθ(x) = exp(−Eθ(x))/Z(θ). The partition function in an EBM Z(θ) = (cid:82)\n\nX exp(−Eθ(x))dx.\n\nSigmoid function σ(x) = 1/(1 + exp(−x)).\n\nNoise distribution in Noise Contrastive Estimation (NCE)\n\nThe number of times noise samples are sampled more frequently than true data samples in NCE.\n\nThe logit of the positive sample classification loss in NCE.\n\nA conditional EBM ρπ(s, a | s+) = exp q(s, a, s+)/Zq(s+). The partition function of a conditional EBM ρπ(s, a | s+): Zq(s+) = (cid:82) Entropy of a policy across (replay) states and goals H(π) = Eρ(s)ρ+(g)π(a|s,g)[− log π(a | s, g)].\n\nS×A exp q(s, a, s+)dsda.\n\nThe (learned) reward in traditional RL settings without goalconditioning.\n\nThe (learned) reward in goal-conditioned divergence minimization.\n\nThe (generalized) reward used in HER-style multi-goal RL.\n\nThe reward function used in HER. It equals 0 when s′ = s+ and −1 when s′ ̸= s+. Also denoted as rHER(·, ·, s′, s+).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nB PROOFS\n\nB.1 MAIN LEMMAS\n\nLemma B.1 (Online-to-offline transformation for goal reaching). Given a goal-conditioned policy π(a | s, g), its corresponding Q-function Qπ(s, a, g), and arbitrary state-action visitation distribution ρμ(s, a) of another policy μ(a | s), the expected temporal difference for online rollouts under π is: Ep(g)ρπ(s,a|g)[(Qπ − γ · P πQπ)(s, a, g)] = Ep(g)ρμ(s,a)π( ̃a|s,g)[Qπ(s, ̃a, g) − γ · P πQπ(s, a, g)]\n\nProof of Lemma 4.1.\n\nEp(g)ρπ(s,a|g)[(Qπ − γ · P πQπ)(s, a, g)] = Ep(g)ρπ(s,a|g)[Qπ(s, a, g) − γEp(s′|s,a),π(a′|s′,g)Qπ(s′, a′, g)]\n\n= (1 − γ)\n\n= (1 − γ)\n\n∞ (cid:88)\n\nt=0\n\n∞ (cid:88)\n\nt\n\nγtE\n\np(g)ρt\n\nπ(s|g)\n\nπ(a|s,g)\n\n(cid:104)\n\nQπ(s, a, g) − γE\n\np(s′|s,a) π(a′|s′,g)\n\nQπ(s′, a′, g)\n\n(cid:105)\n\n(cid:40)\n\nγtE p(g) ρt π(s|g) π(a|s,g)\n\n[Qπ(s, a, g)] − γt+1E p(g)\n\n[Qπ(s, a, g)]\n\n(cid:41)\n\nρt+1 π (s|g) π(a|s,g)\n\n= (1 − γ)Ep(g),ρ0(s),π(a|s,g)[Qπ(s, a, g)]\n\n(cid:40)\n\n∞ (cid:88)\n\n= (1 − γ)\n\nt\n\n∞ (cid:88)\n\nt=0\n\n= (1 − γ)\n\nγtE p(g) μ(s) π(a|s,g)\n\nρt\n\n[Qπ(s, a, g)] − γt+1E p(g) ρt+1 μ (s) π(a|s,g)\n\n(cid:41)\n\n[Qπ(s, a, g)]\n\nγtE\n\np(g)ρt\n\nμ(s,a)\n\nπ( ̃a|s,g)\n\n[Qπ(s, ̃a, g) − γE\n\np(s′|s,a) π(a′|s′,g)\n\nQπ(s′, a′, g)]\n\n= Ep(g)ρμ(s,a)π( ̃a|s,g)[Qπ(s, ̃a, g) − γEp(s′|s,a),π(a′|s′,g)Qπ(s′, a′, g)]\n\nLemma B.2 (Gradient of the noise-contrastive term in energy-based goal-reaching). Given the following definition for the logit of a NCE-like binary classifier, with ρ+(g) = ρ+(g):\n\n∆θ(s, a, g, k) = Qθ(s, a, g) − log Eρ+(g)[exp Qθ(s, a, g)] − log k\n\n(33)\n\nThe gradient of the negative NCE term in the density ratio estimation approaches zero as k → ∞:\n\nEρμ(s,a)ρ+(g)\n\n(cid:104)\n\nk · log\n\n(cid:16)\n\nd dθ\n\n1 − σ(∆θ(s, a, g, k))\n\n(cid:17)(cid:105) k→∞−−−−→ 0\n\nProof of Lemma B.2. σ is the sigmoid function, and Zθ(s, a) = Eρ+(g)[exp Qθ(s, a, g)]:\n\n1 − σ(∆θ(s, a, g, k)) =\n\n1 1 + exp ∆θ(s, a, g, k)\n\n=\n\n1 1 + exp(Qθ(s, a, g) − log Zθ(s, a))/k\n\n(34)\n\nPlugging in the above into the loss and taking the gradient:\n\n(cid:104)\n\n(cid:104)\n\n− k · log\n\n(cid:16) exp Qθ(s, a, g) k · Zθ(s, a)\n\n(cid:17)(cid:105)\n\n+ 1\n\n− k · log\n\n(cid:16)\n\nexp\n\n(cid:16) 1 k\n\nQθ(s, a, g) − log Zθ(s, a)\n\n(cid:17)\n\n(cid:17)(cid:105)\n\n+ 1\n\nd dθ\n\nd dθ\n\n=\n\nEρμ(s,a) ρ+(g)\n\nEρμ(s,a) ρ+(g) (cid:104)\n\n=Eρμ(s,a) ρ+(g)\n\n−\n\nexp Qθ(s, a, g)/Zθ(s, a) 1/k · exp Qθ(s, a, g)/Zθ(s, a) + 1\n\nd dθ\n\n(cid:16)\n\nQθ(s, a, g) − log Zθ(s, a)\n\n(cid:17)(cid:105)\n\nk→∞−−−−→Eρμ(s,a)\n\nρ+(g)\n\n(cid:104)\n\n−\n\nexp Qθ(s, a, g) Eρ+(g)[exp Qθ(s, a, g)]\n\nd dθ\n\n(cid:16)\n\nQθ(s, a, g) − log Eρ+(g)[exp Qθ(s, a, g)]\n\n(cid:17)(cid:105)\n\n15\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\nUnder review as a conference paper at ICLR 2023\n\nThe first term inside the expectation of ρμ(s, a):\n\nThe second term:\n\n−1 Eρ+(g)[exp Qθ(s, a, g)]\n\nEρ+(g)[exp Qθ(s, a, g)\n\nd dθ\n\nQθ(s, a, g)]\n\nEρ+(g)[exp Qθ(s, a, g)] Eρ+(g)[exp Qθ(s, a, g)] 1\nEρ+(g)[exp Qθ(s, a, g)] 1\nEρ+(g)[exp Qθ(s, a, g)]\n\n=\n\n=\n\nd dθ\n\nd dθ\n\nlog Eρ+(g)[exp Qθ(s, a, g)]\n\nEρ+(g)[exp Qθ(s, a, g)]\n\nEρ+(g)[exp Qθ(s, a, g)\n\nd dθ\n\nQθ(s, a, g)]\n\n(39)\n\n(40)\n\n(41)\n\n(42)\n\nThe two terms cancel out and yield a gradient of 0.\n\nLemma B.3 (Goal-conditioned Q-functions estimate PMI on given trajectories). Given that we set apriori the behavioral goal distribution to be ρ+(g) = (cid:82) S×A pμ(s, a, s+)dsda. And assuming that the state-action distribution of π marginalized over behavioral goals Eρ+(g)[ρπ(s, a | g)] is the same as ρμ(s, a), which means π stays in the same state-action visitation distribution as μ. Then: on trajectories generated by μ, the point-wise mutual information (PMI) between a state-action pair (s, a) and a future state g is given by the Q-function at convergence Qπ:\n\nPMI((s, a), g) = Qπ(s, a, g) − log Eρ+(g)[exp Qπ(s, a, g)] − log(r + (γP πQπ − Qπ)(s, a, g))\n\nProof of Lemma B.3. We first start with the optimal T ∗ in the f -divergence bound equation 5, applied to the special case of the function f being a quadratic equation 19:\n\nCombining the two, we get:\n\nT ∗(x) = f ′(p(x)/q(x))\n\nf ′(t) = t − r\n\np(x)/q(x) − r = T ∗(x)\n\n(43)\n\n(44)\n\n(45)\n\nNow applying this identity to the f -divergence minimization problem in equation 15 (note that we have set r(s, a, g) = −T (s, a, g) in our derivation): ρ+(g)ρπ(s, a | g) ρμ(s, a)p+ μ (g | s, a)\n\n= ̄r − (Qπ − γP πQπ)(s, a, g)\n\npπ(s, a, g) pμ(s, a, s+)\n\n(46)\n\n=\n\nWe now take the relationship in equation 21:\n\nρπ(s, a | g) ρπ(s, a)\n\n=\n\nexp Qπ(s, a, s+) Eρ+(g)[exp Qπ(s, a, g)]\n\n= ̄r + (γP πQπ − Qπ)(s, a, g)\n\n(47)\n\n(48)\n\nUsing this substitution, we arrive at:\n\nρ+(g)ρπ(s, a)\n\nρμ(s, a)p+\n\nμ (g | s, a)\n\nexp Qπ(s, a, s+) Eρ+(g)[exp Qπ(s, a, g)]\n\n= ̄r + (γP πQπ − Qπ)(s, a, g)\n\n(49)\n\nSwapping the nominator and denominator and applying the assumption that ρπ(s, a) = ρμ(s, a):\n\nρμ(s, a)p+\n\nμ (g | s, a)\n\nρμ(s, a)ρ+(g)\n\nEρ+(g)[exp Qπ(s, a, g)] exp Qπ(s, a, g)\n\n=\n\n1 ̄r + (γP πQπ − Qπ)(s, a, g)\n\n(50)\n\nTaking the log on both sides, we get the following expression of P M I((s, a), g):\n\nlog\n\np+\n\nμ (g | s, a) ρ+(g)\n\n= Qπ(s, a, g) − log Eρ+(g)[exp Qπ(s, a, g)] − log( ̄r + (γP πQπ − Qπ)(s, a, g))\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nB.2 NCE LOSSES\n\nWe now complete the derivation of the NCE losses in section 4.2, by illustrating that the positive classification loss of NCE reduces to an InfoNCE (Van den Oord et al., 2018) loss in equation equation 23 under our definition of the logit equation 22. More specifically, given that we already have Lemma B.2, we only need to show:\n\nd dθ\n\nlog(1 + exp ∆θ(s, a, s+, k)) k→∞−−−−→ 0\n\nWe simply follow the definition of ∆θ in equation equation 22 and take the gradient of the above:\n\nexp ∆θ(s, a, s+, k) 1 + exp ∆θ(s, a, s+, k)\n\nd dθ\n\n∆θ(s, a, s+, k)\n\n=\n\n1 1 + exp(−∆θ(s, a, s+, k))\n\n(cid:18)\n\n∇θQθ(s, a, s+) −\n\nEρ+(g)[exp Qθ(s, a, g)∇θQθ(s, a, g)] Eρ+(g)[exp Qθ(s, a, g)]\n\n(cid:19)\n\n=\n\n1 + k ·\n\n1\n\nEρ+(g)[exp Qθ(s, a, g)] exp Qθ(s, a, s+)\n\n(cid:18)\n\n∇θQθ(s, a, s+) −\n\nEρ+(g)[exp Qθ(s, a, g)∇θQθ(s, a, g)] Eρ+(g)[exp Qθ(s, a, g)]\n\n(cid:19)\n\nAs k → ∞, we see that the gradient approaches 0 because the scalar on the left approaches 0.\n\nCombining the above result about a part of the positive classification loss in NCE with the Lemma in B.2 which deals with the negative classification loss in NCE, we can arrive at the combined NCE loss at its limit k → ∞, as pointed out in equation 23 of the main text:\n\narg max Q\n\nEρμ(s,a)\n\n(cid:104)\n\nE\n\n(cid:105) π (s+|s,a)[Qθ(s, a, s+)] − log Eρ+(g)[exp Qθ(s, a, g)]\n\np+\n\nAs mentioned, this is roughly equivalent to the InfoNCE loss (Van den Oord et al., 2018), further validating our analysis so far.\n\nB.3 EBM LOSSES\n\np+\n\nπ (s+|s,a)[Qθ(s, a, s+)] − log Eρ+(g)[exp Qθ(s, a, g)]] from an arbitrary To optimize Eρμ(s,a)[E π (s+ | s, a): dataset of behaviors ρμ(s, a, s+), we can easily see that the problem lies in accessing p+ we do not have complete access to the distribution of \"positive samples\", as sampling directly from p+ π (s+ | s, a) requires on-policy rollouts. But this can be resolved by using importance weights and Equation equation 4 to rewrite E\n\nπ (s+|s,a)[Qθ(s, a, s+)]:\n\np+\n\n(1 − γ) · E\n\np(s′|s,a) ρμ(s,a)\n\n[Qθ(s, a, s′)] + γ · E\n\np(s′|s,a)ˆπ(a′|s′) ρ+(g)ρμ(s,a)\n\n(cid:104) p+\n\nπ (g | s′, a′) ρ+(g)\n\n(cid:105)\n\nQθ(s, a, g)\n\n(51)\n\nπ (s+|s, a): Above, we have introduced a new notation ˆπ(a | s) to address the following issue with p+ while π is a goal-conditioned policy, p+ π (s+|s, a) is not conditioned on a goal apriori. A similar problem was encountered (but ignored) in C-Learning (Eysenbach et al., 2020b), which simply assumed that the multi-goal policy was aposteriori conditioned on the same future state that got sampled from this policy in the first place (a contradiction). To avoid this problem, we define ˆπ(a | s) = Ep+(g)[π(a | s, g)] and assume that p+ π (s+|s, a) is sampled under ˆπ(a | s) beyond a single step. We package all the EBM training losses into the following:\n\nEρμ(s,a){−(1 − γ) · Ep(s′|s,a)[Qθ(s, a, s′)]}\n\n(cid:26)\n\nEρμ(s,a)\n\nlog Eρ+(g)[exp Qθ(s, a, g)] − γE\n\np(s′|s,a) π(a′|s′,g) ρ+(g)\n\n(cid:20) p+\n\nπ (g | s′, a′)ˆπ(a′ | s′) ρ+(g)π(a′ | s′, g)\n\nQθ(s, a, g)\n\n(cid:21)(cid:27)\n\n(52)\n\n(53)\n\nTo decompose the importance weight, note that the policy is trying to maximize the Q-values under the entropy constraint in equation 14, resulting in a Boltzmann policy (Haarnoja et al., Eπ(a|s,g)[Q(s, a, g)] − H(π) = 2017; Schulman et al., 2017; Haarnoja et al., 2018): arg maxπ\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nexp Q(s, a, g)/(cid:80) ing loss, with ⊥ (·) being the stop-gradient sign:\n\na exp Q(s, a, g). Minimizing equation 53 is equivalent to minimizing the follow-\n\n(cid:26)\n\n(cid:16)\n\n⊥\n\nexp Q(s, a, g) Eρ+(g)[exp Q(s, a, g)]\n\n− γ ·\n\nE ρμ(s,a) ρ+(g) p(s′|s,a)\n\n(cid:80) Eρ+(g)[(cid:80)\n\na exp Q(s′, a, g)/|A|\n\na exp Q(s′, a, g)/|A|]\n\n(cid:17)\n\n(cid:27)\n\nQθ(s, a, g)\n\n(54)\n\nProof of equation 54. Let webm(s′, a′, g) denote the importance weight:\n\nwebm(s′, a′, g) =\n\nπ (g | s′, a′) p+ ρ+(g)\n\nˆπ(a′ | s′) π(a′ | s′, g)\n\n(55)\n\nFirstly, because of the entropy constraint in equation 14, we have a Boltzmann policy defined on the Q-function: arg maxπ\n\nEπ(a|s,g)[Q(s, a, g)] − H(π) = exp Q(s, a, g)/(cid:80)\n\na exp Q(s, a, g):\n\nSecondly, we note that the first term in importance ratio can be estimated from equation 21. Thus we have:\n\nπ(a | s, g) ∝ exp Q(s, a, g)\n\n(56)\n\nwebm(s′, a′, g) =\n\nexp Q(s′, a′, g) Eρ+(g)[exp Q(s′, a′, g)]\n\n·\n\nEρ+(g)[exp Q(s′, a′, g)]\n\n(cid:80)\n\na\n\nEρ+(g)[exp Q(s′, a, g)]\n\n(cid:80)\n\na exp Q(s′, a, g) exp Q(s′, a′, g)\n\n·\n\n(cid:80) a exp Q(s′, a, g) Eρ+(g)[exp Q(s′, a, g)]\n\n=\n\n(cid:80)\n\na\n\nTaking the gradient of equation equation 53 inside the expectation of Eρμ(s,a)p(s′|s,a)[·]:\n\nEρ+(g)[exp Q(s, a, g)∇θQθ(s, a, g)] Eρ+(g)[exp Q(s, a, g)]\n\n−\n\nγEρ+(g)[(cid:80) (cid:80)\n\na\n\na exp Q(s′, a, g)∇θQθ(s, a, g)] Eρ+(g)[exp Q(s′, a, g)]\n\n(57)\n\n(58)\n\n(59)\n\nDividing both the nominator and denominator of the right-hand side by 1/|A|, putting the expectation Eρμ(s,a)p(s′|s,a)[·] back in, and utilizing the stop-gradient sign ⊥ (·), we arrive at the loss:\n\n(cid:26)\n\n(cid:18)\n\n⊥\n\nexp Q(s, a, g) Eρ+(g)[exp Q(s, a, g)]\n\n− γ ·\n\nE ρμ(s,a) ρ+(g) p(s′|s,a)\n\nB.4 DERIVING HER REWARDS\n\n(cid:80) Eρ+(g)[(cid:80)\n\na exp Q(s′, a, g)/|A|\n\na exp Q(s′, a, g)/|A|]\n\n(cid:19)\n\n(cid:27)\n\nQθ(s, a, g)\n\nThe purpose of this section is to derive the reward function used in HER from the equation 26:\n\narg min Q\n\nE\n\nρμ(s,a)p(s′|s,a)p+\n\nμ (s+|s,a)\n\n(cid:104)\n\n(cid:105) f ∗(−(Qθ − γP πQ)(s, a, s+)) − β · (1 − γ)Qθ(s, a, s′)\n\nRecall that we have defined p+\n\nμ (s+ | s, a) in equation 4:\n\nμ (s+ | s, a) = (1 − γ)p(s+ | s, a) + γ p+\n\n(cid:90)\n\nS×A\n\np(s′ | s, a)μ(a′ | s′)p+\n\nμ (s+ | s′, a′)ds′da′\n\nAnd that we have defined a quadratic form of f ∗ in equation 19 (with r and c being constants):\n\nf ∗(x) = (x + r)2/2 + c\n\nUsing the dynamics to expand the expectation and applying the choice of f ∗ being a quadratic, the loss becomes:\n\nE\n\narg min Q\n\nρμ(s,a)p(s′|s,a)(1 − γ) ·\n\n(cid:16)\n\n(cid:104) 1 2\n\nr + (γP πQ − Qθ)(s, a, s′)\n\n(cid:17)2\n\n− β · Qθ(s, a, s′)\n\n(cid:105)\n\n+ E\n\nρμ(s,a)p(s′|s,a)μ(a′|s′)p+\n\nμ (s+|s′,a′)\n\n(cid:104)\n\nγ ·\n\n(cid:16)\n\n1 2\n\nr + (γP πQ − Qθ)(s, a, s+)\n\n(cid:17)2(cid:105)\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nAssuming that there is a stop gradient sign on P πQ because of the use of a target network, we can rewrite the above as one single quadratic and see that the gradient of the above loss w.r.t Q is equivalent to the gradient of the following squared Bellman residual:\n\narg min Q\n\nE\n\nρμ(s,a)p(s′|s,a)p+\n\nμ (s+|s,a)\n\n(cid:16)\n\n(cid:104) 1 2\n\nr(s, a, s′, s+) + (γP πQ − Qθ)(s, a, s+)\n\n(cid:17)2(cid:105)\n\nwhere the reward function r(s, a, s′, s+) is:\n\n(cid:26)r + β,\n\nr,\n\ns′ = s+ s′ ̸= s+\n\nC ABLATIONS AND HYPER-PARAMETERS\n\nIn this section, we include additional ablations and hyper-parameters. We first describe the goalreaching environments in more details in Figure 9. We use the following thresholds (for Euclidean norms) for determining success: [0.08, 0.08, 0.05, 0.05, 0.1], which are tight thresholds based on our visualizations of the environments. We use the same network architecture, sampling and optimization schedules for all the methods, as described in Table 2. As for γHDM, we set it to be 0.85 in Four Rooms and Lunar Lander, 0.5 in Sawyer Push and Claw Manipulate, and 0.4 for Door Opening. Ablation on this hyper-parameter can be found in Figure 10. For next state relabeling ratio, we set the default to be 0.2, increase it to 0.5 in Lunar Lander, and 0.6 in Sawyer Push and Door Opening. For the soft-Q-learning (Schulman et al., 2017) + HER baseline, we set the temperature parameter to be 0.2, which we have found to empirically perform well.\n\nFigure 9: Goal-reaching environments from GCSL (Ghosh et al., 2019) that we consider in this paper: reaching a goal location in Four Rooms, landing at a goal location in Lunar Lander, pushing a puck to a goal location in Sawyer Push, opening the door to a goal angle in Door Open (Nair et al., 2018b), turning a valve to a goal orientation in Claw Manipulate (Ahn et al., 2020).\n\nFigure 10: Ablation studies on HDM Gamma and Beta. HDM Gamma refers to γhdm in equation 31, and HDM Beta refers to the β term in equation 14. The orange line and the blue line denote HER and GCSL baseline performance. See Section 5.2 for further discussion of these results.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Hyper-parameters\n\nParameters\n\nValue\n\nOptimizer Number of hidden layers (all networks) Number of hidden units per layer Non-linearity Polyak for target network Target update interval Ratio between env vs optimization steps Initial random trajectories Hindsight relabelling ratio Update every # of steps in environment Next state relabelling ratio Learning rate Batch size\n\nAdam (Kingma & Ba, 2014) 2\n[400, 300] (Fujimoto et al., 2018) ReLU 0.995 10 1\n200 0.85 50 0.2 5.e-4 256\n\n20",
    "reference": "# Summary Of The Paper\n\nThis paper analyzes hindsight experience replay (HER) through divergence minimization with energy based models. With this analysis, it presents an approach that combines HER with behavioral cloning (BC) by only imitating actions in the dataset if they move the agent a certain amount towards the goal.\n\nThe paper concludes by comparing this adaptive HER + BC technique (termed hindsight divergence minimization, HDM) to HER, behavioral cloning and a more straightforward implementation of HER +BC.\nThe paper concludes with a hypothesis that BC-based techniques work better if the dataset used for learning has a higher proportion of achieved goals in its trajectories, and validates this hypothesis via experiments.\n\n# Strength And Weaknesses\n\n### Strengths\n* This paper conducts a study of why hindsight replay helps when learning in goal-conditioned RL from the perspective of divergence minimization.\n* This study leads the paper to propose an adaptive algorithm that can take advantage of high quality demonstrations.\n* The perspective of goal-conditioned RL as a form of divergence minimization could be interesting.\n\n### Questions and Weaknesses:\n* The paper does not have a consolidated related works section. Are there other papers that aim to address goal-conditioned RL via a divergence minimization perspective? For example, it seems that the paper should compare to [1,2] in their exposition as well as their experiments.\n* While the rigor in section 4 is to be appreciated, it takes away from ease of understanding and clarity.\n* Section 5.3 was unclear on what _achieved-goal_ means. If that section essentially points out that GCSL works better in situations where the trajectories in the dataset get to the goal efficiently, then it is unclear why this is an important insight. BC with expert trajectories is more likely to work better. A better comparison would have been to also plot HER + HBC to show that these sorts of trajectories do not hard HER learning as much.\n* How do the results in Table 1 differ from the results in Figure 7?\n\n### References:\n[1] Ma, Y.J., Yan, J., Jayaraman, D. and Bastani, O., 2022. How Far I'll Go: Offline Goal-Conditioned Reinforcement Learning via $ f $-Advantage Regression. arXiv preprint arXiv:2206.03023.\n\n[2] Durugkar, I., Tec, M., Niekum, S. and Stone, P., 2021. Adversarial intrinsic motivation for reinforcement learning. Advances in Neural Information Processing Systems, 34, pp.8622-8636.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n* Clarity and Quality: Fair. As pointed out above, the mathiness retracts from ease of following the exposition, but the notation and math seems clear enough.\n* Novelty: Weak. It is unclear how this approach compares and contrasts with prior work. There is no dedicated related work comparison, and significant recent work has not been addressed.\n* Reproducibility: Good: Setup and hyper-parameters have been provided.\n\n# Summary Of The Review\n\nThe paper offers an interesting perspective on HER, and presents an approach to adaptively improve performance by incorporating BC into the objective if it is helping the agent learn. However, there might be related work that is not addressed in the paper, and might perhaps need to be compared to.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nHYBRID NEURO-SYMBOLIC REASONING BASED ON MULTIMODAL FUSION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDeep neural models and symbolic Artificial Intelligence (AI) systems have contrasting advantages and disadvantages. Neural models can be trained from raw, incomplete and noisy data to obtain abstraction of features at various levels, but their uninterpretability is well-known. On the other hand, the traditional rulebased symbolic reasoning encodes domain knowledge, but its failure is often attributed to the acquisition bottleneck. We propose to build a hybrid learning and reasoning system which is based on multimodal fusion approach that brings together advantageous features from both the paradigms. Specifically, we enhance convolutional neural networks (CNNs) with the structured information of ‘if-then’ symbolic logic rules obtained via word embeddings corresponding to propositional symbols and terms. With many dozens of intuitive rules relating the type of a scene with its typical constituent objects, we are able to achieve significant improvement over the base CNN-based classification. Our approach is extendible to handle first-order logical syntax for rules and other deep learning models.\n\n1\n\nINTRODUCTION\n\nDeep learning technology is being employed with increasing frequency in recent years LeCun et al. (2015)Schmidhuber (2014). Various deep learning models have achieved remarkable results in computer vision Krizhevsky et al. (2017), remote sensing Zhu et al. (2017), target classification in SAR images Chen et al. (2016), and speech recognition Graves et al. (2013)Hinton et al. (2012). In the domain of natural language processing (NLP), deep learning methods are used to learn word vector representations through neural language models Mikolov et al. (2013) and performing composition over the learned word-vectors for classification Collobert et al. (2011). Convolutional neural networks (CNNs), for example, utilize layers with convolving filters that are applied to local features. CNN is widely used for image tasks and is currently state-of-the-art for object recognition and detection. Originally invented for computer vision, CNN models have subsequently been shown to be effective for NLP and have achieved excellent results in semantic parsing Yih et al. (2015), search query retrieval Shen et al. (2014), sentence modelling Kalchbrenner et al. (2014), and other traditional NLP tasks Collobert et al. (2011).\n\nHowever, the success of deep learning comes at a cost. The first and foremost is its reliance on large amounts of labeled data, which are often difficult to collect and entail a slow learning process. Second, deep models are brittle in the sense that a trained network that performs well on one task often performs very poorly on a new task, even if the new task is very similar to the one it was originally trained on. Third, they are strictly reactive, meaning that they do not use high-level processes such as planning, causal reasoning, or analogical reasoning. Fourth, human expertise cannot be used which can often reduce the burden of acquiring training data which is often expensive to collect. Purely data-driven learning can lead to uninterpretable and sometimes counter-intuitive results Nguyen et al. (2014)Szegedy et al. (2013).\n\nThe sub-symbolic neural approaches allow us to mimic human cognitive thought processes by extracting features at various levels of abstraction from direct observation and thereby facilitate learning. But humans also learn from general high-level knowledge expressed declaratively in logical syntax. A representation language allows recursive structures to be easily represented and manipulated, which is usually difficult in a neural learning environment. But a symbolic reasoning system is not good to adapt to new environments by learning and reasoning based on traditional theorem-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nproving, which can be computationally expensive. Moreover, a purely symbolic system based on traditional AI requires enormous human effort as knowledge are manually programmed and not learned. Central to classical AI is the use of language-like propositional representations to encode knowledge. The symbolic elements of a representation in classical AI – the constants, functions, and predicates – are typically hand-crafted. Inductive Logic Programming Muggleton (1990) methods learn hypothesis rules given background knowledge, and a set of positive and negative examples. The systems we have discussed until now do not model uncertainty which is essential in practical applications. Various probabilistic logics Halpern (2005) and Markov Logic Networks Richardson & Domingos (2006) (MLNs) handle uncertainty using weight attached to every rule. Practical applications of these networks have been limited as inference is not scalable to a large number of rules.\n\nIt is, therefore, desirable to develop a hybrid approach, embedding declarative representation of knowledge, such as domain and commonsense knowledge, within a neural system. In this paper, hybrid approach is applied to indoor scene classification, which has been extensively studied in field of computer vision Chen et al. (2018). However, compared with outdoor scene classification, this is an arduous issue due to the large variety of density of objects within a typical scene. In addition, high-accuracy models already exist for outdoor scene classification while indoor scene classification is not. In order to accomplish our objective, the acquisition, representation, and utilization of visual commonsense knowledge represents a set of critical opportunities in advancing computer vision past the stage where it simply classifies or identifies which objects occur in imagery Davis et al. (2015).\n\nThe contributions of this paper is summarized as followed:\n\n• A joint representation multimodal fusion framework is applied to exploit the early fusion of vectorized logical knowledge and images for the task of indoor scene classification. Experiments show that higher classification accuracy is obtained compared to traditional image classification methods.\n\n• A ‘if-then’ logical knowledge system is built based on reviews of each indoor scene class which are scraped from Google open source, through Word2Vec and BERT embedding. This helps to get a better contextual representation of words detected by object detection.\n\n• A unique rules embedding approach is proposed, which allows to converge ‘if-then’ logic of probability with image representation. The embedding approach has different representations during training and inference process.\n\nThe rest of the paper is organized as follows. The next section 2 surveys the related work. The hybrid framework is explained in section 3. Section 4 details implementation and evaluation of experiments. Finally, we conclude with some future directions in 5.\n\n2 RELATED WORK\n\nHybrid neural-symbolic systems concern Chen et al. (2016)Garcez et al. (2009)Hammer & Hitzler (2007)Rosenbloom et al. (2017)Sun (1994)Wermter & Sun (2001) the use of problem-specific symbolic knowledge within the neurocomputing paradigm, specifically, symbolic domain and commonsense knowledge within the deep learning paradigm in our case. They are useful for enhancing various tasks, including logical inferencing, extracting relational knowledge Guillame-Bert et al. (2010)Gust et al. (2007), image classification, and action selection.\n\nCombination of logic rules and neural networks has been considered to construct network architectures from given rules to perform reasoning and knowledge acquisition. Neural-symbolic systems, such as EBL-ANN Shavlik & Towell (1989), KBANN Szegedy et al. (2013) and C-ILP Garcez et al. (2009), LENSR Xie et al. (2019), like our proposal, deal with propositional formulae. KBANN, for example, maps problem-specific domain theories, represented in propositional logic, into neural networks and then refines this reformulated knowledge using back-propagation. Propositional symbols are directly represented as nodes whereas we vectorize each propositional symbol as its semantic representation and appended to the abstraction of low-level observations.\n\nOther neural-symbolic systems are exploring on knowledge graph Chen et al. (2020)Kampffmeyer et al. (2019)Li et al. (2019)Zablocki et al. (2019), which is a natural symbol. It is not only a se-\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nmantic network to describe entity relationships, but also a formal description framework for general semantic knowledge.\n\nA large amount of neural-symbolic approaches focus on first-order inference Zhang et al. (2020)Marra et al. (2020)Yang & Song (2020)Cai et al. (2021). But some do not allow one to learn vector representations of symbols from training facts of a knowledge base, such as SHRUTI Shastri (1999), Neural Prolog Ding et al. (1996), CLIP++ Franca et al. (2014), and Lifted Relational Neural Networks Sourek et al. (2015). Neural Reasoner Peng et al. (2015) translates query representations in vector space without rule representations and can, thus, not incorporate domain specific knowledge. The Neural Theorem Prover Rockt ̈aschel & Riedel (2016) and Unification Neural Networks H ̈olldobler (1990)Komendantskaya (2011) build upon differentiable backward chaining, but the former operates on vector representations of symbols whereas the latter on scalar values. Grounded Abductive Learning (GABL) Cai et al. (2021) is proposed to enhance machine learning models with abductive reasoning in a ground domain knowledge base, which offers inexact supervision through a set of logic propositions.\n\nDifferent frameworks of neural-symbolic system rely on various logics Raedt et al. (2019). For example, Logic Tensor Networks Donadello et al. (2017) is based on first-order logic, Lifted Rule Injection Demeester et al. (2016) exploits implication rules, Semantic Loss Function Xu et al. (2018) focuses on propositional logic. Other deep learning systems such as TensorLog Cohen (2016) uses datalog, while DeepProbLog Manhaeve et al. (2019) uses clausal logic. Among these frameworks, Semantic Loss Function and DeepProbLog not only embed logic, but also add probabilistic to neural networks, which are two principal factors of reasoning.\n\nLike our method, DeepProbLog Manhaeve et al. (2019) purposes a framework where expressive probabilistic-logical modeling and reasoning are combined, which could be trained end-to-end. Yang et al. Yang & Song (2020) proposed Neural Logic Inductive Learning (NLIL), an efficient differentiable ILP framework that learns first-order logic rules to explain problem in the scope of inductive logic programming (ILP). Marra et al. Marra et al. (2020) presented Relational Neural Machines (RNM), a novel framework to converge deep architectures and probabilistic logic reasoning. It is able to recover both classical learning in case of pure sub-symbolic learning, and Markov Logic Networks in case of pure symbolic reasoning. Mao et al. Mao et al. (2019) proposed the neuro-symbolic concept learner (NS-CL), which is able to represent object-based scene and translate sentences into executable, symbolic programs.\n\n3 HYBRID NEURO-SYMBOLIC METHOD\n\n3.1 OVERVIEW\n\nLogical knowledge representation is symbolic in nature, i.e. the data structures under consideration basically consist of words over some language. A logic program Lloyd (1984) is a set of (universally quantified) disjunctions, called clauses or rules, which in turn consist of atoms and negated atoms only. Definite rules of ‘if-then’ type have a conjunction of atoms as antecedent and one atom as consequent. Successful connectionist architectures, however, can be understood as networks (essentially, directed graphs) of simple computational units, in which activation is propagated and combined in certain ways adhering to connectionist principles. In many cases like, for example, in multi-layer perceptrons, the activation is encoded as a real number; input and output of such networks consist of tuples (vectors) of real numbers.\n\nIn order to integrate logic and connectionism, we, thus, have bridged the gap between the discrete, symbolic setting of logic, and the continuous, real-valued setting of artificial neural networks. In this paper, we propose an approach to multimodal learning which involves relating information from multiple sources. Specifically, we focus on learning representations for images which are coupled with vectorized features of propositional rules, along the line of multimodal video/audio deep learning in Ngiam et al. (2011) or a multimodal deep Boltzmann machine (DBM) Srivastava & Salakhutdinov (2012). The bimodal DBM in our case models the joint distribution over image and symbolic knowledge inputs. The joint distribution over the multi-modal input variables is written as\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\np(vm, va; θ) =\n\n1 ZM (θ)\n\n×\n\nexp(Replicated Sof tmax Symbolic P athway\n\n(cid:88)\n\nh\n\n+ Gaussian Image P athway\n\n+ Joint F ully Connected Layer)\n\nwhere h is all hidden variables with superscripts in parentheses, Z is the normalization constant depending on the number of propositional symbols in all the rules. The image-specific DBM uses Gaussian distribution to model the distribution over real-valued image features. Similarly, rulespecific specific DBM uses Replicated Softmax to model the distribution over word count vectors.\n\nWord-embeddings of symbolic and subjective rules adopt divergent representations, whilst Word2Vec and BERT are introduced in this paper to represent the contextual knowledge. The usage of BERT representation will provide stronger semantic contextual meaning and relevance with each label, so that the hybrid network is able to understand symbolic rules better Kalchbrenner et al. (2014). The Word2Vec representation captures many linguistic regularities and preserves semantic similarity meaning. We set the value of the absent modality to zero when computing the shared representation, which is consistent with the feature learning phase. Word-embeddings of symbolic rules and objects images are extracted and input into the Word2Vec and BERT model.\n\nThe proposed hybrid framework with CNN has been applied to an indoor scene classification scenario with 5 classes, namely, library, museum, concert, church, mall. Here is how we prepare the labelled instances for training and text from an image corpus and a set of logical rules, as shown in 1. In the scenario, for example, for an input image corpus, the pixel representation of each image is fed into the hybrid model but coupled with zeros if no object is identified in the image, or the vectorized representation of the identified objects within the image; so if the image scene is of type “library” and if the only identified object is “shelf ” then an abstraction of the image pixel array at a certain level is coupled with the word embedding of “shelf ”. The combined representation then becomes a training sample with the label “library” and propagated into a fully connected classification network.\n\nOn the other hand, if we have a domain propositional logic rule “if shelf & table then library (0.8)”, then vectorization of “shelf & table” is coupled with zeros for the image modality. The combined representation also then becomes a training sample, but in this case the component corresponding to “library” of the 1-to-C coding of the label will have 0.8 instead of 1. This hybrid approach is an early fusion of modalities. Moreover, in order to integrate Word2Vec and BERT representations, logic rules are alternated into these two representations to get specific vectorization as explained above, then merged horizontally into a vector.\n\nWe have made use of publicly available images and textual blogs to generate hundreds of domain rules. Like the example above, automatically from applying Bayes’ probability and then merged with additional hand coded domain rules. Each image is also optionally tagged with a number of objects. We can leverage on one or more of many existing object detection frameworks, such as Spatial Pyramid Pooling (SPP) He et al. (2014), OverFeat Sermanet et al. (2013), Multibox SSD Liu et al. (2016), Fast R-CNN Girshick (2015), YOLO Redmon et al. (2016), and Faster R-CNN Ren et al. (2015), and TensorFlow based Google object detection or Caffe tool. The hybrid framework strongly improves over the basic CNN as illustrated by examples in Figure 1, but much more details are in the implementation and evaluation section. To the best of our knowledge, this is a state-ofthe-art approach to tightly integrate traditional ‘if-then’ logic rules with CNN.\n\n3.2 RULES GENERATION\n\nTwo different types of logical rules are generated. One is initiated by object detection algorithm and Bayes’ theorem, another is manually preset by us. Our main approach is based on automatically generated rules from Bayes’ theorem. However, we have compared the effect of two types of rules within fusion network in our experiments, 4 shows the detail.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Hybrid architecture example\n\n3.2.1 AUTOMATICALLY GENERATED RULES\n\nFor each image, through Fast R-CNN object detection algorithm Girshick (2015), we tag them with a number of objects anywhere of 10. For example, in a “library” image, a group of objects, “chair, building, chair, chair, chair, shelf, shelf, table, couch, shelf ” is given. These 10 tags may contain repeated objects, so we implement the counting of repeatedly additions to figure out how many objects there are in each scenario. These objects which emerge less than 10 times in a scenario will be subtracted, since each scenario has 100+ images and the probability is too low to be counted which may influence the accuracy of the representation of image rules. Finally we get 2 to 8 objects each image. For example, an image of a “library” is likely to contain “shelf, chair, table, bookcase”.\n\nCombination theory helps to form rules subsets from single objects to multiple objects. k objects are selected from a set of n objects to produce subsets without ordering. The number of such subsets is denoted by C k n = n!/(n − k)! where k here is set as 2 and 3, which means that the combinations of 2 objects and 3 objects generate and will form rules of 2 features and 3 features. Same as the previous subtraction, these combinations which appear less than 10 times are removed to ensure avoid redundant feature rules generation. Thus, the probability of single objects or multiple object combinations can be calculated for each scenario, that is, the probability of “shelf ”, “shelf & table”, “shelf & table & bookcase” emerge in a library image. Based on the Bayes’ theorem, the probability of a certain scenario when an object or a combination of objects emerge can be calculated. For example, we use the theorem to calculate that if the combination of “shelf & bookcase” is in a scenario, the probability of “library” is 95%. Table 1 shows some image rules calculated using Bayes’ theorem.\n\n3.2.2 HUMAN ENCODED RULES\n\nWithin these artificially generated ‘if-then’ logics, object keywords in the ‘if-then’ logic are picked randomly by hand and are contextually relevant. The probability of each logic are set as follows: if an object appears alone, we haphazardly give the probability of 0.75 or 0.8; if it appears by a combination of two or three objects, then we add 0.05 or 0.1 at random to the probability of one or two objects correspondingly. Table 2 shows some logic rules initialized manually.\n\n5\n\nCNN High-Level Featuresword2vec TransformationBERT Transformation+Object Detectionshelf, table, bookcaseif shelf& tablethen library(0.8)ifwindow & sculpture thenchurch (0.9)Fully Connected LayersLibraryMuseumConcertChurchMallUnder review as a conference paper at ICLR 2023\n\nTable 1: Automatically Generated Image Rules Examples\n\nProb 80 84 100 76 100 100 100 100 74 100 82\n\nClass library library library museum picture museum furniture concert concert church church mall mall\n\ncurtain window bench window vehicle person\n\nFeature 1 Feature 2 shelf chair shelf\n\ntable desk\n\nFeature 3\n\nbookcase\n\nsculpture\n\ncurtain\n\nchair\n\nsculpture\n\nhouseplant\n\nbuilding\n\nplant\n\n3.3 MULTIMODAL NETWORK FRAMEWORK\n\nFigure 2: Multimodal Network Framework\n\nWe have constructed a multimodal neural network as shown in figure 2. Firstly, a 9-layer CNN model is utilized to extract representation of image features. Note that our main objective is not to come up with a best CNN architecture for scene classification but rather to show the relative improvement in the performance when the base architecture is hybridized with a symbolic approach.\n\nNext, Faster R-CNN model is leveraged to implement object detections. 10 objects have been extracted from each image. We then generate several permuted ‘if-then’ rules based on these extracted objects through Bayes statistical method. These logical rules are represented by Word2Vec and BERT embeddings respectively. The pre-trained CBOW-based Word2Vec model is exploited to generate Word2Vec vectors for these rules. What’s more, for engendering BERT vectors, the structure of BERT classifier is as follows: the small pre-trained BERT model from Tensorflow is loaded, followed by a fully connected layer with 5 units (number of classes) with a softmax activation function. After training, we extract intermediate vector as BERT representations of these knowledge.\n\nAfter acquiring image features from CNN network, Word2Vec and BERT representations of logical knowledge rules, we perform multimodal fusion method on them. Specifically, we superimpose the dimension size of these image and rule vectors as the input dimension size of our subsequent fusion network. For each dimension of data, besides its own vector, we set zero values for these positions occupied by other two types of data. All the number of training image and probability-based training rule entries are concatenated for multimodal fusion training. However, when predicting,\n\n6\n\n.........CNNConvolution LayersObjects...Faster R-CNN......Bert VectorsWord2vec Vectors...............5 Indoor Scene CategoriesUnder review as a conference paper at ICLR 2023\n\nwe concatenate the image and its corresponding rules in the image into a vector as input, which is different from training. Obtaining joint representation of these multimodal features, a multi-layer perceptron is used to do fusion classification.\n\n3.4 MODALITIES EMBEDDING\n\n3.4.1 TRAINING\n\nFor training, multiple label inputs for each image are formed as follows, which is divided into two parts. One is the image representation and the other is the rules embedding. The overall input dimension is 300+512+256. In terms of image features, the 300-d plus 512-d zero vectors are concatenated with 256-d last layer of CNN. For rules, the composite 300-d Word2Vec word-embedding and 512-d BERT word-embedding are formed using respective conditions in the antecedent of rules as is done for image tagging. Then, the vector is amplified with a 256-d vector of zeroes, which is used to align with the overall dimension to obtain a training instance for the rule. The label corresponding to this instance is the consequent of the rule. We replicate the rules instance proportional to associated confidence probability as needed to balance two sets of training instances generated from the raw images and rules. For example, if possibility of the rule is 100%, this instance is then replicated 10 times.\n\n3.4.2\n\nINFERENCE\n\nBy including the individual word-embedding of the tagged objects, if any, of the image, the composite 300-d Word2Vec word-embedding and 512-d BERT word-embedding are then augmented with high-level features of 256-d for each labeled image are extracted from CNN. If there are no tagged objects with the image then the 300+512+256 dimensional input vector for the image is formed by padding with zeroes.\n\n4\n\nIMPLEMENTATION AND EVALUATION\n\n4.1 DATASET\n\nWe have totally collected 733 images for all indoor scene classification scenario, almost equally distributed among the 5 classes “library”, “museum”,“concert”, “church” and “mall”. Among 733 images, there are 440 used for training, 146 for validation and 146 for testing. The dataset is originally provided by MIT, which contains 67 Indoor categories, and a total of 15620 images. The number of images varies across categories, but there are at least 100 images per category. These images are of various dimensions and pixel resolution, but we have wrapped each to the fixed dimension 96x96.\n\n4.2 NETWORK SETUP\n\nIn our proof-of-concept demonstration for hybrid learning, the CNN model as in 1 is implemented in TensorFlow. We have made use of Google’s pre-trained Word2Vec model via the Python package genism. The dimension of the embedding vector is 300. The BERT model is pre-trained Tensorflow small English uncased BERT L-4 H-512 A-8 model. The text classifier is built on that which has a single neural layer using Softmax activation. The dimension of the embedding vector is 512.\n\nThe following parameters are used for training the multimodal hybrid network with rules transformation: Number of epochs = 50; Batch size = 64; Input size = 300 + 512 + 256; Learning rate = 0.001. Once the training of the hybrid model is completed, testing for each image from test dataset is done with an input obtained by concatenating extracted 256-d features from the image using the CNN with the composite 300-d Word2Vec word-embedding and 512-d BERT word-embedding of the identified objects in the image.\n\n4.3 RESULT\n\nwe have provided additional knowledge of objects in input images into the hybrid network incorporating those domain rules. The hybrid network has both Word2Vec and BERT transformation\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nword-embedding. After the entire framework is trained and validated on the selected dataset, quantitative results are obtained from the evaluation on the testing set. Additionally, VGGnet16 and MobileNet V3 is referenced as the baseline model. The experiment results are show in 2. It shows the overall classification performance of hybrid network with two word-embeddings for the testing set and of the other two baseline networks. Across the board, the whole figure indicates the overall improvement of all metrics from VGGnet16 and MobileNet V3 to hybrid network, which adequately demonstrates the effectiveness of our framework. Note that for each of tagged object, which is imported as inputs to the network, there is at least one rule with one of the conditions in the antecedent matching the object. What’s more, qualitative results for some images compared to baseline in the testing set are visualized in 3.\n\nFigure 3: Overall improvement in prediction through base CNN to rules embedding\n\nTable 2: Main evaluation results and baseline results\n\nModel VGG16 MobileNet V3 Ours\n\nAccuracy(%) Recall(%) Precision(%) F1(%) 37.17 34.27 73.02\n\n40.41 36.30 73.29\n\n43.15 37.67 73.29\n\n42.75 40.77 74.31\n\n4.4 PERFORMANCE EVALUATION\n\nFirst, we tested the hybrid model to show that its functions like the basic 3-layer CNN when for each input image’s symbolic part is reduced to zeros. This means for an input scene with no tagged identified objects, the base CNN will function the same way as the hybrid with no additional symbolic knowledge and it did in our case.\n\nSecond, we compared fusion of CNN image features plus automatically generated rules and fusion of CNN image features plus manual rules. The comparison results are shown in 3. As we can see, rules taken out from objects detection are better than the human encoded ones. This reveals that the former has more prior knowledge based on objects in each image, while the latter has more information not be restrained in the image.\n\nTable 3: CNN with different kinds of rules\n\nFramework CNN + Human Encoded CNN + Automatically Generated\n\nAccuracy(%) Recall(%) Precision(%) F1(%) 64.61 73.02\n\n63.70 73.29\n\n65.03 74.31\n\n64.38 73.29\n\nThird, we tested that only logic rules are used to predict the classification of indoor scenes. As shown in 4, our fusion framework has overall better performance than rules alone. Though, human\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nencoded rules fused with CNN have more performance gap to rules training only. This also reveals that these automatically generated rules from images can be regarded as a type of extracted feature.\n\nTable 4: Only logic rules\n\nRules Human Encoded Automatically Generated\n\nAccuracy(%) Recall(%) Precision(%) F1(%) 26.96 68.37\n\n34.93 73.29\n\n34.93 73.29\n\n34.93 73.29\n\nFourth, we have implemented three ablation experiments which are 3-layer CNN, 3-layer CNN plus Word2Vec representation and 3-layer CNN plus BERT representation. 5 shows the comparison results with our main hybrid fusion approach. It indicates that rules representation adding to image features helps to improve classification performance. BERT representation has more influence to classification results since the BERT classifier has been trained and represents word vectors better. Futhermore, the performance of our main experiment is beyond the fusion of representations of any single type of rules vector.\n\nTable 5: Ablation study with different word representations\n\nFramework CNN CNN + Word2Vec CNN + BERT CNN + Word2Vec + BERT (Ours)\n\n5 FUTURE WORK\n\nAccuracy(%) Recall(%) Precision(%) F1(%) 53.04 64.02 68.76 73.02\n\n57.34 67.36 69.18 74.31\n\n56.16 66.44 69.18 73.29\n\n56.16 66.44 69.18 73.29\n\nWe have developed a hybrid modeling framework which combines CNN with propositional rules to allow integrating subjective domain knowledge, relating objects to scenes, into the neural models. Our experiment with a scene classification scenario shows substantial improvement over classification by the underlying CNN model without rules. We are currently in the process of extending the hybrid modeling framework with rules expressed in Horn clauses which is a subset of full first-order formulae Lloyd (1984). We are also investigating how to incorporate uncertainty associated with the output of a typical object detection algorithm.\n\nOne application we are actively working on is situation and threat assessment Martin Liggins II (2008) within an area under surveillance, where intelligence comes from a variety of multimodal sources, including human and signal intelligence. Unlike a scene, which is visible, a specific situation or threat is a non-visible abstraction of various visible objects present in the environment. We believe subjective knowledge from a domain can help improving such assessment tasks.\n\nFinally, Baker et al. Baker et al. (2018) observed that deep CNNs have access to some local shape information in the form of local edge relations. Tasks such as automated scene classification and situation assessment require learning of the presence or absence of objects and their spatial relationships. Hence, the scope and usefulness of incorporating domain expert knowledge and known sensor behavior into deep CNNs is very high.\n\nREFERENCES\n\nNicholas Baker, Hongjing Lu, Gennady Erlikhman, and Philip Kellman. Deep convolutional networks do not classify based on global object shape. PLOS Computational Biology, 14, December 2018. doi: 10.1371/journal.pcbi.1006613.\n\nLe-Wen Cai, Wang-Zhou Dai, Yu-Xuan Huang, Yu-Feng Li, Stephen Muggleton, and Yuan In Zhi-Hua Zhou (ed.), ProceedJiang. Abductive learning with ground knowledge base. ings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 1815–1821. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi:\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n10.24963/ijcai.2021/250. URL https://doi.org/10.24963/ijcai.2021/250. Main Track.\n\nBao Xin Chen, Raghavender Sahdev, Dekun Wu, Xing Zhao, Manos Papagelis, and John Tsotsos. Scene classification in indoor environments for robots using context based word embeddings. In International Conference on Robotics and Automation (ICRA) Workshops, 05 2018.\n\nRiquan Chen, Tianshui Chen, Xiaolu Hui, Hefeng Wu, Guanbin Li, and Liang Lin. Knowledge\n\ngraph transfer network for few-shot recognition. In AAAI, 2020.\n\nSizhe Chen, Haipeng Wang, Feng Xu, and Ya-Qiu Jin. Target classification using the deep convolutional networks for sar images. IEEE Transactions on Geoscience and Remote Sensing, 54:1–12, 04 2016. doi: 10.1109/TGRS.2016.2551720.\n\nWilliam W. Cohen. Tensorlog: A differentiable deductive database. 05 2016.\n\nRonan Collobert, Jason Weston, L ́eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November 2011. ISSN 1532-4435.\n\nLarry Davis, Devi Parikh, and Fei-Fei Li. Future directions of visual common sense recognition.\n\n11 2015.\n\nThomas Demeester, Tim Rockt ̈aschel, and Sebastian Riedel. Lifted rule injection for relation embeddings. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1389–1399, 11 2016. doi: 10.18653/v1/D16-1146.\n\nLiya Ding, Hoon Heng Teh, Peizhuang Wang, and Ho Chung Lui. A prolog-like inference system based on neural logic—an attempt towards fuzzy neural logic programming. Fuzzy Sets Syst., 82(2):235–251, September 1996. ISSN 0165-0114. doi: 10.1016/0165-0114(95)00259-6. URL https://doi.org/10.1016/0165-0114(95)00259-6.\n\nIvan Donadello, Luciano Serafini, and Artur D’Avila Garcez. Logic tensor networks for semantic In Proceedings of the 26th International Joint Conference on Artificial\n\nimage interpretation. Intelligence, IJCAI’17, pp. 1596–1602. AAAI Press, 2017. ISBN 9780999241103.\n\nManoel Franca, Gerson Zaverucha, and Artur Garcez. Fast relational learning using bottom clause propositionalization with artificial neural networks. Machine Learning, 94:81–104, 01 2014. doi: 10.1007/s10994-013-5392-1.\n\nArtur Garcez, Lu ́ıs Lamb, and Dov Gabbay. Neural-Symbolic Cognitive Reasoning. Springer, 01\n\n2009. ISBN 978-3-540-73245-7. doi: 10.1007/978-3-540-73246-4.\n\nRoss Girshick. Fast r-cnn. In 2015 IEEE International Conference on Computer Vision (ICCV), pp.\n\n1440–1448, 2015. doi: 10.1109/ICCV.2015.169.\n\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, 38, 03 2013. doi: 10.1109/ICASSP.2013.6638947.\n\nMathieu Guillame-Bert, K. Broda, and Artur Garcez. First-order logic learning in artificial neural\n\nnetworks. pp. 1 – 8, 08 2010. doi: 10.1109/IJCNN.2010.5596491.\n\nHelmar Gust, Kai-Uwe K ̈uhnberger, and Peter Geibel. Learning Models of Predicate Logical Theories with Neural Networks Based on Topos Theory, volume 77, pp. 233–264. 08 2007. ISBN 978-3-540-73953-1. doi: 10.1007/978-3-540-73954-8 10.\n\nJoseph Y. Halpern. Reasoning about Uncertainty. MIT Press, 2005.\n\nBarbara Hammer and Pascal Hitzler. Perspectives of Neural-Symbolic Integration, volume 77. 01\n\n2007. doi: 10.1007/978-3-540-73954-8.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision – ECCV 2014, pp. 346–361, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10578-9.\n\nGeoffrey Hinton, li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Phuongtrang Nguyen, Tara Sainath, and Brian Kingsbury. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29:82–97, 11 2012. doi: 10.1109/MSP. 2012.2205597.\n\nSteffen H ̈olldobler. A structured connectionist unification algorithm. In Proceedings of the Eighth National Conference on Artificial Intelligence - Volume 1, AAAI’90, pp. 587–593. AAAI Press, 1990. ISBN 026251057X.\n\nNal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference, 1, 04 2014. doi: 10.3115/v1/P14-1062.\n\nMichael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, and Eric P. Xing. In International ConferRethinking knowledge graph propagation for zero-shot learning. ence on Learning Representations, 2019. URL https://openreview.net/forum?id= rkgs0oAqFQ.\n\nEkaterina Komendantskaya. Unification neural networks: Unification by error-correction learning.\n\nLogic Journal of the IGPL, 19:821–847, 12 2011. doi: 10.1093/jigpal/jzq012.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep conISSN 0001-0782. doi:\n\nvolutional neural networks. Commun. ACM, 60(6):84–90, May 2017. 10.1145/3065386. URL https://doi.org/10.1145/3065386.\n\nYann LeCun, Y. Bengio, and Geoffrey Hinton. Deep learning. Nature, 521:436–44, 05 2015. doi:\n\n10.1038/nature14539.\n\nAoxue Li, Tiange Luo, Zhiwu Lu, Tao Xiang, and Liwei Wang. Large-scale few-shot learning: In 2019 IEEE/CVF Conference on Computer Vision\n\nKnowledge transfer with class hierarchy. and Pattern Recognition (CVPR), pp. 7205–7213, 2019. doi: 10.1109/CVPR.2019.00738.\n\nWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander Berg. Ssd: Single shot multibox detector. volume 9905, pp. 21–37, 10 2016. ISBN 978-3-319-46447-3. doi: 10.1007/978-3-319-46448-0 2.\n\nJ. W. Lloyd. Foundations of Logic Programming. Springer-Verlag, Berlin, Heidelberg, 1984. ISBN\n\n0387132996.\n\nRobin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt.\n\nDeepproblog: Neural probabilistic logic programming, 07 2019.\n\nJiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=rJgMlhRctm.\n\nGiuseppe Marra, Michelangelo Diligenti, Francesco Giannini, Marco Gori, and Marco Maggini. Relational neural machines. In 24th European Conference on Artificial Intelligence, ECAI-2020, 2020.\n\nJames Llinas Martin Liggins II, David Hall. Handbook of Multisensor Data Fusion, Theory and\n\nPractice, Second Edition. CRC Press, 2008. ISBN 9781420053081.\n\nTomas Mikolov, Kai Chen, G.s Corrado, and Jeffrey Dean. Efficient estimation of word representa-\n\ntions in vector space. Proceedings of Workshop at ICLR, 2013, 01 2013.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nStephen Muggleton. Inductive logic programming. New Generation Computing, pp. 295–318, 02\n\n1990.\n\nJiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Ng. Multimodal\n\ndeep learning. pp. 689–696, 01 2011.\n\nAnh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 12 2014.\n\nBaolin Peng, Zhengdong Lu, Hang Li, and Kam-Fai Wong. Towards neural network-based reason-\n\ning, 08 2015. URL https://arxiv.org/abs/1508.05508.\n\nLuc De Raedt, Robin Manhaeve, Sebastijan Dumancic, Thomas Demeester, and Angelika Kimmig. In Proceedings of the 2019 International\n\nNeuro-symbolic = neural + logical + probabilistic. Workshop on Neural-Symbolic Learning and Reasoning, 2019.\n\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779–788, 06 2016. doi: 10.1109/CVPR.2016.91.\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 06 2015. doi: 10.1109/TPAMI.2016.2577031.\n\nMatthew Richardson and Pedro Domingos. Markov logic networks. Mach. Learn., 62(1–2): 107–136, February 2006. ISSN 0885-6125. doi: 10.1007/s10994-006-5833-1. URL https: //doi.org/10.1007/s10994-006-5833-1.\n\nTim Rockt ̈aschel and Sebastian Riedel. Learning knowledge base inference with neural theorem provers. In Proceedings of the 5th Workshop on Automated Knowledge Base Construction, pp. 45–50, 01 2016. doi: 10.18653/v1/W16-1309.\n\nPaul S. Rosenbloom, Abram Demski, and Volkan Ustun. Toward a neural-symbolic sigma: Introducing neural network learning. In Proceedings of the 15th Annual Meeting of the International Conference on Cognitive Modeling, 2017.\n\nJuergen Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61, 04\n\n2014. doi: 10.1016/j.neunet.2014.09.003.\n\nPierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann Lecun. Overfeat: Integrated recognition, localization and detection using convolutional networks. International Conference on Learning Representations (ICLR) (Banff), 12 2013.\n\nLokendra Shastri. Advances in shruti—a neurally motivated model of relational knowledge representation and rapid inference using temporal synchrony. Applied Intelligence, 11, 05 1999. doi: 10.1023/A:1008380614985.\n\nJude W. Shavlik and Geoffrey G. Towell. Combining explanation-based and neural learning: An\n\nalgorithm and empirical results, 1989.\n\nYelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gr ́egoire Mesnil. A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM ISBN ’14, pp. 101–110, New York, NY, USA, 2014. Association for Computing Machinery. 9781450325981. doi: 10.1145/2661829.2661935. URL https://doi.org/10.1145/ 2661829.2661935.\n\nGustav Sourek, Vojtech Aschenbrenner, Filip Zelezny, and Ondrej Kuzelka. Lifted relational neural\n\nnetworks. Journal of Artificial Intelligence Research (JAIR), 08 2015.\n\nNitish Srivastava and Ruslan Salakhutdinov. Multimodal learning with deep boltzmann machines. In Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2, NIPS’12, pp. 2222–2230, Red Hook, NY, USA, 2012. Curran Associates Inc.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nRon Sun. Integrating Rules and Connectionism for Robust Commonsense Reasoning. John Wiley\n\namp; Sons, Inc., USA, 1994. ISBN 0471593249.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. International Conference on Learning Representations, 12 2013.\n\nStefan Wermter and Ron Sun. The present and the future of hybrid neural symbolic systems: Some\n\nreflections from the nips workshop. AI Magazine, 22:123–126, 03 2001.\n\nYaqi Xie, Ziwei Xu, Mohan S Kankanhalli, Kuldeep S. Meel, and Harold Soh. Embedding Symbolic\n\nKnowledge into Deep Networks. Curran Associates Inc., Red Hook, NY, USA, 2019.\n\nJingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Van den Broeck. A semantic loss In Jennifer Dy and Andreas Krause function for deep learning with symbolic knowledge. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5502–5511. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/xu18h.html.\n\nYuan Yang and Le Song. Learn to explain efficiently via neural logic inductive learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/ forum?id=SJlh8CEYDB.\n\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pp. 1321–1331, 01 2015. doi: 10.3115/v1/P15-1128.\n\nEloi Zablocki, Patrick Bordes, Laure Soulier, Benjamin Piwowarski, and Patrick Gallinari. Contextaware zero-shot learning for object recognition. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7292–7303. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/zablocki19a.html.\n\nYuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi, and Le Song. Efficient probabilistic logic reasoning with graph neural networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rJg76kStwH.\n\nXiaoxiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang, Feng Xu, and Friedrich Fraundorfer. Deep learning in remote sensing: A comprehensive review and list of resources. IEEE Geoscience and Remote Sensing Magazine, 5:8–36, 2017.\n\n13",
    "reference": "# Summary Of The Paper\n\nA neuro-symbolic approach is proposed where a convolution neural network is extended with structured if-then symbolic rules based on word embeddings to improve image classification.\n\n# Strength And Weaknesses\n\n**Strengths**: \n* **Motivation and general idea**: The paper does well in motivating the potential benefits of combining learned classification with crispier logical rules and the introduction is completing and setting the right content and necessary background. \n* **Results**: The proposed approach achieves improved results compared to the baseline.\n\n**Weaknesses**:\n* **Convoluted and heavy-engineered embedding approach**: The pipeline that combines object detection, BERT and word2vec makes the approach unnecessarily complicated and weakens the main idea of the paper, since it becomes unclear whether the empirical improvements in performance come from the approach itself or since we use multiple pretrained models that had access to further strong object-level and textual supervision. Would be good both to have more experiments to justify the contribution of each component, and look for ways to simplify the approach. The most obvious one is the use of both BERT and Word2Vec -- is that really necessary or maybe the reliance on two parallel approaches could be eliminated? Are there ways to maybe e.g. combine representations from different layers of BERT to avoid using also Word2Vec?\n* **Usage of heuristics**: For instance, in the object-detection section, the paper uses a heuristic to avoid duplicated objects by counting them. But faster R-CNN gives many potentially overlapping predictions with different degrees of confidence. Extracting an object count using such heuristic may not work effectively. Likewise, the arbitrary probabilities given to objects, selected by a person, are unjustified, and the need to use rules made by hand might greatly limit the robustness and applicability of the approach to different domains and distributions\n* **No use of standard datasets**: The paper uses a custom dataset rather than standard ones for a task where there could be plenty of public datasets that could be explored. This is a major weakness and this choice is unjustified in the paper either. The constructed dataset is also very small (733 images (440/146/146 for training/validation/testing)), weakening the statistical strength of the empirical results.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**: The writing quality is ok but could be improved, but the paper is generally straightforward. Some sections of the paper could be reworked to become more focused. E.g. the 1-page approach overview in the modeling section, before diving into the architecture details, stands as some sort of a second introduction. It may be good to rework the presentation of this part together with the intro to reach the main ideas faster. The multiple diagrams and visualizations though are semantically clear and helpful to convey the ideas of the paper. Would be good to improve the resolution and some formatting mistakes in e.g. figure 3.\n\n**Novelty**: The high-level idea of augmenting convolution with logical rules is a simple and nice idea in my opinion, that is novel as far as I’m aware (but not certain). From technical perspective, at the more concrete level the novelty of the paper is a bit limited. \n\n**Reproducibility**: The paper provides hyperparameter selection and describes each component of the approach. The use of non-public data reduces the reproducibility though. I didn’t see a mention of releasing the constructed dataset either.\n\n**Minor Comments**:\n* **Spaces**: missing space on the second and fourth line of the introduction. \n* **Citations**: I believe the wrong format is used for some of the citations (citet vs citep). \n* **Subtitle**: on page 7 would be better to call the subsection title “Results” instead of “Result”.\n\n# Summary Of The Review\n\nThe paper explores a nice and simple idea, but the approach is potentially too complicated and heavy-engineered and the actual technical novelty is quite limited. This is more of an application paper than one that studies core new research. The experiments are also limited both in terms of dataset explored and baselines compared to. I therefore at this point unfortunately recommend rejection but encourage the authors to keep working on the paper to improve the discussed aspects.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel."
  }
]