{
  "example_idx": 7,
  "reference": "Under review as a conference paper at ICLR 2023\n\nBRINGING ROBOTICS TAXONOMIES TO CONTINUOUS DOMAINS VIA GPLVM ON HYPERBOLIC MANIFOLDS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRobotic taxonomies have appeared as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite the efforts devoted to design their hierarchy and underlying categories, their use in application fields remains scarce. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. To do so, we formulate a Gaussian process hyperbolic latent variable model and enforce the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We test our model on the whole-body support pose taxonomy to learn hyperbolic embeddings that comply with the original graph structure. We show that our model properly encodes unseen poses from existing or new taxonomy categories, it can be used to generate trajectories between the embeddings, and it outperforms its Euclidean counterparts.\n\n1\n\nINTRODUCTION\n\nRoboticists are often inspired by biological insights to create robotic systems that exhibit human- or animal-like capabilities (Siciliano & Khatib, 2016). In particular, it is first necessary to understand how humans move and interact with their environment to then generate biologically-inspired motions and behaviors of robotics hands, arms or humanoids. In this endeavor, researchers proposed to structure and categorize human hand postures and body poses into hierarchical classifications known as taxonomies. Their structure depends on the variables considered to categorize human motions and their interactions with the environment, as well as on associated qualitative measures.\n\nDifferent taxonomies have been proposed in the area of human and robot grasping (Cutkosky, 1989; Feix et al., 2016; Abbasi et al., 2016; Stival et al., 2019). Feix et al. (2016) introduced a taxonomy of hand grasps whose structure was mainly defined by the hand pose and the type of contact with the object. Later, Stival et al. (2019) claimed that the taxonomy designed in (Feix et al., 2016) heavily depended on subjective qualitative measures, and proposed a quantitative tree-like taxonomy of hand grasps based on muscular and kinematic patterns. A similar data-driven approach was used to design a grasp taxonomy based on sensed contact forces in (Abbasi et al., 2016). Robotic manipulation also gave rise to various taxonomies. Bullock et al. (2013) introduced a hand-centric manipulation taxonomy that classifies manipulation skills according to the type of contact with the objects and the object motion imparted by the hand. A different strategy was developed in (Paulius et al., 2019), where a manipulation taxonomy was designed based on a categorization of contacts and motion trajectories. Humanoid robotics also made significant efforts to analyze human motions, thus proposing taxonomies as high-level abstractions of human motion configurations. Borr`as et al. (2017) analyzed the contacts of the human limbs with the environment and designed a taxonomy of whole-body support poses.\n\nIn addition to being used for analysis purposes in robotics or biomechanics, some of the aforementioned taxonomies were leveraged for modeling grasp actions (Romero et al., 2010; Lin & Sun, 2015), for planning contact-aware whole-body pose sequences (Mandery et al., 2016), and for learning manipulation skills embeddings (Paulius et al., 2020). However, despite most taxonomies carry\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Left: Illustration of the Lorentz L2 and Poincar ́e P 2 models of the hyperbolic manifold. The former is depicted as the gray hyperboloid, while the latter is represented by the blue circle. Both models show a ) lies on the tangent space of x1 such geodesic ( (x2). Right: Subset of the whole-body support pose taxonomy (Borr`as et al., 2017) used in that u = Logx1 our experiments. Each node is a support pose defined by the type of contacts (foot F, hand H, knee K). The lines represent graph transitions between the taxonomy nodes. Contacts are depicted by grey dots.\n\n) between two points x1 ( ) and x2 ( ). The vector u (\n\na well-defined hierarchical structure, it is often overlooked. First, these taxonomies are usually exploited for classification tasks whose target classes are mainly the tree leaves, disregarding the full taxonomy structure (Feix et al., 2016; Abbasi et al., 2016). Second, the discrete representation of the taxonomy categories hinders their use for motion generation (Romero et al., 2010).\n\nWe believe that the difficulty of leveraging robotic taxonomies is due to the lack of computational models that exploit (i) the domain knowledge encoded in the hierarchy, and (ii) the information of the high-dimensional data associated to the taxonomy categories. We tackle this problem from a representation learning perspective by modeling taxonomy data as embeddings that capture the associated hierarchical structure. Inspired by recent advances on word embeddings (Nickel & Kiela, 2017; 2018; Mathieu et al., 2019), we propose to leverage the hyperbolic manifold (Ratcliffe, 2019) to learn such embeddings. An important property of the hyperbolic manifold is that distances grow exponentially when moving away from the origin, and shortest paths between distant points tend to pass through it, resembling a continuous hierarchical structure. Therefore, we hypothesize that the geometry of the hyperbolic manifold allows us to learn embeddings that comply with the original graph structure of robotic taxonomies.\n\nSpecifically, we propose a Gaussian process hyperbolic latent variable model (GPHLVM) to learn embeddings of taxonomy data on the hyperbolic manifold. To do so, we impose a hyperbolic geometry to the latent space of the well-known GPLVM (Lawrence, 2003; Titsias & Lawrence, 2010). This demands to reformulate the Gaussian distribution, the kernel, and the optimization process of the vanilla GPLVM to account for the geometry of the hyperbolic latent space. To do so, we leverage the hyperbolic wrapped Gaussian distribution (Nagano et al., 2019), and provide a positive-definiteguaranteed approximation of the hyperbolic kernel proposed by McKean (1970). Moreover, we resort to Riemannian optimization (Absil et al., 2007; Boumal, 2022) to optimize the GPHLVM parameters. We enforce the taxonomy graph structure in the learned embeddings through graphbased priors on the latent space and via graph-distance-preserving back constraints (Lawrence & Qui ̃nonero Candela, 2006; Urtasun et al., 2008). Our GPHLVM is conceptually similar to the GPLVM for Lie groups introduced in (Jensen et al., 2020), which also imposes geometric properties to the GPLVM latent space. However, our formulation is specifically designed for the hyperbolic manifold and fully built on tools from Riemannian geometry. Moreover, unlike (Tosi et al., 2014) and (Jørgensen & Hauberg, 2021), where the latent space was endowed with a pullback Riemannian metric learned via the GPLVM mapping, we impose the hyperbolic geometry to the GPHLVM latent space as an inductive bias adapted to our targeted applications.\n\nWe test our approach on graphs extracted from the whole-body support pose taxonomy (Borr`as et al., 2017). The proposed GPHLVM learns hyperbolic embeddings of the body support poses that comply with the original graph structure, and properly encodes unseen poses from existing or new taxonomy nodes. Moreover, we show how we can exploit the continuous geometry of the hyperbolic manifold to generate trajectories between different embeddings pairs, which comply with the taxonomy graph structure. To the best of our knowledge, this paper is the first to leverage the hyperbolic manifold for robotic applications.\n\n2\n\nFF2F2HFH2FHF2H2KFKKHK2FKHKH2K2HK2H2FKH2SingleDoubleTripleQuadrupleStandingKneelingUnder review as a conference paper at ICLR 2023\n\n2 BACKGROUND\n\nGaussian Process Latent Variable Models: A GPLVM defines a generative mapping from latent n=1, yn ∈ RD by modeling the correspondvariables {xn}N ing non-linear transformation with Gaussian processes (GPs) (Lawrence, 2003). The GPLVM is described as\n\nn=1, xn ∈ RQ to observations {yn}N\n\nd) with\n\nyn,d ∼ N (yn,d; fn,d, σ2\n\nfn,d ∼ GP(md(xn), kd(xn, xn))\n\nand xn ∼ N (0, I), (1) where yn,d denotes the d-th dimension of the observation yn, md(·) : RQ (cid:55)→ R and kd(·, ·) : RQ × RQ → R are the GP mean and kernel function, respectively, and σ2 d is a hyperparameter. Classically, the hyperparameters and latent variables of the GPLVM were optimized using maximum likelihood or maximum a posteriori (MAP) estimates. As this does not scale gracefully to large datasets, contemporary methods use inducing points and variational approximations of the evidence (Titsias & Lawrence, 2010). Compared to neural-network-based generative models, GPLVMs are data efficient and provide automatic uncertainty quantification.\n\nRiemannian geometry: To understand the hyperbolic manifold, it is necessary to first define some basic Riemannian geometry concepts (Lee, 2018). To begin with, consider a Riemannian manifold M, which is a locally Euclidean topological space with a globally-defined differential structure. For each point x ∈ M, there exists a tangent space TxM that is a vector space consisting of the tangent vectors of all the possible smooth curves passing through x. A Riemannian manifold is equipped with a Riemannian metric, which permits to define curve lengths in M. Shortest-path curves, called geodesics, can be seen as the generalization of straight lines on the Euclidean space to Riemannian manifolds, as they are minimum-length curves between two points in M. To operate with Riemannian manifolds, it is common practice to exploit the Euclidean tangent spaces. To do so, we resort to mappings back and forth between TxM and M, which are the exponential and logarithmic maps. The exponential map Expx(u) : TxM → M maps a point u in the tangent space of x to a point y on the manifold, so that it lies on the geodesic starting at x in the direction u, and such that the geodesic distance dM between x and y equals the distance between x and u. The inverse operation is the logarithmic map Logx(u) : M → TxM. Finally, the parallel transport (cid:0)u(cid:1) : TxM → TyM operates with manifold elements lying on different tangent spaces. Px→y\n\nHyperbolic manifold: The hyperbolic space Hd is the unique simply-connected complete ddimensional Riemannian manifold with a constant negative sectional curvature (Ratcliffe, 2019). There are several isometric models for the hyperbolic space, in particular, the Poincar ́e ball P d and the Lorentz (hyperboloid) model Ld (see Fig. 1-left). The latter representation is chosen here as it is numerically more stable than the former, and thus better suited for Riemannian optimization. However, the Poincar ́e model provides a more intuitive representation and is here used for visualization. This is easily achieved by leveraging the isometric mapping between both models (see App. A for details). An important property of the hyperbolic manifold is the exponential rate of the volume growth of a ball with respect to its radius. In other words, distances in Hd grow exponentially when moving away from the origin, and shortest paths between distant points on the manifold tend to pass through the origin, resembling a continuous hierarchical structure. Because of this, the hyperbolic manifold is often exploited to embed hierarchical data such as trees or graphs (Nickel & Kiela, 2017; Chami et al., 2020). Although its potential to embed discrete data structures into a continuous space is well known in the machine learning community, its application in robotics is presently scarce.\n\nHyperbolic wrapped Gaussian distribution: Probabilistic models on Riemannian manifolds demand to have probability distributions that consider the manifold geometry. We use the hyperbolic wrapped distribution (Nagano et al., 2019), which builds on a Gaussian distribution on the tangent space at the origin μ0 = (1, 0, . . . , 0)T of Hd, that is then projected onto the hyperbolic space after transporting the tangent space to the desired location. Intuitively, the construction of this wrapped distribution is as follows: (1) sample a point ̃v ∈ Rd from the Euclidean normal distribuHd ⊂ Rd+1 by setting v = (0, ̃v)T, (3) apply the tion N (0, Σ), (2) transform ̃v to an element of Tμ0 (cid:0)v(cid:1), and (4) project u to Hd via Expμ(u). The resulting probability parallel transport u = Pμ0→μ density function is\n\nlog NHd (x; μ, Σ) = log N (v; 0, Σ) − (d − 1) log (sinh(∥u∥L)/∥u∥L) ,\n\n(2) (cid:0)u(cid:1), u = Logμ(x), and ∥u∥L = (cid:112)⟨u, u⟩μ. The hyperbolic wrapped distribu-\n\nwhere v = Pμ→μ0 tion (Nagano et al., 2019) has a more general expression given in (Skopek et al., 2020).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3 GAUSSIAN PROCESS HYPERBOLIC LATENT VARIABLE MODEL\n\nWe present the proposed GPHLVM, which extends GPLVM to hyperbolic latent spaces. A GPHLVM defines a generative mapping from the hyperbolic latent space HQ to the observation space, e.g. the data associated to the taxonomy, based on GPs. By considering independent GPs across the observation dimensions, the GPHLVM is formally described as\n\nyn,d ∼ N (yn,d; fn,d, σ2\n\nd) with fn,d ∼ GP(md(xn), kHQ\n\nd (xn, xn)) and xn ∼ NHQ(μ0, αI), (3) where yn,d denotes the d-th dimension of the observation yn ∈ RD and xn ∈ HQ is the corresponding latent variable. Our GPHLVM is built on hyperbolic GPs, characterized by a mean function md(·) : HQ → R (usually set to 0), and a kernel kHQ d (·, ·) : HQ × HQ → R. These kernels encode similarity information in the latent hyperbolic manifold and should reflect its geometry to perform effectively, as detailed in §. 3.1. Also, the latent variable x ∈ HQ is assigned a hyperbolic wrapped Gaussian prior NHQ(μ0, αI) based on Eq. 2, where μ0 is the origin of HQ, and the parameter α controls the spread of the latent variables in HQ. As Euclidean GPLVMs, our GPHLVM can be trained by finding a MAP estimate or via variational inference. However, special care must be taken to guarantee that the latent variables belong to the hyperbolic manifold, as explained in §. 3.2.\n\n3.1 HYPERBOLIC KERNELS\n\nFor GPs in Euclidean spaces, the squared exponential (SE) and Mat ́ern kernels are standard choices (Rasmussen & Williams, 2006). In the modern machine learning literature these were generalized to non-Euclidean spaces such as manifolds (Borovitskiy et al., 2020; Jaquier et al., 2021) or graphs (Borovitskiy et al., 2021). The generalized SE kernels may be connected to the much studied heat kernels. These are given (cf. Grigoryan & Noguchi (1998)) by\n\nkH2\n\n(x, x′) =\n\nσ2 C∞\n\n(cid:90) ∞\n\nρ\n\nse−s2/(2κ2) (cosh(s) − cosh(ρ))1/2\n\nds,\n\nkH3\n\n(x, x′) =\n\nσ2 C∞\n\nρ sinh ρ\n\ne−ρ2/(2κ2),\n\n(4)\n\nwhere ρ = distHd (x, x′) denotes the geodesic distance between x, x′ ∈ Hd, κ and σ2 are the kernel lengthscale and variance, and C∞ is a normalizing constant. To the best of our knowledge, no closed form expression for H2 is known. To approximate the kernel in this case, a discretization of the integral is performed. One appealing option is the Monte Carlo approximation based on the truncated Gaussian density. Unfortunately, such approximations easily fail to be positive semidefinite if the number of samples is not very large. We address this via an alternative Monte Carlo approximation\n\nkH2\n\n(x, x′) ≈\n\nσ2 C ′\n\n∞\n\n1 L\n\nL (cid:88)\n\nl=1\n\nsl tanh(πsl)e(2sli+1)⟨xP ,bl⟩e(2sli+1)⟨x′\n\nP ,bl⟩,\n\n(5)\n\n2 log 1−|xP |2\n\ni.i.d.∼ U (T) with T the unit circle, and sl\n\nwhere ⟨xP , b⟩ = 1 |xP −b|2 is the hyperbolic outer product with xP being the representation of x as a point on the Poincar ́e disk P 2 = D, i, z denote the imaginary unit and complex conjugation, i.i.d.∼ e−s2κ2/21[0,∞)(s). The distributions of respectively, bl bl and sl are easy to sample from: The former is sampled by applying x → e2πix to x ∼ U ([0, 1]) and the latter is (proportional to) a truncated normal distribution. Importantly, the right-hand side of Eq. 5 is easily recognized to be an inner product in the space CL, which immediately implies its positive semidefiniteness (see App. B for the development of Eq. 5). Note that hyperbolic kernels for HQ with Q > 3 are generally defined as integrals of the kernels Eq. 4 (Grigoryan & Noguchi, 1998). Analogs of Mat ́ern kernels for HQ are obtained as integral of the SE kernel of the same dimension (Jaquier et al., 2021).\n\n3.2 MODEL TRAINING\n\nn=1 and hyperparameters Θ = {θd}D\n\nSimilarly to the Euclidean case, training the GPHLVM is equivalent to finding an optimal set of d=1, with xn ∈ HQ and θd the hylatent variables {xn}N perparameters of the d-th GP. For small datasets, the GPHLVM can be trained by maximizing the log posterior of the model, i.e., LMAP = log (cid:0)p(Y |X)p(X)(cid:1) with Y = (y1 . . . yN )T and X = (x1 . . . xN )T. For large datasets, the GPHLVM can be trained, similarly to the so-called Bayesian GPLVM (Titsias & Lawrence, 2010), by maximizing the marginal likelihood of the data,\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\ni.e., LMaL = log p(Y ) = log (cid:82) p(Y |X)p(X)dX. As this quantity is intractable, it is approximated via variational inference by adapting the methodology introduced in (Titsias & Lawrence, 2010) to hyperbolic latent spaces, as explained next.\n\nVariational inference: We approximate the posterior p(X|Y ) by a variational distribution q(X) defined as a hyperbolic wrapped normal distribution over the latent variables, i.e.,\n\nqφ(X) =\n\nN (cid:89)\n\nn=1\n\nNHQ(xn; μn, Σn),\n\n(6)\n\nHQ. Similarly to the with variational parameters φ = {μn, Σn}N Euclidean case (Titsias & Lawrence, 2010), this variational distribution allows the formulation of a lower bound\n\nn=1, with μn ∈ HQ and Σn ∈ Tμn\n\nlog p(Y ) ≥ Eqφ(X) [log p(Y |X)] − KL(cid:0)qφ(X)||p(X)(cid:1). (7) The KL divergence KL(cid:0)qφ(X)||p(X)(cid:1) between two hyperbolic wrapped normal distributions can easily be evaluated via Monte-Carlo sampling (see App. C.1 for details). Moreover, the expectation Eqφ(X) [log p(Y |X)] can be decomposed into individual terms for each observation dimension as (cid:80)D Eqφ(X) [log p(yd|X)], where yd is the d-th column of Y . For large datasets, each term can be evaluated via a variational sparse GP approximation (Titsias, 2009; Hensman et al., 2015). To do m=1 with zd,m ∈ HQ for each observation dimension so, we introduce M inducing inputs {zd,m}M d, whose corresponding inducing variables {ud,m}M m=1 are defined as noiseless observations of the GP in Eq. 3, i.e, ud ∼ GP(md(zd), kHQ d (zd, zd)). Similar to (Hensman et al., 2015), we can write\n\nd=1\n\n(cid:2)log N (yd; fd(X), σ2\n\nd)(cid:3) − KL(cid:0)qλ(ud)||p(ud|Zd)(cid:1),\n\nlog p(yd|X) ≥ Eqλ(fd)\n\n(8) where we defined qλ(fd) = (cid:82) p(fd|ud)qλ(ud)dud with the variational distribution qλ(ud) = N (ud; ̃μd, ̃Σd) and variational parameters λ = { ̃μd, ̃Σd}D d=1. Remember that the inducing variables ud,m are Euclidean, i.e., the variational distribution qλ(ud) is a Euclidean Gaussian and the KL divergence in Eq. 8 has a closed-form solution. In this case, the training parameters of the GPHLVM are the set of inducing inputs {zd,m}M m=1, the variational parameters φ and λ, and the hyperparameters Θ (see App. C.2 for the full derivation of the GPHLVM variational inference process).\n\nOptimization: As several training parameters of the GPHLVM belong to HQ, i.e., the latent variables xn for the MAP estimation, or the inducing inputs zd,m and means μn for variational inference. To account for the hyperbolic geometry of these parameters, we leverage Riemannian optimization methods (Absil et al., 2007; Boumal, 2022) to train the GPHLVM. Each step of first order (stochastic) Riemannian optimization methods is generally of the form\n\nηt ← h(cid:0)grad L(xt), τt−1\n\n(cid:1),\n\nxt+1 ← Expxt\n\n(−αtηt),\n\nτt ← Pxt→xt+1\n\n(cid:0)ηt\n\n(cid:1).\n\n(9)\n\nThe update ηt ∈ TxtM is first computed as a function h of the Riemannian gradient grad of the loss L(xt) and of τt−1, the previous update parallel-transported to the tangent space of the new estimate xt. The estimate xt is then updated by projecting the update ηt scaled by a learning rate αt onto the manifold using the exponential map. The function h is equivalent to computing the update of the Euclidean algorithm, e.g., ηt ← grad L(xt) for a simple gradient descent. Notice that Eq. 9 is applied on a product of manifolds when optimizing several parameters. In this paper, we used the Riemannian Adam (B ́ecigneul & Ganea, 2019) implemented in Geoopt (Kochurov et al., 2020) to optimize the GPHLVM parameters.\n\n4\n\nINCORPORATING TAXONOMY KNOWLEDGE INTO GPHLVM\n\nWhile we are now able to learn hyperbolic embeddings of the data associated to a taxonomy using our GPHLVM, they do not necessarily follow the graph structure of the taxonomy. In other words, the manifold distances between pairs of embeddings do not need to match the graph distances. To overcome this, we introduce graph-distance information as inductive bias to learn the embeddings. To do so, we leverage two well-known techniques in the GPLVM literature: priors on the embeddings and back constraints (Lawrence & Qui ̃nonero Candela, 2006; Urtasun et al., 2008). Both are reformulated to preserve the taxonomy graph structure in the hyperbolic latent space as a function of the node-to-node shortest paths.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nGraph-distance priors: As shown by Urtasun et al. (2008), the structure of the latent space can be modified by adding priors of the form p(X) ∝ e−φ(X)/σ2 φ to the GPLVM, where φ(X) is a function that we aim at minimizing. Incorporating such a prior may also be understood as augmenting the GPLVM loss L with a regularization term −φ(X). Therefore, we propose to augment the loss of the GPHLVM with a distance-preserving graph-based regularizer. Several such losses have been proposed in the literature, see (Cruceru et al., 2021) for a review. Specifically, we define φ(X) as the stress loss\n\nLstress(X) =\n\n(cid:88)\n\n(cid:0) distG(ci, cj) − distHQ (xi, xj)(cid:1)2\n\n,\n\n(10)\n\ni<j\n\nwhere ci denotes the taxonomy node to which the observation yi belongs, and distG, distHQ are the taxonomy graph distance and the geodesic distance on HQ, respectively. The loss Eq. 10 encourages the preservation of all distances of the taxonomy graph in HQ. It therefore acts globally, thus allowing the complete taxonomy structure to be reflected by the GPHLVM. Notice that Cruceru et al. (2021) also survey a distortion loss that encourages the distance of the embeddings to match the graph distance by considering their ratio. We notice, however, that this distortion loss is only properly defined when the embeddings xi and xj correspond to different classes ci ̸= cj. Interestingly, our empirical results using this loss were lackluster and numerically unstable (see App. E).\n\nBack-constraints: The back-constrained GPLVM (Lawrence & Qui ̃nonero Candela, 2006) defines the latent variables as a function of the observations, i.e., xn,q = gq(y1 . . . , yn, wq) with parameters {wq}Q q=1. It allows us to incorporate new observations in the latent space after training, while preserving local similarities between observations in the embeddings. To incorporate graph-distance information into the GPHLVM and ensure that latent variables lie on the hyperbolic manifold, we propose the back-constraints mapping\n\nxn = Expμ0\n\n( ̃xn) with\n\n ̃xn,q =\n\nN (cid:88)\n\nm=1\n\nwq,mkRD\n\n(yn, ym)kG(cn, cm).\n\n(11)\n\nThe mapping Eq. 11 not only expresses the similarities between data in the observation space via the kernel kRJ , but encodes the relationships between data belonging to nearby taxonomy nodes via kG. In other words, similar observations associated to the same (or near) taxonomy nodes will be close to each other in the resulting latent space. The kernel kG is a Mat ́ern kernel on the taxonomy graph following the formulation introduced in (Borovitskiy et al., 2021), which accounts for the graph geometry (see also App. D). We also use a Euclidean SE kernel for kRD . Notice that the back constraints only incorporate local information into the latent embedding. Therefore, to preserve the global graph structure, we pair the proposed back-constrained GPHLVM with the stress prior Eq. 10. Note that both kernels are required in Eq. 11: By defining the mapping as a function of the graph kernel only, the observations of each taxonomy node would be encoded by a single latent point. When using the observation kernel only, dissimilar observations of the same taxonomy node would be distant in the latent space, despite the additional stress prior, as kRD\n\n(yn, ym) ≈ 0.\n\n5 EXPERIMENTS\n\nWe test the proposed GPHLVM to model data of the whole-body support pose taxonomy (Borr`as et al., 2017). Each node of the taxonomy graph (see Fig. 1-right) is a support pose defined by its contacts, so that the distance between nodes can be viewed as the number of contact changes required to go from a support pose to another. We use standing and kneeling poses of the datasets in (Mandery et al., 2016) and (Langenstein, 2020). The former were extracted from recordings of a human walking without hand support, or using supports from a handrail or from a table on one side or on both sides. The latter were obtained from a human standing up from a kneeling position. Each pose is identified with a node of the graph of Fig. 1-right. We test our approach on three different datasets: an unbalanced dataset (i.e., 100 poses composed of 72 standing and 28 kneeling poses); a balanced dataset (i.e., only 60 standing poses); and an joint-space dataset (i.e., same 60 standing poses represented as joint configurations). For the first two datasets each pose is represented as a vector yn = [yLF, yRF, yLH, yRH]T ∈ R12 corresponding to the positions of the human’s feet and hands. Instead, for the last dataset, each pose is represented by vector of joint angles yn ∈ R44. Last but not least, we also test our approach on an augmented version of the whole-body support pose taxonomy, which explicitly distinguishes between left and right contacts. The main results are analyzed in the sequel, while additional experimental details and results are given in App. F and G.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Vanilla\n\n(b) Stress prior\n\n(c) BC + stress prior\n\n(d) Adding poses\n\n(e) Adding a class\n\nFigure 2: The first and last two rows respectively show the latent embeddings and examples of interpolating geodesics in P 2 and R2, followed by pairwise distance matrices. Embeddings colors match those of Fig. 1right, and background colors indicate the GPLVM uncertainty. Added poses (d) and classes (e) are marked with stars and highlighted with red in the distance matrices.\n\nR2\n\nH2\n\nStress ±σ\n\nRegularization\n\n3.71±4.08 0.14±0.20 0.21±0.34 0.22±0.34 0.58±0.65\n\n2.15±2.92 0.86±2.18 1.70±2.91 0.53 ± 0.86 0.85±1.0\n\nNo reg. Stress BC+Stress — ” —: unseen poses — ” —: unseen class No reg. Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nHyperbolic embeddings of support poses: We embed the 100 standing and kneeling poses into 2-dimensional hyperbolic and Euclidean spaces using GPHLVM and GPLVM. For each, we test the model without regularization, with stress prior, and with back-constraints coupled with stress prior (see App. F.2 for the training parameters). Figs. 2a-2c show the learned embeddings alongside distance matrices, which are to be compared with the graph distances in Fig. 3. As shown in Fig. 2a, the models without regularization do not encode any meaningful distance structure in latent space. In contrast, the models with stress prior result in embeddings that comply with the taxonomy graph structure: The embeddings are grouped and organized according to the taxonomy nodes, the geodesic distances match the graph ones, and arguably more so in the hyperbolic case (see Figs. 2b-2c). This is further reflected in the stress values of the latent embeddings with respect to the graph distances (see Table 1). Interestingly, the hyperbolic models also outperform Euclidean models with 3-dimensional latent spaces (see App. G.1). This is due to the fact that the geometry of the hyperbolic manifold leads to exponentially-increasing distances w.r.t the origin, which provides an increased volume to match the graph structure when compared to Euclidean spaces, thus resulting in better low-dimensional representations of taxonomy data. Our GPHLVM also outperformed vanilla and hyperbolic versions of variational autoencoders (VAE) to encode meaningful taxonomy information in the latent space (see App. G.4). In general, the tested VAEs only captured a global structure that separates standing from kneeling poses. Moreover, the average stress of the VAEs’ latent embeddings is higher compared to the GPHLVM’s. Finally, notice that the back constraints further organize the embeddings inside a class according to the similarity between their observations (Fig. 2c). Taxonomy expansion and unseen poses encoding: An advantage of back-constrained GPLVMs is their affordance to “embed” new observations into the latent space. We test the GPHLVM ability\n\nFigure 3: Graph distance between the poses following Fig. 1-right.\n\nTable 1: Average stress per geometry and regularization.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Motions obtained via geodesic interpolation in the back-constrained GPHLVM latent space. Left: F to F2. Right: F to FK. The colorbars identify the support pose of the closest pose in the latent space.\n\nto place unseen poses or taxonomy classes into the latent space, hypothesizing that their respective embeddings would be placed at meaningful distances w.r.t. the rest of the latent points. First, we consider a back-constrained GPHLVM with stress prior previously trained on example poses from the taxonomy (i.e., the model of Fig. 2c) and embedded unseen poses. Fig. 2d shows how these new poses land close to their respective class cluster. Second, we train a new GPHLVM while withholding all poses corresponding to the F1H1 class. We then encode these poses and find that they are located at sensible distance when compared to the model trained on the full dataset. Although this is accomplished by both models, the GPHLVM displays lower stress values (see Table 1).\n\nTrajectory generation via geodesics: The geometry of the GPHLVM latent space can also be exploited to generate trajectories in the latent space by following the geodesic, i.e. the shortest path, between two embeddings. In other words, our GPHLVM intrinsically provides a mechanism to plan motions via geodesics in the low-dimensional latent space. Examples of geodesics between two poses are shown in Figs. 2b-2c, with the colors along the trajectory matching the class corresponding to the closest hyperbolic latent point. Importantly, the geodesics in our GPHLVM latent space follow the transitions between classes defined in the taxonomy. In other words, the shortest paths in the hyperbolic embedding correspond to the shortest paths in the taxonomy graph. For instance, the geodesic from F to F2H2 follows F → F2 → F2H → F2H2, while the geodesic from FH to K2H follows FH → F2H → FKH → KH → K2H. In contrast, straight lines in the Euclidean embeddings often do not match the graph shortest path, resulting in transitions that do not exist in the taxonomy, e.g., F → F2H2, or F2 → FKH in the Euclidean latent space of Figs. 2b-2c (see also App. F.4).\n\nFig. 4 shows examples of motions resulting from geodesic interpolation in the GPHLVM latent space. As expected, the resulting trajectories do not correspond to direct interpolations between the given initial and final poses. This is due to the lack of information about the objects location and the type of contact in the considered poses. Therefore, poses with very different feet and hands positions may belong to the same class, e.g., two-feet contact with a left hand contact on the handrail or a right hand contact on the table both belong to F2H. This results in artifacts throughout the interpolations, which are alleviated by augmenting the taxonomy to differentiate between left and right contacts, as described next. However, it is interesting that the motions are still consistent with the observed transitions, e.g., the hand positions vary little along a path involving only foot and knee contacts.\n\nAugmented taxonomy for enhanced trajectory generation: Here, we aim at improving the quality of the generated motion by augmenting the whole-body support pose taxonomy with additional contact information. To do so, we consider an augmented whole-body support pose taxonomy which explicitly distinguishes between left and right contacts by adapting the nodes and transitions of Fig. 1-right. For instance, the 1-foot contact (F) node is separated into left-foot (Fl) and right-foot (Fr) contact nodes. To facilitate motion planning and to test the GPHLVM ability of dealing with high-dimensional spaces, we represent each pose as a vector yn ∈ R44 of joint angles instead of a vector of hands and feet positions. A video of the resulting motions accompanies this paper.\n\nWe embed the 60 standing poses described in App. G.2 into 3-dimensional hyperbolic and Euclidean spaces using GPHLVM and GPLVM, respectively. For each approach, we test the model without regularization, with stress prior, and with back-constraints coupled with stress prior (see App. G.3 and F.2 for detailed results and training parameters). Fig. 5 shows examples of motions planned by following geodesics in the GPHLVM latent space. We observe that the motions generated by considering the augmented taxonomy result in more realistic interpolations between the given initial and final poses than the trajectories of Fig. 4. Moreover, the previously-observed artifacts are drastically reduced. This is due to the fact that the augmented taxonomy differentiates between left and right contacts, thus allowing very different poses to be placed far apart in the latent space. For example, poses corresponding to FlHr and FrHl in the augmented taxonomy belonged to the same FH node in the original taxonomy, and were embedded close together. It is also interesting to notice that considering joint angles instead of end-effector positions results in more realistic poses. Such poses may also be obtained by considering both end-effector positions and orientations as observations, which would require an extension of the GPHLVM to handle observations on Riemannian manifolds.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Fl to F2Hr\n\n(b) Fl to F2H2\n\n(c) Fr to FrH2\n\n(d) F2Hl to FlH2\n\nFigure 5: Motions obtained via geodesic interpolation in the latent space of the back-constrained GPHLVM trained on the augmented taxonomy (Fig. 10c). Contacts are denoted by gray circles. The colorbars identify the support pose of the closest pose in the latent space.\n\n6 CONCLUSIONS\n\nInspired by the recent developments of taxonomies in different robotics fields, we proposed a computational model GPHLVM that leveraged two types of domain knowledge: the structure of a humandesigned taxonomy and a hyperbolic geometry on the latent space which complies with the intrinsic taxonomy’s hierarchical structure. Our GPHLVM allows us to learn hyperbolic embeddings of the features of the taxonomy nodes while capturing the associated hierarchical structure. To achieve this, our model exploited the curvature of the hyperbolic manifold and the graph-distance information, as inductive bias. We showed that these two forms of inductive bias are essential to: learn taxonomyaware embeddings, encode unseen data, and potentially expand the learned taxonomy. Moreover, we reported that vanilla Euclidean approaches underperformed on all the foregoing cases. Finally, we introduced a mechanism to generate taxonomy-aware motions in the hyperbolic latent space.\n\nIt is important to emphasize that our geodesic motion generation does not use explicit knowledge on how physically feasible the generated trajectories are. We plan to investigate how to include physics constraints or explicit contact data into the GPHLVM to obtain physically-feasible motions that can be executed on real robots. Moreover, we will work on alleviating the computational cost of the hyperbolic kernel in Hd. This could be tackled by using a different sampling strategy: Instead of sampling from a Gaussian distribution for the approximation Eq. 5, we could sample from the Rayleigh distribution. This is because complex numbers, whose real and imaginary components are i.i.d. Gaussian, have absolute value that is Rayleigh-distributed. As our current experimental study focused on testing our model on different graphs extracted from the whole-body support pose taxonomy (Borr`as et al., 2017), we plan to test it with datasets used to design other robotic taxonomies. Finally, we plan to investigate other types of manifold geometries that may accommodate more complex structures coming from highly-heterogeneous graphs (Giovanni et al., 2022).\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES Bahareh Abbasi, Ehsan Noohi, Sina Parastegari, and Miloˇs ˇZefran. Grasp taxonomy based on force distribution. In IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), pp. 1098–1103, 2016. doi: 10.1109/ROMAN.2016.7745245.\n\nMilton Abramowitz and Irene A Stegun. Handbook of mathematical functions with formulas, doi:\n\ngraphs, and mathematical tables, volume 55. US Government printing office, 1964. 10.5555/1098650.\n\nPierre-Antoine Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University Press, 2007. URL https://press.princeton.edu/ absil.\n\nViacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc P. Deisenroth. Mat ́ern In Neural Information Processing SysGaussian processes on Riemannian manifolds. tems (NeurIPS), pp. 12426–12437, 2020. URL https://proceedings.neurips.cc/ paper/2020/file/92bf5e6240737e0326ea59846a83e076-Paper.pdf.\n\nViacheslav Borovitskiy, Iskander Azangulov, Alexander Terenin, Peter Mostowsky, Marc Deisenroth, and Nicolas Durrande. Mat ́ern Gaussian processes on graphs. In Intl. Conf. on Artificial Intelligence and Statistic (AISTATS), pp. 2593–2601, 2021. URL https://proceedings. mlr.press/v130/borovitskiy21a.html.\n\nJ ́ulia Borr`as, Christian Mandery, and Tamim Asfour. A whole-body support pose taxonomy for multi-contact humanoid robot motions. Science Robotics, 2(13), 2017. doi: 10.1126/scirobotics. aaq0560.\n\nJoey Bose, Ariella Smofsky, Renjie Liao, Prakash Panangaden, and Will Hamilton. Latent variable modelling with hyperbolic normalizing flows. In International Conference on Machine Learning (ICML), pp. 1045–1055, 2020. URL https://proceedings.mlr.press/v119/ bose20a.html.\n\nNicolas Boumal. An introduction to optimization on smooth manifolds. To appear with Cambridge\n\nUniversity Press, 2022. URL http://www.nicolasboumal.net/book.\n\nIan M. Bullock, Raymond R. Ma, and Aaron M. Dollar. A hand-centric classification of human IEEE Transactions on Haptics, 6(2):129–144, 2013. doi:\n\nand robot dexterous manipulation. 10.1109/TOH.2012.53.\n\nGary B ́ecigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. In International Conference on Learning Representations (ICLR), 2019. URL https://openreview. net/pdf?id=r1eiqi09K7.\n\nInes Chami, Albert Gu, Vaggos Chatziafratis, and Christopher R ́e. From trees to continuous embeddings and back: Hyperbolic hierarchical clustering. In Neural Information Processing Systems (NeurIPS), pp. 15065–15076, 2020. URL https://proceedings.neurips.cc/ paper/2020/file/ac10ec1ace51b2d973cd87973a98d3ab-Paper.pdf.\n\nIsaac Chavel. Eigenvalues in Riemannian geometry. Academic press, 1984.\n\nSerge Cohen and MA Lifshits. Stationary Gaussian random fields on hyperbolic spaces and on euclidean spheres. ESAIM: Probability and Statistics, 16:165–221, 2012. URL https:// eudml.org/doc/222466.\n\nCalin Cruceru, Gary B ́ecigneul, and Octavian-Eugen Ganea. Computationally tractable Riemannian manifolds for graph embeddings. In AAAI Conf. on Artificial Intelligence, pp. 7133–7141, 2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/16877.\n\nMark R. Cutkosky. On grasp choice, grasp models, and the design of hands for manufacturing tasks. IEEE Transactions on Robotics and Automation, 5(3):269–279, 1989. doi: 10.1109/70.34763.\n\nThomas Feix, Javier Romero, Heinz-Bodo Schmiedmayer, Aaron M. Dollar, and Danica Kragic. The GRASP taxonomy of human grasp types. IEEE Transactions on Human-Machine Systems, 46(1):66–77, 2016. doi: 10.1109/THMS.2015.2470657.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nFrancesco Di Giovanni, Giulia Luise, and Michael M. Bronstein. Heterogeneous manifolds In ICLR 2022 Workshop on Geometrical and Topofor curvature-aware graph embedding. logical Representation Learning, 2022. URL https://openreview.net/forum?id= rtUxsN-kaxc.\n\nAlexander Grigoryan and Masakazu Noguchi. The heat kernel on hyperbolic space. Bulletin of the\n\nLondon Mathematical Society, 30(6):643–650, 1998. doi: 10.1112/S0024609398004780.\n\nJames Hensman, Alexander Matthews, and Zoubin Ghahramani. Scalable variational Gaussian process classification. In Intl. Conf. on Artificial Intelligence and Statistic (AISTATS), 2015. URL https://proceedings.mlr.press/v38/hensman15.html.\n\nNo ́emie Jaquier, Viacheslav Borovitskiy, Andrei Smolensky, Alexander Terenin, Tamim Asfour, and Leonel Rozo. Geometry-aware Bayesian optimization in robotics using Riemannian Mat ́ern In Conference on Robot Learning (CoRL), 2021. URL https://openreview. kernels. net/forum?id=ovRdr3FOIIm.\n\nKristopher Jensen, Ta-Chu Kao, Marco Tripodi, and Guillaume Hennequin. Manifold GPLVMs for discovering non-Euclidean latent structure in neural data. In Neural Information Processing Systems (NeurIPS), pp. 22580–22592, 2020. URL https://proceedings.neurips.cc/ paper/2020/file/fedc604da8b0f9af74b6cfc0fab2163c-Paper.pdf.\n\nMartin Jørgensen and Søren Hauberg. Isometric Gaussian process latent variable model for dissim-\n\nilarity data. In Intl. Conf. on Machine Learning (ICML), 2021.\n\nMax Kochurov, Rasul Karimov, and Serge Kozlukov. Geoopt: Riemannian optimization in PyTorch.\n\narXiv:2005.02819, 2020. URL https://github.com/geoopt/geoopt.\n\nAndr ́e Langenstein. Generating whole-body multi-contact motions between support poses using\n\ndynamical movement primitives. Master’s thesis, Karlsruhe Institute of Technology, 2020.\n\nNeil D. Lawrence. dimensional\n\nhigh 2003. 9657c1fffd38824e5ab0472e022e577e-Paper.pdf.\n\nGaussian process In Neural\n\nfor visualisation of (NeurIPS), Systems https://proceedings.neurips.cc/paper/2003/file/\n\nlatent variable models Information Processing\n\nURL\n\ndata.\n\nNeil D. Lawrence and Joaquin Qui ̃nonero Candela. Local distance preservation in the GP-LVM through back constraints. In Intl. Conf. on Machine Learning (ICML), pp. 513–520, 2006. doi: 10.1145/1143844.1143909.\n\nNikola ̆ı Nikolaevich Lebedev, Richard A Silverman, and DB Livhtenberg. Special functions and\n\ntheir applications. Physics Today, 18(12):70, 1965.\n\nJohn Lee.\n\nIntroduction to Riemannian Manifolds. Springer, 2nd edition, 2018. doi: 10.1007/\n\n978-3-319-91755-9.\n\nYun Lin and Yu Sun. Robot grasp planning based on demonstrated grasp strategies.\n\nInternational Journal of Robotics Research (IJRR), 34(1):26–42, 2015. 0278364914555544.\n\ndoi:\n\nThe 10.1177/\n\nChristian Mandery, J ́ulia Borr`as, Mirjam J ̈ochner, and Tamim Asfour. Using language models to generate whole-body multi-contact motions. In IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), pp. 5411–5418, 2016. doi: 10.1109/IROS.2016.7759796.\n\nEmile Mathieu, Charline Le Lan, Chris J. Maddison, Ryota Tomioka, and Yee Whye Teh. Continuous hierarchical representations with Poincar ́e variational auto-encoders. In Neural Information Processing Systems (NeurIPS), 2019. URL https://proceedings.neurips.cc/ paper/2019/file/0ec04cb3912c4f08874dd03716f80df1-Paper.pdf.\n\nHenry P. McKean. An upper bound to the spectrum of ∆ on a manifold of negative curvature.\n\nJournal of Differential Geometry, 4(3):359–366, 1970. doi: 10.4310/jdg/1214429509.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fujita, and Masanori Koyama. A wrapped normal distribution on hyperbolic space for gradient-based learning. In Intl. Conf. on Machine Learning (ICML), pp. 4693–4702, 2019. URL https://proceedings.mlr.press/v97/ nagano19a.html.\n\nMaximillian Nickel\n\nand Douwe Kiela.\n\nrepresentations.\n\narchical 2017. 59dfa2df42d9e3d41f5b02bfc32229dd-Abstract.html.\n\nhierIn Neural (NeurIPS), https://papers.neurips.cc/paper/2017/hash/\n\nembeddings Information Processing\n\nfor Systems\n\nPoincar ́e\n\nlearning\n\nURL\n\nMaximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the Lorentz model of hyperbolic geometry. In Intl. Conf. on Machine Learning (ICML), pp. 3779–3788, 2018. URL http://proceedings.mlr.press/v80/nickel18a.html.\n\nDavid Paulius, Yongqiang Huang, Jason Meloncon, and Yu Sun. Manipulation motion taxonomy In IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), pp.\n\nand coding for robots. 5596–5601, 2019. doi: 10.1109/IROS40897.2019.8967754.\n\nDavid Paulius, Nicholas Eales, and Yu Sun. A motion taxonomy for manipulation embedding. In Robotics: Science and Systems (R:SS), 2020. URL http://www.roboticsproceedings. org/rss16/p045.pdf.\n\nWei Peng, Tuomas Varanka, Abdelrahman Mostafa, Henglin Shi, and Guoying Zhao. Hyperbolic deep neural networks: A survey. ArXiv, abs/2101.04562, 2021. URL https://arxiv.org/ abs/2101.04562.\n\nCarl Edward Rasmussen and Christopher KI Williams. Gaussian Processes for Machine Learning.\n\nMIT Press, 2006. URL http://www.gaussianprocess.org/gpml/.\n\nJohn G. Ratcliffe. Foundations of Hyperbolic Manifolds. Springer, 3rd edition, 2019. doi: 10.1007/\n\n978-3-030-31597-9.\n\nJavier Romero, Thomas Feix, Hedvig Kjellstr ̈om, and Danica Kragic. Spatio-temporal modeling of grasping actions. In IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), pp. 2103– 2108, 2010. doi: 10.1109/IROS.2010.5650701.\n\nBruno Siciliano and Oussama Khatib. Springer Handbook of Robotics. Springer Cham, 2016. ISBN\n\n978-3-319-32550-7. doi: 10.1007/978-3-319-32552-1.\n\nOndrej Skopek, Octavian-Eugen Ganea, and Gary B ́ecigneul. Mixed-curvature variational auIn International Conference on Learning Representations (ICLR), 2020. URL\n\ntoencoders. https://openreview.net/forum?id=S1g6xeSKDS.\n\nFrancesca Stival, Stefano Michieletto, Matteo Cognolato, Enrico Pagello, Henning M ̈uller, and Manfredo Atzori. A quantitative taxonomy of human hand grasps. Journal of NeuroEngineering and Rehabilitation, 16(28), 2019. doi: 10.1186/s12984-019-0488-x.\n\nMichalis K. Titsias. Variational Learning of Inducing Variables in Sparse Gaussian Processes. In Intl. Conf. on Artificial Intelligence and Statistic (AISTATS), pp. 567–574, 2009. URL https: //proceedings.mlr.press/v5/titsias09a.html.\n\nMichalis K. Titsias and Neil D. Lawrence. Bayesian Gaussian process latent variable model. In Intl. Conf. on Artificial Intelligence and Statistic (AISTATS), pp. 844–851, 2010. URL https: //proceedings.mlr.press/v9/titsias10a.html.\n\nAlessandra Tosi, Søren Hauberg, Alfredo Vellido, and Neil D. Lawrence. Metrics for probabilistic\n\ngeometries. In Conference on Uncertainty in Artificial Intelligence (UAI), 2014.\n\nRaquel Urtasun, David J. Fleet, Andreas Geiger, Jovan Popovi ́c, Trevor J. Darrell, and Neil D. Lawrence. Topologically-constrained latent variable models. In Intl. Conf. on Machine Learning (ICML), pp. 1080–1087, 2008. doi: 10.1145/1390156.1390292.\n\nPeter Whittle. Stochastic processes in several dimensions. Bulletin of the International Statistical\n\nInstitute, 40(2):974–994, 1963.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA HYPERBOLIC MANIFOLD\n\nA.1 EQUIVALENCE OF POINCAR ́E AND LORENTZ MODELS\n\nAs pointed out in the main text (§ 2), it is possible to map points from the Lorentz model to the Poincar ́e ball via an isometric mapping. Formally, such an isometry is defined as the mapping function f : Ld → P d such that\n\nf (x) =\n\n(x1, . . . , xd)T x0 + 1\n\n,\n\n(12)\n\nwhere x ∈ Ld with components x0, x1, . . . , xd. The inverse mapping f −1 : P d → Ld is defined as follows\n\nf −1(y) =\n\n(cid:0)1 + ∥y∥2, 2y1, . . . , 2yd 1 − ∥y2∥\n\n(cid:1)T\n\n,\n\n(13)\n\nwith y ∈ P d with components y1, . . . , yd. Notice that we used the mapping Eq. 12 to represent the hyperbolic embeddings in the Poincar ́e disk throughout the paper, as well as in the computation of the kernel kH2\n\nEq. 4.\n\nA.2 MANIFOLD OPERATIONS\n\nAs mentioned in the main text (§ 2), we resort to the exponential and logarithmic maps to operate with Riemannian manifold data. The exponential map Expx(u) : TxM → M maps a point u in the tangent space of x to a point y on the manifold, while the logarithmic map Logx(u) : M → TxM performs the corresponding inverse operation. In some settings, it is necessary to work with data lying on different tangent spaces of the manifold. In this case, one needs to operate with all data (cid:0)u(cid:1) : on a single tangent space, which can be achieved by leveraging the parallel transport Px→y TxM → TyM. All the aforementioned operators are defined in Table 2 for the Lorentz model Ld. Moreover, we introduce the inner product ⟨u, v⟩x between two points on Ld, which is used to compute the geodesic distance dM(u, v) and all the foregoing operations in the Lorentz model, as shown in Table 2.\n\nOperation\n\n⟨u, v⟩x dM(u, v) Expx(u) Logx(y) (cid:0)v(cid:1) Px→y\n\ni=1 uivi\n\nFormula −u0v0 + (cid:80)d arcosh(−⟨u, v⟩x) cosh(∥u∥L)x + sinh(∥u∥L) u dM(x,y) √\nα2−1\n\n(y + αx) with α = ⟨x, y⟩x\n\n∥u∥L\n\nv + ⟨y,v⟩x\n\n1−⟨x,y⟩x\n\n(x + y)\n\nwith ∥u∥L = (cid:112)⟨u, u⟩x\n\nTable 2: Principal operations on Hd for the Lorentz model. For more details, see (Bose et al., 2020) and (Peng et al., 2021).\n\nB HYPERBOLIC KERNELS\n\nAs mentioned in the main text (§ 3.1), following the developments on kernels on manifolds like Borovitskiy et al. (2020); Jaquier et al. (2021), we may identify the generalized squared exponential kernel with the heat kernel—an important object studied on its own in the mathematical literature. Due to this, we can obtain the expressions Eq. 4. The expression for the case of H2 requires discretizing the integral, which may lead to an approximation that is not positive semidefinite. We address this by suggesting another approximation guaranteed to be positive semidefinite.\n\nReversing the derivation in (Chavel, 1984, p. 246), we obtain\n\nkH2 ∞,κ,σ2(x, x′) =\n\nσ2 C ′\n\n∞\n\n(cid:90) ∞\n\n0\n\nexp(−s2/(2κ2))P−1/2+is(cosh(ρ))s tanh(πs)ds,\n\n(14)\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nwhere ρ = distHd (x, x′) denotes the geodesic distance between x, x′ ∈ H2, κ and σ2 are the kernel lengthscale and variance, C ′ ∞ is a normalizing constant and Pα are Legendre functions Abramowitz & Stegun (1964). Now we prove that these Legendre functions are connected to the spherical functions — special functions closely tied to the geometry of the hyperbolic space and possessing a very important property. Proposition. Assume the disk model of H2 (i.e. its boundary, the circle, by T. Define the hyperbolic outer product by ⟨z, b⟩ = 1 z ∈ D, b ∈ T. Then\n\nthe Poincar ́e disk). Denote the disk by D and |z−b|2 for\n\n2 log 1−|z|2\n\nP−1/2+is(cosh(ρ)) =\n\n(cid:90)\n\nT\n\ne(2si+1)⟨z,b⟩db\n\n=\n\n(cid:90)\n\nT\n\n(cid:124) (cid:125) (cid:123)(cid:122) spherical function φ2s(z)\n\ne(2si+1)⟨z1,b⟩e(2si+1)⟨z2,b⟩db,\n\n(15)\n\nwhere z ∈ D is such that ρ = distH2 (z, 0) and z1, z2 ∈ D are such that ρ = distH2(z1, z2). Here i denotes the imaginary unit and z is the complex conjugation.\n\nProof. Let θ denote the angle between z and b, and note the following simple identities\n\n|z − b|2 = |z|2 + 1 − 2|z| cos(θ) = tanh(ρ)2 + 1 − 2 tanh(ρ) cos(θ), 1 − |z|2 = 1 − tanh(ρ)2 = cosh(ρ)−2.\n\n(16)\n\n(17)\n\nThen, we write\n\ne(2si+1)⟨z,b⟩ =\n\n(cid:19)−si−1/2\n\n(cid:18) |z − b|2 1 − |z|2\n\n= (cid:0)cosh(ρ)2(tanh(ρ)2 + 1 − 2 tanh(ρ) cos(θ))(cid:1)−si−1/2\n\n,\n\n(18) = (cid:0)sinh(ρ)2 + cosh(ρ)2 − 2 sinh(ρ) cosh(ρ) cos(θ)(cid:1)−si−1/2 ,\n(19)\n\n= (cosh(2ρ) + sinh(2ρ) cos(θ))−si−1/2 .\n\n(20)\n\nOn the other hand, by (Lebedev et al., 1965, Eq. 7.4.3), we have Pa(cosh(x)) = 1 sinh(x) cos(θ))adθ, hence\n\nπ\n\n(cid:82) π 0 (cosh(x) +\n\nP−1/2+is(cosh(2ρ)) =\n\n=\n\n=\n\n(cid:90) π\n\n1 π\n1 2π (cid:90)\n\nT\n\n(cosh(2ρ) + sinh(2ρ) cos(θ))−1/2+isdθ,\n\n0 (cid:90) π\n\n−π\n\n(cosh(2ρ) + sinh(2ρ) cos(θ))−1/2+isdθ,\n\ne(−2si+1)⟨z,b⟩db = φ−2s(z).\n\n(21)\n\n(22)\n\n(23)\n\nThis computation roughly follows Cohen & Lifshits (2012, Section 4.3.4). Now, by Cohen & Lifshits (2012, Section 3.5), we have φ−2s(z) = φ2s(z) which proves the first identity. Finally, Lemma 3.5 from Cohen & Lifshits (2012) proves the second identity.\n\nBy combining expressions Eq. 14 and Eq. 15, we get the following Monte Carlo approximation\n\nkH2 ∞,κ,σ2 (x, x′) ≈\n\nσ2 C ′\n\n∞\n\n1 L\n\nL (cid:88)\n\nl=1\n\nsl tanh(πsl)e(2sli+1)⟨xP ,bl⟩e(2sli+1)⟨x′\n\nP ,bl⟩,\n\n(24)\n\ni.i.d.∼ U (T) and sl\n\nwhere bl text (see § 3.1).\n\ni.i.d.∼ e−s2κ2/21[0,∞)(s). This gives the approximation used in the main\n\nHaving established a way to evaluate or approximate the heat kernel, analogs of Mat ́ern kernels can be defined by\n\nkν,κ,σ2(x, x′) =\n\n√\n\nwhere ̃k∞, simplicity. Here Cν is the normalizing constant ensuring that kν,κ,σ2(x, x) = σ2 for all x.\n\n2u,σ2 is the same as k∞,\n\n(cid:90) ∞\n\nuν−1e− 2ν\n\nσ2 Cν 2u,σ2 but with the normalizing constant σ2/C∞ dropped for √\n\n2u,σ2(x, x′)du,\n\nκ2 u ̃k∞,\n\n(25)\n\n√\n\n0\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nC GPHLVM VARIATIONAL INFERENCE\n\nAs mentioned in § 3.2, when training our GPHLVM on large datasets, we resort to variational inference as originally proposed in (Titsias & Lawrence, 2010). Here we provide the mathematical details about the changes that are needed to train our model via variational inference.\n\nC.1 COMPUTING THE KL DIVERGENCE BETWEEN TWO HYPERBOLIC WRAPPED NORMAL\n\nDISTRIBUTIONS\n\nAs mentioned in § 3.2, we approximate the KL divergence between two hyperbolic wrapped distributions via Monte-Carlo sampling. Namely, given two hyperbolic wrapped distributions qφ(x) and p(x), we write\n\nKL(cid:0)qφ(x)||p(x)(cid:1) =\n\n(cid:90)\n\nqφ(x) log\n\nqφ(x) p(x)\n\ndx ≈\n\n1 K\n\nK (cid:88)\n\nk=1\n\nlog\n\nqφ(xk) p(xk)\n\n,\n\n(26)\n\nwhere we used K independent Monte-Carlo samples drawn from qφ(x) to approximate the KL divergence. These samples are obtained via the procedure described in § 2, i.e., by sampling an element on the tangent space of the origin μ0 = (1, 0, . . . , 0)T of Hd, via a Euclidean normal distribution, and then applying the parallel transport operation and the exponential map to project it onto Hd.\n\nC.2 DETAILS OF THE VARIATIONAL PROCESS\n\nAs mentioned in the main text, the marginal likelihood p(Y ) is approximated via variational inference by approximating the posterior p(X|Y ) with the hyperbolic variational distribution qφ(X) as defined by Eq. 6. The lower bound Eq. 7 is then obtained, similarly as in (Titsias & Lawrence, 2010), as\n\nlog p(Y ) = log\n\n= log\n\n(cid:90)\n\n(cid:90)\n\np(Y |X)p(X)dX\n\np(Y |X)p(X)\n\ndX = log Eqφ(X)\n\n≥ Eqφ(X)\n\n(cid:20)\n\nlog\n\n(cid:90)\n\nqφ(X) log\n\nqφ(X) qφ(X) (cid:21) p(Y |X)p(X) qφ(X)\n\n=\n\n(cid:90)\n\n(cid:90)\n\n=\n\nqφ(X) log p(Y |X)dX −\n\nqφ(X) log\n\nqφ(X) p(X)\n\ndX\n\n= Eqφ(X) [log p(Y |X)] − KL(cid:0)qφ(X)||p(X)(cid:1),\n\n(cid:20) p(Y |X)p(X) qφ(X)\n\n(cid:21)\n\np(Y |X)p(X) qφ(X)\n\ndX\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\ninequality in Eq. 29.\n\nthe expectation following Jensen’s Eqφ(X) [log p(Y |X)] can be decomposed into individual terms for each observation dimension as (cid:80)D Eqφ(X) [log p(yd|X)], where yd is the d-th column of Y . We then define the inducing inputs Zd and inducing variables ud the same way as the noiseless observations fd, so that the joint distribution of fd and ud can be written as\n\nAs mentioned in § 3.2,\n\nd=1\n\np(fd, ud) =\n\n(cid:19)\n\n(cid:18)fd ud\n\n= N\n\n(cid:19)\n\n(cid:18)(cid:18) md(X) md(Zd)\n\n,\n\n15\n\n(cid:18) kd(X, X)\n\nkd(X, Zd) kd(Zd, X) kd(Zd, Zd)\n\n(cid:19)(cid:19)\n\n.\n\n(32)\n\nUnder review as a conference paper at ICLR 2023\n\nThe lower bound Eq. 8 is then obtained for each dimension, similarly as in (Hensman et al., 2015), as\n\nlog p(yd|X) =\n\n(cid:90)\n\nlog p(yd|X, ud)p(ud)dud\n\n(cid:90)\n\n= log\n\np(yd|X, ud)p(ud)\n\nqλ(ud) qλ(ud) (cid:21) p(yd|X, ud)p(ud) qλ(ud)\n\n≥ Eqλ(ud)\n\n(cid:20)\n\nlog\n\n(cid:90)\n\n=\n\nqλ(ud) log p(yd|X, ud)dud −\n\n=\n\n(cid:90)\n\ndud = log Eqλ(ud)\n\n(cid:20) p(yd|X, ud)p(ud) qλ(ud)\n\n(cid:21)\n\n(33)\n\n(34)\n\n(cid:90)\n\nqλ(ud) log\n\np(yd|X, ud)p(ud) qλ(ud)\n\ndud (35)\n\nqλ(ud) log\n\nqλ(ud) p(ud)\n\ndud\n\n(cid:2)Ep(fd|ud) [log p(yd|fd(X))](cid:3) − KL(cid:0)qλ(ud)||p(ud)(cid:1)\n\n= Eqλ(ud) [log p(yd|X, ud)] − KL(cid:0)qλ(ud)||p(ud)(cid:1) ≥ Eqλ(ud) = Eqλ(fd) [log p(yd|fd(X))] − KL(cid:0)qλ(ud)||p(ud|Zd)(cid:1) = Eqλ(fd)\n\n(cid:2)log N (yd; fd(X), σ2\n\nd)(cid:3) − KL(cid:0)qλ(ud)||p(ud|Zd)(cid:1),\n\n(36)\n\n(37)\n\n(38)\n\n(39)\n\n(40)\n\nwhere we defined qλ(fd) = (cid:82) p(fd|ud)qλ(ud)dud with the Euclidean variational distribution qλ(ud) = N (ud; ̃μd, ̃Σd), and wrote p(ud|Zd) = p(ud) for simplicity. The inequality Eq. 35 corresponds to Jensen’s inequality, while Eq. 38 is shown in (Titsias, 2009).\n\nFinally, substituting Eq. 40 in Eq. 31 results in the following bound on the marginal likelihood\n\nlog p(Y ) ≥\n\nN (cid:88)\n\nD (cid:88)\n\nn=1\n\nd=1\n\nEqφ(xn)\n\n(cid:2)Eqλ(fn,d)\n\n(cid:2)log N (yn,d; fn,d(xn), σ2\n\nd)(cid:3)(cid:3)\n\n−\n\nD (cid:88)\n\nd=1\n\nKL(cid:0)qλ(ud)||p(ud|Zd)(cid:1) −\n\nN (cid:88)\n\nn=1\n\nKL(cid:0)qφ(xn)||p(xn)(cid:1).\n\n(41)\n\nD MAT ́ERN KERNELS ON TAXONOMY GRAPHS\n\nAs explained in § 4 of the main paper, we leverage the Mat ́ern kernel on graphs proposed by Borovitskiy et al. (2021) to design a kernel for our back-constrained GPHLVM that accounts for the geometry of the taxonomy graph. Here we provide the main equations of such a kernel, and refer the reader to (Borovitskiy et al., 2021) for further details. Formally, let us define a graph G = (V, E) with vertices V and edges E and the graph Laplacian as ∆ = D − W , where W is the graph adjacency matrix and D its corresponding diagonal degree matrix, with Dii = (cid:80) j Wij. The eigendecomposition U ΛU T of the Laplacian ∆ is then used to formulate both the SE and Mat ́ern kernels on graphs, as follows,\n\nkG ∞,κ(cn, cm) = U\n\n(cid:16)\n\n2 Λ(cid:17)\n\ne− κ2\n\nU T,\n\nand kG\n\nν,κ(cn, cm) = U\n\n(cid:19)−ν\n\n(cid:18) 2ν\n\nκ2 + Λ\n\nU T,\n\n(42)\n\nwhere κ is the lengthscale (i.e., it controls how distances are measured) and ν is the smoothness parameter determining mean-squared differentiability of the associated Gaussian process (GP). Note that the graph kernel expressions in Eq. 42 are obtained by considering the connection between Mat ́ern kernel GPs and stochastic partial differential equations, originally proposed by Whittle (1963) and later extended to Riemannian manifolds in (Borovitskiy et al., 2020). This connection establishes that SE and Mat ́ern GPs satisfy\n\ne− κ2\n\n4 ∆f = W,\n\nand\n\n(cid:19) ν\n\n2\n\n(cid:18) 2ν\n\nκ2 + ∆\n\nf = W,\n\n(43)\n\nwhere W ∼ N (0, I) and f : V → R, which lead to definition of graph GPs (Borovitskiy et al., 2021).\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Reg. with Ldistortion\n\n(b) Distances for 6a.\n\n(c) Reg. with (cid:101)Ldistortion\n\n(d) Distances for 6c.\n\nFigure 6: Embeddings learned with distortion regularization. (a) and (c) display the latent embeddings after training our GPHLVM model with an added distortion loss Ldistortion as it was originally defined, and with our modified distortion loss (cid:101)Ldistortion, respectively. These embeddings indeed show that our regularizations failed to encode the distances in the graph (comparing the distances provided in (b) and (d) with Fig. 3).\n\nE DISTORTION LOSS\n\nAs explained in the paper, we focus on two ways of embedding the graph in the hyperbolic space: a global approach using a stress regularization which matches graph distances with geodesic distances, and a combination between this stress regularization and the use of back constraints (see § 4). However, the literature on graph embeddings also surveys a distortion loss (Cruceru et al., 2021) given by\n\nLdistortion(X) =\n\n(cid:88)\n\ni<j\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ndistHQ(xi, xj)2 distG(ci, cj)2 − 1\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n,\n\n(44)\n\nwhich tries to match the graph and manifold distances by minimizing their ratio’s distance to 1.\n\nWe found that our problem is more subtle than usual graph embeddings, given that several points in our dataset may correspond to the same graph node (e.g., two different poses in which the left foot is the only limb in contact). Indeed, notice that Eq. 45 is ill-defined for the case i = j (or equivalently distG(ci, cj)2 = 0). This is because all nodes xi are assumed to be different from each other. However, in our setup, several xi may correspond to the exact same class in the taxonomy.\n\nOur first attempt to remediate this was to add a simple regularizer ε = 10−1 to the denominator. However, this caused the loss to give more weight to the points where distG(ci, cj)2 = 0 (see Fig. 6a6b for the outcome of training a GPHLVM with this type of regularization). We then considered an alternate definition of distortion in which the term inside the sum is given by\n\n(cid:101)Ldistortion(xi, xj) =\n\n(cid:26) λ1 distHQ(xi, xj)\n\nif xi and xj’s classes are identical\n\nλ2Ldistortion(xi, xj) otherwise\n\n(45)\n\nwhere λ1, λ2 ∈ R+ are hyperparameters. λ1 governs how much we encourage latent codes of the same class to collapse into a single point, while λ2 weights how much the geodesic distance should match the graph distance. After manual hyperparameter tuning, we obtained the latent space and distance matrix portrayed in Figs. 6c-6d. As can be seen in both accounts, the distortion loss produced lackluster results and failed to properly match the latent space distances with that of the graph. For these experiments, we used a loss scale of 50, λ1 = 0.01 and λ2 = 10, meaning that we strongly encouraged the distances between non-identical classes to match in ratio.\n\nF ADDITIONAL DETAILS ON THE EXPERIMENTS OF § 5\n\nF.1 DATA\n\nTable 3 describes the data of the whole-body support pose taxonomy used in the experiments reported in § 5. Each pose is identified with a support pose category, i.e., a node of the graph in Fig. 1-right, and with a set of associated contacts. As shown in the table, some support poses include several sets of contacts. For example, the support pose F groups all types of support poses\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nwhere only one foot is in contact with the environment. Notice that some sets of contacts are not represented in the data and thus do not appear in Table 3.\n\nSupport pose\n\nContacts\n\nNumber\n\nF\n\nFH\n\nF2\n\nFH2\n\nF2H\n\nF2H2\n\nK\n\nFK\n\nKH\n\nK2\n\nFKH\n\nKH2\n\nK2H\n\nFKH2 K2H2\n\nLeft foot Right foot Left foot, left hand Right foot, right hand Left foot, right hand Right foot, left hand Left foot, right foot Left foot, left hand, right hand Right foot, left hand, right hand Left foot, right foot, left hand Left foot, right foot, right hand Left foot, right foot, left hand, right hand\n\nLeft knee Right knee Left foot, right knee Right foot, left knee Left knee, left hand Right knee, right hand Left knee, right knee Right foot, left knee, left hand Left foot, right knee, right hand Left knee, left hand, right hand Left knee, right knee, left hand Left knee, right knee, right hand Right foot, left knee, left hand, right hand Left knee, right knee, left hand, right hand\n\n7 6\n5 6\n5 6\n6 6\n6 5\n7 7\n\n1 1\n2 3\n4 1\n1 5\n2 1\n2 1\n2 2\n\nTable 3: Poses description extracted from the whole-body support pose taxonomy (Borr`as et al., 2017) used in § 5 and App. G.\n\nF.2 TRAINING PARAMETERS AND PRIORS\n\nTable 4 describes the hyperparameters used for the experiments reported in § 5 and App. G. We used the hyperbolic kernels defined in § 3.1 for the GPHLVMs, and the classical SE kernel for the Euclidean models. For the back-constraints mapping Eq. 11, we defined kRD (yn, ym) as the product of a Euclidean SE kernel with lengthscale κRD , and kG(cn, cm) as a graph Mat ́ern kernel with smoothness ν = 2.5 and lengthscale κG. We additionally scaled the product of kernels with a variance σRD,G. For training the back-constrained GPHLVM and GPLVM, we used a Gamma prior Gamma(α, β) with shape α and rate β on the lengthscale κ of the kernels. The embeddings of the Euclidean models were initialized with PCA. For the GPHLVMs, the initial embeddings HQ at the origin μ0 ̃v obtained via PCA were transformed to elements of the tangent space Tμ0 by setting v = (0, ̃v)T and then projected to the hyperbolic manifold using the exponential map. All models were trained by maximizing the loss L = LMAP − γLstress, where LMAP denotes the log posterior of the model, Lstress is the stress-based regularization loss defined in Eq. 10, and γ is a parameter balancing the two losses. The optimization was conducted using the Riemannian Adam optimizer (B ́ecigneul & Ganea, 2019) implemented in Geoopt (Kochurov et al., 2020) with a learning rate of 0.05.\n\nFor the first part of the experiments on taxonomy expansion, we encoded unseen poses of each class for the back-constrained GPLVM and GPHLVM with a stress regularization using the models presented in Table 4. For the second part of the experiments, we left the class FH out during training and we “embedded” it using the back-constraints mapping. The newly-trained models also followed the same hyperparameters presented in Table 4.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nExperiment\n\nModel\n\nRegularization Loss scale γ\n\nPrior on κH/RQ\n\nκRD\n\nHyperbolic embeddings of support poses (§ 5)\n\nHyperbolic embeddings in H3 (App. G.1)\n\nHyperbolic embeddings of standing poses (App. G.2)\n\nGPLVM on R2\n\nGPHLVM on H2\n\nGPLVM on R3\n\nGPHLVM on H3\n\nGPLVM on R2\n\nGPHLVM on H2\n\nHyperbolic embeddings of standing poses with an augmented taxonomy (App. G.3)\n\nGPLVM on R3\n\nGPHLVM on H3\n\nNo regularizer Stress BC + Stress No regularizer Stress BC + Stress\n\nNo regularizer Stress BC + Stress No regularizer Stress BC + Stress\n\nNo regularizer Stress BC + Stress No regularizer Stress BC + Stress\n\nNo regularizer Stress BC + Stress No regularizer Stress BC + Stress\n\n0 6\n1.3 0\n6 1.3\n\n0 10 1.5 0\n10 1.5\n\n0 5\n0.7 0\n5 0.7\n\n0 5\n1.5 0\n5 1.5\n\nNone None Gamma(2, 2) None None Gamma(2, 2)\n\nNone None Gamma(2, 2) None None Gamma(2, 2)\n\nNone None Gamma(2, 2) None None Gamma(2, 2)\n\nNone None Gamma(2, 2) None None Gamma(2, 2)\n\n- -\n0.9 -\n- 0.9\n\n- -\n0.9 -\n- 0.9\n\n- -\n0.9 -\n- 0.9\n\n- -\n2.0 -\n- 2.0\n\nκG\n\n- -\n0.6 -\n- 0.6\n\n- -\n0.6 -\n- 0.6\n\n- -\n0.6 -\n- 0.6\n\n- -\n0.8 -\n- 0.8\n\nσRD,G\n\n- -\n2 -\n- 2\n\n- -\n2\n\n- 2\n\n- -\n2 -\n- 2\n\n- -\n2 -\n- 2\n\nTable 4: Summary of experiments and list of hyperparameters.\n\nF.3 MARGINAL LOG-LIKELIHOODS OF TRAINED MODELS\n\nTable 5 shows the marginal loglikelihood (MLL) of the GPHLVM and GPLVM described in § 5. We observe that the hyperbolic models achieve a higher likelihood that their Euclidean counterparts.\n\nRegularization MLL\n\nRegularization MLL\n\nR2\n\nNo reg. Stress BC+Stress\n\n-27.57 -55.33 5.43\n\nH2\n\nNo reg. Stress BC+Stress\n\n-24.37 -49.20 7.67\n\nTable 5: Marginal log-likelihood per geometry and regularization.\n\nF.4 FURTHER DETAILS ON TRAJECTORY GENERATION VIA GEODESICS\n\nTable 6 describes the transitions between support poses obtained by following the geodesic trajectories of the back-constrained GPHLVM and GPLVM with stress prior depicted in Fig. 2c. In contrast to GPHLVM, the Euclidean GPLVM often results in transitions that do not exist in the taxonomy. Interestingly, it also often uses more transitions than those originally needed. Notice that similar results are observed for the GPHLVM and GPLVM with stress prior depicted in Fig. 2b.\n\nStart F\nF F\nF2H F\nF2 FH\n\nEnd F2H F2H2 FH2 FH2 FK K2 K2H FH → F2H→FH2→FKH → KH → K2H FH → F2H → F2→FKH → FK → FKH → FKH2→K2H\n\nTransitions in R2 F→FH2 → FH → F2H F→FH2 → F2H2 F→FH2 F2H → FH → FH2 F→FH2 → FH → F2H → F2→FKH → FK F2→FKH → FK → FKH → FKH2 → KH2→K2\n\nTransitions in H2 F → FH → F2H F → FH → F2H → F2H2 F → FH → FH2 F2H → FH → FH2 F → F2 → FK F2 → FK → K2\n\nTable 6: Transitions (→) between classes of the taxonomy obtained by following the geodesic trajectories depicted in Fig. 2c. The classes and transitions correspond to the colors along the trajectories and match the class corresponding to the closest embedding at each point along the geodesic. Transitions that do not exist in the taxonomy are denoted as →.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Vanilla\n\n(b) Stress prior\n\n(c) BC + stress prior\n\n(d) Adding poses\n\n(e) Adding a class\n\nFigure 7: The first and last two rows respectively show the latent embeddings and examples of interpolating geodesics in P 3 and R3, followed by pairwise distance matrices. Embeddings colors match those of Fig. 1right. Added poses (d) and classes (e) are marked with crosses and highlighted with red in the distance matrices.\n\nG ADDITIONAL EXPERIMENTS\n\nG.1 HYPERBOLIC EMBEDDINGS OF SUPPORT POSES IN H3\n\nIn this section, we embed the 100 poses used in § 5 into 3-dimensional hyperbolic and Euclidean spaces to analyze the performance of the proposed models in higher-dimensional latent spaces. Namely, we test the GPHLVM and GPLVM without regularization, with stress prior, and with back-constraints coupled with stress prior, similarly to the experiments on 2-dimensional latent spaces reported in the paper. Figs. 7a-7c show the learned embeddings alongside the corresponding distance matrices, which are to be compared with the graph distances in Fig. 3. As expected, and similarly to the 2-dimensional embeddings of Fig. 2a, the models without regularization do not encode any meaningful distance structure in the latent spaces (see Fig. 7a). In contrast, the models with stress prior result in embeddings that comply with the taxonomy graph structure, and the back constraints further organize the embeddings inside a class according to the similarity between their observations (see Figs. 7b-7c).\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\n4.04±4.38 0.18±0.34 1.59±1.99 0.16±0.25 0.99±0.74\n\n3.83±4.17 0.15±0.22 0.19±0.28 0.18±0.26 0.68±0.74\n\nRegularization\n\nStress ±σ\n\nH3\n\nR3\n\nWe observed a prominent stress reduction for the Euclidean 3dimensional latent spaces compared to the 2-dimensional ones (see Table 7), as well as a reduction of non-existing transitions when following geodesic trajectories (see Table 8). This is due to the increase of volume available to match the graph structure in R3 relatively to R2. However, all Euclidean models are still\n\nTable 7: Average stress per geometry and regularization.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\noutperformed by the 2-dimensional hyperbolic embeddings presented in § 5 (see Table 1). This is due to the fact that the volume of balls in hyperbolic space increases exponentially with respect to the radius of the ball rather than polynomially as in Euclidean space. In other words, the geometry of the hyperbolic manifold increases the volume available to match the graph structure compared to Euclidean spaces, thus resulting in better low-dimensional representations of taxonomy data. Notice that the GPHLVM models with 3-dimensional hyperbolic latent space result in a similar or slightly reduced stress compared to their 2-dimensional counterparts (presented in § 5). This indicates that the volume of the 2-dimensional hyperbolic latent space is sufficient to represent the considered data. Moreover, similarly as for the 2-dimensional cases, the back-constrained GPHLVM and GPLVM allow us to properly place unseen poses or taxonomy classes into the latent space (see Figs. 7d-7e).\n\nStart F\nF F\nF2H F\nF2 FH\n\nEnd F2H F2H2 FH2 FH2 FK K2 K2H FH → F2H → FKH → KH → K2H FH → F2H → FKH → FKH2→K2H\n\nTransitions in H3 F → FH → F2H F → FH → F2H → F2H2 F → FH → FH2 F2H → FH → FH2 F → F2 → FK F2 → FK → K2\n\nTransitions in R3 F → FH → F2H F → FH→F2H2 F → FH → FH2 F2H → FH → FH2 F → F2 → FK F2 → FK → K2\n\nTable 8: Transitions (→) between classes of the taxonomy obtained by following the geodesic trajectories depicted in Fig. 7c. The classes and transitions correspond to the colors along the trajectories and match the class corresponding to the closest embedding at each point along the geodesic. Transitions that do not exist in the taxonomy are denoted as →.\n\nG.2 HYPERBOLIC EMBEDDINGS OF STANDING POSES\n\nIn this section, we consider a different subset of the whole-body support pose taxonomy, leading to a different graph. Namely, we use 60 standing poses of the dataset in (Mandery et al., 2016) and (Langenstein, 2020), which correspond to graph nodes of standing support poses (left side of the graph in Fig. 1). Specifically, we use a balanced dataset composed of 5 poses for each of the contact sets of the standing support poses described in Table 3. We embed the 60 poses into 2dimensional hyperbolic and Euclidean spaces using GPHLVM and GPLVM. For each approach, we test the model without regularization, with stress prior, and with back-constraints coupled with stress prior using the parameters described in App. F.2 and Table 4.\n\nFigs. 8a-8c show the learned embeddings alongside their corresponding distance matrices, which are to be compared with the graph distances in Fig. 9a. As for the previous experiments, the models with stress prior result in embeddings that comply with the taxonomy graph structure, with additional intra-class organizations for the back-constrained models. It is worth noticing that, despite the fact that the considered taxonomy graph is smaller than for the previous experiments, all Euclidean GPLVMs remain outperformed by the hyperbolic models, which better match the taxonomy structure (see also Table 9b). Similarly to the experiments reported in § 5, the back-constrained GPHLVM and GPLVM allow us to properly place unseen poses or taxonomy classes into the latent space (see Figs. 8d-8e). As mentioned in the main text, our GPHLVM intrinsically provides a mechanism to plan motions via geodesics in the low-dimensional latent space. Examples of geodesics between two standing poses are shown in Figs. 8b-8c, where the trajectory color matches the class corresponding to the closest latent point. The transitions between standing support poses obtained by following these geodesic trajectories are also described in Table 9. As for our previous experiments, the geodesics, i.e., shortest paths, in the GPHLVM latent space correspond to shortest paths in the taxonomy graph. Due to the size of the taxonomy graph, we observe fewer forbidden (i.e. nonexistent) transitions than for the previous experiments in the Euclidean models. However, as their latent space does not match the taxonomy structure, they often require additional transitions and thus do not follow shortest paths in the taxonomy graph.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Vanilla\n\n(b) Stress prior\n\n(c) BC + stress prior\n\n(d) Adding poses\n\n(e) Adding a class\n\nFigure 8: Embeddings of standing poses: The first and last two rows respectively show the latent embeddings of bipedal poses and examples of interpolating geodesics in P 2 and R2, followed by pairwise distance matrices. Embeddings colors match those of Fig. 1-right, and background colors indicate the GPLVM uncertainty. Added poses (d) and classes (e) are marked with stars and highlighted with red in the distance matrices.\n\nR2\n\nH2\n\nRegularization\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nStress ±σ\n\n1.69±1.96 0.62±1.41 0.68±0.96 0.61±0.84 0.47±0.38\n\n1.66±1.95 0.07±0.09 0.15±0.18 0.17±0.20 0.22±0.31\n\n(a) Graph distance between the standing poses.\n\n(b) Average stress per geometry and regularization.\n\nFigure 9: Embeddings of standing poses: (a) shows the graph distance following the left part of Fig. 1-right. (b) shows the stress resulting from the different embeddings of standing poses.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nStart F\nF F\nF2H\n\nEnd F2H F2H2 FH2 FH2\n\nTransitions in H2 F → FH → F2H F → FH → F2H → F2H2 F → FH → FH2 F2H → FH → FH2\n\nTransitions in R2 F → F2 → F → F2 → FH → F2H F → F2 → F→FH2 → F2H2 F → F2 → F→FH2 F2H → FH → F2 → F → F2 → F→FH2 → F2H2 → FH2\n\nTable 9: Embeddings of standing poses: Transitions (→) between classes of the taxonomy obtained by following the geodesic trajectories depicted in Fig. 8c. The classes and transitions correspond to the colors along the trajectories and match the class corresponding to the closest embedding at each point along the geodesic. Transitions that do not exist in the taxonomy are denoted as →.\n\nG.3 HYPERBOLIC EMBEDDINGS OF STANDING POSES WITH AN AUGMENTED TAXONOMY\n\nFOR IMPROVED TRAJECTORY GENERATION\n\nAs shown in § 5, geodesics in the hyperbolic latent space of our GPHLVM intrinsically provide a mechanism to plan motions accounting for the underlying taxonomy. However, as discussed in § 5, the whole-body support pose taxonomy (Borr`as et al., 2017) lacks information about the type of contact in the considered poses, thus leading to artifacts in the geodesic-generated motions. In the main paper, we showed that the quality of the generated motion is improved by augmenting the whole-body support pose taxonomy with additional contact information. To do so, we considered an augmented whole-body support pose taxonomy which explicitly distinguishes between left and right contacts. In other words, the nodes and transitions of Fig. 1-right are adapted to consider left and right contacts. For instance, the 1-foot contact (F) node is separated into left-foot (Fl) and right-foot (Fr) contact nodes. To facilitate motion planning and to test the GPHLVM ability of dealing with high-dimensional spaces, we represent each pose as a vector yn ∈ R44 of joint angles instead of a vector of hands and feet positions.\n\nWe embed the 60 standing poses described in App. G.2 into 3-dimensional hyperbolic and Euclidean spaces using GPHLVM and GPLVM, respectively. For each approach, we test the model without regularization, with stress prior, and with back-constraints coupled with stress prior using the parameters described in App. F.2 and Table 4. Figs. 10a-10c show the learned embeddings alongside their corresponding distance matrices, which are to be compared with the graph distances of the augmented taxonomy in Fig. 11a. Similarly to previous experiments, the models with stress prior result in embeddings complying with the taxonomy graph structure (Fig. 10b), with additional intra-class organizations for the back-constrained models (Fig. 10c). Notice that the embeddings differentiate between left and right contacts according to the augmented taxonomy: For instance, we observe four clusters of orange embeddings corresponding to FlHl, FlHr, FrHl, and FrHr. As shown in Table 11b, the hyperbolic models better represent the taxonomy structure and outperform the Euclidean models. Similarly to previous experiments, the back-constraint mapping introduced in § 4 allows us to properly place unseen poses or taxonomy classes into the latent space (see Figs. 10d-10e).\n\nExamples of motions planned by following geodesics between two standing poses in the hyperbolic latent space are displayed in the main paper (Fig. 5). The corresponding geodesics are shown in Fig. 10c, with the colors along the trajectory matching the class corresponding to the closest hyperbolic latent point. The resulting transitions are given in Table 10. As mentioned in the main paper, we observe that, in contrast to the trajectories of Fig. 4, the motions generated by considering the augmented taxonomy (Fig. 5) result in more realistic – human-like – interpolations between the given initial and final poses. Moreover, these motions look more realistic than the motions obtained via linear interpolation in the Euclidean latent space of the vanilla back-constrained GPLVM. As shown in Fig. 12, the motions planned in the Euclidean latent space sometimes result in unrealistic joint configurations and the same posture is associated with different types of contacts (see middle part of the motions). As shown in Table 10, non-existing transitions arise more frequently when following trajectories generated by the Euclidean model.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Vanilla\n\n(b) Stress prior\n\n(c) BC + stress prior\n\n(d) Adding poses\n\n(e) Adding a class\n\nFigure 10: Embeddings of standing poses considering the augmented whole-body support pose taxonomy: The first and last two rows respectively show the latent embeddings and examples of interpolating geodesics in P 3 and R3, followed by pairwise distance matrices. Embeddings colors match those of Fig. 1-right. Added poses (d) and classes (e) are marked with crosses and highlighted with red in the distance matrices.\n\nR3\n\nH3\n\nRegularization\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nNo regularizer Stress BC+Stress — ” —: unseen poses — ” —: unseen class\n\nStress ±σ\n\n3.85±3.63 0.31±0.31 4.92±7.60 1.93±2.34 2.31±3.24\n\n4.05±3.77 0.23±0.34 1.25±2.14 1.60±2.21 3.20±4.59\n\n(a) Graph distance between the standing poses.\n\n(b) Average stress per geometry and regularization.\n\nFigure 11: Embeddings of standing poses considering the augmented whole-body support pose taxonomy: (b) shows the stress resulting from the different (a) shows the graph distance (colors follow Fig. 1-right). embeddings of standing poses.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Fl to F2Hr\n\n(b) Fl to F2H2\n\n(c) Fr to FrH2\n\n(d) F2Hl to FlH2\n\nFigure 12: Motions obtained via linear interpolation in the latent space of the vanilla Euclidean backconstrained GPHLVM trained on the augmented taxonomy (Fig. 10c). Contacts are denoted by gray circles. The colorbars identify the support pose of the closest pose in the latent space.\n\nStart Fl Fl Fr F2Hl\n\nEnd F2Hr F2H2 FrH2 FlH2\n\nTransitions in H3 Fl→F2Hr Fl→F2Hl → F2H2 Fr → FrHr → FrH2 F2Hl → F2H2 → FlH2\n\nTransitions in R3 Fl → FlHl → F2Hl→F2Hr Fl → FlHl → F2Hl→F2Hr → F2H2 Fr → FrHl→FrHr → FrH2 F2Hl→F2Hr → FlHr → FlH2\n\nTable 10: Embeddings of standing poses considering the augmented whole-body support pose taxonomy: Transitions (→) between classes of the taxonomy obtained by following the geodesic trajectories depicted in Fig. 10. The classes and transitions correspond to the colors along the trajectories and match the class corresponding to the closest embedding at each point along the geodesic. Transitions that do not exist in the taxonomy are denoted as →.\n\nG.4 COMPARISON AGAINST VARIATIONAL AUTOENCODERS\n\nHyperbolic embeddings of support poses: In this section, we compare the trained GPHLVMs of Fig. 2 with two additional baselines: a vanilla variational autoencoder (VAE) and a hyperbolic variant of this VAE in which the latent space is the Lorentz model of hyperbolic geometry (akin to Mathieu et al. (2019)). Both VAEs are designed with 12 input nodes, 6 hidden nodes, a 2dimensional latent space, and a symmetric decoder. Their encoder specifies the mean and standard deviation of a normal distribution (resp. wrapped normal for the hyperbolic VAE), and their decoder specifies the mean and standard deviation of the normal distribution that governs the reconstructions. Both models are trained by maximizing an Evidence Lower Bound (ELBO) under similar regimes as\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\n(a) P 2 Vanilla\n\n(b) R2 Vanilla\n\n(c) P 2 Stress\n\n(d) R2 Stress\n\nFigure 13: Embeddings of the VAE baselines: The first and second rows show the latent spaces of the (hyperbolic) VAE and the distance matrix between the latent codes, respectively. When comparing these distance matrices and encodings with that of our GPHLVMs (see Fig. 2), we notice that our proposed model is better able to preserve the graph distance structure. We argue this is because VAEs enforce latent spaces that follow a unit Gaussian, which is an opposite goal to ours.\n\nthe GPHLVMs, i.e., 1000 epochs with a learning rate of 0.05. The KL divergence for the hyperbolic VAE is computed using Monte Carlo estimates.\n\nImportantly, the VAE models only seem to capture a global structure that separates standing from kneeling poses (except the vanilla hyperbolic VAE in Fig. 13a). Although adding a stress regularization with the same scale as for the GPHLVM (γ = 6) helps preserve the graph distance structure, the embeddings organization is still not competitive with the one achieved by our GPHLVM models (see Fig. 2). Moreover, when compared to our proposed GPHLVM, all VAE models provide a subpar uncertainty modeling in their latent spaces.\n\nRegularization\n\nStress ±σ\n\nR2 No reg.\n\nStress\n\nH2 No reg.\n\nStress\n\n1.88±2.57 0.59±0.89\n\n3.96±4.22 0.52±0.71\n\nTable 11: Average stress per geometry and regularization for VAE baselines.\n\nTable 11 shows that the average stress of the latent embeddings for the VAE baselines (trained with and without stress regularization) is higher than the average stress of our models (see Table 1). Overall, our proposed GPHLVM consistently outperforms all VAEs to encode meaningful taxonomy information in the latent space. We argue that VAEs are not the right tool for our target applications. When training VAEs, the Kullback-Leibler term in the ELBO tries to regularize the latent space to match a unit Gaussian. This regularization is in stark contrast with our goal of separating the embeddings to preserve the taxonomy graph distances.\n\nHyperbolic embeddings of standing poses with an augmented taxonomy: We further compare our GPHLVM model against the vanilla and hyperbolic VAEs in the experiment described in Sec. G.3. Namely, we consider the augmented whole-body support pose taxonomy which explicitly distinguishes between left and right contacts and we represent each pose as a vector of joint angles. This increases the dimensionality of the data to 44.\n\nWe tested the vanilla and hyperbolic VAEs without regularization and with a stress regularization with the same scale as for the GPHLVM (γ = 1.5). Fig. 14 shows the learned embeddings alongside distance matrices, which are to be compared with the GPHLVM model of Figs. 10b-10c and with the ground-truth graph distances of Fig. 11a. Despite the stress regularization, the VAEs’ tendency to have unitnormally distributed latent representations hinders the distance matching. This is further quantified by the mean stress presented in Table 12, which show a higher mean stress (0.34 and 0.44) than our model in the same taxonomy (0.23, see Table 11b).\n\nRegularization\n\nStress ±σ\n\nR3 No reg.\n\nStress\n\nH3 No reg.\n\nStress\n\n2.33±3.10 0.34±0.37\n\n3.01±3.13 0.44±0.63\n\nTable 12: Average stress per geometry and regularization for VAE baselines trained on the augmented taxonomy (see App. G.3).\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\n(a) H3 Vanilla\n\n(b) R3 Vanilla\n\n(c) H3 Stress\n\n(d) R3 Stress\n\nFigure 14: Embeddings of the VAE baselines considering the augmented whole-body support pose taxonomy: The first and second rows show the latent spaces of the (hyperbolic) VAE and the distance matrix between the latent codes, respectively.\n\nTraining\n\nDecoding\n\nGPHLVM Q = 2 GPHLVM Q = 3 GPLVM Q = 2 2.5 × 103 1.33 × 10−2\n\n8.91 1.57 × 10−5\n\n5.9 1.16 × 10−5\n\nGPLVM Q = 3\n\n6.3 1.22 × 10−5\n\nTable 13: Average runtime (in seconds) for training and decoding phases of our GPHLVM and vanilla GPLVM over 5 experiments, using 2 and 3-dimensional latent spaces for both models. Training time was measured over 500 iterations for both models. The implementations are fully developed on Python, and runtime measurements were taken using a standard laptop with 32 GB RAM, Intel Xeon CPU E3-1505M v6 processor and Ubuntu 20.04 LTS.\n\nFig. 15 shows examples of motions planned by following geodesics between two standing poses in the hyperbolic VAE latent space. Similarly as the motions generated in the latent space of the proposed GPHLVM (Fig. 5), these motions result in realistic interpolations between the given initial and final poses.\n\nG.5 RUNTIME\n\nIn order to show the computational cost of our approach, we ran a set of experiments to measure the average runtime for the training and decoding phases, using 2 and 3-dimensional latent spaces. As a reference, we added the runtime measurements of Euclidean counterpart, that is, the vanilla GPLVM. Table 13 shows the runtime measurements. Note that the main computational burden arises in our GPLHVM with a 2-dimensional latent space, which is in sharp contrast with the experiments using a 3-dimensional latent space. As discussed in the main paper, this increase in computational cost is mainly attributed to the 2-dimensional hyperbolic kernel.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Fl to F2Hr\n\n(b) Fl to F2H2\n\n(c) Fr to FrH2\n\n(d) F2Hl to FlH2\n\nFigure 15: Motions obtained via geodesic interpolation in the latent space of the hyperbolic VAE trained on the augmented taxonomy (Fig. 14c). Contacts are denoted by gray circles. The colorbars identify the support pose of the closest pose in the latent space.\n\n28",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes a method to learn an embedding space, which leverages the human-designed taxonomy, for robotics tasks. It extends Gaussian Process Latent Variable Models (GPLVM) to hyperbolic latent spaces and proposes GPHLVM. The motivation for using hyperbolic space is that distances grow exponentially when moving away from the origin, and the shortest paths between distant points tend to pass through it, resembling a continuous hierarchical structure. They propose graph-distance priors and back constraints to introduce taxonomy knowledge into the GPHLVM.\n\nThe experiments are conducted on whole-body support pose taxonomy. The authors analyze the learned Hyperbolic latent embedding space and compare it with the Euclidean one. They also show that the learned embedding enables motion interpolation in the latent space.\n\n# Strength And Weaknesses\n\nStrengths:\n1. The idea of utilizing human-designed taxonomy prior in latent space learning is interesting.\n2. The introduction of Hyperbolic space seems to lead to better structure than Euclidean space.\n\nWeaknesses:\n1. I feel that the experimental part is not enough to demonstrate the advantage of the proposed GPHLVM:\n(a) There is only one task/dataset (whole-body support pose taxonomy) is used in the experiments. It is thus unclear whether the proposed method can be applied to other tasks as well.\n(b) There are no other alternative approaches for comparison (only Gaussian counterpart for visual analysis).\n(c) There is no quantitative comparison on some concrete tasks (e.g., pose classification) to demonstrate the practical benefit of the latent space.\n\n2. An alternative design would be using a neural network to learn an embedding space. We can still apply the priors (i.e., Hyperbolic and taxonomy-aware priors) to the learned embedding space by designing various loss functions. However, this alternative approach will be more straightforward and efficient to implement. Unfortunately, we fail to find such discussion or comparison in the paper.\n\n3. The proposed method hopes that the latent space complies with the human-designed taxonomy's hierarchical structure. However, as shown in the part of the experiments, when the human-designed taxonomy is coarse-grained, the performance of the proposed method is also limited.\n\n4. The runtime information is missing from the text. It's unclear whether the proposed method is an efficient or computationally intractable one. \n\n5. The current latent space is very low-dimensional. Could the authors provide more discussion on the benefits of learning a low-dimensional latent space? Also, it's interesting to know whether the proposed method can handle high-dimensional latent space as well.\n\n6. How shall we interpret the pairwise distance matrices in Figure 2 and compare them with the graph distances in Figure 3. To be honest, it's not easy to tell which one is closer to Figure 3.\n\n7. The interpolated motions in Figure 4 fail to impress me. The transition is not very natural and smooth. Moreover, it includes many physically-infeasible motions. In contrast, Figure 5 shows better performance. Since only motions generated by the proposed GPHLVM are shown, it is hard to demonstrate the advantage of the proposed GPHLVM, as other methods may generate similar or better performances.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: 8/10\nQuality: 7/10\nNovelty: 7/10\nReproducibility: 7/10 (if code released)\n\n# Summary Of The Review\n\nI feel that the experimental part is not enough to demonstrate the advantage of the proposed over existing methods. The practical usage is also unclear.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents a novel approach for bridging the gap between discrete hierarchical structures in robotic taxonomies and high-dimensional heterogeneous data by introducing a Gaussian Process Hyperbolic Latent Variable Model (GPHLVM). This model learns hyperbolic embeddings that respect the original taxonomy structure and incorporates mechanisms such as graph-distance priors and back constraints to maintain taxonomy integrity. The methodology is validated through experiments on a whole-body support pose taxonomy, where the GPHLVM demonstrates superior encoding of unseen poses and trajectory generation capabilities compared to traditional Euclidean models.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include its innovative application of hyperbolic geometry to robotics taxonomies, the effective integration of taxonomy knowledge into the GPHLVM, and the successful demonstration of improved model performance in empirical tests. However, the paper could benefit from a more comprehensive discussion on the computational complexity and efficiency of the proposed model, particularly in real-world applications. Furthermore, while the experiments show promising results, additional datasets and tasks could strengthen the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, making it accessible to readers. The quality of the writing is high, with appropriate use of terminology and background information. The novelty of applying hyperbolic manifold learning in this context is significant, as it provides a fresh perspective on the representation of hierarchical data. However, the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setup and the datasets used.\n\n# Summary Of The Review\nOverall, this paper makes a compelling contribution to the field of robotics by introducing a GPHLVM that utilizes hyperbolic geometry for continuous representation of discrete taxonomies. While the methodology is innovative and the results are promising, further elaboration on computational aspects and additional validation across diverse datasets would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a Gaussian Process Hyperbolic Latent Variable Model (GPHLVM) designed to bridge the gap between discrete hierarchical robotic taxonomies and high-dimensional data. Utilizing hyperbolic embeddings, the GPHLVM enforces taxonomy structures through graph-based priors and distance-preserving constraints. The methodology is grounded in Riemannian geometry, leveraging the properties of hyperbolic manifolds to effectively model and generate robotic poses. Experimental results demonstrate that GPHLVM significantly outperforms traditional Euclidean approaches and variational autoencoders (VAEs) in capturing taxonomy structures and generating realistic trajectories.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to applying hyperbolic geometry to robotic taxonomy modeling, showcasing robust performance in embedding unseen data and generating realistic motion trajectories. The effective integration of geometry into the latent space representation is commendable, as it aligns closely with the underlying taxonomy structures. However, the computational cost associated with hyperbolic kernels presents a significant limitation, potentially hindering scalability. Additionally, the model's reliance on the accuracy of the taxonomy structure could introduce biases if the taxonomy is flawed. Lastly, the feasibility of the generated motions in real-world applications is not addressed, which is crucial for practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodologies. The novelty of employing hyperbolic manifolds for robotic applications is significant, marking a noteworthy advancement in the field. The reproducibility of the results could be enhanced by providing more detailed descriptions of the datasets and hyperparameter configurations used in the experiments. Overall, the clarity and quality of the writing support the paper's findings, although more emphasis on reproducibility would strengthen the contribution.\n\n# Summary Of The Review\nThis paper presents a significant advancement in robotic taxonomy modeling through the introduction of GPHLVM, which effectively utilizes hyperbolic geometry. While the results are promising and demonstrate clear improvements over existing methods, the challenges related to computational efficiency and the reliance on accurate taxonomy structures warrant further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Gaussian Process Hyperbolic Latent Variable Model (GPHLVM), which bridges the gap between discrete robotic taxonomies and high-dimensional data through hyperbolic embeddings. By leveraging the properties of hyperbolic geometry, GPHLVM effectively models human movement and interaction taxonomies in robotics, specifically focusing on whole-body support poses. The methodology includes extending the traditional Gaussian Process Latent Variable Model (GPLVM) to hyperbolic latent spaces and incorporating taxonomy knowledge through graph-distance priors and back constraints. Experimental results demonstrate that GPHLVM significantly outperforms Euclidean models in encoding unseen poses and capturing taxonomy structure, highlighting its effectiveness in robotic motion design.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to combining hyperbolic geometry with latent variable modeling, addressing a significant gap in the existing body of work on robotic taxonomies. The methodology is well-structured, and the experimental results convincingly illustrate the advantages of GPHLVM over traditional models. However, the paper could benefit from a more comprehensive discussion of the limitations of the proposed model, particularly concerning the scalability of the method to larger datasets and the potential challenges in real-world applications. Additionally, while the paper presents clear experimental results, further exploration of the model’s applicability in diverse robotic contexts could enhance its impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, with a logical flow of ideas that guides the reader through the background, methodology, and findings. The quality of the figures and tables aids in understanding the experimental results, while the mathematical formulations are clearly presented. The novelty of the approach in utilizing hyperbolic embeddings for robotic taxonomy modeling is significant and contributes to the field of robotics and machine learning. The reproducibility of the experiments is enhanced by the detailed description of hyperparameters and experimental setups; however, the paper could have included more specifics regarding the implementation details to aid researchers in replicating the study.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective approach for modeling robotic taxonomies using the GPHLVM framework with hyperbolic embeddings. The methodology is well-founded and the results clearly demonstrate its advantages over existing models. However, a deeper exploration of limitations and a more detailed discussion on practical applications could further strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Bringing Robotics Taxonomies to Continuous Domains via GPLVM on Hyperbolic Manifolds\" introduces a novel Gaussian Process Hyperbolic Latent Variable Model (GPHLVM) designed to leverage hyperbolic geometry for representing robotic taxonomies. The methodology incorporates graph-based priors and distance-preserving constraints to enhance the model's ability to encode hierarchical structures and generate embeddings for robotic tasks. Empirical results demonstrate that the GPHLVM outperforms traditional Euclidean models in encoding and generating meaningful robotic poses, albeit with certain limitations in generalizability and computational efficiency.\n\n# Strength And Weaknesses\n**Strengths:**\n- The introduction of hyperbolic geometry in the context of robotics is a significant advance, providing a unique perspective on encoding robotic taxonomies.\n- Empirical results indicate that the GPHLVM excels in task performance compared to baseline models, showcasing its effectiveness in preserving taxonomy structures.\n- The integration of existing taxonomy knowledge via graph-based priors enhances the model's capabilities in generalizing to unseen poses.\n\n**Weaknesses:**\n- The complexity of implementing hyperbolic geometry may limit accessibility for practitioners who are not well-versed in advanced mathematical concepts.\n- The experiments conducted were limited to a specific dataset, raising concerns about the model's versatility across different robotic tasks.\n- Potential overfitting to training data due to reliance on specific priors and constraints could hinder the model's applicability in diverse contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents a clear methodology, supported by comprehensive experimental setups and visualizations. However, the complexity of hyperbolic geometry may make reproducibility challenging for practitioners unfamiliar with the underlying concepts. The novelty of the approach is commendable, as it introduces a fresh perspective to robotics taxonomy representation, although the accessibility of the implementation needs improvement.\n\n# Summary Of The Review\nOverall, the paper presents an innovative approach to robotic taxonomy representation using hyperbolic geometry, backed by solid empirical results. While the contributions are significant, the complexities involved in implementation and the limited scope of empirical validation may restrict its accessibility and applicability.\n\n# Correctness\n4/5 - The methodology and results appear sound, but some limitations concerning generalizability and potential overfitting are noted.\n\n# Technical Novelty And Significance\n5/5 - The application of hyperbolic geometry in this context is a novel and significant contribution to the field of robotics.\n\n# Empirical Novelty And Significance\n4/5 - The empirical results demonstrate the effectiveness of the proposed model, though the limited dataset scope raises questions about its broader applicability.",
    "# Summary Of The Paper\nThe paper presents a novel approach to modeling robotic taxonomies using a Continuous Process Hyperbolic Latent Variable Model (CPHLVM) situated on non-Euclidean spaces, particularly focusing on Riemannian geometries. The primary contributions include the introduction of CPHLVM to extend Gaussian Process Latent Variable Models (GPLVM) for continuous representation of hierarchical taxonomies, effective utilization of hyperbolic geometry to capture inherent relationships, and the incorporation of graph-based regularization that preserves taxonomy structures. Empirical evaluations demonstrate that CPHLVM significantly outperforms traditional Euclidean models, especially in encoding unseen poses and generating realistic motion trajectories.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to overcoming the limitations of existing models dealing with discrete taxonomies, which is particularly relevant in robotics. The application of hyperbolic geometry offers a significant advantage in modeling hierarchical relationships, and the empirical results substantiate the theoretical claims. However, a potential weakness lies in the complexity of the proposed model and its reliance on advanced mathematical concepts, which may limit accessibility for practitioners not well-versed in Riemannian geometry or Gaussian processes.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, with sufficient detail provided to understand the methodology and findings. The novelty of CPHLVM lies in its integration of hyperbolic geometry into latent variable modeling, which is a relatively unexplored area in robotics. The reproducibility of results is supported by comprehensive empirical validation on real datasets; however, further details on implementation and hyperparameter tuning would enhance reproducibility. \n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of robotic taxonomy modeling by introducing a novel framework that effectively utilizes hyperbolic geometry. The empirical results validate its effectiveness in practical applications, although the complexity inherent in the model may pose challenges for broader adoption.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a novel framework for adversarial training aimed at improving the robustness of machine learning models against adversarial attacks. The authors propose a Gaussian Process Latent Variable Model (GPLVM) that incorporates adversarial noise during the training phase, allowing models to learn resilient features. Utilizing hyperbolic geometry, the framework captures complex relationships among data points in high-dimensional spaces, leading to enhanced performance against various state-of-the-art adversarial attacks.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative approach of integrating hyperbolic geometry into adversarial training, which offers a fresh perspective on improving model robustness. The empirical validation is extensive, showcasing the method's effectiveness across multiple datasets and adversarial scenarios. Additionally, the theoretical foundation is well articulated, linking the proposed method to existing literature. However, weaknesses include the complexity of implementation due to the sophisticated nature of hyperbolic geometry, potential scalability issues for larger datasets or real-time applications, and a limited comparison with a broader range of recent advancements in robust machine learning techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical structure that facilitates understanding of the proposed methodology and its implications. The quality of the experiments is high, with thorough analyses presented. The novelty is significant, particularly in how it applies hyperbolic geometry to adversarial training. However, reproducibility could be hindered by the complexity of the proposed methods and the lack of extensive implementation details.\n\n# Summary Of The Review\nThe paper presents a significant advancement in adversarial training by proposing a novel approach that leverages hyperbolic geometry to enhance model robustness. While the methodology is innovative and empirically validated, there are concerns regarding implementation complexity and scalability, which warrant further investigation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Gaussian Process Hyperbolic Latent Variable Model (GPHLVM), claiming it to be a transformative advancement in robotics that bridges the gap between discrete taxonomy structures and continuous data representations. By leveraging hyperbolic geometry, the authors assert that GPHLVM enhances the efficiency and accuracy of high-dimensional data applications in robotics. The paper presents extensive experimental validation, showcasing GPHLVM's superiority over existing Euclidean models in generating robot motion and encoding unseen poses, suggesting a revolutionary potential for robotics.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating hyperbolic geometry with Gaussian processes, which could significantly enhance data representation in robotics. The extensive experimental results bolster the claims of GPHLVM's performance superiority. However, the paper tends to exaggerate claims regarding the model's capabilities, particularly in relation to unseen pose encoding and the dismissal of Variational Autoencoders (VAEs). Such hyperbolic assertions could mislead readers regarding the practical applicability and limitations of the model. \n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper provides a clear presentation of the GPHLVM and its potential implications, the overstatements regarding its capabilities may obscure its practical relevance. The methodology is well-articulated, but the novelty of the approach could have been contextualized better against existing models. Reproducibility is not thoroughly addressed, as there is limited information on the specifics of the experimental setup that could enable others to replicate the findings.\n\n# Summary Of The Review\nThe paper presents a potentially groundbreaking contribution to robotics through the introduction of GPHLVM, although it exaggerates its claims and dismisses existing methods without adequate justification. While the methodology and results are compelling, the paper's clarity is diminished by overstatements regarding the model's capabilities and implications for future research.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Bringing Robotics Taxonomies to Continuous Domains via GPLVM on Hyperbolic Manifolds\" presents a novel Gaussian process hyperbolic latent variable model (GPHLVM) designed to effectively model robotic taxonomies and classify human movements. The methodology leverages hyperbolic embeddings to address the challenges faced in representing hierarchical structures in high-dimensional heterogeneous data. Experimental results demonstrate that the GPHLVM outperforms traditional Euclidean models in terms of stress values and the ability to generate realistic motion trajectories, suggesting that it captures the hierarchical structure of robotic taxonomies effectively.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of a hyperbolic latent variable model that bridges the gap between discrete taxonomies and continuous data representations, utilizing a graph-based approach to enforce hierarchical structures. Additionally, the experimental validation using various datasets highlights the model's capability in both fitting and generalizing to unseen poses, which is a significant advancement in the field. However, a potential weakness is the lack of physical constraints in the model, which may limit the realism of the generated motion trajectories. Furthermore, the paper could benefit from additional comparisons with other state-of-the-art models to reinforce its claims of superiority.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The results are detailed and include modified results that enhance the validity of the claims made. The novelty of applying hyperbolic geometry to robotic taxonomies is significant and opens new avenues for research in this domain. Regarding reproducibility, the paper provides sufficient details on the experimental setup and results, allowing for potential replication of the study.\n\n# Summary Of The Review\nOverall, the paper provides a compelling contribution to the field of robotics by introducing an innovative model that effectively captures the hierarchical structure of human movement taxonomies. While there are areas for improvement, particularly concerning physical realism in motion generation, the findings are robust and demonstrate the model's advantages over traditional approaches.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper explores the utility of hierarchical taxonomies in robotics through a computational lens. It introduces the Gaussian Process Hyperbolic Latent Variable Model (GPHLVM), which leverages hyperbolic geometry to represent taxonomies effectively, claiming superiority over traditional Euclidean models. The authors provide empirical validation of their model on specific datasets, demonstrating its ability to generate meaningful trajectories and encode hierarchical relationships. However, the paper is built on several assumptions regarding the applicability and efficiency of hyperbolic representations and the GPHLVM's performance across varying contexts.\n\n# Strength And Weaknesses\nThe paper presents significant contributions by proposing a novel approach to represent hierarchical taxonomies with hyperbolic geometry and by introducing GPHLVM, which has the potential to enhance robotic applications. However, the reliance on several assumptions raises concerns. For instance, the efficacy of hyperbolic geometry for all taxonomy types is not thoroughly justified, and the generalization of results across unseen data and diverse scenarios lacks comprehensive validation. Additionally, the assumptions regarding the model's generative capabilities and the impact of regularization techniques may limit the practical applicability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly. However, it could benefit from a more in-depth discussion of the assumptions underlying the proposed model. As for reproducibility, while the empirical results are promising, the limited experimental scope raises questions about the generalizability of the findings. The novelty of utilizing hyperbolic geometry in this context is commendable, but the paper would be strengthened by addressing the potential limitations and broader applicability of the proposed methods.\n\n# Summary Of The Review\nWhile the paper offers an intriguing approach to hierarchical taxonomies in robotics using hyperbolic geometry and GPHLVM, it is hampered by several assumptions that require further scrutiny. The empirical validation appears limited, and the generalizability of results remains uncertain. Overall, the contributions are valuable, but the paper would benefit from addressing its inherent limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Gaussian Process Hyperbolic Latent Variable Model (GPHLVM), which effectively integrates hyperbolic embeddings to capture hierarchical structures in robotic taxonomies related to human motion. The methodology involves extending Gaussian Process Latent Variable Models (GPLVM) to hyperbolic spaces using hyperbolic kernels and wrapped Gaussian distributions, while incorporating graph-distance priors to align learned embeddings with the taxonomy's inherent hierarchical relationships. Experimental results demonstrate that GPHLVM significantly outperforms traditional Euclidean models in encoding complex poses and generating realistic motion trajectories, highlighting its effectiveness in robotic applications.\n\n# Strength And Weaknesses\nOne of the key strengths of this work is its innovative approach to bridging the gap between discrete taxonomies and high-dimensional data through the use of hyperbolic geometry, which is well-suited for hierarchical representations. The incorporation of graph-distance priors as inductive biases is an advantageous addition that enhances the model's performance. However, a potential weakness lies in the reliance on complex Riemannian optimization techniques, which may limit the model's accessibility to practitioners not well-versed in advanced mathematical concepts. Additionally, the experiments, while demonstrating promising results, could benefit from a broader range of datasets to fully validate the model's generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the methodology and findings accessible to the reader. The quality of the writing is high, with appropriate definitions and explanations provided for more complex concepts. In terms of novelty, the GPHLVM's use of hyperbolic spaces for modeling hierarchical data is a significant advancement in the field. Reproducibility is somewhat addressed through the detailed description of the model and experiments, but the reliance on specialized techniques may pose challenges for replication in practice.\n\n# Summary Of The Review\nOverall, the paper presents a novel and impactful approach to modeling robotic taxonomies using hyperbolic geometry, showcasing significant advantages over traditional methods. While the contributions are compelling and well-supported by experimental results, the complexity of the techniques employed may hinder broader adoption.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework for addressing a key challenge in machine learning, focusing on enhancing performance through the integration of advanced techniques. The authors propose a multi-faceted approach that combines theoretical insights with practical applications, aiming to outperform existing methods. The findings demonstrate significant improvements in performance metrics across various benchmarks, establishing the effectiveness of the proposed solution.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Framework**: The integration of various techniques reflects a creative and comprehensive approach to a well-known problem, showcasing the authors' deep understanding of the subject matter.\n2. **Strong Theoretical Basis**: The methodology is supported by solid theoretical underpinnings, which lends credibility to the proposed framework and its anticipated impact.\n3. **Thorough Literature Review**: The paper provides a detailed review of existing literature, effectively situating the work within the broader context of machine learning research and illustrating its significance.\n\n**Weaknesses:**\n1. **Clarity Issues**: Some sections lack clarity, and more detailed explanations or descriptions would enhance the reader's understanding of the methodology and its significance.\n2. **Limited Empirical Validation**: While the theoretical contributions are robust, the paper could benefit from additional empirical evaluations to substantiate the claims. A broader range of benchmarks would enhance the findings.\n3. **Reproducibility Concerns**: The lack of comprehensive implementation details and information about datasets raises questions about the reproducibility of the results, which is crucial for the validation of the proposed approach.\n4. **Limited Discussion on Applicability**: The paper does not sufficiently explore the applicability of the proposed method to diverse domains or potential limitations, which would provide a more balanced perspective on its utility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by certain vague sections, which could benefit from more detailed explanations. The overall quality of the writing is good, but the lack of specific implementation details raises reproducibility concerns. The novelty of the approach is commendable, showcasing creativity and innovation. However, the empirical evidence supporting the claims is somewhat limited, affecting the overall strength of the findings.\n\n# Summary Of The Review\nThis paper presents a promising approach to a significant challenge in machine learning, demonstrating innovative methods and a solid theoretical foundation. However, improvements in clarity, empirical validation, and reproducibility are necessary to strengthen its overall impact and utility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to integrating robotic taxonomies with high-dimensional data through the introduction of a Gaussian Process Hyperbolic Latent Variable Model (GPHLVM). This model leverages hyperbolic embeddings to effectively capture the hierarchical structure of taxonomy data, which is crucial for analyzing human movements and interactions in robotics. The authors demonstrate that the GPHLVM can learn hyperbolic embeddings for whole-body support poses, encode unseen poses, generate trajectories, and outperform traditional Euclidean models. The findings suggest that hyperbolic geometry is a more effective framework for continuous representation of robotic taxonomies.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to bridging the gap between discrete taxonomies and continuous data representation in robotics. The use of hyperbolic embeddings is well justified, and the introduction of graph-based priors enhances the model's ability to capture the inherent structure of taxonomy data. However, the paper could benefit from a more thorough empirical evaluation, particularly in terms of computational efficiency and the scalability of the GPHLVM model. Additionally, while the results are promising, further validation across a wider range of robotic tasks and environments would strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its contributions clearly, making it accessible to readers familiar with robotics and machine learning. The methodology is described in sufficient detail, although some sections could benefit from additional clarifications regarding the implementation of the model. The novelty of the approach is significant, particularly in the application of hyperbolic geometry to robotic taxonomies. However, the reproducibility of the results may be hindered by the lack of detailed experimental setups and datasets used, which are crucial for others to replicate the study.\n\n# Summary Of The Review\nOverall, the paper offers a compelling and novel approach to modeling robotic taxonomies using hyperbolic embeddings, yielding promising results that outperform traditional methods. While the contributions are significant and well-articulated, the paper would benefit from a more comprehensive empirical evaluation and enhanced reproducibility measures.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to modeling robotic taxonomies in continuous domains using a Gaussian process hyperbolic latent variable model (GPHLVM). By leveraging hyperbolic geometry, the authors propose a methodology that integrates taxonomy knowledge through graph-based priors and Riemannian optimization. The findings demonstrate that the GPHLVM effectively captures poses and generates trajectories, outperforming traditional Euclidean models and variational autoencoders in terms of stress reduction and meaningful representation of taxonomy structures.\n\n# Strength And Weaknesses\nThe primary strengths of this paper lie in its innovative application of hyperbolic geometry to robotics, addressing a significant gap in the computational modeling of human movement taxonomies. The methodology is rigorous, employing advanced mathematical techniques that enhance the model's ability to maintain latent variables within hyperbolic space. However, the paper could benefit from a more extensive exploration of potential limitations, such as the scalability of the approach to other robotic tasks or the impact of noise in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results, making it accessible to readers with a background in robotics and machine learning. The quality of the experiments is high, with appropriate evaluation metrics and a comprehensive dataset. The novelty of employing hyperbolic geometry in this context is significant, although further elaboration on the reproducibility of the results in diverse settings would strengthen the paper.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of robotics by introducing a GPHLVM that effectively models human movement taxonomies in continuous domains. Its innovative use of hyperbolic geometry and strong empirical results highlight its potential impact on future research and applications in robotics.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a novel Gaussian process hyperbolic latent variable model (GPHLVM) designed to enhance the representation of robotic taxonomies. The methodology integrates hyperbolic geometry into the traditional GPLVM framework, allowing for more effective modeling of the hierarchical structure inherent in robotic motion and behavior. The experimental results demonstrate that the GPHLVM significantly outperforms baseline models, including Euclidean GPLVM and variational autoencoders (VAEs), in terms of both taxonomy representation and motion generation.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative application of hyperbolic geometry to the Gaussian process framework, which addresses a notable gap in the current literature regarding the representation of complex robotic taxonomies. The comprehensive experimental setup and the comparison against multiple baseline models lend credibility to the findings. However, a potential weakness is the limited scope of datasets used, which may affect the generalizability of the results. Additionally, while the mathematical formulation is well-explained, a more detailed discussion on the computational efficiency of the proposed model would enhance the practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and written with clarity, making complex concepts accessible through clear definitions and explanations. The figures and tables effectively illustrate key points and contribute to the reader's understanding of the results. The novelty of the approach is significant, as it represents a new direction in incorporating geometric considerations into latent variable modeling. Reproducibility appears feasible, given the detailed methodology and datasets described, although access to the code and data would further support this.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in the field of robotics through its innovative use of hyperbolic geometry in latent variable modeling. The methodology is robust, and the results are promising, although some limitations related to dataset diversity and computational efficiency warrant consideration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the Gaussian Process Hyperbolic Latent Variable Model (GPHLVM), which addresses the challenges of bridging discrete robotic taxonomies with high-dimensional heterogeneous data using hyperbolic embeddings. It utilizes a combination of Gaussian Process Latent Variable Models (GPLVM) and hyperbolic geometry to preserve hierarchical structures in taxonomy data. The model incorporates graph-based priors and distance-preserving constraints within the latent space to ensure that the learned embeddings reflect the underlying taxonomy structure. Empirical evaluations demonstrate that GPHLVM outperforms traditional Euclidean embeddings in encoding taxonomy-aware representations for whole-body support pose taxonomy.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to integrating hyperbolic geometry into latent variable modeling, which is a significant advancement for robotics applications. The effective empirical validation provides strong evidence for the model's advantages over existing methods. However, the paper could benefit from a more detailed exploration of the limitations of the proposed model and its scalability to larger datasets or more complex scenarios. Additionally, while the mathematical formulations are described, some readers may find them challenging to comprehend without a more intuitive explanation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, and the writing is clear, facilitating a good understanding of the methodology and findings. The quality of the research appears high, with thorough mathematical derivations and empirical evaluations. The novelty of introducing hyperbolic embeddings into the context of robotic taxonomies is significant, marking a new direction in the field. The reproducibility is supported by detailed descriptions of the model training process and experimental setups, although additional supplementary materials could enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the integration of hyperbolic geometry with Gaussian processes for robotic applications. While it demonstrates strong empirical performance and theoretical foundations, further elaboration on scalability and limitations would strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper aims to address a gap in robotic applications by proposing a methodology that utilizes taxonomies alongside Gaussian process hyperbolic latent variable models (GPHLVM). The authors claim that their approach outperforms traditional Euclidean models in specific tasks, although they do not quantify this performance convincingly. The methodology includes graph-based priors and back constraints to enhance model robustness, yet the practical implications and usability of these features remain unclear.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative attempt to bridge theoretical models with practical robotic applications through the use of hyperbolic embeddings. However, the weaknesses are pronounced; the reliance on GPHLVM is ambitious but seems to complicate implementation, and the claims of superiority over existing models lack adequate empirical support. Additionally, the experimental results are inconsistent and fail to translate into clear, actionable insights for practitioners, raising questions about the viability of the proposed approach in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper suffers from a lack of clarity and coherence, as the arguments are not well-articulated and the rationale behind the proposed model is inadequately justified. The novelty of the approach is not compellingly presented, leading to a perception that it does not significantly advance the field. Reproducibility is also a concern due to vague descriptions of experimental results and a failure to provide enough detail on model implementation.\n\n# Summary Of The Review\nOverall, the manuscript presents an interesting attempt to utilize hyperbolic embeddings in robotic applications but falls short in providing clear, actionable insights and robust empirical validation. The complexity of the proposed model, combined with a lack of convincing performance metrics, raises concerns about its practical applicability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents the Gaussian Process Hyperbolic Latent Variable Model (GPHLVM), a novel framework that applies hyperbolic geometry to the representation of robotic taxonomies in continuous domains. The methodology leverages the unique properties of hyperbolic space to generate smooth and realistic motion trajectories, effectively encoding unseen poses and accommodating the expansion of taxonomy knowledge. Key findings indicate that GPHLVM outperforms traditional Euclidean models in capturing complex hierarchical structures, thereby enhancing the adaptability and usability of robots in diverse environments.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach and significant contributions to the field of robotics. The GPHLVM effectively utilizes hyperbolic geometry to address limitations of traditional models, showcasing superior performance in motion generation and unseen pose encoding. Furthermore, the experimental validation on whole-body support pose taxonomy provides strong evidence for the model's efficacy. However, the paper could benefit from a more detailed discussion on the limitations and potential challenges of implementing GPHLVM in practical applications, as well as a clearer outline of future research directions beyond those mentioned.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the novel contributions of the GPHLVM. The quality of the experimental results is commendable, demonstrating the model's effectiveness in real-world applications. The use of Riemannian optimization techniques adds depth to the methodology, showcasing a forward-thinking approach to parameter tuning. However, the reproducibility of the experiments could be enhanced with additional details regarding the datasets used and the specific implementation steps, allowing others to replicate the results more easily.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to robotics by introducing the GPHLVM, which effectively utilizes hyperbolic geometry for robotic taxonomy representation. The strong experimental results and innovative methodology highlight the potential for future advancements in robotic capabilities, although further discussion on practical implementation challenges and reproducibility is recommended.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework called Gaussian Process Hierarchical Latent Variable Model (GPHLVM) which integrates robotic taxonomies with Gaussian Process Latent Variable Models (GPLVM) on hyperbolic manifolds. The authors propose that this approach enhances the representation of hierarchical structures inherent in human motion and interaction with robotic systems. The methodology involves utilizing hyperbolic geometry to maintain hierarchical relationships and employing Riemannian optimization for training, which preserves the geometric properties of the hyperbolic space. Key findings suggest that the GPHLVM can effectively encode complex relationships in hierarchical data, enabling better generalization and motion planning in robotic applications.\n\n# Strength And Weaknesses\nStrengths of the paper include its theoretical contributions to the understanding of hierarchical structures in robotics, notably through the use of hyperbolic geometry, which offers advantages over traditional Euclidean models. The incorporation of graph-based priors and distance-preserving constraints strengthens the model’s ability to reflect taxonomy structures. However, weaknesses include a limited empirical evaluation of the proposed model, which may hinder the demonstration of its practical applicability in real-world scenarios. The reliance on sophisticated Riemannian optimization may also pose challenges for reproducibility and accessibility to practitioners unfamiliar with advanced mathematical techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its theoretical framework clearly, making complex concepts accessible. The quality of writing is high, with appropriate use of technical language. The novelty of the approach lies in the fusion of hyperbolic geometry with Gaussian processes, which is a relatively unexplored area in robotics. However, the reproducibility of the findings may be limited due to the advanced optimization techniques employed and the lack of extensive empirical validation across diverse robotic tasks.\n\n# Summary Of The Review\nOverall, the paper makes a significant theoretical contribution to the field of robotics by introducing a novel framework that leverages hyperbolic geometry for hierarchical data representation. While the model shows promise in enhancing the understanding of motion dynamics, further empirical validation is needed to establish its practical utility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces the Gaussian Process Hyperbolic Latent Variable Model (GPHLVM), designed to model robotic taxonomies within continuous domains by reformulating Gaussian process latent variable models (GPLVM) for hyperbolic spaces. The methodology includes the use of a hyperbolic wrapped Gaussian distribution, adapted hyperbolic kernel functions such as squared exponential and Matérn kernels, and Riemannian optimization techniques including Riemannian Adam for parameter optimization. The findings demonstrate that GPHLVM effectively preserves taxonomy structure during modeling, with empirical results showcasing improved performance metrics, such as stress values and average log-likelihoods, when compared to both vanilla and hyperbolic variational autoencoders.\n\n# Strength And Weaknesses\nThe paper presents several strengths, particularly in its innovative approach to leveraging hyperbolic geometry for latent variable modeling, which is a significant advancement over traditional Euclidean methods. The use of variational inference for scalability and the detailed exploration of kernel adaptations highlight the authors' thorough understanding of the subject. However, the lack of code availability in the abstract raises concerns about reproducibility. While the experimental results are promising, additional clarity on the datasets and comparisons with existing models could strengthen the paper's contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, with detailed explanations of the methodologies employed. The novelty of the approach lies in its adaptation of Gaussian processes to hyperbolic spaces, making a significant contribution to the field. However, the absence of explicit code availability may hinder reproducibility for other researchers. The empirical evaluations are well-presented, yet further clarification on experimental setups would enhance understanding.\n\n# Summary Of The Review\nOverall, the GPHLVM paper makes a noteworthy contribution by adapting Gaussian processes for hyperbolic spaces, providing a robust methodology and empirical evidence to support its effectiveness. While the findings are promising and potentially impactful, the lack of code availability and some clarity in experimental details are areas for improvement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a Gaussian process hyperbolic latent variable model (GPHLVM) aimed at leveraging hyperbolic geometry for improved performance in robotic applications. It proposes a methodology that integrates hyperbolic embeddings to generate realistic motion trajectories and organize embeddings via stress regularization. The authors claim that their model outperforms traditional Euclidean counterparts in various tasks, although empirical comparisons with established frameworks, such as the Gaussian process latent variable model (GPLVM), are limited.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to apply hyperbolic geometry to robotic motion generation, which is an interesting and potentially useful direction. However, the paper suffers from several weaknesses, including a lack of acknowledgment of existing literature that explores similar models and methodologies. The authors fail to substantiate their claims of superiority over established approaches, such as the GPLVM and previous work on hyperbolic embeddings. The absence of comprehensive benchmarks against other motion generation models further limits the ability to assess the GPHLVM's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper varies; while the introduction of the GPHLVM is articulated, the lack of thorough comparisons and contextualization within existing literature detracts from its overall quality. The novelty of the approach is undermined by the oversight of previous work in the field, which dilutes the significance of the contributions. Reproducibility is not thoroughly addressed, as the paper does not provide sufficient details on experimental setups or datasets used, nor does it offer code or supplementary materials for replication.\n\n# Summary Of The Review\nIn summary, while the GPHLVM presents a potentially valuable approach to robotic motion generation using hyperbolic geometry, it lacks thorough comparisons with existing methodologies and underplays the contributions of prior research in the field. This diminishes its overall impact and makes it difficult to assess its true novelty and effectiveness.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Bringing Robotics Taxonomies to Continuous Domains via GPLVM on Hyperbolic Manifolds\" presents a novel approach to integrating robotics taxonomies within continuous domains using Gaussian Process Latent Variable Models (GPLVM) on hyperbolic manifolds. The authors propose a framework that effectively captures complex relationships within robotic motion data, thereby enabling better representation and understanding of robotic behaviors. The methodology includes defining hyperbolic kernels and applying variational inference for model training, ultimately demonstrating improved performance across several benchmark datasets in comparison to existing methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative application of hyperbolic geometry to the domain of robotics, which has been relatively underexplored. The integration of established robotics taxonomies with GPLVM provides a robust theoretical foundation and practical utility for modeling robotic motions. However, the paper has notable weaknesses, primarily in terms of clarity and consistency in formatting. Issues such as inconsistent citation formats, unclear variable definitions in equations, and the need for clearer explanations in the background section detract from the overall quality of the presentation.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the concept presented is novel and significant, the clarity of the paper suffers due to several formatting and consistency issues. The lack of a structured background section and inconsistent terminology can hinder readers' understanding, especially those not familiar with hyperbolic geometry or GPLVM. Moreover, the reproducibility of the experiments could be improved with better descriptions of datasets and methods. Overall, the paper's quality is good in terms of content but requires substantial refinement in presentation to enhance clarity and reproducibility.\n\n# Summary Of The Review\nIn summary, this paper presents a promising approach to incorporating robotics taxonomies into continuous domains through GPLVM on hyperbolic manifolds. However, the paper's clarity and consistency need significant improvement to ensure that its contributions are easily accessible to the audience. Refining these aspects will enhance the paper's overall impact and readability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the implications of hyperbolic manifolds for robotic taxonomies, proposing a novel framework aimed at enhancing the hierarchical organization of robots. The methodology involves the application of geometric principles to classify and differentiate robotic capabilities, with an emphasis on improving the understanding of robot interactions. The findings suggest that this approach can lead to more effective categorization and potentially improve robotic performance in specific scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative application of hyperbolic geometry to robotic classification, which presents a fresh perspective on hierarchical structures. However, the study has significant weaknesses, including a limited exploration of potential applications beyond taxonomy, such as dynamic manipulation or real-time human interaction. The narrow selection of datasets used for testing the model raises concerns about the generalizability of the findings. Moreover, the lack of discussion on multi-modal data integration, scalability, and the ethical implications of robotic applications detracts from the paper's overall impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear in its presentation of concepts, though some sections could benefit from additional detail, particularly regarding the implications of computational efficiency and the selection of manifold geometries. The novelty of using hyperbolic geometry is commendable, yet the paper lacks sufficient exploration of existing models, which would aid in contextualizing its contributions. Reproducibility may be a concern given the limited datasets and absence of comprehensive evaluation metrics.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to robotic taxonomies through the lens of hyperbolic geometry, but it falls short in addressing broader applications and practical considerations. A more in-depth examination of its methodologies and a wider evaluation framework would enhance its contributions to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to Gaussian Process Latent Variable Models (GPLVM) by introducing a Generalized Hyperbolic GPLVM (GPHLVM) that utilizes hyperbolic geometry for better representation of complex data structures. The methodology employs variational inference and Riemannian geometry to optimize the latent variable representation while preserving graph distances through a stress loss function. Empirical results demonstrate that GPHLVM significantly outperforms traditional GPLVMs and vanilla/hyperbolic Variational Autoencoders (VAEs) in terms of model fit and computational efficiency, particularly in preserving taxonomy structures.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative application of hyperbolic geometry to latent variable modeling, which offers a fresh perspective on maintaining structural integrity in the embedded space. The use of Riemannian metrics and advanced regularization techniques to enhance distance preservation further underscores the methodological rigor. However, the paper could benefit from a more comprehensive discussion regarding the limitations of the approach, particularly concerning computational costs associated with hyperbolic space when scaling to very large datasets. Additionally, while the empirical results are promising, the paper lacks extensive comparison with more contemporary methods in manifold learning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and structured, making the complex concepts of hyperbolic geometry and Gaussian processes accessible to the reader. The methodology is described in detail, allowing for reproducibility, especially with respect to the variational inference and stress loss techniques. The novelty of the approach is significant, as it combines established concepts in a unique way to address specific challenges in embedding complex data. However, further clarity could be added regarding the hyperparameter tuning and the implications of different kernel choices in hyperbolic spaces.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling advancement in latent variable modeling by leveraging hyperbolic geometry to improve the representation of complex structures. While it demonstrates strong empirical performance and methodological rigor, there are areas for improvement in discussing limitations and broader comparisons with existing techniques.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel model based on Gaussian Processes for Hierarchical Latent Variable Models (GPHLVM) aimed at generating robotic motion trajectories. It proposes a distortion loss method to enhance motion generation while utilizing a hyperbolic kernel to capture the underlying structure of the data. The findings indicate that the GPHLVM can effectively model whole-body support poses; however, the paper does not rigorously validate its performance across diverse datasets or real-world scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to modeling robotic motion through GPHLVM, which shows promise in capturing complex pose structures. However, significant weaknesses are evident, including the lack of consideration for physical feasibility in generated trajectories, which limits practical applications. Furthermore, the evaluation of the model is narrow, focusing primarily on a specific pose taxonomy, which raises concerns regarding generalizability. The reliance on a hyperbolic kernel presents issues related to computational efficiency, and the instability of the distortion loss method indicates potential weaknesses in the proposed regularization strategy.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but the clarity could be improved by providing more detailed methodologies for future work, particularly regarding the incorporation of physics constraints in motion generation. While the novelty of the approach is commendable, the reproducibility is hindered by the lack of thorough empirical validation across various datasets and tasks. The paper does not sufficiently address alternative sampling strategies or the scaling of the model to more complex scenarios, which are critical for reproducibility and practical application.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to robotic motion generation with promising theoretical contributions; however, it falls short in practical applicability and empirical validation. The limitations in generalizability and computational efficiency need to be addressed for the findings to be more impactful in real-world applications.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper proposes the use of a Gaussian Process Hyperbolic Latent Variable Model (GPHLVM) to learn embeddings for robotic taxonomies aimed at understanding human motion. The authors claim that the hyperbolic manifold is beneficial due to its geometric properties, which they argue helps maintain the structure of taxonomies through graph-based priors. Experimental results suggest that their method outperforms traditional Euclidean models when applied to a whole-body support pose taxonomy.\n\n# Strength And Weaknesses\nThe key strength of the paper lies in its attempt to combine Gaussian processes with hyperbolic geometry for modeling robotic taxonomies, which may provide some theoretical advantages. However, the methodology largely rehashes existing techniques without substantial justification for the proposed innovations. The experiments conducted are conventional and do not present a compelling case for the claimed superiority of the GPHLVM over standard models. Additionally, the paper lacks depth in discussing the implications of their findings and fails to engage meaningfully with existing literature.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the writing is generally clear, the paper is laden with jargon that detracts from its accessibility. The novelty is questionable, as the contributions seem to be incremental rather than groundbreaking. Reproducibility is difficult to assess due to insufficient detail in the methodology section, particularly regarding the implementation of the graph-based priors and the experimental setup.\n\n# Summary Of The Review\nThis paper attempts to leverage Gaussian processes in a hyperbolic setting to enhance robotic taxonomies. However, it falls short of presenting significant innovation or empirical validation, often relying on established concepts without sufficient justification or depth. Overall, the contributions appear to be modest and lack the rigor needed for a meaningful impact in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a Gaussian Process Hyperbolic Latent Variable Model (GPHLVM) designed to leverage hyperbolic geometry for representing robotic taxonomies in a structured manner. The methodology focuses on preserving the hierarchical structure in the latent space, utilizing graph-based priors and back constraints. The findings indicate that the GPHLVM outperforms traditional Euclidean models in terms of stress values and distance preservation, and demonstrates the ability to encode unseen poses and expand the taxonomy effectively.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to using hyperbolic geometry for robotic taxonomies, which addresses a significant gap in existing models. The authors successfully demonstrate the advantages of hyperbolic embeddings over Euclidean counterparts. However, the methodology could benefit from the integration of more advanced generative models, such as Variational Autoencoders (VAEs), and deep learning techniques, which might enhance its robustness. Additionally, the experiments are limited to specific datasets, and a broader application could strengthen the findings. The computational cost associated with hyperbolic kernels is noted as a limitation, suggesting a need for more efficient computational strategies.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making it accessible to a broad audience. The quality of the experiments is sound but could be improved by including comparisons with other manifold learning techniques. The novelty of the approach is notable, particularly in its application to robotic taxonomies. However, the reproducibility of the results may be challenged by the computational costs of the hyperbolic kernels and the need for more diverse datasets in future work.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to using hyperbolic geometry for robotic taxonomies, showcasing significant advantages over traditional methods. While the contributions are noteworthy, there are several areas for improvement, particularly in expanding the experimental scope and enhancing computational efficiency.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a Gaussian Process Hyperbolic Latent Variable Model (GPHLVM) designed for modeling robotic taxonomies, particularly focusing on whole-body support pose taxonomy. The authors demonstrate that GPHLVM outperforms traditional Euclidean models across multiple datasets, achieving significantly lower average stress values that indicate a better fit to the taxonomy graph distances. The model also excels in trajectory generation, accurately following taxonomy graph transitions and embedding unseen poses while preserving meaningful distances in the latent space. Comparative evaluations against variational autoencoders (VAEs) further highlight GPHLVM's strengths, particularly in generating realistic motion trajectories.\n\n# Strength And Weaknesses\nThe GPHLVM presents a strong contribution to the field by effectively addressing the shortcomings of standard Euclidean models in preserving hierarchical structures within robotic taxonomies. The methodology is sound, employing robust benchmarking against both traditional models and VAEs, which strengthens its validity. However, the paper could benefit from a more detailed discussion on the computational complexity of the GPHLVM compared to its counterparts, as well as potential limitations in its application scope. Additionally, further exploration of hyperparameter sensitivity could enhance the robustness of the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings. The quality of the experiments is high, with consistent results across balanced and unbalanced datasets. The novelty of using hyperbolic embeddings for robotic motion modeling is significant, offering a fresh perspective that builds on existing methods. However, the reproducibility of results could be improved by providing additional details about the datasets used and the specific configurations of the models employed in the experiments.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the modeling of robotic taxonomies through the introduction of the GPHLVM, demonstrating clear empirical advantages over traditional models and VAEs. The findings are robust and suggest a promising direction for future research in motion generation and taxonomy representation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to improving the interpretability of machine learning models through the development of a new taxonomy framework that categorizes model behaviors. The methodology involves a combination of theoretical analysis and empirical validation, utilizing a Generalized Probabilistic Hierarchical Latent Variable Model (GPHLVM) to assess how different model configurations affect performance and interpretability. The findings indicate that the proposed framework offers significant enhancements in clarity and understanding of model decisions, particularly in complex datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to taxonomy development, which provides a structured way to analyze and interpret machine learning models. The empirical results support the theoretical claims, demonstrating the framework's practical applicability. However, the paper suffers from issues related to clarity and organization, particularly in the introduction and abstract, which could lead to reader confusion. Additionally, while the methodology is solid, the reliance on specific terminologies without clear definitions may hinder accessibility for a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe overall clarity of the paper is undermined by dense language, particularly in the abstract and introduction, where clearer segmentation of ideas is needed. The quality of the figures is relatively good, although some captions lack sufficient context. The novelty of the proposed taxonomy is commendable, offering a fresh perspective on interpretability in machine learning. However, the reproducibility of the results could be improved by providing more detailed descriptions of the experimental setup and clearer definitions of key terms and abbreviations.\n\n# Summary Of The Review\nOverall, while the paper introduces an innovative framework for model interpretability, it suffers from significant clarity issues that detract from its contributions. Improved organization, clearer definitions, and enhanced presentation of results would greatly benefit the reader's understanding and engagement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.1278869694630984,
    -1.729520011696049,
    -1.787028679491528,
    -1.706282928152153,
    -1.7561103632071886,
    -1.8359326606402837,
    -1.577301068729984,
    -1.8687157308587112,
    -1.7918780917693224,
    -1.8205689286637217,
    -1.7154740449076435,
    -1.479684061056299,
    -1.5676386365967847,
    -1.7463513362272574,
    -1.542804595387571,
    -1.7327845640589696,
    -2.0194559085670796,
    -1.7422495089833558,
    -1.7027918846104586,
    -1.9158640306257733,
    -1.7391004216681158,
    -1.7298650875198913,
    -1.8516035500605355,
    -1.720677495575214,
    -1.9593711476650295,
    -1.942000264748136,
    -1.7304311986185568,
    -1.8869326918916622,
    -1.6999109464324096
  ],
  "logp_cond": [
    [
      0.0,
      -1.8880267209031318,
      -1.9398624599654275,
      -1.8909094237718176,
      -1.9293008510278526,
      -1.9706022775013132,
      -2.0100572576117,
      -1.9652201354540242,
      -1.948518488329857,
      -1.9554032365230354,
      -1.9258655365836512,
      -2.0192661901991342,
      -1.932793957725734,
      -1.9446146171559586,
      -1.9673722053154865,
      -1.9213457520440522,
      -1.9410516749778532,
      -1.9385796907017132,
      -1.9508942679904884,
      -1.971615398554815,
      -1.9847113858708259,
      -1.9829079647746022,
      -2.0038811658795326,
      -1.9859287860927888,
      -1.9528153514130258,
      -1.9113465538925898,
      -1.9393057491047527,
      -1.936206040192836,
      -2.0154133744229914
    ],
    [
      -1.146149880160808,
      0.0,
      -1.1849581746126765,
      -1.0761844572704111,
      -1.2349947932204532,
      -1.2296086045200922,
      -1.3688624628424397,
      -1.2480432403069441,
      -1.1873710122579364,
      -1.2257137076479836,
      -1.145035174710997,
      -1.431659392360403,
      -1.082534108771048,
      -1.205298833257946,
      -1.2792786578987958,
      -1.1929477926645002,
      -1.220428872338977,
      -1.1123706148763048,
      -1.3124160851758209,
      -1.2658860050722678,
      -1.3022137217534453,
      -1.2690294900086923,
      -1.3015282518068667,
      -1.3435851438418482,
      -1.275465355620664,
      -1.1491591985085001,
      -1.1983653038333277,
      -1.220101997524692,
      -1.3915371830902794
    ],
    [
      -1.3357649429421063,
      -1.2443799333330814,
      0.0,
      -1.2519998837448307,
      -1.2670159263206104,
      -1.3079453944166197,
      -1.4361604669842531,
      -1.324500845990867,
      -1.2462325212686487,
      -1.3309365811866924,
      -1.2540170232575172,
      -1.4883692675496185,
      -1.276324751480321,
      -1.2735890497155746,
      -1.3431394277356932,
      -1.2631293753022739,
      -1.3471428608728737,
      -1.309781780991332,
      -1.371095077480889,
      -1.2960286257753482,
      -1.385984609506922,
      -1.418697097486099,
      -1.4245121333327622,
      -1.360275605817548,
      -1.334185540838384,
      -1.3165699995010194,
      -1.250672270486893,
      -1.2504171742336208,
      -1.5083490635605732
    ],
    [
      -1.287042477442288,
      -1.1875671919571258,
      -1.2602352443098628,
      0.0,
      -1.348342494582157,
      -1.3145374600065678,
      -1.4517776617751212,
      -1.3506746232825118,
      -1.28589890380337,
      -1.3765992716936364,
      -1.2426717636074676,
      -1.4771947452549747,
      -1.2407857705728111,
      -1.3158853377761526,
      -1.3483415142578017,
      -1.2440290679946506,
      -1.3403120239130963,
      -1.2864023729397704,
      -1.3629399220830358,
      -1.3587764478628057,
      -1.3986715713769253,
      -1.3959308190536117,
      -1.4395570739492987,
      -1.4395111487699837,
      -1.3787644276168693,
      -1.336914291718006,
      -1.3278332232501306,
      -1.2879112121490923,
      -1.4804973230791276
    ],
    [
      -1.3665456013280115,
      -1.3477553796858737,
      -1.3214422131218087,
      -1.342162663942916,
      0.0,
      -1.384610238286659,
      -1.4725680039062612,
      -1.3805055148679504,
      -1.295644871437853,
      -1.354362999355651,
      -1.339829731818367,
      -1.4926688109237005,
      -1.3378544677001465,
      -1.3412860917840028,
      -1.3969447843515692,
      -1.339372597628337,
      -1.394283472876106,
      -1.3672238274894577,
      -1.3359882597162955,
      -1.389497896000063,
      -1.4755808297872268,
      -1.4115830167400587,
      -1.4800699929304628,
      -1.4534700492957,
      -1.451259363024695,
      -1.3776328242216658,
      -1.311740206901103,
      -1.407783026964804,
      -1.5310116810254828
    ],
    [
      -1.4540014597554398,
      -1.2876830609718952,
      -1.2729706569666468,
      -1.3017688772831735,
      -1.362324778499342,
      0.0,
      -1.5032294261529475,
      -1.3639714224890886,
      -1.3383547026743452,
      -1.3683905802993694,
      -1.2777049236405904,
      -1.56935091146144,
      -1.2828419022512496,
      -1.3312835105998173,
      -1.4142003328527242,
      -1.3082270157266853,
      -1.4402520726868286,
      -1.2825528995935682,
      -1.3616419952053398,
      -1.3595573225572357,
      -1.4274784244023409,
      -1.4215817847710808,
      -1.4859126958356363,
      -1.4379192195071813,
      -1.4107344588140467,
      -1.3775745794359866,
      -1.3636023895029648,
      -1.3533153935887372,
      -1.5178050851047162
    ],
    [
      -1.2673618113104157,
      -1.2352188823488082,
      -1.2252740089147074,
      -1.2198138142094395,
      -1.2183034516931068,
      -1.2339162633965313,
      0.0,
      -1.2322582448848345,
      -1.219715835169012,
      -1.1906593678183284,
      -1.2248093350946754,
      -1.2038452849910197,
      -1.2410982626626779,
      -1.2236385335368396,
      -1.2319453016440247,
      -1.2578149646925187,
      -1.2152990114787416,
      -1.2174525110422794,
      -1.2363109409101063,
      -1.2174824951861205,
      -1.2479728547662512,
      -1.2253003340199558,
      -1.2066897233698208,
      -1.217810616232259,
      -1.221668729980008,
      -1.2224457101389525,
      -1.2287961212170353,
      -1.2350495130367989,
      -1.2378876776497376
    ],
    [
      -1.4919195958309772,
      -1.4187819515393802,
      -1.3922538031376859,
      -1.4013017688889957,
      -1.4253392132851144,
      -1.4449445530349079,
      -1.577392857706098,
      0.0,
      -1.4059364195925022,
      -1.415159054024925,
      -1.4284090221884564,
      -1.5732950629558142,
      -1.3595770020836737,
      -1.4356686887127093,
      -1.4521933786983952,
      -1.4660912866496072,
      -1.5057288397976178,
      -1.3791360272112516,
      -1.4817404631768474,
      -1.4624813011174569,
      -1.5165125295318427,
      -1.5257430214423142,
      -1.5750008903228543,
      -1.5392426217042063,
      -1.5078603592266486,
      -1.4922355238923908,
      -1.3870902625362687,
      -1.4313494047701203,
      -1.5703361298644134
    ],
    [
      -1.4556808443501172,
      -1.297824867753016,
      -1.2667799886525328,
      -1.2687388510368856,
      -1.2650477612294588,
      -1.3292852923385141,
      -1.4830930937931792,
      -1.340339233454929,
      0.0,
      -1.3605702610654637,
      -1.270136396403594,
      -1.5311064109394845,
      -1.2753721947560428,
      -1.2511153912603084,
      -1.3490665665193655,
      -1.3236278473596599,
      -1.364574052534353,
      -1.3120018186929656,
      -1.3919632452482544,
      -1.329696483038036,
      -1.4110445181306857,
      -1.3396498029474684,
      -1.460662210723455,
      -1.4552048145262595,
      -1.3812173775116425,
      -1.3442966693186835,
      -1.3056171803453873,
      -1.2774532095094746,
      -1.484174679427233
    ],
    [
      -1.4637043132624932,
      -1.4112721926516738,
      -1.397978841051225,
      -1.423710333727102,
      -1.4215762600249195,
      -1.405240567594267,
      -1.523396077908865,
      -1.445791699154937,
      -1.408524648552524,
      0.0,
      -1.3831275842957245,
      -1.5468800532679998,
      -1.4314192396157375,
      -1.4198910523940762,
      -1.4271098183762543,
      -1.4306674243379027,
      -1.4506852559854557,
      -1.4227199300214692,
      -1.4826009552284358,
      -1.4289571090181121,
      -1.4931390474373663,
      -1.5128391291951693,
      -1.5003418425627493,
      -1.4976190107205714,
      -1.4500746734147751,
      -1.447036340876983,
      -1.4113165219507908,
      -1.4326702247228562,
      -1.540247657086455
    ],
    [
      -1.311991705935249,
      -1.1941044870144284,
      -1.2437758122888494,
      -1.2234695432641733,
      -1.2778996893233128,
      -1.2642368644729511,
      -1.3886002021302466,
      -1.2874087011970494,
      -1.2178408924708728,
      -1.2729549436222705,
      0.0,
      -1.4399782849370233,
      -1.2399994179861809,
      -1.2830620853840202,
      -1.3421270256399354,
      -1.2813976267447145,
      -1.3004899941284804,
      -1.2610354244611548,
      -1.255738271488175,
      -1.2721466448164895,
      -1.353982871270089,
      -1.3711813275840259,
      -1.3627687555467491,
      -1.4061919234930522,
      -1.3465526810342432,
      -1.2733860757175557,
      -1.289127007594969,
      -1.3088441308592516,
      -1.417009560956088
    ],
    [
      -1.2613122830881698,
      -1.2457397005793316,
      -1.2281549186358731,
      -1.2435346554719717,
      -1.2236935752352578,
      -1.2598533345260443,
      -1.2465146823537958,
      -1.21291854483534,
      -1.264286907399541,
      -1.2351592524201038,
      -1.239744428172304,
      0.0,
      -1.205022033799828,
      -1.243717147353882,
      -1.24624468312422,
      -1.2802138984803337,
      -1.2548014488549346,
      -1.2326554469846729,
      -1.2129605711507376,
      -1.243535554805247,
      -1.2590996780041637,
      -1.2453509478120157,
      -1.2035482272259013,
      -1.2588364736451358,
      -1.2156570792121777,
      -1.2552235643088188,
      -1.1998488564410528,
      -1.2445525427760846,
      -1.2346575463564653
    ],
    [
      -1.1805771674065175,
      -0.9917387874693785,
      -1.0717656007149745,
      -1.026587350647349,
      -1.1323904613499207,
      -1.1697959904026576,
      -1.2742229695207583,
      -1.098012886621122,
      -1.078889202821269,
      -1.1717462426278824,
      -1.0801127832490214,
      -1.3023770179080185,
      0.0,
      -1.1151031414265724,
      -1.206541622561735,
      -1.0980743893484814,
      -1.177109003312879,
      -1.08982815573837,
      -1.1248555541656733,
      -1.1812035679010824,
      -1.2263098274633821,
      -1.2224986477548143,
      -1.2902657908505546,
      -1.2535227128652553,
      -1.2288030650019948,
      -1.1409441417394783,
      -1.0880736260330857,
      -1.102956139222306,
      -1.3312109586489447
    ],
    [
      -1.3557393255949337,
      -1.168680239681702,
      -1.131443653551315,
      -1.1350470635911685,
      -1.1772306697388883,
      -1.2163500590547627,
      -1.3565596406588756,
      -1.2696033996999267,
      -1.133086943113525,
      -1.2511674627001554,
      -1.16920806897175,
      -1.4434047088714534,
      -1.1996251332277348,
      0.0,
      -1.261616263906885,
      -1.2331190115602584,
      -1.268144810014853,
      -1.148171706970634,
      -1.2818783657034238,
      -1.2015846587396688,
      -1.2730689795814714,
      -1.3246674511752257,
      -1.3324356444157843,
      -1.270259172463027,
      -1.3014537417003629,
      -1.2280914122211453,
      -1.2111826547154274,
      -1.1430715741126525,
      -1.416860024306786
    ],
    [
      -1.1659042469927585,
      -1.1354602183554978,
      -1.0685976875112109,
      -1.050309583690873,
      -1.1164898269372774,
      -1.1066077421799907,
      -1.2213318595590104,
      -1.1454093330458763,
      -1.0520391851831057,
      -1.1176563234969263,
      -1.0693827377260903,
      -1.2296626661036862,
      -1.143540375045404,
      -1.0734847898839543,
      0.0,
      -1.1137196442388824,
      -1.1270977385120464,
      -1.1016380852103989,
      -1.1679219712748705,
      -1.0750669631841407,
      -1.1497684147479035,
      -1.1564203452941266,
      -1.1547309997086888,
      -1.1676750419720532,
      -1.1668508365818828,
      -1.1002777007034208,
      -1.1225014940396436,
      -1.0889141370674138,
      -1.1824014387434425
    ],
    [
      -1.2430591018995385,
      -1.146776680901276,
      -1.199219376166589,
      -1.1156908036780575,
      -1.2776827309687682,
      -1.2577764812605259,
      -1.3936481226442798,
      -1.3082630287409656,
      -1.2471012678265616,
      -1.2931539980647933,
      -1.2067169613405464,
      -1.4491692923308679,
      -1.1896024862455523,
      -1.275823218085329,
      -1.317564464738314,
      0.0,
      -1.270978328546874,
      -1.2293749416847888,
      -1.2474121058160301,
      -1.2961933389991154,
      -1.3330788118374355,
      -1.3403169140869122,
      -1.3776537311423072,
      -1.3527197250565959,
      -1.324798968545902,
      -1.2083559899089653,
      -1.302538731089722,
      -1.254278999317398,
      -1.4112427703595336
    ],
    [
      -1.535165579565807,
      -1.4587387146577437,
      -1.4722262129148604,
      -1.4370124936696633,
      -1.576105304889093,
      -1.545071649046243,
      -1.62567190029583,
      -1.6170409967896682,
      -1.4869144639562948,
      -1.4918973758954903,
      -1.4929534066647876,
      -1.7004589515303232,
      -1.527203839907717,
      -1.5071904371051243,
      -1.5579221281427074,
      -1.482237880498389,
      0.0,
      -1.5507663189221055,
      -1.548065501212791,
      -1.5466846753996928,
      -1.5429717485475798,
      -1.5845692698779241,
      -1.6027628160245484,
      -1.5995681952216758,
      -1.5572426651458653,
      -1.5047628513935705,
      -1.523966913152226,
      -1.5443230876608152,
      -1.660582404833934
    ],
    [
      -1.338417083755226,
      -1.160663448947794,
      -1.2448328308629806,
      -1.22266870274624,
      -1.2961559183769897,
      -1.2567010100088583,
      -1.3846210318782104,
      -1.2407324330375686,
      -1.252324453530556,
      -1.276915069416957,
      -1.2392481066485108,
      -1.4480356993954748,
      -1.2116388495419443,
      -1.194708762444231,
      -1.3137885800961355,
      -1.2781918141066972,
      -1.3318668316748752,
      0.0,
      -1.3307531632089018,
      -1.2687880114611492,
      -1.3353546009946717,
      -1.3605421198855407,
      -1.361672976467123,
      -1.4006794769980129,
      -1.3517889037227915,
      -1.2543342246476812,
      -1.27867955860863,
      -1.2521263074915174,
      -1.4508532004352162
    ],
    [
      -1.3438714014055877,
      -1.2915905008400688,
      -1.2539792453363077,
      -1.302194445149549,
      -1.249700561535872,
      -1.2827665303372953,
      -1.4291071507226207,
      -1.2801276374947943,
      -1.2959915258631671,
      -1.3198245104806474,
      -1.2140257419187155,
      -1.422671054977803,
      -1.2524683970381547,
      -1.301025598583497,
      -1.3444969092478651,
      -1.2988115741336703,
      -1.290182124209763,
      -1.31406701674053,
      0.0,
      -1.3457618910030016,
      -1.3725559513111718,
      -1.3505105598317062,
      -1.3984412583400785,
      -1.3357708237149974,
      -1.355217443758935,
      -1.2972387991073877,
      -1.257070722164502,
      -1.3729003199643448,
      -1.4328145215427763
    ],
    [
      -1.5925003578424817,
      -1.5457869918781861,
      -1.4697428907235235,
      -1.5390918197294774,
      -1.5283492904805966,
      -1.5596543945490629,
      -1.6284825764209314,
      -1.518795438146644,
      -1.4765127160695177,
      -1.5327770427785465,
      -1.47301592706821,
      -1.6241238391805168,
      -1.5312151864994212,
      -1.4413555577812815,
      -1.5025301562579434,
      -1.5342964824053387,
      -1.5230512179341915,
      -1.4802368000725052,
      -1.5681754607659788,
      0.0,
      -1.524551358387076,
      -1.5502094858165079,
      -1.5941275068574572,
      -1.5246670924346695,
      -1.5610686882540445,
      -1.5069178503438998,
      -1.5073080534402064,
      -1.4639739240939016,
      -1.619044294492962
    ],
    [
      -1.3998336436561152,
      -1.3613856765409518,
      -1.3206004575153294,
      -1.3425294169836166,
      -1.4097541472505173,
      -1.359044465869972,
      -1.4045982650544815,
      -1.3167214745852336,
      -1.3221223029439546,
      -1.310947141993999,
      -1.2940291418469156,
      -1.476826927891998,
      -1.3728895546466904,
      -1.3322767465574425,
      -1.3434261359163437,
      -1.346527064866287,
      -1.3156488524716186,
      -1.30708852062671,
      -1.4176354697858176,
      -1.339507864436891,
      0.0,
      -1.4080077876022308,
      -1.3628234989972214,
      -1.375396138293325,
      -1.354573946204041,
      -1.3433331933988193,
      -1.352637675090245,
      -1.3134052535709353,
      -1.4230482588886915
    ],
    [
      -1.4010069407631944,
      -1.3348908601073217,
      -1.3207313597220496,
      -1.3484617395422382,
      -1.2948266991071677,
      -1.388714792016318,
      -1.3923246925638704,
      -1.3696298427687554,
      -1.2925567904307613,
      -1.377079946974864,
      -1.3484664091753016,
      -1.4005530717705248,
      -1.3818491750694493,
      -1.3494110364318266,
      -1.3597683167072028,
      -1.3775044566665533,
      -1.3778468125707974,
      -1.3243896332987808,
      -1.3727276946532345,
      -1.3079521724677272,
      -1.3917662448362433,
      0.0,
      -1.4042668162201304,
      -1.3644355845718297,
      -1.39465070426187,
      -1.385644738007706,
      -1.4013885445429943,
      -1.3838834160762263,
      -1.405816700263082
    ],
    [
      -1.5900455816506882,
      -1.5051830352712205,
      -1.4884939540263225,
      -1.5187188380402303,
      -1.53116435938492,
      -1.512209187815918,
      -1.5248887315911381,
      -1.5374450165313398,
      -1.4826763569927925,
      -1.4335381428924718,
      -1.4693202943456054,
      -1.5293346672230947,
      -1.5282132811948856,
      -1.5168102744045935,
      -1.5040020154596452,
      -1.5195429732927206,
      -1.4926700315620458,
      -1.5166446968179808,
      -1.5124939304258003,
      -1.4961701341564466,
      -1.5079723541648626,
      -1.5558270157063037,
      0.0,
      -1.5082269009360971,
      -1.4638382241387196,
      -1.522586533918876,
      -1.4403700883780741,
      -1.5301965066927241,
      -1.527218468913606
    ],
    [
      -1.418189916878009,
      -1.3568723967706786,
      -1.2933732012645285,
      -1.3421296478434146,
      -1.3797473841947474,
      -1.400700194188499,
      -1.4183387139619372,
      -1.3764308187189997,
      -1.3801177118828964,
      -1.3681908582058928,
      -1.3412797620523436,
      -1.4687870834442567,
      -1.3936263119178034,
      -1.3487431062810336,
      -1.341789523614017,
      -1.3403237348639945,
      -1.3973122168658365,
      -1.3718277043109375,
      -1.3985783840801689,
      -1.2764752481239767,
      -1.3573516640175665,
      -1.4064431967942816,
      -1.4209054444375901,
      0.0,
      -1.3742163557983411,
      -1.3425042203845903,
      -1.3417930352972935,
      -1.3375967243357738,
      -1.4105028192713744
    ],
    [
      -1.5649955546518552,
      -1.5482860317440017,
      -1.5096115540346087,
      -1.5473893090131454,
      -1.6364759940308262,
      -1.5776364781673125,
      -1.62383530167773,
      -1.6252823572276056,
      -1.5713812414162691,
      -1.5162080712821833,
      -1.5382369021632154,
      -1.6629196787609224,
      -1.5746348005038069,
      -1.572762064453348,
      -1.5938388618332848,
      -1.586034163691064,
      -1.5953667737736985,
      -1.5214539801003748,
      -1.6119150058554201,
      -1.6063191666160004,
      -1.5997852907592744,
      -1.625014907845223,
      -1.591964334259052,
      -1.6166552344741374,
      0.0,
      -1.5496290030182938,
      -1.5933414077906365,
      -1.5222379743552719,
      -1.6046397096690361
    ],
    [
      -1.4379447568663686,
      -1.3681477368526958,
      -1.4266682426911366,
      -1.3811297758319667,
      -1.5040399595573317,
      -1.4837974723264638,
      -1.5425228135601392,
      -1.5359260855154198,
      -1.4382169615001716,
      -1.4840116686680598,
      -1.411160442708463,
      -1.6726865606118047,
      -1.4416230080322572,
      -1.4532791550608486,
      -1.4813779158463758,
      -1.3845330467394488,
      -1.4400959303214653,
      -1.3988624486740187,
      -1.5322541974429993,
      -1.496930074766157,
      -1.5088520094382278,
      -1.5421198078671665,
      -1.5389592707115614,
      -1.5556433239169518,
      -1.454888533639797,
      0.0,
      -1.4904664917338046,
      -1.4092325428692385,
      -1.6111610699138625
    ],
    [
      -1.3065550514406112,
      -1.265002729596304,
      -1.1828442088876725,
      -1.247759554919044,
      -1.1941543378181367,
      -1.3274885286858584,
      -1.4103147475471751,
      -1.2638985565055263,
      -1.2379350064879742,
      -1.285841403094249,
      -1.2680616845915784,
      -1.4237733167134468,
      -1.2086916672958217,
      -1.2695665844125101,
      -1.3357703839170207,
      -1.294784566200362,
      -1.2709920891168018,
      -1.2559300703674996,
      -1.2911214547442633,
      -1.2798730649005539,
      -1.3230987790238806,
      -1.3803750042102227,
      -1.3504508755352531,
      -1.367509700585021,
      -1.317200127744338,
      -1.2585978515602256,
      0.0,
      -1.280314893815571,
      -1.4353203375099348
    ],
    [
      -1.4429481395724595,
      -1.4193662066714916,
      -1.3772800195740926,
      -1.3894632490195393,
      -1.4820600857397104,
      -1.4507331703494255,
      -1.5705997034791592,
      -1.4463713930453594,
      -1.3679624122945955,
      -1.4534895472895535,
      -1.394573931791868,
      -1.642737116425584,
      -1.4318647572692877,
      -1.390304386789118,
      -1.44821619029404,
      -1.4378665255518321,
      -1.4699508797838743,
      -1.3826150923294809,
      -1.5070538400242635,
      -1.4399940497656631,
      -1.4552878042320598,
      -1.4973730353808121,
      -1.5661326938791913,
      -1.5114308446972338,
      -1.4796005690527703,
      -1.3733783750333004,
      -1.439735666632624,
      0.0,
      -1.5714480256806223
    ],
    [
      -1.4118710088026314,
      -1.3345343881536929,
      -1.3274196173374064,
      -1.3000207445056413,
      -1.3795910635845916,
      -1.312185392840956,
      -1.3575336383995305,
      -1.3730819906408893,
      -1.339364755969649,
      -1.3326833031912741,
      -1.3212676306344526,
      -1.3650764536895434,
      -1.367199280243028,
      -1.3468747259079772,
      -1.3501759791543342,
      -1.3665944377665458,
      -1.324987760102836,
      -1.3444973466116503,
      -1.3469740925839961,
      -1.3432467078634478,
      -1.4171499435565045,
      -1.341936679791944,
      -1.3744857307475054,
      -1.311371380359071,
      -1.355323209615196,
      -1.3853494152778507,
      -1.362363007251863,
      -1.3384962907467783,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.23986024855996657,
      0.1880245094976709,
      0.23697754569128082,
      0.19858611843524576,
      0.15728469196178518,
      0.11782971185139823,
      0.16266683400907422,
      0.17936848113324144,
      0.17248373294006303,
      0.20202143287944718,
      0.10862077926396418,
      0.19509301173736437,
      0.18327235230713979,
      0.1605147641476119,
      0.20654121741904619,
      0.18683529448524516,
      0.1893072787613852,
      0.17699270147261004,
      0.15627157090828336,
      0.14317558359227256,
      0.14497900468849623,
      0.12400580358356583,
      0.14195818337030963,
      0.17507161805007265,
      0.21654041557050863,
      0.18858122035834568,
      0.19168092927026237,
      0.11247359504010701
    ],
    [
      0.5833701315352409,
      0.0,
      0.5445618370833725,
      0.6533355544256378,
      0.4945252184755957,
      0.4999114071759567,
      0.36065754885360923,
      0.48147677138910483,
      0.5421489994381126,
      0.5038063040480654,
      0.5844848369850519,
      0.2978606193356459,
      0.6469859029250009,
      0.524221178438103,
      0.45024135379725316,
      0.5365722190315487,
      0.5090911393570721,
      0.6171493968197441,
      0.4171039265202281,
      0.4636340066237812,
      0.42730628994260367,
      0.4604905216873567,
      0.42799175988918225,
      0.3859348678542007,
      0.45405465607538487,
      0.5803608131875488,
      0.5311547078627212,
      0.5094180141713569,
      0.33798282860576956
    ],
    [
      0.45126373654942165,
      0.5426487461584466,
      0.0,
      0.5350287957466973,
      0.5200127531709176,
      0.47908328507490827,
      0.35086821250727485,
      0.46252783350066107,
      0.5407961582228793,
      0.45609209830483555,
      0.5330116562340108,
      0.2986594119419095,
      0.5107039280112069,
      0.5134396297759534,
      0.4438892517558348,
      0.5238993041892541,
      0.4398858186186543,
      0.4772468985001961,
      0.415933602010639,
      0.49100005371617983,
      0.40104406998460607,
      0.3683315820054289,
      0.36251654615876583,
      0.42675307367397997,
      0.45284313865314396,
      0.4704586799905086,
      0.5363564090046351,
      0.5366115052579072,
      0.2786796159309548
    ],
    [
      0.41924045070986504,
      0.5187157361950272,
      0.4460476838422902,
      0.0,
      0.35794043356999605,
      0.3917454681455852,
      0.25450526637703175,
      0.35560830486964123,
      0.4203840243487831,
      0.3296836564585166,
      0.46361116454468543,
      0.22908818289717825,
      0.46549715757934185,
      0.3903975903760004,
      0.3579414138943513,
      0.46225386015750236,
      0.36597090423905665,
      0.41988055521238254,
      0.34334300606911716,
      0.3475064802893473,
      0.30761135677522766,
      0.3103521090985413,
      0.26672585420285433,
      0.26677177938216934,
      0.32751850053528364,
      0.3693686364341471,
      0.37844970490202234,
      0.41837171600306067,
      0.22578560507302536
    ],
    [
      0.3895647618791771,
      0.40835498352131494,
      0.43466815008537996,
      0.41394769926427255,
      0.0,
      0.37150012492052964,
      0.2835423593009274,
      0.37560484833923824,
      0.4604654917693356,
      0.40174736385153764,
      0.41628063138882165,
      0.2634415522834881,
      0.4182558955070421,
      0.4148242714231858,
      0.3591655788556194,
      0.41673776557885156,
      0.3618268903310826,
      0.3888865357177309,
      0.4201221034908931,
      0.3666124672071256,
      0.2805295334199618,
      0.3445273464671299,
      0.2760403702767258,
      0.3026403139114886,
      0.30485100018249356,
      0.37847753898552283,
      0.44437015630608556,
      0.34832733624238466,
      0.2250986821817058
    ],
    [
      0.3819312008848439,
      0.5482495996683885,
      0.5629620036736369,
      0.5341637833571102,
      0.47360788214094174,
      0.0,
      0.3327032344873362,
      0.47196123815119506,
      0.4975779579659385,
      0.46754208034091427,
      0.5582277369996933,
      0.2665817491788438,
      0.5530907583890341,
      0.5046491500404664,
      0.42173232778755954,
      0.5277056449135984,
      0.39568058795345507,
      0.5533797610467155,
      0.4742906654349439,
      0.476375338083048,
      0.40845423623794286,
      0.41435087586920294,
      0.35001996480464737,
      0.39801344113310244,
      0.425198201826237,
      0.4583580812042971,
      0.47233027113731896,
      0.4826172670515465,
      0.3181275755355675
    ],
    [
      0.3099392574195683,
      0.3420821863811758,
      0.3520270598152766,
      0.35748725452054453,
      0.35899761703687716,
      0.3433848053334527,
      0.0,
      0.3450428238451495,
      0.35758523356097194,
      0.3866417009116556,
      0.3524917336353086,
      0.37345578373896426,
      0.3362028060673061,
      0.3536625351931444,
      0.3453557670859593,
      0.31948610403746525,
      0.36200205725124235,
      0.35984855768770463,
      0.3409901278198777,
      0.3598185735438635,
      0.3293282139637328,
      0.35200073471002824,
      0.37061134536016316,
      0.35949045249772493,
      0.355632338749976,
      0.35485535859103146,
      0.3485049475129487,
      0.3422515556931851,
      0.3394133910802464
    ],
    [
      0.376796135027734,
      0.449933779319331,
      0.47646192772102536,
      0.4674139619697155,
      0.44337651757359686,
      0.42377117782380336,
      0.2913228731526132,
      0.0,
      0.4627793112662091,
      0.4535566768337862,
      0.4403067086702548,
      0.295420667902897,
      0.5091387287750375,
      0.433047042146002,
      0.4165223521603161,
      0.40262444420910404,
      0.3629868910610934,
      0.4895797036474596,
      0.38697526768186385,
      0.40623442974125434,
      0.35220320132686855,
      0.342972709416397,
      0.29371484053585695,
      0.3294731091545049,
      0.3608553716320626,
      0.37648020696632045,
      0.4816254683224426,
      0.43736632608859094,
      0.29837960099429783
    ],
    [
      0.3361972474192052,
      0.49405322401630647,
      0.5250981031167896,
      0.5231392407324369,
      0.5268303305398636,
      0.46259279943080833,
      0.3087849979761432,
      0.45153885831439355,
      0.0,
      0.4313078307038587,
      0.5217416953657283,
      0.2607716808298379,
      0.5165058970132796,
      0.5407627005090141,
      0.4428115252499569,
      0.46825024440966256,
      0.4273040392349694,
      0.47987627307635683,
      0.399914846521068,
      0.46218160873128644,
      0.3808335736386368,
      0.45222828882185406,
      0.3312158810458674,
      0.33667327724306295,
      0.4106607142576799,
      0.4475814224506389,
      0.4862609114239351,
      0.5144248822598478,
      0.3077034123420894
    ],
    [
      0.3568646154012285,
      0.40929673601204786,
      0.4225900876124966,
      0.3968585949366197,
      0.3989926686388021,
      0.4153283610694547,
      0.29717285075485655,
      0.37477722950878456,
      0.41204428011119765,
      0.0,
      0.4374413443679972,
      0.27368887539572184,
      0.3891496890479842,
      0.40067787626964546,
      0.3934591102874674,
      0.38990150432581894,
      0.36988367267826594,
      0.3978489986422524,
      0.33796797343528584,
      0.39161181964560954,
      0.3274298812263554,
      0.3077297994685524,
      0.3202270861009724,
      0.3229499179431503,
      0.3704942552489465,
      0.3735325877867386,
      0.40925240671293084,
      0.38789870394086545,
      0.2803212715772667
    ],
    [
      0.4034823389723945,
      0.5213695578932152,
      0.4716982326187942,
      0.4920045016434702,
      0.4375743555843308,
      0.4512371804346924,
      0.32687384277739695,
      0.4280653437105941,
      0.49763315243677075,
      0.4425191012853731,
      0.0,
      0.27549575997062026,
      0.4754746269214627,
      0.4324119595236233,
      0.3733470192677082,
      0.4340764181629291,
      0.4149840507791631,
      0.45443862044648875,
      0.4597357734194685,
      0.443327400091154,
      0.36149117363755456,
      0.34429271732361766,
      0.3527052893608944,
      0.3092821214145913,
      0.36892136387340035,
      0.44208796919008786,
      0.42634703731267454,
      0.406629914048392,
      0.29846448395155556
    ],
    [
      0.21837177796812912,
      0.23394436047696732,
      0.2515291424204258,
      0.23614940558432718,
      0.2559904858210411,
      0.2198307265302546,
      0.2331693787025031,
      0.266765516220959,
      0.21539715365675782,
      0.24452480863619508,
      0.2399396328839949,
      0.0,
      0.2746620272564708,
      0.23596691370241696,
      0.23343937793207892,
      0.19947016257596517,
      0.2248826122013643,
      0.24702861407162602,
      0.2667234899055613,
      0.236148506251052,
      0.22058438305213524,
      0.2343331132442832,
      0.27613583383039764,
      0.22084758741116306,
      0.2640269818441212,
      0.22446049674748014,
      0.27983520461524614,
      0.23513151828021428,
      0.2450265146998336
    ],
    [
      0.3870614691902672,
      0.5758998491274062,
      0.4958730358818102,
      0.5410512859494356,
      0.435248175246864,
      0.3978426461941271,
      0.2934156670760264,
      0.46962574997566264,
      0.4887494337755156,
      0.3958923939689023,
      0.4875258533477633,
      0.2652616186887662,
      0.0,
      0.4525354951702123,
      0.3610970140350498,
      0.46956424724830326,
      0.39052963328390566,
      0.4778104808584147,
      0.4427830824311114,
      0.3864350686957023,
      0.3413288091334026,
      0.3451399888419704,
      0.2773728457462301,
      0.3141159237315294,
      0.33883557159478994,
      0.4266944948573064,
      0.47956501056369905,
      0.46468249737447875,
      0.23642767794784003
    ],
    [
      0.39061201063232365,
      0.5776710965455554,
      0.6149076826759423,
      0.6113042726360889,
      0.569120666488369,
      0.5300012771724947,
      0.3897916955683818,
      0.4767479365273306,
      0.6132643931137323,
      0.4951838735271019,
      0.5771432672555075,
      0.3029466273558039,
      0.5467262029995226,
      0.0,
      0.4847350723203723,
      0.513232324666999,
      0.47820652621240445,
      0.5981796292566233,
      0.4644729705238335,
      0.5447666774875886,
      0.473282356645786,
      0.4216838850520317,
      0.41391569181147303,
      0.47609216376423036,
      0.4448975945268945,
      0.5182599240061121,
      0.53516868151183,
      0.6032797621146049,
      0.32949131192047143
    ],
    [
      0.37690034839481257,
      0.40734437703207327,
      0.4742069078763602,
      0.4924950116966982,
      0.4263147684502937,
      0.4361968532075804,
      0.32147273582856073,
      0.39739526234169475,
      0.4907654102044654,
      0.42514827189064475,
      0.4734218576614808,
      0.31314192928388485,
      0.399264220342167,
      0.46931980550361674,
      0.0,
      0.4290849511486887,
      0.41570685687552467,
      0.44116651017717223,
      0.37488262411270057,
      0.46773763220343034,
      0.3930361806396676,
      0.38638425009344446,
      0.38807359567888233,
      0.3751295534155179,
      0.37595375880568827,
      0.4425268946841503,
      0.4203031013479275,
      0.4538904583201573,
      0.3604031566441286
    ],
    [
      0.48972546215943114,
      0.5860078831576936,
      0.5335651878923806,
      0.6170937603809121,
      0.45510183309020147,
      0.47500808279844375,
      0.3391364414146898,
      0.4245215353180041,
      0.485683296232408,
      0.4396305659941764,
      0.5260676027184232,
      0.28361527172810175,
      0.5431820778134173,
      0.4569613459736406,
      0.4152200993206556,
      0.0,
      0.46180623551209554,
      0.5034096223741809,
      0.4853724582429395,
      0.43659122505985426,
      0.39970575222153415,
      0.3924676499720574,
      0.3551308329166625,
      0.38006483900237376,
      0.40798559551306757,
      0.5244285741500043,
      0.43024583296924757,
      0.4785055647415717,
      0.32154179369943603
    ],
    [
      0.48429032900127256,
      0.5607171939093358,
      0.5472296956522191,
      0.5824434148974162,
      0.4433506036779866,
      0.4743842595208365,
      0.3937840082712496,
      0.4024149117774114,
      0.5325414446107848,
      0.5275585326715893,
      0.526502501902292,
      0.31899695703675635,
      0.49225206865936255,
      0.5122654714619552,
      0.4615337804243722,
      0.5372180280686907,
      0.0,
      0.4686895896449741,
      0.4713904073542885,
      0.4727712331673868,
      0.4764841600194998,
      0.43488663868915545,
      0.4166930925425312,
      0.41988771334540376,
      0.4622132434212143,
      0.5146930571735091,
      0.4954889954148536,
      0.4751328209062644,
      0.35887350373314564
    ],
    [
      0.4038324252281298,
      0.5815860600355618,
      0.49741667812037527,
      0.5195808062371159,
      0.4460935906063661,
      0.4855484989744976,
      0.35762847710514545,
      0.5015170759457872,
      0.48992505545279985,
      0.46533443956639875,
      0.503001402334845,
      0.294213809587881,
      0.5306106594414115,
      0.5475407465391249,
      0.4284609288872203,
      0.4640576948766586,
      0.41038267730848066,
      0.0,
      0.41149634577445404,
      0.4734614975222067,
      0.4068949079886841,
      0.3817073890978151,
      0.3805765325162329,
      0.34157003198534297,
      0.39046060526056436,
      0.48791528433567466,
      0.4635699503747259,
      0.4901232014918384,
      0.2913963085481397
    ],
    [
      0.3589204832048709,
      0.41120138377038984,
      0.44881263927415094,
      0.40059743946090953,
      0.45309132307458655,
      0.42002535427316334,
      0.27368473388783787,
      0.4226642471156643,
      0.4068003587472915,
      0.3829673741298112,
      0.4887661426917431,
      0.2801208296326556,
      0.4503234875723039,
      0.4017662860269615,
      0.35829497536259347,
      0.4039803104767883,
      0.4126097604006955,
      0.38872486786992866,
      0.0,
      0.357029993607457,
      0.3302359332992868,
      0.3522813247787524,
      0.3043506262703801,
      0.36702106089546116,
      0.34757444085152356,
      0.4055530855030709,
      0.4457211624459565,
      0.3298915646461138,
      0.2699773630676823
    ],
    [
      0.3233636727832916,
      0.37007703874758713,
      0.4461211399022498,
      0.37677221089629587,
      0.3875147401451766,
      0.3562096360767104,
      0.28738145420484185,
      0.3970685924791293,
      0.43935131455625553,
      0.3830869878472267,
      0.4428481035575633,
      0.2917401914452564,
      0.38464884412635203,
      0.4745084728444917,
      0.41333387436782987,
      0.3815675482204346,
      0.39281281269158175,
      0.435627230553268,
      0.34768856985979446,
      0.0,
      0.39131267223869726,
      0.3656545448092654,
      0.3217365237683161,
      0.3911969381911038,
      0.3547953423717287,
      0.40894618028187346,
      0.4085559771855669,
      0.45189010653187167,
      0.2968197361328113
    ],
    [
      0.33926677801200067,
      0.3777147451271641,
      0.41849996415278645,
      0.39657100468449924,
      0.32934627441759856,
      0.3800559557981438,
      0.33450215661363436,
      0.42237894708288226,
      0.41697811872416124,
      0.42815327967411676,
      0.4450712798212002,
      0.26227349377611775,
      0.36621086702142547,
      0.4068236751106733,
      0.3956742857517721,
      0.3925733568018288,
      0.4234515691964973,
      0.43201190104140585,
      0.3214649518822983,
      0.3995925572312249,
      0.0,
      0.33109263406588507,
      0.3762769226708944,
      0.36370428337479077,
      0.3845264754640749,
      0.39576722826929656,
      0.38646274657787094,
      0.42569516809718055,
      0.3160521627794244
    ],
    [
      0.3288581467566969,
      0.3949742274125696,
      0.40913372779784174,
      0.3814033479776531,
      0.4350383884127236,
      0.34115029550357323,
      0.33754039495602095,
      0.3602352447511359,
      0.43730829708913,
      0.3527851405450273,
      0.38139867834458974,
      0.3293120157493665,
      0.34801591245044206,
      0.38045405108806474,
      0.37009677081268855,
      0.352360630853338,
      0.3520182749490939,
      0.40547545422111053,
      0.3571373928666568,
      0.4219129150521641,
      0.338098842683648,
      0.0,
      0.32559827129976093,
      0.36542950294806165,
      0.3352143832580212,
      0.3442203495121854,
      0.328476542976897,
      0.345981671443665,
      0.3240483872568094
    ],
    [
      0.26155796840984724,
      0.3464205147893149,
      0.363109596034213,
      0.3328847120203051,
      0.32043919067561544,
      0.3393943622446174,
      0.3267148184693973,
      0.31415853352919565,
      0.3689271930677429,
      0.4180654071680636,
      0.38228325571493005,
      0.3222688828374407,
      0.3233902688656498,
      0.33479327565594197,
      0.34760153460089027,
      0.33206057676781486,
      0.35893351849848965,
      0.3349588532425547,
      0.3391096196347352,
      0.35543341590408883,
      0.3436311958956728,
      0.2957765343542318,
      0.0,
      0.3433766491244383,
      0.3877653259218159,
      0.32901701614165946,
      0.41123346168246133,
      0.32140704336781134,
      0.32438508114692954
    ],
    [
      0.302487578697205,
      0.3638050988045354,
      0.4273042943106855,
      0.3785478477317994,
      0.3409301113804666,
      0.3199773013867151,
      0.30233878161327676,
      0.34424667685621424,
      0.34055978369231754,
      0.3524866373693212,
      0.37939773352287043,
      0.2518904121309573,
      0.3270511836574106,
      0.37193438929418043,
      0.37888797196119706,
      0.3803537607112195,
      0.3233652787093775,
      0.3488497912642765,
      0.3220991114950451,
      0.44420224745123726,
      0.3633258315576475,
      0.31423429878093234,
      0.29977205113762384,
      0.0,
      0.3464611397768729,
      0.3781732751906237,
      0.37888446027792044,
      0.3830807712394402,
      0.31017467630383955
    ],
    [
      0.39437559301317426,
      0.4110851159210278,
      0.44975959363042084,
      0.4119818386518841,
      0.3228951536342033,
      0.38173466949771706,
      0.33553584598729946,
      0.3340887904374239,
      0.3879899062487604,
      0.44316307638284624,
      0.4211342455018141,
      0.2964514689041071,
      0.3847363471612226,
      0.3866090832116815,
      0.3655322858317447,
      0.37333698397396553,
      0.364004373891331,
      0.4379171675646547,
      0.3474561418096094,
      0.35305198104902913,
      0.35958585690575506,
      0.33435623981980656,
      0.3674068134059776,
      0.3427159131908921,
      0.0,
      0.40974214464673575,
      0.366029739874393,
      0.43713317330975765,
      0.3547314379959934
    ],
    [
      0.5040555078817675,
      0.5738525278954403,
      0.5153320220569995,
      0.5608704889161693,
      0.4379603051908043,
      0.4582027924216723,
      0.3994774511879968,
      0.40607417923271627,
      0.5037833032479644,
      0.45798859608007625,
      0.5308398220396731,
      0.2693137041363314,
      0.5003772567158788,
      0.48872110968728744,
      0.46062234890176024,
      0.5574672180086873,
      0.5019043344266707,
      0.5431378160741174,
      0.40974606730513674,
      0.4450701899819791,
      0.4331482553099082,
      0.3998804568809695,
      0.4030409940365747,
      0.3863569408311842,
      0.48711173110833905,
      0.0,
      0.4515337730143314,
      0.5327677218788975,
      0.3308391948342735
    ],
    [
      0.42387614717794553,
      0.46542846902225277,
      0.5475869897308843,
      0.4826716436995129,
      0.53627686080042,
      0.40294266993269834,
      0.32011645107138165,
      0.46653264211303047,
      0.4924961921305826,
      0.44458979552430766,
      0.46236951402697835,
      0.30665788190511,
      0.5217395313227351,
      0.46086461420604663,
      0.394660814701536,
      0.4356466324181947,
      0.4594391095017549,
      0.47450112825105717,
      0.43930974387429345,
      0.4505581337180029,
      0.40733241959467614,
      0.350056194408334,
      0.37998032308330365,
      0.3629214980335358,
      0.41323107087421884,
      0.47183334705833113,
      0.0,
      0.4501163048029857,
      0.29511086110862195
    ],
    [
      0.44398455231920275,
      0.46756648522017064,
      0.5096526723175696,
      0.4974694428721229,
      0.40487260615195186,
      0.43619952154223673,
      0.316332988412503,
      0.4405612988463028,
      0.5189702795970668,
      0.43344314460210875,
      0.4923587600997943,
      0.24419557546607829,
      0.4550679346223745,
      0.4966283051025442,
      0.4387165015976222,
      0.4490661663398301,
      0.41698181210778795,
      0.5043175995621814,
      0.3798788518673988,
      0.4469386421259991,
      0.4316448876596024,
      0.3895596565108501,
      0.32079999801247094,
      0.3755018471944285,
      0.407332122838892,
      0.5135543168583618,
      0.44719702525903826,
      0.0,
      0.31548466621104
    ],
    [
      0.2880399376297782,
      0.36537655827871673,
      0.37249132909500315,
      0.3998902019267683,
      0.320319882847818,
      0.38772555359145366,
      0.34237730803287914,
      0.32682895579152027,
      0.3605461904627605,
      0.36722764324113544,
      0.37864331579795696,
      0.33483449274286614,
      0.33271166618938164,
      0.3530362205244324,
      0.3497349672780754,
      0.3333165086658638,
      0.37492318632957367,
      0.35541359982075926,
      0.35293685384841345,
      0.3566642385689618,
      0.2827610028759051,
      0.3579742666404655,
      0.32542521568490423,
      0.3885395660733386,
      0.34458773681721366,
      0.31456153115455887,
      0.3375479391805465,
      0.36141465568563125,
      0.0
    ]
  ],
  "row_avgs": [
    0.17346495110663446,
    0.4937797432690803,
    0.45784234980892186,
    0.36465416436364395,
    0.3668004197388947,
    0.4546386648320545,
    0.3503067972515908,
    0.4021899796828729,
    0.43740162523837783,
    0.3701925785052612,
    0.4123561180733005,
    0.24051127594724875,
    0.4085130364263034,
    0.49982448479712177,
    0.41527383156647907,
    0.451706300798843,
    0.473738844891277,
    0.44449653861226707,
    0.38117816258350107,
    0.38295108774344905,
    0.38100688511503034,
    0.36370275924889056,
    0.3421106359202096,
    0.34910080343947175,
    0.3776621779090439,
    0.4624812896172717,
    0.43281596371759756,
    0.4283670593327689,
    0.3487803758848815
  ],
  "col_avgs": [
    0.38315107372352947,
    0.44804420653567795,
    0.45523828192460897,
    0.4582191095895607,
    0.4117981734028091,
    0.40136677707278257,
    0.31530934594436477,
    0.3958248636421393,
    0.44145786481656923,
    0.4108075183749163,
    0.4517261396533721,
    0.2825114366123769,
    0.43560960564969964,
    0.42578912668234103,
    0.38652225244203914,
    0.41758592243778764,
    0.39137178278878315,
    0.4365950869788116,
    0.38240423845229526,
    0.40760499655922494,
    0.36113573433811685,
    0.3546330269856019,
    0.3360021752047183,
    0.3498004482175387,
    0.3749669385476329,
    0.413516032533153,
    0.42032331593316324,
    0.421990076937853,
    0.29654335344082167
  ],
  "combined_avgs": [
    0.27830801241508196,
    0.4709119749023791,
    0.4565403158667654,
    0.4114366369766023,
    0.38929929657085194,
    0.42800272095241854,
    0.33280807159797776,
    0.39900742166250613,
    0.43942974502747356,
    0.3905000484400888,
    0.43204112886333634,
    0.2615113562798128,
    0.42206132103800154,
    0.46280680573973143,
    0.4008980420042591,
    0.43464611161831535,
    0.4325553138400301,
    0.44054581279553934,
    0.38179120051789817,
    0.395278042151337,
    0.3710713097265736,
    0.35916789311724623,
    0.33905640556246397,
    0.3494506258285052,
    0.37631455822833837,
    0.43799866107521235,
    0.42656963982538043,
    0.42517856813531096,
    0.32266186466285157
  ],
  "gppm": [
    597.6501626249249,
    602.4599863518737,
    597.5043446383181,
    594.9338228578024,
    612.0640094493772,
    622.4622189786725,
    665.8552676944045,
    623.9405637708105,
    604.266927667778,
    617.5520347174473,
    599.9324456820685,
    675.3997303335487,
    608.9083129391058,
    613.9676293765226,
    633.6236763612961,
    616.0942498490514,
    625.9935776333963,
    607.8056613691598,
    630.2511269200546,
    617.5486792914679,
    642.1891248794911,
    643.692426482054,
    653.2663709214538,
    645.838285625506,
    632.6645113766432,
    616.8496417455145,
    614.409854169982,
    611.4458622671746,
    672.3937258985172
  ],
  "gppm_normalized": [
    1.4012111801133023,
    1.3882977496789806,
    1.3738302486292004,
    1.374192183046165,
    1.4001070715829678,
    1.4280055765148525,
    1.5303513484119715,
    1.4286115095384988,
    1.3928742198976802,
    1.418492586781897,
    1.3763940791982059,
    1.5459202299352675,
    1.4012331538767888,
    1.4123867456598014,
    1.4584703935306826,
    1.4169829919688353,
    1.4351147282644017,
    1.394113691771277,
    1.4489977765575137,
    1.419567137011914,
    1.47170006558221,
    1.4822418107650683,
    1.498302511723842,
    1.4791101591483626,
    1.4538241255331048,
    1.4180794647148889,
    1.403803308314989,
    1.4044027635766727,
    1.543015985571739
  ],
  "token_counts": [
    548,
    467,
    449,
    492,
    391,
    412,
    438,
    397,
    480,
    439,
    411,
    388,
    456,
    452,
    464,
    438,
    403,
    409,
    446,
    444,
    401,
    457,
    413,
    395,
    434,
    435,
    372,
    423,
    413,
    819,
    418,
    459,
    508,
    546,
    421,
    391,
    416,
    442,
    444,
    460,
    526,
    464,
    390,
    413,
    420,
    381,
    431,
    456,
    441,
    417,
    437,
    387,
    449,
    412,
    382,
    431,
    441,
    393
  ],
  "response_lengths": [
    4041,
    2391,
    2604,
    2895,
    3051,
    2366,
    2268,
    2289,
    2512,
    2499,
    2604,
    2969,
    2545,
    2151,
    2348,
    2446,
    2109,
    2470,
    2596,
    2476,
    2305,
    2478,
    2244,
    2556,
    2344,
    2094,
    2354,
    2468,
    2304
  ]
}