{
  "example_idx": 1,
  "reference": "A THEORY OF REPRESENTATION LEARNING IN NEURAL NETWORKS GIVES A DEEP GENERALISATION OF KERNEL METHODS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe successes of modern deep machine learning methods are founded on their ability to transform inputs across multiple layers to build good high-level representations. It is therefore critical to understand this process of representation learning. However, standard theoretical approaches (formally NNGPs) involving infinite width limits eliminate representation learning. We therefore develop a new infinite width limit, the Bayesian representation learning limit, that exhibits representation learning mirroring that in finite-width models, yet at the same time, retains some of the simplicity of standard infinite-width limits. In particular, we show that Deep Gaussian processes (DGPs) in the Bayesian representation learning limit have exactly multivariate Gaussian posteriors, and the posterior covariances can be obtained by optimizing an interpretable objective combining a loglikelihood to improve performance with a series of KL-divergences which keep the posteriors close to the prior. We confirm these results experimentally in wide but finite DGPs. Next, we introduce the possibility of using this limit and objective as a flexible, deep generalisation of kernel methods, that we call deep kernel machines (DKMs). Like most naive kernel methods, DKMs scale cubically in the number of datapoints. We therefore use methods from the Gaussian process inducing point literature to develop a sparse DKM that scales linearly in the number of datapoints. Finally, we extend these approaches to NNs (which have non-Gaussian posteriors) in the Appendices.\n\n1\n\nINTRODUCTION\n\nThe successes of modern machine learning methods from neural networks (NNs) to deep Gaussian processes (DGPs Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) is based on their ability to use depth to transform the input into high-level representations that are good for solving difficult tasks (Bengio et al., 2013; LeCun et al., 2015). However, theoretical approaches using infinite limits to understand deep models struggle to capture representation learning. In particular, there are two broad families of infinite limit, and while they both use kernel-matrix-like objects they are ultimately very different. The neural network Gaussian process (NNGP Neal, 1996; Lee et al., 2017; Matthews et al., 2018) applies to Bayesian models like Bayesian neural networks (BNNs) and DGPs and describes the representations at each layer (formally, the NNGP kernel is raw second moment of the activities). In contrast, the neural tangent kernel (NTK Jacot et al., 2018) is a very different quantity that involves gradients, and describes how predictions at all datapoints change if we do a gradient update on a single datapoint. As such, the NNGP and NTK are suited to asking very different theoretical questions. For instance, the NNGP is better suited to understanding the transformation of representations across layers, while the NTK is better suited to understanding how predictions change through NN training.\n\nWhile challenges surrounding representation learning have recently been addressed in the NTK setting Yang & Hu (2020), we are the first to address this challenge in the NNGP setting.\n\nAt the same time, kernel methods (Smola & Sch ̈olkopf, 1998; Shawe-Taylor & Cristianini, 2004; Hofmann et al., 2008) were a leading machine learning approach prior to the deep learning revolution Krizhevsky et al. (2012). However, kernel methods were eclipsed by deep NNs because depth\n\n1\n\ngives NNs the flexibility to learn a good top-layer representation (Aitchison, 2020). In contrast, in a standard kernel method, the kernel (or equivalently the representation) is highly inflexible — there are usually a few tunable hyperparameters, but nothing that approaches the enormous flexibility of the top-layer representation in a deep model. There is therefore a need to develop flexible, deep generalisations of kernel method. Remarkably, our advances in understanding representation learning in DGPs give such a flexible, deep kernel method.\n\n2 CONTRIBUTIONS\n\n• We present a new infinite width limit, the Bayesian representation learning limit, that retains representation learning in deep Bayesian models including DGPs. The key insight is that as the width goes to infinity, the prior becomes stronger, and eventually overwhelms the likelihood. We can fix this by rescaling the likelihood to match the prior. This rescaling can be understood in a Bayesian context as copying the labels (Sec. 4.3).\n\n• We show that in the Bayesian representation learning limit, DGP posteriors are exactly λ, is the activation\n\nzero-mean multivariate Gaussian, P (cid:0)f l of the λth feature in layer l for all inputs (Sec. 4.4 and Appendix D).\n\nλ|X, y(cid:1) = N (cid:0)f l\n\n(cid:1) where f l\n\nλ; 0, Gl\n\n• We show that the posterior covariances can be obtained by optimizing the “deep kernel\n\nmachine objective”,\n\nL(G1, . . . , GL) = log P (Y|GL) − (cid:80)L\n\nl=1νl DKL (N (0, Gl)∥N (0, K(Gl−1))) ,\n\nwhere Gl are the posterior covariances, K(Gl−1) are the kernel matrices, and νl accounts for any differences in layer width (Sec. 4.3).\n\n• We give an interpretation of this objective, with log P (Y|GL) encouraging improved performance, while the KL-divergence terms act as a regulariser, keeping posteriors, N (0, Gl), close to the prior, N (0, K(Gl−1)) (Sec. 4.5).\n\n• We introduce a sparse DKM, which takes inspiration GP inducing point literature to obtain a practical, scalable method that is linear in the number of datapoints. In contrast, naively computing/optimizing the DKM objective is cubic in the number of datapoints (as with most other naive kernel methods; Sec. 4.7).\n\n• We extend these results to BNNs (which have non-Gaussian posteriors) in Appendix A.\n\n3 RELATED WORK\n\nOur work is focused on DGPs and gives new results such as the extremely simple multivariate Gaussian form for DGP true posteriors. As such, our work is very different from previous work on NNs, where such results are not available. There are at least three families of such work. First, there is recent work on representation learning in the very different NTK setting (Jacot et al., 2018; Yang, 2019; Yang & Hu, 2020) (see Sec. 1). In contrast, here we focus on NNGPs (Neal, 1996; Williams, 1996; Lee et al., 2017; Matthews et al., 2018; Novak et al., 2018; Garriga-Alonso et al., 2018; Jacot et al., 2018), where the challenge of representation learning has yet to be addressed. Second, there is a body of work using methods from physics to understand representation learning in neural networks (Antognini, 2019; Dyer & Gur-Ari, 2019; Hanin & Nica, 2019; Aitchison, 2020; Li & Sompolinsky, 2020; Yaida, 2020; Naveh et al., 2020; Zavatone-Veth et al., 2021; Zavatone-Veth & Pehlevan, 2021; Roberts et al., 2021; Naveh & Ringel, 2021; Halverson et al., 2021). This work is focuses on perturbational, rather than variational methods. Third, there is a body of theoretical work including (Mei et al., 2018; Nguyen, 2019; Sirignano & Spiliopoulos, 2020a;b; Nguyen & Pham, 2020) which establishes properties such as convergence to the global optimum. This work is focused on two-layer (or one-hidden layer network) networks, and like the NTK, considers learning under SGD rather than Bayesian posteriors.\n\nAnother related line of work uses kernels to give a closed-form expression for the weights of a neural network, based on a greedy, layerwise objective (Wu et al., 2022). This work differs in that it uses the HSIC objective, and therefore does not have a link to DGPs or Bayesian neural networks, and in that it uses a greedy-layerwise objective, rather than end-to-end gradient descent.\n\n2\n\nLayer 1\n\nLayer 2\n\nLayer 3\n\nX\n\nX\n\nX\n\nG0\n\nG0\n\nF1\n\nF1\n\nG1\n\nG1\n\nF2\n\nF2\n\nG2\n\nG2\n\nF3\n\nF3\n\nG3\n\nG3\n\nY\n\nY\n\nY\n\nFigure 1: The graphical model structure for each of our generative models for L = 3. Top. The standard model (Eq. 1), written purely in terms of features, Fl. Middle. The standard model, including Gram matrices as random variables (Eq. 5) Bottom. Integrating out the activations, Fl,\n\n4 RESULTS\n\nWe start by defining a DGP, which contains Bayesian NN (BNNs) as a special case (Appendix A). This model maps from inputs, X ∈ RP ×ν0, to outputs, Y ∈ RP ×νL+1, where P is the number of input points, ν0 is the number of input features, and νL+1 is the number of output features. The model has L intermediate layers, indexed l ∈ {1, . . . , L}, and at each intermediate layer there are Nl features, Fl ∈ RP ×Nl. Both Fl and Y can be written as a stack of vectors, )\n\nY = (y1 y2\n\nFl = (f l\n\n· · · yνL+1 ),\n\n· · ·\n\nf l\n\n1\n\n2\n\nf l Nl\n\nλ ∈ RP gives the value of one feature and yλ ∈ RP gives the value of one output for all where f l P input points. The features, F1, . . . , FL, and (for regression) the outputs, Y, are sampled from a Gaussian process (GP) with a covariance which depends on the previous layer features (Fig. 1 top),\n\nP (Fl|Fl−1) = (cid:81)Nl\n\nP (Y|FL) = (cid:81)νL+1\n\nλ=1N (cid:0)f l λ; 0, K(G(Fl−1))(cid:1) λ=1 N (cid:0)yλ; 0, K(G(FL)) + σ2I(cid:1) .\n\n(1a)\n\n(1b)\n\nNote we only use the regression likelihood to give a concrete example; we could equally use an alternative likelihood e.g. for classification (Appendix B). The distinction between DGPs and BNNs arises through the choice of K(·) and G(·). For BNNs, see Appendix A. For DGPs, G(·), which takes the features and computes the corresponding P × P Gram matrix, is\n\nG(Fl−1) = 1\n\nNl−1\n\n(cid:80)Nl−1\n\nλ=1 f l−1\n\nλ\n\n(f l−1\n\nλ\n\n)T = 1\n\nNl−1\n\nFl−1FT\n\nl−1.\n\n(2)\n\nNow, we introduce random variables representing the Gram matrices, Gl−1 = G(Fl−1), where Gl−1 is a random variable representing the Gram matrix at layer l − 1, whereas G(·) is a deterministic function that takes features and computes the corresponding Gram matrix using Eq. (2). Finally, K(·), transforms the Gram matrices, Gl−1 to the final kernel. Many kernels of interest are isotropic, meaning they depend only on the normalized squared distance between datapoints, Rij, Kisotropic;ij(Gl−1) = kisotropic (Rij(Gl−1)) .\n\n(3)\n\nImportantly, we can compute this squared distance from Gl−1, without needing Fl−1, (cid:1)2\n\n(cid:1)2\n\n(cid:80)N\n\n(cid:80)N\n\n(cid:1)2(cid:1)\n\n(cid:0)(cid:0)Fiλ\n\n− 2FiλFjλ + (cid:0)Fjλ\n\n(cid:0)Fiλ − Fjλ\n\nRij(G) = 1\n\nN\n\n= 1 N\n\nλ=1\n\nλ=1\n\n= Gii − 2Gij + Gjj,\n\n(4)\n\nwhere λ indexes features, i and j index datapoints and we have omitted the layer index for simplicity. Importantly, we are not restricted to isotropic kernels: other kernels that depend only on the Gram matrix, such as the arccos kernels from the infinite NN literature (Cho & Saul, 2009) can also be used (for further details, see Aitchison et al., 2020).\n\n4.1 BNN AND DGP PRIORS CAN BE WRITTEN PURELY IN TERMS OF GRAM MATRICES\n\nNotice that Fl depends on Fl−1 only through Gl−1 = G(Fl−1), and Y depends on FL only through GL = G(FL) (Eq. 1). We can therefore write the graphical model in terms of those Gram matrices (Fig. 1 middle).\n\nP (Fl|Gl−1) = (cid:81)Nl\n\nλ=1N (cid:0)f l\n\nλ; 0, K(Gl−1)(cid:1)\n\nP (Gl|Fl) = δ (Gl − G(Fl)) P (Y|GL) = (cid:81)νL+1\n\nλ=1 N (cid:0)yλ; 0, K(GL) + σ2I(cid:1) .\n\n3\n\n(5a)\n\n(5b)\n\n(5c)\n\nwhere δ is the Dirac-delta, and G0 depends on X (e.g. G0 = 1 ν0 have used a regression likelihood, but other likelihoods could also be used.\n\nXXT ). Again, for concreteness we\n\nNow, we can integrate Fl out of the model, in which case, we get an equivalent generative model written solely in terms of Gram matrices (Fig. 1 bottom), with\n\nP (Gl|Gl−1) =\n\n(cid:90)\n\ndFl P (Gl|Fl) P (Fl|Gl−1) ,\n\n(6)\n\nand with the usual likelihood (e.g. Eq. 5c). This looks intractable (and indeed, in general it is intractable). However for DGPs, an analytic form is available. In particular, note the Gram matrix (Eq. 2) is the outer product of IID Gaussian distributed vectors (Eq. 1a). This matches the definition of the Wishart distribution (Gupta & Nagar, 2018), so we have,\n\nP (Gl|Gl−1) = Wishart\n\nK(Gl−1), Nl\n\n(cid:17)\n\n(cid:16)\n\nGl; 1 Nl log |Gl|− Nl\n\nlog P (Gl|Gl−1) = Nl−P −1\n\n2\n\n2 log |K(Gl−1)|− Nl\n\n2 Tr (cid:0)K−1(Gl−1)Gl\n\n(7)\n\n(cid:1) + const .\n\nThis distribution over Gram matrices is valid for DGPs of any width (though we need to be careful in the low-rank setting where Nl < P ). We are going to leverage these Wishart distributions to understand the behaviour of the Gram matrices in the infinite width limit.\n\n4.2 STANDARD INFINITE WIDTH LIMITS OF DGPS LACK REPRESENTATION LEARNING\n\nWe are now in a position to take a new viewpoint on the DGP analogue of standard NNGP results (Lee et al., 2017; Matthews et al., 2018; Hron et al., 2020; Pleiss & Cunningham, 2021). We can then evaluate the log-posterior for a model written only in terms of Gram matrices,\n\nlog P (G1, . . . , GL|X, Y) = log P (Y|GL) + (cid:80)L\n\nl=1 log P (Gl|Gl−1) + const .\n\n(8)\n\nThen we take the limit of infinite width,\n\nNl = N νl\n\nfor\n\nl ∈ {1, . . . , L}\n\nwith\n\nN → ∞.\n\n(9)\n\nThis limit modifies log P (Gl|Gl−1) (Eq. 7), but does not modify G1, . . . , GL in Eq. (8) as we get to choose the values of G1, . . . , GL at which to evaluate the log-posterior. Specifically, the logprior, log P (Gl|Gl−1) (Eq. 7), scales with Nl and hence with N . To get a finite limit, we therefore need to divide by N ,\n\nlim N→∞\n\n1\n\nN log P (Gl|Gl−1) = νl\n\n2\n\n(cid:0)log (cid:12)\n\n(cid:12)K−1(Gl−1)Gl\n\n(cid:12) (cid:12) − Tr (cid:0)K−1(Gl−1)Gl\n\n(cid:1)(cid:1) + const\n\n= −νl DKL (N (0, Gl)∥N (0, K(Gl−1))) + const .\n\n(10)\n\nAnd remarkably this limit can be written as the KL-divergence between two multivariate is constant wrt N (Eq. 5c), so Gaussians. 1\nlimN→∞\n\nN log P (Y|GL) = 0. The limiting log-posterior is thus,\n\nthe log likelihood,\n\nlog P (Y|GL),\n\nIn contrast,\n\nlim N→∞\n\n1\n\nN log P (G1, . . . , GL|X, Y) = −(cid:80)L\n\nl=1νl DKL (N (0, Gl)∥N (0, K(Gl−1))) + const .\n\nThis form highlights that the log-posterior scales with N , so in the limit as N → ∞, the posterior converges to a point distribution at the global maximum, denoted G∗ L, (see Appendix C for a formal discussion of weak convergence),\n\n1, . . . , G∗\n\n(11)\n\nlim N→∞\n\nP (G1, . . . , GL|X, Y) = (cid:81)L\n\nl=1δ (Gl − G∗\n\nl ) .\n\n(12)\n\nMoreover, it is evident from the KL-divergence form for the log-posterior (Eq. 11) that the unique global maximum can be computed recursively as G∗ XXT . Thus, the limiting posterior over Gram matrices does not depend on the training targets, so there is no possibility of representation learning (Aitchison, 2020). This is deeply problematic as the successes of modern deep learning arise from flexibly learning good top-layer representations.\n\nl−1), with e.g. G∗\n\nl = K(G∗\n\n0 = 1\n\nν0\n\n4\n\n4.3 THE BAYESIAN REPRESENTATION LEARNING LIMIT\n\nIn the previous section, we saw that standard infinite width limits eliminate representation learning because as N → ∞ the log-prior terms, log P (Gl|Gl−1), in Eq. (8) dominated the log-likelihood, P (Y|GL), and the likelihood is the only term that depends on the labels. We therefore introduce the “Bayesian representation learning limit” which retains representation learning. The Bayesian representation learning limit sends the number of output features, NL+1, to infinity as the layerwidths go to infinity,\n\nNl = N νl\n\nfor\n\nl ∈ {1, . . . , L + 1}\n\nwith\n\nN → ∞.\n\n(13)\n\nImportantly, the Bayesian representation learning limit gives a valid probabilistic model with a welldefined posterior, arising from the prior, (Eq. 6) and a likelihood which assumes each output channel is IID,\n\nP ( ̃Y|GL) = (cid:81)NL+1\n\nλ=1 N (cid:0) ̃yλ; 0, K(GL) + σ2I(cid:1) .\n\n(14)\n\nwhere ̃Y ∈ RP ×NL+1 is infinite width (Eq. 13) whereas the usual DGP data, Y ∈ RP ×νL+1, is finite width. Of course, infinite-width data is unusual if not unheard-of. In practice, real data, Y ∈ RP ×νL+1, almost always has a finite number of features, νL+1. How do we apply the DKM to such data? The answer is to define ̃Y as N copies of the underlying data, Y, i.e. ̃Y = (Y · · · Y). As each channel is assumed to be IID (Eq. 5c and 14) the likelihood is N times larger,\n\nlog P ( ̃Y|GL) = N log P (Y|GL) ,\n\n(15)\n\nThe log-posterior in the Bayesian representation learning limit is very similar to the log-posterior in the standard limit (Eq. 11). The only difference is that the likelihood, log P ( ̃Y|GL) now scales with N , so it does not disappear as we take the limit, allowing us to retain representation learning,\n\nL(G1, . . . , GL) = lim\n\nN→∞\n\n1\n\nN log P (G1, . . . , GL|X, ̃Y) + const,\n\n(16)\n\n= log P (Y|GL) − (cid:80)L\n\nl=1νl DKL (N (0, Gl)∥N (0, K(Gl−1))) .\n\n1, . . . , G∗\n\nHere, we denote the limiting log-posterior as L(G1, . . . , GL), and this forms the DKM objective. Again, as long as the global maximum of the DKM objective is unique, the posterior is again a point distribution around that maximum (Eq. 12). Of course, the inclusion of the likelihood term means that the global optimum G∗ L cannot be computed recursively, but instead we need to optimize, e.g. using gradient descent (see Sec. 4.7). Unlike in the standard limit (Eq. 11), it is no longer possible to guarantee uniqueness of the global maximum. We can nonetheless say that the posterior converges to a point distribution as long as the global maximum of L(G1, . . . , GL) is unique, (i.e. we can have any number of local maxima, as long as they all lie below the unique global maximum). We do expect the global maximum to be unique in most practical settings: we know the maximum is unique when the prior dominates (Eq. 11), in Appendix J, we prove uniqueness for linear models, and in Appendix K, we give a number of experiments in nonlinear models in which optimizing from very different initializations found the same global maximum, indicating uniqueness in practical settings.\n\n4.4 THE EXACT DGP POSTERIOR OVER FEATURES IS MULTIVARIATE GAUSSIAN\n\nAbove, we noted that the DGP posterior over Gram matrices in the Bayesian representation learning limit is a point distribution, as long as the DKM objective has a unique global maximum. Remarkably, in this setting, the corresponding posterior over features is multivariate Gaussian (see Appendix D for the full derivation),\n\nP (cid:0)f l\n\nλ|X, y(cid:1) = N (cid:0)f l\n\nλ; 0, G∗\n\nl\n\n(cid:1)\n\n(17)\n\nWhile such a simple result might initially seem remarkable, it should not surprise us too much. In particular, the prior is Gaussian (Eq. 1). In addition, in Fig. 1 (middle), we saw that the next layer features depend on the current layer features only through the Gram matrices, which are just the raw second moment of the features, Eq. (2). Thus, in effect the likelihood only constrains the raw second moments of the features. Critically, that constraints on the raw second moment are tightly connected to Gaussian distributions: under the MaxEnt framework, a Gaussian distribution arises by maximizing the entropy under constraints on the raw second moment of the features (Jaynes, 2003).\n\n5\n\nThus it is entirely plausible that a Gaussian prior combined with a likelihood that “constrains” the raw second moment would give rise to Gaussian posteriors (though of course this is not a proof; see Appendix D for the full derivation).\n\nFinally, note that we appear to use Gl or G∗ l in Eq. (2) and as the posterior covariance in the Bayesian representation learning limit (Eq. 17). In the infinite limit, these two uses are consistent. In particular, consider the value of Gl defined by Eq. (2) under the posterior,\n\nl in two separate senses: as 1\n\nFlFT\n\nNl\n\nGl = lim\n\nN→∞\n\n1 Nl\n\n(cid:80)Nl\n\nλ=1f l\n\nλ(f l\n\nλ)T = E\n\nP(f l\n\nλ|X,y)\n\n(cid:2)f l\n\nλ(f l\n\nλ)T (cid:3) = G∗ l .\n\n(18)\n\nThe second equality arises by noticing that we are computing the average of infinitely many terms, f l λ)T , which are IID under the true posterior (Eq. 17), so we can apply the law of large numbers, λ(f l and the final expectation arises by computing moments under Eq. (17).\n\n4.5 THE DKM OBJECTIVE GIVES INTUITION FOR REPRESENTATION LEARNING\n\nThe form for the DKM objective in Eq. (16) gives a strong intuition for how representation learning occurs in deep networks. In particular, the likelihood, log P (Y|GL), encourages the model to find a representation giving good performance on the training data. At the same time, the KL-divergence terms keep the posterior over features, N (0, Gl), (Eq. 17) close to the prior N (0, K(Gl−1)) (Eq. 1a). This encourages the optimized representations, Gl, to lie close to their value under the standard infinite-width limit, K(Gl−1). We could use any form for the likelihood including classification and regression, but to understand how the likelihood interacts with the other KL-divergence terms, it is easiest to consider regression (Eq. 5c), as this log-likelihood can also be written as a KL-divergence,\n\nlog P (Y|GL) = −νL+1 DKL\n\n(cid:0)N (0, GL+1)(cid:13) (19) Thus, the likelihood encourages K(GL) + σ2I to be close to the covariance of the data, GL+1 = 1\nIn combiνL+1 nation, we would expect the optimal Gram matrices to “interpolate” between the input kernel, G0 = 1 ν0\n\nYYT , while the DGP prior terms encourage all Gl to lie close to K(Gl−1).\n\n(cid:13)N (cid:0)0, K(GL) + σ2I(cid:1)(cid:1) + const\n\nXXT and the output kernel, GL+1.\n\nTo make the notion of interpolation explicit, we consider σ2 = 0 with a linear kernel, K(Gl−1) = Gl−1, so named because it corresponds to a linear neural network layer. With this kernel and with all νl = ν, there is an analytic solution for the (unique) optimum of the DKM objective (Appendix J.1),\n\nG∗\n\n(cid:0)G−1\n\n(cid:1)l/(L+1)\n\nl = G0\n\n(20) which explicitly geometrically interpolates between G0 and GL+1. Of course, this discussion was primarily for DGPs, but the exact same intuitions hold for BNNs, in that maximizing the DKM objective finds a sequence of Gram matrices, G∗ L that interpolate between the input kernel, G0 and the output kernel, GL+1. The only difference is in details of P (Gl|Gl−1), and specifically as slight differences in the KL-divergence terms (see below).\n\n1, . . . , G∗\n\n0 GL+1\n\n,\n\n4.6 THE DKM OBJECTIVE MIRRORS REPRESENTATION LEARNING IN FINITE NETWORKS\n\nHere, we confirm that the optimizing DKM objective for an infinite network matches doing inference in wide but finite-width networks using Langevin sampling (see Appendix F for details).\n\nWe began by looking at DGPs, and confirming that the posterior marginals are Gaussian (Eq. 17; Fig. 3ab). Then, we confirmed that the representations match closely for infinite-width DKMs (Fig. 2 top and bottom rows) and finite-width DGPs (Fig. 2 middle two rows), both at initialization (Fig. 2 top two rows) and after training to convergence (Fig. 2 bottom two rows). Note that the first column, K0 is a squared exponential kernel applied to the input data, and G3 = yyT is the output Gram matrix (in this case, there is only one output feature).\n\nTo confirm that the match improves as the DGP gets wider, we considered the RMSE between elements of the Gram matrices for networks of different widths (x-axis) for different UCI datasets (columns) and different numbers of layers (top row is one-layer, bottom row is two-layers; Fig. 3c). In most cases, we found a good match as long as the width was at least 128, which is around the width of typical fully connected neural network, but is a little larger than typical DGP widths (e.g. Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017).\n\n6\n\nFigure 2: A two hidden layer DGP with 1024 units per hidden layer and DKM with squared exponential kernels match closely. The data was the first 50 datapoints of the yacht dataset. The first column, K0 is a fixed squared exponential kernel applied to the inputs, and the last column, G3 = yyT is the fixed output Gram matrix. The first row is the DKM initialization at the prior Gram matrices and kernels, and the second row is the DGP, which is initialized by sampling from the prior. As expected, the finite width DGP prior closely matches the infinite-width DKM initialization, which corresponds to the standard infinite width limit. The third row is the Gram matrices and kernels for the trained DGP, which has changed dramatically relative to its initialization (second row) in order to better fit the data. The fourth row is the Gram matrices and kernels for the optimized DKM, which closely matches those for the trained DGP.\n\n4.7 THE SPARSE DEEP KERNEL MACHINE AS A DEEP GENERALISATION OF KERNEL\n\nMETHODS\n\nDGPs in the representation learning limit constitute a deep generalisation of kernel methods, with a very flexible learned kernel, which we call the deep kernel machine (DKM; which was introduced earlier just in the context of the objective). Here, we design a sparse DKM, inspired by sparse methods for DGPs (Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) (Appendix L). The sparse DKM scales linearly in the number of datapoints, P , as opposed to cubic scaling of the plain DKM (similar to the cubic scaling in most naive kernel methods).\n\nWe compared DKMs (Eq. 16) and MAP over features (Sec. E) for DGPs. In addition, we considered a baseline, which was a standard, shallow kernel method mirroring the structure of the deep kernel machine but where the only flexibility comes from the hyperparameters. Formally, this model can be obtained by setting, Gl = K(Gl−1) and is denoted “Kernel Hyper” in Table 1. We applied these methods to UCI datasets (Gal & Ghahramani, 2016) using a two hidden layer architecture, with a kernel inspired by DGP skip-connections, K(Gl) = wl 2 and σ are hyperparameters, and Ksqexp(Gl) is a squared-exponential kernel.\n\n2Ksqexp(Gl). Here, wl\n\n1Gl + wl\n\n1, wl\n\nWe used 300 inducing points fixed to a random subset of the training data and not optimised during training. We used the Adam optimizer with a learning rate of 0.001, full-batch gradients and 5000 iterations for smaller datasets and 1000 iterations for larger datasets (kin8nm, naval and protein).\n\nWe found that the deep kernel machine objective gave better performance than MAP, or the hyperparameter optimization baseline (Tab. 1). Note that these numbers are not directly comparable to those in the deep GP literature (Salimbeni & Deisenroth, 2017), as deep GPs have a full posterior so offer excellent protection against overfitting, while DKMs give only a point estimate.\n\n7\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101Figure 3: Wide DGP posteriors converge to the DKM. Here, we trained DGPs with Langevin sampling (see Appendix F), and compared to a trained DKM. a Marginal distribution over features for one input datapoint for a two-layer DGP trained on a subset of yacht. We used a width of N1...L = 1024 and ν1...L = 5 in all plots to ensure that the data had a strong effect on the learned representations. The marginals (blue histogram) are very close to Gaussian (the red line shows the closest fitted Gaussian). Remember that the true posterior over features is IID (Eq. 31), so each column aggregates the distribution over features (and over 10 parallel chains with 100 samples from each chain) for a single input datapoint. b The 2D marginal distributions for the same DGP for two input points (horizontal and vertical axes). c Element-wise RMSE (normalized Frobenius distance) between Gram matrices from a trained DKM compared to trained DGPs of increasing width. The DGP Gram matrices converge to the DKM solution as width becomes larger.\n\nTable 1: RMSE for inducing point methods. (Equal) best methods are displayed in bold. Error bars give two stderrs for a paired tests, which uses differences in performance between that method and best method, (so there are no meaningful error bars on the best performing method itself). The MAP objective was numerically unstable and thus did not run to completion on the boston dataset.\n\ndataset\n\nP\n\nKernel Hyper\n\nMAP\n\nL\n\n506 1,030 768\n\n4.41 ± 0.31 5.38 ± 0.098 0.83 ± 0.076\n\nboston concrete energy kin8nm 8,192 (7.3 ± 0.06)·10-2 (6.4 ± 0.6)·10-4 3.81 ± 0.091 4.21 ± 0.029 0.68 ± 0.0084 0.94 ± 0.058\n\nnaval power protein wine yacht\n\n11,934 9,568 45,730 1,599 308\n\n— 5.60 ± 0.15 0.73 ± 0.049 (7.4 ± 0.05)·10-2 (5.4 ± 0.5)·10-4 3.73 ± 0.14 4.30 ± 0.033 0.66 ± 0.0067 1.14 ± 0.077\n\n4.35 ± 0.51 5.10 0.47 6.6·10-2 4.6·10-4 3.58 4.10 0.64 0.58\n\n5 CONCLUSION\n\nWe introduced the Bayesian representation learning limit, a new infinite-width limit for BNNs and DGPs that retains representation learning. Representation learning in this limit is described by the intuitive DKM objective, which is composed of a log-likelihood describing performance on the task (e.g. classification or regression) and a sum of KL-divergences keeping representations at every layer close to those under the infinite-width prior. For DGPs, the exact posteriors are IID across features and are multivariate Gaussian, with covariances given by optimizing the DKM objective. Empirically, we found that the distribution over features and representations matched those in wide by finite DGPs. We argued that DGPs in the Bayesian representation learning limit form a new\n\n8\n\n505feature0.00.30.6densitya=1505feature=2303input 1303input 2b=1303input 1=20.00.51.0RMSEcbostonyachtconcreteone-layerenergy2123252729211width0.00.51.0RMSEG1G22123252729211width2123252729211width2123252729211widthtwo-layer0.00.20.00.2class of practical deep kernel method: DKMs. We introduce sparse DKMs, which scale linearly in the number of datapoints. Finally, we give the extension for BNNs where the exact posteriors are intractable so must be approximated.\n\nREFERENCES\n\nLaurence Aitchison. Why bigger is not always better: on finite and infinite neural networks. In\n\nInternational Conference on Machine Learning, pp. 156–164. PMLR, 2020.\n\nLaurence Aitchison, Adam X Yang, and Sebastian W Ober. Deep kernel processes. arXiv preprint\n\narXiv:2010.01590, 2020.\n\nJoseph M Antognini. Finite size corrections for neural network gaussian processes. arXiv preprint\n\narXiv:1908.10030, 2019.\n\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.\n\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-\n\ncians. Journal of the American statistical Association, 112(518):859–877, 2017.\n\nT Tony Cai and Linjun Zhang. High-dimensional gaussian copula regression: Adaptive estimation\n\nand statistical inference. Statistica Sinica, pp. 963–993, 2018.\n\nYoungmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In NIPS, pp. 342–350.\n\nCurran Associates, Inc., 2009.\n\nAndreas Damianou and Neil D Lawrence. Deep gaussian processes. In Artificial intelligence and\n\nstatistics, pp. 207–215. PMLR, 2013.\n\nConor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. Ad-\n\nvances in neural information processing systems, 32, 2019.\n\nEthan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. arXiv\n\npreprint arXiv:1909.11304, 2019.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML-16), 2016.\n\nAdri`a Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-\n\nworks as shallow gaussian processes. arXiv preprint arXiv:1808.05587, 2018.\n\nArjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and Hall/CRC, 2018.\n\nJames Halverson, Anindita Maiti, and Keegan Stoner. Neural networks and quantum field theory.\n\nMachine Learning: Science and Technology, 2(3):035002, 2021.\n\nBoris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. arXiv\n\npreprint arXiv:1909.05989, 2019.\n\nThomas Hofmann, Bernhard Sch ̈olkopf, and Alexander J Smola. Kernel methods in machine learn-\n\ning. The annals of statistics, pp. 1171–1220, 2008.\n\nRoger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.\n\nRoger A Horn, Roger A Horn, and Charles R Johnson. Topics in matrix analysis. Cambridge\n\nuniversity press, 1994.\n\nJiri Hron, Yasaman Bahri, Roman Novak, Jeffrey Pennington, and Jascha Sohl-Dickstein. Exact posterior distributions of wide bayesian neural networks. arXiv preprint arXiv:2006.10541, 2020.\n\nArthur Jacot, Franck Gabriel, and Cl ́ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\n9\n\nEdwin T Jaynes. Probability theory: The logic of science. Cambridge university press, 2003.\n\nMichael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction\n\nto variational methods for graphical models. Machine learning, 37(2):183–233, 1999.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\n\nlutional neural networks. Advances in neural information processing systems, 25, 2012.\n\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,\n\n2015.\n\nJaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.\n\nQianyi Li and Haim Sompolinsky. Statistical mechanics of deep linear neural networks: The back-\n\npropagating renormalization group. arXiv preprint arXiv:2012.04030, 2020.\n\nAlexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.\n\nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671, 2018.\n\nGadi Naveh and Zohar Ringel. A self consistent theory of gaussian processes captures feature\n\nlearning effects in finite cnns. arXiv preprint arXiv:2106.04110, 2021.\n\nGadi Naveh, Oded Ben-David, Haim Sompolinsky, and Zohar Ringel. Predicting the outputs of\n\nfinite networks trained with noisy gradients. arXiv preprint arXiv:2004.01190, 2020.\n\nRadford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29–53.\n\nSpringer, 1996.\n\nPhan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean field limit of multi-\n\nlayer neural networks. arXiv preprint arXiv:2001.11443, 2020.\n\nQuynh Nguyen. On connected sublevel sets in deep learning.\n\nIn International Conference on\n\nMachine Learning, pp. 4790–4799. PMLR, 2019.\n\nRoman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.\n\nGeoff Pleiss and John P Cunningham. The limitations of large width in neural networks: A deep gaussian process perspective. Advances in Neural Information Processing Systems, 34, 2021.\n\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pp. 1278–1286. PMLR, 2014.\n\nDaniel A Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory. arXiv\n\npreprint arXiv:2106.10165, 2021.\n\nHugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian\n\nprocesses. arXiv preprint arXiv:1705.08933, 2017.\n\n10\n\nInbar Seroussi and Zohar Ringel. Separation of scales and a thermodynamic description of feature\n\nlearning in some cnns. arXiv preprint arXiv:2112.15383, 2021.\n\nJohn Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge univer-\n\nsity press, 2004.\n\nJustin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central\n\nlimit theorem. Stochastic Processes and their Applications, 130(3):1820–1852, 2020a.\n\nJustin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of\n\nlarge numbers. SIAM Journal on Applied Mathematics, 80(2):725–752, 2020b.\n\nAlex J Smola and Bernhard Sch ̈olkopf. Learning with kernels. MIT Press, 1998.\n\nChristopher Williams. Computing with infinite networks. Advances in neural information process-\n\ning systems, 9, 1996.\n\nChieh Wu, Aria Masoomi, Arthur Gretton, and Jennifer Dy. Deep layer-wise networks have closed-\n\nform weights. arXiv preprint arXiv:2202.01210, 2022.\n\nSho Yaida. Non-gaussian processes and neural networks at finite widths.\n\nIn Mathematical and\n\nScientific Machine Learning, pp. 165–192. PMLR, 2020.\n\nGreg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760, 2019.\n\nGreg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint\n\narXiv:2011.14522, 2020.\n\nJacob Zavatone-Veth and Cengiz Pehlevan. Exact marginal prior distributions of finite bayesian\n\nneural networks. Advances in Neural Information Processing Systems, 34, 2021.\n\nJacob A Zavatone-Veth, Abdulkadir Canatar, and Cengiz Pehlevan. Asymptotics of representation\n\nlearning in finite bayesian neural networks. arXiv preprint arXiv:2106.00651, 2021.\n\n11\n\nA BAYESIAN NEURAL NETWORK EXTENSION\n\nConsider a neural network of the form,\n\nF1 = XW0 Fl = φ(Fl−1)Wl−1 (cid:16)\n\n(cid:17)\n\nW l\n\nλμ ∼ N\n\n0, 1 Nl\n\nfor l ∈ {2, . . . , L + 1} (cid:16)\n\n(cid:17)\n\nW 0\n\nλμ ∼ N\n\n0, 1 ν0\n\n(21a)\n\n(21b)\n\n(21c)\n\nwhere W0 ∈ Rν0×N1, Wl ∈ RNl×Nl+1 and WL+1 ∈ RNL×νL+1 are weight matrices with independent Gaussian priors and φ is the usually pointwise nonlinearity.\n\nIn principle, we could integrate out the distribution over Wl to find P (Fl|Fl−1)\n\nP (Fl|Fl−1) =\n\n(cid:90)\n\ndWl P (Wl) δ (Fl − φ(Fl−1)Wl−1) ,\n\n(22)\n\nwhere δ is the Dirac delta. In practice, it is much easier to note that conditioned on Fl−1, the random variables interest, Fl are a linear combination of Gaussian distributed random variables, Wl. Thus, Fl are themselves Gaussian, and this Gaussian is completely characterised by its mean and variance. We begin by writing the feature vectors, f l\n\nλ in terms of weight vectors, wl λ, λ = φ(Fl−1)wl f l λ. As the prior over weight vectors is IID, the prior over features, conditioned on Fl−1), is also IID,\n\n(23)\n\nP (W) =\n\nP (Fl|Fl−1) =\n\nNl(cid:89)\n\nλ=1\n\nNl(cid:89)\n\nλ=1\n\nP (cid:0)wl\n\nλ\n\n(cid:1) =\n\n(cid:16)\n\nN\n\nNl(cid:89)\n\nλ=1\n\nwl\n\nλ; 0,\n\n1 Nl−1\n\nI\n\n(cid:17)\n\n,\n\nP (cid:0)f l\n\nλ|Fl−1\n\n(cid:1) .\n\n(24)\n\n(25)\n\nThe mean of f l\n\nE (cid:2)f l\n\nλ|Fl−1\n\nλ conditioned on Fl−1 is 0, (cid:3) = E (cid:2)φ(Fl−1)wl\n\nλ|Fl−1\n\n(cid:3) = φ(Fl−1) E (cid:2)wl\n\nλ|Fl−1\n\n(cid:3) = φ(Fl−1) E (cid:2)wl\n\nλ\n\n(cid:3) = 0.\n\n(26)\n\nThe covariance of f l\n\nλ conditioned on Fl−1 is, (cid:104) (cid:104)\n\n(cid:105)\n\nE\n\nf l\n\nλ\n\n(cid:0)f l\n\nλ\n\n(cid:1)T\n\n|Fl−1\n\n(cid:0)φ(Fl−1)wl\n\nλ\n\n(cid:1)T\n\n|Fl−1\n\n(cid:105)\n\nλ(wl\n\nλ)T (cid:3) φT (Fl−1)\n\nλ\n\n= E φ(Fl−1)wl = φ(Fl−1) E (cid:2)wl = 1\n\nNl−1\n\nφ(Fl−1)φT (Fl−1)\n\n(27)\n\nThis mean and variance imply that Eq. (1) captures the BNN prior, as long as we choose KBNN(·) and GBNN(·) such that,\n\nKBNN(GBNN(Fl−1)) = 1\n\nNl−1\n\n(cid:80)Nl−1\n\nλ=1 φ(f l−1\n\nλ\n\n)φT (f l−1\n\nλ\n\n),\n\n(28)\n\nSpecifically, we choose the kernel function, KBNN(·) to be the identity function, and GBNN(·) to be the same outer product as in the main text for DGPs (Eq. 2), except where we have applied the NN nonlinearity,\n\nKBNN(Gl−1) = Gl−1, GBNN(Fl−1) = 1\n\nNl−1\n\n(cid:80)Nl−1\n\nλ=1 φ(f l−1\n\nλ\n\n)φT (f l−1\n\nλ\n\n).\n\n(29)\n\n(30)\n\nThis form retains the average-outer-product form for GBNN(·), which is important for our derivations.\n\nNow, Eq. (16) only gave the DKM objective for DGPs. To get a more general form, we need to consider the implied posteriors over features. This posterior is IID over features (Appendix D.1), and for DGPs, it is multivariate Gaussian (Appendix D.2),\n\nP (Fl|Gl−1, Gl) = (cid:81)N l\n\nλ=1 P (cid:0)f l\n\nλ|Gl−1, Gl\n\n(cid:1) =\n\nfor DGPs\n\n(cid:81)N l\n\nλ=1N (cid:0)f l\n\nλ; 0, Gl\n\n(cid:1) .\n\n(31)\n\n12\n\nFigure 4: The variational DKM closely matches the BNN true posterior obtained with Langevin sampling. a Comparison of Gram matrices. The first two rows show Gram matrices for BNN, with the first row being a random initialization, and the second row being the posterior. The last two rows show the Gram matrices from variational DKMs with a flow approximate posterior (third row) and a multivariate Gaussian approximate posterior (fourth row). In optimizing the variational DKM, we used Eq. (34) with 216 Monte-Carlo samples. The Gram matrices for the flow posterior (third row) closely match those from the BNN posterior (second row), while those for a multivariate Gaussian approximate posterior (fourth row) do not match. b Marginal distributions over features at each layer for one input datapoint estimated using kernel density estimation. The note that the BNN (blue line) marginals are non-Gaussian, but the variational DKM with a flow posterior (red line) is capable of capturing this non-Gaussianity.\n\nNow, we can see that Eq. (16) is a specific example of a general expression. In particular, note that the distribution on the left of the KL-divergence in Eq. (16) is the DGP posterior over features (Eq. 31). Thus, the DKM objective can alternatively be written,\n\nL(G1, . . . , GL) = log P (Y|GL) − (cid:80)L\n\nl=1νl DKL\n\n(cid:0)P (cid:0)f l\n\nλ|Gl−1, Gl\n\n(cid:1)(cid:13) (cid:13)N (0, K(Gl−1))(cid:1) ,\n\n(32)\n\nand this form holds for both BNNs and DGPs (Appendix D.3). As in DGPs, the log-posterior is N times L(G1, . . . , GL) (Eq. 16), so as N is taken to infinity, the posterior for all models becomes a point distribution (Eq. 12) if L(G1, . . . , GL) has a unique global maximum.\n\nIn practice, the true posteriors required to evaluate Eq. (32) are intractable for BNNs, raising the question of how to develop accurate approximations for BNNs. We develop a variational DKM (vDKM) by taking inspiration from variational inference (Jordan et al., 1999; Blei et al., 2017) (Appendix D.4). Of course, variational inference is usually impossible in infinite width models, because it is impossible to work with infinitely large latent variables. Our key insight is that as the true posterior factorises across features (Appendix D.1), we can work with the approximate posterior\n\n13\n\ninitG1G2G3G4vDKM (flow)150indexvDKM (MvG)150index150index150index10010feature0.00.5density10010feature10010feature10010feature150index150indexG0150indexG5404abinitG1G2G3G4vDKM (flow)150indexvDKM (MvG)150index150index150index10010feature0.00.5density10010feature10010feature10010feature150index150indexG0150indexG5404ab BNNvDKM (flow)BNNl=1νl DKL (cid:0)f l\n\n(cid:0)Qθl (cid:1),\n\nover only a single feature vector, Qθl approach allows us to define a vDKM objective, which bounds the true DKM objective,\n\n(cid:1), where θl are the parameters and f l\n\n(cid:0)f l\n\nλ ∈ RP is finite. This\n\nλ\n\nL(Gθ(θ1), . . . , Gθ(θL)) ≥ LV(θ1, . . . , θL),\n\nLV(θ1, . . . , θL) = log P (Y|Gθ(θL)) − (cid:80)L\n\n(33) (cid:1)(cid:13) (cid:13)N (0, K(Gθ(θl−1)))(cid:1)\n\n(cid:0)f l\n\nλ\n\nλ|Gl−1, Gl\n\nwith equality when the true posteriors, P (cid:0)f l (cid:1). The only subtlety here is that it is practically difficult to design flexible ap- (cid:1) where we explicitly specify and optimize the Gram matrices. Instead proximate posteriors Qθl we optimize general approximate posterior parameters, θ, and compute the implied Gram matrices,\n\napproximate posteriors, Qθl\n\nequal\n\n(cid:0)f l\n\nthe\n\nλ\n\nλ\n\nGθ(θl) = 1\n\nNl\n\nlim Nl→∞\n\n(cid:80)Nl\n\nλ=1φ(f l\n\nλ)φT (f l\n\nλ) = E\n\nQθl(f l\n\nλ)\n\n(cid:2)φ(f l\n\nλ)φT (f l\n\nλ)(cid:3) .\n\n(34)\n\nλ are sampled from Qθl\n\n(cid:1), and the second equality arises from the law of large numbers. where f l We can compute the Gram matrix analytically in simple cases (such as a multivariate Gaussian), but in general we can always estimate the Gram matrix using a Monte-Carlo estimate of Eq. (34).\n\n(cid:0)f l\n\nλ\n\nFinally, we checked that the vDKM objective closely matched the posterior under neural networks. This is a bit more involved, as the marginal distributions over features are no longer Gaussian (Fig. 4b). To capture these non-Gaussian marginals, we used a simple normalizing flow. In particular, we first sampled zl λ ∼ N (μl, Σl) from a multivariate Gaussian with a learned mean, μl, and covariance, Σl then we obtained features, f l λ through f , a learned pointwise function parameterised as in a neural spline flow (Durkan et al., 2019). The resulting distribution is a high-dimensional Gaussian copula (e.g. Cai & Zhang, 2018). As shown in Fig. 4, vDKM with multivariate Gaussian (MvG) approximate posterior cannot match the Gram matrices learned by BNN (Fig. 4a), while vDKM with flow is able to capture the non-Gaussian marginals (Fig. 4b) and thus match the learned Gram matrices with BNN.\n\nλ), by passing zl\n\nλ = f (zl\n\nB GENERAL LIKELIHOODS THAT DEPEND ONLY ON GRAM MATRICES\n\nWe consider likelihoods which depend only on the top-layer Gram matrix, GL,\n\n(cid:90)\n\nP (Y|GL) =\n\ndFL+1 P (Y|FL+1) P (FL+1|GL)\n\nwhere,\n\nP (FL+1|GL) =\n\nNL+1 (cid:89)\n\nλ=1\n\nN (cid:0)f L+1\n\nλ\n\n; 0, K(GL)(cid:1) .\n\nThis family of likelihoods captures regression,\n\nP (cid:0)yλ|f L+1\n\nλ\n\n(cid:1) = N (cid:0)yL+1\n\nλ\n\n; f L+1\n\nλ\n\n, σ2I(cid:1) ,\n\n(which is equivalent to the model used in the main text Eq. 1b) and e.g. classification,\n\nP (y|F) = Categorical (y; softmax (FL+1)) ,\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\namoung many others.\n\nC WEAK CONVERGENCE\n\nHere, we give a formal argument for weak convergence of the DGP posterior over Gram matrices to a point distribution in the limit as N → ∞,\n\nPN (G1, . . . , GL|X, ̃Y) d→\n\nL (cid:89)\n\nl=1\n\nδ(Gl − G∗ l )\n\n(39)\n\n14\n\nwhere we have included N in the subscript of the probability distribution as a reminder that this distribution depends on the width. By the Portmanteau theorem, weak convergence is established if all expectations of bounded continuous functions, f , converge\n\nlim N→∞\n\nEPN (G1,...,GL|X, ̃Y) [f (G1, . . . , GL)] = f (G∗\n\n1, . . . , G∗\n\nL).\n\n(40)\n\nTo show this in a reasonably general setting (which the DGP posterior is a special case of), we consider an unnormalized probability density of the form h(g)eN L(g), and compute the moment as,\n\nE [f (g)] =\n\n(cid:82) G dg f (g)h(g)eN L(g) (cid:82) G dg h(g)eN L(g)\n\n(41)\n\nwhere g = (G1, . . . , GL) is all L positive semi-definite matrices, Gl. Thus, g ∈ G, where G is a convex set.\n\nWe consider the superlevel set A(∆) = {g|L(g) ≥ L(g∗) − ∆}, where g∗ is the unique global optimum. We select out a small region, A(∆), surrounding the global maximum, and compute the integral as,\n\nE [f (g)] =\n\n(cid:82)\n\nA(∆) dg f (g)h(g)eN L(g) + (cid:82) A(∆) dg h(g)eN L(g) + (cid:82)\n\n(cid:82)\n\nG\\A(∆) dg f (g)h(g)eN L(g) G\\A(∆) dg h(g)eN L(g)\n\nAnd divide the numerator and denominator by (cid:82)\n\nA(∆) dg h(g)eN L(g),\n\nE [f (g)] =\n\n(cid:82)\n\nA(∆) dg f (g)h(g)eN L(g) (cid:82)\n\nA(∆) dg h(g)eN L(g) +\n\n(cid:82) G\\A(∆) dg f (g)h(g)eN L(g) A(∆) dg h(g)eN L(g)\n\n(cid:82)\n\n1 +\n\n(cid:82)\n\nG\\A(∆) dg h(g)eN L(g) (cid:82) A(∆) dg h(g)eN L(g)\n\n(42)\n\n(43)\n\nNow, we deal with each term separately. The ratio in the denominator can be lower-bounded by zero, and upper bounded by considering a smaller superlevel set, A(∆/2), in the denominator,\n\n0 ≤\n\n(cid:82)\n\nG\\A(∆) dg h(g)eN L(g) (cid:82) A(∆) dg h(g)eN L(g)\n\n≤\n\n≤\n\n=\n\n(cid:82)\n\nG\\A(∆) dg h(g)eN L(g) (cid:82) A(∆/2) dg h(g)eN L(g) eN (L(g∗)−∆) (cid:82) eN (L(g∗)−∆/2) (cid:82) (cid:82)\n\nG\\A(∆) dg h(g) (cid:82) A(∆/2) dg h(g)\n\nG\\A(∆) dg h(g) A(∆/2) dg h(g)\n\ne−N ∆/2\n\n(44)\n\nThe upper bound converges to zero (as h(g) is independent of N ), and therefore by the sandwich theorem the ratio of interest also tends to zero.\n\nThe second ratio in the numerator can be rewritten as,\n\n(cid:82)\n\nG\\A(∆) dg f (g)h(g)eN L(g) A(∆) dg h(g)eN L(g)\n\n(cid:82)\n\n=\n\n(cid:82) G\\A(∆) dg f (g)h(g)eN L(g) (cid:82) G\\A(∆) dg h(g)eN L(g)\n\n(cid:82) G\\A(∆) dg h(g)eN L(g) (cid:82) A(∆) dg h(g)eN L(g)\n\n(45)\n\nThe first term here is an expectation of a bounded function, f (g), so is bounded, while second term converges to zero in the limit (by the previous result).\n\nFinally, we consider the first ratio in the numerator,\n\n(cid:82) A(∆) dg f (g)h(g)eN L(g) (cid:82) A(∆) dg h(g)eN L(g)\n\n(46)\n\nwhich can be understood as an expectation over f (g) in the region A(∆). As f is continuous, for any ε > 0, we can find a δ > 0 such that for all g with |g∗ − g| < δ, we have\n\nf (g∗) − ε < f (g) < f (g∗) + ε.\n\n(47)\n\n15\n\nFurther, because the continuous function, L(g), has a unique global optimum, g∗, for every δ > 0 we are always able to find a ∆ > 0 such that all points g ∈ A(∆) are within δ of g∗ i.e. |g∗ − g| < δ. Thus combining the previous two facts, given an ε, we are always able to find a δ such that Eq. 47 holds for all g with |g∗ − g| < δ, and given a δ we are always able to find a ∆ such that all g ∈ A(∆) have |g∗ − g| < δ. Hence for every ε > 0 we can find a ∆ > 0 such that Eq. 47 holds for all g ∈ A(∆). Choosing the appropriate ε-dependent ∆ and substituting Eq. 47 into Eq. 46, ε also bounds the error in the expectation,\n\nf (g∗) − ε <\n\n(cid:82)\n\nA(∆) dg f (g)h(g)eN L(g) (cid:82) A(∆) dg h(g)eN L(g)\n\n< f (g∗) + ε.\n\n(48)\n\nNow, we use the results in Eq. (44), Eq. (45) and Eq. (48) to take the limit of Eq. (43) (we can compose these limits by the algebraic limit theorem as all the individual limits exist and are finite),\n\nf (g∗) − ε < lim\n\nN→∞\n\nE [f (g)] < f (g∗) + ε.\n\nAnd as this holds for any ε, we have,\n\nf (g∗) = lim\n\nN→∞\n\nE [f (g)] .\n\n(49)\n\n(50)\n\nThis result is applicable to the DGP posterior over Gram matrices, as that posterior can be written as,\n\nPN (G1, . . . , GL|X, ̃Y) ∝ h(g)eN L(g),\n\nwhere L(g) is the usual DKM objective,\n\nL(g) = L(G1, . . . , GL)\n\nand h(g) is the remaining terms in the log-posterior which do not depend on N ,\n\n(cid:32)\n\nh(g) = exp\n\n− P +1 2\n\n(cid:33)\n\nlog |Gl|\n\n(cid:88)\n\nl\n\n(this requires P ≤ N so that Gl is full-rank).\n\n(51)\n\n(52)\n\n(53)\n\nD GENERAL MODELS IN THE BAYESIAN REPRESENTATION LEARNING LIMIT\n\nOverall, our goal is to compute the integral in Eq. (6) in the limit as N → ∞. While the integral is intractable for general models such as BNNs, we can use variational inference to reason about its properties. In particular, we can bound the integral using the ELBO,\n\nlog P (Gl|Gl−1) ≥ ELBOl = EQ(Fl) [log P (Gl|Fl) + log P (Fl|Gl−1) − log Q (Fl)] . (cid:0)f l\n\n(54) (cid:1) in the main text, both because the approximate Note that Q (Fl) here is different from Qθl posterior here, Q (Fl) is over all features jointly, Fl, whereas the approximate posterior in the main text is only over a single feature, f l λ, and because in the main text, we chose a specific family of distribution with parameters θl, while here we leave the approximate posterior, Q (Fl) completely unconstrained, so that it has the flexibility to capture the true posterior. Indeed, if the optimal approximate posterior is equal to the true posterior, Q∗ (Fl) = P (Fl|Gl−1, Gl), then the bound is tight, so we get log P (Gl|Gl−1) = ELBO∗ l . Our overall strategy is thus to use variational inference to characterise the optimal approximate which is equal to the true posterior Q∗ (Fl) = P (Fl|Gl−1, Gl) and use the corresponding ELBO to obtain log P (Gl|Gl−1).\n\nλ\n\nD.1 CHARACTERISING EXACT BNN POSTERIORS\n\nRemember that if the approximate posterior family, Q (Fl) is flexible enough to capture the true posterior P (Fl|Gl−1, Gl), then the Q∗ (Fl) that optimizes the ELBO is indeed the true posterior, the bound is tight, so the ELBO is equal to log P (Gl|Gl−1) (Jordan et al., 1999; Blei et al., 2017). Thus, we are careful to ensure that our approximate posterior family captures the true posterior, by ensuring that we only impose constraints on Q (Fl) that must hold for the true posterior,\n\n16\n\nP (Fl|Gl−1, Gl). In particular, note that P (Gl|Fl) in Eq. (5b) constrains the true posterior to give non-zero mass only to Fl that satisfy Gl = 1 φ(Fl)φT (Fl). However, this constraint is difficult Nl to handle. We therefore consider an alternative, weaker constraint on expectations, which holds for the true posterior (the first equality below) because Eq. (5b) constrains Gl = 1 φ(Fl)φT (Fl), and Nl impose the same constraint on the approximate posterior,\n\nGl = EP(Fl|Gl,Gl−1)\n\n(cid:104) 1 Nl\n\n(cid:105) φ(Fl)φT (Fl)\n\n= EQ(Fl)\n\n(cid:104) 1 Nl\n\n(cid:105) φ(Fl)φT (Fl)\n\n.\n\n(55)\n\nNow, we can solve for the optimal Q (Fl) with this constraint on the expectation. In particular, the Lagrangian is obtained by taking the ELBO (Eq. 54), dropping the log P (Gl|Fl) term representing the equality constraint (that Gl = 1 φ(Fl)φT (Fl)) and including Lagrange multipliers for the Nl expectation constraint, Λ, (Eq. 55) and the constraint that the distribution must normalize to 1, Λ,\n\n(cid:90)\n\nL =\n\ndFl Q (Fl) (log P (Fl|Gl−1) − log Q (Fl))\n\n(cid:18)\n\n(cid:18)\n\nΛ\n\nGl −\n\n(cid:90)\n\n+ 1\n\n2 Tr\n\ndFl Q (Fl) φ(Fl)φT (Fl)\n\n(cid:19)(cid:19)\n\n(cid:18)\n\n+ Λ\n\n1 −\n\n(cid:90)\n\n(cid:19)\n\ndFl Q (Fl)\n\n(56)\n\nDifferentiating wrt Q (Fl), and solving for the optimal approximate posterior, Q∗ (Fl),\n\n0 =\n\n∂L ∂ Q (Fl)\n\n(cid:12) (cid:12) (cid:12) (cid:12)Q∗(Fl)\n\n0 = (log P (Fl|Gl−1) − log Q∗ (Fl)) − 1 − 1\n\n2 Tr (cid:0)Λφ(Fl)φT (Fl)(cid:1) − Λ\n\nSolving for log Q∗ (Fl),\n\nlog Q∗ (Fl) = log P (Fl|Gl−1) − 1\n\n2 Tr (cid:0)Λφ(Fl)φT (Fl)(cid:1) + const .\n\nUsing the cyclic property of the trace,\n\nlog Q∗ (Fl) = log P (Fl|Gl−1) − 1\n\n2 Tr (cid:0)φT (Fl)Λφ(Fl)(cid:1) + const .\n\nThus, log Q (Fl) can be written as a sum over features,\n\nlog Q∗ (Fl) =\n\nNl(cid:88)\n\nλ=1\n\n(cid:2)log P (cid:0)f l\n\nλ|Gl−1\n\n(cid:1) − 1\n\n2 φT (f l\n\nλ)Λφ(f l\n\nλ)(cid:3) + const = (cid:80)NL\n\nλ=1 log Q (cid:0)f l\n\nλ\n\nso, the optimal approximate posterior is IID over features,\n\nQ∗ (Fl) = (cid:81)Nl\n\nλ=1 Q∗ (cid:0)f l\n\nλ\n\n(cid:1) .\n\n(57)\n\n(58)\n\n(59)\n\n(60)\n\n(cid:1)\n\n(61)\n\n(62)\n\nRemember that this approximate posterior was only constrained in expectation, and that this constraint held for the true posterior (Eq. 55). Thus, we might think that this optimal approximate posterior would be equal to the true posterior. However, remember that the true posterior had a tighter equality constraint, that Gl = 1 φ(Fl)φT (Fl), while so far we have only imposed a weaker conNl straint in expectation (Eq. 55). We thus need to check that our optimal approximate posterior does indeed satisfy the equality constraint in the limit as Nl → ∞. This be shown using the law of large numbers, as f l λ are IID under the optimal approximate posterior, and by using Eq. (55) for the final equality,\n\nlim Nl→∞\n\n1 Nl\n\nφ(Fl)φT (Fl) = lim\n\nNl→∞\n\n1 Nl\n\nNl(cid:88)\n\nλ=1\n\nφ(f l\n\nλ)φT (f l\n\nλ) = E\n\nQ(f l\n\nλ)\n\n(cid:2)φ(f l\n\nλ)φT (f l\n\nλ)(cid:3) = Gl.\n\n(63)\n\nThus, the optimal approximate posterior does meet the constraint in the limit as Nl → ∞, so in that limit, the true posterior, like the optimal approximate posterior is IID across features,\n\nP (Fl|Gl−1, Gl) = Q∗ (Fl) = (cid:81)Nl\n\nλ=1 Q∗ (cid:0)f l\n\nλ\n\n(cid:1) = (cid:81)Nl\n\nl=1 P (cid:0)f l\n\nλ|Gl−1, Gl\n\n(cid:1) .\n\n(64)\n\n17\n\nD.2 EXACTLY MULTIVARIATE GAUSSIAN DGP POSTERIORS\n\nFor DGPs, we have φ(f l\n\nλ) = f l λ, so the optimal approximate posterior is Gaussian, (cid:0)f l\n\n(cid:1) = log PDGP\n\n(cid:1) − 1\n\nλ + const\n\nλ)T Λf l\n\nλ|Gl−1\n\n(cid:0)f l\n\nDGP\n\n2 (f l\n\nλ\n\nlog Q∗\n\n= − 1\n\n2 (f l\n\n= log N\n\nλ)T (cid:0)Λ + K−1(Gl−1)(cid:1) f l (cid:16)\n\nλ; 0, (cid:0)Λ + K−1(Gl−1)(cid:1)−1(cid:17) f l\n\nλ + const\n\n.\n\n(65)\n\n(66)\n\n(67)\n\nAs the approximate posterior and true posterior are IID, the constraint in Eq. (55) becomes,\n\nGl = E\n\nPDGP(f l\n\nλ|Gl,Gl−1)\n\n(cid:2)f l\n\nλ(f l\n\nλ)T (cid:3) = E\n\nQ∗\n\nDGP(f l\n\nλ)\n\n(cid:2)f l\n\nλ(f l\n\nλ)T (cid:3) = (cid:0)Λ + K−1(Gl−1)(cid:1)−1\n\n.\n\n(68)\n\nAs the Lagrange multipliers are unconstrained, we can always set them such that this constraint holds. In that case both the optimal approximate posterior and the true posterior become,\n\nPDGP\n\n(cid:0)f l\n\nλ|Gl−1Gl\n\n(cid:1) = Q∗\n\nDGP\n\n(cid:0)f l\n\nλ\n\n(cid:1) = N (cid:0)f l\n\nλ; 0, Gl\n\n(cid:1) ,\n\n(69)\n\nas required.\n\nD.3 GENERAL FORM FOR THE CONDITIONAL DISTRIBUTION OVER GRAM MATRICES\n\nNow that we have shown that the true posterior, P (Fl|Gl−1, Gl) factorises, we can obtain a simple form for log P (Gl|Gl−1). In particular, log P (Gl|Gl−1) is equal to the ELBO if we use the true posterior in place of the approximate posterior,\n\nlim Nl→∞\n\n1\n\nN log P (Gl|Gl−1) = lim\n\nNl→∞\n\n1 N\n\nEP(Fl|Gl−1,Gl)\n\n(cid:20)\n\nlog P (Gl|Fl) + log\n\nP (Fl|Gl−1) P (Fl|Gl−1, Gl)\n\n(cid:21)\n\n.\n\n(70)\n\nUnder the posterior, the constraint represented by log P (Gl|Fl) is satisfied, so in the limit we can include that term in a constant,\n\nlim Nl→∞\n\n1\n\nN log P (Gl|Gl−1) = lim\n\nNl→∞\n\n1 N\n\nEP(Fl|Gl−1,Gl)\n\n(cid:20)\n\nlog\n\nP (Fl|Gl−1) P (Fl|Gl−1, Gl)\n\n(cid:21)\n\n+ const .\n\n(71)\n\nNow, we use the fact that the prior, P (Fl|Gl−1) and posterior, P (Fl|Gl−1, Gl), are IID across features,\n\nlim Nl→∞\n\n1\n\nN log P (Gl|Gl−1) = νl E\n\nP(f l\n\nλ|Gl−1,Gl)\n\n(cid:34)\n\nlog\n\nP (cid:0)f l\n\n(cid:1)\n\nλ|Gl−1 λ|Gl−1, Gl\n\nP (cid:0)f l\n\n(cid:35)\n\n(cid:1)\n\n+ const\n\n(72)\n\nand this expectation is a KL-divergence,\n\nlim Nl→∞\n\n1\n\nN log P (Gl|Gl−1) = −νl DKL\n\n(cid:0)P (cid:0)f l\n\nλ|Gl−1, Gl\n\n(cid:1)(cid:13) (cid:13)P (cid:0)f l\n\nλ|Gl−1\n\n(cid:1)(cid:1) + const,\n\n(73)\n\nwhich gives Eq. (32) when we combine with Eq. (8).\n\nD.4 PARAMETRIC APPROXIMATE POSTERIORS\n\nEq. (64) represents a considerable simplification, as we now need to consider only a single feature, f l λ, rather than the joint distribution over all features, Fl. However, in the general case, it is still not possible to compute Eq. (64) because the true posterior over a single feature is still not tractable. Following the true posteriors derived in the previous section, we could chose a parametric approximate posterior that factorises across features,\n\nQθ (F1, . . . , FL) = (cid:81)L\n\nl=1\n\n(cid:81)Nl\n\nλ=1 Qθl\n\n(cid:0)f l\n\nλ\n\n(cid:1) .\n\n(74)\n\nRemember that we optimize the approximate posterior parameters, θ, directly, and set the Gram matrices as a function of θ (Eq. 34). As before, we can bound, log P (Gl=Gθ(θl)|Gl−1) using the\n\n18\n\nELBO, and the bound is tight when the approximate posterior equals the true posterior,\n\nlog P (Gl = Gθ(θl)|Gl−1)\n\n(cid:34)\n\n= E\n\nP(Fl|Gl−1,Gl=Gθ(θθ))\n\nlog P (Gl=Gθ(θl)|Fl) + log\n\nP (cid:0)Fl\n\nλ|Gl−1\n\n(cid:1)\n\nP (Fl|Gl−1, Gl=Gθ(θl))\n\n(cid:34)\n\n≥ EQθ(Fl)\n\nlog P (Gl=Gθ(θl)|Fl) + log\n\nP (cid:0)Fl Qθl\n\nλ|Gl−1 (Fl)\n\n(cid:35)\n\n(cid:1)\n\n.\n\n(cid:35)\n\n(75)\n\n(76)\n\n(77)\n\nNow, we can cancel the log P (Gl = Gθ(θl)|Fl) terms, as they represent a constraint that holds both under the true posterior, and under the approximate posterior,\n\nE\n\nP(Fl|Gl−1,Gl=Gθ(θl)))\n\n(cid:20)\n\nlog\n\nP (Fl|Gl−1) P (Fl|Gl−1, Gl=Gθ(θl))\n\n(cid:21)\n\n≥ EQθl\n\n(Fl)\n\n(cid:20)\n\nlog\n\nP (Fl|Gl−1)\n\nQθl\n\n(Fl)\n\n(cid:21)\n\n.\n\n(78)\n\nUsing the fact that the prior, posterior and approximate posterior are all IID over features, we can write this inequality in terms of distributions over a single feature, f l\n\nλ and divide by Nl,\n\nE\n\nP(f l\n\nλ|Gl−1,Gl=Gθ(θl))\n\n(cid:34)\n\nlog\n\n(cid:1)\n\nP (cid:0)f l\n\nλ|Gl−1 λ|Gl−1, Gl=Gθ(θl)(cid:1)\n\nP (cid:0)f l\n\n(cid:35)\n\n(cid:34)\n\nlog\n\n≥ E\n\nQθl(f l\n\nλ)\n\nP (cid:0)f l\n\nλ|Gl−1(θ)(cid:1) (cid:0)f l Qθl\n\n(cid:1)\n\nλ\n\n(cid:35)\n\n.\n\n(79)\n\nNoting that both sides of this inequality are negative KL-divergences, we obtain, (cid:1)(cid:13) (cid:13)P (cid:0)f l\n\nλ|Gl−1, Gl=Gθ(θl)(cid:1)(cid:13)\n\n(cid:1)(cid:1) ≥ − DKL\n\n(cid:13)P (cid:0)f l\n\n(cid:0)P (cid:0)f l\n\nλ|Gl−1\n\n− DKL\n\n(cid:0)f l\n\n(cid:0)Qθl\n\nλ\n\nλ|Gl−1\n\n(cid:1)(cid:1) ,\n\n(80)\n\nwhich gives Eq. (33) in the main text.\n\nE THEORETICAL SIMILARITIES IN REPRESENTATION LEARNING IN FINITE\n\nAND INFINITE NETWORKS\n\nIn the main text, we considered probability densities of the Gram matrices, G1, . . . , GL. However, we can also consider probability densities of the features, F1, . . . , FL, for a DGP, 2 log |K (GDGP (Fl−1))| − 1\n\nl K−1 (GDGP (Fl−1)) Fl\n\nlog P (Fl|Fl−1) = − Nl\n\n(cid:1) + const .\n\n2 tr (cid:0)FT\n\n(81)\n\nWe can rewrite the density such that it is still the density of features, Fl, but it is expressed in terms of the DGP Gram matrix, log P (Fl|Fl−1) = − Nl\n\n2 tr (cid:0)K−1(Gl−1)Gl\n\n2 log |K(Gl−1)| − Nl\n\n(cid:1) + const .\n\n(82)\n\nHere, we have used the cyclic property of the trace to combine the Fl and FT l to form Gl, and we have used the fact that our kernels can be written as a function of the Gram matrix. Overall, we can therefore write the posterior over features, P (F1, . . . , FL|X, ̃Y), in terms of only Gram matrices,\n\nJ (G1, . . . , GL) = 1\n\nN log P (F1, . . . , FL|X, ̃Y) = log P (Y|GL) + 1\n\nN\n\nL (cid:88)\n\nl=1\n\nlog P (Fl|Fl−1) ,\n\n(83)\n\nsubstituting Eq. (82),\n\nJ (G1, . . . , GL) = log P (Y|GL) − 1\n\n2\n\n(cid:80)L\n\nl=1νl\n\n(cid:0)log |K(Gl−1)| + tr (cid:0)K−1(Gl−1)Gl\n\n(cid:1)(cid:1)\n\n+ const .\n\n(84)\n\nThus, J (G1, . . . , GL) does not depend on N , and thus the Gram matrices that maximize J (G1, . . . , GL) are the same for any choice of N . The only restriction is that we need Nl ≥ P , to ensure that the Gram matrices are full-rank.\n\nTo confirm these results, we used Adam with a learning rate of 10−3 to optimize full-rank Gram matrices with Eq. (84) and to directly do MAP inference over features using Eq. (81). As expected, as the number of features increased, the Gram matrix from MAP inference over features converged rapidly to that expected using Eq. (84) (Fig. 5).\n\n19\n\nFigure 5: RMSE of trained Gram matrices between one-hidden-layer (first row) and two-hiddenlayer (second row) DGPs of various width trained by gradient descent and the corresponding MAP limit. Columns correspond to different datasets (trained on a subset of 50 datapoints).\n\nF ADDITIONAL EXPERIMENTAL DETAILS\n\nTo optimize the analytic DKM objective for DGPs and the variational DKM objective for DGPs (Figs. 3–11), we parameterised the Gram matrices (or covariances for the variational approximate posterior) as the product of a square matrix, Rl ∈ RP ×P , with itself transposed, Gl = 1 P RlRT l , and we used Adam with a learning rate of 10−3 to learn Rl. To do Bayesian inference in finite BNNs and DGPs, we used Langevin sampling with 10 parallel chains, and a step size of 10−3. Note that in certain senarios, Langevin sampling can be very slow, as the features have a Gaussian prior with covariance K(Gl−1) which has some very small and some larger eigenvalues, which makes sampling difficult. Instead, we reparameterised the model in terms of the standard Gaussian random variables, Vl ∈ RP ×Nl. We then wrote Fl in terms of Vl,\n\nFl = Ll−1Vl.\n\n(85)\n\nHere, Ll−1 is the Cholesky of K(Gl−1), so K(Gl−1) = Ll−1LT l−1. This gives an equivalent distribution P (Fl|Fl−1). Importantly, as the prior on Vl is IID standard Gaussian, sampling Vl is much faster. To ensure that the computational cost of these expensive simulations remained reasonable, we used a subset of 50 datapoints from each dataset.\n\nFor the DKM objective for BNNs, we used Monte-Carlo to approximate the Gram matrices,\n\nGθ(θl) ≈\n\nK (cid:88)\n\nk=1\n\nφ(f l\n\nk)φT (f l\n\nk).\n\n(86)\n\nwith f l k drawn from the appropriate approximate posterior, and K = 216. We can use the reparameterisation trick (Kingma & Welling, 2013; Rezende et al., 2014) to differentiate through these Monte-Carlo estimates.\n\nG ADDITIONAL COMPARISONS WITH FINITE-WIDTH DGPS\n\nIn particular, we Here, we give additional results supporting those in Sec. 4.6, Fig. 3–Fig. 11. give the DGP representations learned by two-layer networks on all UCI datasets (boston, concrete, energy, yacht), except those already given in the main text Fig. 6–8.\n\n20\n\n0.000.050.100.150.20RMSEbostonyachtconcreteone-layerenergy2123252729211width0.000.050.100.150.20RMSEG1G22123252729211width2123252729211width2123252729211widthtwo-layerFigure 6: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of energy. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\nFigure 7: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of boston. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\n21\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101init DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101Figure 8: One hidden layer DGP and DKM with squared exponential kernel trained on a subset of concrete. First and second row: initializations of DGP and DKM. Third and fourth row: trained DGP (by Langevin sampling) and DKM Gram matrices and kernels.\n\n22\n\ninit DKMG1K(G1)G2K(G2)init DGPtrained DGP150indextrained DKM150index150index150index150index150indexK(G0)150indexG3101H THE FLOW POSTERIOR IN A 2-LAYER BNN\n\nHere, we give the 2-layer version (Fig. 9) of Fig. 4 in the main text, which again shows a close match between the variational DKM with a flow posterior, and the BNN true posterior.\n\nFigure 9: Two-layer ReLU BNN and variational DKM with flow. a Initialized (first row) and learned Gram matrices of a width 1024 BNN (second row), vDKM with flow (third row) and vDKM with multivariate Gaussian (fourth row) using 214 Monte-Carlo samples. The Gram matrices between BNN and vDKM (flow) match closely after training. (MvG). b Marginal PDF over features at each layer for one input datapoint using kernel density estimation. The marginal PDFs of BNN are nonGaussian (blue curves), vDKM with flow is able to capture the non-Gaussianity and match closely with BNNs marginals (red curves).\n\n23\n\ninitG1G2vDKM (flow)150indexvDKM (MvG)150index505feature0.00.5density505feature150index150indexG0150indexG30.750.000.75abBNNvDKM (flow)BNNI MULTIVARIATE GAUSSIAN APPROXIMATE POSTERIORS IN DEEPER\n\nNETWORKS\n\nIn particular, we hypothesised that depth is an important factor.\n\nThere is a body of theoretical work (e.g. (Seroussi & Ringel, 2021)), on BNNs that approximates BNN posteriors over features as Gaussian. While we have shown that this is a bad idea in general (Fig. 4 and 9), we can nonetheless ask whether there are circumstances where the idea might work well. In particular, in shallow networks, in order to get GL close to the required representation, we may need the posterior over Fl to be quite different from the prior. In contrast, in deeper networks, we might expect the posterior over Fl to be closer to its (Gaussian) prior, and therefore we might Gaussian approximate posteriors to work better.\n\nHowever, we cannot just make the network deeper, because as we do so, we apply the nonlinearity more times and dramatically alter the network’s inductive biases. To resolve this issue, we derive a leaky relu nonlinearity that allows (approximately) independent control over the inductive biases (or effective depth) and the actual depth (Appendix I.1). Using these nonlinearities, we indeed find that very deep networks are reasonably well approximated by multivariate Gaussian approximate posteriors (Appendix I.2).\n\nI.1 LEAKY RELU NONLINEARITIES\n\nOur goal is to find a pointwise nonlinearity, φ, such that (under the prior), λ)(cid:3) = p E\n\nλ)reluT (f l\n\nλ)φT (f l\n\n(cid:2)relu(f l\n\n(cid:2)φ(f l\n\nE\n\nPDGP(f l\n\nλ|Gl−1)\n\nP(f l\n\nλ|Gl−1)\n\nλ)(cid:3) + (1 − p)Gl−1.\n\n(87)\n\nWe will set p = α/L, where α is the “effective” depth of the network and L is the real depth. These networks are designed such that their inductive biases in the infinite width limit are similar to a standard relu network with depth α. Indeed, we would take this approach if we wanted a well-defined infinite-depth DKM limit.\n\nWithout loss of generality, we consider a 2D case, where x and y are zero-mean bivariate Gaussian,\n\nπ(x, y) = N\n\n(cid:19)\n\n(cid:18)(cid:18)x y\n\n; 0,\n\n(cid:18)Σxx Σxy Σxy Σyy\n\n(cid:19)(cid:19)\n\n(88)\n\nwhere π(x, y) is the probability density for the joint distribution. Note that we use a scaled relu, (cid:26)√\n\nrelu(x) =\n\n0\n\n2 x for 0 < x otherwise\n\nsuch that E (cid:2)relu2(x)(cid:3) = Σxx. Mirroring Eq. 87, we want the nonlinearity, φ, to satisfy,\n\nE (cid:2)φ(x2)(cid:3) = p E (cid:2)relu2(x)(cid:3) + (1 − p)Σxx = Σxx E (cid:2)φ(y2)(cid:3) = p E (cid:2)relu2(y)(cid:3) + (1 − p)Σyy = Σyy\n\nE [φ(x)φ(y)] = p E [relu(x)relu(y)] + (1 − p)Σxy\n\nWe hypothesise that this nonlinearity has the form,\n\nφ(x) = a relu(x) + bx.\n\nWe will write the relu as a sum of x and |x|,\n\nrelu(x) = 1√\n\n2\n\n(x + |x|),\n\nbecause E [f (x, y)] = 0 for f (x, y) = x|y| or f (x, y) = |x|y. It turns out that we get zero expectation for all functions where f (−x, −y) = −f (x, y), which holds for the two choices above. To show such functions have a zero expectation, we write out the integral explicitly,\n\nE [f (x, y)] =\n\n(cid:90) ∞\n\n(cid:90) ∞\n\ndx\n\n−∞\n\n−∞\n\ndy π(x, y)f (x, y).\n\n(93)\n\n24\n\n(89)\n\n(90a)\n\n(90b)\n\n(90c)\n\n(91)\n\n(92)\n\nWe split the domain of integration for y at zero,\n\nE [f (x, y)] =\n\n(cid:90) ∞\n\n(cid:90) 0\n\ndx\n\n−∞\n\n−∞\n\ndy π(x, y)f (x, y) +\n\n(cid:90) ∞\n\n(cid:90) ∞\n\ndx\n\n−∞\n\n0\n\ndy π(x, y)f (x, y).\n\n(94)\n\nWe substitute y′ = −y and x′ = −x in the first integral, (cid:90) ∞\n\n(cid:90) ∞\n\nE [f (x, y)] =\n\ndx′\n\ndy′ π(−x′, −y′)f (−x′, −y′) +\n\n−∞\n\n0\n\n(cid:90) ∞\n\n(cid:90) ∞\n\ndx\n\n−∞\n\n0\n\ndy π(x, y)f (x, y).\n\n(95)\n\nAs the variables we integrate over are arbitrary we can relabel y′ as y and x′ as x, and we can then merge the integrals as their limits are the same,\n\nE [f (x, y)] =\n\n(cid:90) ∞\n\n(cid:90) ∞\n\ndx\n\n−∞\n\n0\n\ndy [π(−x, −y)f (−x, −y) + π(x, y)f (x, y)] .\n\nUnder a zero-mean Gaussian, π(−x, −y) = π(x, y),\n\nE [f (x, y)] =\n\n(cid:90) ∞\n\n(cid:90) ∞\n\ndx\n\n−∞\n\n0\n\ndy π(x, y) (f (−x, −y) + f (x, y)) .\n\n(96)\n\n(97)\n\nThus, if f (−x, −y) = −f (x, y), then the expectation of that function under a bivariate zero-mean Gaussian distribution is zero.\n\nRemember that our overall goal was to design a nonlinearity, φ, (Eq. 91) which satisfied Eq. (90). We therefore compute the expectation,\n\nE [φ(x)φ(y)] = E [(a relu(x) + bx) (a relu(y) + by)]\n\n= E\n\n(cid:104)(cid:16) a√\n\n2\n\n(x + |x|) + bx\n\n(cid:17) (cid:16) a√\n\n(y + |y|) + by\n\n(cid:17)(cid:105)\n\n2\n\nUsing the fact that E [ x|y| ] = E [ |x|y ] = 0 under a multivariate Gaussian,\n\n(cid:104)\n\n= E\n\na2 1√\n\n2\n\n(x + |x|) 1√ 2\n\n= a2 E [relu(x)relu(y)] +\n\n(y + |y|) + (cid:16)√\n\n2ab + b2(cid:17)\n\n(cid:16)√\n\n2ab + b2(cid:17)\n\nxy\n\n(cid:105)\n\nE [xy] .\n\n√\n\np.\n\na =\n\nThus, we can find the value of a by comparing with Eq. (90c),\n\np = a2\n\nFor b, things are a bit more involved,\n\n√\n\n1 − p =\n\n2ab + b2 = (cid:112)2p b + b2\n\n(98)\n\n(99)\n\n(100)\n\n(101)\n\n(102)\n\n(103)\n\nwhere we substitute for the value of a. This can be rearranged to form a quadratic equation in b,\n\n0 = b2 + (cid:112)2p b + (p − 1),\n\nwhich can be solved,\n\nb = 1 2\n\nb = 1 2\n\n(cid:16)\n\n(cid:17) −(cid:112)2p ± (cid:112)2p − 4(p − 1)\n\n(cid:16)\n\n−(cid:112)2p ± (cid:112)4 − 2p\n\n(cid:17)\n\nb = −\n\n(cid:113) p\n\n2 ±\n\n(cid:113)\n\n1 − p\n\n2\n\nOnly the positive root is of interest,\n\nb =\n\nThus, the nonlinearity is,\n\n(cid:113)\n\n1 − p\n\n2 −\n\n(cid:113) p\n\n2\n\n√\n\nφ(x) =\n\np relu(x) +\n\n(cid:16)(cid:113)\n\n1 − p\n\n2 −\n\n(cid:113) p\n\n(cid:17)\n\n2\n\nx\n\n25\n\n(104)\n\n(105)\n\n(106)\n\n(107)\n\n(108)\n\n(109)\n\nFigure 10: Comparison of posterior feature marginal distributions between a BNN of width 1024 (trained by Langevin sampling over features) and a variational DKM with 216 Monte-Carlo samples, in a 4-layer (row 1) and a 32-layer (row 2) network. We give the BNN posterior features from Langevin sampling (blue histogarm) and the best fitting Gaussian (blue line), and compare against the variational DKM approximate posterior Gaussian distribution (red line).\n\nwhere we set p = α/L, and remember we used the scaled relu in Eq. (89). Finally, we established these choices by considering only the cross term, E [φ(x)φ(y)]. We also need to check that the E (cid:2)φ2(x)(cid:3) and E (cid:2)φ2(y)(cid:3) terms are as required (Eq. 90a and Eq. 90b). In particular,\n\nE (cid:2)φ2(x)(cid:3) = E\n\n(cid:104)\n\n(a relu(x) + bx)2(cid:105)\n\n= E\n\n(cid:20)(cid:16) a√\n\n2\n\n(x + |x|) + bx\n\n(cid:17)2(cid:21)\n\n(110)\n\nusing E [x|x|] = 0 as x|x| is an odd function of x, and the zero-mean Gaussian is an even distribution,\n\nE (cid:2)φ2(x)(cid:3) = a2 E (cid:2)relu2(x)(cid:3) +\n\n(cid:16)√\n\n2ab + b2(cid:17)\n\nΣxx\n\nusing Eq. (102) to identify a2 and Eq. (103) to identify\n\n√\n\n2ab + b2,\n\nE (cid:2)φ2(x)(cid:3) = p E (cid:2)relu2(x)(cid:3) + (1 − p)Σxx,\n\nas required.\n\nI.2 MULTIVARIATE GAUSSIAN IN DEEPER NETWORKS\n\n(111)\n\n(112)\n\nIn the main text, we show that a more complex approximate posterior can match the distributions in these networks. Here, we consider an alternative approach. In particular, we hypothesise that these distributions are strongly non-Gaussian because the networks are shallow, meaning that the posterior needs to be far from the prior in order to get a top-layer kernel close to GL+1. We could therefore make the posteriors closer to Gaussian by using leaky-relu nonlinearities (Appendix I.1) with fixed effective depth (α = 2), but increasing real depth, L. In particular, we use multivariate Gaussian approximate posteriors with learned means,\n\nQθl\n\n(cid:0)f l\n\nλ\n\n(cid:1) = N (cid:0)f l\n\nλ; μl, Σl\n\n(cid:1)\n\nso\n\nθl = (μl, Σl).\n\n(113)\n\nAs expected, for a depth 32 network, we have much more similar marginals (Fig. 10 top) and learned representations (Fig. 11 top).\n\n26\n\n50501density=1505=2505=35054 layers=4505feature01density=8505feature=16505feature=24505feature32 layers=32BNN vDKMFigure 11: Comparison of Gram matrices between BNN of width 1024 (trained by Langevin sampling over features) and variational DKM, in 4-layer (row 1-3) and 32-layer networks (row 4-6). Initializations are shown in row 1 and 4, trained BNN Gram matrices are shown in row 2 and 5, and trained variational DKM Gram matrices are shown in row 3 and 6. As in Figure 10, the variational DKM is a poor match to Langevin sampling in a BNN for a 4-layer network, but is very similar in a 32 layer network.\n\n27\n\nG1G2G3G4G8G16G24G32150index150index150index150index150index150indexG0150indexGL+1101 32 layers BNNinitvDKMvDKM4 layers BNNinitJ UNIMODALITY IN LINEAR DEEP KERNEL MACHINES\n\nJ.1 THEORY: UNIMODALITY WITH A LINEAR KERNEL AND SAME WIDTHS\n\nHere, we show that the deep kernel machine objective is unimodal for a linear kernel. A linear kernel simply returns the input Gram matrix,\n\nK (G) = G.\n\n(114)\n\nIt is called a linear kernel, because it arises in the neural network setting (Eq. 21) by choosing the nonlinearity, φ to be the identity, in which case, Fl = Fl−1Wl−1. For a linear kernel the objective becomes,\n\nL(G1, ..., GL) = (cid:80)L+1\n\nl=1\n\nνl 2\n\n(cid:0)log (cid:12)\n\n(cid:12)G−1\n\nl−1Gl\n\n(cid:12) (cid:12) − Tr (cid:0)G−1\n\nl−1Gl\n\n(cid:1)(cid:1)\n\n(115)\n\nwhere we have assumed there is no output noise, σ2 = 0. Taking all νl to be equal, ν = νl (see Appendix J.2 for the general case),\n\nL(G1, ..., GL) = log (cid:12)\n\n(cid:12)G−1\n\n0 GL+1\n\n(cid:12) (cid:12) − ν\n\n2\n\n(cid:80)L+1\n\nl=1 Tr (cid:0)G−1\n\nl−1Gl\n\n(cid:1) .\n\n(116)\n\nNote that G0 and GL+1 are fixed by the inputs and outputs respectively. Thus, to find the mode, we set the gradient wrt G1, . . . , GL to zero,\n\n0 =\n\n∂L ∂Gl\n\n= ν 2\n\n(cid:0)G−1\n\nl−1 − G−1\n\nl Gl+1G−1\n\nl\n\nThus, at the mode, the recursive relationship must hold,\n\nT = G−1\n\nl−1Gl = G−1\n\nl Gl+1.\n\nThus, optimal Gram matrices are given by,\n\nand we can solve for T by noting,\n\nGl = G0Tl,\n\nG−1\n\n0 GL+1 = TL+1.\n\n(cid:1)\n\n(117)\n\n(118)\n\n(119)\n\n(120)\n\nImportantly, T is the product of two positive definite matrices, T = G−1 l−1Gl, so T must have positive, real eigenvalues (but T does not have to be symmetric (Horn & Johnson, 2012)). There is only one solution to Eq. (120) with positive real eigenvalues (Horn et al., 1994). Intuitively, this can be seen using the eigendecomposition, G−1\n\n0 GL+1 = V−1DV, where D is diagonal,\n\nT = (cid:0)V−1DV(cid:1)1/(L+1)\n\n= V−1D1/(L+1)V.\n\n(121)\n\nThus, finding T reduces to finding the (L + 1)th root of each positive real number on the diagonal of D. While there are (L + 1) complex roots, there is only one positive real root, and so T and hence G1, . . . , GL are uniquely specified. This contrasts with a deep linear neural network, which has infinitely many optimal settings for the weights.\n\nNote that for the objective to be well-defined, we need K(G) to be full-rank. With standard kernels (such as the squared exponential) this is always the case, even if the input Gram matrix is singular. However, a linear kernel will have a singular output if given a singular input, and with enough data points, G0 = 1 XXT ) to be ν0 given by applying a positive definite kernel (such as a squared exponential) to 1 XXT . This results ν0 in positive definite G0, as long as the input points are distinct.\n\nXXT is always singular. To fix this, we could e.g. define G0 = K( 1 ν0\n\nJ.2 THEORY: UNIMODALITY WITH A LINEAR KERNEL AND ARBITRARY WIDTHS\n\nIn the main text we showed that the deep kernel machine is unimodal when all νl are equal. Here, we show that unimodality in linear DKMs also holds for all choices of νl. Recall the linear DKM objective in Eq. (115),\n\nL(G1, ..., GL) = (cid:80)L+1 = (cid:80)L+1\n\nl=1\n\nl=1\n\nνl 2\nνl 2\n\n(cid:12)G−1\n\nl−1Gl\n\n(cid:0)log (cid:12) (cid:0)log |Gl| − log |Gl−1| − Tr(cid:0)G−1\n\n(cid:12) (cid:12) − Tr (cid:0)G−1\n\nl−1Gl\n\n(cid:1)(cid:1)\n\nl−1Gl\n\n(cid:1)(cid:1) .\n\n(122)\n\n(123)\n\n28\n\nTo find the mode, again we set the gradient wrt Gl to zero,\n\n0 =\n\n∂L ∂Gl\n\n= − νl+1−νl\n\n2 G−1\n\nl − νl\n\n2 G−1\n\nl−1 + νl+1\n\n2 G−1\n\nl Gl+1G−1\n\nl\n\n,\n\n(124)\n\nfor l = 1, ..., L. Right multiplying by 2Gl and rearranging,\n\nνl+1G−1\n\nl Gl+1 = νlG−1\n\nl−1Gl + (νl+1 − νl) I,\n\nfor l = 1, ..., L.\n\n(125)\n\nEvaluating this expression for l = 1 and l = 2 gives,\n\nν2G−1 ν3G−1\n\n1 G2 = ν1G−1 2 G3 = ν2G−1\n\n0 G1 + (ν2 − ν1) I, 1 G2 + (ν3 − ν2) I = ν1G−1\n\n0 G1 + (ν3 − ν1) I.\n\nRecursing, we get,\n\nνlG−1\n\nl−1Gl = ν1G−1\n\n0 G1 + (νl − ν1) I.\n\n(126)\n\n(127)\n\n(128)\n\nCritically, this form highlights constraints on G1. In particular, the right hand side, G−1 l−1Gl, is the product of two positive definite matrices, so has positive eigenvalues (but may be non-symmetric (Horn & Johnson, 2012)). Thus, all eigenvalues of ν1G−1 0 G1 must be larger than ν1 − νl, and this holds true at all layers. This will become important later, as it rules out inadmissible solutions.\n\nGiven G0 and G1, we can compute any Gl using,\n\nG−1\n\n0 Gl =\n\n(cid:33)\n\nνl′\n\nG−1\n\n0 Gl =\n\n(cid:32) l\n\n(cid:89)\n\nl′=1\n\nl (cid:89)\n\nl′=1\n\nl (cid:89)\n\nl′=1\n\n(cid:0)G−1\n\nl′−1Gl′\n\n(cid:1) =\n\n(cid:81)l\n\n1 l′=1 νl′\n\nl (cid:89)\n\nl′=1\n\n(cid:0)νl′G−1\n\nl′−1Gl′\n\n(cid:1)\n\n(cid:0)ν1G−1\n\n0 G1 + (νl′ − ν1) I(cid:1)\n\n(129)\n\n(130)\n\nwhere the matrix products are ordered as (cid:81)L using our knowledge of GL+1. Computing G−1 (cid:32)L+1 (cid:89)\n\n(cid:33)\n\nL+1 (cid:89)\n\nνl\n\nG−1\n\n0 GL+1 =\n\nl=1\n\nl=1\n\nWe write the eigendecomposition of ν1G−1\n\n0 G1 as, 0 G1 = VDV−1.\n\nν1G−1\n\nl=1 Al = A1 · · · AL. Now, we seek to solve for G1 0 GL+1,\n\n(cid:0)ν1G−1\n\n0 G1 + (νl − ν1) I(cid:1) .\n\n(131)\n\n(132)\n\nThus,\n\n(cid:33)\n\nνl\n\nG−1\n\n0 GL+1 =\n\n(cid:32)L+1 (cid:89)\n\nl=1\n\nwhere Λ is a diagonal matrix,\n\nΛ =\n\nL+1 (cid:89)\n\nl=1\n\nL+1 (cid:89)\n\nl=1\n\n(cid:0)VDV−1 + (νl − ν1) I(cid:1) = VΛV−1\n\n(133)\n\n(D + (νl − ν1) I) .\n\n(134)\n\nThus, we can identify V and Λ by performing an eigendecomposition of the known matrix, (cid:16)(cid:81)L+1 0 GL+1. Then, we can solve for D (and hence G1) in terms of Λ and V. The\n\n(cid:17)\n\nl=1 νl\n\nG−1 diagonal elements of D satisfy,\n\n0 = −Λii +\n\nL+1 (cid:89)\n\nk=1\n\n(Dii + (νl − ν1)) .\n\n(135)\n\nThis is a polynomial, and remembering the constraints from Eq. (128), we are interested in solutions which satisfy,\n\nν1 − νmin ≤ Dii.\n\n(136)\n\n29\n\nwhere,\n\nνmin = min (ν1, . . . , νL+1) .\n\n(137)\n\nTo reason about the number of such solutions, we use Descartes’ rule of signs, which states that the number of positive real roots is equal to or a multiple of two less than the number of sign changes in the coefficients of the polynomial. Thus, if there is one sign change, there must be one positive real root. For instance, in the following polynomial,\n\n0 = x3 + x2 − 1\n\n(138)\n\nthe signs go as (+), (+), (−), so there is only one sign change, and there is one real root. To use Descartes’ rule of signs, we work in terms of D′\n\nii, which is constrained to be positive,\n\n0 ≤ D′\n\nii = Dii − (ν1 − νmin)\n\nDii = D′\n\nii + (ν1 − νmin) .\n\n(139)\n\nThus, the polynomial of interest (Eq. 135) becomes,\n\n0 = −Λii +\n\nL+1 (cid:89)\n\nl=1\n\n(D′\n\nii + (ν1 − νmin) − (ν1 − νl)) = −Λii +\n\nL+1 (cid:89)\n\nl=1\n\n(D′\n\nii + (νl − νmin))\n\n(140)\n\nwhere 0 < νl − νmin as νmin is defined to be the smallest νl (Eq. 137). Thus, the constant term, −Λii is negative, while all other terms, D′ ii)L+1 in the polynomial have positive coefficients. Thus, there is only one sign change, which proves the existence of only one valid real root, as required.\n\nii, . . . , (D′\n\nK UNIMODALITY EXPERIMENTS WITH NONLINEAR KERNELS\n\nFor the posterior over Gram matrices to converge to a point distribution, we need the DKM objective L(G1, . . . , GL) to have one unique global optimum. As noted above, this is guaranteed when the prior dominates (Eq. 11), and for linear models (Appendix J). While we believe that it might be possible to construct counter examples, in practice we expect a single global optimum in most practical settings. To confirm this expectation, we did a number of experiments, starting with many different random initializations of a deep kernel machine and optimizing using gradient descent (Appendix K). In all cases tested, the optimizers converged to the same maximum.\n\nP VlVT\n\nl with Vl ∈ RP ×P being trainable parameters. To We parameterise Gram matrices Gl = 1 make initializations with different seeds sufficiently separated while ensuring stability we initialize Gl from a broad distribution that depends on K(Gl−1). Specifically, we first take the Cholesky decomposition K(Gl−1) = Ll−1LT l where each entry of Ξl ∈ RP ×P is independently sampled from a standard Gaussian, and Dl is a diagonal scaling matrix with each entry sampled i.i.d. from an inverse-Gamma distribution. The variance of the inverse-Gamma distribution is fixed to 100, and the mean is drawn from a uniform distribution U [0.5, 3] for each seed. Since for any random variable x ∼ Inv-Gamma(α, β), E(x) = β (α−1)(α−2) , once we fix the mean and variance we can compute α and β as\n\nl−1, then set Vl = Ll−1ΞlD1/2\n\nα−1 and V(x) =\n\nβ\n\nα =\n\nE(x)2 V(x) β = E(x)(α − 1).\n\n+ 2,\n\n(141)\n\n(142)\n\nWe set νl = 5, and use the Adam optimizer (Kingma & Ba, 2014) with learning rate 0.001 to optimize parameters Vl described above. We fixed all model hyperparameters to ensure that any multimodality could emerge only from the underlying deep kernel machine. As we did not use inducing points, we were forced to consider only the smaller UCI datasets (yacht, boston, energy and concrete). For the deep kernel machine objective, all Gram matrices converge rapidly to the same solution, as measured by RMSE (Fig. 12). Critically, we did find multiple modes for the MAP objective (Fig 13), indicating that experiments are indeed powerful enough to find multiple modes (though of course they cannot be guaranteed to find them). Finally, note that the Gram matrices took a surprisingly long time to converge: this was largely due to the high degree of diversity in the initializations; convergence was much faster if we initialised deterministically from the prior.\n\n30\n\nFigure 12: One-layer DKMs with squared exponential kernel trained on full UCI datasets (through columns) converges to the same solution, despite very different initializations by applying stochastic diagonal scalings described in Appendix F to the standard initialization with different seeds. Standard initialization is shown in dashed line, while scaled initializations are the color lines each denoting a different seed. The first row shows the objective during training for all seeds that all converge to the same value. The second row shows the element-wise RMSE between the Gram matrix of each seed and the optimized Gram matrix obtained from the standard initialization. RMSE converges to 0 as all initializations converge on the same maximum. The last row plots RMSE versus objective value, again showing a single optimal objective value where all Gram matrices are the same.\n\nThis might contradict our usual intuitions about huge multimodality in the weights/features of BNNs and DGPs. This can be reconciled by noting that each mode, written in terms of Gram matrices, corresponds to (perhaps infinitely) many modal features. In particular, in Sec. E, we show that the log-probability for features, P (Fl|Fl−1) (Eq. 82) depends only on the Gram matrices, and note that there are many settings of features which give the same Gram matrix. In particular, the Gram matrix is the same for any unitary transformation of the features, F′ l = FlU, satisfying UUT = I, as l = 1 1\nl = Gl. For DGPs we can use any unitary matrix, so there Nl are infinitely many sets of features consistent with a particular Gram matrix, while for BNNs we can only use permutation matrices, which are a subset of unitary matrices. Thus, the objective landscape must be far more complex in the feature domain than with Gram matrices, as a single optimal Gram matrix corresponds to a large family of optimal features.\n\nFlUlUT\n\nl = 1\n\nFlFT\n\nl FT\n\nlF′T\n\nF′\n\nNl\n\nNl\n\n31\n\n500490480objectiveyacht860850840boston111011001090energy174017301720concrete050000100000iters0.00.51.0RMSE050000100000iters050000100000iters050000100000iters500490480objective0.00.51.0RMSE860850840objective111011001090objective174017301720objectiveFigure 13: One-layer DGP with MAP inference over features as described in Appendix E Eq. (84). Rows and columns are the same as in Figure 12. Using the same randomly scaled initializations described above, we are able to find multiple modes in energy and concrete showing our initializations are diverse enough, albeit there is still only a single global optimum.\n\n32\n\n46504700objectiveyacht64606510boston1240012450energy1575015800concrete050000100000iters0.00.51.0RMSE050000100000iters0100000200000iters050000100000iters46504700objective0.00.51.0RMSE64606510objective1240012450objective1575015800objectiveL INDUCING POINT DKMS\n\nTo do large-scale experiments on UCI datasets, we introduce inducing point DKMs by extending Gaussian process inducing point methods (Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017) to the DKM setting. This approach uses the variational interpretation of the deep kernel machine objective described in Appendix D.\n\nTo do inducing-point variational inference, we need to explicitly introduce top-layer features mirroring FL+1 ∈ RP ×νL+1 in Appendix B, but replicated N times, ̃FL+1 ∈ RP ×NL+1. Formally, each feature, ̃f L+1\n\n, . . . , ̃f L+1 NL+1\n\n1\n\nis IID, conditioned on FL, λ=1N (cid:0) ̃f L+1 λ=1N (cid:0) ̃yλ; ̃f L+1\n\nP ( ̃FL+1|FL) = (cid:81)Nl P ( ̃Y| ̃FL+1) = (cid:81)Nl\n\nλ\n\nλ\n\n; 0, K(G(FL))(cid:1) , , σ2I(cid:1) ,\n\n(143a)\n\n(143b)\n\nwhere we give the likelihood for regression, but other likelihoods (e.g. for classification) are possible (Appendix B).\n\nFurther, we take the total number of points, P , to be made up of Pi inducing points and Pt test/train points, so that P = Pi + Pt. Thus, we can separate all features, Fl ∈ RP ×Nl, into the inducing t ∈ RPt×Nl. Likewise, we separate the inputs, features, Fl X, and outputs, Y, into (potentially trained) inducing inputs, Xi, and trained inducing outputs, Yi, and the real test/training inputs, Xt, and outputs, Yt,\n\ni ∈ RPi×Nl, and the test/train features, Fl\n\nFl =\n\n(cid:19)\n\n(cid:18)Fl i\nFl t\n\n ̃FL+1 =\n\n(cid:18) ̃FL+1 ̃FL+1\n\ni\n\nt\n\n(cid:19)\n\nX =\n\n(cid:19)\n\n(cid:18)Xi Xt\n\nY =\n\n(cid:19)\n\n(cid:18)Yi Yt\n\n ̃Y =\n\n(cid:19)\n\n(cid:18) ̃Yi ̃Yt\n\n(144)\n\nWe follow the usual doubly stochastic inducing point approach for DGPs. In particular, we treat all the features at intermediate layers, F1, . . . , FL, and the top-layer train/test features, FL+1 as latent variables. However, we deviate from the usual setup in treating the top-layer inducing outputs, FL+1 , as learned parameters and maximize over them to ensure that the ultimate method does not require sampling, and at the same time allows minibatched training. The prior and approximate posterior over F1, . . . , FL are given by,\n\ni\n\nt\n\nQ (F1, . . . FL|X) = (cid:81)L P (F1, . . . , FL|X) = (cid:81)L\n\nl=1 Q (Fl|Fl−1) , l=1 P (Fl|Fl−1) ,\n\n(145a)\n\n(145b)\n\nand remember F0 = X, so G0 = 1 N0 factorises into a distribution over the inducing points and a distribution over the test/train points,\n\nXXT . The prior and approximate posterior at each layer\n\nQ (Fl|Fl−1) = P (cid:0)Fl P (Fl|Fl−1) = P (cid:0)Fl\n\nt |Fl t |Fl\n\ni , Fl−1 i , Fl−1\n\n(cid:1) Q (cid:0)Fl (cid:1) P (cid:0)Fl\n\n(cid:1) , i |Fl−1\n\ni\n\ni\n\n(cid:1) .\n\n(146a)\n\n(146b)\n\nthe approximate posterior samples for the test/train points is the conditional prior (cid:1), which is going to lead to cancellation when we compute the ELBO. Likewise,\n\nCritically, P (cid:0)Fl t |Fl the approximate posterior over ̃FL+1\n\ni , Fl−1\n\nt\n\nis the conditional prior, (cid:1) = P (cid:0) ̃FL+1\n\n, FL\n\n|FL+1\n\nt\n\ni\n\nQ (cid:0) ̃FL+1\n\nt\n\n|FL+1\n\ni\n\n, FL\n\n(cid:1) .\n\n(147)\n\nConcretely, the prior approximate posterior over inducing points are given by,\n\nQ (cid:0)Fl\n\nλ=1N (cid:0)f l λ=1N (cid:0)f l The approximate posterior is directly analogous to Eq. (69) and the prior is directly analogous to Eq. (1a), but where we have specified that this is only over inducing points. Now we compute the ELBO\n\ni;λ; 0, Gl i;λ; 0, K(G(Fl−1\n\n(cid:1) = (cid:81)Nl (cid:1) = (cid:81)Nl\n\ni |Fl−1\n\nP (cid:0)Fl\n\n(148b)\n\n(148a)\n\n))(cid:1)\n\n(cid:1) ,\n\nii\n\ni\n\ni\n\ni\n\nELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL ii ) (cid:34)\n\n= EQ\n\nlog P (cid:0) ̃Yt| ̃FL+1\n\nt\n\ni\n\n|FL+1 |FL+1\n\ni\n\n, FL\n\n, FL\n\n(cid:1) P (F1, . . . FL|X) (cid:1) Q (F1, . . . FL|X)\n\n(cid:35)\n\n(149)\n\n(cid:1) + log\n\nP (cid:0) ̃FL+1 Q (cid:0) ̃FL+1\n\nt\n\nt\n\n33\n\nNote that the P (cid:0)Fl we come to describing sampling), substituting Eq. (145–147) and cancelling P (cid:0)Fl P (cid:0) ̃FL+1\n\n(cid:1) terms are going to cancel in the ELBO, we consider them below when (cid:1) and\n\ni , Fl−1\n\ni , Fl−1\n\n|FL+1\n\nt |Fl\n\nt |Fl\n\n(cid:1),\n\n, FL\n\nt\n\ni\n\nELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL\n\nii ) = EQ\n\n(cid:34)\n\nlog P (cid:0) ̃Yt| ̃FL+1\n\nt\n\n(cid:1) +\n\nL (cid:88)\n\nl=1\n\nlog\n\nP (cid:0)Fl\n\ni |Fl−1 (cid:1)\n\ni\n\nQ (cid:0)Fl\n\ni\n\n(cid:35)\n\n(cid:1)\n\n.\n\n(150)\n\nSo far, we have treated the Gram matrices, Gl ii as parameters of the approximate posterior. However, in the infinite limit N → ∞, these are consistent with the features generated by the approximate (cid:0)Fl posterior. In particular the matrix product 1 can be written as an average over infinitely Nl many IID vectors, f l i;λ (first equality), and by the law of large numbers, this is equal to the expectation of one term (second equality), which is Gl\n\nii (by the approximate posterior Eq. (148a)),\n\nFl i\n\n(cid:1)T\n\ni\n\nlim N→∞\n\n1 Nl\n\nFl\n\ni\n\n(cid:1)T\n\n(cid:0)Fl\n\ni\n\n= lim\n\nN→∞\n\n1 Nl\n\n(cid:80)Nl\n\nλ=1f l\n\ni;λ\n\n(cid:1)T\n\n(cid:0)f l\n\ni;λ\n\n= E\n\nQ(f l\n\ni;λ)\n\n(cid:104)\n\nf l i;λ\n\n(cid:0)f l\n\ni;λ\n\n(cid:1)T (cid:105)\n\n= Gl ii.\n\n(151)\n\nBy this argument, the Gram matrix from the previous layer, Gl−1 through Gl−1 DGP, Fl factorise. Thus, in the infinite limit, individual terms in the ELBO can be written,\n\nis deterministic. Further, in a (Eq. 5), and the prior and approximate posterior\n\ni only depends on Fl−1\n\nii\n\ni\n\ni\n\n(cid:34)\n\nlim N→∞\n\n1 N\n\nEQ\n\nlog\n\nP (cid:0)Fl\n\ni |Fl−1 (cid:1)\n\ni\n\nQ (cid:0)Fl\n\ni\n\n(cid:35)\n\n(cid:1)\n\n\n\n= νl EQ\n\nlog\n\n(cid:16)\n\nP\n\ni;λ|Gl−1 f l (cid:16)\n\ni (cid:17)\n\nQ\n\nf l i;λ\n\n(cid:17)\n\n\n\n\n\n= −νl DKL\n\n(cid:0)N (cid:0)0, K(Gl−1\n\ni\n\n)(cid:1)(cid:13)\n\n(cid:13)N (cid:0)0, Gl\n\ni\n\n(152)\n\n(153)\n\n(cid:1)(cid:1) ,\n\nwhere the final equality arises when we notice that the expectation can be written as a KLdivergence. The inducing DKM objective, Lind, is the ELBO, divided by N to ensure that it remains finite in the infinite limit,\n\nLind(FL+1\n\ni\n\n, G1\n\nii, . . . , GL\n\nii )= lim\n\nN→∞\n\n1\n\nN ELBO(FL+1\n\ni\n\n, G1\n\nii, . . . , GL ii )\n\n(154)\n\n= EQ\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nt\n\n(cid:1)(cid:3) −\n\nL (cid:88)\n\nl=1\n\nνl DKL\n\n(cid:0)N (cid:0)0, K(Gl−1\n\nii\n\n)(cid:1)(cid:13)\n\n(cid:13)N (cid:0)0, Gl\n\nii\n\n(cid:1)(cid:1) .\n\nNote that this has almost exactly the same form as the standard DKM objective for DGPs in the main text (Eq. 16). In particular, the second term is a chain of KL-divergences, with the only difference that these KL-divergences apply only to the inducing points. The first term is a “performance” term that here depends on the quality of the predictions given the inducing points. As the copies are IID, we have,\n\nEQ\n\n(cid:2)log P (cid:0) ̃Yt| ̃FL+1\n\nt\n\n(cid:1)(cid:3) = N EQ\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nt\n\n(cid:1)(cid:3) .\n\n(155)\n\n(cid:2)log P (cid:0)Yt|FL+1\n\nNow that we have a simple form for the ELBO, we need to compute the expected likelihood, (cid:1)(cid:3). This requires us to compute the full Gram matrices, including test/train EQ points, conditioned on the optimized inducing Gram matrices. We start by defining the full Gram matrix,\n\nt\n\nGl =\n\n(cid:18)Gl Gl\n\nii Gl ti Gl\n\ntt\n\nit\n\n(cid:19)\n\n(156)\n\nfor both inducing points (labelled “i”) and test/training points (labelled “t”) from just Gl ii. For clarity, we have Gl ∈ RP ×P , Gl tt ∈ RPt×Pt, where Pi is the number of inducing points, Pt is the number of train/test points and P = Pi + Pt is the total number of inducing and train/test points.\n\nii ∈ RPi×Pi, Gl\n\nti ∈ RPt×Pi, Gl\n\nThe conditional distribution over Fl\n\nt given Fl\n\ni is, λ=1N (cid:0)f l (cid:1) = (cid:81)Nl\n\nP (cid:0)Fl\n\nt\n\n(cid:12) (cid:12)Fl\n\ni , Gl−1\n\nt;λ; KtiK−1\n\nii f l\n\ni;λ, Ktt·i\n\n(cid:1)\n\n(157)\n\n34\n\nAlgorithm 1 DKM prediction\n\nl=1\n\nii}L\n\nParameters: {νl}L Optimized Gram matrices {Gl Inducing and train/test inputs: Xi, Xt Inducing outputs: FL+1 Initialize full Gram matrix (cid:19) (cid:18)G0 G0\n\n(cid:18)XiXT XtXT\n\ni XiXT i XtXT\n\nii G0;T ti G0\n\n= 1 ν0\n\nl=1\n\ntt\n\nti\n\ni\n\nt\n\nt\n\n(cid:19)\n\nPropagate full Gram matrix for l in (1, . . . , L) do\n\n(cid:19)\n\n(cid:18)Kii KT ti Kti Ktt\n\n= K\n\nii\n\n(cid:18)(cid:18)Gl−1 Gl−1 ii KT ti .\n\nti\n\n(cid:19)(cid:19)\n\n)T\n\n(Gl−1 Gl−1\n\nti\n\ntt\n\nii\n\niiK−1\n\nii KT\n\nii Gl ii Gl\n\nti = KtiK−1 tt = KtiK−1\n\nKtt·i = Ktt − KtiK−1 Gl Gl end for Final prediction using standard Gaussian process expressions (cid:18)Kii KT (cid:18)(cid:18)GL (GL ii ti GL GL Kti Ktt tt ti , Ktt − KtiK−1\n\n= K Yt ∼ N (cid:0)KtiK−1\n\nti + Ktt·i\n\nti + σ2I(cid:1)\n\nii FL+1\n\nii KT\n\nti )T\n\n(cid:19)(cid:19)\n\n(cid:19)\n\ni\n\nwhere f l feature for all train/test inputs, and f l\n\nt;λ is the activation of the λth feature for all train/test inputs, f l i;λ, and (cid:19)\n\n(cid:18)Kii KT ti Kti Ktt\n\n= K\n\n(cid:16) 1\n\nNl−1\n\n(cid:17)\n\nFl−1FT\n\nl−1\n\nKtt·i = Ktt − KtiK−1\n\nii KT ti .\n\n= K (Gl−1)\n\n(158)\n\n(159)\n\ni;λ is the activation of the λth\n\nIn the infinite limit, the Gram matrix becomes deterministic via the law of large numbers (as in Eq. 151), and as such Git and Gtt become deterministic and equal to their expected values. Using Eq. (157), we can write,\n\nwhere Ξ is a matrix with IID standard Gaussian elements. Thus,\n\nFl\n\nt = KtiK−1\n\nii Fl\n\ni + K1/2\n\ntt·i Ξ.\n\nGl\n\nt (Fl\n\nE (cid:2)Fl ν KtiK−1\n\nti = 1 ν\n= 1 = KtiK−1\n\nii\n\nii Gl\n\nii\n\ni )T (cid:3) E (cid:2)Fl\n\ni (Fl\n\ni )T (cid:3)\n\nand,\n\nGl\n\nt (Fl\n\nE (cid:2)Fl\n\ntt = 1 ν\n= 1 = KtiK−1\n\nt )T (cid:3) E (cid:2)Fl ii GiiK−1\n\nν KtiK−1\n\nii\n\ni (Fl ii KT\n\ni )T (cid:3) K−1 ti + Ktt·i\n\nii KT\n\nti + 1\n\nν K1/2\n\ntt·i\n\n(160)\n\n(161)\n\n(162)\n\n(163)\n\n(164)\n\n(165)\n\n(166)\n\nE (cid:2)ΞΞT (cid:3) K1/2\n\ntt·i\n\nFor the full prediction algorithm, see Alg. 1.\n\n35",
  "translations": [
    "# Summary Of The Paper\n\nLarge-width networks converge to GPs with fixed/ unlearnable kernels, making the networks unable to perform representation learning. In this paper, the authors proposed a simple recipe to allow for representation learning: making the logit layer as wide as the hidden layers and replicating the labels accordingly. The authors argue that this approach allows the kernel to evolve and possible learning a representation. The authors also support their claims using synthetical dataset experiments and UCI datasets.\n\n# Strength And Weaknesses\n\n# Strength.\n1. The paper proposed simple and interesting ideas that allow feature learning in the infinite-width network, which is worth further exploration. \n2. Some experiments from the paper show non-Gaussian behavior of the learned representation that differs from the NNGP limit.  \n\n# Weakness\n\n1. The notations and terminologies are confusing, which makes it hard to parse the paper. \n2. Missing key empirical comparison (vs. NNGP/NTK) for representation learning. I would like to see a comparison using CIFAR10. \n3. Several strong claims that require further justifications.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n1. Many strong claims from the paper are unconvincing. \n\n- Using `**the** representation learning limit` is not proper; there can be many of them, and the proposed approach is just one of many. \n\n - I can't agree with the claim that the approach is `extremely theoretically tractable.` (in the abstract) and it is misleading. Unlike NNGP, the solution can be written analytically as an exact closed-form formula of the input data. The approach in the paper is not analytically tractable (except possibly in the linear network setting) and requires approximation methods. Cited from the paper, \"In practice, the true posteriors required to evaluate Eq. (21) are intractable \".  \n\n- \"We show that DKMs can be scaled to large datasets using inducing point methods from the Gaussian process literature, and we show that DKMs exhibit superior performance to other kernel-based approaches\" (from the abstract.) This is certainly an overclaim. The largest dataset used in UCI, could not justify *superior performance* and scalability to *large datasets*. Again, I expect at least experimental results on CIFAR10 if not more complicated. \n\n- I would like to re-emphasize that the dataset used here (UCI) is too simple to capture *representation learning*. I would expect a comparison using CIFAR-10 against benchmark results for NNGP kernels; see Lee et al. finite vs infinite (https://arxiv.org/abs/2007.15801); neural kernel without tangents (Shankar, https://arxiv.org/abs/2003.02237). I am also interested in the visualization of the learned representation using the image dataset. \n\n2. Notations and terminologies. \n\n- The definition of DNNs is confusing in Sec 3. Do you mean a fully-connected network with trainable parameters that are optimized using gradient descent? This is what DNNs mean for most people in ML. In addition, what does that mean to marginalize the weights in each layer in eq (2)? Do you mean sequentially? Can you verbally explain the difference between (3a) and (3b)? \n\n-  DGP \"Deep Gaussian Processes\" (by Andreas C. Damianou, Neil D. Lawrence, not cited in the paper) is a standard framework. Is it the same thing used in the paper? If so, why not cite the above (or related ) paper? \n\n-  Notations in equation (11) are confusing. What are $G_l/G_{l-1}$ here? Don't they depend on $N$ ? Or are they some *infinite/deterministic* objects that don't rely on $N$, and you overload the notations? Same for equations (12), etc. In addition, Eq (13) seems to be a result in \"Wide Bayesian neural networks have a simple weight posterior: theory and accelerated sampling\" by Hron et al. Please clarify. \n\n3. Others. \n- First citation in the intro. Please cite \"Radford M. Neal. Priors for infinite networks,\" which is the first NNGP (one-hidden layer) paper. \"Lee et al Deep Neural Networks as Gaussian Processes\"  should also be added.\n\n# Summary Of The Review\n\nUpdate: \nI thank the authors for the clarification/updates of several notations/terminologies. I like the idea from the paper, a Bayesian prospective of feature learning. I have increased my score accordingly. However, there are still a lot of room for improvement and several open questions. To name some,  \n\n1. Even though this is a theory paper, authors should not shy away from more empirical work. Even doing a small scale experiments on Cifar10 (baseline against kernel) could really helpful to both practitioners and theorists. \n\n2. Authors should have more detailed discussion/comparison between the meanfield/feature learning limit of neural networks and the current proposed Bayesian framework, both theoretically and empirically. E.g., baselining performance (bayesian feature learning vs NN feature learning), are they learning similar features, etc. \n\n---------------------------------\n\nOverall, I think the idea from the paper is interesting and worth further exploration. However, I also find the paper hard to parse due to clarity issues from notations and terminology, and the theoretical and empirical results from the paper do not support the strong claims from the paper.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces a novel framework for representation learning in neural networks, specifically through a new infinite-width limit termed the Bayesian representation learning limit. This approach retains the advantages of representation learning while simplifying the analysis of deep Gaussian processes (DGPs), which are shown to yield zero-mean multivariate Gaussian posteriors. The authors develop the concept of deep kernel machines (DKMs) as a generalized, flexible approach to kernel methods, and introduce a sparse DKM variant that scales linearly with the number of data points, enhancing computational efficiency. Empirical validation demonstrates the superior performance of sparse DKMs on UCI datasets, reinforcing the theoretical insights provided.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative theoretical contribution, particularly the introduction of the Bayesian representation learning limit. This new limit effectively bridges the gap between traditional infinite-width theories and practical representation learning, showcasing strong theoretical foundations alongside empirical validation. The development of DKMs and their sparse variant addresses computational challenges commonly encountered in kernel methods, marking a significant advancement. However, the paper could benefit from a more extensive exploration of the limitations of the proposed methods and potential challenges in their application to larger, more complex datasets outside of UCI.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers with a background in machine learning and statistics. The quality of the writing is high, with thorough explanations of the methodologies and results. The novelty of the approach is significant, as it reinterprets representation learning within a Bayesian framework, while the reproducibility is supported by detailed descriptions of the experimental setup and results on benchmark datasets.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and novel framework for understanding representation learning in deep models, supported by strong theoretical contributions and empirical validation. The introduction of the Bayesian representation learning limit and the development of sparse deep kernel machines address important challenges in the field, making the work both impactful and relevant.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel theoretical framework termed the Bayesian Representation Learning Limit, which allows for the retention of representation learning within deep Bayesian models, specifically Deep Gaussian Processes (DGPs). Unlike traditional infinite-width limits that discard this feature, the proposed framework establishes that DGPs yield multivariate Gaussian posteriors under this limit. The authors also present a new objective function that integrates log-likelihood and KL-divergences to optimize posterior covariances, and introduce the concept of Deep Kernel Machines (DKMs), which generalize kernel methods while providing a sparse version to enhance scalability from cubic to linear complexity. Empirical results demonstrate that sparse DKMs outperform conventional methods on various UCI datasets, affirming the practical effectiveness of the theoretical insights.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative theoretical contributions, particularly the introduction of the Bayesian Representation Learning Limit, which provides a deeper understanding of representation learning in deep models. Additionally, the development of sparse DKMs presents a meaningful advancement for practical applications, facilitating the handling of larger datasets. The empirical validation is robust, with strong alignment between theoretical predictions and experimental results. However, a notable limitation is the complexity involved in extending the framework to non-Gaussian scenarios, such as Bayesian Neural Networks (BNNs), which may necessitate further approximation techniques. Furthermore, the sensitivity to prior assumptions and the potential computational costs associated with large network implementations are concerns that may impact the usability of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The theoretical insights are novel and significant, marking a substantial advancement in the understanding of representation learning. The experimental design is rigorous, and the results are presented with clarity, supporting reproducibility. The inclusion of graphical models enhances comprehension of the generative processes involved. However, the dependence on prior assumptions could complicate reproducibility in practice, as different choices may lead to varying performance outcomes.\n\n# Summary Of The Review\nThis paper makes a significant contribution to the field of representation learning by introducing a new theoretical framework that integrates deep learning and traditional kernel methods. Its empirical results strongly support the theoretical claims, although challenges remain in extending the methods to non-Gaussian cases and ensuring efficiency in large networks.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel theoretical framework for representation learning in neural networks, termed the \"Bayesian representation learning limit,\" which retains the advantages of representation learning while simplifying the infinite-width model limitations. The authors propose Deep Gaussian Processes (DGPs) that yield multivariate Gaussian posteriors and introduce Deep Kernel Machines (DKMs) as a deep generalization of kernel methods, alongside a sparse DKM implementation that achieves linear scaling with data points. Experimental results demonstrate that the DKM objective effectively aligns with the behavior of wide, finite-width networks, outperforming traditional methods on various datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative theoretical contributions, particularly the introduction of the Bayesian representation learning limit, which addresses gaps in existing infinite-width frameworks. The development of DKMs provides a practical approach to scaling kernel methods. However, the paper could enhance its empirical validation by including more diverse datasets and additional baselines beyond standard shallow kernel methods. Furthermore, while the theoretical insights are compelling, the practical implications and limitations of the proposed methods could be explored in greater depth.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written with clear definitions and a logical flow of ideas, making it accessible to readers familiar with the domain. The presentation of mathematical formulations and experimental setups is adequate, though some sections could benefit from additional explanations to enhance understanding. The novelty of the proposed approaches is significant, as it bridges the gap between deep learning and kernel methods. The reproducibility of the experiments is supported by detailed methodological descriptions, though sharing code and data would further facilitate verification of results.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in understanding representation learning in neural networks through the introduction of a new theoretical framework and practical methodologies. While the contributions are substantial and well-articulated, there is room for improvement in empirical validation and practical exploration of the proposed methods.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to representation learning in deep Bayesian models, introducing the concept of a Bayesian representation learning limit specifically for Deep Gaussian Processes (DGPs). The authors propose deep kernel machines (DKMs) as a scalable alternative that maintains performance while overcoming the limitations of traditional kernel methods. The methodology involves a rigorous theoretical framework linking the new learning limit to established neural network and kernel methods, followed by empirical validation on selected datasets. The findings suggest that DKMs can achieve performance comparable to wide but finite DGPs, while also offering improvements in scalability through a sparse representation.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its novel contribution to the field of representation learning and its theoretical rigor, which provides a solid foundation for understanding the proposed methods. The empirical results lend support to the theoretical claims, demonstrating the practical applicability of DKMs. However, the paper has weaknesses, including a reliance on IID data assumptions, which may not hold in real-world scenarios. The complexity of the proposed methods could hinder adoption, and the focus on specific models limits the generalizability of the findings. Additionally, the exploration of hyperparameters and the potential for overfitting are not adequately addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas clearly, although the theoretical constructs may be challenging for practitioners without a strong mathematical background. The novelty of the research is significant, particularly regarding the introduction of the Bayesian representation learning limit and the development of DKMs. However, reproducibility could be enhanced with more detailed discussions on hyperparameter tuning and the implementation of the proposed methods.\n\n# Summary Of The Review\nThis paper makes a meaningful contribution to the field of representation learning in Bayesian models by introducing a new theoretical limit and validating it with empirical evidence. While the theoretical framework and experimental results are commendable, the practical applicability of the methods and the assumptions made warrant further exploration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel theoretical framework called the \"Bayesian representation learning limit,\" which allows for a deeper understanding of representation learning in neural networks, particularly in the context of Deep Gaussian Processes (DGPs). The authors propose \"deep kernel machines\" (DKMs) as a flexible extension of kernel methods, highlighting a new objective function that combines log-likelihood and KL-divergence terms to retain representation learning capabilities in infinite-width models. Empirical validation demonstrates that DKMs can effectively approximate the performance of finite-width models, showcasing their potential across various datasets.\n\n# Strength And Weaknesses\nThe principal strength of this paper lies in its innovative theoretical contributions, specifically the introduction of the Bayesian representation learning limit and the development of DKMs, which provide a fresh perspective in the intersection of kernel methods and deep learning. The empirical results support the theoretical claims convincingly, showing substantial improvements over traditional kernel methods. However, a potential weakness is the complexity of the proposed framework, which may pose challenges for practical implementation and understanding compared to more established methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, making it accessible to both theoretical and empirical audiences. The quality of the work is high, with rigorous mathematical formulation and comprehensive empirical validation. The novelty is significant, as it not only extends existing theories but also proposes a new class of models that bridge kernel methods with deep learning. Reproducibility is supported through detailed descriptions of the methodology and experiments, though additional implementation details might benefit practitioners wishing to apply the findings.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the understanding of representation learning in neural networks by proposing a novel theoretical framework and practical model. The rigorous theoretical backing, coupled with strong empirical validation, positions this work as a meaningful advancement in the field of machine learning.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel theoretical framework for adversarial training in deep neural networks (DNNs), termed the \"Bayesian adversarial training limit.\" This framework captures the complexities of adversarial examples, addressing the limitations of traditional models. Key contributions include the characterization of adversarial posteriors as multivariate Gaussians, the derivation of an interpretable objective function that incorporates both log-likelihood and KL-divergence terms, and a connection between adversarial training and kernel methods, leading to the concept of \"deep adversarial machines\" (DAMs). The authors also present a scalable implementation of DAMs and validate their theoretical findings through empirical experiments that demonstrate alignment with the behavior of wide but finite networks.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to adversarial training through the introduction of the Bayesian adversarial training limit and its connection to kernel methods, which enhances understanding and applicability in high-dimensional spaces. The empirical validation of the theoretical claims is another strong point, showcasing the practical relevance of the proposed methods. However, the paper could improve by providing a more comprehensive discussion on the implications of its findings for real-world applications, as well as expanding the scope of experiments to include a wider variety of datasets and architectures, which would bolster the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making complex theoretical concepts accessible. The quality of the writing is high, and the methodology is well-explained, allowing for reproducibility of the empirical results. The novelty of the contributions is significant, particularly in how it reframes adversarial training within a Bayesian context and connects it to established kernel methods.\n\n# Summary Of The Review\nOverall, this paper presents a substantial advancement in the theoretical understanding of adversarial training in neural networks through the introduction of the Bayesian adversarial training limit. Its innovative connections to kernel methods and the empirical validation of the theoretical framework make it a valuable contribution to the field, despite some minor shortcomings in broader applicability discussions and experimental scope.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a theoretical framework for representation learning in deep neural networks (DNNs) and deep Gaussian processes (DGPs), claiming to provide foundational insights into the field. The authors introduce a new infinite-width limit that purportedly preserves representation learning, propose the concept of deep kernel machines (DKMs) as a deep generalization of kernel methods, and present a novel objective function combining log likelihood and KL-divergences. They also discuss a sparse DKM for improved computational efficiency and extend their findings to Bayesian neural networks (BNNs). Despite promising empirical results, the claims of transformative impact and novelty are overstated.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its ambitious attempt to unify concepts in representation learning and kernel methods while proposing a theoretically interesting infinite-width limit. However, the weaknesses are significant; the claims regarding the novelty of DKMs and the new objective function do not represent substantial advancements over existing methods. Additionally, the empirical results lack sufficient substantiation, and the authors' failure to adequately engage with related work diminishes the perceived significance of their contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-structured and clear, certain sections, particularly those presenting the theoretical contributions, could benefit from more rigorous justification and clearer exposition. The novelty of the proposed concepts, such as DKMs and the new objective function, is questionable, as they appear to refine existing ideas rather than introduce fundamentally new ones. Reproducibility may be a concern due to the lack of comprehensive details regarding the empirical validation of the claims.\n\n# Summary Of The Review\nThe paper makes an ambitious attempt to advance the theory of representation learning but ultimately falls short in terms of the novelty and significance of its contributions. Claims made about the transformative potential of the proposed methods are overstated and do not align with the findings presented. The work may serve as a starting point for further exploration but requires more rigorous validation and contextualization within the existing literature.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel theoretical framework for understanding representation learning in neural networks, particularly within the context of Deep Gaussian Processes (DGPs). It introduces a new infinite-width limit that retains representation learning, showing that as the width of the neural network increases, the prior becomes increasingly influential. The authors derive that DGP posteriors can be characterized as zero-mean multivariate Gaussians and propose an innovative objective for optimizing posterior covariances through KL-divergence regularization. Additionally, Sparse Deep Kernel Machines (DKMs) are developed, which demonstrate linear scalability with respect to the number of datapoints, outperforming traditional methods that scale cubically. Empirical results highlight the advantages of DKMs over Maximum A Posteriori (MAP) estimates and shallow kernel methods across various datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear theoretical contributions to the understanding of representation learning within deep Bayesian models, along with empirical evidence supporting its claims. The introduction of Sparse DKMs is a significant advancement, offering practical scalability and improved performance metrics. However, the paper could benefit from a more thorough discussion of its limitations, particularly regarding the specific conditions under which the proposed methods are most effective. Additionally, while the experiments are convincing, the scope could be expanded to include more diverse datasets or comparison with other state-of-the-art methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings in a clear and logical manner, making it accessible to readers familiar with deep learning and Bayesian methods. The quality of the writing is high, with a thorough review of related work that establishes the novelty of the authors' contributions. The reproducibility of the results is somewhat supported by the detailed methodology and experimental setup, although providing additional implementation details or code could enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of representation learning by providing a theoretical foundation for DGPs and introducing Sparse DKMs that significantly improve scalability and performance. While the paper is clear and well-argued, it could benefit from a more comprehensive exploration of limitations and broader empirical validation.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper critiques the traditional approach of infinite width limits in neural networks, particularly focusing on the implications of these limits on representation learning. It proposes a Bayesian representation learning limit that aims to preserve representation capabilities by rescaling the likelihood to match the prior. The authors assert that under this new framework, the posteriors of deep Gaussian processes (DGPs) are exactly multivariate Gaussian and claim that deep kernel machines (DKMs) offer more flexibility than standard kernel methods. However, these assertions are accompanied by various assumptions regarding scalability, model behavior, and the independence of layer representations, which may not hold in practical scenarios.\n\n# Strength And Weaknesses\nThe paper presents several innovative ideas, particularly regarding the Bayesian representation learning limit and the flexibility of DKMs. However, it also has notable weaknesses, such as its reliance on potentially oversimplified assumptions, including the Gaussian posterior assumption and the independence of layer representations. The critique of infinite width limits is relevant but may benefit from a more thorough exploration of alternative approaches that better accommodate representation learning. Additionally, the empirical results mainly derive from finite DGPs, raising questions about their generalizability to diverse scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-structured, some sections could benefit from additional clarity, particularly concerning the interpretation of the DKM objective and the assumptions made throughout the analysis. The novelty of the proposed methods is significant, yet the reproducibility of the findings may be compromised due to fixed hyperparameters and a lack of extensive empirical validation across different data types. More comprehensive testing and clearer explanations of complex concepts would enhance the overall quality of the work.\n\n# Summary Of The Review\nThe paper introduces important critiques and novel methodologies regarding infinite width limits and representation learning in neural networks. However, several assumptions and a lack of extensive empirical validation limit the robustness and applicability of the findings. Overall, while the paper contributes valuable insights, further refinement and testing are necessary for its claims to be fully substantiated.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel concept termed the Bayesian representation learning limit, which seeks to improve representation learning in deep Bayesian models, particularly within the framework of Deep Gaussian Processes (DGPs). The authors demonstrate that under this limit, DGPs yield multivariate Gaussian posteriors and introduce deep kernel machines (DKMs) as a flexible generalization of traditional kernel methods. Additionally, a sparse DKM variant is proposed to enhance scalability, making the framework applicable to larger datasets and complex models.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative theoretical development of the Bayesian representation learning limit, which effectively bridges the gap between representation learning and deep Bayesian modeling. The results showcasing multivariate Gaussian posteriors in DGPs provide significant insights and a solid foundation for further research. However, the paper could benefit from more extensive empirical validation of the proposed methods, particularly the sparse DKM, to better illustrate its advantages over existing techniques. Additionally, some theoretical aspects may be challenging for readers without a strong background in Bayesian models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, with clear explanations of complex concepts, though some sections may require further elaboration for accessibility. The quality of the contributions is high, offering a unique perspective on the interplay between DGPs and representation learning. The novelty of the Bayesian representation learning limit and the DKM framework is commendable, although the practical reproducibility of the findings could be improved by providing detailed experimental setups and datasets used in the evaluation.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in the understanding of representation learning within deep Bayesian models, backed by theoretical contributions and an innovative framework. While the proposed methods show promise, further empirical validation and clearer exposition could enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to addressing significant challenges in machine learning, particularly in the areas of representation learning and model scalability. The authors introduce a new theoretical framework that integrates advanced concepts to enhance the understanding and performance of learning systems. The methodology involves the development of new algorithms rooted in this framework, with findings indicating improved efficiency and effectiveness in various learning tasks.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Contributions:** The paper introduces original theoretical insights that contribute positively to the existing body of knowledge in machine learning.\n2. **Theoretical Foundation:** The framework is well-developed and provides a comprehensive basis for the proposed methodologies, facilitating a deeper understanding of the implications of the findings.\n3. **Relevance:** The work addresses timely and critical issues in the field, such as model complexity and generalization, which are highly relevant to the current research landscape.\n4. **Clarity of Presentation:** The structure and logical flow of the paper enhance comprehension, with appropriate terminology used for the intended audience.\n\n**Weaknesses:**\n1. **Lack of Empirical Validation:** The empirical results presented are limited, which undermines the robustness of the theoretical claims. More extensive experiments are needed.\n2. **Limited Comparisons:** The paper lacks comparisons with existing state-of-the-art methods, which would help to contextualize the proposed approaches and demonstrate their advantages.\n3. **Assumptions and Limitations:** Some underlying assumptions in the theoretical framework may restrict the applicability of the methods. A discussion addressing these limitations would provide a more rounded perspective.\n4. **Complexity:** The introduction of new methodologies may add complexity, potentially affecting usability and implementation in practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, facilitating comprehension of complex ideas. The quality of the theoretical contributions is high, although the reproducibility of results may be hampered by the lack of comprehensive empirical validation. The novelty of the theoretical framework is commendable, yet the empirical aspects need to catch up to enhance the overall impact.\n\n# Summary Of The Review\nThis paper makes a significant contribution to the field of machine learning through its innovative theoretical insights and proposed methodologies. However, the empirical validation is insufficient, and addressing the identified weaknesses would substantially improve the work's relevance and impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a new theoretical framework referred to as the Bayesian representation learning limit, which enhances understanding of representation learning in deep neural networks by retaining key aspects of finite-width models in an infinite-width context. This novel limit leads to the derivation of multivariate Gaussian posteriors in Deep Gaussian Processes (DGPs) and proposes deep kernel machines (DKMs) as a flexible and scalable generalization of traditional kernel methods. The paper further presents a sparse DKM variant that addresses the computational inefficiencies of cubic scaling typically associated with kernel methods. Empirical results demonstrate that the learned feature distributions align closely with those from wide but finite DGPs, suggesting the practical applicability of DKMs.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to bridging the gap between deep representation learning and traditional kernel methods, offering a theoretically sound and scalable solution to representation challenges. The introduction of the Bayesian representation learning limit is a significant contribution, as it retains important characteristics of finite-width models while simplifying the infinite-width scenario. However, a potential weakness is that the complexity of deriving exact posteriors in Bayesian Neural Networks (BNNs) is acknowledged but not thoroughly addressed, which may limit the practical implementation of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The theoretical underpinnings are articulated effectively, making complex ideas accessible. The quality of the writing and presentation is high, with a logical flow that aids comprehension. However, while the theoretical aspects are well-explained, the paper could benefit from additional details regarding the reproducibility of empirical results, particularly in the implementation of the proposed DKMs and their comparison to existing methods.\n\n# Summary Of The Review\nOverall, this paper presents a compelling theoretical advancement in representation learning by introducing the Bayesian representation learning limit and proposing deep kernel machines as a practical extension of kernel methods. While the contributions are significant and well-articulated, there remains room for improvement in addressing practical implementation challenges and ensuring reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel theoretical framework for understanding representation learning in neural networks, termed the Bayesian representation learning limit. This new limit allows for the preservation of representation learning characteristics similar to those found in finite-width models, as opposed to the infinite width limits observed in traditional Neural Network Gaussian Processes (NNGPs). The authors propose Deep Kernel Machines (DKMs) as a flexible and scalable generalization of kernel methods, presenting a sparse DKM that scales linearly with the number of data points. Empirical validation of the proposed methods confirms the alignment of Gaussian posteriors in Deep Gaussian Processes (DGPs) with those of wide finite DGPs, showcasing the effectiveness of the DKM objective for practical applications.\n\n# Strength And Weaknesses\nThe strengths of this paper include its introduction of a new theoretical limit that maintains key aspects of representation learning, which is a significant advancement over existing approaches. The development of the DKM framework provides practical relevance, as it addresses scalability issues inherent in traditional kernel methods. However, the paper could benefit from more extensive empirical evaluations across diverse datasets to strengthen the claims regarding the superiority of DKMs over existing methods. Additionally, the clarity of some theoretical discussions could be improved to enhance accessibility for a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a coherent structure that effectively conveys its contributions. The novelty of the approach is significant, particularly with the introduction of the Bayesian representation learning limit and the formulation of DKMs. However, some sections, especially those dealing with theoretical formulations, may be challenging for readers without a strong background in the relevant mathematical concepts. The reproducibility of the results appears to be supported by empirical validation; however, the authors should consider providing supplementary materials or code to facilitate further exploration by the research community.\n\n# Summary Of The Review\nOverall, this paper presents a compelling theoretical framework that bridges traditional kernel methods with modern deep learning through the introduction of the Bayesian representation learning limit and the DKM approach. While the contributions are noteworthy, further empirical validation and clarity in theoretical discussions would enhance the paper's impact and accessibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents significant advancements in the theoretical understanding of representation learning in neural networks, focusing on the relationship between deep learning and kernel methods. The authors introduce an infinite width limit that preserves representation learning, analyze the behavior of deep Gaussian processes (DGPs) within this framework, and propose the deep kernel machine (DKM) alongside scalable versions. Additionally, the findings extend to Bayesian neural networks (BNNs), providing a comprehensive theoretical foundation that links deep learning practices to established kernel methods.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous theoretical framework, which effectively bridges gaps in the current understanding of representation learning. The introduction of the Bayesian representation learning limit and its implications for DGPs and BNNs contributes valuable insights to the field. However, a potential weakness is the limited empirical validation in certain scenarios, which may restrict the practical applicability of the theoretical findings. While the paper discusses experimental setups, further exploration of diverse datasets could enhance its robustness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, clearly articulating complex ideas with appropriate technical language and logical organization. Figures and tables are effectively utilized to aid comprehension of the theoretical concepts. The originality of the contributions is noteworthy, providing a fresh perspective on the interplay between neural networks and kernel methods. The reproducibility of the results appears to be ensured through detailed methodological descriptions, although additional empirical experiments could further solidify this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a compelling theoretical framework that advances our understanding of representation learning in neural networks, linking it to kernel methods in an innovative manner. While the theoretical contributions are strong, the empirical validation could be expanded to enhance practical relevance.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper explores the concept of representation learning within Bayesian neural networks, specifically focusing on Deep Gaussian Processes (DGPs). The authors introduce a new limit termed the Bayesian representation learning limit, which allows for the preservation of representation learning as model width approaches infinity. They derive the characteristics of the posterior distribution in DGPs, demonstrating that it can be represented as a multivariate Gaussian. The paper also presents a novel objective function for DGPs that combines log-likelihood optimization with KL-divergence penalties to maintain proximity to prior distributions. Additionally, the authors propose Deep Kernel Machines (DKMs) as a flexible generalization of kernel methods, achieving linear scaling with respect to data points through sparse approximations.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative introduction of the Bayesian representation learning limit, which effectively bridges the gap between traditional methods and contemporary deep learning frameworks. The characterization of DGP posteriors and the formulation of the objective function are both significant contributions that enhance understanding of representation learning dynamics. However, a potential weakness is the complexity of the proposed methodology, which may pose challenges for practitioners attempting to implement these concepts in real-world applications. Additionally, while the paper presents theoretical validation, further empirical comparisons with other state-of-the-art approaches could strengthen its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and conveys complex concepts with clarity, making it accessible to readers familiar with the field. The quality of the writing is high, with a coherent flow of ideas supported by mathematical rigor. The novelty of the contributions is significant, particularly in the context of integrating Bayesian frameworks with neural networks. However, the reproducibility of results may require additional details on implementation and experimentation, particularly concerning the sparse DKM development.\n\n# Summary Of The Review\nOverall, the paper presents an important advancement in understanding representation learning within Bayesian neural networks, introducing innovative concepts that enhance both theoretical and practical aspects of the field. While the contributions are substantial, the complexity of the methodology and the need for further empirical validation are areas for improvement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a new concept termed the Bayesian representation learning limit, which the authors claim enhances representation learning in deep Bayesian models. The methodology hinges on the connection between deep Gaussian processes (DGPs) and neural networks, suggesting that DGPs under this new limit can create a novel class of deep kernel methods (DKMs). However, the empirical validation is limited, with a lack of comprehensive evaluations across diverse datasets, raising questions about the findings' robustness and generalizability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical exploration of the Bayesian representation learning limit, which attempts to relate DGPs to DKMs. However, this strength is undermined by the lack of empirical evidence supporting the claims made. The experimental section is notably weak, failing to provide robust comparisons to existing state-of-the-art methods. Additionally, the paper does not adequately address the limitations of Gaussian assumptions in deep learning, nor does it sufficiently discuss the challenges posed by the sparse DKM approach. Overall, the work appears to offer limited novel insights and lacks practical relevance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper suffers due to its convoluted narrative and overly complex presentation of ideas. The writing could benefit from being more concise and focused, which would enhance the reader's understanding. In terms of quality, the theoretical constructs presented are not sufficiently grounded in practical applications, limiting the paper's overall impact. The novelty of the proposed concepts seems limited, as they appear to be a rebranding of existing approaches rather than providing significant advancements. Reproducibility is also a concern, as the experimental section lacks thoroughness and does not offer clear guidance on practical implementation.\n\n# Summary Of The Review\nOverall, the paper presents a theoretically intriguing but empirically weak contribution to the field of Bayesian representation learning. The lack of robust evidence, clarity, and practical relevance significantly detracts from its impact, calling into question the validity of the proposed concepts.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel framework for representation learning in neural networks, introducing the concept of the \"Bayesian representation learning limit.\" This framework is applied to Deep Gaussian Processes (DGPs) and leads to the discovery that in this limit, DGPs can achieve exactly multivariate Gaussian posteriors. Additionally, the authors propose Deep Kernel Machines (DKMs) as a deep generalization of kernel methods, emphasizing their scalability with a sparse DKM that operates linearly with respect to the number of data points. The paper also includes extensive empirical validation, demonstrating the effectiveness of the proposed methodologies.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its theoretical contributions, particularly the introduction of the Bayesian representation learning limit and the ability to derive exact multivariate Gaussian posteriors, which could significantly advance the understanding of deep Bayesian models. The development of DKMs and their scalable implementation represents another major contribution, making deep learning more efficient for large datasets. However, the paper may be critiqued for lacking broader empirical comparisons against state-of-the-art methods beyond finite DGPs, which would strengthen the validity of the claims made regarding efficiency and performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its contributions, making it accessible to both theoretical and practical audiences. The quality of the writing is high, with careful attention to detail in the exposition of complex concepts. The novel methodologies introduced are significant and grounded in solid theoretical foundations, which enhances their reproducibility. However, the extent of empirical validation could be broadened to include more diverse datasets and model comparisons, which would provide a clearer picture of the practical implications of the proposed approaches.\n\n# Summary Of The Review\nOverall, this paper makes significant advancements in the field of representation learning by introducing the Bayesian representation learning limit and Deep Kernel Machines, which offer both theoretical insights and practical methodologies. While the contributions are substantial, the empirical validation could be expanded to further substantiate the claims made.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel theoretical framework for understanding representation learning in deep neural networks, termed the Bayesian representation learning limit. This framework retains the essence of representation learning while providing a clearer mathematical structure, contrasting with traditional infinite-width approaches such as Neural Network Gaussian Processes (NNGPs) and Neural Tangent Kernels (NTKs). Key contributions include demonstrating how multivariate Gaussian posteriors emerge in deep Gaussian processes (DGPs), the development of Deep Kernel Machines (DKMs) that bridge deep learning and kernel methods, and extensions to Bayesian Neural Networks (BNNs). The findings suggest that as model width increases, the influence of the prior becomes more pronounced, requiring careful rescaling of likelihoods to preserve representation learning dynamics.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to integrating Bayesian methods with representation learning, which provides a fresh perspective on understanding deep learning models. The introduction of DKMs as a theoretical construct is particularly noteworthy, as it offers a structured way to combine the flexibility of deep learning with the robustness of kernel methods. However, a potential weakness lies in the complexity of the proposed framework, which may pose challenges in terms of practical implementation and understanding for those less familiar with Bayesian inference and probabilistic modeling.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its arguments clearly, making a strong case for the theoretical implications of its findings. The quality of the research is high, with a comprehensive review of related work and a robust theoretical foundation. The novelty of the approach is significant, particularly in its bridging of deep learning and kernel methods, although the intricate nature of the mathematical formulations may hinder reproducibility for some researchers. Overall, the clarity of the presentation is commendable, yet the complexity may require additional supplementary material to facilitate broader understanding.\n\n# Summary Of The Review\nThis paper offers a substantial theoretical contribution to the understanding of representation learning in neural networks through the introduction of the Bayesian representation learning limit. While the insights are both novel and significant, the complexity of the framework may present challenges for practical application and broad accessibility.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to Bayesian representation learning, introducing a new infinite width limit for Deep Gaussian Processes (DGPs) that ensures the retention of representation learning capabilities. The authors derive multivariate Gaussian posteriors through optimization of the Deep Kernel Machine (DKM) objective and develop a sparse version of the DKM that scales linearly with the number of data points. The methodology is extended to Bayesian Neural Networks (BNNs) and validated through experimental setups that include Langevin sampling and Monte-Carlo estimates. Key contributions include the parameterization of Gram matrices, the introduction of inducing points for scalability, and detailed optimization techniques to enhance training efficiency.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to scaling Bayesian models and the clear mathematical formulation of the DKM objective, which provides a solid foundation for deriving posterior covariances. The introduction of inducing points and the application of gradient descent optimization techniques are significant advancements that enhance the practicality of DGPs and BNNs. However, the paper lacks a thorough discussion on the broader implications of the proposed methods and their potential applications. Additionally, there is limited information on the availability of code or reproducibility, which could hinder the adoption of the proposed techniques by the community.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with well-structured sections and illustrative diagrams that help convey the generative models for DGPs. The quality of the research is high, supported by rigorous experimental validation. However, the novelty, while present in the context of Bayesian representation learning, may not be entirely groundbreaking within the broader landscape of deep learning research. Reproducibility is a concern, as specific code implementations are not provided, which could limit the ability of other researchers to replicate the findings accurately.\n\n# Summary Of The Review\nOverall, the paper offers a solid contribution to the field of Bayesian representation learning by introducing new methodologies for DGPs and BNNs that enhance scalability and efficiency. While the technical aspects are well-articulated and supported by experiments, the paper could benefit from a more comprehensive discussion on the practical implications of its findings and improved reproducibility measures.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel infinite width limit termed the Bayesian representation learning limit, aiming to address challenges in representation learning that arise in traditional frameworks. The authors propose a Neural Network Gaussian Process (NNGP) approach and introduce deep kernel machines (DKMs) as a flexible generalization of kernel methods. Experimental validation is provided, showing results consistent with wide but finite deep Gaussian processes (DGPs). However, the paper's claims regarding the novelty and significance of its contributions appear overstated, as it lacks sufficient acknowledgment of related works, especially those involving Neural Tangent Kernels (NTK).\n\n# Strength And Weaknesses\nThe paper presents some interesting ideas, particularly the introduction of DKMs, which could offer flexibility in representation learning. However, it significantly undervalues the contributions of prior research, especially in the context of NTK and existing kernel methods. The experimental results, while validating the proposed approach, do not adequately contextualize their findings within the broader literature. The treatment of traditional kernel methods is somewhat biased, emphasizing the superiority of deep learning frameworks without a balanced discussion of their enduring relevance and successes.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that guides the reader through the proposed methodology and findings. However, the novelty is questionable, as the paper does not sufficiently differentiate its contributions from prior works, particularly those related to NTK and kernel methods. Reproducibility may be an issue due to a lack of detailed comparisons with existing approaches, as well as insufficient context for the experimental results.\n\n# Summary Of The Review\nOverall, while the paper presents some intriguing concepts in representation learning, its claims of novelty and significance are overstated and fail to adequately address prior research contributions. The authors need to provide a more balanced discussion of existing methods and their relevance to the proposed work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel framework for representation learning in neural networks, drawing deep connections to kernel methods. It proposes a theoretical foundation that unifies these two approaches, highlighting the implications for generalization and efficiency in learning tasks. The methodology involves rigorous mathematical formulations and comparative analysis with existing techniques, demonstrating the advantages of the proposed approach through extensive experiments on benchmark datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical contributions, which provide a fresh perspective on the relationship between neural networks and kernel methods. The clarity of the mathematical derivations is commendable, and the empirical results substantiate the theoretical claims. However, the paper suffers from several issues, including inconsistent notation, formatting errors, and a lack of clarity in certain sections. These weaknesses detract from the overall readability and impact of the work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is undermined by typographical errors and inconsistent notation, which may confuse readers. While the novel theoretical insights are significant, the overall quality of presentation could be improved by addressing these issues. The reproducibility of the results is supported by thorough empirical evaluations; however, the lack of detailed descriptions in certain appendices may hinder replicability for some readers.\n\n# Summary Of The Review\nOverall, the paper offers a valuable theoretical contribution that bridges representation learning and kernel methods, with solid empirical validation. However, it requires substantial improvements in clarity, consistency, and presentation to fully realize its potential impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Bayesian representation learning limit, focusing on deep Gaussian processes (DGPs) and Bayesian neural networks (BNNs). It explores the theoretical foundations of these models and presents a methodology for sparse deep kernel machines (DKMs). The findings highlight the efficiency of these models in terms of computational performance and capacity to capture uncertainty in predictions. However, the authors do not sufficiently generalize their approach beyond DGPs and BNNs, which limits practical applicability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical contributions to Bayesian representation learning, particularly in the context of sparse DKMs. However, the scope of the study is somewhat limited; alternative posterior distributions are not considered, and the experimental validation relies on a narrow set of datasets. The assumption of IID features is another weakness, as it overlooks the complexities of real-world data. Furthermore, the paper lacks a thorough discussion of its implications for unsupervised learning and does not adequately address the challenges posed by noisy or incomplete data.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear in its exposition, though some sections could benefit from further elaboration, particularly regarding the implications of the findings. While the technical novelty is significant, the reproducibility of the results may be affected by the limited range of datasets used for empirical validation. The discussion surrounding hyperparameter choices is minimal, which may hinder practical implementation.\n\n# Summary Of The Review\nOverall, the paper presents a valuable theoretical advancement in Bayesian representation learning but suffers from limitations in generalizability and empirical validation. A more comprehensive exploration of its implications for various learning paradigms and real-world applications would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel framework for representation learning in deep Bayesian models, particularly focusing on Deep Gaussian Processes (DGPs). It introduces the \"Bayesian representation learning limit,\" which aims to preserve the benefits of representation learning through statistical testing of posterior distributions. Key contributions include the development of a Deep Kernel Machine (DKM) objective that combines log-likelihood and KL-divergences, and a sparse DKM method that scales linearly with data points. The paper further explores the implications for Bayesian Neural Networks (BNNs) and provides empirical validation of model performance through rigorous statistical analysis, demonstrating enhanced performance metrics like RMSE.\n\n# Strength And Weaknesses\nThe main strengths of the paper are its clear theoretical contributions, particularly the introduction of the Bayesian representation learning limit, which offers a fresh perspective on representation learning within deep Bayesian frameworks. The use of statistical methodologies, such as KL-divergences and rigorous significance testing, adds robustness to the findings. However, a potential weakness lies in the paper's reliance on statistical metrics that may not fully capture the practical implications of the proposed models. Additionally, while the theoretical contributions are well-articulated, some empirical results could benefit from further exploration of different datasets to ensure generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with clear explanations of complex statistical concepts. The quality of the work is high, as it is grounded in solid mathematical foundations and supports its claims with extensive statistical testing. The novelty is significant, particularly in the introduction of the Bayesian representation learning limit and the integration of statistical testing into model evaluation, which is not commonly addressed in existing literature. The reproducibility of the results may be enhanced by providing access to code and datasets used in the experiments, allowing for independent verification of the findings.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of representation learning within deep Bayesian models by introducing innovative theoretical frameworks and rigorous statistical methodologies. While the paper is clear and well-structured, further empirical validation across varied datasets would strengthen its generalizability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a Bayesian representation learning framework utilizing deep kernel machines (DKMs) to enhance scalability in modeling complex data distributions. The authors propose a method for approximating Bayesian neural networks (BNNs) while addressing the challenges posed by non-Gaussian posteriors. Key findings indicate that their approach can achieve a unique global maximum under specific conditions, though the practical implications and robustness of the method remain underexplored.\n\n# Strength And Weaknesses\nThe main contribution of the paper lies in its theoretical framework for Bayesian representation learning and the introduction of a sparse DKM aimed at improving scalability. However, the paper's practical applicability is limited by the reliance on infinite-width models, which do not translate well to real-world scenarios. Furthermore, the assumption of IID data may not reflect the complexities of real datasets, and the lack of comparative analysis with existing methods diminishes the contextual relevance of the findings. Additionally, the experimental validation is confined to specific datasets, raising concerns about the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, although some sections could benefit from more detailed explanations, particularly around the implications of local maxima in optimization and robustness to hyperparameter variations. The novelty of the proposed methods is evident, but the lack of empirical validation across diverse domains and failure to provide a roadmap for future research limit its reproducibility and broader impact.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to Bayesian representation learning with potential theoretical contributions. However, its practical limitations, assumptions about data, and lack of comprehensive experimental validation raise significant concerns about its applicability and generalizability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"A Theory of Representation Learning in Neural Networks Gives a Deep Generalisation of Kernel Methods\" aims to address the theoretical underpinnings of representation learning in neural networks, particularly in the context of infinite-width limits. The authors introduce concepts such as Bayesian Representation Learning Limit and Deep Gaussian Processes (DGPs), asserting that standard approaches eliminate representation learning, while their framework purportedly maintains it. The paper also discusses the optimization of posterior covariances and presents Deep Kernel Machines (DKMs) as a new class of methods, although these concepts largely reiterate existing knowledge in the field.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to formalize and clarify the relationship between deep learning and kernel methods, which is a topic of significant interest in the machine learning community. However, the weaknesses are pronounced; many of the contributions appear to lack novelty and depth, as they seem to recycle established ideas without providing substantial new insights. The reliance on complex terminology and concepts may obscure rather than clarify the authors' intentions, potentially alienating readers who seek practical advancements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by the use of jargon and a convoluted presentation of concepts that could be simplified. While the methodology is described, the novelty is questionable; many claims made by the authors reflect well-known principles in machine learning rather than groundbreaking discoveries. Reproducibility is not adequately addressed, as the presentation lacks sufficient detail on experimental setups or empirical validations that would allow other researchers to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper presents a theoretical exploration of representation learning that ultimately lacks significant novelty and clarity. While it addresses an important topic, the contributions feel more like rehashing established concepts rather than providing fresh insights or practical advancements.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel framework for Deep Kernel Machines (DKMs) that integrates Bayesian representation learning with traditional kernel methods. The methodology involves optimizing the DKM objective using a series of Kullback-Leibler divergences and focuses primarily on Gaussian posteriors for representation learning. Empirical validation demonstrates the DKM's efficacy against traditional kernel methods, showcasing significant advancements in scaling to large datasets, with promising results on UCI datasets.\n\n# Strength And Weaknesses\nThe paper's main strengths lie in its innovative approach to combining deep learning and kernel methods, which opens new avenues for exploration in representation learning. The use of KL divergence for optimization is a compelling aspect that could enhance robustness. However, the focus on Gaussian posteriors limits the model's flexibility; exploring alternative distributions could yield better performance, especially in the presence of outliers. The empirical comparisons, while noteworthy, lack context against contemporary deep learning architectures, which could provide a more comprehensive understanding of the DKM's advantages.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, with a coherent presentation of the proposed framework and its theoretical implications. However, the novelty could be bolstered by incorporating additional representation learning techniques and regularization methods. Although the findings are promising, the reproducibility may be challenged by the lack of practical implementation details and user-friendly tools, which are essential for broader adoption in the machine learning community.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting approach to representation learning through Deep Kernel Machines, highlighting its theoretical contributions and empirical performance. Nonetheless, the work could benefit from a broader exploration of representation techniques and additional practical considerations for implementation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Deep Kernel Machines (DKMs), a novel framework that integrates deep learning with kernel methods, aiming to enhance performance in representation learning tasks. The authors conduct a series of experiments on various UCI datasets, demonstrating that DKMs significantly outperform traditional kernel methods and the Maximum A Posteriori (MAP) objective for Deep Gaussian Processes (DGPs) in terms of Root Mean Square Error (RMSE). Key findings include improved scalability through a sparse DKM variant, indicating linear scaling with the number of datapoints, and robust performance consistency across multiple runs.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive benchmarking against established methods, showcasing significant RMSE improvements and addressing issues of numerical instability in the MAP objective. Additionally, the scalability of DKMs is a notable advantage over traditional kernel methods. However, the paper could benefit from further exploration of diverse datasets to validate the robustness of its findings beyond the UCI benchmarks. Also, while the empirical results are strong, the theoretical underpinnings of why DKMs perform better could be elaborated for deeper understanding.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, results, and conclusions, making it accessible to readers. The quality of the experiments is high, with detailed reporting of RMSE metrics and visual results that reinforce the findings. The novelty of DKMs as a framework that combines deep learning with kernel methods is significant, though the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameter choices.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the integration of deep learning and kernel methods through the introduction of Deep Kernel Machines. The empirical results demonstrate strong performance improvements, although further validation with additional datasets and deeper theoretical insights would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper proposes a novel approach to integrating neural network architectures with Gaussian processes, specifically focusing on the use of deep kernel methods (DKMs) to enhance representation learning in deep models. The methodology involves leveraging infinite-width neural networks to derive insights into the behavior of the proposed DKM framework. The findings demonstrate that the integration leads to improved model performance on various benchmark tasks, with a particular emphasis on scalability and interpretability.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to combining deep learning with Gaussian processes, which addresses existing limitations in representation learning. The use of theoretical insights from infinite-width models is commendable, providing a solid foundation for the proposed techniques. However, the paper suffers from a lack of clarity due to dense jargon and complex sentence structures, which may alienate readers who are not well-versed in the subject matter. Additionally, some sections contain redundant information, which could be streamlined to enhance readability.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a significant technical contribution, its clarity is undermined by the dense language and the extensive use of jargon without sufficient definitions. The overall quality is decent, but improvements in sentence and paragraph structure would enhance comprehension. The novelty of the proposed methods is clear, as they offer a fresh perspective on integrating deep learning with Gaussian processes. However, reproducibility may be hindered by the lack of detailed explanations for certain methodologies and insufficient clarity in the presentation of results.\n\n# Summary Of The Review\nThis paper presents a novel integration of deep kernel methods with Gaussian processes, contributing valuable insights to the field of representation learning. Despite its significant contributions, the paper's clarity and readability are compromised by complex language and redundancy, which could hinder its accessibility to a wider audience. Addressing these issues would greatly enhance the impact of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.491117654657163,
    -1.7010731092361913,
    -2.0761694210798973,
    -1.7743702846828748,
    -1.8011964685465995,
    -1.6816857781885715,
    -1.632757357513797,
    -1.8030205505139878,
    -1.6546771788825037,
    -1.9416150813703061,
    -1.702613753921132,
    -1.5853187588657773,
    -1.7902960380159112,
    -1.7443460259764882,
    -1.7820786688597998,
    -1.7067325521726833,
    -1.8850697436854842,
    -1.7055638811714178,
    -1.7345673461470705,
    -1.7646358571546656,
    -1.9453885697033246,
    -1.6871442329965287,
    -1.8862243012394964,
    -1.7619137748826512,
    -1.8504130452342251,
    -1.9826528527658418,
    -1.995540661164168,
    -1.624599104819202,
    -1.6318078832575151
  ],
  "logp_cond": [
    [
      0.0,
      -2.3718758415847856,
      -2.3817844624031244,
      -2.3744339498254803,
      -2.3950714033966443,
      -2.3848467871623105,
      -2.402106671987486,
      -2.3901971548247687,
      -2.3880744559770473,
      -2.3848597832724336,
      -2.390906891968212,
      -2.4080894387249416,
      -2.3877628219535243,
      -2.3874130643108162,
      -2.3775037343979206,
      -2.379833037082151,
      -2.383107324373678,
      -2.390289645217267,
      -2.383899338833073,
      -2.400348589079834,
      -2.3809516934074324,
      -2.42700833603969,
      -2.4015984568512736,
      -2.378477833337082,
      -2.407159801711018,
      -2.3631651819902273,
      -2.3930889576894465,
      -2.406308379818265,
      -2.4137749772440142
    ],
    [
      -1.364152745759308,
      0.0,
      -1.0790401387683497,
      -1.1369099036007961,
      -1.292880609578193,
      -1.2421907900553943,
      -1.3129087507255315,
      -1.239617829889481,
      -1.1578116026901937,
      -1.199253519607265,
      -1.1889855090878265,
      -1.4262554027843382,
      -1.1592013188636858,
      -1.1416641517371358,
      -1.2413964707327598,
      -1.1690844119428372,
      -1.259035163638791,
      -1.1805692398604195,
      -1.2173249478735277,
      -1.2560189205544812,
      -1.2234920428948615,
      -1.3775713463316508,
      -1.2708157226248085,
      -1.1966469613025672,
      -1.2980373420136546,
      -1.253612626845884,
      -1.313776312407413,
      -1.3285328686994682,
      -1.3618537370012187
    ],
    [
      -1.7432049345887726,
      -1.5135306021405623,
      0.0,
      -1.5935436281269415,
      -1.654909954096435,
      -1.6272997603716093,
      -1.672150581904186,
      -1.5985591910190846,
      -1.5685826734920023,
      -1.621253260499238,
      -1.5872688921086366,
      -1.7850768077989778,
      -1.575247627785494,
      -1.571600331871902,
      -1.6372849880718356,
      -1.5283484335894362,
      -1.6441172019930743,
      -1.5989006719619518,
      -1.614074391184896,
      -1.614262425312438,
      -1.6401248318672639,
      -1.7894484280642649,
      -1.647807485522544,
      -1.5484962714750974,
      -1.696314281135671,
      -1.6015698924850688,
      -1.7099554062871418,
      -1.720060970254447,
      -1.781097160393571
    ],
    [
      -1.4397132404001844,
      -1.2176964771200263,
      -1.2387504786715293,
      0.0,
      -1.3427347113846606,
      -1.3197154107788183,
      -1.3661843881542575,
      -1.3165565712574374,
      -1.286577213463624,
      -1.236096426267769,
      -1.2996085365390118,
      -1.473534802391832,
      -1.217906426902719,
      -1.1669309291991485,
      -1.2554031964754677,
      -1.270575585857862,
      -1.358088630338642,
      -1.23505827557912,
      -1.3156573562508784,
      -1.3047930147267448,
      -1.319946334664206,
      -1.4534381520304878,
      -1.389985795836899,
      -1.2745322444845475,
      -1.4231595554306227,
      -1.3017703998804284,
      -1.4319073315103512,
      -1.4258242558595375,
      -1.4403968776630551
    ],
    [
      -1.4805210434451492,
      -1.3691873819388796,
      -1.3169460454771817,
      -1.3616949585432432,
      0.0,
      -1.379657519093501,
      -1.4078947570905824,
      -1.3935398032561088,
      -1.3796408702330787,
      -1.3241505308669022,
      -1.3748556466948203,
      -1.4391372714872954,
      -1.3357027232562897,
      -1.3254532288665832,
      -1.3527958539974376,
      -1.357938502547459,
      -1.3885286076553962,
      -1.355095847792272,
      -1.4220729492111717,
      -1.4092687222881002,
      -1.3354861642576146,
      -1.4732292749951699,
      -1.3531574186302369,
      -1.3864877492179988,
      -1.3807875897125206,
      -1.4368439682076086,
      -1.4440889949083364,
      -1.482126031374912,
      -1.4880128862881892
    ],
    [
      -1.2851900204498776,
      -1.144235648374589,
      -1.1385983900474832,
      -1.1920324068804566,
      -1.1999234823943719,
      0.0,
      -1.209525963318473,
      -1.1777259869387549,
      -1.2040281788133063,
      -1.2190991444797212,
      -1.193899967158434,
      -1.341918858767152,
      -1.152160101802831,
      -1.1693973573036478,
      -1.2163393389961648,
      -1.137093432969014,
      -1.2391288158165146,
      -1.1863794450824172,
      -1.2510855166732955,
      -1.2722349632973515,
      -1.175915752901322,
      -1.3171364150024307,
      -1.250286203277096,
      -1.178499205051782,
      -1.303450545192179,
      -1.1674119327660697,
      -1.2593304420043576,
      -1.2803405324800192,
      -1.351728473466961
    ],
    [
      -1.3235287936758457,
      -1.2011100617931894,
      -1.1726108515253968,
      -1.1764274695998647,
      -1.2563169208653202,
      -1.2288334714785336,
      0.0,
      -1.2311386849399593,
      -1.2482274873660733,
      -1.2324065358312133,
      -1.2128789000876274,
      -1.3522794857702052,
      -1.134637972009576,
      -1.1691107642629521,
      -1.2572431438264997,
      -1.1828885384682724,
      -1.2896585605003874,
      -1.2137496875730145,
      -1.2262635829050519,
      -1.3067973011825986,
      -1.1958299567588209,
      -1.3467309110446872,
      -1.2627804004326884,
      -1.2404807871296089,
      -1.2999115478411158,
      -1.2249984051885632,
      -1.341728455678085,
      -1.3129332446618682,
      -1.3464666095595976
    ],
    [
      -1.4749946479233484,
      -1.3774698523480564,
      -1.3122018835400506,
      -1.425479826492285,
      -1.4662953214164018,
      -1.3838750651851184,
      -1.464143293977304,
      0.0,
      -1.3861673926024152,
      -1.440185404654111,
      -1.421378655508018,
      -1.5015231538535654,
      -1.380537489037482,
      -1.3858889781530943,
      -1.3977159140502906,
      -1.3576640731106282,
      -1.4543160120699625,
      -1.4054183993197757,
      -1.4121319252288742,
      -1.3939309369553352,
      -1.4108472390759828,
      -1.4979011620735518,
      -1.3993900892318811,
      -1.3473413970177708,
      -1.4365101999815524,
      -1.3820886337104037,
      -1.449012759240884,
      -1.462362309288582,
      -1.4778518656148079
    ],
    [
      -1.386084914219532,
      -1.2498121446314068,
      -1.1947661344771527,
      -1.2994403420152767,
      -1.3360589494615964,
      -1.3141750599422002,
      -1.387771277667302,
      -1.2642050398244702,
      0.0,
      -1.3112239234594458,
      -1.3194813032638877,
      -1.4374176118834296,
      -1.2944013892668527,
      -1.287421169613477,
      -1.3144566703027627,
      -1.2050843199457397,
      -1.3485539872340973,
      -1.3022319241226143,
      -1.3138837967690393,
      -1.2411687564149738,
      -1.3268508415434888,
      -1.365106507739093,
      -1.3200239426583085,
      -1.2612947411455222,
      -1.3233151640219447,
      -1.2773851365693707,
      -1.3535771251896473,
      -1.306040342348991,
      -1.3854660403319887
    ],
    [
      -1.6070127010545043,
      -1.5432656462934897,
      -1.4877506656455863,
      -1.4864602182264341,
      -1.5715985132784527,
      -1.5450032064034913,
      -1.6693138895827024,
      -1.600009408140283,
      -1.5784516803458895,
      0.0,
      -1.5388244255148271,
      -1.6793696090446928,
      -1.528910375310855,
      -1.5106006518213115,
      -1.4844649910366037,
      -1.5183318894454365,
      -1.6245989074743021,
      -1.5161581130933484,
      -1.5481820551735246,
      -1.5885103011697355,
      -1.5603676996849465,
      -1.7191330862297185,
      -1.6096945691859013,
      -1.5315034175324194,
      -1.6709731115425666,
      -1.5789183902541657,
      -1.6400152752868364,
      -1.660971810940422,
      -1.7145647415234242
    ],
    [
      -1.3668088185753993,
      -1.1530975234019314,
      -1.107328587166534,
      -1.1995939975659138,
      -1.2532014278770351,
      -1.2428964185479698,
      -1.3008217141840466,
      -1.2218819183145726,
      -1.2008604617495384,
      -1.2116909047434385,
      0.0,
      -1.382522855683966,
      -1.1359728715242825,
      -1.1738255155025954,
      -1.2591142883318398,
      -1.1374708148255797,
      -1.2036786979867053,
      -1.154497614079169,
      -1.2465287615771117,
      -1.2221651584364053,
      -1.223355070258793,
      -1.3685067786172596,
      -1.275034139243274,
      -1.1525281997826764,
      -1.291012326855339,
      -1.2312391705237107,
      -1.2928232521335856,
      -1.2821187742714344,
      -1.337563867784293
    ],
    [
      -1.3666165589982175,
      -1.3530033583043108,
      -1.325619426624553,
      -1.3161425923358816,
      -1.2862388996070873,
      -1.349902097494717,
      -1.341919733412971,
      -1.310505110217424,
      -1.3443200994229996,
      -1.3111632740437424,
      -1.3258334629474522,
      0.0,
      -1.3469393528989053,
      -1.3420578967777657,
      -1.3110287967846994,
      -1.3533869099248859,
      -1.3115840086164865,
      -1.3480800550756191,
      -1.3638504662815671,
      -1.333123159561966,
      -1.3209908302975752,
      -1.3091315556156118,
      -1.3315779898024618,
      -1.3460280965245925,
      -1.351248499396817,
      -1.3255531696403697,
      -1.3485913903623823,
      -1.3527744477821462,
      -1.3320318625774972
    ],
    [
      -1.4761529542612841,
      -1.270315771387979,
      -1.258810269795807,
      -1.274203152534607,
      -1.388668077465183,
      -1.3534384888230158,
      -1.3402231116580499,
      -1.3450499582193798,
      -1.3268579239318283,
      -1.336817896452083,
      -1.2973678663360946,
      -1.5102900686260883,
      0.0,
      -1.2352548005350033,
      -1.3534099246563673,
      -1.2945264567216803,
      -1.3990179484008973,
      -1.3040556675829174,
      -1.3313063904615603,
      -1.3824908113319776,
      -1.2699811032492145,
      -1.5066174533005763,
      -1.417200778729295,
      -1.3822746282333516,
      -1.436555380273551,
      -1.3492105954373177,
      -1.4094762365521538,
      -1.406706343886293,
      -1.4701270300502294
    ],
    [
      -1.3847036244176114,
      -1.22761667430456,
      -1.223155955177718,
      -1.1965179067517324,
      -1.310415676183785,
      -1.2807153495500896,
      -1.363763164870003,
      -1.287487736612253,
      -1.2773061188319417,
      -1.2673338842653372,
      -1.276114576829246,
      -1.4876111690683387,
      -1.174859698053549,
      0.0,
      -1.293070187297583,
      -1.1826927028332683,
      -1.3355290253508998,
      -1.2178303132477428,
      -1.2643053586964828,
      -1.2717106189666705,
      -1.2341946299951803,
      -1.4260152152895238,
      -1.3661218041817202,
      -1.2659404911100365,
      -1.3796595125598667,
      -1.2580891437746968,
      -1.378587431435079,
      -1.399759759830467,
      -1.4462196968960657
    ],
    [
      -1.4130166168442697,
      -1.3621551564460208,
      -1.3077425314488917,
      -1.2926303728256727,
      -1.3899454192011003,
      -1.4248632713845832,
      -1.4458578889429314,
      -1.4121535973009216,
      -1.3946093003671751,
      -1.301702661202953,
      -1.4070736833227149,
      -1.4887213507466628,
      -1.3521048146694647,
      -1.3768975097449707,
      0.0,
      -1.3878469966938896,
      -1.4218648748427116,
      -1.3798883749631565,
      -1.3941535770563789,
      -1.3803520793603958,
      -1.354721182410114,
      -1.480016718800236,
      -1.3796155727674084,
      -1.3980272620809588,
      -1.464616359694577,
      -1.3224693415070239,
      -1.5025060910611814,
      -1.5270987505887186,
      -1.477828009351914
    ],
    [
      -1.3696109491598758,
      -1.1737730407348423,
      -1.1508861841333233,
      -1.2095208178082677,
      -1.3225140364379209,
      -1.2533321218755593,
      -1.2837594028666819,
      -1.2539031379346186,
      -1.1752409888234292,
      -1.2580279999678534,
      -1.2382115067714703,
      -1.452824618109629,
      -1.1710413016607961,
      -1.1615693141514762,
      -1.2651635767003326,
      0.0,
      -1.3060500368164287,
      -1.1963348551534845,
      -1.2838554324871878,
      -1.270769832504517,
      -1.257483142637533,
      -1.3989252164111265,
      -1.3220795927892142,
      -1.2126359746985218,
      -1.389215434919874,
      -1.2683717835799744,
      -1.347034326525714,
      -1.3532486587255919,
      -1.404103354080349
    ],
    [
      -1.5688895169021595,
      -1.4350661091359922,
      -1.396669201546779,
      -1.5124277632762237,
      -1.4658858854087577,
      -1.5071767412665165,
      -1.5484473490093846,
      -1.4768131129700177,
      -1.4605467936824053,
      -1.5304958379841398,
      -1.4042760513296517,
      -1.6165258984186557,
      -1.4497978790000174,
      -1.458090132968243,
      -1.5285007790344984,
      -1.422669573750429,
      0.0,
      -1.4267206138807387,
      -1.504582697089484,
      -1.4810328379549835,
      -1.463409473715761,
      -1.5658789475181238,
      -1.4349822946481077,
      -1.4528946591680323,
      -1.476579704388305,
      -1.485308546315523,
      -1.4912508712895327,
      -1.5201428689647973,
      -1.5330334744437932
    ],
    [
      -1.3688635792175554,
      -1.2171534484155757,
      -1.178629582575518,
      -1.1807843644024736,
      -1.2878358372508711,
      -1.2756667584791062,
      -1.279905974667895,
      -1.2434032445180894,
      -1.264330541662459,
      -1.1981858561572882,
      -1.214438827503978,
      -1.4277831094506204,
      -1.2068208298230785,
      -1.193085489268006,
      -1.2817478331575303,
      -1.2225159247830133,
      -1.3091462059788233,
      0.0,
      -1.265348968712097,
      -1.2776865943906024,
      -1.2461129736451304,
      -1.3795386310742865,
      -1.312165119313636,
      -1.2187685373008108,
      -1.338060986290149,
      -1.2668900530292584,
      -1.3541616275848678,
      -1.3599431096935088,
      -1.3873709312823155
    ],
    [
      -1.3962988114570611,
      -1.2821277854774942,
      -1.2292512951241787,
      -1.2386206570750067,
      -1.3669910866219734,
      -1.2840848243863814,
      -1.3400456469179403,
      -1.3089635096327112,
      -1.2823750551581916,
      -1.2424313989431544,
      -1.292232275612067,
      -1.4209543025070455,
      -1.2396805597169056,
      -1.239993462484079,
      -1.247424429135285,
      -1.2653509426309808,
      -1.3486183685542024,
      -1.283477792983638,
      0.0,
      -1.2900349184178008,
      -1.2965275406191012,
      -1.414125066943044,
      -1.3385447238005124,
      -1.273154290721506,
      -1.4036374607098068,
      -1.2824394066937828,
      -1.3651835979929707,
      -1.3699602238543838,
      -1.4094060087687428
    ],
    [
      -1.510377974694058,
      -1.3958000909800572,
      -1.3521614894159237,
      -1.4208928199711846,
      -1.4590593170141763,
      -1.4466971832022162,
      -1.461991015960132,
      -1.3936449656134908,
      -1.3339640769574947,
      -1.4032758878275835,
      -1.415054664429627,
      -1.5208446283621473,
      -1.3867916577480524,
      -1.3861388945759927,
      -1.4123951102988874,
      -1.367384027868799,
      -1.4404675737737667,
      -1.4041880833189906,
      -1.4131598848778735,
      0.0,
      -1.4267665737955535,
      -1.4861492207686238,
      -1.4200142597598602,
      -1.3418981142103932,
      -1.4353921137880983,
      -1.419411425195585,
      -1.4462008324580402,
      -1.4503205793017033,
      -1.4668635073590353
    ],
    [
      -1.6201203143594776,
      -1.4564477773473863,
      -1.4960511583415923,
      -1.5564632212025036,
      -1.555174578198929,
      -1.5249401359207262,
      -1.551702779325745,
      -1.4953632543190336,
      -1.5876373186401131,
      -1.5483459594596407,
      -1.521959838071418,
      -1.6776288515919031,
      -1.4465063382860446,
      -1.443442220318663,
      -1.5340236037524806,
      -1.4788891301962344,
      -1.563317574584425,
      -1.5565052465911493,
      -1.535904431277583,
      -1.5569512034900503,
      0.0,
      -1.6477555858686022,
      -1.564133338766198,
      -1.549229529536325,
      -1.6021378410405833,
      -1.5363096965235759,
      -1.5918190844638072,
      -1.604818863878252,
      -1.626942740659699
    ],
    [
      -1.323846805178914,
      -1.2383509467064564,
      -1.2356395909445286,
      -1.305004801724214,
      -1.2841115075182559,
      -1.2805354067170815,
      -1.273400808909108,
      -1.2884960027574088,
      -1.2848647491768241,
      -1.3245384475667135,
      -1.28102187899214,
      -1.2672229748630124,
      -1.3072281676094701,
      -1.2450100181743764,
      -1.3122759000348552,
      -1.331342662407232,
      -1.3376067721714242,
      -1.2605985789936005,
      -1.3178376030401526,
      -1.2966650092525187,
      -1.2629423798184918,
      0.0,
      -1.2769035362886534,
      -1.2738382706105245,
      -1.3266991492977738,
      -1.2768111097004693,
      -1.2942013516486115,
      -1.32675905870717,
      -1.273454845922425
    ],
    [
      -1.4900936531536173,
      -1.3969213978129613,
      -1.363855367738807,
      -1.4599713319232064,
      -1.3992891727364227,
      -1.443038629921892,
      -1.4657411875835038,
      -1.4276031650859513,
      -1.4254337787084,
      -1.395985186183519,
      -1.405977917569489,
      -1.52091743349952,
      -1.3884961104921956,
      -1.4150875978476432,
      -1.4141567137092612,
      -1.393932932776333,
      -1.3672460356124485,
      -1.4137947663214632,
      -1.4081968526494206,
      -1.4268283131469286,
      -1.3989096543764477,
      -1.4853272494225804,
      0.0,
      -1.3649481821152136,
      -1.3783679117542087,
      -1.4260010658433082,
      -1.4268708230069767,
      -1.5071644001505233,
      -1.4892289737353408
    ],
    [
      -1.4711697060394973,
      -1.3178687962894449,
      -1.2717668999014047,
      -1.3356017057733784,
      -1.3825687756280842,
      -1.3467603928661565,
      -1.4149566044340258,
      -1.2643879589724776,
      -1.3088155030926554,
      -1.3491946269181427,
      -1.327394980324139,
      -1.4989429794107556,
      -1.3668962593183422,
      -1.2899432079542452,
      -1.3797006217028496,
      -1.2472515411167047,
      -1.3478615414111021,
      -1.309737941427462,
      -1.354030598280732,
      -1.287579835662839,
      -1.3414614821355277,
      -1.4650386110460638,
      -1.3369136186237216,
      0.0,
      -1.410251740678933,
      -1.3753882052553947,
      -1.394622640053823,
      -1.3853736475609837,
      -1.438419377507931
    ],
    [
      -1.5520507033857363,
      -1.4550928916453216,
      -1.4100397691389146,
      -1.5160854293423691,
      -1.4643980765057996,
      -1.5292154442081622,
      -1.4816753763427146,
      -1.4707176198260634,
      -1.4549026965060157,
      -1.4659909298929557,
      -1.4726240832945894,
      -1.5658605494526272,
      -1.4526267965326394,
      -1.4589248164780648,
      -1.507874458028618,
      -1.4893869948745992,
      -1.4661336150590876,
      -1.456090064546892,
      -1.52799519589588,
      -1.4609574607602795,
      -1.4442238559912748,
      -1.5241677658578763,
      -1.3613752069838903,
      -1.4314041181118091,
      0.0,
      -1.5154597630324684,
      -1.5031868301130054,
      -1.476983902461284,
      -1.4834677209311666
    ],
    [
      -1.5863085019392804,
      -1.4752595512474374,
      -1.399490861315628,
      -1.47477187245575,
      -1.5718214724549147,
      -1.4914587479020347,
      -1.577646924132163,
      -1.4957496318760157,
      -1.4801356243370007,
      -1.5141999751572963,
      -1.5181506549896406,
      -1.701865917844542,
      -1.4815114701310181,
      -1.4684433915316517,
      -1.486197353887535,
      -1.4694993072282028,
      -1.5590134154121496,
      -1.4792188825539405,
      -1.5015845876409082,
      -1.5597432893962597,
      -1.4997436615939965,
      -1.6412566550600567,
      -1.5470194336050955,
      -1.5371227487100652,
      -1.5952817915232615,
      0.0,
      -1.5805104285605296,
      -1.5969160150886543,
      -1.646144318014101
    ],
    [
      -1.6339036841191426,
      -1.5605470034793039,
      -1.5398525140635364,
      -1.5902849453241927,
      -1.5924344305135452,
      -1.5947639670272533,
      -1.613351498282714,
      -1.6040233650709692,
      -1.5929064631990901,
      -1.5968354196565457,
      -1.57570090355598,
      -1.5961731947048161,
      -1.5556198147644908,
      -1.5637839901308128,
      -1.6113558468633626,
      -1.6038258345848988,
      -1.594787122698978,
      -1.546536104671957,
      -1.57631542104494,
      -1.5611636672167015,
      -1.5483342048810822,
      -1.5915972032220167,
      -1.5454456953993858,
      -1.5610090244014632,
      -1.564707280133203,
      -1.5554490894899693,
      0.0,
      -1.5758472679271531,
      -1.6148532225897143
    ],
    [
      -1.3244288798562613,
      -1.2134492145475981,
      -1.1784402476969327,
      -1.2505694943097105,
      -1.2647116221474712,
      -1.257647557842121,
      -1.2569809258221403,
      -1.235841285039987,
      -1.1798823377729801,
      -1.2811371265028402,
      -1.2209999853364928,
      -1.3242773484979893,
      -1.2369114755646524,
      -1.237277833100499,
      -1.2728713108164094,
      -1.2094812062104818,
      -1.2611001748158275,
      -1.2075214346606988,
      -1.2775060127023503,
      -1.2246297470525018,
      -1.2480864284203725,
      -1.2834751582687278,
      -1.2495878568740573,
      -1.151514699740888,
      -1.2636260772565453,
      -1.2620304348083964,
      -1.242714016945096,
      0.0,
      -1.269806548152355
    ],
    [
      -1.3427203050645617,
      -1.2601229256674418,
      -1.2627038849516021,
      -1.2845174125846397,
      -1.285829588084037,
      -1.2933168520375176,
      -1.3029371165361834,
      -1.2479367110951087,
      -1.2809301766641896,
      -1.3078500282679342,
      -1.2962549834838908,
      -1.2906199335832158,
      -1.2854994911663251,
      -1.2919126062508206,
      -1.2944976600624438,
      -1.2899977663975664,
      -1.272166062568329,
      -1.272404413685308,
      -1.3021946411821346,
      -1.2930695932881373,
      -1.274420530160437,
      -1.238806884850609,
      -1.2858171155127138,
      -1.2902787082707805,
      -1.286684627967499,
      -1.2740253299568867,
      -1.2727319137124775,
      -1.2896936681969549,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.11924181307237758,
      0.10933319225403881,
      0.11668370483168289,
      0.09604625126051891,
      0.1062708674948527,
      0.08901098266967722,
      0.10092049983239448,
      0.10304319868011591,
      0.10625787138472953,
      0.10021076268895124,
      0.08302821593222154,
      0.10335483270363888,
      0.10370459034634694,
      0.11361392025924255,
      0.11128461757501196,
      0.10801033028348517,
      0.10082800943989634,
      0.10721831582409003,
      0.09076906557732922,
      0.11016596124973077,
      0.06410931861747304,
      0.08951919780588957,
      0.11263982132008099,
      0.0839578529461451,
      0.12795247266693588,
      0.09802869696771666,
      0.08480927483889822,
      0.07734267741314893
    ],
    [
      0.33692036347688337,
      0.0,
      0.6220329704678416,
      0.5641632056353951,
      0.4081924996579982,
      0.4588823191807969,
      0.3881643585106598,
      0.46145527934671016,
      0.5432615065459976,
      0.5018195896289264,
      0.5120876001483647,
      0.2748177064518531,
      0.5418717903725054,
      0.5594089574990555,
      0.45967663850343143,
      0.5319886972933541,
      0.4420379455974002,
      0.5205038693757718,
      0.4837481613626635,
      0.4450541886817101,
      0.4775810663413298,
      0.32350176290454047,
      0.4302573866113828,
      0.5044261479336241,
      0.40303576722253664,
      0.4474604823903072,
      0.38729679682877816,
      0.3725402405367231,
      0.33921937223497256
    ],
    [
      0.33296448649112476,
      0.562638818939335,
      0.0,
      0.4826257929529558,
      0.4212594669834624,
      0.44886966070828804,
      0.40401883917571135,
      0.4776102300608127,
      0.507586747587895,
      0.4549161605806593,
      0.4889005289712607,
      0.2910926132809195,
      0.5009217932944032,
      0.5045690892079953,
      0.4388844330080617,
      0.5478209874904612,
      0.432052219086823,
      0.4772687491179455,
      0.4620950298950013,
      0.4619069957674593,
      0.43604458921263345,
      0.2867209930156325,
      0.42836193555735336,
      0.5276731496047999,
      0.37985513994422626,
      0.47459952859482857,
      0.3662140147927555,
      0.3561084508254504,
      0.2950722606863263
    ],
    [
      0.3346570442826904,
      0.5566738075628486,
      0.5356198060113455,
      0.0,
      0.4316355732982142,
      0.45465487390405657,
      0.40818589652861736,
      0.4578137134254374,
      0.48779307121925086,
      0.5382738584151059,
      0.47476174814386307,
      0.30083548229104284,
      0.5564638577801557,
      0.6074393554837263,
      0.5189670882074071,
      0.5037946988250128,
      0.4162816543442329,
      0.5393120091037549,
      0.4587129284319964,
      0.46957726995613003,
      0.45442395001866887,
      0.32093213265238707,
      0.38438448884597576,
      0.49983804019832734,
      0.35121072925225216,
      0.4725998848024464,
      0.3424629531725236,
      0.3485460288233373,
      0.3339734070198197
    ],
    [
      0.3206754251014503,
      0.43200908660771997,
      0.4842504230694178,
      0.43950151000335635,
      0.0,
      0.42153894945309855,
      0.3933017114560171,
      0.4076566652904907,
      0.42155559831352085,
      0.4770459376796974,
      0.42634082185177924,
      0.3620591970593041,
      0.46549374529030985,
      0.4757432396800163,
      0.4484006145491619,
      0.44325796599914047,
      0.41266786089120333,
      0.44610062075432744,
      0.37912351933542787,
      0.39192774625849935,
      0.46571030428898497,
      0.32796719355142967,
      0.4480390499163627,
      0.41470871932860076,
      0.42040887883407896,
      0.3643525003389909,
      0.3571074736382631,
      0.3190704371716875,
      0.31318358225841036
    ],
    [
      0.39649575773869383,
      0.5374501298139824,
      0.5430873881410883,
      0.4896533713081148,
      0.4817622957941996,
      0.0,
      0.4721598148700985,
      0.5039597912498166,
      0.4776575993752652,
      0.4625866337088502,
      0.4877858110301374,
      0.33976691942141946,
      0.5295256763857406,
      0.5122884208849237,
      0.4653464391924067,
      0.5445923452195573,
      0.4425569623720569,
      0.49530633310615424,
      0.430600261515276,
      0.4094508148912199,
      0.5057700252872495,
      0.36454936318614073,
      0.4313995749114754,
      0.5031865731367895,
      0.3782352329963925,
      0.5142738454225018,
      0.42235533618421384,
      0.40134524570855223,
      0.32995730472161044
    ],
    [
      0.3092285638379513,
      0.43164729572060767,
      0.46014650598840023,
      0.45632988791393236,
      0.3764404366484768,
      0.4039238860352634,
      0.0,
      0.40161867257383776,
      0.3845298701477238,
      0.40035082168258374,
      0.4198784574261696,
      0.28047787174359184,
      0.49811938550422097,
      0.4636465932508449,
      0.3755142136872973,
      0.44986881904552467,
      0.3430987970134096,
      0.41900766994078253,
      0.40649377460874514,
      0.3259600563311984,
      0.43692740075497616,
      0.2860264464691098,
      0.36997695708110867,
      0.3922765703841882,
      0.3328458096726812,
      0.40775895232523385,
      0.29102890183571195,
      0.3198241128519288,
      0.2862907479541994
    ],
    [
      0.32802590259063935,
      0.4255506981659314,
      0.49081866697393717,
      0.3775407240217028,
      0.33672522909758595,
      0.4191454853288694,
      0.33887725653668377,
      0.0,
      0.4168531579115726,
      0.36283514585987686,
      0.3816418950059699,
      0.3014973966604224,
      0.42248306147650583,
      0.4171315723608935,
      0.4053046364636972,
      0.4453564774033596,
      0.34870453844402527,
      0.39760215119421205,
      0.3908886252851136,
      0.4090896135586526,
      0.39217331143800505,
      0.30511938844043596,
      0.40363046128210667,
      0.455679153496217,
      0.3665103505324354,
      0.4209319168035841,
      0.3540077912731039,
      0.3406582412254058,
      0.32516868489917994
    ],
    [
      0.2685922646629717,
      0.4048650342510969,
      0.4599110444053509,
      0.35523683686722696,
      0.31861822942090723,
      0.34050211894030347,
      0.26690590121520175,
      0.39047213905803346,
      0.0,
      0.3434532554230578,
      0.33519587561861597,
      0.217259566999074,
      0.360275789615651,
      0.3672560092690267,
      0.34022050857974095,
      0.44959285893676393,
      0.3061231916484064,
      0.3524452547598893,
      0.3407933821134643,
      0.41350842246752983,
      0.3278263373390149,
      0.28957067114341073,
      0.3346532362241952,
      0.39338243773698145,
      0.33136201486055894,
      0.37729204231313296,
      0.3011000536928563,
      0.34863683653351263,
      0.26921113855051493
    ],
    [
      0.3346023803158018,
      0.3983494350768164,
      0.4538644157247198,
      0.455154863143872,
      0.37001656809185346,
      0.3966118749668148,
      0.2723011917876037,
      0.3416056732300232,
      0.3631634010244167,
      0.0,
      0.402790655855479,
      0.26224547232561335,
      0.4127047060594511,
      0.4310144295489946,
      0.45715009033370246,
      0.42328319192486963,
      0.317016173896004,
      0.4254569682769578,
      0.39343302619678155,
      0.3531047802005707,
      0.38124738168535965,
      0.22248199514058764,
      0.3319205121844049,
      0.4101116638378868,
      0.2706419698277396,
      0.36269669111614045,
      0.3015998060834697,
      0.28064327042988424,
      0.22705033984688194
    ],
    [
      0.33580493534573264,
      0.5495162305192005,
      0.595285166754598,
      0.5030197563552181,
      0.44941232604409675,
      0.45971733537316206,
      0.4017920397370853,
      0.4807318356065593,
      0.5017532921715935,
      0.49092284917769335,
      0.0,
      0.32009089823716597,
      0.5666408823968494,
      0.5287882384185365,
      0.4434994655892921,
      0.5651429390955522,
      0.49893505593442655,
      0.548116139841963,
      0.4560849923440202,
      0.4804485954847266,
      0.4792586836623389,
      0.33410697530387234,
      0.4275796146778579,
      0.5500855541384555,
      0.41160142706579284,
      0.4713745833974212,
      0.4097905017875463,
      0.4204949796496975,
      0.365049886136839
    ],
    [
      0.2187021998675598,
      0.2323154005614665,
      0.25969933224122443,
      0.26917616652989573,
      0.2990798592586901,
      0.23541666137106043,
      0.24339902545280623,
      0.2748136486483532,
      0.24099865944277776,
      0.2741554848220349,
      0.25948529591832514,
      0.0,
      0.23837940596687202,
      0.24326086208801168,
      0.2742899620810779,
      0.23193184894089147,
      0.27373475024929084,
      0.2372387037901582,
      0.2214682925842102,
      0.2521955993038114,
      0.2643279285682021,
      0.27618720325016555,
      0.25374076906331555,
      0.2392906623411848,
      0.23407025946896032,
      0.2597655892254076,
      0.23672736850339504,
      0.23254431108363116,
      0.25328689628828016
    ],
    [
      0.3141430837546271,
      0.5199802666279323,
      0.5314857682201042,
      0.5160928854813043,
      0.40162796055072825,
      0.43685754919289543,
      0.45007292635786134,
      0.4452460797965314,
      0.46343811408408286,
      0.45347814156382826,
      0.4929281716798166,
      0.2800059693898229,
      0.0,
      0.555041237480908,
      0.43688611335954386,
      0.4957695812942309,
      0.39127808961501387,
      0.48624037043299384,
      0.4589896475543509,
      0.4078052266839336,
      0.5203149347666967,
      0.2836785847153349,
      0.37309525928661613,
      0.40802140978255963,
      0.3537406577423603,
      0.4410854425785935,
      0.3808198014637574,
      0.3835896941296182,
      0.3201690079656818
    ],
    [
      0.35964240155887683,
      0.5167293516719282,
      0.5211900707987702,
      0.5478281192247558,
      0.4339303497927032,
      0.46363067642639866,
      0.3805828611064852,
      0.4568582893642352,
      0.4670399071445466,
      0.477012141711151,
      0.46823144914724235,
      0.25673485690814957,
      0.5694863279229392,
      0.0,
      0.4512758386789053,
      0.5616533231432199,
      0.40881700062558846,
      0.5265157127287454,
      0.4800406672800055,
      0.47263540700981777,
      0.510151395981308,
      0.31833081068696445,
      0.378224221794768,
      0.47840553486645176,
      0.3646865134166215,
      0.4862568822017914,
      0.36575859454140924,
      0.3445862661460213,
      0.2981263290804226
    ],
    [
      0.3690620520155301,
      0.419923512413779,
      0.4743361374109081,
      0.4894482960341271,
      0.39213324965869956,
      0.3572153974752166,
      0.3362207799168684,
      0.3699250715588782,
      0.3874693684926247,
      0.48037600765684685,
      0.37500498553708494,
      0.29335731811313703,
      0.42997385419033507,
      0.4051811591148291,
      0.0,
      0.39423167216591026,
      0.3602137940170882,
      0.4021902938966433,
      0.38792509180342094,
      0.401726589499404,
      0.42735748644968585,
      0.3020619500595638,
      0.4024630960923914,
      0.384051406778841,
      0.3174623091652229,
      0.45960932735277593,
      0.27957257779861844,
      0.25497991827108124,
      0.30425065950788577
    ],
    [
      0.33712160301280747,
      0.5329595114378409,
      0.5558463680393599,
      0.49721173436441557,
      0.3842185157347624,
      0.45340043029712396,
      0.4229731493060014,
      0.45282941423806466,
      0.5314915633492541,
      0.44870455220482985,
      0.468521045401213,
      0.25390793406305434,
      0.5356912505118872,
      0.545163238021207,
      0.44156897547235063,
      0.0,
      0.40068251535625454,
      0.5103976970191988,
      0.42287711968549546,
      0.43596271966816635,
      0.44924940953515025,
      0.3078073357615567,
      0.3846529593834691,
      0.49409657747416147,
      0.3175171172528093,
      0.4383607685927089,
      0.3596982256469692,
      0.3534838934470914,
      0.30262919809233435
    ],
    [
      0.3161802267833247,
      0.450003634549492,
      0.4884005421387052,
      0.37264198040926044,
      0.41918385827672644,
      0.3778930024189677,
      0.33662239467609956,
      0.4082566307154665,
      0.42452295000307894,
      0.3545739057013444,
      0.48079369235583247,
      0.2685438452668285,
      0.43527186468546675,
      0.4269796107172412,
      0.35656896465098575,
      0.4624001699350553,
      0.0,
      0.4583491298047455,
      0.3804870465960002,
      0.40403690573050066,
      0.42166026996972317,
      0.31919079616736035,
      0.4500874490373765,
      0.4321750845174519,
      0.4084900392971791,
      0.3997611973699613,
      0.3938188723959515,
      0.36492687472068686,
      0.35203626924169096
    ],
    [
      0.33670030195386236,
      0.48841043275584206,
      0.5269342985958998,
      0.5247795167689442,
      0.41772804392054663,
      0.4298971226923116,
      0.42565790650352286,
      0.4621606366533284,
      0.4412333395089587,
      0.5073780250141295,
      0.49112505366743986,
      0.2777807717207974,
      0.4987430513483393,
      0.5124783919034117,
      0.4238160480138875,
      0.4830479563884045,
      0.3964176751925945,
      0.0,
      0.4402149124593209,
      0.4278772867808154,
      0.45945090752628737,
      0.32602525009713124,
      0.39339876185778166,
      0.48679534387060697,
      0.36750289488126886,
      0.4386738281421594,
      0.35140225358655,
      0.34562077147790893,
      0.3181929498891023
    ],
    [
      0.3382685346900094,
      0.45243956066957636,
      0.5053160510228918,
      0.49594668907206385,
      0.3675762595250971,
      0.45048252176068915,
      0.3945216992291303,
      0.4256038365143593,
      0.4521922909888789,
      0.4921359472039162,
      0.44233507053500354,
      0.31361304364002507,
      0.4948867864301649,
      0.4945738836629916,
      0.4871429170117856,
      0.46921640351608973,
      0.38594897759286817,
      0.45108955316343247,
      0.0,
      0.4445324277292697,
      0.4380398055279693,
      0.32044227920402646,
      0.39602262234655816,
      0.46141305542556443,
      0.33092988543726376,
      0.4521279394532878,
      0.3693837481540998,
      0.36460712229268677,
      0.32516133737832775
    ],
    [
      0.25425788246060765,
      0.36883576617460845,
      0.4124743677387419,
      0.3437430371834811,
      0.3055765401404893,
      0.31793867395244946,
      0.3026448411945337,
      0.3709908915411748,
      0.4306717801971709,
      0.36135996932708214,
      0.34958119272503874,
      0.24379122879251836,
      0.3778441994066133,
      0.37849696257867294,
      0.3522407468557782,
      0.39725182928586666,
      0.324168283380899,
      0.36044777383567506,
      0.35147597227679217,
      0.0,
      0.33786928335911215,
      0.2784866363860419,
      0.34462159739480547,
      0.42273774294427247,
      0.32924374336656737,
      0.3452244319590807,
      0.31843502469662543,
      0.3143152778529623,
      0.2977723497956304
    ],
    [
      0.32526825534384707,
      0.48894079235593835,
      0.44933741136173233,
      0.38892534850082106,
      0.3902139915043956,
      0.42044843378259844,
      0.39368579037757967,
      0.450025315384291,
      0.3577512510632115,
      0.3970426102436839,
      0.42342873163190653,
      0.2677597181114215,
      0.49888223141728005,
      0.5019463493846616,
      0.41136496595084404,
      0.4664994395070903,
      0.38207099511889964,
      0.3888833231121753,
      0.4094841384257417,
      0.3884373662132743,
      0.0,
      0.2976329838347225,
      0.38125523093712665,
      0.39615904016699965,
      0.3432507286627413,
      0.4090788731797488,
      0.3535694852395175,
      0.34056970582507273,
      0.3184458290436256
    ],
    [
      0.36329742781761465,
      0.4487932862900723,
      0.45150464205200014,
      0.38213943127231476,
      0.40303272547827285,
      0.40660882627944717,
      0.4137434240874207,
      0.3986482302391199,
      0.4022794838197046,
      0.36260578542981525,
      0.4061223540043888,
      0.41992125813351633,
      0.3799160653870586,
      0.4421342148221523,
      0.37486833296167354,
      0.35580157058929673,
      0.3495374608251045,
      0.4265456540029282,
      0.3693066299563761,
      0.39047922374401,
      0.42420185317803694,
      0.0,
      0.41024069670787533,
      0.41330596238600426,
      0.36044508369875494,
      0.4103331232960594,
      0.3929428813479172,
      0.3603851742893587,
      0.41368938707410363
    ],
    [
      0.39613064808587906,
      0.48930290342653504,
      0.5223689335006894,
      0.42625296931629,
      0.48693512850307363,
      0.44318567131760433,
      0.42048311365599256,
      0.45862113615354505,
      0.4607905225310964,
      0.4902391150559773,
      0.48024638367000727,
      0.36530686773997645,
      0.4977281907473008,
      0.4711367033918532,
      0.47206758753023514,
      0.49229136846316335,
      0.5189782656270479,
      0.4724295349180332,
      0.47802744859007573,
      0.45939598809256776,
      0.4873146468630487,
      0.40089705181691593,
      0.0,
      0.5212761191242827,
      0.5078563894852877,
      0.46022323539618815,
      0.4593534782325197,
      0.37905990108897303,
      0.39699532750415556
    ],
    [
      0.29074406884315396,
      0.44404497859320635,
      0.49014687498124654,
      0.4263120691092728,
      0.379344999254567,
      0.41515338201649477,
      0.3469571704486254,
      0.49752581591017364,
      0.45309827178999584,
      0.4127191479645085,
      0.4345187945585123,
      0.2629707954718956,
      0.395017515564309,
      0.471970566928406,
      0.38221315317980165,
      0.5146622337659466,
      0.41405223347154907,
      0.4521758334551893,
      0.4078831766019193,
      0.47433393921981226,
      0.4204522927471235,
      0.2968751638365874,
      0.4250001562589296,
      0.0,
      0.3516620342037182,
      0.3865255696272565,
      0.36729113482882814,
      0.37654012732166753,
      0.3234943973747202
    ],
    [
      0.29836234184848887,
      0.3953201535889035,
      0.4403732760953105,
      0.334327615891856,
      0.38601496872842556,
      0.3211976010260629,
      0.3687376688915105,
      0.3796954254081617,
      0.39551034872820945,
      0.38442211534126947,
      0.37778896193963574,
      0.2845524957815979,
      0.39778624870158574,
      0.3914882287561603,
      0.34253858720560704,
      0.36102605035962587,
      0.38427943017513755,
      0.3943229806873332,
      0.32241784933834516,
      0.38945558447394557,
      0.40618918924295033,
      0.32624527937634884,
      0.48903783825033487,
      0.419008927122416,
      0.0,
      0.3349532822017567,
      0.34722621512121976,
      0.37342914277294104,
      0.3669453243030585
    ],
    [
      0.3963443508265614,
      0.5073933015184044,
      0.5831619914502137,
      0.5078809803100919,
      0.41083138031092714,
      0.4911941048638071,
      0.4050059286336789,
      0.48690322088982607,
      0.5025172284288411,
      0.4684528776085455,
      0.46450219777620116,
      0.2807869349212997,
      0.5011413826348237,
      0.5142094612341901,
      0.49645549887830676,
      0.513153545537639,
      0.42363943735369225,
      0.5034339702119013,
      0.4810682651249336,
      0.4229095633695821,
      0.4829091911718453,
      0.3413961977057851,
      0.43563341916074627,
      0.44553010405577664,
      0.38737106124258025,
      0.0,
      0.4021424242053122,
      0.38573683767718747,
      0.3365085347517407
    ],
    [
      0.3616369770450254,
      0.4349936576848641,
      0.45568814710063155,
      0.4052557158399752,
      0.4031062306506228,
      0.40077669413691464,
      0.382189162881454,
      0.39151729609319874,
      0.4026341979650778,
      0.3987052415076222,
      0.41983975760818804,
      0.39936746645935184,
      0.4399208463996771,
      0.43175667103335513,
      0.3841848143008053,
      0.39171482657926915,
      0.4007535384651899,
      0.44900455649221094,
      0.419225240119228,
      0.4343769939474664,
      0.44720645628308575,
      0.40394345794215125,
      0.45009496576478214,
      0.4345316367627048,
      0.430833381030965,
      0.44009157167419866,
      0.0,
      0.4196933932370148,
      0.38068743857445364
    ],
    [
      0.30017022496294055,
      0.41114989027160376,
      0.4461588571222692,
      0.37402961050949135,
      0.35988748267173065,
      0.366951546977081,
      0.3676181789970616,
      0.388757819779215,
      0.44471676704622176,
      0.3434619783163617,
      0.4035991194827091,
      0.3003217563212126,
      0.38768762925454947,
      0.3873212717187029,
      0.3517277940027925,
      0.41511789860872006,
      0.3634989300033744,
      0.41707767015850306,
      0.3470930921168516,
      0.39996935776670006,
      0.37651267639882935,
      0.34112394655047407,
      0.3750112479451446,
      0.47308440507831384,
      0.3609730275626566,
      0.3625686700108055,
      0.3818850878741058,
      0.0,
      0.35479255666684684
    ],
    [
      0.2890875781929534,
      0.37168495759007336,
      0.369103998305913,
      0.34729047067287544,
      0.3459782951734782,
      0.3384910312199976,
      0.32887076672133175,
      0.3838711721624064,
      0.3508777065933255,
      0.32395785498958096,
      0.33555289977362435,
      0.34118794967429933,
      0.34630839209119,
      0.33989527700669453,
      0.3373102231950713,
      0.3418101168599488,
      0.35964182068918604,
      0.35940346957220703,
      0.32961324207538056,
      0.3387382899693778,
      0.35738735309707814,
      0.39300099840690605,
      0.3459907677448013,
      0.34152917498673463,
      0.3451232552900161,
      0.35778255330062847,
      0.3590759695450376,
      0.3421142150605603,
      0.0
    ]
  ],
  "row_avgs": [
    0.10061986842630789,
    0.4550502382407684,
    0.43745188231552073,
    0.44856519114287957,
    0.4099713849275267,
    0.45975375955621167,
    0.3831870528010607,
    0.38499826906179013,
    0.3430093733088032,
    0.3590093902904536,
    0.46589447072311774,
    0.25106007667539504,
    0.4286386419839903,
    0.4415843321771509,
    0.3806329772302642,
    0.42996517222748354,
    0.3987091860047323,
    0.4285515618989697,
    0.4219982231849295,
    0.34258921524297475,
    0.3942985119886053,
    0.3976010781846571,
    0.4612462367795827,
    0.40406021061883635,
    0.3718804689770785,
    0.4492219068519443,
    0.41477608334212446,
    0.37865244622054534,
    0.3471671357128813
  ],
  "col_avgs": [
    0.3272531172467003,
    0.44254156099685654,
    0.47278130885597675,
    0.42461401034016755,
    0.38130388269397325,
    0.39417345352116523,
    0.36266802789019004,
    0.4116462296687301,
    0.42201539979122893,
    0.4131872509001325,
    0.41439997550545576,
    0.2908243411039833,
    0.4422328844121366,
    0.44585802092120747,
    0.4015392347036748,
    0.4415558369196063,
    0.37875706883108035,
    0.42920335722120456,
    0.39345678033932246,
    0.399845214942053,
    0.41777586042658665,
    0.30922900593653774,
    0.38458190979017626,
    0.42912214352858136,
    0.351457984084279,
    0.40441839949049746,
    0.34821769533709895,
    0.34031641947462654,
    0.3151679712233558
  ],
  "combined_avgs": [
    0.2139364928365041,
    0.4487958996188125,
    0.45511659558574874,
    0.43658960074152353,
    0.39563763381075,
    0.4269636065386885,
    0.37292754034562536,
    0.3983222493652601,
    0.38251238655001607,
    0.38609832059529303,
    0.4401472231142868,
    0.2709422088896892,
    0.43543576319806343,
    0.4437211765491792,
    0.39108610596696947,
    0.4357605045735449,
    0.38873312741790633,
    0.42887745956008716,
    0.407727501762126,
    0.3712172150925139,
    0.406037186207596,
    0.35341504206059743,
    0.4229140732848795,
    0.41659117707370885,
    0.36166922653067873,
    0.4268201531712209,
    0.3814968893396117,
    0.3594844328475859,
    0.33116755346811855
  ],
  "gppm": [
    621.1695925628495,
    649.2527198437627,
    627.5246283422541,
    655.5543889424141,
    675.1181094728407,
    671.7236285323946,
    684.6965312220299,
    661.2553394484733,
    657.4359401445947,
    657.9031357570394,
    663.344116442386,
    714.0105975972668,
    647.3580059736314,
    646.3044260070143,
    667.6966830333488,
    648.3717934491568,
    674.3604950205739,
    655.5153799293354,
    668.7113045163669,
    665.1289447033556,
    658.0676509804376,
    713.363595617846,
    675.4914432284766,
    653.3034792693736,
    690.473305668508,
    662.6004611062571,
    689.637076698665,
    696.3113113199016,
    708.0463272769348
  ],
  "gppm_normalized": [
    1.4525137499693037,
    1.3654411828413249,
    1.3218051381306442,
    1.380750977232061,
    1.4231030268831222,
    1.4145016949972888,
    1.4477545642237912,
    1.3926955341238159,
    1.3876243415595846,
    1.3862047082189737,
    1.3935277453082635,
    1.5212924866759987,
    1.3625633262553407,
    1.3605792616048207,
    1.4035504534336636,
    1.3662247839565576,
    1.4226543666339122,
    1.3786190019050044,
    1.413329740752042,
    1.4058674307561834,
    1.3814813312520542,
    1.499251197767346,
    1.416788291476459,
    1.376687719393655,
    1.4511911906111157,
    1.3935079249359106,
    1.4492483502179407,
    1.470397538807019,
    1.4952885306854926
  ],
  "token_counts": [
    1207,
    434,
    493,
    429,
    428,
    406,
    454,
    432,
    465,
    440,
    395,
    511,
    454,
    459,
    388,
    451,
    431,
    433,
    459,
    475,
    406,
    332,
    377,
    453,
    375,
    404,
    371,
    411,
    404
  ],
  "response_lengths": [
    5557,
    2589,
    2980,
    2536,
    2493,
    2423,
    2596,
    2460,
    2679,
    2588,
    2311,
    2902,
    2657,
    2735,
    2287,
    2724,
    2395,
    2516,
    2706,
    2678,
    2310,
    1955,
    2129,
    2662,
    2149,
    2314,
    2207,
    2296,
    2355
  ]
}