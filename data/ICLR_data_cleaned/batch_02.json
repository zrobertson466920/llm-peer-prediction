[
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nEFFICIENT REWARD POISONING ATTACKS ON ONLINE DEEP REINFORCEMENT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe study reward poisoning attacks on online deep reinforcement learning (DRL), where the attacker is oblivious to the learning algorithm used by the agent and does not necessarily have full knowledge of the environment. We demonstrate the intrinsic vulnerability of state-of-the-art DRL algorithms by designing a general, black-box reward poisoning framework called adversarial MDP attacks. We instantiate our framework to construct several new attacks which only corrupt the rewards for a small fraction of the total training timesteps and make the agent learn a low-performing policy. Our key insight is that state-of-the-art DRL algorithms strategically explore the environment to find a high-performing policy. Our attacks leverage this insight to construct a corrupted environment where (a) the agent learns a high-performing policy that has low performance in the original environment and (b) the corrupted environment is similar to the original one so that the attacker’s budget is reduced. We provide a theoretical analysis of the efficiency of our attack and perform an extensive evaluation. Our results show that our attacks efficiently poison agents learning with a variety of state-of-the-art DRL algorithms, such as DQN, PPO, SAC, etc., under several popular classical control and MuJoCo environments.\n\n1\n\nINTRODUCTION\n\nIn several important applications such as robot control (Christiano et al., 2017) and recommendation systems (Afsar et al., 2021; Zheng et al., 2018), state-of-the-art online deep reinforcement learning (DRL) algorithms rely on human feedbacks in terms of rewards, for learning high-performing policies. This dependency raises the threat of reward-based data poisoning attacks during training: a user can deliberately provide malicious rewards to make the DRL agent learn low-performing policies. Data poisoning has already been identified as the most critical security concern when employing learned models in industry (Kumar et al., 2020). Thus, it is essential to study whether state-ofthe-art DRL algorithms are vulnerable to reward poisoning attacks to discover potential security vulnerabilities and motivate the development of more robust training algorithms.\n\nChallenges in poisoning DRL agents. To uncover practical vulnerabilities, it is critical that the attack does not rely on unrealistic assumptions about the attacker’s capabilities. Therefore for ensuring a practically feasible attack, we require that: (i) the attacker has no knowledge of the exact DRL algorithm used by the agent as well as the parameters of the neural network used for training. Further, it should be applicable to different kinds of learning algorithms (e.g., policy optimization, Q learning) (ii) the attacker does not have detailed knowledge about the agent’s environment, and (iii) to ensure stealthy, the amount of reward corruption applied by the attacker is limited (see Section 3). As we show in Appendix G, these restrictions make finding an efficient attack very challenging.\n\nThis work: efficient poisoning attacks on DRL. To the best of our knowledge, no prior work studies the vulnerability of the DRL algorithms to reward poisoning attacks under the practical restrictions mentioned above. To overcome the challenges in designing efficient attacks and demonstrate the vulnerability of the state-of-the-art DRL algorithms, we make the following contributions:\n\n1. We propose a general, efficient, and parametric reward poisoning framework for DRL algorithms, which we call adversarial MDP attack, and instantiate it to generate several attack methods that are applicable to any kind of learning algorithms and computationally efficient. To the best of our knowledge, our attack is the first one that considers the following four key elements in the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthreat model at the same time: 1. Training time attack, 2. Deep RL, 3. Reward poisoning attack, 4. Complete black box attack (no knowledge or assumption about the learning algorithm and the environment). A detailed explanation for each key point is provided in Appendix A.\n\n2. We provide a theoretical analysis of our attack methods based on certain assumptions on the\n\nefficiency of the DRL algorithms which yields several insightful implications.\n\n3. We provide an extensive evaluation of our attack methods for poisoning the training with several state-of-the-art DRL algorithms, such as DQN, PPO, SAC, etc., in the classical control and MuJoCo environments, commonly used for developing and testing DRL algorithms. Our results show that our attack methods significantly reduce the performance of the policy learned by the agent in the majority of the cases and are considerably more efficient than baseline attacks (e.g., VA2C-P (Sun et al., 2020), reward-flipping (Zhang et al., 2021b)). We further validate the implications of our theoretical analysis by observing the corresponding phenomena in experiments.\n\n2 RELATED WORK\n\nTesting time attack on RL. Testing time attack (evasion attack) in deep RL setting is popular in literature (Huang et al., 2017; Kos & Song, 2017; Lin et al., 2017). For an already trained policy, testing time attacks find adversarial examples where the learned policy has undesired behavior. In contrast, our training time attack corrupts reward to make the agent learn low-performing policies.\n\nData poisoning attack on bandit and tabular RL settings. Jun et al. (2018); Liu & Shroff (2019); Xu et al. (2021b) study data poisoning attack against bandit algorithms. Ma et al. (2019) studies the attack in the offline tabular RL setting. Rakhsha et al. (2020); Zhang et al. (2020b) study the online tabular RL setting relying on full or partial knowledge of the environment and the learning algorithm. Liu & Lai (2021); Xu et al. (2021a) discuss the attack that can work with no knowledge or weak assumptions on the learning algorithm or the environment. Both tabular and bandit settings are simpler than the deep RL setting considered in our work.\n\nObservation perturbation attack and defense. There is a line of work studying observation perturbation attacks during training time (Behzadan & Munir, 2017a;b; Inkawhich et al., 2019) and the corresponding defense (Zhang et al., 2021a; 2020a). The threat model here does not change the actual state or reward of the environment, but instead, it changes the learner’s observation of the environment by generating adversarial examples. In contrast, for the poisoning attack as considered in our work, the actual reward or state of the environment is changed by the attacker. The observation perturbation attack assumes access to perturb the sensor of the agent that is used to observe the environment. Therefore, it is not practical when the attacker does not have access to the agent’s sensor, or the agent does not rely on sensors for interacting with the environment.\n\nData poisoning attack on DRL. The work of Sun et al. (2020) is the only other work that considers reward poisoning attack on DRL and therefore is the closest to ours. There are three main limitations of their attack compared to ours (a) the attack requires the knowledge of the learning algorithm (the update rule for learned policies) used by the agent, which is not the complete black box setting, (b) the attack only works for on-policy learning algorithms, and (c) the attacker in their setting makes the decision about attacking after receiving a whole training batch. This makes the attack infeasible when the agent updates the observation at each time step, as in this case it is impossible for the attacker to apply corruption to previous observations in a training batch. We experimentally compare against them by adapting our general attacks to their restricted setting. Our results in Appendix I show that our attack requires much less computational resources and achieves better attack results.\n\nRobust learning algorithms against data poisoning attack. Robust learning algorithms can guarantee efficient learning under the data poisoning attack. There have been studies on robustness in the bandit (Lykouris et al., 2018; Gupta et al., 2019), and tabular MDP settings (Chen et al., 2021; Wu et al., 2021; Lykouris et al., 2021), but these results are not applicable in the more complex DRL setting. For the DRL setting, Zhang et al. (2021b) proposes a learning algorithm guaranteed to be robust in a simplified DRL setting under strong assumptions on the environment (e.g., linear Q function and finite action space). The algorithm is further empirically tested in actual DRL settings, but the attack method used for testing robustness, which we call reward flipping attack, is not very efficient and malicious as we show in Appendix H. Testing against weak attack methods can provide a false sense of security. Our work provides attack methods that are more suitable for empirically measuring the robustness of learning algorithms.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3 BACKGROUND\n\nReinforcement learning. We consider a standard RL setting where an agent is trained by interacting with an environment. The interaction involves the agent observing a state representation of the environment, taking an action, and receiving a reward. Formally, an environment is represented by a Markov decision process (MDP), M = {S, A, P, R, μ}, where S is the state space, A is the action space, P is the state transition function, R is the reward function, and μ is the distribution of the initial states. The training process consists of multiple episodes where each episode is initialized with a state sampled from μ, and the agent interacts with the environment in each episode until it terminates. A policy π : S → D(A) is a mapping from the state space to the space of probability distribution D(A) over the action space. If a policy π is deterministic, we use π(s) to represent the action it suggests for state s. A value function V π M(s) is the expected reward an agent obtains by following the policy π starting at state s in the environment M. We denote V π M(s0) as the policy value for a policy π in M, which measures the performance of π. The goal of the RL agent is to find the optimal policy with the highest policy value π∗ = arg maxπ V π M. For ease of analysis, the state distribution is defined as μπ(s) = Eπ,s0∼μ[(cid:80) 1[st = s]], representing how often a state is visited under policy π in an episode.\n\nM := Es0∼μV π\n\nt\n\nReward poisoning attack on deep RL. In this work we consider a standard data poisoning attack setting (Jun et al., 2018; Rakhsha et al., 2020) where a malicious adversary tries to manipulate the agent by poisoning the reward received by the agent from the environment during training. The attacker observes the current state, action, and reward tuple (st, at.rt) generated during training at each timestep t and injects a corruption ∆t on the true reward rt. As a result, the environment returns the agent with the corrupted observation (st, at, st+1, rt + ∆t) where st+1 is the next state. Next, we describe the restrictions on the attacker’s capabilities as mentioned in the introduction:\n\n1. Limited budget.\n\nThe attacker can only corrupt a small number of timesteps C,\n\ni.e.,\n\n(cid:80)T\n\nt=0\n\n1{∆t ̸= 0} ≤ C and C ≪ T where T is the total number of training steps.\n\n2. Limited per-step corruption. The corruption at each timestep is limited by |∆t| ≤ B, ∀t ∈ [T ].\n\n3. Limited per-episode corruption: The total corruption across an episode is limited by\n\n(cid:80)\n\nt∈te |∆t| ≤ E where te is the set of all timesteps in an episode e.\n\n4. Oblivious of the DRL algorithm. The attacker has no knowledge of the training algorithm or\n\nany parameters in the network used by the agent while training.\n\n5. Oblivious of the environment. The attacker has no knowledge about the MDP M except for the number of dimensions and range of each dimension in the state and action space S, A. Our attacks do not need knowledge of M but can benefit from having access to a good performing policy in M which could be trained by itself, another agent, or publicly available. We consider these additional cases to study the impact of increasing attacker resources on its efficiency.\n\nLet π0 be the best learned policy when the DRL training finishes, the goal of the attacker is to corrupt training such that the performance of the learned policy π0 in the environment M: V π0 is low. Note that V π M is an intrinsic property of a given policy π in M and is independent of the learning algorithm. Reward poisoning does not change V π M but instead makes the agent learn a policy π with lower V π M. For clarity, we summarize the goal, knowledge, and constraints for our attacker in Appendix B. We consider multiple constraints on the attacker to make the setting more realistic defined above. The full constraints are considered in experiments in Section 6. For the purpose of theoretical analysis, we simplify the problem by dropping the constraint on per-episode corruption E in Section 4 and Section 5.\n\nM\n\n4 FORMULATING REWARD POISONING ATTACK A reward poisoning attack algorithm can be represented by its attack strategy At at each timestep during training. An attack strategy At depends on the full observation before that attack, that is, all the states s1:t, actions a1:t, rewards r1:t−1. The output of At, i.e, the corruption on reward, satisfies ∆t = At(s1:t, a1:t, r1:t−1). A practical attack should be constructed in a computationally efficient manner and should work in a practical setting where it is oblivious to the environment and learning algorithm. In Appendix G we show that searching for the optimal or near-optimal attack is computationally hard regardless of the attack constraints in the DRL setting and requires full knowledge of the learning algorithm and environment. Since it is hard to construct both optimal and\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\npractical attacks, we focus on finding feasible attacks that are not necessarily the optimal ones but still make a learning algorithm learn a policy with low policy value with a limited budget. Formally, the attacker’s objective, i.e., finding a feasible attack, can be stated using the following constraints:\n\nfind ∆t=1,...,T s.t. V π0\n\nM ≤ V ;\n\nT (cid:88)\n\nt=1\n\n1[∆t ̸= 0] ≤ C; |∆t| ≤ B, ∀t ∈ [T ].\n\n(1)\n\nTo solve equation 1, one way is to fix the values of V , B, and C and solve for an attack ∆t. However, it is possible that no solution is feasible for certain V , B, and C. Since each feasibility check can be expensive, we look for a more efficient and convenient way to solve equation 1. We fix the attack and estimate the corresponding V , B, and C to satisfy equation 1. An attack algorithm is efficient if it can satisfy equation 1 with low values for B, C, V with V < V π∗ M otherwise the attack is trivial as V π0 M holds without any attack. As confirmed by our theoretical analysis in Section 5 and experiments in Section 6, finding an efficient attack based on equation 1 is non-trivial.\n\nM ≤ V π∗\n\nAdversarial MDP attack. To find attack algorithms for solving equation 1, we introduce a general parametric attack framework called ”adversarial MDP attack” for poisoning the training of deep RL agents. The high-level idea behind our attack is to construct a fixed adversarial environment to train the agent. This idea has been applied in designing attacks in the simpler bandit (Liu & Shroff, 2019) and tabular setting Rakhsha et al. (2020) where they formulate the problem of finding the best adversarial environment for their attack goal as an optimization problem and solve it directly. Solving such an optimization problem is computationally infeasible in the deep RL setting due to the complexity of both the environment and the learning algorithm. Therefore we design new efficient algorithms that are suited to the deep RL setting and our attack scenario. In our attack, the attacker constructs an adversarial MDP (cid:99)M = {S, A, P, (cid:98)R, μ} for the agent to train on by injecting the corrupted reward (cid:98)R to the environment during training. More specifically, for an adversarial MDP attack with (cid:99)M, its attack strategy at round t only depends on the agent’s current state and action:\n\n∆t = At(st, at) = ˆR(st, at) − R(st, at)\n\n(2)\n\nNext, we compute bounds on V , B, and C such that A1:T constructed using equation 2 from a given (cid:99)M in our framework is a feasible solution to equation 1. The lower these values, the more efficient is the adversarial MDP (cid:99)M. We make two simplifying assumptions: (i) the learning algorithm can always find and report the optimal policy from a fixed environment, i.e.,π0 = π∗ always holds. We note that our experimental results show that our attack succeeds with low values of V, B and C on state-of-the-art deep RL algorithms that do not always learn an optimal policy, and (ii) the learning algorithm explores strategically, that is, instead of uniformly exploring all state-action pairs, the algorithm does not waste many rounds to explore the state action pairs that have little value. This assumption is satisfied by RL algorithms (Dong et al., 2019; Jin et al., 2018; Agarwal et al., 2019) and also validated in our experiments.\n\nLower bounds on V, B, and C for a given (cid:99)M. The bound on V relates to the optimal policy ˆπ∗ := arg maxπ V π that the algorithm will learn under (cid:99)M based on our first assumption, i.e., π0 = (cid:99)M ˆπ∗. We can directly bound V by the policy value of ˆπ∗ under M: V ≥ V ˆπ∗ M . It is straightforward to bound B as B ≥ || ˆR − R||∞. For bounding C, our attack applies corruption whenever the learning algorithm chooses an action at at a state st such that the reward function at this state action pair are different for the real and adversarial MDP, i.e, ˆR(st, at) ̸= R(st, at). Then C can be bound as C ≥ (cid:80)T 1[ ˆR(st, at) ̸= R(st, at)]. Under our second assumption on strategic exploration of the learning algorithm, the bound on C will be low if (cid:99)M satisfies (cid:98)R(s, ˆπ∗(s)) = R(s, ˆπ∗(s)) for all states s, as for most of the time at = ˆπ∗(s), resulting in 1[ ˆR(st, at) ̸= R(st, at)] = 0.\n\nt=1\n\nEffect of given V, B and C on (cid:99)M. Raising the value of B or C increases the number of adversarial MDP’s that satisfy equation 1. To achieve the same value of V , a larger value of B can reduce the requirement on C to ensure existance of feasible solutions, and vice versa. Next, we will instantiate our framework to construct specific (cid:99)M that result in efficient attacks which significantly reduce the performance of the learned policy under a low budget.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n5 POISONING ATTACK METHODS\n\nIn this section, we design new reward poisoning attacks for deep RL by instantiating our adversarial MDP attack framework. Each instantiation provides a parameterized way to construct an adversarial MDP (cid:99)M via a parameter ∆, corresponding to the amount of reward corruption for poisoning applied by the attacker at a timestep. The definition of ∆ yields |∆| = || ˆR−R||∞, resulting in a lower bound on requirement B ≥ |∆|. For a given value of V, let ∆(V ) be the minimum absolute value of the parameter ∆ for the instantiation to satisfy the requirement on V in equation 1. We note that ∆(V ) corresponds to the minimum requirement on B for the attack to satisfy V in equation 1. We provide symbolic expressions for ∆(V ) corresponding to each instantiation and then use it to construct an upper bound on ∆(V ), which is easier to reason about than exact expressions. We define GM V and BM V with respect to value V to be the set of all policies with policy value > V and ≤ V respectively under M. The upper bound on ∆(V ) will be constructed using GM V . We will empirically examine the requirement on C for each instantiation in Section 6. An attack is efficient if to satisfy a certain value of V in equation 1, it requires low value of C and B = ∆(V ). All the attack methods we are going to propose do not require any knowledge about both the learning algorithm and the environment, though one method gain benefits from full or partial knowledge of the environment as we find in practice. For simplicity, we start with attacks on environments with discrete action space and then show how to transfer the attack and the corresponding analysis to continuous action space. Note that our analysis in this section is based on the two assumptions we made in Section 4 that the learning algorithms can always learn the optimal policy and explore strategically. All the proofs for the theorems and lemmas can be found in the Appendix E.\n\nV and BM\n\nUniformly random time(UR) attack. We first introduce a trivial instantiation of adversarial MDP attack framework as a baseline, which we call the UR attack. Here, the attacker randomly decides whether to corrupt the reward at a timestep with a fixed probability p and amount ∆ regardless of the current state and action. Formally, the attack strategy of the UR attacker at time t is: At(st, at) = ∆ with probability p, otherwise At(st, at) = 0. Let Vmax = maxπ V π M be the maximum and minimum policy value computed over all policies under M. We provide the following bounds on ∆(V ) for the UR attack:\n\nM and Vmin = minπ V π\n\nTheorem 5.1. For the UR attack parameterized with probability p, the exact expression and an upper bound on ∆(V ) are:\n\n∆(V ) = min\n\nπ1∈BM\n\nV\n\nV π2\n\nM − V π1 p · |Lπ1 − Lπ2|\n\nM\n\n≤\n\nmax π2∈GM\n\nV\n\np · minπ1∈GM\n\nV\n\nVmax − Vmin maxπ2∈BM\n\nV\n\n|Lπ1 − Lπ2|\n\nwhere Lπ := (cid:80) s μπ(s) is the expected length of an episode for an agent following the policy π. Moreover, the expression only holds for ∆ with the same sign as Lπ1 −Lπ2 for the π1, π2 that realize the min-max condition.\n\nV and BM\n\nTheorem 5.1 shows that the value of ∆(V ) depends on the difference in episode length from policies in GM V . A low value of p also makes high value of ∆(V ). In addition, the sign of ∆ needs to be chosen correctly to make V ˆπ∗ M , otherwise it can make the optimal policies look even better under (cid:99)M, which yields the following implications:\n\nM < V π∗\n\nImplication 1. Compared to the UR attack in the right direction, the UR attack in the wrong direction requires a higher value of ∆ to make the learning agent learn the policy with the same performance (expected reward per episode), or even worse: the attack in the wrong direction can never prevent the learning agent from finding the optimal policy.\n\nImplication 1 suggests that for the UR attack, it is important to find the right direction for corruption.\n\nLearned policy evasion (LPE) attack. The high-level idea behind the LPE attack is to make all policies of good performance appear bad to the learning agent. Intuitively, policies of good performance should share similar behavior as there is usually a certain general strategy to behave well in the environment. Therefore if the attacker can make the actions corresponding to such behavior look bad, then all the good policies will appear bad to the agent. Formally, the LPE attack is characterized by the policy π† available to the attacker, and it penalizes the learning algorithm whenever it chooses an action that corresponds to π†. Correspondingly, the attack strategy is: At(st, at) = ∆ · 1{at = π†(st)}, where ∆ < 0 is a fixed value. π† is learned offline by the\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nattacker before the agent starts learning. We will show different ways the attacker can generate π† in Section 6. Next, we analyze the efficiency of the attack. To help our analysis, we introduce the following definition to measure the similarity D(π1, π2) between two policies π1 and π2: Definition 5.2. (Similarity between policies) The similarity of a policy π1 to a policy π2 is: D(π1, π2) = (cid:80)\n\ns∈S μπ1(s)1[π1(s) = π2(s)].\n\nThe similarity of π1 to π2 increases with the frequency with which π2 takes the same action as π1 in the same state s. Note that D(π1, π2) ̸= D(π2, π1), and D(π1, π2) ≤ Lπ1. Theorem 5.3. For LPE attack with π†, the expression and an upper bound on ∆(V ) are:\n\n∆(V ) = min\n\nπ1∈BM\n\nV\n\nmax π2∈GM\n\nV\n\nV π2\n\nM − V π1 D(π2, π†) − D(π1, π†)\n\nM\n\n≤\n\nminπ∈GM\n\nV\n\nVmax − Vmin D(π, π†) − minπ∈BM\n\nV\n\n.\n\nD(π, π†)\n\nD(π, π†) is likely to be 0 in general cases. Theorem In Appendix E we will show that minπ∈BM 5.3 shows that the requirement on ∆ for the LPE attack is inversely proportional to the minimum similarity between π† and a policy from GM V . In practice, we observe that in most cases there are usually certain behaviors shared in common by the non-trivial policies that have better performance than random ones. This yields the following implications:\n\nV\n\nImplication 2. With the same value of ∆, the LPE attack can make the agent learn policies of worse performance with high performing π† compared to the LPE attack with a random policy.\n\nThe LPE attack can generate a random policy as π†. For experiments in Section 6, to simplify implementation, we generate the random policy through random initialization of a learning algorithm different from the one used by the agent. When the attacker has access to a high-performing policy, it can be used as π† which may provide better attack performance. Implication 2 suggests that the LPE attack should use a high-performing policy to attack if possible. To estimate the requirement on C, we give the following lemma:\n\nLemma 5.4. For the LPE attack, (cid:98)R(s, ˆπ∗(s)) = R(s, ˆπ∗(s)), ∀s ∈ S ⇐⇒ D(π†, ˆπ∗) = 0. Given sufficient ∆, the optimal policy under (cid:99)M given by the LPE attack with π† satisfies D(π†, ˆπ∗) = 0.\n\nRecall that in Section 4, we estimate that the requirement on C will be low if the adversarial and real reward are the same at all (s, ˆπ∗(s)), i.e., (cid:98)R(s, ˆπ∗(s)) = R(s, ˆπ∗(s)). Lemma 5.4 suggests that this condition will hold for the LPE attack given sufficient ∆, as for the LPE attack D(π†, ˆπ∗) = 0 ⇐⇒ (cid:98)R(s, ˆπ∗(s)) = R(s, ˆπ∗(s)). This yields\n\nImplication 3: Under the LPE attack with sufficient |∆|, the learning agent will gradually converge to ˆπ∗ under (cid:99)M eventually and few corruptions will be applied afterward, resulting in a decrease in attack frequency as the training goes on.\n\nRandom policy inducing (RPI) attack. The intuitive idea behind the RPI attack is to make the agent believes that a random policy is an optimal one. To achieve this, the attacker can make all the actions that are different from the ones given by the random policy look bad. The RPI attack is characterized by a randomly generated policy π†, and it penalizes the learning algorithm whenever it doesn’t follow the action that corresponds to π†. Formally, the attack strategy of the RPI attack with a policy π† is: At(st, at) = ∆ · 1{at ̸= π†(st)}, where ∆ < 0 is a fixed value. We have the following expression and upper bound on ∆(V ): Theorem 5.5. For RPI attack with policy π†, the expression and an upper bound on ∆(V ) are:\n\n∆(V ) = min π1∈BV\n\nmax π2∈GV\n\nM − V π1 (Lπ2 − Lπ1) − (D(π2, π†) − D(π1, π†))\n\nV π2\n\nM\n\n≤\n\nVmax − Vmin\n\nminπ∈GM\n\nV\n\n(Lπ − D(π, π†))\n\nTheorem 5.5 suggests that the requirement on B will be low if all policies in GM V have low similarity to π†. With the same observation we give about the similarity between policies better than the random ones, it suggests that ∆(V ) will be less when π† is a random policy. Then theorem 5.5 gives\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nImplication 4: With the same value of ∆, the RPI attack with random π† can make the learning algorithm learn policies of worse performance than the RPI attack with high performing π†.\n\nImplication 4 suggests that the RPI attack should always use a random policy for the attack. To estimate the requirements on C, we give the following lemma: Lemma 5.6. For the RPI attack, (cid:98)R(s, ˆπ∗(s)) = R(s, ˆπ∗(s)) ⇐⇒ ˆπ∗ = π†. Given sufficient ∆, the optimal policy under (cid:99)M constructed by the RPI attack with π† is ˆπ∗ = π†.\n\nLemma 5.6 suggests that with a sufficient value of |∆|, (cid:98)R(s, ˆπ∗(s)) = R(s, ˆπ∗(s)) will hold, resulting in low requirement on C. This also implies that\n\nImplication 5: For the RPI attack with sufficient |∆|, the learning agent will gradually converge to π†, resulting in a decrease in the frequency of corruption as the training goes on.\n\nOur two main attack methods LPE and RPI proposed so far work with negative corruption on reward and avoid attacking the action corresponding to ˆπ∗ in every state. The methods will work well under our assumptions on learning algorithms. An alternative is to attack with positive corruption on the reward. Under the framework of adversarial MDP attack, such an attack can result in less requirements on C when the learning agent does the opposite to our second assumption, i.e., exploring the sub-optimal actions more often. To compare the difference in performance between these types of attack, we propose a variant of RPI attack called random policy promoting (RPP) attack.\n\nRandom policy promoting (RPP) attack. The RPP attack shares the same intuition about highlighting a random policy, but instead, it positively rewards the actions corresponding to the random policy. Formally, the attack strategy of RPP attack with a policy π† is At(st, at) = ∆ · 1{at = π†(st)}, where ∆ > 0 is a fixed value. The expression and an upper bound on ∆(V ) are: Theorem 5.7. Under RPP attack with policy π†, the expression and an upper bound for ∆(V ) are:\n\n∆(V ) = min π1∈BV\n\nmax π2∈GV\n\nV π2\n\nM − V π1 D(π2, π†) − D(π1, π†)\n\nM\n\n≤\n\nVmax − Vmin\n\nmax{Lπ† − maxπ∈GM\n\nV\n\nD(π, π†), 0}\n\n− maxπ∈GM\n\nWe note that compared to the RPI attack, the RPP attack requires more B if Lπ† it results in Lπ† D(π, π†)). Further if Lπ† − maxπ∈GM satisfied with any value of B. This happens when a policy in GM corruption than policies from BM\n\nD(π, π†) < minπ∈GM\n\nLπ − maxπ∈GM\n\nD(π, π†) becomes less than 0, then equation 1 can never be V benefits more from the positive\n\n< minπ∈GM D(π, π†) ≤ minπ∈GM\n\nLπ, as (Lπ −\n\nV . This implies that\n\nV\n\nV\n\nV\n\nV\n\nV\n\nV\n\nImplication 6: For environments where policies of high values are associated with long episodes, the RPI attack can make the learning agent learn worse policy than the RPP attack, and the RPP attack may even be ineffective regardless of ∆.\n\nAs mentioned above, the requirement on C for the RPP and RPI attack depends on how much the learning algorithm deviates from our second assumption in Section 4. From experiments in Section 6, under the same constraints on the attack, we observe that for most learning algorithms, the RPI attack usually achieves better attack results. Our results suggest that although the RPP attack can be more efficient for certain learning algorithms and environments, the RPI attack is in general more reliable and efficient than the RPP attack.\n\nFinally, note that the upper bound we provide for ∆(V ) of all attacks here satisfies ∆(V ) ≥ (Vmax − Vmin)/Lmax, where Lmax := maxπ Lπ is the maximum episode length. Therefore for experiments in Section 6, we always let |∆| > (Vmax − Vmin)/Lmax.\n\nExtension to environments with continuous action space. To extend the above attacks from discrete to continuous action space, we adaptively discretize the continuous action space with respect to the action from the learning agent. Formally, we consider two actions the same if their distance in the action space is less than a given threshold r. Then the aforementioned attack methods can decide whether to apply corruption based on whether two actions are considered the same given r. For example, the attack strategy for the LPE attack with π† in continuous action space is At = ∆ · 1[||at − π†(st)||2 ≤ r]. Accordingly, we define the similarity between policies for continuous action space parameterized by r:\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition 5.8. (Similarity between policies under distance r) The similarity of a policy π1 to a policy π2 in continuous action space parameterized by r is defined as Dc(π1, π2, r) = (cid:80)\n\ns∈S μπ1(s)1[∥π1(s) − π2(s)∥2 ≤ r].\n\nBy replacing D(π1, π2) with Dc(π1, π2, r), we can transfer the analysis for the attack from discrete action space to continuous action space. Note that while we measure the distance in L2-norm, any other norm will also work. The extension adds an additional parameter r for the attack methods. In Appendix D we show how r influences the attacks.\n\n6 EXPERIMENTS\n\nWe evaluate our attack methods from Section 5 for poisoning training with state-of-the-art DRL algorithms in both the discrete and continuous settings. We consider learning in environments typically used for assessing the performance of the DRL algorithms in the literature. As for the implications in Section 5, in Appendix C we experimentally show that they hold when attacking practical DRL algorithms even though they do not necessarily satisfy our assumptions from Section 4.\n\nLearning algorithms and environments. We consider 4 common Gym environments (Brockman et al., 2016) in the discrete case: CartPole, LunarLander, MountainCar, and Acrobot, and 4 continuous cases: HalfCheetah, Hopper, Walker2d, and Swimmer. The DRL algorithms in the discrete setting are: dueling deep Q learning (Duel) (Wang et al., 2016) and double dueling deep Q learning (Double) (Van Hasselt et al., 2016) while for the continuous case we choose: deep deterministic policy gradient (DDPG), twin delayed DDPG (TD3), soft actor critic (SAC), and proximal policy optimization (PPO). Overall, the 6 algorithms we consider cover the popular paradigms in modelfree learning algorithms: policy gradient, Q-learning, and their combination. The implementation of the algorithms is based on the spinningup project (Achiam, 2018).\n\nτ =0\n\nτ ∈te\n\n1[|Aτ > 0] > C or |∆| + (cid:80)\n\nExperimental setup. The attacks set |∆| = B, and the signs of ∆ are specified in each attack’s strategy. We consider more strict and practical constraints (as described in Section 3) on the attacker than in our theoretical analysis. To work with extra constraints, we modify our adversarial MDP attack framework: if applying corruption as per the attack strategy given by the framework in Section 4 will break the constraints on E or C at a time step, i.e. if at time t in episode e, 1 + (cid:80)t−1 |At| > E, then the attacker applies no corruption at that time step. We choose T to ensure that the learning algorithm can converge within T time steps without the attack. We evaluate the effectiveness of our attacks with different values of C such that the ratio C/T is low. Since the attacker is unaware of the learning algorithm in all of our attacks, for each environment, whenever the attacker needs to learn π† offline, it does so with an algorithm different from the agent’s learning algorithm. More specifically, for each environment we select a pair of learning algorithms that are most efficient in learning from the environment (without attack), then while we use one of them for the learning agent to train, the other will be used by the attacker to learn a high performing policy or generate a random policy π†. Our criteria yield cases where the learning algorithms in a pair belong to different learning paradigms and have different architectures of neural networks. Our results demonstrate that the efficiency of our attack methods does not depend upon the similarity between the learning algorithms.\n\nTo determine E and B, we note that Vmax − Vmin represents the maximum environment-specific net reward an agent can get during an episode, and (Vmax−Vmin) represents the range of average reward at a time step for an agent. We set E = λE · (Vmax − Vmin), and B = λB · (Vmax−Vmin) where λE ≤ 1, λB > 1 are normalization constants to ensure that the values of E and B represent similar attack power across different experiments. We choose λE ≤ 1 to ensure that the corruption in an episode is ≤ the maximum net reward a policy can achieve during an episode. We choose λB > 1 that ensures B > (Vmax−Vmin) . This is because the upper bounds on ∆(V ) according to our theorems in Section 5 should be > (Vmax−Vmin) and E for different environments are in the Appendix D.\n\nfor all attack algorithms. The exact values of T , B,\n\nLmax\n\nLmax\n\nLmax\n\nLmax\n\nMain results. A subset of our main results is shown in figure 1. The full set can be found in Appendix D. The x axis is C/T ; the y axis is the policy value of the best policy the learning algorithm learned after each epoch across the whole training process. The y value at each data point is averaged over 10 experiments under the same setting. For the same constraint on C, we consider\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Highest policy value of learned policies by algorithms under no or different attacks.\n\nan attack to be more efficient than another if it has a lower value on the y axis. We consider an attack to be successful if the resulting policy value is lower than the two baselines: the learning agent under no attack and the UR attack. For the UR attack, we set p = C/T so that the corrupted rounds are randomly distributed in the whole training process. We always choose the sign of ∆ that gives the best attack result. In figure 1, the best UR attack has only a small influence on the learning algorithm. We also compare to two other empirical attack methods proposed in previous work (Zhang et al., 2021b; Sun et al., 2020). In Appendix H we explain how the reward flipping attack from Zhang et al. (2021b) works and show that it is no more efficient than the UR attack. In Appendix I we show that the VA2C-P attack in Sun et al. (2020) not only has more limitations as mentioned in Section 2 but also less efficient and is significantly more computationally expensive than our attacks.\n\nFor the LPE attack, we consider three variants based on π†: (1) the attacker does not have any knowledge about the environment and uses a random policy as π†, (2) the attacker trains in the environment for T steps and chooses the best policy as π†, and (3) Same as (2) except that the attacker selects a policy as π† which has a policy value that is the closest to the mean of the policy values in the first two cases. This variant is used to check the effect of learning suboptimal π† on the attack performance. We observe in figure 1 that variants (2) and (3) always succeed while (1) fails in 1 case (LunarLander learned by Double). Comparing the three variants, variants (2), and (3) usually achieve better attack results than variant (1). Especially, we notice that LPE (2) achieves the best attack result for training with the Duel algorithm in MountainCar and with the Double algorithm in Acrobot with corruption budget C/T = 0.04 and 0.02 respectively, as the learning algorithms do not learn policies better than random ones with minimal performance. (−200 and −500 are the minimum rewards from an episode in the two environments respectively)\n\nFor the RPI and RPP attacks, they are effective in most cases except that RPI fails in 2 cases (Acrobot learned by Double, and Swimmer learned by PPO which can be found in the full set of results in Appendix D), and RPP fails in 1 case (CartPole learned by Double). Across all cases, the RPI attack usually has better performance than the RPP attack. Especially, we notice that in Acrobot and MountainCar environments, RPI attack achieves better attack results when the learning algorithm is Duel, and the opposite is true when the learning algorithm is Double. This is probably because the Double algorithm does more exploration in suboptimal state action pairs than the Duel algorithm.\n\n7 CONCLUSION AND LIMITATIONS\n\nIn this work, we studied the security vulnerability of DRL algorithms against training time attacks. We designed a general, parametric framework for reward poisoning attacks and instantiated it to create several efficient attacks. We provide theoretical analysis yielding insightful implications validated by our experiments. Our detailed evaluation confirms the efficiency of our attack methods pointing to the practical vulnerability of popular DRL algorithms. Our attacks have the following limitations: (i) not applicable for other attack goals, e.g, to induce a target policy, (ii) cannot find the optimal attacks, and (iii) do not cover state poisoning attack.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n8 REPRODUCIBILITY STATEMENT\n\nFor the purpose of reproducing the experimental results, we provide the code in the supplementary materials and the necessary instructions in the README file. We provide the experiment details about the setup and hyper-parameters in Section 6 and Appendix D. For checking the correctness of our theoretical analysis, the proof for all theorems and lemmas in Section 5 can be found in Appendix E.\n\nREFERENCES\n\nJoshua Achiam. Spinning Up in Deep Reinforcement Learning. 2018.\n\nM Mehdi Afsar, Trafford Crump, and Behrouz Far. Reinforcement learning based recommender\n\nsystems: A survey. arXiv preprint arXiv:2101.06286, 2021.\n\nAlekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and\n\nalgorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, 2019.\n\nVahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction attacks. In International Conference on Machine Learning and Data Mining in Pattern Recognition, pp. 262–275. Springer, 2017a.\n\nVahid Behzadan and Arslan Munir. Whatever does not kill deep reinforcement learning, makes it\n\nstronger. arXiv preprint arXiv:1712.09344, 2017b.\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\n\nWojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n\nYifang Chen, Simon Du, and Kevin Jamieson. Improved corruption robust algorithms for episodic In International Conference on Machine Learning, pp. 1561–1570.\n\nreinforcement learning. PMLR, 2021.\n\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Proc. Neural Information Processing Systems (NeurIPS), pp. 4299–4307, 2017.\n\nKefan Dong, Yuanhao Wang, Xiaoyu Chen, and Liwei Wang. Q-learning with ucb exploration is\n\nsample efficient for infinite-horizon mdp. arXiv preprint arXiv:1901.09311, 2019.\n\nAnupam Gupta, Tomer Koren, and Kunal Talwar. Better algorithms for stochastic bandits with\n\nadversarial corruptions. In Conference on Learning Theory, pp. 1562–1578. PMLR, 2019.\n\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n\nSandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks\n\non neural network policies. arXiv preprint arXiv:1702.02284, 2017.\n\nMatthew Inkawhich, Yiran Chen, and Hai Li. Snooping attacks on deep reinforcement learning.\n\narXiv preprint arXiv:1905.11832, 2019.\n\nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably effi-\n\ncient? Advances in neural information processing systems, 31, 2018.\n\nKwang-Sung Jun, Lihong Li, Yuzhe Ma, and Xiaojin Zhu. Adversarial attacks on stochastic bandits.\n\narXiv preprint arXiv:1810.12188, 2018.\n\nJernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. arXiv preprint\n\narXiv:1705.06452, 2017.\n\nRam Shankar Siva Kumar, Magnus Nystr ̈om, John Lambert, Andrew Marshall, Mario Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry perspectives. In 2020 IEEE Security and Privacy Workshops (SPW), pp. 69–75. IEEE, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tactics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748, 2017.\n\nFang Liu and Ness Shroff. Data poisoning attacks on stochastic bandits. In International Conference\n\non Machine Learning, pp. 4042–4050. PMLR, 2019.\n\nGuanlin Liu and Lifeng Lai. Provably efficient black-box action poisoning attacks against reinforce-\n\nment learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nThodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial corruptions. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pp. 114–122, 2018.\n\nThodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun. Corruption-robust exploration in episodic reinforcement learning. In Conference on Learning Theory, pp. 3242–3245. PMLR, 2021.\n\nYuzhe Ma, Xuezhou Zhang, Wen Sun, and Xiaojin Zhu. Policy poisoning in batch reinforcement\n\nlearning and control. arXiv preprint arXiv:1910.05821, 2019.\n\nAmin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching via environment poisoning: Training-time adversarial attacks against reinforcement learning. In International Conference on Machine Learning, pp. 7974–7984. PMLR, 2020.\n\nYanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online rl\n\nwith unknown dynamics. arXiv preprint arXiv:2009.00774, 2020.\n\nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-\n\nlearning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.\n\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International conference on machine learning, pp. 1995–2003. PMLR, 2016.\n\nTianhao Wu, Yunchang Yang, Simon Du, and Liwei Wang. On reinforcement learning with adversarial corruption and its application to block mdp. In International Conference on Machine Learning, pp. 11296–11306. PMLR, 2021.\n\nHang Xu, Rundong Wang, Lev Raizman, and Zinovi Rabinovich. Transferable environment poisoning: Training-time attack on reinforcement learning. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1398–1406, 2021a.\n\nYinglun Xu, Bhuvesh Kumar, and Jacob D Abernethy. Observation-free attacks on stochastic ban-\n\ndits. Advances in Neural Information Processing Systems, 34, 2021b.\n\nHuan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on observations. arXiv preprint arXiv:2003.08938, 2020a.\n\nHuan Zhang, Hongge Chen, Duane Boning, and Cho-Jui Hsieh. Robust reinforcement learning on\n\nstate observations with learned optimal adversary. arXiv preprint arXiv:2101.08452, 2021a.\n\nXuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks against reinforcement learning. In International Conference on Machine Learning, pp. 11225– 11234. PMLR, 2020b.\n\nXuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun. Robust policy gradient against strong data corruption. In International Conference on Machine Learning, pp. 12391–12401. PMLR, 2021b.\n\nGuanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. DRN: A deep reinforcement learning framework for news recommendation. In Proc. World Wide Web Conference on World Wide Web, WWW, pp. 167–176. ACM, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA KEY ELEMENTS EXPLANATION\n\nWe mentioned there are four key elements that our attack methods address at the same time. The four key elements are 1. Training time attack, 2. Deep RL, 3. Reward poisoning, 4. Complete black box. To the best of our knowledge, we are the only paper that covers the above four elements at the same time. Below are the explanations for what each key element means and why it is important to be considered:\n\n1. Training time attack: Testing time attacks target an already learned policy, where the attacker wants to make the learned policy misbehave by crafting examples, such as an adversarial state where the policy suggests a sub-optimal action. The target of training time attacks is a learning algorithm trying to learn a policy, where the attacker wants to make the learning agent learn a policy having undesired behavior by corrupting the training process. In this work we propose efficient training time attacks against DRL so that practical threat against DRL is better understood.\n\n2. Deep RL setting: Most of the works on training time attacks study simpler learning settings like bandit and tabular MDP cases. Considering that DRL is more practical in real-world applications, we study training time attacks in the deep MDP settings.\n\n3. Reward poisoning attack: The threat models for RL include three types of poisoning: state, action, and reward poisoning, and all of them have been studied in different literature. In this work we focus on reward poisoning as it is a more practical threat in applications where the agent collects reward from a human user.\n\n4. Complete black box: In practical cases such as a recommendation system, the learning agent needs to formulate the MDP by itself, which is private to the agent and unknown to the attacker. It is also likely that the attacker does not have any knowledge of the learning algorithm as it is also private information held by the learning agent. To model realistic attacks, we consider attackers with no prior knowledge about both the environment and the learning algorithm.\n\n5. Real time attack: To work in a real-time manner, the attacker should satisfy the two following conditions: 1. It is able to compute the corruption at each time step in a short time. 2. It need to decide and apply corruption at each timestep after the\n\nB A SUMMARY OF THE INFORMATION ABOUT THE ATTACKER\n\nFor clarity, we summarize all information about the attacker in our setting, including its knowledge, constraint, and goal.\n\nTable 1: The knowledge, constraint, and goal of the attacker\n\nDescription\n\nKnowledge\n\nConstraints\n\nGoal\n\n1. Has no information about agent’s learning algorithm 2. Only aware of the state and action spaces of the environment. 3. Can observe the true state, action, and reward at each timestep during the training process. 1. corruption can be injected at each step is limited 2. Total corruption can be injected at each episode is limited 3. Number of steps the attacker can inject corruption is limited Minimize the performance of the policy learned by the agent\n\nC EXPERIMENTAL VALIDATION OF IMPLICATIONS\n\nExperimental validation of implications 1, 2, 4, and 6 We empirically examine the best value of V learned by the agent under our attack methods parameterized with different values of ∆. Note that the value of |∆| is also the requirement on B for the attack. To remove the dependency of V on C, we set C = T , E = ∞ so that the attacker is never out of budget due to C and E. We choose\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Experimental observations validating implications 1,2,4, and 6 from section 5.\n\nFigure 3: Experimental observations validating implications 3 and 5 from section 5\n\nrepresentative environments to validate our implications in section 5, and the results are shown in figure 2. The x axis is the value of ∆ in (a) the |∆| for the rest used by the attacks. The y axis has the same meaning as in figure 1. In figure 2(a) we run the UR attack with both signs of ∆ on environment Acrobot. We observe that when corrupting in the wrong direction ∆ < 0, the Duel algorithm can still learn the policies of optimal performance. When corrupting in the right direction ∆ > 0, the Duel algorithm learns much worse policies as ∆ increases. This observation agrees with implication 1. In figure 2(b) we run the three variants of the LPE attack on environment MountainCar and Acrobot. We observe that for the same value of ∆, LPE (2) (3) attack can always result in lower policy value than LPE (1) attack, which agrees with implication 2. In figure 2(c) we run the RPI\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nattack together with its special variant (RPI-h) which uses high performing π†. We observe that the RPI attack with a random policy π† always leads to a lower value of the best learned policy compared to the RPI-h with high performing π†, which agrees with implication 4. In figure 2(d) we run the RPI and RPP attack with random π† on CartPole. CartPole is an environment that satisfy the condition in implication 6 where the a policy’s value is the same as its episode length. We observe that RPI attack leads to very low value of the best learned policy while the RPP attack does not influence the learned policy value no matter how large |∆| it uses, which agrees with implication 6.\n\nExperimental validation of implications 3 and 5. We experimentally examine the requirement on attack budget C for our attack methods with sufficient value of |∆|. For a fixed value of ∆, we remove the constraints on C and E so that the corruption can always be applied following At defined by different attack methods, and then we measure how many steps are corrupted in each epoch under the attack. The environment we choose is MountainCar, and the value of |∆| for both LPE (all the three variants as described in section 6) and RPI attack is 10. The results are shown in figure 3. The x axis is the index of epochs during training, and the y axis is the number of time steps that are corrupted in the epoch. For both LPE and RPI attack, we observe that the number of corrupted steps decrease with time and eventually approaches 0. This suggests that for both attacks, the agent gradually never take actions at at the states st that correspond to the ones where no currption will be applied under the attack, i.e., At(st, at) ̸= 0. This observation agrees with implication 3 and 5.\n\nD EXPERIMENTS DETAILS AND ADDITIONAL EXPERIMENTS\n\nThe hyper parameters for the learning algorithms can be found in the codes. The parameters for the setup of the experiments are given in Table 2. Here the parameter r is the additional parameter for attack against environment with continuous action space as discussed in section 5. The choice on r for the LPE attack and RPI/RPP attack are different. In practice we choose the parameters that significantly reduce the performance of the best learned policy by the learning algorithms. In Table 3 We provide the value of Vmin, Vmax, and Lmax for each environment we use to determine the constraints on the attack. These value are given by either the setup of the environment or empirically estimation. For example, in MountainCar-v0, Lmax = 200 and Vmin = −200 are given by the set up directly, as an episode will be terminated after 200 steps, and the reward is −1 for each step. Vmax = −100 is empirically estimated by the highest reward given by the best policy learned by the most efficient learning algorithm. In Table 4 we provide the policy values (expected total reward from an episode) of π† used by LPE attack (2) and (3). Recall that for LPE attack (2), π† represents an expert policy that have very high performance, and for LPE attack (3), π† represents a median expert policy that also have high performance but less than that for LPE attack (2). The whole set of the main results for our attack methods against learning algorithms under full constraints are shown in figure 4.\n\nTable 2: Parameters for experiments\n\nENVIRONMENT\n\nT\n\nCARTPOLE LUNARLANDER MOUNTAINCAR ACROBOT HALFCHEETAH HOPPER WALKER2D SWIMMER\n\n80000 120000 80000 80000 600000 600000 600000 600000\n\nB\n\n5 4\n2.5 4\n42 25 25 0.8\n\nE\n\nr(LPE)\n\nr(RPI/RPP)\n\n500 800 200 500 6300 2500 2500 80\n\n/ /\n/ /\n2 2\n2.2 2.2\n\n/ /\n/ /\n1.5 1.1 1.5 1.0\n\nTo make the figures clean, the variance of the results are not included in the figures. We present a subset of variance for our main results as an example in the table 5 below to show that our results are statistically stable.\n\nBy the definition 5.8, higher value of r results in higher similarity between policies. By theorem 5.3, the LPE attack should have lower value of ∆(V ) given higher value of r; by theorem 5.5 and\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: The complete main results for our attack methods against different learning algorithms in different environment\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Values used to determine the constraints on attack\n\nENVIRONMENT\n\nLmax\n\nVmax\n\nVmin\n\nVmax − Vmin\n\nVmax−Vmin Lmax\n\nCARTPOLE LUNARLANDER MOUNTAINCAR ACROBOT HALFCHEETAH HOPPER WALKER2D SWIMMER\n\n500 1000 200 500 1000 1000 1000 1000\n\n500 200 -100 -100 12000 4000 5000 120\n\n0 -1000 -200 -500 0\n0 0\n0\n\n500 1200 100 400 12000 4000 5000 120\n\n1 1.2 0.5 0.8 12 4\n5 0.12\n\nTable 4: Policy value of π† used by LPE attack (2) and (3). Here ALG1 and ALG2 are the pair of learning algorithms we use in each environment. V π† M-(2)-1 is the policy value of π† for LPE attack (2) when the learning algorithm for the agent is ALG1 (implying that the learning algorithm used by the attacker to learn π† is ALG2). The meanings for the last three columns are similar.\n\nENVIRONMENT\n\nALG1\n\nALG2\n\nV π†\n\nM -(2)-1\n\nV π†\n\nM -(3)-1\n\nV π†\n\nM -(2)-2\n\nV π†\n\nM -(3)-2\n\nCARTPOLE LUNARLANDER MOUNTAINCAR ACROBOT HALFCHEETAH HOPPER WALKER2D SWIMMER\n\nDUEL DUEL DUEL DUEL DDPG TD3 TD3 DDPG\n\nDOUBLE DOUBLE DOUBLE DOUBLE SAC SAC SAC PPO\n\n500 154 -108 -101 12374 3619 5172 120\n\n220 2\n-158 -200 6007 1828 2552 61\n\n500 202 -101 -100 12766 3562 4622 120\n\n199 10 -156 -199 5974 1801 2426 61\n\nTable 5: The values in the table are the variance of the performance of the best learned policy over 10 runs in identical settings. The values in the brackets are the corresponding average value that is reported in the main result figure 1.\n\nEnvironment-Learning algorithm-Attack C = 0.001\n\nC = 0.005\n\nC = 0.01\n\nC = 0.02\n\nHopper-sac-UR Hopper-sac-LPE(1) Hopper-sac-LPE(2) Hopper-sac-LPE(3) Hopper-sac-RPI Hopper-sac-RPP Hopper-td3-UR Hopper-td3-LPE(1) Hopper-td3-LPE(2) Hopper-td3-LPE(3) Hopper-td3-RPI Hopper-td3-RPP\n\n106(3468) 125 (3463) 684 (3236) 436(3246) 107(3397) 132(3407) 147(3580) 133(3443) 1299(2589) 1034(3054) 87(3452) 175(3538)\n\n114 (3414) 165 (3335) 131 (3424) 860 (3392) 81(3489) 107(3579) 212(3524) 895(2845) 1008(626) 453(179) 860(3072) 386(3385)\n\n296(3355) 1134 (2220) 224 (3332) 206 (3231) 1118(2224) 666(3129) 215(3482) 286(159) 6(5) 306(107) 206(963) 754(2684)\n\n90(3492) 113 (55) 34 (1051) 690 (1006) 628(2901) 604(2961) 1032(3021) 49(23) 1(4) 1(3) 690(1501) 870(1755)\n\n5.7, the RPI and RPP attack should have lower value of ∆(V ) given lower value of r. This gives the follow implications:\n\nImplication 8: Given unlimited buget on C and E, with the same value of |∆|, the LPE attack can make the learning algorithm learn worse policy with higher value of r, and the opposite is true for the RPI and RPP attack.\n\nTo experimentally validate this implication, we run experiments on Hopper environment and TD3 learning algorithm as an example. We set C = T , E = ∞, and |∆| = 25. The results are shown in figure 5. The observation validates implication 8.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Influence of the distance threshold for similarity r on different attack methods\n\nE PROOF FOR THEOREMS AND LEMMAS\n\nAt the beginning we introduce a simple yet useful lemma for the purpose of simplifying the proof of our theorems and lemmas: Lemma E.1. A necessary and sufficient condition for the optimal policy ˆπ∗ under (cid:99)M has a policy value less than V under M is:\n\nV ˆπ∗\n\nM ≤ V ⇐⇒ max π∈BM\n\nV\n\nV π (cid:99)M\n\n> max π∈GM\n\nV\n\nV π (cid:99)M\n\n.\n\nProof: Necessity: When the l.h.s V ˆπ∗ M ≤ V is true, by definition of BM Since (cid:99)M is the policy of highest policy value under (cid:99)M, we have maxπ∈BM maxπ∈GM BM\n\n. Sufficiency: When the r.h.s maxπ∈BM\n\nV , and by the definition of BM\n\nV , we have V ˆπ∗\n\n> maxπ∈GM\n\nM ≤ V .\n\nV π (cid:99)M\n\nV π (cid:99)M\n\nV\n\nV\n\nV\n\nV we have ˆπ∗ ∈ BM V . = maxπ V π >\n(cid:99)M is true, then ˆπ∗ ∈\n\nV π (cid:99)M\n\nV\n\nV π (cid:99)M\n\nFor convenience, we introduce a definition called value loss to measure the difference between the policy value of a policy under the original and adversarial MDP, : Definition E.2. (Value loss) The value loss of a policy π given the environment M and an adversarial MDP (cid:99)M is defined as δV π\n\nμπ(s)(R(s, π(s))) − ˆR(s, π(s))\n\n(cid:88)\n\n:=\n\nM, (cid:99)M\n\ns\n\n, where μπ(s) is the state distribution for the policy π representing in expectation how often a state s will be visited in an episode.\n\nThe definition of value loss can help us rewriting lemma E.1 as\n\nV ˆπ∗\n\nM ≤ V ⇐⇒ max π∈BM\n\nV\n\nV π\n\nM − δV π\n\nM, (cid:99)M\n\n> max π∈GM\n\nV\n\nV π (cid:99)M\n\n− δV π\n\nM, (cid:99)M\n\n.\n\n(3)\n\nWe will frequently using this result for the proof next.\n\nProof for theorem 5.1 Under the UR attack, the value loss of a policy is\n\nδV π\n\nM, (cid:99)M\n\n= −p · ∆ ·\n\n(cid:88)\n\ns\n\nμπ(s) = −p · ∆ · Lπ.\n\nTo make the r.h.s in Equation 3 hold, the following needs to be satisfied:\n\nmax π∈BM\n\nV\n\nV π\n\nM + p · ∆ · Lπ > max\n\nπ∈BM\n\nV\n\nV π\n\nM + p · ∆ · Lπ.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nEquivalently, it requires that there exists π1 ∈ BM M + p · ∆ · Lπ1 ) − (V π2 ing holds (V π1 (V π1 maxπ1∈BM minπ2∈GM\n\nM + p · ∆ · Lπ1) − (V π2\n\nV\n\nV\n\nV\n\nV , the followM + p · ∆ · Lπ2) > 0. This can further be formalized as\n\nsuch that for all π2 ∈ GM\n\nM + p · ∆ · Lπ2) > 0, which gives\n\n|∆| > min\n\nπ1∈BM\n\nV\n\nV π2\n\nM − V π1 p · |Lπ1 − Lπ2|\n\nM\n\n.\n\nmax π2∈GM\n\nV\n\nThen by the definition of ∆(V ), we have ∆(V ) = minπ1∈BM upper bound can be directly derived as\n\nV\n\nmaxπ2∈GM\n\nV\n\nV π2\n\nM −V π1\n\np·|Lπ1 −Lπ2 | , and an\n\nM\n\nmin π1∈BM\n\nV\n\nmax π2∈GM\n\nV\n\nV π2\n\nM − V π1 p · |Lπ1 − Lπ2|\n\nM\n\n≤ min\n\nπ1∈BM\n\nV\n\nmax π2∈GM\n\nV\n\nVmax − Vmin p · |Lπ1 − Lπ2|\n\n=\n\np · maxπ1∈BM\n\nVmax − Vmin minπ2∈GM\n\nV\n\nV\n\nProof for theorem 5.3\n\nThe value loss for a policy π under the LPE attack with policy π† is:\n\nδV π\n\nM, (cid:99)M\n\n= −\n\n(cid:88)\n\ns\n\nμπ(s) · ∆ · 1[π(s) = π†(s)]\n\n= −∆ · D(π, π†).\n\n|Lπ1 − Lπ2|\n\n.\n\n(4)\n\nEquation 4 says that the value loss for policy π is proportional to its similarity with π†. To make the r.h.s in Equation 3 hold, the following should be satisfied:\n\nmax π1∈BM\n\nV\n\nV π\n\nM + ∆ · D(π1, π†) > max π2∈GM\n\nV\n\nV π\n\nM + ∆ · D(π2, π†).\n\nBy similar analysis from proof for theorem 5.1, it can equivalently be rewritten as\n\n|∆| > min\n\nπ1∈BM\n\nV\n\nmax π2∈GM\n\nV\n\nV π2\n\nM − V π1 D(π2, π†) − D(π1, π†)\n\nM\n\n= ∆(V ).\n\nNext we give an upper bound on ∆(V ). Let π0 := arg minπ∈BM policy value of a policy from BM\n\nV is lower bound by\n\nV\n\nD(π, π†) then the maximum\n\nmax π∈BM\n\nV\n\nV π (cid:99)M\n\n= max π∈BM\n\nV\n\n(V π\n\nM − ∆ · D(π, π†)) ≥ V π0\n\nM − ∆ · D(π0, π†) ≥ Vmin − min\n\nπ∈BM\n\nV\n\nD(π, π†)\n\nFor the maximum policy value of a policy from GM\n\nV , it can be directly upper bound by\n\nmax π∈GM\n\nV\n\nV π (cid:99)M\n\n= max π∈GM\n\nV\n\n(V π\n\nM−∆·D(π, π†)) ≤ max\n\nπ∈BM\n\nV\n\nV π\n\nM− min π∈BM\n\nV\n\n∆·D(π, π†) = Vmax−∆· min π∈BM\n\nV\n\nD(π, π†).\n\nThen the upper bound on ∆(V ) can be given as\n\n∆(V ) ≤\n\nminπ∈GM\n\nV\n\nVmax − Vmin D(π, π†) − minπ∈BM\n\nV\n\nD(π, π†)\n\nNote that one can always find a policy π that share no similarity to π† by always choosing a different action to π†, that is π(s) ̸= π†(s), ∀s, then D(π, π†) = 0. In the situation where the number of actions is large than 2, then such policy can still have random behavior which usually corresponds to low policy value, and we can assume that a policy with no similarity to π† can always be found D(π, π†), then the upper bound on ∆(V ) can be rewritten as in BM ∆(V ) ≤\n\nV , suggesting that minπ∈BM\n\nVmax−Vmin\n\nV\n\nminπ∈GM\n\nV\n\nD(π,π†) .\n\nProof for theorem 5.5\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nThe value loss for a policy π under the RPI attack with policy π† is: (cid:88)\n\nμπ(s) · ∆ · 1[π(s) ̸= π†(s)]\n\nδV π\n\n= −\n\nM, (cid:99)M\n\n= −\n\n= −\n\ns (cid:88)\n\ns (cid:88)\n\ns\n\nμπ(s) · ∆ · (1 − 1[π(s) = π†(s)])\n\nμπ(s) · ∆ −\n\n(cid:88)\n\ns\n\nμπ(s) · ∆ · 1[π(s) ̸= π†(s)])\n\n= −∆ · (Lπ − D(π, π†))\n\n(5)\n\nNote that for the attack policy π† itself, its value loss is 0 as Lπ† Equation 3 hold, the following needs to be satisfied\n\n= D(π†, π†). To make the r.h.s in\n\nmax π1∈BM\n\nV\n\nV π\n\nM + ∆ · (Lπ1 − D(π1, π†)) > max π2∈GM\n\nV\n\nV π\n\nM + ∆ · (Lπ2 − D(π2, π†)).\n\nIt can be equivalently rewritten as\n\n|∆| > min\n\nπ1∈BM\n\nV\n\nmax π2∈GM\n\nV\n\nM − V π1 (Lπ2 − Lπ1) − (D(π2, π†) − D(π1, π†))\n\nV π2\n\nM\n\n= ∆(V ).\n\nNext we give an upper bound on ∆(V ). Since π† is randomly generated, we can assume that it has random behavior in the environment resulting in poor performance, then we can lower bound the the maximum policy value of a policy from BM V by M − ∆ · D(π, π†)) ≥ V π†\n\n− D(π†, π†)) = V π†\n\nM − ∆ · (Lπ†\n\nM ≥ Vmin.\n\n(V π\n\nmax π∈BM\n\nV\n\nV π (cid:99)M\n\n= max π∈BM\n\nV\n\nFor the maximum policy value of a policy from GM\n\nV , it can be directly upper bound as\n\nmax π∈GM\n\nV\n\nV π (cid:99)M\n\n= max π∈GM\n\nV\n\n(V π\n\nM − ∆ · (Lπ − D(π, π†)))\n\n≤ max π∈BM\n\nV\n\nV π\n\nM − min π∈BM\n\nV\n\n∆ · (Lπ − D(π, π†))\n\n= Vmax − ∆ · min π∈BM\n\nV\n\n(Lπ − D(π, π†)).\n\nCombing both, an upper bound on ∆(V ) can be given as\n\n∆(V ) ≤\n\nVmax − Vmin\n\nminπ∈GM\n\nV\n\n(Lπ − D(π, π†))\n\n.\n\nProof for theorem 5.7 Note that the attack strategy for RPP attack has the same form as that of the LPE attack, except that the attack applies positive corruption instead of negative. We can directly write down the policy loss for a policy and ∆(V ) under the RPP attack since they share the same form as those for the LPE attack.\n\nδV π\n\nM, (cid:99)M\n\n= −∆ · D(π, π†).\n\n|∆| > min\n\nπ1∈BM\n\nV\n\nmax π2∈GM\n\nV\n\nV π2\n\nM − V π1 D(π2, π†) − D(π1, π†)\n\nM\n\n= ∆(V ).\n\nFor the upper bound on ∆(V ), we can lower bound the maximum policy value in (cid:99)M of a policy in BM\n\nV by that of π†, that is,\n\nmax π∈BM\n\nV\n\nV π (cid:99)M\n\n≥ V π† (cid:99)M\n\n= V π†\n\nM + ∆ · D(π†, π†) ≥ Vmin + ∆ · Lπ†\n\n.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nThe maximum policy value in (cid:99)M of a policy in GM\n\nV can be upper bound by\n\nmax π∈GM\n\nV\n\nV π (cid:99)M\n\n≤ Vmax + ∆ · min π∈GM\n\nV\n\nD(π, π†).\n\nCombing both, an upper bound on ∆(V ) can be given as\n\n∆(V ) ≤\n\nVmax − Vmin\n\nLπ† − maxπ∈GM\n\nV\n\nD(π, π†)\n\n.\n\nProof for lemma 5.4 Under the LPE attack, the value loss for any policy is ≥ 0 as the corruption ∆ is always negative. If a policy π has no similarity to the attack policy π†, that is, D(π, π†) = 0, then its policy loss is 0. Let D0 be the set of policies that has no similarity to π†. Then as the value of |∆| increases, the value loss for all policies not in D0 increase, then eventually the policy with the highest policy value in D0 will have the highest policy value under the LPE attack with sufficient ∆.\n\nProof for lemma 5.6 Under the RPI attack, the value loss for any policy is ≥ 0 as the corruption ∆ is always negative. The only policy that has 0 value loss is the attack policy π† itself. As the value of |∆| increases, the value loss for all policies increase except for π†, then eventually π† will have the highest policy value under the RPI attack with sufficient ∆.\n\nF PROS AND CONS OF ATTACK METHODS\n\nTo make it clear about which attack method is more promising given a scenario, we summarize the strength and weakness of our attack methods.\n\nLPE attack: The main strength is that the attack has less requirement on corruption budget. The attack only applies perturbation for a small subset of actions, so during agent’s random exploration, it only applies perturbation for a small portion of the steps. As a result, in most experiments we find that LPE attack usually have significant influence on learning when the attack budget is small. The attack can also benefit from having access to a high performing policy if possible, and in experiments we find that LPE-(2) and (3) are usually more efficient than LPE-(1). The weakness of the attack is that it may fail if the high performing policies can have very different behavior, which can happen if the environment is easy and many policies can be thought of as good. CartPole is a good example of such environments, and in the experiments we can also find that LPE attack is less efficient.\n\nRPI attack: Compared to LPE attack, its strength is that it still works even if there are high performing policies of distinct behaviors, as it will only make one policy look the best. This is likely to be the reason why RPI attack is more efficient in CartPole environment. As a cost, it will require more corruption budget, as during agent’s random exploration, it applies corruption for most of the time. In experiments we can see that in many scenarios, the RPI attack is more efficient when attack budget is large compared to the LPE attack\n\nRPP attack: RPP attack is more efficient than the RPI attack if the agent explores sub-optimal actions more often and vice versa. The reason is that the RPP attack perturbs the steps where the agent explores the optimal actions in the adversarial environment, and the RPI attack does the opposite. For example, in Acrobot and MountainCar environment, we observe that when the learning algorithm is double, RPP is more efficient; and the opposite is true when the learning algorithm is duel.\n\nG HARDNESS OF FINDING THE OPTIMAL ATTACK ALGORITHM\n\nFirst we show the space for all possible attack algorithms is exponentially large in T . As discussed in section 3, an attack algorithms can be represented by its attack strategies At at each round t, and the attack strategy is a mapping At : S t × At × Rt−1 → C, where C is the corruption space for all possible amount of corruption, S t = S × S . . . × S , and the meaning of At, Rt−1, and Ct is (cid:125)\n\n(cid:123)(cid:122) t times similar. Given the constraints on the budget of the attacker defined in section 3, finding the optimal attack algorithm requires solving the following optimization problem:\n\n(cid:124)\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nmin At=1,...,T\n\nV π0\n\nM, s.t. ∆t = At(s1:t, a1:t, r1:t−1),\n\nT (cid:88)\n\nt=1\n\n1[∆t ̸= 0] ≤ C, |∆t=1,...,T | ≤ B.\n\n(6)\n\nEven in the tabular setting where the sizes of S, A, ∇, C are all finite, O(|S|T 2 computationally infeasible.\n\nthere are |C|T ) many possible attack algorithms. This makes exhaustive enumeration\n\nin total\n\n|R|T 2\n\n|A|T 2\n\nNext we show the hardness of searching for the optimal or near optimal attack strategy. Zhang et al. (2020b) show that the poisoning attack problem in the simpler tabular setting can be formulated as an RL problem which is harder than the RL problem for the learning agent. More specifically, the input state of the RL problem for the attacker needs to include the parameters of the learning algorithm, and correspondingly the transition function P needs to include how such parameters are updated. In the DRL setting, the learning algorithms are more complex compared to the tabular setting. For example, if the learning algorithm is a deep Q learning algorithm, then the input space for the attacker’s RL problem need to include all the parameters in the neural networks. Clearly both the input space and transition functions are more complicated in the DRL setting, making the RL problem for the attacker significantly harder to solve regardless of the additional constraints given by attacker’s budget.\n\nAt last, note that both exhaustive enumeration and the attacker’s RL formulation requires that the attacker has full knowledge of both the environment and the learning algorithm which is a strong assumption on attacker’s capabilities as mentioned in section 4. Considering all the strong requirements for the attacker and difficulties in finding the optimal or near optimal attack, the goal in our work is to not chase optimality but find feasible attack algorithms that can be constructed efficiently without requiring any knowledge about the environment and the learning algorithm.\n\nH EFFICIENCY OF REWARD FLIPPING ATTACK\n\nHere we analyze and empirically examine the performance of a heuristic attack that appears to be effective as believed in Zhang et al. (2021b). The strategy of the attack is to change the sign of the true reward at each time. We call such attack as reward flipping attack, and its attack strategy can be formally written as At(st, at) = −2rt. Note that such attack also construct a stationary adversarial reward function, suggesting that it also falls in our ”adversary MDP attack” framework. = −V π Under such adversarial MDP (cid:99)M, since all rewards have their signs flipped, we have V π M\n(cid:99)M for all policy π, suggesting that the optimal policy under (cid:99)M actually has the worst performance under the true environment M. However, the disadvantage of the attack method is that it needs to apply corruption at every timestep, resulting in too much requirement on C. Note that this is the same issue as the UR attack has. We further empirically test the efficiency of the reward flipping attack. The attacker cannot apply corruption when it runs out of budget on total corruption steps C, and we do not assume any constraint on B. We consider environment Hopper and HalfCheetah, and set C = 0.01 as considered in our experiments while the remaining parameters are unchanged. Our results show that the performance of the reward flipping attack is comparable to the baseline UR attack as shown in the table 6 below, suggesting that the reward flipping attack is not efficient.\n\nTable 6: Performance of reward flipping attack. The values in the table are the performance of the best policy ever learned by the learning algorithm, which is the same as the y axis of our main experiment results in figure 1\n\nEnvironment-Learning algorithm Reward flipping attack UR attack No attack\n\nHopper-td3 Hopper-sac HalfCheetah-ddpg HalfCheetah-td3\n\n3157 3521 7463 7603\n\n21\n\n3482 3355 6622 7694\n\n3502 3496 9341 9610\n\nUnder review as a conference paper at ICLR 2023\n\nI COMPARISON TO VA2C-P ATTACK\n\nHere we compare the effectiveness of our LPE attack, which is our most efficient attack as an example, and the most ”black box” version of VA2C-P attack proposed in Sun et al. (2020) which knows the learning algorithm but does not know the parameters in its model. Note that the constraints for the two attacks are different in the two papers. To make sure that we are not underestimating the effectiveness of the VA2C-P attack, we let both attacks work under the same constraints used in Sun et al. (2020). The constraints here are characterized by two parameters K and ε. The training process is separated into K batches of steps, and the attacker is allowed to corrupt no more than C out of K training batches. In each training batch with t time steps, let δr = {δr1, . . . , δrt} be the injected reward corruption at each time step, then the corruption should satisfy ||δr||2√ ≤ ε. We modify the LPE attack accordingly to work with such constraints. More specifically, the attack strategy is unchanged when applying corruption will not break the constraints, and we forbid the attack to apply corruption if doing so will break the constraints. To avoid anything that could cause a decrease in efficiency of VA2C-P attack, we do not modify any code related to training and attacking provided by Sun et al. (2020), and implement our attack method in their code.\n\nt\n\nSince VA2C-P has more limitations than LPE attack, we only consider the scenarios where both attack are applicable. As an example, we choose Swimmer as the environment and PPO as the learning algorithm. Here we use the metric in Sun et al. (2020) to measure the performance of the attack. More specifically, we measure the average reward per episode collected by the learning agent through the whole training process. We set K = 1, ε = 1, and the length of training to be 600 episodes where each episode consists of 1000 steps. The results for different attacks are shown in table 7. Each result is the average of 10 repeated experiment, and it is clear that our attack is much more efficient.\n\nTable 7: Comparison between VA2C-P and LPE attack\n\nclean\n\nVA2C-P\n\nLPE-(1)\n\nLPE-(3)\n\nLPE-(2)\n\n30.07\n\n25.49\n\n14.75\n\n7.08\n\n-2.16\n\nWe also notice that our LPE attack computes faster than VA2C-P attack. To compute the attack for 600 training episodes in the experiment, our LPE attack takes 135.7 seconds, while the VA2C-P black box attack takes 7049.5 seconds. In this case, the LPE attack computes 52 times faster than VA2C-P attack.\n\nOne may notice that learning efficiency of PPO algorithm here is not as good as what we show in Figure 4. This is due to different implementation of the same algorithm in our work and Sun et al. (2020). It is a known issue that difference in implementations can lead to very different learning results Henderson et al. (2018). As mentioned before, we build our learning algorithms based on spinning up documents Achiam (2018), and the learning performance of our learning algorithms match the results shown in the spinning up documents.\n\n22",
    "reference": "# Summary Of The Paper\n\nThe work designed a set of black-box adversarial attacks to corrupt a small proportion of training time rewards, and make the agent learn a low-performing policy. The goal and contribution of this work is on efficient poisoning attacks on DRL via reward poisoning, assuming the attacker has no knowledge of the exact DRL algorithm and does not have detailed knowledge about the agent's environment, and the attacker has limits on the amount of the reward corruption it can apply. The formulation of the problem is to find a training time reward perturbation such that the resultant learned policy has a bad performance, and also under the limit of the times of reward corruption and magnitude of reward corruption. However, realizing that directly solving the optimization problem is computationally infeasible. The work proposes to evaluate the limit of any reward poisoning attack algorithm on its attack performance, limits on reward perturbation times and magnitude. The trade-off between limits on reward perturbation times and magnitude as well as resultant policy value affects the number of available adversarial MDP that satisfies the constraints. The work then proposes several practical adversarial attacks on rewards. Uniformly random time attack as a baseline, where the attacker randomly perturb reward with a fixed probability and fixed amount. Learned policy evasion attack: the attacker penalizes the learner whenever it chooses a good action that are shared between several good policies. Random policy inducing attack: a fixed reward perturbation is added whenever the agent behaves differently from a random policy. Random policy promoting attack: rewards positively when the agent chooses actions the same as a random policy. Experiment study on these attacks for DQN, Double DQN, DDPG, TD3, SAC, PPO are presented to demonstrate the performance of these attacks.\n\n# Strength And Weaknesses\n\nThe work presents an interesting study on reward poisoning attacks where the attacker does not know the training algorithm of the victim agent. The main novelty is that it does not require knowing the victim agent's policy training details, in particular the training algorithm and related hyper parameters, while other previous works may require this. However, there are several weakness of the work. \n\nFirst, some descriptions over-claims the contribution of the work. For example, the author claimed the attacker does not know the training algorithm of the victim agent, however, in the experiment section, the work only evaluates the case where the attacker and the victim agent use different training algorithms, without evaluating the case when they use the same algorithm. The author also claims the attacker knows no detailed knowledge of the agent's environment, but can observe the full observation, actions, rewards generated during training at each time step, and these information already leaks more than enough information of the training environment, and the attacker may use this knowledge to perform other kinds of adversarial attack, even when the attacker does not know the training algorithm of the victim agent. Similarly, in the experiment section, the attacker has access to environment to learn attack policy offline, which means the attacker has access to the environment, and this is not consistent with the claim that the attacker does not have detailed knowledge of the environment. \n\nSecond, the organization and/or thought flow of the paper is not carefully designed. The author first proposes a very general optimization problem to solve to find the feasible attacks, but then transits to discuss several weakly-related attacks. For example, the uniformly random time attack can serve as a good baseline to attack the victim with randomly proposed reward perturbation, however, it is not clear why the author chooses to talk about learned policy evasion attack as the immediate next attack. For example, the paper can talk about the limitation of UR attack, and then propose LPE attack that addresses part of the limitations. Similarly, why the RPI attack is proposed following LPE attack, and so on. \n\nThird, the experiments are limited and not thoroughly discussed. The paper spends the majority of its pages discussing the theoretical insights of the different methods, however, does not discuss intuitively the pros and cons of the various proposed algorithms. This making it unclear why these methods are proposed and under what circumstances should the attacker use what algorithm. If one algorithm strongly outperforms the other three, why proposing the other algorithms, etc. The data in figure 1 is also not discussed well, and ideally should have error bar.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: the paper is clear about its goal but less clear about its thought flow. For example, why the authors propose the four different attacks, and the pros and cons of these methods. The figure 1 is also not very clear in terms of what the authors want to imply and what conclusion should we draw. It's clear from the figure that all the attacks are successful to bring the performance down, and the case without attack has the best policy return. However, the used markers to represent different classes are hard to distinguish (e.g. LPE2 and LPE3), and the points need to have error bars, and the fonts need to be larger to be readable.\n\nQuality: the presented algorithms are working well, but are not discussed thoroughly, both in terms of the algorithms themselves and in terms of the results on these algorithms. The authors did say algorithm 1 is better/worse than algorithm 2, but did not provide convincing explanations. \n\nNovelty: since the claims in the paper are kind of over-claiming their contributions, it is not clear how much novelty the work has.\n\nReproducibility: code is provided, not sure if the results would match because we don't have the error bars in the figure. Also, the y-axis in figure 1 says policy value, is that the total episodic rewards or really the policy value? Usually the value is associated with a starting state, what is the starting state?\n\n# Summary Of The Review\n\nTo sum, this work presents a relatively novel set of attack methods on reward poisoning for training time attack on DRL. The contribution of the work is limited and unclear. The experiments require a lot of training and work, but can still be improved and probably should have more space in the paper. Overall, this is an interesting work with a good potential but probably needs more work to improve the writing and experiments.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nFIND YOUR FRIENDS: PERSONALIZED FEDERATED LEARNING WITH THE RIGHT COLLABORATORS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nIn the traditional federated learning setting, a central server coordinates a network of clients to train one global model. However, the global model may serve many clients poorly due to data heterogeneity. Moreover, there may not exist a trusted central party that can coordinate the clients to ensure that each of them can benefit from others. To address these concerns, we present a novel decentralized framework, FedeRiCo, where each client can learn as much or as little from other clients as is optimal for its local data distribution. Based on expectationmaximization, FedeRiCo estimates the utilities of other participants’ models on each client’s data so that everyone can select the right collaborators for learning. As a result, our algorithm outperforms other federated, personalized, and/or decentralized approaches on several benchmark datasets, being the only approach that consistently performs better than training with local data only.\n\n1\n\nINTRODUCTION\n\nFederated learning (FL) (McMahan et al., 2017) offers a framework in which a single server-side model is collaboratively trained across decentralized datasets held by clients. It has been successfully deployed in practice for developing machine learning models without direct access to user data, which is essential in highly regulated industries such as banking and healthcare (Long et al., 2020; Sadilek et al., 2021). For example, several hospitals that each collect patient data may want to merge their datasets for increased diversity and dataset size but are prohibited due to privacy regulations.\n\nFigure 1: Left: Noisy data points generated for each client along a sine curve (solid magenta line) where the x-axis and y-axis correspond to input and output respectively. The corresponding model learned by FedAvg (dotted line) fails to adapt to the local data seen by each client, in contrast to the models learned by each client using our FedeRiCo (dashed lines). Right: The weights used by FedeRiCo to average participant outputs for each client. As the client index increases, the data is generated from successive intervals of the sine curve, and collaborator weights change accordingly.\n\nTraditional FL methods like Federated Averaging (FedAvg) (McMahan et al., 2017) can achieve noticeable improvement over local training when the participating clients’ data are homogeneous. However, each client’s data is likely to have a different distribution from others in practice (Zhao et al., 2018; Adnan et al., 2022). Such differences make it much more challenging to learn a global model that works well for all participants. As an illustrative example, consider a simple scenario\n\n1\n\n−3−2−10123−1.0−0.50.00.51.0client 0client 3client 1client 4client 2client 5012345Client 00.00.5012345Client 10.00.5012345Client 20.00.5012345Client 30.00.5012345Client 40.00.5012345Client 50.00.5Under review as a conference paper at ICLR 2023\n\nwhere each client seeks to fit a linear model to limited data, on an interval of the sine curve as shown in Fig. 1. This is analogous to the FL setting where several participating clients would like to collaborate, but each client only has access to data from its own data distribution. It is clear that no single linear model can be adequate to describe the entire joint dataset, so a global model learned by FedAvg can perform poorly, as shown by the dotted line. Ideally, each client should benefit from collaboration by increasing the effective size and diversity of data, but in practice, forcing everyone to use the same global model without proper personalization can hurt performance on their own data distribution (Kulkarni et al., 2020; Tan et al., 2022).\n\nTo address this, we propose Federating with the Right Collaborators (FedeRiCo), a novel framework suitable for every client to find other participants with similar data distributions to collaborate with. Back to our illustration in Fig. 1. FedeRiCo enables each client to choose the right collaborators as shown on the plots on the right-hand side: each client is able to correctly leverage information from the neighboring clients when it is beneficial to do so. The final personalized models can serve the local distributions well, as demonstrated in the left plot.\n\nMore specifically, our FedeRiCo assumes that each client has an underlying data distribution, and exploits the hidden relationship among the clients’ data. By selecting the most relevant clients, each client can collaborate as much or as little as they need, and learn a personalized mixture model to fit the local data. Additionally, FedeRiCo achieves this in a fully decentralized manner that is not beholden to any central authority (Li et al., 2021a; Huang et al., 2021; Kalra et al., 2021).\n\nOur contributions We propose FedeRiCo, a novel decentralized and personalized FL framework derived based on expectation-maximization (EM). Within this framework, we propose a communication-efficient protocol suitable for fully-decentralized learning. Through extensive experiments on several benchmark datasets, we demonstrate that our approach finds good client collaboration and outperforms other methods in the non-i.i.d. data distributions setting.\n\nPaper outline The rest of the paper is organized as follows. In Section 2 we discuss related approaches towards decentralized federated learning and personalization. Section 3 describes our algorithm formulation and its relationship to expectation-maximization, and an efficient protocol for updating clients. We provide experimental results in Section 4, and conclude in Section 5.\n\n2 RELATED WORK FOR PERSONALIZED FL\n\nMeta-learning Federated learning can be interpreted as a meta-learning problem, where the goal is to extract a global meta-model based on data from several clients. This meta-model can be learned using, for instance, the well-known Federated Averaging (FedAvg) algorithm (McMahan et al., 2017), and personalization can then be achieved by locally fine-tuning the meta-model (Jiang et al., 2019). Later studies explored methods to learn improved meta-models. Khodak et al. (2019) proposed ARUBA, a meta-learning algorithm based on online convex optimization, and demonstrates that it can improve upon FedAvg’s performance. Per-FedAvg (Fallah et al., 2020) uses the Model Agnostic Meta-Learning (MAML) framework to build the initial meta-model. However, MAML requires computing or approximating the Hessian term and can therefore be computationally prohibitive. Acar et al. (2021) adopted gradient correction methods to explicitly de-bias the meta-model from the statistical heterogeneity of client data and achieved sample-efficient customization of the meta-model.\n\nModel regularization / interpolation Several works improve personalization performance by regularizing the divergence between the global and local models (Hanzely & Richt ́arik, 2020; Li et al., 2021b; Huang et al., 2021). Similarly, PFedMe (T Dinh et al., 2020) formulates personalization as a proximal regularization problem using Moreau envelopes. FML (Shen et al., 2020) adopts knowledge distillation to regularize the predictions between local and global models and handle model heterogeneity. In recent work, SFL (Chen et al., 2022) also formulates the personalization as a bi-level optimization problem with an additional regularization term on the distance between local models and its neighbor models according to a connection graph. Specifically, SFL adopts GCN to represent the connection graph and learns the graph as part of the optimization to encourage useful client collaborations. Introduced by Mansour et al. (2020) as one of the three methods for achieving personalization in FL, model interpolation involves mixing a client’s local model with a jointly\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ntrained global model to build personalized models for each client. Deng et al. (2020) further derive generalization bounds for mixtures of local and global models.\n\nMulti-task learning Personalized FL naturally fits into the multi-task learning (MTL) framework. MOCHA (Smith et al., 2017) utilizes MTL to address both systematic and statistical heterogeneity but is restricted to simple convex models. VIRTUAL (Corinzia et al., 2019) is a federated MTL framework for non-convex models based on a hierarchical Bayesian network formed by the central server and the clients, and inference is performed using variational methods. SPO (Cui et al., 2021) applies Specific Pareto Optimization to identify the optimal collaborator sets and learn a hypernetwork for all clients. While also aiming to identify necessary collaborators, SPO adopts a centralized FL setting with clients jointly training the hypernetwork. In contrast, our work focuses on decentralized FL where clients aggregate updates from collaborators, and jointly make predictions.\n\nIn a similar spirit to our work, Marfoq et al. (2021) assume that the data distribution of each client is a mixture of several underlying distributions/components. Federated MTL is then formulated as a problem of modeling the underlying distributions using Federated Expectation-Maximization (FedEM). Clients jointly update a set of several component models, and each maintains a customized set of weights, corresponding to the mixing coefficients of the underlying distributions, for predictions. One shortcoming of FedEM is that it uses an instance-level weight assignment in training time but a client-level weight assignment in inference time. As a concrete example, consider a client consisting of a 20%/80% data mixture from distributions A and B. FedEM will learn two models, one for each distribution. Given a new data point at inference time, the client will always predict 0.2·predA +0.8·predB, regardless of whether it came from distribution A or B. This is caused by the mismatched behaviour between training and inference time. On the contrary, FedeRiCo naturally considers a client-level weight assignment for both training and inference in a decentralized setting.\n\nOther approaches Clustering-based approaches are also popular for personalized FL (Sattler et al., 2020; Ghosh et al., 2020; Mansour et al., 2020). Such personalization lacks flexibility since each client can only collaborate with other clients within the same cluster. FedFomo (Zhang et al., 2021) interpolates the model updates of each client with those of other clients to improve local performance. FedPer (Arivazhagan et al., 2019) divides the neural network model into base and personalization layers. Base layers are trained jointly, whereas personalization layers are trained locally.\n\n3 FEDERATED LEARNING WITH THE RIGHT COLLABORATORS\n\n3.1 PROBLEM FORMULATION\n\nWe consider a federated learning (FL) scenario with K clients. Let [K] := {1, 2, . . . , K} denote the set of positive integers up until K. Each client i ∈ [K] consists of a local dataset Di = {(x(i) s=1 where ni is the number of examples for client i, and the input xs ∈ X and output ys ∈ Y are drawn from a joint distribution Di over the space X × Y.\n\ns , y(i)\n\ns )}ni\n\nThe goal of personalized FL is to find a prediction model hi : X (cid:55)→ Y that can perform well on the local distribution Di for each client. One of the main challenges in personalized FL is that we do not know if two clients i and j share the same underlying data distribution. If their data distributions are vastly different, forcing them to collaborate is likely to result in worse performance compared to local training without collaboration. Our method, Federating with the Right Collaborators (FedeRiCo), is designed to address this problem so that each client can choose to collaborate or not, depending on their data distributions. FedeRiCo is a decentralized framework (i.e. without a central server). For better exposition, Section 3.2 first demonstrates how our algorithm works in a hypothetical all-to-all communication setting, an assumption that is then removed in Section 3.3 which presents several practical considerations for FedeRiCo to work with limited communication.\n\n3.2 FEDERICO WITH ALL-TO-ALL COMMUNICATION\n\nΦ∗\n\nDi\n\nzi\n\nNote that every local distribution Di can always be represented as a mixture of {Dj}K j=1 with some client weights πi = [πi1, . . . , πiK] ∈ ∆K, where ∆K is the (K − 1)-dimensional\n\nK Clients\n\nFigure 2: Graphical model\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(x), y) + c for some parameters φ∗\n\nsimplex1. Let zi be the latent assignment variable of client i, and Π := [π1, . . . , πK]⊤ be the prior Πij = Pr(zi = j). Suppose that the conditional probability pi(y|x) satisfies − log pi(y|x) = i ∈ Rd, loss function l : Y × Y (cid:55)→ R+, and normalization l(hφ∗ K] ∈ Rd×K, Fig. 2 shows the graphical constant c. By using the stacked notation Φ∗ = [φ∗ model of how the local dataset is generated. Our goal is to learn the parameters Θ := (Φ, Π) by maximizing the log-likelihood:\n\n1, . . . , φ∗\n\ni\n\nf (Θ) :=\n\n1 n\n\nlog p(D; Θ) =\n\n1 n\n\nK (cid:88)\n\ni=1\n\nlog p(Di; Θ) =\n\n1 n\n\nK (cid:88)\n\nlog\n\nK (cid:88)\n\ni=1\n\nzi=1\n\np(Di, zi; Θ).\n\n(1)\n\nwhere D := ∪iDi and n := (cid:80) i ni. One standard approach to optimization with latent variables is expectation maximization (EM) (Dempster et al., 1977). The corresponding variational lower bound is given by (all detailed derivations of this section can be found in Appendix A)\n\nL(q, Θ) :=\n\n1 n\n\n(cid:88)\n\ni\n\nEq(zi)[log p(Di, zi; Θ)] + C,\n\n(2)\n\nwhere C is a constant not depending on Θ. To obtain concrete objective functions suitable for optimization, we further assume that pi(x) = p(x), ∀i ∈ [K]. Similar to Marfoq et al. (2021), this assumption is required due to technical reasons and can be relaxed if needed. With this assumption, we perform the following updates at each iteration t:\n\n• E-step: For each client, find the best q, which is the posterior p(zi = j|Di; Θ(t−1)) given the\n\ncurrent parameters Θ(t−1):\n\nw(t)\n\nij\n\n:= q(t)(zi = j) ∝ Π(t−1)\n\nij\n\nexp\n\n−\n\n(cid:34)\n\nni(cid:88)\n\ns=1\n\nl\n\n(cid:16)\n\nhφ(t−1)\n\nj\n\n(x(i)\n\ns ), y(i)\n\ns\n\n(cid:35)\n\n(cid:17)\n\n.\n\n• M-step: Given the posterior q(t) from the E-step, maximize L w.r.t. Θ = (Φ, Π):\n\nΠ(t)\n\nij = w(t)\n\nij\n\nand\n\nΦ(t) ∈ argmin\n\n1 n\n\nK (cid:88)\n\ni=1\n\n(cid:98)Lw,i(Φ)\n\nhφj (x(i)\n\ns ), y(i)\n\ns\n\n(cid:17)\n\n.\n\nΦ\n\n(cid:16)\n\nl\n\nK (cid:88)\n\nj=1\n\nw(t)\n\nij\n\nni(cid:88)\n\ns=1\n\nwhere\n\n(cid:98)Lw,i(Φ) :=\n\n(3)\n\n(4)\n\n(5)\n\nBear in mind that each client can only see its local data Di in the federated setting. The E-step is easy to compute once the models from other clients φj, j ̸= i are available. Π(t) is also easy to obtain as the posterior w(t) is stored locally. However, Φ(t) is trickier to compute since each client can potentially update Φ towards different directions due to data heterogeneity amongst the clients. To stabilize optimization and avoid overfitting from client updates, we rely on small gradient steps in lieu of full optimization in each round. To compute Φ(t) algorithmically, each client i:\n\nij\n\nij\n\n1. Fixes w(t) 2. Broadcasts ∇ (cid:98)Lw,i(Φ(t−1)) to and receives ∇ (cid:98)Lw,j(Φ(t−1)) from other clients j ̸= i. The models\n\nij and computes the local gradient ∇ (cid:98)Lw,i(Φ(t−1)) on local Di.\n\nare updated based on the aggregated gradient with step size η > 0:\n\nΦ(t) = Φ(t−1) − η\n\nK (cid:88)\n\nj=1\n\n∇ (cid:98)Lw,j(Φ(t−1)).\n\n(6)\n\nEach client uses (cid:98)hi(x) = (cid:80)\n\nj w(t)\n\nij hφ(t)\n\nj\n\n(x) for prediction after convergence.\n\nRemark 1 The posterior w(t) importance of model φj on the data Di. When w(t) can perform learning by itself without collaborating with others. When w(t)\n\nij ) reflects the ij is one-hot with a one in the ith position, client i ij is more diverse, client i\n\n(or equivalently the prior in the next iteration Π(t)\n\nij\n\n1One-hot πi is always feasible, but other mixing coefficients may exist.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\ncan find the right collaborators with useful models φj. Such flexibility enables each client to make its own decision on whether or not to collaborate with others, hence the name of our algorithm.\n\nRemark 2 Unlike prior work (Mansour et al., 2020; Marfoq et al., 2021), our assignment variable z and probability Π are on the client level. If we assume that all clients share the same prior (i.e., there is only a vector π instead of a matrix Π), the algorithm would be similar to HypCluster (Mansour et al., 2020). Marfoq et al. (2021) used a similar formulation as ours but their assignment variable z is on the instance level: every data point (instead of client) comes from a mixture of distributions. Such an approach can cause several issues at inference time, as the assignment for novel data point is unknown. We refer the interested readers to Section 2 and Section 4 for further comparison.\n\nTheoretical Convergence Under some regularity assumptions, our algorithm converges as follows:\n\nTheorem 3.1. [Convergence] Under Assumptions E.1-E.6, when the clients use SGD with learning rate η = a0√\n\n, and after sufficient rounds T , the iterates of our algorithm satisfy\n\nT\n\n1 T\n\nT (cid:88)\n\nt=1\n\nE∥∇Φf (Φt, Πt)∥2\n\nF ≤ O\n\n(cid:18) 1 √\nT\n\n(cid:19)\n\n,\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∆Πf (Φt, Πt) ≤ O\n\n(cid:18) 1\n\nT 3/4\n\n(cid:19)\n\n,\n\n(7)\n\nwhere the expectation is over the random batch samples and ∆Πf (Φt, Πt) := f (Φt, Πt) − f (Φt, Πt+1) ≥ 0.\n\nDue to space limitations, further details and the complete proof are deferred to Appendix E. The above theorem shows that the gradient w.r.t. the model parameters Φ and the improvement over the mixing coefficients Π becomes small as we increase the round T , thus converging to a stationary point of the log-likelihood objective f .\n\n3.3 COMMUNICATION-EFFICIENT PROTOCOL\n\nSo far, we have discussed how FedeRiCo works in the all-to-all communication setting. In practice, FedeRiCo does not require excessive model transmission, and this subsection discusses several practical considerations to ensure communication efficiency. Specifically, we tackle the bottlenecks in both the E-step (3) and the M-step (4) since they require joint information of all models Φ. Due to space constraint, the pseudocode is provided in Algorithm 1, Appendix B.\n\nj\n\nE-step For client i, the key missing quantity to compute (3) without all-to-all communication is the loss l(φ(t−1) ), or likelihood p(Di|zi = j; Φ(t−1)), of other clients’ models φj, j ̸= i. Since the models Φ are being updated slowly, one can expect that l(φ(t−1) ) will not be significantly different j\nfrom the loss l(φ(t−2) ) of the previous iteration. Therefore, each client can maintain a list of losses for all the clients, sample a subset of clients in each round using a sampling scheme S (e.g., ε-greedy sampling as discussed later), and only update the losses of the chosen clients.\n\nj\n\nM-step To clearly see how Φ is updated in the M-step, let’s focus on the update to a specific client’s model φi. According to (5) and (6), the update to φi is given by\n\n−η\n\nK (cid:88)\n\nj=1\n\nw(t)\n\nji\n\nnj (cid:88)\n\ns=1\n\n(cid:16)\n\n∇φil\n\nhφi(x(j)\n\ns ), y(j)\n\ns\n\n(cid:17)\n\n.\n\n(8)\n\nji\n\ninstead of w(t)\n\nNote that the aggregation is based on w(t) ij . Intuitively, this suggests φi should be updated based on how the model is being used by other clients rather than how client i itself uses it. If φi does not appear to be useful to all clients, i.e. w(t) ji = 0, ∀j, it does not get updated. Therefore, whenever client i is sampled by another client j using the sampling scheme S, it will send φi to j, and receives the gradient update gij := w(t) from client j. One issue here is that gij is governed by w(t) update to φi. We will show how this can be addressed by using an ε-greedy sampling scheme.\n\nji , which could be arbitrarily small, leading to no effective\n\nhφi(x(j)\n\ns ), y(j)\n\ns=1 ∇φil\n\n(cid:80)nj\n\n(cid:17)\n\n(cid:16)\n\nji\n\ns\n\nSampling scheme S We deploy an ε-greedy scheme where, in each round, each client uniformly samples clients with probability ε ∈ [0, 1] and samples the client(s) with the highest posterior(s) otherwise. This allows a trade off between emphasizing gradient updates from high-performing clients\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n(small ε), versus receiving updates from clients uniformly to find potential collaborators (large ε). The number M of sampled clients (neighbors) per round and ε can be tuned based on the specific problem instance. We will show the effect of varying the hyperparameters in the experiments.\n\nTracking the losses for the posterior The final practical consideration is the computation of the posterior w(t) ij . From the E-step (3) and the M-step (4), one can see that w(t) is the softmax transformation of the negative accumulative loss L(t) ij over rounds (see Appendix A for derivation). However, the accumulative loss can be sensitive to noise and initialization. If one of the models, say φj, performs slightly better than other models for client i at the beginning of training, then client i is likely to sample φj more frequently, thus enforcing the use of φj even when other better models exist. To address this, we instead keep track of the exponential moving average of the loss with a momentum parameter β ∈ [0, 1), (cid:98)L(t) ij , and compute w(t) using (cid:98)L(t) ij . This encourages clients to seek new collaborators rather than focusing on existing ones.\n\nij = (1 − β)(cid:98)L(t−1)\n\nij + βl(t)\n\n:= (cid:80)t−1\n\nτ =1 l(τ )\n\nij\n\nij\n\nij\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETTINGS\n\nWe conduct a range of experiments to evaluate the performance of our proposed FedeRiCo with multiple datasets. Additional experiment details and results can be found in Appendix D.\n\nDatasets We compare different methods on several real-world datasets. We evaluate on image-classification tasks with the CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), and OfficeHome2 (Venkateswara et al., 2017) datasets. Particularly, we consider a non-IID data partition among clients by first splitting data by labels into several groups with disjoint label sets. Each group is considered a distribution, and each client samples from one distribution to form its local data. For each client, we randomly divide the local data into 80% training data and 20% test data.\n\nBaseline methods We compare our FedeRiCo to several federated learning baselines. FedAvg (McMahan et al., 2017) trains a single global model for every client. We also compare to other personalized FL approaches including FedAvg with local tuning (FedAvg+) (Jiang et al., 2019), Clustered FL (Sattler et al., 2020), FedEM (Marfoq et al., 2021)3, FedFomo (Zhang et al., 2021), as well as a local training baseline. All accuracy results are reported in mean and standard deviation across different random data split and random training seeds. Unless specified otherwise, we use 3 neighbors with ε = 0.3 and momentum β = 0.6 as the default hyperparamters for FedeRiCo in all experiments. For FedEM, we use 4 components, which provides sufficient capacity to accommodate different numbers of label groups (or data distributions). For FedFomo, we hold out 20% of the training data for client weight calculations. For FedAvg+, we follow Marfoq et al. (2021) and update the local model with 1 epoch of local training.\n\nTraining settings For all models, we use the Adam optimizer with learning rate 0.01. CIFAR experiments use 150 rounds of training, while Office-Home experiments use 400 rounds. CIFAR10 results are reported across 5 different data splits and 3 different training seeds for each data split. CIFAR-100 and Office-Home results are reported across 3 different data splits with a different training seed for each split.\n\n4.2 PERFORMANCE COMPARISON\n\nThe performance of each FL method is shown in Table 1. Following the settings introduced by Marfoq et al. (2021), each client is evaluated on its own local testing data and the average accuracies weighted by local dataset sizes are reported. We observe that FedeRiCo has the best performance across all datasets and number of data distributions. Here, local training can be seen as an indicator to assess if other methods benefit from client collaboration as local training has no collaboration at all. We observe that our proposed FedeRiCo is the only method that consistently outperforms local training, meaning that FedeRiCo is the only method that consistently encourages effective\n\n2This dataset has been made publically available for research purposes only. 3We use implementations from https://github.com/omarfoq/FedEM for Clustered FL and Fe-\n\ndEM\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Accuracy (in percentage) with different number of data distributions. Best results in bold.\n\nCIFAR-10 # of distributions\n\nCIFAR-100 # of distributions\n\nOffice-Home # of distributions\n\nMethod\n\nFedAvg FedAvg+ Local Training Clustered FL FedEM FedFomo FedeRiCo\n\n2\n\n3\n\n4\n\n2\n\n3\n\n4\n\n2\n\n3\n\n4\n\n11.44 ± 3.28 12.45 ± 8.46 40.09 ± 2.84 11.50 ± 3.65 41.21 ± 10.83 42.24 ± 8.32 56.61 ± 2.51\n\n11.73 ± 3.68 29.86 ± 17.85 55.27 ± 3.11 15.24 ± 5.79 55.08 ± 6.71 59.45 ± 5.57 69.76 ± 2.25\n\n13.93 ± 5.74 45.65 ± 21.61 69.03 ± 7.05 16.43 ± 5.17 63.61 ± 9.93 71.05 ± 6.09 78.22 ± 4.80\n\n21.28 ± 5.04 29.95 ± 1.07 16.60 ± 0.64 20.93 ± 3.57 26.25 ± 2.40 12.15 ± 0.57 30.95 ± 1.62\n\n17.41 ± 3.27 35.33 ± 1.77 25.99 ± 2.38 23.15 ± 7.04 24.11 ± 7.36 20.49 ± 2.90 39.19 ± 1.64\n\n18.36 ± 3.68 36.17 ± 3.27 31.05 ± 1.68 15.15 ± 0.60 19.23 ± 2.58 24.53 ± 2.77 41.41 ± 1.07\n\n66.58 ± 1.88 80.21 ± 0.68 76.76 ± 0.23 66.58 ± 1.88 22.59 ± 1.95 78.61 ± 0.78 83.56 ± 0.49\n\n53.36 ± 4.21 81.88 ± 0.91 83.30 ± 0.32 53.36 ± 4.21 28.72 ± 1.83 82.57 ± 0.24 90.28 ± 0.75\n\n51.25 ± 4.37 84.50 ± 1.37 88.05 ± 0.44 51.25 ± 4.37 22.46 ± 3.99 87.86 ± 0.77 93.76 ± 0.12\n\nclient collaborations. Notably, both FedEM and FedFomo performs comparably well to FedeRiCo on CIFAR-10 but worse when the dataset becomes more complex like CIFAR-100. This indicates that building the right collaborations among clients becomes a harder problem for more complex datasets. Moreover, FedEM can become worse as the number of distributions increases, even worse than local training, showing that it is increasingly hard for clients to participate effectively under the FedEM framework for complex problems with more data distributions.\n\nIn addition, Clustered FL has similar performance to FedAvg, indicating that it is hard for Clustered FL to split into the right clusters. In Clustered FL (Sattler et al., 2020), every client starts in the same cluster and cluster split only happens when the FL objective is close to a stationary point, i.e. the norm of averaged gradient update from all clients inside the cluster is small. Therefore, in a noni.i.d setting like ours, the averaged gradient update might always be noisy and large, as clients with different distributions are pushing diverse updates to the clustered model. As a result, the cluster splitting rarely happens which makes clustered FL more like FedAvg.\n\n4.3 CLIENT COLLABORATION\n\nFigure 3: Client weights over time of FedeRiCo with CIFAR100 data and four different client distributions. Clients are color coded by their private data’s distribution.\n\nIn this section, we investigate client collaboration by plotting the personalized client weights w(t) ij of FedeRiCo over training. With different client data distributions, we show that FedeRiCo can assign more weight to clients from the same distribution. As shown in Fig. 3, we observe that clients with similar distributions collaborate to make the final predictions. For example, clients 3, 4 and 7 use a mixture of predictions from each other (in light blue) whereas client 0 only uses itself for prediction since it is the only client coming from distribution 0 (in dark blue) in this particular random split.\n\nOn the contrary, as shown in Fig. 4, even with 4 components, FedEM fails to use all of them for predictions for the 4 different data distributions. In fact, clients 2, 3, 4, 6 and 7 coming from two different distributions are using only the model of component 3 for prediction, whereas component 0 is never used by any client. Based on this, we find FedeRiCo better encourages the clients to\n\n7\n\n050100150Rounds0.00.20.40.60.81.0Client WeightsClient 0 (distribution 0)client 0client 1client 2client 3client 4client 5client 6client 7050100150Rounds0.00.20.40.60.81.0Client WeightsClient 1 (distribution 2)050100150Rounds0.00.10.20.30.40.50.60.70.8Client WeightsClient 2 (distribution 3)050100150Rounds0.00.10.20.30.40.50.6Client WeightsClient 3 (distribution 1)050100150Rounds0.00.10.20.30.40.50.6Client WeightsClient 4 (distribution 1)050100150Rounds0.00.20.40.60.81.0Client WeightsClient 5 (distribution 2)050100150Rounds0.00.20.40.60.8Client WeightsClient 6 (distribution 3)050100150Rounds0.00.10.20.30.40.50.60.7Client WeightsClient 7 (distribution 1)Under review as a conference paper at ICLR 2023\n\nFigure 4: Component weights over training for FedEM with 4 components, on CIFAR100 data with 4 different client distributions. Clients are color coded by their private data’s distribution.\n\ncollaborate with other similar clients and less with different clients. Each client can collaborate as much or as little as they need. Additionally, since all the non-similar clients have a weight of (almost) 0, each client only needs a few models from their collaborators for prediction.\n\n4.4 EFFECT OF USING EXPONENTIAL MOVING AVERAGE LOSS\n\n(a) Client weights with accumulative loss.\n\n(b) Client weights with exponential moving average.\n\nFigure 5: Effect on client weights with different implementations. The client weights on CIFAR-10 with 2 different client distributions are reported.\n\nHere, we visualize the effect of using the exponential moving average loss by plotting client weights with both accumulative loss and exponential moving average loss in Fig. 54. We observe that with the accumulative loss in Fig. 5a, the client weights quickly converge to one-hot, while with the exponential moving average loss in Fig. 5b, the client weights are more distributed to similar clients. This corresponds to our expectation stated in Section 3.3: the clients using exponential moving average loss are expected to seek for more collaboration compared to using accumulative loss.\n\n4.5 HYPERPARAMETER SENSITIVITY\n\nIn this section, we explore the effect of hyperparameters of our proposed FedeRiCo.\n\nEffect of ε-greedy sampling Here we show the effect of different ε values. Recall that each client deploys an ε-greedy selection strategy. The smaller the value of ε, the more greedy the client is in selecting the most relevant collaborators with high weights, leading to less exploration. Fig. 6a shows the accuracy along with training rounds with different ε values on the Office-Home dataset. One can see that there is a trade-off between exploration and exploitation. If ε is too high (e.g., ε = 1, uniform sampling), then estimates of the likelihoods/losses are more accurate. However, some gradient updates will vanish because the client weight is close to zero (see Section 3.3), resulting\n\n4We used uniform sampling for Fig. 5a (ε = 1) as most of the client weights are 0 after a few rounds.\n\n8\n\n050100150Rounds0.00.20.40.60.81.0Client WeightsClient 0 (distribution 0)component 0component 1component 2component 3050100150Rounds0.00.20.40.60.81.0Client WeightsClient 1 (distribution 2)050100150Rounds0.00.20.40.60.81.0Client WeightsClient 2 (distribution 3)050100150Rounds0.00.20.40.60.81.0Client WeightsClient 3 (distribution 1)050100150Rounds0.00.20.40.60.81.0Client WeightsClient 4 (distribution 1)050100150Rounds0.00.20.40.60.81.0Client WeightsClient 5 (distribution 2)050100150Rounds0.00.20.40.60.81.0Client WeightsClient 6 (distribution 3)050100150Rounds0.00.20.40.60.81.0Client WeightsClient 7 (distribution 1)050100150Rounds0.00.51.0Client Weightsclient 0client 0client 1client 2client 3client 4client 5client 6client 7050100150Rounds0.00.51.0Client Weightsclient 1050100150Rounds0.00.10.20.3Client Weightsclient 0050100150Rounds0.00.10.20.3Client Weightsclient 1Under review as a conference paper at ICLR 2023\n\n(a) ε for sampling.\n\n(b) # of sampled neighbors.\n\n(c) Momentum β.\n\nFigure 6: Test accuracy with different hyperparameters.\n\nin slow convergence. On the other hand, if ε is too small, the client may miss some important collaborators due to a lack of exploration. As a result, we use a moderate ε = 0.3 in all experiments.\n\nEffect of number of sampled neighbors We plot accuracy with number of neighbors M ∈ {0, 1, 3, 5, 7} on CIFAR100 with 4 different client distributions, where M = 0 is similar to Local Training as no collaboration happens. As shown in Fig. 6b, when the number of neighbors increases, FedeRiCo converges more slowly as each client is receiving more updates on other client’s models. While a smaller number of neighbors seems to have a lower final accuracy, we notice that even with M = 1, we still observe significant improvement compared to no collaboration. Therefore, we use M = 3 neighbors in our experiments as it has reasonable performance and communication cost.\n\nFigure 7: Client weights (client 0) with different momentum values β on the client weight update. Effect of client weight momentum We plot the overall test accuracy of client 0 on the Office-Home dataset with 4 different data distributions over β ∈ {0.1, 0.3, 0.6, 0.9} in Fig. 6c and similarly for the client weights in Fig. 7. With smaller β, as shown in Fig. 7, we observe a smoother update on the client weights, which is expected as the old tracking loss dominates the new one. Although various values produce similar final client weights, a bigger β can lead to more drastic changes in early training. However, one shouldn’t pick a very small β just because it can produce smoother weights. As shown in Fig. 6c, the algorithm may converge more slowly with smaller β. Therefore, we use β = 0.6 as it encourages smoother updates and also maintains good convergence speed.\n\n5 CONCLUSION AND DISCUSSION\n\nIn this paper, we proposed FedeRiCo, a novel framework for decentralized and personalized FL derived from EM for non-i.i.d client data. We evaluated FedeRiCo across different datasets and demonstrated that FedeRiCo outperforms multiple existing personalized FL baselines and encourages clients to collaborate with similar clients, i.e., the right collaborators.\n\nDecentralized FL provides an alternative architecture in the absence of a commonly trusted server. For example, CB-DEM ()forero2008consensus studies the distributed EM algorithm for classification in wireless networks with local bridge sensors to ensure all sensors reach consensus. Compared to our decentralized communication-efficient protocol, CB-DEM requires global information every round for the consensus synchronization. While decentralized FL removes the risk of single point failure compared to centralized FL by using peer-to-peer communication, it also raises concerns about security risks with the absence of a mutually trusted central server. Therefore, a promising direction is to incorporate trust mechanisms long with decentralization (Kairouz et al., 2019), such as blockchain frameworks (Qin et al., 2022). Additionally, FL schemes do not provide an explicit guarantee that private information will not be leaked. However, most FL frameworks, including ours, are compatible with privacy-preserving training techniques such as differential privacy, which is another promising and interesting research direction (Wei et al., 2020; Truex et al., 2020).\n\n9\n\n050100150200250300350400Rounds0.700.750.800.850.900.95Accuracyε=0.1ε=0.3ε=0.5ε=0.7ε=0.9ε=1.0020406080100120140Rounds0.00.10.20.30.4AccuracyM=0M=1M=3M=5M=7050100150200250300350400Rounds0.700.750.800.850.900.95Accuracyβ=0.1β=0.3β=0.6β=0.9050100150200250300350400Rounds0.00.10.20.30.4Client Weightsβ=0.1client 0client 1client 2client 3client 4client 5client 6client 7client 8client 9050100150200250300350400Rounds0.00.10.20.30.40.5Client Weightsβ=0.3050100150200250300350400Rounds0.00.10.20.30.40.50.60.70.8Client Weightsβ=0.6050100150200250300350400Rounds0.00.10.20.30.40.50.60.7Client Weightsβ=0.9Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDurmus Alp Emre Acar, Yue Zhao, Ruizhao Zhu, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Debiasing model updates for improving personalized federated training. In Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2021. URL https://proceedings.mlr.press/ v139/acar21a.html.\n\nMohammed Adnan, Shivam Kalra, Jesse C. Cresswell, Graham W. Taylor, and Hamid R. Tizhoosh. Federated learning and differential privacy for medical image analysis. Scientific reports, 12(1): 1–10, 2022.\n\nManoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Fed-\n\nerated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019.\n\nFengwen Chen, Guodong Long, Zonghan Wu, Tianyi Zhou, and Jing Jiang. Personalized federated\n\nlearning with graph. arXiv preprint arXiv:2203.00829, 2022.\n\nLuca Corinzia, Ami Beuret, and Joachim M Buhmann. Variational federated multi-task learning.\n\narXiv preprint arXiv:1906.06268, 2019.\n\nSen Cui, Jian Liang, Weishen Pan, Kun Chen, Changshui Zhang, and Fei Wang. Collaboration equilibrium in federated learning, 2021. URL https://arxiv.org/abs/2108.07926.\n\nArthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1): 1–22, 1977.\n\nYuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated\n\nlearning, 2020. URL https://arxiv.org/abs/2003.13461.\n\nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.\n\nPersonalized federated learning with theoretical guarantees: In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 3557–3568. Curran Associates, URL https://proceedings.neurips.cc/paper/2020/file/ Inc., 24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf.\n\nA model-agnostic meta-learning approach.\n\n2020.\n\nAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered federated learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19586–19597. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/e32cc80bf07915058ce90722ee17bb71-Paper.pdf.\n\nFilip Hanzely and Peter Richt ́arik. Federated learning of a mixture of global and local models, 2020.\n\nURL https://arxiv.org/abs/2002.05516.\n\nYutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, and Yong Zhang. Personalized cross-silo federated learning on non-iid data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35-9, pp. 7865–7873, 2021.\n\nYihan Jiang, Jakub Koneˇcn`y, Keith Rush, and Sreeram Kannan. Improving federated learning per-\n\nsonalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019.\n\nPeter Kairouz, H. Brendan McMahan, Brendan Avent, Aur ́elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri`a Gasc ́on, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Koneˇcn ́y, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancr`ede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer ̈Ozg ̈ur, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram`er, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nShivam Kalra, Junfeng Wen, Jesse C. Cresswell, Maksims Volkovs, and Hamid R. Tizhoosh. arXiv preprint\n\nProxyfl: Decentralized federated learning through proxy model sharing. arXiv:2111.11343, 2021.\n\nMikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based metalearning methods. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ f4aa0dd960521e045ae2f20621fb4ee9-Paper.pdf.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nViraj Kulkarni, Milind Kulkarni, and Aniruddha Pant. Survey of personalization techniques for federated learning. In 2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4), pp. 794–797. IEEE, 2020.\n\nChengxi Li, Gang Li, and Pramod K Varshney. Decentralized federated learning via mutual knowl-\n\nedge transfer. IEEE Internet of Things Journal, 2021a.\n\nTian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning, pp. 6357– 6368. PMLR, 2021b.\n\nGuodong Long, Yue Tan, Jing Jiang, and Chengqi Zhang. Federated learning for open banking. In\n\nFederated learning, pp. 240–254. Springer, 2020.\n\nYishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.\n\nOthmane Marfoq, Giovanni Neglia, Aur ́elien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. Advances in Neural Information Processing Systems, 34, 2021.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 1273–1282. PMLR, 2017.\n\nZhen Qin, Shuiguang Deng, Xueqiang Yan, Schahram Dustdar, and Albert Y Zomaya. Secure and efficient decentralized federated learning with data representation protection. arXiv preprint arXiv:2205.10568, 2022.\n\nAdam Sadilek, Luyang Liu, Dung Nguyen, Methun Kamruzzaman, Stylianos Serghiou, Benjamin Rader, Alex Ingerman, Stefan Mellem, Peter Kairouz, Elaine O. Nsoesie, Jamie MacFarlane, Anil Vullikanti, Madhav Marathe, Paul Eastham, John S. Brownstein, Blaise Aguera y. Arcas, Michael D. Howell, and John Hernandez. Privacy-first health research with federated learning. npj Digital Medicine, 4(1), September 2021. doi: 10.1038/s41746-021-00489-2. URL https: //doi.org/10.1038/s41746-021-00489-2.\n\nFelix Sattler, Klaus-Robert M ̈uller, and Wojciech Samek. Clustered federated learning: Modelagnostic distributed multitask optimization under privacy constraints. IEEE transactions on neural networks and learning systems, 32(8):3710–3722, 2020.\n\nTao Shen, Jie Zhang, Xinkang Jia, Fengda Zhang, Gang Huang, Pan Zhou, Kun Kuang, Fei Wu, and\n\nChao Wu. Federated mutual learning. arXiv preprint arXiv:2006.16765, 2020.\n\nVirginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task\n\nlearning. Advances in neural information processing systems, 30, 2017.\n\nCanh T Dinh, Nguyen Tran, and Josh Nguyen. Personalized federated learning with moreau en-\n\nvelopes. Advances in Neural Information Processing Systems, 33:21394–21405, 2020.\n\nAlysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning.\n\nIEEE Transactions on Neural Networks and Learning Systems, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nStacey Truex, Ling Liu, Ka-Ho Chow, Mehmet Emre Gursoy, and Wenqi Wei. Ldp-fed: Federated learning with local differential privacy. In Proceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking, pp. 61–66, 2020.\n\nHemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5018–5027, 2017.\n\nKang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS Quek, and H Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. IEEE Transactions on Information Forensics and Security, 15:3454–3469, 2020.\n\nMichael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M. Alvarez. Personalized fedIn International Conference on Learning\n\nerated learning with first order model optimization. Representations, 2021. URL https://openreview.net/forum?id=ehJqJQk9cw.\n\nYue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated\n\nLearning with Non-IID Data, 2018. URL https://arxiv.org/abs/1806.00582.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe paper proposes FedeRiCo, a decentralized federated learning framework that allows clients to collaborate much or little with other participating clients to enhance the performance of the federated learned models. FedeRiCo leverages an EM-style optimization procedure to solve the global model weights and clients' collaboration weights concurrently. Both theoretical analysis and empirical results are shown to justify the effectiveness of the proposed FedeRiCo method.\n\n# Strength And Weaknesses\n\nStrength:\n- the paper is well-written and the targetted problem is well motivated.  \n- both theoretical and empirical results are shown to demonstrate the effectiveness of the proposed method.  \n\nWeaknesses:\n- the proposed EM algorithm seems to be somewhat known, which limits the novelty of the proposed FedeRiCo method.  \n- the privacy guarantees for the decentralized federated learning framework are not clear. It seems the individual client's gradient can be seen by the other clients (in the small gradient steps in lieu of full optimization in each round). The authors are expected to discuss the privacy guarantees more explicitly.\n- the scale of the experiment is small, how does FedeRiCo perform on the ImageNet scale datasets?\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- Clarity: the presentation of the paper is clear, and the proposed idea is easy to follow.   \n- Quality: the writing quality of the paper is good. Both theoretical and experimental results are shown to justify the effectiveness of the proposed method.\n- Novelty: the proposed method seems to be somewhat known, and thus the novelty is limited.\n- Reproducibility: enough details are shown for the implementation and experimental setup. I thus believe the experiments can be reproduced.\n\n# Summary Of The Review\n\nThe paper seeks to tackle an interesting and important problem in federated learning to enhance collaboration among participating clients. The proposed method makes sense but is somewhat known. I am thus concerned about the novelty of the paper. Moreover, the privacy guarantees of the proposed method are not clear, which is a clear weakness.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nASYNCHRONOUS GRADIENT PLAY IN ZERO-SUM MULTI-AGENT GAMES\n\nRuicheng Ao Peking University archer arc@pku.edu.cn\n\nShicong Cen & Yuejie Chi Carnegie Mellon University shicongc,yuejiec\n\n{\n\n@andrew.cmu.edu\n\n}\n\nABSTRACT\n\nFinding equilibria via gradient play in competitive multi-agent games has been attracting a growing amount of attention in recent years, with emphasis on designing efficient strategies where the agents operate in a decentralized and symmetric manner with guaranteed convergence. While significant efforts have been made in understanding zero-sum two-player matrix games, the performance in zerosum multi-agent games remains inadequately explored, especially in the presence of delayed feedbacks, leaving the scalability and resiliency of gradient play open to questions. In this paper, we make progress by studying asynchronous gradient plays in zero-sum polymatrix games under delayed feedbacks. We first establish that the last iterate of entropy-regularized optimistic multiplicative weight updates (OMWU) method converges linearly to the quantal response equilibrium (QRE), the solution concept under bounded rationality, in the absence of delays. While the linear convergence continues to hold even when the feedbacks are randomly delayed under mild statistical assumptions, it converges at a noticeably slower rate due to a smaller tolerable range of learning rates. Moving beyond, we demonstrate entropy-regularized OMWU—by adopting two-timescale learning rates in a delay-aware manner—enjoys faster last-iterate convergence under fixed delays, and continues to converge provably even when the delays are arbitrarily bounded in an average-iterate manner. Our methods also lead to finite-time guarantees to approximate the Nash equilibrium (NE) by moderating the amount of regularization. To the best of our knowledge, this work is the first that aims to understand asynchronous gradient play in zero-sum polymatrix games under a wide range of delay assumptions, highlighting the role of learning rates separation.\n\n1\n\nINTRODUCTION\n\nFinding equilibria of multi-player games via gradient play lies at the heart of game theory, which permeates a remarkable breadth of modern applications, including but not limited to competitive reinforcement learning (RL) (Littman, 1994), generative adversarial networks (GANs) (Goodfellow et al., 2014) and adversarial training (Mertikopoulos et al., 2018). While conventional wisdom leans towards the paradigm of centralized learning (Bertsekas & Tsitsiklis, 1989), retrieving and sharing information across multiple agents raise questions in terms of both privacy and efficiency, leading to a significant amount of interest in designing decentralized learning algorithms that utilize only local payoff feedbacks, with the updates at different agents executed in a symmetric manner.\n\nIn reality, there is no shortage of scenarios where the feedback can be obtained only in a delayed manner (He et al., 2014), i.e., the agents only receive the payoff information sent from a previous round instead of the current round, due to communication slowdowns and congestions, for example. Substantial progress has been made towards reliable and efficient online learning with delayed feedbacks in various settings, e.g., stochastic multi-armed bandit (Pike-Burke et al., 2018; Vernade et al., 2017), adversarial multi-armed bandit (Cesa-Bianchi et al., 2016; Li et al., 2019), online convex optimization (Quanrud & Khashabi, 2015; McMahan & Streeter, 2014) and multi-player game (Meng et al., 2022; H ́eliou et al., 2020; Zhou et al., 2017). Typical approaches to combatting delays include subsampling the payoff history (Weinberger & Ordentlich, 2002; Joulani et al., 2013), or adopting\n\nAuthor are sorted alphabetically.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nLearning rate\n\nType of delay\n\nsingle-timescale\n\ntwo-timescale\n\nnone\n\nstatistical\n\nconstant\n\nbounded\n\nIteration complexity\n\nε-QRE\n\nε-NE\n\nτ −1dmax\n\nA\n\nA\n\nτ −2d2\n\nmax ∥\n\nτ −1dmax τ −2nd3\n\n∥ max ∥\n\n∥\n\nA\n\n∥∞ log ε−1 ∥\n2 ∞ (γ + 1)2 log ε−1 ∥∞ (γ + 1)2 log ε−1 ∞ (γ + 1)5/2ε−1\n\nA\n\n3\n\n∥\n\ndmax\n\n∥∞ ε−1 A\n∥ 2\n∞ (γ + 1)2ε−2 ∥\n∥∞ (γ + 1)2ε−1 3\n∞ (γ + 1)5/2ε−3 ∥\n\nd2\n\nmax ∥\n\ndmax\n\nA\n\nA\n\n∥ A\nmax ∥\n\nnd3\n\nTable 1: Iteration complexities of the proposed OMWU method for finding ε-QRE/NE of zero-sum polymatrix games, where logarithmic dependencies are omitted. Here, γ denotes the maximal time delay when the delay is bounded, n denotes the number of agents in the game, dmax is the maximal ∞ is the l∞ norm of the entire payoff matrix A degree of the graph, and ∥\n(over all games in the network). We only present the result under statistical delay when the delays are bounded for ease of comparison, while more general bounds are given in Section 3.2.\n\n∥∞ = maxi,j\n\nAi,j\n\nA\n\n∥\n\n∥\n\nadaptive learning rates suggested by delay-aware analysis (Quanrud & Khashabi, 2015; McMahan & Streeter, 2014; Hsieh et al., 2020; Flaspohler et al., 2021). Most of these efforts, however, have been limited to either the asymptotic convergence to the equilibrium (Zhou et al., 2017; H ́eliou et al., 2020) or the study of individual regret, which characterizes the performance gap between an agent’s learning trajectory and the best policy in hindsight. It remains highly inadequate when it comes to guaranteeing finite-time convergence to the equilibrium in a multi-player environment, especially in the presence of delayed feedbacks, thus leaving the scalability and resiliency of gradient play open to questions.\n\nIn this work, we initiate the study of asynchronous learning algorithms for an important class of games called zero-sum polymatrix games (also known as network matrix games (Bergman & Fokin, 1998)), which generalizes two-player zero-sum matrix games to the multiple-player setting and serves as an important stepping stone to more general multi-player general-sum games. Zero-sum polymatrix games are commonly used to describe situations in which agents’ interactions are captured by an interaction graph and the entire system of games are closed so that the total payoffs keep invariant in the system. They find applications in an increasing number of important domains such as security games (Cai et al., 2016), graph transduction (Bernardi, 2021), and more.\n\nIn particular, we focus on finite-time last-iterate convergence to two prevalent solution concepts in game theory, namely Nash Equilibrium (NE) and Quantal Response Equilibrium (QRE) which considers bounded rationality (McKelvey & Palfrey, 1995). Despite the seemingly simple formulation, few existing works have achieved this goal even in the synchronous setting, i.e., with instantaneous feedback. Leonardos et al. (2021) studied a continuous-time learning dynamics that converges to the QRE at a linear rate. Anagnostides et al. (2022) demonstrated Optimistic Mirror Descent (OMD) (Rakhlin & Sridharan, 2013) enjoys finite-time last-iterate convergence to the NE, yet the analysis therein requires continuous gradient of the regularizer, which incurs computation overhead for solving a subproblem every iteration. In contrast, an appealing alternative is the entropy regularizer, which leads to closed-form multiplicative updates and is computationally more desirable, but remains poorly understood. In sum, designing efficient learning algorithms that provably converge to the game equilibria has been technically challenging, even in the synchronous setting.\n\n1.1 OUR CONTRIBUTIONS\n\nIn this paper, we develop provably convergent algorithms—broadly dubbed as asynchronous gradient play—to find the QRE and NE of zero-sum polymatrix games in a decentralized and symmetric manner with delayed feedbacks. We propose an entropy-regularized Optimistic Multiplicative Weights Update (OMWU) method (Cen et al., 2021), where each player symmetrically updates their strategies without access to the payoff matrices and other players’ strategies, and initiate a systematic investigation on the impacts of delays on its convergence under two schemes of learning rates schedule. Our main contributions are summarized as follows.\n\n• Finite-time last-iterate convergence of single-timescale OMWU. We begin by showing that, in the synchronous setting, the single-timescale OMWU method—when the same learning rate is adopted for extrapolation and update—achieves last-iterate convergence to the QRE at a linear\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n(\n\n(ε−2).\n\n(ε−1) iterations by adjusting the regularization parameter, where (cid:101) O\n\nrate, which is independent of the number of agents as well as the size of action spaces (up to logarithmic factors). In addition, this implies a last-iterate convergence to an ε-approximate NE in ) hides logarithmic depen- (cid:101) ·\nO dencies. While the last-iterate linear convergence to QRE continues to hold in the asynchronous setting, as long as the delay sequence follows certain mild statistical assumptions, it converges at a slower rate due to a smaller tolerable range of learning rates, with the iteration complexity to find an ε-NE degenerating to (cid:101) O\n\n• Finite-time convergence of two-timescale OMWU. To accelerate the convergence rate in the presence of delayed feedback, we propose a two-timescale OMWU method which separates the learning rates of extrapolation and update in a delay-aware manner for applications with constant and known delays (e.g. from timestamp information). The learning rate separation is critical in bypassing the convergence slowdown encountered in the single-timescale case, where we show that two-timescale OMWU achieves a faster last-iterate linear convergence to QRE in the presence (ε−1) iteration complexity to ε-NE that matches the rate of constant delays, with an improved (cid:101) O\nwithout delay. We further tackle the more practical yet challenging setting where the feedback sequence is permutated by bounded delays—possibly in an adversarial manner—and demonstrate provable convergence to the equilibria in an average-iterate manner.\n\nWe summarize the iteration complexities of the proposed methods for finding ε-approximate solutions of QRE and NE in Table 1. To the best of our knowledge, this work presents the first algorithm design and analysis that focus on equilibrium finding in a multi-player game with delayed feedbacks. In contrast, most of existing works concerning individual regret in the synchronous/asynchronous settings typically yield average-iterate convergence guarantees (see e.g., Bailey (2021); Meng et al. (2022)) and fall short of characterizing the actual learning trajectory to the equilibrium.\n\n1.2 NOTATION AND PAPER ORGANIZATION\n\n∥\n\n{\n\n1,\n\n, n\n\n· · ·\n\n∆(S),\n\np′(cid:1) := (cid:80)\n\nand by ∆(S) the probability simplex over the set S. Given Denote by [n] the set }\ntwo probability distributions p, p′ to p is defined by KL(cid:0)p k∈S p(k) log p(k) Rn, we use exp(z) to represent [exp(zi)]1≤i≤n. The rest of this paper is organized as follows. Section 2 provides the preliminary on zero-sum polymatrix games and solution concepts. Performance guarantees of single-timescale OMWU and two-timescale OMWU are presented in Section 3 and Section 4, respectively. Numerical experiments are provided in Section 5 to corroborate the theoretical findings, and finally, we conclude in Section 6. The proofs are deferred to the appendix.\n\np′(k) . For any vector z = [zi]1≤i≤n\n\nthe KL divergence from p′\n\n∈\n\n∈\n\n2 PRELIMINARIES\n\nIn this section, we introduce the formulation of zero-sum polymatrix games as well as the solution concept of NE and QRE. We start by defining the polymatrix game. Definition 1 (Polymatrix game). Let Aij matrix game, where each element in the tuple is defined as follows.\n\nbe an n-player poly-\n\n}(i,j)∈E}\n\n(V, E),\n\ni∈V ,\n\n:=\n\nSi\n\nG\n\n{\n\n}\n\n{\n\n{\n\n• An undirected graph (V, E), with V = [n] denoting the set of players and E the set of edges;\n\n• For each player i\n\n∈ • For each edge (i, j)\n\nV , Si represents its action set, which is assumed to be finite;\n\n∈ associated with player i and j, i.e., when player i and player j choose si received payoffs are given by Aij(si, sj), Aji(sj, si), respectively.\n\n∈\n\n∈\n\n∈\n\nR|Sj |×|Si| represent the payoff matrices Sj, the\n\nSi and sj\n\n∈\n\nE, Aij\n\nR|Si|×|Sj | and Aji\n\nUtility function. Given the strategy profile s = (s1, R of player i is given by the utility function ui : S\n\n· · ·\n\n→\n\n, sn)\n\n∈\n\nS = (cid:81)\n\ni∈V Si taken by all players,\n\nui(s) =\n\n(cid:88)\n\nj:(i,j)∈E\n\nAij(si, sj).\n\nSuppose that player i adopts a mixed/stochastic strategy or policy, πi of selecting si\n\n∆(Si), where the probability Si is specified by πi(si). With slight abuse of notation, we denote the expected\n\n∈\n\n∈\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nutility of player i with a mixed strategy profile π = (π1,\n\nui(π) =\n\nE si∼πi,∀i∈V\n\n[ui(s)] =\n\nj:(i,j)∈E\n\n· · ·\n\n(cid:88)\n\n, πn)\n\n∆(S) as\n\n∈ π⊤ i Aijπj.\n\n(1)\n\nIt turns out to be convenient to treat πi and π as vectors in R|Si| and R(cid:80) and concatenate all payoff matrices associated with player i into\n\ni∈V |Si| without ambiguity,\n\nwhere Aij is set to 0 whenever (i, j) / ∈\nthese notation in place, we can rewrite the expected utility function (1) as\n\n· · · E. In particular, it follows that Aii = 0 for all i\n\n∈\n\nAi = (Ai1,\n\n, Ain)\n\nR|Si|×(cid:80)\n\nj∈V |Sj |,\n\nui(π) = π⊤\n\ni Aiπ,\n\n(2)\n\nV . With\n\n∈\n\n(3)\n\nwhere Aiπ dition, we denote the maximum entrywise absolute value of payoff by maxi degree of player i. Moreover, we denote Smax = maxi over all players.\n\nR|Si| can be interpreted as the expected utility of the actions in Si for player i. In ad- ∈\n∞ = ∞, and the maximum degree of the graph by dmax = maxi∈V degi, where degi is the as the maximum size of the action space\n\n∥∞ = maxi,j\n\nAij\n\nAi\n\nSi\n\nA\n\n∥\n\n∥\n\n∥\n\n∥\n\n∥\n\n|\n\n|\n\nZero-sum polymatrix games. The game (cid:80) s\nfollows that (cid:80) i∈V ui(π) = 0.\n\ni∈V ui(s) = 0,\n\n∈\n\n∀\n\nS. This immediately implies that for any strategy profile π\n\nis a zero-sum polymatrix game if it holds that ∆(S), it\n\nG\n\n∈\n\nNash equilibrium (NE). A mixed strategy profile π⋆ = (π⋆ n) is a Nash equilibrium (NE) 1, when each player i cannot further increase its own utility function ui by unilateral deviation, i.e., ui(π′ ∆(Si), where the existence is guaranteed by the work (Cai et al., 2016). Here we denote the mixed strategies of all players other than i by π−i and write ui(πi, π−i) = ui(π). To measure how close a strategy π\n\n∆(S) is to an NE, we introduce\n\n−i), for all i\n\nui(π⋆\n\nV, π′\n\ni , π⋆\n\ni, π⋆\n\ni ∈\n\n, π⋆\n\n−i)\n\n· · ·\n\n≤\n\n∈\n\nNE-Gap(π) = max i∈V\n\n(cid:20)\n\nmax i∈∆(Si)\n\nπ′\n\nui(π′\n\ni, π−i)\n\n−\n\n(cid:21)\n\nui(π)\n\n,\n\n∈\n\nwhich measures the largest possible gain in the expected utility when players deviate from its strategy unilaterally. A mixed strategy profile π is called an ε-approximate Nash equilibrium (ε-NE) when NE-Gap(π) ∆(Si). i, π−i)\n\nε, which ensures that ui(π′\n\nui(πi, π−i)+ε, for all i\n\nV, π′\n\n≤\n\n∈\n\ni ∈\n\nQuantal response equilibrium (QRE). The quantal response equilibrium (QRE), proposed by McKelvey & Palfrey (1995), generalizes the classical notion of NE under uncertain payoffs or bounded rationality, while balancing exploration and exploitation. A mixed strategy profile π⋆ τ = (π⋆ n,τ ) is a QRE when each player assigns its probability of action according to the expected utility of every action in a Boltzmann fashion, i.e., for all i\n\n1,τ ,\n\n, π⋆\n\n· · ·\n\nV ,\n\n≤\n\nπ⋆\n\ni,τ (k) =\n\n(cid:80)\n\nexp([Aiπ⋆\n\nτ ]k/τ )\n\nexp([Aiπ⋆\n\nτ ]k/τ )\n\nk∈Si\n\n,\n\nSi,\n\nk\n\n∈\n\n(4)\n\n∈\n\nwhere τ > 0 is the regularization parameter or temperature. Equivalently, this amounts to maximizing an entropy-regularized utility of each player (Mertikopoulos & Sandholm, 2016), i.e., ui,τ (π′ V , π′ i, π⋆ ∆(Si). Here, the entropy-regularized utility −i,τ ) function ui : S\n\nui,τ (π⋆ −i,τ ) for all i R of player i is given by\n\ni,τ , π⋆\n\ni ∈\n\n∈\n\nui,τ (π) = ui(π) + τ\n\n(πi),\n\n(5)\n\nH\n\n(πi) =\n\nlog πi denotes the Shannon entropy of πi. In Leonardos et al. (2021), it is where shown that a unique QRE exists in a zero-sum polymatrix game. Similarly, we can measure the proximity of a strategy π to a QRE by\n\nH\n\n−\n\nπ⊤ i\n\nQRE-Gapτ (π) = max\n\ni∈V\n\n(cid:20)\n\nmax i∈∆(Si)\n\nπ′\n\nui,τ (π′\n\ni, π−i)\n\n−\n\n(cid:21)\n\nui,τ (π)\n\n.\n\n(6)\n\nA mixed strategy profile π is called an ε-QRE when QRE-Gapτ (π) ε. According to the straight- ≤\nQRE-Gapτ (π) + τ log Smax, it follows immediately that we forward relationship NE-Gap(π) can link an ε/2-QRE to ε-NE by setting τ = . This facilitates the translation of convergence to the QRE to one regarding the NE by appropriately setting the regularization parameter τ .\n\nε 2 log Smax\n\n≤\n\n4\n\n≤ →\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Entropy-regularized OMWU, agent i 1: Initialize π(0) i = π(0) 2: for t = 0, 1, 2, . . . do\n\ni\n\nas uniform distribution. Learning rates η, and η (optional).\n\n3: 4: When t\n\nReceive payoff vector Aiπ(κ(t)\n\ni ).\n\n1, update πi according to\n\n≥\n\nπ(t)\n\ni (k)\n\n∝\n\nπ(t−1)\n\ni\n\n(k)1−ητ exp(η[Aiπ(κ(t)\n\ni )]k),\n\nk\n\n∀\n\n∈\n\nSi.\n\n5:\n\nUpdate πi according to the single-timescale rule\n\nπ(t+1)\n\ni\n\n(k)\n\nor the two-timescale rule\n\nπ(t+1)\n\ni\n\n(k)\n\n∝\n\n∝\n\n6: end for\n\ni (k)1−ητ exp(η[Aiπ(κ(t) π(t)\n\ni )]k),\n\ni (k)1−ητ exp(η[Aiπ(κ(t) π(t)\n\ni )]k),\n\nk\n\n∀\n\n∈\n\nSi.\n\nk\n\n∀\n\n∈\n\nSi.\n\n(9)\n\n(10)\n\n3 PERFORMANCE GUARANTEES OF SINGLE-TIMESCALE OMWU\n\nIn this section, we present and study the entropy-regularized OMWU method (Cen et al., 2021) for finding the QRE of zero-sum polymatrix games. Whilst the method is originally proposed for finding QRE in a two-player zero-sum game, the update rule naturally generalizes to the multi-player setting as\n\nπ(t+1)\n\ni (k)1−ητ exp(η[Aiπ(t+1)]k), where η > 0 is the learning rate and π(t+1) serves as a prediction for π(t+1) via an extrapolation step\n\nSi,\n\n(k)\n\n(7)\n\n∝\n\n∈\n\n∀\n\nk\n\ni\n\nπ(t)\n\nπ(t+1)\n\ni\n\n(k)\n\nπ(t)\n\ni (k)1−ητ exp(η[Aiπ(t)]k),\n\n∝\n\nk\n\n∀\n\n∈\n\nSi.\n\n(8)\n\nIn the asynchronous setting, however, each agent i receives a delayed payoff vector Aiπ(κ(t) i ) instead of Aiπ(t) in the t-th iteration, where κ(t) 0 representing the length t\n{ of delay. The detailed procedure is outlined in Algorithm 1 using the single-timescale rule (9) for extrapolation.\n\n, with γ(t)\n\ni = max\n\ni ≥\n\nγ(t)\n\n, 0\n\n−\n\n}\n\ni\n\n3.1 PERFORMANCE GUARANTEES WITHOUT DELAYS\n\nWe first present our theorem concerning the last-iterate convergence of single-timescale OMWU for finding the QRE in the synchronous setting, i.e. γ(t) V , i∈V KL(cid:0)πi let KL(cid:0)π Theorem 1 (Last-iterate convergence without delays). Suppose that the learning rate η of single-\n\ni = 0 for all i\n\n0. For any π, π′\n\nπ′(cid:1) = (cid:80)\n\nV and t\n\nπ′ i\n\n(cid:1).\n\n≥\n\n∈\n\n∈\n\n∥\n\n∥\n\n(cid:110) 1\n\n2τ ,\n\n(cid:111)\n\n1 4dmax∥A∥∞\n\n≤\n\n, then for any T\n\n0, the\n\n≥\n\ntimescale OMWU in Algorithm 1 obeys 0 < η\n\nmin\n\niterates π(T ) and π(T s) converge at a linear rate according to π(T +1)(cid:1) ητ )T KL(cid:0)π⋆ KL(cid:0)π⋆\n\nπ(0)(cid:1), KL(cid:0)π⋆\n\nπ(T )(cid:1)\n\n(1\n\nτ ∥\n\n≤\n\n−\n\nτ ∥\n\nτ ∥\n\nFurthermore, the QRE-gap also converges linearly according to\n\n2(1\n\n−\n\nητ )T KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1). (11a)\n\n≤\n\nQRE-Gapτ (π(T ))\n\n≤\n\n(cid:0)η−1 + 2τ −1d2\n\nA\n\n(cid:1)(1\n\n2 ∞\n\n∥\n\n−\n\nητ )T −1KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1).\n\n(11b)\n\nmax∥\n\nTheorem 1 demonstrates that as long as the learning rate η is sufficiently small, the last iterate of single-timescale OMWU converges to the QRE at a linear rate. Compared with prior works for finding approximate equilibrium for zero-sum polymatrix games, our approach features a closedform multiplicative update and a fast linear last-iterate convergence. Some remarks are in order.\n\n• Linear convergence to the QRE. Theorem 1 implies an iteration complexity of (cid:101) O\n\nfor finding an ε-QRE in a last-iterate manner, which leads to an iteration complexity of\n\nητ log 1\n\nε\n\n(cid:16) 1\n\n(cid:17)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n(cid:16)(cid:16) dmax∥A∥∞\n\n(cid:17)\n\n(cid:17)\n\nlog 1 ε\n\nτ\n\n+ 1\n\nby optimizing the learning rate in Theorem 1.The result is especially\n\n(cid:101) O\nappealing as it avoids direct dependency on the number of agents n as well as the size of action spaces (up to logarithmic factors), suggesting that learning in competitive multi-agent games can be made quite scalable as long as the interactions among the agents are sparse (so that the maximum degree of the graph dmax is much smaller than the number of agents n).\n\n• Last-iterate convergence to ε-NE. By setting τ appropriately, we end up with an iteration com-\n\nplexity of (cid:101) O\n\n(cid:16) dmax∥A∥∞\n\n(cid:17)\n\nε\n\nthe best existing last-iterate rate of (cid:101) O\nfactor of n/(dmaxε).\n\nfor achieving last-iterate convergence to an ε-NE, which outperforms\n\n(cid:0)n\n\nA\n\n∥\n\n∥∞/ε2(cid:1) from Leonardos et al. (2021) by at least a\n\nRemark 1. Our results trivially extend to the setting of weighted zero-sum polymatrix games i∈V at each player. (Leonardos et al., 2021), which amounts to adopting different learning rates . In addition, our convergence\n\nIn this case, the iteration complexity becomes (cid:101) O\nresult readily translates to a bound on individual regret as detailed in Appendix C.\n\nηiτ log 1\n\nmaxi∈V\n\nηi\n\n(cid:17)\n\n(cid:16)\n\n}\n\n{\n\n1\n\nε\n\n3.2 PERFORMANCE GUARANTEES UNDER RANDOM DELAYS\n\nWe continue to examine single-timescale OMWU in the more challenging asynchronous setting. In particularly, we show that the last iterate of single-timescale OMWU continues to converge linearly to the QRE at a slower rate, as long as the delays satisfy some mild statistical assumptions given below.\n\nAssumption 1 (Random delays). Assume that for all i generated and satisfies\n\n∈\n\nV , t\n\n≥\n\n0, the delay γ(t)\n\ni\n\nis independently\n\nE\n\nγ(t)\n\ni ≥l\n\n(cid:105)\n\n(cid:104) γ(t)\n\ni\n\n:= E\n\n(cid:104) γ(t)\n\ni\n\n(cid:12) (cid:12) γ(t)\n\ni ≥\n\n(cid:105)\n\nl\n\n≤\n\nE(l),\n\nl = 0, 1, . . . .\n\n∀\n\n(12)\n\nAdditionally, there exists some constant ζ > 1, such that L ≜ (cid:80)∞\n\nl=0 ζ lE(l) <\n\n. ∞\n\nWe remark that Assumption 1 is a rather mild condition that applies to typical delay distributions, such as the Poisson distribution (Zhang et al., 2020), as well as distributions with bounded support (Recht et al., 2011; Liu et al., 2014; Assran et al., 2020). Roughly speaking, Assumption 1 implies that the probability of the delay decays exponentially with its length, where ζ −1 approximately indicates the decay rate. We have the following theorem.\n\nTheorem 2 (Last-iterate convergence with random delays). Under Assumption 1, suppose that the regulari-zation parameter τ < min and the learning rate η of single-timescale OMWU in Algorithm 1 obeys\n\n1, dmax\n\n∥∞}\n\nA\n\n∥\n\n{\n\n0 < η\n\nmin\n\n≤\n\n(cid:40)\n\nτ\n\n24d2\n\nA\n\n∞ (L + 1)\n\n2 ∥\n\nmax ∥\n\n(cid:41)\n\n1\n\n,\n\nζ\n\n,\n\n− τ ζ\n\n1, the iterates π(T ) and π(T ) converges to π⋆\n\nτ at the rate\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(T )(cid:1)(cid:105)\n\n,\n\n(cid:104)\n\nE\n\n1 2\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(T )(cid:1)(cid:105)(cid:27)\n\n(1\n\n−\n\n≤\n\nητ )T KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1).\n\nthen for any T\n\n≥ (cid:104)\n\n(cid:26)\n\nmax\n\nE\n\nFurthermore, the QRE-gap also converges linearly according to\n\nE\n\n(cid:104)\n\n(cid:105) QRE-Gapτ (π(T ))\n\n4η−1(1\n\n−\n\n≤\n\nητ )T KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1).\n\n(13)\n\n(14a)\n\n(14b)\n\ncomplexity\n\nto\n\nε-QRE is\n\nno more\n\nthan\n\nTheorem 2 d2\n\nmax\n\n(cid:110)\n\n(cid:16)\n\nsuggests 2\n∥\n\nthat ∞ (L + 1),\n\nA\n\nthe ζ\nζ−1\n\n(cid:111) 1\n\niteration (cid:17) τ 2 log 1\n\nε\n\nmax ∥\n\n(cid:101) O\nmore limited compared with the requirement in Theorem 1without delays. In particular, the range of the learning rate is proportional to the regularization parameter τ , an issue we shall try to address by resorting to two-timescale learning rates in OMWU. To facilitate further understanding, we showcase the iteration complexity for finding ε-QRE/NE under two typical scenarios: bounded delay and Poisson delay.\n\nafter optimizing the learning rate, whose range is\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n• Bounded random delay. When the delays are bounded above by some maximum delay γ, Assumption 1 is met with ζ = 1 + γ−1 and L = eγ(γ + 1). Plugging into Theorem 2 yields an (cid:17) (cid:17)\n\nlog 1 iteration complexity of (cid:101) ε\nO for finding an ε-NE, which increases quadratically as the maximum delay increases. Note that these rates are worse than those without delays (cf. Theorem 1).\n\nfor finding an ε-QRE, or (cid:101) O\n\nmax∥A∥2 ∞(γ+1)2 τ 2\n\nmax∥A∥2 ∞(γ+1)2 ε2\n\n(cid:16) d2\n\n(cid:16) d2\n\n• Poisson delay. When the delays follow the Poisson distribution with parameter 1/T , it suffices to\n\n−1\n\nset ζ = 1 + T ∞T 2\n\nmax∥A∥2\n\n(cid:18)\n\nd2\n\nand L = eT (1 + T ) Assumption 1. This leads to an iteration complexity of max∥A∥2\n\n∞T 2\n\n(cid:19)\n\n(cid:19)\n\n(cid:18)\n\nd2\n\nfor finding an ε-NE, which is\n\nε2\n\n(cid:101) O\nsimilar to the bounded random delay case.\n\nfor finding an ε-QRE, or (cid:101) O\n\nτ 2\n\nlog 1 ε\n\n4 PERFORMANCE GUARANTEES OF TWO-TIMESCALE OMWU\n\nWhile Theorem 2 demonstrates provable convergence of single-timescale OMWU with random delays, it remains unclear whether the update rule can be better motivated in more general asynchronous settings, and whether the convergence can be further ensured under adversarial delays. Indeed, theoretical insights from previous literature (Mokhtari et al., 2020; Cen et al., 2021) suggest the critical role of π(t) as a predictive surrogate for π(t) in enabling fast convergence, which no longer holds when π(t) is replaced by a delayed feedback from π(κ(t) i ). To this end, we propose to replace the extrapolation update (9) with one equipped with a different learning rate:\n\nπ(t+1)\n\ni\n\n(k)\n\n∝\n\ni (k)1−ητ exp(η[Aiπ(κ(t) π(t)\n\ni )]k),\n\nk\n\n∀\n\n∈\n\nSi,\n\nwhich adopts a larger learning rate ̄η > η to counteract the delay. Intuitively, a choice of η 1)η would allow π(κ(t) π(l) : κ(t) i ) to approximate π(t) by taking the intermediate updates }\n{ into consideration. We refer to this update rule as the two-timescale entropy-regularized OMWU, whose detailed procedure is again outlined in Algorithm 1 using (10) for extrapolation.\n\n≈ i ≤\n\ni +\n\n(15)\n\n(γ(t) l < t\n\n4.1 PERFORMANCE GUARANTEES UNDER CONSTANT AND KNOWN DELAYS\n\nTo highlight the potential benefit of learning rate separation, we start by studying the convergence of two-timescale OMWU in the asynchronous setting with constant and known delays, which has been studied in (Weinberger & Ordentlich, 2002; Flaspohler et al., 2021; Meng et al., 2022). We have the following theorem, which reveals a a faster linear convergence to the QRE by using a delay-aware two-timescale learning rate design. Theorem 3 (Last-iterate convergence with fixed delays). Suppose that the delays γ(t) and known. Suppose that the learning rate η of two-timescale OMWU in Algorithm 1 satisfies\n\ni = γ are fixed\n\nmin\n\nη\n\n≤\n\n(cid:26)\n\n1 2τ (γ + 1)\n\n,\n\n5dmax\n\n1\n\n(cid:27)\n\nA\n\n∥∞ (γ + 1)2\n\n∥\n\nητ )(γ+1), then the last iterate π(t) and π(t) converge to the\n\nand η is determined by 1 −\nQRE at a linear rate: for T 1\n2\n\nmax (cid:8)KL(cid:0)π⋆\n\nπ(T +1)(cid:1),\n\nτ ∥\n\nητ = (1 γ,\n\n−\n\n≥ KL(cid:0)π⋆\n\nπ(T −γ+1)(cid:1)(cid:9)\n\n(1\n\n≤\n\n−\n\nητ )T +1KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1) + (1\n\n−\n\nητ )T +1−γ.\n\nτ ∥\n\nIn addition, the QRE-gap converges linearly according to\n\nQRE-Gapτ (π(T −γ+1))\n\n2 max\n\n≤\n\n(cid:110) d2\n\nmax ∥ τ\n\nA\n\n2 ∞\n∥\n\n,\n\n(cid:111)(cid:16)\n\n(1\n\n1 η\n\n−\n\nητ )T +1KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1) + (1\n\n−\n\nητ )T +1−γ(cid:17)\n\n.\n\nBy optimizing the learning rate η, Theorem 3 implies that two-timescale OMWU takes at most\n\n(cid:17)\n\n(cid:16) dmax∥A∥∞(γ+1)2\n\nτ\n\n(cid:101) O\niteration complexity of (cid:101) O\n\nlog 1 ε\n\niterations to find an ε-QRE in a last-iterate manner, which translates to an (cid:16) dmax∥A∥∞(γ+1)2\n\n(cid:17)\n\nthe iteration complexity of (cid:101) O\npositive role of adopting two-timescale learning rate in enabling faster convergence.\n\nmax ∥\n\nfor finding an ε-NE. This significantly improves over ∞ (γ + 1)2/ε2(cid:1) for single-timescale OMWU, verifying the 2\n∥\n\n(cid:0)d2\n\nA\n\nε\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n4.2 PERFORMANCE GUARANTEES WITH PERMUTED BOUNDED DELAYS\n\nThe above result requires the exact information of the delay, which may not always be available. Motivated by the need to address arbitrary or even adversarial delays, we consider a more realistic scenario, where the payoff sequence arrives in a permuted order (Agarwal & Duchi, 2011) constrained by a maximum bounded delay (McMahan & Streeter, 2014; Wan et al., 2022).\n\nAssumption 2 (Bounded delay). For any i Assumption 3 (Permuted feedback). For any t > 0, the payoff vector at the t-th iteration is received by agent i only once. The payoff at the 0-th iteration can be used multiple times.\n\nV and t > 0, it holds that γ(t)\n\ni ≤\n\nγ.\n\n∈\n\nThe following theorem unveils the convergence of two-timescale OMWU to the QRE in an average sense under permutated bounded delays. Theorem 4 (Average-iterate convergence under permutated delays). Under Assumption 2 and 3, suppose that the learning rate η of two-timescale OMWU in Algorithm 1 satisfies η\n\n(cid:110)\n\n1\n\nmin\n\n2τ (γ+1) , T > 2γ, it holds that\n\n1 28dmax∥A∥∞(γ+1)5/2\n\n(cid:111)\n\n, and η is determined by 1\n\nητ = (1\n\n−\n\n−\n\n≤ ητ )(γ+1), then for\n\n1\n\n−\n\nT\n\nmax\n\n2γ\n\n1\n\n≤\n\nητ (T\n\n2γ)\n\n(cid:26) T −1 (cid:88)\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1),\n\nt=2γ (cid:16)\n\nKL(cid:0)π⋆\n\nπ(0)\n\ni\n\n(cid:1) + n\n\ni,τ ∥\n\n1 3\n\n(cid:17)\n\nT −1 (cid:88)\n\nt=2γ\n\nKL(cid:0)π⋆\n\nτ ∥\n\n(cid:27)\n\nπ(t−γ+1)(cid:1)\n\n+\n\n24nγ log Smax\n\n2γ\n\nT\n\n−\n\n.\n\n(16)\n\n− Furthermore, the average QRE-gap can be bounded by\n\n1\n\n−\n\nT\n\n≤\n\nT\n\n2γ\n\n1\n\n−\n\nT −1 (cid:88)\n\nt=2γ\n\nQRE-Gapτ (π(t+1))\n\nmax\n\n2γ\n\n(cid:110) 3d2\n\nA\n\n2 ∞\n\n∥\n\n, τ\n\n(cid:111)(cid:16) 1 ητ\n\n(KL(cid:0)π⋆\n\ni,τ ∥\n\nπ(0)\n\ni\n\n(cid:1) + n) + 36nγ log Smax\n\n(cid:17)\n\n.\n\nmax ∥ 2τ\n\nTheorem 4 guarantees that\n\n2γ<t≤T is an ε-QRE as long as\n\n(cid:17)\n\n(cid:16) nd3\n\nmax∥A∥3 ∞(γ+1)5/2 ε3\n\n, which translates to an iteration complexity of\n\nT is on the order of (cid:101) O\nfor finding an ε-NE. While the rate seems slower than the previous the-\n\n(cid:101) O\norems, Theorem 4 holds under arguably the weakest delay assumptions, where it can be even adversarially bounded. We remark that the result in (16) also guarantees the convergence of the last iterate π(t) to the QRE asymptotically, although without a finite-time rate. This is in sharp contrast to typical average-iterate analysis that only applies to 1 t=1 π(t) without implications on the convergence of the last iterate π(t). Remark 2. The analysis in this section can be generalized to more commonly-used delay models where the reward information is not assumed to be observed once per round (Quanrud & Khashabi, 2015; Joulani et al., 2013), i.e., in every round an agent may observe multiple reward feedbacks from previous iterations or receive no information. This can be achieved by storing reward feedbacks in a buffer memory and picking one for policy update every round in a First-In-First-Out manner.\n\n(cid:80)T\n\nT\n\nthe best max∥A∥3\n\n(cid:16) nd3\n\n∞(γ+1)5/2 τ 2ε\n\niterate among (cid:17)\n\n{\n\nπ(t)\n\n}\n\n5 NUMERICAL EXPERIMENTS\n\nIn this section, we verify our theoretical findings by investigating the performance of both singletimescale and two-timescale OMWU on randomly generated zero-sum entropy-regularized polymaA⊤ trix games with n = 10, ji ∈\n1, 1]. All the results with entries of Aij independently sampled from the uniform distribution over [ are averaged over five independent runs.\n\nV and τ = 0.1. For each (i, j)\n\nE, we set Aij =\n\n= 10, i\n\nSi\n\n−\n\n−\n\n∈\n\n|\n\n|\n\nIn Fig. 1 (a), we compare the performance of single-timescale OMWU in both synchronous and asynchronous settings, with delay uniformly sampled from . We adopt the opti- }\nmal learning rate η from that yields the highest accuracy. The method\n\n0.1, 0.05, 0.02, 0.01, . . .\n\n0, 1, . . . , 10\n\n{\n\n{\n\n}\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\nτ ∥\n\nFigure 1: KL(cid:0)π⋆ π(t)(cid:1) of single-timescale and two-timescale OMWU with respect to different values of learning rate and delay. (a): performance of single-timescale OMWU in the synchronous setting and asynchronous setting. (b) & (c): performance of the two methods after 5000 iterations ητ )γ+1) in (b) and η fixed to under various choices of η and η, with ̄η fixed to ̄η = τ −1(1 0.001 in (c).\n\n(1\n\n−\n\n−\n\nachieves linear convergence in both cases, yet the convergence rate is slowed down by delayed feedbacks in the asynchronous setting. Fig. 1 (b) and (c) compare the effect of different choices of learning rates η, η on the performance of the proposed methods, where the feedback is permutated with bounded delay γ = 25 (cf. Assumptions 2 and 3). In general, two-timescale OMWU outperforms single-timescale OMWU given appropriate choices of learning rate η. On the other hand, (c) demonstrates that the choice of ̄η = τ −1(1 ητ )γ+1) suggested by the theory (marked with star) indeed leads to near-optimal performance of two-timescale OMWU. π(t)(cid:1) with respect to the number of iterations of single-timescale and twoFigure 2 shows KL(cid:0)π⋆ timescale OMWU under different asynchronous scenarios, with optimal choices of η and τ = 0.1. In particular, two-timescale OMWU adopts the extrapolation learning rate suggested by theory ̄η = ητ )γ+1). While both methods yield linear convergence to the QRE, two-timescale τ −1(1 method outperforms its single-timescale counterpart in the case with constant and known delay and the case where the feedback is permutated with bounded delay, which verifies our theory.\n\nτ ∥\n\n(1\n\n(1\n\n−\n\n−\n\n−\n\n−\n\n(a) random delay\n\n(b) fixed delay\n\n(c) permuted feedback\n\nπ(t)(cid:1) with respect to iteration count t of single-timescale and two-timescale Figure 2: KL(cid:0)π⋆ OMWU under various asynchronous settings. (a): random delays bounded by γ = 25. (b): constant delays γ = 50. (c): permuted feedback with delay bounded by γ = 25.\n\nτ ∥\n\n6 CONCLUSION\n\nThis paper studies asynchronous gradient play in zero-sum polymatrix games, by investigating the convergence behaviors of entropy-regularized OMWU with delayed feedbacks under two different schedules of the learning rates. We demonstrate that single-timescale OMWU enjoys a linear lastiterate convergence to the QRE even under mild statistical delays. However, the presence of the delay noticeably limits the allowable range of learning rates and slows down the convergence. To mitigate the impact, we further show that the method benefits from adopting a two-timescale learning rate in a delay-aware manner, which achieves a faster last-iterate convergence when the delay is fixed and known, and continues to converge provably even when the delays are arbitrarily bounded in an average-iterate manner. We believe our work lays the foundation for further understandings of delayed feedback in games under symmetric and independent learning.\n\n9\n\n020004000Iterationcountt10−410−310−210−1100KL(π?τ||π(t))nodelayrandomdelay10−410−2Learningrateη100101KL(π?τ||π(t))single-timescaletwo-timescale10−410−2Learningrate ̄η100101KL(π?τ||π(t))different ̄ηproposed ̄ηsingle-timescale05000100001500020000Iterationcountt10−1100KL(π?τ||π(t))single-timescaletwo-timescale020004000Iterationcountt10−410−310−210−1100KL(π?τ||π(t))single-timescaletwo-timescale05000100001500020000Iterationcountt10−1100KL(π?τ||π(t))single-timescaletwo-timescalePublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENT\n\nS. Cen and Y. Chi are supported in part by the grants ONR N00014-19-1-2404, NSF CCF-1901199, CCF-2106778 and CNS-2148212. S. Cen is also gratefully supported by Wei Shen and Xuehong Zhang Presidential Fellowship, and Nicholas Minnici Dean’s Graduate Fellowship in Electrical and Computer Engineering at Carnegie Mellon University. R. Ao is supported by the Elite Undergraduate Training Program of School of Mathematical Sciences at Peking University.\n\nREFERENCES\n\nAlekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. Advances in neural\n\ninformation processing systems, 24, 2011.\n\nIoannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On last-iterate\n\nconvergence beyond zero-sum games. arXiv preprint arXiv:2203.12056, 2022.\n\nMahmoud Assran, Arda Aytekin, Hamid Reza Feyzmahdavian, Mikael Johansson, and Michael G Rabbat. Advances in asynchronous parallel and distributed optimization. Proceedings of the IEEE, 108(11):2013–2031, 2020.\n\nJames P Bailey. O(1/T ) time-average convergence in a generalization of multiagent zero-sum\n\ngames. arXiv preprint arXiv:2110.02482, 2021.\n\nLM Bergman and IN Fokin. On separable non-cooperative zero-sum games. Optimization, 44(1):\n\n69–84, 1998.\n\nRiccardo Bernardi. Interactive image segmentation using graph transduction games. 2021.\n\nDimitri P Bertsekas and John N Tsitsiklis. Parallel and distributed computation: numerical methods,\n\n1989.\n\nYang Cai, Ozan Candogan, Constantinos Daskalakis, and Christos Papadimitriou. Zero-sum polymatrix games: A generalization of minmax. Mathematics of Operations Research, 41(2):648– 655, 2016.\n\nShicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural policy gradient methods with entropy regularization. arXiv preprint arXiv:2007.06558, 2020.\n\nShicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive games with entropy regularization. Advances in Neural Information Processing Systems, 34:27952– 27964, 2021.\n\nShicong Cen, Yuejie Chi, Simon Du, and Lin Xiao. Faster last-iterate convergence of policy opti-\n\nmization in zero-sum markov games. arXiv preprint arXiv:2210.01050, 2022.\n\nNicol‘o Cesa-Bianchi, Claudio Gentile, Yishay Mansour, and Alberto Minora. Delay and cooperation in nonstochastic bandits. In Conference on Learning Theory, pp. 605–622. PMLR, 2016.\n\nConstantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and\n\nconstrained min-max optimization. arXiv preprint arXiv:1807.04252, 2018.\n\nConstantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms In Proceedings of the twenty-second annual ACM-SIAM symposium on\n\nfor zero-sum games. Discrete Algorithms, pp. 235–254. SIAM, 2011.\n\nMichail Fasoulakis, Evangelos Markakis, Yannis Pantazis, and Constantinos Varsos. Forward looking best-response multiplicative weights update methods for bilinear zero-sum games. In International Conference on Artificial Intelligence and Statistics, pp. 11096–11117. PMLR, 2022.\n\nGenevieve E Flaspohler, Francesco Orabona, Judah Cohen, Soukayna Mouatadid, Miruna Oprescu, Paulo Orenstein, and Lester Mackey. Online learning with optimism and delay. In International Conference on Machine Learning, pp. 3363–3373. PMLR, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nYoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games\n\nand Economic Behavior, 29(1-2):79–103, 1999.\n\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint arXiv:1406.2661, 2014.\n\nXinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising, pp. 1–9, 2014.\n\nAm ́elie H ́eliou, Panayotis Mertikopoulos, and Zhengyuan Zhou. Gradient-free online learning in continuous games with delayed rewards. In International conference on machine learning, pp. 4172–4181. PMLR, 2020.\n\nYu-Guan Hsieh, Franck Iutzeler, J ́erˆome Malick, and Panayotis Mertikopoulos. Multi-agent arXiv preprint\n\nonline optimization with delays: Asynchronicity, adaptivity, and optimism. arXiv:2012.11579, 2020.\n\nPooria Joulani, Andras Gyorgy, and Csaba Szepesv ́ari. Online learning under delayed feedback. In\n\nInternational Conference on Machine Learning, pp. 1453–1461. PMLR, 2013.\n\nChung-Wei Lee, Christian Kroer, and Haipeng Luo. Last-iterate convergence in extensive-form\n\ngames. Advances in Neural Information Processing Systems, 34, 2021.\n\nStefanos Leonardos, Georgios Piliouras, and Kelly Spendlove. Exploration-exploitation in multiagent competition: convergence with bounded rationality. Advances in Neural Information Processing Systems, 34:26318–26331, 2021.\n\nBingcong Li, Tianyi Chen, and Georgios B Giannakis. Bandit online learning with unknown delays. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 993–1002. PMLR, 2019.\n\nMichael L Littman. Markov games as a framework for multi-agent reinforcement learning.\n\nIn\n\nMachine Learning Proceedings, pp. 157–163. Elsevier, 1994.\n\nJi Liu and Stephen J Wright. Asynchronous stochastic coordinate descent: Parallelism and conver-\n\ngence properties. SIAM Journal on Optimization, 25(1):351–376, 2015.\n\nJi Liu, Steve Wright, Christopher R ́e, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. In International Conference on Machine Learning, pp. 469–477. PMLR, 2014.\n\nRichard D McKelvey and Thomas R Palfrey. Quantal response equilibria for normal form games.\n\nGames and economic behavior, 10(1):6–38, 1995.\n\nBrendan McMahan and Matthew Streeter. Delay-tolerant algorithms for asynchronous distributed\n\nonline learning. Advances in Neural Information Processing Systems, 27, 2014.\n\nMin Meng, Xiuxian Li, and Jie Chen. Decentralized nash equilibria learning for online game with\n\nbandit feedback. arXiv preprint arXiv:2204.09467, 2022.\n\nPanayotis Mertikopoulos and William H Sandholm. Learning in games via reinforcement and regu-\n\nlarization. Mathematics of Operations Research, 41(4):1297–1324, 2016.\n\nPanayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on\n\nregularized learning. Discrete Algorithms, pp. 2703–2717. SIAM, 2018.\n\nAryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In International Conference on Artificial Intelligence and Statistics, pp. 1497–1507. PMLR, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nYu Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Opti-\n\nmization, 16(1):235–249, 2005a.\n\nYu Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):\n\n127–152, 2005b.\n\nYu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM\n\nJournal on Optimization, 22(2):341–362, 2012.\n\nCiara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, and Steffen Grunewalder. Bandits with delayed, aggregated anonymous feedback. In International Conference on Machine Learning, pp. 4105–4113. PMLR, 2018.\n\nKent Quanrud and Daniel Khashabi. Online learning with adversarial delays. Advances in neural\n\ninformation processing systems, 28, 2015.\n\nAlexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable se-\n\nquences. arXiv preprint arXiv:1311.1869, 2013.\n\nBenjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. Advances in neural information processing systems, 24, 2011.\n\nSamuel Sokota, Ryan D’Orazio, J Zico Kolter, Nicolas Loizou, Marc Lanctot, Ioannis Mitliagkas, Noam Brown, and Christian Kroer. A unified approach to reinforcement learning, quantal response equilibria, and two-player zero-sum games. arXiv preprint arXiv:2206.05825, 2022.\n\nQianqian Tong, Guannan Liang, and Jinbo Bi. Effective federated adaptive gradient methods with\n\nnon-iid decentralized data. arXiv preprint arXiv:2009.06557, 2020.\n\nClaire Vernade, Olivier Capp ́e, and Vianney Perchet. Stochastic bandit models for delayed conver-\n\nsions. arXiv preprint arXiv:1706.09186, 2017.\n\nYuanyu Wan, Wei-Wei Tu, and Lijun Zhang. Online strongly convex optimization with unknown\n\ndelays. Machine Learning, 111(3):871–893, 2022.\n\nChen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive Markov games. arXiv preprint arXiv:2102.04540, 2021.\n\nMarcelo J Weinberger and Erik Ordentlich. On delayed prediction of individual sequences. IEEE\n\nTransactions on Information Theory, 48(7):1959–1976, 2002.\n\nXin Zhang, Jia Liu, and Zhengyuan Zhu. Taming convergence for asynchronous stochastic gradiIn 2020 59th IEEE Conference on\n\nent descent with unbounded delay in non-convex learning. Decision and Control (CDC), pp. 3580–3585. IEEE, 2020.\n\nZhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter W Glynn, and Claire Tomlin. Countering feedback delays in multi-agent learning. Advances in Neural Information Processing Systems, 30, 2017.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAppendix\n\nTable of Contents\n\nA Further related works\n\nB Proof for single-timescale OMWU (Section 3)\n\nB.1 Proof of Theorem 1 .\n\nB.2 Proof of Theorem 2 .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nC Regret analysis of single-timescale OMWU\n\nC.1 Proof of Theorem 5 .\n\nC.2 Proof of Theorem 6 .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nD Proof for two-timescale OMWU (Section 4)\n\nD.1 Proof of Theorem 3 .\n\nD.2 Proof of Theorem 4 .\n\nE Proof of auxiliary lemmas\n\nE.1 Proof of Lemma 1 .\n\nE.2 Proof of Lemma 2 .\n\nE.3 Proof of Lemma 3 .\n\nE.4 Proof of Lemma 4 .\n\nE.5 Proof of Lemma 5 .\n\nE.6 Proof of Lemma 6 .\n\nE.7 Proof of Lemma 7 .\n\nE.8 Proof of Lemma 8 .\n\nE.9 Proof of Lemma 9 .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nA FURTHER RELATED WORKS\n\n13\n\n14\n\n14\n\n16\n\n22\n\n23\n\n26\n\n29\n\n29\n\n33\n\n39\n\n39\n\n39\n\n40\n\n41\n\n42\n\n42\n\n43\n\n44\n\n46\n\nO\n\nO\n\nLearning in two-player zero-sum matrix games. Freund & Schapire (1999) proved that Multi- (1/√T ) plicative Weights Update (MWU) method achieve an average-iterate convergence rate of through the lens of regret analysis. Daskalakis et al. (2011) is the first to achieve an optimal conver- (1/T ) with the excessive gap technique of Nesterov (Nesterov, 2005a;b). Rakhlin gence rate of & Sridharan (2013) achieved the same rate with OMD, which is more commonly referred to as OMWU when entropy regularization is in use for the mirror descent update rule. In terms of lastiterate convergence, Daskalakis & Panageas (2018) established asymptotic last-iterate convergence for OMWU assuming the uniqueness of NE. Wei et al. (2021) improved upon the analysis under the same assumption by showing a problem-dependent linear rate of convergence, which is extended to a class of extensive-form games (Lee et al., 2021). Cen et al. (2021) showed that entropy-regularized OMWU converges linearly to the QRE of two-player zero-sum matrix game, which translates to an (1/T ) for finding an ε-NE, without assuming its uniqueness; the linear iteration complexity of (cid:101) O\nconvergence to the QRE continues to hold with smooth value updates (Cen et al., 2022). Sokota et al. (2022) showed that linear convergence to QRE can be achieved without resorting to optimistic update rules, e.g., using entropy-regularized MWU, albeit with a more restrictive learning rate. It is worth pointing out that the idea of learning rate separation has been explored for equilibrium finding in two-player zero-sum games with instant feedback (Fasoulakis et al., 2022) and online learning with delayed feedback (Hsieh et al., 2020), but lacks study in an asynchronous multi-player game setting.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAsynchronous optimization. Asynchronous and decentralized optimization algorithms have been extensively studied since the proposal of Bertsekas & Tsitsiklis (1989), where a number of agents seek to find an approximate global optimizer of a common loss function, by performing iterative gradient-based methods in a collaborative manner. Typical approaches including parallelizing the computation of gradient with regard to data (Tong et al., 2020), or parallelizing the model updates by imposing coordinate update rules (Nesterov, 2012; Liu et al., 2014; Liu & Wright, 2015). Delayed gradient (feedback) is also common in these scenarios due to the existence of other agents updating the model. Moreover, the zero-sum polymatrix setting considered in this work is inherently noncollaborative by requiring every agent to maximize its own utility function and compete with other agents, and leads to substantially difference analysis techniques.\n\nB PROOF FOR SINGLE-TIMESCALE OMWU (SECTION 3)\n\nBefore delving into the main proof, we first record a useful lemma pertaining to a basic property V , we denote by of zero-sum polymatrix games; the proof is deferred to Appendix E.1. For i the neighbors of agent i in the graph (V, E). For notational simplicity, we\n\nj : (i, j)\n\nN denote by x 1= y the equivalence between two vectors x and y up to a global shift, i.e.,\n\ni =\n\nE\n\n∈\n\n∈\n\n}\n\n{\n\n· R, where 1 is the all-one vector.\n\nx = y + c\n\n1\n\nfor some constant c\n\n∈\n\nLemma 1. For any zero-sum polymatrix game\n\n, it holds that for π, π′\n\nG\n\n(cid:2)ui(πi, π′\n\n−i) + ui(π′\n\ni, π−i)(cid:3) = 0.\n\n(cid:88)\n\ni∈V\n\n∆(S) that\n\n∈\n\n(17)\n\n(18)\n\nOr equivalently, (cid:80)\n\ni∈V\n\n(cid:88)\n\n(cid:10)πi\n\ni∈V\n\n−\n\nπ′\n\ni, Ai(π\n\n−\n\n(cid:2)π⊤\n\ni Aiπ′ + (π′ (cid:88) π′)(cid:11) =\n\ni)⊤Aiπ(cid:3) = 0. It follows that (cid:2)π⊤\n\n[ui(π) + ui(π′)]\n\n(cid:88)\n\ni∈V\n\n−\n\ni∈V\n\ni Aiπ′ + (π′\n\ni)⊤Aiπ(cid:3) = 0.\n\nB.1 PROOF OF THEOREM 1\n\nWe start with the following lemma that characterizes the iterates of OMWU, which generalizes Cen et al. (2021, Lemma 1) for zero-sum two-player games to zero-sum polymatrix games. The proof can be found in Appendix E.2.\n\nLemma 2. The iterates of OMWU based on the update rule (9) satisfy\n\n(cid:10) log π(t+1)\n\n(1\n\n−\n\n−\n\nητ ) log π(t)\n\n−\n\nητ log π⋆\n\nτ , π(t+1)\n\n(cid:11) = 0.\n\nπ⋆\n\nτ\n\n−\n\nTo continue, by the definition of KL divergence, we have\n\n(cid:10) log π(t+1) −\n= (cid:10) log π(t+1)\n\n(1\n\nητ ) log π(t)\n\n−\n\n−\n\n− (1 (cid:10) log π(t+1) −\nητ )KL(cid:0)π(t+1) (cid:10) log π(t+1)\n\n−\n\nητ log π⋆\n\nτ , π(t+1)(cid:11)\n\n− ητ ) log π(t) log π(t+1), π(t+1)(cid:11)\n\n−\n\nητ log π⋆\n\nπ(t)(cid:1) + ητ KL(cid:0)π(t+1)\n\n∥ log π(t+1), π(t+1)\n\nτ , π(t+1)(cid:11) (cid:10) log π(t+1) π⋆ ∥\nπ(t+1)(cid:11).\n\nτ\n\n−\n\n−\n\n= (1\n\n−\n\n−\n\n−\n\nIn addition,\n\nlog π(t+1), π(t+1)\n\n− (cid:1) + KL(cid:0)π(t+1)\n\nπ(t+1)(cid:1)\n\n∥\n\nπ(t+1)(cid:11)\n\n−\n\n(cid:10) log π(t+1)\n\n(1\n\nητ ) log π(t)\n\nητ log π⋆\n\nτ , π⋆\n\nτ\n\n(cid:11) = KL(cid:0)π⋆\n\nπ(t+1)(cid:1)\n\n− −\nSumming up the above two relations, in view of Lemma 2, it holds that\n\n−\n\n−\n\nτ ∥\n\n(1\n\n−\n\n−\n\nητ )KL(cid:0)π⋆\n\nτ ∥\n\nπ(t)(cid:1).\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1) = (1\n\nητ )KL(cid:0)π⋆ τ ∥ −\n+ (cid:10) log π(t+1)\n\nπ(t)(cid:1)\n\n(1\n\n−\n\n−\n\nlog π(t+1), π(t+1)\n\nητ )KL(cid:0)π(t+1)\n\nπ(t)(cid:1)\n\nKL(cid:0)π(t+1)\n\n∥ π(t+1)(cid:11)\n\n−\n\nητ KL(cid:0)π(t+1)\n\n−\n\n−\n\n−\n\nπ(t+1)(cid:1) (cid:1).\n\n∥ π⋆\n\nτ\n\n(19)\n\n∥\n\nWe now proceed to bound the terms of interest one by one.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nBounding KL(cid:0)π⋆ τ ∥ update rule of π(t+1)\n\nπ(t)(cid:1). We aim to control the right-hand-side (RHS) of (19). Based on the in Algorithm 1, we have\n\ni\n\nlog π(t+1)\n\ni\n\nlog π(t+1)\n\ni\n\n−\n\n1= ηAi(π(t) 1= ηAi(π(t)\n\n−\n\n−\n\nπ(t+1))\n\n(20)\n\nπ(t)) + ηAi(π(t)\n\nπ(t+1)).\n\n−\n\nIt follows that (cid:10) log π(t+1)\n\n= η\n\nη\n\n≤\n\nj∈Ni (cid:88)\n\nj∈Ni\n\ni (cid:88)\n\n− (π(t+1)\n\ni\n\nlog π(t+1)\n\ni\n\n, π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:11)\n\nπ(t+1)\n\ni\n\n−\n\n−\n\n)⊤Aij(π(t)\n\nj −\n\nAij\n\n∥\n\n∥∞\n\n(cid:13) (cid:13)π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:13) (cid:13)π(t)\n\nj −\n\n−\n\nj∈Ni (cid:13) (cid:13)1 + η\n\nπ(t)\n\nj\n\nπ(t)\n\nj ) + η\n\n(cid:88)\n\n(π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n)⊤Aij(π(t)\n\nj −\n\n−\n\nπ(t+1)\n\nj\n\n)\n\n(cid:88)\n\nj∈Ni\n\nAij\n\n∥\n\n∥∞\n\n(cid:13) (cid:13)π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:13) (cid:13)π(t)\n\nj −\n\n−\n\nπ(t+1)\n\nj\n\n(cid:13) (cid:13)1\n\nη 2 ∥\n\nA\n\n∥∞\n\n≤\n\n(cid:88)\n\n(cid:16)(cid:13)\n\n(cid:13)π(t)\n\nj −\n\nπ(t)\n\nj\n\n(cid:13) 2\n(cid:13)\n\n1 + (cid:13)\n\n(cid:13)π(t+1)\n\nj\n\nπ(t)\n\nj\n\n−\n\n(cid:13) 2\n(cid:13)\n\n1 + 2(cid:13)\n\n(cid:13)π(t+1)\n\ni\n\n(cid:17)\n\nπ(t+1)\n\ni\n\n(cid:13) 2\n(cid:13) 1\n\n−\n\nη\n\nA\n\n∥\n\n∥∞\n\n≤\n\nj∈Ni (cid:88)\n\n(cid:16)\n\nj∈Ni\n\nKL(cid:0)π(t)\n\nj ∥\n\nπ(t)\n\nj\n\n(cid:1) + KL(cid:0)π(t+1)\n\nj\n\n∥\n\nπ(t)\n\nj\n\n(cid:1) + 2KL(cid:0)π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:1)(cid:17)\n\n,\n\n∥\n\n(21)\n\nwhere the last line follows from Pinsker’s inequality. Summing the inequality over i\n\nV , we get\n\n∈\n\nlog π(t+1), π(t+1)\n\nπ(t+1)(cid:11)\n\n(cid:10) log π(t+1)\n\n−\n\nηdmax\n\nA\n\n∥\n\n∥∞\n\n≤\n\n(cid:16)\n\nKL(cid:0)π(t)\n\n∥\n\n−\n\nπ(t)(cid:1) + KL(cid:0)π(t+1)\n\nπ(t)(cid:1) + 2KL(cid:0)π(t+1)\n\n∥\n\nπ(t+1)(cid:1)(cid:17)\n\n.\n\n∥\n\nPlugging the above inequality back into (19) yields\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1)\n\n(1\n\n−\n\n≤\n\n−\n\n−\n\nητ )KL(cid:0)π⋆\n\nτ ∥ (1 2ηdmax ∥\nητ KL(cid:0)π(t+1)\n\n−\n\nπ(t)(cid:1)\n\n(1\n\nητ\n\n−\n\n−\n\n−\n\nA\n\n∥∞)KL(cid:0)π(t+1) (cid:1).\n\nπ⋆\n\nτ\n\n∥\n\n∥∞) KL(cid:0)π(t+1)\n\nηdmax A\nπ(t+1)(cid:1) + ηdmax\n\nA\n\n∥\n\nπ(t)(cid:1) ∥∞ KL(cid:0)π(t)\n\n∥\n\n∥\n\n∥\n\nπ(t)(cid:1)\n\n(22)\n\n∥\n\nWith the choice of the learning rate\n\n0 < η\n\nmin\n\n≤\n\n(cid:26) 1 2τ\n\n,\n\n(cid:27)\n\n,\n\n1\n\n4dmax\n\nA\n\n∥\n\n∥∞\n\nit holds that 1\n\nητ\n\n−\n\n−\n\nηdmax\n\nA\n\n∥∞ > 0 and\n\n∥\n\nηdmax\n\nA\n\n∥\n\n∥∞ ≤\n\n1 4 ≤\n\n(1\n\n−\n\nητ )(1\n\n2ηdmax\n\nA\n\n∥\n\n∥∞).\n\n−\n\n(23)\n\nThis allows us to further relax (22) by\n\nKL(cid:0)π⋆\n\nτ ∥\n\n(1\n\n(1\n\n≤\n\n≤\n\n−\n\n−\n\nπ(t+1)(cid:1) + (1 ητ )KL(cid:0)π⋆ (cid:16)\n\nτ ∥ KL(cid:0)π⋆\n\nητ )\n\n2ηdmax A\n− ∥\nπ(t)(cid:1) + ηdmax π(t)(cid:1) + (1\n\n∥∞)KL(cid:0)π(t+1) ∥∞ KL(cid:0)π(t)\n\nA\n\n∥ 2ηdmax\n\n∥\n\nπ(t+1)(cid:1) π(t)(cid:1) ∥∞)KL(cid:0)π(t)\n\n∥\n\nA\n\n∥\n\n∥\n\n−\n\nτ ∥\n\nπ(t)(cid:1)(cid:17)\n\n.\n\nLet us now introduce the potential function of iterates\n\nL(t) := KL(cid:0)π⋆\n\nτ ∥\n\nπ(t)(cid:1) + (1\n\n2ηdmax\n\n−\n\nA\n\n∥\n\n∥∞)KL(cid:0)π(t)\n\n∥\n\nπ(t)(cid:1),\n\nwhich allows us to simply the previous inequality as\n\nL(t+1)\n\n(1\n\nητ )L(t)\n\n(1\n\nητ )t+1L(0) = (1\n\nτ ∥ where the last equality follows from the definition π(0) = π(0). Hence, we have\n\n−\n\n−\n\n≤\n\n−\n\n≤\n\nητ )t+1KL(cid:0)π⋆\n\nπ(0)(cid:1),\n\n(24)\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t)(cid:1)\n\n≤\n\nL(t)\n\n(1\n\n−\n\n≤\n\nητ )tKL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1).\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nBounding KL(cid:0)π⋆\n\nπ(t+1)(cid:1). Following similar approaches to (21), we can bound\n\nτ ∥\n\n(cid:10)π⋆\n\ni,τ −\n\n− = η(π(t+1)\n\ni\n\nη\n\nA\n\n∥\n\n∥∞\n\n≤\n\nπ(t+1)\n\ni\n\n, log π(t+1)\n\ni\n\nlog π(t+1)\n\ni\n\n(cid:11)\n\n−\n\ni,τ )⊤Ai(π(t) π⋆ (cid:16) KL(cid:0)π(t)\n\n− (cid:88)\n\nπ(t)) + η(π(t+1) (cid:1) + KL(cid:0)π(t+1)\n\ni\n\nj\n\n− π(t)\n\nj\n\nπ⋆\n\ni,τ )⊤Ai(π(t) (cid:1) + 2KL(cid:0)π⋆\n\n− π(t)\n\nj\n\nπ(t+1))\n\nπ(t+1)\n\ni\n\n(cid:1)(cid:17)\n\n.\n\n(25)\n\n−\n\ni,τ ∥\n\nj∈Ni\n\nj ∥\n\nSumming the inequality over i (cid:10)π⋆\n\nπ(t+1), log π(t+1)\n\n∈\n\nV leads to\n\nτ − ηdmax\n\n−\n\n≤\n\nA\n\n∥\n\n∥∞\n\n(cid:2)KL(cid:0)π(t)\n\nlog π(t+1)(cid:11) −\nπ(t)(cid:1) + KL(cid:0)π(t+1)\n\nOn the other hand, by the definition of KL divergence, we have π(t+1)(cid:1) KL(cid:0)π⋆\n\nπ(t+1)(cid:1) = KL(cid:0)π⋆\n\nKL(cid:0)π(t+1)\n\nπ(t+1)(cid:1)\n\n(cid:10)π⋆\n\nτ ∥\n\nτ ∥\n\n∥\n\n∥\n\n−\n\nπ(t)(cid:1) + 2KL(cid:0)π⋆\n\nπ(t+1)(cid:1)(cid:3).\n\nτ ∥\n\n−\n\nτ −\n\nπ(t+1), log π(t+1)\n\nlog π(t+1)(cid:11). (26)\n\n−\n\n∥\n\n∥\n\nCombining the above two inequalities, we get\n\n2ηdmax\n\n− KL(cid:0)π⋆\n\nτ ∥\n\nA\n\n∥∞)KL(cid:0)π⋆ τ ∥ ∥\nπ(t+1)(cid:1) + ηdmax\n\n(1\n\n≤\n\nπ(t+1)(cid:1) (cid:16)\n\nA\n\n∥\n\n∥∞\n\nKL(cid:0)π(t)\n\n∥\n\nπ(t)(cid:1) + KL(cid:0)π(t+1)\n\nπ(t)(cid:1)(cid:17)\n\n.\n\n∥\n\nPlugging the above inequality back into (22), we have\n\n2ηdmax\n\nA ∥\nητ )KL(cid:0)π⋆\n\n− (1\n\nητ\n\n−\n\n(1\n\nτ ∥\n\nπ(t+1)(cid:1)\n\nτ ∥ A\n\n∥∞)KL(cid:0)π⋆ π(t)(cid:1) ∥∞)KL(cid:0)π(t+1) π(t)(cid:1) + 2ηdmax A\n2ηdmax\n\n−\n\n−\n\n∥\n\n− (1\n\n2ηdmax ∥\n− ητ )KL(cid:0)π⋆ τ ∥\n\nπ(t)(cid:1) + (1\n\n− (1 −\nKL(cid:0)π⋆\n\nτ ∥\n\n(1\n\n≤\n\n≤\n\n≤\n\n∥∞)KL(cid:0)π(t+1)\n\nπ(t)(cid:1) ∥∞ KL(cid:0)π(t)\n\n∥\n\n∥\n\n2dmaxη\n\nA\n\n− ∥\nπ(t+1)(cid:1) + 2ηdmax ∥∞ KL(cid:0)π(t) A\n\nA ∥\nπ(t)(cid:1) π(t)(cid:1) = L(t),\n\n∥\n\n∥\n\n∥\n\n∥∞)KL(cid:0)π(t)\n\n∥\n\nητ KL(cid:0)π(t+1)\n\n− π(t)(cid:1)\n\n(cid:1)\n\nπ⋆\n\nτ\n\n∥\n\nwhere the second and third inequalities follow from the choice of the learning rate, and the last line follows from the definition of the potential function L(t). Then the result follows from (24) as\n\n1 2\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1)\n\n(1\n\n−\n\n≤\n\n2ηdmax\n\nA\n\n∥∞)KL(cid:0)π⋆\n\nτ ∥\n\n∥\n\nπ(t+1)(cid:1)\n\nL(t)\n\n(1\n\n−\n\n≤\n\n≤\n\nητ )tKL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1).\n\nBounding the QRE-Gap. Finally, we bound the QRE-gap, which can be linked to the KL divergence using the following lemma. The proof can be found in Appendix E.3.\n\nLemma 3. For any π\n\n∈\n\n∆(S) and QRE π⋆\n\nQRE-Gapτ (π)\n\n≤\n\nLemma 3 tells us\n\nτ ∈ τ KL(cid:0)π\n\n∆(S), it holds that\n\n(cid:1) +\n\nπ⋆\n\nτ\n\n∥\n\nd2\n\nmax ∥ τ\n\nA\n\n2 ∞\n∥\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(cid:1).\n\nQRE-Gapτ (π(t))\n\nτ KL(cid:0)π(t)\n\nπ⋆\n\n(cid:1) +\n\nτ ∥ π(t)(cid:1) controlled in the above, we still need to control KL(cid:0)π(t)\n\n≤\n\n∥\n\nτ\n\nKL(cid:0)π⋆\n\nπ(t)(cid:1).\n\nd2\n\nmax ∥ τ\n\nA\n\n2 ∞\n∥\n\n(27)\n\n(cid:1). From (22), it\n\nπ⋆ τ\n\n∥\n\n(cid:1)\n\nπ⋆\n\nτ\n\nη−1(1\n\nητ )L(t−1)\n\n≤ Plugging them back to (27), we arrive at\n\n−\n\n∥\n\nη−1(1\n\n−\n\n≤\n\nητ )tL(0) = η−1(1\n\nητ )tKL(cid:0)π⋆\n\nτ ∥\n\n−\n\nπ(0)(cid:1).\n\nQRE-Gapτ (π(t))\n\n≤\n\nB.2 PROOF OF THEOREM 2\n\n(cid:0)η−1 + 2τ −1d2\n\nA\n\n(cid:1)(1\n\n2 ∞\n\n∥\n\n−\n\nητ )t−1KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1).\n\nmax∥\n\nWe begin with bounding the KL divergence KL(cid:0)π⋆ linking it to the KL divergence.\n\nτ ∥\n\nπ(t)(cid:1) and then move to bound the QRE-gap by\n\n16\n\nWith KL(cid:0)π⋆ follows that τ KL(cid:0)π(t)\n\nτ ∥\n\nPublished as a conference paper at ICLR 2023\n\nBounding the term KL(cid:0)π⋆\n\nπ(t)(cid:1). We start with the following equation\n\n(1\n\n−\n\nητ )KL(cid:0)π⋆\n\ni,τ ∥\n\nπ(t)\n\ni\n\nτ ∥ (cid:1) = (1\n\nητ )KL(cid:0)π(t+1)\n\ni\n\nπ(t) (cid:1)\n\n(cid:1) + ητ KL(cid:0)π(t+1) (cid:10)log π(t+1)\n\ni\n\n∥ log π(t+1)\n\nπ⋆\n\ni,τ\n\n(cid:1) + KL(cid:0)π(t+1) , π(t+1)\n\n∥ π(t+1)\n\nπ(t+1) i\n(cid:11)\n\ni\n\n(cid:1)\n\ni\n\ni\n\ni\n\n∥ π(t+1)\n\ni\n\n− + KL(cid:0)π⋆\n\ni,τ ∥ + η(π(t+1)\n\ni\n\n−\n\ni\n\n− i,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\n− π⋆ τ )\n\n−\n\ni\n\n−\n\n(28)\n\nwhere its proof follows a similar deduction as (19). Our first target is to bound the last two terms on the RHS of (28) with\n\nητ KL(cid:0)π(t+1)\n\ni\n\nπ⋆\n\ni,τ\n\n(cid:1) + KL(cid:0)π(t+1)\n\ni\n\n∥ Let us introduce the potential function of iterates\n\n∥\n\nπ(t+1)\n\ni\n\n(cid:1) + (1\n\n−\n\nητ )KL(cid:0)π(t+1)\n\ni\n\nπ(t)\n\ni\n\n(cid:1).\n\n∥\n\nΨ(l)\n\ni\n\n:= KL(cid:0)π(l+1)\n\ni\n\nπ(l)\n\ni\n\n∥\n\n(cid:1)+KL(cid:0)π(l)\n\ni ∥\n\nπ(l)\n\ni\n\n(cid:1), Ψ(l) =\n\n(cid:88)\n\ni∈V\n\nΨ(l)\n\ni = KL(cid:0)π(l+1)\n\n∥\n\nπ(l)(cid:1)+KL(cid:0)π(l)\n\nπ(l)(cid:1),\n\n∥\n\nwhich will be used repetitively in the rest of this proof. For notational simplicity, let Ψ(l) l < 0.\n\ni = 0 when\n\nStep 1: bounding (cid:10)log π(t+1) (21), we get\n\ni\n\n−\n\nlog π(t+1)\n\ni\n\n, π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:11). Following a similar argument as\n\n−\n\n(cid:11)\n\n(cid:10)log π(t+1)\n\nlog π(t+1)\n\ni\n\ni (cid:88)\n\n− (π(t+1)\n\ni\n\n= η\n\ni\n\n, π(t+1)\n\nπ(t+1) −\n)⊤Aij(π(κ(t+1)\n\ni\n\ni\n\nj\n\n)\n\nπ(t+1)\n\ni\n\n−\n\nj∈Ni\n\nπ(κ(t)\n\ni )\n\nj\n\n)\n\n−\n\nηdmax\n\n≤\n\nA\n\n∥∞ KL(cid:0)π(t+1)\n\ni\n\n∥\n\n∥\n\nπ(t+1)\n\ni\n\n(cid:1) +\n\nη\n\n∥\n\nA 2\n\n∥∞\n\n(cid:88)\n\nj∈Ni\n\n(cid:13)\n\n(cid:13)π(κ(t+1)\n\ni\n\nj\n\nπ(κ(t)\n\ni )\n\nj\n\n(cid:13) 2\n(cid:13)\n\n1.\n\n(29)\n\n)\n\n−\n\nTo control the term (cid:13)\n\n(cid:13)π(κ(t+1)\n\ni\n\nj\n\n)\n\n− π(κ(t)\n\ni )\n\nj\n\nπ(κ(t)\n\ni )\n\nj\n\n(cid:13) 2\n(cid:13)\n\n1, when t = 0, we have\n\n(cid:13) 2\n(cid:13)\n\n1 = (cid:13)\n\n(cid:13)π(κ(t+1)\n\ni\n\nj\n\n)\n\nπ(0)\n\nj\n\n(cid:13) 2\n(cid:13) 1 ≤\n\n(cid:13) (cid:13)π(1)\n\nj −\n\nπ(0)\n\nj\n\n(cid:13) 2\n(cid:13) 1 ≤\n\n−\n\n2Ψ(0)\n\nj\n\n(30)\n\n(cid:13)\n\n(cid:13)π(κ(t+1)\n\ni\n\nj\n\n)\n\n−\n\nby Pinsker’s inequality. For t\n\n≥\n\n1, consider the decomposition\n\nπ(t)\n\nj −\n\nπ(t−k)\n\nj\n\n=\n\nit then follows that\n\n(cid:13) (cid:13)π(t)\n\nj −\n\nπ(t−k)\n\nj\n\n(cid:13) 2\n(cid:13) 1 ≤\n\nk\n\nt−1 (cid:88)\n\nl=t−k\n\nt−1 (cid:88)\n\nl=t−k\n\n(cid:16)\n\nπ(l+1)\n\nj\n\n(cid:17)\n\n,\n\nπ(l)\n\nj\n\n−\n\n1\n\n∀\n\n≤\n\nk\n\n≤\n\nt,\n\n(cid:13) (cid:13)π(l+1)\n\nj\n\nπ(l)\n\nj\n\n(cid:13) 2\n(cid:13) 1\n\n−\n\n(cid:16)(cid:13)\n\n(cid:13)π(l+1)\n\nj\n\nπ(l)\n\nj\n\n−\n\n(cid:13) 2\n(cid:13)\n\n1 + (cid:13)\n\n(cid:13)π(l)\n\nj −\n\n(cid:17)\n\nπ(l)\n\nj\n\n(cid:13) 2\n(cid:13) 1\n\n2k\n\n≤\n\n4k\n\n≤\n\nt−1 (cid:88)\n\nl=t−k\n\nt−1 (cid:88)\n\nl=t−k\n\nΨ(l) j ,\n\n(31)\n\n> 0, we proceed to\n\nwhere the last line applies Pinsker’s inequality. Depending on whether γ(t+1) bound the terms (cid:13)\n\nπ(κ(t)\n\n(cid:13)π(κ(t+1)\n\n(cid:13) 2\n(cid:13)\n\ni )\n\n1 in (29) considering the following two cases based on (31).\n\nj\n\nj\n\n)\n\ni\n\ni\n\n−\n\n• γ(t+1)\n\ni\n\n= 0. Then\n\n(cid:13)\n\n(cid:13)π(κ(t+1)\n\ni\n\nj\n\nπ(κ(t)\n\ni )\n\nj\n\n)\n\n−\n\n(cid:13) 2\n(cid:13) 1 ≤\n\n2(cid:13) (cid:13)π(t+1)\n\nj\n\nπ(t)\n\nj\n\n−\n\n17\n\n(cid:13) 2\n(cid:13)\n\n1 + 2(cid:13)\n\n(cid:13)π(t)\n\nj −\n\nπ(κ(t)\n\ni )\n\nj\n\n(cid:13) 2\n(cid:13) 1\n\nPublished as a conference paper at ICLR 2023\n\n8Ψ(t)\n\nj + 8γ(t)\n\ni\n\n≤\n\nt−1 (cid:88)\n\nΨ(l) j ,\n\nl=t−γ(t)\n\ni\n\nwhere the last step uses (31) and\n\n(cid:13) (cid:13)π(t+1)\n\nj\n\nπ(t)\n\nj\n\n(cid:13) 2\n(cid:13) 1 ≤\n\n2\n\n−\n\n(cid:16)(cid:13)\n\n(cid:13)π(t+1)\n\nj\n\nπ(t)\n\nj\n\n−\n\n(cid:13) 2\n(cid:13)\n\n1 + (cid:13)\n\n(cid:13)π(t)\n\nj −\n\n(cid:17)\n\nπ(t)\n\nj\n\n(cid:13) 2\n(cid:13) 1\n\n4Ψ(t)\n\nj\n\n≤\n\nvia again Pinsker’s inequality.\n\n• γ(t+1)\n\ni\n\n> 0. Then it follows similarly that\n\n(cid:13)\n\n(cid:13)π(κ(t+1)\n\ni\n\nj\n\nπ(κ(t)\n\ni )\n\nj\n\n)\n\n−\n\n(cid:13) 2\n(cid:13) 1 ≤\n\nt−1 (cid:88)\n\nl=t+1−γ(t+1)\n\ni\n\n(cid:13) (cid:13)π(l+1)\n\nj\n\nπ(l)\n\nj\n\n(cid:13) 2\n(cid:13)\n\n1 +\n\n−\n\nt−1 (cid:88)\n\nl=t−γ(t)\n\ni\n\n(cid:13) (cid:13)π(l+1)\n\nj\n\nπ(l)\n\nj\n\n(cid:13) 2\n(cid:13) 1\n\n−\n\n4γ(t+1)\n\ni\n\n≤\n\nt−1 (cid:88)\n\nl=t−γ(t+1)\n\ni\n\nCombining the above two bounds together, we get\n\nΨ(l)\n\nj + 4γ(t)\n\ni\n\nt−1 (cid:88)\n\nΨ(l) j .\n\nl=t−γ(t)\n\ni\n\n(cid:13)\n\n(cid:13)π(κ(t+1)\n\ni\n\nj\n\nπ(κ(t)\n\ni )\n\nj\n\n)\n\n−\n\n(cid:13) 2\n(cid:13) 1 ≤\n\n8Ψ(t)\n\nj + 8γ(t)\n\ni\n\nt−1 (cid:88)\n\nl=t−γ(t)\n\ni\n\nΨ(l)\n\nj + 4γ(t+1)\n\ni\n\nt−1 (cid:88)\n\nl=t−γ(t+1)\n\ni\n\nΨ(l)\n\nj\n\n(32)\n\nwhen t > 0. In view of (30) when t = 0, the above bound (32) holds for all t above inequality into (29) yields\n\n≥\n\n0. Plugging the\n\n(cid:10)log π(t+1)\n\ni\n\n−\n\nlog π(t+1)\n\ni\n\n, π(t+1)\n\ni\n\n2η\n\nA\n\n∥\n\n∥∞\n\n≤\n\n(cid:88)\n\nt−1 (cid:88)\n\nj∈Ni\n\nl=t−γ(t+1)\n\ni\n\n− γ(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:11)\n\nΨ(l)\n\nj + 4η\n\nA\n\n∥\n\n∥∞\n\n(cid:88)\n\nj∈Ni\n\n+ 4η\n\nA\n\n∥\n\n∥∞\n\n(cid:88)\n\nj∈Ni\n\nΨ(t)\n\nj + ηdmax\n\nA\n\n∥\n\n∥∞ KL(cid:0)π(t+1)\n\ni\n\n∥\n\ni\n\nl=t−γ(t) (cid:1).\n\nπ(t+1)\n\ni\n\nt−1 (cid:88)\n\ni Ψ(l) γ(t)\n\nj\n\n(33)\n\nStep 2: bounding (π(t+1) sition\n\ni\n\n−\n\ni,τ )⊤Ai(π(κ(t+1) π⋆\n\ni\n\n)\n\n−\n\nπ⋆\n\nτ ). Let us begin with the following decompo-\n\n(π(t+1)\n\ni\n\n−\n\ni,τ )⊤Ai(π(κ(t+1) π⋆\n\ni\n\n)\n\n−\n\nτ ) = (π(t+1) π⋆\n\ni\n\n− + (π(t+1)\n\ni\n\nπ⋆\n\ni,τ )⊤Ai(π(t+1)\n\n− i,τ )⊤Ai(π(κ(t+1) π⋆\n\ni\n\nπ⋆ τ )\n\n−\n\nπ(t+1)),\n\n(34)\n\n)\n\n−\n\nwhere the second term in the RHS of (34) can be bounded by\n\n(π(t+1)\n\ni\n\n(cid:88)\n\n=\n\nj∈Ni\n\n− (π(t+1)\n\ni\n\n−\n\ni,τ )⊤Ai(π(κ(t+1) π⋆\n\ni\n\n)\n\nπ(t+1))\n\n−\n\ni,τ )⊤Aij(π(κ(t+1) π⋆\n\ni\n\nj\n\n)\n\n−\n\nπ(t+1)\n\nj\n\n)\n\nA\n\n≤ ∥\n\n∥∞\n\n(cid:88)\n\n(cid:13) (cid:13)π(t+1)\n\ni\n\nj∈Ni\n\n1 2 ∥\n\nA\n\n∥∞\n\n(cid:18)\n\n(cid:88)\n\ndmax\n\nj∈Ni\n\n−\n\nτ\n\nτ KL(cid:0)π(t+1)\n\ni\n\nπ⋆\n\ni,τ\n\n(cid:1) +\n\n∥\n\n≤\n\n≤\n\n∥\n\nA\n\n∥∞ dmax\n\nπ⋆\n\ni,τ\n\n(cid:13) (cid:13)1\n\n(cid:13)\n\n(cid:13)π(κ(t+1)\n\ni\n\nj\n\nπ(t+1)\n\nj\n\n(cid:13) (cid:13)1\n\n)\n\n−\n\n(cid:13) (cid:13)π(t+1)\n\ni\n\nπ⋆\n\ni,τ\n\n(cid:13) 2\n(cid:13)\n\n1 +\n\ndmax\n\nA\n\n∥ τ\n\n∥∞\n\n−\n\n(cid:13)\n\n(cid:13)π(κ(t+1)\n\ni\n\nj\n\n(cid:19)\n\nπ(t+1)\n\nj\n\n(cid:13) 2\n(cid:13) 1\n\n)\n\n−\n\nA\n\n∥ 2τ\n\n2 ∞\n∥\n\n(cid:88)\n\nj∈Ni\n\n(cid:13)\n\n(cid:13)π(κ(t+1)\n\ni\n\nj\n\nπ(t+1)\n\nj\n\n(cid:13) 2\n(cid:13)\n\n1.\n\n)\n\n−\n\nFollowing similar deduction of (32) for the second term, we attain\n\n(π(t+1)\n\ni\n\n−\n\ni,τ )⊤Ai(π(κ(t+1) π⋆\n\ni\n\n)\n\nπ(t+1))\n\n−\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nτ KL(cid:0)π(t+1)\n\ni\n\nπ⋆\n\ni,τ\n\n(cid:1) +\n\n∥\n\n≤\n\nA\n\n4dmax ∥\nτ\n\n2 ∞\n∥\n\n(cid:88)\n\n(cid:16)\n\nΨ(t)\n\nj +\n\nj∈Ni\n\nt−1 (cid:88)\n\nγ(t+1)\n\ni\n\nΨ(l)\n\nj\n\n(cid:17) .\n\nl=t−γ(t+1)\n\ni\n\nPlugging the above inequality back to (34) results in\n\n(π(t+1)\n\ni\n\n−\n\ni,τ )⊤Ai(π(κ(t+1) π⋆\n\ni\n\n)\n\n− + τ KL(cid:0)π(t+1)\n\ni\n\nπ⋆ τ )\n\n≤\n\n(π(t+1)\n\ni\n\nπ⋆\n\ni,τ )⊤Ai(π(t+1)\n\nπ⋆ τ )\n\n−\n\n− 4dmax ∥\nτ\n\nA\n\n2 ∞\n\n∥\n\nπ⋆\n\ni,τ\n\n(cid:1) +\n\n∥\n\n(cid:88)\n\n(cid:16)\n\nΨ(t)\n\nj +\n\nj∈Ni\n\nt−1 (cid:88)\n\nγ(t+1)\n\ni\n\nΨ(l)\n\nj\n\n(cid:17)\n\n.\n\nl=t−γ(t+1)\n\ni\n\nStep 3: combining the bounds. For simplicity, we introduce the short-hand notation\n\ncτ = 1 +\n\ndmax\n\nA\n\n∥ τ\n\n∥∞\n\nand\n\ncA = dmax\n\nA\n\n∥\n\n∥∞ .\n\n(35)\n\n(36)\n\nCombining (33) and (35) into (28), and summing over i π(t)(cid:1) τ ∥ ητ )KL(cid:0)π(t+1)\n\nητ )KL(cid:0)π⋆\n\n− (1\n\n(1\n\nV gives\n\n∈\n\n≥\n\n−\n\n4η\n\nA\n\n∥\n\n∥∞\n\n−\n\nπ(t)(cid:1) + (1 (cid:32) t−1 (cid:88)\n\n∥ (cid:88)\n\n−\n\n(cid:88)\n\n2ηcA)KL(cid:0)π(t+1)\n\nπ(t+1)(cid:1) + KL(cid:0)π⋆\n\n∥\n\nt−1 (cid:88)\n\nπ(t+1)(cid:1) (cid:33)\n\nτ ∥ j + cτ Ψ(t)\n\nj\n\ni Ψ(l) γ(t)\n\ncτ γ(t+1)\n\ni\n\nΨ(l)\n\nj +\n\ni∈V\n\nj∈Ni\n\nl=t−γ(t+1)\n\ni\n\nl=t−γ(t)\n\ni\n\nKL(cid:0)π⋆\n\nτ ∥\n\n≥\n\nπ(t+1)(cid:1) + (1\n\n4η\n\nA\n\n∥\n\n∥∞\n\n−\n\n(cid:88)\n\n(cid:88)\n\ni∈V\n\nj∈Ni\n\nwhere we make use of the fact\n\n4ηcA(cτ + 1)) Ψ(t) (cid:32)\n\n−\n\nt−1 (cid:88)\n\ncτ\n\nγ(t+1)\n\ni\n\nΨ(l)\n\nj +\n\nl=t−γ(t+1)\n\ni\n\nl=t−γ(t)\n\ni\n\nt−1 (cid:88)\n\n(cid:33)\n\n,\n\ni Ψ(l) γ(t)\n\nj\n\n(37)\n\n(cid:88)\n\ni∈V\n\n(π(t+1)\n\ni\n\n−\n\nπ⋆\n\ni,τ )⊤Ai(π(t+1)\n\nπ⋆\n\nτ ) = 0\n\n−\n\nfrom Lemma 1 in the first inequality, and the second inequality uses the relation\n\n(cid:88)\n\n(cid:88)\n\ni∈V\n\nj∈Ni\n\nΨ(t)\n\nj =\n\n(cid:88)\n\ni∈V\n\ndiΨ(t)\n\ni ≤\n\ndmaxΨ(t).\n\nτ ∥\n\nStep 4: KL(cid:0)π⋆ π(t+1)(cid:1). Recall that we use subscript E γ(t) γ(t) = {\nserving that π(l+1) l), we have E(t\n\nfinishing up via averaging the delay. We now evaluate the expectation of ] to represent the conditional expectation given γ(t) [ ·\n\ni∈V . We shall first control the conditional expectation of the last term in (37). Ob1. Using the definition of\n\nare independent of γ(t)\n\ni and l\n\n, π(l)\n\nfor j\n\ni }\n\n∈ N\n\n−\n\n≤\n\nt\n\nj\n\nj\n\ni\n\n−\n\n(cid:88)\n\n(cid:88)\n\ni∈V\n\nj∈Ni\n\n(cid:34)\n\nE\n\nγ(t)\n\nγ(t)\n\ni\n\nt−1 (cid:88)\n\n(cid:35)\n\nΨ(l)\n\nj\n\n=\n\nl=t−γ(t)\n\ni\n\n(cid:88)\n\nt−1 (cid:88)\n\n(cid:88)\n\ni∈V\n\nl=0\n\nj∈Ni\n\nE\n\nt−l≤γ(t)\n\ni\n\n(cid:104) i Ψ(l) γ(t)\n\nj\n\n(cid:105)\n\nt−1 (cid:88)\n\nl=0\n\nt−1 (cid:88)\n\nl=0\n\n≤\n\n=\n\nE(t\n\n−\n\n(cid:88)\n\n(cid:88)\n\nl)\n\nΨ(l)\n\nj\n\ni∈V\n\nj∈Ni\n\nE(t\n\nl)\n\n−\n\n(cid:88)\n\n(cid:88)\n\ni∈V\n\nj∈Ni\n\nΨ(l)\n\ni\n\ndmax\n\n≤\n\nt−1 (cid:88)\n\nl=0\n\nE(t\n\n(cid:88)\n\nl)\n\ni∈V\n\n−\n\nΨ(l)\n\ni = dmax\n\nt−1 (cid:88)\n\nl=0\n\nE(t\n\n−\n\nl)Ψ(l),\n\n(38)\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nwhere the second line follows from the definition of E(t ilar argument to bound (cid:80)\n\nγ(t+1)\n\n− (cid:80)t−1\n\n(cid:80)\n\nE\n\n(cid:104)\n\ni∈V\n\nj∈Ni\n\nγ(t+1)\n\ni\n\n(cid:105)\n\nΨ(l)\n\nj\n\nl=t−γ(t+1)\n\ni\n\nl) in Assumption 1. Applying a sim-\n\n, and taking expectation of\n\nγ(t), γ(t+1) on both sides of (37), we get\n\nητ )E\n\nγ(t)\n\n(1\n\n−\n\n(cid:104)\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t)(cid:1)(cid:105)\n\n≥\n\nE\n\nγ(t),γ(t+1)\n\n(cid:104)\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1) + (1\n\n−\n\n4cA(cτ + 1))Ψ(t)(cid:105)\n\n4ηcA(cτ + 1)\n\n−\n\nt−1 (cid:88)\n\nl=0\n\nl)Ψ(l).\n\nE(t\n\n−\n\nTaking expectation on both sides over all the delays yields\n\n(cid:104)\n\nητ )E\n\nKL(cid:0)π⋆\n\nπ(t)(cid:1)(cid:105)\n\n(1\n\n≥\n\n− E\n\n(cid:104) KL(cid:0)π⋆\n\nτ ∥\n\nτ ∥ π(t+1)(cid:1)(cid:105)\n\n(cid:104) + E\n\n(1\n\n(cid:124)\n\n−\n\n4ηcA(cτ + 1)) Ψ(t)\n\n−\n\n4ηcA(cτ + 1)\n\n(cid:123)(cid:122) =:U (t)\n\n(cid:88)t−1 l=0\n\nE(t\n\n(cid:105) .\n\n−\n\nl)Ψ(l) (cid:125)\n\n(39)\n\nTelescoping over t = 0, 1, . . . , T , we get\n\n(1\n\n−\n\nητ )T +1KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1)\n\n(cid:104)\n\nE\n\nKL(cid:0)π⋆\n\nτ ∥\n\n≥\n\nπ(T +1)(cid:1)(cid:105)\n\n+\n\nT (cid:88)\n\nt=0\n\n(1\n\n−\n\nητ )T −tE\n\n(cid:104)\n\nU (t)(cid:105)\n\n,\n\n(40)\n\nwhich leads to the desired bound if\n\n(41)\n\n(42a)\n\nt (cid:88)\n\nt=0\n\n(1\n\n−\n\nητ )T −tE\n\n(cid:104)\n\nU (t)(cid:105)\n\n0.\n\n≥\n\nProof of (41). To begin, notice that with the choice of the learning rate\n\n0 < η\n\nmin\n\n≤\n\n(cid:40)\n\nτ\n\n24d2\n\nA\n\n∞ (L + 1)\n\n2 ∥\n\nmax ∥\n\n(cid:41)\n\n1\n\n,\n\nζ\n\n,\n\n− ζτ\n\nit follows that\n\nand\n\n4ηcA(cτ + 1)(L + 1) < 4\n\n1\n\nητ ≤\n\n1\n\n−\n\nζ\n\nτ\n\n24d2\n\nmax ∥ τ\n6dmax\n\nA\n\n=\n\n∥\n\n∥∞\n\nA\n\n2 ∥\n\n∞ (L + 1) (cid:18) dmax\n\n2 +\n\ndmax\n\nA\n\n∥\n\nA\n\n∥∞\n\n∥ τ\n\n∥∞ (cid:19)\n\n=\n\n(cid:18)\n\n2 +\n\ndmax\n\n(cid:19)\n\nA\n\n∥ τ\n\n∥∞\n\n(L + 1)\n\nτ 3dmax\n\n∥\n\nA\n\n∥∞\n\n+\n\n1 6 ≤\n\n1 2\n\n(42b)\n\nas τ\n\ndmax\n\nA\n\n∥\n\n∥∞. Both of these relations will be useful in our follow-up analysis.\n\n≤\n\nNow, taking the definition of U (t) (cf. (39)), we have\n\nT (cid:88)\n\nt=0\n\n(1\n\n−\n\nητ )T −tU (t) =\n\nT (cid:88)\n\n(1\n\nt=0\n\n−\n\n(cid:34)\n\nητ )T −t\n\n(1\n\n−\n\n4ηcA(cτ + 1)) Ψ(t)\n\n4ηcA(cτ + 1)\n\n−\n\nt−1 (cid:88)\n\nl=0\n\nE(t\n\n−\n\n(cid:35)\n\nl)Ψ(l)\n\n,\n\nwhere the second half of the RHS can be further controlled via\n\nT (cid:88)\n\n(1\n\nt=0\n\n−\n\nητ )T −t\n\nt−1 (cid:88)\n\nl=0\n\nl)Ψ(l) =\n\nE(t\n\n−\n\n≤\n\nT (cid:88)\n\n(1\n\nΨ(t)\n\nl=t+1\n\n−\n\nητ )T −lE(l\n\nt)\n\n−\n\nΨ(t)\n\nT −t (cid:88)\n\nl′=0\n\n(1\n\n−\n\nητ )T −(t+l′)E(l′)\n\nT (cid:88)\n\nt=0\n\nT (cid:88)\n\nt=0\n\n20\n\nPublished as a conference paper at ICLR 2023\n\n=\n\n≤\n\n=\n\nT (cid:88)\n\nt=0\n\nT (cid:88)\n\nt=0\n\nT (cid:88)\n\nt=0\n\n(1\n\n−\n\n(1\n\n−\n\n(1\n\n−\n\nητ )−l′\n\nE(l′)\n\n−\n\nητ )T −tΨ(t)\n\nT −t (cid:88)\n\n(1\n\nητ )T −tΨ(t)\n\nl′=0\n\n∞ (cid:88)\n\nl=0\n\nητ )T −tLΨ(t),\n\nζ lE(l)\n\nwhere the first line follows by changing the order of summation, the second line follows from the change of variable l′ = l t, and the last line follows from (42a) and the definition of L in Assumption 1. Plugging the above relation back leads to\n\n−\n\nT (cid:88)\n\n(1\n\nt=0\n\n−\n\nητ )T −tU (t)\n\nT (cid:88)\n\nt=0\n\nT (cid:88)\n\nt=0\n\n≥\n\n≥\n\nητ )T −t [(1\n\n(1\n\n−\n\n−\n\n4ηcA(cτ + 1))\n\n−\n\n4ηcA(cτ + 1)L] Ψ(t)\n\n1 2\n\n(1\n\n−\n\nητ )T −tΨ(t)\n\n0,\n\n≥\n\n(43)\n\nwhere the second line results from (42b).\n\nBounding the term KL(cid:0)π⋆\n\nπ(t+1)(cid:1). With a similar deduction of (19), we get\n\nτ ∥ ητ )KL(cid:0)π⋆\n\nτ ∥\n\n(1\n\n−\n\n= KL(cid:0)π⋆\n\nπ(t+1)(cid:1) + (1\n\nτ ∥ Following the similar argument of (35), we have\n\n−\n\nπ(t)(cid:1) + η\n\n(cid:88)\n\n(π(t+1)\n\ni\n\ni∈V\n\n− ητ )KL(cid:0)π(t+1)\n\nτ )⊤Ai(π(κ(t) π⋆\n\ni )\n\nπ⋆ τ )\n\n−\n\nπ(t)(cid:1) + ητ KL(cid:0)π(t+1)\n\n(cid:1).\n\nπ⋆\n\nτ\n\n∥\n\n∥\n\n(44)\n\n(π(t+1)\n\ni\n\n−\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\nπ⋆ τ )\n\n−\n\n≤\n\n(π(t+1)\n\ni\n\nπ⋆\n\ni,τ )⊤Ai(π(t+1)\n\nπ⋆ τ )\n\n−\n\n+\n\nτ 2\n\nKL(cid:0)π(t+1)\n\ni\n\nπ⋆\n\ni,τ\n\n(cid:1) +\n\n∥\n\n− 8dmax ∥\nτ\n\nA\n\n2 ∞\n\n∥\n\n(cid:88)\n\n(cid:16)\n\nΨ(t)\n\nj +\n\nj∈Ni\n\nt−1 (cid:88)\n\ni Ψ(l) γ(t)\n\nj\n\n(cid:17)\n\n.\n\nl=t−γ(t)\n\ni\n\nSumming over i\n\n∈\n\nV and plugging into (44) yields\n\n(1\n\n−\n\nητ )KL(cid:0)π⋆\n\nτ ∥\n\nπ(t)(cid:1) +\n\n8ηdmax τ\n\nA\n\n∥\n\n2 ∞\n\n∥\n\n(cid:88)\n\n(cid:16)\n\nΨ(t)\n\nj +\n\nt−1 (cid:88)\n\n(cid:17)\n\ni Ψ(l) γ(t)\n\nj\n\n(i,j)∈E\n\ni\n\nl=t−γ(t) KL(cid:0)π(t+1)\n\n(cid:1)\n\nπ⋆\n\nτ\n\n∥\n\nKL(cid:0)π⋆\n\nτ ∥\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1) + (1 ητ 2\n\nπ(t+1)(cid:1) +\n\n≥\n\n≥\n\nητ )KL(cid:0)π(t+1)\n\n− KL(cid:0)π(t+1)\n\n∥ (cid:1).\n\nπ⋆\n\nτ\n\n∥\n\nπ(t)(cid:1) +\n\nητ 2\n\nTaking expectation on both sides over all delays and using (38) leads to\n\nητ )E\n\n(1\n\n−\n\n(cid:104)\n\nE\n\nKL(cid:0)π⋆\n\nτ ∥\n\n≥\n\n(cid:104) KL(cid:0)π⋆\n\nπ(t)(cid:1)(cid:105)\n\nτ ∥ π(t+1)(cid:1)(cid:105)\n\n+\n\n8ηd2\n\nmax ∥ τ\n\nA\n\n(cid:34)\n\n2 ∞\n\n∥\n\nE\n\nΨ(t) +\n\n(cid:35)\n\nt−1 (cid:88)\n\nl=0\n\nl)Ψ(l)\n\nE(t\n\n−\n\n(cid:104)\n\nE\n\nKL(cid:0)π(t+1)\n\n(cid:1)(cid:105)\n\n.\n\nπ⋆\n\nτ\n\n∥\n\n(45)\n\n+\n\nητ 2\n\nNotice that with the choice of the learning rate\n\n0 < η\n\nmin\n\n≤\n\n(cid:40)\n\nτ\n\n24d2\n\nA\n\n∞ (L + 1)\n\n2 ∥\n\nmax ∥\n\n(cid:41)\n\n1\n\n,\n\nζ\n\n,\n\n− ζτ\n\nwe have\n\n8(L + 1)ηd2\n\nmax ∥\n\nτ\n\nA\n\n2 ∞\n∥\n\n1 2\n\n≤\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nand\n\n(1\n\n−\n\nητ )t+1KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1)\n\n1 2\n\nt (cid:88)\n\nl=0\n\n(1\n\n−\n\n≥\n\nητ )t−lE\n\n(cid:104)\n\nΨ(l)(cid:105)\n\nby combining (43) and (40). It follows that\n\n(cid:104)\n\nΨ(t)(cid:105)\n\nE\n\n2(1\n\n−\n\n≤\n\nητ )t+1KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1)\n\nand\n\n(cid:34)t−1 (cid:88)\n\nE\n\nl=0\n\nl)Ψ(l)\n\nE(t\n\n−\n\n(cid:35) (i)\n\n≤\n\nE\n\nE\n\n≤\n\n(cid:34)t−1 (cid:88)\n\n(1\n\nl=0 (cid:34)t−1 (cid:88)\n\nl=0\n\nητ )t−lΨ(l)\n\nE(t\n\n·\n\n−\n\nl)ζ t−l\n\n−\n\nητ )t−lΨ(l)\n\n(1\n\n−\n\nt−1 (cid:88)\n\nl=0\n\nl)ζ t−l\n\nE(t\n\n−\n\n(cid:35)\n\n(cid:35)\n\nwhere (i) is by the bound (1 ≤\nthe above inequalities into (45) leads to\n\nητ )−1\n\n−\n\n(ii)\n\n2L(1\n\nητ )t+1KL(cid:0)π⋆\n\nπ(0)(cid:1),\n\n−\n\n≤ ζ and (ii) uses the definition of L in Assumption 1. Plugging\n\nτ ∥\n\n(cid:104)\n\nητ )E\n\nKL(cid:0)π⋆\n\nπ(t)(cid:1)(cid:105)\n\n(1\n\n≥\n\n− E\n\n(cid:104)\n\nKL(cid:0)π⋆\n\nτ ∥\n\nτ ∥ π(t+1)(cid:1)(cid:105)\n\n+\n\n+ (1\n\nητ 2\n\nE\n\nητ )t+1KL(cid:0)π⋆\n\nπ(0)(cid:1)\n\n− (cid:104) KL(cid:0)π(t+1)\n\nτ ∥ (cid:1)(cid:105)\n\nπ⋆\n\nτ\n\n.\n\n∥\n\nThen from (40) we have (cid:104)\n\nE\n\nKL(cid:0)π⋆\n\nπ(t+1)(cid:1)(cid:105)\n\nτ ∥\n\n(cid:104)\n\nE\n\n≤\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1)(cid:105)\n\n(1\n\n−\n\n≤ = 2(1\n\nητ )t+1KL(cid:0)π⋆ τ ∥ ητ )t+1KL(cid:0)π⋆ τ ∥\n\n−\n\nKL(cid:0)π(t+1)\n\n(cid:1)(cid:105)\n\nπ⋆\n\nτ\n\n∥ ητ )t+1KL(cid:0)π⋆\n\n(cid:104)\n\nE\n\n+\n\nητ 2\nπ(0)(cid:1) + (1 π(0)(cid:1).\n\n−\n\nτ ∥\n\nπ(0)(cid:1)\n\n(46)\n\nBounding the QRE-Gap. Combining (27) and (46), we have\n\nE\n\n(cid:105) (cid:104) QRE-Gapτ (π(t+1))\n\n≤\n\n≤\n\n≤\n\n(cid:104)\n\nτ E\n\nKL(cid:0)π(t+1)\n\n∥ KL(cid:0)π(t+1)\n\n2 η\n\n(cid:16) ητ 2\n\n(cid:104)\n\nE\n\n(cid:1)(cid:105)\n\nπ⋆\n\nτ\n\n+\n\nd2\n\nmax ∥ τ\n\nA\n\n2 ∞\n∥\n\nE\n\n(cid:104) KL(cid:0)π⋆\n\nπ(t+1)(cid:1)(cid:105)\n\n(cid:1)(cid:105)\n\nπ⋆\n\nτ\n\n(cid:104)\n\n+ E\n\nKL(cid:0)π⋆\n\nτ ∥\n\n∥\n\nτ ∥ π(t+1)(cid:1)(cid:105)(cid:17)\n\n4(1\n\nητ )t+1 η\n\n−\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1),\n\nwhere the second line uses the learning rate bound\n\n2 η\n\n>\n\n24d2\n\nmax ∥\n\nA ∥\nτ\n\n2\n\n∞ (L + 1)\n\n>\n\nd2\n\nmax ∥ τ\n\nA\n\n2 ∞\n∥\n\n.\n\nC REGRET ANALYSIS OF SINGLE-TIMESCALE OMWU\n\nFor completeness, we also provide the regret analysis of single-timescale OMWU in both synchronous and asynchronous settings, which might be of independent interest. To begin, for τ 0, the regret for each player i\n\nV is defined as\n\n≥\n\n∈\n\nRegreti,τ\n\n(cid:0)T (cid:1) = max\n\nπi∈∆(Si)\n\nT (cid:88)\n\nt=1\n\nui,τ (πi, π(t) −i)\n\nT (cid:88)\n\nt=1\n\n−\n\nui,τ (π(t)),\n\n(47)\n\nwhich measures the performance gap compared to the optimal fixed strategy in hindsight for player i, when the rest of the players follow the strategies derived from Algorithm 1.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nSynchronous setting. We begin with the following no-regret guarantee of single-timescale OMWU in the synchronous setting. Theorem 5 (No-regret without delays). Suppose all players i in Algorithm 1 with the initialization π(0) 1, it holds that 4dmax∥A∥∞+4τ . Then, for T\n\n|Si| 1 and the learning rate obeys 0 < η\n\nV follow single-timescale OMWU\n\ni = 1\n\n≤\n\n∈\n\n1\n\n≥\n\nRegreti,τ\n\n(cid:0)T (cid:1)\n\n1 η\n\n≤\n\nlog\n\nSi |\n\n|\n\n+ 16η degi ∥\n\nA\n\n2 ∞\n∥\n\n(cid:88)\n\nk∈V\n\nlog\n\nSk\n\n. |\n\n|\n\nBy optimizing the learning rate η, Theorem 5 suggests that the regret is bounded by\n\nmax i∈V\n\nRegreti,τ\n\n(cid:16)\n\n(cid:0)T (cid:1) ≲ (cid:101) O\n\nA\n\n∥\n\n∥∞\n\n(cid:112)\n\nndmax\n\n(cid:17)\n\nup to logarithmic factors. Compared with the OMD method for multi-agent games in Anagnostides et al. (2022), which only provided the regret bound for τ = 0, our bound is more general by allowing entropy regularization. Moreover, our bound is tighter by a factor of n/dmax by exploiting the graph connectivity pattern, which is significant for large sparse graphs.\n\nAsynchronous setting. We next move to the asynchronous case, and show that single-timescale OMWU continues to enjoy no-regret learning as long as the delays have finite second-order moments. Assumption 4 (Random delays). Recall the definition of E(l) in (12). There exists some constant σ > 0, such that E\n\nσ2, for all t\n\n(cid:104) (γ(t)\n\n0 and i\n\n(cid:80)∞\n\nV .\n\ni )2(cid:105)\n\nl=0 E(l)\n\n≤\n\n≤\n\n≥\n\n∈\n\nClearly Assumption 4 is weaker than Assumption 1, since it only requires the second-order moments to be finite, instead of an exponential decay of γ(t) Theorem 6 (No-regret with random delays). Under Assumption 4, suppose all players i single-timescale OMWU in Algorithm 1 with the initialization π(0) parameter τ < min\n\nV follow |Si| 1, the regularization . Then, for\n\n, and the learning rate obeys 0 < η\n\n. We have the following theorem.\n\ni = 1\n\n1,\n\nA\n\n∈\n\ni\n\nτ max∥A∥2\n\n24d2\n\n∞(σ2+1)\n\n≤\n\n{\n\n∥\n\n∥∞}\n\nT\n\n1, it holds that\n\n≥ E (cid:2)Regreti,τ\n\n(cid:0)T (cid:1)(cid:3)\n\n1 η\n\n≤\n\nlog\n\nSi\n\n|\n\n|\n\n+ 8dmax\n\nA\n\n∥\n\n∥∞\n\n(cid:18) dmax\n\n(cid:19)\n\n∥∞\n\n+ 2\n\nA\n\n∥ τ\n\n(σ2 + 1)\n\n(cid:88)\n\ni∈V\n\nlog\n\nSi\n\n. |\n\n|\n\n(48)\n\nTheorem 6 guarantees that the iterate among\n\nπ(t)\n\n{\n\n}\n\nt≥1 enjoys a regret bound on the order of (cid:33)\n\n(cid:32)\n\n(cid:0)T (cid:1)(cid:3) ≲ (cid:101) O\n\nσ2ndmax τ\n\nA\n\n2 ∞\n∥\n\n∥\n\nE (cid:2)Regreti,τ\n\nmax i∈V\n\nby optimizing the learning rate η.\n\nC.1 PROOF OF THEOREM 5\n\nRecall the expression of the regret\n\nRegreti,τ\n\n(cid:0)T (cid:1) = max\n\nπi∈∆(Si)\n\nRegreti,τ\n\n(cid:0)πi, T (cid:1),\n\nwhere\n\nRegreti,τ\n\n(cid:0)πi, T (cid:1) :=\n\nT (cid:88)\n\nt=1\n\nui,τ (πi, π(t) −i)\n\nT (cid:88)\n\nt=1\n\n−\n\nui,τ (π(t))\n\n=\n\nT (cid:88)\n\nt=0\n\n(cid:16)(cid:10)πi\n\n−\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)(cid:11) + τ\n\n(πi)\n\nτ\n\nH\n\n−\n\nH\n\n(π(t+1)\n\ni\n\n(cid:17) )\n\n.\n\n(49)\n\nTherefore, it is sufficient to bound Regreti,τ following useful lemma whose proof can be found in Appendix E.4.\n\n∈\n\n(cid:0)πi, T (cid:1) for any πi\n\n∆(Si). To begin, we record the\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nLemma 4. For any τ\n\n0 and T\n\n≥\n\n≥\n\n0, we have\n\nRegreti,τ\n\n(cid:0)T + 1(cid:1) =\n\n(cid:88)\n\ni∈V\n\n(cid:88)\n\ni∈V\n\nmax πi∈∆(Si)\n\n(cid:16)(cid:10)πi\n\nT (cid:88)\n\nt=0\n\n−\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)(cid:11) + τ\n\n(πi)\n\nτ\n\nH\n\n−\n\nH\n\n(π(t+1)\n\ni\n\n(cid:17) )\n\n0.\n\n≥\n\nLet us now proceed with the following regret decomposition\n\n(cid:10)πi −\n= (cid:10)πi\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)(cid:11) + τ\n\n(πi)\n\ni\n\nπ(t+1) −\n(cid:10)π(t+1)\n\ni\n\n−\n\nH , Aiπ(t+1)(cid:11) + τ\n\nH , Aiπ(t+1)\n\nπ(t+1)\n\ni\n\n−\n\ni\n\nτ\n\n(π(t+1) −\nH (πi) + (cid:10)π(t+1) Aiπ(t)(cid:11).\n\ni\n\n−\n\n)\n\n−\n\nπ(t+1)\n\ni\n\n, Aiπ(t)(cid:11)\n\n(π(t+1)\n\ni\n\n)\n\nτ\n\nH\n\n−\n\n(50)\n\nWe proceed to bound each term on the RHS of (50).\n\n• To begin, note that log π(t+1)\n\ni\n\n1= (1\n\n−\n\nητ ) log π(t)\n\ni + ηAiπ(t+1). The first term in (50) can then be\n\nwritten as (cid:10)πi\n\ni\n\nπ(t+1)\n\n, Aiπ(t+1)(cid:11) + τ\n\n(πi)\n\n− 1\n(cid:10)log π(t+1) η\n(cid:18) 1\n\n(cid:19)\n\ni\n\nτ\n\nη −\n\n− KL(cid:0)πi\n\n=\n\n=\n\nH , πi\n\nlog π(t)\n\ni\n\n(cid:1)\n\nπ(t)\n\ni\n\n∥\n\n−\n\nπ(t+1)\n\ni\n\n− (cid:16) 1\nKL(cid:0)πi η\n\n(cid:11) + τ (cid:10)log π(t)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:11)\n\n, πi\n\n−\n\nτ (cid:10)log πi, πi\n\n(cid:11)\n\nπ(t+1)\n\ni\n\n(cid:1) + KL(cid:0)π(t+1)\n\ni\n\n∥\n\n∥\n\nπ(t)\n\ni\n\nτ (cid:10)log π(t)\n\ni\n\n, π(t+1)\n\ni\n\n(cid:11),\n\n−\n\n(51)\n\n− (cid:1)(cid:17)\n\nwhere the second step is derived from the definition of KL divergence.\n\n• Similarly, the second term in (50) has the form\n\ni\n\n(cid:10)π(t+1) (cid:16) 1\nη\n\n=\n\nπ(t+1)\n\ni\n\n− KL(cid:0)π(t+1)\n\ni\n\n+ τ (cid:10)log π(t)\n\ni\n\n, Aiπ(t)(cid:11)\n\n(cid:1)\n\nπ(t)\n\ni\n\n∥ , π(t+1)\n\n− (cid:11).\n\ni\n\nτ\n\n(π(t+1)\n\n)\n\ni\n\n− H\nKL(cid:0)π(t+1)\n\ni\n\n(cid:1)(cid:17)\n\nπ(t+1)\n\ni\n\n∥\n\n(cid:18) 1\n\n−\n\nη −\n\n(cid:19)\n\nτ\n\nKL(cid:0)π(t+1)\n\ni\n\n(cid:1)\n\nπ(t)\n\ni\n\n∥\n\n(52)\n\n• Moving to the third term on the RHS of (50), we first make the following claim, which shall be\n\nproven at the end of this proof:\n\n(cid:13) (cid:13)π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:13) (cid:13)1 ≤\n\n−\n\nη(cid:13) (cid:13)Aiπ(t+1)\n\nAiπ(t)(cid:13)\n\n(cid:13)∞.\n\n−\n\n(53)\n\nWith (53) in place, we have\n\n(cid:10)π(t+1)\n\ni\n\n−\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)\n\n−\n\nAiπ(t)(cid:11)\n\n−\n\n(cid:88)\n\nj∈Ni\n\nA\n\n∥\n\n∥∞\n\n(cid:13) (cid:13)π(t+1)\n\ni\n\n−\n\nπ(t+1)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:13) (cid:13)π(t+1)\n\nj\n\nπ(t)\n\nj\n\n(cid:13) (cid:13)1\n\n−\n\n(cid:88)\n\nη\n\nj∈Ni\n\nA\n\n∥\n\n∥∞\n\n(cid:13) (cid:13)Ai(π(t+1)\n\nπ(t))(cid:13)\n\n(cid:13)∞\n\n(cid:13) (cid:13)π(t+1)\n\nj\n\nπ(t)\n\nj\n\n(cid:13) (cid:13)1\n\n−\n\n−\n\n≤\n\n≤\n\nη\n\nA\n\n∥\n\n2 ∞\n\n∥\n\n≤\n\n(cid:16) (cid:88)\n\nj∈Ni\n\n(cid:13) (cid:13)π(t+1)\n\nj\n\n−\n\nπ(t)\n\nj\n\n(cid:13) (cid:13)1\n\n(cid:17)2\n\n.\n\nThe latter term can be further bounded by\n\n(cid:16) (cid:88)\n\nj∈Ni\n\n(cid:13) (cid:13)π(t+1)\n\nj\n\n(cid:17)2\n\nπ(t)\n\nj\n\n(cid:13) (cid:13)1\n\n−\n\n≤\n\n≤\n\n≤\n\ndegi\n\n(cid:88)\n\n(cid:13) (cid:13)π(t+1)\n\nj\n\nj + π(t) π(t)\n\nj −\n\n−\n\nπ(t)\n\nj\n\n(cid:13) 2\n(cid:13) 1\n\n(cid:16)(cid:13)\n\n(cid:13)π(t+1)\n\nj\n\nπ(t)\n\nj\n\n−\n\n(cid:13) 2\n(cid:13)\n\n1 + (cid:13)\n\n(cid:13)π(t)\n\nj −\n\n(cid:17)\n\nπ(t)\n\nj\n\n(cid:13) 2\n(cid:13) 1\n\nKL(cid:0)π(t+1)\n\nj\n\nπ(t)\n\nj\n\n∥\n\n(cid:1) + KL(cid:0)π(t)\n\nj ∥\n\n(cid:1)(cid:17)\n\n,\n\nπ(t)\n\nj\n\nj∈Ni\n\n(cid:88)\n\n2 degi\n\n4 degi\n\nj∈Ni (cid:88)\n\n(cid:16)\n\nj∈Ni\n\n24\n\nPublished as a conference paper at ICLR 2023\n\n2 a + b where it follows respectively from Cauchy-Schwarz inequality, ∥\nPinsker’s inequality. Plugging this into the previous inequality leads to\n\n∥\n\n1 ≤\n\n2(cid:0)\n\na\n\n1 +\n\n2 ∥\n\n∥\n\n(cid:1), and\n\n2 1\n\nb\n\n∥\n\n∥\n\n(cid:10)π(t+1)\n\ni\n\n−\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)\n\n−\n\nAiπ(t)(cid:11)\n\n−\n\n4η degi ∥\n\nA\n\n≤\n\n2 ∞\n∥\n\n(cid:88)\n\nj∈Ni\n\n(cid:0)KL(cid:0)π(t+1)\n\nj\n\nπ(t)\n\nj\n\n∥\n\n(cid:1) + KL(cid:0)π(t)\n\nj ∥\n\nπ(t)\n\nj\n\n(cid:1)(cid:1).\n\n(54)\n\nPlugging (51), (52), and (54) into (50) yields , Aiπ(t+1)(cid:11) + τ (cid:10)πi\n\n(πi)\n\n(π(t+1)\n\n)\n\ni\n\nπ(t+1) (cid:16)\n\nKL(cid:0)πi\n\n− 1\nη\n\n∥ τ KL(cid:0)πi\n\n−\n\nπ(t)\n\ni\n\n(cid:1)\n\n−\n\nH KL(cid:0)πi\n\n− π(t+1)\n\ni\n\n∥\n\nπ(t)\n\ni\n\n(cid:1) + 4η degi ∥\n\nA\n\n∥\n\n2 ∞\n\n∥\n\nτ H\n(cid:1)(cid:17)\n\ni 1\nη −\n(cid:88)\n\nj∈Ni\n\n≤\n\nKL(cid:0)π(t+1)\n\ni\n\n∥ KL(cid:0)π(t+1)\n\n(cid:16)\n\nj\n\nπ(t+1)\n\ni\n\nπ(t)\n\nj\n\n∥\n\n(cid:18) 1\n\n(cid:1)\n\n−\n\nη − (cid:1) + KL(cid:0)π(t)\n\n(cid:1)(cid:17)\n\n.\n\nπ(t)\n\nj\n\nj ∥\n\n(cid:19)\n\nτ\n\nKL(cid:0)π(t+1)\n\ni\n\n(cid:1)\n\nπ(t)\n\ni\n\n∥\n\nTelescoping the sum over t = 0, 1, . . . , T leads to\n\nRegreti,τ\n\n(cid:0)πi, T + 1(cid:1)\n\n1 η\n\n≤\n\nKL(cid:0)πi\n\n(cid:1)\n\nπ(0)\n\ni\n\n∥\n\n−\n\n1 η\n\nT (cid:88)\n\nt=0\n\nKL(cid:0)π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:1)\n\n∥\n\n(cid:18) 1\n\n−\n\nη −\n\n(cid:19) T\n\n(cid:88)\n\nτ\n\nt=0\n\nKL(cid:0)π(t+1)\n\ni\n\n(cid:1)\n\nπ(t)\n\ni\n\n∥\n\n+ 4η degi ∥\n\nA\n\n2 ∞\n∥\n\nT (cid:88)\n\n(cid:88)\n\nt=0\n\nj∈Ni\n\n(cid:16)\n\nKL(cid:0)π(t+1)\n\nj\n\nπ(t)\n\nj\n\n∥\n\n(cid:1) + KL(cid:0)π(t)\n\nj ∥\n\n(cid:1)(cid:17)\n\nπ(t)\n\nj\n\n(55)\n\n1 η\n\n≤\n\nlog\n\nSi\n\n+ 4η degi ∥\n\nA\n\n∥\n\n|\n\n|\n\n2 ∞\n\nT (cid:88)\n\n(cid:16)\n\nt=0\n\nKL(cid:0)π(t+1)\n\n∥\n\nπ(t)(cid:1) + KL(cid:0)π(t)\n\nπ(t)(cid:1)(cid:17)\n\n,\n\n∥\n\n(56)\n\nwhere the last line follows from the fact that KL(cid:0)πi thus complete if we can establish\n\n∥\n\n(cid:1)\n\nπ(0)\n\ni\n\nlog\n\nSi\n\n|\n\n|\n\n≤\n\nand 1/η > τ . The proof is\n\nT (cid:88)\n\nt=0\n\nKL(cid:0)π(t)\n\nπ(t)(cid:1) +\n\n∥\n\nT (cid:88)\n\nt=0\n\nKL(cid:0)π(t+1)\n\nπ(t)(cid:1)\n\n∥\n\n(cid:88)\n\n4\n\nk∈V\n\n≤\n\nlog\n\n.\n\nSk |\n\n|\n\n(57)\n\nTherefore, it remains to establish (53) and (57), which shall be completed as follows.\n\nProof of (53). By the update rules of π(t+1)\n\ni\n\n(cid:10) log π(t+1)\n\ni\n\nlog π(t+1)\n\ni\n\n, π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n−\n\n−\n\nBy Pinsker’s inequality, we have\n\ni\n\nand π(t+1) (cid:11) = η(cid:10)Ai(π(t)\n\n, from (20) we can deduce that\n\nπ(t+1)), π(t+1)\n\ni\n\n−\n\nπ(t+1)\n\ni\n\n(cid:11).\n\n−\n\n(58)\n\n(cid:10) log π(t+1)\n\ni\n\n−\n\nlog π(t+1)\n\ni\n\n, π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:11)\n\n−\n\n≥\n\n(cid:13) (cid:13)π(t+1)\n\ni\n\n−\n\nπ(t+1)\n\ni\n\n(cid:13) 2\n(cid:13)\n\n1.\n\nIn addition,\n\n(cid:10)Ai(π(t)\n\nπ(t+1)), π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:11)\n\n(cid:13) (cid:13)π(t+1)\n\ni\n\n−\n\n≤\n\n−\n\n−\n\nπ(t+1)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:13) (cid:13)Ai(π(t)\n\nπ(t+1))(cid:13)\n\n(cid:13)∞.\n\n−\n\nPlugging the above two relations into (58) then leads to (53).\n\nProof of (57). Summing (55) over i\n\n(cid:88)\n\ni∈V\n\nRegreti,τ\n\n(cid:0)πi, T + 1(cid:1)\n\n1 η\n\n≤\n\nKL(cid:0)π\n\n∥\n\n∈ π(0)(cid:1)\n\nV gives\n\n1 η\n\nT (cid:88)\n\nt=0\n\n−\n\nKL(cid:0)π(t+1)\n\n2 ∞\n\n∥\n\nT (cid:88)\n\n(cid:16)\n\nt=0\n\nKL(cid:0)π(t+1)\n\n∥\n\n∥\n\nπ(t+1)(cid:1)\n\n(cid:18) 1\n\n−\n\nη −\n\n(cid:19) T\n\n(cid:88)\n\nτ\n\nt=0\n\nKL(cid:0)π(t+1)\n\nπ(t)(cid:1)\n\n∥\n\nπ(t)(cid:1) + KL(cid:0)π(t)\n\nπ(t)(cid:1)(cid:17)\n\n∥\n\n+ 4ηd2\n\nA\n\nmax ∥\n\n(cid:16)\n\nKL(cid:0)π\n\n1 η\n\n≤\n\n∥\n\nπ(0)(cid:1) + KL(cid:0)π(0)\n\nπ(0)(cid:1)(cid:17)\n\n∥\n\n(cid:18) 1\n\n−\n\nη −\n\nτ\n\n−\n\n25\n\n4ηd2\n\nA\n\nmax ∥\n\n2 ∞\n\n∥\n\n(cid:19) T\n\n(cid:88)\n\nt=0\n\nKL(cid:0)π(t+1)\n\nπ(t)(cid:1)\n\n∥\n\nPublished as a conference paper at ICLR 2023\n\n(cid:18) 1\n\n−\n\nη −\n\n4ηd2\n\nmax ∥\n\nA\n\n2 ∞\n∥\n\n(cid:19) T\n\n(cid:88)\n\nt=0\n\nKL(cid:0)π(t)\n\nπ(t)(cid:1)\n\n∥\n\n1 η\n\n(cid:88)\n\nk∈V\n\n≤\n\nlog\n\nSk |\n\n| −\n\n1 4η\n\nT (cid:88)\n\nt=0\n\nKL(cid:0)π(t+1)\n\nπ(t)(cid:1)\n\n∥\n\n−\n\n1 4η\n\nT (cid:88)\n\nt=0\n\nKL(cid:0)π(t)\n\nπ(t)(cid:1),\n\n∥\n\nwhere the last line follows from π(0) = π(0), KL(cid:0)π k∈V log |\na uniform distribution, as well as the choice of the learning rate such that\n\nπ(0)(cid:1)\n\n(cid:80)\n\n≤\n\n∥\n\n(59)\n\nfor any π since π(0) is\n\nSk\n\n|\n\n4ηd2\n\nmax ∥\n\nA\n\n2 ∥\n\n∞ ≤\n\n1 4η\n\nand\n\n1 2η\n\n.\n\nτ\n\n≤\n\nTaking supremum over π on both sides of (59) and applying Lemma 4 gives (57) as advertised.\n\nC.2 PROOF OF THEOREM 6\n\nSimilar to the proof of Theorem 5 in Appendix C.1, it suffices to bound Regreti,τ πi\n\n∆(Si), where\n\n∈\n\n(cid:0)πi, T (cid:1) for any\n\nRegreti,τ\n\n(cid:0)πi, T (cid:1) :=\n\nT (cid:88)\n\nt=1\n\nui,τ (πi, π(t) −i)\n\nT (cid:88)\n\nt=1\n\n−\n\nui,τ (π(t))\n\n=\n\nT (cid:88)\n\nt=0\n\n(cid:16)(cid:10)πi\n\n−\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)(cid:11) + τ\n\n(πi)\n\nτ\n\nH\n\n−\n\nH\n\n(π(t+1)\n\ni\n\n(cid:17) )\n\n.\n\n(60)\n\nConsider the following decomposition:\n\n(π(t+1)\n\n)\n\nπ(t+1)\n\ni\n\n(cid:10)πi −\n= (cid:10)πi = (cid:10)πi\n\n−\n\nπ(t+1)\n\ni\n\nπ(t+1)\n\ni\n\ni\n\n, Aiπ(t+1)(cid:11) + τ , Aiπ(κ(t+1) , Aiπ(κ(t+1) π(t+1)\n\ni\n\n− (cid:10)π(t+1)\n\ni\n\n− + (cid:10)πi\n\ni\n\n− π(t+1)\n\ni\n\n−\n\n, Aiπ(t+1)\n\n−\n\nτ\n\n−\n\n(πi) H\n)(cid:11) + (cid:10)πi )(cid:11) + (cid:10)π(t+1)\n\n−\n\ni\n\ni\n\nH π(t+1)\n\ni\n\n− Aiπ(κ(t) )(cid:11).\n\ni\n\n− Aiπ(κ(t+1)\n\n, Aiπ(κ(t+1)\n\ni\n\n)\n\nπ(t+1) i\ni )(cid:11)\n\n, Aiπ(t+1)\n\nAiπ(κ(t+1)\n\ni\n\n− i )(cid:11) , Aiπ(κ(t)\n\n)(cid:11) + τ\n\n(πi)\n\nτ\n\nH\n\n−\n\nH\n\n(π(t+1)\n\ni\n\n)\n\n(61)\n\nWe now bound each term on the RHS of (61). For simplicity, we reuse the short-hand notation in (36).\n\n• To begin with, note that π(t+1)\n\ni\n\nconstant ci. Thus we have\n\n= (1\n\n−\n\nητ )π(t)\n\ni + ηAiπ(κ(t+1)\n\ni\n\n) + ci1 for some normalization\n\nπ(t+1)\n\ni\n\n, Aiπ(κ(t+1)\n\ni\n\n)(cid:11)\n\n(cid:10)πi\n\n=\n\n=\n\ni\n\n− 1\n(cid:10)log π(t+1) η\n1 η\n\nKL(cid:0)πi\n\n(cid:16)\n\n∥\n\nlog π(t)\n\ni\n\n, πi\n\n−\n\n−\n\nπ(t+1)\n\ni\n\n(cid:11) + τ (cid:10)log π(t)\n\ni\n\n, πi\n\nπ(t+1)\n\ni\n\n(cid:11)\n\nπ(t)\n\ni\n\nKL(cid:0)πi\n\n(cid:1)\n\n−\n\n∥\n\nπ(t+1)\n\ni\n\n(cid:1)\n\n−\n\nKL(cid:0)π(t+1)\n\ni\n\n∥\n\nπ(t)\n\ni\n\n− (cid:1)(cid:17)\n\n+ τ (cid:10)log π(t)\n\ni\n\n, πi\n\nπ(t+1)\n\ni\n\n(cid:11),\n\n−\n\n(62)\n\nwhere the second step is derived from the definition of KL-divergence.\n\n• Similarly, it holds that\n\n, Aiπ(κ(t)\n\ni )(cid:11)\n\ni\n\n(cid:10)π(t+1) (cid:16) 1\nη\n\n=\n\nπ(t+1)\n\ni\n\n− KL(cid:0)π(t+1)\n\ni\n\n(cid:1)\n\nπ(t)\n\ni\n\n∥\n\n−\n\nKL(cid:0)π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:1)\n\n∥\n\n−\n\nKL(cid:0)π(t+1)\n\ni\n\n(cid:1)(cid:17)\n\nπ(t)\n\ni\n\n∥\n\n+ τ (cid:10)log π(t)\n\ni\n\n, π(t+1)\n\ni\n\n26\n\n(cid:11).\n\nπ(t+1)\n\ni\n\n− (63)\n\nPublished as a conference paper at ICLR 2023\n\n• For the term\n\nin (61), following the deduction of (33), we get\n\n(cid:10)π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n, Aiπ(κ(t+1)\n\ni\n\n)\n\n−\n\n−\n\nAiπ(κ(t)\n\ni )(cid:11)\n\n−\n\n(cid:10)π(t+1)\n\ni\n\n−\n\n−\n\nπ(t+1)\n\ni\n\n, Aiπ(κ(t+1)\n\ni\n\n)\n\n2\n\nA\n\n∥\n\n∥∞\n\n≤\n\n(cid:88)\n\n(cid:16)\n\nΨ(t)\n\nj +\n\nj∈Ni\n\nAiπ(κ(t)\n\ni )(cid:11)\n\n−\n\nγ(t+1)\n\ni\n\nΨ(l)\n\nj +\n\nt−1 (cid:88)\n\nl=t−γ(t+1)\n\ni\n\n• For the last term in (61), it similarly follows that\n\nt−1 (cid:88)\n\ns=t−γ(t)\n\ni\n\n(cid:17)\n\ni Ψ(l) γ(t)\n\nj\n\n+ 2cAKL(cid:0)π(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:1).\n\n∥\n\n(64)\n\n(cid:10)πi −\n= (cid:10)πi\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)\n\nπ(t)\n\ni\n\n−\n\n, Aiπ(t+1)\n\n−\n\n)(cid:11)\n\ni\n\nAiπ(κ(t+1) Aiπ(κ(t+1)\n\ni\n\n2cAKL(cid:0)π(t+1)\n\ni\n\nπ(t)\n\ni\n\n∥\n\n≤\n\n)(cid:11) + (cid:10)π(t)\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)\n\nAiπ(κ(t+1)\n\ni\n\n)(cid:11)\n\n− (cid:1) + 4cτ\n\nA\n\n∥\n\n∥∞\n\n(cid:88)\n\n(cid:16)\n\nj∈Ni\n\ni − Ψ(t)\n\nj +\n\nt−1 (cid:88)\n\nl=t−γ(t+1)\n\ni\n\n− Ψ(l) γ(t+1)\n\nj\n\ni\n\n(cid:17)\n\n.\n\n(65)\n\nPlugging (62) (63) (64) (65) into (61) yields\n\n(cid:10)πi\n\n≤\n\n(cid:16)\n\n− 1\nη\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)(cid:11) + τ\n\n(cid:16)\n\nKL(cid:0)πi\n\n(cid:1)\n\nπ(t)\n\ni\n\n∥\n\n−\n\nKL(cid:0)πi\n\n∥\n\n(cid:17) )\n\n(πi)\n\nH π(t+1)\n\n− H (cid:1)(cid:17)\n\ni\n\ni\n\n(π(t+1) (cid:18) 1\n\n−\n\nη −\n\n(cid:19)\n\n2cA\n\nΨ(t)\n\ni\n\n+ 4cτ\n\nA\n\n∥\n\n∥∞\n\n(cid:88)\n\nj∈Ni\n\nΨ(t)\n\nj + 2γ(t)\n\nA\n\ni ∥\n\n(cid:88)\n\nt−1 (cid:88)\n\n∥∞\n\nj∈Ni\n\nl=t−γ(t)\n\ni\n\nΨ(l)\n\nj\n\n+ 4cτ\n\nA\n\n∥\n\n∥∞ γ(t+1)\n\ni\n\n(cid:88)\n\nt−1 (cid:88)\n\nj∈Ni\n\nl=t−γ(t+1)\n\ni\n\nΨ(l)\n\nj + τ (cid:10)log π(t)\n\ni\n\nπ(t)\n\ni\n\n(cid:11) + τ\n\n, πi\n\n−\n\n(cid:16)\n\nH\n\n(πi)\n\n− H\n\n(π(t+1)\n\ni\n\n)\n\n(cid:17)\n\n.\n\nNote that\n\n(πi)\n\nH\n\n− H\n\n(π(t+1)\n\ni\n\n) + (cid:10)log π(t)\n\ni\n\n, πi\n\nπ(t+1)\n\ni\n\n−\n\n(cid:11) =\n\n(cid:10)log πi −\n= KL(cid:0)π(t+1)\n\n−\n\ni\n\nlog π(t)\n\nπ(t)\n\ni\n\n∥\n\n−\n\ni (cid:1)\n\n, πi KL(cid:0)πi\n\n(cid:11) + (cid:10)log π(t+1) π(t)\n\n(cid:1).\n\ni\n\ni\n\n∥\n\nlog π(t)\n\ni\n\n, π(t+1)\n\ni\n\n(cid:11)\n\n−\n\nThen we have (cid:10)πi\n\nπ(t+1)\n\ni\n\n(cid:16)\n\n− 1\nη\n\n≤\n\n, Aiπ(t+1)(cid:11) + τ\n\n(cid:16)\n\nKL(cid:0)πi\n\n(cid:1)\n\nπ(t)\n\ni\n\n∥\n\n−\n\nKL(cid:0)πi\n\n∥\n\n(cid:17)\n\n)\n\n(πi)\n\nH π(t+1)\n\n− H (cid:1)(cid:17)\n\ni\n\ni\n\n(π(t+1) (cid:18) 1\n\n−\n\nη −\n\n(cid:19)\n\nΨ(t)\n\ni\n\n2cA\n\nτ\n\n−\n\n+ 4cτ\n\nA\n\n∥\n\n∥∞\n\n(cid:88)\n\nj∈Ni\n\nΨ(t)\n\nj + 2γ(t)\n\nA\n\ni ∥\n\n(cid:88)\n\nt−1 (cid:88)\n\n∥∞\n\nj∈Ni\n\nl=t−γ(t)\n\ni\n\nΨ(l)\n\nj + 4cτ\n\nA\n\n∥∞ γ(t+1)\n\ni\n\n∥\n\n(cid:88)\n\nt−1 (cid:88)\n\nj∈Ni\n\nl=t−γ(t+1)\n\ni\n\nΨ(l) j .\n\n(66)\n\nSince the learning rate satisfies\n\n1 η ≥\n\n24d2\n\nA\n\n2 ∞\n∥\n\n(σ2 + 1)\n\n2dmax\n\nA\n\n∥∞ + τ = 2cA + τ,\n\n∥\n\n≥\n\nmax ∥ τ\n\ntaking expectation on both sides of (66) leads to\n\nE\n\n(cid:104)(cid:10)πi\n\nE\n\n1 η\n\n≤\n\n− (cid:104) KL(cid:0)πi\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)(cid:11) + τ\n\n(cid:16)\n\n(cid:1)\n\nπ(t)\n\ni\n\n∥\n\n−\n\nKL(cid:0)πi\n\n∥\n\n(cid:17)(cid:105) )\n\n(πi)\n\nH π(t+1)\n\ni\n\n− H (cid:1)(cid:105)\n\ni\n\n(π(t+1) (cid:18) 1\n\n2cA\n\n(cid:19)\n\n(cid:104)\n\nE\n\n(cid:105)\n\nΨ(t)\n\ni\n\nτ\n\n−\n\n−\n\nη −\n\n27\n\nPublished as a conference paper at ICLR 2023\n\n+ 4cτ\n\nA\n\n∥\n\n∥∞\n\n(cid:104)\n\nΨ(t)(cid:105)\n\nE\n\n+ 4cτ\n\nE\n\nA\n\n∥\n\n∥∞\n\n(cid:34)t−1 (cid:88)\n\nl=0\n\n(cid:35)\n\nl)Ψ(l)\n\nE(t\n\n−\n\n(cid:104)\n\nE\n\nKL(cid:0)πi\n\n1 η\n\n≤\n\n(cid:1)\n\nπ(t)\n\ni\n\n∥\n\n−\n\nKL(cid:0)πi\n\n∥\n\n(cid:1)(cid:105)\n\nπ(t+1)\n\ni\n\n+ 4cτ\n\nA\n\n∥\n\n∥∞\n\n(cid:104)\n\nΨ(t)(cid:105)\n\nE\n\n+ 4cτ\n\nE\n\nA\n\n∥\n\n∥∞\n\n(cid:34)t−1 (cid:88)\n\nl=0\n\nE(t\n\n−\n\n(cid:35)\n\nl)Ψ(l)\n\n,\n\nwhere we use the fact (cid:80) j ≤ −\ndefinition in Assumption 4, summing (67) over t = 0, 1, . . . , T yields\n\nΨ(l) and the definition of E(t\n\nΨ(l)\n\nj∈Ni\n\nl). Since (cid:80)∞\n\nl=0 E(l)\n\n(67)\n\nσ2 by\n\n≤\n\nE (cid:2)Regreti,τ\n\n(cid:0)T + 1(cid:1)(cid:3)\n\n1 η\n\n1 η\n\n≤\n\n≤\n\nE\n\n(cid:104) KL(cid:0)πi\n\n∥\n\n(cid:1)(cid:105)\n\nπ(0)\n\ni\n\n+ 4cτ\n\nA\n\n∥∞ (σ2 + 1)E\n\n∥\n\n(cid:35)\n\nΨ(t)\n\n(cid:34) T\n\n(cid:88)\n\nt=0\n\nlog\n\nSi\n\n|\n\n|\n\n+ 4cτ\n\nA\n\n∥\n\n∥∞ (σ2 + 1)E\n\n(cid:35)\n\nΨ(t)\n\n.\n\n(cid:34) T\n\n(cid:88)\n\nt=0\n\nIt remains to establish\n\n(cid:34) T\n\n(cid:88)\n\nE\n\n(cid:35)\n\nΨ(t)\n\nt=0\n\n(cid:88)\n\n2\n\ni∈V\n\n≤\n\nlog\n\n.\n\nSi |\n\n|\n\n(68)\n\n(69)\n\nProof of (69). Summing (66) over i\n\nV gives\n\n(cid:88)\n\n(cid:10)πi\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)(cid:11) + τ\n\n(cid:16)\n\n−\n\n(πi)\n\nH\n\n(π(t+1)\n\ni\n\n(cid:17) )\n\ni∈V\n\n1 η\n\n≤\n\n(cid:16)\n\nKL(cid:0)π\n\nπ(t)(cid:1)\n\n∥\n\n−\n\nKL(cid:0)π\n\n∥\n\nπ(t+1)(cid:1)(cid:17)\n\n−\n\nη −\n\n(cid:19)\n\nΨ(t)\n\n2cA\n\nτ\n\n−\n\n∈\n\n− H (cid:18) 1\n\n+ 4cτ\n\nA\n\n∥\n\n∥∞\n\n(cid:88)\n\n(cid:88)\n\ni∈V\n\nj∈Ni\n\nΨ(t)\n\nj + 2γ(t)\n\nA\n\ni ∥\n\n(cid:88)\n\n(cid:88)\n\nt−1 (cid:88)\n\n∥∞\n\ni∈V\n\nj∈Ni\n\nl=t−γ(t)\n\ni\n\nΨ(l)\n\nj + 4cτ\n\nA\n\n∥\n\n∥∞\n\n(cid:88)\n\ni∈V\n\n(cid:88)\n\nγ(t+1)\n\ni\n\nt−1 (cid:88)\n\nΨ(l) j .\n\nj∈Ni\n\nl=t−γ(t+1)\n\ni\n\nTaking expectation of on both sides and using (38) leads to\n\n(cid:34)\n\nE\n\n(cid:88)\n\n(cid:10)πi\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)(cid:11) + τ\n\n(cid:16)\n\n−\n\n(πi)\n\nH\n\n− H\n\n(π(t+1)\n\ni\n\n(cid:35)\n\n(cid:17) )\n\ni∈V 1\nη\n\nE\n\n≤\n\n(cid:104)\n\nKL(cid:0)π\n\n∥\n\n+ 4cA(cτ + 1)E\n\n− (cid:34)t−1 (cid:88)\n\nl=0\n\nπ(t)(cid:1)\n\nKL(cid:0)π\n\nπ(t+1)(cid:1)(cid:105)\n\n∥\n\n(cid:18) 1\n\n−\n\nη −\n\n4cA(cτ + 1)\n\nτ\n\n−\n\n(cid:19)\n\n(cid:104)\n\nΨ(t)(cid:105)\n\nE\n\nl)Ψ(l)\n\nE(t\n\n−\n\n(cid:35)\n\nτ 2\n\n−\n\n(cid:104)\n\nE\n\nKL(cid:0)π\n\nπ(t)(cid:1)(cid:105)\n\n.\n\n∥\n\nSumming over t = 0, 1, . . . , T yields\n\n(cid:34)\n\nE\n\n(cid:88)\n\ni∈V\n\nRegreti,τ\n\n(cid:0)π, T + 1(cid:1)\n\n(cid:35)\n\n1 η\n\n1 η\n\n≤\n\n≤\n\n(cid:104) KL(cid:0)π\n\nE\n\n(cid:104) KL(cid:0)π\n\nE\n\n∥\n\n∥\n\nπ(0)(cid:1)(cid:105)\n\n(cid:18) 1\n\n−\n\nη −\n\n4cA(cτ + 1)(σ2 + 1)\n\n(cid:34) T\n\n(cid:88)\n\n(cid:19)\n\nE\n\n(cid:35)\n\nΨ(t)\n\nt=0\n\nπ(0)(cid:1)(cid:105)\n\nE\n\n1 2η\n\n−\n\n(cid:35)\n\nΨ(t)\n\n,\n\n(cid:34) T\n\n(cid:88)\n\nt=0\n\n(70)\n\nwhere the second line follows from\n\n4cA(cτ + 1)(σ2 + 1)η\n\n4dmax\n\nA\n\n∥\n\n∥∞\n\n≤\n\n(cid:18)\n\n2 +\n\ndmax\n\n(cid:19)\n\nA\n\n∥ τ\n\n∥∞\n\n(σ2 + 1)\n\nτ 2\n∞ (σ2 + 1) ∥\n\nA\n\n<\n\n1 2\n\n24d2\n\nmax ∥\n\ndue to τ\n\ndmax\n\nA\n\n∥∞ and η\n\n≤ sides, in view of Lemma 4, we arrive at the advertised bound (69).\n\n≤\n\n∥\n\nτ max∥A∥2\n\n24d2\n\n∞(σ2+1) . Taking supremum with respect to π on both\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nD PROOF FOR TWO-TIMESCALE OMWU (SECTION 4)\n\nD.1 PROOF OF THEOREM 3\n\nτ ∥\n\nBounding KL(cid:0)π⋆ π(t)(cid:1). For notational convenience, we set π(t) = π(t) = π(0) for t < 0. The following lemma parallels Lemma 2 by focusing on delayed feedbacks. The proof is postponed to Appendix E.5. Lemma 5. Assuming constant delays γ(t) satisfy\n\ni = γ, the iterates of OMWU based on the update rule (10)\n\n(cid:10) log π(t+1)\n\n(1\n\nητ ) log π(t)\n\nητ log π⋆\n\nτ , π(t−γ+1)\n\n(cid:11) = 0.\n\nπ⋆\n\nτ\n\n−\n\n− By following a similar argument in (19), we conclude that KL(cid:0)π⋆\n\nπ(t+1)(cid:1) = (1\n\n(1\n\n−\n\n−\n\nητ )KL(cid:0)π⋆ τ ∥ + (cid:10)log π(t−γ+1)\n\n−\n\nπ(t)(cid:1) −\nlog π(t+1), π(t−γ+1)\n\n−\n\nτ ∥\n\n−\n\nητ )KL(cid:0)π(t−γ+1)\n\nπ(t)(cid:1)\n\n∥ π(t+1)(cid:11)\n\n−\n\n−\n\nKL(cid:0)π(t+1) ∥\nητ KL(cid:0)π(t−γ+1)\n\n−\n\nπ(t−γ+1)(cid:1) (cid:1). π⋆ (71)\n\nτ\n\n∥\n\nIt boils down to control the term by taking logarithm on the both sides of the update rules (7) and (10), we have\n\nlog π(t+1), π(t−γ+1)\n\n−\n\n−\n\n−\n\n(cid:10)log π(t−γ+1)\n\nπ(t+1)(cid:11). When t\n\nγ,\n\n≥\n\nand\n\nlog π(t−γ+1)\n\ni\n\n1= (1\n\n−\n\nητ ) log π(t−γ)\n\ni\n\n+ ηAiπ(t−2γ)\n\nlog π(t+1)\n\ni\n\n1= (1\n\n1= (1\n\n−\n\n−\n\nητ ) log π(t)\n\ni + ηAiπ(t−γ+1)\n\nητ )γ+1 log π(t−γ)\n\ni\n\n+ η\n\nγ (cid:88)\n\n(1\n\nl=0\n\n−\n\nητ )lAiπ(t−γ−l+1).\n\nSubtracting the above equalities and taking inner product with π(t−γ+1)\n\ni\n\nπ(t+1)\n\ni\n\ngives\n\n−\n\n(cid:10)log π(t−γ+1)\n\nlog π(t+1)\n\n, π(t−γ+1)\n\ni\n\ni γ\n(cid:88)\n\nl=0\n\n= η\n\n(1\n\n−\n\ni\n\n− ητ )l(cid:10)π(t−γ+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:11)\n\n−\n\nπ(t+1)\n\ni\n\n, Ai(π(t−2γ)\n\n−\n\n−\n\nπ(t−γ−l+1))(cid:11),\n\nwhere the log π(t−γ) i\n\nV ,\n\ni\n\n∈\n\nterms cancel out due to the choice 1\n\nητ = (1\n\n−\n\n−\n\nητ )γ+1. Summing over\n\n(cid:10)log π(t−γ+1)\n\nlog π(t+1), π(t−γ+1)\n\nπ(t+1)(cid:11)\n\n−\n\n(1\n\n−\n\n(cid:88)\n\n= η\n\n(cid:88)\n\nγ (cid:88)\n\ni∈V\n\nl=0\n\nη\n\nA\n\n∥\n\n∥∞\n\n≤\n\nητ )l (cid:10)π(t−γ+1)\n\ni\n\n−\n\n− π(t+1)\n\ni\n\nγ (cid:88)\n\n(1\n\nητ )l(cid:13)\n\n(cid:13)π(t−γ+1)\n\ni\n\n−\n\n(i,j)∈E\n\nl=0\n\n, Ai(π(t−2γ)\n\n−\n\nπ(t−γ−l+1))(cid:11)\n\nπ(t+1)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:13) (cid:13)π(t−2γ)\n\nj\n\nπ(t−γ−l+1)\n\nj\n\n(cid:13) (cid:13)1.\n\n−\n\n−\n\n(72)\n\nUsing the triangle inequality, we can bound (cid:13)\n\n(cid:13)π(t−2γ)\n\nπ(t−γ−l+1)(cid:13)\n\n(cid:13)1 as\n\nπ(l1−γ+1)\n\nj\n\n(cid:13) (cid:13)1\n\n−\n\n−\n\n(cid:13) (cid:13)π(l1−γ)\n\ni\n\n(cid:16)(cid:13)\n\n(cid:13)π(l1−γ)\n\ni\n\nπ(l1)\n\ni\n\n−\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)π(l1−γ+1)\n\nj\n\nπ(l1)\n\nj\n\n(cid:13) (cid:13)1\n\n(cid:17)\n\n.\n\n−\n\n(cid:13) (cid:13)π(t−2γ)\n\n−\n\nπ(t−γ−l+1)(cid:13)\n\n(cid:13)1 ≤\n\nt−l (cid:88)\n\nl1=t−γ\n\nt−l (cid:88)\n\n≤\n\nl1=t−γ\n\nSubstitution of the bound into (72) yields (cid:10)log π(t−γ+1)\n\nlog π(t+1), π(t−γ+1)\n\n−\n\nπ(t+1)(cid:11)\n\n−\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nη\n\nA\n\n∥\n\n∥∞\n\n≤\n\n(cid:88)\n\nγ (cid:88)\n\n(i,j)∈E\n\nl=0\n\nητ )l\n\n(1\n\n−\n\nt−l (cid:88)\n\nl1=t−γ\n\n(cid:13) (cid:13)π(t−γ+1)\n\ni\n\n= η\n\nA\n\n∥\n\n∥∞\n\n(cid:88)\n\nt (cid:88)\n\nt−l1(cid:88)\n\n(i,j)∈E\n\nl1=t−γ\n\nl=0\n\n(1\n\n−\n\nητ )l(cid:13)\n\n(cid:13)π(t−γ+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:13) (cid:13)1\n\nπ(t+1)\n\ni\n\n(cid:13) (cid:13)1\n\n−\n\n−\n\n(cid:16)(cid:13)\n\n(cid:13)π(l1−γ)\n\nj\n\n(cid:16)(cid:13)\n\n(cid:13)π(l1−γ)\n\nj\n\nπ(l1)\n\nj\n\n−\n\nπ(l1)\n\nj\n\n−\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)π(l1−γ+1)\n\nj\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)π(l1−γ+1)\n\nj\n\n(cid:17)\n\nπ(l1)\n\nj\n\n(cid:13) (cid:13)1\n\n(cid:17)\n\nπ(l1)\n\nj\n\n(cid:13) (cid:13)1\n\n−\n\n−\n\n1 2\n\n≤\n\nη\n\nA\n\n∥\n\n∥∞\n\n(cid:88)\n\n(cid:20)\n\n2\n\nt (cid:88)\n\nt−l1(cid:88)\n\n(i,j)∈E\n\nl1=t−γ\n\nl=0\n\n(1\n\n−\n\nητ )l(cid:13)\n\n(cid:13)π(t−γ+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:13) 2\n(cid:13) 1\n\n−\n\nt (cid:88)\n\nt−l1(cid:88)\n\n+\n\nl1=t−γ\n\nl=0\n\n(1\n\n−\n\nητ )l (cid:16)(cid:13)\n\n(cid:13)π(l1−γ)\n\nj\n\nπ(l1)\n\nj\n\n−\n\n(cid:13) 2\n(cid:13)\n\n1 + (cid:13)\n\n(cid:13)π(l1−γ+1)\n\nj\n\n(cid:17) (cid:21)\n\nπ(l1)\n\nj\n\n(cid:13) 2\n(cid:13) 1\n\n−\n\nηdmax\n\nA\n\n∥\n\n∥∞\n\n≤\n\n(cid:20) 2(γ + 1)2KL(cid:0)π(t+1)\n\nπ(t−γ+1)(cid:1)\n\n∥\n\nt (cid:88)\n\nt−l1(cid:88)\n\n+\n\nl1=t−γ\n\nl=0\n\n(1\n\n−\n\nητ )l (cid:16)\n\nKL(cid:0)π(l1)\n\n∥\n\nπ(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17) (cid:21)\n\n.\n\n∥\n\n(73)\n\nPlugging the above inequality into (71) and recursively applying the inequality gives KL(cid:0)π⋆\n\nπ(t−γ+1)(cid:1) + ητ KL(cid:0)π(t−γ+1)\n\nπ(t+1)(cid:1) + KL(cid:0)π(t+1)\n\nπ⋆\n\n(cid:1)\n\nτ\n\n∥\n\nτ ∥\n\n(1\n\n−\n\n≤\n\nητ )t+1−γKL(cid:0)π⋆\n\nτ ∥\n\n∥ π(γ)(cid:1)\n\nt (cid:88)\n\n−\n\nl1=γ\n\n(1\n\n−\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n+ ηdmax\n\nA\n\n∥\n\n∥∞\n\n(cid:20)\n\n2(γ + 1)2\n\nt (cid:88)\n\nl1=γ\n\n(1\n\n−\n\nητ )t−l1KL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1)\n\n∥\n\n+\n\nt (cid:88)\n\nt2=γ\n\n(1\n\n−\n\nητ )t−l2\n\nl2(cid:88)\n\nl2−l1(cid:88)\n\nl1=l2−γ\n\nl=0\n\nητ )l(cid:16)\n\nKL(cid:0)π(l1)\n\n∥\n\n(1\n\n−\n\nπ(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)(cid:21)\n\n∥\n\n(i)\n\n≤\n\n(1\n\n−\n\nητ )t+1−γKL(cid:0)π⋆\n\nτ ∥\n\nπ(γ)(cid:1)\n\nt (cid:88)\n\n−\n\nl1=γ\n\n(1\n\n−\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n+ 2(γ + 1)2ηdmax\n\n+ 2(γ + 1)2ηdmax\n\nA\n\n∥\n\n∥∞\n\nA\n\n∥\n\n∥∞\n\nt (cid:88)\n\nl1=γ\n\n(1\n\n−\n\nt (cid:88)\n\nl1=0\n\n(1\n\n−\n\nητ )t−l1KL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1)\n\n∥\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\n(ii)\n\n≤\n\n(1\n\n−\n\nητ )t+1−γKL(cid:0)π⋆\n\nτ ∥\n\nπ(γ)(cid:1)\n\n+ 2(γ + 1)2ηdmax\n\nA\n\n∥\n\n∥∞\n\nγ−1 (cid:88)\n\nl1=0\n\n(1\n\n−\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nwhere (i) results from basic calculation\n\nπ(l1−γ+1)(cid:1) + (1\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n∥\n\n−\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\nπ(l1)(cid:1)(cid:17)\n\n,\n\n∥\n\n∥\n\n(74)\n\nt (cid:88)\n\nt2=γ\n\n(1\n\n−\n\nt (cid:88)\n\n(1\n\n=\n\nl1=0\n\nt (cid:88)\n\n(1\n\n=\n\nl1=0\n\n−\n\n−\n\nητ )t−l2\n\nl2(cid:88)\n\nl2−l1(cid:88)\n\nl1=l2−γ\n\nl=0\n\nητ )t−l1\n\nl1+γ (cid:88)\n\nl2−l1(cid:88)\n\nl2=l1\n\nl=0\n\nητ )l(cid:16)\n\nKL(cid:0)π(l1)\n\n∥\n\nπ(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\nητ )l1−l2+l(cid:16)\n\nKL(cid:0)π(l1)\n\n∥\n\nπ(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n(1\n\n−\n\n(1\n\n−\n\nητ )t−l1\n\nγ (cid:88)\n\nl′ (cid:88)\n\n(1\n\nl′=0\n\nl=0\n\n−\n\nητ )l−l′(cid:16)\n\nKL(cid:0)π(l1)\n\n∥\n\nπ(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nt (cid:88)\n\n(1\n\nl1=0\n\n−\n\nητ )t−l1(γ + 1)2(cid:16)\n\n1\n\n1 2(γ + 1)\n\n−\n\n(cid:17)−(γ+1)\n\n(1\n\n−\n\n(cid:16)\n\nKL(cid:0)π(l1)\n\nητ )\n\n∥\n\nπ(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n2(γ + 1)2\n\n2(γ + 1)2\n\nt (cid:88)\n\n(1\n\nl1=0\n\nt (cid:88)\n\n(1\n\nl1=0\n\n−\n\n−\n\nητ )t−l1(1\n\n−\n\n(cid:16)\n\nητ )\n\nKL(cid:0)π(l1)\n\n∥\n\nπ(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n≤\n\n≤\n\n≤\n\n1 5dmax∥A∥∞(γ+1)2\n\n(cid:111)\n\n. To proceed, we introduce the following\n\nπ(γ)(cid:1), with the proof postponed to Appendix E.6.\n\ni = γ, the iterates of OMWU based on the update rule (10)\n\nγ−1 (cid:88)\n\n−\n\nl1=0\n\n(1\n\n−\n\nητ )γ−1−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\nWith the lemma above in mind, we can continue to bound (74) by KL(cid:0)π⋆\n\nητ )t+1−γηγ2dmax\n\nA\n\n∥\n\n∥∞\n\nπ(l1−γ+1)(cid:1) + (1\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n∥\n\n+ 2(γ + 1)2ηdmax\n\nA\n\n∥\n\n∥∞\n\nγ−1 (cid:88)\n\n(1\n\nl1=0\n\n−\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n(1\n\nητ )t+1KL(cid:0)π⋆\n\nπ(0)(cid:1) + (1\n\nητ )t+1−γ.\n\n−\n\n≤ Bounding KL(cid:0)π⋆\n\nτ ∥ π(t−γ+1)(cid:1). By definition of KL divergence, we have\n\n−\n\n(cid:110)\n\n1\n\n≤\n\nmin\n\n2τ (γ+1) ,\n\nand (ii) is due to η lemma concerning the error KL(cid:0)π⋆ τ ∥ Lemma 6. With constant delays γ(t) satisfy KL(cid:0)π⋆\n\nπ(γ)(cid:1)\n\nτ ∥\n\n(1\n\n−\n\n≤\n\nητ )γKL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1)\n\n+ 2ηγ2dmax\n\nA\n\n∥\n\n∥∞.\n\nτ ∥\n\nπ(t+1)(cid:1) ητ )t+1KL(cid:0)π⋆\n\n(1\n\n≤\n\n−\n\nγ−1 (cid:88)\n\n−\n\nl1=0\n\n(cid:16)\n\nητ )t−l1\n\n(1\n\n−\n\nτ ∥\n\nπ(0)(cid:1) + 2(1\n\n− KL(cid:0)π(l1+1)\n\nτ ∥\n\nπ(t−γ+1)(cid:1)\n\nτ ∥\n\nKL(cid:0)π⋆ = KL(cid:0)π⋆ = KL(cid:0)π⋆\n\nτ ∥ τ ∥\n\nπ(t+1)(cid:1) + (cid:10)π⋆ π(t+1)(cid:1) + KL(cid:0)π(t+1) It remains to control the term (cid:10)π⋆ argument in (73), we have (cid:10)π⋆\n\nπ(t+1), log π(t+1)\n\nτ −\n\n(cid:88)\n\n= η\n\nγ (cid:88)\n\ni∈V\n\nl=0\n\n(1\n\n−\n\n− ητ )l (cid:10)π⋆\n\nτ −\n\nlog π(t−γ+1)(cid:11)\n\nτ , log π(t+1)\n\n−\n\nlog π(t−γ+1)(cid:11)\n\n∥\n\nπ(t−γ+1)(cid:1) + (cid:10)π⋆\n\nτ − π(t+1), log π(t+1)\n\nπ(t+1), log π(t+1)\n\n(75) log π(t−γ+1)(cid:11). By following a similar\n\nlog π(t−γ+1)(cid:11).\n\n−\n\n−\n\nπ(t+1)\n\ni\n\n, Ai(π(t−2γ)\n\nπ(t−γ−l+1))(cid:11)\n\n−\n\ni,τ −\n\nη\n\nA\n\n∥\n\n∥∞\n\n≤\n\nη\n\nA\n\n∥\n\n∥∞\n\n≤\n\n(cid:88)\n\nγ (cid:88)\n\n(i,j)∈E\n\nl=0\n\n(cid:88)\n\nγ (cid:88)\n\n(i,j)∈E\n\nl=0\n\n(1\n\n−\n\nητ )l (cid:13)\n\n(cid:13)π⋆\n\ni,τ −\n\nπ(t+1)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:13) (cid:13)π(t−2γ)\n\nj\n\nπ(t−γ−l+1)\n\nj\n\n(cid:13) (cid:13)1\n\n−\n\nητ )l\n\n(1\n\n−\n\nt−l (cid:88)\n\nl1=t−γ\n\n(cid:13) (cid:13)π⋆\n\ni,τ −\n\nπ(t+1)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:16)(cid:13)\n\n(cid:13)π(l1−γ)\n\nj\n\n= η\n\nA\n\n∥\n\n∥∞\n\n(cid:88)\n\nt (cid:88)\n\nt−l1(cid:88)\n\n(i,j)∈E\n\nl1=t−γ\n\nl=0\n\n(1\n\n−\n\nητ )l(cid:13)\n\n(cid:13)π⋆\n\ni,τ −\n\nπ(t+1)\n\ni\n\n(cid:13) (cid:13)1\n\n31\n\n(cid:16)(cid:13)\n\n(cid:13)π(l1−γ)\n\nj\n\nπ(l1)\n\nj\n\n−\n\nπ(l1)\n\nj\n\n−\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)π(l1−γ+1)\n\nj\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)π(l1−γ+1)\n\nj\n\n(cid:17)\n\nπ(l1)\n\nj\n\n(cid:13) (cid:13)1\n\n(cid:17)\n\nπ(l1)\n\nj\n\n(cid:13) (cid:13)1\n\n−\n\n−\n\nPublished as a conference paper at ICLR 2023\n\nηdmax\n\nA\n\n∥\n\n∥∞\n\n≤\n\n(cid:20) 2(γ + 1)2KL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1)\n\nt (cid:88)\n\nt−l1(cid:88)\n\n+\n\nl1=t−γ\n\nl=0\n\n(1\n\n−\n\nητ )l (cid:16)\n\nKL(cid:0)π(l1)\n\n∥\n\nπ(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17) (cid:21)\n\n.\n\n∥\n\nτ ∥\n\nπ(t−γ+1)(cid:1) + ητ KL(cid:0)π(t−γ+1)\n\nSubstitution of the above inequality into (75) yields KL(cid:0)π⋆ = (1 + 2(γ + 1)2ηdmax)KL(cid:0)π⋆ t−l1(cid:88)\n\nt (cid:88)\n\nτ ∥\n\nπ⋆\n\nπ(t+1)(cid:1) + KL(cid:0)π(t+1)\n\n∥\n\n(cid:1)\n\nτ\n\nητ )l (cid:16)\n\nKL(cid:0)π(l1)\n\n+ ηdmax\n\nA\n\n∥\n\n∥∞\n\nl1=t−γ\n\nl=0\n\n(1\n\n−\n\n∥\n\nπ(t−γ+1)(cid:1) + ητ KL(cid:0)π(t−γ+1)\n\n∥ π(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\n(cid:1)\n\nπ⋆\n\nτ\n\n∥\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n(cid:16)\n\n2\n\n(i)\n\n≤\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1) + KL(cid:0)π(t+1)\n\n∥\n\nπ(t−γ+1)(cid:1) + ητ KL(cid:0)π(t−γ+1)\n\n(cid:1)(cid:17)\n\nπ⋆\n\nτ\n\n∥\n\n+ 2(γ + 1)ηdmax\n\nA\n\n∥\n\n∥∞\n\nt (cid:88)\n\n(1\n\nl1=0\n\n−\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n(ii)\n\n≤\n\n2(1\n\n−\n\nητ )t+1−γKL(cid:0)π⋆\n\nτ ∥\n\nπ(γ)(cid:1)\n\nt (cid:88)\n\nl1=γ\n\n2\n\n−\n\n(1\n\n−\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n+ 4(γ + 1)2ηdmax\n\n+ 6(γ + 1)2ηdmax\n\nA\n\n∥\n\n∥∞\n\nA\n\n∥\n\n∥∞\n\n2(1\n\n−\n\n≤\n\nητ )t+1−γKL(cid:0)π⋆\n\nτ ∥\n\n+ 6(γ + 1)2ηdmax\n\nA\n\n∥\n\n∥∞\n\nwhere (i) results from\n\nt (cid:88)\n\nl1=γ\n\n(1\n\n−\n\nt (cid:88)\n\n(1\n\nl1=0\n\nπ(γ)(cid:1)\n\n−\n\nγ−1 (cid:88)\n\nl1=0\n\n(1\n\n−\n\nητ )t−l1KL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1)\n\n∥\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1) + (1\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n∥\n\n−\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\nπ(l1)(cid:1)(cid:17)\n\n,\n\n∥\n\n∥\n\nt (cid:88)\n\nt−l1(cid:88)\n\nl1=t−γ\n\nl=0\n\n(1\n\n−\n\nητ )l (cid:16)\n\nKL(cid:0)π(l1)\n\n∥\n\nπ(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\nt (cid:88)\n\nl1=t−γ\n\nt (cid:88)\n\nl1=t−γ\n\nητ )t−l1\n\n(1\n\n−\n\nt−l1(cid:88)\n\nl=0\n\n(1\n\n−\n\nητ )l+l1−t (cid:16)\n\nKL(cid:0)π(l1)\n\n∥\n\nπ(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\nητ )t−l1 (γ + 1)(1\n\n(1\n\n−\n\nητ )−(γ+1)(1\n\n−\n\n−\n\n(cid:16)\n\nKL(cid:0)π(l1)\n\nητ )\n\n∥\n\nπ(l1−γ)(cid:1) + KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n2(γ + 1)\n\nt (cid:88)\n\n(1\n\nl1=0\n\n−\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n.\n\n∥\n\n=\n\n≤\n\n≤\n\nand (ii) is due to the bound established in (74). Finally, applying Lemma 6 yields KL(cid:0)π⋆\n\nπ⋆\n\n(cid:1)\n\nτ ∥\n\n2(1\n\n−\n\n≤\n\nπ(t−γ+1)(cid:1) + ητ KL(cid:0)π(t−γ+1) π(0)(cid:1) + 4(1 ητ )t+1KL(cid:0)π⋆\n\nτ\n\n∥\n\nτ ∥\n\n(cid:16)\n\nητ )t−l1\n\n− KL(cid:0)π(l1+1)\n\nγ−1 (cid:88)\n\n(1\n\nl1=0\n\n−\n\n2\n\n−\n\nητ )t+1−γηγ2dmax\n\n∥\n\nA\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n−\n\n∥∞ ητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n+ 6(γ + 1)2ηdmax\n\nA\n\n∥\n\n∥∞\n\nγ−1 (cid:88)\n\n(1\n\nl1=0\n\n−\n\nητ )t−l1\n\n(cid:16)\n\nKL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1) + (1\n\n∥\n\n−\n\nητ )KL(cid:0)π(l1−γ+1)\n\nπ(l1)(cid:1)(cid:17)\n\n∥\n\n32\n\nPublished as a conference paper at ICLR 2023\n\n2(1\n\n−\n\n≤\n\nητ )t+1KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1) + 2(1\n\n−\n\nητ )t+1−γ.\n\nBounding the QRE gap. With Lemma 3, we have\n\n(76)\n\n(cid:1)\n\nπ⋆\n\nτ\n\n∥\n\nKL(cid:0)π⋆\n\nπ(t−γ+1)(cid:1) + τ KL(cid:0)π(t−γ+1)\n\n2 ∞\n∥\n\nd2\n\nA\n\nmax ∥ τ\n(cid:110) d2\n\nmax\n\nτ ∥ 1\nη\n\n,\n\nA\n\n2 ∞\n∥\n\nmax ∥ τ\n\nQRE-Gapτ (π(t−γ+1))\n\n≤\n\n≤\n\n≤\n\n(cid:111)(cid:16)\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t−γ+1)(cid:1) + ητ KL(cid:0)π(t−γ+1)\n\n(cid:1)(cid:17)\n\nπ⋆\n\nτ\n\n∥\n\n2 max\n\n(cid:110) d2\n\nA\n\n2 ∞\n\n∥\n\n,\n\n(cid:111)(cid:16)\n\n(1\n\n1 η\n\n−\n\nητ )t+1KL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1) + (1\n\n−\n\nητ )t+1−γ(cid:17)\n\n,\n\nmax ∥ τ\n\nwhere the last step results from (76).\n\nD.2 PROOF OF THEOREM 4\n\nBounding the term KL(cid:0)π⋆\n\nπ(t)(cid:1). Recall that the update rule of π(t)\n\ni (k) is given by\n\nτ ∥ π(t)\n\ni (k)\n\nπ(t−1)\n\ni\n\n(k)1−ητ exp(η[Aiπ(κ(t)\n\ni )]k).\n\n∝\n\nWe introduce an auxiliary variable (cid:101)π(t)\n\ni\n\n:\n\n(cid:101)π(t) i (k)\n\nπ(t−1)\n\n(k)1−(cid:101)η(t)\n\ni τ exp\n\ni\n\n(cid:16)\n\n(cid:101)η(t)\n\ni\n\n[Aiπ(κ(t)\n\ni )]k\n\n∝ which can be viewed as a conceptual alternative update of π(t) satisfying\n\nor equivalently\n\n(1\n\n− (cid:101)η(t)\n\ni τ )(1\n\n−\n\nητ )t−κ(t)\n\ni = 1\n\nητ\n\n−\n\n(77)\n\n(78)\n\n(cid:17)\n\n,\n\ni with a different step size (cid:101)η(t)\n\ni > 0\n\ni τ = (1\n\n1\n\n− (cid:101)η(t) η. Since κ(t)\n\nητ )γ+1−t+κ(t)\n\ni\n\n.\n\n−\n\nt, we have 1\n\ni )ητ i τ i ≤ ≥\n(γ + 1)η. For notational convenience, we set (cid:101)π(t) i = π(0), i ≤ 0. The following lemma establishes a one-step analysis, with the\n\n(γ + 1\n\n−\n\n≥\n\n−\n\n1\n\n− (cid:101)η(t)\n\nt + κ(t)\n\nIt directly follows that (cid:101)η(t)\n\n(γ + 1)ητ , which implies (cid:101)η(t)\n\n1 −\ni = η and κ(t) (cid:101)η(t) ≤\nproof postponed to Appendix E.7.\n\ni = 0 when t\n\ni ≥\n\nLemma 7. When t\n\n≥\n\n1, it holds that\n\n= (1\n\n+\n\nwhere\n\nψ(t)\n\ni\n\n:=\n\n(cid:16)\n\n1\n\n−\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\nπ(t)\n\ni\n\n(cid:1) + ητ KL(cid:0)π(κ(t)\n\ni )\n\ni\n\nητ )KL(cid:0)π⋆ (cid:10)log π(κ(t)\n\ni,τ ∥ i )\n\ni\n\nπ(t−1)\n\ni\n\n(cid:1)\n\nlog (cid:101)π(t)\n\ni\n\n−\n\n− η\n\n(cid:101)η(t)\n\ni\n\n(cid:1)\n\ni,τ\n\nπ⋆ ∥\nη(π(κ(t) −\n, π(κ(t)\n\ni )\n\ni\n\ni\n\ni )\n\ni )\n\ni,τ )⊤Ai(π(κ(t) π⋆ (cid:11),\n\n−\n\n− (cid:101)π(t)\n\ni\n\nπ⋆ τ )\n\n−\n\n−\n\nψ(t)\n\ni\n\nKL(cid:0)π(t)\n\ni ∥\n\nπ(t−1)\n\ni\n\n(cid:1)\n\nη\n\n(cid:17)\n\n(cid:101)η(t)\n\ni\n\n(cid:20)\n\n+\n\nη\n\n(cid:101)η(t)\n\ni\n\n(1\n\ni τ )KL(cid:0)π(κ(t)\n\ni )\n\ni\n\n− (cid:101)η(t)\n\nπ(t−1)\n\ni\n\n(cid:1) + KL(cid:0)\n\n(cid:101)π(t) i ∥\n\n∥\n\nπ(κ(t)\n\ni )\n\ni\n\n(cid:1) + KL(cid:0)π(t)\n\ni ∥ (cid:101)π(t)\n\ni\n\n(79)\n\n(cid:21)\n\n(cid:1)\n\n.\n\nWe proceed to control the term (cid:10)log π(κ(t)\n\ni )\n\nlog (cid:101)π(t)\n\ni\n\n1= (1\n\n1= (1\n\n− (cid:101)η(t) − (cid:101)η(t)\n\ni\n\nlog (cid:101)π(t) + (cid:101)η(t)\n\ni\n\n− i τ ) log π(t−1)\n\ni\n\ni τ )(1\n\nητ )t−κ(t)\n\ni\n\n−\n\ni )\n\ni Aiπ(κ(t) log π(κ(t)\n\ni −1)\n\n, π(κ(t)\n\ni )\n\ni\n\n(cid:11). By definition, we have\n\n− (cid:101)π(t)\n\ni\n\n33\n\nPublished as a conference paper at ICLR 2023\n\n+ (cid:101)η(t)\n\ni\n\nand\n\n(cid:18)\n\nAiπ(κ(t)\n\ni ) +\n\nt−1 (cid:88)\n\nl=κ(t)\n\ni\n\n(1\n\n− (cid:101)η(t)\n\ni τ )(1\n\n−\n\nητ )t−1−lAiπ(κ(l) i )\n\n(cid:19)\n\nwhen κ(t)\n\ni ≥ log π(κ(t) i )\n\ni\n\nlog π(κ(t)\n\ni )\n\ni\n\n1= (1\n\n−\n\nητ ) log π(κ(t)\n\ni −1) + ηAiπ(κ\n\ni\n\n(κ\n\n(t) i\n\n−1)\n\n)\n\n1. Subtracting the two equations yields\n\n(cid:18)\n\nlog (cid:101)π(t)\n\ni\n\n1= (cid:101)η(t)\n\ni\n\n−\n\n(κ\n\n(t) i\n\nAi(π(κ\n\ni\n\n−1)\n\n)\n\n−\n\nπ(κ(t)\n\ni ))\n\nt−1 (cid:88)\n\n+\n\nl=κ(t)\n\ni\n\n(1\n\n− (cid:101)η(t)\n\ni τ )(1\n\n−\n\nητ )t−1−lAi(π(κ\n\n(t) i\n\n(κ\n\ni\n\nπ(κ(l)\n\ni ))\n\n(cid:19) ,\n\n−1)\n\n)\n\n−\n\n(80)\n\nwhere the log π(κ(t)\n\ni −1) terms cancel out due to (1\n\n− (cid:101)η(t)\n\ni τ )(1\n\n−\n\nητ )t−κ(t)\n\ni = 1\n\n−\n\nητ . It follows that\n\ni )\n\n(cid:10)log π(κ(t) (cid:18)\n\ni\n\n= (cid:101)η(t)\n\ni\n\n− (cid:10)π(κ(t)\n\ni )\n\ni\n\ni\n\nlog (cid:101)π(t) − (cid:101)π(t)\n\ni\n\n, π(κ(t)\n\ni )\n\ni\n\n(cid:11)\n\n− (cid:101)π(t)\n\ni\n\n(κ\n\n(t) i\n\n, Ai(π(κ\n\ni\n\n−1)\n\n)\n\n−\n\nπ(κ(t)\n\ni ))(cid:11)\n\nt−1 (cid:88)\n\n+\n\nl=κ(t)\n\ni\n\n(1\n\n− (cid:101)η(t)\n\ni τ )(1\n\n−\n\nητ )t−1−l(cid:10)π(κ(t)\n\ni )\n\ni\n\n− (cid:101)π(t)\n\ni\n\n(κ\n\n, Ai(π(κ\n\ni\n\n(t−1) i\n\n)\n\n)\n\n−\n\n(cid:19)\n\nπ(κ(l)\n\ni ))(cid:11)\n\n≤ (cid:101)η(t) i ∥\n\nA\n\n∥∞\n\n(cid:13)\n\n(cid:13)π(κ(t)\n\ni )\n\ni\n\n− (cid:101)π(t)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:88)\n\nt (cid:88)\n\nj∈Ni\n\nl=κ(t)\n\ni\n\n(cid:13)\n\n(cid:13)π(κ(l)\n\ni )\n\nj\n\nπ(κ\n\nj\n\n−\n\n(t−1) i\n\n(κ\n\ni\n\n)\n\n)\n\n(cid:13) (cid:13)1.\n\n(81)\n\nThe next lemma establishes an upper bound on the term (cid:80)t proof postponed to Appendix E.8. Lemma 8. Let νj(t) denote the time index when agent j receives the payoff from the t-th iteration, = t. For t = 0, we set νj(0) to an arbitrary index that satisfies κ(νj (0)) i.e., κ(νj (t)) = 0. When t\n\n2γ + 1, it holds that\n\n(cid:13) (cid:13)1, with the\n\nl=κ(t)\n\nπ(κ\n\ni )\n\n(cid:13)π(κ(l)\n\n−\n\n(cid:13)\n\nj\n\nj\n\nj\n\nj\n\n)\n\ni\n\ni\n\n(κ\n\n(t−1) i\n\n)\n\nt (cid:88)\n\nl=κ(t)\n\ni\n\n(cid:13)\n\n(cid:13)π(κ(l)\n\ni )\n\nj\n\nπ(κ\n\nj\n\n−\n\n(t−1) i\n\n(κ\n\ni\n\n)\n\n)\n\n(cid:13) (cid:13)1 ≤\n\n4√2(γ + 1)\n\nt+γ (cid:88)\n\n(cid:113)\n\nl=t−2γ\n\nψ(l)\n\nj + 2√2(γ + 1)2\n\n(cid:114)\n\n(κ\n\n(t−1) i\n\n)\n\n))\n\n,\n\nψ(νj (κ\n\ni\n\nj\n\n≥\n\nPlugging Lemma 8 into (81) gives\n\n(cid:10)log π(κ(t)\n\ni )\n\ni\n\nlog (cid:101)π(t)\n\ni\n\n−\n\n, π(κ(t)\n\ni )\n\ni\n\n(cid:11)\n\n− (cid:101)π(t)\n\ni\n\n≤ (cid:101)η(t) i ∥\n\nA\n\n∥∞\n\n(cid:13)\n\n(cid:13)π(κ(k)\n\ni\n\ni\n\n)\n\n− (cid:101)π(t)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:88)\n\n(cid:104)\n\nj∈Ni\n\n4√2(γ + 1)\n\nt+γ (cid:88)\n\nl=t−2γ\n\n(cid:113)\n\nψ(l)\n\nj + 2√2(γ + 1)2\n\n(i)\n\n≤\n\n1\n\n2 (cid:101)η(t) i ∥\n\nA\n\n(cid:26)\n\n∥∞\n\n14dmax(γ + 1)3/2(cid:13)\n\n(cid:13)π(κ(t)\n\ni )\n\ni\n\n− (cid:101)π(t)\n\ni\n\n(cid:13) 2\n(cid:13) 1\n\n(cid:114)\n\n(κ\n\n(t−1) i\n\n)\n\n(cid:105)\n\n))\n\nψ(νj (κ\n\ni\n\nj\n\n(cid:88)\n\n(cid:104)\n\n+\n\n8(γ + 1)3/2\n\nt+γ (cid:88)\n\nl=t−2γ\n\nψ(l)\n\nj + 4(γ + 1)5/2ψ(νj (κ\n\nj\n\ni\n\n(κ\n\n(t−1) i\n\n(cid:105)(cid:27)\n\n)\n\n))\n\nj∈Ni\n\n(cid:26)\n\n(ii)\n\n≤ (cid:101)η(t) i ∥\n\nA\n\n∥∞\n\n14dmax(γ + 1)5/2ψ(t)\n\ni +\n\n(cid:88)\n\n(cid:104)\n\nj∈Ni\n\n4(γ + 1)3/2\n\nt+γ (cid:88)\n\nl=t−2γ\n\nψ(l)\n\nj + 2(γ + 1)5/2ψ(νj (κ\n\nj\n\ni\n\n(κ\n\n(t−1) i\n\n(cid:105)(cid:27)\n\n)\n\n))\n\n,\n\n34\n\n(82)\n\nPublished as a conference paper at ICLR 2023\n\nwhere (i) results from Young’s inequality\n\n(cid:13)\n\n(cid:13)π(κ(t)\n\ni )\n\ni\n\n(cid:113)\n\n− (cid:101)π(t)\n\ni\n\n(cid:13) (cid:13)1\n\nψ(l)\n\nj ≤\n\n(cid:16)\n\n1 2\n\n1 √2(γ + 1)1/2\n\n(cid:13)\n\n(cid:13)π(κ(t)\n\ni )\n\ni\n\n− (cid:101)π(t)\n\ni\n\n(cid:13) 2\n1 + √2(γ + 1)1/2ψ(l) (cid:13)\n\nj\n\n(cid:17)\n\nand (ii) follows from\n\n(cid:13) π(κ(t) (cid:13) (cid:13) (cid:13)\n\ni\n\ni )\n\n− (cid:101)π(t)\n\ni\n\n(79) and summing over i\n\nπ(κ(t)\n\ni )\n\ni\n\n(cid:1)\n\n≤\n\n2(γ + 1)ψ(t)\n\ni\n\n. Plugging (82) into\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n1 ≤\n\n2KL(cid:0)\n\n(cid:101)π(t) i ∥\n\n(cid:1)\n\nπ⋆\n\nτ\n\n∥\n\nV yields\n\n∈ KL(cid:0)π(κ(t)\n\ni )\n\ni\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t)(cid:1) + ητ\n\n(cid:88)\n\ni∈V\n\n(1\n\n−\n\n≤\n\nητ )KL(cid:0)π⋆\n\nτ ∥\n\nπ(t−1)(cid:1)\n\nη\n\n−\n\n(cid:88)\n\ni∈V\n\n(π(κ(t)\n\ni )\n\ni\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\nπ⋆ τ )\n\n−\n\n−\n\n(1\n\n−\n\n−\n\n14ηdmax\n\nA\n\n∥\n\n∥∞ (γ + 1)5/2)\n\n(cid:88)\n\ni∈V\n\nψ(t)\n\ni + 2η\n\nA\n\n∥\n\n∥∞ (γ + 1)5/2 (cid:88)\n\n(i,j)∈E\n\n+ 4ηdmax\n\nA\n\n∥∞ (γ + 1)3/2\n\n∥\n\nt+γ (cid:88)\n\nψ(l),\n\nl=t−2γ\n\n(κ\n\n(t−1) i\n\nψ(νj (κ\n\ni\n\nj\n\n)\n\n))\n\n(83)\n\nwhere we denote (cid:80) over t = 2γ + 1,\n\ni∈V ψ(l) , T . Before proceeding, we note that\n\ni by ψ(l) for notation simplicity. We then seek to sum the above equation\n\n· · ·\n\nT (cid:88)\n\nt+γ (cid:88)\n\nt=2γ+1\n\nl=t−2γ\n\nψ(l)\n\n≤\n\nT +γ (cid:88)\n\nl+2γ (cid:88)\n\nl=1\n\nt=l−γ\n\nψ(l)\n\n≤\n\n3(γ + 1)\n\nT +γ (cid:88)\n\nl=1\n\nψ(l),\n\nand that\n\nT (cid:88)\n\n(cid:88)\n\nt=2γ+1\n\n(i,j)∈E\n\n(κ\n\nψ(νj (κ\n\ni\n\nj\n\n(t−1) i\n\n)\n\n))\n\n(cid:88)\n\nT +γ−1 (cid:88)\n\n≤\n\n(i,j)∈E\n\nt=0\n\nψ(t)\n\nj ≤\n\ndmax\n\nT +γ−1 (cid:88)\n\nt=1\n\nψ(t),\n\nwhere the first step is due to the mapping t Assumptions 2, 3). Note that ψ(t) together, we arrive at\n\nj = 0 when t\n\n(cid:55)→\n\n≤\n\nνj(κ(κ(t−1)\n\ni\n\n)\n\ni\n\n) being injective when t\n\n2γ + 1 (cf. 0 and hence can be safely discarded. Taken\n\n≥\n\nT (cid:88)\n\nητ\n\nt=2γ+1\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t)(cid:1) + ητ\n\nT (cid:88)\n\n(cid:88)\n\nt=2γ+1\n\ni∈V\n\nKL(cid:0)π(κ(t)\n\ni )\n\ni\n\n(cid:1)\n\nπ⋆\n\ni,τ\n\n∥\n\n(1\n\n−\n\n≤\n\nητ )KL(cid:0)π⋆\n\nτ ∥\n\nπ(2γ)(cid:1)\n\nη\n\n−\n\nT (cid:88)\n\n(cid:88)\n\nt=2γ+1\n\ni∈V\n\n(π(κ(t)\n\ni )\n\ni\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\nπ⋆ τ )\n\n−\n\n−\n\n(cid:16)\n\n1\n\n−\n\n−\n\n14ηdmax\n\n∥∞ (γ + 1)5/2(cid:17)\n\nA\n\n∥\n\nT (cid:88)\n\nt=2γ+1\n\nψ(t) + 12ηdmax\n\nA\n\n∥∞ (γ + 1)5/2\n\n∥\n\nT +γ (cid:88)\n\nl=1\n\nψ(l)\n\n+ 2ηdmax\n\nA\n\n∥\n\n∥∞ (γ + 1)5/2\n\nT +γ−1 (cid:88)\n\nψ(t)\n\nt=1\n\nT (cid:88)\n\n(cid:88)\n\nt=2γ+1\n\ni∈V\n\n(1\n\n−\n\n≤\n\nητ )KL(cid:0)π⋆\n\nτ ∥\n\nπ(2γ)(cid:1)\n\nη\n\n−\n\n(π(κ(t)\n\ni )\n\ni\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\nπ⋆ τ )\n\n−\n\n−\n\n(cid:16)\n\n1\n\n−\n\n−\n\n28ηdmax\n\n∥∞ (γ + 1)5/2(cid:17)\n\nA\n\n∥\n\nT (cid:88)\n\nt=2γ+1\n\n35\n\nψ(t) + 14ηdmax\n\nA\n\n∥\n\n∥∞ (γ + 1)5/2 (cid:88)\n\nl∈Γ\n\nψ(l)\n\nPublished as a conference paper at ICLR 2023\n\n(1\n\n−\n\n≤\n\nητ )KL(cid:0)π⋆\n\nτ ∥\n\nπ(2γ)(cid:1)\n\nη\n\n−\n\nT (cid:88)\n\n(cid:88)\n\nt=2γ+1\n\ni∈V\n\n(π(κ(t)\n\ni )\n\ni\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\n−\n\nπ⋆\n\nτ ) +\n\n−\n\n1 3\n\n(cid:88)\n\nl∈Γ\n\nψ(l), (84)\n\nwhere Γ =\n\n1,\n\n, 2γ\n\nT + 1,\n\n, T + γ\n\n. The last step results from the choice of learn-\n\n{\n\n· · ·\n\n} ∪ { 1\n\n· · ·\n\n}\n\n28dmax∥A∥∞(γ+1)5/2 . It now remains to bound the terms (cid:80)T\n\nt=2γ+1\n\n(cid:80)\n\ni∈V (π(κ(t)\n\ni )\n\ni\n\n−\n\nτ ), KL(cid:0)π⋆ π⋆\n\nπ(2γ)(cid:1) and (cid:80)\n\nτ ∥\n\n−\n\nl∈Γ ψ(l). In view of Lemma 1, we have\n\ning rate η i,τ )⊤Ai(π(κ(t) π⋆\n\n≤\n\ni )\n\nT (cid:88)\n\n(cid:88)\n\nt=2γ+1\n\ni∈V\n\n(π(κ(t)\n\ni )\n\ni\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\nπ⋆ τ )\n\n−\n\n−\n\nT (cid:88)\n\n(cid:88)\n\nt=γ+1\n\ni∈V\n\n(π(t)\n\ni −\n\nπ⋆\n\ni,τ )⊤Ai(π(t)\n\nπ⋆ τ )\n\n−\n\n−\n\nT (cid:88)\n\n(cid:88)\n\nt=2γ+1\n\ni∈V\n\n(π(κ(t)\n\ni )\n\ni\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\nπ⋆\n\nτ ).\n\n−\n\n−\n\n−\n\n=\n\nWe remark that each (π(κ(t) t\n\n− i being injective when t\n\nκ(t)\n\ni )\n\ni\n\n(cid:55)→\n\n(π(t)\n\ni −\n\nπ⋆\n\ni,τ )⊤Ai(π(t)\n\n−\n\n≥ π⋆\n\nτ ) =\n\ni,τ )⊤Ai(π(κ(t) π⋆ γ. In addition, we have a crude bound\n\nπ⋆\n\ni )\n\n−\n\nτ ) term will cancel out due to the mapping\n\n(cid:88)\n\nj∈Ni\n\n(π(t)\n\ni −\n\ni,τ )⊤Aij(π(t) π⋆\n\nj −\n\nπ⋆\n\nj,τ )\n\n4dmax\n\nA\n\n∥\n\n∥∞\n\n≤\n\nfor every i\n\nV, t\n\n∈\n\n≥\n\n0. Applying the bound to the remaining nγ terms gives\n\nT (cid:88)\n\n(cid:88)\n\n−\n\nt=2γ+1\n\ni∈V\n\n(π(κ(t)\n\ni )\n\ni\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\n−\n\nπ⋆ τ )\n\n−\n\n≤\n\n4nγdmax\n\nA\n\n∥\n\n∥∞ .\n\n(85)\n\nThe remaining terms KL(cid:0)π⋆ proof postponed to Appendix E.9.\n\nτ ∥\n\nπ(2γ)(cid:1) and ψ(l) can be bounded with the following lemma, with the\n\nLemma 9. It holds for all i\n\nV and t\n\n0 that\n\n∈ ψ(t)\n\ni ≤\n\n≥ η(dmax\n\nA\n\n∥∞ (2γ + 11) + 3τ log\n\nSi |\n\n). |\n\n∥\n\nIn addition, we have\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\n(cid:1)\n\nπ(2γ)\n\ni\n\n≤\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\nπ(0)\n\ni\n\n(cid:1) + 4ηdmax\n\nA\n\n∥\n\n∥∞ γ.\n\nPutting all pieces together, we continue from (84) and show that\n\nT (cid:88)\n\nητ\n\nt=2γ+1\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t)(cid:1)\n\n(86)\n\n(87)\n\n(1\n\n−\n\nητ )KL(cid:0)π⋆\n\nτ ∥\n\nπ(2γ)(cid:1)\n\nη\n\n−\n\nT (cid:88)\n\n(cid:88)\n\nt=2γ+1\n\ni∈V\n\n(π(κ(t)\n\ni )\n\ni\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\n−\n\nπ⋆\n\nτ ) +\n\n−\n\n1 3\n\n(cid:88)\n\nl∈Γ\n\nψ(l)\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\nπ(0)\n\ni\n\n(cid:1) + 8ηnγdmax\n\nA\n\n∥\n\n∥∞ + ηγ\n\n(cid:16)\n\nndmax\n\nA\n\n∥\n\n∥∞ (2γ + 11) + 3τ\n\nlog\n\n(cid:88)\n\ni∈V\n\n(cid:17)\n\n|\n\nSi |\n\n(cid:17)(cid:105)\n\ndmax\n\nA\n\n∥\n\n∥∞ (2γ + 11) + 3τ log Smax\n\nKL(cid:0)π⋆\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\nπ(0)\n\ni\n\nπ(0)\n\n(cid:104) (cid:1) + 8ηn\n\nγdmax\n\n∥∞ + γ (cid:1) + n + 24ητ nγ log Smax.\n\nA\n\n∥\n\n(cid:16)\n\n≤\n\n≤\n\n≤\n\nπ(t−γ+1)(cid:1). By definition of KL divergence, we have\n\n≤\n\ni,τ ∥ Bounding the term KL(cid:0)π⋆\n\ni\n\nτ ∥\n\nπ(t−γ+1)\n\ni\n\n(cid:1)\n\nKL(cid:0)π⋆ i,τ ∥ = KL(cid:0)π⋆ = KL(cid:0)π⋆\n\nπ(t+1)\n\ni\n\nπ(t+1)\n\ni\n\n(cid:1) + (cid:10)π⋆ (cid:1)\n\n−\n\ni,τ ∥\n\ni,τ ∥\n\ni,τ , log π(t+1)\n\ni\n\nKL(cid:0)π(t−γ+1)\n\ni\n\ni\n\n∥\n\n(cid:11)\n\nlog π(t−γ+1) (cid:1) + (cid:10)π⋆\n\n− π(t+1)\n\ni\n\ni,τ −\n\nπ(t−γ+1)\n\ni\n\n, log π(t+1)\n\ni\n\n−\n\nlog π(t−γ+1)\n\ni\n\n(cid:11). (88)\n\n36\n\nPublished as a conference paper at ICLR 2023\n\nIt follows directly from the update rules that\n\n \n\n\n\nlog π(t−γ+1)\n\ni\n\n1= (1\n\nlog π(t+1)\n\ni\n\n1= (1\n\n−\n\n−\n\nητ ) log π(t−γ)\n\ni\n\n+ ηAiπ(κ(t−γ)\n\ni\n\n)\n\nητ )γ+1 log π(t−γ)\n\ni\n\n+ η\n\nt+1 (cid:88)\n\nl=t−γ+1\n\n(1\n\n−\n\nητ )t−l+1Aiπ(κ(l) i )\n\n,\n\nwhich enables us to control the term (cid:10)π⋆\n\nπ(t−γ+1)\n\ni\n\n, log π(t+1)\n\ni\n\nlog π(t−γ+1)\n\ni\n\n(cid:11) as\n\n−\n\ni,τ −\n\nπ(t−γ+1)\n\ni\n\n, log π(t+1)\n\ni\n\nlog π(t−γ+1)\n\n(cid:11)\n\ni\n\n(cid:10)π⋆\n\ni,τ −\n\n= η\n\nt+1 (cid:88)\n\n(1\n\n−\n\nl=t−γ+1\n\n− ητ )t−l+1 (cid:10)π⋆\n\nπ(t−γ+1)\n\ni\n\n, Ai(π(κ(t−γ)\n\ni\n\n)\n\nπ(κ(l)\n\ni ))(cid:11)\n\n−\n\ni,τ −\n\nη\n\nA\n\n∥\n\n∥∞\n\n≤\n\n(cid:13) (cid:13)π⋆\n\ni,τ −\n\nπ(t−γ+1)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:88)\n\nt+1 (cid:88)\n\nj∈Ni\n\nl=t−γ+1\n\n(cid:13)\n\n(cid:13)π(κ(t−γ)\n\ni\n\nj\n\nπ(κ(l)\n\ni )\n\nj\n\n(cid:13) (cid:13)1.\n\n(89)\n\n)\n\n−\n\nIn the same vein as Lemma 8, we can bound the term (cid:80)t+1 ,\ni } as detailed in the following lemma. The proof is omitted due to its similarity with that of Lemma 8. Lemma 10. When t\n\n2γ, it holds that\n\nψ(l) {\n\nl=t−γ+1\n\n(cid:13) (cid:13)1 with\n\n(cid:13)π(κ(t−γ)\n\nπ(κ(l)\n\ni )\n\n−\n\n(cid:13)\n\nj\n\nj\n\n)\n\ni\n\nt+1 (cid:88)\n\nl=t−γ+1\n\n(cid:13)\n\n(cid:13)π(κ(t−γ)\n\ni\n\nj\n\nπ(κ(l)\n\ni )\n\nj\n\n)\n\n−\n\n(cid:13) (cid:13)1 ≤\n\n4√2(γ + 1)\n\nt+γ+1 (cid:88)\n\n(cid:113)\n\nl=t−2γ+1\n\nψ(l)\n\ni + 2√2(γ + 1)2\n\n(cid:114)\n\nψ(νj (κ(t−γ)\n\ni\n\nj\n\n))\n\n.\n\n≥\n\nPlugging the above lemma into (89), we have (cid:11) (cid:10)π⋆ log π(t−γ+1) , log π(t+1)\n\nπ(t−γ+1)\n\ni,τ −\n\ni\n\ni\n\n−\n\ni\n\nη\n\nA\n\n∥\n\n∥∞\n\n(cid:13) (cid:13)π⋆\n\ni,τ −\n\nπ(t−γ+1)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:18)\n\n(cid:88)\n\nj∈Ni\n\n4√2(γ + 1)\n\nt+γ+1 (cid:88)\n\n(cid:113)\n\nl=t−2γ+1\n\nψ(l)\n\ni + 2√2(γ + 1)2\n\n(cid:114)\n\nψ(νj (κ(t−γ)\n\ni\n\nj\n\n(cid:19)\n\n))\n\n≤\n\n(i)\n\n≤\n\n(ii)\n\n≤\n\n(cid:26)\n\n1 2\n\nη\n\nA\n\n∥\n\n∥∞\n\n14dmax(γ + 1)3/2(cid:13)\n\n(cid:13)π⋆\n\ni,τ −\n\nπ(t−γ+1)\n\ni\n\n(cid:13) 2\n(cid:13) 1\n\n(cid:88)\n\n(cid:104)\n\n+\n\n8(γ + 1)3/2\n\nt+γ+1 (cid:88)\n\nl=t−2γ+1\n\nj + 4(γ + 1)5/2ψ(νj (κ(t−γ) ψ(l)\n\nj\n\ni\n\nj∈Ni\n\n(cid:26)\n\nη\n\nA\n\n∥\n\n∥∞\n\n14dmax(γ + 1)3/2KL(cid:0)π⋆\n\ni,τ ∥\n\nπ(t−γ+1)\n\ni\n\n(cid:1)\n\n(cid:88)\n\n(cid:104)\n\n+\n\n4(γ + 1)3/2\n\nt+γ+1 (cid:88)\n\nj∈Ni\n\nl=t−2γ+1\n\nj + 2(γ + 1)5/2ψ(νj (κ(t−γ) ψ(l)\n\nj\n\ni\n\n(cid:105)(cid:27)\n\n))\n\n(cid:105)(cid:27)\n\n))\n\n,\n\nwhere (i) results from similar arguments in (82) and (ii) invokes Pinsker’s inequality. Substitution of the above inequality into (88) and summing over i π(t−γ+1)(cid:1)\n\nV leads to\n\n(1\n\nA\n\n∈\n\n14ηdmax\n\n∥∞ (γ + 1)3/2)KL(cid:0)π⋆\n\n∥\n\n− KL(cid:0)π⋆\n\n≤\n\nπ(t+1)(cid:1) + η\n\nA\n\n∥\n\n∥∞\n\n(cid:88)\n\n(i,j)∈E\n\nτ ∥\n\nτ ∥ (cid:104)\n\n4(γ + 1)3/2\n\nt+γ+1 (cid:88)\n\nl=t−2γ+1\n\nj + 2(γ + 1)5/2ψ(νj (κ(t−γ) ψ(l)\n\nj\n\ni\n\n(cid:105)\n\n))\n\nKL(cid:0)π⋆\n\nτ ∥\n\n≤\n\nπ(t+1)(cid:1) + 4ηdmax\n\nA\n\n∥∞ (γ + 1)3/2\n\n∥\n\nt+γ+1 (cid:88)\n\nl=t−2γ+1\n\nSumming (cid:80) (cid:80)T −1 t=2γ\n\nthe\n\nabove\n\ni∈V KL(cid:0)π(κ(t+1)\n\ni\n\ni\n\ninequality )\n\nover (cid:1) to the both sides,\n\nt\n\nπ⋆ i,τ\n\n=\n\n∥\n\nT −1 (cid:88)\n\nt=2γ\n\n(cid:16) 2 3\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t−γ+1)(cid:1) +\n\n(cid:88)\n\ni∈V\n\nKL(cid:0)π(κ(t+1)\n\ni\n\ni\n\n(cid:1)(cid:17)\n\nπ⋆\n\ni,τ\n\n)\n\n∥\n\n37\n\nψ(l) + 2ηdmax\n\nA\n\n∥∞ (γ + 1)5/2ψ(νj (κ(t−γ)\n\ni\n\n∥\n\n)).\n\n2γ\n\n1,\n\n· · ·\n\n, T\n\n−\n\n−\n\n1\n\nand\n\nadding\n\nPublished as a conference paper at ICLR 2023\n\nT −1 (cid:88)\n\n≤\n\nt=2γ\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1) +\n\nT −1 (cid:88)\n\n(cid:88)\n\nt=2γ\n\ni∈V\n\nKL(cid:0)π(κ(t+1)\n\ni\n\ni\n\n(cid:1)\n\nπ⋆\n\ni,τ\n\n)\n\n∥\n\n+ 4ηdmax\n\nA\n\n∥∞ (γ + 1)3/2\n\n∥\n\nT −1 (cid:88)\n\nt+γ+1 (cid:88)\n\nt=2γ\n\nl=t−2γ+1\n\nψ(l) + 2ηdmax\n\nA\n\n∥∞ (γ + 1)5/2\n\n∥\n\nT −1 (cid:88)\n\nt=2γ\n\nψ(νj (κ(t−γ)\n\ni\n\n))\n\n(cid:26)\n\n(1\n\n(i)\n\n≤\n\n1 ητ\n\n(cid:16)\n\n1\n\n−\n\n−\n\n−\n\nητ )KL(cid:0)π⋆\n\nτ ∥\n\nπ(2γ)(cid:1)\n\nη\n\n−\n\nT (cid:88)\n\n(cid:88)\n\nt=2γ+1\n\ni∈V\n\n(π(κ(t)\n\ni )\n\ni\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\nπ⋆ τ )\n\n−\n\n−\n\n28ηdmax\n\n∥∞ (γ + 1)5/2(cid:17)\n\nA\n\n∥\n\nT (cid:88)\n\nt=2γ+1\n\nψ(t) + 14ηdmax\n\nA\n\n∥\n\n∥∞ (γ + 1)5/2 (cid:88)\n\nl∈Γ\n\n(cid:27)\n\nψ(l)\n\n+ 12ηdmax\n\nA\n\n∥\n\n∥∞ (γ + 1)5/2\n\nT +γ (cid:88)\n\nl=1\n\nψ(l) + 2ηdmax\n\nA\n\n∥∞ (γ + 1)5/2\n\n∥\n\nT +γ−1 (cid:88)\n\nψ(l)\n\nt=0\n\n(cid:26)\n\n(1\n\n=\n\n1 ητ\n\n(cid:16)\n\n1\n\n−\n\n−\n\n−\n\nητ )KL(cid:0)π⋆\n\nτ ∥\n\nπ(2γ)(cid:1)\n\nη\n\n−\n\nT (cid:88)\n\n(cid:88)\n\nt=2γ+1\n\ni∈V\n\n(π(κ(t)\n\ni )\n\ni\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\nπ⋆ τ )\n\n−\n\n−\n\n28(1 +\n\nητ 2\n\n)ηdmax\n\n∥∞ (γ + 1)5/2(cid:17)\n\nA\n\n∥\n\nT (cid:88)\n\nt=2γ+1\n\nψ(t) + 14(1 + ητ )ηdmax\n\nA\n\n∥\n\n∥∞ (γ + 1)5/2 (cid:88)\n\nl∈Γ\n\n(cid:27)\n\n.\n\nψ(l)\n\nHere, (i) invokes the bound established in (84). We remark that our choice of learning rate\n\nmin\n\nη\n\n≤\n\n(cid:110)\n\n1 2τ (γ + 1)\n\n,\n\n42dmax\n\nguarantees 1 9 gives\n\n−\n\n28(1 + ητ\n\n2 )ηdmax\n\nA\n\n∥\n\n∥∞ (γ + 1)5/2\n\n≥\n\n1\n\n(cid:111)\n\nA\n\n∥∞ (γ + 1)5/2\n\n∥ 0. This taken together with (85) and Lemma\n\nT −1 (cid:88)\n\nt=2γ\n\n(cid:16) 2 3\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t−γ+1)(cid:1) +\n\n(cid:88)\n\ni∈V\n\nKL(cid:0)π(κ(t+1)\n\ni\n\ni\n\n(cid:1)(cid:17)\n\nπ⋆\n\ni,τ\n\n)\n\n∥\n\n1 ητ\n\n1 ητ\n\n1 ητ\n\n≤\n\n≤\n\n≤\n\n(cid:26)\n\n(1\n\n−\n\nητ )KL(cid:0)π⋆\n\nτ ∥\n\nπ(2γ)(cid:1)\n\nη\n\n−\n\nT (cid:88)\n\n(cid:88)\n\nt=2γ+1\n\ni∈V\n\n(π(κ(t)\n\ni )\n\ni\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\nπ⋆\n\nτ ) +\n\n−\n\n−\n\n(cid:27)\n\nψ(l)\n\n1 2\n\n(cid:88)\n\nl∈Γ\n\n(cid:26)\n\n(cid:26)\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\nπ(0)\n\ni\n\nπ(0)\n\ni\n\n(cid:104) (cid:1) + 8ηn\n\nA\n\nγdmax\n\n∥∞ + (cid:1) + n + 36ητ nγ log Smax\n\n∥\n\n3γ 2\n(cid:27)\n\n(cid:16)\n\ndmax\n\nA\n\n∥\n\n∥∞ (2γ + 11) + 3τ log Smax\n\n(cid:17)(cid:105)(cid:27)\n\n.\n\n(90)\n\nBounding the QRE gap. With Lemma 3, we have\n\nT −γ−1 (cid:88)\n\nt=2γ\n\nQRE-Gapτ (π(t+1))\n\n≤\n\n≤\n\nT −γ−1 (cid:88)\n\nt=2γ\n\n(cid:16) d2\n\nmax ∥ τ\n\nA\n\n2 ∞\n∥\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1) + τ KL(cid:0)π(t+1)\n\n(cid:1)(cid:17)\n\nπ⋆\n\nτ\n\n∥\n\nmax\n\n(cid:110) 3d2\n\nmax ∥ 2τ\n\nA\n\n2 ∞\n\n∥\n\n, τ\n\n(cid:111)\n\nT −γ−1 (cid:88)\n\nt=2γ\n\n(cid:16) 2 3\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1) + KL(cid:0)π(t+1)\n\n(cid:1)(cid:17)\n\n.\n\nπ⋆\n\nτ\n\n∥\n\nSince the mapping t\n\nνi(t) is injective, we have\n\n(cid:55)→\n\nT −γ−1 (cid:88)\n\n(cid:88)\n\nt=2γ\n\ni∈V\n\nKL(cid:0)π(t+1)\n\ni\n\nπ⋆\n\ni,τ\n\n(cid:1) =\n\n∥\n\nT −γ−1 (cid:88)\n\n(cid:88)\n\nt=2γ\n\ni∈V\n\nKL(cid:0)π(κ(νi(t+1))\n\ni\n\ni\n\n(cid:1)\n\nπ⋆\n\ni,τ\n\n)\n\n∥\n\n≤\n\nT −1 (cid:88)\n\n(cid:88)\n\nt=2γ\n\ni∈V\n\nKL(cid:0)π(κ(t+1)\n\ni\n\ni\n\nπ⋆\n\ni,τ\n\n(cid:1).\n\n)\n\n∥\n\nCombining the above two equalities gives\n\nT −γ−1 (cid:88)\n\nt=2γ\n\nQRE-Gapτ (π(t+1))\n\n38\n\nPublished as a conference paper at ICLR 2023\n\n(cid:110) 3d2\n\nmax ∥ 2τ\n\nA\n\n2 ∞\n\n∥\n\n, τ\n\n(cid:111)(cid:16)\n\nT −γ−1 (cid:88)\n\nt=2γ\n\n2 3\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t+1)(cid:1) +\n\nmax\n\nmax\n\nmax\n\n≤\n\n≤\n\n≤\n\n(cid:110) 3d2\n\n(cid:110) 3d2\n\nA\n\nA\n\n2 ∞\n\n∥\n\n, τ\n\n(cid:111) T −1 (cid:88)\n\n2 ∞\n\n∥\n\n, τ\n\nt=2γ\n\n(cid:16)\n\n(cid:111) 1 ητ\n\nmax ∥ 2τ\n\nmax ∥ 2τ\n\nT −1 (cid:88)\n\nt=2γ\n\n(cid:88)\n\ni∈V\n\nKL(cid:0)π(t+1)\n\n(cid:1)(cid:17)\n\nπ⋆\n\nτ\n\n∥\n\nKL(cid:0)π(κ(t+1)\n\ni\n\ni\n\n(cid:1)(cid:17)\n\nπ⋆\n\ni,τ\n\n)\n\n∥\n\n(cid:16) 2 3\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(t−γ+1)(cid:1) +\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\nπ(0)\n\ni\n\n(cid:1) + n + 36ητ nγ log Smax\n\n(cid:17)\n\n,\n\nwhere the last step results from (90).\n\nE PROOF OF AUXILIARY LEMMAS\n\nE.1 PROOF OF LEMMA 1\n\nTo prove this lemma, we recall a key observation in Cai et al. (2016) that allows one to transform a zero-sum polymatrix game into a pairwise constant-sum polymatrix game (cid:101) G\n\n{ {\n(cid:101)Aij }(i,j)∈E}\n\nAij such that\n\n(V, E), {\ni∈V ,\n\nG (V, E),\n\n}(i,j)∈E}\n\n= Si\n\ni∈V ,\n\nSi\n\n=\n\n{\n\n}\n\n{\n\n}\n\n{\n\n(1) For every player i\n\n∈\n\nV , it has the same payoff in\n\nG\n\n: and (cid:101) G\ns\n\nS.\n\nui(s) = (cid:101)ui(s),\n\n(2) For each pair (i, j)\n\n∈ αij = αji, such that\n\nE, i\n\n∀ = j, the two-player game (cid:101) is constant-sum, i.e., there exist constants G\n\n∈\n\nholds for all si\n\nSi, sj\n\nSj.\n\n∈\n\n∈\n\n(cid:101)Aij(si, sj) + (cid:101)Aji(sj, si) = αij\n\n(91)\n\nWe are now in a place to prove Lemma 1. Let (cid:101) G\nassociated with\n\nG (cid:88)\n\nafter the above payoff preserving transformation. We have (cid:2)ui(πi, π′\n\n−i) + ui(π′\n\ni, π−i)(cid:3)\n\nbe the pairwise constant-sum polymatrix game\n\ni∈V\n\n=\n\n=\n\n=\n\n=\n\n(cid:2) (cid:101)ui(πi, π′\n\n−i) + (cid:101)ui(π′\n\ni, π−i)(cid:3)\n\n(cid:88)\n\ni∈V\n\n(cid:105)\n\n(cid:101)Aij(si, sj)\n\n+\n\nE i,sj ∼πj\n\nsi∼π′\n\n(cid:35)\n\n(cid:104)\n\n(cid:105)\n\n(cid:101)Aij(si, sj)\n\n(cid:105)\n\n(cid:101)Aij(si, sj)\n\n+\n\nE i,sj ∼πj\n\nsi∼π′\n\n(cid:104)\n\nαij\n\n−\n\n(cid:105)\n\n(cid:101)Aji(sj, si)\n\n(cid:35)\n\n(cid:34)\n\nE si∼πi,sj ∼π′\n\nj\n\n(cid:34)\n\nE si∼πi,sj ∼π′\n\nj\n\n(cid:104)\n\n(cid:104)\n\nαij = 0,\n\n(cid:88)\n\n(i,j)∈E\n\n(cid:88)\n\n(i,j)∈E (cid:88)\n\n(i,j)∈E\n\nwhere the penultimate line uses (91), and the last line uses the fact that (cid:101) G\nmatrix game, which satisfies\n\nis also a zero-sum poly-\n\n(cid:88)\n\nαij =\n\n(cid:88)\n\n(cid:104)\n\n(i,j)∈E\n\n(i,j)∈E\n\n(cid:101)Aij(si, sj) + (cid:101)Aji(sj, si)\n\n(cid:105)\n\n=\n\n(cid:88)\n\ni∈V\n\n(cid:101)ui(s) +\n\n(cid:88)\n\nj∈V\n\n(cid:101)uj(s) = 0\n\nfor any arbitrary s\n\nS.\n\n∈\n\nE.2 PROOF OF LEMMA 2\n\nIn view of the update rule (7), we have\n\nlog π(t+1)\n\ni\n\n= (1\n\n−\n\nητ ) log π(t)\n\ni + ηAiπ(t+1) + ci1\n\n39\n\n̸ Published as a conference paper at ICLR 2023\n\nfor some constant ci. On the other hand, it follows from the expression of QRE in (4) that\n\nητ log π⋆\n\ni,τ = ηAiπ⋆\n\nτ + c⋆\n\ni 1\n\n(92)\n\nfor some constant c⋆ π(t+1) π⋆\n\ni,τ , we have\n\ni\n\n−\n\ni . By combining the above two equalities and taking the inner product with\n\n(cid:10)log π(t+1)\n\n(1\n\nητ ) log π(t)\n\ni\n\ni − Summing the above equality over i\n\n−\n\n−\n\nV gives\n\n∈\n\nητ log π⋆\n\ni,τ , π(t+1)\n\ni\n\nπ⋆\n\ni,τ\n\n(cid:11) = η(π(t+1)\n\ni\n\n−\n\nπ⋆\n\ni,τ )⊤Ai(π(t+1)\n\n−\n\nπ⋆ τ ). (93)\n\n−\n\n(cid:10) log π(t+1) (cid:88)\n\n= η\n\n− (π(t+1)\n\ni\n\n(1\n\n−\n\n−\n\nητ ) log π(t)\n\nητ log π⋆\n\nτ , π(t+1)\n\n−\n\n(cid:11)\n\nπ⋆\n\nτ\n\n−\n\nπ⋆\n\ni,τ )⊤Ai(π(t+1)\n\nπ⋆ τ )\n\n−\n\n(cid:104) (π(t+1)\n\ni\n\n)⊤Aiπ(t+1) + (π⋆\n\ni,τ )⊤Aiπ⋆\n\nτ\n\n(cid:105)\n\nη\n\n−\n\n(cid:88)\n\ni∈V\n\n(cid:104) (π(t+1)\n\ni\n\n)⊤Aiπ⋆\n\nτ + (π⋆\n\ni,τ )⊤Aiπ(t+1)(cid:105)\n\n(cid:104) ui(π(t+1)) + ui(π⋆ τ )\n\n(cid:105)\n\n= 0,\n\n= η\n\n= η\n\ni∈V (cid:88)\n\ni∈V (cid:88)\n\ni∈V\n\nwhere the last line follows from (cid:80) as well as that the game is zero-sum.\n\ni∈V\n\n(cid:104)\n\n(π(t+1)\n\ni\n\nE.3 PROOF OF LEMMA 3\n\nRecalling the definition\n\n)⊤Aiπ⋆\n\nτ + (π⋆\n\ni,τ )⊤Aiπ(t+1)(cid:105)\n\n= 0 due to Lemma 1,\n\nQRE-Gapτ (π) = max\n\ni∈V\n\n(cid:20)\n\nmax i∈∆(Si)\n\nπ′\n\nui,τ (π′\n\ni, π−i)\n\n(cid:21)\n\nui,τ (π)\n\n−\n\n(cid:20)\n\nmax i∈∆(Si)\n\nπ′\n\n(cid:88)\n\ni∈V\n\nui,τ (π′\n\ni, π−i)\n\nui,τ (π)\n\n−\n\n(cid:21)\n\nmax\n\ni∈V :π′\n\ni∈∆(Si)\n\n(cid:88)\n\ni∈V\n\n[ui,τ (π′\n\ni, π−i)\n\n−\n\nui,τ (πi, π−i)] ,\n\n≤\n\n=\n\nwhere the inequality holds since maxπ′ for all i\n\nV . We now proceed to decompose\n\ni∈∆(Si) ui,τ (π′\n\ni, π−i)\n\nui,τ (π)\n\n−\n\n≥\n\nui,τ (πi, π−i)\n\nui,τ (π) = 0\n\n−\n\n∈\n\n(cid:88)\n\ni∈V\n\n[ui,τ (π′\n\ni, π−i)\n\n−\n\nui,τ (πi, π−i)]\n\n=\n\n=\n\n(cid:88)\n\ni∈V (cid:88)\n\ni∈V\n\n(cid:2)ui,τ (π′\n\ni, π−i)\n\n(cid:2)ui,τ (π′\n\ni, π−i)\n\n−\n\n−\n\nui,τ (π⋆\n\ni,τ , π⋆\n\n−i,τ )(cid:3)\n\nτ\n\n−\n\n(cid:88)\n\n(cid:0)\n\ni∈V\n\n(πi)\n\nH\n\n− H\n\n(π⋆\n\ni,τ )(cid:1)\n\nui,τ (π′\n\ni, π⋆\n\n−i,τ )\n\nui,τ (π⋆\n\ni,τ , π−i) + ui,τ (π⋆\n\ni,τ , π⋆\n\n−i,τ )(cid:3)\n\n−\n\n+\n\n+\n\n(cid:88)\n\ni∈V (cid:88)\n\ni∈V\n\n(cid:2)ui,τ (π⋆\n\ni,τ , π−i)\n\n(cid:2)ui,τ (π′\n\ni, π⋆\n\n−i,τ )\n\n−\n\n−\n\nui,τ (π⋆\n\ni,τ , π⋆\n\n−i,τ )\n\nτ (cid:0)\n\n(πi)\n\nH\n\n− H\n\n(π⋆\n\ni,τ )(cid:1)(cid:3)\n\n(94)\n\n− −i,τ )(cid:3)\n\nui,τ (π⋆\n\ni,τ , π⋆\n\nwhere the first line follows from (cid:80) (π⋆ by the definition of zero-sum games. It boils down to control the terms on the RHS of (94).\n\n(πi)) = (cid:80)\n\ni∈V (ui,τ (π)\n\n(cid:0)ui,τ (π⋆ τ )\n\ni∈V\n\nH\n\nH\n\n−\n\n−\n\nτ\n\nτ\n\ni,τ )(cid:1) = 0\n\n• To control the first term, by the definition of ui,τ in (5) (see also (3)), it follows that\n\nui,τ (π′ = ui(π′\n\ni, π−i)\n\n− i, π−i)\n\nui,τ (π′ ui(π′\n\ni, π⋆ i, π⋆\n\n−i,τ ) −i,τ )\n\n−\n\n−\n\n−\n\nui,τ (π⋆ ui(π⋆\n\ni,τ , π−i) + ui,τ (π⋆\n\ni,τ , π−i) + ui(π⋆\n\ni,τ , π⋆\n\n−i,τ )\n\ni,τ , π⋆ −i,τ )\n\n40\n\nPublished as a conference paper at ICLR 2023\n\n= (π′\n\ni −\n\nπ⋆\n\ni,τ )⊤Ai(π\n\nπ⋆\n\nτ ) =\n\n−\n\n(cid:88)\n\nj∈Ni\n\n(π′\n\ni −\n\nπ⋆\n\ni,τ )⊤Aij(πj\n\nπ⋆\n\nj,τ ),\n\n−\n\nwhich each summand can be further bounded by Young’s inequality and Pinsker’s inequality as\n\n(π′\n\ni −\n\nπ⋆\n\ni,τ )⊤Aij(πj\n\nπ⋆\n\nj,τ )\n\n−\n\nπ⋆\n\ni,τ\n\n(cid:13) (cid:13)1\n\n(cid:13) (cid:13)πj\n\nπ⋆\n\nj,τ\n\n(cid:13) (cid:13)1\n\nA\n\n≤ ∥ 1\n2 ∥\n\n≤\n\n∥∞\n\n(cid:13) (cid:13)π′ i − (cid:18)\n\nA\n\n∥∞ (cid:18)\n\ndmax τ\n\nA\n\nτ\n\n∥\n\nA\n\n− (cid:13) (cid:13)π′\n\n∥∞ KL(cid:0)π′\n\ndmax\n\nA\n\n∥\n\n∥∞\n\nπ⋆\n\ni,τ\n\n(cid:13) 2\n(cid:13) 1\n\n+\n\ndmax\n\nA\n\n∥ τ\n\n∥∞\n\n(cid:13) (cid:13)πj\n\n−\n\nπ⋆\n\nj,τ\n\n(cid:13) 2\n(cid:13) 1\n\ni −\n\nπ⋆\n\ni,τ\n\n(cid:1) +\n\ndmax\n\nA\n\n∥ τ\n\n∥∞\n\nKL(cid:0)π⋆\n\nj,τ ∥\n\n(cid:19)\n\n(cid:1)\n\n.\n\nπj\n\ni ∥\n\n(cid:19)\n\n−\n\ni, π⋆\n\n−i,τ )\n\nui,τ (π⋆\n\ni,τ , π−i) + ui,τ (π⋆\n\ni,τ , π⋆\n\n−i,τ )(cid:3)\n\n−\n\n∥∞ Summing the inequality over i, j gives (cid:2)ui,τ (π′\n\ni, π−i)\n\n≤ ∥\n\nui,τ (π′\n\n(cid:88)\n\ni∈V\n\nτ KL(cid:0)π′\n\n(cid:1) +\n\nπ⋆\n\nτ\n\n∥\n\n≤\n\nd2\n\nmax ∥ τ\n\nA\n\n2 ∞\n\n∥\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(cid:1).\n\n• Regarding the second term, we have (cid:2)ui,τ (π⋆\n\ni,τ , π−i)\n\n(cid:88)\n\nui,τ (π⋆\n\n−\n\ni,τ , π⋆\n\n−i,τ )\n\nτ (cid:0)\n\n−\n\n(πi)\n\nH\n\n− H\n\n(π⋆\n\ni,τ )(cid:1)(cid:3)\n\ni∈V\n\n=\n\n=\n\n=\n\n(cid:88)\n\ni∈V (cid:88)\n\ni∈V (cid:88)\n\ni∈V\n\n(cid:2)(π⋆\n\ni,τ )⊤Ai(π\n\n(cid:2)(π⋆\n\ni,τ )⊤Ai(π\n\n(cid:2)(π⋆\n\ni,τ )⊤Ai(π\n\n−\n\n−\n\n−\n\n(π⋆\n\ni,τ )⊤ log π⋆\n\ni,τ )(cid:3)\n\nπ⋆\n\nτ ) + τ (π⊤\n\ni\n\nlog πi\n\n− τ ) + τ (cid:0)(cid:10)πi, log πi π⋆\n\nlog π⋆\n\ni,τ\n\n(cid:11) + (cid:10)πi\n\n−\n\nπ⋆\n\ni,τ , log π⋆\n\ni,τ\n\n−\n\nπ⋆\n\nτ ) + (πi\n\n−\n\nπ⋆\n\ni,τ )⊤Aiπ⋆\n\nτ + τ KL(cid:0)πi\n\n(cid:1)(cid:3)\n\nπ⋆\n\ni,τ\n\n∥\n\n= τ KL(cid:0)π\n\n(cid:1),\n\nπ⋆\n\nτ\n\n∥\n\nwhere the penultimate step follows from (92) and the last step invokes Lemma 1.\n\n• Moving to the last term, we have\n\n(95)\n\n(96)\n\n(cid:11)(cid:1)(cid:3)\n\nui,τ (π⋆\n\ni,τ , π⋆\n\n−i,τ )\n\nui,τ (π′\n\ni, π⋆\n\n−i,τ ) = (π⋆\n\n−\n\n= τ (π⋆ i,τ − = τ KL(cid:0)π′ i ∥ where the second line follows again from (92).\n\ni,τ −\n\nπ′\n\ni)⊤Aiπ⋆ τ − i)⊤ log π⋆ π′ (cid:1). π⋆\n\ni,τ\n\nτ (π⋆\n\ni,τ )⊤ log π⋆ τ (π⋆\n\ni,τ )⊤ log π⋆\n\ni,τ −\n\ni,τ + τ (π′\n\ni)⊤ log π′\n\ni\n\ni,τ + τ (π′\n\ni)⊤ log π′ (97)\n\ni\n\nPlugging (95), (96) and (97) into (94) gives\n\n(cid:88)\n\ni∈V\n\n[ui,τ (π′\n\ni, π−i)\n\n−\n\nui,τ (πi, π−i)]\n\nτ KL(cid:0)π\n\n(cid:1) +\n\nπ⋆\n\nτ\n\n∥\n\n≤\n\nTaking maximum over π′ finishes the proof.\n\nE.4 PROOF OF LEMMA 4\n\nd2\n\nmax ∥ τ\n\nA\n\n2 ∞\n\n∥\n\nKL(cid:0)π⋆\n\nτ ∥\n\nπ(cid:1).\n\nLet (cid:101)π(T ) = 1\n\nT +1\n\n(cid:80)T\n\nt=0 π(t+1), then (cid:101)π(T ) (cid:88)\n\n∈ (cid:0)T + 1(cid:1)\n\nRegreti,τ\n\ni∈V\n\n∆(S). The proof is completed if we can show\n\n(cid:88)\n\n≥\n\ni∈V\n\nRegreti,τ\n\n(cid:0)\n\n(cid:101)π(T )\n\ni\n\n, T + 1(cid:1)\n\n0,\n\n≥\n\n(98)\n\nwhere the first inequality holds trivially since Regreti,τ It then boils down to show the second inequality of the above relation. From the definition of zero-sum polymatrix games, it holds that\n\nRegreti,τ\n\n≥\n\n(cid:0)\n\n(cid:101)π(T )\n\ni\n\n, T (cid:1).\n\n(cid:0)T + 1(cid:1)\n\n(cid:88)\n\nT (cid:88)\n\n(cid:10)\n\ni∈V\n\nt=0\n\n(cid:101)π(T ) i −\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)(cid:11) =\n\n(cid:88)\n\nT (cid:88)\n\ni∈V\n\nt=0\n\n41\n\n(cid:10)\n\n(cid:101)π(T )\n\ni\n\n, Aiπ(t+1)(cid:11)\n\nPublished as a conference paper at ICLR 2023\n\n=\n\n(cid:88)\n\ni∈V\n\n(cid:10)\n\n(cid:101)π(T )\n\ni\n\n, Ai\n\nT (cid:88)\n\nt=0\n\nπ(t+1)(cid:11)\n\n= (T + 1)\n\n(cid:88)\n\ni∈V\n\n(cid:10)\n\n(cid:101)π(T )\n\ni\n\n, Ai(cid:101)π(T )(cid:11) = 0.\n\nIn addition, applying Jensen’s inequality gives\n\nT (cid:88) t=0 H\n\n((cid:101)π(T )\n\ni\n\n) = (T + 1)\n\n((cid:101)π(T )\n\ni\n\n)\n\nH\n\n≥\n\nT (cid:88) t=0 H\n\n(π(t+1)\n\ni\n\n).\n\nCombining the above two relations yields\n\nRegreti,τ\n\n(cid:0)\n\n(cid:101)π(T )\n\ni\n\n, T + 1(cid:1)\n\n(cid:88)\n\ni∈V\n\n(cid:88)\n\nT (cid:88)\n\n≥\n\ni∈V\n\nt=0\n\n(cid:16)(cid:10)\n\n(cid:101)π(T ) i −\n\nπ(t+1)\n\ni\n\n, Aiπ(t+1)(cid:11) + τ\n\n((cid:101)π(T )\n\ni\n\n)\n\nH\n\nτ\n\nH\n\n−\n\n(π(t+1)\n\ni\n\n)\n\n(cid:17)\n\n0,\n\n≥\n\nwhich concludes the proof.\n\nE.5 PROOF OF LEMMA 5\n\nTaking logarithm on the both sides of (7), we have\n\nOn the other hand, the definition of QRE in (4) gives\n\nlog π(t+1)\n\ni\n\n1= (1\n\n−\n\nητ ) log π(t)\n\ni + ηAiπ(t−γ+1).\n\n(99)\n\nητ log π⋆\n\ni,τ\n\n1= ηAiπ⋆ τ .\n\nSubtracting the two equalities and taking inner product with π(t−γ+1)\n\n(cid:10)log π(t+1) i\n(cid:16)\n\n= η\n\n(1\n\n−\n\n−\n\nητ ) log π(t) i − (cid:16) (cid:17)⊤\n\nπ(t−γ+1)\n\ni\n\nπ⋆\n\ni,τ\n\nAi\n\nπ(t−γ+1)\n\nητ log π⋆\n\n(cid:17)\n\n.\n\nπ⋆\n\nτ\n\n−\n\n− Summing the above equality over i\n\ni\n\n− i,τ , π(t−γ+1)\n\ni\n\nπ⋆\n\ni,τ , we get\n\n(cid:11)\n\nπ⋆\n\ni,τ\n\n−\n\nV leads to\n\n∈\n\n(1\n\nητ ) log π(t)\n\n− −\nπ(t−γ+1)\n\ni\n\n(cid:17)⊤\n\nπ⋆\n\ni,τ\n\n−\n\n(cid:10)log π(t+1) (cid:16)\n\n(cid:88)\n\n= η\n\ni∈V\n\nητ log π⋆ (cid:16)\n\nπ(t−γ+1)\n\n− Ai\n\nτ , π(t−γ+1)\n\ni\n\n(cid:11)\n\nπ⋆\n\nτ\n\n− = 0,\n\n(cid:17)\n\nπ⋆\n\nτ\n\n−\n\nwhere the final step results from Lemma 1.\n\nE.6 PROOF OF LEMMA 6\n\nRecall from (71) that π(t+1)(cid:1) = (1 KL(cid:0)π⋆\n\nτ ∥\n\nητ )KL(cid:0)π⋆ τ ∥ + (cid:10)log π(t−γ+1)\n\n−\n\n(1\n\nπ(t)(cid:1) −\nlog π(t+1), π(t−γ+1)\n\n−\n\nητ )KL(cid:0)π(t−γ+1)\n\n∥ π(t+1)(cid:11)\n\nπ(t)(cid:1)\n\nKL(cid:0)π(t+1) ∥\nητ KL(cid:0)π(t−γ+1)\n\n−\n\n−\n\n−\n\n−\n\nπ(t−γ+1)(cid:1) (cid:1). π⋆ τ\n(100)\n\n∥\n\nWhen t < γ, we have π(t−γ+1)\n\ni\n\n= π(0). It follows that\n\nlog π(t−γ+1)\n\ni\n\n= log π(0) 1= 0,\n\nand that\n\nlog π(t+1)\n\ni\n\n1= (1\n\n−\n\nητ )t+1 log π(0) + η\n\nt (cid:88)\n\n(1\n\nl=0\n\n−\n\nητ )lAiπ(t−γ−l+1)\n\n42\n\nPublished as a conference paper at ICLR 2023\n\n1= η\n\nt (cid:88)\n\nl=0\n\n(1\n\n−\n\nητ )lAiπ(0).\n\nTherefore, we can bound the term (cid:10)log π(t−γ+1)\n\nlog π(t+1), π(t−γ+1)\n\n−\n\nπ(t+1)(cid:11) as\n\n−\n\n(cid:10)log π(t−γ+1)\n\n−\n\nlog π(t+1), π(t−γ+1)\n\nπ(t+1)(cid:11) = (cid:10)η\n\n−\n\nt (cid:88)\n\nl=0\n\n(1\n\n−\n\nητ )lAiπ(0), π(0)\n\nπ(t+1)(cid:11)\n\n−\n\nη(t + 1)dmax\n\n∥ 2η(t + 1)dmax\n\n≤\n\n≤\n\n(cid:13) (cid:13)π(0) (cid:13)\n\nA\n\n∥∞ ∥∞. A\n\n∥\n\nπ(t+1)(cid:13) (cid:13) (cid:13)1\n\n−\n\n(101)\n\nPlugging the above inequality into (100) leads to KL(cid:0)π⋆\n\nπ(t+1)(cid:1)\n\n(1\n\n(1\n\n≤\n\nητ )KL(cid:0)π⋆ −\n+ 2η(t + 1)dmax\n\nτ ∥\n\nπ(t)(cid:1) A\n\n−\n\n∥∞.\n\n∥\n\nητ )KL(cid:0)π(t−γ+1)\n\nπ(t)(cid:1)\n\n∥\n\n−\n\nKL(cid:0)π(t+1)\n\n∥\n\nπ(t−γ+1)(cid:1)\n\n−\n\nApplying the above inequality recursively to the iterates 0, 1, . . . , γ KL(cid:0)π⋆\n\nπ(γ)(cid:1)\n\n1, we arrive at\n\n−\n\nτ ∥\n\nτ ∥\n\n(1\n\n−\n\n≤\n\nητ )γKL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1)\n\nγ−1 (cid:88)\n\n−\n\nl1=0\n\n(1\n\n−\n\nητ )γ−1−l1\n\n(cid:104)\n\n(1\n\n+ 2η\n\nγ−1 (cid:88)\n\nl1=0\n\n(1\n\n−\n\nητ )γ−1−l1(l1 + 1)dmax\n\nA\n\n∥\n\n∥∞\n\n(1\n\n−\n\n≤\n\nητ )γKL(cid:0)π⋆\n\nτ ∥\n\nπ(0)(cid:1)\n\nγ−1 (cid:88)\n\n−\n\nl1=0\n\n(1\n\n−\n\nητ )γ−1−l1\n\n(cid:104)\n\n(1\n\n+ 2ηγ2dmax\n\nA\n\n∥∞.\n\n∥\n\nE.7 PROOF OF LEMMA 7\n\nητ )KL(cid:0)π(l1−γ+1)\n\nητ )KL(cid:0)π(l1−γ+1)\n\n∥\n\n∥\n\n−\n\n−\n\nπ(l1)(cid:1) + KL(cid:0)π(l1+1)\n\nπ(l1)(cid:1) + KL(cid:0)π(l1+1)\n\nπ(l1−γ+1)(cid:1)(cid:105)\n\nπ(l1−γ+1)(cid:1)(cid:105)\n\n∥\n\n∥\n\nTaking logarithm on the both sides of (77) and (78), we get\n\nor equivalently\n\nη(cid:0)log (cid:101)π(t)\n\ni −\n\nlog π(t−1)\n\ni\n\n(cid:1) 1= (cid:101)η(t)\n\ni\n\n(cid:0)log π(t)\n\ni −\n\nlog π(t−1)\n\ni\n\n(cid:1),\n\nlog π(t)\n\ni\n\n1=\n\nη\n\n(cid:101)η(t)\n\ni\n\nlog (cid:101)π(t)\n\ni +\n\n(cid:16)\n\n1\n\nη\n\n(cid:17)\n\n−\n\n(cid:101)η(t)\n\ni\n\nlog π(t−1)\n\ni\n\n.\n\nTaking inner product with π⋆\n\nπ(t)\n\ni\n\n,\n\n(cid:10)log π(t)\n\ni −\n\nlog (cid:101)π(t)\n\ni −\n\n(cid:16)\n\n1\n\nη\n\n(cid:17)\n\n−\n\n(cid:101)η(t)\n\ni\n\nlog π(t−1)\n\ni\n\n, π⋆\n\ni,τ −\n\nπ(t)\n\ni\n\n(cid:11) = 0.\n\ni,τ − η\n\n(cid:101)η(t)\n\ni\n\nBy definition of KL divergence, we have\n\n(cid:10)log π(t)\n\ni −\n\nη\n\n(cid:101)η(t)\n\ni\n\nlog (cid:101)π(t)\n\ni −\n\n= (cid:10)(log π(t)\n\ni −\n\nlog π⋆\n\ni,τ )\n\n−\n\n(cid:16)\n\n1\n\nη\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\n−\n\nπ(t)\n\ni\n\n(cid:1) +\n\n(cid:16)\n\n1\n\n=\n\nand\n\nη\n\n(cid:17)\n\nlog π(t−1)\n\ni\n\n(cid:11)\n\n, π⋆\n\ni,τ\n\n−\n\ni\n\n(cid:101)η(t) (log (cid:101)π(t)\n\ni −\n\nη\n\n(cid:17)\n\nKL(cid:0)π⋆\n\n(cid:101)η(t)\n\ni\n\n−\n\n(cid:101)η(t)\n\ni\n\nlog π⋆\n\ni,τ )\n\n(cid:16)\n\n−\n\nπ(t−1)\n\ni\n\n(cid:1) +\n\ni,τ ∥\n\nη\n\n(cid:17)\n\ni\n\n(cid:101)η(t) KL(cid:0)π⋆\n\n1\n\n− η\n\n(cid:101)η(t)\n\ni\n\n(log π(t−1)\n\ni\n\nlog π⋆\n\ni,τ ), π⋆\n\ni,τ\n\n(cid:11)\n\n−\n\ni,τ ∥ (cid:101)π(t)\n\ni\n\n(cid:1),\n\n(cid:10)log π(t)\n\ni −\n\nη\n\n(cid:101)η(t)\n\ni\n\nlog (cid:101)π(t)\n\ni −\n\n(cid:16)\n\n1\n\nη\n\n(cid:17)\n\n−\n\n(cid:101)η(t)\n\ni\n\nlog π(t−1)\n\ni\n\n(cid:11)\n\n, π(t)\n\ni\n\n43\n\nPublished as a conference paper at ICLR 2023\n\n=\n\nη\n\n(cid:101)η(t)\n\ni\n\nKL(cid:0)π(t)\n\ni ∥ (cid:101)π(t)\n\ni\n\n(cid:1) +\n\n(cid:16)\n\n1\n\nη\n\n(cid:17)\n\n−\n\n(cid:101)η(t)\n\ni\n\nKL(cid:0)π(t)\n\ni ∥\n\nπ(t−1)\n\ni\n\n(cid:1).\n\nTaken together, we get\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\nπ(t)\n\ni\n\nη\n\n(cid:1) +\n\ni\n\n(cid:101)η(t) KL(cid:0)π⋆\n\n(cid:17)\n\nKL(cid:0)π(t)\n\ni\n\ni ∥ (cid:101)π(t) (cid:1) +\n\nπ(t−1)\n\ni\n\ni,τ ∥\n\n(cid:1) +\n\n(cid:16)\n\n1\n\nη\n\n(cid:17)\n\n−\n\ni\n\n(cid:101)η(t) i,τ ∥ (cid:101)π(t)\n\ni\n\nKL(cid:0)π⋆\n\nη\n\n(cid:101)η(t)\n\ni\n\n(cid:16)\n\n1\n\n=\n\nη\n\n−\n\n(cid:101)η(t)\n\ni\n\nKL(cid:0)π(t)\n\ni ∥\n\nπ(t−1)\n\ni\n\n(cid:1)\n\n(cid:1).\n\n(102)\n\nOn the other hand, taking logarithm of (78) and making inner product with π(κ(t)\n\ni )\n\ni\n\nπ⋆\n\ni,τ gives\n\n(1\n\n(cid:10)log (cid:101)π(t) = (cid:101)η(t)\n\ni − i (π(κ(t)\n\ni\n\ni )\n\n− (cid:101)η(t)\n\ni τ ) log π(t−1) i,τ )⊤Ai(π(κ(t) π⋆\n\ni\n\ni )\n\n−\n\n− (cid:101)η(t)\n\nπ⋆\n\nτ ).\n\n−\n\ni τ log π⋆\n\ni,τ , π(κ(t)\n\ni )\n\ni\n\nπ⋆\n\ni,τ\n\n−\n\nFollowing a similar discussion in (19) gives\n\n− (cid:11)\n\nKL(cid:0)π⋆\n\ni,τ ∥ (cid:101)π(t)\n\ni\n\n(cid:1) = (1\n\nπ(t−1)\n\ni\n\n(cid:1)\n\ni τ )KL(cid:0)π⋆ i,τ ∥ i τ KL(cid:0)π(κ(t) i )\n\n− (cid:101)η(t) − (cid:101)η(t) + (cid:10)log π(κ(t)\n\ni )\n\ni\n\ni\n\nπ⋆\n\ni,τ\n\n∥\n\nlog (cid:101)π(t)\n\ni\n\n−\n\n(1\n\n(cid:1)\n\n− KL(cid:0) −\n, π(κ(t)\n\ni )\n\ni\n\n− (cid:101)η(t) (cid:101)π(t) i ∥ − (cid:101)π(t)\n\ni\n\ni τ )KL(cid:0)π(κ(t)\n\ni )\n\ni\n\nπ(t−1)\n\ni\n\n(cid:1)\n\n∥\n\nπ(κ(t)\n\ni )\n\n(cid:1)\n\ni\n\n(cid:11)\n\ni (π(κ(t)\n\ni )\n\ni\n\n− (cid:101)η(t)\n\n−\n\ni,τ )⊤Ai(π(κ(t) π⋆\n\ni )\n\nπ⋆\n\nτ ).\n\n− (103)\n\nPlugging the above equation into (102),\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\nπ(t)\n\ni\n\n(cid:1) +\n\nη\n\n(cid:101)η(t)\n\ni\n\nKL(cid:0)π(t)\n\ni ∥ (cid:101)π(t)\n\ni\n\n(cid:32)\n\n1\n\n(cid:1) +\n\n= (1\n\n−\n\nητ )KL(cid:0)π⋆ (cid:20) η\n\n(1\n\ni,τ ∥ − (cid:101)η(t) (cid:10)log π(κ(t)\n\ni )\n\ni\n\n−\n\n+\n\n(cid:101)η(t)\n\ni η\n\n(cid:101)η(t)\n\ni\n\nπ(t−1)\n\ni\n\n(cid:1)\n\nη(π(κ(t)\n\ni )\n\ni\n\n−\n\ni τ )KL(cid:0)π(κ(t)\n\ni )\n\ni\n\n− π(t−1)\n\ni\n\n∥\n\nlog (cid:101)π(t)\n\ni\n\n−\n\n, π(κ(t)\n\ni )\n\ni\n\n(cid:11).\n\n− (cid:101)π(t)\n\ni\n\n(cid:33)\n\nη\n\nKL(cid:0)π(t)\n\nπ(t−1)\n\ni\n\n(cid:1)\n\ni\n\n−\n\ni ∥\n\n(cid:101)η(t) i,τ )⊤Ai(π(κ(t) π⋆ (cid:1) + (cid:101)η(t)\n\ni τ KL(cid:0)π(κ(t)\n\ni )\n\n− i )\n\ni\n\nπ⋆ τ )\n\nπ⋆\n\ni,τ\n\n(cid:1) + KL(cid:0)\n\n(cid:101)π(t) i ∥\n\n∥\n\n(cid:21)\n\n(cid:1)\n\nπ(κ(t)\n\ni )\n\ni\n\nRearranging the terms finishes the proof.\n\nE.8 PROOF OF LEMMA 8\n\nFor notational convenience, we set\n\nπ(t−1)\n\ni\n\n(cid:13) (cid:13)1\n\nφ(t)\n\ni =\n\n(cid:16)\n\n1\n\ni −\n\nη\n\n(cid:17)(cid:13)\n\n(cid:13)π(t)\n\ni\n\n(cid:101)η(t) (cid:16)(cid:13) (cid:13)π(κ(t)\n\ni\n\ni )\n\n−\n\nη\n\n(cid:101)η(t)\n\ni\n\n+\n\nπ(t−1)\n\ni\n\n−\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)(cid:101)π(t)\n\ni −\n\nπ(κ(t)\n\ni )\n\ni\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)π(t)\n\ni − (cid:101)π(t)\n\ni\n\n(cid:17)\n\n(cid:13) (cid:13)1\n\n0. By triangular inequality, we have φ(t)\n\n(cid:13) (cid:13)\n\n(cid:13)π(t)\n\ni −\n\nπ(t−1)\n\ni\n\n(cid:13) (cid:13) (cid:13)1\n\ni ≥\n\n. In addition, we\n\nt1, t2\n\nand t1\n\n}\n\n∨\n\n{\n\nt2 := max\n\nt1, t2\n\n. For 0 < t1 < t2, it holds that\n\n{\n\n}\n\nfor all i\n\nV, t\n\n∈\n\ndenote by t1\n\n≥ t2 := min\n\n∧ π(κ(t2 )\n\ni\n\nj\n\n(cid:13)\n\n(cid:13)π(κ(t1)\n\ni\n\nj\n\n)\n\n−\n\n(cid:13)\n\n(cid:13)π(νj (κ(t1)\n\ni\n\nj\n\n))\n\n)\n\n(cid:13) (cid:13)1 π(νj (κ(t2)\n\ni\n\nj\n\nνj (κ(t1)\n\ni\n\n− )∨νj (κ(t2) (cid:88)\n\ni\n\n)\n\n≤\n\n≤\n\nl=(νj (κ(t1 )\n\ni\n\n)+1)∧(νj (κ(t2)\n\ni\n\n)+1)\n\n))\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)π(κ(t1)\n\nj\n\ni\n\n)\n\n−\n\nπ(νj (κ(t1 )\n\ni\n\nj\n\n))\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)π(κ(t2 )\n\nj\n\ni\n\n)\n\n−\n\nπ(νj (κ(t2 )\n\ni\n\nj\n\n))\n\n(cid:13) (cid:13)1\n\n(cid:13) (cid:13)π(l)\n\nj −\n\nπ(l−1)\n\nj\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)π(κ(t1)\n\nj\n\ni\n\n)\n\n− (cid:101)π(νj (κ(t1)\n\nj\n\ni\n\n))\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)(cid:101)π(νj (κ(t1)\n\nj\n\ni\n\n))\n\n−\n\n44\n\nπ(νj (κ(t1)\n\ni\n\nj\n\n))\n\n(cid:13) (cid:13)1\n\nPublished as a conference paper at ICLR 2023\n\n+ (cid:13)\n\n(cid:13)π(κ(t2)\n\ni\n\nj\n\n)\n\n− (cid:101)π(νj (κ(t2)\n\nj\n\ni\n\n))\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\nνj (κ(t1)\n\ni\n\n)∨νj (κ(t2) (cid:88)\n\ni\n\n)\n\ni\n\nj\n\n(cid:13)(cid:101)π(νj (κ(t2) ̃η(νj (κ(t1) η\n\nj\n\ni\n\n))\n\n))\n\nπ(νj (κ(t2)\n\ni\n\nj\n\n))\n\n−\n\nφ(νj (κ(t1)\n\ni\n\nj\n\n))\n\n+\n\n(cid:13) (cid:13)1 ̃η(νj (κ(t2) η\n\nj\n\ni\n\n))\n\nφ(νj (κ(t2)\n\ni\n\nj\n\n))\n\nφ(l)\n\nj +\n\nl=(νj (κ(t1 )\n\ni\n\n)+1)∧(νj (κ(t2)\n\ni\n\n)+1)\n\nνj (κ(t1)\n\ni\n\n)∨νj (κ(t2) (cid:88)\n\ni\n\n)\n\nj + (γ + 1)φ(νj (κ(t1) φ(l)\n\nj\n\ni\n\n))\n\n+ (γ + 1)φ(νj (κ(t2)\n\ni\n\nj\n\n))\n\n.\n\n(104)\n\nl=(νj (κ(t1 )\n\ni\n\n)+1)∧(νj (κ(t2)\n\ni\n\n)+1)\n\n≤\n\n≤\n\nTherefore, we have\n\nt (cid:88)\n\nk=κ(t)\n\ni\n\n(cid:13)\n\n(cid:13)π(κ(k)\n\ni\n\nj\n\n)\n\n−\n\n(κ\n\nπ(κ\n\nj\n\ni\n\n(t−1) i\n\n)\n\n)\n\n(cid:13) (cid:13)1\n\n(cid:40)\n\nt (cid:88)\n\n≤\n\nk=κ(t)\n\ni\n\n(t−1) i\n\n)\n\n(κ\n\nνj (κ i\n\n)∨νj (κ(k)\n\n)\n\ni\n\n(cid:88)\n\n(κ\n\n(t−1) i\n\nl=(νj (κ i\n\n)\n\n)+1)∧(νj (κ(k)\n\ni\n\n)+1)\n\nj + (γ + 1)φ(νj (κ φ(l)\n\nj\n\ni\n\n(κ\n\n(t−1) i\n\n)\n\n))\n\n+ (γ + 1)φ(νj (κ(k)\n\ni\n\nj\n\n(cid:41)\n\n))\n\n.\n\n(105)\n\nSince 0\n\n(t\n\n∨\n\n−\n\nγ)\n\nκ(t)\n\ni ≤\n\nt\n\n≤\n\n≤\n\nνi(t)\n\n≤\n\nt + γ for all i\n\nV , t\n\n∈\n\n≥\n\n0, the first term can be bounded by\n\n(κ\n\n(t−1) i\n\n)\n\nνj (κ\n\ni\n\n)∨νj (κ(k)\n\n)\n\ni\n\n(cid:88)\n\n(κ\n\nl=(νj (κ i\n\n(t−1) i\n\n)\n\n)+1)∧(νj (κ(k)\n\ni\n\n)+1)\n\nφ(l)\n\nj ≤\n\n(t+γ−1)∨(k+γ) (cid:88)\n\nl=(t−2γ)∧(k−γ+1)\n\nφ(l)\n\nj ≤\n\nt+γ (cid:88)\n\nφ(l) j .\n\nl=t−2γ\n\nIn addition, the mapping k that\n\n(cid:55)→\n\nνj(κ(k)\n\ni\n\n) is injective when k\n\n≥\n\nγ (cf. Assumption 2 and 3). It follows\n\nt (cid:88)\n\nk=κ(t)\n\ni\n\nφ(νj (κ(k)\n\ni\n\nj\n\n))\n\n≤\n\nt+γ (cid:88)\n\nl=κ(t)\n\ni −γ\n\nφ(l)\n\nj ≤\n\nt+γ (cid:88)\n\nφ(l)\n\nj\n\nl=t−2γ\n\nPlugging the above inequalities into (105) yields\n\nt (cid:88)\n\nk=κ(t)\n\ni\n\n(cid:13)\n\n(cid:13)π(κ(k)\n\ni\n\nj\n\nπ(κ\n\nj\n\n)\n\n−\n\n(t−1) i\n\n(κ\n\ni\n\n)\n\n)\n\n(cid:13) (cid:13)1\n\n(t + 1\n\nκ(t) i )\n\n−\n\nt+γ (cid:88)\n\nl=t−2γ\n\nφ(l)\n\nj + (t + 1\n\n−\n\ni )(γ + 1)φ(νj (κ κ(t)\n\nj\n\ni\n\n(κ\n\n(t−1) i\n\n)\n\n))\n\n+ (γ + 1)\n\nt+γ (cid:88)\n\nφ(l)\n\nj\n\nl=t−2γ\n\n2(γ + 1)\n\nt+γ (cid:88)\n\nl=t−2γ\n\nj + (γ + 1)2φ(νj (κ φ(l)\n\nj\n\ni\n\n(κ\n\n(t−1) i\n\n)\n\n))\n\n.\n\n≤\n\n≤\n\nFinally, we control the term φ(t)\n\n(φ(t)\n\ni )2 =\n\n(cid:18)(cid:16)\n\n1\n\n−\n\nη\n\n(cid:17)1/2\n\n(cid:101)η(t)\n\ni\n\nas:\n\n(cid:17)1/2(cid:13)\n\n1\n\ni with ψ(t) (cid:16)\n\nη\n\ni\n\ni\n\n·\n\n−\n\n(cid:101)η(t) i τ )−1(cid:17)1/2\n\n(1\n\n− (cid:101)η(t)\n\n(cid:13)π(t)\n\ni −\n\nπ(t−1)\n\ni\n\n(cid:13) (cid:13)1\n\n(cid:16) η (cid:101)η(t)\n\ni\n\n·\n\n(1\n\n− (cid:101)η(t)\n\ni τ )\n\n(cid:17)1/2(cid:13)\n\n(cid:13)π(κ(t)\n\ni )\n\ni\n\nπ(t−1)\n\ni\n\n(cid:13) (cid:13)1\n\n−\n\n(cid:17)1/2\n\n(cid:17)1/2(cid:16)(cid:13)\n\n(cid:16) η (cid:101)η(t) (cid:0)2 + (1\n\ni\n\ni )\n\ni\n\nπ(κ(t)\n\ni −\n\n(cid:13)(cid:101)π(t) i τ )−1(cid:1)(cid:17)(cid:20)(cid:16)\n\n− (cid:101)η(t)\n\n·\n\nη\n\n(cid:101)η(t)\n\ni\n\n1\n\nη\n\n−\n\n(cid:101)η(t)\n\ni\n\n(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)π(t)\n\ni\n\ni − (cid:101)π(t) (cid:17)(cid:13)\n\n(cid:13)π(t)\n\ni −\n\n(cid:17)(cid:19)2\n\n(cid:13) (cid:13)1\n\nπ(t−1)\n\ni\n\n(cid:13) 2\n(cid:13) 1\n\n(i)\n\n(cid:16)\n\n≤\n\n1\n\n+\n\n−\n\n(cid:101)η(t)\n\ni\n\n+\n\n+\n\n(cid:16) η (cid:101)η(t) i\n(cid:16) η (cid:101)η(t)\n\ni η\n\n45\n\nPublished as a conference paper at ICLR 2023\n\nη\n\n(cid:16)\n\n+\n\ni\n\n(cid:101)η(t) 2 (cid:0)2 + (1\n\n(ii)\n\n≤\n\n(1\n\ni τ )(cid:13)\n\n(cid:13)π(κ(t)\n\ni )\n\ni\n\n− (cid:101)η(t)\n\nπ(t−1)\n\ni\n\n−\n\n(cid:13) 2\n(cid:13)\n\n1 + (cid:13)\n\n(cid:13)(cid:101)π(t)\n\ni −\n\nπ(κ(t)\n\ni )\n\ni\n\n(cid:13) 2\n(cid:13)\n\n1 + (cid:13)\n\n(cid:13)π(t)\n\ni − (cid:101)π(t)\n\ni\n\n(cid:17)(cid:21)\n\n(cid:13) 2\n(cid:13) 1\n\ni τ )−1(cid:1)\n\n− (cid:101)η(t)\n\n(cid:20)(cid:16)\n\n1\n\nη\n\n(cid:17)\n\n−\n\n(cid:101)η(t)\n\ni\n\nKL(cid:0)π(t)\n\ni ∥\n\nπ(t−1)\n\ni\n\n(cid:1)\n\ni τ )KL(cid:0)π(κ(t)\n\ni )\n\ni\n\n− (cid:101)η(t)\n\nπ(t−1)\n\ni\n\n(cid:1) + KL(cid:0)\n\n(cid:101)π(t) i ∥\n\n∥\n\nπ(κ(t)\n\ni )\n\ni\n\n(cid:1) + KL(cid:0)π(t)\n\ni ∥ (cid:101)π(t)\n\ni\n\n+\n\nη\n\n(cid:16)\n\n(cid:101)η(t)\n\ni\n\n(1\n\n(iii)\n\n≤\n\n8ψ(t)\n\ni\n\n,\n\n(cid:1)(cid:17)(cid:21)\n\n(106)\n\nwhere (i) applies Cauchy-Schwarz inequality, (ii) invokes Pinsker’s inequality and (iii) is due to\n\n(cid:101)η(t) i τ\n\n≤\n\n(γ + 1)ητ\n\n≤\n\n1/2. Combining the above two inequalities finishes the proof.\n\nE.9 PROOF OF LEMMA 9\n\nWe start with verifying the claim (86). Recall that\n\nKL(cid:0)π(t)\n\ni ∥\n\nπ(t−1)\n\ni\n\n(cid:1)\n\nψ(t)\n\ni\n\n:=\n\n(cid:16)\n\n1\n\n−\n\nη\n\n(cid:17)\n\n(cid:101)η(t)\n\ni\n\n(cid:20)\n\n+\n\nη\n\n(cid:101)η(t)\n\ni\n\n(1\n\ni τ )KL(cid:0)π(κ(t)\n\ni )\n\ni\n\n− (cid:101)η(t)\n\nπ(t−1)\n\ni\n\n(cid:1) + KL(cid:0)\n\n(cid:101)π(t) i ∥\n\n∥\n\nπ(κ(t)\n\ni )\n\ni\n\n(cid:1) + KL(cid:0)π(t)\n\ni ∥ (cid:101)π(t)\n\ni\n\n(cid:21)\n\n(cid:1)\n\n.\n\nWe introduce the following standard Lemma (see e.g., (Cen et al., 2020, Appendix A.2)), which allows us to bound control KL(cid:0)πi\n\n(cid:1) properly:\n\nπ′ i\n\nLemma 11. Given πi, π′\n\ni ∈\n\n∥ ∆(Si) and w\n\nKL(cid:0)πi\n\n(cid:1)\n\nπ′\n\ni\n\n∥\n\n≤\n\nR|Si| with log πi\n\ni + w, we have\n\n∈ (cid:13) (cid:13)log πi\n\nlog π′\n\ni\n\n−\n\n(cid:13) (cid:13)∞ ≤\n\n1= log π′ (cid:13)w(cid:13) 2(cid:13)\n\n(cid:13)∞.\n\nTherefore, it suffices to figure out the terms log π(t) log π(κ(t)\n\nand log π(κ(t)\n\nlog π(t−1)\n\ni )\n\ni )\n\n.\n\ni −\n\ni\n\ni\n\ni\n\nlog π(t−1)\n\ni\n\n, log π(t)\n\ni −\n\nlog (cid:101)π(t)\n\ni\n\n, log (cid:101)π(t)\n\ni −\n\n• Bounding KL(cid:0)π(t)\n\n(77) and (78):\n\ni ∥\n\n− π(t−1)\n\ni\n\n(cid:1) and KL(cid:0)π(t)\n\ni ∥ (cid:101)π(t)\n\ni\n\n(cid:1). The following equations follow directly from\n\n \n\n\n\nlog π(t)\n\ni − log π(t)\n\ni\n\nlog π(t−1) log (cid:101)π(t)\n\ni\n\ni )]k\n\n1= η([Aiπ(κ(t) 1= (η − (cid:101)η(t)\n\ni −\n\n−\n\nτ log π(t−1)\n\ni\n\n)\n\ni )([Aiπ(κ(t)\n\ni )]k\n\nτ log π(t−1)\n\ni\n\n− the order of (cid:13) (cid:13)log π(t−1)\n\ni\n\n.\n\n)\n\n(107)\n\n(cid:13) (cid:13)∞, which we shall\n\nIn addition, we have the following bound w.r.t. establish momentarily.\n\nThis taken together with Lemma 11 yields\n\n(cid:13) (cid:13)τ log π(t−1)\n\ni\n\n(cid:13) (cid:13)∞ ≤\n\nτ log\n\nSi |\n\n|\n\n+ 2dmax\n\nA\n\n∥∞ .\n\n∥\n\n(cid:40)\n\nKL(cid:0)π(t)\n\ni ∥ KL(cid:0)π(t)\n\n(cid:1)\n\ni\n\n(cid:1)\n\n≤\n\nπ(t−1) i ∥ (cid:101)π(t) ≤\n(cid:1). When κ(t)\n\ni\n\ni ≥\n\n(κ\n\n(t) i\n\n(cid:18)\n\nAi(π(κ\n\ni\n\n• Bounding KL(cid:0)\n\n(cid:101)π(t) i ∥\n\nπ(κ(t)\n\ni )\n\ni\n\nlog π(κ(t)\n\ni )\n\ni\n\nlog (cid:101)π(t)\n\ni\n\n1= (cid:101)η(t)\n\ni\n\n−\n\nη(3dmax ((cid:101)η(t)\n\ni −\n\nA\n\n∥∞ + τ log\n\n∥ η)(3dmax\n\n|\n\n) |\n\nSi ∥∞ + τ log\n\nA\n\n∥\n\n1, we recall from (80) that:\n\n−1)\n\n)\n\n−\n\nπ(κ(t)\n\ni ))\n\nητ )t−1−lAi(π(κ\n\ni\n\n(κ\n\n(t) i\n\nt−1 (cid:88)\n\n+\n\nl=κ(t)\n\ni\n\n(1\n\n− (cid:101)η(t)\n\ni τ )(1\n\n−\n\n46\n\n(108)\n\n(109)\n\n. )\n\nSi |\n\n|\n\n−1)\n\n)\n\n−\n\nπ(κ(l)\n\ni ))\n\n(cid:19)\n\n, (110)\n\nPublished as a conference paper at ICLR 2023\n\n∥ (cid:101)π(t)\n\ni\n\n≤ (cid:101)η(t)\n\ni dmax\n\nA\n\n∥\n\n∥∞ (t\n\n−\n\nκ(t)\n\ni + 1)\n\n≤ (cid:101)η(t)\n\ni dmax\n\nA\n\n∥∞ (γ + 1).\n\n∥\n\nwhich leads to a crude bound KL(cid:0)π(κ(t)\n\ni )\n\n(cid:1)\n\ni\n\nWhen κ(t) log π(κ(t)\n\ni = 0, we have log (cid:101)π(t)\n\ni )\n\ni\n\ni\n\n−\n\n1=\n\n1=\n\ni\n\nlog (cid:101)π(t) − (cid:101)η(t)\n\n(1\n\n−\n\n−\n\n− (cid:101)η(t)\n\ni\n\nητ )t−1 log π(0)\n\ni τ )(1 (cid:18)\n\n− Aiπ(κ(t)\n\ni ) +\n\nt−1 (cid:88)\n\n(1\n\n− (cid:101)η(t)\n\ni τ )(1\n\n−\n\nητ )t−1−lAiπ(κ(l) i )\n\n(cid:19)\n\n− (cid:101)η(t)\n\ni τ )(1\n\n−\n\nητ )t−1−lAiπ(κ(l) i )\n\n(cid:19)\n\n,\n\nl=κ(t)\n\ni +1\n\nt−1 (cid:88)\n\n(1\n\nl=κ(t)\n\ni +1\n\nA\n\n∥∞ t\n\n∥\n\n≤ (cid:101)η(t)\n\ni dmax\n\nA\n\n∥\n\n∥∞ (γ + 1).\n\n(111)\n\nlog π(t−1)\n\ni\n\n).\n\n(cid:18)\n\n1=\n\n−(cid:101)η(t)\n\ni\n\nAiπ(κ(t)\n\ni ) +\n\nwhich yields\n\nKL(cid:0)π(κ(t)\n\ni )\n\ni\n\ni dmax\n\n∥ (cid:101)π(t)\n\ni\n\n(cid:1)\n\n≤ (cid:101)η(t) (cid:1). Note that\n\nπ(t−1)\n\ni\n\n• Bounding KL(cid:0)π(κ(t)\n\ni )\n\ni\n\n∥ log π(t−1)\n\nKL(cid:0)π(κ(t)\n\ni )\n\ni\n\nπ(t−1)\n\ni\n\n(cid:1)\n\n∥ ∥\nPutting all pieces together, we conclude that (cid:17)\n\n(cid:16)\n\nη\n\nψ(t)\n\ni =\n\n1\n\n−\n\nKL(cid:0)π(t)\n\ni ∥\n\nπ(t−1)\n\ni\n\n(cid:1)\n\n(cid:101)η(t)\n\ni η\n\n(cid:20)\n\ni )\n\nlog π(κ(t) i\nThis yields, by equations (107), (110), (111) and associated bounds,\n\ni ) + (log (cid:101)π(t)\n\n= (log π(κ(t)\n\nlog (cid:101)π(t)\n\ni −\n\ni )\n\n−\n\n−\n\ni\n\ni\n\nlog π(t)\n\ni ) + (log π(t)\n\ni −\n\n≤ (cid:101)η(t)\n\ni dmax\n\nA\n\n∥∞ (γ + 1) + (cid:101)η(t)\n\ni (3dmax\n\nA\n\n∥∞ + τ log\n\n|\n\n∥\n\nSi\n\n). |\n\ni τ )KL(cid:0)π(κ(t)\n\ni )\n\ni\n\nπ(t−1)\n\ni\n\n∥\n\n+\n\n(cid:101)η(t) 3η(3dmax\n\ni\n\n≤ = η(dmax\n\n∥ A\n\n(1\n\nA\n\n− (cid:101)η(t) ∥∞ + τ log\n\n∥∞ (2γ + 11) + 3τ log\n\nSi |\n\n|\n\n∥\n\nSi |\n\n|\n\n) + 2ηdmax ).\n\n(cid:1) + KL(cid:0)\n\n(cid:101)π(t) i ∥ ∥∞ (γ + 1)\n\nA\n\n∥\n\nπ(κ(t)\n\ni )\n\ni\n\n(cid:1) + KL(cid:0)π(t)\n\ni ∥ (cid:101)π(t)\n\ni\n\n(cid:21)\n\n(cid:1)\n\nIt remains to prove the claim (87): KL(cid:0)π⋆\n\nπ(2γ)\n\ni,τ ∥\n\ni\n\n(cid:1) = KL(cid:0)π⋆ KL(cid:0)π⋆\n\n≤\n\ni,τ ∥\n\ni,τ ∥\n\ni,τ , log π(0)\n\n(cid:1) + (cid:10)π⋆ (cid:1) + (cid:13)\n\n(cid:13)log π(0)\n\ni −\n\n(cid:11)\n\ni − log π(2γ)\n\nlog π(2γ) i\n(cid:13) (cid:13)∞\n\ni\n\nKL(cid:0)π⋆\n\ni,τ ∥\n\n≤\n\ni,τ ∥ where the third step results from log π(2γ) and Lemma 11.\n\n≤\n\ni\n\nKL(cid:0)π⋆\n\n2γ (cid:88)\n\nη\n\n(cid:1) + 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:1) + 4ηdmax\n\nl=1\n\nητ )2γ−lAiπ(κ(l) i )\n\n(1\n\n−\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∞\n\nA\n\n∥\n\n∥∞ γ, i + η (cid:80)2γ\n\nl=1(1\n\nητ )2γ log π(0)\n\n1= (1\n\n−\n\nητ )2γ−lAiπ(κ(l) i )\n\n−\n\nπ(0)\n\ni\n\nπ(0)\n\ni\n\nπ(0)\n\ni\n\nπ(0)\n\ni\n\nProof of the claim (108). First, we prove by induction that for any k, l\n\nlog π(t)\n\ni (k)\n\nlog π(t)\n\ni (l)\n\n−\n\nA\n\n2dmax ∥\nτ\n\n∥∞\n\n,\n\n≤\n\nNote that the claim trivially holds for t = 0 with the uniform initialization π(0)\n\nAssume that (112) holds for all t′ we have log π(t)\n\nlog π(t)\n\ni (l) = (1\n\ni (k)\n\n−\n\nητ )\n\n−\n\nt\n\n−\n\n≤\n\n1. Note that log π(t)\n\ni\n\n1= (1\n\n(cid:16)\n\nlog π(t−1)\n\ni\n\n(k)\n\n−\n\nlog π(t−1)\n\ni\n\n47\n\n− (cid:17)\n\n(l)\n\n+ η\n\nSi,\n\n∈\n\n0.\n\nt ∀\n\n≥\n\n(112)\n\ni = 1\n\nV .\n\n|Si| 1, i\n∀ + ηAiπ(κ(t)\n\n∈\n\ni ),\n\nητ ) log π(t−1)\n\ni\n\n(cid:16)\n\n[Aiπ(κ(t)\n\ni )]k\n\n[Aiπ(κ(t)\n\ni )]l\n\n(cid:17)\n\n−\n\nPublished as a conference paper at ICLR 2023\n\nA\n\n2dmax ∥\nτ\n\n∥∞\n\n+ 2ηdmax\n\nA\n\n∥\n\n∥∞\n\n≤\n\n=\n\n(1\n\nητ )\n\n− 2dmax ∥\nτ\n\nA\n\n∥∞\n\n,\n\nwhere the second line follows from the induction hypothesis (112). This completes the induction at the t-th iteration. It follows that for all i (cid:19)\n\nV and t\n\n0,\n\n(cid:18)\n\n∈\n\n≥ ∥∞\n\nA\n\n2dmax ∥\nτ\n\n−\n\nlog\n\nSi\n\n|\n\n| −\n\n≥ −\n\nA\n\n2dmax ∥\nτ\n\n∥∞\n\n.\n\n(113)\n\nlog π(t)\n\ni (l)\n\nlog\n\n≥\n\nmax k∈Si\n\nπ(t)\n\ni (k)\n\n48",
    "reference": "# Summary Of The Paper\n\nThis paper studies asynchronous gradient plays in zero-sum polymatrix games under delayed feedbacks, while significant efforts have been made to understand zero-sum two-player matrix games. They first establish that the last iterate of the entropy-regularized optimistic multiplicative weight updates (OMWU) method converges linearly to the quantal response equilibrium (QRE), the solution concept under bounded rationality, in the absence of delays or under the randomly delayed feedbacks with some assumptions. They further demonstrate that entropy-regularized OMWU with two-timescale learning rates enjoys faster last-iterate convergence under fixed delays and continues to converge provably even when the delays are arbitrarily bounded.\n\n# Strength And Weaknesses\n\nThis paper seems to propose a new problem and then use some technique to solve it, but the property of the proposed problem and intuition behind the technique is unclear:\n\nWhy are the zero-sum polymatrix games under delay feedback important? What does ‘delay feedback’ mean in zero-sum polymatrix games? What are real-world scenarios that can be modeled by zero-sum polymatrix games under delay feedback? Why do we need to care about NE and QRE in these games?\n\nWhat is the challenge (e.g., complexity) for computing the Nash equilibrium and Quantal response equilibrium in zero-sum polymatrix games under delay feedback?\n\nWhy does synchronous optimization not work in zero-sum polymatrix games under delay feedback?\n\nAre there any existing algorithms that can solve the new problem? \n\nHow to obtain Eq.(8)? \\pi in QRE and NE may not be the same due to Eq.(5). Is it possible to show the result for NE instead of using this formula and the results of QRE?\n\nHow to remove t in the rate, i.e., O, before Remark 1?\n\nWhat is the intuition of Assumptions 1, 2, and 3? Are they realistic in real-world scenarios?\n\nWhy do we need to consider a two-timescale problem?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSome parts of this paper are hard to read:\n\nHow to obtain Eq.(8). \\pi in QRE and NE may not be the same due to Eq.(5)?\n\nIn Eq.(6), \\tau is the regularization parameter or temperature, then what is u_i(\\tau)?\n\nHow to remove t in the rate, i.e., O, before Remark 1?\n\nWhat is the intuition of Assumptions 1, 2, and 3? Are they realistic in a real-world scenario?\n\nWhy does KL increase in Figures 1(b) and 1(c)?\n\nMinor:\nAt the beginning of Section 1.2, ‘excessive gap technique  of Nesterov’ needs a reference.\n\n# Summary Of The Review\n\nThis paper seems to propose a new problem and then use some technique to solve it, but the property of the proposed problem and the intuition behind the technique is unclear.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nCORRUPTED IMAGE MODELING FOR SELF-SUPERVISED VISUAL PRE-TRAINING\n\nYuxin Fang 1, 2∗ Li Dong 2 Hangbo Bao 2 Xinggang Wang 1† 1 School of EIC, Huazhong University of Science & Technology {yxf,xgwang}@hust.edu.cn\n\n2 Microsoft Research\n\nFuru Wei 2\n\nABSTRACT\n\nWe introduce Corrupted Image Modeling (CIM) for self-supervised visual pretraining. CIM uses an auxiliary generator with a small trainable BEiT (Bao et al., 2021) to corrupt the input image instead of using artificial [MASK] tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our approach achieves compelling results in vision benchmarks, such as ImageNet classification and ADE20K semantic segmentation.\n\n1\n\nINTRODUCTION\n\nVision Transformers (ViTs) (Dosovitskiy et al., 2020) are transferring the landscape of computer vision, not only in terms of the network architecture design, but also the self-supervised pre-training recipe. Masked image modeling (MIM) (Bao et al., 2021), which randomly masks out some input tokens and then recovers the masked content by conditioning on the visible context, is able to learn rich visual representations and shows promising performance on various vision benchmarks (Zhou et al., 2021; He et al., 2021; Xie et al., 2021; Dong et al., 2021; Wei et al., 2021).\n\nOriginated in masked language modeling (Devlin et al., 2019), MIM (Figure 1a) is tailor-made for specific architectures (Vaswani et al., 2017), which is generally capable of receiving and processing tokenized inputs such as the artificial [MASK] tokens. Meanwhile, the more common and natural input signal in computer vision is the image in RGB domain with 2D regular grid structures. In order to apply MIM pre-training for images, ViT has to “patchify” the input image into a 1D sequence of non-overlapping patch embeddings, and then use [MASK] tokens to perturb them.\n\nMIM is tightly coupled with the Transformer family, and the usage of [MASK] tokens limits its scope of application to some extent. More importantly, MIM is not directly suitable for convolutional neural networks (CNNs) (LeCun et al., 1989), the dominant architecture for computer vision in the last decade. Introducing [MASK] tokens in any intermediate stage of CNN is infeasible, as convolution’s intrinsic dense-sliding-window paradigm causes information leakage between visual features in previous layers and therefore impedes the MIM. Therefore the large CNN family cannot directly benefit from the upsurge of this new pre-training scheme. Moreover, the usage of [MASK] tokens causes a discrepancy between pre-training and fine-tuning (Devlin et al., 2019; Clark et al., 2020), as the artificial [MASK] tokens never appear in the fine-tuning stage.\n\nIn this paper, we present a new visual pre-training framework, called Corrupted Image Modeling (CIM, Figure 1b), which avoids directly manipulating [MASK] tokens on pre-trained models and generalizes quite well to both ViT and CNN architectures. Rather than directly using artificial [MASK] tokens to corrupt a portion of non-overlapping patch embeddings as in MIM, CIM uses\n\n∗Contribution during internship at Microsoft Research. †Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) Masked Image Modeling (MIM).\n\n(b) Corrupted Image Modeling (CIM).\n\nFigure 1: Overview of our Corrupted Image Modeling (CIM) and comparisons with Masked Image Modeling (MIM). MIM (Figure 1a) requires the pre-trained architecture to receive and process the artificial [MASK] tokens, while CIM (Figure 1b) relaxes these restrictions by using a trainable generator to sample corrupted images serving as the input for the enhancer. Similar to BEiT, the small generator learns to predict the golden visual token produced by the pre-trained frozen image tokenizer encoder (not shown in the figure) based on partial observations of the input. The enhancer can be various architectures including CNN and learns either a generative or a discriminative visual pre-training objective. After pre-training, we throw out the generator and fine-tune the enhancer on downstream tasks. The dice icon in Figure 1b refers to the visual tokens’ stochastic sampling process, and the lock icon means the pre-trained image tokenizer decoder is frozen.\n\na small trainable BEiT (Bao et al., 2021) as an auxiliary generator to corrupt the input image. Specifically, the BEiT generator learns to predict visual tokens at the masked positions, where we utilize the predicted distribution to sample visual tokens’ replacements. The replaced visual tokens together with the golden tokens that directly produced by a pre-trained frozen image tokenizer encoder (e.g., the DALL-E (Ramesh et al., 2021) dVAE encoder) given the same input as the small trainable BEiT are then mapped back to the image RGB domain by a pre-trained frozen tokenizer decoder (e.g., the DALL-E dVAE decoder). The resulting corrupted image serves as the input of the enhancer, which is the model to be pre-trained and transferred.\n\nFor the enhancer, the choice of pre-training objectives is quite flexible. We study two representatives: a generative objective that regresses all the original image pixels given the corrupted image (Dosovitskiy et al., 2020; Chen et al., 2020a), dubbed as Pixel Residual learning (RESPIX), and a discriminative objective that predicts whether each visual token is replaced by the small generator or not (Clark et al., 2020), dubbed as Replaced Visual token Detection (REVDET). After pre-training, the enhancer can be used as a strong feature extractor for visual downstream tasks.\n\nOverall, CIM is a general and flexible pre-training framework suited for different kinds of visual encoders. For the first time, we demonstrate that both ViT and CNN can learn rich visual representations using a unified non-Siamese structure. Experimental results show that our approach achieves compelling results in vision benchmarks, such as ImageNet classification and ADE20K semantic segmentation. We hope CIM can serve as a promising starting point for exploring flexible & unified visual representation learning of various architectures.\n\n2 CORRUPTED IMAGE MODELING (CIM)\n\nFigure 1b shows the overview of CIM. Our approach simultaneously learns two neural networks: an auxiliary generator and an enhancer. The generator is used to corrupt the input image, while the enhancer receives the corrupted image (Figure 2) and learns either a generative or a discriminative visual pretext task. After pre-training, we throw out the generator and fine-tune the enhancer on downstream tasks.\n\n2.1 GENERATOR\n\nRather than using artificial [MASK] tokens to corrupt the input image, we learn a trainable auxiliary generator to relax the architectural constraints of MIM. Moreover, the generator enriches the diversity\n\n2\n\nViTObjective:Masked PartsPrediction(e.g., predict maskedvisual tokens or pixels)Masked Input ViT, CNN, etc.Small BEiTGeneratorEnhancerWhether is the Visual Token of Each PatchReplaced or Not ? Generative Objective(e.g., allpixelsregression)Discriminative Objective(e.g., all tokens binary classification)ORCorrupted Image Masked Input DecoderPublished as a conference paper at ICLR 2023\n\n(a) Corrupted image samples from ImageNet-1K training set. Although the model is trained using the same dataset, the corrupted image samples still vary to a certain extent. Therefore during pre-training, the generator is able to continuously provide abundant and diverse corrupted samples for the enhancer.\n\n(b) Corrupted image samples from COCO val split (Lin et al., 2014) using ImageNet-1K pre-trained model.\n\nFigure 2: Visualizations of some corrupted image samples. For each image set, we show (from left to right) the original image, the masked image, and four different corrupted images sampled from the generator output distribution with the same masked input. Simple stochastic sampling can greatly enrich the corrupted image distribution in terms of both low-level features and high-level semantics, which feeds the enhancer better.\n\nof corrupted images via stochastic sampling, which helps the enhancer generalize. The generator consists of a pre-trained frozen image tokenizer, and a small trainable BEiT (Bao et al., 2021).\n\nThe frozen image tokenizer in CIM is a pre-trained discrete variational autoencoder (dVAE) (Rolfe, 2016; Van Den Oord et al., 2017), consisting of a paired encoder and decoder. The tokenizer encoder maps the input image into a sequence of discrete visual tokens with a fixed vocabulary size. The tokenizer decoder can recover semantically plausible images given a permutation of appropriate and meaningful visual tokens. We directly use the DALL-E (Ramesh et al., 2021) tokenizer, following BEiT.\n\nThe small BEiT consists of several Transformer encoder layers and is trained to perform MIM, which uses two views for each input image, i.e., a sequence of non-overlapping patch embeddings, and their corresponding discrete visual tokens. Patch embeddings are linearly embedded from non-overlapping input image patches. Discrete visual tokens are from the DALL-E tokenizer encoder, serving as the prediction target for BEiT.\n\nGiven a sequence of patch embeddings, the small BEiT randomly masks out a set of positions. The patch embeddings at the masked positions are replaced with special mask embeddings. The small BEiT takes this corrupted sequence of patch embeddings as the input, and learns to predict the corresponding discrete visual tokens at all masked positions given the visible context only. In CIM pre-training, the size of the small BEiT we use is typically a quarter or a half of the enhancer.\n\nUsing discrete visual tokens to represent images enables CIM to perform stochastic sampling during the corrupted image’s generation process, which greatly enriches the output set of the generator. In this paper, we directly sample from softmax with a temperature of 1 at all the masked positions according to the small BEiT output distribution. All the masked tokens are replaced by the sampled visual tokens. The sampled tokens together with the golden tokens that are directly produced by the image tokenizer encoder at all the non-masked positions constitute the input for the image tokenizer decoder. Then the decoder maps those plausible visual tokens to a corrupted image (refer to examples in Figure 2), which serves as the input for the enhancer.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n(a) CIM-RESPIX pre-training objective with sliding window normalized pixels as the enhancer prediction target.\n\n(b) CIM-RESPIX pre-training objective with unnormalized pixels as the enhancer prediction target.\n\nFigure 3: Example visualization results on COCO val split images from vanilla ViT-Base/16 model pre-trained with the RESPIX objective using ImageNet-1K training data. For each image quadruplet, we show the original input image (1st column), the masked input image for the generator (2nd column), the corrupted image sampled from the generator output (3rd column), and the enhancer output (4th column). Given the corrupted image, the enhancer is able to perform image denoising, deblurring and completion, etc., and learns to predict plausible output in terms of both low-level features as well as high-level semantics.\n\n2.2 ENHANCER\n\nGiven the corrupted image sampled from the auxiliary generator, the enhancer learns either a generative or a discriminative visual pretext task. The prediction head is a simple linear layer, and the choice of pre-training objectives is quite flexible. In this paper, we study two representative objectives, coined as Pixel Residual learning (RESPIX) and Replaced Visual token Detection (REVDET).\n\nRESPIX (Figure 3) is a generative visual pretext task that requires the enhancer to predict the uncorrupted pixel value for all positions given the corrupted input. Instead of directly regressing the original pixel, MAE (He et al., 2021) suggests learning the normalized counterpart. Specifically, the image is partitioned into a set of non-overlapping patches, and each pixel is normalized by the mean and standard deviation of all pixels in the patch it lives in, i.e., patches with layer normalization (Ba et al., 2016) are the reconstruction target.\n\nIn CIM, we further propose to normalize the prediction target inside a sliding window, i.e., each pixel is normalized by all pixels in a local 8 × 8 sized window centered at where the target pixel lives in. We observe improved representation quality using the sliding window normalization paradigm.\n\nFigure 4: Normalizations as learning templates for RESPIX. For each image triplet, we visualize the original image (left), the template of using non-overlapping window normalization (He et al., 2021), and the template of the proposed sliding window normalization paradigm. Our approach can provide more accurate and moderate hints that can boost the enhancer’s pre-training as well as improve its representation quantity.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nNaive pixel recovery without normalization tends to waste modeling capability on learning short-range dependencies and high-frequency details (Ramesh et al., 2021; Bao et al., 2021), while the normalized target can mitigate irrelevant information fittings. From another perspective, normalizations are equal to providing learning templates, as shown in Figure 4. With the normalized prediction target, the enhancer only needs to learn the residual pixel value at each position given the normalized pixel value, while the unnormalized target provides no hint therefore the enhancer has to “learn to see in the dark” (i.e., regress from RGB: 0, 0, 0). It is also hard for the enhancer to learn without a template since the corrupted image usually provides bad priors (refer to the corrupted image samples in Figure 2 and Figure 3). Therefore, we believe appropriate and moderate hints will help the enhancer see better.\n\nREVDET is a discriminative visual pretext task that requires the enhancer to determine whether each visual token is replaced by a generator sample or not. To be specific, the visual tokens produced by the pre-trained frozen image tokenizer encoder are considered as golden tokens. If a generated visual token is different from the golden token at the same position, that generated token is considered “replaced”, and vice versa.\n\nREVDET is inspired by ELECTRA (Clark et al., 2020) in language modeling. The main difference is, in the proposed CIM, the determining criterion of replacement is hidden in the corrupted image. Token replacement is a kind of local, high-frequency operation by nature. However, the visual token set after sampling and replacement is further smoothed and processed by the image tokenizer decoder. Therefore the token sampling and replacement operations are finally embodied as non-local, high-level semantics changes in the corrupted image. The enhancer is required to “decrypt” it and identify all the replaced tokens given the corrupted input, which yields a nontrivial and meaningful visual pretext task1. To some extent, REVDET also learns the DALL-E dVAE’s visual codebook similar to BEiT, but in a discriminative manner.\n\nThe enhancer is regarded as the visual encoder after pre-training. Moreover, unlike masked image modeling, CIM does not assume too many architectural priors for the pre-trained network. We successfully pre-train a high-capacity vanilla ResNet-50 (He et al., 2016), ResNet-50x2 and ResNet50x4 enhancers that achieve compelling transfer learning performance using a similar configuration as pre-training a ViT enhancer. For the first time, we demonstrate that both ViT and CNN can learn strong visual representations using a unified non-Siamese framework.\n\n2.3 TRAINING AND OPTIMIZATION\n\nThe auxiliary generator and the enhancer are simultaneously trained and synergistically (rather than adversarially as GAN (Goodfellow et al., 2014)) updated. The trainable part of the generator, i.e., the small BEiT, learns a MIM objective in the same vein as in (Bao et al., 2021). The whole pre-trained image tokenizer is frozen.\n\nFor the RESPIX visual pretext task, the enhancer is optimized by a combination of l1 and l2 loss. For the REVDET visual pretext task, the enhancer is learned by binary cross-entropy loss. Notice that the gradients of the enhancer are not back-propagated through the generator. A detailed formulation is presented in Appendix A.3.\n\n3 EXPERIMENTS\n\nWe study CIM self-supervised pre-trained vanilla ViT-Small/16 (Touvron et al., 2021a), vanilla ViT-Base/16 (Dosovitskiy et al., 2020) and vanilla ResNet-50 (He et al., 2016) models. We use the actual processed images / views to measure the pre-training epochs (PT epochs). ImageNet-1K (Deng et al., 2009) training data is used to pre-train the small BEiT and the enhancer. Our pre-training setting generally follows BEiT (Bao et al., 2021). Unlike BEiT, CIM only uses cropping and flipping for data argumentation, while dropout (Srivastava et al., 2014) and stochastic depth (Huang et al., 2016) are not applied. The detailed pre-training settings are summarized in the Appendix A.4. Notably, the pre-training configurations are almost the same for both ViT and CNN architectures.\n\nIn order to evaluate the pre-trained representations from CIM, for both ViT and CNN architectures, we conduct supervised end-to-end fine-tuning (FT) experiments on ImageNet-1K (Deng et al., 2009) image classification in §3.1, and ADE20K (Zhou et al., 2019) semantic segmentation in §3.2.\n\n1Therefore, REVDET can be also interpreted as “Reverse token Detection from corrupted image”.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: ImageNet-1K end-to-end fine-tuning top-1 accuracy of vanilla ViT-Small/16 and ViT-Base/16 models. †Doubled attention heads.\n\n‡Our reproduction.\n\nTable 2: ImageNet-1K end-to-end fine-tuning top-1 accuracy of vanilla ResNet-50 model. RSB (Wightman et al., 2021) is the current vanilla ResNet stateof-the-art training procedure.\n\nModels\n\nPT Epochs Top-1\n\nModels\n\nPT Epochs Top-1\n\nViT-Small/16 model results Scratch (Touvron et al., 2021a) MoCo-v3† (Chen et al., 2021) DINO (Caron et al., 2021) BEiT (Bao et al., 2021) CIM-RESPIX (Ours) CIM-REVDET (Ours)\n\nViT-Base/16 model results Scratch (Touvron et al., 2021a) Scratch (He et al., 2021) DINO (Caron et al., 2021) MoCo-v3 (Chen et al., 2021) BEiT (Bao et al., 2021) BEiT (Bao et al., 2021) MAE‡ (He et al., 2021) CIM-REVDET (Ours) CIM-RESPIX (Ours)\n\n600 1600 300 300 300\n\n1600 600 300 800 800 300 300\n\n79.9 81.4 81.5 81.3 81.5 81.6\n\n81.8 82.3 82.8 83.2 82.9 83.2 83.1 83.3 83.3\n\nFine-tuning for 100 epochs RSB A3 (Wightman et al., 2021) CIM-REVDET (Ours)\n\nFine-tuning for 300 epochs RSB A2 (Wightman et al., 2021) SimSiam (Chen & He, 2021) MoCo-v2 (Chen et al., 2020c) SimCLR (Chen et al., 2020b) SimCLR (Chen et al., 2020b) BYOL (Grill et al., 2020) SwAV (Caron et al., 2020) CIM-RESPIX (Ours) CIM-REVDET (Ours)\n\nFine-tuning for 600 epochs RSB A1 (Wightman et al., 2021) CIM-REVDET (Ours)\n\n300\n\n400 400 800 2000 400 600 300 300\n\n300\n\n78.1 78.8\n\n79.8 79.1 79.6 79.9 80.0 80.0 80.1 79.9 80.5\n\n80.4 80.7\n\nAblation study on ImageNet-1K is presented in §3.3. For ImageNet-1K, we observe ∼0.2 Top-1 acc. fluctuations. For ADE20K, we observe ∼0.5 mIoU fluctuations. We report key results using the median of 3 independent runs.\n\n3.1\n\nIMAGE CLASSIFICATION\n\nViT. The ImageNet-1K end-to-end fine-tuning top-1 accuracy of vanilla ViT-Small/16 and ViTBase/16 models are presented in Table 1. We fine-tune the small-sized model for 200 epochs, and the base-sized model for 100 epochs. Other self-supervised methods in Table 1 use the same or longer fine-tuning schedule. The fine-tuning hyperparameters mostly follow BEiT, while our layerwise lr decay rate is set to 0.8 as suggested by Clark et al. (2020). See Appendix A.4 for detailed configurations.\n\nAs shown in Table 1, CIM is able to achieve better accuracy with fewer pre-training epochs compared with other representative self-supervised vanilla ViT models. Moreover, we find both REVDET and RESPIX visual pretext task can help the ViT enhancer learn useful representations.\n\nResNet-50. We demonstrate that CIM can also pre-train a high-capacity ResNet-50 model with the fewest possible modifications from the ViT pre-training settings that can achieve compelling fine-tuning performances on ImageNet-1K. We use the AdamW optimizer (Loshchilov & Hutter, 2017) for fine-tuning, and other configurations basically follow the advanced training recipe of RSB (Wightman et al., 2021). For other self-supervised baselines, we select the best lr out of {5e-3, 8e-3, 12e-3} and keep other settings unchanged to ensure a fair and challenging competition. The detailed configurations are given in Appendix A.4.\n\nAs shown in Table 2, under such a demanding training procedure, CIM pre-trained ResNet-50 model can still outperform several representative self-supervised methods based on the Siamese framework as well as the modernized state-of-the-art ResNet-50 results. Using the improved fine-tuning recipe, we also observe performance degeneration for some self-supervised baselines compared with the RSB from scratch results. Notably, even with the extreme 600-epoch training schedule, the CIM representation can still improve the state-of-the-art RSB A1 by 0.3%.\n\n3.2 SEMANTIC SEGMENTATION\n\nWe study the transfer learning performance of CIM pre-trained vanilla ViT-Base/16 and ResNet-50 models on the ADE20K semantic segmentation benchmark. The pre-trained models are used as an encoder, and we purposefully choose simple decoders to better reveal the pre-trained representations. Experiments are based on the code of Bao et al. (2021); MMSegmentation (2020).\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nSpecifically, for ViT-Base/16 we use a simple linear layer as the decoder, and for ResNet-50 we choose the ubiquitous FCN (Long et al., 2015) as the decoder. For ViT, the baseline settings as well as the fine-tuning recipes are from (Bao et al., 2021). We select the best lr out of {1e-4, 3e-4, 5e-4, 7e-4} for DINO. For BEiT we use the default setting (lr 7e4 with a decay rate of 0.65). For CIM pretrained ViT, we set the fine-tuning lr equal to 3e-4 with a decay rate of 0.8 as suggested by Clark et al. (2020). For ResNet-50, we use the canonical configuration for all methods, i.e., the optimizer is SGD with a momentum of 0.9, lr follows a poly decay schedule, and the batch size is 16. The training crop size is set to 512 for all models, and we use singlescale inference.\n\nAs summarized in Table 3, when transferred to semantic segmentation task, CIM pre-trained models can still achieve competitive performances compared with other approaches. Notably, for ResNet-50, as the fine-tuning schedule becomes longer (i.e., 80k iterations → 160k iterations), the performance gain from the ImageNet-1K supervised pre-trained representation is small. Moreover, the performance is even worse than training from scratch. Meanwhile, the CIM pre-trained ResNet-50 representation can provide sustaining performance gain for a longer fine-tuning schedule.\n\nTable 3: ADE20K semantic segmentation performances (mIoU) of ViT and ResNet-50 models.\n\nModels\n\nPT Epochs Top-1\n\nFine-tuning for 160k iterations DINO (Caron et al., 2021) BEiT (Bao et al., 2021) CIM-RESPIX (Ours) CIM-REVDET (Ours)\n\n1600 300 300 300\n\n43.0 43.2 43.5 43.6\n\n(a) Vanilla ViT-Base/16 as encoder with one linear layer as decoder.\n\nModels\n\nPT Epochs mIoU\n\nFine-tuning for 80k iterations Training from Scratch IN1K Supervised† (He et al., 2019) CIM-REVDET (Ours)\n\nFine-tuning for 160k iterations Training from Scratch IN1K Supervised (He et al., 2019) BYOL (Grill et al., 2020) SimSiam (Chen & He, 2021) SwAV (Caron et al., 2020) MoCo-v2 (Chen et al., 2020c) SimCLR (Chen et al., 2020b) SimCLR (Chen et al., 2020b) CIM-RESPIX (Ours) CIM-REVDET (Ours)\n\n120 300\n\n120 400 400 600 400 800 2000 300 300\n\n29.9 35.9 36.2\n\n36.7 36.1 37.1 37.1 37.2 37.5 37.6 37.7 38.7 39.0\n\n(b) Vanilla ResNet-50 as encoder with a classic FCN as decoder.\n\nTogether with the observation from §3.1, we demonstrate CIM is a general, non-Siamese framework that is capable of pre-training both strong ViT and CNN visual encoders.\n\n3.3 ABLATION STUDIES\n\nAblation studies are conducted using 300-epoch CIM-RESPIX pre-trained ViT-Base model with 100 epochs fine-tuning on ImageNet-1K unless specified. Some additional analysis is available in Appendix A.1.\n\nMasking Strategy and Masking Ratio. As shown in Table 4, we observe CIM works better with simple random masking (He et al., 2021; Xie et al., 2021) compared with the blockwise masking strategy (Bao et al., 2021).\n\nThe optimal random masking ratio is around 50%, which we find also holds for the REVDET pretext task, in part because it provides almost equal amounts of positive and negative training samples.\n\nThe Small BEiT Depth and Weight Sharing. Following Meng et al. (2021); Chi et al. (2021), we adjust the size of the small trainable BEiT by varying its depth (i.e., the number of Transformer encoder layers) instead of its width (i.e., the feature dimension). As summarized in Table 5, the small BEiT with 4 to 6 layers is generally fine.\n\nIt is also beneficial to share the patch embedding layer as well as the first two Transformer encoder layers between the small BEiT and enhancer as long as the enhancer is also ViT. We hypothesize that sharing the earlier layers can help calibrate the enhancer since the small BEiT receives the real inputs while the enhancer sees the same sources but with corrupted views.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Ablation study: masking strategy and masking ratio.\n\nTable 5: Ablation study: depth of the small BEiT in the generator and weight sharing.\n\nMasking Strategy Masking Ratio Top-1 Acc.\n\n# Enc. Layers Weight Sharing Top-1 Acc.\n\nBlockwise Blockwise Blockwise\n\nRandom Random Random\n\n40% 50% 60%\n\n40% 50% 60%\n\n82.8 82.9 82.8\n\n83.0 83.3 83.1\n\n4 4\n5 6\n7 8\n\n✗ ✓\n✓ ✓\n✓ ✓\n\n83.1 83.3 83.2 83.2 83.1 82.9\n\nTable 6: Ablation study: pixel reconstruction target for RESPIX pre-training objective.\n\nTable 7: Ablation study: sampling strategy for visual tokens.\n\nRESPIX Recon. Target\n\nTop-1 Acc.\n\nSampling Strategy\n\nTop-1 Acc.\n\nw/o norm. norm. w/ non-overlap win. norm. w/ sliding win.\n\n82.8 83.0 83.3\n\nUniform sampling argmax sampling softmax sampling\n\n77.2 78.5 83.3\n\nTarget for RESPIX. We believe an appropriate normalization technique can provide moderate hints that can help improve the enhancer’s representation quality with the RESPIX visual pretext task (see our discussion of Figure 4). As shown in Table 6, the proposed sliding window normalization improves the fine-tuning accuracy by 0.5% vs. the reconstruction target without normalization, and is also 0.3% better than the normalization method proposed in He et al. (2021).\n\nSampling Strategy for Visual Tokens. Using discrete visual tokens to represent images enables CIM to use stochastic sampling techniques during the corrupted image’s generation process, which can greatly enrich the output set of the generator and help the enhancer generalize well. For masked image modeling, randomly masking out a portion of patch embeddings can help regularize the pre-training, while for our approach, regularization for the enhancer mainly comes from the diversity of the corrupted images, therefore regularizations such as dropout & droppath are not used in CIM.\n\nAs presented in Table 7, the visual token representation with simple stochastic sampling from the generator output distribution is crucial for CIM. In contrast, we find that uniform sampling from the codebook of the image tokenizer regardless of the generator distribution or argmax sampling from the distribution cannot provide meaningful or diverse samples and therefore fails to pre-train the enhancer as expected.\n\nImage Corrupting Strategy. We find that it is crucial to use a generator with a small trainable BEiT to corrupt images in order to successfully pre-train CNN with the proposed CIM. We experiment with another generative visual pretext task for ResNet-50 pre-training, i.e., using 50% random erasing (Zhong et al., 2020) to corrupt the input image, and the model is required to recover the erased pixels based on the visible context. We find this pretext task fails to transfer well. A parallel work Tian et al. (2022) also finds that only using hand-crafted transformations to corrupt images is not quite satisfactory in generative visual pre-training of ViT.\n\nMethods\n\nPT Epochs FT Epochs Top-1 Acc.\n\nTable 8: Scaling CIM pre-training to larger ResNet.\n\nResNet-50x2 (#params: 94M) From Scratch SimCLR (Chen et al., 2020b) CIM-REVDET (Ours)\n\nScaling CIM to Larger CNNs. We study the scaling behavior of our CIM to larger CNNs. We choose two popular architectures in self-supervised learning literature: ResNet-50x2 and ResNet-50x4 (with width multipliers of 2x and 4x of vanilla ResNet50, respectively), and study the endto-end fine-tuning performance on ImageNet-1K in Table 8. We use an improved training recipe following Touvron et al. (2021a); Wightman et al. (2021), therefore our from scratch and SimCLR baselines are much higher (∼2 points higher) than the original results in Chen et al. (2020b). Notice that it is non-trivial to pre-train those large CNNs (e.g., ResNet-50x4 is 14 times bigger than ResNet-50 in #params). Under the end-to-end fine-tuning protocol, CIM is better than\n\nResNet-50x4 (#params: 375M) From Scratch SimCLR (Chen et al., 2020b) SimMIM (Xie et al., 2021) CIM-REVDET (Ours)\n\n100 / 200 81.6 / 82.1 100 / 200 81.7 / 82.2\n\n- 1000 300 300\n\n80.9 82.6 81.6 82.6\n\n400 100 100 100\n\n- 1000 300\n\n81.1\n\n400\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nthe recent MIM-based approach SimMIM and competitive with the representative Siamese model SimCLR.\n\nScaling CIM to Larger ViT. We study the scaling behavior of our CIM to ViT-Large in Table 9. Indeed, our approach can give ViT-Large a better initialization compared with the random initialization, and can also achieve better performance than MoCov3 that based on the canonical Siamese framework. Meanwhile, CIM still lags behind the MIM-based BEiT. Nevertheless, we believe CIM can serve as a promising starting point for exploring unified visual pre-training of various architectures.\n\nTable 9: Scaling CIM pre-training for ViT-Large.\n\nMethods\n\nTop-1 Acc.\n\nViT-Large (#params: 304M) From Scratch (He et al., 2021) MoCo-v3 (Chen et al., 2021) BEiT (Bao et al., 2021) CIM-RESPIX (Ours)\n\n82.6 84.1 85.2 84.3\n\nLimitation and Discussion. The image corrupting process of CIM still has a large room for improvement, which determines the characteristics and styles of the corrupted image distribution. The tokenizer we currently use is essentially a large CNN and adds nontrivial overhead during pre-training, i.e., the wall-clock time of 1-epoch training is about 2× of BEiT. Other image tokenizers, such as ViT-VQGAN (Yu et al., 2021), which report much higher throughput and better generation quality, deserve an in-depth study for CIM pre-training in the future.\n\n4 RELATED WORK\n\nSiamese Framework is the dominating self-supervised visual pre-training approach over the past few years, which typically relies on strong hand-crafted data augmentations to generate different views of the same image and learns in a contrastive manner. To maintain a large and informative negative sample set, memory banks (He et al., 2020) or large batch size (Chen et al., 2020b) is used. Follow-ups (Grill et al., 2020; Chen & He, 2021) further eliminate the requirement of using negative samples. Recent works (Caron et al., 2021; Chen et al., 2021) study self-supervised visual pre-training of ViT within Siamese frameworks.\n\nMasked Image Modeling (MIM) learns rich visual representations via masked parts prediction by conditioning on visible context only. ViT (Dosovitskiy et al., 2020) and iGPT (Chen et al., 2020a) report the first meaningful MIM visual pre-training results. BEiT (Bao et al., 2021) greatly improves MIM’s performance via masked visual token prediction, and PeCo (Dong et al., 2021) finds injecting perceptual similarity during visual codebook learning benefits MIM pre-trained representation. Recent work (He et al., 2021; Xie et al., 2021; Wei et al., 2021) re-explore pixel / feature regression in MIM, while Li et al. (2021); Zhou et al. (2021); El-Nouby et al. (2021) incorporate MIM within Siamese frameworks. As MIM is originated in masked language modeling (Devlin et al., 2019), CIM is inspired by Clark et al. (2020). In CIM, visual-token-based MIM plays an important role during the corrupted image generation process, as the stochastic sampling ability greatly enriches the corrupted image set.\n\n5 CONCLUSION\n\nWe introduce a general self-supervised visual pre-training framework with few architectural constraints for the model to be pre-trained and transferred. Unlike the mainstream Siamese pre-training methods based on strong artificial data augmentations as well as MIM pre-training relying on randomly inserting artificial [MASK] tokens to input embeddings, CIM pre-trained encoder learns from the corrupted view generated from a trainable neural network’s output distribution. Given the stochastic sampling ability, CIM defends using discrete visual token representations during pre-training to some extent. Experimental results show that our approach achieves competitive performance on canonical ViT and CNN models. We hope CIM can serve as a promising starting point for exploring flexible & unified visual representation learning of various architectures.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENT\n\nThis work is in part supported by the National Key Research and Development Program of China under Grant 2022YFB4500602. We would like to acknowledge Yaru Hao for the helpful discussions.\n\nREFERENCES\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n\narXiv:1607.06450, 2016.\n\nHangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of image transformers. arXiv preprint\n\narXiv:2106.08254, 2021.\n\nMaxim Berman, Hervé Jégou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain: a\n\nunified image embedding for classes and instances. arXiv preprint arXiv:1902.05509, 2019.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021.\n\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\n\nGenerative pretraining from pixels. In ICML, 2020a.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n\ncontrastive learning of visual representations. In ICML, 2020b.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021.\n\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\n\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020c.\n\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\n\ntransformers. arXiv preprint arXiv:2104.02057, 2021.\n\nZewen Chi, Shaohan Huang, Li Dong, Shuming Ma, Saksham Singhal, Payal Bajaj, Xia Song, and Furu Wei. Xlm-e: cross-lingual language model pre-training via electra. arXiv preprint arXiv:2106.16138, 2021.\n\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text\n\nencoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\n\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 702–703, 2020.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\n\nhierarchical image database. In CVPR, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\n\nbidirectional transformers for language understanding. In NAACL, 2019.\n\nXiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Peco: Perceptual codebook for bert pre-training of vision transformers. arXiv preprint arXiv:2111.12710, 2021.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Hervé Jegou, and Edouard arXiv preprint\n\nGrave. Are large-scale datasets necessary for self-supervised pre-training? arXiv:2112.10740, 2021.\n\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\n\nsynthesis. In CVPR, 2021.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\n\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n\nrecognition. In CVPR, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\n\nunsupervised visual representation learning. In CVPR, 2020.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked\n\nautoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\n\nTong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for\n\nimage classification with convolutional neural networks. In CVPR, 2019.\n\nElad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment\n\nyour batch: better training with larger batches. arXiv preprint arXiv:1901.09335, 2019.\n\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\n\nstochastic depth. In ECCV, 2016.\n\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with\n\nconditional adversarial networks. In CVPR, 2017.\n\nYann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1989.\n\nZhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong Zhu, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao, Ming Tang, et al. Mst: Masked self-supervised transformer for visual representation. NeurIPS, 2021.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\n\nsegmentation. In CVPR, 2015.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017.\n\nYu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, and Xia Song. COCO-LM: Correcting and contrasting text sequences for language model pretraining. In NeurIPS, 2021.\n\nMMSegmentation. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark.\n\nhttps://github.com/open-mmlab/mmsegmentation, 2020.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\n\nJason Tyler Rolfe. Discrete variational autoencoders. arXiv preprint arXiv:1609.02200, 2016.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.\n\narXiv preprint arXiv:1803.02155, 2018.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n\nDropout: A simple way to prevent neural networks from overfitting. JMLR, 2014.\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking\n\nthe inception architecture for computer vision. In CVPR, 2016.\n\nYunjie Tian, Lingxi Xie, Jiemin Fang, Mengnan Shi, Junran Peng, Xiaopeng Zhang, Jianbin Jiao, Qi Tian, and Qixiang Ye. Beyond masking: Demystifying token-based pre-training for vision transformers. arXiv preprint arXiv:2203.14313, 2022.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In ICML, 2021a.\n\nHugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going\n\ndeeper with image transformers. arXiv preprint arXiv:2103.17239, 2021b.\n\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n\nChen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. arXiv preprint arXiv:2112.09133, 2021.\n\nRoss Wightman, Hugo Touvron, and Hervé Jégou. Resnet strikes back: An improved training\n\nprocedure in timm. arXiv preprint arXiv:2110.00476, 2021.\n\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. arXiv preprint arXiv:2111.09886, 2021.\n\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021.\n\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\n\neffectiveness of deep features as a perceptual metric. In CVPR, 2018.\n\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmenta-\n\ntion. In AAAI, 2020.\n\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\n\nSemantic understanding of scenes through the ade20k dataset. IJCV, 2019.\n\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot:\n\nImage bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 ADDITIONAL ANALYSIS\n\nRelationship between the type of the generator and the performance of the enhancer. What makes a \"good\" generator for the enhancer? We believe there are three main factors that affect the output quality of the generator: (1) The masking strategy and masking ratio of the generator’s inputs. (2) The size / capacity of the small trainable BEiT. (3) The type of image tokenizers.\n\nWhile there are many perspectives / ways to evaluate a generator, this study focuses on visual pretraining of the enhancer, so we are particularly interested in how these factors affect the enhancer’s fine-tuning performance on downstream visual recognition tasks.\n\nFactor 1 & 2 has already been well studied in Table 4 & Table 5 respectively: either a too “weak” generator (e.g., too much masking or the size of the trainable BEiT is too small) or a too “strong” generator (e.g., too less masking or the trainable BEiT is too large) is harmful to the fine-tuning performance of the enhancer.\n\nAs for Factor 3, the image tokenizer represents a given image in the RGB domain as a permutation of discrete tokens with a fixed vocabulary size. This compact representation along with the stochastic sampling process can generate an abundant & diverse input set to feed the enhancer better. However, if the generator is too strong & robust that can always generate near ground truth output regardless of the stochastic sampling, the enhancer can hardly learn useful representations or even be wrongly penalized.\n\nTo show that, in Table 10 we study another well-established and open-sourced image tokenizer, VQGAN (Esser et al., 2021), on the ViT-B enhancer with 300 epochs pre-training & 100 epochs fine-tuning on ImageNet-1k. We also study the effects of directly using a MAE-Base model as the generator.\n\nTable 10: Study of different generator tpye of CIM pre-training for ViT-Base.\n\nGenerator Type of CIM\n\nMAE-style generator w/ 50% masking ratio BEiT-Style generator w/ VQGAN tokenizer BEiT-Style generator w/ DALL-E tokenizer (our default setting)\n\nTop-1 Acc.\n\n82.6 (-0.7) 82.9 (-0.4) 83.3\n\nFor the MAE-style generator, we sample RGB color values at all masked positions of the MAE decoder outputs. Since the stochastic sampling is performed on the RGB domain, only some low-level features (mainly color) can be changed and corrupted. Therefore the enhancer only learns to correct low-level attributes.\n\nFor the BEiT-style generator w/ VQGAN tokenizer, compared with the DALL-E tokenizer used as default, the VQGAN tokenizer is trained with two additional losses, i.e., the perceptual loss (Zhang et al., 2018) as well as the GAN loss (Isola et al., 2017). These two additional losses are originally intended for high-quality image synthesis, but could make the tokenizer become too strong & robust to generate appropriate corrupted samples for the enhancer. We visualize the corrupted samples from the VQGAN tokenizer, and we find it nearly reconstructs the original input even with stochastic token sampling. Therefore the samples from the VQGAN tokenizer are not diverse enough and cannot provide rich supervision for the enhancer to learn transferable representations.\n\nOverall, it is hard to find a good indicator from the generator that can directly reflect and measure the representation quality of the enhancer. By now, the best way is to honestly fine-tune the pre-trained enhancer on downstream tasks.\n\nStudy of training the generator first and keeping it fixed for the enhancer’s pre-training. We tried first train the generator separately for 300 epochs and then pre-train the enhancer for another 300 epochs while keeping the generator’s weights fixed. The performance suffers from a 0.4% degeneration. We hypothesize synergetic & simultaneous training provides a curriculum-like pretraining strategy for the enhancer where the generator starts off weak but gets better throughout training.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAdditional training cost of CIM compared to simple mask prediction with the same mask ratio. We study the relationship between the pre-training time and downstream performances of different approaches for both ViTs and ConvNets in Table 11 and Table 12 respectively.\n\nSince CIM is built upon BEiT, we choose BEiT as the masked image modeling baseline approach of ViTs. Here, we first study the ViT-B model’s 100-epoch fine-tuning performance on ImageNet-1k val set with different pre-training schedules in Table 11. The wall-clock time of 1-epoch pre-training of CIM is about 1.8x of BEiT (CIM has an additional tokenizer decoder compared with BEiT) on the same machine.\n\nTable 11: Study of the training cost for ViT-Base pre-training.\n\nMethods\n\nPT Epochs Relative PT Time\n\nTop-1 Acc.\n\nBEiT BEiT BEiT\n\nCIM CIM\n\n300 800 1600\n\n800 800\n\n1.0x 2.7x 5.3x\n\n1.8x 4.8x\n\n82.9 83.2 83.3\n\n83.3 83.4\n\nIn Table 12, we also study the ResNet-50x4 model’s 100-epoch fine-tuning performance on ImageNet1k val set with different pre-training schedules. We choose SimMIM as the masked image modeling baseline approach of ConvNets, for it reports the ResNet-50x4 model’s result in Appendix E of its paper. The wall-clock time of 1-epoch pre-training of CIM is about 2.6x of SimMIM (CIM has an additional generator, including a small BEiT and a tokenizer encoder & decoder compared with SimMIM) on the same machine.\n\nTable 12: Study of the training cost for ResNet-50x4 pre-training.\n\nMethods\n\nPT Epochs Relative PT Time Top-1 Acc.\n\nSimMIM CIM\n\n300 100\n\n1.0x 0.9x\n\n81.6 82.2\n\nThese results imply that CIM can obtain better fine-tuning performance with less pre-training time compared with baseline approaches for both ViTs and ConvNets.\n\nA.2 A NOTE ON VISUALIZATIONS IN §2.2 AND FIGURE 3\n\nSince there exists information loss in any form of normalization, we have to inject the original image’s information in order to visualize the enhancer output (4th column in Figure 3a). In order to comprehensively demonstrate our method’s behavior, we also include the unnormalized counterpart in Figure 3b for reference, where there is no additional information injection during visualization.\n\nA.3 TRAINING AND OPTIMIZATION DETAILS\n\nThe auxiliary generator and the enhancer are simultaneously trained and synergistically (rather than adversarially as GAN (Goodfellow et al., 2014)) updated. The trainable part of the generator, i.e., the small BEiT, learns a MIM objective in the same vein as in BEiT (Bao et al., 2021). Formally, given an input image’s patch embedding sequence x = (x1, ..., xn), we randomly mask k embeddings at positions m = (m1, ..., mk) using [MASK] token2. The resulting masked input sequence xmasked for BEiT is:\n\nmi ∼ uniform{1, n},\n\nfor i = 1, ..., k,\n\nxmasked = replace(x, m, [MASK]),\n\n(1)\n\nwhere the replace(x, m, [MASK]) operation denotes using the special [MASK] token to replace patch embeddings of x at positions m. The small BEiT then encodes xmasked and learns to maximize\n\n2Typically, we set k equal to 100 ∼ 120 given the input sequence length n of 196, i.e., about 50% ∼ 60% of\n\nthe total input patch embeddings are masked out.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nlog pBEiT(g | xmasked), i.e., the log-likelihood of the golden visual tokens g = (g1, ..., gk) at the masked positions m conditioned on xmasked. Notice that the golden tokens are obtained by feeding the original image to the image tokenizer encoder.\n\nIn order to generate corrupted image samples I corrupted for the enhancer, we sample tokens’ replacements from the BEiT output distribution pBEiT at each masked position j of the encoded xmasked:\n\nj\n\nxsampled\n\n∼ pBEiT(xsampled xcorrupted = replace(g, m, xsampled),\n\n| xmasked),\n\nj\n\nfor j ∈ m,\n\n(2)\n\nwhere the replace(g, m, xsampled) operation denotes using the sampled visual token xsampled to replace golden tokens of g at positions m. Next, the image tokenizer decoder maps xcorrupted to a corrupted image I corrupted. The whole image tokenizer is frozen (i.e., not updated throughout the pre-training phase), which directly uses the publicly available3 pre-trained DALL-E dVAE weight (Ramesh et al., 2021) following BEiT.\n\nThe enhancer takes the corrupted image I corrupted as input. For the RESPIX visual pretext task, the enhancer is optimized by a combination of l1 and l2 loss for pixel regression. For the REVDET variant, the enhancer is learned by binary cross-entropy loss for replaced visual token detection. The gradients of the enhancer are not back-propagated through the generator.\n\nIn this paper, we study CIM self-supervised pre-trained vanilla ViT (Dosovitskiy et al., 2020) and vanilla ResNet (He et al., 2016) models. The vanilla ViT models refer to the design from (Dosovitskiy et al., 2020; Touvron et al., 2021a) without further architectural change such as using relative position embeddings (Shaw et al., 2018) and LayerScale (Touvron et al., 2021b). The vanilla ResNet-50 model refers to the torchvision ResNet-50 (Paszke et al., 2019) without any architectural change. The larger ResNet-50x2 and ResNet-50x4 models follows the canonical design in SimCLR (Chen et al., 2020b). We conduct experiments on 16× or 32× V100 GPUs with 32GB memory.\n\n3https://github.com/openai/DALL-E\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nA.4 PRE-TRAINING & FINE-TUNING CONFIGURATIONS\n\nA.4.1 THE IMAGENET-1K CIM PRE-TRAINING CONFIGURATIONS FOR VANILLA VIT AND\n\nRESNET MODELS\n\nPre-training Config. (ViT & ResNet)\n\nValue\n\nOptimizer Pre-training Epochs Peak Learning Rate Batch Size Weight Decay Optimizer Momentum (β1, β2) Learning Rate Schedule Gradient Clipping Warmup Epochs # Masked Patches for the Generator The Generator’s Depth The Generator’s Width The Enhancer’s Loss Weight Data Augmentation Dropout (Srivastava et al., 2014) Stochastic Depth (Huang et al., 2016) LayerScale (Touvron et al., 2021b) Pos. Emb. in Transformer Layers\n\nPatch Size Pre-training Resolution\n\nAdamW (Loshchilov & Hutter, 2017) 300 1.5e-3 2048 0.05 (0.9, 0.98) (Vaswani et al., 2017) Cosine Decay 3.0 10 100 to 120, Random Masking 4 to 6 Same to the Enhancer (ViT), 384 (ResNet) 1 for REVDET, 10 for RESPIX RandomResizedCrop Only ✗\n✗ ✗\n1-D Absolute Pos. Emb. (Dosovitskiy et al., 2020) 16 224\n\nTable 13: The ImageNet-1K CIM pre-training settings for vanilla ViT-S/16, ViT-B/16 and ResNet-50 models. Notably, the pre-training configurations are almost the same for different architectures. We implement the pre-training using the codebase of BEiT (Bao et al., 2021). Mixed precision and deepspeed acceleration are used.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nA.4.2 THE IMAGENET-1K IMAGE CLASSIFICATION FINE-TUNING CONFIGURATIONS FOR\n\nVANILLA VIT MODELS\n\nFine-tuning Config. (ViT)\n\nValue\n\nOptimizer Fine-tuning Epochs Peak Learning Rate\n\nLayer-wise Learning Rate Decay (Bao et al., 2021) Batch Size Weight Decay Optimizer Momentum (β1, β2) Learning Rate Schedule Warmup Epochs Gradient Clipping Dropout (Srivastava et al., 2014) Stochastic Depth (Huang et al., 2016) Label Smoothing (Szegedy et al., 2016) Mixup (Zhang et al., 2017) CutMix (Yun et al., 2019) Random Augmentation (Cubuk et al., 2020) Patch Size Fine-tuning Resolution Test Resolution Test Crop Ratio Loss Function\n\nAdamW (Loshchilov & Hutter, 2017) 200 for ViT-S/16, 100 for ViT-B/16 3e-3 for ViT-B/16 RESPIX, 5e-3 for ViT-B/16 REVDET, 3e-3 or 4e-3 for ViT-S/16 0.8 (Clark et al., 2020)\n\n1024 0.05 (0.9, 0.999) Cosine Decay 5\n✗ ✗\n0.1 0.1 0.8 1.0 9 / 0.5 16 224 224 0.95 Cross Entropy Loss\n\nTable 14: The ImageNet-1K image classification fine-tuning recipes for vanilla ViT-S/16 and ViT-B/16. We implement the fine-tuning using the codebase of BEiT (Bao et al., 2021). Mixed precision and deepspeed acceleration are used. We select the best learning rate out of {3e-3, 4e-3, 5e-3} for different sized models and pre-training objectives, and the absolute difference between the worst and the best learning rate is less than 0.3 in terms of the top-1 accuracy.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nA.4.3 THE IMAGENET-1K IMAGE CLASSIFICATION FINE-TUNING CONFIGURATIONS FOR\n\nVANILLA RESNET-50\n\nFine-tuning Config. (ResNet-50)\n\n100 Epoch FT 300 Epoch FT 600 Epoch FT\n\nOptimizer Peak Learning Rate Layer-wise Learning Rate Decay (Bao et al., 2021) Batch Size Learning Rate Schedule Loss Function Warmup Epochs Weight Decay Fine-tuning Resolution Test Resolution Test Crop Ratio Repeated Augmentation (Berman et al., 2019; Hoffer et al., 2019) Random Augmentation (Cubuk et al., 2020) Mixup (Zhang et al., 2017) CutMix (Yun et al., 2019) Label Smoothing (Szegedy et al., 2016) Stochastic Depth (Huang et al., 2016) Dropout (Srivastava et al., 2014) Layer-wise Learning Rate Decay\n\nAdamW (Loshchilov & Hutter, 2017) 12e-3 ✗\n\n2048 Cosine Decay Binary Cross Entropy Loss 5\n0.02 224 224 0.95 ✓\n\n0.02 160\n\n✗\n\n0.01 224\n\n✓\n\n6 / 0.5\n\n7 / 0.5\n\n7 / 0.5\n\n0.1\n\n0.1 ✗\n\n0.1 1.0 ✗\n✗ ✗\n✗\n\n0.2\n\n0.1 0.05\n\nTable 15: The ImageNet-1K image classification fine-tuning recipes for vanilla ResNet-50. We use the AdamW optimizer. The hyperparameter settings basically follows (Wightman et al., 2021). We implement the fine-tuning based on the codebase of BEiT (Bao et al., 2021). Mixed precision and deepspeed acceleration are used. For other self-supervised baseline approaches we compared in Table 2, we select the best learning rate out of {5e-3, 8e-3, 12e-3} and keep other settings unchanged.\n\n18",
    "reference": "# Summary Of The Paper\n\nThis paper focuses on the self-supervised learning. Based on masked image modeling, the authors introduce a new algorithm which uses corrupted as training sources instead of masked ones. They utilize a new trainable module including a pretrained transformer decoder to generate such corrupted images. The models are trained to reconstruct the original images or predict whether a patch is corrupted for self-supervision.\n\n# Strength And Weaknesses\n\nStrength:\n1. The analysis about the difference between CIM and MIM and the advantage of CIM is interesting.\n2. The authors provide abundant experiments on several datasets to validation the effectiveness of their method.\n\nWeaknesses:\n1. How are the special mask embeddings designed? Are they trainable?\n2. The authors refer to “golden token” several times before providing its definition in the REVDET paragraph in Sec.2.1. It would be better if the authors can reorganize for this problem.\n3. It seems that the generated corrupted images from the pretrained dVAE decoder are somehow the smoothed version of the original images. Therefore I wonder if it is possible to directly produce corrupted image by artificial smoothing on some random regions without training any new modules?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is clearly written and in good quality. The whole pipeline is easy to reproduce based on the code provided by the authors.\n\n# Summary Of The Review\n\nThe proposed method is generally a good one, with inspiring information on using non-contrasive self-supervised learning on CNN. However I am concerned with the necessity of using an extra module for the generation of corrupted images.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nA CLOSER LOOK AT SELF-SUPERVISED LIGHTWEIGHT VISION TRANSFORMERS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nSelf-supervised learning on large-scale Vision Transformers (ViTs) as pre-training methods has achieved promising downstream performance. Yet, how much these pre-training paradigms promote lightweight ViTs’ performance is considerably less studied. In this work, we mainly develop and benchmark self-supervised pre-training methods, e.g., contrastive-learning-based MoCo-v3, masked-imagemodeling-based MAE on image classification tasks, and some downstream dense prediction tasks. We surprisingly find that if proper pre-training is adopted, even vanilla lightweight ViTs show comparable performance on ImageNet to previous SOTA networks with delicate architecture design. We also point out some defects of such pre-training, e.g., failing to benefit from large-scale pre-training data and showing inferior performance on data-insufficient downstream tasks. Furthermore, we analyze and clearly show the effect of such pre-training by analyzing the properties of the layer representation and attention maps for related models. Finally, based on the above analyses, a distillation strategy during pre-training is developed, which leads to further downstream performance improvement for MAE-based pre-training.\n\n1\n\nINTRODUCTION\n\nSelf-supervised learning (SSL) has shown great progress in representation learning without heavy reliance on expensive labeled data. SSL focuses on various pretext tasks for pre-training. Among them, several works (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2020; Chen et al., 2021a; Caron et al., 2021) based on contrastive learning (CL) have achieved comparable or even better accuracy than supervised pre-training when transferring the learned representations to downstream tasks. Recently, another trend focuses on masked image modeling (MIM) (Bao et al., 2021; He et al., 2021; Zhou et al., 2022), which perfectly fits Vision Transformers (ViTs) (Dosovitskiy et al., 2020) for vision tasks, and achieves improved generalization performance. Most of these works, however, involve large networks with little attention paid to smaller ones. Some works (Fang et al., 2020; Abbasi Koohpayegani et al., 2020; Choi et al., 2021) focus on contrastive self-supervised learning on small convolutional networks (ConvNets) and improve the performance by distillation. However, the pre-training of lightweight ViTs is considerably less studied.\n\nEfficient neural networks are essential for modern on-device computer vision. Recent study on achieving top-performing lightweight models mainly focuses on designing network architectures (Sandler et al., 2018; Howard et al., 2019; Graham et al., 2021; Ali et al., 2021; Heo et al., 2021; Touvron et al., 2021b; Mehta & Rastegari, 2022; Chen et al., 2021b; Pan et al., 2022), while with little attention on how to optimize the training strategies for these models. We believe the latter is also of vital importance, and the utilization of pre-training is one of the most hopeful approaches along this way, since it has achieved great progress on large models. To this end, we develop and benchmark recently popular self-supervised pre-training methods, e.g., CL-based MoCo-v3 (Chen et al., 2021a) and MIM-based MAE (He et al., 2021), along with fully-supervised pre-training for lightweight ViTs as the baseline on both ImageNet and some other classification tasks as well as some dense prediction tasks, e.g., object detection and segmentation. We surprisingly find that if proper pre-training is adopted, even vanilla lightweight ViTs show comparable performance to previous SOTA networks with delicate design on ImageNet, which achieves 78.5% top-1 accuracy on ImageNet with vanilla ViT-Tiny (5.7M). We also observe some intriguing defects of such pre-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ntraining, e.g., failing to benefit from large-scale pre-training data and showing inferior performance on data-insufficient downstream tasks.\n\nThese findings motivate us to dive deep into the working mechanism of these pre-training methods for lightweight ViTs. More specifically, we introduce a variety of model analysis methods to study the pattern of layer behaviors during pre-training and fine-tuning, and investigate what really matters for downstream performance. First, we find that lower layers of the pre-trained models matter more than higher ones if sufficient downstream data is provided, while higher layers matter in data-insufficient downstream tasks. Second, we observe that the pre-training alters the attention behaviors of the final recognition model little, without introducing locality inductive bias, which is, however, the commonly adopted rule for recent network architecture design (Mehta & Rastegari, 2022; Heo et al., 2021; Touvron et al., 2021b; Liu et al., 2021). Based on the above analyses, we also develop a distillation strategy for MAE-based pre-training, which improves the pre-training of lightweight ViTs. Better downstream performance is achieved especially on data-insufficient classification tasks and detection tasks.\n\n2 PRELIMINARIES AND EXPERIMENTAL SETUP\n\nViTs. We use ViT-Tiny (Touvron et al., 2021a) as the base model in our study to examine its downstream performance with pre-training, which contains 5.7M parameters. We adopt the vanilla architecture, consisting of 12 layers with the embedding dimension of 192, except that the number of heads is increased to 12 as we find it can improve the model’s expressive power. We use this improved version by default. ViT-Tiny is chosen for study because it is an ideal experimental object, on which almost all existing pre-training methods can directly apply, and has a rather naive structure, which can eliminate the influence of the model architecture on our analysis to a great extent.\n\nEvaluation Metrics. Linear probing has been a popular protocol to evaluate the quality of the pre-trained weights (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2020), in which only the prediction head is tuned based on the downstream training set while the pre-trained representations are kept frozen. However, prior works point out that linear evaluation does not always correlate with utility (He et al., 2021; Newell & Deng, 2020).\n\nFine-tuning is another evaluation protocol, in which all the layers are tuned by first initializing them with the pre-trained models. We adopt this by default. Besides, layer-wise lr decay (Bao et al., 2021) is also taken into consideration. By default, we do the evaluation on ImageNet (Deng et al., 2009) by fine-tuning on the train split and evaluating on the validation split. Several other downstream classification datasets (Nilsback & Zisserman, 2008; Parkhi et al., 2012; Maji et al., 2013; Krause et al., 2013; Krizhevsky et al., 2009; Van Horn et al., 2018) and object detection and segmentation tasks on COCO (Lin et al., 2014) are also exploited for comparison in our study.\n\nCompared Methods. Baseline: We largely follow the recipe in DeiT (Touvron et al., 2021a) except for some hyper-parameters of augmentations (see Appendix A.1 for our improved recipe) and fully-supervised train a ViT-Tiny from scratch for 300 epochs on the training set of ImageNet1k. It achieves 74.5% top-1 accuracy on the validation set of ImageNet-1k, surpassing that in the original architecture (72.2%) through modifying the number of heads to 12 from 3, and further reaches 75.8% by adopting the improved training recipe, which finally serves as our strong baseline to examine the pre-training. We denote this supervised trained model by DeiT-Tiny.\n\nMAE: MAE (He et al., 2021) is selected as a representative for MIM-based pre-training methods, which has a simple framework with low training cost. We largely follow the design of MAE except that the encoder is altered to ViT-Tiny. Several basic factors and components are adjusted to fit the smaller encoder (see Appendix A.2). By default, we do pre-training on the train split of ImageNet-1k (Deng et al., 2009) (dubbed IN1K) for 400 epochs, and denote the pre-trained model as MAE-Tiny.\n\nMoCov3: We also implement a contrastive SSL pre-training counterpart to achieve a more thorough study. MoCo-v3 (Chen et al., 2021a) is selected for its simplicity. We use MoCov3-Tiny to denote this pre-trained model with 400 epochs. Details are provided in Appendix A.3.\n\nSome other methods, e.g., MIM-based SimMIM Xie et al. (2022) and CL-based DINO Caron et al. (2021) are also involved, but are moved to Appendix B.5 due to the space limitation.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparisons on pre-training methods. We report top-1 accuracy on the validation set of ImageNet1k (Deng et al., 2009). IN1K and IN21K indicate the training set of ImageNet-1k and ImageNet-21k (Deng et al., 2009). The pre-training time is measured on 8×V100 GPU machine. ViT-Tiny is adopted for all entries. ‘ori.’ represents the training recipe in Touvron et al. (2021a) and ‘impr.’ represents our improved recipe (see Appendix A.1).\n\nMethods\n\nData\n\nEpochs Time (hour)\n\nPre-training\n\nFine-tuning recipe Top-1 Acc. (%)\n\nfrom scratch from scratch Supervised (Steiner et al., 2021) Supervised (Steiner et al., 2021) MoCo-v3 (Chen et al., 2021a) MAE (He et al., 2021)\n\n- -\nIN21K w/ labels IN21K w/ labels IN1K w/o labels IN1K w/o labels\n\n- -\n30 300 400 400\n\n- -\n20 200 52 23\n\nori. impr. impr. impr. impr. impr.\n\n74.5 75.8 76.9 77.8 73.7 78.0\n\n3 HOW WELL DOES PRE-TRAINING WORK ON LIGHTWEIGHT VITS?\n\nMAE outperforms other pre-training methods on ImageNet. We develop and benchmark fullysupervised and self-supervised pre-training methods on ImageNet, as reported in Tab. 1. For all of the pre-trained models, we fine-tune them for 300 epochs on IN1k for fair comparisons. It can be seen that most of these supervised and self-supervised pre-training methods improve the downstream performance, whilst MAE outperforms others and consumes moderate training cost. Meanwhile the pre-training of MoCo-v3 leads to performance degradation. We denote the fine-tuned model based on the pre-training of MAE-Tiny as MAE-Tiny-FT.\n\nEnhanced vanilla ViTs with pre-training are comparable to previous SOTA networks. We further compare the enhanced ViT-Tiny (5.7M) with MAE pre-training to the DeiT-Tiny (Touvron et al., 2021a) baseline and other previous lightweight ConvNets and ViT derivatives in Tab. 2. We report top-1 accuracy along with the model parameter count and the throughput, which is borrowed from PyTorch Image Models (timm) (Wightman, 2019). In specific, we extend the training epochs during fine-tuning to 1000 epochs following Touvron et al. (2021a). The resulting models are on par with or even outperform most previous ConvNets and ViT derivatives with comparable parameters or throughput. This demonstrates the usefulness of the advanced lightweight ViT pre-training strategy which is orthogonal to the network architecture design strategy in the ViT derivatives. Besides, we also compare with the methodology of pre-training lightweight ConvNets or ViT derivatives for a more fair comparison (Fang et al., 2020; Abbasi Koohpayegani et al., 2020; Gao et al., 2021; Choi et al., 2021). We find that most of them are evaluated under the linear probing protocol. We thus implemented the above methodology by ourselves, i.e., adopting the pre-training with SEED (Fang et al., 2020) on EfficientNet-B0 (Tan & Le, 2019) under the fine-tuning protocol. The result, however, shows no improvement (from 77.7% to 77.2%).\n\nTable 3: Effect of pre-training data. Top-1 accuracy is reported.\n\nThe pre-training benefits little from large-scale data. Furthermore, we observe that MAE is robust to the pretraining dataset scale and class distribution in contrast to MoCo-v3 as shown in Tab. 3. We consider two subsets of IN1K containing 1% and 10% of the total examples (1% IN1K and 10% IN1K) balanced in terms of classes (Assran et al., 2021), one subset with long-tailed class distribution (Liu et al., 2019) (IN1K-LT), and IN21K. This observation is consistent with El-Nouby et al. (2021) on larger ViTs. pre-training methods that they fail to benefit from large-scale pre-training data.\n\nIN1K\n\nDatasets MoCo-v3\n\nMAE\n\n73.7\n\n78.0 77.9 (-0.1) 1% IN1K 73.1 (-0.6) 10% IN1K 73.5 (-0.2) 78.0 (+0.0) 73.0 (-0.7) IN1K-LT 77.9 (-0.1) 73.8 (+0.1) 78.0 (+0.0) IN21K\n\nIt also reveals the limitation of these\n\nDownstream data scale matters. As shown in Tab. 4, we transfer the learned representations of different pre-trained models to several other downstream tasks (Nilsback & Zisserman, 2008; Parkhi et al., 2012; Maji et al., 2013; Krause et al., 2013; Krizhevsky et al., 2009; Van Horn et al., 2018) to investigate their effects. In addition to using the self-supervised pre-trained models, i.e., MAE-Tiny and MoCov3-Tiny, both of which are pre-trained for 400 epochs, a fully-supervised counterpart based on IN1K with 300-epoch pre-training (i.e., DeiT-Tiny) is also involved. An interesting observation is that the self-supervised pre-training approaches achieve downstream performance far\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Comparisons with previous SOTA networks on ImageNet-1k. We report top-1 accuracy on ImageNet-1k validation set (Deng et al., 2009), ImageNet Real (Beyer et al., 2020) and ImageNet V2 matched frequency (Recht et al., 2019), along with throughput and parameter count. The throughput is borrowed from timm (Wightman, 2019), which is measured on a single RTX 3090 GPU with a batch size fixed to 1024 and mixed precision. IN1K and IN21K indicate the training set of ImageNet-1k and ImageNet-21k. †indicates that distillation is adopted during the supervised training (or fine-tuning). ⋆ indicates the original architecture of ViT-Tiny, and others use the improved architecture (number of heads is changed to 12), e.g., MAE-Tiny-FT in the table.\n\nMethods\n\npre-train data\n\n#param.\n\nthroughput Val Real\n\nV2\n\n(image/s)\n\nTop-1 Top-1 Top-1\n\nResNet-18 (He et al., 2016) ResNet-50 (He et al., 2016; Wightman et al., 2021)\n\n- -\n\nConvNets\n\nEfficientNet-B0 (Tan & Le, 2019) EfficientNet-B0 (Fang et al., 2020) EfficientNet-B1 (Tan & Le, 2019)\n\nMobileNet-v2 (Sandler et al., 2018) MobileNet-v3 (Howard et al., 2019) MobileNet-v3†(Beyer et al., 2021)\n\n- IN1K w/o labels -\n\n- -\n-\n\n12M 25M\n\n5M 5M 8M\n\n4M 5M 5M\n\nVision Transformers Derivative\n\nLeViT-128 (Graham et al., 2021) LeViT-192 (Graham et al., 2021)\n\nXCiT-T12/16†(Ali et al., 2021)\n\nPiT-Ti†/ 1000 epochs (Heo et al., 2021)\n\nCaiT-XXS-24†(Touvron et al., 2021b)\n\nMobileViT-S (Mehta & Rastegari, 2022)\n\nSwin-1G (Liu et al., 2021; Chen et al., 2021b)\n\nLVT (Yang et al., 2021)\n\nEdgeViT-XS (Pan et al., 2022)\n\nMobile-Former-294M (Chen et al., 2021b)\n\n- -\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\nVanilla Vision Transformers\n\nDeiT-Tiny⋆ (Touvron et al., 2021a) DeiT-Tiny⋆†/ 1000 epochs (Touvron et al., 2021a) MAE-Tiny-FT MAE-Tiny-FT / 1000 epochs\n\n- -\nIN1K w/o labels IN1K w/o labels\n\n9M 11M\n\n7M\n\n5M\n\n12M\n\n6M\n\n7M\n\n6M\n\n7M\n\n11M\n\n6M 6M 6M 6M\n\n8951 2696\n\n5369 5369 2953\n\n7909 9113 9113\n\n13276 11389\n\n3157\n\n4547\n\n1351\n\n1900\n\n-\n\n-\n\n-\n\n-\n\n4844 4764 4020 3956\n\n69.7 80.4\n\n77.7 77.2 78.8\n\n72.0 75.2 77.0\n\n78.6 80.0\n\n78.6\n\n76.4\n\n78.4\n\n78.3\n\n77.3\n\n74.8\n\n77.5\n\n77.9\n\n72.2 76.6 78.0 78.5\n\n77.3 85.7\n\n84.0 83.5 84.6\n\n80.2 82.2 -\n\n84.8 85.6\n\n84.1\n\n82.0\n\n85.2\n\n84.3\n\n-\n\n-\n\n-\n\n-\n\n57.2 68.7\n\n66.3 65.9 67.5\n\n60.2 63.4 -\n\n66.6 67.9\n\n67.0\n\n63.1\n\n67.4\n\n66.9\n\n-\n\n-\n\n-\n\n-\n\n80.1 83.9 84.3 85.3\n\n60.4 65.4 66.2 67.1\n\nTable 4: Transfer evaluation on classification tasks and dense-prediction tasks. Self-supervised pretraining approaches generally show inferior performance to the fully-supervised counterpart. Top-1 accuracy is reported for classification tasks and AP is reported for object detection (det.) and instance segmentation (seg.) tasks.The description of each dataset is represented as (train-size/test-size/#classes).\n\nInit.\n\nDatasets Flowers (2k/6k/102)\n\nPets Aircraft Cars (7k/3k/100)\n\n(8k/8k/196)\n\n(4k/4k/37)\n\nCifar100 (50k/10k/100)\n\niNat18 (438k/24k/8142)\n\nCOCO(det.) COCO(seg.) (118k/50k/80)\n\nsupervised DeiT-Tiny\n\nself-supervised MoCov3-Tiny MAE-Tiny\n\n96.4\n\n93.1\n\n73.5\n\n85.6\n\n94.8 85.8\n\n87.8 76.5\n\n73.7 64.6\n\n83.9 78.8\n\n85.8\n\n83.9 78.9\n\n63.6\n\n54.5 60.6\n\n40.7\n\n40.0 38.9\n\n36.5\n\n36.0 35.1\n\nbehind the fully-supervised counterpart, while the performance gap is narrowed more or less as the data scale of the downstream task increases. Moreover, MAE even shows inferior results to MoCov3. We conjecture that it is due to their different layer behaviors during pre-training and fine-tuning, e.g., undesired representations of the higher layers in MAE-Tiny, which will be discussed in detail in the following section. We refer the reader to Appendix A.4 for more details about those tasks.\n\nFor a more thorough study, we further evaluate on downstream object detection and segmentation tasks on COCO (Lin et al., 2014) based on Li et al. (2021) (see Appendix A.5 for details), with different pre-trained models as initialization of the backbone, as shown in Tab. 4. The self-supervised pre-training also lags behind the fully-supervised counterpart and MAE-Tiny still shows worse results than MoCov3-Tiny.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Layer representation similarity within and across models as heatmaps (the left three columns), with x and y axes indexing the layers (the 0 index indicates the patch embedding layer), and higher values indicate higher similarity. We also plot the corresponding layer similarity in the last column based on the diagonal elements of the left heatmaps.\n\n4 REVEALING THE SECRETS OF THE PRE-TRAINING\n\nIn this section, we introduce some model analysis methods to study the pattern of layer behaviors during pre-training and fine-tuning, and investigate what matters for downstream performances.\n\n4.1 LAYER REPRESENTATION ANALYSES\n\nWe first adopt Centered Kernel Alignment (CKA) method1 (Cortes et al., 2012; Nguyen et al., 2020) to analyze the layer representation similarity across and within networks. Specifically, CKA computes the normalized similarity in terms of the Hilbert-Schmidt Independence Criterion (HSIC (Song et al., 2012)) between two feature maps or representations, which is invariant to the orthogonal transformation of representations and isotropic scaling (detailed in Appendix A.6). Fourier analysis of feature maps is also involved in analyzing the behaviors of the models.\n\nLower layers matter more than higher ones if sufficient downstream data is provided. We visualize the layer representation (Rep.) similarity between several pre-trained models and DeiTTiny as heatmaps in Fig. 1. The similarity within DeiT-Tiny is also presented for reference (the left column). We also plot the corresponding layer similarity in the last column based on the diagonal elements of the left heatmaps. We choose DeiT-Tiny as the reference because we consider the higher similarity between the pre-trained models and DeiT-Tiny (classification model fullysupervised trained from scratch) indicates more relevance to recognition for the self-supervised layers. Although the similarity does not directly indicate whether the downstream performance is good or not, it indeed reflects the pattern of layer representation to a certain extent. In Appendix B.1, stronger supervised trained ViTs are introduced as reference models, and we find that these supervised ViTs generally have similar layer representation structures.\n\nFirst, We observe a relatively high similarity between MAETiny and DeiT-Tiny for lower layers, while low similarity for higher layers. Similar phenomenon is observed w.r.t. other reference models as shown in Appendix B.1. It indicates fewer semantics are extracted for MAE-Tiny at a more abstract level in higher layers. Another empirical evidence is the low linear probing performance of MAE-Tiny (23.4% top-1 accuracy). In contrast, MoCov3-Tiny aligns DeiT-Tiny well across almost all layers. However, the fine-tuning evaluation in Tab. 1 shows that adopting the MAE-Tiny as initialization significantly improves the performance while MoCov3-Tiny degrades performance. Thus, we hypothesize that lower layers matter much more than higher ones for the pre-trained models. In order to verify the hypothesis, we design another experiment by only reserving several leading blocks of pre-trained models and randomly initializing the others, and then fine-tuning them on ImageNet (for the sake of simplicity, we\n\nFigure 2: Lower layers of pre-trained models contribute to most gains on downstream ImageNet dataset.\n\n1https://github.com/AntixK/PyTorch-Model-Compare\n\n5\n\n0246810127071727374757677ImageNetNumberofReservedBlocksAcc(%)MAE-TinyMoCov3-TinyUnder review as a conference paper at ICLR 2023\n\nFigure 4: The contributions from higher layers on performance gain increase as the downstream dataset scale shrinks, which indicates higher layers matter in data-insufficient downstream tasks.\n\nonly fine-tune these models on IN1K for 100 epochs). Fig. 2 shows that reserving only a certain number of leading blocks achieves a significant performance gain over randomly initializing all the blocks (i.e., totally training from scratch) for both MAE-Tiny and MoCov3-Tiny. Whereas, further reserving higher layers leads to marginal gain for MAE-Tiny or even degradation for MoCov3-Tiny, which demonstrates our hypothesis.\n\nThen we examine why MoCov3-Tiny performs worse than MAE-Tiny. The Fourier analysis of feature maps is carried out as a supplement beyond CKA-based similarity, since we find that the similarity is largely dominated by low-frequency components. In Fig. 3, we plot the ∆log amplitude across layers, which is the difference between the log amplitude at normalized low frequency (0.0π) and high frequency (1.0π) in each layer. It is also used in Park & Kim (2021) to analyze the differences between ViTs and ConvNets. We find that a large amount of high-frequency signals are reduced in the first layer of MoCov3-Tiny (i.e., patch embedding layer), which shows a great difference from other models. This behavior possibly strengthens the robustness against various image augmentations, which is beneficial to the instance discrimination task, but results in an over-spatially-smoothed feature map at the very beginning of the network forward processing, leading to an inferior downstream performance on ImageNet.\n\nFigure 3: MoCov3-Tiny behaves differently at the first layer, reducing a lot of high-frequency signals.\n\nHigher layers matter in data-insufficient downstream tasks. Previous works (Touvron et al., 2021a; Raghu et al., 2021) demonstrate the importance of a relatively large dataset scale for highperformance ViTs with large model sizes. We also observe a similar phenomenon on lightweight ViTs even with the self-supervised pre-training adopted as discussed in Sec. 3. It motivates us to study the key factor in downstream performance on data-insufficient tasks.\n\nWe conduct similar experiments as those in Fig. 2 on small-scale downstream datasets. The results are shown in Fig. 4. We observe consistent performance improvement as the number of reserved blocks from pre-trained models increases. And the smaller the dataset scale, the more the performance gain from higher layers. It demonstrates that higher layers are still valuable and matter in data-insufficient downstream tasks. Furthermore, we observe comparable performance for the transfer performance of MAE-Tiny and MoCov3-Tiny when only a certain number of lower layers are reserved, while MoCov3-Tiny surpasses when higher layers are further adopted. It indicates the higher layers of MoCov3-Tiny work better than MAE-Tiny on data-insufficient downstream tasks, which is also consistent with our CKA-based analyses shown in Fig. 1, that MoCov3-Tiny learns more semantics at abstract level relevant to recognition in higher layers (high similarity to reference recognition models in higher layers) than MAE-Tiny. And also we conjecture that high-frequency information matters less in these relatively easier tasks.\n\n4.2 ATTENTION MAP ANALYSES\n\nThe attention maps reveal the behaviors for aggregating information in the attention mechanism, which are computed from the compatibility of queries and keys by dot-product operation. We intro-\n\n6\n\n024681012405060708090CIFAR10050k/10k/100NumberofReservedBlocksAcc(%)MAE-TinyMoCov3-Tiny02468101220406080Cars8k/8k/102NumberofReservedBlocksAcc(%)MAE-TinyMoCov3-Tiny02468101220406080Pets4k/4k/37NumberofReservedBlocksAcc(%)MAE-TinyMoCov3-Tiny024681012−4−3.5−3−2.5−2−1.5−1−0.50Depth∆LogAmplitudeDeiT-TinyMAE-TinyMoCov3-TinyUnder review as a conference paper at ICLR 2023\n\nFigure 5: Attention distance and entropy analyses. We visualize the averaged attention distance and entropy across all tokens in different attention heads w.r.t. the layer number.\n\nduce two metrics for further analyses on the pre-trained models, i.e., attention distance and attention entropy. The attention distance for the j-th token of h-th head is calculated as:\n\nDh,j =\n\n(cid:88)\n\ni\n\nsoftmax(Ah)i,jGi,j,\n\n(1)\n\nRl×l is the attention map for the h-th attention head, and Gi,j is the Euclidean distance where Ah between the spatial locations of the i-th and j-th tokens. l is the number of tokens. And the attention entropy is calculated as:\n\n∈\n\nEh,j =\n\n(cid:88)\n\n−\n\ni\n\nsoftmax(Ah)i,jlog(softmax(Ah)i,j),\n\n(2)\n\nSpecifically, the attention distance reveals how much local vs. global information is aggregated, and a lower distance indicates that each token focuses more on neighbor tokens. The attention entropy reveals the concentration of the attention distribution, and lower entropy indicates that each token attends to fewer tokens. We analyze the averaged attention distance and entropy across all the tokens in different attention heads, as shown in Fig. 5.\n\nThe pre-training with MAE alters the attention behaviors of the final recognition model little. First, we compare MAE-Tiny-FT with DeiT-Tiny. The former adopts MAE-Tiny as initialization and then is fine-tuned on IN1K, and the latter is supervised trained from scratch on IN1K. We observe very similar attention behaviors between them. They both have diverse attention heads in lower layers, which aggregate both local and global tokens with both concentrated and broad focus, and more global and broad attention in higher layers. It indicates that the pre-training does not alter the behaviors of the ultimate recognition model much, but provides a better initial state.\n\nThe pre-training with MAE improves the results not by bringing locality inductive bias. Then, we focus on the attention behaviors of MAE-Tiny. It shows similar patterns to DeiT-Tiny on lower layers, but more distinct patterns on higher layers, which means its higher layers concentrate (low entropy) on local spatial information (low distance). We hypothesize that the behaviors are related to the aim of the pixel reconstruction task in MIM (Masked Image Modeling). Considering only lower layers of MAE-Tiny matter as analyzed previously, we think the pre-training improves the performance not by introducing the locality inductive bias. It indicates that the bias may not be necessary for lightweight models to achieve top performance, which is, however, the key idea of many successful lightweight network architectures (Mehta & Rastegari, 2022; Touvron et al., 2021b; Heo et al., 2021). As for the MoCov3-Tiny, we observe relatively global and broad attention with low diversity in lower layers, which may be not suitable for downstream tasks.\n\n5 DISTILLATION IMPROVES PRE-TRAINED MODELS\n\nIn this section, we focus on developing a distillation strategy for the top-performing MAE pretraining on lightweight models, to remedy some defects of this strategy. In the previous section,\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Distillation helps to compress the good representation of the teacher (MAE-Base) to the student, thus the distilled student shows higher similarity to the supervised trained DeiT-Tiny.\n\nFigure 7: Distillation on attention maps of higher layers improves performance most.\n\nwe have conjectured that it is hard for MAE to learn good representation relevant to recognition in higher layers, which results in unsatisfactory performance on data-insufficient downstream tasks. A natural question is that can it gain more semantic information by scaling up the models. We further examine a large pre-trained model, MAE-Base (He et al., 2021), and find it achieves a better alignment to DeiT-Tiny, as shown in the left column of Fig. 6. It indicates that it is possible to extract features relevant to recognition in higher layers for the scaled-up encoder in MAE pre-training.\n\nThese observations motivate us to compress the knowledge of large pre-trained models to tiny ones, i.e., applying knowledge distillation during the pre-training phase for lightweight ViTs. Although it is a common practice to perform distillation to obtain pre-trained compressed language models (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020; 2021; Sun et al., 2020; Su et al., 2021), how to apply distillation to obtain better lightweight ViT pre-training under the masked image modeling framework is still unexplored. We fill this gap and propose some useful techniques.\n\nDistillation methods. Specifically, a pre-trained MAE-Base (He et al., 2021) is introduced as the teacher network. We adopt the attention-based distillation, which is formulated as follows based on the mean squared error (MSE) between the corresponding layers of the teacher and student:\n\nLattn = MSE(AT , M AS),\n\n(3)\n\n∈\n\nRh×l×l and AS\n\nRh′×l×l refer to the attention maps of the teacher and student with where AT Rh×h′ h and h′ attention heads, and l is the number of tokens. A learnable mapping matrix M is introduced to align the number of heads. The teacher is also applied only on the same unmasked patches in the encoder as the student during the distillation.\n\n∈\n\n∈\n\nDistillation on lower or higher layers? We first examine applying the above layer-wise distillation on which layer contributes to the most performance gain. Though it is a direct way to apply distillation on all corresponding layers of the teacher and student, it actually slows down the training speed. As shown in Fig. 7, only distilling on the attention maps of the last transformer blocks promote the performance most, even surpassing those distilling on all layers or other single lower layers (for the sake of simplicity, we only fine-tune the pre-trained models on IN1K for 100 epochs). It is consistent with the analyses in Sec. 4. Specifically, the lower layers learn good representation themselves during the pre-training with MAE and thus distilling on these layers contributes to marginal improvement, while the higher layers rely on a good teacher to guide them to capture rich semantic features.\n\nDistillation improves pre-trained models for downstream tasks. We further evaluate the distilled pre-trained model on several downstream classification tasks (Nilsback & Zisserman, 2008; Parkhi et al., 2012; Maji et al., 2013; Krause et al., 2013; Krizhevsky et al., 2009; Van Horn et al., 2018; Deng et al., 2009) and dense prediction tasks. For simplicity, we only apply distillation on the attention maps of the last layer. The visualization results in Fig. 6 show that the good representation relevant to the recognition of the pre-trained teacher is compressed to the distilled MAE-Tiny. Especially the quality of higher layers is improved. It contributes to better downstream performance on both classification and dense-prediction tasks as shown in Tab. 5, especially on object detection and segmentation tasks, surpassing the supervised pre-training counterpart by a large margin.The above results also support our insight in Sec. 4.1.\n\n8\n\n3691275.57676.57777.5w/odistill.distill.alllayersDistilledLayerIndexAcc(%)Under review as a conference paper at ICLR 2023\n\nTable 5: Distillation improves downstream performance on classification tasks and object detection and segmentation tasks. Top-1 accuracy is reported for classification tasks and AP is reported for object detection (det.) and instance segmentation (seg.) tasks.\n\nDatasets\n\nInit.\n\nsupervised DeiT-Tiny\n\nFlowers\n\nPets\n\nAircraft\n\nCars\n\nCifar100\n\niNat18\n\nImageNet COCO(det.) COCO(seg.)\n\n96.4\n\n93.1\n\n73.5\n\n85.6\n\n85.8\n\n63.6\n\n-\n\n40.7\n\n36.5\n\nself-supervised MAE-Tiny\n\nDistilled MAE-Tiny 95.2 (+9.4) 89.1 (+12.6) 79.2 (+14.6) 87.5 (+8.7) 85.0 (+6.1) 63.6 (+3.0)\n\n85.8\n\n76.5\n\n64.6\n\n78.8\n\n78.9\n\n60.6\n\n78.0 78.4 (+0.4)\n\n38.9 42.7 (+3.8)\n\n35.1 38.2 (+3.1)\n\n6 RELATED WORKS\n\nSelf-supervised learning (SSL) focuses on different pretext tasks (Gidaris et al., 2018; Zhang et al., 2016; Noroozi & Favaro, 2016; Dosovitskiy et al., 2014) for pre-training without using manually labeled data. Among them, contrastive learning (CL) has been popular and shows promising results on various convolutional networks (ConvNets) (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2020) and ViTs (Chen et al., 2021a; Caron et al., 2021). Recently, methods based on masked image modeling (MIM) achieve the state-of-the-art on ViTs (He et al., 2021; Bao et al., 2021; Zhou et al., 2022) It has been demonstrated that these methods can scale up well on larger models, while their performance on lightweight ViTs is seldom investigated.\n\nVision Transformers (ViTs) Dosovitskiy et al. (2020) apply a Transformer architecture (a stack of attention modules (Vaswani et al., 2017)) on image patches and show very competitive results in various visual tasks (Touvron et al., 2021a; Liu et al., 2021; Li et al., 2022). The performance of ViTs has been largely improved thanks to better training recipes (Touvron et al., 2021a; Steiner et al., 2021; Touvron et al., 2022). As for lightweight ViTs, most works focus on integrating ViTs and ConvNets (Graham et al., 2021; Heo et al., 2021; Mehta & Rastegari, 2022; Chen et al., 2021b; Yan et al., 2021), while few focus on how to optimize the networks.\n\nKnowledge Distillation is a mainstream approach for model compression(Buciluˇa et al., 2006), in which a large teacher network is trained first and then a more compact student network is optimized to approximate the teacher (Hinton et al., 2015; Romero et al., 2014; Shen & Xing, 2022). Touvron et al. (2021a) achieves better accuracy on ViTs by adopting a ConvNet as the teacher. With regard to the compression of the pre-trained networks, some works (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020; 2021; Sun et al., 2020; Su et al., 2021) attend to distill large-scale pre-trained language models. In the context of computer vision, a series of works (Fang et al., 2020; Abbasi Koohpayegani et al., 2020; Choi et al., 2021; Shen et al., 2021) focus on transferring knowledge of large pre-trained networks based on CL to lightweight ConvNets. There are few works focusing on improving the quality of lightweight pre-trained ViTs based on MIM by distillation thus far.\n\n7 DISCUSSIONS\n\nLimitations Our study is restricted to classification tasks and some dense-prediction tasks, e.g., object detection and segmentation. We leave the exploration of more tasks for further work.\n\nConclusions We investigate the self-supervised pre-training of lightweight ViTs, and demonstrate the usefulness of the advanced lightweight ViT pre-training strategy in improving the performance of downstream tasks. Some properties about the pre-training are revealed, e.g., these methods fail to benefit from large-scale pre-training data, and show more dependency on the downstream dataset scale. We also present some insights on what matters for the downstream performance with pretraining by analyzing the layer representation and attention map. They may indicate potential future directions in improving pre-training on lightweight models, the value of which has also been demonstrated as it guides the design of our proposed distillation strategy and helps to achieve much better downstream performance. We expect our research may provide useful experience and advance the study of self-supervised learning on lightweight ViTs.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSoroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Compress: Selfsupervised learning by compressing representations. Adv. Neural Inform. Process. Syst., 33: 12980–12992, 2020.\n\nAlaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-covariance image transformers. Adv. Neural Inform. Process. Syst., 34, 2021.\n\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, Nicolas Ballas, and Michael Rabbat. Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples. In Int. Conf. Comput. Vis., pp. 8443–8452, 2021.\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n\narXiv:1607.06450, 2016.\n\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\n\narXiv:2106.08254, 2021.\n\nLucas Beyer, Olivier J H ́enaff, Alexander Kolesnikov, Xiaohua Zhai, and A ̈aron van den Oord. Are\n\nwe done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n\nLucas Beyer, Xiaohua Zhai, Am ́elie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. arXiv preprint arXiv:2106.05237, 2021.\n\nCristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, pp.\n\n535–541, 2006.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Adv. Neural Inform. Process. Syst., 2020.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Int. Conf. Comput. Vis., pp. 9650–9660, October 2021.\n\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\n\nImproved baselines with momentum\n\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020.\n\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\n\ntransformers. In Int. Conf. Comput. Vis., pp. 9640–9649, 2021a.\n\nYinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobile-former: Bridging mobilenet and transformer. arXiv preprint arXiv:2108.05895, 2021b.\n\nHee Min Choi, Hyoa Kang, and Dokwan Oh. Unsupervised representation transfer for small networks: I believe i can distill on-the-fly. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Adv. Neural Inform. Process. Syst., 2021. URL https://openreview. net/forum?id=BYrJYl1rexa.\n\nCorinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based\n\non centered alignment. The Journal of Machine Learning Research, 13:795–828, 2012.\n\nEkin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Adv. Neural Inform. Process. Syst., volume 33, pp. 18613–18624. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale In IEEE Conf. Comput. Vis. Pattern Recog., pp. 248–255. Ieee,\n\nhierarchical image database. 2009.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. Adv. Neural Inform. Process. Syst., 27:766–774, 2014.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2020.\n\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv ́e Jegou, and Edouard arXiv preprint\n\nGrave. Are large-scale datasets necessary for self-supervised pre-training? arXiv:2112.10740, 2021.\n\nZhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng Liu. Seed:\n\nSelf-supervised distillation for visual representation. In Int. Conf. Learn. Represent., 2020.\n\nYuting Gao, Jia-Xin Zhuang, Ke Li, Hao Cheng, Xiaowei Guo, Feiyue Huang, Rongrong Ji, and Xing Sun. Disco: Remedy self-supervised learning on lightweight models with distilled contrastive learning. arXiv preprint arXiv:2104.09124, 2021.\n\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by\n\npredicting image rotations. In Int. Conf. Learn. Represent., 2018.\n\nPriya Goyal, Piotr Doll ́ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n\nBenjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv ́e J ́egou, and Matthijs Douze. Levit: A vision transformer in convnet’s clothing for faster inference. In Int. Conf. Comput. Vis., pp. 12259–12269, October 2021.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Pires, Zhaohan Guo, Mohammad Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In Adv. Neural Inform. Process. Syst., 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 770–778, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In IEEE Conf. Comput. Vis. Pattern Recog., pp.\n\nunsupervised visual representation learning. 9729–9738, 2020.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked\n\nautoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\n\nByeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh. In Int. Conf. Comput. Vis., pp. 11936–\n\nRethinking spatial dimensions of vision transformers. 11945, 2021.\n\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\n\npreprint arXiv:1503.02531, 2015.\n\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig Adam. Searching for mobilenetv3. In Int. Conf. Comput. Vis., October 2019.\n\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\n\nstochastic depth. In Eur. Conf. Comput. Vis., pp. 646–661. Springer, 2016.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by\n\nreducing internal covariate shift. In Int. Conf. Machine Learning., pp. 448–456, 2015.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of Empirical Methods in Natural Language Process., pp. 4163–4174, 2020.\n\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained\n\ncategorization. In Int. Conf. Comput. Vis. Worksh., pp. 554–561, 2013.\n\nAlex Krizhevsky et al. Learning multiple layers of features from tiny images. Technical Report,\n\n2009.\n\nYanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He, and Ross Girshick. Benchmarking detection transfer learning with vision transformers. arXiv preprint arXiv:2111.11429, 2021.\n\nYanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer back-\n\nbones for object detection. arXiv preprint arXiv:2203.16527, 2022.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Eur. Conf.\n\nDoll ́ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. Comput. Vis., pp. 740–755. Springer, 2014.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Int. Conf. Comput. Vis., pp. 10012–10022, 2021.\n\nZiwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Largescale long-tailed recognition in an open world. In IEEE Conf. Comput. Vis. Pattern Recog., June 2019.\n\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv\n\npreprint arXiv:1608.03983, 2016.\n\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\n\nvisual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n\nSachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose, and mobileURL https://\n\nIn Int. Conf. Learn. Represent., 2022.\n\nfriendly vision transformer. openreview.net/forum?id=vh-0sUt8HlG.\n\nAlejandro Newell and Jia Deng. How useful is self-supervised pretraining for visual tasks? In IEEE\n\nConf. Comput. Vis. Pattern Recog., pp. 7345–7354, 2020.\n\nThao Nguyen, Maithra Raghu, and Simon Kornblith. Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth. In Int. Conf. Learn. Represent., 2020.\n\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722–729. IEEE, 2008.\n\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw\n\npuzzles. In Eur. Conf. Comput. Vis., pp. 69–84. Springer, 2016.\n\nJunting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz Dudziak, Hongsheng Li, Georgios Tzimiropoulos, and Brais Martinez. Edgevits: Competing light-weight cnns on mobile devices with vision transformers. arXiv preprint arXiv:2205.03436, 2022.\n\nNamuk Park and Songkuk Kim. How do vision transformers work? In Int. Conf. Learn. Represent.,\n\n2021.\n\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 3498–3505, 2012. doi: 10.1109/CVPR.2012.6248092.\n\nMaithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? Adv. Neural Inform. Process. Syst., 34, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers In International Conference on Machine Learning, pp. 5389–5400.\n\ngeneralize to imagenet? PMLR, 2019.\n\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and\n\nYoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.\n\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 4510–4520, 2018.\n\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of\n\nbert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n\nZhiqiang Shen and Eric Xing. A fast knowledge distillation framework for visual recognition. In\n\nEur. Conf. Comput. Vis., pp. 673–690. Springer, 2022.\n\nZhiqiang Shen, Zechun Liu, Jie Qin, Lei Huang, Kwang-Ting Cheng, and Marios Savvides. S2-bnn: Bridging the gap between self-supervised real and 1-bit neural networks via guided distribution calibration. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 2165–2174, 2021.\n\nLe Song, Alex Smola, Arthur Gretton, Justin Bedo, and Karsten Borgwardt. Feature selection via\n\ndependence maximization. The Journal of Machine Learning Research, 13(5), 2012.\n\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021.\n\nWeiyue Su, Xuyi Chen, Shikun Feng, Jiaxiang Liu, Weixin Liu, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-tiny: a progressive distillation framework for pretrained transformer compression. arXiv preprint arXiv:2106.02241, 2021.\n\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a compact task-agnostic bert for resource-limited devices. In Association for Computational Linguistics, pp. 2158–2170, 2020.\n\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural net-\n\nworks. In Int. Conf. Machine Learning., pp. 6105–6114. PMLR, 2019.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In Int. Conf. Machine Learning., volume 139, pp. 10347–10357, July 2021a.\n\nHugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv ́e J ́egou. Going\n\ndeeper with image transformers. In Int. Conf. Comput. Vis., pp. 32–42, 2021b.\n\nHugo Touvron, Matthieu Cord, and Herv ́e J ́egou. Deit iii: Revenge of the vit. arXiv preprint\n\narXiv:2204.07118, 2022.\n\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In IEEE Conf. Comput. Vis. Pattern Recog., June 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Adv. Neural Inform. Process. Syst., 30, 2017.\n\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. Adv. Neural Inform. Process. Syst., 33:5776–5788, 2020.\n\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. Minilmv2: Multi-head selfattention relation distillation for compressing pretrained transformers. In Findings of Int. Joint Conf. on Natural Language Process., pp. 2140–2151, 2021.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nRoss Wightman.\n\nPytorch image models.\n\nhttps://github.com/rwightman/\n\npytorch-image-models, 2019.\n\nRoss Wightman, Hugo Touvron, and Herv ́e J ́egou. Resnet strikes back: An improved training\n\nprocedure in timm. arXiv preprint arXiv:2110.00476, 2021.\n\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In IEEE Conf. Comput. Vis. Pattern Recog., 2022.\n\nHaotian Yan, Zhe Li, Weijian Li, Changhu Wang, Ming Wu, and Chuang Zhang. Contnet: Why not\n\nuse convolution and transformer at the same time? arXiv preprint arXiv:2104.13497, 2021.\n\nChenglin Yang, Yilin Wang, Jianming Zhang, He Zhang, Zijun Wei, Zhe Lin, and Alan Yuille. Lite\n\nvision transformer with enhanced self-attention. arXiv preprint arXiv:2112.10809, 2021.\n\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Int. Conf. Comput. Vis., pp. 6023–6032, 2019.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. In Int. Conf. Learn. Represent., 2018.\n\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In Eur. Conf. Comput.\n\nVis., pp. 649–666. Springer, 2016.\n\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong.\n\nibot:\n\nImage bert pre-training with online tokenizer. Int. Conf. Learn. Represent., 2022.\n\n14",
    "reference": "# Summary Of The Paper\n\nThis is an empirical paper focusing on exploring the task of self-supervised learning on lightweight vision transformers (ViTs). In this paper, the authors observed several discoveries, such as: the tiny model fails to benefit from large-scale pre-training data and shows inferior performance on data-insufficient downstream tasks. They further proposed a distillation strategy during pretraining to improve the representation ability of compact ViTs. Experiments are conducted on ImageNet pre-training and multiple downstream tasks and datasets.\n\n# Strength And Weaknesses\n\n### Strengths:\n\n   - It is interesting to explore suitable methods for lightweight vision transformers or other efficient models in the self-supervised learning manner. \n\n   - This paper provided extensive experiments on ImageNet pre-training and multiple downstream datasets and tasks, such as classification, object detection, and segmentation.\n\n### Weaknesses:\n\n   - Though this is an empirical paper, the novelty and originality in it are fairly limited, as well as the significance and contribution which are also not strong. The observations that tiny models fail to benefit from large-scale pre-training data, and rely more on the downstream dataset scale are a little bit straightforward and not surprising. The use of knowledge distillation for self-supervised learning on lightweight models also has been proposed for a long time, e.g., on low-bit efficient models [1] and mobile-level models [2].\n\n[1] Shen, Z., Liu, Z., Qin, J., Huang, L., Cheng, K. T., & Savvides, M. (2021). S2-bnn: Bridging the gap between self-supervised real and 1-bit neural networks via guided distribution calibration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2165-2174).\n\n[2] Fang, Zhiyuan, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng Liu. \"SEED: Self-supervised Distillation For Visual Representation.\" In International Conference on Learning Representations. 2021.\n\n   - Some statements in this paper are not well supported, such as “lower layers of the pre-trained models matter more than higher ones if sufficient downstream data is provided, while higher layers matter in data-insufficient downstream tasks.” I think a better and fair comparison design is crucial and also necessary for this argument. The current experiments for this part are not rigorous to prove it.\n\n   - The writing and organization of this paper can also be improved. For instance, it’s not clear to me why Table 1 is located in the early part of the paper. I did not get much information from it and do not know what the insight of this table is.\n\n   - Overall, this paper seems a little bit incremental without providing new conclusions or discoveries over previous literature.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity of this paper is qualified. Novelty and originality are somewhat limited. Since this paper did not introduce any new and concrete approach, reproducibility is not applicable.\n\n# Summary Of The Review\n\nOverall, this is an empirical paper with some trivial observations which are not well supported by the experiments and some are even not new. Also, the organization and writing can be improved significantly in this paper. Thus, I tend to reject it.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nHARD-META-DATASET++: TOWARDS UNDERSTANDING FEW-SHOT PERFORMANCE ON DIFFICULT TASKS\n\nSamyadeep Basu ∗, Megan Stanley, John Bronskill, Soheil Feizi, Daniela Massiceti {sbasu12, sfeizi}@umd.edu, {jfb54}@cam.ac.uk {meganstanley, dmassiceti}@microsoft.com\n\nABSTRACT\n\nFew-shot classification is the ability to adapt to any new classification task from only a few training examples. The performance of current top-performing fewshot classifiers varies widely across different tasks where they often fail on a subset of ‘difficult’ tasks. This phenomenon has real-world consequences for deployed few-shot systems where safety and reliability are paramount, yet little has been done to understand these failure cases. In this paper, we study these difficult tasks to gain a more nuanced understanding of the limitations of current methods. To this end, we develop a general and computationally efficient algorithm called FASTDIFFSEL to extract difficult tasks from any large-scale vision dataset. Notably, our algorithm can extract tasks at least 20x faster than existing methods enabling its use on large-scale datasets. We use FASTDIFFSEL to extract difficult tasks from META-DATASET, a widely-used few-shot classification benchmark, and other challenging large-scale vision datasets including ORBIT, CURE-OR and OBJECTNET. These tasks are curated into HARD-META-DATASET++, a new fewshot testing benchmark to promote the development of methods that are robust to even the most difficult tasks. We use HARD-META-DATASET++ to stress-test an extensive suite of few-shot classification methods and show that state-of-the-art approaches fail catastrophically on difficult tasks. We believe that our extraction algorithm FASTDIFFSEL and HARD-META-DATASET++ will aid researchers in further understanding failure modes of few-shot classification models.\n\n1\n\nINTRODUCTION\n\nFew-shot classification is the ability to distinguish between a set of novel classes when given only a few labelled training examples of each class (Lake et al., 2011; Fei-Fei et al., 2006). This holds potential across many real-world applications – from robots that can identify new objects (Ren et al., 2020), to drug discovery pipelines that can predict the properties of new molecules (Stanley et al., 2021). A few-shot image classifier is given a few labelled training images of the new object classes, called the support set. Once the classifier has adapted to this support set, it is then evaluated on novel test images of those classes, called the query set. Together, the support and query set is called a task.\n\nRecent years have seen rapid progress in few-shot image classification (Snell et al., 2017; Finn et al., 2017b; Ye et al., 2020; Li et al., 2021; Radford et al., 2021; Kolesnikov et al., 2019), however, current top-performing methods display a wide range in performance over different tasks at test time (Fu et al., 2022; Agarwal et al., 2021). On META-DATASET (MD), a widely-used few-shot classification benchmark (Triantafillou et al., 2019), state-of-the-art classifiers obtain accuracies as low as 22% on some individual tasks though their average task accuracy is >55% (see Fig 1). Few works have undertaken a detailed examination of these ‘difficult’ tasks, yet they remain critical to interrogate for both future algorithmic development and the safety and reliability of deployed systems.\n\nThis paper aims to gain a more nuanced understanding of these ‘difficult’ tasks and the limitations of current methods. We define a difficult task as one on which a few-shot classifier performs poorly on the task’s query set, after being adapted to its support set. Current methods for finding supports sets that lead to poor query performance rely on greedy search-based algorithms (Agarwal et al., 2021). These approaches, however, incur a high computational cost when sampling for a large numbers of\n\n∗Work done partly during a research internship at Microsoft Research, Cambridge (UK)\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: A state-of-the-art method (Hu et al., 2022) performs consistently worse on difficult tasks in the MD split (HARD-MD) of HARD-META-DATASET++ compared to tasks in META-DATASET (MD) across all 10 MD sub-datasets. The method uses ViT-S initialized with self-supervised DINO weights and is further meta-trained with ProtoNets on MD’s ilsvrc 2012 split.\n\ntasks, and for tasks with large support sets, as are common (and best) practices in few-shot evaluation protocols. As a result, the study of difficult tasks has been limited to small-scale datasets which lacks the challenging examples and the setup of large benchmarks such as META-DATASET.\n\nTo address this, we develop a general and computationally efficient algorithm called FASTDIFFSEL to extract difficult tasks from any large-scale dataset. Given a (meta-)trained few-shot classifier, a query set, and a search pool of support images, we formulate a constrained combinatorial optimization problem which learns a selection weight for each image in the pool such that the loss on the query set is maximised. The top-k (i.e. most difficult) images per class are then extracted into a support set and paired with the query set to form a difficult task. This optimization can be repeated to obtain any number of difficult tasks. In practice, we find that FASTDIFFSEL is at least 20x faster than existing greedy search-based algorithms (Agarwal et al., 2021), with greater gains as the support pools and support set sizes increase.\n\nWe leverage the scalability of FASTDIFFSEL to extract difficult tasks from a wide range of largescale vision datasets including META-DATASET (Triantafillou et al., 2019), OBJECTNET (Barbu et al., 2019), CURE-OR (Temel et al., 2018) and ORBIT (Massiceti et al., 2021) and collect these tasks into a new testing set called HARD-META-DATASET++ (HARD-MD++). The addition of datasets beyond META-DATASET is motivated by their real-world nature and that they provide image annotations of quality variations (e.g. object occluded, blurred, poorly framed) to enable future research into why a task is difficult. We provide early insights into this question in our analyses.\n\nWe stress test an extensive suite of state-of-the-art few-shot classification methods on HARD-MD++, cross-validating the difficulty of our extracted tasks across these top-performing methods. In Fig 1, we show one such method (Hu et al., 2022) performing consistently worse on the META-DATASET test split in HARD-MD++ than on the original MD test split across all 10 sub-datasets. In Section 5, we find that this trend holds true across a wide-range of few-shot classification methods.\n\nWe release HARD-MD++ along with a broad set of baselines to drive future research in methods that are robust to even the most difficult tasks. In summary, our contributions are the following:\n\n1. FASTDIFFSEL, an efficient algorithm to extract difficult tasks from any large-scale vision\n\ndataset.\n\n2. HARD-META-DATASET++, a new test-only few-shot classification benchmark composed of difficult tasks extracted from the widely-used few-shot classification benchmark METADATASET and other large-scale real-world datasets: OBJECTNET, CURE-OR and ORBIT. 3. Extensive stress testing and novel empirical insights for a wide range of few-shot classification methods, including transfer- and meta-learning based approaches on HARD-MD++.\n\n2 FEW-SHOT CLASSIFICATION: PRELIMINARIES AND NOTATIONS A few-shot classification task is typically composed of (i) a support set S which contains a few labelled examples from a set of N classes (e.g., kj examples for each class index j ∈ [1, N ])\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 FASTDIFFSEL: Efficient algorithm for extracting a difficult few-shot task\n\nRequire: Q : task query set; N : number of classes (way), P : search pool for extracting task j=1: set\n\nsupport set; fθ: (meta-)trained base model; M : size of search pool; α: learning rate; {kj}N containing number of shots per class. w ← CONCAT(wj) for j in N do\n\n▷ Concatenate randomly initialized vectors for each class\n\n∀j ∈ [1, N ]\n\ncj ← (cid:80)|Pj |\n\ni=1 wi\n\njfθ(xj)/ (cid:80)|Pj |\n\ni=1 wi\n\nj\n\nend for c ← [c1, ..., cN ] L ← PROTO-LOSS(Q, c, fθ) L.BACKWARD(w) w ← w + α∇wL(w) wj ← PROJ(wj, kj) sj ←EXTRACT(kj,wj, P) S ← CONCAT(sj)\n\n∀j ∈ [1, N ]\n\n∀j ∈ [1, N ]\n\n∀j ∈ [1, N ]\n\n▷ Compute weighted class prototypes for each class\n\n▷ Store the weighted prototypes for each class ▷ Compute prototypical loss (Snell et al., 2017) ▷ Compute gradients with respect to selection weights ▷ Gradient ascent for updating weights ▷ Projection step per class ▷ Extract kj examples with the highest weights ▷ Obtain the final difficult support set S\n\nand (ii) a query set Q which contains a disjoint set of test examples for each of those N classes (e.g., qj examples for each class j ∈ [1, N ]). Given a trained base model fθ, the goal in few-shot classification is to use S to adapt fθ to the task such that the model can then make predictions on the unseen examples in Q. This is typically done for many tasks which are randomly sampled from a given dataset D = {(xi, yi)}T i=1 consisting of T examples and J classes in total, where x is the input image and y is the corresponding class label. In few-shot sampling, tasks are constructed by first sampling a set of N classes from the larger set of J classes and subsequently sampling the support and query set. Tasks are typically referred to as N -way, k-shot tasks if kj = k, ∀j. However, if the number of classes N varies across tasks and the number of shots per class varies across the N classes (kj, ∀j ∈ [1, N ]), they are referred to as variable-way, variable-shot tasks. Usually the number of query examples is kept fixed across all the classes (i.e. qj = q, ∀j ∈ [1, N ]).\n\nBase model training. The underlying model fθ is typically trained using one of two approaches: (i) meta-learning (Snell et al., 2017; Lee et al., 2019; Finn et al., 2017a) which involves training the model in an episodic manner on tasks sampled from a base training dataset, or (ii) transfer learning (Tian et al., 2020; Hu et al., 2022; Chen et al., 2019) which involves first pre-training a feature extractor on a large base dataset in an supervised or self-supervised manner, and then finetuning the final classification layer (or the entire model) at test time for each new test task.\n\n3 FASTDIFFSEL: AN EFFICIENT ALGORITHM TO SELECT DIFFICULT\n\nSUPPORT SETS\n\nThe first step to understanding the limitations of current few-shot classification methods is to study the difficult tasks on which they fail or perform poorly. We define a difficult task as one for which a given few-shot classifier, after being adapted to its support set, performs significantly worse on its query set compared to the mean query performance over all tasks. Finding a support set from a given search pool that leads to poor performance on a given query set for the purposes of study, however, has combinatorial complexity. Current solutions have therefore turned to greedy search-based algorithms (Agarwal et al., 2021; Fu et al., 2022), however, the computational cost quickly becomes infeasible for larger search pools and support set sizes, thus limiting study to small-scale datasets (e.g. CIFAR-FS (Bertinetto et al., 2019), mini-ImageNet (Vinyals et al., 2016)). We specifically address this limitation by proposing a fast and general optimization-based algorithm – FASTDIFFSEL which offers a speedup of at least 20-25x over greedy search-based algorithms, allowing the extraction of difficult few-shot tasks to be scaled to a wide array of large-scale vision datasets.\n\n3.1 PROPOSED METHOD\n\nOverview. We present FASTDIFFSEL, that can sample a difficult few-shot classification task in a deterministic way. The key intuition of our approach is use a model’s loss on the task’s query set (i.e. after the model has been adapted to the support set) as a proxy for the difficulty of the task. This follows Arnold et al. (2021); Dhillon et al. (2019a) which show that a task’s query loss is an effective surrogate for task difficulty and is also simple to compute. Given a model, a fixed query set and a pool of examples, we cast support set extraction as a constrained optimization problem and learn a selection weight for each example in the pool such that the loss on the query set is maximized. We can then construct a difficult support set by drawing the examples with the highest selection weights. The extracted support set is then paired with the fixed query set to form the difficult task. We can repeat this optimization for any number of query sets to extract any number of difficult tasks.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFormally, given a dataset D, we first sample N unique classes and a query set Q. Here Q = {(xr, yr)}N ×q r=1 , where x is the input image and y is the class label. Let D′ ⊂ D denote a sub-dataset containing examples from only the N sampled classes and let P = D′−Q denote the set of examples from D′ without the query set Q. Allowing P to be the search pool from which the difficult support set will be extracted, the goal of the extraction algorithm is to find a support set S ⊂ P such that the loss on the query set Q is maximized after the base model fθ has been adapted on S. To this end, we assume selection weights w ∈ RM , where wi is associated with the ith example in P and M = |P| denotes the cardinality of P. The optimization objective is to learn the selection weights w which result in the maximal loss for query set Q with a sparsity constraint on the weights. Formally:\n\nmax w\n\nN ×q (cid:88)\n\nr=1\n\nl((xr, yr), P, w, fθ)\n\n(1)\n\ns.t. wi ∈ {0, 1}, ∥wj∥0 ≤ kj,\n\n∀i ∈ [1, M ]\n\n∀j ∈ [1, N ]\n\nwhere fθ is the base model trained with either meta-learning or supervised learning, l is the loss after adapting fθ on P where each of its examples are weighted by w, and wj is the selection weight vector corresponding to the jth class. Here w = w1 ⊕ w2 ⊕ ... ⊕ wN and wi j is the selection weight for the ith example in the weight vector wj for the jth class. Note w are the only learnable parameters. The optimization constraints ensure that each selection weight is either 0 or 1, and that a maximum of kj examples are selected for each jth class. Different approaches can be used to adapt fθ (Requeima et al., 2019; Chen et al., 2020). In our work, we adopt a ProtoNets adaptation (Snell et al., 2017) as it is highly efficient and has no learnable parameters. This approach computes a mean embedding for each class, with the loss based on the Euclidean distance between a query image embedding to each of the class prototypes.\n\nWe solve Equation (1) in two steps: (i) first, we take 1 gradient ascent step on the selection weights w to obtain ˆw; (ii) second, we project the selection weight vector of each class ˆwj to the l0 norm ball to obtain the final selection weights ̄wj (∀j ∈ [1, N ]). In practice, (ii) is known to be difficult to solve as it is NP-hard and the l0 norm constraint is non-convex (Candes et al., 2005; Candes & Tao, 2005; Natarajan, 1995; Donoho, 2006). We, therefore, relax the l0 norm to an l1 norm to make the constraint convex following Donoho (2006) which shows that the l1 norm relaxation gives effective sparse solutions. The projection step with l1 relaxation for the jth class can be formalized as follows:\n\nmin wj\n\n1 2\n\n∥wj − ˆwj∥2\n\n2\n\ns.t. ∥wj∥1 ≤ kj\n\n(2)\n\nWe solve the dual form of the above projection step via Lagrange multipliers (Boyd & Vandenberghe, 2004) to obtain the optimal sparse weight vector ̄wj:\n\n ̄wj = arg max\n\nλj ≥0\n\nmin wj\n\n1 2\n(cid:124)\n\n∥wj − ˆwj∥2\n\n2 + λj(∥wj∥1 − kj) (cid:125)\n\n(cid:123)(cid:122) g(λj ,wj )\n\n(3)\n\nwhere λj is the Lagrange multiplier corresponding to the projection step for the jth class. In practice, we solve the projection step per class to ensure at least a few-examples per class are selected, and we find that 1 projection step per class is sufficient to learn appropriate selection weights. After 1 gradient ascent and 1 projection step (i.e. weight vectors have been learned), we select the examples from P to include in the difficult support set. For each jth class, we sort the final selection weight vector ̄wj in descending order and extract the kj examples from P which have the highest weights.\n\nThe pseudo-code for FASTDIFFSEL is shown in Algorithm 1 and a detailed derivation of the steps for solving the optimization along with the associated hyperparameters can be found in Appendix A.2.\n\nComputational complexity. The key advantage of FASTDIFFSEL is that it does not require an iterative exhaustive search through the search pool to select a difficult support set. Consider selecting a task with N classes and k support examples per class from a dataset D′, where each class has d examples on average. The greedy algorithm in Agarwal et al. (2021) runs for r iterations and thus\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: FASTDIFFSEL extracts tasks with similar accuracy to those extracted by greedy search algorithms (Agarwal et al., 2021) but is at least 20x faster. We extract 50 5-way 5-shot tasks per sub-dataset using Hu et al. (2022) as the base model (ViT-S initialized with SSL DINO weights then meta-trained with ProtoNets on MD’s ilsvrc 2012) on an A5000 GPU (64GB RAM).\n\nhas a search time complexity of O(N.k.d.r) along with as many adaptation steps. In comparison, our algorithm removes the need for this exhaustive search and large number of adaptation steps, thus offering a significant speedup. In practice, we find that our algorithm offers speedups of at least 20x when compared to a greedy algorithm (see Fig 2 and Fig 6). 4 DIFFICULT SUPPORT SET EXTRACTION ON META-DATASET Since FASTDIFFSEL is general and highly efficient, we can use it to extract difficult tasks from any large-scale vision dataset. MD is one of the most widely-used benchmarks for few-shot classification, thus we primarily use it to validate the effectiveness of our extraction algorithm. We include further analyses on difficult tasks extracted from further datasets in Section 4.2.\n\nMETA-DATASET (MD) (Triantafillou et al., 2019) contains 10 image sub-datasets from a diverse set of domains. All the sub-datasets (except mscoco and traffic signs) are split into disjoint train/val/test classes, whereas mscoco and traffic sign are test only sub-datasets. During few-shot training, variable-way variable-shot tasks are randomly sampled from the train classes of each sub-dataset. During few-shot testing, 600 variable-way variable-shot tasks are sampled from the test classes of each sub-dataset with the average task classification accuracy reported for each sub-dataset. 4.1 TEST TASK SAMPLERS FOR META-DATASET\n\nWe compare tasks sampled from the test split of MD’s sub-datasets using 3 methods: (i) MD’s default sampler; (ii) the greedy approach of Agarwal et al. (2021) and (iii)FASTDIFFSEL(Ours). Note, only (ii) and (iii) can be used to deterministically sample difficult tasks:\n\nMD’s default sampler. The default task sampler in MD samples imbalanced tasks of variable-way, variable-shots. The shot, way and size of the query set depends on the size of the sub-dataset in MD. For more details, refer to Sec 3.2 in Triantafillou et al. (2019). We note, however, that the default sampler cannot deterministically sample difficult tasks.\n\nGreedy search-based sampler. We use Agarwal et al. (2021) to extract difficult tasks from MD. This approach works by iteratively replacing each example in a support set of fixed size with one from a given search pool such that the query loss is maximized. Because of the large computational cost and time associated with searching the pool each time an example is replaced, greedy approaches do not scale well to MD’s sampler where task ways can vary as large as 50 and shots as large as 100. We therefore only consider fixed-way, fixed-shot tasks when sampling via greedy search on MD.\n\nFASTDIFFSEL. We use FASTDIFFSEL to extract difficult tasks from MD. As the trained base model fθ, we choose a state-of-the-art method (Hu et al., 2022) on MD which employs a ViT-S feature extractor (Touvron et al., 2020) initialized with self-supervised DINO weights (Caron et al., 2021) pre-trained on ilsvrc 2012. The extractor is then further meta-trained on the ilsvrc 2012 split from MD. To compare against the above samplers, we consider two configurations:\n\n• Variable-way, variable-shot tasks. Here, we exactly match the variable way and shots per class of each task yielded by MD’s default sampler. Specifically, we use the default sampler to generate and save the shot and way for a fixed number of tasks, which we then feed into our algorithm to extract difficult tasks of equivalent specifications.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: We evaluate a wide range of top-performing methods over 200 variable-way, variable-shot difficult tasks per sub-dataset in HARD-MD and find a consistent 20-30% drop in performance compared to MD tasks regardless of feature extractor, pretraining method, and adaptation strategy. We evaluate on all MD sub-datasets except ilsvrc 2012. We include performance on each sub-dataset and their 95% confidence intervals in Appendix H.\n\n• Fixed-way, fixed-shot tasks. Here, we set the way and shots per class to be fixed for all tasks. While less challenging, it allows us (i) to compare our algorithm to greedily sampled difficult tasks; and (ii) to control for a task’s way being a source of difficulty.\n\n4.2 VALIDATION OF DIFFICULT META-DATASET TASKS\n\nHere we compare the efficacy of the above 3 methods and their extracted tasks across the 10 test sub-datasets in MD. FASTDIFFSEL vs. default sampler. When compared over 600 variable-way variable-shot tasks per sub-dataset, we find that our algorithm extract tasks which are more difficult on an average than MD’s default sampler, as shown in Fig 1. This is consistent across all 10 sub-datasets. We also show that these tasks are consistently difficult for a wide range of top-performing few-shot classification methods, as shown in Fig 3 (see Section 5.3.1 for details). For certain sub-datasets (e.g. quickdraw), the drop in classification accuracy can be as large as 50% when compared to tasks sampled from MD’s default sampler. FASTDIFFSEL vs. greedy sampler. We compare our algorithm to the greedy sampler over 50 5-way 5-shot tasks sampled per sub-dataset. For fairness, we run both methods for the same set of query sets, and report the average task accuracy and extraction time in Fig 2. Here, we see that the average task accuracy is comparably low for each sub-dataset but that our algorithm is almost 1-2 orders of magnitude faster. This speedup is most significant when the search pool is large (e.g. for ilsvrc 2012, quickdraw). Note that the greedy sampler takes between 20-200 minutes per task, highlighting its impracticality for larger search pools, support set sizes and numbers of tasks.\n\nIn all, FASTDIFFSEL is able to consistently extract difficult tasks on which state-of-the-art methods achieve low classification accuracy, while also offering a significant speedup in extraction time compared to existing task samplers. It can, therefore, readily be leveraged to further the study of failure modes in few-shot classification methods.\n\n5 STRESS TESTING WITH HARD-META-DATASET++\n\nFew-shot classification benchmarks like MD (Triantafillou et al., 2019) and VTAB+MD (Dumoulin et al., 2021b) are highly challenging but are not specifically geared to driving performance on difficult tasks. We leverage our extraction algorithm to fill this gap and introduce HARD-MD++, a new test-only benchmark of exclusively difficult few-shot classification tasks extracted from 4 large-scale vision datasets. We extensively stress test a wide range of top-performing methods on HARD-MD++, presenting novel insights on the robustness of current methods to difficult tasks.\n\n5.1 TEST DATASETS HARD-MD++ is composed of difficult tasks extracted from MD (Triantafillou et al., 2019), ORBIT (Massiceti et al., 2021), CURE-OR (Temel et al., 2018) and OBJECTNET (Barbu et al., 2019). We motivate the choice of MD by the fact that it is one of the most widely-used few-shot classification benchmarks. The remaining datasets are chosen because they specifically curate images with real-world variations and provide corresponding ‘quality’ annotations. They can, therefore, be lever-\n\n6\n\nPublished as a conference paper at ICLR 2023\n\naged to derive deeper insights into the properties of difficult tasks as we explore in Section 5.3.2. Together, these datasets cover a broad range of challenging and real-world tasks.\n\nFollowing Section 4, we use FASTDIFFSEL to extract 200 difficult tasks from each dataset. We use a trained base model fθ of ViT-S (Dosovitskiy et al., 2020) pre-trained using DINO (Caron et al., 2021) and further meta-trained on MD’s ilsvrc 2012 split. For MD, we extract variable-way variable-shot tasks to align with its existing evaluation protocol. For the remaining datasets, we extract fixed-way fixed-shot tasks to enable controlled analysis on task properties beyond their way and shot. Together, HARD-MD++ comes to a total of 2400 difficult test tasks extracted across these datasets. We include further implementations details in Appendix B.\n\nMETA-DATASET. We extract 200 variable-way variable-shot difficult tasks from each test subdatasets in MD. Note, we exclude the ilsvrc 2012 subset so as not to prevent the use of feature extractors pre-trained on it.\n\nORBIT contains 3822 videos of 486 objects captured by people who are blind on their mobile phones. Each frame is annotated with 7 quality issues including blur, framing, lighting and occlusion. We extract 200 fixed 5-way, 5-shot difficult tasks from the test split of ORBIT which cover 1198 videos of 158 objects.\n\nCURE-OR contains 1M test images of 100 objects captured under controlled viewpoint variations (e.g., front, back, side etc.) with corresponding annotations. We extract 200 fixed 5-way, 5-shot difficult tasks from CURE-OR.\n\nOBJECTNET contains 50K test images of 313 objects captured under rotation, background, and viewpoint variations. We extract 200 fixed-way fixed shot difficult tasks from OBJECTNET. 5.2 METRICS AND TRAINING\n\nMetrics. Model performance on HARD-MD++ should be reported as the average classification accuracy and 95% confidence interval over the difficult tasks per sub-dataset. This should be accompanied by performance on MD to provide a more complete characterization of model performance.\n\nTraining. We primarily advocate for cross-domain or strong generalization (Triantafillou, 2021), thus following Hu et al. (2022), any pre-trained checkpoint, algorithm and model architecture can be used when evaluating on HARD-MD++. This aligns with more recent few-shot learning practices which allow the wide range of publicly-available datasets and pre-trained models to be leveraged.\n\n5.3 RESULTS HARD-MD++ is a challenging benchmark of difficult tasks from a diverse range of datasets. We stress test a wide range of top-performing few-shot classification methods on HARD-MD++ to gauge their robustness to these tasks. We look first at MD and then ORBIT, CURE-OR, and OBJECTNET, primarily focusing on the robustness of different model architectures and pre-training strategies.\n\nState-of-the-art methods. Recent works (Tian et al., 2020; Hu et al., 2022; Dhillon et al., 2019b; Chen et al., 2019; Dumoulin et al., 2021a) show that transfer learning with a powerful feature extractor performs extremely well on few-shot classification tasks compared to previous meta-learning methods (Finn et al., 2017a; Requeima et al., 2019). We therefore consider a wide range of feature extractors and pre-training paradigms: ResNets (He et al., 2015), Vision Transformers (ViTs) (Dosovitskiy et al., 2020; Touvron et al., 2020), self-supervised variants of ResNets and ViTs (Caron et al., 2021; Bao et al., 2021) and the visual encoder of zero-shot models such as CLIP (Radford et al., 2021). We investigate two adaptation strategies when adapting to each difficult test task in HARD-MD++: (i) computing and classifying by mean class prototypes following Prototypical Networks (Snell et al., 2017) and (ii) fine-tuning the entire feature extractor using strong augmentations following (Hu et al., 2022). Further details are provided in Appendix E.\n\n5.3.1 RESULTS ON DIFFICULT TASKS FROM META-DATASET\n\nOur key finding is that state-of-the-art methods across the board consistently drop at least 20-25% in classification accuracy when adapting to difficult tasks in HARD-MD compared with tasks randomly sampled from MD, shown in Fig 3. Note, HARD-MD refers to the MD split in HARD-MD++.\n\nDetailed findings. We find that there is a consistent drop in classification accuracy on HARDMD across all state-of-the-art methods, regardless of adaptation strategy, feature extractor and pretraining paradigm (see Fig 3). This validates our proposed algorithm in its ability to extract generally challenging tasks, but also highlights the limitations of current approaches. In particular, approaches\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: We stress test a wide range of few-shot classifiers on difficult tasks from OBJECTNET, CURE-OR and ORBIT in HARD-MD++. We find that ViT-B (CLIP) outperforms all the other models by a large margin for CURE-OR and OBJECTNET, while being extremely competitive for ORBIT. Evaluation across 200 tasks per domain using Prototypical Networks as the adaptation strategy. More results with fine-tuning in Appendix H.\n\nthat employ a fine-tuning adaptation strategy see a more significant drop in accuracy compared to those employing a prototypical-style adaptation. For example, fine-tuning a ViT-B feature extractor pre-trained on ImageNet-21k (‘vit base 21k’ in Fig 3) (Dosovitskiy et al., 2020) on each test task leads to a drop of ∼30% in accuracy on HARD-MD while a prototypical-style adaptation leads to a drop of only ∼20% (note, this approach is strongest on original MD in our implementation). This suggests that although fully fine-tuning a model can significantly increase its accuracy on general tasks, it may not hold for specifically difficult tasks. On the other hand, we find the visual encoder of CLIP which also uses a ViT-B architecture (‘clip vit base’ in Fig 3) (Radford et al., 2021) to be more robust to difficult tasks. Despite under-performing on MD (ranks 6th), it displays strong performance on HARD-MD across both adaptation strategies, achieving the highest accuracy across all methods with fine-tuning. This trend is consistent for fixed-way, fixed-shot tasks on HARD-MD (see Appendix H). These early results suggest that large-scale vision-language pre-training may offer more robustness when generalising to new difficult tasks.\n\nEffect of meta-training on ilsvrc 2012. Hu et al. (2022) show that a method’s performance on MD can be improved by further meta-training its pre-trained feature extractor on MD’s ilsvrc 2012 split. We investigate whether this can also improve performance on HARD-MD. We further metatrain a subset of the methods on the ilsvrc 2012 split using Prototypical Networks (Snell et al., 2017) and compare the average task classification accuracy in Appendix H. We find that despite the further meta-training, the tested methods still display a consistent drop of 20-30% on HARD-MD. This suggests that novel algorithmic contributions, rather than further training, may be required to improve robustness to difficult tasks. 5.3.2 RESULTS ON DIFFICULT TASKS FROM CURE-OR, ORBIT AND OBJECTNET\n\nSimilar to Section 5.3.1, our key finding is that state-of-the-art methods consistently achieve low classification accuracy on difficult tasks from CURE-OR, ORBIT and OBJECTNET, shown in Fig 4. In particular, ViT-B pre-trained with CLIP (Radford et al., 2021) outperforms all other methods by a significant margin on both CURE-OR (by >4%) and OBJECTNET (by >15%). On ORBIT, ViT-B pretrained on ilsvrc 2012 (Deng et al., 2009) performs best though ViT-B with CLIP is still competitive.\n\nWe can go beyond task classification accuracy with CURE-OR and ORBIT by leveraging their perimage annotations to investigate the properties of difficult tasks. In particular, we use the object viewpoint annotations in CURE-OR, and all quality issue annotations (except object not present) in ORBIT. We compare difficult tasks extracted by our algorithm for each dataset with ‘easy’ tasks for a fixed 5-way 5-shot setting. We extract ‘easy’ tasks by instead minimizing the objective in Equation (1) for the same set of query sets. In Fig 5, we compare the composition of these difficult and easy tasks by the annotated attributes in their supports sets.\n\nCURE-OR. We randomly sample 200 query sets containing images only showing the object with a front or back viewpoint. The sampled query sets contain 10 examples per class. For each query set, we use our algorithm to extract an easy and a difficult support set and visualize the distribution of their annotations in Fig 5-(Left). Here, we see that the easy support sets have a higher proportion of images with front or back viewpoints relative to the total size of the support set, while the difficult support sets have a significantly higher proportion of images with side viewpoints.\n\nORBIT. Unlike CURE-OR, ORBIT was not collected in a controlled setting and hence its quality issue annotations have a long-tailed distribution. We therefore randomly sample 200 query sets, and for each, we use FASTDIFFSEL to extract an easy and a difficult support set. For each quality issue, we compute the difference (gap) in the proportion of images in the support versus query set with that\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Difficult few-shot tasks have a larger mismatch between the properties of their support and query sets compared to easy tasks. (Left): Difficult tasks in CURE-OR have support sets with the majority of images showing the object in side view when the query sets contain images of the object only in front/back view; (Right): Difficult tasks in ORBIT have a larger difference in the proportion of quality issues contained in their support versus query sets compared to easy tasks.\n\nparticular issue. In Fig 5-(Right), we report this gap averaged over 200 tasks. Here, we observe that difficult tasks have a larger gap (i.e. difference in support and query set) in quality issues than easier tasks. Together, these results suggest that a mismatch between image characteristics within a support versus query set (e.g., viewpoint, occlusion) can be a source of task difficulty. While these annotations do not cover all possible image characteristics, they provide a starting point to explore the robustness of few-shot classifiers to more qualitative distribution shifts. In Appendix C, we curate few-shot tasks leveraging image-level annotations from ORBIT, CURE-OR and MS-COCO – to show that tasks with distribution shifts with respect to natural characteristics have a lower accuracy. 6 RELATED WORKS\n\nDifficult tasks. Previous works (Agarwal et al., 2021; Arnold et al., 2021; Dhillon et al., 2019a) have shown that state-of-the-art few-shot classifiers generally display a wide range in performance when adapting to different test tasks. Agarwal et al. (2021) use this observation to develop a greedy search-based algorithm that can specifically extract difficult tasks for further study. They consider only meta-learning approaches and, due to the computational requirements of a greedy search, are limited to small-scale datasets including mini-ImageNet and CIFAR-FS. (Arnold et al., 2021) also study difficult tasks through a correlation-based analysis. We extend on both of these works by (i) proposing a scalable algorithm – FASTDIFFSEL that can extract difficult tasks from any largescale vision dataset, and (ii) conducting a deep empirical evaluation on the robustness of a broader range of meta-learning and transfer learning approaches on these difficult tasks. Potentially ideas from subset selection (Wei et al., 2015; Killamsetty et al., 2020) can be adapted for few-shot task extraction, but we leave it for future work.\n\nFew-shot classification benchmarks. MD (Triantafillou et al., 2019) and VTAB+MD (Dumoulin et al., 2021b) are two of the most challenging few-shot image classification benchmarks in the current literature. They cover a wide range of domains and primarily evaluate the ability of a few-shot classifier to generalise to novel object classes, datasets and domains. Other few-shot benchmarks have been introduced to specifically target adaptation to images with high real-world variation, including ORBIT (Massiceti et al., 2021) and cross-domain transfer beyond natural images, including BSCD-FSL (Guo et al., 2020). We note, however, that unlike HARD-MD++, none of these benchmarks specifically target difficult tasks for few-shot classification.\n\n7 CONCLUSION\n\nWe introduce a general and scalable algorithm – FASTDIFFSEL to extract difficult tasks for few-shot classification. We apply FASTDIFFSEL to 4 large-scale vision datasets: META-DATASET, ORBIT, CURE-OR, and OBJECTNET, and introduce HARD-MD++, a new test-only suite of 2400 difficult tasks from across these datasets. We stress test a wide range of top-performing few-shot methods on HARD-MD and demonstrate a consistent drop of 20-25% in classification accuracy compared to MD. We conduct additional quantitative analyses on CURE-OR and ORBIT which show that difficult tasks typically have a distribution shift in the characteristics between a support and query set (e.g. viewpoint, blur). We believe that the efficiency of FASTDIFFSEL along with HARD-MD++ can drive the study of failure modes in few-shot classification methods which is a under explored area of research.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n8 ACKNOWLEDGEMENTS\n\nThis project was supported in part by Meta grant 23010098, NSF CAREER AWARD 1942230, HR001119S0026 (GARD), ONR YIP award N00014-22-1-2271, Army Grant No. W911NF2120076 and the NSF award CCF2212458.\n\n9 REPRODUCIBILITY STATEMENT\n\nOur work primarily consists of (i) an algorithm – FASTDIFFSEL for extracting difficult few-shot tasks from any vision dataset. (ii) a few-shot testing benchmark called HARD-MD++. For (i), we provide the steps for reproducing the algorithm in Algorithm 1. The hyper-parameters for the algorithm can be found in Appendix A.2. For (ii), we provide all the relevant details in Appendix B and Section 5. The details for training and fine-tuning the various models that we stress test on HARD-MD++ can be referred in Appendix F. We will publically release the code and HARD-MD++ upon the acceptance of our manuscript.\n\nREFERENCES\n\nAlessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. CoRR, abs/1902.03545, 2019. URL http://arxiv.org/abs/1902.03545.\n\nMayank Agarwal, Mikhail Yurochkin, and Yuekai Sun. On sensitivity of meta-learning to support\n\ndata, 2021. URL https://arxiv.org/abs/2110.13953.\n\nS ́ebastien M. R. Arnold, Guneet S. Dhillon, Avinash Ravichandran, and Stefano Soatto. Uniform sampling over episode difficulty, 2021. URL https://arxiv.org/abs/2108.01662.\n\nHangbo Bao, Li Dong, and Furu Wei. Beit: BERT pre-training of image transformers. CoRR,\n\nabs/2106.08254, 2021. URL https://arxiv.org/abs/2106.08254.\n\nAndrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́eBuc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/ paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf.\n\nEtienne Bennequin, Myriam Tami, Antoine Toubhans, and C ́eline Hudelot. Few-shot image classification benchmarks are too far from reality: Build back better with semantic task sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 4767–4776, June 2022.\n\nLuca Bertinetto, Joao F. Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differentiable closed-form solvers. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HyxnZh0ct7.\n\nMalik Boudiaf, Imtiaz Masud Ziko, J ́erˆome Rony, Jos ́e Dolz, Pablo Piantanida, and Ismail Ben Ayed. Transductive information maximization for few-shot learning. CoRR, abs/2008.11297, 2020. URL https://arxiv.org/abs/2008.11297.\n\nStephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n\nJohn Bronskill, Daniela Massiceti, Massimiliano Patacchiola, Katja Hofmann, Sebastian Nowozin, and Richard E. Turner. Memory efficient meta-learning with large images, 2021. URL https: //arxiv.org/abs/2107.01105.\n\nE.J. Candes and T. Tao. Decoding by linear programming.\n\nIEEE Transactions on Information\n\nTheory, 51(12):4203–4215, 2005. doi: 10.1109/TIT.2005.858979.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nEmmanuel Candes, Justin Romberg, and Terence Tao. Stable signal recovery from incomplete and\n\ninaccurate measurements, 2005. URL https://arxiv.org/abs/math/0503066.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. CoRR, abs/2104.14294, 2021. URL https://arxiv.org/abs/2104.14294.\n\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classification. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HkxLXnAcFQ.\n\nYinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, and Trevor Darrell. A new meta-baseline for few-shot learning. CoRR, abs/2003.04390, 2020. URL https://arxiv.org/abs/2003. 04390.\n\nArkabandhu Chowdhury, Mingchao Jiang, and Chris Jermaine. Few-shot image classification: Just use a library of pre-trained feature extractors and a simple classifier. CoRR, abs/2101.00562, 2021. URL https://arxiv.org/abs/2101.00562.\n\nGeorge Corliss. Which root does the bisection algorithm find? SIAM Review, 19(2):325–327, 1977.\n\ndoi: 10.1137/1019044. URL https://doi.org/10.1137/1019044.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.\n\nGuneet S. Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classification. 2019a. doi: 10.48550/ARXIV.1909.02729. URL https:// arxiv.org/abs/1909.02729.\n\nGuneet S. Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classification. CoRR, abs/1909.02729, 2019b. URL http://arxiv.org/ abs/1909.02729.\n\nD. L. Donoho. Compressed sensing. IEEE Trans. Inf. Theor., 52(4):1289–1306, apr 2006. ISSN 0018-9448. doi: 10.1109/TIT.2006.871582. URL https://doi.org/10.1109/TIT. 2006.871582.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs/2010.11929, 2020. URL https://arxiv.org/abs/2010.11929.\n\nVincent Dumoulin, Neil Houlsby, Utku Evci, Xiaohua Zhai, Ross Goroshin, Sylvain Gelly, and Hugo Larochelle. Comparing transfer and meta learning approaches on a unified few-shot classification benchmark, 2021a. URL https://arxiv.org/abs/2104.02638.\n\nVincent Dumoulin, Neil Houlsby, Utku Evci, Xiaohua Zhai, Ross Goroshin, Sylvain Gelly, and Hugo Larochelle. A unified few-shot classification benchmark to compare transfer and meta learning approaches. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021b.\n\nLi Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(4):594–611, 2006. doi: 10.1109/TPAMI.2006.79.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. CoRR, abs/1703.03400, 2017a. URL http://arxiv.org/abs/1703. 03400.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1126–1135. PMLR, 06–11 Aug 2017b. URL https://proceedings.mlr.press/v70/ finn17a.html.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nMinghao Fu, Yun-Hao Cao, and Jianxin Wu. Worst case matters for few-shot recognition, 2022.\n\nURL https://arxiv.org/abs/2203.06574.\n\nYunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella, John R Smith, Kate Saenko, Tajana Rosing, and Rogerio Feris. A broader study of cross-domain few-shot learning. ECCV, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. CoRR, abs/2106.09685, 2021. URL https://arxiv.org/abs/2106.09685.\n\nShell Xu Hu, Da Li, Jan St ̈uhmer, Minyoung Kim, and Timothy M. Hospedales. Pushing the limits of simple pipelines for few-shot learning: External data and fine-tuning make a difference, 2022. URL https://arxiv.org/abs/2204.07305.\n\nYuqing Hu, Vincent Gripon, and St ́ephane Pateux. Leveraging the feature distribution in transferbased few-shot learning. CoRR, abs/2006.03806, 2020. URL https://arxiv.org/abs/ 2006.03806.\n\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning, 2022. URL https://arxiv.org/abs/2203. 12119.\n\nKrishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister: Generalization based data subset selection for efficient and robust learning. 2020. doi: 10.48550/ ARXIV.2012.10630. URL https://arxiv.org/abs/2012.10630.\n\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Large scale learning of general visual representations for transfer. CoRR, abs/1912.11370, 2019. URL http://arxiv.org/abs/1912.11370.\n\nBrenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning\n\nof simple visual concepts. Cognitive Science, 33, 2011.\n\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with\n\ndifferentiable convex optimization. In CVPR, 2019.\n\nWei-Hong Li, Xialei Liu, and Hakan Bilen. Cross-domain few-shot learning with task-specific\n\nadapters, 2021. URL https://arxiv.org/abs/2107.00358.\n\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ́ar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/ 1405.0312.\n\nDaniela Massiceti, Lida Theodorou, Luisa Zintgraf, Matthew Tobias Harris, Simone Stumpf, Cecily Morrison, Edward Cutrell, and Katja Hofmann. Orbit: A real-world few-shot dataset for teachable object recognition collected from people who are blind or low vision, 2021. URL https://city.figshare.com/articles/dataset/ORBIT_A_real-world_ few-shot_dataset_for_teachable_object_recognition_collected_ from_people_who_are_blind_or_low_vision/14294597.\n\nB. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24(2):227–234, 1995. doi: 10.1137/S0097539792240406. URL https://doi.org/10. 1137/S0097539792240406.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. CoRR, abs/2103.00020, 2021. URL https://arxiv.org/abs/2103.00020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMengye Ren, Michael L. Iuzzolino, Michael C. Mozer, and Richard S. Zemel. Wandering within a world: Online contextualized few-shot learning. CoRR, abs/2007.04546, 2020. URL https: //arxiv.org/abs/2007.04546.\n\nJames Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E. Turner. Fast and flexible multi-task classification using conditional neural adaptive processes. 2019. doi: 10.48550/ARXIV.1906.07697. URL https://arxiv.org/abs/1906.07697.\n\nJake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning,\n\n2017. URL https://arxiv.org/abs/1703.05175.\n\nMegan Stanley, John F Bronskill, Krzysztof Maziarz, Hubert Misztela, Jessica Lanini, Marwin FS-mol: A few-shot learning dataset Segler, Nadine Schneider, and Marc Brockschmidt. In Thirty-fifth Conference on Neural Information Processing Systems Datasets of molecules. and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id= 701FtuyLlAd.\n\nD. Temel, J. Lee, and G. AlRegib. Cure-or: Challenging unreal and real environments for object recognition. In 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), 2018.\n\nYonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, and Phillip Isola. Rethinking few-shot image classification: a good embedding is all you need? CoRR, abs/2003.11539, 2020. URL https://arxiv.org/abs/2003.11539.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers & distillation through attention. CoRR, abs/2012.12877, 2020. URL https://arxiv.org/abs/2012.12877.\n\n2021.\n\nEleni Triantafillou.\n\nTowards Strong Generalization from Few Examples.\n\nPhD theURL https://www.proquest.com/dissertations-theses/\n\nsis, towards-strong-generalization-few-examples/docview/2610926400/ se-2. Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2022-01-14.\n\nEleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset of datasets for learning to learn from few examples. CoRR, abs/1903.03096, 2019. URL http://arxiv.org/abs/1903.03096.\n\nOriol Vinyals, Charles Blundell, Timothy P. Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. CoRR, abs/1606.04080, 2016. URL http://arxiv. org/abs/1606.04080.\n\nKai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1954–1963, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ wei15.html.\n\nChengming Xu, Siqian Yang, Yabiao Wang, Zhanxiong Wang, Yanwei Fu, and Xiangyang Xue. Exploring efficient few-shot adaptation for vision transformers. Transactions of Machine Learning Research, 2022. URL https://openreview.net/forum?id=n3qLz4eL1l.\n\nHan-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-shot learning via embedding adaptation with set-to-set functions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8808–8817, 2020.\n\n13\n\n(5)\n\n(6)\n\nPublished as a conference paper at ICLR 2023\n\nA FASTDIFFSEL: SUPPORT SET EXTRACTION ALGORITHM\n\nA.1 STEPS FOR SOLVING THE PROJECTION STEP\n\nIn this section, we provide details on how to solve the projection step in Equation (3). The projection step is solved separately for each of the jth class, where j ∈ [1, N ]. ˆwj is the selection weight vector for the jth class obtained after a step of gradient ascent on Equation (1). The dual form of Equation (2) can be expressed via Lagrange multipliers as the following:\n\n ̄wj = arg max\n\nλj ≥0\n\nmin wj\n\n1 2\n(cid:124)\n\n∥wj − ˆwj∥2\n\n2 + λj(∥wj∥1 − kj) (cid:125)\n\n(cid:123)(cid:122) g(λj ,wj )\n\n(4)\n\nWe solve Equation (4) in two steps: (i) First, we solve minwj g(λj, wj) via proximal operators; (ii) Then, we obtain the optimal values of the dual parameters λj.\n\nKKT optimality conditions (due to stationarity) states that ∇wj g(λj, wj) = 0. However, note that g(λj, wj) is a combination of a smooth function and a non-smooth function which can be solved by proximal operators. Considering wj ∈ Rn, the KKT optimality condition can be stated as the following:\n\n∇wj\n\n1 2\n\n∥wj − ˆwj∥2 + ∇wj λj(∥wj∥1 − kj) = 0\n\nThe value in the ith index in wj, which is wi\n\nj can be obtained through:\n\ni)2\n\n1 2\n\n∂(wi\n\nj − ˆwj ∂wi j\n\n+ λj\n\n= 0\n\n∂|wi j| ∂wi j\nj − ˆwi i > λj. Similarly, when wi\n\nj > 0, then the derivative in Equation (6) is wi j + λj. Therefore Equation (6) can be i −λj, which holds true for ˆwj j +λj j ∈ [−λj, λj], the minimizer is at the only point of differentiability for which\n\nj < 0, ̄wi\n\nj = ˆwi\n\nIf wi expressed as: ̄wi is the minimizer. For ˆwi wi\n\nj = ˆwj\n\nj = 0. This operation is called soft-thresholding and can be expressed as:\n\n ̄wi\n\nj = Proxλ∗∥.∥1( ˆwi\n\nj) = sign( ˆwi\n\nj) max(| ˆwj\n\ni| − λ∗\n\nj , 0)\n\n(7)\n\nThus, ̄wj = Proxλ∗∥.∥1 ( ˆwj) = [Proxλ∗∥.∥1( ˆw1 the value of the dual parameter λ∗\n\nj ), ...., Proxλ∗∥.∥1( ˆwn\n\nj )]. The next step is to compute\n\nj . We then compute the derivative g′(λj, ̄wj) as the following:\n\ng′(λj, ̄wj) = ∥Proxλ∗∥.∥1( ˆwj)∥1 − kj\n\n=\n\nn (cid:88)\n\ni=1\n\n(| ˆwi\n\nj| − λj)+ − kj\n\n(8)\n\n(9)\n\nWe solve Equation (9) by the root finding method in (Corliss, 1977), since the optimal λ∗ [0, ∥ ˆwj∥∞]. The upper bound ∥ ˆwj∥∞ ensures that g′(λj, ̄wj) does not become negative.\n\nj ∈\n\nA.2 HYPERPARAMETERS OF FASTDIFFSEL\n\nEmpirically, we perform a grid-search for the learning rate α ∈ [0.01, 500] with a step size of 10. We specifically find that a high learning rate with only one gradient ascent and projection step suffices to obtain difficult tasks. In our experiments, we use a learning rate α = 200, as we find this value to result in the extraction of the most difficult tasks.\n\nA.3 EMPIRICAL RUNNING TIMES\n\nIn Fig. (6), we plot the average running time of our framework to extract a single difficult support set across all the domains in META-DATASET. In practice, we find a large speedup (at least 20x) using our framework over (Agarwal et al., 2021) for support set extraction. The main advantage of our framework is the elimination of the iterative search in (Agarwal et al., 2021), which is computionally expensive. The speedup is crucial for scalability of difficult task extraction to large-scale vision datasets such as MD.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: Average running times of our framework (In practice): For extracting a difficult task on an average, we find a speedup of at least 20x (depending on the dataset) when compared to the greedy method in (Agarwal et al., 2021). Tasks are fixed-way, fixed-shot tasks where way=5, shot = 5. On hardware with a5000 GPU with 64GB RAM.\n\nB DESIGN CHOICES FOR DATASETS\n\nChoice for META-DATASET. META-DATASET(Triantafillou et al., 2019) is arguably one of the most challenging few-shot learning benchmarks comprising of 10 different sub-datasets spanning different domains. We primarily build HARD-MD++ on top of MD, to enable fast adoption of our benchmark by the few-shot learning community. We envision HARD-MD++ to be an add-on to MD, which the community uses to report performance on difficult tasks.\n\nChoice for CURE-OR and ORBIT. MD though a challenging few-shot learning benchmark, does not come with image-level annotations which can be leveraged to understand the properties of difficult tasks. Understanding the properties of difficult tasks is crucial, as they can further guide the development of future algorithmic methods specifically for tackling difficult few-shot tasks. We ground our choice of annotated datasets in CURE-OR(Temel et al., 2018) and ORBIT(Massiceti et al., 2021), as they reflect the variations present in real-world images. While ORBIT is an existing few-shot learning benchmmark comprising of real-world videos of everyday objects, CURE-OR is primarily an object recognition dataset. In CURE-OR, each image has objects in 5 different viewpoints, which are curated in a controlled environment. ORBIT consists of 7 unique annotations about various quality issues for each frame in the video. These annotations are blur, viewpoint, framing, occlusion, overexposure, underexposure and object not present. In our experiments, we specifically leverage all the quality annotations except object not present. These issues are reflective of a subset of properties which can arise in real-world images. We use these annotations from ORBIT and CURE-OR to understand the properties of difficult few-shot tasks. In Section 5.3.2, we provide some early analyses in this regard, where we find that difficult tasks have a distribution shift between the support and query set, with respect to various image-level properties such as blur, viewpoint etc. Methodological evaluation on CURE-OR and ORBIT, will enable researchers to understand the specific dimensions of image characteristics in which in which robustness has improved.\n\nChoice for OBJECTNET. A commonly used high-variation test-set in supervised deep learning is OBJECTNET, which is often used for testing the out-of-domain generalization capabilities of models. In particular OBJECTNET comprises of images of objects in various viewpoints, backgrounds and rotations which results in high variation within a class of objects. However, we note that these annotations are not made public by the (Barbu et al., 2019). Considering the ability of our algorithm to extract tasks with a distribution shift between support and query with respect to various image characteristics in high variation datasets such as ORBIT and CURE-OR, we extract difficult tasks from OBJECTNET and add them to our benchmark HARD-MD++.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nAs the next step, the difficult tasks from the MD split in HARD-MD++ can be annotated similar to CURE-OR or ORBIT to furthen our understanding of the properties of difficult few-shot tasks.\n\nC WHAT CAN MAKE A FEW-SHOT TASK DIFFICULT?\n\nPrior works (Agarwal et al., 2021; Massiceti et al., 2021; Dhillon et al., 2019a; Arnold et al., 2021) have shown that the composition of the support or query set is crucial in determining the difficulty level of a task. This difficulty level is usually characterized by the loss or accuracy on the query set. While an intuitive characterization, it gives limited insights into why the given few-shot task was difficult. In this section, we aim to shed light on this question by specifically focusing on the role that natural image characteristics (e.g., quality issues, multiple objects) play in making a task difficult. These natural characteristics can arise in the wild, hence we specifically focus on them. We note that there can be other factors controlling task difficulty such as fine-grained classes, however it is outside the scope of this paper (see Table 1 for more details). We hypothesize that tasks which are difficult for a given few-shot learner have a mismatch (distribution shift) between the support and the query set with respect to certain image-level characteristics. To validate our hypothesis, we curate annotated few-shot tasks where there is a mismatch in the characteristics between the support and query set. We leverage three vision datasets which are annotated with various image-level attributes to curate the few-shot tasks: (i) ORBIT (Massiceti et al., 2021) which contains annotations designating if the frame is cluttered or not as well annotations of various quality issues such as blur, occlusion, overexposure, underexposure, viewpoint and framing. (ii) CURE-OR (Temel et al., 2018) consists of images of various objects in different viewpoints. (iii) MSCOCO (Lin et al., 2014) consists of annotations about the number of objects in an image.\n\nFigure 7: Curated tasks with a distribution shift between their support and query sets with respect to image-level attributes have a lower accuracy on average. Across 600 tasks (5-way, 5-shot) per dataset on ViT-S(DINO). Task curation details in Appendix D.\n\nSampling Protocol. For each of the annotated datasets, we curate difficult few-shot tasks in two steps: (i) We sample a query set Q which contains a certain set of attributes; (ii) For curating tasks without shift, we curate a support set S containing similar attributes as Q. For tasks with shift, the curated support set contains attributes different from those in Q. Further task curation details are in Appendix D.\n\nResults. From Fig 7, we find that tasks that have a distribution shift between the attributes of their support versus query set have lower query accuracy on average compared to tasks that do not have this shift. We also find a strong correlation between the query accuracy and the inverse of the query\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nloss with a Spearman correlation of 0.90 (See Fig 8). This shows that tasks with a high query loss (low accuracy) is representative of distribution shifts between the support and query with respect to natural image characteristics.\n\nC.1 CORRELATION PLOTS\n\nFigure 8: Spearman correlation of 0.90 between the query accuracy and inverse of the query loss for tasks sampled from CURE-OR and ORBIT. 600 tasks are sampled from each of the dataset.\n\nD HEURISTIC FEW-SHOT TASK CURATION DETAILS\n\nIn Fig 7, we heuristically curate two types of tasks leveraging attribute annotated vision datasets such as CURE-OR, ORBIT and MS-COCO: (i) Tasks with shift between the support and query set. (ii) Tasks without shift between the support and query set. Below we provide details on how these tasks are curated:\n\nD.1 MS-COCO\n\nTasks with Shift. We sample N classes (where N = 5) and randomly select a query set Q from the pool of images having only one object. The support set S is randomly selected from the pool of examples containing more than 3 objects. Each task is a 5-way, 5-shot task, where there is a large mismatch between the number of objects present in the support vs. query.\n\nTasks without Shift. We sample N classes (where N = 5) and randomly select a query set Q from the pool of images having only one object. The support set S is randomly selected from the pool of examples containing only one or two objects. Each task is a 5-way, 5-shot task, where there is no significant mismatch between the number of objects present in the support vs. query.\n\nD.2 ORBIT\n\nD.2.1 ORBIT-CLUTTER\n\nTasks with Shift. We first randomly sample a user and N classes (where N =5). The query set Q is then selected from the clean frames. The support set corresponding the the same user and N classes is then selected randomly from the cluttered frames. This sampling procedure ensures that there is a shift between the support and query with respect to the number of objects in the frame.\n\nTasks without Shift. We first randomly sample a user and N classes (where N =5). The query set Q is then selected from the clean frames. The support set corresponding the the same user and N classes is then also selected randomly from the clean frames. This sampling procedure ensures that there is no shift between the support and query with respect to the number of objects in the frame.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nD.2.2 ORBIT-QUALITY ISSUES\n\nTasks with Shift. We first randomly sample a user and N classes (N = 5) corresponding to the user. We then select a query set Q which contains images without any quality issues. To create the support set, we create a subset by selecting frames which have one or more quality issues (i.e. blur==True OR occlusion==True OR overexposure==True OR underexposure==True OR viewpoint==True OR framing==True). Then, we randomly sample a support set S from this subset which contains frames with quality issues. This design choice ensures that the tasks contain a significant shift between the support and query with respect to various quality issues.\n\nTasks without Shift. To create tasks without any shift, we use the default sampler from ORBIT, but modify it to select fixed-way, fixed-shot tasks with the query set Q and support set S. Because the tasks are sampled randomly, a majority of the tasks will only have minor shifts with respect to the various quality issues.\n\nD.3 CURE-OR\n\nTasks with Shift. We sample N classes (where N = 5) and randomly select a query set Q which has objects in either (i) front or back viewpoints; (ii) side-viewpoints and (iii) top viewpoints. If (i) is selected, then the support set S is composed of images only with side-viewpoints. If (ii) is selected, then the support set S is composed of images with only the front or back viewpoints. If (iii) is selected, then the support set S is composed of images with front, back or the side viewpoints. This ensures that the tasks have a shift between the support and query with respect to the viewpoint attribute.\n\nTasks without Shift. We sample N classes (where N = 5) and randomly select a query set Q which has objects in either (i) front or back viewpoints; (ii) side-viewpoints and (iii) top viewpoints.If (i) is selected, then the support set S is composed of images only with the front or back viewpoints. If (ii) is selected, then the support set S is composed of images only with the side viewpoints. Similarly, if (iii) is selected, the support set is composed of images containing only the top viewpoint of the object. This design decision ensures that tasks do not have any shift between the support and query with respect to the viewpoint attribute.\n\nWe note that these rules or heuristics are not exhaustive to create tasks which have a shift between the support and query set with respect to various attributes. However as shown in Fig 7, these rules are able to sample tasks which have a lower accuracy than tasks which do not have any shift between the respective support and query sets. Moreover, designing such heuristic rules is not straightforward and requires a significant amount of human effort. FASTDIFFSEL effectively addresses this problem and provides a way to automatically extract difficult tasks, where the support and query set have shifts with respect to various factors.\n\nE MODEL CHOICES\n\nIn the past few years, a tremendous amount of research has been initiated in developing few-shot learning algorithms mostly spanning meta-learning (Snell et al., 2017; Lee et al., 2019; Bertinetto et al., 2019; Finn et al., 2017a). However recent works (Chen et al., 2020; Tian et al., 2020) have shown that a well-trained feature extracted via supervised learning is competitive and often surpasses state-of-the-art meta-learning algorithms. In (Hu et al., 2022), the authors show that upgrading the feature extractor and large-scale pre-training improves few-shot learning performance on challenging benchmarks such as MD. In our paper, we motivate our model choices from (Hu et al., 2022) and focus on a range of pretrained models which are adapted via Prototypical Networks (Snell et al., 2017) or full-model fine-tuning. Below we provide the architectural details for the models tested on HARD-MD++:\n\n• resnet18-101. We use 4 different pre-trained ResNet (He et al., 2015) architectures (resnet-\n\n18,34,50,101) where the pre-training corpus is ilsvrc 2012 (Deng et al., 2009).\n\n• dino small patch16. We use ViT-S/16 pre-trained with DINO (Caron et al., 2021) on\n\nilsvrc 2012 (Deng et al., 2009).\n\n• dino base patch16.\n\nViT-B/16 pre-trained with DINO (Caron et al., 2021) on\n\nilsvrc 2012 (Deng et al., 2009).\n\n18\n\nPublished as a conference paper at ICLR 2023\n\n• dino base 21k. ViT-B/16 pre-trained on ImageNet-21k (Deng et al., 2009). • vit small 1k. ViT-S/16 pre-trained on ilsvrc 2012 (Deng et al., 2009). • vit base 1k. ViT-B/16 pre-trained on ilsvrc 2012 (Deng et al., 2009). • beit base 21k. ViT-B/16 pre-trained using BeiT (Bao et al., 2021) on ImageNet-21k (Deng\n\net al., 2009). • dino resnet50.\n\nResNet-50 pre-trained with DINO (Caron et al., 2021) on\n\nilsvrc 2012 (Deng et al., 2009).\n\n• clip resnet50. Visual encoder from CLIP (Radford et al., 2021) pretrained on YFCC100M\n\n- ResNet-50.\n\n• clip vit base. Visual encoder from CLIP (Radford et al., 2021) pretrained on YFCC100M\n\n- ViT-B/16.\n\nWe believe that these model choices cover a wide-range of pre-training paradigms including both self-supervised and supervised pre-training.\n\nF FINETUNING DETAILS\n\nFine-tuning during adaptation. We adopt the fine-tuning recipe from (Hu et al., 2022) for finetuning the pre-trained checkpoints. We specifically fine-tune the entire backbone for all the models, rather than partial adaptation. The fine-tuning algorithm from (Hu et al., 2022) has three hyperparameters: (i) learning rate; (ii) number of fine-tuning steps and (iii) probability of switching on data-augmentation for the support set. In our experiments, we set the number of fine-tuning steps as 50 and the probability of switching on data-augmentation for the support set as 0.9. We select the optimal learning rate from {0.0001, 0.001, 0.01} with the help of a separate validation set which comprises of 5 tasks per sub-dataset in HARD-MD++.\n\nProtoNets during adaptation. In this adaptation strategy, the support set is first used to compute class-specific prototypes. Then each query example is assigned the class, based on its closest distance to the class-prototypes. During adaptation, no parameters are learnt which makes it extremely fast during adaptation.\n\nFor both the types of adaptation on MD and HARD-MD++, images are resized to 128x128 following (Hu et al., 2022) to ensure fairness in comparison. We note that increasing the resolution to 224x224 can slightly improve performance on MD (Bronskill et al., 2021) with LITE training strategies however we leave it for future exploration.\n\nG META-TRAINING DETAILS\n\nThe model used for extracting the difficult support set is meta-trained using Prototypical Networks (Snell et al., 2017) from its pre-trained initialization on the ilsvrc 2012 split from MD. We train this model using distributed training on 8 a6000 GPUs. We run the training for 50 epochs, where in each epoch 2000 episodes from ilsvrc 2012 are sampled. In total, we meta-train on 100k episodes variable-way, variable-shot episodes from ilsvrc 2012. We also train the models for 100 epochs, but observe that the best model gets selected before the 50th epoch, hence resort to training for 50 epochs across. We train using the SGD optimizer with a momentum of 0.9. We use a learning rate of 5e-4 with cosine scheduler for our experiments. During training, following (Hu et al., 2022), we resize the images to 128x128.\n\nH MORE STRESS-TESTING RESULTS\n\nH.1 FIXED-WAY, FIXED-SHOT RESULTS\n\nUsing our algorithm we also extract fixed 5-way, 5-shot tasks from MD. We evaluate all the 13 models on these fixed tasks from the original MD as well as HARD-MD. Overall, across both the adaptation strategies we find a drop of ∼30% in performance on HARD-MD. Similar to the results for variable-way, variable-shot sampling, we find that ViT-B with CLIP initialization has a strong few-shot performance. In particular, ViT-B with CLIP has the best performance on HARD-MD when it is fully finetuned during adaptation, while ranks 2nd when Prototypical Networks is used during\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nEvaluation of fixed 5-way,5-shot tasks on META-DATASET and HARD-METAFigure 9: DATASET): We find a consistent drop in performance across a fixed-way, fixed-shot setting with ViT-B(CLIP) performing the best with fine-tuning, while it ranks 2nd with Prototypical Networks.\n\nFigure 10: Evaluation of fixed 5-way, 5-shot tasks on from OBJECTNET, CURE-OR and ORBIT with full-finetuning: With ORBIT, we find that vision transformers (ViT-S and ViT-B) pre-trained on ilsvrc 2012 to be performing the best. For OBJECTNET and CURE-OR, we find that CLIP pretraining to be still performing the best amongst all the other models.\n\nadaptation. However note that ViT-B with CLIP lags behind other models on fixed-way, fixed-shot tasks sampled from original MD.\n\nThe combined results with Section 4 imply that vision-language pre-training can offer more robustness to difficult tasks, than tasks sampled randomly. A thorough and more fine-grained study of vision-language models on HARD-MD can be a direction of future work.\n\nH.2 FINETUNING RESULTS FOR ORBIT, CURE-OR AND OBJECTNET\n\nThe full finetuning results for difficult tasks from OBJECTNET, CURE-OR and ORBIT can be referred at Fig 10. In particular for OBJECTNET, we find ViT-B with CLIP to be performing the best, while for CURE-OR, we find ResNet-50 with CLIP initialization to be performing the best. For ORBIT though, we find that vision transformers (ViT-S or ViT-B) pre-trained on ilsvrc 2012 to have the strongest performance.\n\nH.3 BREAKDOWN OF RESULTS FOR HARD-META-DATASET\n\nIn this section, we provide breakdown of the results on MD and HARD-MD at the sub-dataset level for both the adaptation strategies. In particular we report the average task accuracy along with the 95% confidence interval.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nH.3.1 ADAPTATION: PROTOTYPICAL NETWORKS\n\nFigure 11:\n\nilsvrc 2012\n\nFigure 12: MSCOCO\n\nFigure 13: Aircraft 21\n\nPublished as a conference paper at ICLR 2023\n\nFigure 14: Cu-birds\n\nFigure 15: Traffic-Sign\n\n22 Figure 16: Fungi\n\nPublished as a conference paper at ICLR 2023\n\nFigure 17: DTD\n\nFigure 18: Quickdraw\n\n23 Figure 19: Omniglot\n\nPublished as a conference paper at ICLR 2023\n\nFigure 20: VGG-Flower\n\nH.3.2 ADAPTATION: FULL FINETUNING\n\nFigure 21:\n\nilsvrc 2012\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nFigure 22: MSCOCO\n\nFigure 23: Aircraft\n\nFigure 24: Cu-birds 25\n\nPublished as a conference paper at ICLR 2023\n\nFigure 25: Traffic-Signs\n\nFigure 26: Fungi\n\n26 Figure 27: DTD\n\nPublished as a conference paper at ICLR 2023\n\nFigure 28: Quickdraw\n\nFigure 29: Omniglot\n\nFigure 30: VGG-Flower\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nH.4\n\nIMPACT OF META-TRAINING ON ILSVRC 2012\n\nWe meta-train a subset of the 13 models on the ilsvrc 2012 split from META-DATASET. In particular, we meta-train dino small patch16, dino base patch16, dino resnet50 and vit base 1k from their pretrained initialization. Prior work (Hu et al., 2022; Chowdhury et al., 2021) has shown that fine-tuning from a strong pre-trained checkpoint during adaptation is more beneficial than meta-training on ilsvrc 2012, therefore we primarily focus on the full fine-tuning experiments (see Section 4) in our paper. However, to verify if meta-training on ilsvrc 2012 leads to any significant improvements, we evaluate 4 meta-trained models on HARD-MD. From Fig 31, we find that the ilsvrc 2012 metatrained models still suffer on HARD-MD and incur a ∼20-30% performance drop.\n\nFigure 31: Effectiveness of Meta-Training on the ilsvrc 2012 split from META-DATASET. We observe a ∼20-30% drop in performance on the MD split from HARD-MD.\n\nI VISUALIZATION OF DIFFICULT TASKS\n\nIn this section, we provide visualizations of some of the difficult tasks which are extracted by our algorithm. We provide only a few visualizations for representative purposes and believe a large-scale qualitative study of the difficult tasks in HARD-META-DATASET++ is a future course of study.\n\nFigure 32: Difficult Task from Traffic-Sign: This extracted task from traffic sign has an accuracy of 15%. We find that this task has a distribution shift between the support and query with respect to various lighting conditions. For e.g., Class 5 in the extracted support set has almost no light when compared to the images from Class 5 in the query set.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nFigure 33: Difficult Task from aircraft: This extracted task from aircraft has an accuracy of 21%. We find that this task has a distribution shift between the support and query with respect to background. For e.g., Class 5 in the extracted support set has all the airplanes in the sky, whereas class 5 in the query set has all the airplanes on the ground. Similar for Class 1 and Class 2, where there is a distribution shift with respect to background characteristics.\n\nFigure 34: Difficult Task from fungi: This extracted task from fungi has an accuracy of 23%. For Class 4, our algorithm extracts images which consistently have the fungi in a background of vegetation, whereas in the query set only one image in Class 4 has a background of vegetation.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nFigure 35: Difficult Task from ilsvrc 2012: Although we do not include ilsvrc 2012 in HARDMD++, we present a difficult task which has an accuracy of 20%. Class 5 has a clear distribution shift with respect to viewpoint and background. There are multiple objects in the extracted support set of Class 1, whereas Class 1 in the query set only contains images of single objects.\n\nThese are some randomly picked tasks from the pool of difficult tasks extracted by our algorithm, where we find that difficult tasks have a distribution shift between the support and query with respect to various characteristics such as lighting, background, viewpoint. Annotation of the difficult tasks in HARD-MD++ is the next step to understand failure modes of few-shot learning methods on a deeper level.\n\nJ\n\nINTEGRATING OUR FRAMEWORK WITH FINE-GRAINED CLASS SAMPLERS\n\nOur framework does not put any explicit constraints on the class-sampling procedure. As shown in Equation (1), our framework extracts difficult tasks where there is a weak relationship between the support and query. In all our experiments, we follow the same class sampling protocol from MD (Triantafillou et al., 2019). Note that the ilsvrc 2012 split from MD uses a fine-grained class-sampling protocol and our framework extracts more difficult tasks than the ones sampled using fine-grained class samplers for this setting (see Fig 30). Therefore, our framework is complementary to any fine-grained class-samplers. Recently (Bennequin et al., 2022) introduce a new fine-grained classsampler for tieredImageNet consisting of tasks with classes which are semantically closer. The sampler can select tasks with different levels of coarsity, thereby providing control over the difficulty of the meta-test set. To understand the complementary nature of our framework, we integrate the class-sampling protocol from (Bennequin et al., 2022) with our optimization steps. We pre-train a ResNet-12 on the training split from tieredImageNet consisting of 351 classes following the superised pre-training protocol in (Bennequin et al., 2022). We then use this base model along with our framework to extract difficult tasks from the test-set of tiered-ImageNet.\n\nFine-grained protocol Ours + Fine-grained protocol\n\n1st. Qtl 42.01 24.3\n\n2nd. Qtl 51.2 29.3\n\n3rd. Qtl 56.7 31.4\n\n4th. Qtl 62.7 33.1\n\nTable 1: Our framework is complementary to the fine-grained sampling protocol from (Bennequin et al., 2022). We find that our framework when used with the fine-grained sampling protocol extracts more difficult tasks, than fine-grained tasks itself, highlighting its complementary nature. Across 5000 tasks (5-way, 5-shot). Different quartiles represent different levels of task coarsity (smaller the coarsity, higher is the difficulty).\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nK EFFECT OF THE NUMBER OF OPTIMIZATION STEPS\n\nIn our framework, we take one step of gradient ascent and one subsequent step of the projection step per class. Empirically, we find that only one step is sufficient to extract difficult support sets, if the learning rate (α) for gradient ascent is set as high as 200. In practice, in our experiment test-bed, we observe that the difficulty of the task decreases if the number of iterations increases to greater than 1. This could be attributed to the fact that the selection weight vector becomes sparse after one iteration, which might not be a good starting point for further steps of the optimization. Below we plot the effect of the number of iterations on the weighted prototype loss for a subset of the domains from HARD-MD++, which we use as a measure of the stopping criterion for our algorithm.\n\nFigure 36: Number of Optimization Steps (Omniglot).\n\nFigure 37: Number of Optimization Steps (Aircraft).\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nFigure 38: Number of Optimization Steps (cu-birds).\n\nFigure 39: Number of Optimization Steps (vgg-flower).\n\nNote that we use a post-processing step from the learnt selection weights to extract tasks with a given value of shot per class (i.e. kj samples for each of the jth class). Empirically, we track the query accuracy with the extracted support set and always find the lowest accuracy on the query set to be after one iteration of the algorithm.\n\nL RESULTS ON MS-COCO (FULL)\n\nThe MS-COCO split in META-DATASET consists of object-cropped images based on the bounding box annotations. However, the original MS-COCO (Lin et al., 2014) (hereby denoted as MS-COCO (Full)) with bounding box and segmentation annotations can be a measurement for the number of objects present in the image. To understand, if a distribution shift between the support and query with respect to the number of objects is a factor in increasing task difficulty, we perform a finegrained study using our framework and MS-COCO (Full). In particular we fix the query set to have images with objects in the set: {1, 2, 4, 6, 10}. For these fixed query sets, we use our framework to extract easy and difficult support sets.\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nFigure 40: Large distribution shift with respect to number of objects between support and query for difficult tasks from MS-COCO (Full).\n\nFrom Fig 40, we find that our framework extracts difficult tasks where there is a large distribution shift between the support and query with respect to the number of objects. Along with the results for ORBIT and CURE-OR (Section 5.3.2), this validates that our framework extracts tasks which have a weak relationship between the natural characteristics of the support and query set (e.g., occlusion, viewpoint, number of objects).\n\nM RESULTS WITH 600 TASKS PER DATASET\n\nFigure 41: Results with 600 tasks on HARD-MD.\n\nFrom Fig 41, we find that the ordering of the different models in terms of performance remain the same even when evaluated on 600 tasks per dataset. For e.g., ViT-B with CLIP initialization still has the best performance on the HARD-MD split from HARD-MD++.\n\nN FORMULATION AND RESULTS WITH TASK2VEC\n\nTask2Vec (Achille et al., 2019) is a task embedding method which can provide vectorial representations for tasks. The core idea in task2vec is to compute an embedding based on the diagonal of the Fisher information matrix of a probe network. Given a probe network pw parameterized by weights w, the task embedding is the diagonal of the Fisher Information Matrix (FIM) F : F = Ex,y∼ ˆp(x)pw(y|x)[∇w log(pw(y|x))∇w log(pw(y|x)T )] In practice, the diagonal of the Fisher Information Matrix, which is a vector (where t = Diag(F )) is used as the task embedding. In principle task2vec can be used with our optimization framework to extract difficult tasks. Given a fixed query set Q and a search pool P , where each example in P has a selection weight wi associated to it:\n\n(10)\n\nmax w\n\nl(t(Q), t(P, w))\n\n(11)\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nDINO + Proto-Loss DINO + Task2Vec\n\nmscoco 21.06 43.1\n\ntraffic sign 11.4 39.2\n\nilsvrc 2012 30.5 51.3\n\nomniglot 43.8 49.2\n\naircraft 29.7 43.2\n\ncu birds 67.1 78.1\n\ndtd 66.5 76.4\n\nquickdraw fungi 8.2 32.1\n\n25.7 39.2\n\nvgg flower 83.5 89.3\n\nTable 2: Accuracy of extracted tasks using Proto-Loss vs. Task2Vec across different domains from META-DATASET: Using task2vec within our framework results in easier tasks than using Proto-Loss (current loss in our framework). Across 600 variable-way, variable-shot tasks per dataset. (Lower is better)\n\ns.t. wi ∈ {0, 1}, ∥wj∥0 ≤ kj,\n\n∀i ∈ [1, M ]\n\n∀j ∈ [1, N ]\n\nwhere t is the task embedding from task2vec and l(.) is the mean-squared error. We solve eq. (11) using the same projection algorithm used in eq. (3). In this framework, we use ViT-S pre-trained with DINO (Caron et al., 2021) as the probe network.\n\nFindings. We find that using the task2vec formulation results in extracted tasks (Table. (2)) which are easier than the Proto-Loss formulation used in our framework eq. (1). Second, using task2vec is more computationally expensive than our method, as it involves computing the diagonal of the FIM which involves computing the gradients, whereas our current formulation only needs to compute the Proto-Loss which requires one forward pass.\n\nO ABLATIONS WITH DIFFERENT BASE MODELS\n\nmscoco\n\nViT (S) + DINO 21.06 ViT (B) + DINO 21.4 22.4 ViT (B) + CLIP 22.1 ViT (S) + DeiT 22.3 ViT (B) + DeiT\n\ntraffic sign 11.4 11.9 13.4 14.2 12.3\n\nilsvrc 2012 30.5 31.3 35.4 34.1 33.2\n\nomniglot 43.8 43.2 46.7 45.1 43.9\n\naircraft 29.7 28.8 29.1 30.1 31.3\n\ncu birds 67.1 67.9 70.1 69.3 71.1\n\ndtd 66.5 67.1 69.1 70.1 68.1\n\nquickdraw fungi 8.2 9.1 13.1 11.3 13.2\n\n25.7 25.3 27.1 26.5 28.1\n\nvgg flower 83.5 83.9 84.1 83.4 83.6\n\nTable 3: Effectiveness of our framework in extracting difficult support sets using different base models. Across 600 variable-way, variable-shot tasks per domain from META-DATASET.\n\nIn Table. (3), we find that our framework is flexible enough to be used with different base models for extracting difficult tasks. While in principle any base model can be used with our optimization framework, we choose ViT(S)+DINO as its the few-shot pipeline used by (Hu et al., 2022) which results in strong cross-domain few-shot performances on MD.\n\nP INTEGRATION WITH DIFFERENT ADAPTATION STRATEGIES\n\nIn our paper, we choose the loss with prototypical networks (Snell et al., 2017) as the adaptation strategy due to it’s fast adaptation. In principle, other relatively fast adaptation strategies such as R2D2 (Bertinetto et al., 2019) can be used. However, there is an inherent problem using adaptation strategies such as R2D2 with MD. The adaptation step requires using the entire search pool P with the adaptation strategy. With META-DATASET’s variable-way, variable-shot sampling protocol, the search pool P can be extremely large due to a large number of ways in certain tasks. For e.g., R2D2 requires solving a ridge-regressor in the adaptation step, where the optimal ridge regressor weights are denoted by:\n\nW = X T (XX T + λI)−1Y\n\n(12)\n\nwhere X contains the embeddings of the support set and X ∈ R|P |×e, where |P | is the size of the search pool and e is the embedding size of the features from the base model. XX T ∈ R|P |×|P | and grows quadratically with the size of the search pool size, which can be as large as 50k for certain datasets in MD such as quickdraw or ilsvrc 2012. Given the high dimensionality of XX T , its inversion will be extremely expensive making it infeasible to use it for fast difficult task extraction from large-scale vision datasets.\n\n34\n\nPublished as a conference paper at ICLR 2023\n\nQ COMPARISON WITH FILTERED OUT DIFFICULT TASKS\n\nFigure 42: Our extracted difficult tasks have lower number of ways than filtered out difficult tasks from META-DATASET’s random sampler.\n\nWe use our framework to extract 200 difficult tasks and run MD’s random sampler for 10000 iterations and filter out 200 tasks with the lowest accuracies. We then compare the number of ways in both the set of difficult tasks. Previously (Triantafillou et al., 2019) has shown that a large number of ways in a given task can be a source of task difficulty. From Fig 42, we observe that across all the domains in MD, our framework is able to extract tasks with a lower number of ways than the filtered out difficult tasks from MD’s random sampler. This shows that a large number of ways is not the primary source of difficulty in the tasks extracted by our framework. Furthermore in Section 5.3.2, through the lens of annotated datasets, we validate that the difficult tasks extracted by our framework have a distribution shift between the support and query with respect to various natural characteristics.\n\nR RESULTS ON ADVERSARIAL TRAINING WITH DIFFICULT TASKS\n\nWe note that adversarial training with difficult tasks have been previously investigated in (Agarwal et al., 2021). Similar to the observations in (Agarwal et al., 2021), we find that adversarial training does not have intended improvements during meta-testing. In particular, we meta-train ViT-S (pretrained with DINO) with prototypical networks on 10000 difficult training tasks extracted using our framework and 10000 randomly sampled tasks. We find that adversarial meta-training using difficult tasks does not lead to improvements on HARD-META-DATASET. This shows that one needs to carefully design new training paradigms for improving on the difficult tasks from HARD-METADATASET.\n\nViT (S) + DINO ViT (S) + DINO + Adversarial Training\n\nmscoco 21.06\n\ntraffic sign 11.4\n\nilsvrc 2012 30.5\n\nomniglot 43.8\n\naircraft 29.7\n\ncu birds 67.1\n\ndtd 66.5\n\nquickdraw fungi 8.2\n\n25.7\n\nvgg flower 83.5\n\n21.4\n\n12.3\n\n30.1\n\n42.3\n\n29.5\n\n66.5\n\n67.1\n\n9.1\n\n26.1\n\n83.2\n\nTable 4: Adversarial training does not improve performance on the HARD-MD split from HARD-MD++. Across 600 variable-way, variable-shot tasks from each domain.\n\nWe note that the results in Table 4 are initial results on adversarial meta-training and more carefully designed adversarial meta-training methods might be required to improve performance on HARDMD++.\n\n35\n\nPublished as a conference paper at ICLR 2023\n\nS INITIAL RESULTS WITH TRANSDUCTIVE APPROACHES\n\nWe use the transductive fine-tuning approach introduced in (Dhillon et al., 2019a) and evaluate it on the tasks from HARD-MD++. We note that in our paper, we primarily evaluate inductive approaches in line with recently developed methods for MD (Triantafillou et al., 2019; Hu et al., 2022; Li et al., 2021). Evaluating and understanding the true effectiveness of transductive approaches will require a separate line of work. However, in this section we provide some initial results on how baseline transductive fine-tuning approaches fare on the HARD-MD split from HARD-MD++. Essentially, we add a regularizer to the fine-tuning cross-entropy loss to seek outputs from the query examples with a peaked posterior (i.e. low Shannon Entropy) similar to (Dhillon et al., 2019a).\n\nFigure 43: Transductive Fine-tuning vs. Normal Fine-tuning (FT) on HARD-MD.\n\nFrom Fig 43, we find no clear benefit with transductive fine-tuning for the HARD-MD split from HARD-MD++. However, investigation of more transductive approaches such as (Boudiaf et al., 2020; Hu et al., 2020) is warranted to fully validate the effectiveness of transductive approaches and can be a good direction for future work.\n\nT RESULTS WITH ADAPTERS\n\nWe leverage recently developed parameter efficient fine-tuning adapter based methods such as (Xu et al., 2022) and compare it with full network fine-tuning on the HARD-MD split from HARD-MD++.\n\nFigure 44: Adapter Fine-tuning vs. Normal Fine-tuning (FT) on the HARD-MD split from HARD-MD++.\n\nNote that the adapter design in (Xu et al., 2022) is developed for ViT backbones and we find the effectiveness of their method on HARD-MD with 7 different vision transformer pre-trained backbones. In general from Fig 44, we find that carefully designed adapter based methods outperform full-network fine-tuning (though by a small margin). A thorough evaluation of parameter efficient fine-tuning techniques (Jia et al., 2022; Hu et al., 2021) on HARD-MD is a direction for future work.\n\n36",
    "reference": "# Summary Of The Paper\n\nThe paper studies the failure cases in FSL and proposes an efficient algorithm to extract the difficult tasks from large-scale datasets. Based on the proposed algorithm, the paper builds a new test-only few-shot classification benchmark named HARD-META-DATASET++.\n\n# Strength And Weaknesses\n\nStrength:\n1) The paper is writing clearly and easy to read.\n2) The paper builds a new test-only few-shot classification benchmark named HARD-META-DATASET++. \n\nWeaknesses:\n\n1) The support samples of \"difficult task\" is selected by fixing the query samples which means the difficulty of each task is highly correlated with the fixed query data. The hypothesis is somewhat unreasonable, since the FSL aims to adapt to the whole new class not the fixed query samples. The reviewer supposes that the tasks in the HARD-META-DATASET++ may not be a reliable estimator in the test-time in FSL. \n\n2) The proposed method can be seen as an extension of the paper[1], since the paper[1] have already proposed that FSL are extremely sensitive to the data used for adaptation and used a greedy algorithm to find those difficult tasks. Although the proposed method is more efficient, the novelty is somewhat limited.\n\n3)The evaluation process consists 200 tasks using Prototypical Network, which is not enough(suffering from high randomness). In recent literature, the number of evaluation tasks is usually more than 2000.\n\nAs for questions, I would like to ask:\n\n1 In FSL, we usually report the mean and the variance of the accuracy over 2000 tasks. The variance of the accuracy also denotes the model's performance of the challenging tasks(The higher variance, the lower accuracy on more difficult tasks). Besides, the average accuracy of several worst cases can also be a good estimator. Why is the accuracy on HARD-MD  a better criterion for evaluation?\n\n2 How to use the \"difficult task\" in the training phase in FSL? Can the \"difficult task\" in base classes help the model improve the generality to novel classes?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well written. However, the novelty is somewhat limited, since it can be considered as an extension of the paper[1]. \n\nReproducibility is unclear since there are no codes or links provided.\n\n# Summary Of The Review\n\nThis paper builds a new test-only few-shot classification benchmark named HARD-META-DATASET++. However, I have some concerns that whether the HARD-META-DATASET++ can be a reliable estimation (Please refer to 1) in Weaknesses).\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nINFOOT: INFORMATION MAXIMIZING OPTIMAL TRANSPORT\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nOptimal transport aligns samples across distributions by minimizing the transportation cost between them, e.g., the geometric distances. Yet, it ignores coherence structure in the data such as clusters, does not handle outliers well, and cannot integrate new data points. To address these drawbacks, we propose InfoOT, an information-theoretic extension of optimal transport that maximizes the mutual information between domains while minimizing geometric distances. The resulting objective can still be formulated as a (generalized) optimal transport problem, and can be efficiently solved by projected gradient descent. This formulation yields a new projection method that is robust to outliers and generalizes to unseen samples. Empirically, InfoOT improves the quality of alignments across benchmarks in domain adaptation, cross-domain retrieval, and single-cell alignment.\n\n1\n\nINTRODUCTION\n\nOptimal Transport (OT) provides a general framework with a strong theoretical foundation to compare probability distributions based on the geometry of their underlying spaces (Villani, 2009). Besides its fundamental role in mathematics, OT has increasingly received attention in machine learning due to its wide range of applications in domain adaptation (Courty et al., 2017; Redko et al., 2019; Xu et al., 2020), generative modeling (Arjovsky et al., 2017; Bousquet et al., 2017), representation learning (Ozair et al., 2019; Chuang et al., 2022), and generalization bounds (Chuang et al., 2021). The development of efficient algorithms (Cuturi, 2013; Peyr ́e et al., 2016) has significantly accelerated the adoption of optimal transport in these applications.\n\nComputationally, the discrete formulation of OT seeks a matrix, also called transportation plan, that minimizes the total geometric transportation cost between two sets of samples drawn from the source and target distributions. The transportation plan implicitly defines (soft) correspondences across these samples, but provides no mechanism to relate newly-drawn data points. Aligning these requires solving a new OT problem from scratch. This limits the applicability of OT, e.g., to streaming settings where the samples arrive in sequence, or very large datasets where we can only solve OT on a subset. In this case, the current solution cannot be used on future data. To overcome this fundamental constraint, a line of work proposes to directly estimate a mapping, the pushforward from source to target, that minimizes the transportation cost (Perrot et al., 2016; Seguy et al., 2017). Nevertheless, the resulting mapping is highly dependent on the complexity of the mapping function (Galanti et al., 2021).\n\nOT could also yield alignments that ignore the intrinsic coherence structure of the data. In particular, by relying exclusively on pairwise geometric distances, two nearby source samples could be mapped to disparate target samples, as in Figure 1, which is undesirable in some settings. For instance, when applying OT for domain adaptation, source samples with the same class should ideally be mapped to similar target samples. To mitigate this, prior work has sought to impose structural priors on the OT objective, e.g., via submodular cost functions (Alvarez-Melis et al., 2018) or a Gromov-Wasserstein regularizer (Vayer et al., 2018b;a). However, these methods still suffer from sensitivity to outliers (Mukherjee et al., 2021) and imbalanced data (Hsu et al., 2015; Tan et al., 2020).\n\nThis work presents Information Maximization Optimal Transport (InfoOT), an information-theoretic extension of the optimal transport problem that generalizes the usual formulation by infusing it with global structure in form of mutual information. In particular, InfoOT seeks alignments that maximize mutual information, an information-theoretic measure of dependence, between domains. To\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustration of InfoOT on 2D point cloud. Compared to classic OT, InfoOT preserves the cluster structure, where the source points from the same cluster are mapped to the same target cluster. For projection estimation (dashed lines), the new conditional projection improves over barycentric projection with better outlier robustness and out-of-sample generalization.\n\ndo so, we treat the pairs selected by the transportation plan as samples drawn from the joint distribution and estimate the mutual information with kernel density estimation based on the paired samples (Moon et al., 1995). Interestingly, this results in an OT problem where the cost is the log ratio between the estimated joint and marginal distributions fXY (x, y)/(fX (x)fY (y)). Empirically, we show that using a cost combining mutual information with geometric distances yields better alignments across different applications. Moreover, akin to Gromov-Wasserstein (M ́emoli, 2011), the mutual information estimator only relies on intra-domain distances, which —unlike the standard OT formulation— makes it suitable for aligning distributions whose supports lie in different metric spaces, e.g., supports with different modalities or dimensionality (Alvarez-Melis & Fusi, 2020; Demetci et al., 2020).\n\nBy estimating a joint density, InfoOT naturally yields a novel method for out-of-sample transportation by taking an expectation over the estimated densities conditioned on the source samples, which we refer to as conditional projection. Typically, samples are mapped via a barycentric projection (Ferradans et al., 2014; Flamary et al., 2016), which corresponds to the weighted average of target samples, where the weights are determined by the transportation plan. The barycentric projection inherits the disadvantages of standard OT: sensitivity to outliers and failing to generalize to new samples. In contrast, our proposed conditional projection is robust to outliers and cross-domain class-imbalanced data (Figure 1 and 4) by averaging over samples with importance sampling, where the weight is, again, the ratio between the estimated joint and marginal densities. Furthermore, this projection is well-defined even for unseen samples, which widens the applicability of OT in streaming or large-scale settings where solving OT for the complete dataset is prohibitive.\n\nIn short, this work makes the following contributions:\n\n• We propose InfoOT, an information-theoretic extension to the optimal transport that regularizes\n\nalignments by maximizing mutual information;\n\n• We develop conditional projection, a new projection method for OT that is robust to outliers and\n\nclass imbalance in data, and generalizes to new samples;\n\n• We evaluate our approach via experiments in domain adaptation, cross-domain retrieval, and\n\nsingle-cell alignment.\n\n2 RELATED WORKS\n\nOptimal Transport Optimal transport provides an elegant framework to compare and align distributions. The discrete formulation, also called Earth Mover’s Distance (EMD), finds an optimal coupling between empirical samples by solving a linear programming problem (Bonneel et al., 2011). To speed up the computation, Cuturi (2013) propose the Sinkhorn distance, an entropic regularized version of EMD that can be solved more efficiently via the Sinkhorn-Knopp algorithm (Knight, 2008). Compared to EMD, this regularized formulation typically yields denser transportation plans, where samples can be associated with multiple target points. Various extensions of OT have been proposed to impose stronger priors, e.g., Alvarez-Melis et al. (2018) incorporate additional structure by leveraging a submodular transportation cost, while Flamary et al. (2016) induce class coherence through a group-sparsity regularizer. The Gromov-Wasserstein (GW) distance (M ́emoli, 2011) is a variant of OT in which the transportation cost is defined upon intra-domain pairwise distances. Therefore, GW has been adopted to align ‘incomparable spaces’ (Alvarez-Melis & Jaakkola, 2018; Demetci et al., 2020) as the source and target domains do not need to lie in the same space. Since the GW objective is no longer a linear program, it is typically optimized using projected gradient\n\n2\n\nOTInfoOTInfoOT w/ BarycentricInfoOT w/ ConditionaloutliersUnder review as a conference paper at ICLR 2023\n\ndescent (Peyr ́e et al., 2016; Solomon et al., 2016). The Fused-GW, which combines the OT and GW objectives, was proposed by Vayer et al. (2018a) to measure graph distances.\n\nMutual Information and OT The proposed InfoOT extends the standard OT formulation by maximizing a kernel density estimated mutual information. Recent works (Bai et al., 2020; Khan & Zhang, 2022) also explore the connection between OT and information theory. Liu et al. (2021) consider a semi-supervised setting for estimating a variant of mutual information, where the unpaired samples are leveraged to minimize the estimation error. Ozair et al. (2019) replace the KL divergence in mutual information with Wasserstein distance and develop a loss function for representation learning. In comparison, the objective of InfoOT is to seek alignments that maximize the mutual information while being fully unsupervised by parameterizing the joint densities with the transportation plan. Another line of work also combines OT with kernel density estimation (Canas & Rosasco, 2012; Mokrov et al., 2021), but focuses on different applications.\n\n3 BACKGROUND ON OT AND KDE\n\nOptimal Transport Let {xi}n i=1 ∈ Y m be the empirical samples and C ∈ Rn×m be the transportation cost for each pair, e.g,. Euclidean cost Cij = ∥xi − yj∥. Given two sets + where (cid:80)n of weights over samples p ∈ Rn i=1 qi = 1, and a cost matrix C, Kantorovich’s formulation of optimal transport solves\n\ni=1 ∈ X n and {yi}m\n\ni=1 pi = (cid:80)m\n\n+ and q ∈ Rm\n\nmin Γ∈Π(p,q)\n\n⟨Γ, C⟩, Π(p, q) = {Γ ∈ Rn×m\n\n|γ1m = p, γT 1n = q},\n\n+\n\nwhere Π(p, q) is a set of transportation plans that satisfies the flow constraint. In practice, the Sinkhorn distance (Cuturi, 2013), an entropic regularized version of OT, can be solved In particular, the Sinkhorn distance solves more efficiently via the Sinkhorn-Knopp algorithm. minΓ∈Π(p,q)⟨Γ, C⟩ − εH(Γ), where H(Γ) = − (cid:80) i,j Γij log Γij is the entropic regularizer that smooths the transportation plan.\n\nKernel Density Estimation Kernel Density Estimation (KDE) is a non-parametric density estimation method based on kernel smoothing (Parzen, 1962; Rosenblatt, 1956). Here, we consider a generalized KDE for metric spaces (X , dX ) and (Y, dY ) (Li et al., 2020; Pelletier, 2005). In particular, given a paired dataset {xi, yi}n i=1 ∈ {X n, Y n} sampled i.i.d. from an unknown joint density fXY and a kernel function K : R → R, KDE estimates the marginals and the joint density as\n\nˆfX (x) =\n\n1 n\n\n(cid:88)\n\ni\n\nKh1 (dX (x, xi)) ;\n\nˆfXY (x, y) =\n\n1 n\n\n(cid:88)\n\ni\n\nKh1 (dX (x, xi)) Kh2 (dX (y, yi)) ,\n\n(1)\n\nwhere Kh(t) = K( t h )/Zh and the normalizing constant Zh makes equation 1 integrate to one. The bandwidth parameter h controls the smoothness of the estimated densities. Figure 2 illustrates an example of KDE on 1D data. In this work, we do not need to estimate the normalizing constant as only the ratio between joint and marginal densities ˆfXY (x, y)/( ˆfX (x) ˆfY (y)) is considered while estimating the mutual information. For all the presented experiments, we adopt the Gaussian kernel:\n\nKh (dX (x, x′)) =\n\n1 Zh\n\n(cid:18)\n\nexp\n\n−\n\ndX (x, x′)2 2h2σ2\n\n(cid:19)\n\n,\n\nFigure 2: Example of KDE. The bandwidth affect the smoothness.\n\nwhere σ2 controls the variance. The Gaussian kernel has been successfully adopted for KDE beyond the Euclidean space (Li et al., 2020; Said et al., 2017), and we found it to work well in our experiments. For simplicity, we also set h1 = h2 = h for all the experiments.\n\n4\n\nINFORMATION MAXIMIZING OT\n\nOptimal transport captures the geometry of the underlying space through the ground metric in its objective. Additional information is not directly captured in this metric —such as coherence structure— will therefore be ignored when solving the problem. This is undesirable in applications\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Measuring Structure with Mutual information. (a) The Γa and Γb are two one-to-one mappings where Γa preserves the cluster structure and Γb is a random permutation; (b) The estimated joint density of Γa is more concentrated than the one of Γb, which also leads to higher mutual information under KDE.\n\nwhere this additional structure matters, for instance in domain adaptation, where class coherence is crucial. As a concrete example, the cluster structure in the dataset in Figure 1 is ignored by classic OT. Intuitively, the reason for this issue is that the classic OT is too local: the transportation cost considers each sample separately, without respecting coherence across close-by samples. Next, we show that mutual information estimated with KDE can introduce global structure into OT maps.\n\n4.1 MEASURING GLOBAL STRUCTURE WITH MUTUAL INFORMATION\n\nFormally, mutual information measures the statistical dependence of two random variables X, Y :\n\nI(X, Y ) =\n\n(cid:90)\n\n(cid:90)\n\nY\n\nX\n\nfX,Y (x, y) log\n\n(cid:18) fXY (x, y) fX (x)fY (y)\n\n(cid:19)\n\ndxdy\n\n(2)\n\nwhere fXY is joint density and fX , fY are marginal probability density functions. For paired datasets, various mutual information estimators have been defined (Belghazi et al., 2018; Moon et al., 1995; Poole et al., 2019). In contrast, we are interested in the inverse: given unpaired samples {xi}n\n\ni=1, can we find alignments that maximize the mutual information?\n\ni=1, {yj}m\n\nDiscrete v.s. Continuous. An immediate idea is to treat the discrete transportation plan Γ as the joint distribution between X and Y , and write the mutual information as (cid:80) i,j Γij log(nmΓij) = log(nm) − H(Γ). In this case, maximizing mutual information would be equivalent to minimizing the entropic regularizer H(Γ) introduced by (Cuturi, 2013). For a finite set of samples, this mutual information estimator is trivially maximized for any one-to-one mapping as then H(Γ) = 0. Figure 3 (a) illustrates two one-to-one mappings Γa and Γb between points sampled from multi-mode Gaussian distributions, where Γa preserves the cluster structure and Γb is simply a random permutation. They both maximize the mutual information estimate above, yet Γa is a better alignment with high coherence. In short, directly using the transportation plan estimated from finite samples as the joint distribution to estimate mutual information between continuous random variables is problematic. In contrast, joint distributions estimated with KDE tend to be smoother, such as Γa in Figure 3 (b). This suggests that KDE may lead to a better objective for the alignment problem.\n\n4.2\n\nINFOOT: MAXIMIZING MUTUAL INFORMATION WITH KDE\n\nInstead of directly interpreting the OT plan as the joint distribution for the mutual information, we use it to inform the definition of a different one. In particular, we treat Γij as the weight of pair (xi, yj) within the empirical samples drawn from the unknown joint distribution with density fXY . Intuitively, Γij defines what empirical samples we obtain by sampling from the joint distribution. Given a transportation plan Γ, the kernelized joint density in equation 1 can be rewritten as\n\nˆfΓ(x, y) =\n\n(cid:88)\n\n(cid:88)\n\ni\n\nj\n\nΓijKh (dX (x, xi)) Kh (dY (y, yj)) .\n\n(3)\n\nThe 1/n factor is dropped as the plan Γ is already normalized ((cid:80) replace the prespecified paired samples in equation 1 with the ones selected by the plan Γ.\n\nij Γij = 1). Specifically, we\n\n4\n\nPlan ΓaPlan ΓbJoint Density of ΓaJoint Density of ΓbHigh MILow MI(a) One-to-one Mappings(b) Joint Density estimated with KDEUnder review as a conference paper at ICLR 2023\n\nDefinition 1 (Kernelized Mutual Information). The KDE estimated mutual information reads\n\nˆIΓ(X, Y ) =\n\n(cid:88)\n\ni,j\n\nΓij log\n\nˆfΓ(xi, yj) ˆf (xi) ˆf (yj)\n\n(cid:88)\n\n=\n\nΓij log\n\ni,j\n\nnm · (cid:80) (cid:80)\n\nk,l ΓklKh (dX (xi, xk)) Kh (dY (yj, yl))\n\nk Kh (dX (xi, xk)) · (cid:80)\n\nl Kh (dY (yj, yl))\n\n.\n\nThe estimation has two folds: (1) approximating the joint distribution with KDE, and (2) estimating the integral in equation 2 with paired empirical sample (xi, yj) weighted by Γij. The normalizing constant Zh in equation (1) cancels out while calculating the ratio between joint and marginal probability densities. To maximize the empirical mutual information ˆIΓ(X, Y ), the plan has to map close-by points i, k to close-by points j, l. Maximizing this information can be interpreted as an optimal transport problem:\n\n(InfoOT)\n\nmax Γ∈Π(p,q)\n\nˆIΓ(X, Y ) = min\n\nΓ∈Π(p,q)\n\n(cid:88)\n\ni,j\n\nΓij · log\n\n(cid:32) ˆf (xi) ˆf (yj) ˆfΓ(xi, yj)\n\n(cid:33)\n\n.\n\n(4)\n\nInstead of pairwise (Euclidean) distances, the transportation cost is now the log ratio between the estimated marginal and joint densities. The following lemma illustrates the asymptotic relation between the kernel estimated mutual information and the entropic regularizer. Lemma 2. When h → 0 and K(·) is the Gaussian kernel, we have ˆIΓ(X, Y ) → −H(Γ)+log(nm).\n\nWhen the bandwidth h goes to zero, the estimated density is the sum of delta functions centered at the samples, and the estimated mutual information degenerates back to the standard entropic regularizer (Cuturi, 2013).\n\nNote that the formulation of InfoOT does not require the support of X and Y to be comparable. Similar to Gromov-Wasserstein (M ́emoli, 2011), InfoOT only relies on intra-domain distances, which makes it an appropriate objective for aligning distributions when the supports do not lie in the same metric space, e.g., supports with different modalities or dimensionalities, as section 6.4 shows.\n\nFused InfoOT: Incorporating the Geometry. When the geometry between domains is informative, the mutual information can act as a regularizer that refines the alignment. Along with a weighting parameter λ, we define the Fused InfoOT as\n\n(F-InfoOT) min\n\nΓ∈Π(p,q)\n\n⟨Γ, C⟩ − λ ˆIΓ(X, Y ) = min\n\nΓ∈Π(p,q)\n\n(cid:32)\n\nΓij ·\n\nCij + λ · log\n\n(cid:32) ˆf (xi) ˆf (yj) ˆfΓ(xi, yj)\n\n(cid:33)(cid:33)\n\n.\n\n(cid:88)\n\ni,j\n\nThe transportation cost becomes the weighted sum between the pairwise distances C and the log ratio of joint and marginals densities. As Figure 1 illustrates, the mutual information regularizer excludes alignments that destroy the cluster structure while minimizing the pairwise distances. Practically, we found F-InfoOT suitable for general OT applications such as unsupervised domain adaptation (Flamary et al., 2016) and color transfer (Ferradans et al., 2014) where the geometry between source and target is informative.\n\n4.3 NUMERICAL OPTIMIZATION\n\nAs the transportation cost is dependent on Γ, the objective is no longer linear in Γ and cannot be solved with linear programming. Instead, we adopt the projected gradient descent introduced in (Peyr ́e et al., 2016). In particular, Benamou et al. (2015) show that the projection can be done by simply solving the Sinkhorn distance (Cuturi, 2013) if the non-linear objective is augmented with the entropic regularizer H(Γ). For instance, we can augment F-InfoOT as follows:\n\nmin Γ∈Π(p,q)\n\n⟨Γ, C⟩ − λ ˆIΓ(X, Y ) − εH(Γ).\n\nIn this case, the update of projected gradient descent reads\n\nΓt+1 ← arg min Γ∈Π(p,q)\n\n(cid:68)\n\nΓ, C − λ∇Γ ˆIΓt(X, Y )\n\n(cid:69)\n\n− εH(Γ).\n\n(5)\n\nThe update is done by solving the sinkhorn distance (Cuturi, 2013), where the cost function is the gradient to the objective of F-InfoOT. We provide a detailed derivation of (5) in Appendix A.2.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nMatrix Computation Practically, the optimization can be efficiently computed with matrix multiplications. The gradient with respect to the transportation plan Γ is\n\n∂ ˆIΓ(X, Y ) ∂Γij\n\n= log\n\n(cid:33)\n\n(cid:32) ˆfΓ(xi, yj) ˆf (xi) ˆf (yj)\n\n(cid:88)\n\n+\n\nΓkl\n\nk,l\n\nKh (dX (xi, xk)) Kh (dY (yj, yl)) ˆfΓ(xk, yl)\n\n.\n\nLet KX and KY be the kernel matrices where (KX )ij = Kh (dX (xi − xj)), (KY )ij = Kh ((dY (yi − yj)). The gradient has the following matrix form: (cid:1) + KX\n\n∇Γ ˆIΓ(X, Y ) = log (cid:0)KX ΓK T\n\n(cid:0)Γ ⊘ KX ΓK T\n\nY ⊘ MX M T\n\n(cid:1) K T\n\nY\n\nY\n\nY\n\nwhere (MX )i = ˆf (xi), (MY )i = ˆf (yi) are the marginal density vectors and ⊘ denotes elementwise division. The gradient can be computed with matrix multiplications in O(n2m + nm2).\n\n5 CONDITIONAL PROJECTION WITH INFOOT\n\nMany applications of optimal transport involve mapping source points to a target domain. For instance, when applying OT for domain adaptation, the classifiers are trained on projected source samples that are mapped to the target domain. When X = Y, given a transportation plan Γ, a barycentric projection maps source samples to the target domain by minimizing the weighted cost to target samples (Flamary et al., 2016; Perrot et al., 2016). The mapping is equivalent to the weighted average of the target samples when the cost function is the squared Euclidean distance c(x, y) = ∥x − y∥2:\n\nxi (cid:55)→ arg min\n\ny∈Y\n\nm (cid:88)\n\nj=1\n\nΓij∥y − yj∥2 =\n\n1 j=1 Γij\n\n(cid:80)m\n\nm (cid:88)\n\nj=1\n\nΓijyj.\n\n(6)\n\nDespite its simplicity, the barycentric projection fails when (a) aligning data with outliers, (b) imbalanced data, and (c) mapping new samples. For instance, if sample xi is mostly mapped to an outlier yj, then its projection will be close to yj. Similar problems occur when applying OT for domain adaptation. If the size of a same class differs between domains, false alignments would emerge due to the flow constraint of OT as Figure 4 illustrates, which worsen the subsequent projections.\n\nSince the barycentric projection relies on the transportation plan to calculate the weights, any new source sample requires re-computing OT to obtain the transportation plan for it. This can be computationally prohibitive in large-scale settings. In the next section, we show that the densities estimated via InfoOT can be leveraged to compute the conditional expectation, which leads to a new mapping approach that is both robust and generalizable.\n\n5.1 CONDITIONAL EXPECTATION VIA KDE\n\nWhen treating the transportation plan as a probability mass function in the right-hand side of (equation 6), the barycentric projection resembles the conditional expectation E[Y |X = x]. Indeed, the classic definition of barycentric projection (Ambrosio et al., 2005) is defined as the integral over the conditional distribution. But, this again faces the issues discussed in section 4. Instead, equipped with the densities estimated via KDE and InfoOT, the conditional expectation can be better estimated with classical Monte-Carlo importance sampling using samples from the marginal PY : ˆfΓ(x, yj) ˆfX (x) ˆfY (yj)\n\n(cid:20) fXY (x, y) fX (x)fY (y)\n\n(cid:20) fY |X=x(y) fY (y)\n\nx (cid:55)→ E\n\n[y] = E\n\n= E\n\nPY |X=x\n\n1 Z\n\ny∼PY\n\ny∼PY\n\n(7)\n\nyj\n\n≈\n\n(cid:21)\n\n(cid:21)\n\ny\n\ny\n\nj=1\n\nwhere Z = (cid:80)m ˆfΓ(x, yj)/( ˆfX (x) ˆfY (yj)) is the normalizing constant. Compared to the barycentric projection, the importance weight for each yj is the ratio between the joint and the marginal densities. To distinguish the KDE conditional expectation with barycentric projection, we will refer to the proposed mapping as conditional projection. Robustness against Noisy Data. By definition in equation 3, the joint density ˆfΓ(x, y) measures the similarity of (x, y) to all other pairs selected by the transportation plan Γ. Even if x is aligned with outliers or wrong clusters, as long as the points near x are mostly aligned with the correct samples, the conditional projection will project x to similar samples as they are upweighted by the joint density in (7). This makes the mapping much less sensitive to outliers and imbalanced datasets. See Figure 1 and Figure 4 for illustrations.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Projection under imbalanced samples. When the cluster sizes mismatch between source and target, barycentric projection wrongly projects samples to the incorrect cluster. In contrast, increasing the bandwidth of conditional projection gradually improves the robustness and yields better projection.\n\nOut-of-sample Mapping. The conditional projection is well-defined for any x ∈ X , and naturally generalizes to new samples without recomputing the OT. Importantly, the importance weight ˆfΓ(x, y)/( ˆfX (x) ˆfY (y)) can be interpreted as a similarity score between (x, y), which is useful for retrieval tasks as section 6.3 shows.\n\nThe conditional projection tends to cluster points together with larger bandwidths that lead to more averaging. We found that using a smaller bandwidth (e.g., h = 0.1) for the conditional projection improves the diversity of the projection when the dataset is less noisy, e.g., the data in Figure 1. For noisy or imbalanced datasets, the same bandwidth used for optimizing InfoOT works well. Note that analogous to Lemma 2, when the bandwidth h → 0, the conditional projection converges to the barycentric projection, making the barycentric projection a special case of the conditional projection (Figure 4).\n\n6 EXPERIMENTS We now evaluate InfoOT with experiments in point cloud matching, domain adaptation, crossdomain retrieval, and single-cell alignment. All the optimal transport approaches are implemented or adopted from the POT library (Flamary et al., 2021). Detailed experimental settings and additional experiments can be found in the appendix.\n\n6.1 POINT CLOUD MATCHING\n\nWe begin with a 2D toy example, where both source and target samples are drawn from a Gaussian distribution with 2 modes, but the latter is rotated and has two outliers added to it, as Figure 1 shows. We compare the behavior of different variants of OT and mappings on this data. Perhaps not surprisingly, standard OT maps the source points in the same cluster to two different target clusters, overlooking the intrinsic structure of the data. In comparison, the alignment of InfoOT retains the cluster structure. On the right hand side, the barycentric projection maps two source points wrongly to the target outliers, while the conditional projection is not affected by the outliers. Lastly, we demonstrate an out-of-sample mapping with the conditional projection, where newly sampled points are correctly mapped to clusters.\n\nFigure 4 depicts an class-imbalanced setting, where the corresponding clusters in source and target have different numbers of samples. Therefore, the barycentric projection wrongly maps samples from the same source cluster to different target clusters. When increasing the bandwidth in the conditional projection, the smoothing effect of KDE gradually corrects the mapping and yields more concentrated projections. In appendix B.1, we further demonstrate that InfoOT improves the baselines in a color transfer task, where pixels are treated as points in RGB space.\n\n6.2 DOMAIN ADAPTATION\n\nNext, we apply the fused version of InfoOT to two domain adaptation benchmarks: MNIST-USPS and the Office-Caltech dataset (Gong et al., 2012). The MNIST (M) and USPS (U) are digit classification datasets, and the Office-Caltech dataset contains 4 domains: Amazon (A), Dslr (D), Webcam (W) and Caltech10 (C), with images labeled as one of 10 classes. For MNIST and USPS, the raw images are directly used to compute the distances, while we adopt decaf6 features (Donahue et al., 2014) extracted from pretrained neural networks for Office-Caltech. Following previous works on OT for domain adaptation (Alvarez-Melis et al., 2018; Flamary et al., 2016; Perrot et al., 2016), the source samples are first mapped to the target, and 1-NN classifiers are then trained on the projected samples with source labels. The barycentric projection is adopted for all the baselines, while F-InfoOT is tested with both barycentric and conditional projection.\n\n7\n\nh 0 (Barycentric)→h = 0.1h = 0.3h = 0.5Under review as a conference paper at ICLR 2023\n\nFigure 5: tSNE visualization of projections. We show the t-SNE visualization of projectrd source samples (circles) along with the target samples (triangles) on A→C. Classes are indicated by colors.\n\nOT\n\nSinkhorn\n\nGL-OT\n\nFGW\n\nLinear\n\nF-InfoOT\n\nF-InfoOT∗\n\nMNIST-USPS\n\nM→U 46.6±1.2 U→M 48.1±1.1\n\n62.7±2.0 58.6±0.9\n\n63.0±2.0 58.8±1.0\n\n49.6±7.3 37.8±5.3\n\n60.8±2.3 59.5±1.0\n\n69.9±3.1 65.1±1.4\n\n61.1±1.9 57.0±1.7\n\nOffice-Caltech\n\nC→D 61.3±10.9 C→W 64.0±7.0 C→A 78.6±4.7 D→W 90.7±3.8 73.8±4.5 D→A 67.0±3.0 D→C 81.3±7.8 W→D 64.8±4.6 W→C 67.3±4.9 W→A A→D 73.1±10.6 A→W 64.7±6.3 65.4±5.3 A→C 71.0±2.5 AVG\n\n71.9±15.9 66.0±9.3 77.3±4.8 90.7±4.7 73.4±2.9 66.4±3.8 80.6±10.4 65.8±4.1 69.3±5.4 68.8±8.8 70.0±7.5 69.3±6.0 72.4±3.7\n\n76.9±18.6 66.7±9.4 83.3±6.1 93.7±4.3 84.4±2.9 76.5±2.9 85.6±10.6 73.5±5.4 79.6±3.0 76.3±8.2 70.0±7.0 79.8±5.8 78.9±4.5\n\n61.3±10.9 64.0±7.0 79.0±4.5 90.7±3.8 73.8±4.5 67.0±3.0 81.3±7.8 64.8±4.6 67.3±4.9 73.1±10.6 64.7±6.3 65.5±5.7 71.0±2.5\n\n58.1±12.5 62.0±7.4 77.9±6.7 89.0±6.3 72.7±3.9 67.5±4.1 79.4±7.2 65.5±4.5 70.5±4.8 66.9±7.8 69.3±6.6 66.9±4.5 70.5±2.4\n\n78.1±13.9 79.7±4.3 87.0±3.7 91.0±4.1 83.6±4.3 70.2±3.7 79.4±8.9 75.8±3.3 85.5±1.9 80.6±7.5 82.0±7.2 74.4±3.4 80.6±3.3\n\n87.5±7.8 81.0±6.7 90.6±2.0 93.3±4.7 89.8±1.8 80.8±1.8 89.4±11.8 74.4±3.7 89.3±2.3 81.3±9.8 87.0±4.6 81.2±3.6 85.6±3.3\n\nTable 1: Optimal Transport for Domain Adaptation. The Fused-InfoOT with conditional projection (FInfoOT∗) performs significantly better than the barycentric counterpart (F-InfoOT) and the other baselines when the dataset exhibit class imbalance, e.g., Office-Caltech.\n\nP@1\n\nOffice-Caltech P@5\n\nP@15\n\nP@1\n\nImageCLEF P@5\n\nP@15\n\n70.0±13.9 L2-NN Sinkhorn+NN 69.0±11.6 69.6±11.7 FGW+NN\n\n62.9±14.0 65.1±6.4 65.6±6.6\n\n53.6±12.1 58.0±6.9 58.5±7.0\n\n80.4±10.5 81.9±10.2 81.9±10.4\n\n77.6±9.5 81.1±10.6 81.2±10.6\n\n71.7±9.3 79.2±10.4 79.3±10.3\n\nF-InfoOT\n\n76.4±9.0\n\n75.1±9.1\n\n70.4±10.6\n\n81.2±9.3\n\n81.9±9.8\n\n80.0±10.7\n\nTable 2: Optimal Transport for Cross-Domain Retrieval. With conditional projection, InfoOT is capable to perform alignment for unseen samples without any modification.\n\nFollowing Flamary et al. (2016), we present the results over 10 independent trials. In each trial of Office-Caltech, the target data is divided into 90%/10% train-test split, where OT and 1-NN classifiers are only computed on the training set. For MNIST-USPS, only 2000 samples from the source and target training set are used, while the original test sets are used. The strength of the entropy regularizer ε is set to 1 for every entropic regularized OT, and the λ of F-InfoOT is set to 100 for all the experiments. The bandwidth for each benchmark is selected from {0.2, 0.3, ..., 0.8} with the circular validation procedure (Bruzzone & Marconcini, 2009; Perrot et al., 2016; Zhong et al., 2010) on M→U and A→D, which is 0.4 and 0.5, respectively. We compare F-InfoOT with Sinkhorn distance (Cuturi, 2013), group-lasso regularized OT (Flamary et al., 2016), fused Gromov-Wasserstein (FGW) (Vayer et al., 2019), and linear-mapping OT (Perrot et al., 2016). For OTs involving intradomain distances such as F-InfoOT and FGW, we adopt the following class-conditional distance for the source: ∥xi − xj∥ + 5000 · 1f (xi)̸=f (xj ), where the second term is a penalty on class mismatch (Alvarez-Melis & Fusi, 2020; Yurochkin et al., 2019) and f is the labeling function. As Table 1 shows, F-InfoOT with barycentric projection outperforms the baselines in both benchmarks, demon-\n\n8\n\nA->CF-InfoOT BarycentricF-InfoOT ConditionalSinkhornRaw DataUnder review as a conference paper at ICLR 2023\n\nstrating that mutual information captures the intrinsic structure of the datasets. In Office-Caltech, many datasets exhibit the class-imbalance problem, which makes F-InfoOT with conditional projection significantly outperform the barycentric projection and the other baselines. Figure 5 visualizes the projected source and target samples with tSNE (Van der Maaten & Hinton, 2008). The barycentric projection tends to produce one-to-one alignments, which suffer from class-imbalanced data. In contrast, conditional projection yields concentrated projections that preserves the class structure.\n\nscGEM FOS, Acc,\n\nSNAREseq FOS. Acc..\n\nUnionCom MMD-MA SCOT (GW)\n\nInfoOT\n\n0.210 0.201 0.190\n\n0.178\n\n58.2 58.8 57.6\n\n68.9\n\n0.265 0.150 0.150\n\n0.156\n\n42.3 94.2 98.2\n\n98.8\n\nTable 3: Single-Cell Alignment Performance. Similar to GW, InfoOT also performs well in cross-domain alignment.\n\nFigure 6: scGEM alignment with InfoOT. The cell types are indicated by colors.\n\n6.3 CROSS-DOMAIN RETRIEVAL We now consider unsupervised cross-domain image retrieval, where given a source sample, the algorithms have to determine the top-k similar target samples. Given fixed source and target samples, this can be formulated as an optimal transport problem, where the transportation plan Γij gives the similarity score between the candidate source sample xi and target samples yj. Nevertheless, this formulation fails when new source samples come. For standard OT, one has to solve the OT problem again to obtain the alignment for new samples. In contrast, the importance weight ˆfΓ(x, y)/( ˆfX (x) ˆfY (y)) defined in conditional projection (7) naturally provides the similarity score between the candidate x and each target sample y. We test F-InfoOT on the Office-Caltech (Gong et al., 2012) and ImageClef datasets (Caputo et al., 2014), where we adopt the same hyperparameter for Office-Caltech from the previous section. In the unsupervised setting, the in-domain transportation cost for the source is the Euclidean distance instead of the class-conditional distance. To compare with standard OTs, we adopt a nearest neighbor approach for the baselines: (1) retrieve the nearest source sample given an unseen sample, and (2) use the transportation plan of the nearest source sample to retrieve target samples. Along with a simple nearest neighbor retrieval baseline (L2-NN), the average top-k precision over 10 trials is shown in Table 2. The fuesed InfoOT significantly outperforms the baselines on Office-Caltech across different choices of k.\n\n6.4 SINGLE CELL ALIGNMENT Finally, we examine InfoOT in unsupervised alignment between incomparable spaces with the single-cell multi-omics dataset from (Demetci et al., 2020). Recent techniques allow to obtain different cellular features at the single-cell resolution (Buenrostro et al., 2015; Chen et al., 2019; Stoeckius et al., 2017). Nevertheless, different features are typically collected from different sets of cells, and aligning them is crucial for unified data analysis. We examine InfoOT with the sc-GEM (Cheow et al., 2016) and SNARE-seq (Chen et al., 2019) dataset provided by (Demetci et al., 2020) and follow the same data preprocessing steps, distance calculation, and evaluation setup. Here, two evaluation metrics are considered: “fraction of samples closer than the true match” (FOSCTTM) (Liu et al., 2019) and the label transfer accuracy (Cao et al., 2020). We compare InfoOT with UnionCom (Cao et al., 2020), MMD-MA (Liu et al., 2019), and SCOT (Demetci et al., 2020), where SCOT is an optimal transport baseline with Gromov-Wasserstein distance. Similarly, the bandwidth for each dataset is selected from {0.2, 0.3, ..., 0.8} with the circular validation procedure. As Table 3 shows, InfoOT significantly improves the baselines on the sc-GEM dataset, while being comparable on the SNARE-seq dataset, demonstrating the applicability of InfoOT on cross-domain alignment. Figure 6 further visualizes the barycentric projection with InfoOT, where we can see that cells with the same type are well aligned.\n\n7 CONCLUSION In this work, we propose InfoOT, an information-theoretic extension of optimal transport. InfoOT produces smoother, coherent alignments by maximizing the mutual information estimated with KDE. InfoOT leads to a new mapping method, conditional projection, that is robust to class imbalance and generalizes to unseen samples. We extensively demonstrate the applicability of InfoOT across benchmarks in different modalities.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDavid Alvarez-Melis and Nicolo Fusi. Geometric dataset distances via optimal transport. Advances\n\nin Neural Information Processing Systems, 33:21428–21439, 2020.\n\nDavid Alvarez-Melis and Tommi Jaakkola. Gromov-wasserstein alignment of word embedding In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\n\nspaces. Processing, pp. 1881–1890, 2018.\n\nDavid Alvarez-Melis, Tommi Jaakkola, and Stefanie Jegelka. Structured optimal transport.\n\nIn\n\nInternational Conference on Artificial Intelligence and Statistics, pp. 1771–1780. PMLR, 2018.\n\nDavid Alvarez-Melis, Stefanie Jegelka, and Tommi S Jaakkola. Towards optimal transport with global invariances. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1870–1879. PMLR, 2019.\n\nLuigi Ambrosio, Nicola Gigli, and Giuseppe Savar ́e. Gradient flows: in metric spaces and in the\n\nspace of probability measures. Springer Science & Business Media, 2005.\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein generative adversarial networks.\n\nIn International conference on machine learning, pp. 214–223. PMLR, 2017.\n\nYikun Bai, Xiugang Wu, and Ayfer ̈Ozg ̈ur. Information constrained optimal transport: From talagrand, to marton, to cover. In 2020 IEEE International Symposium on Information Theory (ISIT), pp. 2210–2215. IEEE, 2020.\n\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International conference on machine learning, pp. 531–540. PMLR, 2018.\n\nJean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyr ́e. Iterative bregman projections for regularized transportation problems. SIAM Journal on Scientific Computing, 37(2):A1111–A1138, 2015.\n\nNicolas Bonneel, Michiel Van De Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement interpolation using lagrangian mass transport. In Proceedings of the 2011 SIGGRAPH Asia conference, pp. 1–12, 2011.\n\nOlivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl-Johann Simon-Gabriel, and Bernhard Schoelkopf. From optimal transport to generative modeling: the vegan cookbook. arXiv preprint arXiv:1705.07642, 2017.\n\nLorenzo Bruzzone and Mattia Marconcini. Domain adaptation problems: A dasvm classification technique and a circular validation strategy. IEEE transactions on pattern analysis and machine intelligence, 32(5):770–787, 2009.\n\nJason D Buenrostro, Beijing Wu, Howard Y Chang, and William J Greenleaf. Atac-seq: a method for assaying chromatin accessibility genome-wide. Current protocols in molecular biology, 109 (1):21–29, 2015.\n\nGuillermo Canas and Lorenzo Rosasco. Learning probability measures with respect to optimal\n\ntransport metrics. Advances in Neural Information Processing Systems, 25, 2012.\n\nKai Cao, Xiangqi Bai, Yiguang Hong, and Lin Wan. Unsupervised topological alignment for single-\n\ncell multi-omics integration. Bioinformatics, 36(Supplement 1):i48–i56, 2020.\n\nBarbara Caputo, Henning M ̈uller, Jesus Martinez-Gomez, Mauricio Villegas, Burak Acar, Novi Patricia, Neda Marvasti, Suzan ̈Usk ̈udarlı, Roberto Paredes, Miguel Cazorla, et al. Imageclef 2014: Overview and analysis of the results. In International Conference of the Cross-Language Evaluation Forum for European Languages, pp. 192–211. Springer, 2014.\n\nSong Chen, Blue B Lake, and Kun Zhang. High-throughput sequencing of the transcriptome and\n\nchromatin accessibility in the same cell. Nature biotechnology, 37(12):1452–1457, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nLih Feng Cheow, Elise T Courtois, Yuliana Tan, Ramya Viswanathan, Qiaorui Xing, Rui Zhen Tan, Daniel SW Tan, Paul Robson, Yuin-Han Loh, Stephen R Quake, et al. Single-cell multimodal profiling reveals cellular epigenetic heterogeneity. Nature methods, 13(10):833–836, 2016.\n\nLenaic Chizat, Gabriel Peyr ́e, Bernhard Schmitzer, and Franc ̧ois-Xavier Vialard. Scaling algorithms for unbalanced optimal transport problems. Mathematics of Computation, 87(314):2563–2609, 2018.\n\nChing-Yao Chuang, Youssef Mroueh, Kristjan Greenewald, Antonio Torralba, and Stefanie Jegelka. Measuring generalization with optimal transport. Advances in Neural Information Processing Systems, 34:8294–8306, 2021.\n\nChing-Yao Chuang, R Devon Hjelm, Xin Wang, Vibhav Vineet, Neel Joshi, Antonio Torralba, Stefanie Jegelka, and Yale Song. Robust contrastive learning against noisy views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16670–16681, 2022.\n\nAlexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv ́e J ́egou.\n\nWord translation without parallel data. arXiv preprint arXiv:1710.04087, 2017.\n\nNicolas Courty, R ́emi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. Advances in Neural Information Processing Systems, 30, 2017.\n\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural\n\ninformation processing systems, 26, 2013.\n\nPinar Demetci, Rebecca Santorella, Bj ̈orn Sandstede, William Stafford Noble, and Ritambhara Singh. Gromov-wasserstein optimal transport to align single-cell multi-omics data. BioRxiv, 2020.\n\nJeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pp. 647–655. PMLR, 2014.\n\nSira Ferradans, Nicolas Papadakis, Gabriel Peyr ́e, and Jean-Franc ̧ois Aujol. Regularized discrete\n\noptimal transport. SIAM Journal on Imaging Sciences, 7(3):1853–1882, 2014.\n\nR Flamary, N Courty, D Tuia, and A Rakotomamonjy. Optimal transport for domain adaptation.\n\nIEEE Trans. Pattern Anal. Mach. Intell, 1, 2016.\n\nR ́emi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aur ́elie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, L ́eo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78): 1–8, 2021. URL http://jmlr.org/papers/v22/20-451.html.\n\nCharlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learn-\n\ning with a wasserstein loss. Advances in neural information processing systems, 28, 2015.\n\nTomer Galanti, Sagie Benaim, and Lior Wolf. Risk bounds for unsupervised cross-domain mapping\n\nwith ipms. J. Mach. Learn. Res., 22:90–1, 2021.\n\nMuhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE transactions on pattern analysis and machine intelligence, 39(7):1414–1430, 2016.\n\nBoqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In 2012 IEEE conference on computer vision and pattern recognition, pp. 2066–2073. IEEE, 2012.\n\nTzu Ming Harry Hsu, Wei Yu Chen, Cheng-An Hou, Yao-Hung Hubert Tsai, Yi-Ren Yeh, and YuChiang Frank Wang. Unsupervised domain adaptation with imbalanced cross-domain data. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4121–4129, 2015.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nGabriel Khan and Jun Zhang. When optimal transport meets information geometry. Information\n\nGeometry, pp. 1–32, 2022.\n\nPhilip A Knight. The sinkhorn–knopp algorithm: convergence and applications. SIAM Journal on\n\nMatrix Analysis and Applications, 30(1):261–275, 2008.\n\nDidong Li, Yulong Lu, Emmanuel Chevallier, and David B Dunson. Density estimation and model-\n\ning on symmetric spaces. arXiv preprint arXiv:2009.01983, 2020.\n\nJie Liu, Yuanhao Huang, Ritambhara Singh, Jean-Philippe Vert, and William Stafford Noble. Jointly embedding multiple single-cell omics measurements. In Algorithms in bioinformatics:... International Workshop, WABI..., proceedings. WABI (Workshop), volume 143. NIH Public Access, 2019.\n\nYanbin Liu, Makoto Yamada, Yao-Hung Hubert Tsai, Tam Le, Ruslan Salakhutdinov, and Yi Yang. Lsmi-sinkhorn: Semi-supervised mutual information estimation with optimal transport. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 655– 670. Springer, 2021.\n\nMingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S Yu. Transfer feature learning with joint distribution adaptation. In Proceedings of the IEEE international conference on computer vision, pp. 2200–2207, 2013.\n\nMingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S Yu. Transfer joint matching for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1410–1417, 2014a.\n\nMingsheng Long, Jianmin Wang, Jiaguang Sun, and S Yu Philip. Domain invariant transfer kernel learning. IEEE Transactions on Knowledge and Data Engineering, 27(6):1519–1532, 2014b.\n\nFacundo M ́emoli. Gromov–wasserstein distances and the metric approach to object matching. Foun-\n\ndations of computational mathematics, 11(4):417–487, 2011.\n\nPetr Mokrov, Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, and Evgeny Burnaev. Large-scale wasserstein gradient flows. Advances in Neural Information Processing Systems, 34:15243–15256, 2021.\n\nYoung-Il Moon, Balaji Rajagopalan, and Upmanu Lall. Estimation of mutual information using\n\nkernel density estimators. Physical Review E, 52(3):2318, 1995.\n\nDebarghya Mukherjee, Aritra Guha, Justin M Solomon, Yuekai Sun, and Mikhail Yurochkin. Outlier-robust optimal transport. In International Conference on Machine Learning, pp. 7850– 7860. PMLR, 2021.\n\nKhai Nguyen, Dang Nguyen, Tung Pham, Nhat Ho, et al. Improving mini-batch optimal transport via partial transportation. In International Conference on Machine Learning, pp. 16656–16690. PMLR, 2022.\n\nSherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron Van den Oord, Sergey Levine, and Pierre Sermanet. Wasserstein dependency measure for representation learning. Advances in Neural Information Processing Systems, 32, 2019.\n\nEmanuel Parzen. On estimation of a probability density function and mode. The annals of mathe-\n\nmatical statistics, 33(3):1065–1076, 1962.\n\nBruno Pelletier. Kernel density estimation on riemannian manifolds. Statistics & probability letters,\n\n73(3):297–304, 2005.\n\nMicha ̈el Perrot, Nicolas Courty, R ́emi Flamary, and Amaury Habrard. Mapping estimation for\n\ndiscrete optimal transport. Advances in Neural Information Processing Systems, 29, 2016.\n\nGabriel Peyr ́e, Marco Cuturi, and Justin Solomon. Gromov-wasserstein averaging of kernel and distance matrices. In International Conference on Machine Learning, pp. 2664–2672. PMLR, 2016.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nBen Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational In International Conference on Machine Learning, pp. 5171–\n\nbounds of mutual information. 5180. PMLR, 2019.\n\nIevgen Redko, Nicolas Courty, R ́emi Flamary, and Devis Tuia. Optimal transport for multi-source domain adaptation under target shift. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 849–858. PMLR, 2019.\n\nMurray Rosenblatt. Remarks on some nonparametric estimates of a density function. The annals of\n\nmathematical statistics, pp. 832–837, 1956.\n\nSalem Said, Lionel Bombrun, Yannick Berthoumieu, and Jonathan H Manton. Riemannian gausIEEE Transactions on\n\nsian distributions on the space of symmetric positive definite matrices. Information Theory, 63(4):2153–2170, 2017.\n\nVivien Seguy, Bharath Bhushan Damodaran, R ́emi Flamary, Nicolas Courty, Antoine Rolet, and arXiv preprint\n\nMathieu Blondel. Large-scale optimal transport and mapping estimation. arXiv:1711.02283, 2017.\n\nJustin Solomon, Gabriel Peyr ́e, Vladimir G Kim, and Suvrit Sra. Entropic metric alignment for\n\ncorrespondence problems. ACM Transactions on Graphics (ToG), 35(4):1–13, 2016.\n\nMarlon Stoeckius, Christoph Hafemeister, William Stephenson, Brian Houck-Loomis, Pratip K Chattopadhyay, Harold Swerdlow, Rahul Satija, and Peter Smibert. Simultaneous epitope and transcriptome measurement in single cells. Nature methods, 14(9):865–868, 2017.\n\nBaochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In\n\nProceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.\n\nShuhan Tan, Xingchao Peng, and Kate Saenko. Class-imbalanced domain adaptation: An empirical\n\nodyssey. In European Conference on Computer Vision, pp. 585–602. Springer, 2020.\n\nEric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:\n\nMaximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.\n\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\n\nlearning research, 9(11), 2008.\n\nTitouan Vayer, Laetita Chapel, R ́emi Flamary, Romain Tavenard, and Nicolas Courty. Fused theoretical foundations and mathematical\n\ngromov-wasserstein distance for structured objects: properties. arXiv preprint arXiv:1811.02834, 2018a.\n\nTitouan Vayer, Laetitia Chapel, R ́emi Flamary, Romain Tavenard, and Nicolas Courty. Optimal transport for structured data with application on graphs. arXiv preprint arXiv:1805.09114, 2018b.\n\nTitouan Vayer, Laetitia Chapel, R ́emi Flamary, Romain Tavenard, and Nicolas Courty. Optimal transport for structured data with application on graphs. In ICML 2019-36th International Conference on Machine Learning, pp. 1–16, 2019.\n\nC ́edric Villani. Optimal transport: old and new, volume 338. Springer, 2009.\n\nJindong Wang, Wenjie Feng, Yiqiang Chen, Han Yu, Meiyu Huang, and Philip S Yu. Visual domain In Proceedings of the 26th ACM\n\nadaptation with manifold embedded distribution alignment. international conference on Multimedia, pp. 402–410, 2018.\n\nRenjun Xu, Pelen Liu, Liyan Wang, Chao Chen, and Jindong Wang. Reliable weighted optimal transport for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4394–4403, 2020.\n\nMikhail Yurochkin, Amanda Bower, and Yuekai Sun. Training individually fair ml models with\n\nsensitive subspace robustness. arXiv preprint arXiv:1907.00020, 2019.\n\nErheng Zhong, Wei Fan, Qiang Yang, Olivier Verscheure, and Jiangtao Ren. Cross validation framework to choose amongst models and datasets for transfer learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 547–562. Springer, 2010.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS\n\nA.1 PROOF OF LEMMA 2\n\nProof. In the limit when h → 0, the Gaussian kernel converges to\n\nKh (t) =\n\n(cid:26)1/Zh 0\n\nif t = 0 otherwise.\n\nTherefore, the kernel Kh(dX (xi, xk)) will only have non-zero value when xi = xk, which implies that the kernelized mutual information will converge as follows:\n\nlim h→0\n\nˆIΓ(X, Y ) = lim\n\nh→0\n\n(cid:88)\n\ni,j\n\nΓij log\n\nn2 · (cid:80) (cid:80)\n\nk,l ΓklKh (dX (xi, xk)) Kh (dY (yj, yl))\n\nk Kh (dX (xi, xk)) · (cid:80)\n\nl Kh (dY (yj, yl))\n\n.\n\nΓij log\n\nn2 · Γij/Z 2 1/Z 2 h\n\nh\n\nΓij log Γij + 2 log(n)\n\n=\n\n=\n\n(cid:88)\n\ni,j (cid:88)\n\ni,j\n\n= −H(Γ) + 2 log(n).\n\nA.2 PROJECTED GRADIENT DESCENT\n\nThe classic mirror descent iteration is written as:\n\nxt+1 ← arg min\n\nx\n\n{τ ⟨∇f (xt), x⟩ + D(x∥xt)} .\n\nWhen D(y||x) is the KL divergence: DKL(y||x) = (cid:80)\n\ni yi log yi\n\nxi\n\n, the update has the following form:\n\n(xt+1)i = elog(xt)i−τ ∇f (xt) = (xt)ie−τ ∇f (xt).\n\nIn our case, before the projection, the update reads\n\n(cid:16)\n\nΓ′\n\nt+1 =\n\nΓt ⊙ e−τ (C−λ∇Γt\n\nˆIΓt (X,Y )−ε∇H(Γt))\n\n(cid:17)\n\n.\n\nNext, we solve the following projection w.r.t. KL metric:\n\nΓt+1 = arg min Γ∈Π(p,q)\n\nDKL(Γ∥Γ′\n\nt+1).\n\nAs Benamou et al. (2015) shows, the KL projection is equivalent to solving the entropic regularized optimal transport problem, which is usually refer to the sinkhorn distance (Cuturi, 2013). Following (Peyr ́e et al., 2016), we set the stepsize τ = 1/ε to simplify the iterations and reach the following update rule:\n\nΓt+1 ← arg min Γ∈Π(p,q)\n\n(cid:68)\n\nΓ, C − λ∇Γ ˆIΓt(X, Y )\n\n(cid:69)\n\n− εH(Γ).\n\nB ADDITIONAL EXPERIMENTS\n\nB.1 COLOR TRANSFER\n\nColor transfer aims to transfer the colors of the target images into the source image. Optimal transport achieves this by treating pixels as points in the RGB space, and maps the source pixels to the target ones. Here, 500 pixels are sampled from each image to compute the OT, then the barycentric projection is applied to map all the source pixels to target. We compare fused InfoOT with standard OT, Sinkhorn distance (Cuturi, 2013), and linear mapping estimation (Perrot et al., 2016) and show the results in Figure 7. We can see that InfoOT produces a sharper results than the baselines while decently recovering the colors in the target image.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Color Transfer via Optimal Transport. Fused InfoOT produces sharper results while preserving the target color compared to the baselines.\n\nEN-ES\n\nEN-FR\n\nEN-DE\n\nEN-IT\n\nEN-RU\n\nSupervision →\n\n← →\n\n← →\n\n← →\n\n← →\n\nPROCRUSTES Adv-NN InvOT InfoOT (h=0.55)\n\nGW\n\n5K Words None None None\n\nNone\n\n81.2 81.7 81.3 81.6\n\n84.3\n\n82.3 83.3 81.8 78.5\n\n83.2\n\n81.2 82.3 82.9 82.4\n\n84.8\n\n82.2 82.1 81.6 80.5\n\n83.6\n\n73.6 74.0 73.8 75.4\n\n77.4\n\n71.9 72.2 71.1 74.2\n\n75.2\n\n76.3 77.4 77.7 78.6\n\n82.5\n\n75.5 76.1 77.7 75.7\n\n79.8\n\n51.7 52.4 41.7 48.1\n\n52.0\n\n←\n\n63.7 61.4 55.4 52.9\n\n61.4\n\nTable 4: Cross-lingual Word Alignment. The InfoOT achieves comparable performance to GW, demonstrating its potential in recovering cross-lingual correspondence.\n\nB.2 WORD EMBEDDING ALIGNMENT\n\nHere, we explore the possibility of applying InfoOT for unsupervised word embedding alignment. We follow the setup in (Alvarez-Melis & Jaakkola, 2018), where the goal is to recover cross-lingual correspondences with word embedding in different languages. In this case, the pairwise distance between domains might not be meaningful, as the word embedding models are trained separately. Previous works suggest that cross-lingual word vector spaces are approximately isometric, which makes Gromov-Wasserstein an ideal choice due to its ability to align isometric spaces. Here, we treat GW as the oracle, and show that InfoOT can perform comparably to GW (Alvarez-Melis & Jaakkola, 2018) and other baselines such as InvOT (Alvarez-Melis et al., 2019), Adv-NN (Conneau et al., 2017), and supervised PROCRUSTES. We report the results on the dataset of Conneau et al. (2017) in Table 4, where both GW and InfoOT are trained with 12000 words and refined with Cross-Domain Similarity Scaling (CSLS) (Conneau et al., 2017). The entropy regularizer is 0.0001 and 0.02 for GW and InfoOT, respectively. We can see that InfoOT performs comparably with the baselines and GW, demonstrating its applicability in recovering cross-lingual correspondence.\n\nB.3 DIFFERENT HYPERPARAMETER FOR INFOOT\n\nHere, we report the performance of InfoOT with different weights for entropic regularizer and mutual information on domain adaptation. As Table 6 shows, Fused-InfoOT performs consistently well across different hyperparameter selections.\n\nB.4 ADDITIONAL BASELINE: UNBALANCE OT\n\nIn this section, we additional include the results of unbalanced OT (UOT) Chizat et al. (2018); Frogner et al. (2015), which solves the following constrain optimization problem with generalized Sinkhorn-Knopp matrix scaling algorithm:\n\nmin Γ∈Π(p,q)\n\n⟨Γ, C⟩ + εmDKL(Γ1, p) + εmDKL(ΓT 1, q) − εH(Γ).\n\nWe show the best results of UOT in Table 5 by selecting ε and εm within (1, 5, 10) and (0.5, 1, 10). We can see that InfoOT still outperform UOT by a non-trivial margin.\n\nF-InfoOT∗\n\nUOT\n\nC→D 87.5±7.8 C→W 81.0±6.7 C→A 90.6±2.0 D→W 93.3±4.7 89.8±1.9 D→A D→C 80.8±1.8 W→D 89.4±11.8 74.4±3.7 W→C 89.3±2.3 W→A A→D 81.3±9.8 A→W 87.0±4.6 81.2±3.6 A→C\n\n81.9±11.9 77.3±6.4 87.8±3.5 93.3±5.7 87.8±3.2 78.8±2.7 98.8±2.6 76.7±4.3 80.1±3.3 76.3±8.2 69.3±8.6 77.4±3.7\n\nAVG\n\n85.6±5.6\n\n82.1±8.3\n\nTable 5: Unbalanced OT.\n\n15\n\nSourceTargetSinkhornOTF-InfoOTLinearUnder review as a conference paper at ICLR 2023\n\n(λ, ε)\n\n(100, 1)\n\n(100, 10)\n\n(100, 20)\n\n(10, 1)\n\n(200, 1)\n\nC→D 87.5±7.8 C→W 81.0±6.7 C→A 90.6±2.0 D→W 93.3±4.7 89.8±1.9 D→A D→C 80.8±1.8 W→D 89.4±11.8 W→C 74.4±3.7 89.3±2.3 W→A A→D 81.3±9.8 A→W 87.0±4.6 A→C 81.2±3.6\n\n86.3±8.7 86.7±7.7 90.5±2.1 92.7±6.0 89.9±2.1 81.2±1.9 86.9±10.4 74.2±3.7 89.3±2.0 80.6±9.5 83.3±6.3 80.8±4.0\n\n85.0±8.9 88.0±5.9 90.7±2.1 91.3±5.3 89.6±1.9 81.5±1.6 83.8±11.5 74.0±3.4 89.3±2.0 82.5±11.7 83.0±6.0 80.2±3.9\n\n87.5±7.8 80.0±6.1 89.4±2.4 93.3±5.7 89.6±1.7 80.7±1.8 91.9±12.2 74.2±4.6 86.4±2.8 81.9±10.4 83.8±6.5 81.2±3.3\n\n86.9±8.0 81.7±7.2 90.6±2.0 94.0±4.4 89.8±1.5 80.6±1.8 90.0±11.9 74.4±3.8 89.3±2.3 82.5±9.2 87.0±4.6 82.2±2.7\n\nAVG\n\n85.6±5.6\n\n85.2±5.3\n\n84.9±5.1\n\n85.0±5.6\n\n85.7±5.5\n\nTable 6: InfoOT with different hyperparameters. We test the Fused-InfoOT with conditional projection by varying the regularizer weights (λ, ε). Note that Table 1 in the main paper shows the results of (λ = 100, ε = 1).\n\n1-NN\n\n5-NN\n\n10-NN\n\n20-NN\n\nLinear\n\nOT Sinkhorn GL-OT FGW Linear F-InfoOT F-InfoOT∗\n\n71.0±8.8 72.4±7.3 78.9±7.3 71.0±8.8 70.5±8.4 80.6±5.7 85.5±5.6\n\n77.0±6.4 76.0±5.3 80.7±5.3 76.9±6.5 75.9±6.2 81.4±5.8 85.4±5.5\n\n78.3±4.8 76.3±4.0 80.5±4.3 78.3±4.8 77.4±5.4 79.7±5.0 85.4±5.5\n\n77.5±6.8 75.2±6.3 78.2±7.1 77.5±6.8 77.5±7.1 76.4±7.1 81.7±7.2\n\n77.8±8.6 76.7±9.8 78.1±9.7 77.5±7.9 76.7±7.5 82.9±7.0 81.4±5.3\n\nTable 7: Results beyond 1-NN. We evaluate the performance with k-NN classifiers and linear classifiers.\n\nGFK CORAL\n\nSCA JDA TJM DDC DAN MEDA F-InfoOT∗\n\nC→D 86.6 C→W 77.6 C→A 88.2 D→W 99.3 D→A 76.3 D→C 71.4 W→D 100 W→C 69.8 W→A 76.8 A→D 82.2 A→W 70.9 A→C 79.2\n\nAVG\n\n81.5\n\n84.7 80.0 92.0 99.3 85.5 76.8 100 75.5 81.2 84.1 74.6 83.2\n\n84.7\n\n87.9 85.4 89.5 98.6 90.0 78.1 100 74.8 86.1 85.4 75.9 78.8\n\n85.9\n\n89.8 85.1 89.6 99.7 91.7 85.5 100 84.8 90.3 80.3 78.3 83.6\n\n88.2\n\n84.7 81.4 88.8 99.3 90.3 83.8 100 83.0 84.6 76.4 71.9 84.3\n\n86.0\n\n88.8 85.4 91.9 98.2 89.5 81.1 100 78.0 84.9 89.0 86.1 85.0\n\n88.2\n\n89.3 90.6 92.0 98.5 90.0 80.3 100 81.2 92.1 91.7 91.8 84.1\n\n90.1\n\nTable 8: Baselines beyond OT.\n\n91.1 95.6 93.4 97.6 93.2 87.5 99.4 93.2 99.4 88.1 88.1 87.4\n\n92.8\n\n87.9 85.8 91.1 97.3 91.3 82.9 96.2 80.3 90.0 81.5 85.4 82.5\n\n87.7\n\nB.5 EXPERIMENTS BEYOND 1-NN CLASSIFIER\n\nWe report the performances of InfoOT and baselines with general k-NN classifiers and linear SVM classifiers in Table 7. We can see that fused-InfoOT consistently outperforms the baselines beyond 1-NN classifiers on Office-Caltech domain adaptation benchmark. In addition, compared to the baselines, the performance of InfoOT is more robust to the choice of the number of neighbors k.\n\nB.6 BASELINES BEYOND OPTIMAL TRANSPORT\n\nWe compare InfoOT with the following non-OT baselines: Geodesic Flow Kernel (GFK) (Gong et al., 2012), CORrelation Alignment (CORAL) (Sun et al., 2016), Scatter Component Analysis (SCA) (Ghifary et al., 2016), Joint distribution alignment (JDA) (Long et al., 2013), Transfer Joint\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nMatching (TJM) (Long et al., 2014a), Deep Domain Confusion (DDC) (Tzeng et al., 2014), Deep Adaptation Network (DAN) (Long et al., 2014b), and Manifold Embedded Distribution Alignment (MEDA) (Wang et al., 2018). For fair comparison, we report the performance of Fused-InfoOT calculated with full source and target dataset instead of the 10-fold setting in the main context. As Table 8 shows, InfoOT performs comparably to many baselines without training or finetuning neural networks.\n\nC LIMITATIONS\n\nWhile we have illustrated successful applications of InfoOT, there are limitations. One could expect InfoOT to perform worse when the geometry of input spaces provides little information. In particular, for raw inputs such as image datasets, InfoOT would not perform well without pre-extracted features. It is also non-trivial to directly apply InfoOT to very large-scale problems with millions of data points. Computational-efficient extensions such as mini-batch optimal transport (Nguyen et al., 2022) should be considered to apply InfoOT to large-scale datasets.\n\n17",
    "reference": "# Summary Of The Paper\n\nThe authors propose InfoOT which maximizes the mutual information between domains and minimize distances between input distributions. The proposed InfoOT address several drawbacks of OT, e.g., coherence structure (clustering, outliers) and easy to integrate new data points. Empirically, the authors evaluate the proposed method on domain adaption, cross-domain retrieval and single-cell alignment.\n\n# Strength And Weaknesses\n\nStrength\n+ The proposed InfoOT address several drawbacks of OT \n+ The proposed method works well in applications.\n\nWeaknesses\n+ The advantage of the proposed method (using kernel density estimation for continuous setting) is not clear enough over discrete ones (e.g., entropic regularization in Sinkhorn), especially in the case input distributions are discrete (empirical distributions). It will be a plus if the authors elaborate this points with more details.\n+ It is unclear how many samples are required for the kernel density estimation used in InfoOT\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe proposed ideas are interesting. The proposed method, InfoOT can address several drawbacks from OT, especially about coherence structure (cluster, outlier) and the ability to integrate new data points.\n\nI have some following concerns:\n+ In case the input distributions are discrete, e.g., empirical distributions. It is not clear the advantages of using kernel density estimation (as in the proposed method) comparing to the entropic regularization (in Sinkhorn) for measuring global structure with mutual information (as in Section 4.1). Could the authors elaborate it with more details?\n\n+ How many samples are required for the kernel density estimation (KDE)? and how to choose the bandwidth for the Gaussians used in KDE for the proposed InfoOT? especially for high-dimensional setting?\n\n+ Could the author discuss the relation between the proposed InfoOT with Liu'2021 which is also based on mutual information and OT from given unpaired data?\n\n+ For the robustness against noisy data, it is better if the authors compare the proposed method with the unbalanced OT approach (which also use to address this problem for OT). Could the authors discuss about it (and better to have some empirical comparison)?\n\n+ In experiments, it is well-known that the entropic regularization affects performances of entropic OT, why the authors set it to 1 in experiments?\n--- How the \\lambda in Fused InfoOT affects its performances in applications? Why the authors set it to 100? Should one need to use \\lambda to control the effect of the regularization?\n\n# Summary Of The Review\n\nThe proposed method is interesting. The proposed methods address several drawbacks of OT.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nBATCH NORMALIZATION AND BOUNDED ACTIVATION FUNCTIONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nSince Batch Normalization was proposed, it has been commonly located in front of activation functions, as proposed by the original paper. Swapping the order, i.e., using Batch Normalization after activation functions, has also been attempted, but it is generally not much different from the conventional order when ReLU is used. However, in the case of bounded activation functions like Tanh, we discovered that the swapped order achieves considerably better performance on various benchmarks and architectures than the conventional order. We report this remarkable phenomenon and closely examine what contributes to this performance improvement in this paper. One noteworthy thing about swapped models is the extreme saturation of activation values, which is usually considered harmful. Looking at the output distribution of individual activation functions, we found that many of them are highly asymmetrically saturated. The experiments inducing a different degree of asymmetric saturation support the hypothesis that asymmetric saturation helps improve performance. In addition, we found that Batch Normalization after bounded activation functions has another important effect: it relocates the asymmetrically saturated output of activation functions near zero. This enables the swapped model to have higher sparsity, further improving performance. Extensive experiments with Tanh, LeCun Tanh, and Softsign show that the swapped models achieve improved performance with a high degree of asymmetric saturation.\n\n1\n\nINTRODUCTION\n\nBatch Normalization (BN) has become a widely used technique in deep learning. It was proposed to address the internal covariate shift problem by maintaining a stable output distribution among layers. The characteristics of the output distribution of weighted summation operation, which is a symmetric, non-sparse, and “more Gaussian” (Hyv ̈arinen & Oja, 2000), Ioffe & Szegedy (2015) placed the BN between the weight and activation function. Thus, the “weight-BN-activation” order, which we call “Convention” in this paper, has been widely used to construct one block in many architectures (Simonyan & Zisserman, 2014; Howard et al., 2017). “Swap” models, swapping the order of BN and the activation function in a block, have been also attempted but no significant and consistent difference between the two orders has been observed in the case of ReLU. For instance, Hasani & Khotanlou (2019) evaluated the effect of position of BN in terms of training speed and concluded that there is no clear winner and the result depends on the datasets and architecture types.\n\nHowever, in the case of bounded activation functions, we empirically found that Swap order exhibits substantial improvements in test accuracy than the Convention order with diverse architectures and datasets. We investigate the reason for this accuracy difference between the Convention and the Swap model with bounded activation function based on empirical analysis. For simplicity, our analyses are mainly conducted on Tanh model, but applicable to similar antisymmetric and bounded activation functions. We presents the results with LeCun Tanh and Softsign at the end of the experimental section.\n\nOne key difference between Swap and Convention models is the distribution of activation values, as shown in Figure 1. In the Swap model, most activation values are near the asymptotic values of the bounded activation function, that is, highly saturated. This is unanticipated since it is a common belief that high saturation should be avoided. To investigate this paradox, we took one step further\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The activation distributions of a layer are almost symmetric (left) in both Convention and Swap models with Tanh. However, the activation distributions of channels in the layer are quite different. Symmetric distributions similar to that of the layer appeared similar to layer distribution in channels in the Convention model (right top). On the other hand, the Swap model have a onesided distribution of boundary (bottom right). We chose ten consecutive channels from the 8th layer of the VGG16 model trained on CIFAR-100.\n\nand looked at the output distribution of individual activation functions, not just a whole layer. To our very surprise, even though the distribution is fairly symmetric at the layer level, the activation values of each channel are biased toward either one of the asymptotic values, or asymmetrically saturated. We assume that this asymmetric saturation is a key factor for the performance improvement of the Swap model since it enables Tanh to behave like a one-sided activation function. In the experiments we designed to examine whether asymmetric saturation is related to the performance of models with bounded activation functions, we can observe that the accuracy and the degree of asymmetric saturation are highly correlated.\n\nBN after Tanh does not just incur asymmetric saturation but also shifts the biased distribution near zero, which has the important effect of increasing sparsity. Sparsity is generally considered to be a desirable property. For instance, Glorot et al. (2011) studied the benefits of ReLU compared to Tanh in terms of sparsity. One thing to note is that if each channel is symmetrically saturated, BN will not increase sparsity much since the mean is already close to 0. In contrast, the one-sided property of asymmetric saturation causes at least half of the sample values after normalization to be almost zero, allowing the Swap model to have even higher sparsity than the Convention model. Ramachandran et al. (2017) explored novel activation functions by an automatic search for different activation functions. The top activation functions found by search are one-sided, and the boundary value is near zero, similar to ReLU. The penalized Tanh activation (Xu et al., 2016), inserting leaky ReLU before Tanh, also introduces skewed distribution, and the penalized Tanh achieved the same level of generalization as ReLU-activated CNN. Analogous to the activation functions found in the previous studies, asymmetric saturation combined with normalization makes a bounded activation function behave much like ReLU, achieving comparable performance.\n\nOur findings are as follows:\n\n• The Swap model using Batch Normalization after bounded activation functions performs\n\nbetter than the Convention model in many architectures and datasets.\n\n• We discover the asymmetric saturation at the channel level and investigate its importance\n\nthrough carefully-designed experiments.\n\n• We identify the high sparsity induced by Batch Normalization after bounded activation functions and perform an experiment to examine the impact of sparsity on performance.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Illustration of Block designs of the Convention order (left) and Swap order (right), and locations for property measurement.\n\n2 SETTINGS FOR INVESTIGATION AND NOTATION\n\nModels. The main purpose of the investigation is to analyze the benefits of using BN after bounded activation functions, more specifically, a bounded activation function that is an odd function and has two boundaries. We examine the VGG-like model trained on CIFAR-100 with replacing the activation function from ReLU to Tanh. However, because the VGG architecture was proposed for the ImageNet dataset, the model is overparameterized for the CIFAR dataset. It incurs poor performance and difficulty to investigate the Swap model. Thus, we cut out the last convolution layers and select the best model based on the validation accuracy. The model with five cut-out layers shows the best accuracy as in Appendix A.6. We call this model “VGG16 11” and use this architecture to investigate Conv and Swap orders. Although a VGG11 model has already been proposed in Simonyan & Zisserman (2014), the validation accuracy of VGG16 11 is significantly higher than VGG11 (VGG11: Conv 64.55%, Swap 69.94, VGG16 11: Conv 69.5%, Swap 74.11% ). At inference time, The BN normalizes the input distribution to have zero-mean and unit-variance by using the running statistics (e.g., ˆμ for running mean and ˆσ for related to running variance), and then applies the affine transformation, which has a scaling parameter γ and a shifting parameter β. The Convention model normalizes the outputs of the weighted summation operation conducted in the weight layer, and then Tanh activates the block outputs. On the other hand, in the Swap model, Tanh directly activates the weight layer outputs, and then BN is applied to generate block outputs.\n\nMetrics. We consider 3 properties to investigate each order: saturation, asymmetric saturation, and sparsity. We measure the degree of saturation at the outputs of Tanh in the layer units. To measure the asymmetric saturation, we collect the outputs of Tanh in channel units. For the sparsity measure, we collect the outputs of each block in the channel units. Layer structure and measurement locations are illustrated in Figure 2.\n\nSetups for experiment. For the experiment in Section 4.2, the weight decay on the convolution layer is fixed, and we vary the weight decay intensity on BN. This experiment’s learning rate and the convolution layer’s weight decay followed the NWDBN model’s hyperparameters. NWDBN is the Convention based model, but the affine parameters of BN are zero. Based on these hyperparameters, we increase the intensity of weight decay on β in BN from 0.0 to 0.001 by 0.0001. For the experiment in Section 5.3, the learning rate and convolution layer’s weight decay followed the Swap model’s hyperparameters. Then, we change the weight decay intensity on the affine transformation parameters in BN. The intensity list of weight decay are 0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, and 5e-4. For the experiment in Section 7.1, we train models on 4 benchmarks (CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet), 2 base-architectures (VGG16 11, MobileNet), and 2 activation functions (ReLU, Tanh). Because Tanh has non-linearity in everyplace except the origin, it can not follow the design of residual connection proposed in He et al. (2016). Thus, we choose architectures where a skip connection does not exist. For the experment in Section 7.2, we trained VGG16 11 with 3 activation functions (Tanh, Lucun Tanh, Softsign) on CIFAR-100 dataset. All results except the ImageNet dataset are conducted on 3 random seeds and averaged over seeds for all the measure values and accuracy. We use the SGD optimizer, weight decay regularization, and a 2-step learning rate decaying strategy that decays by 0.1. We conduct a grid search to obtain the best model for investigation. We explore learning rate and weight decay. The hyperparameters that we use are demonstrated in Appendix A.1.\n\n3\n\nWeightBNTanhWeightTanhBNAsymmetric saturationSaturationSparsityPROPERTIES TO MEASUREUnder review as a conference paper at ICLR 2023\n\n3 OVERLY SATURATED TANH BUT WELL-GENERALIZED MODEL\n\nSaturation refers to a situation where most of the outputs of bounded activation functions are close to the asymptotic value of the function. When training a neural network with a bounded activation function whose center is the origin, the output increases due to the weight gradually increasing. The increased output values map close to the near asymptote in bounded activation functions, as shown in the experiment in Glorot & Bengio (2010). Thus, saturation is bound to occur. However, excessive saturation results in a gradient vanishing problem. The gradient of points near the asymptotic values is almost 0. Therefore, the gradients of saturated activations vanished. Various methods were proposed to prevent excessive saturation. Glorot & Bengio (2010) proposed an initialization scheme, Rakitianskaia & Engelbrecht (2015a;b) proposed a metric to measure the degree of saturation for monitoring the training, Bhat et al. (1990) pre-scaled the inputs of the activation function, and Chen & Chang (1996) proposed adaptable bounded activation.\n\n3.1 SATURATION METRIC\n\n1, gl\n\n2...gl\n\nWe introduce a saturation metric based on how closely outputs the values to the maximum absolute value of the output range of the function. The target outputs for measuring the saturation N ] ∈ RN is the flattened outputs of lth layer in fully-connected block or convoluGl = [gl tion block. N is SDl for fully-connected blocks and SC lH lW l for convolution blocks, where S denotes the total number of test samples, Dl denotes the dimension size of layer outputs in lth fullyconnected block, and C l, H l, W l respectively denotes the number of channels, height, and width in lth convolution block. We take the absolute value of the input and divide it by the maximum absolute value to normalize it to [0, 1]. The formulation for normalization of ith element in lth layer feature map, ˆgl\n\ni, is as follows:\n\nˆgl\n\ni =\n\n|gl i| ̃gl ,\n\n(1)\n\nwhere ̃gl ∈ R is the maximum absolute value of Gl. Since the possible output range of the bounded function is fixed. We use the absolute asymptotic value of the bounded function as a all element of ̃gl for measuring saturation. For instance, we set gl d to 1 for the Tanh model. We averaged all the normalized values in a layer for our saturation metric. The formulation of our saturation metric on lth layer, tl, is as follows:\n\ntl =\n\ni=1ˆgl ΣN N\n\ni\n\n.\n\n(2)\n\ntl has the range of [0, 1]; it approaches 1 if Gl is highly saturated as illustrated in Appendix A.3. Also, as an implementation issue, the calculation was performed in units of mini-batch, and the details are described in appendix A.11.\n\n3.2 HIGH SATURATION IN THE SWAP MODEL\n\nEven if only the layer order was changed from the Convention order to the Swap order, there was a 4.61%p test accuracy improvement. The results of this model and other models can be found in Table 1. However, when we measure the layer saturation in both models, the Swap model has highly saturated layers. The maximum saturation of the Swap model (0.86) is significantly higher than the Convention model (0.45). The saturation of the Swap model shows over 0.7 in almost half of the layers. Even more, some layers are overly saturated at almost 0.86. On the other hand, the saturation of the Convention model is lower than 0.5 over all layers. (Figure 3) This is counterintuitive as excessive saturation is considered an undesirable situation in the previous works.\n\n4\n\nFigure 3: Layer Saturation of Convention and Swap models\n\n123456789Block depth0.00.10.20.30.40.50.60.70.80.91.0SaturationConventionSwapUnder review as a conference paper at ICLR 2023\n\n4 ASYMMETRIC SATURATION\n\nOur saturation metric can dismiss the channel properties due to the summarization of channels in the layer. Thus, we conduct channel inspection. Interestingly, when we examine channel distribution, the saturation in that layer has biased to one asymptotic value. Asymmetric saturation appears in most channels on the excessively saturated layer in the Swap model. In contrast, the channel distribution of the Convention is almost zero centralized.\n\n4.1 ASYMMETRIC SATURATION METRIC\n\nThe target outputs for measuring the asymmetry Ql,c = [ql,c M ] ∈ RM is the flattened activation outputs of lth layer and cth dimension for fully-connected block or cth channel for convolution block. M is S for fully-connected blocks and SH lW l for convolution blocks. To measure the channel asymmetry more precisely, we introduce skewness, the metric for measuring the asymmetry. The formulation of the sample skewness for lth layer and cth channel , kl,c, is as follows:\n\n2 ...ql,c\n\n1 , ql,c\n\nkl,c =\n\n(cid:112)M (M − 1) M − 2\n\n1\n\nM ΣM [ 1 M ΣM\n\ni=1(ql,c i=1(ql,c\n\ni − μc)3 i − μc)2] 3\n\n2\n\n,\n\n(3)\n\nwhere μc ∈ R is the mean of lth layer and cth channel’s activation outputs. The skewness value has directional distribution information, negative for left-skewed and positive for right-skewed. However, we want to measure asymmetry regardless of direction. Thus we take the absolute value to remove the directional information. The metric for the layer skewness, kl, is as below:\n\nkl =\n\n1 C\n\nΣC\n\ni=1|kl,i|.\n\n(4)\n\nThe layer distributions in both Convention and Swap models are symmetry, but the channel distributions are quite different. Thus, we measure the asymmetry on channel-wise, not layer-wise, like the saturation metric. As an implementation issue, the calculation was performed in units of mini-batch, and the details are described in appendix A.11.\n\nAs shown in Figure 4, All of the layer skewness in the Convention model measured close to 0. Therefore there has little asymmetric distribution. However, in the Swap model, the skewness of layers is relatively higher than in the Convention model. Furthermore, the skewness values are high along the high saturation blocks. It, therefore, implies that saturation occurs with asymmetry. The relationship between our skewness metric and the different distribution shapes is illustrated in Appendix A.3.\n\n4.2 EFFECT OF ASYMMETRIC SATURATION ON GENERALIZATION PERFORMANCE\n\nFigure 4: Layer Skewness in Convention and Swap models\n\nIn order to demonstrate the effectiveness of asymmetric saturation, we introduce a method to control the level of asymmetry in the Convention model. First, let us organize the reason why the Convention model cannot make use of asymmetric saturation. We assume that the Convention model can not generate asymmetric saturation well due to the weight decay effect on affine transform parameters in BN. In the experiment to verify the mean and variance effects on skewness, we can confirm that both statistical values, the mean and variance of Tanh input, affect asymmetry on Tanh output. The skewness value of Tanh’s output on the different input mean and standard deviation can be found in Appendix A.4. From this perspective, the affine parameters with weight decay generate the input of Tanh to utilize the center of Tanh by decreasing the mean and variance of its input. Thus, it could decrease the asymmetry of the Tanh output. Therefore, we train a model with no weight decay on BN to encourage asymmetric saturation in the Convention model. As a result, the NWDBN model shows improved accuracy of 72.27% compared to the Convention model 69.5%. To closely examine the effects of asymmetric saturation on test accuracy,\n\n5\n\n123456789Block depth0.01.02.03.04.05.0SkewnessConventionSwapUnder review as a conference paper at ICLR 2023\n\nFigure 6: Shapes of combined Tanh with normalization functions, and samples related to BN statistics. The functions are plotted as lines, and the samples are plotted as dots. We choose some normal distributions whose samples generate ˆμ and ˆσ after the Tanh and randomly generate input samples for Tanh. Note that the ˆμ and ˆσ are the statistics of Tanh output in the Swap order.\n\nwe increase the intensity of weight decay on the Beta parameter, which can eliminate the biasing of the asymmetric saturation in the NWDBN model. As shown in Figure 5, increasing weight decay intensity decreases the skewness in the NWDBN model. Additionally, the test accuracy decreased along with the skewness.\n\n5 SPARSITY\n\n5.1 ASYMMETRIC SATURATION WITH BATCH NORMALIZATION CAN INDUCE HIGH\n\nSPARSITY\n\nSparsity is a desirable property in deep learning. One of the successes of the method that introduces a sparsity is the Relu. ReLU achieves a high generalization performance by utilizing the strengths of sparsity (Glorot et al., 2011). The sparsity of ReLU is due to the one boundary placed at 0. Thus ReLU activates all negative inputs to 0. The other work that shows the advantage of having one asymptote at 0 is Ramachandran et al. (2017). They conducted an automatic search strategy to look up various activation functions used. The top prominent activation functions identified through search are one-sided with a boundary value close to zero, like ReLU. Also, Xu et al. (2016) introduced penalized Tanh activation, which places leaky ReLU before Tanh to enhance the performance of Tanh, which perform as well as ReLU and introduce asymmetry in Tanh.\n\nFigure 5: Relation between accuracy and averaged skewness over layers. The ”Avg.Skewness” averaged all the layerwise skewness in each model with difThe ferent weight decay intensity. NWDBN model is denoted as 0.0 intensity in the graph.\n\nWe found that the Swap model also can increase the sparsity by shifting the majority of the values to 0 when asymmetric saturation occurs. The normalization in BN makes the distribution to be zero mean. When the asymmetric saturation occured on precede Tanh, the majority of activations are saturated on one side of Tanh output. Thus, the normalization applied on this distribution the majority of values are shifted to near zero which incurs a increasement of sparsity.\n\n5.2 SPARSITY COMPARISON\n\nThe NWDBN model shows better performance than the Convention model by inspiring the asymmetry, but it underperforms the Swap model. We found that the rise of asymmetric saturation in the NWDBN model gives a benefit in terms of asymmetry but decreases the sparsity. In other words,\n\n6\n\n1050510x10.07.55.02.50.02.55.07.5Nor,(Tanh(x))Nor,(Tanh(x)),x(,)+0.92, 0.180.85, 0.27Nor,(Tanh(x))=+0, =1+0.92, 0.180.85, 0.270.01e-042e-043e-044e-045e-046e-047e-048e-049e-041e-03Intensity of Weight decay on Beta0.20.40.60.81.01.21.41.6Avg.Skewness69.57070.57171.57272.5AccuracyAvg.SkewnessAccuracyUnder review as a conference paper at ICLR 2023\n\nincreased asymmetry of activations in the Convention model generates more activation values close to -1 or 1, which incurs less sparse block output. Based on this intuition, we hypothesize that the Swap model has strength on sparsity. To compare the models, we introduce our sparsity metric to verify the sparsity on each model.\n\nWe leverage our saturation metric and modify it for the sparsity metric. Our saturation metric measures the degree to which many values are saturated with the maximum value. On the other hand, sparsity is measured by how a small number of coefficients contain a large proportion of the energy. The more saturated the distribution, the more coefficients divide the total energy. In short, higher saturation decreases sparsity. Therefore, the sparsity metric can be regarded as the reverse of the saturation metric. However, there is differences to the saturation metric. Whereas the saturation is measured on the output of Tanh, sparsity is measured on the output of the blocks, i.e., the sparsity of the Conv model is measured on the Tanh output, and the sparsity of the Swap model is measured on the BN output. Thus, for the measuring the sparsity, we modify ̃gl in Equation 1 to the vector of maximum absolute output in unit-wise, ̄gl ∈ RDl for convolution i| by the corresponding unit value in ̄gl. The formulation of modified block. Then, we normalize |gl normalized element , ̇gl i|/ ̄gl i, is |gl d, where d is the corresponding dimension or channel index of ith output. Consequently, the modified saturation metric, ̄tl, is ΣN i/N and our sparsity metric for lth layer, sl, is 1 − ̄tl. Also, we investigated how our sparsity metric satisfies the conditions of the sparsity metric. We demonstrate our sparsity metric based on the 6 desired heuristic criteria of sparsity measures described in Hurley & Rickard (2009). Our sparsity metric satisfies 5 criteria among 6 criteria. The proof can be found in Appendix A.10.\n\nfor fully connected block and RCl\n\ni=1 ̇gl\n\nWe first measured saturation on each model’s block output to measure the sparsity and subtracted the saturation value from 1. Then, averaged the sparsity over layers. The sparsity of each model is as follow: Convention (0.717951), NWDBN (0.287974), Swap (0.848927). The Swap model shows the largest sparsity. The result also shows that the Convention model can generate sparse distribution. Because of the weight decay on BN, a zero-centered distribution insert to the Tanh in Convention model. Lastly, as we expect, the NWDBN model shows the lowest sparsity. However, Since the NWDBN model has a higher asymmetry than the Convention model, the NWDBN model can outperform the Convention model.\n\n5.3 EFFECT OF SPARSITY ON GENERALIZATION PERFORMANCE\n\nIn this section, we encourage the sparsity in the Swap model and investigate its effects on test accuracy. As mentioned in Section 5.1, the Swap order can enhance the sparsity when asymmetric saturation occurs. This sparsity can be promoted in training by affine parameters in BN. Decaying on affine parameters gathers the most values to 0 during the training phase. Note that the normalization operation shifts the majority near zero, and affine transformation imposes the majority of distribution more centered to 0. To enhance the sparsity of the Swap model, we increase the weight decay of affine transformation parameters. The larger weight decay may further increase the sparsity of BN output. As shown in Figure 7, the increase in the model’s sparsity and accuracy are highly correlated.\n\nFigure 7: Influence of sparsity on accuracy. we measure the averaged saturation over layers in the Swap model trained with each random seed and calculate the sparsity by our sparsity metric.\n\n6 SUMMARY OF THE MAIN ANALYSIS\n\nWe trained 3 types (Convention, NWDBN, Swap) of models in the above analysis experiments. Each model creates a different output distribution of layers due to differences in structure and regularization effects. Output distributions of these models are described in Figure 8. The Convention model, which is illustrated in Figure 8 (top), normalizes extracted features from the convolution\n\n7\n\n0.01e-065e-061e-055e-051e-045e-04Intensity of Weight decay on Gamma and Beta0.780.790.800.810.820.830.840.85Avg.Sparsity72.072.573.073.574.074.5AccuracyAvg.SparsityAccuracyUnder review as a conference paper at ICLR 2023\n\nFigure 8: The distribution of VGG16’s 5th block’s output on randomly chosen 3 channels. We chose a block where all 3 models were considerably saturated. All test samples in the CIFAR-100 dataset are used to construct the distribution.\n\nTable 1: Test accuracy with different activation functions and layer orders for VGG16 and MobileNet.\n\nDataset\n\nCIFAR-10 CIFAR-100 Tiny ImageNet ImageNet\n\nVGG16 Tanh\n\nMobileNet Tanh\n\nVGG16 Relu\n\nMobileNet Relu\n\nConvention 91.75 64.84 49.29 60.85\n\nSwap Convention 92.90 72.17 57.05 67.04\n\n91.54 64.47 50.85 64.26\n\nSwap 92.48 70.63 51.79 72.07\n\nConvention 93.69 73.68 61.54 73.83\n\nSwap 93.04 71.79 59.045 72.95\n\nConvention 92.2 70.06 59.79 70.48\n\nSwap 91.93 69.49 59.1 71.1\n\nlayer. After that, affine parameters are applied to the normalized features. These affine parameters generate zero centralized activation caused by the effect of weight decay. The NWDBN order also normalizes the extracted feature from convolution layer. Still, Unlike the Convention model, there are no downscaling effects on affine transform parameters. For this reason, the input distribution to Tanh can generate a distribution away from zero and produce a relatively high asymmetry distribution than the Convention model. We can observe that asymmetric saturation is generated through Tanh in Figure 8 (middle). However, the asymmetric saturation in the NWDBN model leads to low sparsity, which negates the benefits of sparsity. Far from the above models, the Swap model applied Tanh to the extracted features from convolution layer, and BatchNorm is followed. Therefore, if Tanh generates asymmetric saturation, then it could be a significant number of activations will be moved near zero, helping to increase sparsity. The layer output distribution can be found in Figure 8 (bottom).\n\n7 EXTENDED EXPERIMENTS\n\n7.1 RESULTS ON VARIOUS DATASETS AND ARCHITECTURES\n\nWe mainly investigated VGG16 11 with Tanh model trained on CIFAR-100 dataset. In this section, we adopt Swap order on varied settings, which are various datasets (CIFAR-10, CIFAR-100, Tiny ImageNet, ImageNet), architectures (VGG, MobileNet), and activation functions (ReLU, Tanh).\n\nThe Swap order and the Convention order of the ReLU model do not show a large difference in generalization performance than the difference of Tanh model, and this could be ReLU has the\n\n8\n\nBNBN w/o weight decay BNConventionNWDBNSwapLowMiddleAsymmetrySparsityMiddleLowAsymmetrySparsityHighHighAsymmetrySparsityUnder review as a conference paper at ICLR 2023\n\nTable 2: VGG16 11 with bounded activation functions on CIFAR-100, we used averaged skewness over layers for calculating the difference of skewness.\n\nActivation\n\nTanh LeCun Tanh Softsign\n\nOrder\n\nSwap - Convention\n\nConvention 69.5 67.82 70.01\n\nSwap ∆Avg.Skewness 74.11 74.46 73.65\n\n2.38 1.90 1.28\n\nstructural ability to produce asymmetric and sparse activations. However, in the case of Tanh, every model with Swap order outperforms the Convention ordered models with significant generalization improvement. The Convention order slightly performs better than the Swap order except for the ImageNet dataset on ReLU model. The Swap MobileNet with Tanh especially performs better than the Convention Mobilenet with ReLU on CIFAR and ImageNet datasets. The results can be found in Table 1. Also, all Swap models generate asymmetry on Tanh.\n\nThe asymmetric saturation tends to occur from the front layers. Also, we can find that the range of the asymmetric saturation existence block is related to the amount of dataset information and dataset resolution. For example, when comparing the CIFAR-10 and CIFAR-100, the asymmetrically saturated layers happen further back. When comparing the Tiny ImageNet, and ImageNet, the model trained on the ImageNet generates asymmetric saturation until the last convolution layer. These results are shown in Figure 9.\n\n7.2 RESULTS OF OTHER BOUNDED ACTIVATION FUNCTIONS\n\nFigure 9: Asymmetric saturation of the Swap model on various dataset. There are no BN on fully connected layer in VGG16 for Tiny ImagaNet and ImageNet dataset, we only measure the skewness on a convolution layer.\n\nOur main investigations are based on the Tanh activation function. In this section, we test whether similar behavior is observed with other activation functions, such as LeCun Tanh (LeCun et al., 2012) and Softsign (Turian et al., 2009). In detail, we use the formula of LeCun tanh as follows 1.7159×tanh( 2×input ). They are bounded and antisymmetric, just like Tanh. Softsign was proposed to prevent vanishing gradients by alleviating the saturation of neurons. It grows polynomially rather than exponentially, approaching its asymptotes much slower (Glorot & Bengio, 2010). LeCun Tanh has a gentle slope and a wider output range than Tanh. The asymmetric saturation caused by the Swap order occurs not only in Tanh but also in other activation functions. The shapes of these functions and layer skewness were shown in Appendix A.5. The Swap with Softsign and LeCun Tanh have improved performance compared to the Convention. It can be found in Table 2. When swapping, asymmetric saturation happens the least in Softsign, which makes it challenging to create a saturation state. Furthermore, the Softsign model shows lower performance than the Tanh model, which could generate more saturation with the most significant slope in the Swap, even though the Convention model had the highest performance.\n\n3\n\n8 CONCLUSION\n\nIn this work, we report that the Swap models perform better than the Convention models in many cases and analyze what brings about performance improvement. Asymmetric saturation at the channel level and sparsity induced by BN are two key factors explaining the better performance of the Swap models. With asymmetric saturation and normalization by BN, the final distributions generated by BN layers of the Swap models much resemble those by ReLU. This explains why the Swap models outperform the Convention models and often show results comparable to the ReLU models.\n\n9\n\n1234567891011121314Block depth0.02.04.06.08.010.012.0SkewnessCIFAR-10CIFAR-100TinyImageNetImageNetUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nNaveen V Bhat, Peter A Minderman, Thomas McAvoy, and Nam Sun Wang. Modeling chemical process systems via neural computation. IEEE Control Systems Magazine, 10(3):24–30, 1990.\n\nChyi-Tsong Chen and Wei-Der Chang. A feedforward neural network with function shape autotun-\n\ning. Neural networks, 9(4):627–641, 1996.\n\nXavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010.\n\nXavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks.\n\nIn Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 315–323. JMLR Workshop and Conference Proceedings, 2011.\n\nMoein Hasani and Hassan Khotanlou. An empirical study on position of the batch normalization layer in convolutional neural networks. In 2019 5th Iranian Conference on Signal Processing and Intelligent Systems (ICSPIS), pp. 1–4. IEEE, 2019.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nAndrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. ArXiv, abs/1704.04861, 2017.\n\nNiall Hurley and Scott Rickard. Comparing measures of sparsity. IEEE Transactions on Information\n\nTheory, 55(10):4723–4741, 2009.\n\nAapo Hyv ̈arinen and Erkki Oja.\n\nIndependent component analysis: algorithms and applications.\n\nNeural networks, 13(4-5):411–430, 2000.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448–456. PMLR, 2015.\n\nYann A LeCun, L ́eon Bottou, Genevieve B Orr, and Klaus-Robert M ̈uller. Efficient backprop. In\n\nNeural networks: Tricks of the trade, pp. 9–48. Springer, 2012.\n\nAnna Rakitianskaia and Andries Engelbrecht. Measuring saturation in neural networks. In 2015\n\nIEEE Symposium Series on Computational Intelligence, pp. 1423–1430. IEEE, 2015a.\n\nAnna Rakitianskaia and Andries Engelbrecht. Saturation in pso neural network training: Good or evil? In 2015 IEEE Congress on Evolutionary Computation (CEC), pp. 125–132. IEEE, 2015b.\n\nPrajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv\n\npreprint arXiv:1710.05941, 2017.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nJoseph Turian, James Bergstra, and Yoshua Bengio. Quadratic features and deep architectures for chunking. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pp. 245–248, 2009.\n\nBing Xu, Ruitong Huang, and Mu Li. Revise saturated activation functions. arXiv preprint\n\narXiv:1602.05980, 2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Training hyperparameters of the VGG16 Tanh models\n\nConvention\n\nCIFAR-10 CIFAR-100\n\nTraining Epochs Learning Rate Learning Rate Drop Weight Decay Batch Size\n\n200 0.1 100, 150 0.0001 128\n\n200 0.01 100, 150 0.0005 128\n\nTiny ImageNet 200 0.01 100, 150 0.001 128\n\nImageNet CIFAR-10 CIFAR-100 200 0.01 100, 150 0.001 128\n\n200 0.1 100, 150 0.0005 128\n\n100 0.01 30, 60 0.0001 256\n\nSwap\n\nTiny ImageNet 200 0.01 100, 150 0.001 128\n\nImageNet 100 0.01 60, 90 0.001 256\n\nTable 4: Training hyperparameters of the VGG16 ReLU models\n\nConvention\n\nCIFAR-10 CIFAR-100\n\nTraining Epochs Learning Rate Learning Rate Drop Weight Decay Batch Size\n\n200 0.01 100, 150 0.001 128\n\n200 0.01 100, 150 0.005 128\n\nTiny ImageNet 200 0.1 100, 150 0.0001 128\n\nImageNet CIFAR-10 CIFAR-100 200 0.01 100, 150 0.001 128\n\n200 0.01 100, 150 0.005 128\n\n100 0.1 30, 60 0.0001 256\n\nSwap\n\nTiny ImageNet 200 0.01 100, 150 0.001 128\n\nImageNet 100 0.01 60, 90 0.0005 256\n\nTable 5: Training hyperparameters of the MobileNet Tanh models\n\nConvention\n\nCIFAR-10 CIFAR-100\n\nTraining Epochs Learning Rate Learning Rate Drop Weight Decay Batch Size\n\n200 0.1 100, 150 0.0001 128\n\n200 0.1 100, 150 0.0005 128\n\nTiny ImageNet 200 0.01 100, 150 0.0001 128\n\nImageNet CIFAR-10 CIFAR-100 200 0.1 100, 150 0.0001 128\n\n200 0.1 100, 150 0.0005 128\n\n100 0.1 30, 60 0.0001 256\n\nSwap\n\nTiny ImageNet 200 0.1 100, 150 0.0001 128\n\nImageNet 100 0.1 60, 90 0.0001 256\n\nTable 6: Training hyperparameters of the MobileNet ReLU models\n\nImageNet CIFAR-10 CIFAR-100 200 0.01 100, 150 0.001 128\n\n200 0.01 100, 150 0.005 128\n\n100 0.01 30, 60 0.0001 256\n\nSwap\n\nTiny ImageNet 200 0.01 100, 150 0.005 128\n\nImageNet 100 0.1 60, 90 0.0001 256\n\nConvention\n\nCIFAR-10 CIFAR-100\n\nTraining Epochs Learning Rate Learning Rate Drop Weight Decay Batch Size\n\n200 0.01 100, 150 0.001 128\n\n200 0.01 100, 150 0.005 128\n\nTiny ImageNet 200 0.01 100, 150 0.005 128\n\nA APPENDIX\n\nA.1 TRAINING HYPERPARAMETER\n\nThe hyperparameters used in training are shown in Table 3, 4, 5, 6. We sweep the learning rate and weight decay hyperparameter. The learning rate was 0.1 and 0.01. For CIFAR and Tiny-ImageNet datasets, we trained models with a batch size of 128, and the learning rate was reduced by one-tenth at 100 and 150 of the total 200 epochs, and we swept 4 weight decay of 0.005, 0.001, 0.0005, and 0.0001. For ImageNet datasets, we trained models with a batch size of 256, and the learning rate was reduced by one-tenth at 30 and 60 of the total 100 epochs, and we swept 3 weight decay of 0.001, 0.0005, and 0.0001. We chose the best averaged-accuracy model for the 3 random seeds and averaged the values of these three models for all measurements for analysis. Because of the computation issue, we only use 1 seed for ImageNet dataset with early stopping.\n\nA.2 NO BN\n\nWe also compare the saturation and skewness between the Convention model and the model without BN, we call this “NoBN” model. As shown in Figure 10, asymmetric saturation also occurs in the model without BN, we call this “NoBN” model. However, the NoBN model can not utilize the advantages of batch normalization (e.g., high learning rate), it shows low test accuracy than the Convention model even though asymmetric saturation exists compared to the Convention model. The accuracy of the Convention model is 64.84% and the accuracy of the NoBN model is 61.06%.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Layer saturation (left) and skewness (right) of the Convention VGG and the NoBN VGG model trained on CIFAR-100.\n\nA.3 SATURATION AND SKEWNESS MEASUREMENT VALUES\n\nOur saturation metric becomes 0 when the distribution is gathered to 0, and it increases as the elements in the distribution close to the maximum expression range. For the uniform distribution, the degree of saturation was measured at 0.5. The measurement on different distributions can be found in Figure 11 (left). Skewness is the metric for measuring the asymmetry of the distribution. Skewness is calculated as 0 when the distribution is symmetric, and it increases as the asymmetry increase. We calculate the absolute on skewness in our asymmetry metric, thus the increases are regardless of the direction. The measurement on different distributions can be found in Figure 11 (right).\n\nFigure 11: The degree of saturation on different distributions (left) and the degree of skewness on different distributions (right)\n\nA.4 THE EFFECTS OF THE MEAN AND STANDARD DEVIATION OF INPUT DISTRIBUTION ON\n\nTANH\n\nThe mean and variance of input distribution on Tanh affect the asymmetry of Tanh output. The skewness of Tanh output depends on the mean, and standard deviation can be found in Figure 12. The maximum skewness of varied mean distribution is increased on the increase of mean. However, the maximum skewness does not align with the input standard deviation increases. The skewness decreases not only the small input standard deviation but also the large input standard deviation. Additionally, in the same mean condition, a decrease in standard deviation from the maximum skewness point more rapidly decreases the skewness than an increase in standard devation.\n\n12\n\n1234567891011121314Block depth0.00.10.20.30.40.50.60.70.80.91.0SaturationConventionNoBN1234567891011121314Block depth01234567SkewnessConventionNoBN1.00.50.00.51.0Activation value0.00.51.01.52.0DensitySaturation0.70.50.31.00.50.00.51.0Activation value02468Skewness5.02.00.0Under review as a conference paper at ICLR 2023\n\nFigure 12: The skewness of Tanh output depend on the mean and standard deviation of Tanh input\n\nA.5 SKEWNESS TENDENCIES ON VARIOUS ACTIVATION FUNCTIONS\n\nThe key to the success of the Tanh model with the Swap order is asymmetric saturation. We show that asymmetric saturation also appears in the other bounded activations, such as LeCun Tanh and SoftSign. The Conv model with the 3 types of activation functions shows low layer-wise skewness. The skewness is less than 1 over the overall layer. However, a significant skewness increment arises when the Swap order is applied. The SoftSign shows a minor improvement in skewness due to its property of preventing saturation.\n\nFigure 13: Shapes of activation functions (left) and skewness tendency of different activation functions (right), dashed line represents the Convention model and the solid line represents the Swap model.\n\nA.6 BEST DEPTH MODEL SEARCHING ON VGG16\n\nTo find an appropriate model for CIFAR, we measured the accuracy of models without the last convolution layers of VGG16. We train them from scratch using VGG16’s training hyperparameters. The accuracy gradually increases until the VGG16 11 model, and decreases after that. The results are shown in Table 7. One thing to note is that the omitted layers have a low skewness in the VGG16 model. The layer-wise skewness considerably decrease after the 8th block, which is the same number of convolution layers in the best performance model. The layer-wise skewness is shown in Figure 14. Table 7: Performance of shortened Swap VGG16 models. The number of removed convolution layers in the VGG16 n model is the difference between 16 and n.\n\nAccuracy\n\nVGG16 VGG16 15 VGG16 14 VGG16 13 VGG16 12 VGG16 11 VGG16 10 VGG16 9 VGG16 8 73.76\n\n70.91\n\n70.69\n\n73.85\n\n73.92\n\n72.57\n\n73.48\n\n72.17\n\n73.02\n\n13\n\n0.10.51.01.52.0Input Standard deviation0.00.51.01.52.02.5SkewnessInput Mean0.00.30951960.693147181.47221949505Input1.51.00.50.00.51.01.5Activation value123456789Block depth0.01.02.03.04.05.0SkewnessTanhSoftSignLecunTanhConventionSwapUnder review as a conference paper at ICLR 2023\n\nFigure 14: Skewness of the layers in the original VGG16 models\n\nA.7 THE INPUTS OF WEIGHT LAYER AND THE GRADIENTS OF TANH AND WEIGHT\n\nThe vanishing gradients problem is inevitable when excessive saturation occurs. However, the Swap model can alleviate the gradient vanishing problem. The forward propagation among the convolution and Tanh layers in the Swap model is as follows: y = W x, a = T anh(y). Here, x is a hwc-by-1 vector, and W is a d-by-n matrix, where h is the height, w is the width, c is the number of channels, d is the number of filters, and n is the size of column x, i.e., n = hwc. In backpropagation, the gradient of W is obtained by the x of the corresponding dimension element. As a result, the larger x can solve the vanishing gradients problem. The Conv block’s output is Tanh’s output in the range of [-1, 1], while the Swap block’s output can have a larger value since BN has no limit. A vanishing gradient occurs at Tanh of the Swap model in the experiment. However, it is alleviated on the gradient of convolution weight due to the large x, and shows a similar scale to the gradients of convolution weight in the Conv model. In the gradient on the shallow layers, the backpropagation gradients on Tanh of the Swap model are smaller than those of the Conv model. On the other hand, the Swap model has a larger scale of x than the Conv model. Thus, Conv and Swap models have a similar scale when looking at the gradient of the convolution weight.\n\nFigure 15: Plots for mean of absolute value of Convolution input(left) and mean of absolute gradient of Convolution weight value(center) and mean of absolute gradient of tanh input value(right) in the Convention(top) model and the Swap(bottom) model\n\n14\n\n1234567891011121314Block depth0.01.02.03.04.05.06.0SkewnessSwap0255075100125150175200Epochs0.00.51.01.52.02.53.0Mean of absolute convolution inputLayer 1 meanLayer 2 meanLayer 3 meanLayer 4 meanLayer 5 mean0255075100125150175200Epochs0.0000.0010.0020.0030.0040.0050.0060.007Mean of absolute gradient of conv weight valueLayer 1 meanLayer 2 meanLayer 3 meanLayer 4 meanLayer 5 mean0255075100125150175200Epochs0.00.51.01.52.02.53.03.54.0Mean of absolute gradient value of tanh input1e5Layer 1 meanLayer 2 meanLayer 3 meanLayer 4 meanLayer 5 mean0255075100125150175200Epochs0.51.01.52.02.53.0Mean of absolute convolution inputLayer 1 meanLayer 2 meanLayer 3 meanLayer 4 meanLayer 5 mean0255075100125150175200Epochs0.0010.0020.0030.0040.0050.0060.007Mean of absolute gradient of conv weight valueLayer 1 meanLayer 2 meanLayer 3 meanLayer 4 meanLayer 5 mean0255075100125150175200Epochs0.00.51.01.52.02.53.03.54.0Mean of absolute gradient value of tanh input1e5Layer 1 meanLayer 2 meanLayer 3 meanLayer 4 meanLayer 5 meanUnder review as a conference paper at ICLR 2023\n\nA.8 LEARNING CURVE OF CONV AND SWAP MODELS\n\nBoth models were trained with the same hyperparameters. At the beginning of training, the training loss of the Swap model decreases faster than that of the Conv model, but when training is complete, the training losses of the two models become almost the same. However, through the validation loss, we can see that the Swap model has better generalization ability. The training loss is shown in the Figure 16 and the test loss is shown in the Figure 17.\n\nFigure 16: Training loss of Conv and Swap models.\n\nFigure 17: Test loss of Conv and Swap models.\n\nA.9 RELATION BETWEEN PERFORMANCE AND SPARSITY FOR LARGE AFFINE PARAMETERS\n\nWe followed the As the size of the weight decay applied to the affine parameters increased, the sparsity decreased. Accordingly, it was confirmed that the performance also decreased.\n\nFigure 18: Accuracy drops as the sparsity decreases for large affine parameters.\n\nA.10 PROOF OF PROPERTIES FOR SPARSITY METRICS OF INVERSE SATURATION\n\nGl is a vector [g1, g2, g3, ..., gN] ˆGl is a vector [g1, g2, g3, ..., gN]\n\n15\n\n050100150200250300EPOCHS02004006008001000120014001600Train LossVGG11_SwapVGG11_Convention050100150200250300EPOCHS406080100120140Test LossVGG11_SwapVGG11_Convention1e-031e-021e-01Intensity of Weight decay on Gamma and Beta0.80.810.820.830.840.850.86Avg.Sparsity60657075AccuracyAvg.SparsityAccuracyUnder review as a conference paper at ICLR 2023\n\nTheorem 1.1: S satifies S(αGl) = S(Gl), ∀α ∈ R, α > 0.\n\nProof: scaling the Gl also scale the ̃gl.\n\n∴ ˆGl(αGl) =\n\nα|Gl| α ̃Gl =\n\nGl ̃gl\n\n= ˆGl(Gl)\n\nTheorem 1.2: S satifies S(α + Gl) < S(Gl), α ∈ R, α > 0 (We also exclude the case mentioned in Hurley & Rickard (2009) that all elements of Gl are the same.)\n\nProof:\n\nS(Gl + α) =\n\nΣN\n\ni + N α\n\ni=1gl N ̃gl + N α\n\nif N ̃gl > ΣN\n\ni=1gl\n\ni then ΣN\n\ni=1gl\n\ni+N α\n\nN ̃gl+N α > ΣN\n\ni=1gl N ̃gl\n\ni\n\n∴ S(α + Gl) < S(Gl)\n\nTheorem 1.3: S satifies S(Gl) = S(Gl||Gl||...||Gl)\n\n(|| is concatenation)\n\nProof: We define concat(X, t) which means concatenate vector X as t S(concat(Gl, t)) = tΣN\n\ni=1 ˆgl tN = S(Gl)\n\ni\n\ntimes.\n\nThen\n\nTheorem 1.4: S satifies ∀i∃β = βi > 0, such that ∀α > 0: i + β + α...]) > S([gl S([gl\n\n1...gl\n\n1...gl\n\ni + β...])\n\nWe choose sufficiently large β that |gl S([gl Then\n\ni + β...]).\n\n1...gl\n\ni| + β > ̃gl. Let assume that S([gl\n\n1...gl\n\ni + β + α...]) ≤\n\n≤ 1 −\n\nk=1gl ΣN N (gl\n\nk + β i + β)\n\n1 −\n\nΣN k=1gl N (gl k=1gl ΣN N (gl k + gl Σk̸=igl gl i + β + α\n\nk + β + α i + β + α) k + β + α i + β + α) i + β + α\n\n≥\n\n≥\n\nk=1gl ΣN k + β N (gl i + β) k + gl Σk̸=igl gl i + β Σk̸=igl gl i + β ≱ 1 gl i + β i + β + α...]) > S([gl\n\nΣk̸=igl gl i + β + α 1\ngl i + β + α\n\n1...gl\n\n≥\n\nk\n\nk\n\ni + β\n\n∴ S([gl\n\n1...gl\n\ni + β...]).\n\nTheorem 1.5: S satifies S(Gl||0) > S(Gl)\n\nProof:\n\nA.11 ALGORITHMS\n\n1 −\n\nΣN k=1ˆgl k\nN + 1\n\n> 1 −\n\nΣN\n\nk=1ˆgl N\n\nk\n\nFor more details, we take channel-wise summation with respect to batchs, but we divide the summation value by D and accumulate as batch statistics. Because the whole step is same as taking average with respect to total sample, we can divide by the total size first and sum all values as batchs later. We follow this step due to the memory usage.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Calculating skewness over the layers Input: xs(s = 1, 2, ..., S) = mini − batch ∈ Rbs×H×W\n\nN = totalnumberof samples\n\nOutput: skewness ∈ RL\n\n1 Function Statistics(X,mean,variance,skewness,condition):\n\nX = {X1, X2, ..., XL}. for l = 1, 2, ..., L do\n\nC l, bs, hl, wl is the shape of X l B = bs × hl × wl, D = N × hl × wl\n\n# RCl×B ← RCl×bs×hl×wl blockl ← f lat(X l) by channels if condition is mean then\n\n← RCl×B\n\n# RCl 1. batch statisticl ← sum of blockl along the channels 2. batch statisticl ← batch statisticl D\n3. meanl += batch statisticl\n\nend if condition is variance then ← RCl×B\n\n# RCl 1. batch statisticl ← sum of (blockl − meanl)2 along the channels 2. batch statisticl ← batch statisticl 3. variancel += batch statisticl\n\nD\n\nend if condition is skewness then ← RCl×B\n\n# RCl 1. batch statisticl ← sum of (blockl−meanl)3\n\n√\n\nalong the channels\n\nvariancel 3\n\n√\n\nD×(D−1)\n\nD−2\n\n2. batch statisticl ← batch statisticl 3. skewnessl += batch statisticl\n\nD\n\n×\n\nend\n\nend\n\n26 27 end 28 mean = {mean1, mean2, ..., meanL}. 29 variance = {variance1, variance2, ..., varianceL}. 30 skewness = {skewness1, skewness2, ..., skewnessL}. 31 for s = 1, 2, ..., S do\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n32\n\n36\n\nX ← getactivation(xs) : get activation output values over the layers. update mean with Statistics (X,mean,condition ← mean)\n\nX ← getactivation(xs) : get activation output values over the layers. update variance with Statistics (X,mean,variance,condition ← variance)\n\n33 34 end 35 for s = 1, 2, ..., S do\n\n37 38 end 39 for s = 1, 2, ..., S do\n\n40\n\n41\n\nX ← getactivation(xs) : get activation output values over the layers. update skewness with\n\nStatistics (X,mean,variance,skewness,condition ← skewness)\n\n42 end 43 # R ← RCl 44 skewnessl ← Average of absolute of each channel values in skewnessl along the layers. 45 return skewness\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2: Calculating (Empirical) saturation over the layers Input: xs(s = 1, 2, ..., S) = mini − batch ∈ Rbs×H×W\n\nN = total number of samples saturation type = empirical or not activationtypeisLeCun or not.\n\nOutput: saturation ∈ RL\n\n1 if saturation type is empirical then\n\n2\n\nupper ← channel-wise maximum absolute value\n\n3 else if activation type is LeCun tanh then\n\nupper ← 1.7159\n\n4 5 else\n\nupper ← 1\n\n6 7 end\n\n8 saturation = {saturation1, saturation2, ..., saturationL}. 9 for s = 1, 2, ..., S do\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\nif saturation type is empirical then\n\nX ← getblock(xs) : get block output values over the layers.\n\nend else\n\nX ← getactivation(xs) : get activation output values over the layers.\n\nend X = {X1, X2, ..., XL}. for l = 1, 2, ..., L do\n\nC l, bs, hl, wl is the shape of X l B = bs × hl × wl, D = N × C l × hl × wl\n\n# RCl×B ← RCl×bs×hl×wl 1. blockl ← f lat(X l) by channels. 2. Take absolute to blockl. 3. blockl ← blockl upper .\n\n# R ← RCl×B 4. sum ← sum of all values of blockl. s ← sum 5. saturationl D . 6. saturationl += saturationl s.\n\nend\n\n28 29 end 30 return saturation\n\n18",
    "reference": "# Summary Of The Paper\n\nThis paper conducts an empirical analysis of the interaction between batch normalization and bounded activation functions. Specifically, the paper compares the architecture using batch normalization after a bounded activation(Swap model) and the architecture using a bounded activation after batch normalization(Convention model). Motivated by the observation that the swap model outperforms the convention model significantly when a bounded activation is used, the authors designed experiments to identify the reasons for these performance differences. The paper shows that in terms of asymmetric saturation, the Swap model and the Convention model behave differently and argues that high sparsity induced from the asymmetric saturation has a strong association with the generalization performance.\n\n# Strength And Weaknesses\n\n### Strengths\n- The authors discover that in the Swap model with a bounded activation, each feature map is saturated on one side of the asymptotic value of the bounded activation.\n\n### Weaknesses\n- It is a bit confusing whether the asymmetric saturation has a strong association with generalization performance.\n    - Although there is another noticeable observation that the saturation is very low in higher block depths, this is not discussed at all.\n    - Can you explain more how to exclude the possibility that low saturation at higher blocks or the combination of both could be a reason for better generalization?\n- It is also confusing whether the sparsity has a strong association with generalization performance\n    - Since the sparsity metric is $s^l = 1 - t^l$ where $t_l$ is the saturation metric, layerwise sparsity can be obtained from Figure 3. The relation between the sparsity of the Swap model and the Convention model is different depending on which layer is considered. In such case, it seems a bit of a stretch to draw a conclusion that the higher the sparsity is the better the generalization is. \n    - In a sense, this contradicts with the authors' argument 'Our saturation metric can dismiss the channel properties due to the summarization of channels in the layer.'\n    - Can we say that different sparsity distributions over layers with the same average sparsity will have similar generalization performance?\n- The coverage of the analysis is a bit limited. \n    - The analysis is claimed to be valid with bounded nonlinearity and without residual connection, excluding many widely used architectures. Also, it seems difficult to generalize or apply the claim of the paper to commonly used cases.\n    - Even though it is subjective, it does not seem that the Swap model with Tanh performs comparably to the Convention model with ReLU. \n\n### Questions\n- Can you elaborate on ' Because Tanh has non-linearity in everyplace except the origin, it can not follow the design of residual connection proposed'?\n    - Does that mean that Tanh has gradient 1 at the origin? What does it mean by nonlinear in everyplace?\n    - What does it mean by 'following the design of residual connection'?\n- In Table 1, with ReLu, the Convention model is better than the Swap model. Have you considered or performed a similar analysis to understand this reversed behavior?\n- What is 'the center of the function'? This term is not defined precisely. The center of the domain of the function or the center of the image of the function?\n- NWDBN is not explained until Figure 8 and is frequently used before Figure 8. Even though I guess that NWDBN may stand for No Weight Decay Batch Normalization, acronyms should be explained when it is first used.\n- What is the formula of LeCun Tanh? There are many typos for LeCun Tanh.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nEven though the observation that the Swap model with a bounded activation is interesting, the arguments that connect the observation and other experiments to the conclusions are not convincing. It seems that the training details are well-provided enough to enable reproducibility.\n\n# Summary Of The Review\n\nIt is interesting to know that with a bounded activation, the order between BN and the activation causes drastically different qualitative behavior. However, the presentation of the idea can be improved further by replacing vaguely defined terms and expressions. The arguments supporting the conclusions seem weak.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTIME TO AUGMENT SELF-SUPERVISED VISUAL REPRESENTATION LEARNING\n\nArthur Aubret1∗ Markus R. Ernst2∗ Céline Teulière1 Jochen Triesch2 1Clermont Auvergne Université, CNRS, Clermont Auvergne INP, Institut Pascal 2Frankfurt Institute for Advanced Studies {arthur.aubret, celine.teuliere}@uca.fr {mernst, triesch}@fias.uni-frankfurt.de\n\nABSTRACT\n\nBiological vision systems are unparalleled in their ability to learn visual representations without supervision. In machine learning, self-supervised learning (SSL) has led to major advances in forming object representations in an unsupervised fashion. Such systems learn representations invariant to augmentation operations over images, like cropping or flipping. In contrast, biological vision systems exploit the temporal structure of the visual experience during natural interactions with objects. This gives access to “augmentations” not commonly used in SSL, like watching the same object from multiple viewpoints or against different backgrounds. Here, we systematically investigate and compare the potential benefits of such time-based augmentations during natural interactions for learning object categories. Our results show that incorporating time-based augmentations achieves large performance gains over state-of-the-art image augmentations. Specifically, our analyses reveal that: 1) 3-D object manipulations drastically improve the learning of object categories; 2) viewing objects against changing backgrounds is important for learning to discard background-related information from the latent representation. Overall, we conclude that time-based augmentations during natural interactions with objects can substantially improve self-supervised learning, narrowing the gap between artificial and biological vision systems.\n\n1\n\nINTRODUCTION\n\nLearning object representations without supervision is a grand challenge for artificial vision systems. Recent approaches for visual self-supervised learning (SSL) acquire representations invariant to data-augmentations based on simple image manipulations like crop/resize, blur or color distortion (Grill et al., 2020; Chen et al., 2020). The nature of these augmentations determines what information is retained and what information is discarded, and therefore how useful these augmentations are for particular downstream tasks (Jaiswal et al., 2021; Tsai et al., 2020). Biological vision systems, in contrast, appear to exploit the temporal structure of visual input during interactions with objects for unsupervised representation learning. According to the slowness principle (Wiskott and Sejnowski, 2002; Li and DiCarlo, 2010; Wood and Wood, 2018), biological vision systems strive to discard high-frequency variations (e.g. individual pixel intensities) and retain slowly varying information (e.g. object identity) in their representation. Incorporating this idea into SSL approaches has led to recent time-contrastive learning methods that learn to map inputs occurring close in time onto close-by latent representations (Oord et al., 2018; Schneider et al., 2021). How these systems generalize depends on the temporal structure of their visual input. In particular, visual input arising from embodied interactions with objects may lead to quite different generalizations compared to what is possible with simple image manipulations. For instance, human infants learning about objects interact with them in various ways (Smith et al., 2018). First, infants rotate, bring closer/farther objects while playing with them (Byrge et al., 2014). Second, as they gain mobility, they can move in the environment while holding an object, viewing it in different contexts and against different backgrounds. We refer to (simulations of) such interactions as natural interactions.\n\n∗Equal contribution.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nHere, we systematically study the impact of such natural interactions on representations learnt through time-contrastive learning in different settings. We introduce two new simulation environments based on the near-photorealistic simulation platform ThreeDWorld (TDW) (Gan et al., 2021) and combine them with a recent dataset of thousands of 3D object models (Toys4k) (Stojanov et al., 2021). Then we validate our findings on two video datasets of real human object manipulations, ToyBox (Wang et al., 2018) and CORe50 (Lomonaco and Maltoni, 2017).\n\nOur experiments show that adding time-based augmentations to conventional data-augmentations considerably improves category recognition. Furthermore, we show that the benefit of time-based augmentations during natural interactions has two main origins. First, 3-D object rotations boost generalization across object shapes. Second, viewing objects against different backgrounds while moving with them reduces harmful effects of background clutter. We conclude that exploiting natural interactions via time-contrastive learning greatly improves self-supervised visual representation learning.\n\n2 RELATED WORK\n\nData-augmented self-supervised learning. The general idea behind most recent approaches for self-supervised learning is that two semantically close/different inputs should be mapped to close/distant points in the learnt representation space. Applying a certain transformation to an image such as flipping it horizontally generates an image that will be very different at the pixel level, but has a similar semantic meaning. A learning objective for SSL will therefore try to ascertain that the representations of an image and its augmented version are close in latent space, while being far from the representations of other unrelated images.\n\nA concrete approach may work as follows: sample an image x, apply transformations to it taken from a predefined set of transformations, resulting in new images x′, also called a positive pair. The same procedure is applied to a batch of different images. Embeddings of positive pairs are brought together while keeping the embeddings of the batch overall distant from one another. There are three main categories of approaches for doing so: contrastive learning methods (Chen et al., 2020; He et al., 2020) explicitly push away the embeddings of a batch of inputs from one another; distillation-based methods (Grill et al., 2020; Chen and He, 2021) use an asymmetric embedding architecture, allowing the model to discard the “push away” part; entropy maximization methods (Bardes et al., 2022; Ermolov and Sebe, 2020) maintain a high entropy in the embedding space.\n\nImage manipulations as data-augmentations. Most self-supervised learning approaches have used augmentations based on simple image manipulations to learn representations. Frequently used are color distortion, cropping/resizing a part of an image, the horizontal flipping of an image, gray scaling the image, and blurring the image (Chen et al., 2020). Other augmentations can be categorized in three ways (Shorten and Khoshgoftaar, 2019; Jaiswal et al., 2021): 1) geometric augmentations include image rotations (Chen et al., 2020) or image translations (Shorten and Khoshgoftaar, 2019); 2) Context-based augmentations include jigsaw puzzle augmentations (Noroozi and Favaro, 2016; Misra and Maaten, 2020), pairing images (Inoue, 2018), greyed stochastic/saliency-based occlusion (Fong and Vedaldi, 2019; Zhong et al., 2020), or automatically modifying the background (Ryali et al., 2021); 3) Color-oriented transformations can be the selection of color channels (Tian et al., 2020) or Gaussian noise (Chen et al., 2020). A related line of work also proposes learning how to generate/select data-augmentations (Cubuk et al., 2019; Tian et al., 2020), but since it takes advantage of labels, the approach is no longer self-supervised.\n\nTime-based data-augmentations. Several works have proposed using the temporality of interactions to learn visual representations. A recent line of work proposes to learn embeddings of video frames using the temporal contiguity of frames: Knights et al. (2021) propose a learning objective that makes codes of adjacent frames within a video clip similar, however the system still needs to have information about where each video starts and ends. In contrast, our setups expose the system to a continuous stream of visual inputs. Other methods showed the importance of time-based augmentations based on videos for object tracking (Xu and Wang, 2021), category recognition (Gordon et al., 2020; Parthasarathy et al., 2022; Orhan et al., 2020) or adversarial robustness (Kong and Norcia, 2021). Unlike us they do not make an in-depth analysis of the impact of different kinds of natural interactions. Schneider et al. (2021) showed the importance of natural interactions with objects for learning object\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Top view of the VHE used to situate our agent in a house. Blue triangles/orange circles show possible agent/object positions, respectively. In this episode, the agent is located in the office (blue arrow) and interacts with a plush toy. Successive first person views of object interactions are shown in the lower left.\n\nrepresentations, but only considered small datasets of few objects, without complex backgrounds and without studying generalization over categories of objects. Wang et al. (2021) have shown that one can replace crop augmentations by saccade-like magnifications of images, establishing a link between conventional image-based and more natural time-based augmentations. Aubret et al. (2022) studied the impact of embodiment constrains on object representations learnt through time-contrastive learning during natural interactions. They highlight that complex backgrounds negatively impact the representations of objects. Here we show that allowing the agent to move with objects so that they are seen against changing backgrounds improves robustness against background complexity.\n\nWe note that a large body of work also considers time-contrastive learning in the context of reinforcement learning (Oord et al., 2018; Laskin et al., 2020; Stooke et al., 2021; Okada and Taniguchi, 2021) and intrinsic motivation (Guo et al., 2021; Yarats et al., 2021; Li et al., 2021; Aubret et al., 2021).\n\n3 METHODS\n\nOur goal is to study the potential of time-based augmentations during natural interactions with objects for visual representation learning and to evaluate the utility of the learned representations for object categorization.1 We focus on two kinds of natural interactions: object manipulations (3D translations and rotations) and ego-motion. We introduce two computer-rendered environments, the Virtual Home Environment (VHE) and the 3D Shape Environment (3DShapeE) to compare and disentangle the effects of these different types of natural interactions.\n\nWe also validate our findings using two real-world first-person video datasets. The ToyBox environment (Wang et al., 2018) permits studying the relative benefits of object translations and rotations. The CORe50 environment (Lomonaco and Maltoni, 2017) allows us to explore the effects of smooth visual tracking and to mimic the effects of ego-motion.\n\n3.1 VIRTUAL HOME ENVIRONMENT\n\nTo simulate a diverse set of natural interactions of an infant in its home, we take inspiration from (Aubret et al., 2022) and place an agent within an environment that resembles a residential house, where it can “play” with objects and change positions. Figure 1 depicts a bird’s eye view showing the floor plan of the aforementioned house. In comparison with Aubret et al. (2022), we integrate a two order of magnitude higher number of fully textured toy objects, as well as novel interactions with these objects. The agent is spawned in a randomly sampled location, staying there for 100-timestep-long\n\n1The source code is available at https://github.com/trieschlab/TimeToAugmentSSL\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Examples of time-based augmentations while “playing” in the VHE (left, A-C): A) object rotation; B) ego-motion; C) depth change. CORe50 (right, D-E): D) close-by frames of objects in 2 different recording sessions; E) different exemplars of the object category “cellphones” shown in different contexts. ToyBox (left, F-H): F) depth change, G) lateral translation, H) object rotation.\n\nepisodes (or sessions). Its position in the world determines what background will be seen. Each background contains a unique combination of static objects and floor/wall textures.\n\nToys4k dataset. The agent interacts with objects from the Toys4k dataset, which is composed of 4,179 diverse 3-D toy objects distributed in an unbalanced way into 105 categories (Stojanov et al., 2021). It was designed to match the objects typically encountered by infants. We place textured objects in different locations of the full house. We modify/remove 3D models, only keeping quick-to-process and correct-quality objects in our simulations (cf. Appendix C.1 for more details). To study the ability of our agent to generalize over categories, we use two thirds of the objects of each category for training and keep the last third for testing. We construct a test set composed of 5 randomly rotated views of the test objects in different locations of the house. Except for rotation, no additional transformations have been applied to build the test set. We also introduce 3 novel houses to test the robustness of the object-representation with respect to novel out-of-distribution backgrounds (cf. Appendix C.2).\n\nObject manipulations. At only a few months old, infants start to hold objects and move and rotate them (Byrge et al., 2014). Similarly, our agent can manipulate the objects in two different ways. First, it can rotate the object around the vertical axis (yaw) by a random angle drawn uniformly from the interval [0, rot] degrees, where the hyperparameter rot sets the maximal speed of object rotations (cf. Figure 2A). Second, the agent can also bring the object closer or move it further away, as shown in Figure 2C; we model this as a change of distance uniformly sampled from [−d, d], where d = 7.5 cm is the maximal distance change. The distance is bounded between 0.65 m and 1.1 m to reflect morphological limitations of the agent’s arms.\n\nEgo-motion. When infants become mobile, they can play with objects while moving in their environment. To simulate this, our agent can turn its body towards a neighboring location while holding the same object so that it is seen against a different background. As shown in Figure 2B, the new background will usually have similar floors and walls. Sometimes an infant will also engage with a new object. We model both possibilities by deterministically turning the agent every Ns time steps and switching to a new object with probability 1 − po = 0.1. A small/medium Ns respectively simulates in an unnatural/natural fashion an agent that always/sometimes moves with its object. When this ego-motion is disabled, the agent simultaneously picks up a new object and turns every Ns = 10 time steps. For some experiments (Table 1 and Figure 5A, B), we also allow the agent to visit another room every Ns steps while continuing to interact with the same object. This allows us to investigate the effect of more drastic background changes, but may be a less valid model of a human infant’s interaction with objects.\n\n3.2\n\n3D SHAPE ENVIRONMENT\n\nIn order to study the influence of natural interactions on shape generalization while minimizing the impact of object colors or background, we introduce a simple 3D Shape Environment (3DShapeE). It differs from the VHE in two respects: 1) we import untextured versions of toy objects from the Toys4k dataset, making them appear grey; 2) we place the agent in an empty environment so that objects are seen against a blank background. Examples of inputs are given in Figure 4E and augmentations are the same as in the VHE environment (Figure 2A-C).\n\n4\n\nADEFGHBCPublished as a conference paper at ICLR 2023\n\n3.3 TOYBOX ENVIRONMENT\n\nThe ToyBox dataset (Wang et al., 2018) contains first-person videos of an observer manipulating 360 objects (12 categories, 30 objects each). In each clip, a person applies one type of transformation: a translation (x, y or z), rotation (around x, y or z axis), a “hodgepodge,” nothing specific, or in some cases, nothing if there is no object. We sample two frames per second from each clip (cf. Appendix C.3 for more details). Two successively sampled frames form a positive pair. At the end of a clip, a positive pair is formed from the last frame sampled from the clip and the first image of the randomly sampled next clip.\n\n3.4 CORE50 ENVIRONMENT\n\nThe CORe50 dataset (Lomonaco and Maltoni, 2017) contains first-person views of an observer manipulating objects. The dataset comprises 167,866 images of 50 different objects belonging to 10 distinct object categories. All objects were filmed in 11 sessions corresponding to different natural environments. The images from all but one object per class are used to form the training and validation sets. Specifically, every 10th image enters the validation set, the others form the training set. The images of the held-out object of each class form the test set. This allows us to test generalization to unknown objects from familiar categories.\n\nThe videos in this dataset show real object manipulations, but we also use it to mimic the effects of ego-motion through the way we sample images from the different video clips. In general, successively sampled images form positive pairs. We create sequences of images by starting from a particular frame of one of the video clips showing an object manipulation and defining a probability po that the manipulation of the same object continues. Otherwise an interaction with a new object will start. In case the interaction continues, the next frame in our sampled sequence will be randomly chosen as either the next or the previous frame in the video clip. Thus, as long as the object manipulation continues, the sampled sequence of frames corresponds to a random walk through the frames of the video. A high/low po leads to long/short random walks. Specifically, the expected number of successive frames No showing the same object is given by:\n\nE [No|po] = 1 +\n\n∞ (cid:88)\n\nx=0\n\nxpx\n\no (1 − po) = 1 +\n\npo 1 − po\n\n.\n\n(1)\n\nTo mimic the effects of ego-motion causing an object to be seen against changing backgrounds, we also define a probability ps that the sequence of views stays within the same session, i.e., the same video clip. Thus, there is a chance of 1 − ps that the sequence of views of the same object “jumps” to a different video clip showing the same object in a different context. In this case, a positive pair is generated from two images of the same object recorded in different recording sessions. This cross session sampling (CSS) is analogous to the role of ego-motion in the VHE.\n\nAs an alternative to the random walk procedure described above we also consider a variant where the next view of the same object is chosen uniformly at random from the current video clip. This leads to images forming a positive pair, which stem from distant time points in the original video clip. While this is not a realistic model of viewing sequences experienced by an infant, it allows us to assess the benefits of positive pairs containing more distinct views of the same object. This is analogous to greater speed of object rotation in the VHE.\n\nFor implementation details of our sampling procedure see Appendix B.5. In brief, it is an extension of the procedure used in Schneider et al. (2021) to allow CSS.\n\n3.5 EVALUATION AND TRAINING PROCEDURE FOR ALL ENVIRONMENTS\n\nLearning algorithms. We consider three SOTA representatives of existing algorithms for dataaugmented SSL: SimCLR for contrastive learning (Chen et al., 2020), BYOL (Grill et al., 2020) for distillation-based methods and VICReg (Bardes et al., 2022) for entropy maximization methods. We refer to the versions of these algorithms using time-based augmentations as, e.g., SimCLR through time (SimCLR-TT), etc. (Schneider et al., 2021).\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Sequences of input images during object rotations (rot(360)) and ego-motion and their augmented positive pair according to the augmentation method (same column). Baseline refers to conventional augmentations; *-TT refers to time-based augmentations; *-TT+ refers to combined augmentations.\n\nPositive pair construction. We consider three ways to augment an image. First, we consider time-based augmentations during natural interactions (SimCLR-TT, etc.). For this, unless stated otherwise, we set the positive pair of a source image as the next image in the temporal sequence. Second, as a comparison baseline, we augment a sampled image with a widely adopted SOTA set of augmentations (Chen et al., 2020; Shijie et al., 2017), i.e. crop/resize, grayscale, color jittering and horizontal flip with their default parameters, except for the crop/resize augmentation (cf. Appendix D). Third, we propose to combine the two kinds of augmentations (SimCLR-TT+, etc.). For this we apply the conventional SOTA augmentations to the image that temporally follows the sampled source image. We refer to Figure 3 for examples of the different ways to augment source images.\n\nControlling for the number of augmentations. While we can generate an arbitrary number of positive pairs from one sample image with conventional augmentations, we only have access to one positive time-based pair per sample image for the VHE, 3DShapE, and the ToyBox environment. To control for the impact of the number of positive pairs, we therefore only construct one positive pair per sample for all algorithms and augmentation methods: 1) for the ToyBox environment, we sample positive pairs from a pre-built dataset of the same size as the original one, but composed of conventionally augmented images; 2) for the VHE and 3DShapeE the agent collects images, augments them and stores them in a buffer of size 100,000. It simultaneously trains on images randomly sampled from the buffer at every time step. We show in Appendix B.1 that controlling for the number of augmentations does not impact the results of our analysis.\n\nControlling for the number/distribution of source images. We want to make sure that we compare the quality of the augmentations, and not the amount/diversity of data that feeds the SSL algorithms. Thus, for the comparison baseline, we consider the same natural interactions to generate source images, but we do not use temporal information to generate the positive pairs. This ensures that the number/distribution of non-augmented images is exactly the same across methods (source images in Figure 3). We make an exception with depth changes in 3DShapeE since we found it to be harmful for the baselines.\n\nEvaluation. We compute the representation with a ResNet18 on ToyBox and CORe50 and with a simpler convolutional neural network on 3DShapeE and VHE. We evaluate the learnt representation after 480,000 steps for the VHE and 3DShapeE, 60 epochs for ToyBox and 100 epochs for CORe50. The quality of the learned representation is assessed by training linear readouts on top of the learned representation in a supervised fashion (Chen et al., 2020). Depending on the question, we train linear readouts to either predict the identity of an object, the category of an object, or the location/session of the object. We average all results over 3 random seeds. Because of the small number of objects per category in the CORe50 dataset, we apply a cross-validation with 5 splits. Hyperparameters are given in Appendix D.\n\n6\n\nSourceimages*-TTBaseline*-TT+TimePublished as a conference paper at ICLR 2023\n\nMethod\n\nSimCLR\n\nVHE\n\n3DShapeE\n\nToyBox\n\nCORe50\n\n0.328 ± 0.002\n\n0.527 ± 0.005\n\n0.313 ± 0.001\n\n0.544 ± 0.074\n\nSimCLR-TT\n\n0.369 ± 0.005\n\n0.598 ± 0.007\n\n0.189 ± 0.009\n\n0.449 ± 0.152\n\nSimCLR-TT+\n\n0.537 ± 0.001\n\n0.621 ± 0.001\n\n0.378 ± 0.005\n\n0.610 ± 0.090\n\nBYOL\n\nBYOL-TT\n\nBYOL-TT+\n\n0.352 ± 0.017\n\n0.496 ± 0.012\n\n0.354 ± 0.007\n\n0.552 ± 0.063\n\n0.369 ± 0.007\n\n0.570 ± 0.001\n\n0.213 ± 0.014\n\n0.448 ± 0.160\n\n0.516 ± 0.002\n\n0.536 ± 0.001\n\n0.393 ± 0.031\n\n0.614 ± 0.128\n\nVICReg\n\n0.169 ± 0.006\n\n0.512 ± 0.002\n\n0.381 ± 0.019\n\n0.410 ± 0.073\n\nVICReg-TT\n\n0.264 ± 0.012\n\n0.383 ± 0.167\n\n0.158 ± 0.008\n\n0.245 ± 0.084\n\nVICReg-TT+\n\n0.435 ± 0.013\n\n0.574 ± 0.008\n\n0.418 ± 0.016\n\n0.503 ± 0.057\n\nTable 1: Top 1 accuracy ± standard deviation (over 3 seeds, CORe50 over 5 training/testing splits) under linear evaluation of previous SOTA (BYOL, SimCLR, VICReg) versus our time-based augmentations (*-TT) and their combination (*-TT+). In 3DShapeE, *-TT+ use crop/resize, color jittering and rotations. In VHE, *-TT+ use rotations, Ns = 1 with room changes, color jittering and gray scaling. In both VHE and 3D Shape Environments, *-TT use rotations, Ns = 1 with room changes (VHE only) and depth changes. In CORe50 we use ps = 0.5, po = 0.9. We refer to Appendix B.2 for the ablation study that motivates these choices. In ToyBox, we always apply all their respective transformations. Unlike *-TT+, we apply all standard augmentations for the baseline, as we found that removing some of them (crop/resize and flip) was harmful.\n\n4 RESULTS\n\nIn this section, we first study the benefits of using time-based augmentations during natural interactions with objects for self-supervised visual representation learning. Then, we provide an in-depth analysis of the impact of 3-D object rotations and ego-motion on the learned representations. An ablation study is performed in Appendix B.\n\n4.1 TIME-BASED AUGMENTATIONS DERIVED FROM NATURAL INTERACTIONS BOOST\n\nPERFORMANCE OF SSL\n\nIn Table 1, we compare the quality of the learnt representation when using conventional dataaugmentations (SimCLR, BYOL, VICReg), time-based augmentations (*-TT), and their combination (*-TT+) in our four test environments. We observe that our combined approach (*-TT+) increases the average test category accuracy with respect the the baselines (SimCLR, BYOL, VICReg) by a range of [0.037; 0.266] points across all environments and all SSL methods. To formally validate our approach, we ran additional experiments for SimCLR and SimCLR-TT+ and applied a t-test to statistically compare their performance, supporting the superiority of SimCLR-TT+ (p < 0.05 in our four test environments). Time-based augmentations during natural interactions (*-TT) do not perform very well on their own, presumably because they do not create invariance to color/grayscale changes in our test environments (cf. Appendix B for ablation studies of augmentations). In the next sections, we analyse the impact of 3-D rotations and ego-motion on the learnt representations.\n\n4.2 OBJECT MANIPULATIONS INCLUDING 3-D ROTATIONS SUPPORT SHAPE GENERALIZATION\n\nTo assess the importance of object manipulations to generalize over object shapes, we conducted experiments in 3DShapeE. Figure 4A shows that increasing the speed of object rotations systematically increases the category recognition accuracy. Similarly, in the CORe50 Environment (Figure 4D) the uniform sampling of views clearly outperforms the default random walk sampling procedure, in particular for short object manipulations (low po). Interestingly, we observe the inverse effect on individual object recognition accuracy (Figure 4B). We conclude that fast object rotations (in the extreme case constructing positive pairs from randomly rotated views of an object) tend to discard object-specific details and focus the representation on more general shape features, which will be similar for different objects of the same category.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: A) 3DShapeE test category recognition accuracy for different maximal rotation speeds S, without other augmentations. B) 3DShapeE test object recognition accuracy for different maximum rotation speeds, without other augmentations. The shaded area displays the +/- standard deviation of test category accuracy over seeds. C) Test category accuracy with a linear classifier trained on one single orientation of objects, and tested on the same orientation of test objects. We used 5 seeds, all our conventional augmentations but only rotations ([0, 360] degrees) as time-based augmentation. We also apply the majority vote on datasets of six different orientations: 0°, 10°, -45°, 80°, -80°, -90°. D) Category accuracy for the CORe50 Environment when varying the duration of object manipulations (ps = 0.95). The shaded area shows ± standard error. A-D) We used SimCLR-TT. E) Example of three views for eight objects from four categories in 3DShapeE. In order from left to right for each object, we show orientations of: 0° (side view); -45° (3/4 view); 90° (back view). The red rectangles highlight visually similar views of different objects.\n\nFinally, we investigate the relation between rotation invariance and object shape. To do so, we design a new test setup. In the single-single setup, we train the linear classifier on the representations of a single-view dataset that contains only one view from a common orientation for each training object (back, side, . . . ). We assess the classifier on images of the test objects shown in the same orientation. This means that the linear classifier can not learn the invariance to rotation and does not explicitly need it for classification. We consider datasets of orientations of -90° (front view), -45° (3/4 view), 0° (side view) as they contain the extreme cases in terms of the amount of visible surface (Blanz et al., 1999).\n\nWe hypothesize that including 3-D rotations in time-based augmentations allows the model to encode 3-D shape features in the learned representation, which results in an improved category recognition. To test this hypothesis, we first evaluate category recognition based on several views of an object (thus its whole shape). In Figure 4C, we compute a majority vote among classification results from the different single-single classifiers as follows: if more views (e.g., 90°, 80°, 10°) are classified as category A than category B (e.g., 0°, -90°), category A is selected. We observe a large increase of accuracy for SimCLR. We conclude that encoding the 3-D shape improves category recognition. Importantly, we do not observe such an increase of accuracy for SimCLR-TT+ with the majority vote. We deduce that SimCLR-TT+ better infers the 3-D shape of the object from one view, which strongly improves category recognition.\n\n4.3 MOVING WITH OBJECTS MITIGATES THE EFFECT OF BACKGROUND CLUTTER\n\nTo evaluate the impact of ego-motion while holding objects, we trained our agent in the VHE with different levels of motion. In Figure 5A, we see that increasing the frequency 1/Ns and the range (room) of motion considerably boosts the test category accuracy in backgrounds from training and novel houses. At the same time, we observe a decrease of the accuracy when classifying the background (Figure 5B). We conclude that moving while holding objects improves the representation’s invariance with respect to the background.\n\nMoreover, we analysed the frequency of session changes compared to the duration of object manipulations as one of the hyperparameters of our approach in the CORe50 Environment, see Figure 5C,\n\n8\n\n0.30.40.50.00.10.20.3Million stepsAccuracyrot(10)rot(30)rot(90)rot(360)3DShapeE category acc.A0.60.70.80.90.00.10.20.3Million stepsrot(10)rot(30)rot(90)rot(360)3DShapeE object acc.B0.40.50.60.7SimCLR−TT+SimCLRMethodAccuracyFront viewsSide views3/4 viewsMajority3DShapeE category acc., single−singleC++Published as a conference paper at ICLR 2023\n\nFigure 5: A and B. Impact of the frequency (Ns) of body motions and whether they change rooms on the final test category accuracy (A) and the final background accuracy (B). We also used object rotations ([0, 360] degrees), color jittering and gray scaling. Vertical bars indicate one standard deviation. C and D. Mean CORe50 accuracy for category (C) and session (D) classification dependent on the duration of object manipulations (No) and the frequency of session switches (Ns). SimCLRTT, random walk procedure, mean based on 3 training/testing splits.\n\nD. Qualitatively, for category classification we observe low accuracies for high-frequency object switches, low session changes and high accuracies for low-frequency object switches, high session changes. Accuracy seems to more or less saturate at the main diagonal. We infer from this that intermediate values may actually be better than going to the extreme, but it could also be partly because Ns is bounded by No. The classification of sessions consistently shows that session information is being disregarded when the frequency of session changes rises.\n\n5 CONCLUSION\n\nWe investigated the benefits of time-based data-augmentations during infant-inspired natural object interactions for self-supervised visual representation learning. We studied these interactions through a novel 3-D simulation environment where an agent “plays” with objects in a house, and we validated our findings on two real-world video datasets of object manipulations. We find that combining timebased augmentations with conventional data-augmentations (*-TT+ algorithms) greatly improves the ability of the learned representations to generalize over object categories. Our analysis shows that 1) 3-D object rotations are crucial to build good representations of shape categories. 2) Ego-motion while holding an object so that it is seen against different backgrounds prevents the background from cluttering the learned representation.\n\nOur work raises the question whether time-based augmentations during natural interactions with objects could fully replace conventional ones. Indeed, we showed that 3-D rotations are superior to the flip data-augmentation for shape generalization (Appendix B.3). However, we found that color-based transformations like color jittering and grayscaling remain important. It is an interesting question for future work if time-based augmentations during changes of direct and indirect lighting or shadows cast by other objects could replace augmentations based on simple image color manipulations.\n\nOur work has a number of limitations. First, while we have portrayed the time-based augmentations as stemming from object manipulations, we did not render a hand holding the object or similar in the VHE and 3DShapeE. Second, the CORe50 and ToyBox environments are only medium-sized. Large scale experiments may give additional insights. Finally, in this work, we only considered naive behavioural strategies for, e.g., turning objects or ego-motion. This contrasts with the way infants learn about their environment (Bambach et al., 2016). For example, they are biased towards creating planar views of objects and often prefer unfamiliar objects (Roder et al., 2000). Thus, we expect that learning to actively select successive views will further unveil the potential of time-based augmentations.\n\nACKNOWLEDGEMENTS\n\nThis work was sponsored by a public grant overseen by the French National Agency through the IMobS3 Laboratory of Excellence (ANR-10-LABX-0016) and the IDEX-ISITE initiative CAP 20-25 (ANR-16-IDEX-0001). Financial support was also received from Clermont Auvergne Metropole through a French Tech-Clermont Auvergne professorship. We gratefully acknowledge support\n\n9\n\n0.000.250.500.751.00Test housesTraining houseAccuracyN = 10N = 5N = 1N = 1,room changeVHE category acc.A0.000.250.500.751.00Test housesTraining houseAccuracyVHE background acc.BssssPublished as a conference paper at ICLR 2023\n\nfrom GENCI–IDRIS (Grant 2022-AD011011623R1) for providing computing and data-processing resources needed for this work. Additional support was received by the Deutsche Forschungsgemeinschaft (DFG project “Abstract REpresentations in Neural Architectures (ARENA)”), as well as the projects “The Adaptive Mind” and “The Third Wave of Artificial Intelligence” funded by the Excellence Program of the Hessian Ministry of Higher Education, Science, Research and Art (HMWK). JT was supported by the Johanna Quandt foundation.\n\nREFERENCES\n\nAubret, A., Hassas, S., et al. (2021). Distop: Discovering a topological representation to learn diverse\n\nand rewarding skills. arXiv preprint arXiv:2106.03853.\n\nAubret, A., Teulière, C., and Triesch, J. (2022). Toddler-inspired embodied vision for learning object representations. In IEEE 21st International Conference on Development and Learning (ICDL).\n\nBambach, S., Crandall, D. J., Smith, L. B., and Yu, C. (2016). Active viewing in toddlers facilitates visual object learning: An egocentric vision approach. In Proceedings of the 38th Annual Conference of the Cognitive Science Society, pages 1631–1636. Cognitive Science Society.\n\nBardes, A., Ponce, J., and Lecun, Y. (2022). VICReg: Variance-invariance-covariance regularization for self-supervised learning. In Proceedings of the 10th International Conference on Learning Representations (ICLR).\n\nBlanz, V., Tarr, M. J., and Bülthoff, H. H. (1999). What object attributes determine canonical views?\n\nPerception, 28(5):575–599.\n\nByrge, L., Sporns, O., and Smith, L. B. (2014). Developmental process emerges from extended\n\nbrain–body–behavior networks. Trends in Cognitive Sciences, 18(8):395–403.\n\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020). A simple framework for contrastive learning of visual representations. In III, H. D. and Singh, A., editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1597–1607. PMLR.\n\nChen, X. and He, K. (2021). Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15750–15758.\n\nCubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. (2019). Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n\nErmolov, A. and Sebe, N. (2020). Latent world models for intrinsically motivated exploration. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 5565–5575. Curran Associates, Inc.\n\nFong, R. and Vedaldi, A. (2019). Occlusions for effective data augmentation in image classification. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 4158–4166.\n\nGan, C., Schwartz, J., Alter, S., Mrowca, D., Schrimpf, M., Traer, J., De Freitas, J., Kubilius, J., Bhandwaldar, A., Haber, N., et al. (2021). Threedworld: A platform for interactive multi-modal physical simulation. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1).\n\nGordon, D., Ehsani, K., Fox, D., and Farhadi, A. (2020). Watching the world go by: Representation\n\nlearning from unlabeled videos. arXiv preprint arXiv:2003.07990.\n\nGrill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., Piot, B., kavukcuoglu, k., Munos, R., and Valko, M. (2020). Bootstrap your own latent - a new approach to self-supervised learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 21271–21284. Curran Associates, Inc.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nGuo, Z. D., Azar, M. G., Saade, A., Thakoor, S., Piot, B., Pires, B. A., Valko, M., Mesnard, T., Lattimore, T., and Munos, R. (2021). Geometric entropic exploration. arXiv preprint arXiv:2101.02055.\n\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In\n\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n\nInoue, H. (2018). Data augmentation by pairing samples for images classification. arXiv preprint\n\narXiv:1801.02929.\n\nJaiswal, A., Babu, A. R., Zadeh, M. Z., Banerjee, D., and Makedon, F. (2021). A survey on contrastive\n\nself-supervised learning. Technologies, 9(1).\n\nKnights, J., Harwood, B., Ward, D., Vanderkop, A., Mackenzie-Ross, O., and Moghadam, P. (2021). Temporally coherent embeddings for self-supervised video representation learning. In 2020 25th International Conference on Pattern Recognition (ICPR), pages 8914–8921. IEEE.\n\nKong, N. and Norcia, A. (2021). Are models trained on temporally-continuous data streams more\n\nadversarially robust? In SVRHM 2021 Workshop @ NeurIPS.\n\nLaskin, M., Srinivas, A., and Abbeel, P. (2020). Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pages 5639–5650. PMLR.\n\nLi, N. and DiCarlo, J. J. (2010). Unsupervised natural visual experience rapidly reshapes size-invariant\n\nobject representation in inferior temporal cortex. Neuron, 67(6):1062–1075.\n\nLi, S., Zheng, L., Wang, J., and Zhang, C. (2021). Learning subgoal representations with slow\n\ndynamics. In International Conference on Learning Representations (ICLR).\n\nLomonaco, V. and Maltoni, D. (2017). Core50: a new dataset and benchmark for continuous object\n\nrecognition. In Conference on Robot Learning, pages 17–26. PMLR.\n\nLoshchilov, I. and Hutter, F. (2018). Decoupled weight decay regularization.\n\nIn International\n\nConference on Learning Representations (ICLR).\n\nMisra, I. and Maaten, L. v. d. (2020). Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n\nNoroozi, M. and Favaro, P. (2016). Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision (ECCV), pages 69–84. Springer International Publishing.\n\nOkada, M. and Taniguchi, T. (2021). Dreaming: Model-based reinforcement learning by latent imagination without reconstruction. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 4209–4215. IEEE.\n\nOord, A. v. d., Li, Y., and Vinyals, O. (2018). Representation learning with contrastive predictive\n\ncoding. arXiv preprint arXiv:1807.03748.\n\nOrhan, E., Gupta, V., and Lake, B. M. (2020). Self-supervised learning through the eyes of a child. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 9960–9971. Curran Associates, Inc.\n\nParthasarathy, N., Eslami, S., Carreira, J., and Hénaff, O. J. (2022). Self-supervised video pretraining\n\nyields strong image representations. arXiv preprint arXiv:2210.06433.\n\nRoder, B. J., Bushnell, E. W., and Sasseville, A. M. (2000). Infants’ preferences for familiarity and\n\nnovelty during the course of visual processing. Infancy, 1(4):491–507.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nRyali, C., Schwab, D. J., and Morcos, A. S. (2021). Learning background invariance improves generalization and robustness in self-supervised learning on imagenet and beyond. In NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future.\n\nSchneider, F., Xu, X., Ernst, M. R., Yu, Z., and Triesch, J. (2021). Contrastive learning through time.\n\nIn SVRHM 2021 Workshop @ NeurIPS.\n\nShijie, J., Ping, W., Peiyi, J., and Siping, H. (2017). Research on data augmentation for image classification based on convolution neural networks. In 2017 Chinese automation congress (CAC), pages 4165–4170. IEEE.\n\nShorten, C. and Khoshgoftaar, T. M. (2019). A survey on image data augmentation for deep learning.\n\nJournal of Big Data, 6(1):60.\n\nSmith, L. B., Jayaraman, S., Clerkin, E., and Yu, C. (2018). The developing infant creates a curriculum\n\nfor statistical learning. Trends in Cognitive Sciences, 22(4):325–336.\n\nStojanov, S., Thai, A., and Rehg, J. M. (2021). Using shape to categorize: Low-shot learning with an explicit shape bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1798–1808.\n\nStooke, A., Lee, K., Abbeel, P., and Laskin, M. (2021). Decoupling representation learning from reinforcement learning. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9870–9879. PMLR.\n\nTian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C., and Isola, P. (2020). What makes for good views for contrastive learning? In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 6827–6839. Curran Associates, Inc.\n\nTsai, Y.-H. H., Wu, Y., Salakhutdinov, R., and Morency, L.-P. (2020). Self-supervised learning from\n\na multi-view perspective. In International Conference on Learning Representations (ICLR).\n\nWang, B., Mayo, D., Deza, A., Barbu, A., and Conwell, C. (2021). On the use of cortical magnification and saccades as biological proxies for data augmentation. In SVRHM 2021 Workshop@ NeurIPS.\n\nWang, X., Ma, T., Ainooson, J., Cha, S., Wang, X., Molla, A., and Kunda, M. (2018). The toybox\n\ndataset of egocentric visual object transformations. arXiv preprint arXiv:1806.06034.\n\nWiskott, L. and Sejnowski, T. J. (2002). Slow feature analysis: Unsupervised learning of invariances.\n\nNeural Computation, 14(4):715–770.\n\nWood, J. N. and Wood, S. M. (2018). The development of invariant object recognition requires visual\n\nexperience with temporally smooth objects. Cognitive Science, 42(4):1391–1406.\n\nXu, J. and Wang, X. (2021). Rethinking self-supervised correspondence learning: A video frame-level similarity perspective. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 10075–10085.\n\nYarats, D., Fergus, R., Lazaric, A., and Pinto, L. (2021). Reinforcement learning with prototypical In Meila, M. and Zhang, T., editors, Proceedings of the 38th International representations. Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11920–11931. PMLR.\n\nZhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y. (2020). Random erasing data augmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13001–13008.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA SAMPLING VIEWS IN THE CORE50 ENVIRONMENT\n\nTo simulate No and Ns we do not directly sample from the data set, but rather build a buffer of positive pairs and iterate through it.\n\nBuilding the training buffer is done in cycles, see Figure 6A. In every cycle, each different object of the data set is chosen exactly once — the order in which the objects are presented is a new random permutation during each cycle. For each sampled object, the procedure consists of three steps:\n\n1. We randomly sample a view v1. 2. We sample a view from the same object v2 to form the same-object pair (v1, v2) using a) random walk, b) uniform. If we reach Ns, we sample v2 from a different session (same object but with a different background).\n\n3. We repeat 2. to form (v2, v3) and continue until we reached No of object manipulation\n\nsteps.\n\nAfter that, we sample a view from the next object according to the cycle (which is also a positive pair) and repeat the procedure. Thus, a cycle consists of a total of Nobj × No object views. During the manipulation of one object, multiple cross-session transitions can occur, see Figure 6B. Several cycles of Nobj × No views are stored in the buffer after another.\n\nDuring training, batches of pairs of subsequent views are sampled uniformly from the buffer (dashed brackets) and filled into the batch. We consider an epoch to be an entire run through the buffer, which is chosen to match the size of the underlying CORe50 dataset. In consequence, every training image on average is presented once each epoch.\n\nFigure 6: Dynamic sampling procedure for the CORe50 dataset. A) Buffer with deterministic No = 3. Digits correspond to frame number of the video. B) Buffer with cross session sampling, colors represent different sessions.\n\nB COMPLEMENTARY ANALYSIS\n\nB.1 ADDITIONAL CONTROL EXPERIMENTS\n\nThe number of conventional positive pairs per sample does not matter. To fairly compare the quality of time-based augmentations with conventional ones in Table 1, we controlled for the number of conventional positive pairs in VHE, 3DShapeE and ToyBox by fixing 1 positive pair per image. Here, we aim to check whether time-based augmentations are useful even if we increase the number of positive pairs per sample (a new one for each training minibatch). In this case, the number of positive pairs per sample equals the number of epochs.\n\nIn the ToyBox environment (Figure 7A), we observe increasing the number of conventional positive pairs improves both the baseline (SimCLR) and the combined augmentations (SimCLR-TT+) and does not change their relative ordering. In addition, it does not significantly impact the accuracy of the 3DShapeE (Figure 7B) and VHE (Figure 7C). Overall, this confirms that time-based augmentations provide complementary and better information about the object shape in comparison to conventional data-augmentations.\n\nThe linear classifier learns about rotation invariance, but it does not fully explain our results. Here, we want to assess the importance of the role of linear classifiers to learn the invariance over rotations in our results. We focus on the VHE and reuse the single-single setup used in Section 4.2 and compare the results with a new multi-single setup. Through the multi-single setup, we train\n\n13\n\nrandomwalk, No = 5, Ns = 2randomwalk, No = 3...1 cycle...1234569894329......ABPublished as a conference paper at ICLR 2023\n\nFigure 7: Test category accuracy according to whether we use one positive pair per image (One aug) or several positive pairs, one per minibatch (More aug). We compare on A) the ToyBox environment; B) 3DShapeE and C) VHE.\n\nFigure 8: A) Test category accuracy with a linear classifier trained on the multi-view dataset (several orientations of objects), and tested on test objects with one orientation. B) Replicated from Figure 4C for readability. Test category accuracy with a linear classifier trained on one single orientation of objects, and tested on the same orientation of test objects. We used 5 seeds, all our conventional augmentations but only rotations ([0, 360] degrees) as time-based augmentation. We apply the majority vote on datasets of six different orientations: 0°, 10°, 45°, 80°, -80°, -90°.\n\na linear classifier on representations gained from the usual multi-view dataset, but test it on the single-view datasets.\n\nWe observe an overall decrease of performance in SimCLR when testing with the multi-single setup (Figure 8A) in comparison to single-single setup (Figure 8B). In contrast, SimCLR-TT+ It suggests that (with rotations only) is more robust to the training set of the linear classifier. learning rotation invariance with the linear classifier is mostly harmful for SimCLR. However, the remaining performance discrepancy in the single-single setup shows that it does not explain the entire performance discrepancy in Table 1.\n\nTT+ significantly outperforms its SimCLR counterpart. As mentioned in the main text we underline the results presented in Section 4.1 by statistically testing our SimCLR-TT+ method against its SimCLR counterpart for all of our 4 different datasets/environments. We did so by rerunning the experiment with additional random-seeds and comparing test-set accuracy with a two sample independent t-test. We found significant differences for all of our comparisons p < .05, the details of this analysis can be found in Table 2.\n\nSimCLR-TT+ also improves over SimCLR in VHE and 3DShapeE with a ResNet18. We wanted to verify whether TT+ during object rotations improves over TT in VHE and 3DShapeE with a ResNet18 neural network. In Table 3, we repeat the SimCLR-based experiments described in Section 4 with the following modifications: 1– we use a ResNet18 backbone; 2- we set the object manipulation time to 50 time steps; 3- we increase the number of positive pairs per sample (like\n\n14\n\n0.00.10.20.30.4More augOne augTimeUsed augmentationsAccuracySimCLR−TT+SimCLRSimCLR−TTToyBox category acc.A0.00.20.40.6More augOne augTimeUsed augmentationsAccuracy3DShapeE category acc.B0.00.20.4More augOne augTimeUsed augmentationsAccuracyVHE category acc.C0.40.50.6SimCLR−TT+SimCLRMethodAccuracyFront viewsSide views3/4 viewsMajority3DShapeE category acc., multi−singleA0.40.50.6SimCLR−TT+SimCLRMethodAccuracyFront viewsSide views3/4 viewsMajority3DShapeE category acc., single−singleBPublished as a conference paper at ICLR 2023\n\nSimCLR\n\nSimCLR-TT+\n\nM\n\nSD\n\nM\n\nSD\n\nVHE\n\n3DShapeE\n\nToyBox\n\nCORe50\n\n0.326\n\n0.523\n\n0.315\n\n0.554\n\n0.004\n\n0.004\n\n0.004\n\n0.070\n\n0.534\n\n0.617\n\n0.378\n\n0.629\n\n0.004\n\n0.002\n\n0.007\n\n0.081\n\ndf\n\n18\n\n17\n\n10\n\nt(df )\n\n112.22\n\n55.477\n\n18.409\n\n18 −2.228\n\np\n\n2.2 × 10−16 2.2 × 10−16 4.8 × 10−09 0.039\n\nTable 2: Detailed results of the statistical comparison between SimCLR and SimCLR-TT+ regarding test-set accuracy. P-values are bold.\n\nEnvironment\n\nSimCLR\n\nSimCLR-TT\n\nSimCLR-TT+\n\n3DShapeE\n\n0.617 ± 0.007\n\n0.598 ± 0.001\n\n0.674 ± 0.001\n\nVHE\n\n0.626 ± 0.002\n\n0.43 ± 0.007\n\n0.634 ± 0.003\n\nTable 3: Results of different augmentations with SimCLR and a ResNet18 on the VHE and 3DShapeE environments.\n\nin Appendix B.1); 4- we change the maximum rotation speed in the VHE from 360 degrees to 10 degrees, as we found it to work better in this case. We hypothesize that high rotation speeds favor the encoding of color information, since we did not observe this effect in 3DShapeE. We observe that SimCLR-TT+ gives the best performances in both environments.\n\nB.2 ABLATION OF DATA-AUGMENTATIONS\n\nIn this section, we do an ablation experiment of data-augmentations in ToyBox, 3DShapeE and VHE since we can choose to remove positive pairs that come from specific natural interactions. We also consider conventional data-augmentations in VHE and 3DShapeE since we found the complete set of augmentations to be suboptimal.\n\nShape Environment. In Figure 9A, we test different sets of augmentations in the 3DShapeE 3D, progressively and cumulatively combined. Since the combinatorial combination of all augmentations is computationally infeasible, we run a greedy forward algorithm to select the order of augmentations:\n\nFigure 9: A) Ablation analysis of all data-augmentations in the 3D Shape environment. Horizontal bars indicate the minimum and maximum accuracy over 3 seeds. We note: R=Rotations; D=Depth changes; J=Color Jittering; G=Grayscale; C=Crop and Resize; H=Horizontal flip. B) Analysis of the impact of the maximal change of depth distance d.\n\n15\n\n0.580.590.600.610.62RRCRCJRCJDRCJGRCJHRDUsed augmentationsAverage accuracyTest category acc.A0.450.500.550.600.00.10.20.30.40.5Million stepsAverage accuracyd = 0.075d = 0.15d = 0.3d = 0.45no depthTest category acc.BPublished as a conference paper at ICLR 2023\n\nFigure 10: A) Ablation analysis of all data-augmentations in the VHE. Horizontal bars indicate the minimum and maximum accuracy over 3 seeds. We note: R=Rotations; D=Depth changes; J=Color Jittering; G=Grayscale; C=Crop and Resize; H=Horizontal flip; E=Ego-motion with Ns=1 and room changes. B) Ablation study on the ToyBox environment. We apply SimCLR-TT+ but remove hodgepodge (-Ho), translations (-Tr), rotations (-R) and all time-based augmentations (-T). Experiments use the same training data, and we only change how we sample the positive pair (except for None and Depth experiments).\n\nat each step, we keep the augmentation with the largest increase in accuracy and stop when there is no more improvement. We find three main augmentations: rotations, crop and resize and color jittering. Adding other augmentations does not seem to have a significant large effect on the accuracy.\n\nTo better understand the impact of time-based augmentations during depth changes on the representation, we also combine rotations with depth changes (R+D). Adding depth changes achieve similar performance to the addition of crop and resize. We hypothesize it mimics the “resize” part of the crop. An additional analysis (Figure 9B) shows that the results are robust to the maximal speed of depth changes.\n\nVirtual Home Environment. We test different sets of augmentations in the VHE, following the same procedure as in Appendix B.2. In Figure 10A, we observe that rotations, ego-motion and color-based transformations are crucial for category recognition. Yet, we can not conclude other augmentations are useless, since it may be the redundancy with other augmentations that is harmful, as we found with depth changes in Figure 9A.\n\nToyBox environment. Figure 10B shows the impact of removing time-based augmentations in the ToyBox environment. We observe that removing augmentations based on translations (-Ho,Tr) and rotations (-Ho,R) both hurt the quality of the representation. Removing only some translations and translations (-Ho) do not affect the performance, presumably because other clips (with rotations and translations alone) already provide information about the shape of the object.\n\nB.3 ROTATIONS ALLOW FOR BETTER SHAPE GENERALIZATION THAN THE FLIP\n\nAUGMENTATION\n\nHere, we investigate how object rotations relate to the standard horizontal flip augmentation, since the flip resembles a 180-degrees rotation for symmetric objects and uniform background. In Figure 11A, we combine the flip augmentation and object rotations with other conventional augmentations. We see that when the object rotation speed is large enough (rot(360)), there is no additional benefit in using the horizontal flip augmentation. However, rotations always boost performance compared to the pure flipping augmentation. Thus, we conclude that using 3-D object rotations rather than flipping results in considerably better representation of object shape.\n\nB.4 HIGH-FREQUENCY OBJECT SWITCHES HURT CATEGORY RECOGNITION.\n\nHere, we study the impact of the object manipulation duration No. In 3DShapeE (Figure 11B), we observe a sweet spot at No = 10 when using a simple convolutional neural network (ConvNet). This effect seems to be architecture-dependent since we observe a consistent increase of accuracy as we\n\n16\n\n0.20.30.40.5NoneRREREJREJGREJGCREJGDREJGHUsed augmentationsAverage accuracyVHE category acc.A0.00.10.20.30.4All−Ho−Ho,R−Ho,Tr−TUsed augmentationsToyBox category acc.BPublished as a conference paper at ICLR 2023\n\nFigure 11: A) 3DShapeE test category accuracy according to different combinations of flip and rotation speeds. B) 3DShapeE test category accuracy according to different manipulation duration No with a simple ConvNet and a ResNet18.\n\nincrease No with a ResNet18. Importantly, in both cases, we observe that high-frequency object switches (low No) hurt the downstream object categorization.\n\nIn CORe50 (Table 4), the combined randomwalk (TT+ rw) method is sensitive to a high frequency of object switches within the same session, which considerably improves validation set category accuracy (A). We assume that this is because the additional contrastive information is considerably higher for the randomwalk method, whereas uniform view sampling already achieves some degree of background variation due to sampling contrasts from one whole session compared to the neighboring frames. For test set accuracies, we observe a similar trend, albeit not as pronounced (B). Note how test-accuracies for the random walk procedure are considerably higher than for the uniform sampling.\n\nB.5\n\nINFLUENCE OF THE SESSION SWITCH FREQUENCY IN THE CORE50 ENVIRONMENT\n\nDetailed analysis of varying the session frequency in the CORe50 environment. To complete the results from the CORe50 dataset we add overview tables and a more thorough analysis regarding the randomwalk procedure. The mean and standard errors of the analysis corresponding to Figure 4D can be found in Table 4 together with a supervised and SimCLR baseline trained with the same hyperparameters. The uniform view sampling (uni) performs best on the validation set, whereas the two combined methods (*-TT+) deliver the best generalization capabilities across splits.\n\nalgorithm\n\n2\n\nsupervised\n\n.530 ± .080\n\nSimCLR\n\n.544 ± .030\n\n5\n\n\"\n\n\"\n\nE [No]\n\n10\n\n\"\n\n\"\n\n20\n\n\"\n\n\"\n\n50\n\n\"\n\n\"\n\n-TT rw\n\n-TT uni\n\n-TT+ rw\n\n-TT+ uni\n\n.229 ± .018\n\n.327 ± .033\n\n.386 ± .046\n\n.429 ± .055\n\n.427 ± .052\n\n.426 ± .055\n\n.435 ± .060\n\n.440 ± .056\n\n.450 ± .059\n\n.440 ± .056\n\n.501 ± .032\n\n.589 ± .030\n\n.615 ± .029\n\n.629 ± .031\n\n.638 ± .032\n\n.618 ± .084\n\n.608 ± .036\n\n.620 ± .036\n\n.616 ± .036\n\n.616 ± .033\n\nTable 4: Category accuracy and standard error on CORe50, based on SimCLR, ps = 0.95. Standard error based on five different training/testing splits. Training occurred for 100 epochs, batchsize 512. Best performance is highlighted in bold, cf. Figure 4D.\n\n17\n\n0.400.450.500.550.600.00.10.20.3Million stepsrot(360) + fliprot(360)fliprot(90) + fliprot(90)3DShapeE category acc.A0.30.40.50.625102050Manipulation duration NAccuracyAccuracyConvNetResNet183DShapeE category acc.BoPublished as a conference paper at ICLR 2023\n\nFigure 12: Analysis of the SimCLR-TT+ approach for different values of ps, CORe50 Environment. Error bars indicate standard error based on three different training/testing splits.\n\nWe also report means and standard errors for category and session accuracy (Table 5) of our randomwalk procedure when evaluated with different values for ps and po.\n\nE [Ns]\n\n2\n\n5\n\nCategory accuracy\n\nE [No]\n\n10\n\n20\n\n50\n\n2\n\n5\n\n10\n\n20\n\n50\n\n.993 ± .001\n\n.994 ± .001\n\n.994 ± .001\n\n.997 ± .000\n\n.998 ± .000\n\n.898 ± .009\n\n.974 ± .002\n\n.986 ± .000\n\n.989 ± .001\n\n.992 ± .001\n\n.572 ± .005\n\n.909 ± .005\n\n.956 ± .004\n\n.972 ± .002\n\n.981 ± .003\n\n.439 ± .004\n\n.629 ± .008\n\n.814 ± .010\n\n.908 ± .009\n\n.939 ± .004\n\n.401 ± .007\n\n.454 ± .006\n\n.522 ± .007\n\n.617 ± .003\n\n.711 ± .006\n\nE [Ns]\n\n2\n\n5\n\nSession accuracy\n\nE [No]\n\n10\n\n20\n\n50\n\n2\n\n5\n\n10\n\n20\n\n50\n\n.572 ± .010\n\n.535 ± .011\n\n.540 ± .015\n\n.507 ± .008\n\n.518 ± .010\n\n.665 ± .012\n\n.644 ± .014\n\n.658 ± .018\n\n.671 ± .011\n\n.704 ± .011\n\n.646 ± .016\n\n.695 ± .019\n\n.718 ± .016\n\n.718 ± .014\n\n.707 ± .009\n\n.670 ± .026\n\n.673 ± .003\n\n.716 ± .021\n\n.740 ± .013\n\n.735 ± .013\n\n.704 ± .012\n\n.689 ± .025\n\n.732 ± .012\n\n.728 ± .015\n\n.731 ± .020\n\nTable 5: Category accuracy and standard error on CORe50, SimCLR-TT algorithm with randomwalk sampling procedure. Standard error based on three different training/testing splits. Training occurred for 100 epochs, batchsize 512, cf. Figure 5C, D.\n\nInfluence of session changes on combined augmentations. Since our main analysis with the CORe50 dataset aims to give a solid overview over the presented approaches, we have picked a medium level of session changes as the default (ps = 0.95, E [Ns|ps] = 20). The results show that the combined approaches (*-TT+) outperform the conventional SimCLR method, but we were also interested in how sensitive the novel methods are to the ps parameter. Results of that analysis are depicted in Figure 12. In line with results from our Virtual Home Environment experiments, we observe that for very low values of Ns the internal representation contains less information about the session for the randomwalk approach (C,D). The uniform method, however, shows robustness to this effect.\n\n18\n\nTT+TT+Published as a conference paper at ICLR 2023\n\nC ASSETS\n\nC.1\n\n3D OBJECTS IN THE VIRTUAL HOME ENVIRONMENT\n\nWe import two versions of objects into the ThreeDWorld Software (TDW):\n\n3DShapeE. This is the grey version of objects. We export the 3D models to .obj format and remove the texture files. We discard objects that took too long to process in TDW.\n\nTextured objects. This is the colored version of objects. In order to import them in TDW, we apply a series of operators: 1) we reduce the complexity of some meshes to make the next step computationally feasible and reduce the TDW processing complexity; 2) we manually bake most of the 3D models to obtain a single texture file with the meshes; 3) we import the models into TDW; 4) we resize the models to marginalize the effect of size on category recognition and avoid harmful collisions. We visually check the quality of all models. When we do not manage to obtain good-looking textures for some objects, we remove them from our dataset (< 15% of the dataset). The tree category does not contain enough good-looking objects, so we remove the whole category.\n\nC.2 TEST DATASETS OF THE VIRTUAL HOME ENVIRONMENT.\n\nOur House test set is composed of 7,740 images, from 1,548 objects distributed into 105 categories. By showcasing the category “elephant” in Figure 13 we exhibit the diversity of objects within one category. The elephant category includes flying elephants, bipedal elephants, red elephants, differently positioned elephants or geometrically simplified elephants. We also show the sequence of input images in the VHE according to different rotation speeds.\n\nIn Figure 14, we display the three test houses used in Figure 5B,C. House scenes were taken from pre-provided bundles in TDW and we added the agent and objects in the scene similarly to Figure 1.\n\nThe underlying ThreeDWorld (TDW) software used is licenced under BSD 2-Clause.\n\nC.3 TOYBOX ENVIRONMENT\n\nThe ToyBox environment is a collection of video-clips. We sample two frames per second from each clip, resulting in 159,627 images. Converted clips contain 2 to 50 sampled frames, which corresponds to the No parameter in the CORe50 environment. Two thirds of objects are used for the training set and the rest for the test set.\n\nD HYPERPARAMETERS\n\nFor all conducted experiments, we apply a weight decay of 10−6 and update weights with the AdamW optimizer (Loshchilov and Hutter, 2018) and a learning rate of 5 · 10−4.\n\nVirtual Home Environment and 3D Shape Environment. The agent in the Virtual Home Environment perceives the world around it as 128 × 128 pixel RGB images. Unless stated otherwise, these images are encoded by a succession of convolutional layers with the following [channels, kernel size, stride, padding] structure: [64, 8, 4, 2], [128, 4, 2, 1], [256, 4, 2, 1], [256, 4, 2, 1]. The output is grouped by an average pooling layer and a linear layer ending with 128 units. Each convolution layer is followed by a non-linear ReLU activation function and a dropout layer (p = 0.5) to prevent over-fitting. We do not use projection heads. We consider a temperature hyperparameter of 0.1 for SimCLR. We use a batch size of 256 and a buffer size of 100,000. Average training time was 72 hours per run for VHE, and 20 hours for the 3DShapeE. All experiments ran on GPUs of type NVIDIA V100.\n\nWe did a hyperparameter search of the best crop minimal size in {0.08, 0.2, 0.5, 0.75} for the Combined and SimCLR aug experiments.\n\nCORe50 Dataset. The additional diversity of the CORe50 training data required a more capable encoder network, which we chose to be a ResNet-18 (He et al., 2016). The encoder transforms the\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 13: All objects of the category “elephant” used in our Virtual Home Environment.\n\nFigure 14: Top view of houses, without ceiling, used only when testing category recognition in unfamiliar houses experiments. A) House 2; B) House 3; C) House 4.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\ninput images into a 128-dimensional latent representation, which is followed by a 2-layer linear projection head [256, 128]. For the projection head, we use batch normalization and a ReLU activation function in-between the two layers. We chose a batch size of 512 and training was done for 100 epochs on a GPU of type NVIDIA V100 or NVIDIA RTX2070 SUPER. Average training time was 17.5h per run. For BYOL we use τ = 0.996 and for VICReg the default error weighting as described in Bardes et al. (2022). The CORe50 dataset is licensed under CC-BY-4.0.\n\nToyBox environment. We use the same hyperparameters as for the CORe-50 experiments. However, we made an ablation study of the crop minimal size hyperparameter to adapt to the small size and off-center position of the objects. We found a value of 0.5 to be the best in {0.08, 0.2, 0.5, None}.\n\n21",
    "reference": "# Summary Of The Paper\n\nThis paper proposes and experiments with the idea of time-based augmentations: Human infants manipulate and move objects in front of themselves, move their eyes, making multiple fixations on an object, and carry objects into different rooms, providing independence of object appearance from background. All of these manipulations can be thought of as time-based augmentations of the data. Using these ideas with self-supervised learning provides a biologically-inspired way of learning representations of objects. The paper tests these ideas on three different self-supervised algorithms on four different datasets, and finds that adding time-based augmentations to standard ones significantly increases performance. The improvements are not in the 1-2% range that some papers show; rather, many results show a 10-20% increase, suggesting a real impact on self-supervised learning. Interestingly, time-based augmentations are not by themselves particularly effective (except in simulation); it is only in combination with standard augmentations that there is a substantial effect.\n\nI have read the other reviews (which I think are substantially unfair, and/or are from reviewers who, for some reason I fail to understand, want to kill the paper), the authors' rebuttals, and skimmed the updated paper. \n\nMy evaluation still stands: This is an excellent and exciting paper, well worthy of publication. It could be a good talk at the conference.\n\n# Strength And Weaknesses\n\nStrengths:\n\n+ The idea is well-motivated by data from infants\n+ The improvements are substantial, and impact three different categories of self-supervised systems.\n+ The analysis of the different factors affecting the outcome is fairly good.\n+ The method is tested in simulation, where the kinds of interaction can be controlled, and on real-world video of object manipulations.\n\nWeaknesses, with concrete, actionable feedback\n\n- The idea is not completely novel; several authors have proposed similar ideas. The difference here is the thoroughness of testing, the systematic analysis, and the very large datasets used, generated via simulation.\n\n- The paper is somewhat dense, due to all the experiments conducted, and is a little difficult to follow at times.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: Due to the complexity of the experiments, it is not always easy to follow what was being done.\n\nQuality: This is a very thorough analysis of the effects of \"natural\" interactions on object recognition. The results are impressive.\n\nNovelty: While temporal augmentation has been done before, this paper investigates it in a very thorough and systematic manner.\n\nReproducibility: the code will be made available. \n\nI didn't see any figures showing what the stimuli look like for 3DShapeE? Did I miss it?\n\np2: have shown -> have been shown\nallowing to discard _> allowing the model to discard\nproposed to use -> proposed using\nproposes to learn -> proposes learning\n\np5: we approximate different kind of -> we approximate different kinds of\n\np6:, near top: replace the neither nor construction with either or.\n\np7: 2nd line from bottom: It means that -> This means that \n\nI'm unclear on what the \"speed of object rotations\" means in terms of the sequence of stimuli. Does this mean that object pairs may be in completely different orientations? A picture would help a lot here. It could be in the appendix, if necessary. \n\np8: allows to encode -> allows the model to encode\n\nencoding the shape similarity makes easier category ->\nencoding the shape similarity enables easier category\n\nsimilarity makes easier -> similarity enables easier \n\np9: embodied agents that interacts -> embodied agents that interact\n\n# Summary Of The Review\n\nThis is an exciting paper, showing that \"natural\" temporal interactions with objects significantly boosts the performance of self-supervised algorithms.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nNERF-SOS: ANY-VIEW SELF-SUPERVISED OBJECT SEGMENTATION ON COMPLEX SCENES\n\nZhiwen Fan1, Peihao Wang1, Yifan Jiang1, Xinyu Gong1, Dejia Xu1, Zhangyang Wang1 1Department of Electrical and Computer Engineering, University of Texas at Austin {zhiwenfan,atlaswang}@utexas.edu\n\nABSTRACT\n\nNeural volumetric representations have shown the potential that Multi-layer Perceptrons (MLPs) can be optimized with multi-view calibrated images to represent scene geometry and appearance without explicit 3D supervision. Object segmentation can enrich many downstream applications based on the learned radiance field. However, introducing hand-crafted segmentation to define regions of interest in a complex real-world scene is non-trivial and expensive as it acquires per view annotation. This paper carries out the exploration of self-supervised learning for object segmentation using NeRF for complex real-world scenes. Our framework, called NeRF with Self-supervised Object Segmentation (NeRF-SOS), couples object segmentation and neural radiance field to segment objects in any view within a scene. By proposing a novel collaborative contrastive loss in both appearance and geometry levels, NeRF-SOS encourages NeRF models to distill compact geometry-aware segmentation clusters from their density fields and the self-supervised pre-trained 2D visual features. The self-supervised object segmentation framework can be applied to various NeRF models that both lead to photo-realistic rendering results and convincing segmentation maps for both indoor and outdoor scenarios. Extensive results on the LLFF, BlendedMVS, CO3Dv2, and Tank & Temples datasets validate the effectiveness of NeRF-SOS. It consistently surpasses 2D-based self-supervised baselines and predicts finer object masks than existing supervised counterparts. Code is available at: https://github.com/VITA-Group/NeRF-SOS.\n\n1\n\nINTRODUCTION\n\nScene modeling and representation are essential to the computer vision community. For instance, portable Augmented Reality (AR) devices such as the Magic Leap One can reconstruct the scene geometry and localize users (DeChicchis, 2020). but they often struggle to comprehend the surrounding objects. This limitation poses challenges when designing interactions between humans and the environment. Although human-annotated data from diverse environments could mitigate the hurdles of understanding and segmenting the surrounding objects, collecting such data is often costly and time-consuming. Therefore, there is growing interest in developing intelligent geometry modeling frameworks that can learn from unsupervised or self-supervised techniques.\n\nRecently, neural volumetric rendering techniques, such as neural radiance field (NeRF) and its variants (Mildenhall et al., 2020a; Zhang et al., 2020; Barron et al., 2021), have demonstrated exceptional performance in scene reconstruction, utilizing multi-layer perceptrons (MLPs) and calibrated multi-view images to generate fine-grained, unseen views. While several recent works have explored scene understanding with these techniques (Vora et al., 2021; Yang et al., 2021; Zhi et al., 2021), they often require either dense view annotations to train a heavy 3D backbone for capturing semantic representations (Vora et al., 2021; Yang et al., 2021), or human intervention to provide sparse semantic labels (Zhi et al., 2021). Although recent self-supervised object discovery approaches on neural radiance fields (Yu et al., 2021c; Stelzner et al., 2021) have been effective in decomposing objects on synthetic indoor data, there is still a significant gap to be filled in applying these approaches to complex real-world scenarios.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Visual examples. From left to right: ground truth color images, annotated object masks, object masks rendered by NeRF-SOS, 2D image co-segmentation using DINO (Amir et al., 2021), and object masks rendered by Semantic-NeRF (Zhi et al., 2021), respectively. NeRF-SOS outperforms the previous methods by generating object masks with more precise local details.\n\nIn contrast to previous works, we investigate a more generic setting, by using general NeRF models to segment 3D objects in real-world scenes. We propose a new self-supervised object segmentation framework for NeRF that utilizes a collaborative contrastive loss. Our approach combines features from a self-supervised pre-trained 2D backbone (“appearance level”) with knowledge distilled from the geometry cues of a scene, using the density field of NeRF representations (“geometry level”). To be more specific, we adopt a self-supervised approach to learn from a pre-trained 2D feature extractor, such as DINO-ViT (Caron et al., 2021) and incorporate the inter-view visual correlations to generate distinct segmentation feature clusters within the NeRF framework. We introduce a geometry-level contrastive loss by formulating a geometric correlation volume between NeRF’s density field and the segmentation clusters to make the learned feature clusters aware of scene geometry. Our proposed self-supervised object segmentation framework tailored for NeRF, dubbed NeRF-SOS, serves as a general implicit framework and can be applied to any existing NeRF models with end-to-end training. We implement and evaluate NeRF-SOS, using vanilla NeRF (Mildenhall et al., 2020a) for real-world forward-facing datasets (LLFF (Mildenhall et al., 2019)), object-centric datasets (BlendedMVS (Yao et al., 2020) and CO3Dv2 (Reizenstein et al., 2021)); and using NeRF++ (Zhang et al., 2020) for outdoor unbounded dataset (Tank and Temples (Riegler & Koltun, 2020)). Experiments show that NeRF-SOS significantly outperforms existing object discovery methods and produces view-consistent segmentation clusters: a few examples are shown in Figure 1.\n\nWe summarize the main contributions as follows:\n\n• We explore how to effectively apply the self-supervised learned 2D visual feature for 3D representations through an appearance contrastive loss, which forms compact feature clusters to allow any-view object segmentation in complex real-world scenes.\n\n• We propose a new geometry contrastive loss for object segmentation. By leveraging its density field, our proposed framework injects scene geometry into the segmentation field, making the learned segmentation clusters geometry-aware.\n\n• The proposed collaborative contrastive framework can be implemented upon NeRF and NeRF++, for object-centric, indoor, and unbounded real-world scenarios. Experiments show that our self-supervised object segmentation quality consistently surpasses 2D object discovery methods and even yields finer segmentation results than the supervised NeRF counterpart (Zhi et al., 2021).\n\n2 RELATED WORK\n\nNeural Radiance Fields NeRF is first proposed by Mildenhall et al. (Mildenhall et al., 2020b), which models the underlying 3D scenes as continuous volumetric fields of color and density via layers of MLP. The input of a NeRF is a 5D vector, containing a 3D location (x, y, z) and a 2D viewing direction (θ, φ). Several following works emerge trying to address its limitations and improve\n\n2\n\nColor ImagesMask AnnotationsOursDINO-CoSegSemanticNeRFPublished as a conference paper at ICLR 2023\n\nthe performance, such as unbounded scenes training (Zhang et al., 2020; Barron et al., 2021), fast training (Sun et al., 2021; Deng et al., 2021), efficient inference (Rebain et al., 2020; Liu et al., 2020; Lindell et al., 2020; Garbin et al., 2021; Reiser et al., 2021; Yu et al., 2021a; Lombardi et al., 2021), better generalization (Schwarz et al., 2020a; Trevithick & Yang, 2020; Wang et al., 2021b; Chan et al., 2020; Yu et al., 2021b; Johari et al., 2021; Varma T et al., 2022), supporting unconstrained scene (Martin-Brualla et al., 2020; Chen et al., 2021; Xu et al., 2022), editing (Liu et al., 2021; Jiakai et al., 2021; Wang et al., 2021a; Jang & Agapito, 2021; Kundu et al., 2022; Fan et al., 2022), multi-task learning (Zhi et al., 2021). In this paper, we treat NeRF as a powerful implicit scene representation and study how to segment objects from a complex real-world scene without any supervision.\n\nObject Co-segmentation without Explicit Learning Our work aims to discover and segment visually similar objects in the radiance field and render novel views with object masks. It is close to the object co-segmentation (Rother et al., 2006) which aims to segment the common objects from a set of images (Li et al., 2018). Object co-segmentation has been widely adopted in computer vision and computer graphics applications, including browsing in photo collections (Rother et al., 2006), 3D reconstruction (Kowdle et al., 2010), semantic segmentation (Shen et al., 2017), interactive image segmentation (Rother et al., 2006), object-based image retrieval (Vicente et al., 2011), and video object tracking/segmentation (Rother et al., 2006). The authors in (Rother et al., 2006) first shows that segmenting two images outperforms the independent counterpart. This idea is analogous to the contrastive learning way in later approaches. Especially, the authors in (H ́enaff et al., 2022) propose the self-supervised segmentation framework using object discovery networks. (Sim ́eoni et al., 2021) localizes the objects with a self-supervised transformer. The paper (Hamilton et al., 2022) introduces the feature correspondences that distinguish between different classes. Most recently, a new co-segmentation framework based on DINO feature (Amir et al., 2021) has been proposed and achieves better results on object co-segmentation and part co-segmentation. However, extending 2D object discovery to NeRF is non-trivial as they cannot learn the geometric cues in multi-view images. uORF (Yu et al., 2021c) and ObSuRF (Stelzner et al., 2021) use slot-based CNN encoders and object-centric latent codes for unsupervised 3D scene decomposition. COLF (Smith et al., 2022) proposes a light field compositor module to accelerate NeRF-based object decomposition. Although they enable unsupervised 3D scene segmentation and novel view synthesis, experiments are on synthetic datasets with pre-defined categories, leaving a gap for complex real-world applications. NVOS (Ren et al., 2022) leverages users’ scribbles for weakly-supervised object segmentation. The concurrent work, RFP (Liu et al., 2022) enables label-free object segmentation in real-world scenes with a propagation strategy. Panoptic NeRF Field (Kundu et al., 2022) proposes to represent each object instance using a separate MLP with supervisions from other models. N3F (Tschernezki et al., 2022) minimizes the distance between NeRF’s rendered feature and 2D feature for scene editing. Most recently, DFFs (Kobayashi et al., 2022) propose to distill the visual feature from supervised CLIP-LSeg or self-supervised DINO into a 3D feature field via an element-wise feature distance loss function. It can discover the object using a query text prompt or a patch. In contrast, we design a new collaborative contrastive loss on both appearance and geometry levels to find the objects with a similar appearance and location without any annotations. The collaborative design is general and can be plug-and-play to different NeRF models.\n\n3 METHOD\n\nOverview This paper presents an extension to existing NeRF models to enable object segmentation. As shown in Figure 2, we augment NeRF models by appending a parallel segmentation branch to predict point-wise implicit segmentation features. Specifically, NeRF-SOS can render depth (σ), segmentation (s), and color (c). We then use a self-supervised pre-trained framework (such as DINO-ViT (Caron et al., 2021)) to generate a feature tensor (f ) from the rendered color patch (c), constructing an appearance-segmentation correlation volume between f and s. Similarly, we instantiate a geometry-segmentation correlation volume using σ and s. By generating positive/negative pairs from different views, we can distill the correlation patterns in both the visual feature and scene geometry into the compact segmentation field s. During inference, we use a clustering operation (such as K-means) to generate object masks based on the rendered feature field.\n\n3.1 PRELIMINARIES\n\nNeural Radiance Fields NeRF (Mildenhall et al., 2020a) represents 3D scenes as radiance fields via several layer MLPs, where each point has a value of color and density. Such a radiance field can\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: The overall pipeline of the proposed NeRF-SOS. Input with rays cast from multiple views, we render the corresponding color patch (c), segmentation patch (s), and depth patch (σ). Then, appearance-segmentation correlations and geometry-segmentation correlations are used to formulate a collaborative contrastive loss, enabling NeRF-SOS to render object masks from any viewpoint using the distilled segmentation field.\n\nbe formulated as F : (x, θ) (cid:55)→ (c, σ), where x ∈ R3 is the spatial coordinate, θ ∈ [−π, π]2 denotes the viewing direction, and c ∈ R3, σ ∈ R+ represent the RGB color and density, respectively. To form an image, NeRF traces a ray r = (o, d, θ) for each pixel on the image plane, where o ∈ R3 denotes the position of the camera, d ∈ R3 is the direction of the ray, and θ ∈ [−π, π]2 is the angular viewing direction. Afterwards, NeRF evenly samples K points {ti}K i=1 between the near-far bound [tn, tf ] along the ray. Then, NeRF adopts volumetric rendering and numerically evaluates the ray integration (Max, 1995) by the quadrature rule:\n\nC(r) =\n\nK (cid:88)\n\nk=1\n\nT (k)(1 − exp(−σkδk))ck where T (k) = exp\n\n−\n\n(cid:32)\n\n(cid:33)\n\nσlδl\n\n,\n\nk−1 (cid:88)\n\nl=1\n\n(1)\n\nwhere δk = tk+1 − tk are intervals between sampled points, and (ck, σk) = F (o + tkd, θ) are output from the neural network. With this forward model, NeRF optimizes the photometric loss between rendered ray colors and ground-truth pixel colors defined as follows: Lphotometric = (cid:80)\n\nwhere R defines a dataset collecting all pairs of ray and ground-truth\n\n(r, (cid:98)C)∈R\n\n(cid:13) (cid:13) (cid:13)C(r) − (cid:98)C\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\ncolors from captured images.\n\n3.2 CROSS VIEW APPEARANCE CORRESPONDENCE\n\nSemantic Correspondence across Views Tremendous works have explored and demonstrated the importance of object appearance when generating compact feature correspondence across views (H ́enaff et al., 2022; Li et al., 2018). This peculiarity is then utilized in self-supervised 2D semantic segmentation frameworks (H ́enaff et al., 2022; Li et al., 2018; Chen et al., 2020) to generate semantic representations by selecting positive and negative pairs with either random or KNN-based rules (Hamilton et al., 2022). Drawing inspiration from these prior arts, we construct the visual feature correspondence for NeRF at the appearance using a heuristic rule. To be more specific, we leverage the self-supervised model (e.g., DINO-ViT (Caron et al., 2021)) learned from 2D image sets to distill the rich representations into compact and distinct segmentation clusters. A four-layer MLP is appended to segment objects in the radiance field parallel to the density and appearance branches. During training, we first render multiple image patches from different viewpoints using Equation 1, then we feed each batch into DINO-ViT to generate feature tensors of shape H ′ ×W ′ ×C ′. They are then used to generate the appearance correspondence volume (Teed & Deng, 2020; Hamilton et al., 2022) across views, measuring the similarity between two regions of different views:\n\nFhwh′w′ =\n\n(cid:88)\n\nc\n\nfchw |fhw|\n\nf ′ |f ′\n\nch′w′\n\nh′w′|\n\n,\n\n(2)\n\nwhere f and f ′ stand for the extracted DINO feature from two random patches in different views, c is the feature dimension of DINO, (h, w) and (h′, w′) denote the spatial information on feature tensor for f and f ′, respectively, and the c traverses through the feature channel dimension.\n\nDistilling Semantic Correspondence into Segmentation Field The correspondence volume F from DINO has been verified it has the potential in unsupervised semantic segmentations (Hamilton\n\n4\n\nx,y,z,θ,φMLPMLPDensity:Color:cσMLPMLPSegmentation:sDINOgeometry-segmentationAppearance-segmentationcorrelationGσsGeo.-seg.corr. volumesFApp.-seg.corr.volumecViewingfrustumRenderedRenderedRenderedRenderedGeometry-segmentationcorrelationappearance-segmentationPublished as a conference paper at ICLR 2023\n\net al., 2022). We next explore how to learn a segmentation field s by leveraging F . Inspired by CRF and STEGO (Hamilton et al., 2022) where they refine the initial predictions using color or feature-correlated regions in the 2D image. We propose to append an extra segmentation branch to predict the segmentation field, formulating segmentation correspondence volume by leveraging its predicted segmentation logits using the same rule with Equation 2. Then, we construct the appearance-segmentation correlation aims to enforce the elements of s and s′ closer if f and f ′ are tightly coupled, where the expression with and without the superscript indicates two different views. The volume correlation can be achieved via an element-wise multiplication between S and F , and thereby, we have the appearance contrastive loss Lapp:\n\nCapp(r, b) = −\n\n(cid:88)\n\nhwh′w′\n\n(Fhwh′w′ − b)Shwh′w′\n\nLapp = λidCapp(rid, bid) + λnegCapp(rneg, bneg)\n\n(3)\n\n(4)\n\ns′ |s′\n\nc\n\nch′w′\n\nschw |shw|\n\nwhere Shwh′w′ = (cid:80) h′w′ | indicates the segmentation correspondence volume between two views, r is the cast ray fed into NeRF, b is a hyper-parameter to control the positive and negative pressure. λid and λneg indicate loss force between identity pairs (positive) and distinct pairs (negative). The intuition behind the above equation is that minimizing Lapp with respect to S, to enforce entries in segmentation field s to be large when F − b are positive items and pushes entries to be small if F − b are negative items. Discover Patch Relationships To construct Equation 4, we build a cosine similarity matrix to effectively discover the positive/negative pairs of given patches. For each matrix, we take N randomly selected patches as inputs and adopt a pre-trained DINO-ViT to extract meaningful representations. We use the [CLS] token from ViT architecture to represent the semantic features of each patch and obtain N positive pairs by the diagonal entries and N negative pairs by querying the lowest score in each row. An example using three patches from different views is shown in Figure 3. Similar to Tumanyan et al. (2022), we observe that the [CLS] token from a self-supervised pre-trained ViT backbone can capture high-level semantic appearances and can effectively discover similarities between patches during the proposed end-to-end optimization process.\n\nFigure 3: Cosine similarity matrix calculated on scene Fortress.\n\n3.3 CROSS VIEW GEOMETRY CORRESPONDENCE\n\nConstructing the “appearance-segmentation correlation” enables the clustered features with similar appearance together. However, appearance cue alone may cause spatial discontinuities, as DINO-ViT may overfocus to capture semantic parts rather than making the clusters spatial smooth (see Figure 7). Therefore, we propose geometric correlation volume to penalize discontinuities between neighboring points, by formulating the attractive/repulsive force using point-wise distance.\n\nGeometry Correspondence across Views Apart from distilling the visual feature from DINO into the segmentation field s, we propose to leverage the density field that already exists in NeRF models to formulate a new geometry contrastive loss to encourage spatial coherence. Specifically, given a batch of M cast ray r as NeRF’s input, we can obtain the density field of size M × K where K indicates the number of sampled points along each ray. By accumulating the discrete bins along each ray, we can roughly represent the density field as a single 3D point:\n\np = ro + rd · D\n\nD(r) =\n\nK (cid:88)\n\nk=1\n\nT (k)(1 − exp(−σkδk))tk\n\n(5)\n\n(6)\n\nwhere p is the accumulated 3D point along the ray, D is the estimated depth value of the corresponding pixel index. Inspired by Point Transformer (Zhao et al., 2021) which uses point-wise distance as representation, we utilize the estimated point position as a geometry cue to formulate a new geometry\n\n5\n\n1.00.830.710.831.00.680.710.681.0Published as a conference paper at ICLR 2023\n\nTable 1: Quantitative comparison of the novel view synthesis and object segmentation of LLFF dataset on the scenes Flower and Fortress.\n\nScene “Flower”\n\nPSNR ↑\n\nSSIM ↑ LPIPS ↓ NV-ARI ↑\n\nIoU(BG) ↑\n\nIoU(FG) ↑ mIoU ↑\n\nIEM (Savarese et al., 2021) DOCS (Li et al., 2018) DINO+CoSeg (Amir et al., 2021) NeRF-SOS (Ours) Semantic-NeRF (Zhi et al., 2021) (Supervised) Scene “Fortress”\n\nIEM (Savarese et al., 2021) DOCS (Li et al., 2018) DINO+CoSeg (Amir et al., 2021) NeRF-SOS (Ours) Semantic-NeRF (Zhi et al., 2021) (Supervised)\n\n- -\n- 25.96 25.52 PSNR ↑\n\n- -\n- 29.78 29.78\n\n- -\n- 0.7717 0.7500 SSIM ↑ LPIPS ↓ NV-ARI ↑\n\n0.2666 0.0097 0.5946 0.9529 0.9104\n\n- -\n- 0.1502 0.1739\n\n0.7123 0.4824 0.9036 0.9869 0.9743 IoU(BG) ↑\n\n0.4267 0.2461 0.5961 0.9503 0.9090\n\n0.5695 0.3643 0.7498 0.9686 0.9417 IoU(FG) ↑ mIoU ↑\n\n- -\n- 0.8517 0.8578\n\n- -\n- 0.1079 0.0906\n\n0.3700 0.7412 0.9503 0.9802 0.9838\n\n0.7799 0.9329 0.9886 0.9955 0.9963\n\n0.4526 0.7265 0.9395 0.9751 0.9799\n\n0.6163 0.8297 0.9640 0.9853 0.9881\n\nFigure 4: Qualitative results on scene Flower and Fortress of LLFF dataset. In the fourth column, DINO-CoSeg mistakenly matches several discrete patches, as DINO has higher activation on just a few tokens, which may lead to view-inconsistent and disconnected co-segmentation results. ∗ superscript denotes the supervised method. DOCS and DINO-CoSeg are not able to perform novel view synthesis, and thus we perform rendering before segmentation using a vanilla NeRF.\n\nlevel correspondence volume across views by measuring point-wise absolute distance:\n\nGhwh′w′ =\n\n(cid:88)\n\nc\n\n1 |gchw − g′ ch′w′| + ε\n\n(7)\n\nwhere g and g′ are the estimated 3D point positions in two random patches of different views, c is 3, (h, w) and (h′, w′) denote the spatial location on feature tensor for g and g′, respectively. Injecting Geometry Coherence into Segmentation Field To inject the geometry cue from the density field to the segmentation field, we formulate segmentation correspondence volume S and geometric correspondence volume G using the same rule of Equation 3. By pulling/pushing positive/negative pairs for the geometry-segmentation correlation of Equation 8, we come up with a new geometry-aware contrastive loss Lgeo: (cid:88)\n\nCgeo(r, b) = −\n\n(Ghwh′w′ − b)Shwh′w′\n\n(8)\n\nhwh′w′\n\nLgeo = λidCgeo(rid, bid) + λnegCgeo(rneg, bneg)\n\n(9)\n\nSame as appearance contrastive loss, we find positive pairs and negative pairs via the pair-wise cosine similarity of the [CLS] tokens.\n\n3.4 OPTIMIZING WITH STRIDE RAY SAMPLING\n\nWe adopt patch-wise ray casting during the training process, while we also leverage a Stride Ray Sampling strategy, similar to prior works (Schwarz et al., 2020b; Meng et al., 2021) to handle GPU memory bottleneck. Overall, we optimize the pipeline using a balanced loss function: L = λ0Lphotometric + λ1Lapp + λ2Lgeo,\n\n(10)\n\nwhere λ0, λ1, and λ2 are balancing weights.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENT SETUP\n\nDatasets We evaluate all methods on four representative datasets: Local Light Field Fusion (LLFF) dataset (Mildenhall et al., 2019), BlendedMVS (Yao et al., 2020), CO3Dv2 (Reizenstein et al., 2021),\n\n6\n\nColor ImagesMask AnnotationsOursDINO-CoSegDOCSSem.NeRF*Published as a conference paper at ICLR 2023\n\nFigure 5: Novel view object segmentation results on object-centric datasets: BlendedMVS (the 1st row) and CO3Dv2 (the 2nd row). NeRF-SOS (the 3rd column) produces masks with finer details.\n\nFigure 6: Novel view object segmentation results on unbounded scene Truck. NeRF-SOS (the 3rd column) produces more view-consistent masks than other self-supervised methods. It even generates finer details than supervised Semantic-NeRF++ (see the gaps between wooden slats in the top row and the side view mirror in the bottom row).\n\nand Tank and Temples (T&T) dataset (Riegler & Koltun, 2020). Particularly, we use the forwardfacing scenes {Flower, Fortress} from LLFF dataset, two object-centric scenes from BlendedMVS dataset, two common objects {Backpack, Apple} captured by video sequences from CO3Dv2 dataset, and unbounded scene Truck from hand-held 360◦ capture large-scale Tank and Temples dataset. We choose these representative scenes because they contain at least one common object among most views. We manually labeled all views as a binary mask to provide a fair comparison for all methods and used them to train Semantic-NeRF. Foreground objects appearing in most views are labeled as 1, while others are labeled as 0. We train and test all methods with the original image resolutions.\n\nTraining Details We first implement the collaborative contrastive loss upon the original NeRF (Mildenhall et al., 2020a). In training, we first train NeRF-SOS without segmentation branch following the NeRF training recipe (Mildenhall et al., 2020b) for 150k iterations. Next, we load the weight and start to train the segmentation branch alone using the stride ray sampling for another 50k iterations. Model weights except the segmentation branch are kept frozen in the second phase. The loss weights λ0, λ1, λ2, λid, and λneg are set 0, 1, 0.01, 1 and 1 in training the segmentation branch. The segmentation branch is formulated as a four-layer MLP with ReLU as the activation function. The dimensions of hidden layers and the number of output layers are set as 256 and 2, respectively. The segmentation results are based on K-means clustering on the segmentation logits. We train Semantic-NeRF (Zhi et al., 2021) for 200k in total for fair comparisons. We randomly sample eight patches from different viewpoints (a.k.a batch size N is 8) in training. The patch size of each sample is set as 64 × 64, with the patch stride as 6. We use the official pre-trained DINO-ViT in a self-supervised manner on ImageNet dataset as our 2D feature extractor. The pre-trained DINO backbone is kept frozen for all layers during training. All hyperparameters are carefully tuned by a grid search, and the best configuration is applied to all experiments. All models are trained on an NVIDIA RTX A6000 GPU with 48 GB memory. We reconstruct N positives and N negatives pairs on the fly during training, given N rendered patches. More details can be found in the appendix.\n\nMetrics We adopt the Adjusted Rand Index in novel views as a metric to evaluate the clustering quality, noted as NV-ARI. We also adopt mean Intersection-over-Union to measure segmentation quality for both object and background, as we set the clusters with larger activation as foreground by DINO. To evaluate the rendering quality, we follow NeRF (Mildenhall et al., 2020a), adopting peak signal-to-noise ratio (PSNR), the structural similarity index measure (SSIM) (Wang et al., 2004), and learned perceptual image patch similarity (LPIPS) (Zhang et al., 2018) as evaluation metrics.\n\n7\n\nColorImagesMaskAnnotationsOursDINO-CoSegDOCSSem.NeRF*Color ImagesMask AnnotationsOursDINO-CoSegDOCSSem.NeRF*Published as a conference paper at ICLR 2023\n\nTable 2: Quantitative evaluation of the novel view synthesis and object segmentation on BlendedMVS and CO3Dv2 datasets, with several 2D object discovery frameworks and the supervised SemanticNeRF. Results on each dataset are averaged on all scenes.\n\nBlendedMVS\n\nPSNR ↑\n\nSSIM ↑ LPIPS ↓ NV-ARI ↑\n\nIoU(BG) ↑\n\nIoU(FG) ↑ mIoU ↑\n\nIEM (Savarese et al., 2021) DOCS (Li et al., 2018) DINO+CoSeg (Amir et al., 2021) NeRF-SOS (Ours) Semantic-NeRF (Zhi et al., 2021) (Supervised) CO3Dv2\n\nIEM (Savarese et al., 2021) DOCS (Li et al., 2018) DINO+CoSeg (Amir et al., 2021) NeRF-SOS (Ours) Semantic-NeRF (Zhi et al., 2021) (Supervised)\n\n- -\n- 23.86 23.84 PSNR ↑\n\n- -\n- 30.37 31.17\n\n- -\n- 0.8089 0.8080 SSIM ↑ LPIPS ↓ NV-ARI ↑\n\n- -\n- 0.1288 0.1339\n\n0.1339 0.7031 0.9074 0.9280 0.9359\n\n0.5615 0.9183 0.9692 0.9756 0.9803 IoU(BG) ↑\n\n0.3715 0.7030 0.917 0.9347 0.9391\n\n0.4665 0.8107 0.9431 0.9552 0.9597 IoU(FG) ↑ mIoU ↑\n\n- -\n- 0.9358 0.9405\n\n- -\n- 0.073 0.0603\n\n0.4784 0.8918 0.8199 0.9381 0.9399\n\n0.7983 0.9684 0.9559 0.9813 0.9821\n\n0.5708 0.8928 0.8222 0.9401 0.9410\n\n0.6845 0.9307 0.8891 0.9607 0.9615\n\nTable 3: Quantitative results of the object segmentation results on outdoor unbounded scene Truck, with several 2D object discovery frameworks and the supervised Semantic-NeRF.\n\nScene “Truck”\n\nPSNR ↑\n\nSSIM ↑ LPIPS ↓ NV-ARI ↑\n\nIoU(BG) ↑\n\nIoU(FG) ↑ mIoU ↑\n\nIEM (Savarese et al., 2021) DOCS (Li et al., 2018) DINO+CoSeg (Amir et al., 2021) NeRF-SOS (Ours) Semantic-NeRF++ (Zhi et al., 2021) (Supervised)\n\n- -\n- 22.20 21.08\n\n- -\n- 0.7000 0.6350\n\n- -\n- 0.2691 0.4114\n\n0.3341 0.1517 0.8571 0.9207 0.9674\n\n0.6791 0.6845 0.9408 0.9689 0.9869\n\n0.5998 0.2463 0.9080 0.9455 0.9782\n\n0.6395 0.4654 0.9244 0.9572 0.9826\n\n4.2 COMPARISONS\n\nSelf-supervised Object Segmentation on LLFF We build NeRF-SOS on the vanilla NeRF (Mildenhall et al., 2020a) to validate its effectiveness on LLFF datasets. Two groups of current object segmentation are adopted for comparisons: i. NeRF-based methods, including our NeRF-SOS, and supervised Semantic-NeRF (Zhi et al., 2021) trained with annotated masks; ii. image-based object co-segmentation methods: DINO-CoSeg (Amir et al., 2021) and DOCS (Li et al., 2018); and iii. single-image based unsupervised segmentation: IEM (Savarese et al., 2021) follows CIS (Yang et al., 2019) to minimize the mutual information of foreground and background. As imagebased segmentation methods cannot generate novel views, we pre-render the new views using NeRF and construct image pairs between the first image in the test set with others for DINO-CoSeg (Amir et al., 2021) and DOCS (Li et al., 2018). Evaluations on IEM also use pre-rendered color images.\n\nQuantitative comparisons against other segmentation methods are provided in Table 1, together with qualitative visualizations shown in Figure 4. These results convey several observations to us: 1). NeRF-SOS consistently outperforms image-based co-segmentation in evaluation metrics and view consistency. 2). Compared with SoTA supervised NeRF segmentation method (Semantic-NeRF (Zhi et al., 2021)), our method effectively segments the object within the scene and performs on par in both evaluation metrics and visualization. Self-supervised Object Segmentation on Object-centric Scenes For the object-centric datasets BlendedMVS and CO3Dv2, we uniformly select 12.5% of total images for testing. CO3Dv2 provides coarse segmentation maps using PointRend (Kirillov et al., 2020) while parts of the annotations are missing. Therefore, we manually create faithful binary masks for training the Semantic-NeRF and evaluations. As we can see in Table 2 and Figure 5, our self-supervised NeRF method consistently surpasses other 2D methods. We deliver more details comparisons in our supplementary materials. Self-supervised Object Segmentation on Unbounded Scene To test the generalization ability of the proposed collaborative contrastive loss, we implement it on NeRF++ (Zhang et al., 2020) to test with a more challenging unbounded scene. Here, we mainly evaluate all previously mentioned methods on scene Truck as it is the only scene captured surrounding an object provided by NeRF++. We re-implement Semantic-NeRF using NeRF++ as the backbone model for unbounded setting, termed Semantic-NeRF++. Compared with supervised Semantic-NeRF++, NeRF-SOS achieves slightly worse results on quantitative metrics (see Table 3). Yet from the visualizations, we see that NeRF-SOS yields quite decent segmentation quality. For example, 1). In the first row of Figure 6, NeRF-SOS can recognize the side view mirror adjacent to the truck. 2). In the second row of Figure 6, NeRF-SOS can distinguish the apertures between the wooden slats as those apertures have distinct depths than the neighboring slats, thanks to the geometry-aware contrastive loss. Further, we show the 3-center clustering results on the distilled segmentation field in Figure 8.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Object segmentations using three loss variants are shown in columns 3, 4, and 5: the collaborative loss (APP.+Geo.), appearance-only loss (App.); geometric-only loss (Geo.).\n\nFigure 8: Qualitative results on scene Truck with different cluster centers on its distilled segmentation field. Note that the cross-view visualized colors of multiple-center clustering are not corresponding to the subject ID, as we perform unsupervised clustering.\n\nTable 4: Experiments on multiple NeRF-SOS variants. We show the results of joint training of the NeRF and contrastive loss in the first row, NeRF-SOS with ResNet50 as feature extractor in the second row, and our final model in the last row.\n\nScene “Flower”\n\nPSNR ↑\n\nSSIM ↑ LPIPS ↓ NV-ARI ↑\n\nIoU(BG) ↑\n\nIoU(FG) ↑ mIoU ↑\n\nNeRF-SOS (Joint training) NeRF-SOS (ResNet) NeRF-SOS (Two-stage training)\n\n16.96 25.96 25.96\n\n0.4585 0.7717 0.7717\n\n0.7238 0.1502 0.1502\n\n0.1961 0.8672 0.9529\n\n0.7951 0.9421 0.9869\n\n0.2220 0.8827 0.9503\n\n0.5091 0.9124 0.9686\n\n4.3 ABLATION STUDY\n\nImpact of the Collaborative Contrastive Loss To study the effectiveness of the collaborative contrastive loss, we adopt two baseline models by only using appearance contrastive loss or geometric contrastive loss on NeRF++ backbone. As shown in Figure 7, we observe that the segmentation branch failed to cluster spatially continuous objects without geometric constraints (mIoU: 0.5029). Similarly, without visual cues, the model lost the perception of the central object (mIoU: 0.5516). Our full model constructs precise clusters with spatial coherence (mIoU: 0.9689). Joint Training with NeRF Optimization To demonstrate the advantages of two-stage training, we conduct an ablation study by jointly optimizing vanilla NeRF rendering loss and the proposed two-level collaborative contrastive loss. As shown in Table 4, both the novel view synthesis quality and the segmentation quality significantly decreased when we optimize the two losses together. We conjecture the potential reason to be the fact that the optimization process of NeRF training is affected by the conflicting update directions, the reconstruction loss and the contrastive loss, which remains a notorious challenge in the multi-task learning area (Yu et al., 2020). CNN-based Backbone for Feature Extraction DINO-ViT firstly concludes that ViT architecture can extract stronger semantic information than ConvNets when being self-supervised trained. To study its effect on discovering the semantic layout of scenes, we apply self-supervised ResNet50 (He et al., 2020) as backbones. The results in the second row of Table 4 imply that the ViT architecture is more suitable for our NeRF object segmentation in both expressiveness and pair-selection perspectives.\n\n5 CONCLUSION, DISCUSSION OF LIMITATION\n\nIn this paper, we introduce NeRF-SOS, a self-supervised framework that learns object segmentation for any view in complex real-world scenes. NeRF-SOS proposes a collaborative contrastive loss in both the appearance and geometry levels. Comprehensive experiments are conducted on four different types of datasets with state-of-the-art image-based object (co-)segmentation frameworks and fully supervised Semantic-NeRF. The results show that NeRF-SOS consistently outperforms imagebased methods and sometimes generates finer segmentation details than its supervised counterparts. However, similar to other scene-specific NeRF methods, one limitation of NeRF-SOS is that it cannot segment across scenes, which we plan to explore in future work.\n\n9\n\nColor Images(GT)Seg. Masks(Annotated)Seg. Masks(App.+Geo.)Seg. Masks(OnlyApp.)Seg. Masks(Only Geo.)Depth Maps(Estimated)ColorImage2Clusters3ClustersColorImage2Clusters3ClustersPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENT\n\nWe would like to express our gratitude to Xinhang Liu from HKUST and Zhongzheng Ren from UIUC for their invaluable contribution to the experimental work presented in this paper. Their expertise and time proved to be immensely helpful in conducting the necessary comparisons with their methods, especially since their codes were not publicly available. We greatly appreciate their generosity and support throughout the research process.\n\nREFERENCES\n\nShir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual\n\ndescriptors. arXiv preprint arXiv:2112.05814, 2021.\n\nJonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In IEEE International Conference on Computer Vision (ICCV), 2021.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9650–9660, 2021.\n\nEric Chan, Marco Monteiro, Peter Kellnhofer, Jiajun Wu, and Gordon Wetzstein.\n\npiGAN: Periodic implicit generative adversarial networks for 3D-aware image synthesis. https://arxiv.org/abs/2012.00926, 2020.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nXingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Feng Ying, Xuan Wang, and Jue Wang. Hallucinated\n\nneural radiance fields in the wild, 2021.\n\nJoseph DeChicchis. Semantic understanding for augmented reality and its applications. 2020.\n\nKangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views\n\nand faster training for free. arXiv preprint arXiv:2107.02791, 2021.\n\nZhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia Xu, and Zhangyang Wang. Unified implicit neural stylization. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XV, pp. 636–654. Springer, 2022.\n\nStephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf:\n\nHigh-fidelity neural rendering at 200fps. https://arxiv.org/abs/2103.10380, 2021.\n\nMark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T Freeman. Unsupervised semantic segmentation by distilling feature correspondences. arXiv preprint arXiv:2203.08414, 2022.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nOlivier J H ́enaff, Skanda Koppula, Evan Shelhamer, Daniel Zoran, Andrew Jaegle, Andrew Zisserman, Jo ̃ao Carreira, and Relja Arandjelovi ́c. Object discovery and representation networks. arXiv preprint arXiv:2203.08777, 2022.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv\n\npreprint arXiv:1503.02531, 2(7), 2015.\n\nWonbong Jang and Lourdes Agapito. Codenerf: Disentangled neural radiance fields for object categories. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 12949–12958, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nZhang Jiakai, Liu Xinhang, Ye Xinyi, Zhao Fuqiang, Zhang Yanshun, Wu Minye, Zhang Yingliang, Xu Lan, and Yu Jingyi. Editable free-viewpoint video using a layered neural representation. In ACM SIGGRAPH, 2021.\n\nMohammad Mahdi Johari, Yann Lepoittevin, and Franc ̧ois Fleuret. Geonerf: Generalizing nerf with\n\ngeometry priors. arXiv preprint arXiv:2111.13539, 2021.\n\nAlexander Kirillov, Yuxin Wu, Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9799–9808, 2020.\n\nSosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via\n\nfeature field distillation. arXiv preprint arXiv:2205.15585, 2022.\n\nAdarsh Kowdle, Dhruv Batra, Wen-Chao Chen, and Tsuhan Chen.\n\ninteractive cosegmentation for object of interest 3d modeling. In European Conference on Computer Vision, pp. 211–224. Springer, 2010.\n\nimodel:\n\nAbhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas J Guibas, Andrea Tagliasacchi, Frank Dellaert, and Thomas Funkhouser. Panoptic neural fields: A semantic object-aware neural scene representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12871–12881, 2022.\n\nlabelme. labelme. https://github.com/wkentaro/labelme.\n\nWeihao Li, Omid Hosseini Jafari, and Carsten Rother. Deep object co-segmentation. In Asian\n\nConference on Computer Vision, pp. 638–653. Springer, 2018.\n\nDavid Lindell, Julien Martel, and Gordon Wetzstein. AutoInt: Automatic integration for fast neural\n\nvolume rendering. https://arxiv.org/abs/2012.01714, 2020.\n\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel\n\nfields. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, 2020.\n\nSteven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, and Bryan Russell.\n\nEditing conditional radiance fields, 2021.\n\nXinhang Liu, Jiaben Chen, Huai Yu, Yu-Wing Tai, and Chi-Keung Tang. Unsupervised multi-view object segmentation using radiance field propagation. arXiv preprint arXiv:2210.00489, 2022.\n\nStephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Jason\n\nSaragih. Mixture of volumetric primitives for efficient neural rendering, 2021.\n\nRicardo Martin-Brualla, Noha Radwan, Mehdi Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the wild: Neural radiance fields for unconstrained photo collections. https://arxiv.org/abs/2008.02268, 2020.\n\nNelson Max. Optical models for direct volume rendering. IEEE Transactions on Visualization and\n\nComputer Graphics (TVCG), 1995.\n\nQuan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, and Jingyi Yu. Gnerf: Gan-based neural radiance field without posed camera. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6351–6361, 2021.\n\nBen Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 2019.\n\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pp. 405–421. Springer, 2020a.\n\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pp. 405–421. Springer, 2020b.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nDaniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, and Andrea Tagliasacchi. DeRF:\n\nDecomposed radiance fields. https://arxiv.org/abs/2011.12490, 2020.\n\nChristian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In IEEE International Conference on Computer Vision (ICCV), 2021.\n\nJeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10901–10911, 2021.\n\nZhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexander G Schwing, and Oliver Wang. Neural volumetric object selection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6133–6142, 2022.\n\nGernot Riegler and Vladlen Koltun. Free view synthesis. In European Conference on Computer\n\nVision (ECCV), 2020.\n\nCarsten Rother, Tom Minka, Andrew Blake, and Vladimir Kolmogorov. Cosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), volume 1, pp. 993– 1000. IEEE, 2006.\n\nPedro Savarese, Sunnie SY Kim, Michael Maire, Greg Shakhnarovich, and David McAllester. In Proceedings of the\n\nInformation-theoretic segmentation by inpainting error maximization. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4029–4039, 2021.\n\nKatja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3D-aware image synthesis. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, 2020a.\n\nKatja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. Advances in Neural Information Processing Systems, 33: 20154–20166, 2020b.\n\nTong Shen, Guosheng Lin, Lingqiao Liu, Chunhua Shen, and Ian Reid. Weakly supervised semantic\n\nsegmentation based on co-segmentation. In BMVC, 2017.\n\nOriane Sim ́eoni, Gilles Puy, Huy V Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick P ́erez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised transformers and no labels. arXiv preprint arXiv:2109.14279, 2021.\n\nCameron Smith, Hong-Xing Yu, Sergey Zakharov, Fredo Durand, Joshua B Tenenbaum, Jiajun Wu, and Vincent Sitzmann. Unsupervised discovery and composition of object light fields. arXiv preprint arXiv:2205.03923, 2022.\n\nKarl Stelzner, Kristian Kersting, and Adam R Kosiorek. Decomposing 3d scenes into objects via\n\nunsupervised volume segmentation. arXiv preprint arXiv:2104.01148, 2021.\n\nCheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast conver-\n\ngence for radiance fields reconstruction. arXiv preprint arXiv:2111.11215, 2021.\n\nZachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European\n\nconference on computer vision, pp. 402–419. Springer, 2020.\n\nAlex Trevithick and Bo Yang. GRF: Learning a general radiance field for 3D scene representation\n\nand rendering. https://arxiv.org/abs/2010.04595, 2020.\n\nVadim Tschernezki, Iro Laina, Diane Larlus, and Andrea Vedaldi. Neural feature fusion fields: 3d distillation of self-supervised 2d image representations. arXiv preprint arXiv:2209.03494, 2022.\n\nNarek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic\n\nappearance transfer. arXiv preprint arXiv:2201.00424, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMukund Varma T, Peihao Wang, Xuxi Chen, Tianlong Chen, Subhashini Venugopalan, Zhangyang\n\nWang, et al. Is attention all nerf needs? arXiv preprint arXiv:2207.13298, 2022.\n\nSara Vicente, Carsten Rother, and Vladimir Kolmogorov. Object cosegmentation. In CVPR 2011, pp.\n\n2217–2224. IEEE, 2011.\n\nSuhani Vora, Noha Radwan, Klaus Greff, Henning Meyer, Kyle Genova, Mehdi SM Sajjadi, Etienne Pot, Andrea Tagliasacchi, and Daniel Duckworth. Nesf: Neural semantic fields for generalizable semantic segmentation of 3d scenes. arXiv preprint arXiv:2111.13260, 2021.\n\nCan Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text-and-image\n\ndriven manipulation of neural radiance fields. arXiv preprint arXiv:2112.05139, 2021a.\n\nQianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021b.\n\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600–612, 2004.\n\nDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang Wang. Sinnerf: Training neural radiance fields on complex scenes from a single image. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXII, pp. 736–753. Springer, 2022.\n\nBangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Learning object-compositional neural radiance field for editable scene rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13779–13788, 2021.\n\nYanchao Yang, Antonio Loquercio, Davide Scaramuzza, and Stefano Soatto. Unsupervised moving object detection via contextual information separation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 879–888, 2019.\n\nYao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-scale dataset for generalized multi-view stereo networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1790–1799, 2020.\n\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In IEEE International Conference on Computer Vision (ICCV), 2021a.\n\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021b.\n\nHong-Xing Yu, Leonidas J Guibas, and Jiajun Wu. Unsupervised discovery of object radiance fields.\n\narXiv preprint arXiv:2107.07905, 2021c.\n\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33: 5824–5836, 2020.\n\nKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving\n\nneural radiance fields. arXiv preprint arXiv:2010.07492, 2020.\n\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586–595, 2018.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nHengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16259–16268, 2021.\n\nShuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew J Davison. In-place scene labelling and understanding with implicit scene representation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15838–15847, 2021.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 ADDITIONAL TRAINING DETAILS\n\nImplementation of the Patch Selection We reconstruct positive and negative pairs on the fly during training. Given N rendered patches from N different viewpoints in training, we fed the patches into the DINO-ViT and obtained the [CLS] tokens. Next, we compute a N × N similarity matrix using the cosine similarity with the [CLS] tokens. The negative pairs are selected from the pair with the lowest similarity in each row; the positive pairs are set as the identity pairs. Overall, 2N pairs (N positives + N negatives) are formulated per iteration to compute the collaborative contrastive loss.\n\nImplementation of Stride Ray Sampling Neural radiance field casts a number of rays (typically not adjacent) from the camera origin, intersecting the pixel, to generate input 3D points in the viewing frustum. Our model requires patch-wise rendering of size (P, P ) to formulate the collaborative contrastive loss. However, we can only render a patch less than 64 × 64 in each view due to GPU memory bottleneck (Garbin et al., 2021). Thus, it hardly covers a sufficient receptive field to capture the global context, using the pre-trained DINO. To solve this problem, we adopt a Strided Ray Sampling strategy (Schwarz et al., 2020b; Meng et al., 2021), to enlarge the receptive field of the patches while keeping computational cost fixed. Specifically, instead of sampling a patch of adjacent locations P × P , we sample rays with an interval k, resulting in a receptive field of (P × k) × (P × k).\n\nHyperparameters Selection The hyperparameters of NeRF-SOS on different datasets are shown in Table 5. We adopt the number of bknn and bself in (Hamilton et al., 2022) as the hyperparameter for our appearance level loss (bneg and bid, respectively.). We share the appearance level hyperparameters across all datasets. We set the weights of bneg and bid in the geometry level loss with physical intuition. For example, since the radius of the foreground object in LLFF datasets is roughly 0.5 meters, we set the bneg and bid to be 0.5 and 3, respectively. Analogously, for the unbounded scene (e.g., scene Truck), the bneg and bid are set to be 1 and 5, respectively.\n\nTable 5: Hyperparameters of NeRF-SOS on different datasets. We share the hyperparameters of appearance level for all datasets while we set the bneg and bid for geometry level loss with physical intuition.\n\nparameter name\n\nLLFF BlendedMVS CO3Dv2 Tank and Temples\n\nbid(Lgeo) bneg(Lgeo)\n\n0.50 3.00\n\n0.12 0.60\n\n0.25 1.00\n\n1.00 5.00\n\nSelf-supervised Learned 2D Representations We adopt DINO-ViT (Caron et al., 2021) as our feature extractor for distillation. The training process of DINO-ViT largely simplifies self-supervised learning by applying a knowledge distillation paradigm (Hinton et al., 2015) with a momentum encoder (He et al., 2020), where the model is simply updated by a cross-entropy loss.\n\nA.2 HUMAN ANNOTATION DETAILS\n\nWe use the publicly available annotation tool: (labelme) for the foreground and background annotation. To be specific, we annotate all training and testing views of different scenes using multiple-polygon, extract the polygons and convert them to binary masks. The masks of scene Flower are included in our supplementary and we provide usage guidelines in README. All annotated data will be made public.\n\nA.3 ADDITIONAL EXPERIMENTS\n\nComparisons with Semantic-NeRF using Sparse Label As Semantic-NeRF ables to perform label propagation with sparse annotation, we simulate sparse user annotation by randomly applying {1, 1%, 5%, 10%} foreground annotated object pixels while leaving the rest unlabeled. We can see from Figure 9, the foreground boundaries are gradually refined when more annotations are included, which\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Comparisons with Semantic-NeRF using sparse labels. Results are calculated on scene Fortress\n\nScene “Fortress” User Click ↓ NV-ARI ↑\n\nIoU(BG) ↑\n\nIoU(FG) ↑ mIoU ↑\n\nSemantic NeRF Semantic NeRF Semantic NeRF Semantic NeRF Semantic NeRF NeRF-SOS\n\n1 1% 5% 10% 100% 0\n\n0.9615 0.9731 0.9783 0.9788 0.9838 0.9802\n\n0.9812 0.9938 0.9950 0.9952 0.9963 0.9955\n\n0.9528 0.9667 0.9731 0.9735 0.9799 0.9751\n\n0.967 0.9803 0.9841 0.9843 0.9881 0.9853\n\nFigure 9: Visual comparisons among NeRF-SOS and Semantic-NeRF with sparse annotations.\n\nTable 7: Comparisons among several NeRF-based methods for object segmentation.\n\nLLFF Dataset\n\nSupervision Type NV-ARI ↑\n\nIoU(BG) ↑\n\nIoU(FG) ↑ mIoU ↑\n\nRFP NeRF-SOS NVOS ObjectNeRF\n\nSelf-supervised Self-supervised Weak-supervised Supervised\n\n0.9267 0.9665 0.9217 0.9666\n\n0.9812 0.9912 0.9793 0.9909\n\n0.9178 0.9627 0.9145 0.9639\n\n0.9495 0.9769 0.9469 0.9774\n\nis consistent with reported results in the original paper of Semantic-NeRF. However, we can see from Table 6, all sparse annotation experiments show insufficient accurate salient foreground segmentation, compared with dense annotated Semantic-NeRF. Whereas NeRF-SOS performs comparably with the dense annotation counterpart.\n\nComparisons with other NeRF-based Object Segmentation Label-free NeRF segmentation on real-world scenes remains a challenging problem. ObjectNeRF (Yang et al., 2021) appends an object branch input with object feature and object code, with object-level supervision (a.k.a. 2D instance masks) to enable object manipulation after training. Although Neural Volumetric Object Selection (NVOS) (Ren et al., 2022) solves interactive weak-supervised object segmentation, it requires users to provide several scribbles for supervision. The concurrent work, RFP (Liu et al., 2022), tackles the label-free NeRF segmentation on real-world scenes, but there is still room for their segmentation accuracy. To handle the high-quality label-free NeRF segmentation on real-world scenes, NeRFSOS leverages the proposed collaborative contrastive loss to do self-supervised object segmentation. Experiments in the following Tables 7 and Figure 10 demonstrate our label-free method consistently outperforms RFP and weakly-supervised NVOS.\n\nQualitative Visualization on More Views Qualitative comparisons on LLFF dataset can be found in Figure 11 and Figure 12, respectively. Qualitative comparisons on BlendedMVS dataset can be found in Figure 13 and Figure 14, respectively. Qualitative comparisons on CO3Dv2 dataset can be found in Figure 15 and Figure 16, respectively. Qualitative comparisons on Tank and Temple dataset can be found in Figure 17. Here, we visualize three different views to show the segmentation consistency across views. Visualized video can be found in the supplementary.\n\n16\n\nSemantic-NeRF(1 click)Semantic-NeRF(1% click)Semantic-NeRF(5% click)Semantic-NeRF(10% click)Mask AnnotationsSemantic-NeRF(100% click)NeRF-SOSPublished as a conference paper at ICLR 2023\n\nFigure 10: Visual comparisons of the segmentation masks among NeRF-SOS (self-supervised), RFP (self-supervised), NVOS (weakly-supervised), and ObjectNeRF (fully-supervised).\n\nFigure 11: Novel view object segmentation results on scene Flower of LLFF dataset.\n\nFigure 12: Novel view object segmentation results on scene Foretress of LLFF dataset.\n\n17\n\nMask AnnotationsNeRF-SOSObjectNeRFRFPColor ImageNVOSColor ImagesMask AnnotationsOursDINO-CoSegDOCSSem.NeRF*Color ImagesMask AnnotationsOursDINO-CoSegDOCSSem.NeRF*Published as a conference paper at ICLR 2023\n\nFigure 13: Novel view object segmentation results on scene 5a3 of BlendedMVS dataset.\n\nFigure 14: Novel view object segmentation results on scene 5a6 of BlendedMVS dataset.\n\n18\n\nColor ImagesMask AnnotationsOursDINO-CoSegDOCSSem.NeRF*Color ImagesMask AnnotationsOursDINO-CoSegDOCSSem.NeRF*Published as a conference paper at ICLR 2023\n\nFigure 15: Novel view object segmentation results on scene Apple of CO3Dv2 dataset.\n\n19\n\nColor ImagesMask AnnotationsOursDINO-CoSegDOCSSem.NeRF*Published as a conference paper at ICLR 2023\n\nFigure 16: Novel view object segmentation results on scene Backpack of CO3Dv2 dataset.\n\nFigure 17: Novel view object segmentation results on scene Truck of Tank and Temples dataset.\n\n20\n\nColor ImagesMask AnnotationsOursDINO-CoSegDOCSSem.NeRF*ColorImagesMaskAnnotationsOursDINO-CoSegDOCSSem.NeRF*",
    "reference": "# Summary Of The Paper\n\nThis work proposes an appearance contrastive loss to apply the self-supervised learned 2D visual feature for 3D representations.\nA new geometry contrastive loss is proposed for object segmentation to involve geometric information for segmentation clustering.\nThe segmentation of the proposed method has a more refined result than its supervised counterpart.\n\n# Strength And Weaknesses\n\nStrengths:\n1. This is a well-written paper.\n2. The proposed method is compared with various methods. The experiments are complete and convincing.\n3. Some visualizations are helpful to understand.\nWeaknesses:\n1. Although the visual results of the proposed method are better than the supervised method. The evaluation metrics of the proposed method are worse than Semantic-NeRF in Tab 2 and Tab 3. What causes that problem?\n2. The paper lacks a quantitative analysis of the effects. May you give an analysis of your design?\n3. The ablations are not sufficient enough. Ablation should contain all of your extra design including geometry contrastive loss and appearance contrastive loss.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nGood. The paper is well-written. The authors describe their method in detail, which guides for reproducing the result.\nThe authors provide various results on different datasets and the comparison of different methods, making the results convincible. \nThe proposed method is somewhat innovative and gets more optimal segmentation results.\n\n# Summary Of The Review\n\nThe paper is well-written, and the result of object segmentation is better than its counterpart obviously.\nI think it is marginally above the acceptance threshold.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "IS CONDITIONAL GENERATIVE MODELING ALL YOU NEED FOR DECISION-MAKING?\n\nAnurag Ajay∗ †§¶, Yilun Du *§¶, Abhi Gupta*‡§¶, Joshua Tenenbaum¶, Tommi Jaakkola‡§¶, Pulkit Agrawal†§¶ Improbable AI Lab† Operations Research Center‡ Computer Science and Artificial Intelligence Lab§ Massachusetts Institute of Technology¶\n\nABSTRACT\n\nRecent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decisionmaking. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a returnconditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.\n\n1\n\nINTRODUCTION\n\nOver the last few years, conditional generative modeling has yielded impressive results in a range of domains, including high-resolution image generation from text descriptions (DALL-E, ImageGen) (Ramesh et al., 2022; Saharia et al., 2022), language generation (GPT) (Brown et al., 2020), and step-by-step solutions to math problems (Minerva) (Lewkowycz et al., 2022). The success of generative models in countless domains motivates us to apply them to decision-making. Conveniently, there exists a wide body of research on recovering high-performing policies from data logged by already operational systems (Kostrikov et al., 2022; Kumar et al., 2020; Walke et al., 2022). This is particularly useful in real-world settings where interacting with the environment is not always possible, and exploratory decisions can have fatal consequences (Dulac-Arnold et al., 2021). With access to such offline datasets, the problem of decision-making reduces to learning a probabilistic model of trajectories, a setting where generative models have already found success.\n\nIn offline decision-making, we aim to recover optimal reward-maximizing trajectories by stitching together sub-optimal reward-labeled trajectories in the training dataset. Prior works (Kumar et al., 2020; Kostrikov et al., 2022; Wu et al., 2019; Kostrikov et al., 2021; Dadashi et al., 2021; Ajay et al., 2020; Ghosh et al., 2022) have tackled this problem with reinforcement learning (RL) that uses dynamic programming for trajectory stitching. To enable dynamic programming, these works learn a value function that estimates the discounted sum of rewards from a given state. However, value function estimation is prone to instabilities due to function approximation, off-policy learning, and bootstrapping together, together known as the deadly triad (Sutton & Barto, 2018). Furthermore, to stabilize value estimation in offline regime, these works rely on heuristics to keep the policy within the dataset distribution. These challenges make it difficult to scale existing offline RL algorithms.\n\n∗ denotes equal contribution. Correspondence to aajay@mit.edu, yilundu@mit.edu, abhig@mit.edu\n\n1\n\nFigure 1: Decision Making using Conditional Generative Modeling. Framing decision making as a conditional generative modeling problem allows us to maximize rewards, satisfy constraints and compose skills.\n\nIn this paper, we ask if we can perform dynamic programming to stitch together sub-optimal trajectories to obtain an optimal trajectory without relying on value estimation. Since conditional diffusion generative models can generate novel data points by composing training data (Saharia et al., 2022), we leverage it for trajectory stitching in offline decision-making. Given a dataset of reward-labeled trajectories, we adapt diffusion models (Sohl-Dickstein et al., 2015) to learn a return-conditional trajectory model. During inference, we use classifier-free guidance with lowtemperature sampling, which we hypothesize to implicitly perform dynamics programming, to capture the best behaviors in the dataset and glean return maximizing trajectories (see Appendix A). Our straightforward conditional generative modeling formulation outperforms existing approaches on standard D4RL tasks (Fu et al., 2020).\n\nViewing offline decision-making through the lens of conditional generative modeling allows going beyond conditioning on returns (Figure 1). Consider an example (detailed in Appendix A) where a robot with linear dynamics navigates an environment containing two concentric circles (Figure 2). We are given a dataset of state-action trajectories of the robot, each satisfying one of two constraints: (i) the final position of the robot is within the larger circle, and (ii) the final position of the robot is outside the smaller circle. With conditional diffusion modeling, we can use the datasets to learn a constraintconditioned model that can generate trajectories satisfying any set of constraints. During inference, the learned trajectory model can merge constraints from the dataset and generate trajectories that satisfy the combined constraint. Figure 2 shows that the constraint-conditioned model can generate trajectories such that the final position of the robot lies between the concentric circles.\n\nFigure 2: Illustrative example. We visualize the 2d robot navigation environment and the constraints satisfied by the trajectories in the dataset derived from the environment. We show the ability of the conditional diffusion model to generate trajectories that satisfy the combined constraints.\n\nHere, we demonstrate the benefits of modeling policies as conditional generative models. First, conditioning on constraints allows policies to not only generate behaviors satisfying individual constraints but also generate novel behaviors by flexibly combining constraints at test time. Further, conditioning on skills allows policies to not only imitate individual skills but also generate novel behaviors by composing those skills. We instantiate this idea with a state-sequence based diffusion probabilistic model (Ho et al., 2020) called Decision Diffuser, visualized in Figure 1. In summary, our contributions include (i) illustrating conditional generative modeling as an effective tool in offline decision making, (ii) using classifier-free guidance with low-temperature sampling, instead of dynamic programming, to get return-maximizing trajectories and, (iii) leveraging the framework of conditional generative modeling to combine constraints and compose skills during inference flexibly.\n\n2 BACKGROUND 2.1 REINFORCEMENT LEARNING\n\nWe formulate the sequential decision-making problem as a discounted Markov Decision Process (MDP) defined by the tuple ⟨ρ0, S, A, T , R, γ⟩, where ρ0 is the initial state distribution, S and A are state and action spaces, T : S × A → S is the transition function, R : S × A × S → R gives the reward at any transition and γ ∈ [0, 1) is a discount factor. The agent acts with a stochastic policy π : S → ∆A, generating a sequence of state-action-reward transitions or trajectory τ := (sk, ak, rk)k≥0 with probability pπ(τ ) and return R(τ ) := (cid:80) k≥0 γkrk. The standard objective in RL is to find a return-maximizing policy π∗ = arg maxπ\n\nEτ ∼pπ [R(τ )].\n\n2\n\nττττττττττττττττττττskillsconstraintsrewardlowx2 + y2 ≥ r2x2 + y2 ≤ R2runjumphighcomposeskillssatisfyconstraintsmaximizerewardr2 ≤ x2 + y2 ≤ R2run and jumpoptimalττττττττττττττττττττDecision Diffusergenerated trajectorieslabelled trajectoriesEnvironmentTraining DatasetGeneration(x,y)x2 + y2 ≤ R2r2 ≤ x2 + y2 ≤ R2x2 + y2 ≥ r2Temporal Difference Learning TD methods (Fujimoto et al., 2018; Lillicrap et al., 2015) estimate Q∗(s, a) := Eτ ∼pπ∗ [R(τ )|s0 = s, a0 = a], the return achieved under the optimal policy π∗ when starting in state s and taking action a, with a parameterized Q-function. This requires minimizing the following TD loss:\n\nLTD(θ) := E(s,a,r,s′)∈D[(r + γ max\n\na′∈A\n\nQθ(s′, a′) − Qθ(s, a))2]\n\n(1)\n\nContinuous action spaces further require learning a parametric policy πφ(a|s) that plays the role of the maximizing action in equation 1. This results in a policy objective that must be maximized:\n\nJ (φ) := Es∈D,a∼πφ(·|s)[Q(s, a)]\n\n(2)\n\nHere, the dataset of transitions D evolves as the agent interacts with the environment and both Qθ and πφ are trained together. These methods make use of function approximation, off-policy learning, and bootstrapping, leading to several instabilities in practice (Sutton, 1988; Van Hasselt et al., 2018). Offline RL requires finding a return-maximizing policy from a fixed dataset of transitions collected by an unknown behavior policy μ (Levine et al., 2020). Using TD-learning naively causes the state visitation distribution dπφ(s) to move away from the distribution of the dataset dμ(s). In turn, the policy πφ begins to take actions that are substantially different from those already seen in the data. Offline RL algorithms resolve this distribution-shift by imposing a constraint of the form D(dπφ||dμ), where D is some divergence metric, directly in the TD-learning procedure. The constrained optimization problem now demands additional implementation heuristics to achieve any reasonable performance (Kumar et al., 2021). The Decision Diffuser, in comparison, doesn’t have any of these disadvantages. It does not require estimating any kind of Q-function, thereby sidestepping TD methods altogether. It also does not face the risk of distribution-shift as generative models are trained with maximum-likelihood estimation.\n\n2.2 DIFFUSION PROBABILISTIC MODELS\n\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are a specific type of generative model that learn the data distribution q(x) from a dataset D := {xi}0≤i<M . They have been used most notably for synthesizing high-quality images from text descriptions (Saharia et al., 2022; Nichol et al., 2021). Here, the data-generating procedure is modelled with a predefined forward noising process αkxk, (1 − αk)I) and a trainable reverse process pθ(xk−1|xk) := q(xk+1|xk) := N (xk+1; N (xk−1|μθ(xk, k), Σk), where N (μ, Σ) denotes a Gaussian distribution with mean μ and variance Σ, αk ∈ R determines the variance schedule, x0 := x is a sample, x1, x2, ..., xK−1 are the latents, and xK ∼ N (0, I) for carefully chosen αk and long enough K. Starting with Gaussian noise, samples are then iteratively generated through a series of ”denoising” steps.\n\n√\n\nAlthough a tractable variational lower-bound on log pθ can be optimized to train diffusion models, Ho et al. (2020) propose a simplified surrogate loss:\n\n(3)\n\nLdenoise(θ) := Ek∼[1,K],x0∼q,ε∼N (0,I)[||ε − εθ(xk, k)||2] The predicted noise εθ(xk, k), parameterized with a deep neural network, estimates the noise ε ∼ N (0, I) added to the dataset sample x0 to produce noisy xk. This is equivalent to predicting the mean of pθ(xk−1|xk) since μθ(xk, k) can be calculated as a function of εθ(xk, k) (Ho et al., 2020). Guided Diffusion Modelling the conditional data distribution q(x|y) makes it possible to generate samples with attributes of the label y. The equivalence between diffusion models and scorematching (Song et al., 2021), which shows εθ(xk, k) ∝ ∇xk log p(xk), leads to two kinds of methods for conditioning: classifier-guided (Nichol & Dhariwal, 2021) and classifier-free (Ho & Salimans, 2022). The former requires training an additional classifier pφ(y|xk) on noisy data so that samples may be generated at test-time with the perturbed noise εθ(xk, k) − ω 1 − ̄αk∇xk log p(y|xk), where ω is referred to as the guidance scale. The latter does not separately train a classifier but modifies the original training setup to learn both a conditional εθ(xk, y, k) and an unconditional εθ(xk, k) model for the noise. The unconditional noise is represented, in practice, as the conditional noise εθ(xk, Ø, k) where a dummy value Ø takes the place of y. The perturbed noise εθ(xk, k) + ω(εθ(xk, y, k) − εθ(xk, k)) is used to later generate samples.\n\n√\n\n3 GENERATIVE MODELING WITH THE DECISION DIFFUSER\n\nIt is useful to solve RL from offline data, both without relying on TD-learning and without risking distribution-shift. To this end, we formulate sequential decision-making as the standard problem of\n\n3\n\nFigure 3: Planning with Decision Diffuser. Given the current state st and conditioning, Decision Diffuser uses classifier-free guidance with low-temperature sampling to generate a sequence of future states. It then uses inverse dynamics to extract and execute the action at that leads to the immediate future state st+1.\n\nconditional generative modeling:\n\nEτ ∼D[log pθ(x0(τ )|y(τ ))]\n\nmax θ\n\n(4)\n\nOur goal is to estimate the conditional data distribution with pθ so we can later generate portions of a trajectory x0(τ ) from information y(τ ) about it. Examples of y could include the return under the trajectory, the constraints satisfied by the trajectory, or the skill demonstrated in the trajectory. We construct our generative model according to the conditional diffusion process:\n\nq(xk+1(τ )|xk(τ )),\n\npθ(xk−1(τ )|xk(τ ), y(τ ))\n\n(5)\n\nAs usual, q represents the forward noising process while pθ the reverse denoising process. In the following, we discuss how we may use diffusion for decision making. First, we discuss the modeling choices for diffusion in Section 3.1. Next, we discuss how we may utilize classifier-free guidance to capture the best aspects of trajectories in Section 3.2. We then discuss the different behaviors that may be implemented with conditional diffusion models in Section 3.3. Finally, we discuss practical training details of our approach in Section 3.4.\n\n3.1 DIFFUSING OVER STATES\n\nIn images, the diffusion process is applied across all pixel values in an image. Na ̈ıvely, it would therefore be natural to apply a similar process to model the state and actions of a trajectory. However, in the reinforcement learning setting, directly modeling actions using a diffusion process has several practical issues. First, while states are typically continuous in nature in RL, actions are more varied, and are often discrete in nature. Furthermore, sequences over actions, which are often represented as joint torques, tend to be more high-frequency and less smooth, making them much harder to predict and model (Tedrake, 2022). Due to these practical issues, we choose to diffuse only over states, as defined below:\n\nxk(τ ) := (st, st+1, ..., st+H−1)k\n\n(6)\n\nHere, k denotes the timestep in the forward process and t denotes the time at which a state was visited in trajectory τ . Moving forward, we will view xk(τ ) as a noisy sequence of states from a trajectory of length H. We represent xk(τ ) as a two-dimensional array with one column for each timestep of the sequence.\n\nActing with Inverse-Dynamics. Sampling states from a diffusion model is not enough for defining a controller. A policy can, however, be inferred from estimating the action at that led the state st to st+1 for any timestep t in x0(τ ). Given two consecutive states, we generate an action according to the inverse dynamics model (Agrawal et al., 2016; Pathak et al., 2018):\n\nat := fφ(st, st+1)\n\n(7)\n\nNote that the same offline data used to train the reverse process pθ can also be used to learn fφ. We illustrate in Table 2 how the design choice of directly diffusing state distributions, with an inverse dynamics model to predict action, significantly improves performance over diffusing across both states and actions jointly. Furthermore, we empirically compare and analyze when to use inverse dynamics and when to diffuse over actions in Appendix F.\n\n4\n\nst+hat+hreturnsconstraintsskillsst+2at+1st+1atst+hat+hst+2at+1st+1atst+hat+hst+2at+1st+1st3.2 PLANNING WITH CLASSIFIER-FREE GUIDANCE Given a diffusion model representing the different trajectories in a dataset, we next discuss how we may utilize the diffusion model for planning. To use the model for planning, it is necessary to additionally condition the diffusion process on characteristics y(τ ). One approach could be to train a classifier pφ(y(τ )|xk(τ )) to predict y(τ ) from noisy trajectories xk(τ ). In the case that y(τ ) represents the return under a trajectory, this would require estimating a Q-function, which requires a separate, complex dynamic programming procedure.\n\nOne approach to avoid dynamic programming is to directly train a conditional diffusion model conditioned on the returns y(τ ) in the offline dataset. However, as our dataset consists of a set of sub-optimal trajectories, the conditional diffusion model will be polluted by such sub-optimal behaviors. To circumvent this issue, we utilize classifier-free guidance (Ho & Salimans, 2022) with low-temperature sampling, to extract high-likelihood trajectories in the dataset. We find that such trajectories correspond to the best set of behaviors in the dataset. For a detailed discussion comparing Q-function guidance and classifier-free guidance, please refer to Appendix K. Formally, to implement classifier free guidance, a x0(τ ) is sampled by starting with Gaussian noise xK(τ ) and refining xk(τ ) into xk−1(τ ) at each intermediate timestep with the perturbed noise:\n\nˆε := εθ(xk(τ ), Ø, k) + ω(εθ(xk(τ ), y(τ ), k) − εθ(xk(τ ), Ø, k)),\n\n(8)\n\nwhere the scalar ω applied to (εθ(xk(τ ), y(τ ), k) − εθ(xk(τ ), Ø, k)) seeks to augment and extract the best portions of trajectories in the dataset that exhibit y(τ ). With these ingredients, sampling from the Decision Diffuser becomes similar to planning in RL. First, we observe a state in the environment. Next, we sample states later into the horizon with our diffusion process conditioned on y and history of last C states observed. Finally, we identify the action that should be taken to reach the most immediate predicted state with our inverse dynamics model. This procedure repeats in a standard receding-horizon control loop described in Algorithm 1 and visualized in Figure 3.\n\n3.3 CONDITIONING BEYOND RETURNS So far we have not explicitly defined the conditioning variable y(τ ). Though we have mentioned that it can be the return under a trajectory, we may also consider guiding our diffusion process towards sequences of states that satisfy relevant constraints or demonstrate specific behavior. Maximizing Returns To generate trajectories that maximize return, we condition the noise model on the return of a trajectory so εθ(xk(τ ), y(τ ), k) := εθ(xk(τ ), R(τ ), k). These returns are normalized to keep R(τ ) ∈ [0, 1]. Sampling a high return trajectory amounts to conditioning on R(τ ) = 1. Note that we do not make use of any Q-values, which would then require dynamic programming. Satisfying Constraints Trajectories may satisfy a variety of constraints, each represented by the set Ci, such as reaching a specific goal, visiting states in a particular order, or avoiding parts of the state space. To generate trajectories satisfying a given constraint Ci, we condition the noise model on a one-hot encoding so that εθ(xk(τ ), y(τ ), k) := εθ(xk(τ ), 1(τ ∈ Ci), k). Although we train with an offline dataset in which trajectories satisfy only one of the available constraints, at inference we can satisfy several constraints together. Composing Skills A skill i can be specified from a set of demonstrations Bi. To generate trajectories that demonstrate a given skill, we condition the noise model on a one-hot encoding so that εθ(xk(τ ), y(τ ), k) := εθ(xk(τ ), 1(τ ∈ Bi), k). Although we train with individual skills, we may further compose these skills together during inference.\n\nAssuming we have learned the data distributions q(x0(τ )|y1(τ )), . . . , q(x0(τ )|yn(τ )) for n different conditioning variables, we can sample from the composed data distribution q(x0(τ )|y1(τ ), . . . , yn(τ )) using the perturbed noise (Liu et al., 2022):\n\nˆε := εθ(xk(τ ), Ø, k) + ω\n\nn (cid:88)\n\n(εθ(xk(τ ), yi(τ ), k) − εθ(xk(τ ), Ø, k))\n\n(9)\n\ni=1\n\nThis property assumes that {yi(τ )}n i=1 are conditionally independent given the state trajectory x0(τ ). However, we empirically observe that this assumption doesn’t have to be strictly satisfied as long as the composition of conditioning variables is feasible. For more detailed discussion, please refer to Appendix D. We use this property to compose more than one constraint or skill together at test-time. We also show how Decision Diffuser can avoid particular constraint or skill (NOT) in Appendix J.\n\n5\n\nAlgorithm 1 Conditional Planning with the Decision Diffuser\n\n// Maintain a history of length C\n\nObserve state s; h.insert(s); Initialize xK (τ ) ∼ N (0, αI) for k = K . . . 1 do\n\n1: Input: Noise model εθ, inverse dynamics fφ, guidance scale ω, history length C, condition y 2: Initialize h ← Queue(length = C), t ← 0 3: while not done do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end while\n\nxk(τ )[: length(h)] ← h ˆε ← εθ(xk(τ ), k) + ω(εθ(xk(τ ), y, k) − εθ(xk(τ ), k)) (μk−1, Σk−1) ← Denoise(xk(τ ), ˆε) xk−1 ∼ N (μk−1, αΣk−1)\n\nend for Extract (st, st+1) from x0(τ ) Execute at = fφ(st, st+1); t ← t + 1\n\n// Constrain plan to be consistent with history\n\n// Classifier-free guidance\n\nFigure 4: Results Overview. Decision Diffuser performs better than both TD learning (CQL) and Behavorial Cloning (BC) across D4RL locomotion tasks, D4RL Kitchen tasks and Kuka Block Stacking tasks (single constraint) using only a conditional generative modeling objective. For performance metric, we use normalized average returns (Fu et al., 2020) for D4RL tasks (Locomotion and Kitchen) and success rate for Block Stacking.\n\n3.4 TRAINING THE DECISION DIFFUSER\n\nThe Decision Diffuser, our conditional generative model for decision-making, is trained in a supervised manner. Given a dataset D of trajectories, each labeled with the return it achieves, the constraint that it satisfies, or the skill that it demonstrates, we simultaneously train the reverse diffusion process pθ, parameterized through the noise model εθ, and the inverse dynamics model fφ with the following loss: L(θ, φ) := Ek,τ ∈D,β∼Bern(p)[||ε−εθ(xk(τ ), (1−β)y(τ )+βØ, k)||2]+E(s,a,s′)∈D[||a−fφ(s, s′)||2]\n\nFor each trajectory τ , we first sample noise ε ∼ N (0, I) and a timestep k ∼ U{1, . . . , K}. Then, we construct a noisy array of states xk(τ ) and finally predict the noise as ˆεθ := εθ(xk(τ ), y(τ ), k). Note that with probability p we ignore the conditioning information and the inverse dynamics is trained with individual transitions rather than trajectories. Architecture We parameterize εθ with a temporal U-Net architecture, a neural network consisting of repeated convolutional residual blocks (Janner et al., 2022). This effectively treats a sequence of states xk(τ ) as an image where the height represents the dimension of a single state and the width denotes the length of the trajectory. We encode the conditioning information y(τ ) as either a scalar or a one-hot vector and project it into a latent variable z ∈ Rh with a multi-layer perceptron (MLP). When y(τ ) = Ø, we zero out the entries of z. We also parameterize the inverse dynamics fφ with an MLP. For implementation details, please refer to the Appendix B. In the denoising step of Algorithm 1, we compute μk−1 and Low-temperature Sampling Σk−1 from a noisy sequence of states and a predicted noise. We find that sampling xk−1 ∼ N (μk−1, αΣk−1) where the variance is scaled by α ∈ [0, 1) leads to better quality sequences (corresponding to sampling lower temperature samples). For a proper ablation study, please refer to Appendix C.\n\n4 EXPERIMENTS\n\nIn this section, we explore the efficacy of the Decision Diffuser on a variety of decision-making tasks (performance illustrated in Figure 4). In particular, we evaluate (1) the ability to recover effective RL policies from offline data, (2) the ability to generate behavior that satisfies multiple sets of constraints, (3) the ability compose multiple different skills together. In addition, we empirically justify use of classifier-free guidance, low-temperature sampling (Appendix C), and inverse dynamics (Appendix F) and test the robustness of Decision Diffuser to stochastic dynamics (Appendix G).\n\n4.1 OFFLINE REINFORCEMENT LEARNING\n\nSetup We first test whether the Decision Diffuser can generate return-maximizing trajectories. To test this, we train a state diffusion process and inverse dynamics model on publicly available\n\n6\n\nD4RL Locomotion255075100Decision DiffuserPerformanceD4RL KitchenKuka Block StackingTD-learningBehavior CloningD4RL datasets (Fu et al., 2020). We compare with existing offline RL methods, including modelfree algorithms like CQL (Kumar et al., 2020) and IQL (Kostrikov et al., 2022), and model-based algorithms such as trajectory transformer (TT, Janner et al. (2021)) and MoReL (Kidambi et al., 2020). We also compare with sequence-models like the Decision Transformer (DT) (Chen et al. (2021) and diffusion models like Diffuser (Janner et al., 2022).\n\nDataset Environment\n\nBC CQL IQL\n\nDT\n\nTT MOReL Diffuser DD\n\nMed-Expert Med-Expert Med-Expert Walker2d\n\nHalfCheetah Hopper\n\n91.6 55.2 52.5 105.4 107.5 108.8\n\n86.8 95 107.6 110.0 108.7\n\n86.7 91.5 109.6 108.1 101.9 95.6\n\n53.3\n\nMedium Medium Medium\n\nHalfCheetah Hopper Walker2d\n\nMed-Replay HalfCheetah Med-Replay Hopper Med-Replay Walker2d\n\n42.6 52.9 75.3\n\n36.6 18.1 26.0\n\n44.0 58.5 72.5\n\n45.5 95 77.2\n\n47.4 66.3 78.3\n\n44.2 94.7 73.9\n\n42.6 67.6 74.0\n\n36.6 82.7 66.6\n\n46.9 61.1 79\n\n41.9 91.5 82.6\n\n42.1 95.4 77.8\n\n40.2 93.6 49.8\n\nAverage\n\n51.9\n\n77.6\n\n77\n\n74.7\n\n78.9\n\n72.9\n\nMixed Partial\n\nKitchen Kitchen\n\nAverage\n\n51.5 38\n\n52.4 50.1\n\n44.8\n\n51.2\n\n51 46.3\n\n48.7\n\n- -\n\n-\n\n- -\n\n-\n\n- -\n\n-\n\n79.8 107.2 108.4\n\n44.2 58.5 79.7\n\n42.2 96.8 61.2\n\n75.3\n\n- -\n\n-\n\n90.6 ±1.3 111.8 ±1.8 108.8 ±1.7\n\n49.1 ±1.0 79.3 ±3.6 82.5 ±1.4\n\n39.3 ±4.1 100 ±0.7 75 ±4.3\n\n81.8\n\n65 ±2.8 57 ±2.5\n\n61\n\nTable 1: Offline Reinforcement Learning Performance. We show that Decision Diffuser (DD) either matches or outperforms current offline RL approaches on D4RL tasks in terms of normalized average returns (Fu et al., 2020). We report the mean and the standard error over 5 random seeds.\n\nResults Across different offline RL tasks, we find that the Decision Diffuser is either competitive or outperforms many offline RL baselines (Table 1). It also outperforms Diffuser and sequence modeling approaches, such as Decision Transformer and Trajectory Transformer. The difference between Decision Diffuser and other methods becomes even more significant on harder D4RL Kitchen tasks which require long-term credit assignment.\n\nTo convey the importance of classifier-free guidance, we also compare with the baseline CondDiffuser, which diffuses over both state and action sequences as in Diffuser without classifierIn Table 2, we observe that CondDiffuser improves over Diffuser in 2 out of 3 guidance. environments. Decision Diffuser further improves over CondDiffuser, performing better across all 3 environments. We conclude that learning the inverse dynamics is a good alternative to diffusing over actions. We further empirically analyze when to use inverse dynamics and when to diffuse over actions in Appendix F. We also compare against CondMLPDiffuser, a policy where the current action is denoised according to a diffusion process conditioned on both the state and return. We see that CondMLPDiffuser performs the worst amongst diffusion models. Till now, we mainly tested on offline RL tasks that have deterministic (or near deterministic) environment dynamics. Hence, we test the robustness of Decision Diffuser to stochastic dynamics and compare it to Diffuser and CQL as we vary the stochasticity in environment dynamics, in Appendix G. Finally, we analyze the runtime characteristics of Decision Diffuser in Appendix E. 4.2 CONSTRAINT SATISFACTION\n\nSetup We next evaluate how well we can generate trajectories that satisfy a set of constraints using the Kuka Block Stacking environment (Janner et al., 2022) visualized in Figure 5. In this domain, there are four blocks which can be stacked as a single tower or rearranged into several towers. A constraint like BlockHeight(i) > BlockHeight(j) requires that block i be placed above block j. We train the Decision Diffuser from 10, 000 expert demonstrations each satisfying one of these constraints. We randomize the positions of these blocks and consider two tasks at inference: sampling trajectories that satisfy a single constraint seen before in the dataset or satisfy a group of constraints for which demonstrations were never provided. In the latter, we ask the Decision Diffuser to generate trajectories so BlockHeight(i) > BlockHeight(j) > BlockHeight(k) for three of the four blocks i, j, k. For more details, please refer to Appendix H. Results In both the stacking and rearrangement settings, Decision Diffuser satisfies single constraints with greater success rate than Diffuser (Table 3). We also compare with BCQ (Fujimoto et al., 2019) and CQL (Kumar et al., 2020), but they consistently fail to stack or rearrange the blocks leading to a 0.0 success rate. Unlike these baselines, our method can just as effectively satisfy several constraints together according to Equation 9. For a visualization of these generated trajectories, please see the website https://anuragajay.github.io/decision-diffuser/.\n\n7\n\nHopper-*\n\nMed-Expert Medium Med-Replay\n\nDiffuser CondDiffuser CondMLPDiffuser Decision Diffuser\n\n107.6 58.5 96.8\n\n111.3 66.3 76.5\n\n105.6 54.1 66.5\n\n111.8 ±1.6 79.3 ±3.6 100 ±0.7\n\nTable 2: Ablations. Using classifier-free guidance with Diffuser, resulting in CondDiffuser, improves performance in 2 (out of 3) environments. Additionally, using inverse dynamics for action prediction in Decision Diffuser improves performance in all 3 environments. CondMLPDiffuser, that diffuses over current action given the current state and the target return, doesn’t perform as well.\n\nEnvironment\n\nDiffuser\n\nDD\n\nSingle Constraint - Stacking Single Constraint - Rearrangement\n\n45.6 ±3.1 58.9 ±3.4\n\n58.0 ±3.1 62.7 ±3.1\n\nSingle Constraint Average\n\n52.3\n\n60.4\n\nMultiple Constraints - Stacking Multiple Constraints - Rearrangement\n\nMultiple Constraints Average\n\n- -\n\n-\n\n60.3 ±3.1 67.2 ±3.1\n\n63.8\n\nTable 3: Block Stacking through Constraint Minimization. Decision Diffuser (DD) improves over Diffuser in terms of the success rate of generating trajectories satisfying a set of block-stacking constraints. It can also flexibly combine multiple constraints during test time. We report the mean success rate and the standard error over 5 random seeds.\n\nFigure 5: Kuka Block Stacking task.\n\n4.3 SKILL COMPOSITION\n\nSetup Finally, we look at how to compose different skills together. We consider the Unitree-gorunning environment (Margolis & Agrawal, 2022), where a quadruped robot can be found running with various gaits, like bounding, pacing, and trotting. We explore if it is possible to generate trajectories that transition between these gaits after only training on individual gaits. For each gait, we collect a dataset of 2500 demonstrations on which we train Decision Diffuser. Results During testing, we use the noise model of our reverse diffusion process according to equation 9 to sample trajectories of the quadruped robot with entirely new running behavior. Figure 6 shows a trajectory that begins with bounding but ends with pacing. Appendix I provides additional visualizations of running gaits being composed together. Although it visually appears that trajectories generated with the Decision Diffuser contain more than one gait, we would like to quantify exactly how well different gaits can be composed. To this end, we train a classifier to predict at every time-step or frame in a trajectory the running gait of the quadruped (i.e. bound, pace, or trott). We reuse the demonstrations collected for training the Decision Diffuser to also train this classifier, where our inputs are defined as robot joint states over a fixed period of time (i.e. state sub-sequences of length 10) and the label is the gait demonstrated in this sequence. The complete details of our gait classification procedure can be found in Appendix I.\n\nCondition\n\nTrott\n\nPace Bound\n\nOnly Bound Only Pace Bound + Pace\n\n0.8 1.4 1.4\n\n1.0 97.7 38.5\n\n98.2 0.9 60.1\n\nFigure 7: Classifying Running Gaits. A classifier predicts the running gait of the quadruped at every timestep. On trajectories generated by conditioning on a single skill, like only bounding or pacing, the classifier predicts the respective gait with largest probability. When conditioned on both skills, some timesteps are classified as bounding while others as pacing.\n\nWe use our running gait classifier in two ways: to evaluate how the behavior of the quadruped changes over the course of a single, generated trajectory and to measure how often each gait emerges over several generated trajectories. In the former, we first sample three trajectories from the Decision Diffuser conditioned either on the bounding gait, the pacing gait, or both. For every trajectory, we separately plot the classification probability of each gait over the length of the sequence. As shown in the plots of Figure 7, the classifier predicts bound and pace respectively to be the most likely running gait in trajectories sampled with this condition. When the trajectory is generated by conditioning on both gaits, the classifier transitions between predicting one gait with largest probability to the\n\n8\n\nTrottGait ProbabilityOnly BoundOnly PaceBound + Pace501001502005010015020050100150200.20.40.60.80PaceBoundFigure 6: Composing Movement Skills. Decision Diffuser can imitate individual running gaits using expert demonstrations and compose multiple different skills together during test time. The results are best illustrated by videos viewable at https://anuragajay.github.io/decision-diffuser/.\n\nother. In fact, there are several instances where the behavior of the quadruped switches between bounding and pacing according to the classifier. This is consistent with the visualizations reported in Figure 6. In the table depicted in Figure 7, we consider 1000 trajectories generated with the Decision Diffuser when conditioned on one or both of the gaits as listed. We record the fraction of time that the quadruped’s running gait was classified as either trott, pace, or bound. It turns out that the classifier identifies the behavior as bounding for 38.5% of the time and as pacing for the other 60.1% when trajectories are sampled by composing both gaits. This corroborates the fact that the Decision Diffuser can indeed compose running behaviors despite only being trained on individual gaits. 5 RELATED WORK Diffusion Models Diffusion Models is proficient in learning generative models of image and text data (Saharia et al., 2022; Nichol et al., 2021; Nichol & Dhariwal, 2021). It formulates the data sampling process as an iterative denoising procedure (Sohl-Dickstein et al., 2015; Ho et al., 2020). The denoising procedure can be alternatively interpreted as parameterizing the gradients of the data distribution (Song et al., 2021) optimizing the score matching objective (Hyv ̈arinen, 2005) and thus as a Energy-Based Model (Du & Mordatch, 2019; Nijkamp et al., 2019; Grathwohl et al., 2020). To generate data samples (eg: images) conditioned on some additional information (eg:text), prior works (Nichol & Dhariwal, 2021) have learned a classifier to facilitate the conditional sampling. More recent works (Ho & Salimans, 2022) have argued to leverage gradients of an implicit classifier, formed by the difference in score functions of a conditional and an unconditional model, to facilitate conditional sampling. The resulting classifier-free guidance has been shown to generate better conditional samples than classifier-based guidance. Recent works have also used diffusion models to imitate human behavior (Pearce et al., 2023) and to parameterize policy in offline RL (Wang et al., 2022). Janner et al. (2022) generate trajectories consisting of states and actions with an unconditional diffusion model, therefore requiring a trained reward function on noisy state-action pairs. At inference, the estimated reward function guides the reverse diffusion process towards samples of high-return trajectories. In contrast, we do not train reward functions or diffusion processes separately, but rather model the trajectories in our dataset with a single, conditional generative model. This ensures that the sampling procedure of the learned diffusion process is the same at inference as it is during training. Reward Conditioned Policies Prior works (Kumar et al., 2019; Schmidhuber, 2019; Emmons et al., 2021; Chen et al., 2021) have studied learning of reward conditioned policies via reward conditioned behavioral cloning. Chen et al. (2021) used a transformer (Vaswani et al., 2017) to model the reward conditioned policies and obtained a performance competitive with offline RL approaches. Emmons et al. (2021) obtained similar performance as Chen et al. (2021) without using a transformer policy but relied on careful capacity tuning of MLP policy. In contrast, Decision Diffuser can also model constraints or skills and their resulting compositions.\n\n6 DISCUSSION We propose Decision Diffuser, a conditional generative model for sequential decision making. It frames offline sequential decision making as conditional generative modeling and sidesteps the need of reinforcement learning, thereby making the decision making pipeline simpler. By sampling for high returns, it is able to capture the best behaviors in the dataset and outperforms existing offline RL approaches on standard D4RL benchmarks. In addition to returns, it can also be conditioned on constraints or skills and can generate novel behaviors by flexibly combining constraints or composing skills during test time. In this work, we focused on offline sequential decision making, thus circumventing the need for exploration. Using ideas from Zheng et al. (2022), future works could look into online fine-tuning of Decision Diffuser by leveraging entropy of the state-sequence model for exploration. While our work focused on state based environments, it can be extended to image based environments by performing the diffusion in latent space, rather than observation space, as done in Rombach et al. (2022). For a detailed discussion on limitations of Decision Diffuser, please refer to Appendix L.\n\n9\n\nBoundPaceBound + Pace ACKNOWLEDGEMENTS\n\nThe authors would like to thank Ofir Nachum, Anthony Simeonov and Richard Li for their helpful feedback on an earlier draft of the work; Jay Whang and Ge Yang for discussions on classifierfree guidance; Gabe Margolis for helping with unitree experiments; Micheal Janner for providing visualization code for Kuka block stacking; and the members of Improbable AI Lab for discussions and helpful feedback. We thank MIT Supercloud and the Lincoln Laboratory Supercomputing Center for providing compute resources. This research was supported by an NSF graduate fellowship, a DARPA Machine Common Sense grant, a MURI grant, an MIT-IBM grant, and ARO W911NF-21-10097.\n\nThis research was also partly sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19- 2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein.\n\nAUTHOR CONTRIBUTIONS\n\nAnurag Ajay conceived the framework of viewing decision-making as conditional diffusion generative modeling, implemented the Decision Diffuser algorithm, ran experiments on Offline RL and Skill Composition, and helped in paper writing.\n\nYilun Du helped in conceiving the framework of viewing decision-making as conditional diffusion generative modeling, ran experiments on Constraint Satisfaction, helped in paper writing and advised Anurag.\n\nAbhi Gupta helped in running experiments on Offline RL and Skill Composition, participated in research discussions, and played the leading role in paper writing and making figures.\n\nJoshua Tenenbaum participated in research discussions.\n\nTommi Jaakkola participated in research discussions and suggested the experiment of classifying running gaits.\n\nPulkit Agrawal was involved in research discussions, suggested experiments related to dynamic programming, provided feedback on writing, positioning of the work, and overall advising.\n\nREFERENCES\n\nPulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke by poking: Experiential learning of intuitive physics. Advances in neural information processing systems, 29, 2016.\n\nAnurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline primitive discovery for accelerating offline reinforcement learning. arXiv preprint arXiv:2010.13611, 2020.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems, 2021.\n\n10\n\nRobert Dadashi, Shideh Rezaeifar, Nino Vieillard, L ́eonard Hussenot, Olivier Pietquin, and Matthieu Geist. Offline reinforcement learning with pseudometric learning. arXiv preprint arXiv:2103.01948, 2021.\n\nYilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. In\n\nAdvances in Neural Information Processing Systems, 2019.\n\nGabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. Machine Learning, 110(9):2419–2468, 2021.\n\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for\n\noffline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep\n\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n\nScott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In International conference on machine learning, pp. 1587–1596. PMLR, 2018.\n\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without\n\nexploration. In International Conference on Machine Learning, 2019.\n\nDibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline rl policies should be trained to be adaptive. In International Conference on Machine Learning, pp. 7513–7530. PMLR, 2022.\n\nWill Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, and Richard Zemel. Learning the stein discrepancy for training and evaluating energy-based models without sampling. In International Conference on Machine Learning, 2020.\n\nAbhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Metareinforcement learning of structured exploration strategies. Advances in neural information processing systems, 31, 2018.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, 2018.\n\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n\n2022.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in\n\nNeural Information Processing Systems, 2020.\n\nAapo Hyv ̈arinen. Estimation of non-normalized statistical models by score matching. Journal of\n\nMachine Learning Research, 2005.\n\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\n\nmodeling problem. In Advances in Neural Information Processing Systems, 2021.\n\nMichael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for\n\nflexible behavior synthesis. In International Conference on Machine Learning, 2022.\n\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. MOReL: Modelbased offline reinforcement learning. In Advances in Neural Information Processing Systems, 2020.\n\nDiederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances\n\nin neural information processing systems, 34:21696–21707, 2021.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\n\nConference on Learning Representations, 2015.\n\nIlya Kostrikov, Jonathan Tompson, Rob Fergus, and Ofir Nachum. Offline reinforcement learning\n\nwith fisher divergence critic regularization. arXiv preprint arXiv:2103.08050, 2021.\n\n11\n\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\n\nQ-learning. In International Conference on Learning Representations, 2022.\n\nAviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. arXiv preprint\n\narXiv:1912.13465, 2019.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline\n\nreinforcement learning. In Advances in Neural Information Processing Systems, 2020.\n\nAviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for offline\n\nmodel-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813, 2021.\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,\n\nreview, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.\n\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual\n\ngeneration with composable diffusion models. arXiv preprint arXiv:2206.01714, 2022.\n\nCalvin Luo. Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970,\n\n2022.\n\nGabriel Margolis and Pulkit Agrawal. Walk these ways: Gait-conditioned policies yield diversified\n\nquadrupedal agility. In Conference on Robot Learning, 2022.\n\nDiganta Misra. Mish: A self regularized non-monotonic neural activation function.\n\nIn British\n\nMachine Vision Conference, 2019.\n\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\n\nIn International Conference on Machine Learning, 2021.\n\nErik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent nonpersistent short-run MCMC toward energy-based model. In Advances in Neural Information Processing Systems, 2019.\n\nPedro A Ortega, Markus Kunesch, Gr ́egoire Del ́etang, Tim Genewein, Jordi Grau-Moya, Joel Veness, Jonas Buchli, Jonas Degrave, Bilal Piot, Julien Perolat, et al. Shaking the foundations: delusions in sequence models for interaction and control. arXiv preprint arXiv:2110.10819, 2021.\n\nKeiran Paster, Sheila McIlraith, and Jimmy Ba. You can’t count on luck: Why decision transformers\n\nfail in stochastic environments. arXiv preprint arXiv:2205.15967, 2022.\n\nDeepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 2050–2053, 2018.\n\nTim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imitating human behaviour with diffusion models. arXiv preprint arXiv:2301.10677, 2023.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\n12\n\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ̈orn Ommer. HighIn Proceedings of the IEEE/CVF\n\nresolution image synthesis with latent diffusion models. Conference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\n\nJuergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards–just map them to\n\nactions. arXiv preprint arXiv:1912.02875, 2019.\n\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, 2015.\n\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models.\n\nIn\n\nInternational Conference on Learning Representations, 2021.\n\nRichard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,\n\n1988.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nRuss Tedrake. Underactuated Robotics. 2022. URL http://underactuated.mit.edu.\n\nHado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz In Advances in Neural Information\n\nKaiser, and Illia Polosukhin. Attention is all you need. Processing Systems, 2017.\n\nAdam R Villaflor, Zhe Huang, Swapnil Pande, John M Dolan, and Jeff Schneider. Addressing optimism bias in sequence modeling for reinforcement learning. In International Conference on Machine Learning, pp. 22270–22283. PMLR, 2022.\n\nHomer Walke, Jonathan Yang, Albert Yu, Aviral Kumar, Jedrzej Orbik, Avi Singh, and Sergey Levine. Don’t start from scratch: Leveraging prior data to automate robotic reinforcement learning. arXiv preprint arXiv:2207.04703, 2022.\n\nZhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy\n\nclass for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022.\n\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.\n\narXiv preprint arXiv:1911.11361, 2019.\n\nYuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision,\n\n2018.\n\nMengjiao Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. Dichotomy of control: Separating what you can control from what you cannot. arXiv preprint arXiv:2210.13435, 2022.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\nQinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. arXiv preprint\n\narXiv:2202.05607, 2022.\n\n13\n\nAppendix\n\nIn this appendix, we discuss details of the illustrative examples in Section A. Next, we discuss hyperparameters and architectural details in Section B. We analyze the importance of low temperature sampling in Section C, further explain composition of conditioning variable in Section D, discuss the run-time characteristics of decision diffuser in Section E, discuss when to use inverse dynamics in Section F and analyze robustness of Decision Diffuser to stochastic dynamics in Section G. Finally, we provide details of the Kuka Block Stacking environment in Section H and the Unitree environment in Section I.\n\nA ILLUSTRATIVE EXAMPLES\n\nA.1\n\nIMPLICIT DYNAMIC PROGRAMMING\n\nFigure A1: Illustrative example. We demonstrate the ability of Decision Diffuser to stitch together suboptimal trajectories in training dataset to obtain (near) optimal trajectories, thereby implicitly performing dynamic programming in Maze2D-open environment from Fu et al. (2020).\n\nWe empirically demonstrate the ability of Decision Diffuser to perform implicit dynamic programming in Maze2D-open environment from Fu et al. (2020). The task in Maze2D-open environment is to reach point C and the reward is negative distance from point C. The training dataset consists of 500 trajectories from point A to point B and 500 trajectories from point B to point C. The maximum trajectory length is 50. During test time, the agent starts from point A and needs to reach point C as quickly as possible. As shown in Figure A1, Decision Diffuser can stitch trajectories in training dataset to form trajectories that goes from point A to point B in (near) straight lines.\n\nA.2 CONSTRAINT COMBINATION\n\nIn linear system robot navigation, Decision Diffuser is trained on 1000 expert trajectories Setup either satisfying the constraint ∥sT ∥ ≤ R (R = 1) or the constraint ∥sT ∥ ≥ r (r = 0.7). Here, sT = [xT , yT ] represents the final robot state in a trajectory, specifying its final 2d position. The maximum trajectory length is 50. During test time, Decision Diffuser is asked to generate trajectories satisfying ∥sT ∥ ≤ R and ∥sT ∥ ≥ r to test its ability to satisfy single constraints. Furthermore, Decision Diffuser is also asked to generate trajectories satisfying r ≤ ∥sT ∥ ≤ R to test its ability to satisfy combined constraints. Results Figure 2 shows that Decision Diffuser learns to generate trajectories perfectly (i.e. with 100% success rate) satisfying single constraints in linear system robot navigation. Furthermore, it learns to generate trajectories satisfying the composed constraint in linear system robot navigation with 91.3%(±2.6%) accuracy where the standard error is calculated over 5 random seeds.\n\nB HYPERPARAMETER AND ARCHITECTURAL DETAILS\n\nIn this section, we describe various architectural and hyperparameter details:\n\n• We represent the noise model εθ with a temporal U-Net (Janner et al., 2022), consisting of a U-Net structure with 6 repeated residual blocks. Each block consisted of two temporal convolutions, each followed by group norm (Wu & He, 2018), and a final Mish nonlinearity (Misra, 2019). Timestep and condition embeddings, both 128-dimensional vectors, are produced by separate 2-layered MLP (with 256 hidden units and Mish nonlinearity) and are concatenated together before getting added to the activations of the first temporal convolution within each block. We borrow the code for temporal U-Net from https://github.com/jannerm/diffuser.\n\n14\n\nTraining datasetGeneration• We represent the inverse dynamics fφ with a 2-layered MLP with 512 hidden units and ReLU\n\nactivations.\n\n• We represent the gait classifier with a 3-layered MLP with 1024 hidden units and ReLU activations.\n\n• We train εθ and fφ using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 2e − 4\n\nand batch size of 32 for 2e6 train steps.\n\n• We train the gait classifier using the Adam optimizer with a learning rate of 2e − 4 and batch size\n\nof 64 for 1e6 train steps.\n\n• We choose the probability p of removing the conditioning information to be 0.25.\n\n• We use K = 100 diffusion steps.\n\n• We use a planning horizon H of 100 in all the D4RL locomotion tasks, 56 in D4RL kitchen tasks, 128 in Kuka block stacking, 56 in unitree-go-running tasks, 50 in the illustrative example and 60 in Block push tasks.\n\n• We use a guidance scale s ∈ {1.2, 1.4, 1.6, 1.8} but the exact choice varies by task.\n\n• We choose α = 0.5 for low temperature sampling.\n\n• We choose context length C = 20.\n\nC IMPORTANCE OF LOW TEMPERATURE SAMPLING\n\nIn Algorithm 1, we compute μk−1 and Σk−1 from a noisy sequence of states and predicted noise. We find that sampling xk−1 ∼ N (μk−1, αΣk−1) (where α ∈ [0, 1)) with a reduced variance produces high-likelihood state sequences. We refer to this as low-temperature sampling. To empirically show its importance, we compare performances of Decision Diffuser with different values of α (Table A1). We show that low temperature sampling (α = 0.5) gives the best average returns. However, reducing the α to 0 eliminates the entropy in sampling and leads to lower returns. On the other hand, α = 1.0 leads to a higher variance in terms of returns of the trajectories.\n\nDecision Diffuser Hopper-Medium-Expert\n\nα = 0 α = 0.5 α = 1.0\n\n104.3 ± 0.7 111.8 ±1.6 107.1 ± 3.5\n\nTable A1: Low-temperature sampling (α = 0.5) allows us to get high return trajectories consistently. While α = 1.0 leads to a higher variance in returns of the trajectories, α = 0.0 eliminates entropy in the sampling and leads to lower returns.\n\nD COMPOSING CONDITIONING VARIABLES\n\ni=1 composes these conditioning variables together.\n\nIn this section, we detail how Decision Diffuser trained with different conditioning variables {yi(τ )}n It learns the denoising model εθ(xk(τ ), yi(τ ), k) for a given conditioning variable yi(τ ). From the derivations outlined in prior works (Luo, 2022; Song et al., 2021), we know that ∇xk(τ ) log q(xk(τ )|yi(τ )) ∝ −εθ(xk(τ ), yi(τ ), k). Therefore, each conditional trajectory distribution {q(xk(τ )|yi(τ ))}n i=1 can be modelled with a single denoising model εθ that conditions on the respective variable yi(τ ).\n\nIn order to compose n different conditioning variables (i.e. skills or constraints), we would like to model q(xk(τ )|{yi(τ )}n i=1 are conditionally independent given\n\ni=1). We assume that {yi(τ )}n\n\n15\n\nxk(τ ). Thus, we can factorize as follows:\n\nq(xk(τ )|{yi(τ )}n\n\ni=1) ∝ q(xk(τ ))\n\nn (cid:89)\n\ni=1\n\nq(xk(τ )|yi(τ )) q(xk(τ ))\n\n(Bayes Rule)\n\n⇒ log q(xk(τ )|{yi(τ )}n\n\ni=1) ∝ log q(xk(τ )) +\n\nn (cid:88)\n\ni=1\n\n(log q(xk(τ )|yi(τ )) − log q(xk(τ )))\n\n⇒ ∇xk(τ ) log q(xk(τ )|{yi(τ )}n\n\ni=1) = ∇xk(τ ) log q(xk(τ ))\n\nn (cid:88)\n\n(∇xk(τ ) log q(xk(τ )|yi(τ )) − ∇xk(τ ) log q(xk(τ )))\n\n+\n\ni=1\n\n⇒ εθ(xk(τ ), {yi(τ )}n\n\ni=1, k) = εθ(xk(τ ), Ø, k) +\n\nn (cid:88)\n\n(εθ(xk(τ ), yi(τ ), k) − εθ(xk(τ ), Ø, k))\n\ni=1\n\nUsing the above equations, we can sample from q(x0(τ )|{yi(τ )}n using the perturbed noise:\n\ni=1) with classifier free guidance\n\nˆε := εθ(xk(τ ), Ø, k) + ω(εθ(xk(τ ), {yi(τ )}n\n\ni=1, k) − εθ(xk(τ ), Ø, k))\n\n= εθ(xk(τ ), Ø, k) + ω\n\nn (cid:88)\n\ni=1\n\n(εθ(xk(τ ), yi(τ ), k) − εθ(xk(τ ), Ø, k))\n\nWe use the perturbed noise to compose skills or combine constraints at test time. This derivation was borrowed from Liu et al. (2022) and is presented here for completeness.\n\nWhile the composition of conditioning variables {yi(τ )}n i=1 requires them to be conditionally independent given the state trajectory x0(τ ), we empirically observe that this condition doesn’t have to be strictly satisfied. However, we require composition of conditioning variables to be feasible (i.e. ∃ x0(τ ) that satisfies all the conditioning variables). When the composition is infeasible, Decision Diffuser produces trajectories with incoherent behavior, as expected. This is best illustrated by videos viewable at https://anuragajay.github.io/decision-diffuser/. Requirements on the dataset First, the dataset should have a diverse set of demonstrations that shows different ways of satisfying each conditioning variable yi(τ ). This would allow Decision Diffuser to learn diverse ways of satisfying each conditioning variable yi(τ ). Since we use inverse dynamics to extract actions from the predicted state trajectory x0(τ ), we assume that the state trajectory x0(τ ) resulting from the composition of different conditioning variables contains consecutive state pairs (st, st+1) that come from the same distribution that generated the demonstration dataset. Otherwise, inverse dynamics can give erroneous predictions.\n\nE RUNTIME CHARACTERISTIC OF DECISION DIFFUSER\n\nWe analyze the runtime characteristics of Decision Diffuser in this section. After training the Decision Diffuser on trajectories from the D4RL Hopper-Medium-Expert dataset, we plan in the corresponding environment according to Algorithm 1. Every action taken in the environment requires running 100 reverse diffusion steps to generate a state sequence taking on average 1.26s in wall-clock time. We can improve the run-time of planning by warm-starting the state diffusion as suggested in Janner et al. (2022). Here, we start with a generated state sequence (from the previous environment step), run forward diffusion for a fixed number of steps, and finally run the same number of reverse diffusion steps from the partially noised state sequence to generate another state sequence. Warm-starting in this way allows us to decrease the number of denoising steps to 40 (0.48s on average) without any loss in performance, to 20 (0.21s on average) with minimal loss in performance, and to 5 with less than 20% loss in performance (0.06s on average). We demonstrate the trade-off between performance, measured by normalized average return achieved in the environment, and planning time, measured in wall-clock time after warm-starting the reverse diffusion process, in Figure A2.\n\n16\n\nFigure A2: Performance vs planning time. We visualize the trade-off between performance, measured by normalized average return achieved in the environment, and planning time, measured in wall-clock time after warm-starting the reverse diffusion process.\n\nEnvironment\n\nBC\n\nCondDiffuser Decision Diffuser\n\nPosition Control Torque Control\n\n57.3 ±1.2 55.2 ±1.5\n\n87.3 ±3.1 71.8 ±3.4\n\n87.8 ±2.8 84.7 ±2.2\n\nTable A2: Block pushing with different controls. Decision Diffuser and CondDiffuser perform similarly when the agent uses position control. However, when the agent uses torque control, CondDiffuser performs worse than Decision Diffuser given it’s harder to diffuse over non-smooth action trajectories. We use the success rate of the red cube reaching the green circle as the performance metric. We report the mean success rate and the standard error over 5 random seeds.\n\nFigure A3: Block push environment.\n\nF WHEN TO USE INVERSE DYNAMICS?\n\nIn this section, we try to analyze further when using inverse dynamics is better than diffusing over actions. Table 2 showed that Decision Diffuser outperformed CondDiffuser on 3 hopper environment, thereby suggesting that inverse dynamics is a better alternative to diffusing over actions. Our intuition was that sequences over actions, represented as joint torques in our environments, tend to be more high-frequency and less smooth, thus making it harder for the diffusion model to predict (Kingma et al., 2021). We now try to verify this intuition empirically.\n\nSetup We choose Block Push environment adapted from Gupta et al. (2018) where the goal is to push the red cube to the green circle. When the red cube reaches the green circle, the agent gets a reward of +1. The state space is 10-dimensional consisting of joint angles (3) and velocities (3) of the gripper, COM of the gripper (2) and position of the red cube (2). The green circle’s position is fixed and at an initial distance of 0.5 from COM of the gripper. The red cube (of size 0.03) is initially at a distance of 0.1 from COM of the gripper and at an angle θ sampled from U(−π/4, π/4) at the start of every episode. The task horizon is 60 timesteps.\n\nThere are 2 control types: (i) torque control, where the agent needs to specify joint torques (3 dimensional) and (ii) position control where the agent needs to specify the position change of COM of the gripper and the angular change in gripper’s orientation (∆x, ∆y, ∆φ) (3 dimensional). While action trajectories from position control are smooth, the action trajectories from torque control have higher frequency components.\n\nOffline dataset collection To collect the offline data, we use Soft Actor-Critic (SAC) (Haarnoja et al., 2018) first to train an expert policy for 1 million environment steps. We then use 1 million environment transitions as our offline dataset, which contains expert trajectories collected towards the end of the training and random action trajectories collected at the beginning of the training. We collect 2 datasets, one for each control type.\n\n17\n\nResults Table A2 shows that Decision Diffuser and CondDiffuser perform similarly when the agent uses position control. This is because action trajectories resulting from position control are smoother and hence easier to model with diffusion. However, when the agent uses torque control, CondDiffuser performs worse than Decision Diffuser, given the action trajectories have higher frequency components and hence are harder to model with diffusion.\n\nG ROBUSTNESS TO STOCHASTIC DYNAMICS\n\np\n\n0.00 0.05 0.10 0.15\n\nBC\n\nDecision Diffuser Diffuser\n\nCQL\n\n55.2±1.5 49.3±3.6 25.8±3.8 15.1±4.3\n\n84.7±2.2 77.3±3.1 53.2±4.1 41.3±4.9\n\n72.4±1.4 63.2±2.9 52.3±4.6 41.6±5.1\n\n73.2±2.3 61.8±3.7 51.2±4.3 42.2±5.5\n\nTable A3: Robustness to stochastic dynamics. Decision Diffuser’s performance suffers when stochasticity is introduced in dynamics function. While it still outperforms Diffuser and CQL when p = 0.05, its performance becomes similar to that of Diffuser and CQL for higher p values. We use the success rate of the red cube reaching the green circle as the performance metric. We report the mean success rate and the standard error over 5 random seeds.\n\nWe empirically analyze robustness of Decision Diffuser to stochasticity in dynamics function. Setup We use Block Push environment, described in Appendix F, with torque control. However, we inject stochasticity into the environment dynamics. For every environment step, we either sample a random action from U([−1, −1, −1], [1, 1, 1]) with probability p or execute the action given by the policy with probability (1 − p). We use p ∈ {0, 0.05, 0.1, 0.15} in our experiments. Offline dataset collection We collect separate offline datasets for different block push environments, each characterized by a different value of p. Each offline dataset consists of 1 million environment transitions collected using the method described in Appendix F. Results Table A3 characterizes how the performance of BC, Decision Diffuser, Diffuser, and CQL changes with increasing stochasticity in the environment dynamics. We observe that the Decision Diffuser outperforms Diffuser and CQL for p = 0.05, however all methods including the Decision Diffuser settle to a similar performance for larger values of p.\n\nSeveral works (Paster et al., 2022; Yang et al., 2022) have shown that the performance of returnconditioned policies suffers as the stochasticity in environment dynamics increases. This is because the return-conditioned policies aren’t able to distinguish between high returns from good actions and high returns from environment stochasticity. Hence, these return-conditioned policies can learn sub-optimal actions that got associated with high-return trajectories in the dataset due to environment stochasticity. Given Decision diffuser uses return conditioning to generate actions in offline RL, its performance also suffers when stochasticity in environment dynamics increases.\n\nSome recent works (Yang et al., 2022; Villaflor et al., 2022) address the above issue by learning a latent model for future states and then conditioning the policy on predicted latent future states rather than returns. Conditioning Decision Diffuser on future state information, rather than returns, would make it more robust to stochastic dynamics and could be an interesting avenue for future works.\n\nH KUKA BLOCK STACKING\n\nIn the Kuka blocking stacking environment, the underlying goal is to stack a set of blocks on top of each other. Models have trained on a set of demonstration data, where a set of 4 blocks are sequentially stacked on top of each other to form a block tower.\n\nWe construct state-space plans of length 128. Following (Janner et al., 2022), we utilize a close-loop controller to generate actions for each state in our state-space plan (controlling the 7 degrees of freedom in joints). The total maximum trajectory length plan in Kuka block stacking is 384. We detail differences between the two consider conditional stacking environments below:\n\n18\n\n• Stacking In the stacking environment, at test time we wish to again construct a tower of four\n\nblocks.\n\n• Rearrangement In the rearrangement environment, at test time wish to stack blocks in a configuration where a set of blocks are above a second set. This set of stack-place relations may not precisely correspond to a single block tower (can instead construct two block towers), making this environment an out-of-distribution challenge.\n\nIn addition to Diffuser (Janner et al., 2022), we used goal-conditioned variants of CQL (Kumar et al., 2020) and BCQ (Fujimoto et al., 2019) as baselines for the block stacking and rearrangement with single constraint. However, they get a success rate of 0.0.\n\nI UNITREE GO RUNNING\n\nWe consider Unitree-go-running environment (Margolis & Agrawal, 2022) where a quadruped robot runs in 3 different gaits: bounding, pacing, and trotting. The state space is 56 dimensional, the action space is 12 dimensional, and the maximum trajectory length is 250.\n\nAs described in Section 4.3, we train Decision Diffuser on expert trajectories demonstrating individual gaits. During testing, we compose the noise model of our reverse diffusion process according to equation 9. This allows us to sample trajectories of the quadruped robot with entirely new running behavior. Figures A4,A5,A6 shows the ability of Decision Diffuser to imitate bounding, trotting and pacing and their combinations.\n\nI.1 QUANTITATIVE VERIFICATION OF COMPOSITION\n\nWe now try to quantitatively verify whether the trajectories resulting from composition of 2 gaits does indeed contain only those 2 gaits. Setup We learn a gait classifier that takes in a sub-sequence of states (of length 10) and predicts the gait-ID. It is represented by a 3-layered MLP with 1024 hidden units and ReLU activations that concatenates the sub-sequence of states (of length 10) into a single vector of dimension 560 before taking it in as an input. We train the gait classifier on the demonstration dataset. To ensure that the learned classifier can predict gait-ID on trajectories generated by the composition of skills, we use MixUp-style (Zhang et al., 2017) data augmentation during training. We create a synthetic subsequence of length 10 by concatenating two sampled sub-sequence (from the demonstration dataset) one-hot(i) + of length li and lj (where li + lj = 10) from gaits with ID i and j and give it a label\n\nli li+lj\n\nlj one-hot(j). During training, we sample a sub-sequence from the demonstration dataset with li+lj 70% probability and a sythenthic sub-sequence with 30% probability. We train the classifier for 2e6 train steps with a learning rate of 2e − 4 and a batch size of 64. Results Figures A4,A5,A6 show that the classifier’s prediction is consistent with the visualized composed trajectories. Furthermore, we use Decision diffuser to act in the environment and generate 1000 trott trajectories, 1000 pace trajectories, 1000 bound trajectories, and 1000 composed trajectories for each possible pair of individual gaits. We then evaluate the learned gait classifier on these trajectories and compute the percentage of timesteps a particular gait has the highest probability. From Figures A4,A5,A6, we can see that if trajectories are generated by the composition of two gaits, then those two gaits will have the two highest probabilities across different timesteps in those trajectories.\n\nI.2 A SIMPLE BASELINE FOR COMPOSITION\n\nLet one-hot(i) and one-hot(j) represent two different gaits that can be generated using noise models εθ(xk(τ ), one-hot(i), k) and εθ(xk(τ ), one-hot(j), k) respectively. To compose these gaits, we compose the above-mentioned noise models using equation 9. As an alternative, we see if the noise model εθ(xk(τ ), one-hot(i) + one-hot(j), k) can lead to composed gaits. However, we observe that εθ(xk(τ ), one-hot(i) + one-hot(j), k) catastrophically fail to generate any gait (see videos at https://anuragajay.github.io/decision-diffuser/). This happens because the condition variable one-hot(i) + one-hot(j) was never seen by the noise model εθ during training.\n\n19\n\nCondition\n\nTrott\n\nPace Bound\n\nOnly Trott Only Pace Trott + Pace\n\n97.2 0.8 55.6\n\n1.7 98.1 43.6\n\n1.1 1.1 0.8\n\nFigure A4: Composing Trott and Pace. Decision Diffuser can imitate individual running gaits using expert demonstrations and compose multiple different skills together during test time. The results are best illustrated by videos viewable at https://anuragajay.github.io/decision-diffuser/.\n\nCondition\n\nTrott\n\nPace Bound\n\nOnly Trott Only Bound Trott + Bound\n\n96.4 1.6 51.3\n\n2.2 0.6 0.9\n\n1.4 97.8 47.8\n\nFigure A5: Composing Trott and Bound. Decision Diffuser can imitate individual running gaits using expert demonstrations and compose multiple different skills together during test time. The results are best illustrated by videos viewable at https://anuragajay.github.io/decision-diffuser/.\n\nJ NOT COMPOSITIONS WITH DECISION DIFFUSER\n\nDecision diffuser can also support ”NOT” composition. Suppose we wanted to sample from q(x0(τ )|NOT yj(τ )). Let {yi(τ )}n i=1 be the set of all conditioning variables. Then, following derivations from Liu et al. (2022) and using β = 1, we can sample from q(x0(τ )|NOT yj(τ )) using the perturbed noise:\n\nˆε := εθ(xk(τ ), Ø, k) + ω(\n\n(cid:88)\n\n(εθ(xk(τ ), yi(τ ), k) − εθ(xk(τ ), Ø, k))\n\n− (εθ(xk(τ ), yj(τ ), k) − εθ(xk(τ ), Ø, k)))\n\ni̸=j\n\nWe demonstrate the ability of Decision Diffuser to support ”NOT” composition by using it to satisfy constraint of type BlockHeight(i) > BlockHeight(j) AND (NOT BlockHeight(j) > BlockHeight(i)) at stacking https://anuragajay.github.io/decision-diffuser/. As the Decision Diffuser does not provide an explicit density estimate for each skill, it can’t natively support OR composition.\n\nin Kuka\n\nvisualized\n\nvideos\n\nblock\n\ntask,\n\nas\n\nin\n\n20\n\nTrottPaceTrott + PaceTrottGait ProbabilityOnly TrottOnly PaceTrott + Pace501001502005010015020050100150200.20.40.60.80PaceBoundTrottBoundTrott + BoundTrottGait ProbabilityOnly TrottOnly BoundTrott + Bound501001502005010015020050100150200.20.40.60.80PaceBoundCondition\n\nTrott\n\nPace Bound\n\nOnly Bound Only Pace Bound + Pace\n\n0.8 1.4 1.4\n\n1.0 97.7 38.5\n\n98.2 0.9 60.1\n\nFigure A6: Composing Bound and Pace. Decision Diffuser can imitate individual running gaits using expert demonstrations and compose multiple different skills together during test time. The results are best illustrated by videos viewable at https://anuragajay.github.io/decision-diffuser/.\n\nK COMPARING Q-FUNCTION GUIDED DIFFUSION AND CLASSIFIER-FREE\n\nGUIDED DIFFUSION\n\nClassifier-free guided diffusion and Q-value guided diffusion are theoretically equivalent. However, as noted in several works (Nichol et al., 2021; Ho & Salimans, 2022; Saharia et al., 2022), classifier-free guidance performs better than classifier guidance (i.e. Q function guidance in our case) in practice. This is due to following reasons:\n\n• Classifier-guided diffusion models learns an unconditional diffusion model along with a classifier (Q-function in our case) and uses gradients from the classifier to perform conditional sampling. However, the unconditional diffusion model doesn’t need to focus on conditional modeling during training and only cares about conditional generation during testing after it has been trained. In contrast, classifier-free guidance relies on conditional diffusion model to estimate gradients of the implicit classifier. Since the conditional diffusion model, learned when using classifier-free guidance, focuses on conditional modeling during train time, it performs better in conditional generation during test time.\n\n• Q function trained on an offline dataset can erroneously predict high Q values for out-ofdistribution actions given any state. This problem has been extensively studied in offline RL literature (Kumar et al., 2020; Fujimoto et al., 2019; Levine et al., 2020). In online RL, this issue is automatically corrected when the policy acts in the environment, thinking an action to be good but then receives a low reward for it. In offline RL, this issue can’t be corrected easily; hence, the learned Q-function can often guide the diffusion model towards out-of-distribution actions that might be sub-optimal. In contrast, classifier-free guidance circumvents the issue of learning a Q-function and directly conditions the diffusion model on returns. Hence, classifier-free guidance doesn’t suffer due to errors in learned Q-functions and hence performs better than Q-function guided diffusion.\n\nL LIMITATIONS OF DECISION DIFFUSER\n\nWe summarize the limitations of Decision Diffuser:\n\n• No partial observability Decision Diffuser works with fully observable MDPs. Naive extensions to partially observed MDPs (POMDPs) may cause self-delusions (Ortega et al., 2021) in Decision Diffuser. Hence, extending Decision Diffuser to POMDPs could be an exciting avenue for future work.\n\n21\n\nBoundPaceBound + Pace TrottGait ProbabilityOnly BoundOnly PaceBound + Pace501001502005010015020050100150200.20.40.60.80PaceBound• Inability to explore the environment and update itself in online setting In this work, we focused on offline sequential decision making, thus circumventing the need for exploration. Using ideas from Zheng et al. (2022), future works could look into online fine-tuning of Decision Diffuser by leveraging entropy of the state-sequence model for exploration.\n\n• Experiments on only state-based environments While our work focused on state based environments, it can be extended to image based environments by performing the diffusion in latent space, rather than observation space, as done in Rombach et al. (2022).\n\n• Only AND and NOT compositions are supported Since Decision Diffuser does not provide an explicit density estimate for each condition variable, it can’t natively support OR composition.\n\n• Performance degradation in environments with stochastic dynamics In environments with highly stochastic dynamics, Decision Diffuser loses its advantage and performs similarly to Diffuser and CQL. To tackle environments with stochastic dynamics, recent works (Yang et al., 2022; Villaflor et al., 2022) propose learning a latent model for future states and then conditioning the policy on predicted latent future states rather than returns. Conditioning Decision Diffuser on future state information, rather than returns, would make it more robust to stochastic dynamics and could be an interesting avenue for future works.\n\n• Performance in limited data regime Since diffusion models are prone to overfitting in case\n\nof limited data, Decision Diffuser is also prone to overfitting in limited data regime.\n\n22",
    "reference": "# Summary Of The Paper\n\nThis paper presents Decision Diffuser, a conditional generative model for sequential decision making. It frames offline sequential decision making as conditional generative modeling by considering  two other variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrates a composition of skills. Experiments are conducted on a couple of different decision making tasks.\n\n# Strength And Weaknesses\n\nPros:\nThis paper is well motivated and easy to follow. Inspired from diffusion models in vision domain, this paper formulate the decision making process as a condition generation problem and which can naturally achieved by leveraging diffusion models. I like the way that this paper very clearly introducing the technical background and formulate the problem. From the experimental results, the proposed Decision Diffuser is promising on a couple of evaluating tasks.\n\nCos:\nMy main concern is the limited technical novelty. The key contribution of this work is to formulate the decision making problems as a conditional generation problem. Based on this, this paper train a diffusion model on offline datasets. However, it is more like an application of diffusion models on decision making tasks. I don't see obvious novelty from either model design and/or training objectives.\nThe evaluations are weak. It only evaluate the DD variants on a few tasks. No comparisons of DD with the other state-of-the-art approaches are shown. Also, no ablation and discussion are shown. It is not convincing to me without such extensive study.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is well motivated and easy to follow. Inspired from diffusion models in vision domain, this paper formulate the decision making process as a condition generation problem and which can naturally achieved by leveraging diffusion models. I like the way that this paper very clearly introducing the technical background and formulate the problem. From the experimental results, the proposed Decision Diffuser is promising on a couple of evaluating tasks.\nMy main concern is the limited technical novelty. The key contribution of this work is to formulate the decision making problems as a conditional generation problem. Based on this, this paper train a diffusion model on offline datasets. However, it is more like an application of diffusion models on decision making tasks. I don't see obvious novelty from either model design and/or training objectives.\nThe evaluations are weak. It only evaluate the DD variants on a few tasks. No comparisons of DD with the other state-of-the-art approaches are shown. Also, no ablation and discussion are shown. It is not convincing to me without such extensive study.\nIt did not submit the source code, so the reproducibility is hard to be validated.\n\n# Summary Of The Review\n\nThis paper is well motivated and easy to follow. Inspired from diffusion models in vision domain, this paper formulate the decision making process as a condition generation problem and which can naturally achieved by leveraging diffusion models. I like the way that this paper very clearly introducing the technical background and formulate the problem. From the experimental results, the proposed Decision Diffuser is promising on a couple of evaluating tasks.\nMy main concern is the limited technical novelty. The key contribution of this work is to formulate the decision making problems as a conditional generation problem. Based on this, this paper train a diffusion model on offline datasets. However, it is more like an application of diffusion models on decision making tasks. I don't see obvious novelty from either model design and/or training objectives.\nThe evaluations are weak. It only evaluate the DD variants on a few tasks. No comparisons of DD with the other state-of-the-art approaches are shown. Also, no ablation and discussion are shown. It is not convincing to me without such extensive study.\nIn addition, it did not submit the source code, so the reproducibility is hard to be validated.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTOWARDS EQUIVARIANT GRAPH CONTRASTIVE LEARNING VIA CROSS-GRAPH AUGMENTATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nLeading graph contrastive learning (GCL) frameworks conform to the invariance mechanism by encouraging insensitivity to different augmented views of the same graph. Despite the promising performance, invariance worsens representation when augmentations cause aggressive semantics shifts. For example, dropping the super-node can dramatically change a social network’s topology. In this case, encouraging invariance to the original graph can bring together dissimilar patterns and hurt the task of instance discrimination. To resolve the problem, we get inspiration from equivariant self-supervised learning and propose Equivariant Graph Contrastive Learning (E-GCL) to encourage the sensitivity to global semantic shifts. Viewing each graph as a transformation to others, we ground the equivariance principle as a cross-graph augmentation – graph interpolation – to simulate global semantic shifts. Without using annotation, we supervise the representation of cross-graph augmented views by linearly combining the representations of their original samples. This simple but effective equivariance It principle empowers E-GCL with the ability of cross-graph discrimination. shows significant improvements over the state-of-the-art GCL models in unsupervised learning and transfer learning. Further experiments demonstrate E-GCL’s generalization to various graph pre-training frameworks. Code is available at https://anonymous.4open.science/r/E-GCL/\n\n1\n\nINTRODUCTION\n\nGraph contrastive learning (GCL) (You et al., 2020; Suresh et al., 2021; Xu et al., 2021) is a prevailing paradigm for self-supervised learning (Chen et al., 2020; Zbontar et al., 2021) on graph-structured data. It typically pre-trains a graph neural network (GNN) (Dwivedi et al., 2020) without labeled data, in an effort to learn generalizable representations and boost the fine-tuning on downstream tasks. The common theme across recent GCL studies is instance discrimination (Dosovitskiy et al., 2014; Purushwalkam & Gupta, 2020) — viewing each graph as a class of its own, and differing it from other graphs. It galvanizes representation learning to capture discriminative characteristics of graphs.\n\nTowards this end, leading GCL works usually employ two key modules: graph augmentation and contrastive learning. Specifically, graph augmentation adopts the “intra-graph” strategy to create multiple augmented views of each graph, such as randomly dropping nodes (You et al., 2020) or adversarially perturbing edges (Suresh et al., 2021). The views stemming from the same graph constitute the positive samples of this class, while the views of other graphs are treated as negatives. Consequently, contrastive learning encourages the agreement between positive samples and the discrepancy between negatives. This procedure essentially imposes “invariance” (Purushwalkam & Gupta, 2020; Dangovski et al., 2022) upon representations — making the anchor graph’s representation invariant to its intra-graph augmentations (Figure 1a). Formally, let g be the anchor graph, ) be the GNN encoder. The “invariance to P\n— the representation φ(g) intra-graph augmentations” mechanism states φ(g) = φ(Tp(g)), is insensitive to the changes in augmentation p, where Tp(g) is the action of augmentation p on graph g. We refer to works adopting this mechanism as Invariant Graph Contrastive Learning (I-GCL).\n\nbe the groups of intra-graph augmentations, and φ( ·\n\n∈ P\n\n∀\n\np\n\nHowever, we argue that invariance to intra-graph augmentations alone is insufficient to improve the semantic quality of graph representations and boost the downstream performance:\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Invariance.\n\n(b) Equivariance\n\nFigure 1: (a) invariance to intra-graph augmentations; (b) equivariance to cross-graph augmentations.\n\n• Limiting the augmentations to local substructures of an individual graph is aggressive (Purushwalkam & Gupta, 2020; Wang et al., 2022) insofar as the augmented views fragmentarily or even wrongly describe the characteristics of the anchor graph. Take a molecule graph as an example. After randomly dropping some nodes, one view could hold a cyano group (-C N) that determines the property of molecule hypertoxic, while another could corrupt this functional group. Thus, intra-graph augmentations are inadequate for presenting a holistic view of the anchor graph.\n\n≡\n\n• Worse still, aggressive augmentations easily make two positive views far from each other, but the invariance mechanism blindly forces their representations to be invariant. Considering the molecule graph’s views again (cf. Figure 1a), invariance-guided contrastive learning simply maximizes their representation agreement, regardless of the changes in the hypertoxic property. Therefore, it might amplify the negative impact of aggressive intra-graph augmentations and restrain representations from reflecting the instance semantics faithfully.\n\nTo mitigate these negative influences, we get inspiration from the recent work on equivariant selfsupervised learning (E-SSL) (Dangovski et al., 2022). It splits the augmentations into two parts, to which representations should be insensitive and sensitive, and then establishes the invariance and equivariance mechanisms correspondingly. The idea of “equivariance” is our focus, which makes . Here we formulate it representations aware of semantic changes caused by certain augmentations H\nas φ(Th(g)) = T ′ h(φ(g)) are the actions of augmentation h on graph g and representation φ(g), respectively. Jointly learning equivariance to sensitive augmentations is promising to shield representations from the H\nP harms of aggressive augmentations. Nonetheless, it is hard, without domain knowledge (Dangovski et al., 2022; Chuang et al., 2022) or extensive testing (Dangovski et al., 2022), to tell apart sensitive and insensitive augmentations.\n\nand invariance to insensitive augmentations\n\n, where Th(g) and T ′\n\nh(φ(g)),\n\n∈ H\n\nh\n\n∀\n\nTo embody equivariance in GCL, we propose a simple but effective approach of Equivariant Graph Contrastive Learning (E-GCL). E-GCL is an instantiation of E-SSL for graphs. Unlike previous E-SSL works, E-GCL leaves existing intra-graph augmentations untouched, and creates new augmentations through the “cross-graph” strategy. Concretely, inspired by mixup (Guo & Mao, 2021; Zhang et al., 2018), the cross-graph augmentation interpolates the raw features of two graphs (i.e., Th), while employing the same interpolation strategy on the graph labels that are portrayed by graph representations (i.e., T ′ h). The augmentations across graphs not only maintain the holistic information on self-discrimination, but also are orthogonal to the intra-graph augmentations. On the top of intraand cross-graph of augmentations, E-GCL separately builds the invariance and equivariance principles to guide the representation learning. The equivariance to cross-graph augmentations diminishes the harmful invariance to aggressive augmentations that change global semantics. Integrating two principles enables representations to be sensitive to global semantic shifts across different graphs and insensitive to local substructure perturbations of single graphs. Experiments show that EGCL achieves promising performances to surpass current state-of-the-art GCL models, across diverse settings. We also demonstrate E-GCL’s generalization to various SSL frameworks, including BarlowTwins (Zbontar et al., 2021), GraphCL (You et al., 2020) and SimSiam (Chen & He, 2021).\n\n2 PRELIMINARIES: INVARIANT GRAPH CONTRASTIVE LEARNING\n\nWe begin by presenting the instance discrimination task and the invariance mechanism of I-GCL, and then introduce two key ingredients: graph augmentations and contrastive learning.\n\n2\n\n<latexit sha1_base64=\"G0XCydthVvZSM9aDf4rVDBqn8AQ=\">AAAB63icbVC7SgNBFL0bXzG+opZpBkPAKuyKaMqAjWUE84AkyOxkNjtkZnaZmRXCks7axkIRW3/FD7DTD/AL/ABnkxSaeODC4Zx7ufceP+ZMG9f9cHIrq2vrG/nNwtb2zu5ecf+gpaNEEdokEY9Ux8eaciZp0zDDaSdWFAuf07Y/usj89i1VmkXy2oxj2hd4KFnACDaZ1ItDdlMsu1V3CrRMvDkp10uVu++3r8/GTfG9N4hIIqg0hGOtu54bm36KlWGE00mhl2gaYzLCQ9q1VGJBdT+d3jpBFasMUBApW9Kgqfp7IsVC67HwbafAJtSLXib+53UTE9T6KZNxYqgks0VBwpGJUPY4GjBFieFjSzBRzN6KSIgVJsbGU7AheIsvL5PWSdU7q55e2TRqMEMeSnAEx+DBOdThEhrQBAIh3MMjPDnCeXCenZdZa86ZzxzCHzivPxjdkrg=</latexit><latexit sha1_base64=\"G0XCydthVvZSM9aDf4rVDBqn8AQ=\">AAAB63icbVC7SgNBFL0bXzG+opZpBkPAKuyKaMqAjWUE84AkyOxkNjtkZnaZmRXCks7axkIRW3/FD7DTD/AL/ABnkxSaeODC4Zx7ufceP+ZMG9f9cHIrq2vrG/nNwtb2zu5ecf+gpaNEEdokEY9Ux8eaciZp0zDDaSdWFAuf07Y/usj89i1VmkXy2oxj2hd4KFnACDaZ1ItDdlMsu1V3CrRMvDkp10uVu++3r8/GTfG9N4hIIqg0hGOtu54bm36KlWGE00mhl2gaYzLCQ9q1VGJBdT+d3jpBFasMUBApW9Kgqfp7IsVC67HwbafAJtSLXib+53UTE9T6KZNxYqgks0VBwpGJUPY4GjBFieFjSzBRzN6KSIgVJsbGU7AheIsvL5PWSdU7q55e2TRqMEMeSnAEx+DBOdThEhrQBAIh3MMjPDnCeXCenZdZa86ZzxzCHzivPxjdkrg=</latexit><latexit sha1_base64=\"t8Js8zp7a2kADg5d2elWPt3VR3c=\">AAAB7nicbZDLSgMxFIbP1Kq13qou3USLUDdlpoh2IxTcuKxgL9IOJZNm2tBMJiQZoQx9CDcuFHHrY/gM7nwb08tCW38IfPz/OeScE0jOtHHdbyezll3f2Mxt5bd3dvf2CweHTR0nitAGiXms2gHWlDNBG4YZTttSURwFnLaC0c00bz1SpVks7s1YUj/CA8FCRrCxVqsrh6w0OO8Vim7ZnQmtgreAYg1di+znw0m9V/jq9mOSRFQYwrHWHc+Vxk+xMoxwOsl3E00lJiM8oB2LAkdU++ls3Ak6s04fhbGyTxg0c393pDjSehwFtjLCZqiXs6n5X9ZJTFj1UyZkYqgg84/ChCMTo+nuqM8UJYaPLWCimJ0VkSFWmBh7obw9gre88io0K2Xvsnxx5xVrVZgrB8dwCiXw4ApqcAt1aACBETzBC7w60nl23pz3eWnGWfQcwR85Hz+wfZFa</latexit>(g)<latexit sha1_base64=\"0kiBn4BsdNmHrLDjQd23x7EAoG8=\">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZcFF7qsYB/QDiWTZtrQTGZI7ghl6Ge4caGIW7/GnX9jpq2grQcCh3PuJeeeIJHCoOt+OYW19Y3NreJ2aWd3b/+gfHjUMnGqGW+yWMa6E1DDpVC8iQIl7ySa0yiQvB2Mb3K//ci1EbF6wEnC/YgOlQgFo2ilbi+iOGJUZrfTfrniVt0ZyCrxFqRShzka/fJnbxCzNOIKmaTGdD03QT+jGgWTfFrqpYYnlI3pkHctVTTixs9mkafkzCoDEsbaPoVkpv7eyGhkzCQK7GQe0Sx7ufif100xvPYzoZIUuWLzj8JUEoxJfj8ZCM0ZyokllGlhsxI2opoytC2VbAne8smrpHVR9S6rtftapf5TRxFO4BTOwYMrqMMdNKAJDGJ4ghd4ddB5dt6c9/lowVnsHMMfOB/f0sCRpA==</latexit>G<latexit sha1_base64=\"1CPgZhYAQv3ukE0+WFhrNlGH2Fo=\">AAAB+nicbVDLSsNAFL3xWesr1aWbwSLUTUmkqMuCC11WsA9oQplMJ+3QySTMTJQS+yluXCji1i9x5984aSto64GBwzn3cs+cIOFMacf5slZW19Y3Ngtbxe2d3b19u3TQUnEqCW2SmMeyE2BFORO0qZnmtJNIiqOA03Ywusr99j2VisXiTo8T6kd4IFjICNZG6tklLxmyihdhPSSYZ9eT055ddqrOFGiZuHNSrsMMjZ796fVjkkZUaMKxUl3XSbSfYakZ4XRS9FJFE0xGeEC7hgocUeVn0+gTdGKUPgpjaZ7QaKr+3shwpNQ4CsxknlEtern4n9dNdXjpZ0wkqaaCzA6FKUc6RnkPqM8kJZqPDcFEMpMVkSGWmGjTVtGU4C5+eZm0zqruebV2WyvXf+oowBEcQwVcuIA63EADmkDgAZ7gBV6tR+vZerPeZ6Mr1nznEP7A+vgGM06T/w==</latexit>(G)<latexit sha1_base64=\"SnVd6K1rYJmhzhdaAZPzCw8Cm4k=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiDkGvHhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORoUS27ZXYBsEm9FSnVYojEofvWHMUsjlIYJqnXPcxPjZ1QZzgTOCv1UY0LZhI6wZ6mkEWo/Wxw6I1dWGZIwVrakIQv190RGI62nUWA7I2rGet2bi/95vdSENT/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1WurOPJwAZdwDR7cQh3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzByNwjSo=</latexit>g<latexit sha1_base64=\"LXemwC7qSXvDxrmNfwDpJqp96hw=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyGoB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0q6UvZtytVkp1aqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+r9jQA=</latexit>C<latexit sha1_base64=\"LXemwC7qSXvDxrmNfwDpJqp96hw=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyGoB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0q6UvZtytVkp1aqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+r9jQA=</latexit>C<latexit sha1_base64=\"IiYni4SMw6/Km5OrcF+NkoGznNo=\">AAAB8nicbVDLSgNBEJz1GeMr6tHLYhByCrshqMdALjlGMA/YLGF20kmGzM4sM71iWPIZXjwo4tWv8ebfOHkcNLGgoajqprsrSgQ36Hnfztb2zu7efu4gf3h0fHJaODtvG5VqBi2mhNLdiBoQXEILOQroJhpoHAnoRJP63O88gjZcyQecJhDGdCT5kDOKVgp6CE+Y1RuzfqVfKHplbwF3k/grUqyRJZr9wldvoFgag0QmqDGB7yUYZlQjZwJm+V5qIKFsQkcQWCppDCbMFifP3GurDNyh0rYkugv190RGY2OmcWQ7Y4pjs+7Nxf+8IMXhXZhxmaQIki0XDVPhonLn/7sDroGhmFpCmeb2VpeNqaYMbUp5G4K//vImaVfK/k25el8t1kqrOHLkklyREvHJLamRBmmSFmFEkWfySt4cdF6cd+dj2brlrGYuyB84nz9q+5FM</latexit>CH2<latexit sha1_base64=\"IiYni4SMw6/Km5OrcF+NkoGznNo=\">AAAB8nicbVDLSgNBEJz1GeMr6tHLYhByCrshqMdALjlGMA/YLGF20kmGzM4sM71iWPIZXjwo4tWv8ebfOHkcNLGgoajqprsrSgQ36Hnfztb2zu7efu4gf3h0fHJaODtvG5VqBi2mhNLdiBoQXEILOQroJhpoHAnoRJP63O88gjZcyQecJhDGdCT5kDOKVgp6CE+Y1RuzfqVfKHplbwF3k/grUqyRJZr9wldvoFgag0QmqDGB7yUYZlQjZwJm+V5qIKFsQkcQWCppDCbMFifP3GurDNyh0rYkugv190RGY2OmcWQ7Y4pjs+7Nxf+8IMXhXZhxmaQIki0XDVPhonLn/7sDroGhmFpCmeb2VpeNqaYMbUp5G4K//vImaVfK/k25el8t1kqrOHLkklyREvHJLamRBmmSFmFEkWfySt4cdF6cd+dj2brlrGYuyB84nz9q+5FM</latexit>CH2<latexit sha1_base64=\"IiYni4SMw6/Km5OrcF+NkoGznNo=\">AAAB8nicbVDLSgNBEJz1GeMr6tHLYhByCrshqMdALjlGMA/YLGF20kmGzM4sM71iWPIZXjwo4tWv8ebfOHkcNLGgoajqprsrSgQ36Hnfztb2zu7efu4gf3h0fHJaODtvG5VqBi2mhNLdiBoQXEILOQroJhpoHAnoRJP63O88gjZcyQecJhDGdCT5kDOKVgp6CE+Y1RuzfqVfKHplbwF3k/grUqyRJZr9wldvoFgag0QmqDGB7yUYZlQjZwJm+V5qIKFsQkcQWCppDCbMFifP3GurDNyh0rYkugv190RGY2OmcWQ7Y4pjs+7Nxf+8IMXhXZhxmaQIki0XDVPhonLn/7sDroGhmFpCmeb2VpeNqaYMbUp5G4K//vImaVfK/k25el8t1kqrOHLkklyREvHJLamRBmmSFmFEkWfySt4cdF6cd+dj2brlrGYuyB84nz9q+5FM</latexit>CH2<latexit sha1_base64=\"IiYni4SMw6/Km5OrcF+NkoGznNo=\">AAAB8nicbVDLSgNBEJz1GeMr6tHLYhByCrshqMdALjlGMA/YLGF20kmGzM4sM71iWPIZXjwo4tWv8ebfOHkcNLGgoajqprsrSgQ36Hnfztb2zu7efu4gf3h0fHJaODtvG5VqBi2mhNLdiBoQXEILOQroJhpoHAnoRJP63O88gjZcyQecJhDGdCT5kDOKVgp6CE+Y1RuzfqVfKHplbwF3k/grUqyRJZr9wldvoFgag0QmqDGB7yUYZlQjZwJm+V5qIKFsQkcQWCppDCbMFifP3GurDNyh0rYkugv190RGY2OmcWQ7Y4pjs+7Nxf+8IMXhXZhxmaQIki0XDVPhonLn/7sDroGhmFpCmeb2VpeNqaYMbUp5G4K//vImaVfK/k25el8t1kqrOHLkklyREvHJLamRBmmSFmFEkWfySt4cdF6cd+dj2brlrGYuyB84nz9q+5FM</latexit>CH2<latexit sha1_base64=\"LXemwC7qSXvDxrmNfwDpJqp96hw=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyGoB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0q6UvZtytVkp1aqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+r9jQA=</latexit>C<latexit sha1_base64=\"LXemwC7qSXvDxrmNfwDpJqp96hw=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyGoB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0q6UvZtytVkp1aqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+r9jQA=</latexit>C<latexit sha1_base64=\"LXemwC7qSXvDxrmNfwDpJqp96hw=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyGoB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0q6UvZtytVkp1aqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+r9jQA=</latexit>C<latexit sha1_base64=\"WKuzwCv4nZVmBXcuZ17eKfhI434=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DXjxJAuYByRJmJ73JmNnZZWZWCEu+wIsHRbz6Sd78GyePgyYWNBRV3XR3BYng2rjut5Pb2Nza3snvFvb2Dw6PiscnLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2d++wmV5rF8MJME/YgOJQ85o8ZKjft+seSW3TnIOvGWpFSDBer94ldvELM0QmmYoFp3PTcxfkaV4UzgtNBLNSaUjekQu5ZKGqH2s/mhU3JhlQEJY2VLGjJXf09kNNJ6EgW2M6JmpFe9mfif101NeONnXCapQckWi8JUEBOT2ddkwBUyIyaWUKa4vZWwEVWUGZtNwYbgrb68TlpXZa9arjQqpVplGUcezuAcLsGDa6jBHdShCQwQnuEV3pxH58V5dz4WrTlnOXMKf+B8/gD8SY0N</latexit>N<latexit sha1_base64=\"IiYni4SMw6/Km5OrcF+NkoGznNo=\">AAAB8nicbVDLSgNBEJz1GeMr6tHLYhByCrshqMdALjlGMA/YLGF20kmGzM4sM71iWPIZXjwo4tWv8ebfOHkcNLGgoajqprsrSgQ36Hnfztb2zu7efu4gf3h0fHJaODtvG5VqBi2mhNLdiBoQXEILOQroJhpoHAnoRJP63O88gjZcyQecJhDGdCT5kDOKVgp6CE+Y1RuzfqVfKHplbwF3k/grUqyRJZr9wldvoFgag0QmqDGB7yUYZlQjZwJm+V5qIKFsQkcQWCppDCbMFifP3GurDNyh0rYkugv190RGY2OmcWQ7Y4pjs+7Nxf+8IMXhXZhxmaQIki0XDVPhonLn/7sDroGhmFpCmeb2VpeNqaYMbUp5G4K//vImaVfK/k25el8t1kqrOHLkklyREvHJLamRBmmSFmFEkWfySt4cdF6cd+dj2brlrGYuyB84nz9q+5FM</latexit>CH2<latexit sha1_base64=\"IiYni4SMw6/Km5OrcF+NkoGznNo=\">AAAB8nicbVDLSgNBEJz1GeMr6tHLYhByCrshqMdALjlGMA/YLGF20kmGzM4sM71iWPIZXjwo4tWv8ebfOHkcNLGgoajqprsrSgQ36Hnfztb2zu7efu4gf3h0fHJaODtvG5VqBi2mhNLdiBoQXEILOQroJhpoHAnoRJP63O88gjZcyQecJhDGdCT5kDOKVgp6CE+Y1RuzfqVfKHplbwF3k/grUqyRJZr9wldvoFgag0QmqDGB7yUYZlQjZwJm+V5qIKFsQkcQWCppDCbMFifP3GurDNyh0rYkugv190RGY2OmcWQ7Y4pjs+7Nxf+8IMXhXZhxmaQIki0XDVPhonLn/7sDroGhmFpCmeb2VpeNqaYMbUp5G4K//vImaVfK/k25el8t1kqrOHLkklyREvHJLamRBmmSFmFEkWfySt4cdF6cd+dj2brlrGYuyB84nz9q+5FM</latexit>CH2<latexit sha1_base64=\"LXemwC7qSXvDxrmNfwDpJqp96hw=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyGoB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0q6UvZtytVkp1aqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+r9jQA=</latexit>C<latexit sha1_base64=\"LXemwC7qSXvDxrmNfwDpJqp96hw=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyGoB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0q6UvZtytVkp1aqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+r9jQA=</latexit>C<latexit sha1_base64=\"LXemwC7qSXvDxrmNfwDpJqp96hw=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyGoB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0q6UvZtytVkp1aqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+r9jQA=</latexit>C<latexit sha1_base64=\"WKuzwCv4nZVmBXcuZ17eKfhI434=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DXjxJAuYByRJmJ73JmNnZZWZWCEu+wIsHRbz6Sd78GyePgyYWNBRV3XR3BYng2rjut5Pb2Nza3snvFvb2Dw6PiscnLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2d++wmV5rF8MJME/YgOJQ85o8ZKjft+seSW3TnIOvGWpFSDBer94ldvELM0QmmYoFp3PTcxfkaV4UzgtNBLNSaUjekQu5ZKGqH2s/mhU3JhlQEJY2VLGjJXf09kNNJ6EgW2M6JmpFe9mfif101NeONnXCapQckWi8JUEBOT2ddkwBUyIyaWUKa4vZWwEVWUGZtNwYbgrb68TlpXZa9arjQqpVplGUcezuAcLsGDa6jBHdShCQwQnuEV3pxH58V5dz4WrTlnOXMKf+B8/gD8SY0N</latexit>N<latexit sha1_base64=\"IiYni4SMw6/Km5OrcF+NkoGznNo=\">AAAB8nicbVDLSgNBEJz1GeMr6tHLYhByCrshqMdALjlGMA/YLGF20kmGzM4sM71iWPIZXjwo4tWv8ebfOHkcNLGgoajqprsrSgQ36Hnfztb2zu7efu4gf3h0fHJaODtvG5VqBi2mhNLdiBoQXEILOQroJhpoHAnoRJP63O88gjZcyQecJhDGdCT5kDOKVgp6CE+Y1RuzfqVfKHplbwF3k/grUqyRJZr9wldvoFgag0QmqDGB7yUYZlQjZwJm+V5qIKFsQkcQWCppDCbMFifP3GurDNyh0rYkugv190RGY2OmcWQ7Y4pjs+7Nxf+8IMXhXZhxmaQIki0XDVPhonLn/7sDroGhmFpCmeb2VpeNqaYMbUp5G4K//vImaVfK/k25el8t1kqrOHLkklyREvHJLamRBmmSFmFEkWfySt4cdF6cd+dj2brlrGYuyB84nz9q+5FM</latexit>CH2<latexit sha1_base64=\"IiYni4SMw6/Km5OrcF+NkoGznNo=\">AAAB8nicbVDLSgNBEJz1GeMr6tHLYhByCrshqMdALjlGMA/YLGF20kmGzM4sM71iWPIZXjwo4tWv8ebfOHkcNLGgoajqprsrSgQ36Hnfztb2zu7efu4gf3h0fHJaODtvG5VqBi2mhNLdiBoQXEILOQroJhpoHAnoRJP63O88gjZcyQecJhDGdCT5kDOKVgp6CE+Y1RuzfqVfKHplbwF3k/grUqyRJZr9wldvoFgag0QmqDGB7yUYZlQjZwJm+V5qIKFsQkcQWCppDCbMFifP3GurDNyh0rYkugv190RGY2OmcWQ7Y4pjs+7Nxf+8IMXhXZhxmaQIki0XDVPhonLn/7sDroGhmFpCmeb2VpeNqaYMbUp5G4K//vImaVfK/k25el8t1kqrOHLkklyREvHJLamRBmmSFmFEkWfySt4cdF6cd+dj2brlrGYuyB84nz9q+5FM</latexit>CH2<latexit sha1_base64=\"IiYni4SMw6/Km5OrcF+NkoGznNo=\">AAAB8nicbVDLSgNBEJz1GeMr6tHLYhByCrshqMdALjlGMA/YLGF20kmGzM4sM71iWPIZXjwo4tWv8ebfOHkcNLGgoajqprsrSgQ36Hnfztb2zu7efu4gf3h0fHJaODtvG5VqBi2mhNLdiBoQXEILOQroJhpoHAnoRJP63O88gjZcyQecJhDGdCT5kDOKVgp6CE+Y1RuzfqVfKHplbwF3k/grUqyRJZr9wldvoFgag0QmqDGB7yUYZlQjZwJm+V5qIKFsQkcQWCppDCbMFifP3GurDNyh0rYkugv190RGY2OmcWQ7Y4pjs+7Nxf+8IMXhXZhxmaQIki0XDVPhonLn/7sDroGhmFpCmeb2VpeNqaYMbUp5G4K//vImaVfK/k25el8t1kqrOHLkklyREvHJLamRBmmSFmFEkWfySt4cdF6cd+dj2brlrGYuyB84nz9q+5FM</latexit>CH2<latexit sha1_base64=\"IiYni4SMw6/Km5OrcF+NkoGznNo=\">AAAB8nicbVDLSgNBEJz1GeMr6tHLYhByCrshqMdALjlGMA/YLGF20kmGzM4sM71iWPIZXjwo4tWv8ebfOHkcNLGgoajqprsrSgQ36Hnfztb2zu7efu4gf3h0fHJaODtvG5VqBi2mhNLdiBoQXEILOQroJhpoHAnoRJP63O88gjZcyQecJhDGdCT5kDOKVgp6CE+Y1RuzfqVfKHplbwF3k/grUqyRJZr9wldvoFgag0QmqDGB7yUYZlQjZwJm+V5qIKFsQkcQWCppDCbMFifP3GurDNyh0rYkugv190RGY2OmcWQ7Y4pjs+7Nxf+8IMXhXZhxmaQIki0XDVPhonLn/7sDroGhmFpCmeb2VpeNqaYMbUp5G4K//vImaVfK/k25el8t1kqrOHLkklyREvHJLamRBmmSFmFEkWfySt4cdF6cd+dj2brlrGYuyB84nz9q+5FM</latexit>CH2<latexit sha1_base64=\"dGiXgCejlq2KIbqKmwWPmj3/GZg=\">AAAB8nicbZDLSgMxFIYzrZdab1WXblKLUDdlRkS7LLhxWaE3mA5DJs20oZlkSDJCGerOR3DjQhG3PocP4M5HcWem7UJbfwh8/P855JwTxIwqbdtfVi6/tr6xWdgqbu/s7u2XDg47SiQSkzYWTMhegBRhlJO2ppqRXiwJigJGusH4Osu7d0QqKnhLT2LiRWjIaUgx0sZyW34a+860Ojwr+qWKXbNngqvgLKDSKH+X8w8f902/9NkfCJxEhGvMkFKuY8faS5HUFDMyLfYTRWKEx2hIXIMcRUR56WzkKTw1zgCGQprHNZy5vztSFCk1iQJTGSE9UstZZv6XuYkO615KeZxowvH8ozBhUAuY7Q8HVBKs2cQAwpKaWSEeIYmwNlfKjuAsr7wKnfOac1m7uHUqjTqYqwCOwQmoAgdcgQa4AU3QBhgI8AiewYulrSfr1Xqbl+asRc8R+CPr/QdN3JOY</latexit>Tp1(g)<latexit sha1_base64=\"D6tB+XInLzevV7d2AI7P43r1StU=\">AAAB8nicbZDLSgMxFIYz1kutt6pLN6lFqJsyU0S7LLhxWaE3mA5DJs20oZlkSDJCGerOR3DjQhG3PocP4M5HcWem7UJbfwh8/P855JwTxIwqbdtf1lpufWNzK79d2Nnd2z8oHh51lEgkJm0smJC9ACnCKCdtTTUjvVgSFAWMdIPxdZZ374hUVPCWnsTEi9CQ05BipI3ltvw09mvTyvC84BfLdtWeCa6Cs4Byo/Rdyj183Df94md/IHASEa4xQ0q5jh1rL0VSU8zItNBPFIkRHqMhcQ1yFBHlpbORp/DMOAMYCmke13Dm/u5IUaTUJApMZYT0SC1nmflf5iY6rHsp5XGiCcfzj8KEQS1gtj8cUEmwZhMDCEtqZoV4hCTC2lwpO4KzvPIqdGpV57J6ceuUG3UwVx6cgFNQAQ64Ag1wA5qgDTAQ4BE8gxdLW0/Wq/U2L12zFj3H4I+s9x9PZZOZ</latexit>Tp2(g)<latexit sha1_base64=\"HL38iiYqwqN0y33kaRbG0v0owfo=\">AAAB73icbZC7SgNBFIbPJl5ivEUtbSYGwSrsBtGUARvLCLlBEpbZyWwyZHZ2nZkVwhI7X8DGQhFbX8QHsPNR7JxNUmjiDwMf/38Oc87xIs6Utu0vK5NdW9/YzG3lt3d29/YLB4ctFcaS0CYJeSg7HlaUM0GbmmlOO5GkOPA4bXvjqzRv31GpWCgaehLRfoCHgvmMYG2sTsNNIrcyzbuFkl22Z0Kr4CygVCt+F7MPH/d1t/DZG4QkDqjQhGOluo4d6X6CpWaE02m+FysaYTLGQ9o1KHBAVT+ZzTtFp8YZID+U5gmNZu7vjgQHSk0Cz1QGWI/Ucpaa/2XdWPvVfsJEFGsqyPwjP+ZIhyhdHg2YpETziQFMJDOzIjLCEhNtTpQewVleeRValbJzUT6/cUq1KsyVg2M4gTNw4BJqcA11aAIBDo/wDC/WrfVkvVpv89KMteg5gj+y3n8AwBiSww==</latexit>Tp2<latexit sha1_base64=\"cQkVA/sjvo1J+OKm/lVmF7UCJLg=\">AAAB73icbZC7SgNBFIbPGi8x3qKWNhODYBV2RTRlwMYyQm6QLMvsZDYZMju7zswKYYmdL2BjoYitL+ID2Pkods4mKTTxh4GP/z+HOef4MWdK2/aXtZJbXVvfyG8WtrZ3dveK+wctFSWS0CaJeCQ7PlaUM0GbmmlOO7GkOPQ5bfujqyxv31GpWCQaehxTN8QDwQJGsDZWp+GlsedMCl6xbFfsqdAyOHMo10rfpdzDx33dK372+hFJQio04ViprmPH2k2x1IxwOin0EkVjTEZ4QLsGBQ6pctPpvBN0Ypw+CiJpntBo6v7uSHGo1Dj0TWWI9VAtZpn5X9ZNdFB1UybiRFNBZh8FCUc6QtnyqM8kJZqPDWAimZkVkSGWmGhzouwIzuLKy9A6qzgXlfMbp1yrwkx5OIJjOAUHLqEG11CHJhDg8AjP8GLdWk/Wq/U2K12x5j2H8EfW+w++kpLC</latexit>Tp1<latexit sha1_base64=\"OJ5dyR2Y8Mv/B/jcSK0rqjbhJiM=\">AAAB8HicbVDLSgNBEOyNUWN8RT16GQ1CvIRdEc1FCHjxGMG8SJYwO5lNhszMLjOzQljyFV48KOLVr/AbvPk3Th4HTSxoKKq66e4KYs60cd1vJ7OWXd/YzG3lt3d29/YLB4cNHSWK0DqJeKRaAdaUM0nrhhlOW7GiWAScNoPR7dRvPlKlWSQfzDimvsADyUJGsLFSuxsPWWnQ8857haJbdmdAq8RbkGIV3cjsZ/uk1it8dfsRSQSVhnCsdcdzY+OnWBlGOJ3ku4mmMSYjPKAdSyUWVPvp7OAJOrNKH4WRsiUNmqm/J1IstB6LwHYKbIZ62ZuK/3mdxIQVP2UyTgyVZL4oTDgyEZp+j/pMUWL42BJMFLO3IjLEChNjM8rbELzll1dJ46LsXZUv720aFZgjB8dwCiXw4BqqcAc1qAMBAU/wAq+Ocp6dN+d93ppxFjNH8AfOxw/ZpZH9</latexit>(g1)<latexit sha1_base64=\"0kiBn4BsdNmHrLDjQd23x7EAoG8=\">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZcFF7qsYB/QDiWTZtrQTGZI7ghl6Ge4caGIW7/GnX9jpq2grQcCh3PuJeeeIJHCoOt+OYW19Y3NreJ2aWd3b/+gfHjUMnGqGW+yWMa6E1DDpVC8iQIl7ySa0yiQvB2Mb3K//ci1EbF6wEnC/YgOlQgFo2ilbi+iOGJUZrfTfrniVt0ZyCrxFqRShzka/fJnbxCzNOIKmaTGdD03QT+jGgWTfFrqpYYnlI3pkHctVTTixs9mkafkzCoDEsbaPoVkpv7eyGhkzCQK7GQe0Sx7ufif100xvPYzoZIUuWLzj8JUEoxJfj8ZCM0ZyokllGlhsxI2opoytC2VbAne8smrpHVR9S6rtftapf5TRxFO4BTOwYMrqMMdNKAJDGJ4ghd4ddB5dt6c9/lowVnsHMMfOB/f0sCRpA==</latexit>G<latexit sha1_base64=\"1CPgZhYAQv3ukE0+WFhrNlGH2Fo=\">AAAB+nicbVDLSsNAFL3xWesr1aWbwSLUTUmkqMuCC11WsA9oQplMJ+3QySTMTJQS+yluXCji1i9x5984aSto64GBwzn3cs+cIOFMacf5slZW19Y3Ngtbxe2d3b19u3TQUnEqCW2SmMeyE2BFORO0qZnmtJNIiqOA03Ywusr99j2VisXiTo8T6kd4IFjICNZG6tklLxmyihdhPSSYZ9eT055ddqrOFGiZuHNSrsMMjZ796fVjkkZUaMKxUl3XSbSfYakZ4XRS9FJFE0xGeEC7hgocUeVn0+gTdGKUPgpjaZ7QaKr+3shwpNQ4CsxknlEtern4n9dNdXjpZ0wkqaaCzA6FKUc6RnkPqM8kJZqPDcFEMpMVkSGWmGjTVtGU4C5+eZm0zqruebV2WyvXf+oowBEcQwVcuIA63EADmkDgAZ7gBV6tR+vZerPeZ6Mr1nznEP7A+vgGM06T/w==</latexit>(G)<latexit sha1_base64=\"pp9xfBmgFzws+/HXdVciswv09Es=\">AAAB6nicbVDLSsNAFL2pr1pfVZfdDJaCq5KIaJcFNy4rmrbQhjKZTtKhk0mYmQgldOfWjQtF3PotfoA7/QC/wA9w+lho64ELh3Pu5d57/IQzpW37w8qtrK6tb+Q3C1vbO7t7xf2DpopTSahLYh7Lto8V5UxQVzPNaTuRFEc+py1/eDHxW7dUKhaLGz1KqBfhULCAEayNdB32nF6xbFftKdAyceakXC9V7r7fvj4bveJ7tx+TNKJCE46V6jh2or0MS80Ip+NCN1U0wWSIQ9oxVOCIKi+bnjpGFaP0URBLU0Kjqfp7IsORUqPIN50R1gO16E3E/7xOqoOalzGRpJoKMlsUpBzpGE3+Rn0mKdF8ZAgmkplbERlgiYk26RRMCM7iy8ukeVJ1zqqnVyaNGsyQhxIcwTE4cA51uIQGuEAghHt4hCeLWw/Ws/Uya81Z85lD+APr9Qf3EJII</latexit>g1<latexit sha1_base64=\"N0oqi0eHBrxyXdjRhTSlzHwu9Wc=\">AAAB6nicbVC7SgNBFL0bXzG+opZpBkPAKuwGiSkDNpYRzQOSJcxOZjdDZmeXmVkhLOlsbSwUsfVb/AA7/QC/wA9w8ig08cCFwzn3cu89XsyZ0rb9YWXW1jc2t7LbuZ3dvf2D/OFRS0WJJLRJIh7JjocV5UzQpmaa004sKQ49Ttve6GLqt2+pVCwSN3ocUzfEgWA+I1gb6TroV/r5ol22Z0CrxFmQYr1Quvt++/ps9PPvvUFEkpAKTThWquvYsXZTLDUjnE5yvUTRGJMRDmjXUIFDqtx0duoElYwyQH4kTQmNZurviRSHSo1Dz3SGWA/VsjcV//O6ifZrbspEnGgqyHyRn3CkIzT9Gw2YpETzsSGYSGZuRWSIJSbapJMzITjLL6+SVqXsVMtnVyaNGsyRhQKcwCk4cA51uIQGNIFAAPfwCE8Wtx6sZ+tl3pqxFjPH8AfW6w/4lJIJ</latexit>g2<latexit sha1_base64=\"Nj7KURJseFlru7tU2jTMsd8dKtU=\">AAAB8HicbVBLSgNBEK2Jvxh/UZfZNIaAqzAjolkG3LiMYD6ShNDT05M06e4ZunuEMGTnDdy4UMStB/EA7vQAnsAD2PksNPFBweO9Kqrq+TFn2rjuh5NZWV1b38hu5ra2d3b38vsHDR0litA6iXikWj7WlDNJ64YZTluxolj4nDb94cXEb95SpVkkr80opl2B+5KFjGBjpZuOYTygaX/cyxfdsjsFWibenBSrhdLd99vXZ62Xf+8EEUkElYZwrHXbc2PTTbEyjHA6znUSTWNMhrhP25ZKLKjuptODx6hklQCFkbIlDZqqvydSLLQeCd92CmwGetGbiP957cSElW7KZJwYKslsUZhwZCI0+R4FTFFi+MgSTBSztyIywAoTYzPK2RC8xZeXSeOk7J2VT69sGhWYIQsFOIJj8OAcqnAJNagDAQH38AhPjnIenGfnZdaaceYzh/AHzusPLt2VGg==</latexit> ̃g<latexit sha1_base64=\"6I3RK528PgjZrBbq6Ms5bEpJ1LE=\">AAAB+HicbVDLSsNAFL2pVWt9NOrSzWgR6qYkItqNUHDjsoJ9SBPKZDpph04mYWYi1NAvceNCEbd+gd/gzr9x+lho9cCFwzn3cu89QcKZ0o7zZeVW8qtr64WN4ubW9k7J3t1rqTiVhDZJzGPZCbCinAna1Exz2kkkxVHAaTsYXU399j2VisXiVo8T6kd4IFjICNZG6tklLxmyiqcZ79NsMDnp2WWn6syA/hJ3Qcp1dCnyH3eHjZ796fVjkkZUaMKxUl3XSbSfYakZ4XRS9FJFE0xGeEC7hgocUeVns8Mn6NgofRTG0pTQaKb+nMhwpNQ4CkxnhPVQLXtT8T+vm+qw5mdMJKmmgswXhSlHOkbTFFCfSUo0HxuCiWTmVkSGWGKiTVZFE4K7/PJf0jqtuufVsxuTRg3mKMABHEEFXLiAOlxDA5pAIIVHeIYX68F6sl6tt3lrzlrM7MMvWO/fluOVQA==</latexit>( ̃g)<latexit sha1_base64=\"rnrtwe4FqV+ZdWV9b9/pIdnFn3w=\">AAAB8HicbVDLSgNBEOzVqDG+oh69jAYhXsJuEM1FCHjxGMG8SJYwO5lNhszMLjOzQljyFV48KOLVr/AbvPk3Th4HTSxoKKq66e4KYs60cd1vZ209s7G5ld3O7ezu7R/kD48aOkoUoXUS8Ui1AqwpZ5LWDTOctmJFsQg4bQaj26nffKRKs0g+mHFMfYEHkoWMYGOldjcesuKgV77o5QtuyZ0BrRJvQQpVdCMzn+3TWi//1e1HJBFUGsKx1h3PjY2fYmUY4XSS6yaaxpiM8IB2LJVYUO2ns4Mn6NwqfRRGypY0aKb+nkix0HosAtspsBnqZW8q/ud1EhNW/JTJODFUkvmiMOHIRGj6PeozRYnhY0swUczeisgQK0yMzShnQ/CWX14ljXLJuypd3ts0KjBHFk7gDIrgwTVU4Q5qUAcCAp7gBV4d5Tw7b877vHXNWcwcwx84Hz/bKpH+</latexit>(g2)<latexit sha1_base64=\"e9JH+quwPh7jXscQCTYj8Wle8MU=\">AAAB8HicbVDLSgMxFL1TX7W+qi7dBIvgxjIjRbssuHFZwdZKO5RMJtOG5jEkGaEM/Qo3LhRx6+e4829MHwttPRA4nHMuufdEKWfG+v63V1hb39jcKm6Xdnb39g/Kh0dtozJNaIsornQnwoZyJmnLMstpJ9UUi4jTh2h0M/Ufnqg2TMl7O05pKPBAsoQRbJ30GFz0uAvHuF+u+FV/BrRKggWpNGCOZr/81YsVyQSVlnBsTDfwUxvmWFtGOJ2UepmhKSYjPKBdRyUW1IT5bOEJOnNKjBKl3ZMWzdTfEzkWxoxF5JIC26FZ9qbif143s0k9zJlMM0slmX+UZBxZhabXo5hpSiwfO4KJZm5XRIZYY2JdRyVXQrB88ippX1aDq2rtrlZp1Bd1FOEETuEcAriGBtxCE1pAQMAzvMKbp70X7937mEcL3mLmGP7A+/wBbt2QLg==</latexit>1<latexit sha1_base64=\"e9JH+quwPh7jXscQCTYj8Wle8MU=\">AAAB8HicbVDLSgMxFL1TX7W+qi7dBIvgxjIjRbssuHFZwdZKO5RMJtOG5jEkGaEM/Qo3LhRx6+e4829MHwttPRA4nHMuufdEKWfG+v63V1hb39jcKm6Xdnb39g/Kh0dtozJNaIsornQnwoZyJmnLMstpJ9UUi4jTh2h0M/Ufnqg2TMl7O05pKPBAsoQRbJ30GFz0uAvHuF+u+FV/BrRKggWpNGCOZr/81YsVyQSVlnBsTDfwUxvmWFtGOJ2UepmhKSYjPKBdRyUW1IT5bOEJOnNKjBKl3ZMWzdTfEzkWxoxF5JIC26FZ9qbif143s0k9zJlMM0slmX+UZBxZhabXo5hpSiwfO4KJZm5XRIZYY2JdRyVXQrB88ippX1aDq2rtrlZp1Bd1FOEETuEcAriGBtxCE1pAQMAzvMKbp70X7937mEcL3mLmGP7A+/wBbt2QLg==</latexit>1<latexit sha1_base64=\"btjGN37odafvN88LSfYFF+PvyVQ=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqsxIqV0W3LisYB/QDiWTybShmcyQ3BHK0I9w40IRt36PO//G9LHQ1gOBwznnkntPkEph0HW/ncLW9s7uXnG/dHB4dHxSPj3rmCTTjLdZIhPdC6jhUijeRoGS91LNaRxI3g0md3O/+8S1EYl6xGnK/ZiOlIgEo2il7kDaaEiH5YpbdRcgm8RbkUoTlmgNy1+DMGFZzBUySY3pe26Kfk41Cib5rDTIDE8pm9AR71uqaMyNny/WnZErq4QkSrR9CslC/T2R09iYaRzYZExxbNa9ufif188wavi5UGmGXLHlR1EmCSZkfjsJheYM5dQSyrSwuxI2ppoytA2VbAne+smbpHNT9erV2kOt0mys6ijCBVzCNXhwC024hxa0gcEEnuEV3pzUeXHenY9ltOCsZs7hD5zPH5Oxj7w=</latexit><latexit sha1_base64=\"btjGN37odafvN88LSfYFF+PvyVQ=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqsxIqV0W3LisYB/QDiWTybShmcyQ3BHK0I9w40IRt36PO//G9LHQ1gOBwznnkntPkEph0HW/ncLW9s7uXnG/dHB4dHxSPj3rmCTTjLdZIhPdC6jhUijeRoGS91LNaRxI3g0md3O/+8S1EYl6xGnK/ZiOlIgEo2il7kDaaEiH5YpbdRcgm8RbkUoTlmgNy1+DMGFZzBUySY3pe26Kfk41Cib5rDTIDE8pm9AR71uqaMyNny/WnZErq4QkSrR9CslC/T2R09iYaRzYZExxbNa9ufif188wavi5UGmGXLHlR1EmCSZkfjsJheYM5dQSyrSwuxI2ppoytA2VbAne+smbpHNT9erV2kOt0mys6ijCBVzCNXhwC024hxa0gcEEnuEV3pzUeXHenY9ltOCsZs7hD5zPH5Oxj7w=</latexit><latexit sha1_base64=\"G0XCydthVvZSM9aDf4rVDBqn8AQ=\">AAAB63icbVC7SgNBFL0bXzG+opZpBkPAKuyKaMqAjWUE84AkyOxkNjtkZnaZmRXCks7axkIRW3/FD7DTD/AL/ABnkxSaeODC4Zx7ufceP+ZMG9f9cHIrq2vrG/nNwtb2zu5ecf+gpaNEEdokEY9Ux8eaciZp0zDDaSdWFAuf07Y/usj89i1VmkXy2oxj2hd4KFnACDaZ1ItDdlMsu1V3CrRMvDkp10uVu++3r8/GTfG9N4hIIqg0hGOtu54bm36KlWGE00mhl2gaYzLCQ9q1VGJBdT+d3jpBFasMUBApW9Kgqfp7IsVC67HwbafAJtSLXib+53UTE9T6KZNxYqgks0VBwpGJUPY4GjBFieFjSzBRzN6KSIgVJsbGU7AheIsvL5PWSdU7q55e2TRqMEMeSnAEx+DBOdThEhrQBAIh3MMjPDnCeXCenZdZa86ZzxzCHzivPxjdkrg=</latexit><latexit sha1_base64=\"G0XCydthVvZSM9aDf4rVDBqn8AQ=\">AAAB63icbVC7SgNBFL0bXzG+opZpBkPAKuyKaMqAjWUE84AkyOxkNjtkZnaZmRXCks7axkIRW3/FD7DTD/AL/ABnkxSaeODC4Zx7ufceP+ZMG9f9cHIrq2vrG/nNwtb2zu5ecf+gpaNEEdokEY9Ux8eaciZp0zDDaSdWFAuf07Y/usj89i1VmkXy2oxj2hd4KFnACDaZ1ItDdlMsu1V3CrRMvDkp10uVu++3r8/GTfG9N4hIIqg0hGOtu54bm36KlWGE00mhl2gaYzLCQ9q1VGJBdT+d3jpBFasMUBApW9Kgqfp7IsVC67HwbafAJtSLXib+53UTE9T6KZNxYqgks0VBwpGJUPY4GjBFieFjSzBRzN6KSIgVJsbGU7AheIsvL5PWSdU7q55e2TRqMEMeSnAEx+DBOdThEhrQBAIh3MMjPDnCeXCenZdZa86ZzxzCHzivPxjdkrg=</latexit><latexit sha1_base64=\"G0XCydthVvZSM9aDf4rVDBqn8AQ=\">AAAB63icbVC7SgNBFL0bXzG+opZpBkPAKuyKaMqAjWUE84AkyOxkNjtkZnaZmRXCks7axkIRW3/FD7DTD/AL/ABnkxSaeODC4Zx7ufceP+ZMG9f9cHIrq2vrG/nNwtb2zu5ecf+gpaNEEdokEY9Ux8eaciZp0zDDaSdWFAuf07Y/usj89i1VmkXy2oxj2hd4KFnACDaZ1ItDdlMsu1V3CrRMvDkp10uVu++3r8/GTfG9N4hIIqg0hGOtu54bm36KlWGE00mhl2gaYzLCQ9q1VGJBdT+d3jpBFasMUBApW9Kgqfp7IsVC67HwbafAJtSLXib+53UTE9T6KZNxYqgks0VBwpGJUPY4GjBFieFjSzBRzN6KSIgVJsbGU7AheIsvL5PWSdU7q55e2TRqMEMeSnAEx+DBOdThEhrQBAIh3MMjPDnCeXCenZdZa86ZzxzCHzivPxjdkrg=</latexit>view1view2<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>C<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>C<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>C<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>C<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>C<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>C<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>C<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>C<latexit sha1_base64=\"01L3kmZfyZLNjUnTcLUS8oU0MCM=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DXnJMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ/dzvPKHSPJYPZpqgH9GR5CFn1FipWR8US27ZXYBsEm9FSjVYojEofvWHMUsjlIYJqnXPcxPjZ1QZzgTOCv1UY0LZhI6wZ6mkEWo/Wxw6I1dWGZIwVrakIQv190RGI62nUWA7I2rGet2bi/95vdSEd37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdkUbAje+subpH1T9qrlSrNSqlVWceThAi7hGjy4hRrUoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB/MxjQc=</latexit>H<latexit sha1_base64=\"01L3kmZfyZLNjUnTcLUS8oU0MCM=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DXnJMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ/dzvPKHSPJYPZpqgH9GR5CFn1FipWR8US27ZXYBsEm9FSjVYojEofvWHMUsjlIYJqnXPcxPjZ1QZzgTOCv1UY0LZhI6wZ6mkEWo/Wxw6I1dWGZIwVrakIQv190RGI62nUWA7I2rGet2bi/95vdSEd37GZZIalGy5KEwFMTGZf02GXCEzYmoJZYrbWwkbU0WZsdkUbAje+subpH1T9qrlSrNSqlVWceThAi7hGjy4hRrUoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB/MxjQc=</latexit>H<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>C<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>C<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>C<latexit sha1_base64=\"mqqgzKID3Hsh2yDAVtA3/PJAgXc=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKiB4DuXhMwDwgWcLspDcZMzu7zMwKYckXePGgiFc/yZt/4+Rx0MSChqKqm+6uIBFcG9f9dnJb2zu7e/n9wsHh0fFJ8fSsreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA+KJbfsLkA2ibcipRos0RgUv/rDmKURSsME1brnuYnxM6oMZwJnhX6qMaFsQkfYs1TSCLWfLQ6dkSurDEkYK1vSkIX6eyKjkdbTKLCdETVjve7Nxf+8XmrCOz/jMkkNSrZcFKaCmJjMvyZDrpAZMbWEMsXtrYSNqaLM2GwKNgRv/eVN0r4pe9VypVkp1SqrOPJwAZdwDR7cQg3uoQEtYIDwDK/w5jw6L86787FszTmrmXP4A+fzB+udjQI=</latexit>CUnder review as a conference paper at ICLR 2023\n\nN\n\n,\n\n=\n\nby (\n\nInstance Discrimination. Let graph instance g be represented as an adjacency matrix A node u to node v holds, otherwise Auv = 0. Moreover, each node v features xv ∈\n\nn=1 be the set of unlabeled graph instances. We denote a . This graph structure can from could have d1-dimensional\n\n|V|×|V|, where Auv = 1 if the edge (u, v) }\n\nmight have d2-dimensional features euv ∈\n\nRd1 , while each edge (u, v)\n\n) involving the node set\n\nand the edge set\n\n∈ E Rd2 .\n\ngn}\n\n∈ V\n\n∈ G\n\n∈ E\n\n∈ {\n\n0, 1\n\nV\n\nV\n\nG\n\nE\n\nE\n\n{\n\nG\n\nwithout annotations, contrastive self-supervised learning (SSL) aims to pre-train On the graph data Rd that projects the graph instances to a d-dimensional space, so as a graph encoder φ : to enhance the encoder’s representation ability and facilitate its fine-tuning in downstream tasks. Towards this end, a prevailing task of pre-training is instance discrimination (Dosovitskiy et al., 2014; Purushwalkam & Gupta, 2020; Li et al., 2021) — treating each graph instance as one single class, and distinguishing it from the other graph instances.\n\nG →\n\nInvariance. A leading solution to instance discrimination is to maximize the representation agreement between augmented views of the same graph, while minimizing the representation agreement between views of two different graphs. It essentially encourages each instance’s representation to be invariant to the augmentations (Dangovski et al., 2022; Grill et al., 2020; Zbontar et al., 2021). Mathematically, invariance can be described by groups (Dangovski et al., 2022; Kondor & Trivedi, 2018; Maron et al., be a group of augmentations (aka. transformations). Invariance makes the encoder φ 2019a). Let P\ninsensitive to the actions T :\n\non the graphs\n\nof the group\n\n, formally:\n\nP × G → G\n\nP\n\nG\n\nφ(g) = φ(Tp(g)),\n\n(1) where Tp(g) := T (p, g) is an action of applying the augmentation p on the instance g. Dictating invariance to the encoder will output the same representations for the original and augmented graphs. Probing into Equation (1), we find two key ingredients: intra-graph augmentation and contrastive learning, and will present their common practices in prior studies.\n\n∈ P\n\n∈ G\n\n∀\n\n∀\n\np\n\ng\n\n,\n\n,\n\nP\n\nIntra-graph Augmentation. Typically, the augmentation group is pre-determined to imply prior knowledge of graph data. Early studies (Hu et al., 2020; Qiu et al., 2020; You et al., 2020; Zhu et al., 2020) instantiate augmentations as randomly corrupting the topological structure, node features, or edge features of individual graph instances. For example, AttrMasking (Hu et al., 2020) masks node and edge attributes, and applies an objective to reconstruct them. GCC (Qiu et al., 2020) explores random walks over the anchor graph to create different subgraph views. GraphCL (You et al., 2020) systematically investigates the combined effect of various random augmentations. Despite the success, random corruptions are too aggressive to maintain the semantic consistency (Guo & Mao, 2021) between the anchor graph and its augmented views. The invariance principle blindly ignores the semantic shift, thus easily pushing dissimilar patterns together and making a pernicious impact on the representation learning. Some follow-on studies (Zhu et al., 2021; Subramonian, 2021; Suresh et al., 2021; Xu et al., 2021) learn augmentations instead to underscore salient substructures, so as to mitigate the semantic shift. For instance, GCA (Zhu et al., 2021) applies node centralities to discover important substructures in social networks. MICRO-Graph (Subramonian, 2021) learns chemically meaningful motifs to help the informative subgraph sampling. More recently, AD-GCL (Suresh et al., 2021) adopts the idea of information bottleneck to adversarially learn the salient subgraphs.\n\nContrastive Learning. Upon the augmented views, the contrastive learning objective is to classify whether they come from identical instances. Specifically, it pulls the augmented views derived from the same instance (i.e., positive samples) together and pushes the views of different instances (i.e., negative samples) apart (Chen et al., 2020; He et al., 2020). The common practices of this objective are InfoNCE (van den Oord et al., 2018), NCE (Misra & van der Maaten, 2020), and NT-Xent (Chen et al., 2020). Here we consider the NT-Xent adopted by GraphCL. Given a minibatch of graph instances\n\nN\n\ngi}\n\ni=1, it first generates two different augmented views, denoted as {\ng2 g2 and i = Tp2 (g), p2 ∼ P} i | {\ni = ρ(φ(g1 z1 z1 as i=1 and i )) i | Formally, the loss of NT-Xent is:\n\ni=1, and then feeds them into the encoder to yield the representations ) is an MLP projection head. i=1, where ρ( ·\n\ng1 i = Tp1 (g), p1 ∼ P}\n\ni = ρ(φ(g2 z2\n\ng1 i |\n\nz2 i |\n\nN i=1\n\ni ))\n\n}\n\n{\n\n{\n\n}\n\n{\n\nN\n\nN\n\nN\n\nl(\n\nz1\n\ni }\n\n{\n\nN\n\ni=1,\n\nz2\n\ni }\n\n{\n\nN\n\ni=1) =\n\nN (cid:88)\n\nlog\n\n1 N\n\n−\n\ni , z2\n\nexp(s(z1 j=1,j̸=i exp(s(z1\n\ni )/τ ) i , z2\n\n(cid:80)N\n\nj )/τ )\n\ni=1 ) is the function of cosine similarity, and τ is a temperature hyperparameter. where s( ·\n\n,\n\n(2)\n\nIn a nutshell, the interplay between intra-graph augmentation and contrastive learning is tailor-made for invariance to make the encoder insensitive to differences between the anchor and augmented\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The framework of E-GCL. The GNN encoder learns invariance for intra-graph augmentation of dropNode and equivariance for cross-graph augmentation of graph interpolation.\n\nviews. In this work, we explore equivariance on cross-graph augmentations to make the encoder sensitive to the changes in self-discriminative information.\n\n3 METHODOLOGY: EQUIVARIANT GRAPH CONTRASTIVE LEARNING\n\nHere we present the E-GCL framework, which imposes two principles — invariance to intra-graph augmentations (Figure 2 left) and equivariance to cross-graph augmentations (Figure 2 right) — on the representation learning, aiming to mitigate the potential limitations of I-GCL. Next, we start with the concepts of equivariance and cross-graph augmentations.\n\n3.1 EQUIVARIANCE\n\n-equivariant w.r.t. the actions T :\n\nand the representation space Rd, if\n\nInspired by the recent E-SSL studies (Dangovski et al., 2022; Chuang et al., 2022), we aim to patch invariance’s potential limitations with equivariance. Mathematically, with a group of augmentations ,\nH Rd ) is said to be the encoder φ( ·\nH applied on the graph space of the group H\nφ(Th(g)) = T ′\n\nG h(φ(g)), where Th(g) is the action of applying the transformation h on the graph instance g, while T ′ h(φ(g)) -equivariant requires that, given a transformation is the action of h on the representation φ(g). , h’s influence on the graph should be faithfully reflected by the change of the graph’s h\nrepresentation. Taking Figure 1b as an example, given the graph’s global semantics are perturbed by graph interpolation, the representation yielded by the equivariant encoder should transform in a definite way. Jointly analyzing Equations (1) and (3), it can be shown that invariance is a special case of equivariance when setting T ′ h as the identity mapping. However, generalizing GCL to equivariance remains unexplored, thus a focus of our work.\n\nH × G → G\n\nand T ′ :\n\n∈ H\n\n∈ H\n\nH ×\n\n∈ G\n\nRd\n\n(3)\n\n→\n\nH\n\nh\n\n∀\n\n∀\n\ng\n\n,\n\n,\n\nP\n\nand equivariance to other transformations\n\nH (Dangovski et al., 2022) sets grayscale of images as language processing, DiffCSE (Chuang et al., 2022) treats the model dropout as word replacement as\n\nFurthermore, as suggested in the recent E-SSL studies (Dangovski et al., 2022; Chuang et al., 2022), is jointly imposing invariance to some transformations H\nand promising to result in better representations than relying solely on one of them. Here we term as insensitive and sensitive transformations, respectively. For example, in computer vision, E-SSL ; in natural , while using the and . Nonetheless, these studies either conduct extensive testings on the impact of different partitions H\n(Dangovski et al., 2022) which is time-consuming, or exploit domain knowledge to heuristically partition (Chuang et al., 2022) which might generalize poorly to other domains. Hence, it is infeasible to apply these strategies on graph augmentations. Worse still, different graph augmentations stem mostly from the perturbation of graph structures, thus highly likely to corrupt the same attributes of graphs. Taking the graph g in Figure 1a as an example, masking the nitrogen N atom or dropping the N bond will both corrupt the cyano group and break the corresponding molecular properties. In C\na nutshell, owing to (1) the common paradigm of structure corruption and (2) the risk of categorizing\n\n. Clearly, it is of crucial importance to partition augmentations into\n\n, while treating rotations as\n\nH\n\nH\n\n≡\n\nP\n\nP\n\nP\n\nP\n\n4\n\nProjectorProjectorEquivarianceFeature interpolationRepresentation interpolationProjectorProjectorInvarianceIntra-graph augmentationCross-graph augmentationGNN encoderGNN encoderGNN encoderGNN encoderUnder review as a conference paper at ICLR 2023\n\nthem all as insensitive augmentations, we conservatively argue that it is hard to partition graph augmentations into sensitive\n\nand insensitive parts\n\n.\n\nH\n\nP\n\nIn this work, leaving partitioning untouched, we remain intra-graph augmentations as the insensitive .\ntransformations\n\nand propose new augmentations across graphs as the sensitive transformations\n\nH\n\nP\n\n3.2 CROSS-GRAPH AUGMENTATION\n\nWe first introduce graph interpolation (Guo & Mao, 2021) to create cross-graph augmentations as .\nH Different from previous work, we propose an extension of graph interpolation for SSL (Section 3.3). We also connect it to group theory and address its limitation of sensitivity to the relative permutation.\n\nand , we employ mixup (Zhang et al., 2018), a simple yet effective linear interpolation approach,\n\nInterpolating Graphs as Cross-graph Augmentations. Given two graph instances g g′ on the input features and class labels, respectively:\n\n∈ G\n\n∈ G\n\nλ)g′,\n\n−\n\n ̃g = λg + (1\n\n ̃y = λy + (1\n\n(4) where y and y′ separately denote the one-hot encodings to indicate the instance identities of g and g′ in the instance discrimination task; λ [0, 1] is the interpolation ratio sampled from a Beta distribution, in which α is a hyperparameter. This mixup strategy is initially proposed for supervised learning, aiming to put the interpolated samples in-between different classes and make the decision boundary robust to slightly corrupted samples (Verma et al., 2019; Zhang et al., 2018; 2021). Despite the success of mixing image and text, it is challenging to interpolate graphs due to the structural differences between graph instances (e.g., varying topologies and sizes).\n\nBeta(α, α)\n\n∼\n\n−\n\n∈\n\nλ)y′,\n\nTo this end, we draw inspiration from the recent work (Guo & Mao, 2021) to perform linear ′), we mitigate their interpolation between graphs. Specifically, with g = ( structural differences by padding virtual nodes and edges, which are associated with zero features 0. Assuming nodes, where the original node set dummy virtual nodes, and the original nodes connect the virtual nodes with dummy virtual edges. Having padded two graphs to the same size, now we can directly add them up. Before the interpolation, we first merge two node and edge sets as the new ones: ̃ λ)g′ in Equation (4) is achieved by exerting V\nlinear interpolation on the adjacency matrices, node features, and edge features:\n\n, g can be updated as a new graph with\n\nremains unchanged but adds\n\n′. Then, ̃g = λg + (1\n\n) and g′ = (\n\n|V| ≤ |V\n\n′, ̃ E\n\n| − |V|\n\nV ∪ V\n\nE ∪ E\n\n|V\n\n|V\n\n−\n\n=\n\n=\n\n′,\n\nV\n\nV\n\nV\n\nE\n\nE\n\n|\n\n|\n\n,\n\n′\n\n′\n\n′\n\nλ)A′,\n\n ̃A = λA + (1\n\n(5) −\nwhere A and A′ are the adjacency matrices of g and g′ after padding; xv and x′ v are the features of node v uv separately denote the ∈\nfrom g and g′. Consequently, we generate a cross-graph augmentation. features of edge (u, v)\n\n, which separately come from g and g′; similarly, euv and e′\n\n ̃euv = λeuv + (1\n\n ̃xv = λxv + (1\n\nuv,\n\n ̃ V\n\nv,\n\n−\n\n−\n\nλ)x′\n\nλ)e′\n\n ̃ E\n\n∈\n\nConnecting Cross-graph Augmentations to Groups. In the language of groups, we can describe the cross-graph augmentation in Equation (4) as a group of transformations. Given the input (λ, g, g′), we can systemize the graph interpolation as two steps: (1) feature rescaling: ˆg = λg, which rescales the node and edge features of g with the ratio λ; (2) instance composition: ̃g = C(ˆg, ˆg′) = ˆg + ˆg′, which λ)g′. To construct a closed space for graph interpolation, adds the other rescaled graph ˆg′ = (1 we first define ˆ to enable direct 1, 1], g =\nG sampling of rescaled graphs. We allow λ < 0 to include the inverse elements of graphs. Then, we > by combining the graphs in ˆ =< ˆ generate , C) G\nG forms a group in Appendix A.1. It is worth noting that each instance can be viewed as a transformation to others, i.e., C(g,\n\n), such that semantic shifts can be described via algebraic operators.\n\nvia instance composition. We show that (\n\nby performing feature rescaling on\n\n− ∈ G}\n\nλ ˆg |\n\n[ −\n\n∈\n\nG\n\nI\n\nI\n\n{\n\n) := Cg( ·\n·\n\nGroup Averaging for Insensitivity to Relative Permutation. Note that Equation (5) can output different interpolations, when the node orders of one graph or padding positions of dummy nodes change. We ascribe operations on “node orders” and “padding positions” to the “relative permutation” between two input graphs. Unlike images and texts, the canonical permutations (orderings) of nodes in graphs are unlabeled. The default node permutations encode nothing useful about graph semantics (Xu et al., 2019; Hamilton et al., 2017). Thus, we enforce insensitivity to relative permutations by randomly permuting nodes in the bigger graph before graph interpolation. Next, we justify and develop this design with group averaging.\n\nGroup averaging can make known architectures invariant to new symmetries (Puny et al., 2022; Sn be a random Yarotsky, 2022). It can be used for strict invariance to relative permutations. Let P\n\n∼\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\npermutation, and TP ◦ interpolation as λTP ◦\n\ng be permuting graph g by P . Assuming g + (1\n\nλ)g′ and its representation as φ(λTP ◦\n\ng |\n\n−\n\nstrict invariance to relative permutations, we apply group averaging on the permutation operators:\n\ng′ |\ng + (1\n\n| ≥ |\n\n, we obtain the graph λ)g′). To achieve\n\n−\n\nΦ(λ, g, g′) =\n\n1\n\n(cid:88)\n\nSn|\n\n|\n\nP ∈Sn\n\nφ(λTP −1\n\ng + (1\n\n◦\n\n−\n\nλ)g′),\n\n(6)\n\ng, g′) = Φ(λ, g, TP ′\n\nwhere Φ is the function of group averaging. Φ is invariant to relative permutations between g and g′, in the sense that Φ(λ, g, g′) = Φ(λ, TP ◦ Sn. Intuitively, it is achieved by averaging over all relative permutations. See Appendix A.2 for the proof. However, the intractability of averaging over Sn naturally arises as a problem. Following Murphy et al. (2019), λ)g′) is an unbiased estimator of Φ. Further, this our random permutation strategy φ(λTP ◦ strategy optimizes ρ φ toward an optima insensitive to the relative permutation. By Proposition 1, we conclude that using random permutation is a tractable surrogate for optimizing an invariant network to relative permutation. Appendix D.2 shows the influence of sampled permutation numbers. For simplicity, we still use λg + (1\n\nλ)g′ to represent graph interpolation in the rest text.\n\ng′) for all P, P ′\n\ng + (1\n\n∼\n\n−\n\n◦\n\n◦\n\nProposition 1. The contrastive loss with φ(λTP ◦ network to relative permutation 1\n\n(cid:80)\n\ng +(1\n\n|Sn|\n\nP ∈Sn ρ(φ(λiTP ◦\n\nλ)g′) upper bounds the loss of an invariant gi + (1\n\nλi)g′\n\ni)).\n\n−\n\n−\n\n−\n\n3.3 EQUIVARIANCE TO CROSS-GRAPH AUGMENTATIONS\n\nRevisiting Equation (4), we can find the two terms of interpolation strategy align with the equivariance mechanism in Equation (3). The semantic change caused by the feature interpolation is equivalently reflected by the label interpolation. Hence, we can instantiate the equivariance mechanism based on the feature and label interpolations. In the SSL setting, we interpolate graph representations as the alternative for label interpolation. Graph representations are derived from the encoder φ( ) with a ·\nglobal readout layer to summarize the graphs’ global semantics. Hence, as shown by the right side of Figure 2, we can parameterize equivariance approximately as:\n\nφ(λg + (1\n\nλ)g′)\n\nλφ(g) + (1\n\nλ)φ(g′).\n\n(7)\n\n−\n\n≈\n\n−\n\nMinimizing the distance between Equation (7)’s two sides allows the encoder to improve sensitivity to the global semantic shifts caused by cross-graph augmentations.\n\nAlthough the strict equivariance is hardly guaranteed, experiments show that approaching Equation (7) can boost the performance on downstream tasks (cf. Section 4.2). Furthermore, if the encoder is powerful enough to distinguish the interpolated graphs, it has a deterministic reflection in the repre- (Non-trivial Equivariance sentation space φ( (Dangovski et al., 2022)). See Appendix A.1 for the proof.\n\n) for a fixed transformation C(g,\n\n) in graph space ·\n\nG\n\nG\n\nProposition 2. Assuming the encoder can detect the isomorphism of interpolated graphs, there exists a GNN encoder φ that is non-trivially equivariant to the graph interpolation transformation.\n\n3.4\n\nIMPLEMENTING E-GCL\n\nThis section details our implementation of E-GCL (Figure 2). Specifically, given a minibatch of graph instances i=1, we impose (1) invariance to intra-graph augmentation and (2) equivariance to cross-graph augmentation simultaneously on the shared encoder.\n\ngi}\n\n{\n\nN\n\nInvariance. For the invariance principle, we follow the I-GCL paradigm to resort to the standard (e.g., randomly dropping nodes in GraphCL), and create two augmented intra-graph augmentations P\ng2 g1 i=1. views of individual graphs: i | i | {\ni=1 and ) brings forth two representation lists: Consequently, the encoder φ( {\n·\n\ng2 i = Tp2(g), p2 ∼ P} z1 i |\n\ng1 i = Tp1(g), p1 ∼ P}\n\ni = ρ(φ(g1 z1\n\ni=1 and\n\ni ))\n\n}\n\n{\n\nN\n\nN\n\nN\n\ni = ρ(φ(g2 z2\n\ni ))\n\nN\n\ni=1, in which ρ(\n\nz2 i |\n\n) is an MLP projector. ·\n\nN\n\n}\n\ng2\n\n{ Equivariance. For the equivariance principle, we first randomly shuffle the graphs in termed as\n\ni=1, [N ] is the function of random shuffling. Following the left [N ], we create the feature λ)λg2 g1 interpolations and then generate their representations as i=1. Meanwhile, according to the right side of Equation (7), we arrive at the representation interpolations as\n\nSn for all i side of Equation (7) and applying random permutations Pi ∼ z3 z3 i = ρ(φ(λTPi ◦ i |\n\ni=1, where π : [N ]\n\nπ(i)))\n\ng2 i }\n\nλ)φ(λg2\n\ni +(1\n\nπ(i)}\n\n→\n\n−\n\n∈\n\n{\n\n{\n\n}\n\n{\n\nN\n\nN\n\nN\n\ni = ρ(λφ(g1 z4\n\ni ) + (1\n\ni=1.\n\nπ(i)))\n\n−\n\n}\n\nz4 i |\n\n{\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\ndenotes our reproduced results using the released codes. Table 1: Main experiment performances. Other baseline results are borrowed from the original papers. Bold indicates the best performance and underline indicates the second best performance.\n\n∗\n\n(a) Unsupervised learning accuracies (%) on the TU datasets.\n\nDataset\n\nNCI1\n\nPROTEINS\n\nDD\n\nMUTAG COLLAB\n\nRDT-B\n\nRDT-M5K IMDB-B AVG GAIN\n\nNo Pre-train* InfoGraph* GraphCL* JOAO* AD-GCL* GraphMAE* R-GCL* E-GCL\n\n72.67±1.16 78.27±0.64 79.25±0.40 78.55±0.17 72.95±0.45 75.00±0.95 78.79±0.42 79.95±0.25\n\n73.81±0.30 74.56±0.57 74.50±0.85 74.39±0.80 73.62±0.63 73.92±0.97 74.60±0.71 75.18±0.40\n\n77.01±0.90 77.01±0.90 78.24±0.99 77.44±0.85 76.06±0.44 76.15±0.99 79.14±0.39 77.83±0.52\n\n84.26±1.16 87.87±1.85 87.65±1.66 88.85±1.51 89.25±1.29 87.17±1.02 88.12±1.38 88.20±0.85\n\n62.92±0.02 70.73±0.48 71.59±0.52 70.52±0.63 70.70±0.46 72.92±3.88 71.44±0.60 74.63±0.28\n\n72.45±0.47 83.22±2.78 90.54±0.30 88.49±0.76 87.03±1.18 81.27±2.51 90.11±0.41 90.71±0.57\n\n45.32±0.30 55.82±0.29 55.67±0.45 56.04±0.24 54.81±0.42 49.63±1.67 55.96±0.42 56.90±0.29\n\n67.54±0.27 70.96±0.60 71.38±0.40 71.44±0.52 71.62±0.66 71.96±0.65 71.84±0.76 72.02±0.90\n\n69.50 74.81 76.10 75.72 74.51 73.50 76.25 76.93\n\n- 5.31 6.60 6.22 5.01 4.00 6.75 7.43\n\n(b) Transfer learning ROC-AUC (%) scores on the MoleculeNet. GTS denotes GraphTrans.\n\nDataset\n\nBBBP\n\nTox21\n\nToxCast\n\nSIDER ClinTox MUV\n\nHIV\n\nBACE AVG GAIN\n\nNo Pre-train* Infomax* ContextPred* GraphCL JOAO ADGCL* GraphLOG* GraphMAE* RGCL* E-GCL E-GCL, GTS\n\n67.8±1.6 68.5±1.1 72.2±1.1 69.7±0.7 71.4±0.9 70.5±1.8 71.0±1.1 72.2±0.9 71.2±0.9 72.3±0.6 72.3±0.8\n\n73.9±0.9 75.4±0.3 75.6±0.6 73.9±0.7 74.3±0.6 74.5±0.7 74.9±0.4 75.1±0.4 75.3±0.5 74.9±0.7 77.9±0.6\n\n62.4±0.4 62.6±0.3 63.5±0.3 62.4±0.6 63.2±0.5 63.0±0.5 62.8±0.5 63.0±0.3 63.1±0.3 64.0±0.3 66.0±0.6\n\n58.3±1.5 58.6±0.7 60.6±0.9 60.5±0.9 60.5±0.7 59.1±0.9 59.7±0.9 58.5±0.7 61.2±0.6 62.8±0.5 62.4±1.0\n\n62.6±4.4 71.2±2.5 70.2±2.6 76.0±2.6 81.0±1.6 78.5±3.7 76.9±1.9 80.5±2.0 85.0±0.8 83.1±2.5 80.7±3.0\n\n73.4±2.7 73.1±1.9 74.3±1.4 69.8±2.7 73.7±1.0 71.5±2.2 70.8±2.0 75.7±1.2 73.1±1.2 78.8±0.8 79.4±2.1\n\n76.5±1.7 76.8±1.0 77.6±0.5 78.5±1.2 77.5±1.2 75.9±1.4 75.8±1.4 76.4±0.8 77.3±0.8 76.3±0.6 77.8±1.1\n\n76.8±2.8 74.4±1.1 79.0±0.9 75.4±1.4 75.5±1.3 74.0±2.2 82.9±0.9 81.3±1.0 75.7±1.3 78.1±1.1 79.7±2.4\n\n69.0 70.1 71.6 70.8 72.1 70.9 71.8 72.8 72.7 73.8 74.5\n\n- 1.1 2.6 1.8 3.1 1.9 2.8 3.8 3.7 4.8 5.5\n\nCooperative Game between Invariance and Equivariance. Based on the representations, we [0, 1]: optimize the invariance and equivariance losses together with a weighting hyperparameter ω\n\nE-GCL = (1\n\nL\n\nω)\n\nl( (cid:124)\n\n·\n\n−\n\n{\n\nz1\n\nN\n\nz2\n\ni }\n\ni=1, (cid:123)(cid:122) invariance loss\n\n{\n\ni }\n\nN\n\ni=1) (cid:125)\n\n+ω\n\nl( (cid:124)\n\n{\n\n·\n\nz3\n\nN\n\nz4\n\ni }\n\ni=1, (cid:123)(cid:122) equivariance loss\n\ni }\n\n{\n\nN\n\n, i=1) (cid:125)\n\n∈\n\n(8)\n\n, ·\n\nwhere l( ) is the loss encouraging the insensitivity between augmented views of the same graph, ·\nwhich is determined by the SSL backbone, such as NT-Xent (cf. Equation (2)) adopted by the GraphCL. Beyond contrastive learning, E-GCL is also applicable to various other SSL backbones, including BarlowTwins and SimSiam. In a nutshell, the invariance loss underscores the insensitivity to intra-graph augmentations, while the equivariance loss induces the sensitivity to cross-graph augmentations. The cooperative game between these two losses helps resolve the potential limitations of the conventional I-GCL paradigm, thus improving the expressive power of the encoder.\n\n4 EXPERIMENT\n\nIn this section, we conduct experiments to answer the following research questions: RQ1: How effective is the proposed E-GCL in graph representation learning, and how does it generalize to existing SSL frameworks? RQ2: What are the properties of E-GCL and the effects of its components?\n\nIn Appendix D.1, we present more ablation studies about 1) using interpolating representations at different positions, and 3) interpolating large and small graphs.\n\n-mixup (Han et al., 2022), 2)\n\nG\n\n4.1 EXPERIMENTAL SETUP\n\nHere we briefly introduce the baselines, datasets and evaluations. Details are in Appendix E. For a fair comparison, E-GCL uses the same intra-graph augmentation as GraphCL. If not noted, E-GCL employs a BarlowTwins backbone. We study E-GCL’s generalization to other SSL frameworks later.\n\nBaselines. We compare E-GCL with the following state-of-the-art graph pre-training methods: Infomax (Veliˇckovi ́c et al., 2019), InfoGraph (Sun et al., 2020), ContextPred (Hu et al., 2020), GraphCL (You et al., 2020), JOAO (You et al., 2021), AD-GCL (Suresh et al., 2021), GraphLOG (Xu et al., 2021), GraphMAE (Hou et al., 2022), and RGCL (Li et al., 2022).\n\nUnsupervised Learning evaluates the pre-trained GNNs for prediction on the same dataset. Following (You et al., 2020), we evaluate E-GCL on the eight TU datasets, including biochemical graphs\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Generalization to diverse SSL frameworks. Red denotes equivariance improves performance.\n\n(a) Unsupervised learning accuracies (%).\n\nDataset\n\nNCI1\n\nPROTEINS\n\nDD\n\nMUTAG COLLAB\n\nRDT-B\n\nRDT-M5K IMDB-B AVG GAIN\n\nGraphCL +Equivariance\n\nBarlowTwins +Equivariance\n\n79.25±0.40 80.22±0.38\n\n79.60±0.42 79.95±0.25\n\n74.50±0.85 74.57±0.46\n\n74.90±0.47 75.18±0.40\n\n78.24±0.99 79.15±0.98\n\n77.22±0.91 77.83±0.52\n\n87.65±1.66 89.79±1.07\n\n86.92±1.87 88.20±0.85\n\n71.59±0.52 72.75±0.66\n\n72.94±0.62 74.63±0.28\n\n90.54±0.30 91.28±0.32\n\n90.11±0.85 90.71±0.57\n\n55.67±0.45 55.80±0.24\n\n55.40±0.47 56.90±0.29\n\n71.38±0.40 71.70±0.49\n\n71.28±0.58 72.02±0.90\n\n76.10 76.91\n\n76.05 76.93\n\n- 0.81\n\n- 0.88\n\n(b) Transfer learning ROC-AUC (%) scores. GTS denotes GraphTrans.\n\nDataset\n\nGraphCL\n\n+Equivariance\n\nSimSiam\n\n+Equivariance\n\nBarlowTwins\n\n+Equivariance\n\nBarlowTwins, GTS +Equivariance\n\nBBBP\n\nTox21\n\nToxCast\n\nSIDER ClinTox MUV\n\nHIV\n\nBACE AVG GAIN\n\n69.7±0.7 71.8±0.5\n\n70.5±1.3 71.4±0.8\n\n70.6±1.6 72.3±0.6\n\n71.0±1.1 72.3±0.8\n\n73.9±0.7 75.5±0.4\n\n74.4±0.5 75.0±0.6\n\n74.3±0.4 74.9±0.7\n\n77.0±0.9 77.9±0.6\n\n62.4±0.6 63.5±0.4\n\n63.3±0.3 63.2±0.6\n\n63.8±0.3 64.0±0.3\n\n65.0±0.7 66.0±0.6\n\n60.5±0.9 60.6±0.4\n\n60.8±0.6 59.3±0.9\n\n61.3±0.6 62.8±0.5\n\n61.9±1.7 62.4±1.0\n\n76.0±2.6 74.0±2.1\n\n77.4±1.6 81.4±2.4\n\n82.0±1.6 83.1±2.5\n\n76.9±5.2 80.7±3.0\n\n69.8±2.7 74.7±0.7\n\n70.5±1.0 73.5±1.4\n\n73.0±0.7 78.8±0.8\n\n79.4±1.8 79.4±2.1\n\n78.5±1.2 76.5±0.7\n\n77.3±0.7 77.6±1.0\n\n77.1±1.2 76.3±0.6\n\n78.1±1.5 77.8±1.1\n\n75.4±1.4 78.7±0.8\n\n76.2±1.6 75.8±1.2\n\n73.9±0.2 78.1±1.1\n\n77.5±2.1 79.7±2.4\n\n70.8 71.9\n\n71.3 72.2\n\n72.0 73.8\n\n73.4 74.5\n\n- 1.1\n\n- 0.9\n\n- 1.8\n\n- 1.1\n\nand social networks. Specifically, we pre-train a three-layer GIN (Xu et al., 2019) and feed the generated graph representations into SVMs for evaluation. We report the average values and standard deviations of accuracies (%) of five different runs, each of which corresponds to a 10-fold evaluation. Following (You et al., 2021), we report test accuracy of the epoch selected by validation set.\n\nTransfer Learning tests the pre-trained GNN’s transferability to downstream tasks. Following (Hu et al., 2020), we use the two million molecule samples from the ZINC15 dataset (Sterling & Irwin, 2015) for pre-training and eight multi-label classification datasets derived from the MoleculeNet (Wu et al., 2018) for fine-tuning. Fine-tuning datasets are divided by scaffold split to create distribution shifts among the train/valid/test sets, so as to provide more realistic estimations of the molecule property prediction performance. Following (Hu et al., 2020), we implement E-GCL with a five-layer GINE. Further, we push the limits of E-GCL with the GraphTrans (Wu et al., 2021) backbone. GraphTrans stacks a four-layer Transformer (Vaswani et al., 2017) on top of the GINE to learn long range interactions. For evaluation, we pre-train a model and repeatedly fine-tune it on downstream datasets ten times. We report the averages and standard deviations of ROC-AUC (%) scores. Following (You et al., 2020), we report test set performance of the last epoch.\n\n4.2 MAIN RESULTS (RQ1)\n\nUnsupervised Learning Results. Table 1a presents the unsupervised learning performance in TU datasets. The last column denotes the improvement compared to a randomly initialized GNN. E-GCL achieves the best performance in six out of eight datasets and the top three performances in the other two datasets. It also achieves the best average improvement of 7.43% compared to a randomly initialized GNN. We attribute E-GCL’s good performances to the equivariance principle of crossgraph augmentation. Other methods apply only the invariance principle of intra-graph augmentation, thus failing to generate representations as discriminative as E-GCL.\n\nTransfer Learning Results. Table 1b presents the fine-tuning performances in transfer learning. EGCLs have achieved the best performances among all methods. Specifically, E-GCL with GraphTrans achieves the best performances in four out of eight datasets and the best average improvement of 5.5% compared to a randomly initialized GNN. When using the GINE backbone, E-GCL continues to show improvements over all the baseline models in average performance. It shows that the equivariant pre-training of cross-graph augmentation gives E-GCL a better starting point for fine-tuning. Previous models apply only the intra-graph augmentation, which might bring together dissimilar patterns. Notice that, E-GCL and other models’ improvements over previous works are not consistent across fine-tuning datasets. We attribute the inconsistency to the Out of Distribution (OOD) evaluation setting in MoleculeNet. The validation set does not overlap the test set, which makes preventing overfitting and underfitting troublesome. We follow the evaluation protocol of previous works (You et al., 2020; Xu et al., 2021), which, however, does not guarantee the convergence of performance. We leave a more stable fine-tuning for the OOD evaluation as future work.\n\nIn summary, E-GCL establishes a new state-of-the-art in unsupervised learning and transfer learning.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Different Beta(α, α) distributions.\n\n(b) Different equivariance and invariance trade-off.\n\nFigure 3: Hyper-parameter sensitivity study in unsupervised learning.\n\nGeneralization to Different SSL Frameworks. To highlight E-GCL’s improvement and generalization ability, we apply the equivariance principle to three representative SSL frameworks of different flavors: GraphCL – GCL, SimSiam (Chen & He, 2021) – asymmetric Siamese Networks, and BarlowTwins – decorrelating feature dimensions (Table 2). We observe that equivariance consistently improves about 1% of SSL backbones’ average performances in both unsupervised learning and transfer learning. The results demonstrate the effectiveness of equivariance and its generalization ability to diverse SSL frameworks and different experimental settings.\n\n4.3 ANALYZING THE PROPERTIES OF E-GCL (RQ2)\n\nHyper-parameter Sensitivity. Figure 3 presents E-GCL’s sensitivity with respect to the shape parameter α of Beta(α, α) distribution and the trade-off factor ω between invariance and equivariance. Shown by Figure 3a, α = 0.1 gives the best average performance for both the BarlowTwins and GraphCL backbones in unsupervised learning. We also observe that the optimal ω value differs for SSL backbones (Figure 3b). The best ω for BarlowTwins and GraphCL are 0.4 and 0.3, respectively. When ω = 1, the invariance loss vanishes and the performances drop to the lowest. This demonstrates that the invariance mechanism and equivariance mechanism are complementary to each other and their cooperation makes for better graph representation learning.\n\nTraining Dynamics of Alignment and Uniformity. To understand how E-GCL improves over I-GCL, we study their behaviors through the lens of alignment and uniformity losses (Wang & Isola, 2020), which constitute the asymptotic objective of contrastive learning. On a unit hypersphere, Alignment measures the closeness of the positive pairs and uniformity measures the evenness of the sample distribution. We apply the contrastive backbone GraphCL with dropNode as the intra-graph augmentation and graph interpolation as the cross-graph augmentation. Figure 4 shows the losses on each type of augmented samples and their concatenations (i.e., intra+cross aug). Compared to E-GCL cross-aug, E-GCL intra+cross aug has better uniform loss but worse alignment loss. E-GCL achieves much better alignment and uniformity on cross-graph augmentations ⋆ than I-GCL, with a slight sacrifice of alignment on intra-augmentations . This is in expectation as E-GCL applies equivariance to explicitly optimize the cross-graph augmentations and trade-off the optimization of intra-graph augachieves better alignment mentations. Combining intra- and cross-graph augmentations, E-GCL and uniformity than I-GCL , which explains the better performance.\n\nFigure 4: The alignment and uniformity losses when pre-training with chemical molecules. Losses are evaluated every 100 pre-train steps and lower numbers are better. Arrows denote the losses’ changing directions.\n\n5 CONCLUSION AND FUTURE WORKS\n\nIn this paper, we propose Equivariant Graph Contrastive Learning (E-GCL) that combines equivariance and invariance to learn better graph representations. E-GCL encourages the sensitivity to global semantic shifts by grounding the equivariance principle as a cross-graph augmentation of graph interpolation. This equivariance principle protects GNNs from aggressive intra-graph augmentations that can harmfully align dissimilar patterns and enables GNNs to discriminate cross-graph augmented samples. Extensive experiments in unsupervised learning and transfer learning demonstrate E-GCL’s significant improvements over state-of-the-art methods and its generalization ability to different SSL frameworks. In the future, we will explore more groundings of the equivariance principle in graphs.\n\n9\n\n0.10.4124BarlowTwins (=0.4)76.076.577.0Accuracy (%)76.9376.6876.5576.4376.470.10.4124GraphCL (=0.3)76.9176.7576.4576.3476.38BackboneE-GCL0.10.20.30.40.51.0BarlowTwins (=0.1)76.5576.6376.6976.9376.6876.230.10.20.30.40.51.0GraphCL (=0.1)76.7076.6576.9176.7576.8276.45BackboneE-GCL3.8753.8503.8253.8003.7753.7503.7253.7003.675uniform loss0.20.30.40.50.60.7align lossI-GCL intra-augI-GCL cross-augE-GCL intra-augE-GCL cross-augE-GCL intra+cross augUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nWaiss Azizian and Marc Lelarge. Expressive power of invariant and equivariant graph neural networks.\n\nIn ICLR, 2021.\n\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization\n\nfor self-supervised learning. In ICLR, 2022.\n\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\n\nMichael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning:\n\nGrids, groups, graphs, geodesics, and gauges. CoRR, abs/2104.13478, 2021.\n\nSourav Chatterjee. Matrix estimation by universal singular value thresholding. The Annals of\n\nStatistics, 43(1):177–214, 2015.\n\nHaiwei Chen, Shichen Liu, Weikai Chen, Hao Li, and Randall Hill. Equivariant point network for 3d\n\npoint cloud analysis. In CVPR, pp. 14514–14523, 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for\n\ncontrastive learning of visual representations. In ICML, pp. 1597–1607, 2020.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021.\n\nYung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljacic, Shang-Wen Li, Wen-tau Yih, Yoon Kim, and James Glass. DiffCSE: Difference-based contrastive learning for sentence embeddings. In NAACL, 2022.\n\nTaco Cohen and Max Welling. Group equivariant convolutional networks. In ICML, 2016.\n\nTaco S. Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical cnns. In ICLR, 2018.\n\nRumen Dangovski, Li Jing, Charlotte Loh, Seungwook Han, Akash Srivastava, Brian Cheung, Pulkit\n\nAgrawal, and Marin Soljacic. Equivariant contrastive learning. In ICLR, 2022.\n\nSander Dieleman, Kyle W. Willett, and Joni Dambre. Rotation-invariant convolutional neural\n\nnetworks for galaxy morphology prediction. CoRR, abs/1503.07077, 2015.\n\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In NeurIPS, pp. 766–774, 2014.\n\nVijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.\n\nBenchmarking graph neural networks. CoRR, abs/2003.00982, 2020.\n\nMatthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In\n\nICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent - A new approach to self-supervised learning. In NeurIPS, 2020.\n\nHongyu Guo and Yongyi Mao. Ifmixup: Towards intrusion-free graph mixup for graph classification.\n\narXiv, 2021.\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In\n\nNIPS, volume 30, 2017.\n\nXiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-mixup: Graph data augmentation for\n\ngraph classification. CoRR, 2022.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for\n\nunsupervised visual representation learning. In CVPR, pp. 9726–9735, 2020.\n\nZhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang.\n\nGraphmae: Self-supervised masked graph autoencoders. In KDD, pp. 594–604, 2022.\n\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.\n\nStrategies for pre-training graph neural networks. In ICLR, 2020.\n\nRisi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural\n\nnetworks to the action of compact groups. In ICML, volume 80, pp. 2752–2760, 2018.\n\nAA Leman and Boris Weisfeiler. A reduction of a graph to a canonical form and an algebra arising\n\nduring this reduction. Nauchno-Technicheskaya Informatsiya, pp. 12–16, 1968.\n\nHaoyang Li, Xin Wang, Ziwei Zhang, Zehuan Yuan, Hang Li, and Wenwu Zhu. Disentangled\n\ncontrastive learning on graphs. In NeurIPS, pp. 21872–21884, 2021.\n\nSihang Li, Xiang Wang, An Zhang, Yingxin Wu, Xiangnan He, and Tat-Seng Chua. Let invariant\n\nrationale discovery inspire graph contrastive learning. In ICML, pp. 13052–13065, 2022.\n\nHaggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph\n\nnetworks. In ICLR, 2019a.\n\nHaggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant\n\nnetworks. In ICML, 2019b.\n\nIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations.\n\nIn CVPR, pp. 6706–6716, 2020.\n\nRyan L. Murphy, Balasubramaniam Srinivasan, Vinayak A. Rao, and Bruno Ribeiro. Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs. In ICLR (Poster). OpenReview.net, 2019.\n\nAnnamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. CoRR, abs/1707.05005, 2017.\n\nOmri Puny, Matan Atzmon, Heli Ben-Hamu, Edward J Smith, Ishan Misra, Aditya Grover, and Yaron\n\nLipman. Frame averaging for invariant and equivariant network design. In ICLR, 2022.\n\nSenthil Purushwalkam and Abhinav Gupta. Demystifying contrastive self-supervised learning:\n\nInvariances, augmentations and dataset biases. In NeurIPS, 2020.\n\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. GCC: graph contrastive coding for graph neural network pre-training. In KDD, pp. 1150–1160, 2020.\n\nTeague Sterling and John J. Irwin. ZINC 15 - ligand discovery for everyone. J. Chem. Inf. Model., 55\n\n(11):2324–2337, 2015.\n\nArjun Subramonian. Motif-driven contrastive learning of graph representations.\n\nIn AAAI, pp.\n\n15980–15981, 2021.\n\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. In ICLR, 2020.\n\nSusheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve\n\ngraph contrastive learning. In NeurIPS, pp. 15920–15933, 2021.\n\nYuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics\n\nwithout contrastive pairs. In ICML, pp. 10268–10278, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\n\ncoding. CoRR, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998–6008, 2017.\n\nPetar Veliˇckovi ́c, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon\n\nHjelm. Deep graph infomax. In ICLR, 2019.\n\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In ICML, 2019.\n\nVikas Verma, Meng Qu, Kenji Kawaguchi, Alex Lamb, Yoshua Bengio, Juho Kannala, and Jian Tang. Graphmix: Improved training of gnns for semi-supervised learning. In AAAI, pp. 10024–10032, 2021.\n\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-\n\nment and uniformity on the hypersphere. In ICML, pp. 9929–9939, 2020.\n\nYifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Chaos is a ladder: A new\n\ntheoretical understanding of contrastive learning via augmentation overlap. In ICLR, 2022.\n\nYiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph\n\nclassification. In WWW, pp. 3663–3674, 2021.\n\nZhanghao Wu, Paras Jain, Matthew A. Wright, Azalia Mirhoseini, Joseph E. Gonzalez, and Ion Stoica. Representing long-range context for graph neural networks with global attention. In NeurIPS, pp. 13266–13279, 2021.\n\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513–530, 2018.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n\nnetworks? ICLR, 2019.\n\nMinghao Xu, Hang Wang, Bingbing Ni, Hongyu Guo, and Jian Tang. Self-supervised graph-level representation learning with local and global structure. In ICML, volume 139, pp. 11548–11558, 2021.\n\nDmitry Yarotsky. Universal approximations of invariant maps by neural networks. Constructive\n\nApproximation, pp. 407–474, 2022.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\n\ncontrastive learning with augmentations. In NeurIPS, 2020.\n\nYuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated.\n\nIn ICML, Proceedings of Machine Learning Research, pp. 12121–12132, 2021.\n\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and\n\nAlexander J Smola. Deep sets. In NeurIPS, 2017.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\n\nlearning via redundancy reduction. In ICML, 2021.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. In ICLR, 2018.\n\nYifan Zhang, Bryan Hooi, Dapeng Hu, Jian Liang, and Jiashi Feng. Unleashing the power of contrastive self-supervised visual models via contrast-regularized fine-tuning. In NeurIPS, pp. 29848–29860, 2021.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\n\nrepresentation learning. CoRR, 2020.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning\n\nwith adaptive augmentation. In WWW, pp. 2069–2080, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOF\n\nA.1 PROOFS ON GRAPH INTERPOLATION\n\nProof of that (\n\n, C) forms a group. We first recall of the definition of (\n\n, C) as follows:\n\nI\n\nI\n\nWhen viewing g as the anchor being augmented, we can systemize the mixup as a combination of two steps: (1) feature rescaling: ˆg = λg, which rescales the node and edge features of g with the ratio λ; (2) instance composition: ̃g = C(ˆg, ˆg′) = ˆg + ˆg′, which adds another rescaled graph ˆg′. Let ˆ\n> be the G\ngenerated group by combining the graphs in ˆ , C) forms G\na group.\n\nbe the enlarged set of graphs via feature rescaling,\n\nvia instance composition. We show that (\n\n=< ˆ G\nI\n\n[0, 1], g\n\nλ ˆg |\n\n∈ G}\n\n=\n\n∈\n\nI\n\n{\n\nProof. show that (\n\nI\n\nI\n\nis a set of graphs and C(\n\n, C) is a group if it satisfies the following conditions:\n\n, ·\n\n) is a binary instance composition operation on ·\n\n. It suffices to\n\nI\n\n• Associativity Law. C(g1, C(g2, g3)) = g1 + (g2 + g3) = (g1 + g2) + g3 = C(C(g1, g2), g3) . Notice that, both resulting node feature matrices and adjacency\n\nfor all g1, g2, g3 ∈ I\n\nmatrices of both sides are equivalent.\n\n• Existence of Identity. Let e be the empty graph with no nodes and no edges, then C(g, e) = , then the empty graph e is the identity.\n\ng = C(e, g) for all g\n\n∈ I\n\n• Existence of Inverse. For any graph g\n\n, there exists a graph g−1\n\nstructure of g but with inverse values of node features and edge weights, aka. Xg−1 = and Ag−1 =\n\nAg, such that C(g, g−1) = g + g−1 = e.\n\n−\n\n∈ I\n\n∈ I\n\nwho has the same Xg\n\n−\n\nProposition 2. Assuming the encoder can detect the isomorphism of interpolated graphs, there exists a GNN encoder φ that is non-trivially equivariant to the graph interpolation transformation.\n\nProof. For proof, it suffices to show that graph interpolation transformation satisfies the assumption of E-SSL’s (Dangovski et al., 2022) Non-trivial Equivariance Proposition. We can then apply the Non-trivial Equivariance Proposition to conclude the proof.\n\nThe assumption states that, “given g = g′ and p = p′ for all p, p′ interpolation formulation of equivariance: for all λg1, (1 λ)g2) = φ(λ′g′ [0, 1], if φ(λg1+(1 or 2) λg1 = (1\n\n∈ G −\n2), then either 1) λg1 = λ′g′ 1, (1 λg2) = λ′g1. We now prove this assumption.\n\nas the group of our interest, if φ(Tp(g)) = φ(T ′ and g, g′\n\np(g′)), then .” We rewrite the assumption with the graph\n\nˆ 2 ∈ G\nλ)g2 = (1\n\n− λ′)g2, (1\n\nλ)g2, λ′g′\n\nwith λ, λ′\n\nP ∈ P\n\n∈ λ′)g′ 2,\n\n1+(1\n\nλ′)g′\n\nλ′)g′\n\n1, (1\n\n−\n\n−\n\n−\n\n−\n\n−\n\n−\n\nWe consider graphs without edge features. We have assumed that GNN encoder can detect graph isomorphism of interpolated graphs. Therefore, we can infer the equivalence of the graphs based on the equivalence of graph embeddings. Let ̃g = λg1 + (1 2, we have ̃g = ̃g′ because of φ( ̃g) = φ( ̃g′).\n\nλ)g2 and ̃g′ = λ′g′\n\n1 + (1\n\nλ′)g′\n\n−\n\n−\n\nUsing the Intrusion-Freeness Theorem in (Guo & Mao, 2021), the graph interpolation transformation is invertible and the resulting original graph pair and mixup coefficient is unique, aka. the equivalence of the interpolated graphs infers the equivalence of the original graphs and the interpolation coefficient λ. Thus, we have either 1) λ = λ′, g1 = g′ 2, g2 = g1. In either case, the assumption holds.\n\n2, or 2) λ = 1\n\nλ′, g1 = g′\n\n1, g2 = g′\n\n−\n\nDiscussion. Proposition 2 relies on a powerful GNN encoder that can detect the isomorphism of interpolated graphs. This powerful GNN encoder exists both in theory and practice: 1) in theory, the universality of GNNs has been proved that they can approximate any function on graphs (Azizian & Lelarge, 2021); 2) in practice, Puny et al. (2022) have proposed a family of universal GNNs. The possible limitation lies in the complexity of these methods (Puny et al., 2022; Maron et al., 2019b). We have tested E-GCL with the GIN and GraphTrans architectures. GIN is at most as powerful as the\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n1-WL test (Leman & Weisfeiler, 1968; Azizian & Lelarge, 2021) and GraphTrans improves its ability to model long range interactions. Our experiments show that GIN and GraphTrans is sufficient to demonstrate the improvement of E-GCL over previous methods. Therefore, we leave the adoption of universal GNNs (Puny et al., 2022) as future work.\n\nThe Intrusion-Freeness Theorem (Guo & Mao, 2021) relies on the assumption in either Lemma 2 or Lemma 3 of their paper. The assumption of their Lemma 2 states that: “The node feature vectors for all graphs take values from a finite set and the values in the set are linearly independent”. This assumption holds strictly for chemical molecules (Hu et al., 2020), in which the node features are atom types that are finite and linearly independent (i.e., you cannot combine two atoms to form another). It is also automatically satisfied by anonymous social networks without any node information, i.e., COLLAB, RDT-B, RDT-M5K, IMDB-B in experiments. For other graphs with continuous node features such as word vectors, Lemma 3 provides a much weaker condition. Lemma 3 requires the linear independence of the entire node feature matrix for each graph in the dataset, which is more likely to hold in practice.\n\nA.2 PROOFS ON GROUP AVERAGING\n\nProof of that Φ(λ, g, g′) is invariant to the relative permutation between g and g′ in the sense that Φ(λ, g, g′) = Φ(λ, TP ◦ Proof. For all P ′\n\ng, g′) = Φ(λ, g, TP ′\n\ng′) for all P, P ′\n\nSn, we have\n\nSn.\n\n∼\n\n◦\n\n∈\n\nΦ(λ, TP ′\n\n◦\n\ng, g′) =\n\n=\n\n1\n\n(cid:88)\n\nSn|\n\n|\n\n1\n\nSn|\n\n|\n\nP ∈Sn (cid:88)\n\nP ∈Sn\n\nφ(λTP −1\n\nTP ′\n\n◦\n\n◦\n\ng + (1\n\n−\n\nλ)g′)\n\nφ(λTP −1P ′\n\ng + (1\n\n◦\n\n−\n\nλ)g′)\n\nLet P ′′−1 = P −1P ′, we have P = P ′P ′′ and\n\nΦ(λ, TP ′\n\n◦\n\ng, g′) =\n\n=\n\n(cid:88)\n\nP ′P ′′∈Sn\n\n(cid:88)\n\nP ′′∈P ′−1Sn\n\n1\n\nSn|\n\n|\n\n1\n\nSn|\n\n|\n\n1\n\n(cid:88)\n\n=\n\n|\n\nSn| P ′′∈Sn = Φ(λ, g, g′)\n\nφ(λTP ′′−1\n\ng + (1\n\n◦\n\n−\n\nλ)g′)\n\nφ(λTP ′′−1\n\ng + (1\n\n◦\n\n−\n\nλ)g′)\n\nφ(λTP ′′−1\n\ng + (1\n\n◦\n\n−\n\nλ)g′)\n\nWe can similarly prove Φ(λ, g, g′) = Φ(λ, g, TP ′\n\ng′).\n\n◦ g +(1\n\nProposition 1. The contrastive loss with φ(λTP ◦ network to relative permutation 1\n\n(cid:80)\n\n|Sn|\n\nP ∈Sn ρ(φ(λiTP ◦\n\n−\n\nλ)g′) upper bounds the loss of an invariant gi + (1\n\nλi)g′\n\ni)).\n\n−\n\nProof. The loss of NT-Xent is:\n\nl(\n\nz3\n\ni }\n\n{\n\nN\n\ni=1,\n\nz4\n\ni }\n\n{\n\nN\n\ni=1) =\n\n=\n\n1 N\n\n1 N\n\n−\n\n− (cid:124)\n\nN (cid:88)\n\ni=3\n\nN (cid:88)\n\nlog\n\ni , z4\n\nexp(s(z3 j=1,j̸=i exp(s(z3\n\ni )/τ ) i , z4\n\n(cid:80)N\n\nj )/τ )\n\n+\n\n1 N\n\nN (cid:88)\n\nlog\n\nN (cid:88)\n\ni=1\n\nj=1,j̸=i\n\nexp(s(z3\n\ni , z4\n\nj )/τ )\n\ns(z3\n\ni , z4\n\ni )/τ\n\ni=1\n\n(cid:123)(cid:122) lpos\n\n(cid:125)\n\nwhere lpos aims to minimize the distance between positive views.\n\nLet be the batch of original graph pairs and the mixup coefficients. If we use MSE to minimize the distance between the feature interpolation view and\n\n2), ..., (λN , gN , g′\n\n(λ1, g1, g′ {\n\n1), (λ2, g2, g′\n\nN )\n\n}\n\n14\n\n(9)\n\n(10)\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\nUnder review as a conference paper at ICLR 2023\n\nrepresentation interpolation view and omit the temperature hyperparameter τ , lpos can be written as:\n\nlpos =\n\n1 N\n\nN (cid:88)\n\ni=1\n\nρ(ψ(λi, TPi ◦\n\n||\n\ngi, g′\n\ni))\n\n−\n\nρ(λiφ(gi) + (1\n\nλi)φ(g′\n\ni))\n\n2 ||\n\n−\n\n(17)\n\nwhere Pi ∼\n\nSn for all i\n\n[N ]. We have\n\n∈\n\n(cid:34)\n\nEP ∼Sn[lpos] = EP ∼Sn\n\n1 N\n\nN (cid:88)\n\ni=1\n\nρ(ψ(λi, TP ◦\n\n||\n\ngi, g′\n\ni))\n\n−\n\nρ(λiφ(gi) + (1\n\nλi)φ(g′\n\ni))\n\n2 ||\n\n−\n\n(18)\n\n(cid:35)\n\n1 N\n\n1 N\n\n=\n\n≥\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\n1\n\n(cid:88)\n\nSn|\n\n|\n\nPj ∈Sn\n\nρ(ψ(λi, TPj ◦\n\n||\n\ngi, g′\n\ni))\n\n1\n\n(cid:88)\n\n||\n\nSn|\n\n|\n\nPj ∈Sn\n\nρ(ψ(λi, TPj ◦\n\ngi, g′\n\ni))\n\n−\n\n−\n\nρ(λiφ(gi) + (1\n\nρ(λiφ(gi) + (1\n\nλi)φ(g′\n\ni))\n\n2 (19)\n\n||\n\nλi)φ(g′\n\ni))\n\n2 (20)\n\n||\n\n−\n\n−\n\nwhere the last step is by Jensen’s inequality.\n\nNotice that, the contrastive loss upper bounds the distance between ρ(λiφ(gi) + (1 i)) and ψ. By the property of group averaging (Puny et al., 2022; Yarotsky, 2022; Murphy et al., 2019), it follows that\n\ni)), which is the group averaging of ρ\n\ngi, g′\n\n1 |Sn|\n\n(cid:80)\n\n−\n\n◦\n\nλi)φ(g′\n\nPj ∈Sn ρ(ψ(λi, TPj ◦ gi, g′\n\nPj ∈Sn ρ(ψ(λi, TPj ◦\n\n(cid:80)\n\n1 |Sn|\n\ni)) is invariant to relative permutation as well.\n\nB RELATED WORKS\n\nWe have briefly introduced GCL methods in Section 2. In this section, we first discuss E-GCL’s connections and differences with E-SSL and IfMixup. Then, we present E-GCL’s relations to other graph mixup methods and geometric deep learning.\n\nE-SSL. E-GCL is inspired by the pioneer E-SSL works (Dangovski et al., 2022; Chuang et al., 2022) in CV and NLP. Dangovski et al. (2022) find that, when the previous insensitive objective fail on certain augmentations, applying a sensitive objective to the same augmentations can improve performance. Specifically, they apply a sensitive objective on the four-fold rotations of images to improve existing SSL methods. In NLP, Chuang et al. (2022) implement the sensitive objective as discriminating word replacement to improve sentence level embedding. Adapting E-SSL for graphs is challenging because existing intra-graph augmentations share the common paradigm of structure corruption, making it hard to categorize them into sensitive and insensitive augmentations. This works is different from previous E-SSL works that we introduce cross-graph augmentation to create global semantic shifts. By encouraging the sensitivity to cross-graph augmentation, we protect representations from the harmful aggressive intra-graph augmentations.\n\nIfMixup. In this work, we extend IfMixup (Guo & Mao, 2021) for cross-graph augmentation in SSL. IfMixup mitigates the structural differences by padding virtual nodes for graph interpolation. It is previously developed for supervised learning. For SSL, we propose to supervise mixed graphs by the interpolation of the original graphs’ representations. We also connect graph mixup to group theory and address its limitation of sensitivity to the relative permutation.\n\nGraph Mixup. Graph mixup has been a challenging task due to graphs’ irregular structures. GraphMix (Verma et al., 2021) sidesteps the structural differences by mixing only node features. Wang et al. (2021) mixup the graph representations for graph classification. -Mixup (Han et al., 2022) samples adjacency matrices from the mixed graphons of two classes as graph mixup. We opt out\n\n-Mixup in our method due to its following limitations:\n\nG -mixup does not support node feature mixup. Their paper has no experiments on attributed graphs. G\nMoreover, their instruction for sampling mixed node features is vague: the instruction does not describe the sampling strategy and the used distribution.\n\nG\n\n-mixup does not scale to large graphs due to its O(N 3) complexity (N is the number of nodes). G\nThe high complexity is due to the SVD (Chatterjee, 2015) in graphon estimation. In comparison, IfMixup is scalable to large graphs with a linear complexity to edge and node numbers O(E + N ).\n\n15\n\n•\n\n•\n\nUnder review as a conference paper at ICLR 2023\n\n•\n\n-mixup requires class labeling to estimate the graphon in each class. In SSL, class labeling is G\nunavailable. If we were to take risks and treated each graph as a class, the obtained graphon would be suboptimal due to the limited sample. In this case, it is outperformed by IfMixup (Table 3).\n\nGeometric Deep Learning. Invariance and equivariance have been heavily studied under the scope of geometric deep learning (Bronstein et al., 2021). The goal is to explore geometric symmetries in neural architecture designs for effective weight sharing to reduce sample complexity (Cohen & Welling, 2016). For example, generalizing the convolution operation from the Z2 grids to the p4 group makes convolution equivariant to the four-fold rotation (Cohen & Welling, 2016); the message passing operation in GNNs maintains the node-level output equivariant to the permutation group Sn (Battaglia et al., 2018). Exploring geometric symmetries like rotation and permutation have greatly improved performances in various applications, including galaxy morphology (Dieleman et al., 2015), point clouds (Chen et al., 2021; Zaheer et al., 2017), and spherical images (Cohen et al., 2018). Our work is different from geometric deep learning because we do not study neural architectures. We study transformations that change the underlying semantics of graphs rather than their “poses” in the geometric space.\n\nC LIMITATIONS\n\nSSL is limited in that it has little knowledge of the downstream tasks. Each type of intra-graph augmentation represents a human prior that performs differently on different downstream datasets (Purushwalkam & Gupta, 2020). Our work grounds the equivariance mechanism as a domain agnostic cross-graph augmentation to facilitate representations with the sensitivity to global semantic shifts.\n\nPrevious E-SSL works (Dangovski et al., 2022; Chuang et al., 2022) in CV and NLP divide existing data augmentations into two sets of sensitive augmentations and insensitive augmentations. Our limitation is that we leave the existing intra-graph augmentations untouched as the insensitive augmentations, although the insensitivity to some aggressive intra-graph augmentations might diminish the sensitivity to cross-graph augmentations. However, disentangling the aggressive augmentations from the others requires extensive tests or domain knowledge. We leave it as future work. Further, our equivariance branch is a patch to the limitations of the invariance branch. In future, we will explore GCL of the complete focus on equivariance without the invariance branch.\n\nEquivariance is a high-level mathematical concept unifying sensitivity and insensitivity. It has promising potential in graph representation learning. Our work is limited that we explore the equivariance to a simple cross-graph augmentation of graph mixup. We believe there are other equivariance principles worth exploring.\n\nThe limitations and assumptions of the theoretical results have been discussed in Appendix A.\n\nD EXPERIMENT\n\nD.1 ABLATION STUDIES FOR GRAPH INTERPOLATION\n\nTable 3: Unsupervised learning accuracies (%) on the TU datasets.\n\nDataset\n\nCOLLAB\n\nRDT-B\n\nRDT-M5K IMDB-B\n\nAVG\n\nBarlowTwins BarlowTwins + G-Mixup (discrete) BarlowTwins + IfMixup (continuous)\n\n72.94±0.62 71.11±1.19 74.63±0.28\n\n90.11±0.85 91.08±0.54 90.71±0.57\n\n55.40±0.47 55.90±0.13 56.90±0.29\n\n71.28±0.58 71.58±0.31 72.02±0.90\n\n72.40 72.42 73.57\n\n-mixup. Our E-GCL framework is agnostic to the graph mixup strategy for crossComparison with G\n-Mixup (Han et al., 2022) for the cross-graph augmentation. graph augmentation. We also exploit -Mixup yields Different from the linear interpolation strategy of IfMixup (Guo & Mao, 2021), discrete adjacency matrices by sampling from the mixed graphons of two classes. To adapt -Mixup to the SSL setting, we use their source code and treat each graph as a class. Grid search is conducted to tune the hyperparameters: α and ω. Table 3 shows the performance comparison between different graph mixup strategies. We do not compare performances on attributed graph datasets because\n\nG\n\nG\n\nG\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Average fine-tuning performances (ROC-AUC (%)) in transfer learning downstream datasets.\n\nInterpolation Position GraphCL BarlowTwins\n\nNo Equivariance Before Projector After Projector Similarity Score\n\n70.8 71.9 71.8 71.5\n\n72.0 73.8 72.5 -\n\nTable 5: Unsupervised learning accuracies (%) of E-GCL on the TU datasets.\n\nNCI1\n\nPROTEINS\n\nDD\n\nMUTAG COLLAB\n\nRDT-B\n\nRDT-M5K IMDB-B AVG\n\nE-GCL +Mixup different sizes\n\n79.95±0.25 78.54±0.47\n\n75.18±0.40 74.86±0.52\n\n77.83±0.52 79.05±0.58\n\n88.20±0.85 89.74±1.38\n\n74.63±0.28 73.70±0.46\n\n90.71±0.57 90.74±0.68\n\n56.90±0.29 56.17±0.35\n\n72.02±0.90 72.30±0.33\n\n76.93 76.89\n\n-Mixup does not support mixing node features. We have discussed the limitations of\n\nG the related works section (Appendix B).\n\n-mixup in\n\nG\n\nOur linear interpolation strategy substantially outperforms IfMixup also shows 1.15% better average accuracy than BarlowTwins baseline in the last three datasets, it performs worse in the COLLAB dataset.\n\n-mixup in three out of four TU datasets. -mixup improves over the\n\n-mixup. While\n\nG\n\nG\n\nG\n\nInterpolation Position. The cross-graph augmentation is supervised by the interpolation of the original graphs’ representations. We interpolate the representations before the projector to let the gradient mainly optimize the equivariance of the GNN encoder φ. With this design, the encoder is trained to approach Equation (7), which is equivariance to cross-graph augmentation. However, strict equivariance is hardly achieved by GNNs. Therefore, we have the projector learn to ignore the subtle difference between the two sides of Equation (7) and reduce the task’s difficulty. Table 4 compares the performances of not using equivariance and the performances of E-GCL when applying representation interpolation at different positions: before projector, after projector, and interpolating the cosine similarity scores. We verify that 1) before the projector is the optimal choice for both GraphCL and BarlowTwins, and 2) using equivariance at all positions consistently outperforms No Equivariance. More than better performance, before projector also allows our equivariance principle to work with a broader family of SSL frameworks (Zbontar et al., 2021; Bardes et al., 2022).\n\nInfluence of Mixing Dummy Nodes. We pad dummy nodes to the original graphs to the same size before mixup. As dummy nodes have zero features, adding them to original graphs does not introduce any noise into the interpolations. We demonstrate the neutral effect of dummy nodes by comparing the performances of mixing graphs with very different sizes and the original E-GCL in Table 5. The performance difference is only 0.04%.\n\nWe deliberately create size differences between mixed graphs in the different size experiment. Specifically, we sort the in-batch graphs by their sizes , such that g2| ≤ i + 1)-th graph. In this way, we create size ... differences in every batch such that the smallest graph g1 will be mixed with the largest graph gN . We use the BarlowTwins SSL framework for the experiments.\n\n. Then, we mix the i-th graph with the (N\n\ng1, g2, ..., gN }\n\ng1| ≤ |\n\ngN |\n\n≤ |\n\n−\n\n{\n\n|\n\nD.2\n\nINFLUENCE OF SAMPLED PERMUTATION NUMBERS\n\nExperimental Settings. In the existing experiments (cf. Section 4), we randomly permute one of the input graphs before graph interpolation. This strategy is a 1-sample estimator of the group averaging and shows improvements over the state-of-the-art baselines. Here we conduct ablation experiments to show that: 1) using random permutation improves performance; 2) using more samples to approximate group averaging improves performance, and 3) the improvement of using more samples is only marginal and comes at the cost of increased complexity. We conduct unsupervised learning experiments using the TU datasets. We employ E-GCL with BarlowTwins framework, set α = 0.1, and perform 5 experiments of different ω = values. We report the average and max values of the mean accuracies (%) of these 5 different experiments. The forward time is measured across 100 batches of size 128 on the COLLAB dataset.\n\n0.1, 0.2, 0.3, 0.4, 0.5\n\n}\n\n{\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Unsupervised learning performance in TU datasets. We ablate E-GCL using different numbers of samples to approximate group averaging. We report the average and max of mean accuracies for E-GCL with different ω =\n\n0.1, 0.2, 0.3, 0.4, 0.5\n\n. We set α = 0.1.\n\n{\n\n}\n\nNo rand. perm.\n\n1-sample\n\n3-sample\n\n10-sample\n\nAverage Max Forward time (ms)\n\n76.65 76.85 10.5±2.4\n\n76.70 76.92 10.7±2.1\n\n76.73 76.95 15.1±3.1\n\n76.80 76.97 29.4±8.0\n\nTable 7: ACR with GraphCL backbone. Lower is better\n\nBefore pre-training After pre-training\n\nGraphCL +Equivariance\n\n0.983\n\n0.463 0.424\n\nExperimental Results. Table 6 shows that using random permutation consistently outperforms not using random permutation (No rand. perm.). Specifically, the 1-sample estimator shows better performance and adds no computational cost compared to No rand. perm.. Further, it shows that using more samples to estimate the group averaging improves performance. The 10-sample estimator gives the best performance. However, the 10-sample estimator’s improvement is only marginal (0.05% 0.11%) compared to the 1-sample estimator, but leads to almost three times increase in time complexity. Thus, we recommend the 1-sample estimator in implementation.\n\n∼\n\nD.3\n\nINFLUENCE OF EQUIVARIANCE ON AGGRESSIVE AUGMENTATIONS.\n\nIntra-graph augmentations are problematic that they sometimes harmfully enforce insensitivity to semantically shifted graphs (i.e., aggressive augmentations). To patch the problem, cross-graph augmentations always enforce sensitivity to semantically shifted graphs that are generated by graph interpolation. Consequently, the equivariance to cross-graph augmentations diminish the harmful invariance of aggressive intra-graph augmentations that change global semantics, leading to better performance.\n\nTo justify that equivariance can mitigate the negative effect of aggressive augmentations, we conduct experiments w.r.t. Average Confusion Ratio (ACR) (Wang et al., 2022). ACR is the metric to measure the ratio that the anchor graph’s nearest neighbors are the views of different graphs, rather than the other views of the same anchor. Higher ACR indicates that the graph representations are less powerful to distinguish different graphs, thus reflecting worse negative influences of aggressive augmentations. We use the GraphCL checkpoints from Table 2b and report the ACR scores on the ZINC15 dataset.\n\nAs shown by Table 7, applying equivariance to cross-graph augmentation improves the ACR for GraphCL. This demonstrates that the equivariance principle mitigates the negative influences of aggressive augmentations, thus leading to better graph discrimination performance.\n\nD.4 RESULTS IN THE FIRST SUBMISSION.\n\nWe include the results from our first submission for your reference (Table 8). We use Table 1 to replace Table 8 to report baseline performances under a consistent evaluation protocol.\n\nE IMPLEMENTATION DETAILS\n\nE.1 E-GCL PSEUDO CODE\n\nAlgorithm 1 presents the pseudo code of E-GCL.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 8: Main experiment performances. † denotes results borrowed from (Li et al., 2022). denotes reproduced results using the released codes. Other baseline results are borrowed from (You et al., 2021). Bold indicates the best performance and underline indicates the second best performance.\n\n∗\n\n(a) Unsupervised learning accuracies (%) on the TU datasets.\n\nDataset\n\nNCI1\n\nPROTEINS\n\nDD\n\nMUTAG COLLAB\n\nRDT-B\n\nRDT-M5K IMDB-B AVG GAIN\n\nNo Pre-train† graph2vec InfoGraph GraphCL JOAO ADGCL† GraphMAE* RGCL† E-GCL\n\n65.40±0.17 73.22±1.81 76.20±1.06 77.87±0.41 78.36±0.53 73.91±0.77 75.00±0.95 78.14±1.08 77.93±0.41\n\n72.73±0.51 73.30±2.05 74.44±0.31 74.39±0.45 74.07±1.10 73.28±0.46 73.92±0.97 75.03±0.43 75.05±0.60\n\n75.67±0.29 -\n72.85±1.78 78.62±0.40 77.40±1.15 75.79±0.87 76.15±0.99 78.86±0.48 79.07±0.53\n\n87.39±1.09 83.15±9.25 89.01±1.13 86.80±1.34 87.67±0.79 88.74±1.85 87.17±1.02 87.66±1.01 88.70±2.20\n\n65.29±0.16 -\n70.05±1.13 71.36±1.15 69.33±0.34 72.02±0.56 72.92±3.88 70.92±0.65 73.84±0.33\n\n76.86 ±0.25 75.78±1.03 82.50±1.42 89.53±0.84 86.42±1.45 90.07±0.85 81.27±2.51 90.34±0.58 91.59±0.54\n\n48.48±0.28 47.86±0.26 53.46±1.03 55.99±0.28 56.03±0.27 54.33±0.32 49.63±1.67 56.38±0.40 56.48±0.35\n\n69.37±0.37 71.10±0.54 73.03±0.87 71.14±0.44 70.83±0.25 70.21±0.68 71.96±0.65 71.85±0.84 72.10±0.76\n\n70.15 -\n74.02 75.71 75.01 74.79 73.50 76.15 76.85\n\n- -\n3.87 5.56 4.86 4.64 3.35 6.00 6.70\n\n(b) Transfer learning ROC-AUC (%) scores on the MoleculeNet. GTS denotes GraphTrans.\n\nDataset\n\nBBBP\n\nTox21\n\nToxCast\n\nSIDER ClinTox MUV\n\nHIV\n\nBACE AVG GAIN\n\nNo Pre-train Infomax ContextPred GraphCL JOAO ADGCL† GraphLOG† GraphMAE* RGCL† E-GCL E-GCL, GTS\n\n65.8±4.5 68.8±0.8 68.0±2.0 69.7±0.7 71.4±0.9 68.3±1.0 71.0±1.9 72.2±0.9 71.4±0.7 72.3±0.6 72.3±0.8\n\n74.0±0.8 75.3±0.5 75.7±0.7 73.9±0.7 74.3±0.6 73.6±0.7 74.6±0.6 75.1±0.4 75.2±0.3 74.9±0.7 77.9±0.6\n\n63.4±0.6 62.7±0.4 63.9±0.6 62.4±0.6 63.2±0.5 63.1±0.7 62.3±0.5 63.0±0.3 61.4±0.6 64.0±0.3 66.0±0.6\n\n57.3±1.6 58.4±0.8 60.9±0.6 60.5±0.9 60.5±0.7 59.2±0.9 57.9±1.4 58.5±0.7 61.4±0.6 62.8±0.5 62.4±1.0\n\n58.0±4.4 69.9±3.0 65.9±3.8 76.0±2.6 81.0±1.6 77.6±4.2 78.7±2.6 80.5±2.0 83.4±0.9 83.1±2.5 80.7±3.0\n\n71.8±2.5 75.3±2.5 75.8±1.7 69.8±2.7 73.7±1.0 74.9±2.5 75.0±2.0 75.7±1.2 76.7±1.0 78.8±0.8 79.4±2.1\n\n75.3±1.9 76.0±0.7 77.3±1.0 78.5±1.2 77.5±1.2 75.4±1.3 75.2±2.0 76.4±0.8 77.9±0.8 76.3±0.6 77.8±1.1\n\n70.1±5.4 75.9±1.6 79.6±1.2 75.4±1.4 75.5±1.3 75.0±1.9 82.6±1.2 81.3±1.0 76.0±0.8 78.1±1.1 79.7±2.4\n\n67.0 70.3 70.9 70.8 72.1 70.9 72.2 72.8 73.2 73.8 74.5\n\n- 3.3 3.9 3.8 5.1 3.9 5.2 5.8 6.2 6.8 7.5\n\nE.2\n\nIMPLEMENTATION\n\nWe implement GNNs with the PyTorch Geometric library (Fey & Lenssen, 2019), which is opensource under the MIT license. We conduct experiments using an NVIDIA V100 GPU (32 GB memory) on a server with a 40-core Intel CPU.\n\nBaselines. For comparison, we report the performances of the following baseline methods:\n\n• graph2vec (Narayanan et al., 2017) treats each graph as a document and employs a document\n\nembedding approach to learn graph embeddings.\n\n• Infomax (Veliˇckovi ́c et al., 2019) maximizes the mutual information between the patch representa-\n\ntions that summarize the subgraphs centered around nodes and the global readout of graphs.\n\n• ContextPred (Hu et al., 2020) aims to predict the surrounding graph structures using the embeddings of the local subgraphs. The prediction problem is formulated as a binary prediction with negative samples to resolve the intractability of predicting graph structures.\n\n• InfoGraph (Sun et al., 2020) learns representations by maximizing the mutual information between\n\ngraph-level representations and graph substructures of different scales, e.g., nodes and edges.\n\n• GraphCL (You et al., 2020) explores the combination of intra-graph augmentations, such as node\n\ndropping, edge dropping, and subgraph, for contrastive graph representation learning.\n\n• AD-GCL (Suresh et al., 2021) trains an augmenter to adversarially drop edges to remove redundant\n\ninformation.\n\n• GraphLOG (Xu et al., 2021) utilizes clustering to contrast local instances and the corresponding\n\nhierarchical prototypes at every clustering layer.\n\n• JOAO (You et al., 2021) aims to automate the selection of graph augmentations via solving a\n\nbi-level optimization problem.\n\n• GraphMAE (Hou et al., 2022) explores GNN pretraining by reconstructing node features using a\n\nmasked autoencoder.\n\n• RGCL (Li et al., 2022) learns a rationale generator to protect the salient features during data\n\naugmentation.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 PyTorch-style pseudocode for E-GCL\n\n# phi: GNN encoder backbone # rho: MLP projector network # ssl_loss: loss function that encourages insensitivity of positive views # alpha: shape parameter of beta distribution # omega: weight hyper-parameter of equivariance\n\nfor g in loader:\n\n# intra-graph augmentation g1 = augment(g) g2 = augment(g)\n\n# cross-graph augmentation lamb = random.beta(alpha, alpha, size=len(g)) # sample interpolation coefficients perm = random.randperm(len(g)) # generate random permutation of graph list mix_g = mix_graph_list(g1, g2[perm], lamb) # interpolate between g1[i] and g2[perm[i]]\n\n# invariance loss h1 = phi(g1) h2 = phi(g2) inv_loss = ssl_loss(rho(h1), rho(h2))\n\n# equivariance loss h3 = phi(mix_g) # feature interpolation output h4 = lamb * h1 + (1-lamb) * h2[perm] # representation interpolation output eqv_loss = ssl_loss(rho(h3), rho(h4))\n\n# cooperation loss of invariance mechanism and equivariance mechanism loss = (1-omega) * inv_loss + omega * eqv_loss\n\n# optimization step loss.backward() optimizer.step()\n\ndef mix_graph_list(g_list1, g_list2, lamb_list):\n\n# minimal graph mixup implementation without edge features mix_g_list = [] for g1, g2, lamb in zip(g_list1, g_list2, lamb_list):\n\n# pad node features and adjacency matrices to the same size N = max(size(g1), size(g2)) x1, x2 = pad_x(g1.x, size=N), pad_x(g2.x, size=N) # shape = (N, D) adj1, adj2 = pad_adj(g1.adj, size=N), pad_adj(g2.adj, size=N) # shape = (N, N)\n\n# mix node features and adjacency matrices mix_x = lamb * x1 + (1-lamb) * x2 # shape = (N, D) mix_adj = lamb * adj1 + (1-lamb) * adj2 # shape = (N, N)\n\n# create mixed graph instance mix_g = Graph(mix_x, mix_adj) mix_g_list.append(mix_g)\n\nreturn mix_g_list\n\nWe do not compare with DGCL (Li et al., 2021) because their experiments follow a different protocol. DGCL selects GNN layers, dimension sizes and batch sizes based on the test set performance on each dataset. However, other baselines and our methods stick to the same GNN configuration for all the datasets. Also, re-implementation is hard because the source code has not been released.\n\nAugmentations. Our intra-graph augmentation follows GraphCL (You et al., 2020). We use dropNode for the unsupervised learning experiments and use both dropNode and subgraph for the transfer learning experiments. For the cross-graph augmentation of graph mixup, we include the self-loops of virtual nodes in the adjacency matrix due to better empirical performance. Before graph mixup, we randomly shuffle the node order of one of the input graphs to have random relative permutations between input graphs, which leads to slightly better empirical performance.\n\nImplementation to Different SSL Frameworks. For the BarlowTwins and SimSiam backbones, which use no negative samples, E-GCL follows strictly to their original loss functions. For the contrastive backbone GraphCL, E-GCL uses both intra-graph augmentations and cross-graph augmentations as negative samples to facilitate better cross-graph discrimination. Specifically, we have two types of embeddings for cross-graph augmentations: the feature interpolation embedN dings i=1 and representation interpolation embeddings z4 i=1. To avoid overfitting to one type of cross-graph i | {\naugmentations, we use half of each type as the anchor graphs and the other half as the negative\n\ni = ρ(φ(λg1 z3 {\nz4 i = ρ(λφ(g1 i ) + (1\n\nλ)λg2 π(i)))\n\n− λ)φ(λg2\n\ni + (1\n\nπ(i)))\n\nz3 i |\n\n−\n\n}\n\n}\n\nN\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: Hyper-parameters in unsupervised learning.\n\nBackbones\n\nlearning rate\n\nbatch size\n\nweight decay\n\nepochs\n\nGraphCL BarlowTwins\n\n0.001 0.001\n\n128 128\n\n0 0\n\n60 60\n\nα\n\n0.1 0.1\n\nω\n\n0.2 0.4\n\nProjector dimensions\n\n[32, 32, 32] [32, 128, 128, 128]\n\nTable 10: Hyper-parameters in transfer learning.\n\nBackbones\n\nlearning rate\n\nbatch size\n\nweight decay\n\nepochs\n\nGraphCL SimSiam BarlowTwins BarlowTwins, GraphTrans\n\n0.001 0.0005 0.001 0.0001\n\n256 2048 2048 2048\n\n0 0.00001 0\n0\n\n80 100 100 100\n\nα\n\n1 4\n2 2\n\nω\n\n0.1 0.3 0.5 0.3\n\nProjector dimensions\n\n[300, 300, 300] [300, 300, 300, 300] [300, 1200, 1200, 1200] [128, 1200, 1200, 1200]\n\nTable 11: GNN configurations in transfer learning.\n\nModel\n\nTraining Time #Parameters GNN Layers Transformer Layers GNN dim Transformer dim Pooling\n\nGINE GraphTrans\n\n16.3 hours 32.5 hours\n\n1.85M 2.73M\n\n5 5\n\n- 4\n\n300 300\n\n- 128\n\nMean <CLS>\n\nj = z3+1(j>N/2)\n\nj = z4−1(j≤N/2) samples. Define m3 returns 1 when the condition holds and returns 0 otherwise. interpolation embeddings and half of the representation interpolation embeddings; {\nthe other half. The loss function when using GraphCL backbone can be written as:\n\n) is a binary indicator that ·\nj=1 contains half of the feature m4 j=1 contains\n\n, where 1(\n\nand m4\n\nm3\n\nj }\n\nj }\n\n{\n\nN\n\nN\n\nj\n\nj\n\ninvariance:\n\nequivariance:\n\nl({z1\n\ni }N\n\ni=1, {z2\n\ni }N\n\ni=1) = −\n\nl({m3\n\ni }N\n\ni=1, {m4\n\ni }N\n\ni=1)\n\n1\n\nN\n\nN (cid:88)\n\ni=1\n\nlog\n\n(cid:80)N\n\nj=1,j̸=i exp(s(z1\n\ni , z2\n\nexp(s(z1 j )/τ ) + (cid:80)N\n\ni , z2 j=1,j /∈{i,π(i)} exp(s(z1\n\ni )/τ )\n\n,\n\ni , m4\n\ni )/τ )\n\n= −\n\n1\n\nN\n\nN (cid:88)\n\ni=1\n\nlog\n\n(cid:80)N\n\nj=1,j /∈{i,π(i)} exp(s(m3\n\nexp(s(m3\n\ni , m4 j )/τ ) + (cid:80)N\n\ni , m4\n\ni )/τ ) j=1,j /∈{i,π(i)} exp(s(m3\n\ni , z2\n\ni )/τ )\n\nLE-GCL = (1 − ω) · l({z1\n\ni }N\n\ni=1, {z2\n\ni }N\n\ni=1) + ω · l({m3\n\ni }N\n\ni=1, {m4\n\ni }N\n\ni=1)\n\n,\n\n(21)\n\nNotice that, we treat sample pairs that share partial graph identities (e.g., (m3 as semi-positives and exclude them from negative samples.\n\nj , z1\n\nj ) and (m3\n\nj , m4\n\nπ(j)))\n\nAlignment and Uniformity Loss. We use dropNode as the intra-graph augmentation and graph interpolation as the cross-graph augmentation. We pre-train GraphCL for 1000 steps before evaluation. The GraphCL setup follows that in Table 10. We split the original pre-training dataset into two subsets. The 80% subset is used for pre-training, and 51200 samples from the other 20% subset are used for loss evaluation.\n\nE.3 HYPER-PARAMETERS\n\nUnsupervised Learning. The hyper-parameters are shown in Table 9. We use the same three-layer GIN from (You et al., 2020). Following Zbontar et al. (2021), the BarlowTwins backbone uses a threelayer MLP projector with hidden dimensions that are four times the input dimension. Following You et al. (2020), we use a learning rate 0.01, batch size 128 and no weight decay. For E-GCL, we conduct .\ngrid search for α and ω in the following ranges α = }\nWe train the GNN for 60 epochs and evaluate the generated embedding using non-linear SVMs every 10 epochs. Following You et al. (2020), we search for the regularization parameter of SVMs in . Following (You et al., 2021), we report test accuracy of the epoch selected by validation set.\n\n0.001, 0.01, 0.1, 1, 10, 100, 1000\n\n0.1, 0.2, 0.3, 0.4, 0.5\n\n0.1, 0.4, 1, 2, 4\n\n, ω =\n\n{\n\n}\n\n{\n\n}\n\n{\n\nTransfer Learning. The hyper-parameters are shown in Table 10. We use a large batch size 2048 for BarlowTwins and SimSiam to speed up pre-training. The large batch size increases no complexity because Barlowwins and SimSiam use no negative samples. Following Zbontar et al. (2021), the BarlowTwins uses a three-layer MLP projector with hidden dimensions that are four times the input\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nTable 12: Hyperparameters for reproducing baselines. Parentheses include the original range of hyperparameters if they are different from our reproduction.\n\n(a) Hyperparameters for unsupervised learning baselines.\n\nGNN layers\n\nh-dim\n\nMax Epochs\n\nLearning rate\n\nMetric\n\nEpoch selection\n\nInfoGraph GraphCL JOAO ADGCL GraphMAE RGCL\n\n4 (4,8,12) 3\n3 5\n3 (2,3,5) 3\n\n32 32 32 32 32 (32,256,512) 32\n\n60 (100) 60 (20) 60 (40) 60 (150) 60 (300) 60 (40)\n\n1e-3 (1e-2,1e-3,1e-4) 1e-2 1e-3 1e-3 (1e-2,5e-3,1e-3) 1.5e-4 (1.5e-4,5e-4,1e-3) 1e-2\n\nAccuracy Accuracy Accuracy Accuracy Accuracy (F1) Accuracy\n\nValidation set Validation set Validation set Validation set Validation set (Last epoch) Validation set\n\n(b) Hyperparameters for transfer learning baselines.\n\nGNN layers\n\nh-dim\n\nPretrain epochs\n\nPretrain learning rate\n\nFine-tune epochs\n\nEpoch selection\n\nInfomax ContextPred ADGCL GraphLOG GraphMAE RGCL\n\n5 5\n5 5\n5 5\n\n300 300 300 300 300 300\n\n100 100 100 (20,50,80,100) 1 local + 10 global 100 100\n\n1e-3 1e-3 1e-3 1e-3 1e-3 1e-3\n\n100 100 100 100 100 100\n\nLast epoch (Validation set) Last epoch (Validation set) Last epoch (Validation set) Last epoch Last epoch Last epoch\n\nTable 13: Statistics of unsupervised learning datasets.\n\nDataset\n\nNCI1 PROTEINS DD MUTAG COLLAB RDT-B RDT-M IMDB-B\n\nCategory\n\n#Graphs\n\n#Avg. Node\n\n#Avg. Edges\n\nBiochemical Molecules Biochemical Molecules Biochemical Molecules Biochemical Molecules Social Networks Social Networks Social Networks Social Networks\n\n4110 1113 1178 188 5000 2000 4999 1000\n\n29.87 39.06 284.32 17.93 74.49 429.63 508.52 19.77\n\n64.6 145.63 1431.32 39.59 4914.43 995.51 1189.75 193.06\n\nTable 14: Statistics of biochemical graph datasets for transfer learning.\n\nDatasets\n\nUtilization\n\n#Graphs\n\n#Avg. Node\n\n#Avg. Edges\n\nZINC-2M Pre-training Fine-tuning BBBP Fine-tuning Tox21 Fine-tuning ToxCast Fine-tuning SIDER Fine-tuning ClinTox Fine-tuning MUV Fine-tuning HIV Fine-tuning BACE\n\n2000000 2039 7831 8576 1427 1477 93087 41127 1513\n\n26.62 24.06 18.57 18.78 33.64 26.15 24.23 25.51 34.08\n\n57.72 51.91 38.59 38.52 70.72 55.77 52.56 54.94 73.72\n\ndimension. The SimSiam (Chen & He, 2021) uses a two-layer MLP projector and a two-layer MLP predictor. Following Tian et al. (2021), we let the predictor network use a learning rate that is ten times that for the GNN encoder and the projector. The SimSiam backbone does not perform well with the default learning rate of 0.001 and weight decay of 0. Thus, we search for the SimSiam backbone’s ,\nlearning rate and weight decay values in the following ranges: learning rate = }\nweight decay = . We then fix the learning rate and weight decay and add the equivariance mechanism. For E-GCL, we search for α and ω from a random subset of the following ranges α = . We do not conduct a grid search because the dataset is large. For fine-tuning, the pre-trained model are re-trained 100 epochs on the transferred dataset. The fine-tuning learning rate is 1e 4 for GraphTrans. Following (You et al., 2020), we report test performance of the last epoch.\n\n0.1, 0.2, 0.3, 0.4, 0.5\n\n3 for GINE and 1e\n\n0.1, 0.4, 1, 2, 4\n\n, ω =\n\n3, 5e\n\n4, 1e\n\n5, 5e\n\n1e\n\n1e\n\n−\n\n−\n\n−\n\n−\n\n−\n\n−\n\n−\n\n{\n\n{\n\n{\n\n}\n\n}\n\n}\n\n{\n\n5\n\n4\n\nTable 11 shows the configuration details of our used GNNs, in which the GINE is from (Hu et al., 2020) and the GraphTrans is from the Molpcba experiment of (Wu et al., 2021).\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nTable 15: Complexity of E-GCL and representative GCL baselines. O(X) = O(2BL(N F 2 + EF )).\n\nGraphCL BarlowTwins\n\nSimSiam\n\nRGCL\n\nGNN encoding Loss function\n\nO(X) O(B2F )\n\nO(X) O(BF 2)\n\nO(X) O(BF )\n\nO(2X)\n\nO(2B2F ) O(2BF 2)\n\nE-GCL\n\nO(2X)\n\nTable 16: Complexity of graph augmentations.\n\nDrop Node Drop Edge\n\nSubgraph\n\nG-mixup Graph Interpolation (Ours)\n\nO(N + E)\n\nO(E)\n\nO((1 + kD)N + E)\n\nO(N 3)\n\nO(N F + EF )\n\nBaseline Hyperparameters. We have re-evaluated some baselines to present a consistent experimental comparison with E-GCL. In the re-evaluation, we report the test performance selected by validation set for unsupervised learning; we report the last epoch test performance for transfer learning. When reproducing baselines, we change only the evaluation setting and leave other hyperparameters unchanged as much as possible. Table 12 summarizes hyperparameter details. We now describe the process of selecting the baseline hyperparameters. For unsupervised learning, we use the validation set to make decisions when a range of hyperparameters are provided in the original paper. We use the same set of hyperparameters for all datasets. For transfer learning, we fine-tune the same pre-training checkpoint for all downstream datasets for fair comparison.\n\nE.4 DATASET STATISTICS\n\nTable 13 and Table 14 present the statistics of our used datasets.\n\nF COMPLEXITY ANALYSIS\n\nThe equivariant principle is computationally affordable. In this section, we analyze the complexity of GCL methods in two parts: 1) neural computation, and 2) data augmentation.\n\nSymbols. Formally, we define the symbols as follows: B number of nodes in a graph; E GNN layers; k maximum degree of nodes in graph; F graph interpolation of two graphs, let N , E, and D refer to the values of the bigger graph.\n\nZ+ is the ∈\nZ+ is the number of Z+ is the Z+ is the dimension of features of nodes and edges. For\n\n(0, 1) is the ratio of the cutted subgraph for subgraph augmentation; D\n\nZ+ is the number of edges in a graph; L\n\nZ+ is the batch size; N\n\n∈\n\n∈\n\n∈\n\n∈\n\n∈\n\n∈\n\nComplexity of Neural Computation. We consider the complexity of GNN encoding and SSL loss function. Here, we analyze the complexity when using the GIN architecture. The E-GCL uses the BarlowTwins framework. Let O(X) = O(2BL(N F 2 + EF )) be the complexity of encoding two batches of intra-graph augmentation graphs.\n\nShown by Table 15, E-GCL’s complexity is comparable to RGCL. E-GCL’s complexity is at most twice of BarlowTwin. The additional O(X) complexity of GNN encoding comes from encoding the cross-graph augmentation batch, whose size is at most the sum of the two intra-graph augmentations batches. Similarly, E-GCL’s complexity of loss function is twice of that of BarlowTwins.\n\nComplexity of Graph Augmentation. Table 16 shows the complexity of some popular graph augmentations (You et al., 2020; Han et al., 2022). Note that the complexity of our Graph Interpolation -mixup and scalable to is linear to the graph size times the feature dimension. It is lower than large graphs with thousands of nodes. Although the complexity of graph interpolation is higher than Drop Node and Drop Edge, it is scalable to large datasets. In practice, a PyTorch dataloader with 4 multiprocessing workers can process graph interpolation of 2048 chemical molecules in batches without putting GPU on wait.\n\nG\n\n23",
    "reference": "# Summary Of The Paper\n\nThe authors propose an equivariant graph contrastive learning framework that adopts two principles: invariance to intra-graph\naugmentations and equivariance to cross-graph augmentations. They use Mixup as the method for cross-graph augmentation and argue that the cross-graph augmentation captures global semantic shifts and yield better performance.\n\n# Strength And Weaknesses\n\nStrength:\n1. Extensive results on multiple datasets.\nWeakness:\n1. Novelty: Though the authors make a great effort in explaining the concept of equivalence, the framework itself boils down to a combination of Mixup and contrastive learning. This combination is not new. For instance, at a high level, [1] and [2] both adopt the combination.\n2. The experimental results are not very convincing. In table 1(a), on the datasets that the proposed method performs the best, the increase is usually below 1% , not to mention that on the remaining 3 datasets, the proposed method underperforms. Similarly, in table 1(b), the proposed method only wins 4/8 datasets and by a small margin.\n\nLiterature:\n[1]i-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning\n[2]Improving Contrastive Learning by Visualizing Feature Transformation\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing is clear, but the novelty needs to be improved.\n\n# Summary Of The Review\n\nIn general, I am not a fan of this paper mainly for the following two reasons: (1) Combining Mixup and contrastive learning is not new as several papers have adopted a similar idea though in the CV domain. (2) The experimental results are not exciting.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTRAVERSING BETWEEN MODES IN FUNCTION SPACE FOR FAST ENSEMBLING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDeep ensemble is a simple yet powerful way to improve the performance of deep neural networks. Under this motivation, recent works on mode connectivity have shown that parameters of ensembles are connected by low-loss subspaces, and one can efficiently collect ensemble parameters in those subspaces. While this provides a way to efficiently train ensembles, for inference, one should still execute multiple forward passes using all the ensemble parameters, which often becomes a serious bottleneck for real-world deployment. In this work, we propose a novel framework to reduce such costs. Given a low-loss subspace connecting two modes of a neural network, we build an additional neural network predicting outputs of the original neural network evaluated at a certain point in the low-loss subspace. The additional neural network, what we call a “bridge”, is a lightweight network taking minimal features from the original network, and predicting outputs for the low-loss subspace without forward passes through the original network. We empirically demonstrate that we can indeed train such bridge networks and significantly reduce inference costs with the help of the bridge networks.\n\n1\n\nINTRODUCTION\n\nDeep Ensemble (DE) (Lakshminarayanan et al., 2017) is a simple algorithm to improve both predictive accuracy and uncertainty calibration of deep neural networks, where a neural network is trained multiple times using the same data but with different random seeds. Due to this randomness, the parameters obtained from the multiple training runs reach different local optima, called modes, on the loss surface (Fort et al., 2019). These parameters represent a set of diverse functions serving as an effective approximation for Bayesian Model Averaging (BMA) (Wilson and Izmailov, 2020).\n\nAn apparent drawback of DE is that it requires multiple training runs. This cost can be huge especially for large-scale settings for which parallel training is not feasible. Garipov et al. (2018); Draxler et al. (2018) showed that modes in the loss surface of a deep neural network are connected by relatively simple low-dimensional subspaces where every parameter on those subspaces retains low training error, and the parameters along those subspaces are good candidates for ensembling. Based on this observation, Garipov et al. (2018); Huang et al. (2017) proposed algorithms to quickly construct deep ensembles without having to run multiple independent training runs.\n\nWhile the fast ensembling methods based on mode connectivity reduce training costs, they do not address another important drawback of DE; the inference cost. One should still execute multiple forward passes using all the parameters collected for ensemble, and this cost often becomes critical for a real-world scenario, where the training is done in a resource-abundant setting with plenty of computation time, but for the deployment, the inference should be done in a resource-limited environment. For such settings, reducing the inference cost is much more important than reducing the training cost.\n\nIn this paper, we propose a novel approach to scale up DE by reducing inference cost. We start from an assumption; if two modes in an ensemble are connected by a simple subspace, we can predict the outputs corresponding to the parameters on the subspace using only the outputs computed from the modes. In other words, we can predict the outputs evaluated at the subspace without having to forward the actual parameters on the subspace through the network. If this is indeed possible, for instance, given two modes, we can approximate an ensemble of three models consisting of\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Comparing ensembles with a Bezier curve (left) and a type II bridge network (right).\n\nparameters collected from three different locations (one from a subspace connecting two modes, and two from each mode) with only two forward passes and a small auxiliary forward pass.\n\nWe show that we can actually implement this idea using an additional lightweight network whose inference cost is relatively low compared to that of the original neural network. This additional network, what we call a “bridge network”, takes some features from the original neural network, (e.g., features from the penultimate layer), and directly predict the outputs computed from the connecting subspace. In other words, the bridge network lets us travel between modes in the function space.\n\nWe present two types of bridge networks depending on the number of modes involved in prediction, network architectures for bridge networks, and training procedures. Through empirical validation on various image classification benchmarks, we show that 1) bridge networks can predict outputs of connecting subspaces quite accurately with minimal computation cost, and 2) DEs augmented with bridge networks can significantly reduce inference costs without big sacrifice in performance.\n\n2 PRELIMINARIES\n\n2.1 PROBLEM SETUP\n\nIn this paper, we discuss the K-way classification problem taking D-dimensional inputs. A classifier is constructed with a deep neural network fθ : RD → RK which is decomposed into a feature extractor f (ft) φ (x). Here, φ ∈ Φ and ψ ∈ Ψ denote the parameters for the feature extractor and classifier, respectively, θ = (φ, ψ) ∈ Θ, and Dft is the dimension of the feature. An output from the classifier corresponds to a class probability vector.\n\nφ : RD → RDft and a classifier f (cls)\n\n: RDft → RK, i.e., fθ(x) = f (cls)\n\n◦ f (ft)\n\nψ\n\nψ\n\n2.2 FINDING LOW-LOSS SUBSPACES\n\nWhile there are few low-loss subspaces that are known to connect modes of deep neural networks, in this paper, we focus on Bezier curves as suggested in (Garipov et al., 2018). Let θi and θj be two parameters (usually corresponding to modes) of a neural network. The quadratic Bezier curve between them is defined as\n\n(cid:110)\n\n(1 − r)2θi + 2r(1 − r)θ(be)\n\ni,j + r2θj | r ∈ [0, 1]\n\n(cid:111) ,\n\n(1)\n\nwhere θ(be) low-loss subspace connecting (θi, θj) is found by minimizing the following loss w.r.t. θ(be) i,j ,\n\nis a pin-point parameter characterizing the curve. Based on this curve paramerization, a\n\ni,j\n\n(cid:90) 1\n\n0\n\n(cid:16)\n\nL\n\nθ(be)\n\ni,j (r)\n\n(cid:17)\n\ndr,\n\nwhere θ(be)\n\ni,j (r) denotes the point at the position r of the curve,\n\ni,j (r) = (1 − r)2θi + 2r(1 − r)θ(be) θ(be)\n\ni,j + r2θj,\n\n(2)\n\n(3)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nand L : Θ → R is the loss function evaluating parameters (e.g., cross entropy). Since the integration above is usually intractable, we instead minimize the stochastic approximation:\n\nEr∼U (0,1)\n\n(cid:16)\n\n(cid:104)\n\nL\n\n(cid:17)(cid:105)\n\nθ(be)\n\ni,j (r)\n\n,\n\n(4)\n\nwhere U(0, 1) is the uniform distribution on [0, 1]. For more detailed procedure for the Bezier curve training, please refer to Garipov et al. (2018).\n\n2.3 ENSEMBLES WITH BEZIER CURVES\n\nLet {θ1, . . . , θm} be a set of parameters independently trained as a deep ensemble. Then, for each pair (θi, θj), we can construct a low-loss Bezier curve. Since all the parameters along those Bezier curves achieve low loss, we can actually add them to the ensemble for improved performance. For instance, choosing r = 0.5, we can collect θ(be) i,j (0.5) for all (i, j) pairs, and construct an ensembled predictor as\n\n1 m + (cid:0)m\n\n2\n\n(cid:1)\n\n(cid:18) m (cid:88)\n\ni=1\n\nfθi(x) +\n\n(cid:19)\n\nfθ(be)\n\ni,j (0.5)(x)\n\n.\n\n(cid:88)\n\ni<j\n\n(5)\n\nWhile this strategy provide an effective way to increase the number of ensemble members, for inference, an additional O(m2) number of forward passes are required. Our primary goal in this paper is to reduce this additional cost by bypassing the direct forward passes with θ(be)\n\ni,j (r).\n\n3 MAIN CONTRIBUTION\n\nIn this section, we present a novel method that directly predicts outputs of neural networks evaluated at parameters on Bezier curves without actual forward passes with them.\n\n3.1 BRIDGE NETWORKS\n\nLet us first recall our key assumption stated in the introduction; if two modes in an ensemble are connected by a simple low-loss subspace (Bezier curve), then we can predict the outputs corresponding to the parameters on the subspace using only the information obtained from the modes. The intuition behind this assumption is that, since the parameters are connected with a simple curve, the corresponding outputs may also be connected via a relatively simple mapping which is far less complex than the original neural network. If such mapping exists, we may learn them via a lightweight neural network.\n\n(x) and vi\n\n:= fθi (x) = f (cls)\n\n:= f (ft) More specifically, let zi (zi) for i ∈ {1, . . . , m}. Let φi vi,j(r) := fθ(be) i,j (r)(x). In order to use vi,j(r) with vi to get an ensemble, we should forward i,j (r), starting from the bottom layer. Instead, we reuse zi to predict vi,j(r) with a x through fθ(be) lightweight neural network. We call such lightweight neural network a “bridge network”, since it lets us directly move from vi to vi,j(r) in the function space, not through the actual parameter space. A bridge network is usually constructed with a Convolutional Neural Network (CNN) whose inference cost is much lower than that of fθi.\n\nψi\n\nFrom the following, we introduce two types of bridge networks depending on the number of modes involved in the computation.\n\nType I bridge networks A type I bridge network h(r) predicts vi,j(r) as\n\ni,j takes a feature zi from only one mode, and\n\nvi,j(r) ≈ ̃vi,j(r) = h(r)\n\ni,j (zi).\n\n(6)\n\nA type I bridge network can be constructed between any pair of connected modes (θi, θj) and an ensembled prediction for specific mode θi with its Bezier parameter θ(be)\n\ni,j can be approximated as\n\n(cid:19)\n\nvi + h(r)\n\ni,j (zi)\n\n(cid:18)\n\n1 2\n\n,\n\n(7)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nis nearly identical\n\nwhose inference cost to that of vi (nearly single forward pass). One can also connect θi with multiple modes {θj1 , . . . , θjk }, learn bridge networks between (i, j1), . . . , (i, jk), and construct an ensemble\n\n(cid:18)\n\n1 1 + k\n\nvi +\n\nk (cid:88)\n\nj=1\n\n(cid:19)\n\n(zi)\n\n.\n\nh(r) i,jk\n\n(8)\n\nStill, since the costs for h(r) s are far lower than vi, the inference cost does not significantly increase.\n\ni,jk\n\nType II bridge networks A type II bridge network between (θi, θj) takes two features (zi, zj) to predict vi,j(r).\n\nvi,j(r) ≈ ̃vi,j(r) = H (r)\n\ni,j (zi, zj).\n\n(9)\n\nAn ensembled prediction with the type II bridge network is then constructed as\n\nvi + vj + H (r)\n\ni,j (zi, zj)\n\n(cid:19) ,\n\n(10)\n\n(cid:18)\n\n1 3\n\nwhere we construct an ensemble of three models with effectively two forward passes (for vi and vj). Similar to the type I bridge networks, we may construct multiple bridges between a single curves and use them together for an ensemble. Fig. 1 presents a schematic diagram comparing forward passes of ensembles with- /without a type II bridge network.\n\n3.2 LEARNING BRIDGE NETWORKS\n\nAlgorithm 1 Training bridge networks\n\nRequire: Training dataset D, a pair of parameters (θ1, θ2) and corresponding Bezier parameter θ(be) 1,2 (type I) or H (r) 1,2 (type II) with parameters ω, learning rate η, a regularization scale λ, a mixup coefficient α.\n\n1,2 , a bridge network h(r)\n\nInitialize ω. while not converged do\n\nSample a mini-batch B ∼ D. for i = 1, . . . , |B| do\n\nTake the input xi from B. xi ← mixup(xi, α) z1 ← f (ft) φ1 v1,2(r) ← fθ(be) if type I then\n\n(xi), v1 ← f (cls) ψ1 1,2 (0.5)(xi).\n\n(z1).\n\n ̃v1,2(r) ← h(0.5)\n\n1,2 (z1; ω).\n\nelse\n\n(z2).\n\nz2 ← f (ft) φ2 ̃v1,2(r) = H (0.5)\n\n(xi), v2 ← f (cls) ψ2 1,2 (z1, z2; ω).\n\nend if li ← DKL(v1,2(0.5)|| ̃v1,2(0.5))\n\n−λDKL(v1|| ̃v1,2(0.5)).\n\nend for ω ← ω − η∇ω\n\n1 |B|\n\n(cid:80)\n\ni li.\n\nend while return ω.\n\nFixing a position r on Bezier curves In the definition of the bridge networks above, we fixed the value r. In principle, we may parameterize the bridge networks to take r as an additional input to predict vi,j(r) for any r ∈ [0, 1], but we found this to be ineffective due to the difficulty of learning all the outputs corresponding to arbitrary r values. Moreover, as we empirically observed in Fig. 2, the ensembling with Bezier parameters are most effective with r = 0.5, and adding additional parameters evaluated at different r values does not significantly improve the performance. To this end, we fix r = 0.5 and aim to learn bridge networks predicting vi,j(0.5) throughout the paper.\n\nTraining procedure Let {θ1, . . . , θm} be a set of parameters in an ensemble. Given a set of Bezier parameters {θ(be) i,j } connecting them, we learn bridge networks (either type I or II) for each Bezier curve. The training procedure is straightforward. We first minimize the Kullback-Leibler divergence between the actual output from the Bezier parameters and the prediction made from the bridge network. It makes the bridge network imitate the original function defined by the Bezier parameters in the same manner as a conventional knowledge distillation (Hinton et al., 2015). In addition, we also maximize the Kullback-Leibler divergence between the base prediction and the bridge prediction to regularize the bridge to predict differently from the base model. Such regularization is quite important, when the training error of the base model is near zero; the base network and the target network (the one on the Bezier curve) will produce almost identical outputs. Further, we apply the mixup (Zhang et al., 2018) method to explore more diverse responses, preventing the bridge from learning to just copy the outputs of the base model. Refer to Algorithm 1 for the detailed training procedure.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Performance of an ensemble of two modes and a parameter from the Bezier curve connecting them, evaluated for ResNet-32×4 on CIFAR-100. Here, r ∈ (0, 1) denotes a position on the curve. Top row shows ensemble performances when one member from Bezier curve r is added to DE-2. Bottom row shows ensemble performances when members are sequentially added to DE-2 from Bezier curve. For accuracy, higher is the better, and for NLL, ECE and BS, lower is the better.\n\n4 RELATED WORKS\n\nMode connectivity The geometric properties of deep neural networks’ loss surfaces have been studied, and one notable property is the mode connectivity (Garipov et al., 2018; Draxler et al., 2018); there exists a simple path between modes of a neural network on which the network retains low training error along that path. From this, fast ensembling methods that collect ensemble members on the mode-connecting-paths have been proposed (Huang et al., 2017; Garipov et al., 2018). Extending this idea, Izmailov et al. (2020) approximated the posteriors of Bayesian neural nets via the low-loss subspace and used them for BMA. Wortsman et al. (2021) also presented a method for further improving performance by ensembling over the subspaces.\n\nEfficient ensembling Despite the superior performance of DE (Lakshminarayanan et al., 2017; Ovadia et al., 2019), it suffers from additional computation costs for both the training and the inference. There have been several works that reduce the computational burden in training by collecting ensemble members efficiently (Huang et al., 2017; Garipov et al., 2018; Benton et al., 2021), but they did not consider inference costs that arose from multiple forward passes. On the other hand, there also exist inference-efficient ensembling methods by sharing parameters (Wen et al., 2020; Dusenberry et al., 2021) or sharing representations (Lee et al., 2015; Siqueira et al., 2018; Antoran et al., 2020; Havasi et al., 2021). In particular, Antoran et al. (2020) and Havasi et al. (2021) presented the methods to obtain an ensemble prediction by a single forward pass. Nevertheless, these methods do not scale well for complex large-scale datasets or require large network capacity.\n\n5 EXPERIMENTS\n\nIn this section, we are going to answer the following three big questions:\n\n• Do bridge networks really learn to predict the outputs of a function from the Bezier curves? • How much ensemble gain we obtain via bridge networks with lower computational complexity? • How many bridge networks do we have to make in order to achieve certain ensemble performance?\n\nWe sequentially answer them in Sections 5.2 to 5.4 with empirical validation.\n\n5.1 SETUP\n\nDatasets and networks We evaluate the proposed bridge networks on various image classification benchmarks, including CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet datasets. Throughout the experiments, we use the family of residual networks introduced in He et al. (2016) as a base\n\n5\n\n0.10.30.50.70.9Bezier r74.075.076.077.0ACC (↑)0.10.30.50.70.9Bezier r0.900.920.940.96NLL (↓)0.10.30.50.70.9Bezier r0.020.030.04ECE (↓)0.10.30.50.70.9Bezier r0.330.340.35BS (↓)DE 2DE 2 + Bezier r012345# of Bezier θ75.075.576.076.577.0ACC (↑)012345# of Bezier θ0.860.900.940.98NLL (↓)012345# of Bezier θ0.020.030.04ECE (↓)012345# of Bezier θ0.330.340.35BS (↓)DE 2DE 2 + n BezierUnder review as a conference paper at ICLR 2023\n\nFigure 3: Bar plots in the third column depict whether the bridge network (orange) outputs the same logit values as the base model with the Bezier parameters (blue) for given test inputs displayed in the first column. We also depict the predicted logits from the base model with θ1 and θ2 in the second and fourth columns, respectively. Additional results are available in Fig. 6.\n\nTable 1: R2 scores quantify how similar the following models to the target function defined with Bezier parameters θ(be) 1,2 (0.5) are in output probabilities; ‘Type I/II Bridge’, ‘Other Type I/II Bridge’, and ‘Other Bezier’. Refer to the main text in Section 5.2 for a detailed description for each model. All values are measured on the test split of each dataset.\n\n(a) CIFAR-10\n\n(b) CIFAR-100\n\n(c) Tiny ImageNet\n\nModel\n\nR2 (↑)\n\nModel\n\nR2 (↑)\n\nModel\n\nR2 (↑)\n\nType I Bridge Other Type I Bridge\n\n0.910 ±0.003 0.885 ±0.003\n\nType I Bridge Other Type I Bridge\n\n0.784 ±0.005 0.741 ±0.005\n\nType I Bridge Other Type I Bridge\n\n0.746 ±0.006 0.728 ±0.007\n\nType II Bridge Other Type II Bridge\n\n0.924 ±0.002 0.895 ±0.002\n\nType II Bridge Other Type II Bridge\n\n0.814 ±0.002 0.752 ±0.003\n\nType II Bridge Other Type II Bridge\n\n0.765 ±0.003 0.732 ±0.005\n\nOther Bezier\n\n0.871 ±0.005\n\nOther Bezier\n\n0.726 ±0.003\n\nOther Bezier\n\n0.712 ±0.003\n\nmodel: ResNet-32×2 for CIFAR-10, ResNet-32×4 for CIFAR-100, ResNet-18 for Tiny ImageNet and ResNet-50 for ImageNet, where ×2 and ×4 respectively denotes doubling and quadrupling of the number of channels for convolutional layers. We construct bridge networks with CNNs with a residual path whose inference costs are relatively low compared to those of ResNet base models. For detailed training settings, including bridge network architectures or hyperparameter settings, please refer to Appendix A.2.\n\nBy changing the channel size of the convolutional layers in the bridge network, we can balance the trade-off between performance gains with computational costs. We check this trade-off in Table 2. We refer to a bridge network with less than 15% of floating-point operations (FLOPs) compared to the base model as Bridgesm(small version), and a bridge with more than 15% as Bridgemd(medium version).\n\nEfficiency metrics We choose FLOPs and the number of parameters (#Params) for efficiency evaluation as these metrics are commonly used to consider the efficiency (Dehghani et al., 2021). Because FLOPs and #Params of the base model are different for each dataset, we report the relative FLOPs and the relative #Params with respect to the corresponding base model instead for better comparison.\n\nUncertainty metrics As suggested by Ashukha et al. (2020), along with the classification accuracy (ACC), we report the calibrated versions of Negative Log-likelihood (NLL), Expected Calibration Error (ECE), and Brier Score (BS) as metrics for uncertainty evaluation. We also measure the Deep Ensemble Equivalent (DEE) score proposed in Ashukha et al. (2020), which shows the relative performance for DE in terms of NLL and roughly be interpreted as effective number of models for an ensemble. See Appendix A.3 for more details.\n\n6\n\nship−20246logitr=0r=0.5Bezier Bridger=1dogairplaneautobirdcatdeerdogfroghorseshiptruck−202468logitairplaneautobirdcatdeerdogfroghorseshiptruckBezier BridgeairplaneautobirdcatdeerdogfroghorseshiptruckUnder review as a conference paper at ICLR 2023\n\nTable 2: FLOPs, #Params, R2 scores, and ensemble performance metrics of various type II bridge network sizes on CIFAR-100. We use ResNet-32×4 as a base model and 3 blocks of CNN with a residual connection as bridge networks. The number after CNN indicates the number of channels. R2 scores are measured with respect to the target Bezier r = 0.5.\n\nBridge\n\nFLOPs (↓)\n\n#Params (↓) R2 (↑)\n\nACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nCNN 32 ch CNN 64 ch CNN 128 ch CNN 256 ch\n\n× 0.012 × 0.029 × 0.079 × 0.246\n\n× 0.009 × 0.022 × 0.060 × 0.188\n\n0.709 ± 0.004 0.758 ± 0.004 0.793 ± 0.003 0.814 ± 0.002\n\n75.62 ± 0.17 75.78 ± 0.30 75.98 ± 0.20 76.13 ± 0.14\n\n0.914 ± 0.005 0.901 ± 0.004 0.894 ± 0.003 0.890 ± 0.004\n\n0.013 ± 0.001 0.016 ± 0.002 0.021 ± 0.002 0.023 ± 0.003\n\n0.342 ± 0.002 0.338 ± 0.001 0.335 ± 0.001 0.334 ± 0.002\n\nTable 3: Performance improvement of the ensemble by adding type I bridges to the single base ResNet model on Tiny ImageNet dataset. FLOPs, #Params, and DEE metrics are measured with respect to the single base model. Bridgesm and Bridgemd denote the small and the medium versions of the bridge network based on their FLOPs.\n\nModel\n\nFLOPs (↓)\n\n#Params (↓) ACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nDEE (↑)\n\nResNet (DE-1)\n\n+ 1 Bridgesm + 2 Bridgesm + 3 Bridgesm\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd\n\nDE-2\n\n× 1.000\n\n× 1.088 × 1.176 × 1.264\n\n× 1.277 × 1.554 × 1.831\n\n× 2.000\n\n× 1.000\n\n63.42 ± 0.23\n\n1.618 ± 0.005\n\n0.037 ± 0.002\n\n0.485 ± 0.003\n\n1.000\n\n× 1.093 × 1.186 × 1.279\n\n× 1.290 × 1.580 × 1.870\n\n65.38 ± 0.09 65.55 ± 0.15 65.61 ± 0.10\n\n65.94 ± 0.15 66.59 ± 0.09 66.79 ± 0.11\n\n1.444 ± 0.005 1.405 ± 0.005 1.388 ± 0.003\n\n1.418 ± 0.003 1.372 ± 0.001 1.353 ± 0.001\n\n0.015 ± 0.001 0.013 ± 0.001 0.014 ± 0.002\n\n0.018 ± 0.002 0.016 ± 0.002 0.015 ± 0.001\n\n0.461 ± 0.001 0.456 ± 0.001 0.455 ± 0.000\n\n0.453 ± 0.001 0.445 ± 0.000 0.443 ± 0.000\n\n2.179 ± 0.110 2.750 ± 0.086 3.022 ± 0.079\n\n2.562 ± 0.056 3.437 ± 0.036 3.967 ± 0.043\n\n× 2.000\n\n66.21 ± 0.10\n\n1.456 ± 0.004\n\n0.022 ± 0.002\n\n0.450 ± 0.002\n\n2.000\n\n5.2 CORRESPONDENCE BETWEEN BRIDGE NETWORK AND BEZIER CURVE\n\nTo assess the quality of the prediction of bridge networks, we use a set of ensemble parameters {θ1, θ2, . . . , θm} and Bezier curves between them. If the bridge network H (0.5) predicts v1,2(0.5) well compared to the other baselines, we can confirm that there exists the correspondence between the bridge network and the Bezier curve. To this end, we measure the R2 score which quantifies how similar outputs of the following baselines to that of the target function fθ(be) 1,2 (0.5); (1) ‘Type I/II Bridge’ denote the bridge network imitating the function of θ(be) 1,2 (0.5), (2) ‘Other Type I/II Bridge’ denote the bridge network imitating the function of θ(be) i,j (0.5) for some (i, j) ̸= (1, 2), and (3) ‘Other Bezier’ denotes the base model with the parameters θ(be) i,j (0.5) for some (i, j) ̸= (1, 2).\n\n1,2\n\nTable 1 summarizes the results. Compared to the baselines (i.e., ‘Other Type I/II Bridge’ and ‘Other Bezier’), the bridge networks produce more similar outputs to the target outputs. The R2 values between the predictions and targets are significantly higher than those from the wrong targets, demonstrating that the bridge predictions indeed are approximating our target outputs of interest.\n\nFig. 3 visualizes whether the bridge network H (0.5) 1,2 (0.5). To be more 1,2 (0.5), and the bridge network H (0.5) specific, we visualize the predicted logits from θ1, θ2, θ(be) 1,2 , for two test examples of CIFAR-10. Indeed, the bridge network predicts well the logits from the Bezier parameter. Appendix B.1 provides additional examples which further verify this.\n\npredicts the logits from θ(be)\n\n1,2\n\nRelation between model size and regression result We measure the relation between the size of bridge networks and the goodness of fits of prediction measured by R2 scores. Table 2 shows that we can achieve decent R2 scores with a small number of parameters, and the prediction gets better as we increase the flexibility of our bridge network. Also, the results show that a higher R2 score leads to better ensemble results.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: The cost-performance plots of type I bridge(s) compared to DE on Tiny ImageNet. The x-axis denotes the relative FLOPs quantifying the inference cost of the model compared to a single base model, and the y-axis shows the corresponding predictive performance. On the basis of DE (black dashed line), the upper left position is preferable in ACC, and the lower left position is preferable in NLL, ECE, and BS.\n\nFigure 5: The cost-performance plots of type II bridge(s) compared to DE on Tiny ImageNet. Others are identical to Fig. 4 except that we extend the DE basis from DE-2 to DE-7 (black dashed lines).\n\n5.3 CLASSIFICATION WITH BRIDGE NETWORKS\n\n5.3.1 TYPE I BRIDGE NETWORKS\n\nSingle Model performances with type I bridge networks In situations where multiple forward passes are not allowed for inference, we can approximate an ensemble of a single base model and the ones from Bezier curves with type I bridge networks. The results are shown in Table 3. The results show that for Stochastic Gradient Descent (SGD) trained single base ResNet model, an ensemble with type I bridge networks improves the performance both in terms of accuracy and uncertainty estimation. Only adding one small type I bridge to the base model (ResNet + 1 Bridgesm) dramatically improves the accuracy (≈ ×1.701) and DEE (≈ ×2.179).\n\nUsing multiple type I bridge networks As type I bridge network requires features from only one mode of each curve for inference, we can use multiple type I bridge networks for a single base model without significantly increasing inference cost, as we mentioned at Eq. 8. Table 3 reports the performance gain of a single base model with increasing number of type I bridges. Each bridge approximates the models on different Bezier curves between a single mode and others (i.e., Bezier curves between modes A-B, A-C, and so on where A, B, and C are different modes.), not the models on a single Bezier curve. Adding more bridge networks introduces more diverse outputs to the ensembles. One can see that the performance continuously improves as the number of bridges increases, with low additional inference cost. Fig. 4 shows how much type I bridge networks efficiently increase the performances proportional to FLOPs.\n\n5.3.2 TYPE II BRIDGE NETWORKS\n\nPerformance Table 4 summarizes the classification results comparing DE, DE with Bezier curves, and DE with type II bridge networks. For the more experimental results including other datasets,\n\n8\n\n11.251.51.752Relative FLOPs64.065.066.067.0ACC (↑)11.251.51.752Relative FLOPs1.351.401.451.501.551.60NLL (↓)11.251.51.752Relative FLOPs0.0150.0200.0250.0300.035ECE (↓)11.251.51.752Relative FLOPs0.450.460.470.48BS (↓)DE-nResNet + n BridgesmResNet + n Bridgemd234567Relative FLOPs67.068.069.0ACC (↑)234567Relative FLOPs1.251.301.351.401.45NLL (↓)234567Relative FLOPs0.0130.0150.0180.0200.022ECE (↓)234567Relative FLOPs0.420.430.440.45BS (↓)DE-nDE-n + k BridgesmDE-n + k BridgemdUnder review as a conference paper at ICLR 2023\n\nTable 4: Performance improvement of the ensemble by adding type II bridges as members to existing DE ensembles on Tiny ImageNet dataset. FLOPs, #Params, and DEE metrics are measured with respect to corresponding DEs. Type II bridges consistently improve the accuracy and uncertainty metrics of the ensemble before saturation. Bridgesm and Bridgemd denote the small and the medium versions of the bridge network based on their FLOPs.\n\nModel\n\nDE-4\n\n+ 1 Bridgesm + 2 Bridgesm + 4 Bridgesm + 6 Bridgesm\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd + 4 Bridgemd\n\nDE-5\n\nFLOPs (↓)\n\n#Params (↓) ACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nDEE (↑)\n\n× 4.000\n\n× 4.125 × 4.250 × 4.500 × 4.750\n\n× 4.352 × 4.704 × 5.056 × 5.408\n\n× 5.000\n\n× 4.000\n\n68.16 ± 0.11\n\n1.352 ± 0.001\n\n0.016 ± 0.000\n\n0.427 ± 0.001\n\n4.000\n\n× 4.132 × 4.264 × 4.528 × 4.792\n\n× 4.367 × 4.734 × 5.101 × 5.468\n\n68.48 ± 0.07 68.67 ± 0.10 68.69 ± 0.18 68.58 ± 0.04\n\n68.68 ± 0.17 68.98 ± 0.14 69.01 ± 0.17 69.14 ± 0.13\n\n1.315 ± 0.002 1.297 ± 0.002 1.281 ± 0.002 1.276 ± 0.002\n\n1.309 ± 0.002 1.287 ± 0.002 1.275 ± 0.001 1.266 ± 0.001\n\n0.013 ± 0.001 0.015 ± 0.001 0.012 ± 0.002 0.011 ± 0.001\n\n0.016 ± 0.002 0.016 ± 0.001 0.015 ± 0.001 0.015 ± 0.001\n\n0.423 ± 0.000 0.422 ± 0.000 0.421 ± 0.000 0.422 ± 0.000\n\n0.422 ± 0.000 0.419 ± 0.000 0.417 ± 0.000 0.416 ± 0.000\n\n5.962 ± 0.127 7.239 ± 0.226 8.432 ± 0.383 8.768 ± 0.441\n\n6.377 ± 0.178 7.986 ± 0.347 8.892 ± 0.389 9.539 ± 0.481\n\n× 5.000\n\n68.54 ± 0.08\n\n1.329 ± 0.001\n\n0.018 ± 0.001\n\n0.422 ± 0.001\n\n5.000\n\nplease refer to Appendix B. From Table 4, one can see that with only sightly increase in the computational costs, the ensembles with bridge networks achieves almost DEE 1.962 ensemble gain for DE-4 case. This gain is not specific only for DE-4; the ensembles with type II bridge networks consistently improved predictive accuracy and uncertainty calibration with negligible increase in the inference costs. Fig. 5 shows how much our type II bridge network achieve high performance in the perspective of relative FLOPs.\n\nComputational cost We report FLOPs for inference on Table 4 to indicate how much relative computational costs are required for the competing models. Fig. 5 summarizes the tradeoff between FLOPs and performance in various metrics. As one can see from these results, our bridge networks could achieve remarkable gain in performance, so for some cases, adding bridge ensembles achieved performance gains larger than those might be achieved by adding entire ensemble members. For instance, in Tiny ImageNet experiments, DE-4 + 2 bridges was better than DE-5 (DEE ≈ ×7.239). Please refer to Appendix B for the full results including various DE size and other datasets.\n\n5.4 HOW MANY TYPE II BRIDGES ARE REQUIRED?\n\nFor an ensemble of m parameters, the number of pairs can be connected by Bezier curves is (cid:0)m (cid:1), which grows quadratically with m. In the previous experiment, we constructed Bezier curves and bridges for all possible pairs (which explains the large inference costs for Bezier ensembles), but in practice, we found that it is not necessary to use bridge networks for all of those pairs. As an example, we compare the performance of DE-4 + bridge ensembles with increasing number of bridges on Tiny ImageNet dataset. The results are summarized in Table 4. Just one bridge dramatically increases the performance, and the performance gain gradually saturates as we add more bridges. Notably, only one bridge would suffice to outperform DE-5 (DEE ≈ ×5.962).\n\n2\n\n6 CONCLUSION\n\nIn this paper, we proposed a novel framework for efficient ensembling that reduces inference costs of ensembles with a lightweight network called bridge networks. Bridge networks predict the neural network outputs corresponding to the parameters obtained from the Bezier curves connecting two ensemble parameters without actual forward passes through the network. Instead, they reuse features and outputs computed from the ensemble members and predict the outputs corresponding to Bezier parameters directly in function spaces. Using various image classification benchmarks, we demonstrate that we can train such bridge networks with simple CNNs with minimal inference costs, and bridge augmented ensembles could achieve significant gain both in terms of accuracy and uncertainty calibration.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nReproducibility statement Please refer to Appendix A for full experimental detail including datasets, models, evaluation metrics and computing resources.\n\nREFERENCES\n\nA. Ashukha, A. Lyzhov, D. Molchanov, and D. P. Vetrov. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. In International Conference on Learning Representations (ICLR), 2020. 6, 13\n\nG. W. Benton, W. J. Maddox, S. Lotfi, and A. G. Wilson. Loss surface simplexes for mode connecting volumes and fast ensembling. In Proceedings of The 38th International Conference on Machine Learning (ICML 2021), 2021. 5\n\nM. Dehghani, Y. Tay, A. Arnab, L. Beyer, and A. Vaswani. The efficiency misnomer. In International\n\nConference on Learning Representations (ICLR), 2021. 6, 13\n\nF. Draxler, K. Veschgini, M. Salmhofer, and F. Hamprecht. Essentially no barriers in neural network energy landscape. In Proceedings of The 35th International Conference on Machine Learning (ICML 2018), 2018. 1, 5\n\nS. Fort, H. Hu, and B. Lakshminarayanan. Deep ensembles: A loss landscape perspective.\n\narXiv:1912.02757, 2019. 1\n\nT. Garipov, P. Izmailov, D. Podoprikhin, D. Vetrov, and A. G. Wilson. Loss surfaces, mode connectivity, and fast ensembling of DNNs. In Advances in Neural Information Processing Systems 31 (NeurIPS 2018), 2018. 1, 2, 3, 5\n\nC. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In Proceedings of The 34th International Conference on Machine Learning (ICML 2017), 2017. 13\n\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings\n\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 5\n\nG. E. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network.\n\nIn Deep\n\nLearning and Representation Learning Workshop, NIPS 2014, 2015. 4, 20\n\nG. Huang, Y. Li, G. Pleiss, Z. Liu, J. E. Hopcroft, and K. Q. Weinberger. Snapshot ensembles: Train 1, get M for free. In International Conference on Learning Representations (ICLR), 2017. 1, 5\n\nS. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning (ICML 2015), 2015. 12\n\nP. Izmailov, W. J. Maddox, P. Kirichenko, T. Garipov, D. Vetrov, and A. G. Wilson. Subspace\n\ninference for bayesian deep learning. In Uncertainty in Artificial Intelligence, 2020. 5\n\nP. Izmailov, S. Vikram, M. D. Hoffman, and A. G. Wilson. What are bayesian neural network posteriors really like? In Proceedings of The 38th International Conference on Machine Learning (ICML 2021), 2021. 12\n\nA. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images, 2009. 12\n\nB. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017. 1, 5\n\nF.-F. Li, A. Karpathy, and J. Johnson. Tiny ImageNet. https://www.kaggle.com/c/\n\ntiny-imagenet, 2017. [Online; accessed 19-May-2022]. 12\n\nY. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. V. Dillon, B. Lakshminarayanan, and J. Snoek. Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems 32 (NeurIPS 2019), 2019. 5\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32. 2019. 13\n\nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, Imagenet large scale visual recognition challenge.\n\nM. Bernstein, A. C. Berg, and L. Fei-Fei. International Journal of Computer Vision (IJCV), 2015. 12\n\nS. Singh and S. Krishnan. Filter response normalization layer: Eliminating batch dependence in the training of deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 12\n\nF. Wenzel, K. Roth, B. S. Veeling, J. ́Swikatkowski, L. Tran, S. Mandt, J. Snoek, T. Salimans, R. Jenatton, and S. Nowozin. How good is the bayes posterior in deep neural networks really? In Proceedings of The 37th International Conference on Machine Learning (ICML 2020), 2020. 12\n\nA. G. Wilson and P. Izmailov. Bayesian deep learning and a probabilistic perspective of generaliza-\n\ntion. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), 2020. 1\n\nM. Wortsman, M. Horton, C. Guestrin, A. Farhadi, and M. Rastegari. Learning neural network subspaces. In Proceedings of The 38th International Conference on Machine Learning (ICML 2021), 2021. 5\n\nH. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization.\n\nIn International Conference on Learning Representations, 2018. 4\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA EXPERIMENTAL DETAILS\n\nA.1 FILTER RESPONSE NORMALIZATION\n\nThroughout experiments using convolutional neural networks, we use the Filter Response Normalization (FRN; Singh and Krishnan, 2020) instead of the Batch Normalization (BN; Ioffe and Szegedy, 2015) to avoid recomputation of BN statistics along the subspaces. Besides, FRN is fully made up of learned parameters and it does not utilize dependencies between training examples, thus, it gives us a more clear interpretation of the parameter space (Wenzel et al., 2020; Izmailov et al., 2021).\n\nA.2 DATASETS AND MODELS\n\nDataset We use CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), Tiny ImageNet (Li et al., 2017) and ImageNet (Russakovsky et al., 2015) datasets. We apply the data augmentation consisting of random cropping of 32 pixels with padding of 4 pixels and random horizontal flipping. We subtract per-channel means from input images and divide them by per-channel standard deviations.\n\nNetwork We use CNN with residual path similar to the ResNet block structure. To use the features of base models, we embed one or more features from different layers of base models.\n\nFor CIFAR-10 dataset, we use ResNet-32×2 as a base network which consists of 15 blocks and 32 layers with widen factor of 2, and we use CNN 3 blocks as type I and type II bridge networks. The bridge networks use the features z of the third to last block.\n\nFor CIFAR-100 dataset, we use ResNet-32×4 as a base network which is almost same as ResNet-32 with widen factor of 2, and we use CNN 3 blocks as type I and type II bridge networks. The bridge networks use the features z of the third to last block.\n\nFor Tiny ImageNet dataset, we use ResNet-18 as a base network which consists of 8 blocks and 18 layers, and we use CNN 2 blocks as a type I and type II bridge network. The bridge networks use the features z of the third to last and the second to last blocks.\n\nFor ImageNet dataset, we use ResNet-50 as a base network which consists of 17 blocks and 50 layers, and we use CNN 3 blocks as a type I and type II bridge network. The bridge networks use the features z of the third to last and the second to last blocks.\n\nOptimization We train base ResNet networks for 200 epochs with learning rate 0.1. We use the SGD optimizer with momentum 0.9 and adjust learning rate with simple cosine scheduler. We give weight decay 0.001 for CIFAR-10 dataset, 0.0005 for CIFAR-100 and Tiny ImageNet dataset, and 0.0001 for ImageNet dataset.\n\nRegularization We introduced two additional hyperparameters for training bridge models; 1) the regularization scale λ and 2) the mixup coefficient α. Since the training error of the base network is near zero for the family of residual networks on CIFAR-10/100, given a training input without any modification, the base network and the target network (the one on the Bezier curve) will produce almost identical outputs, so the bridge trained with them will just copy the outputs of the base network. To prevent this, we perturb the inputs via mixup, and regularize the bridge to produce outputs different from the ones computed from the base models. On the other hand, for the datasets such as ImageNet where the models fail to achieve near zero training errors, the base network and the target networks are already distinct enough, so we found that the bridge can be trained easily without such tricks (i.e., we used λ = 0.0 and α = 0.0). We search 0.0 ≤ λ ≤ 0.4 for the regularization scale λ. We use α = 0.4 for CIFAR-10/100 and Tiny ImageNet datasets. We do not use mixup(α = 0.0) for ImageNet dataset.\n\nA.3 EVALUATION\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nEfficiency metrics Dehghani et al. (2021) pointed out that there can be contradictions between commonly used metrics (e.g., FLOPs, the number of parameters, and speed) and suggested refraining from reporting results using just a single one. So, we present FLOPs and the number of parameters in the results.\n\nUncertainty metrics Let p(x) ∈ [0, 1]K be a predicted probabilities for a given input x, where p(k) denotes the kth element of the probability vector, i.e., p(k) is a predicted confidence on kth class. We have the following common metrics on the dataset D consists of inputs x and labels y:\n\n• Accuracy (ACC):\n\nACC(D) = E(x,y)∈D\n\n(cid:20)(cid:20)\n\ny = arg max\n\nk\n\n(cid:21)(cid:21)\n\np(k)(x)\n\n.\n\n• Negative log-likelihood (NLL):\n\nNLL(D) = E(x,y)∈D\n\n• Brier score (BS):\n\n(cid:104)\n\n− log p(y)(x)\n\n.\n\n(cid:105)\n\nBS(D) = E(x,y)∈D\n\n(cid:20)(cid:13) (cid:13) (cid:13)p(x) − y\n\n(cid:21)\n\n,\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n(11)\n\n(12)\n\n(13)\n\nwhere y denotes one-hot encoded version of the label y, i.e., y(y) = 1 and y(k) = 0 for k ̸= y.\n\n• Expected calibration error (ECE):\n\nECE(D, Nbin) =\n\nNbin(cid:88)\n\nb=1\n\nnb|δb| n1 + · · · + nNbin\n\n,\n\n(14)\n\nwhere Nbin is the number of bins, nb is the number of examples in the bth bin, and δb is the calibration error of the bth bin. Specifically, the bth bin consists of predictions having the maximum confidence values in [(b − 1)/K, b/K), and the calibration error denotes the difference between accuracy and averaged confidences. We fix Nbin = 15 in this paper.\n\nWe evaluate the calibrated metrics that compute the aforementioned metrics with the temperature scaling (Guo et al., 2017), as Ashukha et al. (2020) suggested. Specifically, (1) we first find the optimal temperature which minimizes the NLL over the validation examples, and (2) compute uncertainty metrics including NLL, BS, and ECE using temperature scaled predicted probabilities under the optimal temperature. Moreover, we evaluate the following Deep Ensemble Equivalent (DEE) score, which measure the relative performance for DE in terms of NLL,\n\nDEE(D) = min {m ≥ 0 | NLL(D) ≤ NLLDE-m(D)},\n\n(15)\n\nwhere NLLDE-m(D) denotes the NLL of DE-m on the dataset D. Here, we linearly interpolate NLLDE-m(D) values for m ∈ R and make the DEE score continuous.\n\nA.4 COMPUTING RESOURCES\n\nWe conduct Tiny ImageNet experiments on 8 TPUv2 and 8 TPUv3 cores, supported by TPU Research Cloud1 and the others on 8 RTX3090 cores. We attached code to the supplimentary material. We use PyTorch (Paszke et al., 2019) with BSD-style license. Visit PyTorch GitHub repository2 for more details.\n\n1https://sites.research.google/trc/about/ 2https://github.com/pytorch/pytorch/blob/master/LICENSE\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB ADDITIONAL EXPERIMENTS\n\nB.1 ADDITIONAL EXAMPLES\n\nWe visually inspect the logit regression of type II bridge network. Our bridge network very accurately predicts the logits of r = 0.5 from Bezier curve when the two base models (r = 0 and r = 1) gives similar output logits (deer, ship, and frog). When the base models are not confident on the samples (airplane, bird, cat and horse), the network recovers the scale of logits approximately. However it fails to predict some very difficult samples (truck and dog) when even the base models are very confused.\n\nB.2 FULL TYPE I AND TYPE II BRIDGE RESULTS\n\nWe report full experimental results for classification tasks; 1) Type I bridge network results in Table 5, Table 7, Table 9, and Table 11, 2) Type II bridge network results in Table 6, Table 8, Table 10 and Table 12.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Bar plots in the third column depict whether the bridge network (orange) outputs the same logit values as the base model with the Bezier parameters θ(be) 1,2 (0.5) (blue), for a given test inputs displayed in the first column. We also depicts the predicted logits from θ1 and θ2 in the second and fourth columns, respectively.\n\n15\n\ndeer−202468logitr=0r=0.5Bezier Bridger=1airplane−4−202468logitBezier Bridgetruck−2024logitBezier Bridgebird−202468logitBezier Bridgecat−202468logitBezier Bridgehorse−20246logitBezier Bridgeship−202468logitBezier Bridgefrog−202468logitBezier Bridgedogairplaneautobirdcatdeerdogfroghorseshiptruck−2024logitairplaneautobirdcatdeerdogfroghorseshiptruckBezier BridgeairplaneautobirdcatdeerdogfroghorseshiptruckUnder review as a conference paper at ICLR 2023\n\nTable 5: Full result of performance improvement of the ensemble by adding type I bridges on CIFAR-10 dataset. We use same settings as described in Table 3.\n\nModel\n\nFLOPs (↓)\n\n#Params (↓) ACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nDEE (↑)\n\nResNet (DE-1)\n\n+ 1 Bridgesm + 2 Bridgesm + 3 Bridgesm + 4 Bridgesm\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd + 4 Bridgemd\n\nDE-2\n\n× 1.000\n\n× 1.062 × 1.124 × 1.186 × 1.248\n\n× 1.205 × 1.410 × 1.615 × 1.820\n\n× 2.000\n\n× 1.000\n\n91.78 ± 0.10\n\n0.287 ± 0.001\n\n0.019 ± 0.001\n\n0.126 ± 0.001\n\n1.000\n\n× 1.048 × 1.096 × 1.144 × 1.192\n\n× 1.159 × 1.318 × 1.477 × 1.636\n\n92.07 ± 0.04 92.13 ± 0.06 92.13 ± 0.04 92.12 ± 0.03\n\n92.09 ± 0.05 92.17 ± 0.06 92.15 ± 0.04 92.14 ± 0.05\n\n0.254 ± 0.001 0.250 ± 0.001 0.250 ± 0.000 0.250 ± 0.000\n\n0.253 ± 0.001 0.249 ± 0.001 0.248 ± 0.001 0.247 ± 0.001\n\n0.009 ± 0.001 0.008 ± 0.001 0.009 ± 0.000 0.009 ± 0.001\n\n0.009 ± 0.000 0.009 ± 0.000 0.008 ± 0.001 0.009 ± 0.001\n\n0.119 ± 0.000 0.118 ± 0.000 0.118 ± 0.000 0.119 ± 0.000\n\n0.119 ± 0.000 0.118 ± 0.000 0.118 ± 0.000 0.118 ± 0.000\n\n1.606 ± 0.035 1.677 ± 0.033 1.692 ± 0.033 1.695 ± 0.041\n\n1.623 ± 0.034 1.705 ± 0.047 1.729 ± 0.048 1.736 ± 0.049\n\n× 2.000\n\n93.07 ± 0.11\n\n0.233 ± 0.003\n\n0.012 ± 0.001\n\n0.107 ± 0.001\n\n2.000\n\nTable 6: Full result of performance improvement of the ensemble by adding type II bridges on CIFAR-10 dataset. We use same settings as described in Table 4.\n\nModel\n\nDE-2\n\n+ 1 Bridgesm\n\n+ 1 Bridgemd\n\n+ 1 Bezier\n\nDE-3\n\n+ 1 Bridgesm + 2 Bridgesm + 3 Bridgesm\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd\n\n+ 3 Bezier\n\nDE-4\n\n+ 1 Bridgesm + 2 Bridgesm + 3 Bridgesm + 4 Bridgesm + 5 Bridgesm + 6 Bridgesm\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd + 4 Bridgemd + 5 Bridgemd + 6 Bridgemd\n\n+ 6 Bezier\n\nDE-5\n\nFLOPs (↓)\n\n#Params (↓) ACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nDEE (↑)\n\n× 2.000\n\n× 2.081\n\n× 2.247\n\n× 3.000\n\n× 3.000\n\n× 3.081 × 3.162 × 3.243\n\n× 3.247 × 3.494 × 3.741\n\n× 6.000\n\n× 4.000\n\n× 4.081 × 4.162 × 4.243 × 4.324 × 4.405 × 4.486\n\n× 4.247 × 4.494 × 4.741 × 4.988 × 5.235 × 5.482\n\n× 10.000\n\n× 5.000\n\n× 2.000\n\n93.07 ± 0.11\n\n0.233 ± 0.003\n\n0.012 ± 0.001\n\n0.107 ± 0.001\n\n2.000\n\n× 2.063\n\n× 2.192\n\n93.09 ± 0.09\n\n0.220 ± 0.003\n\n0.008 ± 0.001\n\n0.104 ± 0.001\n\n2.774 ± 0.084\n\n93.07 ± 0.07\n\n0.219 ± 0.003\n\n0.006 ± 0.001\n\n0.103 ± 0.001\n\n2.808 ± 0.105\n\n× 3.000\n\n93.09 ± 0.10\n\n0.227 ± 0.004\n\n0.009 ± 0.000\n\n0.104 ± 0.001\n\n2.357 ± 0.080\n\n× 3.000\n\n93.37 ± 0.03\n\n0.216 ± 0.004\n\n0.009 ± 0.001\n\n0.100 ± 0.001\n\n3.000\n\n× 3.063 × 3.126 × 3.189\n\n× 3.192 × 3.384 × 3.576\n\n93.46 ± 0.06 93.50 ± 0.04 93.50 ± 0.10\n\n93.45 ± 0.04 93.54 ± 0.02 93.53 ± 0.08\n\n0.206 ± 0.002 0.204 ± 0.002 0.203 ± 0.002\n\n0.206 ± 0.002 0.203 ± 0.002 0.202 ± 0.002\n\n0.007 ± 0.002 0.006 ± 0.001 0.006 ± 0.001\n\n0.007 ± 0.001 0.006 ± 0.002 0.006 ± 0.001\n\n0.098 ± 0.001 0.097 ± 0.001 0.097 ± 0.001\n\n0.097 ± 0.001 0.097 ± 0.001 0.097 ± 0.001\n\n3.880 ± 0.118 4.214 ± 0.143 4.384 ± 0.178\n\n3.918 ± 0.117 4.301 ± 0.157 4.539 ± 0.171\n\n× 6.000\n\n93.44 ± 0.10\n\n0.207 ± 0.003\n\n0.008 ± 0.001\n\n0.097 ± 0.001\n\n3.852 ± 0.214\n\n× 4.000\n\n93.59 ± 0.10\n\n0.205 ± 0.002\n\n0.010 ± 0.001\n\n0.096 ± 0.001\n\n4.000\n\n× 4.063 × 4.126 × 4.189 × 4.252 × 4.315 × 4.378\n\n× 4.192 × 4.384 × 4.576 × 4.768 × 4.960 × 5.152\n\n93.58 ± 0.06 93.64 ± 0.03 93.58 ± 0.04 93.64 ± 0.06 93.58 ± 0.04 93.60 ± 0.10\n\n93.57 ± 0.04 93.61 ± 0.05 93.64 ± 0.09 93.64 ± 0.10 93.58 ± 0.08 93.62 ± 0.05\n\n0.199 ± 0.002 0.197 ± 0.002 0.196 ± 0.002 0.196 ± 0.002 0.196 ± 0.002 0.196 ± 0.002\n\n0.198 ± 0.002 0.197 ± 0.002 0.196 ± 0.002 0.195 ± 0.002 0.195 ± 0.002 0.195 ± 0.002\n\n0.007 ± 0.001 0.006 ± 0.001 0.006 ± 0.002 0.007 ± 0.001 0.005 ± 0.001 0.005 ± 0.000\n\n0.007 ± 0.000 0.006 ± 0.001 0.006 ± 0.001 0.005 ± 0.001 0.005 ± 0.001 0.005 ± 0.000\n\n0.094 ± 0.001 0.094 ± 0.001 0.094 ± 0.001 0.094 ± 0.001 0.094 ± 0.001 0.094 ± 0.001\n\n0.094 ± 0.001 0.094 ± 0.001 0.094 ± 0.001 0.094 ± 0.001 0.094 ± 0.001 0.094 ± 0.001\n\n5.017 ± 0.088 5.333 ± 0.044 5.484 ± 0.090 5.590 ± 0.123 5.647 ± 0.165 5.607 ± 0.120\n\n5.055 ± 0.064 5.428 ± 0.069 5.610 ± 0.141 5.774 ± 0.183 5.849 ± 0.156 5.855 ± 0.114\n\n× 0.000\n\n93.63 ± 0.10\n\n0.197 ± 0.002\n\n0.005 ± 0.001\n\n0.094 ± 0.001\n\n5.322 ± 0.151\n\n× 5.000\n\n93.68 ± 0.10\n\n0.199 ± 0.002\n\n0.011 ± 0.001\n\n0.093 ± 0.001\n\n5.000\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: Full result of performance improvement of the ensemble by adding type I bridges on CIFAR-100 dataset. We use same settings as described in Table 3.\n\nModel\n\nFLOPs (↓)\n\n#Params (↓) ACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nDEE (↑)\n\nResNet (DE-1)\n\n+ 1 Bridgesm + 2 Bridgesm + 3 Bridgesm + 4 Bridgesm\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd + 4 Bridgemd\n\nDE-2\n\n× 1.000\n\n× 1.062 × 1.124 × 1.186 × 1.248\n\n× 1.211 × 1.422 × 1.633 × 1.844\n\n× 2.000\n\n× 1.000\n\n73.11 ± 0.10\n\n1.094 ± 0.008\n\n0.047 ± 0.003\n\n0.379 ± 0.002\n\n1.000\n\n× 1.047 × 1.094 × 1.141 × 1.188\n\n× 1.161 × 1.322 × 1.483 × 1.644\n\n73.94 ± 0.16 74.02 ± 0.05 74.00 ± 0.09 74.06 ± 0.06\n\n74.18 ± 0.11 74.41 ± 0.14 74.52 ± 0.07 74.54 ± 0.02\n\n0.981 ± 0.004 0.957 ± 0.004 0.948 ± 0.002 0.944 ± 0.001\n\n0.965 ± 0.002 0.940 ± 0.000 0.930 ± 0.001 0.925 ± 0.001\n\n0.022 ± 0.003 0.018 ± 0.002 0.017 ± 0.001 0.018 ± 0.001\n\n0.024 ± 0.001 0.020 ± 0.001 0.020 ± 0.002 0.021 ± 0.001\n\n0.363 ± 0.001 0.360 ± 0.001 0.360 ± 0.001 0.360 ± 0.000\n\n0.358 ± 0.001 0.353 ± 0.000 0.352 ± 0.000 0.352 ± 0.000\n\n1.802 ± 0.036 1.984 ± 0.064 2.078 ± 0.070 2.143 ± 0.062\n\n1.914 ± 0.046 2.228 ± 0.077 2.416 ± 0.055 2.517 ± 0.061\n\n× 2.000\n\n75.62 ± 0.17\n\n0.952 ± 0.005\n\n0.033 ± 0.002\n\n0.344 ± 0.002\n\n2.000\n\nTable 8: Full result of performance improvement of the ensemble by adding type II bridges on CIFAR-100 dataset. We use same settings as described in Table 4.\n\nModel\n\nDE-2\n\n+ 1 Bridgesm\n\n+ 1 Bridgemd\n\n+ 1 Bezier\n\nDE-3\n\n+ 1 Bridgesm + 2 Bridgesm + 3 Bridgesm\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd\n\n+ 3 Bezier\n\nDE-4\n\n+ 1 Bridgesm + 2 Bridgesm + 3 Bridgesm + 4 Bridgesm + 5 Bridgesm + 6 Bridgesm\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd + 4 Bridgemd + 5 Bridgemd + 6 Bridgemd\n\nFLOPs (↓)\n\n#Params (↓) ACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nDEE (↑)\n\n× 2.000\n\n× 2.079\n\n× 2.246\n\n× 3.000\n\n× 3.000\n\n× 3.079 × 3.158 × 3.237\n\n× 3.246 × 3.492 × 3.738\n\n× 6.000\n\n× 4.000\n\n× 4.079 × 4.158 × 4.237 × 4.316 × 4.395 × 4.474\n\n× 4.246 × 4.492 × 4.738 × 4.984 × 5.230 × 5.476\n\n× 2.000\n\n75.62 ± 0.17\n\n0.952 ± 0.005\n\n0.033 ± 0.002\n\n0.344 ± 0.002\n\n2.000\n\n× 2.060\n\n75.98 ± 0.20\n\n0.894 ± 0.003\n\n0.021 ± 0.002\n\n0.335 ± 0.001\n\n3.158 ± 0.061\n\n× 2.188\n\n76.13 ± 0.14\n\n0.890 ± 0.004\n\n0.023 ± 0.003\n\n0.334 ± 0.002\n\n3.290 ± 0.084\n\n× 3.000\n\n76.21 ± 0.10\n\n0.909 ± 0.002\n\n0.021 ± 0.003\n\n0.334 ± 0.001\n\n2.818 ± 0.034\n\n× 3.000\n\n76.58 ± 0.11\n\n0.899 ± 0.003\n\n0.025 ± 0.003\n\n0.329 ± 0.001\n\n3.000\n\n× 3.060 × 3.120 × 3.180\n\n× 3.188 × 3.376 × 3.564\n\n76.74 ± 0.09 76.82 ± 0.16 77.04 ± 0.12\n\n76.82 ± 0.13 77.06 ± 0.08 77.09 ± 0.04\n\n0.859 ± 0.003 0.844 ± 0.002 0.836 ± 0.003\n\n0.858 ± 0.003 0.841 ± 0.002 0.831 ± 0.002\n\n0.019 ± 0.002 0.016 ± 0.001 0.017 ± 0.002\n\n0.020 ± 0.002 0.020 ± 0.003 0.018 ± 0.002\n\n0.325 ± 0.001 0.323 ± 0.001 0.322 ± 0.001\n\n0.324 ± 0.001 0.321 ± 0.001 0.320 ± 0.001\n\n4.357 ± 0.107 5.100 ± 0.109 5.693 ± 0.089\n\n4.419 ± 0.095 5.333 ± 0.093 6.123 ± 0.098\n\n× 6.000\n\n77.37 ± 0.14\n\n0.840 ± 0.003\n\n0.017 ± 0.001\n\n0.317 ± 0.001\n\n5.407 ± 0.147\n\n× 4.000\n\n77.14 ± 0.16\n\n0.867 ± 0.001\n\n0.023 ± 0.001\n\n0.321 ± 0.001\n\n4.000\n\n× 4.060 × 4.120 × 4.180 × 4.240 × 4.300 × 4.360\n\n× 4.188 × 4.376 × 4.564 × 4.752 × 4.940 × 5.128\n\n77.29 ± 0.10 77.35 ± 0.06 77.34 ± 0.06 77.38 ± 0.05 77.33 ± 0.06 77.35 ± 0.02\n\n77.40 ± 0.14 77.34 ± 0.13 77.44 ± 0.11 77.41 ± 0.12 77.45 ± 0.12 77.41 ± 0.14\n\n0.838 ± 0.003 0.826 ± 0.002 0.820 ± 0.001 0.815 ± 0.002 0.812 ± 0.002 0.811 ± 0.002\n\n0.837 ± 0.002 0.824 ± 0.001 0.817 ± 0.001 0.811 ± 0.001 0.809 ± 0.002 0.806 ± 0.002\n\n0.019 ± 0.001 0.016 ± 0.001 0.015 ± 0.001 0.016 ± 0.001 0.014 ± 0.002 0.016 ± 0.001\n\n0.020 ± 0.001 0.016 ± 0.001 0.017 ± 0.002 0.016 ± 0.001 0.018 ± 0.002 0.018 ± 0.002\n\n0.318 ± 0.001 0.317 ± 0.001 0.317 ± 0.001 0.317 ± 0.001 0.317 ± 0.001 0.317 ± 0.001\n\n0.317 ± 0.001 0.316 ± 0.001 0.315 ± 0.001 0.315 ± 0.001 0.315 ± 0.001 0.315 ± 0.001\n\n5.553 ± 0.116 6.825 ± 0.122 7.675 ± 0.280 8.307 ± 0.331 8.695 ± 0.363 8.862 ± 0.381\n\n5.578 ± 0.079 7.015 ± 0.156 7.987 ± 0.347 8.814 ± 0.398 9.228 ± 0.529 9.563 ± 0.528\n\n+ 6 Bezier\n\n× 10.000\n\n× 10.000\n\n77.82 ± 0.06\n\n0.808 ± 0.002\n\n0.016 ± 0.002\n\n0.310 ± 0.001\n\n9.270 ± 0.386\n\nDE-5\n\n× 5.000\n\n× 5.000\n\n77.49 ± 0.12\n\n0.845 ± 0.001\n\n0.021 ± 0.001\n\n0.316 ± 0.000\n\n5.000\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: Full result of performance improvement of the ensemble by adding type I bridges on Tiny ImageNet dataset. We use same settings as described in Table 3.\n\nModel\n\nFLOPs (↓)\n\n#Params (↓) ACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nDEE (↑)\n\nResNet (DE-1)\n\n+ 1 Bridgesm + 2 Bridgesm + 3 Bridgesm + 4 Bridgesm\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd + 4 Bridgemd\n\nDE-2\n\n× 1.000\n\n× 1.088 × 1.176 × 1.264 × 1.352\n\n× 1.277 × 1.554 × 1.831 × 2.108\n\n× 2.000\n\n× 1.000\n\n63.42 ± 0.23\n\n1.618 ± 0.005\n\n0.037 ± 0.002\n\n0.485 ± 0.003\n\n1.000\n\n× 1.093 × 1.186 × 1.279 × 1.372\n\n× 1.290 × 1.580 × 1.870 × 2.160\n\n65.38 ± 0.09 65.55 ± 0.15 65.61 ± 0.10 65.68 ± 0.06\n\n65.94 ± 0.15 66.59 ± 0.09 66.79 ± 0.11 66.88 ± 0.15\n\n1.444 ± 0.005 1.405 ± 0.005 1.388 ± 0.003 1.380 ± 0.002\n\n1.418 ± 0.003 1.372 ± 0.001 1.353 ± 0.001 1.342 ± 0.001\n\n0.015 ± 0.001 0.013 ± 0.001 0.014 ± 0.002 0.012 ± 0.002\n\n0.018 ± 0.002 0.016 ± 0.002 0.015 ± 0.001 0.018 ± 0.001\n\n0.461 ± 0.001 0.456 ± 0.001 0.455 ± 0.000 0.454 ± 0.000\n\n0.453 ± 0.001 0.445 ± 0.000 0.443 ± 0.000 0.441 ± 0.000\n\n2.179 ± 0.110 2.750 ± 0.086 3.022 ± 0.079 3.233 ± 0.084\n\n2.562 ± 0.056 3.437 ± 0.036 3.967 ± 0.043 4.450 ± 0.062\n\n× 2.000\n\n66.21 ± 0.10\n\n1.456 ± 0.004\n\n0.022 ± 0.002\n\n0.450 ± 0.002\n\n2.000\n\nTable 10: Full result of performance improvement of the ensemble by adding type II bridges on Tiny ImageNet dataset. We use same settings as described in Table 4.\n\nModel\n\nDE-2\n\n+ 1 Bridgesm\n\n+ 1 Bridgemd\n\n+ 1 Bezier\n\nDE-3\n\n+ 1 Bridgesm + 2 Bridgesm + 3 Bridgesm\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd\n\n+ 3 Bezier\n\nDE-4\n\n+ 1 Bridgesm + 2 Bridgesm + 3 Bridgesm + 4 Bridgesm + 5 Bridgesm + 6 Bridgesm\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd + 4 Bridgemd + 5 Bridgemd + 6 Bridgemd\n\nFLOPs (↓)\n\n#Params (↓) ACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nDEE (↑)\n\n× 2.000\n\n× 2.125\n\n× 2.352\n\n× 3.000\n\n× 3.000\n\n× 3.125 × 3.250 × 3.375\n\n× 3.352 × 3.704 × 4.056\n\n× 6.000\n\n× 4.000\n\n× 4.125 × 4.250 × 4.375 × 4.500 × 4.625 × 4.750\n\n× 4.352 × 4.704 × 5.056 × 5.408 × 5.760 × 6.112\n\n× 2.000\n\n66.21 ± 0.10\n\n1.456 ± 0.004\n\n0.022 ± 0.002\n\n0.450 ± 0.002\n\n2.000\n\n× 2.132\n\n67.25 ± 0.26\n\n1.371 ± 0.001\n\n0.017 ± 0.002\n\n0.438 ± 0.001\n\n3.478 ± 0.052\n\n× 2.367\n\n67.57 ± 0.13\n\n1.355 ± 0.003\n\n0.016 ± 0.002\n\n0.434 ± 0.001\n\n3.904 ± 0.097\n\n× 3.000\n\n67.43 ± 0.04\n\n1.385 ± 0.004\n\n0.017 ± 0.002\n\n0.436 ± 0.001\n\n3.108 ± 0.121\n\n× 3.000\n\n67.56 ± 0.08\n\n1.388 ± 0.001\n\n0.019 ± 0.001\n\n0.435 ± 0.001\n\n3.000\n\n× 3.132 × 3.264 × 3.396\n\n× 3.367 × 3.734 × 4.101\n\n67.98 ± 0.09 68.21 ± 0.21 68.29 ± 0.25\n\n68.29 ± 0.14 68.43 ± 0.15 68.57 ± 0.17\n\n1.336 ± 0.002 1.314 ± 0.002 1.303 ± 0.002\n\n1.327 ± 0.002 1.300 ± 0.002 1.285 ± 0.001\n\n0.017 ± 0.002 0.015 ± 0.003 0.014 ± 0.003\n\n0.016 ± 0.000 0.014 ± 0.001 0.014 ± 0.000\n\n0.429 ± 0.001 0.427 ± 0.001 0.426 ± 0.000\n\n0.426 ± 0.000 0.423 ± 0.000 0.421 ± 0.001\n\n4.702 ± 0.079 6.015 ± 0.139 6.822 ± 0.191\n\n5.159 ± 0.189 7.048 ± 0.250 8.110 ± 0.312\n\n× 6.000\n\n68.72 ± 0.21\n\n1.307 ± 0.002\n\n0.016 ± 0.002\n\n0.420 ± 0.001\n\n6.464 ± 0.140\n\n× 4.000\n\n68.16 ± 0.11\n\n1.352 ± 0.001\n\n0.016 ± 0.000\n\n0.427 ± 0.001\n\n4.000\n\n× 4.132 × 4.264 × 4.396 × 4.528 × 4.660 × 4.792\n\n× 4.367 × 4.734 × 5.101 × 5.468 × 5.835 × 6.202\n\n68.48 ± 0.07 68.67 ± 0.10 68.66 ± 0.09 68.69 ± 0.18 68.63 ± 0.11 68.58 ± 0.04\n\n68.68 ± 0.17 68.98 ± 0.14 69.01 ± 0.17 69.14 ± 0.13 69.12 ± 0.12 69.15 ± 0.19\n\n1.315 ± 0.002 1.297 ± 0.002 1.287 ± 0.001 1.281 ± 0.002 1.279 ± 0.002 1.276 ± 0.002\n\n1.309 ± 0.002 1.287 ± 0.002 1.275 ± 0.001 1.266 ± 0.001 1.261 ± 0.001 1.257 ± 0.000\n\n0.013 ± 0.001 0.015 ± 0.001 0.012 ± 0.001 0.012 ± 0.002 0.013 ± 0.001 0.011 ± 0.001\n\n0.016 ± 0.002 0.016 ± 0.001 0.015 ± 0.001 0.015 ± 0.001 0.014 ± 0.001 0.015 ± 0.000\n\n0.423 ± 0.000 0.422 ± 0.000 0.421 ± 0.000 0.421 ± 0.000 0.422 ± 0.000 0.422 ± 0.000\n\n0.422 ± 0.000 0.419 ± 0.000 0.417 ± 0.000 0.416 ± 0.000 0.416 ± 0.000 0.416 ± 0.000\n\n5.962 ± 0.127 7.239 ± 0.226 7.970 ± 0.298 8.432 ± 0.383 8.596 ± 0.431 8.768 ± 0.441\n\n6.377 ± 0.178 7.986 ± 0.347 8.892 ± 0.389 9.539 ± 0.481 9.916 ± 0.516 10.198 ± 0.493\n\n+ 6 Bezier\n\n× 10.000\n\n× 10.000\n\n69.26 ± 0.07\n\n1.271 ± 0.002\n\n0.016 ± 0.001\n\n0.414 ± 0.000\n\n9.118 ± 0.383\n\nDE-5\n\n× 5.000\n\n× 5.000\n\n68.54 ± 0.08\n\n1.329 ± 0.001\n\n0.018 ± 0.001\n\n0.422 ± 0.001\n\n5.000\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 11: Full result of performance improvement of the ensemble by adding type I bridges on ImageNet dataset. We use same settings as described in Table 3.\n\nModel\n\nFLOPs (↓)\n\n#Params (↓) ACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nDEE (↑)\n\nResNet (DE-1)\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd + 4 Bridgemd\n\nDE-2\n\n× 1.000\n\n× 1.194 × 1.388 × 1.582 × 1.776\n\n× 2.000\n\n× 1.000\n\n75.85 ± 0.06\n\n1.618 ± 0.005\n\n0.037 ± 0.002\n\n0.485 ± 0.003\n\n1.000\n\n× 1.222 × 1.444 × 1.666 × 1.888\n\n76.57 ± 0.02 76.74 ± 0.05 76.85 ± 0.05 76.96 ± 0.03\n\n1.418 ± 0.003 1.372 ± 0.001 1.353 ± 0.001 1.342 ± 0.001\n\n0.018 ± 0.002 0.016 ± 0.002 0.015 ± 0.001 0.018 ± 0.001\n\n0.453 ± 0.001 0.445 ± 0.000 0.443 ± 0.000 0.441 ± 0.000\n\n2.562 ± 0.056 3.437 ± 0.036 3.967 ± 0.043 4.450 ± 0.062\n\n× 2.000\n\n77.20 ± 0.07\n\n1.456 ± 0.004\n\n0.022 ± 0.002\n\n0.450 ± 0.002\n\n2.000\n\nTable 12: Full result of performance improvement of the ensemble by adding type II bridges on ImageNet dataset. We use same settings as described in Table 4.\n\nModel\n\nDE-2\n\n+ 1 Bridgemd\n\n+ 1 Bezier\n\nDE-3\n\n+ 1 Bridgemd + 2 Bridgemd + 3 Bridgemd\n\n+ 3 Bezier\n\nDE-4\n\nFLOPs (↓)\n\n#Params (↓) ACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nDEE (↑)\n\n× 2.000\n\n× 2.243\n\n× 3.000\n\n× 3.000\n\n× 3.243 × 3.486 × 3.729\n\n× 6.000\n\n× 4.000\n\n× 2.000\n\n77.20 ± 0.07\n\n0.880 ± 0.002\n\n0.013 ± 0.001\n\n0.317 ± 0.001\n\n2.000\n\n× 2.256\n\n77.43 ± 0.05\n\n0.870 ± 0.001\n\n0.012 ± 0.000\n\n0.314 ± 0.000\n\n2.564 ± 0.046\n\n× 3.000\n\n77.65 ± 0.08\n\n0.861 ± 0.001\n\n0.011 ± 0.001\n\n0.311 ± 0.001\n\n3.059 ± 0.082\n\n× 3.000\n\n77.64 ± 0.04\n\n0.862 ± 0.001\n\n0.013 ± 0.001\n\n0.311 ± 0.000\n\n3.000\n\n× 3.256 × 3.512 × 3.768\n\n77.76 ± 0.07 77.82 ± 0.07 77.92 ± 0.06\n\n0.856 ± 0.001 0.853 ± 0.000 0.851 ± 0.001\n\n0.012 ± 0.001 0.012 ± 0.001 0.012 ± 0.001\n\n0.310 ± 0.000 0.309 ± 0.000 0.308 ± 0.000\n\n3.559 ± 0.038 3.850 ± 0.069 4.010 ± 0.063\n\n× 6.000\n\n78.30 ± 0.05\n\n0.834 ± 0.001\n\n0.012 ± 0.000\n\n0.303 ± 0.001\n\n7.821 ± 0.391\n\n× 4.000\n\n77.87 ± 0.04\n\n0.851 ± 0.001\n\n0.012 ± 0.001\n\n0.308 ± 0.000\n\n4.000\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nTable 13: FLOPs, R2 scores, and model performance metrics between type I bridge network and CNN(ft) ×n + CNN models with various sizes on CIFAR-100. Here, ×n denotes the number of convolution layers to use as a frontal feature extractor CNN(ft), and CNN denotes the same architecture used in the type I bridge network. R2 scores are measured with respect to the target Bezier r = 0.5.\n\nModel\n\nFLOPs (↓) R2 (↑)\n\nACC (↑)\n\nNLL (↓)\n\nECE (↓)\n\nBS (↓)\n\nType I Bridge\n\n× 0.211\n\n0.786 ±0.003\n\n72.18 ±0.19\n\n1.016 ±0.004\n\n0.031 ±0.002\n\n0.379 ±0.001\n\nCNN(ft) ×2 + CNN CNN(ft) ×4 + CNN CNN(ft) ×6 + CNN CNN(ft) ×8 + CNN\n\n× 0.230 × 0.373 × 0.515 × 0.648\n\n0.626 ±0.007 0.682 ±0.005 0.685 ±0.006 0.701 ±0.001\n\n63.45 ±0.71 67.39 ±0.42 67.43 ±0.55 68.36 ±0.35\n\n1.318 ±0.019 1.166 ±0.015 1.156 ±0.019 1.121 ±0.009\n\n0.015 ±0.002 0.013 ±0.001 0.014 ±0.001 0.012 ±0.002\n\n0.481 ±0.007 0.436 ±0.004 0.433 ±0.006 0.422 ±0.004\n\nB.3 COMPARISON WITH THE TYPICAL KNOWLEDGE DISTILLATION\n\nWe note that mimicking the original function defined by deep neural networks using relatively cheaper networks reminds of Knowledge Distillation (KD) (Hinton et al., 2015), and thus one can think of the proposed approach as a special instance of the knowledge distillation. However, the proposed bridge network differs fundamentally from KD in that; 1) it uses a very small network that cannot be properly trained with a typical distillation procedure, and 2) while KD builds a student mapping input to the output, ours reuses outputs from the models related to the target function to be mimicked, and this actually plays a key role in the function matching.\n\nHere, we empirically validate the claim. Specifically, Table 13 compares bridge networks mimicking output probabilities from θ(be) 1,2 1) when it takes inputs x as in the typical knowledge distillation framework, and 2) when it takes outputs from θ1 and θ2 as we proposed. The former consistently underperforms compared to the latter, even if we introduce some frontal convolutional layers for dealing with image inputs. It indicates that the typical knowledge distillation procedure suffers from an insufficient capacity of the bridge network, while our proposed method does not. Consequently, our proposed method, which reuses informative outputs from θ1 and θ2, is distinct from the typical knowledge distillation when the capacity of the bridge network is limited.\n\n20",
    "reference": "# Summary Of The Paper\n\nThis paper aims to address an important drawback of Deep Ensemble, the inference cost of executing multiple models. The intuition behind the proposed method is that the outputs in the function subspace can be estimated from the modes without having to forward the actual parameters on the subspace. Based on this intuition, an additional lightweight network is trained as a bridge network to predict the outputs from the connecting subspace.\n\n# Strength And Weaknesses\n\n### Strength \n\n* Reducing the inference cost of DE is an important topic. This paper is well-motivated. \n* The presentation of this paper is clear and easy to follow. \n* Extensive experiments demonstrate the effectiveness of the proposed two types of bridge networks on TinyImageNet and Cifar. \n\n### Weaknesses\n* In Table 4, why do more bridge models lead to better results? \n* What's the functional difference between type I and II bridge models according to theoretical and empirical results? Is there any conclusion we can reach about how to choose types I and II?\n* Are there any important/sensitive hyper-parameters during the training of bridge models? How to determine the optimal number of bridge models in practice? \n* I am curious about how the proposed methods perform on large-scale datasets like ImageNet.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nsee above\n\n# Summary Of The Review\n\nI have some concerns about \n1) fundamental difference between type I and II bridge models\n2) Why number of bridge models impact \n3) generalization of this method to the large-scale datasets\n\nI am willing to raise my score if the above concerns are addressed.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLANGUAGE MODELING USING TENSOR TRAINS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nTensor networks have previously been shown to have potential in language modeling in theory but lack practical evidence support. We propose a novel Tensor Train Language Model (TTLM) based on Tensor-Train decomposition. To show the usefulness of TTLM, we perform a principled experimental evaluation on real-world language modeling tasks, showing that our proposed variants, TTLM-Large and TTLM-Tiny, can be more effective than Vanilla RNNs with low-scale of hidden sizes. Also, we demonstrate the relationship between TTLM and Second-order Recurrent Neural Networks (RNNs), Recurrent Arithmetic Circuits, and Multiplicative Integration RNNs in the sense that the architectures of all of these are, essentially, special cases of that of TTLM.1\n\n1\n\nINTRODUCTION\n\nA language model assigns probabilities of sequences of words from the vocabulary V ; the number of texts increases exponentially w.r.t to length N . Hence the domain of a language model is, by definition, the exponential space VN . However, due to the vanilla exponential space being intractable, existing work tends to use recurrent or auto-regressive architectures to generate conditional probabilities based on the context (typically encapsulated as a fixed-length dense vector). This indeed simplifies the calculation.\n\nRecently, researchers (Pestun & Vlassopoulos, 2017; Miller et al., 2021; Zhang et al., 2019) have reconsidered the view of language models as joint probabilities of text, as it leads to exponential representations in tensor space. Word connections could be preserved in the exponential tensor space when measuring joint probabilities. To deal with the exponential space complexity, a mathematical tool called ‘tensor network’ 2 has been used to reduce the exponential space of language modeling to a tractable one (Pestun & Vlassopoulos, 2017). However, the so-called ‘tensor network language model’ in Pestun & Vlassopoulos (2017) is currently a concept that needs to be proved practically.\n\nAs proof-of-concept work, we derive a Tensor Train Language Model (TTLM) (the simplest tensor network). Technically, we represent a sentence based on the exponential semantic space constructed by the tensor product of word representations. The probability of the sentence is obtained by the inner product of two high-dimensional tensors: the input Φ(X) and the global coefficients A.\n\nUnder the framework of TTLM, we propose two variants: TTLM-Tiny and TTLM-Large. Also, we clarify the relationship between the proposed TTLM and a series of Recurrent Neural Networks (RNNs) (i.e., Second-order RNNs (Goudreau et al., 1994), Recurrent Arithmetic Circuits (RACs) (Levine et al., 2018), and Multiplicative Integration RNNs (MI-RNNs) (Wu et al., 2016)). These connections open a new eye to understanding RNNs and give some natural implementations for TTLM.\n\nWe benchmark these TTLM variants and analyze the difference in their working mechanism and behaviors. Experimental results on language modeling tasks show that our TTLM variants could outperform than Vanilla-RNNs under the same training setting. These demonstrate the feasibility of TTLM.\n\nThe main contributions of our work can be summarized as follows:\n\n1The code is available at https://github.com/tensortrainlm/tensortrainlm 2Tensor networks are, roughly, decompositions of large tensors into sets of smaller tensors and have been employed in physics, mathematics, and machine learning (Sun et al., 2020; Novikov et al., 2015; Cohen et al., 2016; Stoudenmire & Schwab, 2016b; Cheng et al., 2019; Novikov et al., 2016; Selvan & Dam, 2020).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n1. We propose a novel Tensor Train Language Model, as a first attempt to apply tensor net-\n\nworks on real-world language modeling tasks.\n\n2. We propose two novel TTLM variants, TTLM-Large and TTLM-Tiny, and theoretically\n\ndemonstrate the relationship between TTLM and a series of existing RNNs.\n\n3. Compared to Vanilla-RNNs on WikiText-2 and PTB datasets, TTLM-Large reduces perplexity by 14.3 and 16.0, respectively, and TTLM-Tiny reduces perplexity by 1.7 and 8.5, respectively.\n\n2 RELATED WORK\n\nPrevious studies on tensor networks in machine learning have mainly been devoted to analyzing the theoretical properties of neural networks. A better understanding of feed-forward, convolutional and recurrent architectures has been gained, including compression parameters (Novikov et al., 2015), expressive power (Cohen et al., 2016; Cohen & Shashua, 2016; Khrulkov et al., 2018), and depth efficiency for long-term memory (Levine et al., 2018).\n\nFocusing on natural language modeling, certain studies have tensorized existing network architectures (Novikov et al., 2015), while few studies have applied tensor networks alone as a language model. To the best of our knowledge, tensor network language models have remained a theoretical proposal instead of an empirical model (Pestun & Vlassopoulos, 2017; Pestun et al., 2017). Perhaps, Miller et al. (2021) is the only other work that uses tensor networks for probabilistic sequence modeling, while it fails to scale up its model for real-world sequence modeling tasks. We first derive a tensor network language model in the way that it can be applied to real-world language modeling datasets, while its variants outperform Vanilla RNNs with lower-scale hidden sizes.\n\n3 PRELIMINARIES\n\nWe briefly recapitulate basic notions and notations 3; full technical introductions can be found in standard textbooks (e.g., Bi et al. (2022); Itskov (2009)).\n\nNotation. For the purposes of this paper, every tensor A is a multidimensional array of elements (called components) of R, each denoted by its integer coordinates in the array; e.g., for a twodimensional array, the component at position i, j ∈ N is denoted Aij. The order of a tensor is how many indices it has (e.g., a vector v is a first-order tensor, a matrix M is a second-order tensor, etc.). The dimension of a tensor refers to the number of values that a particular index (or so-called mode) can take, e.g., the dimension of B ∈ RI1×I2×I3 is I1 × I2 × I3.\n\nTensor product. For two tensors C ∈ RI1×···×Ij (order j) and D ∈ RIj+1,×···×Ij+k (order k), their tensor product is denoted by ⊗ and return a tensor Ei1···ij+k = Ci1...ij · Dij+1···ij+k (order j + k).\n\nGeneralized inner product. For two tensor X, Y ∈ RI1×I2×···×IN of the same size, their inner product is defined as ⟨X, Y⟩ = (cid:80)I1 i2=1 · · · (cid:80)IN iN =1 Xi1,i2,...,iN Yi1,i2,...,iN . For two tensors X ∈ RI1×I2×···×IN ×Ix and Y ∈ RI1×I2×···IN ×Iy sharing N modes of the same size, the “generalized inner product” defined in (Kossaifi et al., 2020) is calculated as\n\n(cid:80)I2\n\ni1=1\n\n⟨X, Y⟩N =\n\nI1(cid:88)\n\nI2(cid:88)\n\nIN(cid:88)\n\n· · ·\n\ni1=1\n\ni2=1\n\niN =1\n\nXi1,i2,...,iN Yi1,i2,...,iN\n\nwith ⟨X, Y⟩N ∈ RIx×Iy .\n\n4 LANGUAGE MODELING USING TENSOR TRAINS\n\nWe introduce a language model in tensor space in Sec. 4.1. We define our general Tensor Train Language Model in Sec. 4.2, and its special case, TTLM, in Sec. 4.3.\n\n3Most of the notations here follow the textbook Deep Learning Goodfellow et al. (2016).\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: A quick introduction to tensor diagram notation. There are two rules of tensor diagrams. (1) tensors are notated by solid shapes with a number of ’legs’ corresponding to their indices. (2) connecting two index lines implies a contraction or summation over the connected indices. In this paper, we augment our equations with these diagrams to make them easier to visualize.\n\n4.1 LANGUAGE MODELS IN A TENSOR SPACE\n\nNatural language typically has complex dependencies between features (e.g., tokens or words) (Hou et al., 2013)4 that are not captured well by standard methods such as feature concatenation. One could also see a similar interaction between any arbitrary features in factorization machines (Rendle, 2010). Suppose a given text consists of N words X = [x(1), x(2), · · · , x(N )] and let fi ∈ RIi be a feature extractor (it can be one-hot encoding or word embedding). We now define a representation of X designed to capture these dependencies:\n\nΦ(X) = f1(x(1)) ⊗ f2(x(2)) · · · ⊗ fN (x(N ))\n\n=\n\nN (cid:79)\n\ni=1\n\nfi(x(i))\n\n(1)\n\nwhere the tensor space is RI1 ⊗ RI2 ⊗ · · · ⊗ RIN . Each component of fi represents independent meaning-bearing units, such as morphemes or latent factors. For simplicity, we assume that a text shares the same one-hot encoding f (x(t)) ∈ R|V | in later sections. Consequently, Φ(X) is a |V |N - dimensional tensor that records all possible combinations of words in X.\n\nInspired by Kossaifi et al. (2020); Zhang et al. (2019), we define a tensor regression model to obtain the estimated probability for each text X:\n\np(X) = ⟨A, Φ(X)⟩\n\n|V | (cid:88)\n\n=\n\ni1,i2,··· ,iN =1\n\nAi1,··· ,iN · Φ(X)i1,··· ,iN\n\n(2)\n\nwhere ⟨·⟩ denotes the inner product of two same-sized tensors, and A is a regression weight tensor of the same shape as Φ(X) in the tensor space V⊗N = V ⊗ · · · ⊗ V where V refers to R|V |. Similar (cid:125)\n\n(cid:124)\n\nfunctions were considered in Novikov et al. (2016); Stoudenmire & Schwab (2016a); Khrulkov et al. (2018); Zhang et al. (2019).\n\n(cid:123)(cid:122) N\n\n4.2 GENERAL TENSOR TRAIN LANGUAGE MODEL\n\nSuppose the sequence of indices of words in the text X is w1, w2, · · · , wN , where wi ∈ {1, 2, · · · , |V |} and its corresponding weight in A is denoted as Aw1w2···wN . We use TT decomposition to represent Aw1w2···wN in the TT format (Oseledets, 2011) as follows:\n\n4Such dependencies (including collocation) have been viewed as an analogy of entanglement (Hou et al.,\n\n2013).\n\n3\n\nVectorMatrixOrder-3 tensori∈[I!]i∈[I!]j∈[I\"]a)=i∈[I!]j∈[I\"]k∈[I#]jkijkC!\"=##$!B!\"#v#Order-N tensorTi%∈[I%]i&∈[I&]i’(%∈[I’(%]i’∈[I’]b)vABBvCTensor ContractionUnder review as a conference paper at ICLR 2023\n\nFigure 2: a) Tensor Train Language Model based on Eq. 5. b) TT core of TTLM-Tiny. c) TT core of TTLM-Large. The dashed line in the square represents A, Φ(X), or G. Note that the only difference between TTLM-Large and TTLM-Tiny is whether to use tensor Weh.\n\nAw1w2...wN = G(1) :,w1 (cid:124) (cid:123)(cid:122) (cid:125) 1×R1\n\nG(2) :,w2,: (cid:124) (cid:123)(cid:122) (cid:125) R1×R2\n\n· · · G(N ) :,wN (cid:124) (cid:123)(cid:122) (cid:125) RN −1×1\n\nR1(cid:88)\n\nR2(cid:88)\n\n=\n\n· · ·\n\nRN −1 (cid:88)\n\nα1=1\n\nα2=2\n\nαN −1=1\n\nG(1)\n\nw1α1 G(2)\n\nα1w2α2 · · · G(N )\n\nαN −1wN\n\n(3)\n\nwhere the tensors G(t) ∈ RRt−1×|V |×Rt (t = 1, ..., d, R0 = RN = 1 by definition) are called TT cores. We now combine Φ(X) and A in the TT format to define general TTLM. The elements of G(t) Eq. 3 can be represented as:\n\n:,wt,: in\n\nG(t)\n\nαt−1wtαt =\n\n|V | (cid:88)\n\ni=1\n\nf (x(t))iG(t)\n\nαt−1iαt\n\n(4)\n\nwhere each f (x(t)) is a one-hot vector having wt = 1 for at most one t, and has zeros elsewhere. Therefore, one-hot encoding enables us to integrate the input data into the TT format of A by inserting Eq. 4 into Eq. 3:\n\nAw1w2...wN =\n\n|V | (cid:88)\n\nR1(cid:88)\n\nRN −1 (cid:88)\n\n· · ·\n\ni1,··· ,iN =1\n\nα1=1\n\nαN −1=1\n\nf (x(1))i1 G(1)\n\ni1α1\n\nf (x(2))i2G(2)\n\nα1i2α2\n\n· · · f (x(N ))iN G(N )\n\nαN −1iN\n\nR1(cid:88)\n\nR2(cid:88)\n\nRN −1 (cid:88)\n\n· · ·\n\n=\n\nα1=1\n\nα2=2\n\nαN −1=1\n\n= ⟨A, Φ(X)⟩ = p(X)\n\nG(1)\n\nw1α1G(2)\n\nα1w2α2\n\n· · · G(N )\n\nαN −1wN\n\n(5)\n\nwhere Φ(X) =\n\nN (cid:78)\n\ni=1\n\nf (x(i)). The difference between Eq. 5 and Eq. 3 is that Eq. 5 has combined A\n\nand Φ(X) in the low-dimensional form. This is because that Eq. 3 can compute the elements of A (Oseledets, 2011), and because Φ(X) here is the tensor product of one-hot vectors, so that Eq. 5 can compute Eq. 2. Further, since Eq. 5 now has input data (one-hot vectors) and weights (TT cores), we name Eq. 5 as our general TTLM.\n\n4.3 TTLM\n\nHere we consider a special class of general TTLM. Despite its site-dependent TT cores G(t) potentially giving it more expressiveness for language modeling, this property currently generates unnecessary obstacles to its applicability, like the choice of Rt. We here provide a detailed explanation of its special case: TTLM.\n\n4\n\n|V|RG(\")fx\"...ΦXA|V|GRGR|V|G($)fx%fx&a) TTLMRW’(|V|RW’(W()|V|RRRc) TTLM-Largeb) TTLM-TinyGGRRδW))W))RδRRRRUnder review as a conference paper at ICLR 2023\n\nto each other G = Definition. Suppose all the intermediate TT cores are equal G(2), . . . , G(N −1) ∈ RR×|V |×R and G(1) = G(N ) ∈ R|V |×R in Eq. 5. Then, TTLM is defined as follows:\n\n|V | (cid:88)\n\nR (cid:88)\n\np(X) =\n\ni1,··· ,iN =1\n\nα1,··· ,αN −1=1\n\nf (x(1))i1G(1)\n\ni1α1\n\nf (x(2))i2 Gα1i2α2 · · · f (x(N ))iN G(N )\n\nαN −1iN\n\nR (cid:88)\n\n=\n\nα1,··· ,αN −1=1\n\nG(1)\n\nw1α1 Gα1w2α2 · · · GαN −2wN −1αN −1G(N )\n\nαN −1wN\n\n(6)\n\nwhere its tensor diagram notation is shown in Figure 2a.\n\nRecursive information. We recursively unfold the calculation of TTLM in Eq. 6 and find that G has two sources of “input”: the information from the previous recursive unfolding, and the input data f (x(t)) (see Eq. 15 for a detailed version). From this perspective, G acts as a bilinear map G : R|V | × RR → RR, and we can regard the information in the previous step as a hidden state h(t)\n\nTTLM, given by:\n\nh(t)\n\nTTLM = f (x(t))T Gh(t−1)\n\nTTLM\n\n(7)\n\nwhere f (x(t)), G, and h(t−1) to R|V |×R×R which does not change the number of indices).\n\nTTLM are contracted together (we permute the indices of G from RR×|V |×R\n\nRecursive Probability Computation. Here, we here provide further details about the process of computing p(X) by TTLM in practice.\n\nIn language modeling, p(X) is often decomposed using the chain rule (Bahl et al., 1983) as follows:\n\np(X) =\n\nN (cid:89)\n\nt=1\n\np(x(t)|x(1:t−1))\n\nwhere x(1:t−1) denotes the text [x(1), x(2), · · · , x(t−1)]. At time t, the output prediction of a model, y(t) ∈ V, is a probability distribution of word x(t) given x(1:t−1).\n\nIn TTLM, we define y(t) as follows:\n\ny(t) = softmax\n\n(cid:16)\n\nG(t)h(t−1)\n\nTTLM\n\n(cid:17)\n\n(8)\n\nFigure 3: Recursive calculation of conditional probability in TTLM. Here we provide an example that given the text x(1:3), y(4) = softmax(G(4)h(3) TTLM) where y(4) ∈ V is the probability distribution of word x(4).\n\nwhere G(t) ∈ R|V |×R is the last TT core in TT format at time t. Such a definition is the same as that of RNNs, which use hidden states and a weight matrix to calculate word probabilities. Fig. 3 provides a simple example.\n\nWe can derive the definition of y(t) in high-dimensional space, if we substitute h(t−1) Eq. 6 and Eq. 7:\n\nTTLM in Eq. 8 by\n\ny(t) = softmax\n\n\n\n\n\n|V | (cid:88)\n\nR (cid:88)\n\nf (x(1))i1G(1)\n\ni1α1\n\nf (x(2))i2 Gα1i2α2 · · · G(t)\n\nαt−1\n\n (9)\n\n\n\ni1,··· ,it−1=1\n\nα1,··· ,αt−1=1\n\n(cid:32)\n\n= softmax\n\n⟨A(1:t)),\n\n(cid:33)\n\nf (x(i))⟩t−1\n\nt−1 (cid:79)\n\ni=1\n\n= softmax\n\n(cid:16)\n\n⟨A(1:t)), Φ(X (1:t−1))⟩t−1\n\n(cid:17)\n\n(10)\n\n(11)\n\nwhere A(1:t) ∈ V⊗t, Φ(X (1:t−1)) ∈ V⊗t−1 and ⟨·⟩t−1 denotes the ”generalized inner product defined” in Preliminary. Note that Eq. 9 is the low-dimensional form of Eq. 11, similarly to the relationship between Eq. 5 and Eq. 2.\n\n5\n\nG(\")f(x!)f(x\")ΦX($:&)A($:()fx\"fx$|V|RRRy(()|V|fx%GG(&)G|V||V|Under review as a conference paper at ICLR 2023\n\nBy these definitions, there are some interesting properties of TTLM. 1) We can use teacher forcing (Jurafsky, 2000) to learn parameters of TT cores. 2) The hidden-to-output tensor G(t) is defined to be the same as the input-to-hidden tensor G(1). 3) G and G(t) have no parameters in common. We provide a detailed explanation of the relationship between different TT cores in Appendix A.\n\n5 TTLM VARIANTS\n\nTo show the versatility and practical applicability of the TTLM framework, we now propose two new variants: TTLM-Large and TTLM-Tiny in Sec. 5.1. In Sec. 5.2, we briefly summarize the relationship between TTLM and some widely-used RNNs.\n\n5.1 NEW VARIANTS: TTLM-LARGE AND TTLM-TINY\n\nThe TT core G in TTLM is an entire third-order tensor. In the two variants, we decompose G into several separate tensors without violating the TT format, as shown in Fig. 2b and Fig. 2c. We define TTLM-Tiny and TTLM-Large as follows:\n\nh(t)\n\nTiny = f (x(t))T WxeδW hhh(t−1) Large = f (x(t))T WxeWehδW hhh(t−1)\n\nLarge\n\nTiny\n\nh(t)\n\n(12)\n\nwhere Wxe ∈ R|V |×R×R is the input-to-hidden tensor ; Weh ∈ RR×R×R×R; and δ ∈ RR×R×R×R is a fourth-order diagonal tensor such that δijkl = 1 iff the i = j = k = l, and δijkl = 0 otherwise. The relationship between our proposed models and TTLM is as follows: Wxe in both models take the same role as G(t) in TTLM (i.e. input-to-hidden and hidden-to-output), while G = WxeδW hh in TTLM-Tiny and G = WxeWehδW hh in TTLM-Large.\n\nAs in RNNs, we compute the conditional probability recursively for TTLM-Large and TTLM-Tiny as:\n\ny(t) = softmax(VPh(t)) (13) where V ∈ RR×|V |×R is an output embedding tensor, and P ∈ RR×R×R is a projector tensor. Then we tie the input tensor Wxe to the output embedding tensor V (we provide a detailed explanation in Appendix C).\n\nOne obvious advantage of our models is to utilize information from the hidden layer and input data separately. Such interaction, particularly TTLM-Tiny, can potentially avoid overfitting, similarly to Wu et al. (2016) where multiplication integration between two sources of ”input” can outperform many other methods. In Sec 6.2, we provide relevant experimental evidence.\n\n5.2 EXISTING TTLM VARIANTS\n\nGiven the fact that TT scores of TTLM can vary, Appendix B provides a detailed illustration that three existing models, namely second-order RNNs, RACs and MI-RNNs can be considered as one of the ”special” implementations of TTLM.\n\nWe briefly summarize the differences between the three models: 1) Second-order RNNs use the third-order T as the TT cores with an activation function given Eq. 14; 2) RACs use W hx ⊙ W hh as the TT cores given Eq. 18; 3) MI-RNNs use W hx ⊙ W hh as the TT cores with an activation function given Eq. 19.\n\nAlong with our two proposed models, we study the experimental performance of second-order RNNs, RACs and MI-RNNs compared to TTLM-Large and TTLM-Tiny in Section 6.\n\n6 EXPERIMENTAL EVALUATION\n\nTo further understand the properties of TTLM variants, we now investigate the effectiveness of TTLM-Large and TTLM-Tiny compared to Second-order RNNs, RACs, MI-RNNs, and VanillaRNNs. We conduct experiments from two distinct perspectives: (1) The rank of TT decomposition\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nRNN (Mikolov & Zweig, 2012) LSTM (Zaremba et al., 2014) LSTM (Grave et al., 2016) LSTM (Merity et al., 2017) Vanilla-RNNs* Second-order RNNs* RACs* MI-RNNs* TTLM TTLM-Tiny TTLM-Large\n\nWikiText-2\n\nParams -\n- -\n-\n\nPPL -\n- 99.3 100.9 11.6M 96.6 11.8M 96.0 11.6M 97.6 11.6M 99.6 12.2M 546.4 11.6M 94.9 11.8M 82.3\n\nPTB\n\nParams -\n- -\n-\n\nPPL 124.7 114.5 82.3 80.6 4.0M 115.3 4.2M 108.2 4.0M 116.8 4.0M 119.1 4.2M 559.8 4.0M 106.8 99.3 4.2M\n\nHidden (Rank) 300 200 1024 650 20 20 20 20 20 20 20\n\nLayer\n\nEmbed Size\n\n1 2\n1 2\n1 1\n1 1\n1 1\n1\n\n- -\n- -\n400 400 400 400 400 400 400\n\nTable 1: PPL evaluation on test set on WikiText-2 and PTB. Models tagged with ∗ indicate that they are re-implemented by ourselves. The symbol ”−” means these data are not available in their original paper. Params are the training parameters, the details are in Appendix C\n\nhas been proved to be the dimension of the hidden states of RNNs (Khrulkov et al., 2018). Here, we study the influence of rank on the effectiveness of our TTLM variants in Sec 6.2. (2) In Sec.6.3, we analyse the influence of non-linearity for TTLM variants.\n\n6.1 EXPERIMENTAL SETTING\n\nTasks, Datasets, and Metrics. We conduct experiments on word-level language model datasets: English Penn Treebank (PTB), which consists of 929k training words, 73k validation words, and 82k test words. It has 10k words in its vocabulary (Marcinkiewicz, 1994). The WikiText-2 dataset (Merity et al., 2016) is derived from Wikipedia articles and consists of 2088k training words, 217k validation words, 45k test words, and a vocabulary of over 30,000 words. We compare these models on the language modelling task, evaluated by the Perplexity (PPL) (Meister & Cotterell, 2021): the lower the perplexity, the better the model.\n\nBaselines. Vanilla RNNs, Second-RNNs, RACs and MI-RNNs are our baselines. We also provide some original results of RNN-based models as references (Mikolov & Zweig, 2012; Zaremba et al., 2014; Grave et al., 2016; Merity et al., 2017). The implementation details are introduced in Appendix C.\n\nHyperparameters. To compare the effectiveness of comparable models in the same scale: 1) We set the rank/hidden size of TTLM variants/Vanilla-RNNs as [5, 10, 20, 25, 30, 35, 40, 45, 50]. The embedding size of these models is the squared number of hidden sizes/ranks. 3) To avoid the impact of the large embedding size on the model performance, we also provide several common choices of embedding size in Vanilla-RNNs by setting its embedding size as [100, 200, 300] (we name them as RNNs-100, RNNs-200, RNNs-300 correspondingly and use them in Fig. 5). 4) The random seed is fixed to ensure the experimental results are not influenced by initializing the weights. 5) We train all models for 50 epochs and choose the best model in the validation set to predict the result in the test set.\n\n6.2 RANK AND EFFECTIVENESS ANALYSIS\n\nThe rank of the TT format has been used to explain the expressive power or long-term memory capacity of RNNs (Khrulkov et al., 2018; Levine et al., 2018). However, the relationship between rank and effectiveness in language modelling has yet to be shown practically. Later, we will evaluate the effectiveness of our models w.r.t rank.\n\nEffectiveness. (1) Fig. 4 shows the influence of the rank on our models based on the validation PPL. The validation PPL of TTLM-Large drops down at the early training step but easily increases when the rank increases. In contrast, the validation PPL of TTLM-Tiny stably decreases as the rank increases. (2) Compared to Vanilla-RNNs, the influence of the rank on our models based on the test\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Validation perplexity on TTLM-Large\n\n(b) Validation perplexity on TTLM-Tiny\n\nFigure 4: Rank analysis for the TTLM-Large and TTLM-Tiny on PTB.\n\nPPL is shown in Fig. 5. Our models outperform Vanilla-RNNs on all used parameter settings. (3) Table 1 provides a supplementary example to show the comparison of our models with baselines and some references. TTLM-Large and TTLM-Tiny are more effective than baselines.\n\nOverfitting. Based on Fig. 4a and Fig. 4b, we find that TTLM-Large is more prone to overfitting than TTLM-Tiny. As the only difference between the two models is Weh, this suggests that the simpler parameterization of the TT cores, the more easily the model avoids overfitting. This finding is consistent with the comparison between MI-RNNs and Second-order RNNs by Wu et al. (2016).\n\nLow-scale. Despite the effectiveness of our models under the current hyperparameter settings, Fig. 5 reveals their limited potential when the rank is larger than 40 where the test perplexity of Vanilla-RNNs still stably decreases. Therefore, we expect that our model can outperform vanilla RNNs with a low-scale of hidden size (i.e. range from 5 to 50), but not larger scales; this is a clear tradeoff of using the simple parameterization of TT cores as we do in TTLMs.\n\n6.3 NON-LINEARITY ANALYSIS.\n\nFigure 5: Comparison of test perplexity on PTB. RNNs here is Vanilla-RNNs, and its embedding size is the same as TTLM-Large and TTLM-Tiny. RNNs100, RNNs-200 and RNNs-300 are the RNNs with fixed embedding sizes of 100, 200 and 300, respectively.\n\nFig. 6 shows the effects of the tanh activation function on TT variants based on validation PPL. Regarding the speed of convergence, tanh speeds up TTLM-Large-tanh, TTLM-Tiny-tanh, MI-RNNs while barely influencing second-order RNNs. Regarding the magnitude of the lowest validation perplexity, tanh impairs the performance TTLM-Large and TTLM-Tiny, but has little influence on multiplicative integration and the third-tensor T in Second-order RNNs.\n\nThus, the influence of non-linearity on TTLM variants depends on TT cores settings, both for the convergence of validation PPL and the magnitude of the lowest validation PPL. Thus, from an experimental point of view, the effect of non-linearity functions on one TT variant cannot simply be transferred or analogized to another TT variant. This also suggests that one should be wary of the analogy between tensor decomposition and existing neural network models at the implementation level declared by previous research (Khrulkov et al., 2018; Levine et al., 2018). The activation function could be a factor to influence such an analogy.\n\n7 CONCLUSION\n\nWe first apply TT decomposition to real-world language modeling and name the framework as TTLM. We propose two variants: TTLM-Large and TTLM-Tiny, and show that they are more ef-\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) TTLM-Large v.s. TTLM-Large-tanh\n\n(b) TTLM-Tiny v.s. TTLM-Tiny-tanh\n\n(c) RACs v.s. MI-RNNs\n\n(d) Second-linear v.s. Second-order RNNs\n\nFigure 6: Comparison of influence of non-linearity on TTLM variants on PTB . The suffix -tanh refers to a model using the tanh activation function. Second-linear refers to Second-order RNNs without activation function.\n\nfective compared to Vanilla-RNNs with low-scale of hidden sizes. Meanwhile, we demonstrate that Second-order RNNs, RACs and MI-RNNs are special implementations of TTLM.\n\nREFERENCES\n\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer. A maximum likelihood approach to continuous speech recognition. IEEE transactions on pattern analysis and machine intelligence, (2): 179–190, 1983.\n\nYingyue Bi, Yingcong Lu, Zhen Long, Ce Zhu, and Yipeng Liu. Chapter 1 - tensor decompositions: computations, applications, and challenges. In Yipeng Liu (ed.), Tensors for Data Processing, pp. 1–30. Academic Press, 2022. ISBN 978-0-12-824447-0. doi: https://doi.org/10.1016/ B978-0-12-824447-0.00007-8. URL https://www.sciencedirect.com/science/ article/pii/B9780128244470000078.\n\nSong Cheng, Lei Wang, Tao Xiang, and Pan Zhang. Tree tensor networks for generative modeling.\n\nPhysical Review B, 99(15):155131, 2019.\n\nNadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions. 2016. doi: 10.48550/ARXIV.1603.00162. URL https://arxiv.org/abs/1603. 00162.\n\nNadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor\n\nanalysis. In Conference on learning theory, pp. 698–728. PMLR, 2016.\n\nIan Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.\n\nMIT Press, 2016.\n\nMark W Goudreau, C Lee Giles, Srimat T Chakradhar, and Dong Chen. First-order versus secondorder single-layer recurrent neural networks. IEEE Transactions on Neural Networks, 5(3):511– 513, 1994.\n\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a\n\ncontinuous cache. arXiv preprint arXiv:1612.04426, 2016.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nYuexian Hou, Xiaozhao Zhao, Dawei Song, and Wenjie Li. Mining pure high-order word associations via information geometry for information retrieval. ACM Transactions on Information Systems (TOIS), 31(3):1–32, 2013.\n\nMikhail Itskov. Tensor Algebra and Tensor Analysis for Engineers: With Applications to Continuum Mechanics. Springer Publishing Company, Incorporated, 2nd edition, 2009. ISBN 3540939067.\n\nDan Jurafsky. Speech & language processing. Pearson Education India, 2000.\n\nValentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural In International Conference on Learning Representations, 2018. URL https://\n\nnetworks. openreview.net/forum?id=S1WRibb0Z.\n\nJean Kossaifi, Zachary C Lipton, Arinbj ̈orn Kolbeinsson, Aran Khanna, Tommaso Furlanello, and Anima Anandkumar. Tensor regression networks. The Journal of Machine Learning Research, 21(1):4862–4882, 2020.\n\nYoav Levine, Or Sharir, and Amnon Shashua. Benefits of depth for long-term memory of recurrent\n\nnetworks. 2018.\n\nMary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Using\n\nLarge Corpora, 273, 1994.\n\nDiego Maupom ́e and Marie-Jean Meurs. Language modeling with a general second-order RNN. In Proceedings of the 12th Language Resources and Evaluation Conference, pp. 4749–4753, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-9554634-4. URL https://aclanthology.org/2020.lrec-1.584.\n\nClara Meister and Ryan Cotterell. Language model evaluation beyond perplexity.\n\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5328–5339, Online, August 2021. Association for Computational Linguistics.\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\n\nmodels. arXiv preprint arXiv:1609.07843, 2016.\n\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm lan-\n\nguage models. arXiv preprint arXiv:1708.02182, 2017.\n\nTomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model.\n\nIn 2012 IEEE Spoken Language Technology Workshop (SLT), pp. 234–239. IEEE, 2012.\n\nJacob Miller, Guillaume Rabusseau, and John Terilla. Tensor networks for probabilistic sequence modeling. In International Conference on Artificial Intelligence and Statistics, pp. 3079–3087. PMLR, 2021.\n\nAlexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural\n\nnetworks. Advances in neural information processing systems, 28, 2015.\n\nAlexander Novikov, Mikhail Trofimov, and Ivan Oseledets. Exponential machines. arXiv preprint\n\narXiv:1605.03795, 2016.\n\nIvan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):2295–\n\n2317, 2011.\n\nVasily Pestun and Yiannis Vlassopoulos.\n\nTensor network language model.\n\narXiv preprint\n\narXiv:1710.10248, 2017.\n\nVasily Pestun, John Terilla, and Yiannis Vlassopoulos. Language as a matrix product state. arXiv\n\npreprint arXiv:1711.01416, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nOfir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint\n\narXiv:1608.05859, 2016.\n\nGuillaume Rabusseau, Tianyu Li, and Doina Precup. Connecting weighted automata and recurrent neural networks through spectral learning. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1630–1639. PMLR, 2019.\n\nSteffen Rendle. Factorization machines. In 2010 IEEE International conference on data mining, pp.\n\n995–1000. IEEE, 2010.\n\nRaghavendra Selvan and Erik B Dam. Tensor networks for medical image classification. In Medical\n\nImaging with Deep Learning, pp. 721–732. PMLR, 2020.\n\nEdwin Stoudenmire\n\nand David J Schwab.\n\nIn D. D. Lee, M. Sugiyama, U. V. Luxburg,\n\nworks. nett Curran Associates, 6211-supervised-learning-with-tensor-networks.pdf.\n\nSupervised learning with tensor netand R. GarI. Guyon, Information Processing Systems 29, pp. 4799–4807. URL http://papers.nips.cc/paper/\n\n(eds.), Advances in Neural\n\n2016a.\n\nInc.,\n\nEdwin Stoudenmire and David J Schwab. Supervised learning with tensor networks. Advances in\n\nNeural Information Processing Systems, 29, 2016b.\n\nZheng-Zhi Sun, Cheng Peng, Ding Liu, Shi-Ju Ran, and Gang Su. Generative tensor network classification model for supervised machine learning. Physical Review B, 101(7):075135, 2020.\n\nIlya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural net-\n\nworks. In ICML, 2011.\n\nPaul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the\n\nIEEE, 78(10):1550–1560, 1990.\n\nRonald J Williams and Jing Peng. An efficient gradient-based algorithm for on-line training of\n\nrecurrent network trajectories. Neural computation, 2(4):490–501, 1990.\n\nYuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan Salakhutdinov. On Multiplicative Integration with Recurrent Neural Networks, November 2016. URL http://arxiv. org/abs/1606.06630. arXiv:1606.06630 [cs].\n\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\n\narXiv preprint arXiv:1409.2329, 2014.\n\nLipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, and Dawei Song. A generalized language model in tensor space. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 7450–7458, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA RELATIONSHIP BETWEEN TT CORES IN TTLM\n\nTo help readers understand the roles of TT cores in TTLM, we here provide a detailed calculation of the probability of a text X = [x(1), x(2), · · · , x(N )] by TTLM. Note that all the intermediate TT cores are equal to each other: G = G(2), ..., G(N −1) and G(1) = G(N ).\n\nthe conditional probability of x(t) given x(1:t−1)) at time t) can be The calculation of y(t) (i.e. described as three steps. As step I, suppose f (x(1)) is a one-hot vector having f (x(1))1 = 1. The calculation of G(1)f (x(1) in TTLM is as follows:\n\nG(1)f (x(1) =\n\n\n\n \n\n\nf (cid:0)x(1)(cid:1) f (cid:0)x(1)(cid:1) . . . f (cid:0)x(1)(cid:1)\n\n1\n\n2\n\n\n\n \n\n\n\n\n \n \n\n|V |\n\n(cid:104) G(1)\n\n=\n\n11 , G(1)\n\n12 , · · · , G(1)\n\n1R\n\n12\n\n11\n\nG(1) G(1) G(1) G(1) 21 22 . . . . . . |V |1 G(1) G(1) (cid:105)T\n\n|V |2\n\n1R\n\n. . . G(1) . . . G(1) 2R . . . . . . . . . G(1)\n\n|V |R\n\n\n\n \n \n\n= h(1)\n\nTTLM\n\nAs step II, TTLM will calculate f (x(i))Gh(i−1) is calculated in Eq. 7 at time t = 2 as follows:\n\nTTLM where i ∈ {2, 3, · · · , t − 1}. For example, h(2)\n\nTTLM\n\nh(2)\n\nTTLM = f (x(2))T Gh(1)\n\nTTLM\n\nAs step III, TTLM will output y(t) as follows:\n\nG(t)h(t−1)\n\nTTLM =\n\n=\n\n\n\n \n \n\n\n\n \n \n\n1R\n\n. . . G(t) . . . G(t) 2R . . . . . . . . . G(t) \n\n|V |R\n\n\n\n\n\n \n \n\n \n \n\n\n\n \n \n\nTTLM1\n\nh(t−1) h(t−1) TTLM2 . . . h(t−1)\n\nTTLMR\n\n12\n\n11\n\nG(t) G(t) G(t) G(t) 21 22 . . . . . . |V |1 G(t) G(t) 1i h(t−1) i=1 G(t) 2i h(t−1) i=1 G(t) . . . Rih(t−1) i=1 G(t)\n\n(cid:80)R\n\n(cid:80)R\n\n(cid:80)R\n\n|V |2\n\nTTLM1\n\nTTLM2\n\nTTLMR\n\n \n \n\nObserving the calculation, G(1), G and G(t) theoretically have no parameters in common (though we set G(1) = G(t) for simplicity). Further, their roles in TTLM are different: G(1) can be viewed as a word embedding matrix; G deals with two sources of information, i.e. hidden state and input word; G(t) extracts the evidence provided in h(t−1)\n\nTTLM and generates a set of scores over vocabulary.\n\nB RELATIONSHIP BETWEEN TTLM AND SOME RNNS\n\nWe now demonstrate the relationship between TTLM and Second-order RNNs, Recurrent Arithmetic Circuits (RACs) and Multiplicative Integration RNNs (MI-RNNs).\n\nTo avoid symbol clutter when representing different RNNs, the notation is: W hx ∈ RR×|V | denotes the input-to-hidden matrix, W hh ∈ RR×R denotes hidden-to-hidden matrix, φ(·) is a element-wise nonlinear activation function. Also, different hidden states are denoted as: Second-order RNNs (h(t)\n\nRAC) and MI-RNNs (h(t)\n\n2nd), RACs (h(t)\n\nMI ).\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nB.1 RELATION TO SECOND-ORDER RNNS\n\nUnlike Vanilla-RNNs (Mikolov & Zweig, 2012) that have additive blocks, Second-order RNNs have interaction between hidden states and input data in multiplicative form. This is achieved by a third-order tensor T with the i-th coordinate of the hidden states h(t) 2nd defined as (Hochreiter & Schmidhuber, 1997; Maupom ́e & Meurs, 2020):\n\nh(t)\n\n2ndi\n\n= φ(f (x(t))T Ti,:,:h(t−1)\n\n2nd + b)\n\n(14)\n\nwhere Ti,:,: ∈ RM ×R is the ith slice of tensor T ∈ RM ×R×R, and b is a bias vector. For simplicity, we will ignore b for other variants of RNNs since b can be seen as 0th component of f (x(t)) which equals to 1. Rabusseau et al. (2019) has provided that Tensor Trains can generalize linear Secondorder RNNs. We here provide a basic proof from the perspective of recursive property in TTLM. Claim B.1. The third-order tensor T in Second-order RNNs equals the TT cores in TTLM. The hidden states of Second-order RNNs is identical to that of TTLM if they are accompanied by an activation function.\n\nProof. The proof is based on the following observation: We recursively unfold the calculation of TTLM in Eq. 5:\n\nAw1,··· ,wN =\n\n|V | (cid:88)\n\ni=1\n\nf (x(1))i1 G(1)\n\ni1α1\n\n· · ·\n\n=\n\n=\n\n|V | (cid:88)\n\nR (cid:88)\n\ni1,i2=1\n\nα1=1\n\n...\n\nf (x(1))i1 G(1)\n\ni1α1\n\nf (x(2))i2 Gα1i2α2 · · ·\n\n(15)\n\n|V | (cid:88)\n\nR (cid:88)\n\ni1,··· ,iN =1\n\nα1,··· ,αN −1=1\n\nf (x(1))i1 G(1)\n\ni1α1\n\nf (x(2))i2 Gα1i2α2 · · · f (x(N ))iN G(N )\n\nαN −1iN\n\nObserve in the above, that at each time step, G has two sources of “input”: the information from the previous recursive unfolding (e.g., in the second line, the first line is the previous information), and the input data f (x(t)). From this perspective, G acts as a bilinear map G : R|V | × RR → RR, and we can regard the information in the previous line as a hidden state h(t)\n\nTTLM, given by:\n\nh(t)\n\nTTLMαt\n\n=\n\n|V | (cid:88)\n\nR (cid:88)\n\nit=1\n\nαt=1\n\nf (x(t))it Gitαtαt−1 h(t−1)\n\nTTLMαt−1\n\n(16)\n\nwhere we permute the indices of Gαt−1itαt as Gitαtαt−1 ( note that this does not change the number of indices).\n\nWe can also represent the hidden states in Second-order RNNs shown by Eq. 14 in element-wise fashion:\n\nh(t) 2ndi\n\n= φ(f (x(t))T Ti,:,:h(t−1)\n\n2nd\n\n)\n\n= φ\n\n\n\n\n\n|V | (cid:88)\n\nR (cid:88)\n\nj=1\n\nk=1\n\nf (x(t))jTjikh(t−1)\n\n2ndk\n\n\n\n\n\n(17)\n\nwhere j, k are the dummy indices as it, αt; i specifies the coordinate of h(t) Thus, T and G are the same-sized trainable bi-linear map.\n\n2nd just like αt for h(t)\n\nTTLM.\n\nAfter demonstrating that the third-order tensor T in Second-order RNNs equals the TT cores G, the only difference between the hidden states in Eq. 17 and in Eq. 16 is an activation function. If we add an activation function for h(t) TTLM, the hidden states of Second-order RNNs and TTLM are identical, as shown in Fig. 7a.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: a) Second-order RNNs under TTLM framework. b) Hidden state of RACs under TTLM framework. c) hidden state of MI-RNNs under TTLM framework. The dashed line in the square denotes A, Φ(X) or G. The small hollow circles denote the activation functions.\n\nB.2 RELATION TO RACS AND MI-RNNS\n\nWe here focus on Multiplicative Integration (MI), a way to connect two sources of inputs by the Hadamard product ‘⊙’. MI has been used in RACs, Multiplicative RNNs (M-RNNs) (Sutskever et al., 2011) and MI-RNNs:\n\nRecurrent Arithmetic Circuits (RACs) are recurrent networks with hidden states h(t) (Levine et al., 2018):\n\nRAC defined as\n\nh(t)\n\nRAC = W hxf (x(t)) ⊙ W hhh(t−1)\n\nRAC\n\n(18)\n\nwhere these hidden states are also used as an intermediate term in M-RNNs.\n\nMultiplicative Integration RNNs (MI-RNNs) are RACs with an activation function and hidden states h(t)\n\nMI defined as (Wu et al., 2016):\n\nh(t)\n\nMI = φ(W hxf (x(t)) ⊙ W hhh(t−1)\n\nMI\n\n)\n\n(19)\n\nClaim B.2. Given the condition the TT-scores: G = W hx ⊙ W hh. The hidden states of RACs are identical to that of TTLM. The hidden states of MI-RNNs are identical to that of TTLM if they are accompanied by an activation function.\n\nProof. The proof is based on the following observation: In the language of tensor contractions, Eq. 18 involves contracting the input weights matrix W hx with the input vector f (x(t)), and contracting the hidden weights matrix W hh with h(t−1) RAC . The Hadamard product of the two is a third-order diagonal tensor δ ∈ RR×R×R such that δijk = 1 iff the i = j = k, and δijk = 0 otherwise. Thus, we can write Eq. 18 in element-wise fashion:\n\nh(t)\n\nRACαt\n\n=\n\n=\n\n|V | (cid:88)\n\nR (cid:88)\n\nit=1\n\nαt=1\n\n|V | (cid:88)\n\nR (cid:88)\n\nit=1\n\nαt=1\n\nf (x(t))it W hx\n\nitj δjαtkW hh\n\nkαt−1 h(t−1)\n\nRACαt−1\n\nf (x(t))it Gitαtαt−1 h(t−1)\n\nRACαt−1\n\n(20)\n\nwhere G = W hx ⊙ W hh. In this case, the hidden state of TTLM in Eq. 16 is equal to the hidden state of RACs in Eq. 20, as shown in Fig. 7b. Similarly, if Eq. 16 is accompanied with an activation function φ, Eq. 16 is equal to the hidden state of MI-RNNs in Eq. 19 as shown in Fig. 7c.\n\nGiven Claim B.1 and B.2, the three models shall be simulated by TTLM with a non-linear activation function and we leave finding a theoretical proof of this conjecture to a future work.\n\n14\n\n|V|RTfx!...ΦXA|V|RTR|V|Tfx\"fx#a) Second-orderRNNsb) RACsc) MI-RNNsW!!W!\"δ|V|RRRRGW!!W!\"δ|V|RRRGRUnder review as a conference paper at ICLR 2023\n\nC IMPLEMENTATIONS\n\nWe implement all RNNs models, TTLM, TTLM-Large, and TTLM-Tiny using PyTorch. The weights in the models are adjusted to minimize the average cross entropy loss over training sequences via stochastic gradient descent computed using the truncated backpropagation through time algorithm. (Werbos, 1990; Williams & Peng, 1990)).\n\nFor RNNs, there are five matrix parameters: W xe ∈ RE×|V | is the input embedding matrix, W eh ∈ RE×H is the embedding-to- hidden matrix, W hh ∈ RH×H is the hidden-to-hidden matrix. We tie (share the same training parameters) the input embedding W xe and output embedding V which has been proved lead to a significant reduction in perplexity (Press & Wolf, 2016). So there is a projection matrix P ∈ RH×E before the output embedding. All this process is introduced in (Press & Wolf, 2016). For TTLM models, we tie the input tensor Wxe and V. The implementation of δ is functioned by a reshape function, so the interaction between hidden and input can be computed by matrix product. We also let G(1) have the same parameters along the dimension |V | (i.e. G(1) is simplified into a G(1) ∈ R1×R and can be viewed as initial hidden state h(0)).\n\nModel Vanilla-RNN\n\nMI-RNNs\n\nRACs\n\nSecond-order RNNs\n\nTTLM TTLM-Tiny\n\nTTLM-Large\n\nTraining Parameters W xe ∈ RE×|V |, W eh ∈ RE×H , W hh ∈ RH×H , P ∈ RH×E, V ∈ RE×|V | W xe ∈ RE×|V |, W eh ∈ RE×H , W hh ∈ RH×H , P ∈ RH×E, V ∈ RE×|V | W xe ∈ RE×|V |, W eh ∈ RE×H , W hh ∈ RH×H , P ∈ RH×E, V ∈ RE×|V | W xe ∈ RE×|V |, T ∈ RH×H×H , W hh ∈ RE×H , P ∈ RH×E, V ∈ RE×|V | G ∈ RR×|V |×R, G(t) ∈ RR×|V |, G(1) ∈ RR×|V | Wxe ∈ RR×|V |×R, W hh ∈ RR×R, P ∈ RR×R×R, V ∈ RR×R×|V | Wxe ∈ RR×|V |×R, Weh ∈ RR×R×R×R, W hh ∈ RR×R, P ∈ RR×R×R, V ∈ RR×R×|V |\n\nTable 2: Training parameters in our implementation. E is the embedding size, H is the hidden size in RNNs, and R is the rank in the TTLM. We set H = R and E = R2 to make the parameters of all models in the same scale. The parameters of W xe and Wxeare uniformly initialized in the interval [−0.1, 0.1], W eh, Weh and W hh are uniformly initialized between [− 1√\n\n].\n\n, 1√ H\n\nH\n\n15",
    "reference": "# Summary Of The Paper\n\nThe paper gives an experimental characterization and some theoretical connections regarding language models built on recurrent tensor train (TT) models, a form of tensor network (TN). Although TNs have received an increasing amount of interest within machine learning, with TTs having been proposed as promising language models, this appears to be the first work to actually evaluate the performance of such models in language modeling.\n\n# Strength And Weaknesses\n\n+ Giving concrete experimental results in language modeling is an important step for research into the use of TNs in machine learning. These models have many interesting properties and capabilities that aren't shared by neural nets, but (largely owing to the origins of TNs in physics and mathematics) prior work in this area has skewed heavily towards theory. I want to stress to other reviewers that even though the empirical results reported are a far cry from that of modern neural language models, the fact that simple recurrent TT models are in the same ballpark as pre-Transformer state of the art RNNs is an important result in itself.\n\n+ The authors prove concrete connections between recurrent TT language models and several prior models, namely recurrent arithmetic circuits, second-order RNNs, and multiplicative integration RNNs. These connections aren't surprising and the proofs are basic, but it is nonetheless good to clearly state these connections so that they can be appreciated by the broader machine learning community.\n\n- Many important implementational details are unspecified or unclear, and the paper feels hastily written. I address these points in more detail in the following section, but given that the primary contribution of this work is experimental, ensuring that these experiments are well-explained and fully reproducible should be a priority.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n* Not including the size of the embedding layer in the model size comparison in Table 1 feels misleading, as the TTLM models will have these embedding layers scale much more rapidly with the number of hidden units R than the other models on the table. In particular, the embedding layer for the TTLMs will have $|V| R^2$ parameters, whereas other models will require $|V| R$ or less. Given that the vocabulary is much larger than any of the model parameters here, this represents a significant memory and runtime overhead in larger models that isn't remarked on anywhere in the paper. I would strongly encourage the authors to include this in their parameter count in Table 1, either as a part of the model size or as a separate column.\n\n* In light of the point above, the many references to the TTLM-Tiny model using \"half the parameters of vanilla RNNs\" should probably be toned down. Would this comparison remain as favorable if the value of R is increased? If not, I would encourage the authors to not use this as a selling point of their model.\n\n* The paper has many points that could be made more clear, and I list some of these below in the form of questions or unexplained points that I encountered while reading the paper.\n\n    * How does the TTLM ensure its probabilities are actually non-negative numbers? The function g(X) in Eq. (2), which together with Eq. (5) determines the probabilities assigned to text X, can be positive or negative depending on the value of the TT cores, and no strategy is mentioned for handling the possibility of negative values.\n    * How does the TTLM ensure its probabilities are properly normalized? This point is also not remarked on in the paper.\n    * What is the hidden unit count (i.e. the value of R) used to get the results in Table 1?\n    * What vocabulary size was used for each experiment? Is the vocabulary formed from individual words, individual characters, or something else (e.g. tokens resulting from a different tokenization process)?\n    * How are the parameters of the TTLM model initialized?\n\n* It is confusing that the TTLM is introduced in Section 3.3 and Figure 2 as using site-dependent TT cores $G^{(i)}$, when in fact all cores are chosen the same in later sections of the paper.\n\n* As a small suggestion, it would seem more natural to describe the operation of the TTLMs in Figure 3 as mapping the one-hot encoded vector $f_\\theta(x^{(t)})$ directly to an RxR matrix $M^{(t)}$, rather than to an $R^2$ dimensional vector which is reshaped into a matrix. This is trivial for the TTLM-Tiny model (this already appears in Figure 2a), and can be done for the TTLM-Large model by depicting $W^{eh}$ as a fourth-order tensor mapping RxR matrices to other RxR matrices.\n\n* The paper has multiple small typos, and I would encourage the authors to use a spelling and grammar checker to fix some of the more obvious issues along these lines.\n\n# Summary Of The Review\n\nThe experimental contributions of the paper are important, and the theoretical results prove several useful relationships between previous model families. However, the lack of clarity in the description of the model and experiments, along with the absence of significantly novel architectural or theoretical contributions, detract from the paper's score.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nMODEL ENSEMBLE INSTEAD OF PROMPT FUSION: A SAMPLE-SPECIFIC KNOWLEDGE TRANSFER METHOD FOR FEW-SHOT PROMPT TUNING\n\nXiangyu Peng† Chen Xing‡ Prafulla Kumar Choubey‡ Chien-Sheng Wu‡ Caiming Xiong‡ †Georgia Institute of Technology xpeng62@gatech.edu, {cxing,pchoubey,wu.json,cxiong}@salesforce.com\n\n‡Salesforce Research\n\nABSTRACT\n\nPrompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance of prompt tuning by transferring knowledge from soft prompts of source tasks. Recognizing the good generalization capabilities of ensemble methods in low-data regime, we first experiment and show that a simple ensemble of model predictions based on different source prompts, outperforms existing multi-prompt knowledge transfer approaches such as source prompt fusion in the few-shot setting. Motivated by this observation, we further investigate model ensembles and propose Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the contribution of each source model for each target sample separately when ensembling source model outputs. Through this way, SESoM inherits the superior generalization of model ensemble approaches and simultaneously captures the sample-specific competence of each source prompt. We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-{base, large, XL}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.\n\n1\n\nINTRODUCTION\n\nRecent few years have witnessed the great success of large pre-trained language models (PLM) (Kenton & Toutanova, 2019; Liu et al., 2019; Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020). The size of pre-trained models which can easily go to billions of parameters (Brown et al., 2020; Raffel et al., 2020), however, hinder their real-world deployments and applications. The huge size of pre-trained language models can make model fine-tuning for downstream NLP tasks computationally expensive and memory-inefficient. To alleviate this problem, many parameterefficient fine-tuning methods are proposed (Li & Liang, 2021; Houlsby et al., 2019; Zhang et al., 2021; Lester et al., 2021; Liu et al., 2021b). Among them, prompt tuning (Lester et al., 2021) is one of the most widely adopted methods. Given a downstream task, prompt tuning methods keep the entire pre-trained model frozen. Only the newly added task-specific soft prompts are updated on the training data from a target task, conditioning on the original pre-trained model. Compared to traditional fine-tuning methods that update the entire pre-trained model, prompt tuning consumes significantly less memory and less training time per iteration (Table 10 in Gu et al. (2022)).\n\nDespite prompt tuning’s advantages in practice and its continuously improved performances on various NLP tasks (Liu et al., 2021a; Vu et al., 2022), its performances in few-shot settings where labeled training data is limited, still have large space for improvements (Gu et al., 2022). In low-data scenarios, one of the most widely applied approaches to alleviate data shortage of the target task, is to seek help from source tasks where labeled training data is abundant. Although such knowledge transfer approaches from multiple source tasks are analyzed on full-model training in other domains\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(Chen, 2021; Li et al., 2020; Sasso et al., 2022; Lee et al., 2019), relevant methods for few-shot prompt-tuning still remain under explored. Therefore, in this work, we seek to find an effective strategy to use trained soft prompts from multiple source tasks to benefit few-shot prompt tuning on a new target task.\n\nWith soft prompts trained from several source tasks and full training data from a target task, there are a few existing approaches one could adopt. Vu et al. (2022) finds the most suitable source soft prompt to initialize the soft prompt of the target task. Alternatively, Asai et al. (2022) directly fuses all source soft prompts together with a target task-specific prompt. Although both source soft prompt based initialization and fusion improve performance with enough training data for a target task, we empirically find them not as effective under few-shot settings. Another tempting alternative we could employ to use source prompts is model ensemble, which is known to provide good generalization and low variance (Hansen & Salamon, 1990). For instance, Dvornik et al. (2019) and Liu et al. (2020) show that simple ensemble methods outperform complicated approaches in few-shot settings in the computer vision domain. Therefore, for few-shot prompt tuning, we wonder whether an ensemble of model outputs given different source prompts achieve better performance compared to existing approaches employing source prompts. If so, what is the most effective model ensemble strategy for the knowledge transfer from multiple source prompts?\n\nTo answer these questions, we conduct empirical analysis and find that a simple uniform logitaveraging ensemble of model predictions based on different source prompts, can already outperform existing multi-source knowledge transfer approaches for few-shot prompt tuning. Motivated by this observation, we further look into ensemble approaches and propose our solution, a sample-specific ensemble of source models (SESoM). Source models refer to the trained soft prompts of source tasks, together with the pre-trained language model that the source soft prompts are trained with. As the name suggests, SESoM learns from the few-shot target samples to adaptively decide how much each source task should contribute given different target samples. Specifically, our method trains an attention-style network to generate weights to ensemble the outputs of different source models, in order to make the prediction given each target sample. Through this way, our model is able to capture the sample-specific preferences to ensemble different source models given the few-shot labelled target data. Therefore, compared to existing knowledge transfer approaches for prompt tuning that provide a fixed knowledge transfer strategy for all target samples, SESoM is more effective due to its sample-specific strategy.\n\nWe conduct experiments across six source tasks and eight target tasks on three model scales, T5Base, T5-Large and T5-XL. Experimental results show that SESoM outperforms existing methods, such as source prompt fusion approaches and other model ensemble methods, by a large margins in every scenario tested. Moreover, we also find that SESoM can consistently achieve better performance compared to existing methods when the number of few-shot labeled target data increases. Even in full-data settings, SESoM outperforms existing methods although not as significantly as in few-shot settings. Finally, we find that SESoM can achieve better performance when the number of source tasks increases, even when the newly added tasks are less preferable in general for the target task. Our case study also shows that SESoM can generate different ensemble weights for different samples of one target task. The generated weights are also aligned with the sample-specific performance of different source models.\n\n2 RELATED WORK\n\nKnowledge transfer approaches in the context of prompt tuning. Since the emergence of prompt tuning methods, much recent research has focused on improving the performance of prompt-tuning methods on full-data fine-tuning. Some of them focus on transferring knowledge from other tasks which are similar to the target task, to facilitate prompt tuning of the target task. Among them, SPoT (Vu et al., 2022) first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. SPoT significantly boosts the performance of prompt-tuning across many tasks. Similarly, PPT (Gu et al., 2022) pre-trains the soft prompt of the target task with data formulated similarly with target data. These two methods provide a fixed knowledge transfer strategy for all target samples, given that they both provide initialization before few-shot prompt tuning of the target task. Different from them, our method provides sample-specific knowledge transfer from source models to each target samples, leading to better performance on few-shot fine-\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ntuning. More recently, Asai et al. (2022) proposes a sample-specific prompt fusion method for full-data prompt tuning. In such method, the soft prompts of source tasks are fused together to construct a new prompt for each target sample, instead of ensembling the source models’ outputs given each target sample in SESoM.\n\nEnsemble learning has been a popular approach to obtain a low-variance and generalizable model (Hansen & Salamon, 1990). The basic ensemble techniques include voting (Hansen & Salamon, 1990), bagging (Breiman, 1994), boosting (Schapire, 1990; Freund & Schapire, 1997) and stacking (Wolpert, 1992), which have been applied across many NLP problems such as model debiasing (Elazar & Goldberg, 2018; Stacey et al., 2020), cross-lingual transfer learning (Wang et al., 2021), calibrating sequence classification and generation models (Reich et al., 2020), domain adaptation (Kim et al., 2017). Within the paradigm of prompt-based learning, Schick & Sch ̈utze (2021a) explores unweighted average of logits corresponding to different human-selected verbalizers for zeroshot classification. Lester et al. (2021) uses majority voting over logits from five prompts trained on the same task. They all employ identical ensemble strategy for all test samples, which could be sub-optimal. Schick & Sch ̈utze (2021a) also found that the oracle that selects the best verbalizer for every test sample significantly outperforms the unweighted-averaging model. SESoM fills this gap by taking over the role of oracle and train a separate attention module to generate samplespecific importance weight for every source model. The sample-specific weights help to measure each source model’s competence in making prediction for a sample, and enable better utilization of available prompts.\n\n3 METHOD\n\n3.1 PRELIMINARIES\n\nIn SESoM, given the training data for source tasks S1, ..., ST and a pre-trained language model, we first train a soft prompt Pj (j ∈ [1, T ]) for each source task by running prompt tuning (Lester et al., 2021). Following a typical T5-setting, we restructure all downstream tasks to text-to-text generation format, where each label of a training sample is represented by a verbalizer (Schick & Sch ̈utze, 2021c) and, optionally, a task-specific template (Schick & Sch ̈utze, 2021c;b; Mishra et al., 2021). We represent an instance in a source or target task as (X, y), where X is a sequence of token embeddings (X = [x1, ..., xl] ∈ Rl×d, l is the length of the input token sequence and d is the embedding size of PLM), and y is a classification label. Then, we map the class label y to its corresponding verbalizer or the verbalizer-template sequence, and call it Y 1. Each soft prompt Pj = [p1, ..., pm] ∈ Rm×d is also a sequence of embeddings, where m is the number of soft prompt embeddings for the task.\n\nPrompt tuning prepends a randomly initialized task-specific soft prompt Pj to the input embedding sequence X, resulting in the final input [Pj; X]. Then, [Pj; X] is fed into the pre-trained model to make the prediction. The target task is modeled as Prθ(Y | [Pj; X]) where the parameters of the pre-trained model (θ) is frozen during the tuning and only the soft prompt Pj is updated in order to maximize Prθ(Y | [Pj; X]). We show the process of prompt tuning in Fig.1 (a).\n\n3.2 SAMPLE-SPECIFIC ENSEMBLE OF SOURCE MODELS\n\nGiven a collection of source prompts [P1; ...; PT ] from source tasks [S1; ...; ST ] trained using prompt-tuning, and the pre-trained language model θ that these soft prompts are conditioned on, SESoM aims to use their knowledge on a new target task under few-shot settings. We call the prompt from a source task together with the PLM as a source model, represented as [Pj; θ]. In SESoM, we first train each source model [Pj; θ] with the few-shot target data in a prompt-tuning manner. This enforces the source models to generate target task’s verbalizers given the target input sample.\n\nThen taking a labeled instance (X, y) from the few-shot target task Ttarget, SESoM first feeds [Pj; X] into the corresponding source model [Pj; θ] and obtain the pre-softmax logit lx,j. It then uses the lx,j and X to compute sample-specific attention weight representing the competence of the\n\n1We detail all the verbalizers used in our experiments in Appx. B.5.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n(a) Training of source prompts.\n\n(b) Training of attention module G.\n\nFigure 1: Overview of SESoM. Components with\n\nare updated during training while those with are frozen. (a) describes the training of source prompts in SESoM. Given the input token embedding sequence X of different source tasks and a pre-trained model, prompt tuning trains task specific source prompts[P1; ...; PT ] for source tasks [S1; ...; ST ] by prepending them to X as inputs and feeding to the frozen pre-trained model. (b) describes how SESoM trains the attention module G. G takes the logits [lx,1; ...; lx,T ] from all T source models as input and usesˆx as the key to calculate the attention weights ax of source logits. Then source logits are weighted averaged accordingly to construct lx for the final prediction.\n\nsource model [Pj; θ] for the given input X. We train an attention-style network G (§ 3.2.1) to compute these sample-specific weights for every source model. Finally, it takes the weighted average of logits (Lx = [lx,1; ...; lx,T ] ∈ RT ×v, where v is the vocabulary size of the pre-trained model) from all source models using their attention weights to make its prediction for X.\n\n3.2.1 ATTENTION MODULE G.\n\nThe goal of the attention module G is to learn to generate weights to ensemble the source models’ pre-softmax logits based on their competence on a target sample X. One input of G is therefore the representation of target sample X. Other candidate inputs of G which might benefit this samplespecific weight generation process are either source prompts or pre-softmax logits of source models. We empirically find using X and source prompts as input for G are less effective compared to taking X and source model logits as G inputs under few-shot settings. We hypothesize that it is because prompts are not self-sufficient as their competency is not solely determined by them. They are effective only when used along with the PLM for which it was tuned. With abundant training samples, the prompt-based attention module may learn to identify source prompts that are relevant for an input sample from the target task. But with few training samples, it is unable to learn to accurately weight predictions from different source models, required to capture their effectiveness for every sample. Logits of source models, on the other hand, are a function of both the prompt and the PLM and are also capable of modeling the uncertainty in predictions from different models. Consequently, we propose to use the source model logits lx,j along with the input X in our attention module. We first perform max-pool over the token embedding sequence X = [x1; ...; xl] ∈ Rl×d, to obtain ˆx ∈ Rd as the representation of X, Then given the representation ˆx of the target input X, we apply a feed-forward network to project ˆx non-linearly to a new representational space,\n\nhx = W T\n\nu,x · γ(W T\n\nd,x · ˆx) where γ(·) is a non-linear activation function, Wd,x ∈ Rd×d′ are trainable weights. Then we apply Layer Norm (Ba et al., 2016) to get hx ∈ Rd′ — the final projected representation of target input X. Similarly, we project the pre-softmax logit lx,j ∈ Rv of each source model given X into the same space,\n\nx and Wu,x ∈ Rd′\n\nx×d′\n\n(1)\n\nhl,j = W T\n\nu,l · γ(W T\n\nd,l · lx,j),\n\n(2)\n\nwhere Wd,l ∈ Rv×d′ representations of all source logits {hl,j}T\n\nl and Wu,l ∈ Rd′\n\nl×d′\n\nare trainable weights. Then, given hx and the projected j=1, we compute the attention score ax,j for the source\n\n4\n\nPre-trained ModelSoftmaxPre-trained ModelSoftmaxPre-trained ModelSoftmaxAttention Module SoftmaxPre-trained ModelPre-trained ModelPublished as a conference paper at ICLR 2023\n\nlogit lj by\n\nax,j =\n\ne(hl,j ·hx) k=1 e(hl,k·hx)\n\n(cid:80)T\n\n.\n\n(3)\n\nSESoM produces the final output logit lx ∈ Rv by computing a linear combination of [lx,1; ...; lx,T ] given the computed input-logit attention,\n\nlx = G(ˆx, [lx,1; ...; lx,T ]) =\n\nT (cid:88)\n\nj=1\n\nax,jlx,j\n\n(4)\n\nThen following the prompt tuning approach in Section 3.1, we freeze all the parameters θ in pretrained language model and source soft prompts ([P1; ...; Pt]). We also show the forward pass of SESoM in Fig.1 (b). Attention module G is trained by minimizing its cross-entropy loss between softmax(lx) and the label Y. During the few-shot training, we update the attention module G with the few-shot labeled target samples. Through this way, G is trained to capture the sample-specific preference of different source models. At inference, we also use G to calculate the sample specific ensemble weight of all source logits, and calculate the weighted average of them as the final logit to make the prediction.\n\nParameter efficiency. As mentioned in Lester et al. (2021), prompt tuning methods are a natural fit for model ensemble approaches from the perspective of parameter efficiency. Unlike other models for which an ensemble of T models leads to T times more model parameters, an ensemble of T different prompt-tuning models only leads to T times more soft prompts. Because the pre-trained model that soft prompts are trained to condition on, is identical for all models to be ensembled. Therefore, although SESoM is a model ensemble approach, the additional model parameters introduced by the ensemble are only the soft prompts of 6 source tasks, i.e., T × m × d parameters (0.6M in our experiment). SESoM also trains one attention module which includes four projection layers l × d′ + 4d′ parameters (∼ 0.9M in and two layer norms. It requires d × d′ our experiment). Therefore, the total number of additional trainable model parameters in SESoM is only less than 0.5% of a pre-trained T5-base model.\n\nx × d′ + v × d′\n\nx + d′\n\nl + d′\n\n4 EXPERIMENTS\n\nIn this section, we first describe our experimental setup, then our main empirical results compared to baselines. Finally we conduct various empirical analysis and case study to show the effectiveness of different components of SESoM and reasonings behind our design choices.\n\n4.1 EXPERIMENTAL SETUP\n\nFew-shot prompt tuning setups and hyper-parameters of SESoM. For our main experiments, we follow existing work (Gu et al., 2022) to set the number of few-shot training and validation samples for every target task as 32. We evaluate all models with the full original validation set of every target task. All presented results are an average of 20 runs with different random seeds. For SESoM, we randomly initialize attention module G based on Section 3.2. Details of other hyperparameters of SESoM, such as the embedding sizes of G, the optimizer and learning rate during training, etc, can be found in Appendix B.3. We use pre-trained T5-{base,large,XL}(Raffel et al., 2020) as language model θ.\n\nSource and target tasks. Following existing work(Asai et al., 2022), we use six tasks that have been considered generally helpful to other NLP tasks as our source tasks. They are MNLI (Williams et al., 2018), QNLI (Demszky et al., 2018), QQP (Wang et al., 2018), SST2 (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016), and ReCoRD (Zhang et al., 2018). Given each pre-trained model tested (T5-{base,large,XL}), we train soft prompts for these source tasks separately using prompt tuning. The hyper-parameters for training the soft prompts can be found in Appendix B.1.\n\nWe then evaluate our model on the following GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) tasks: WNLI (Wang et al., 2018), MRPC (Dolan et al., 2005), BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018), RTE (Giampiccolo et al., 2007), WiC (Pilehvar & CamachoCollados, 2019), WSC (Levesque et al., 2012), and CB (De Marneffe et al., 2019). They cover a\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Few-shot performance of all methods. All the scores are the average of 20 runs with different random seeds, with standard errors included in subscripts. The best scores are in bold. “Uni Ensemble” is the acronym for Uniform Ensemble. “MV Ensemble” stands for Majority-vote Ensemble. “FW Ensemble” stands for Fixed-weight Ensemble.\n\nT5-Base\n\nWNLI MRPC RTE MultiRC BoolQ WiC WSC CB Avgstd Model Method 54.44 SPoT-t 47.89 PPT 52.95 ATTEMPT 56.34 Uni Ensemble MV Ensemble 50.24 50.07 FW Ensemble 56.34 SESoM 54.93 SPoT-t 43.66 PPT 42.25 ATTEMPT 56.34 Uni Ensemble MV Ensemble 49.58 56.34 FW Ensemble 56.34 SESoM 56.34 SPoT-t 50.07 PPT 49.37 ATTEMPT 56.34 Uni Ensemble MV Ensemble 41.29 56.27 FW Ensemble 56.34 SESoM\n\n56.87 54.51 47.98 43.48 51.9210.64 51.81 57.25 63.46 50.00 58.401.03 54.28 54.48 43.89 43.74 54.616.48 70.12 62.69 45.19 57.14 61.223.87 73.97 71.59 49.71 65.82 64.662.15 72.28 63.14 51.39 43.74 62.284.58 74.80 63.68 53.56 74.02 67.542.16 53.66 56.26 48.76 53.70 58.7214.23 67.68 53.61 61.54 80.36 65.951.00 68.42 54.33 60.04 69.64 61.926.37 78.83 64.42 60.42 58.78 68.383.35 78.58 72.67 53.09 56.60 65.812.88 81.28 61.44 56.73 62.85 69.243.19 81.24 67.06 63.08 84.73 74.690.89 60.97 59.53 50.58 70.89 62.7714.52 69.89 58.62 59.73 78.92 67.100.83 60.39 42.50 45.38 80.18 61.976.65 81.07 58.77 63.40 78.57 72.453.61 77.48 64.43 49.49 82.83 68.572.87 82.83 66.79 57.55 73.48 73.542.41 82.50 68.56 63.41 84.15 76.221.44\n\n50.61 60.32 59.52 65.92 71.39 66.32 66.91 64.83 65.50 70.11 74.36 73.60 73.63 74.40 51.07 67.57 71.26 76.56 75.30 76.20 76.90\n\n54.15 67.51 52.63 57.40 59.87 65.03 66.66 65.14 75.81 62.63 67.14 58.19 77.97 82.80 75.16 75.83 84.24 89.16 84.24 89.58 89.71\n\n53.32 68.97 75.35 75.00 74.65 77.57 84.34 72.46 79.41 67.96 86.76 84.20 83.74 87.84 77.65 76.22 62.46 75.73 73.46 85.62 88.21\n\nT5-XL\n\nT5-Large\n\nwide range of NLP tasks including question answering, sentiment analysis and textual entailment. Details of source and target tasks can be found in Appendix A.\n\nBaselines. We compare SESoM not only with existing knowledge transfer approaches in the context of prompt tuning, but also with multiple existing ensemble strategies. The knowledge transfer methods for prompt tuning are,\n\n• SPoT-t (Vu et al., 2022): SPoT-t originally first trains soft prompt with the full training set of the target task for a few epochs, and then retrieve the source soft prompt which shares the highest cosine similarity with this target prompt to re-initialize the target prompt. Then, it re-trains the target prompt starting from this new initialization. To adapt SPoT-t to few-shot settings, we keep everything unchanged except that we train the target prompt with the few-shot labelled data, instead of with the full target training set.\n\n• PPT(Gu et al., 2022): PPT pre-trains the target prompt with additional data by pre-processing the additional data to the format of the target task. For fair comparison with other approaches, we use the training data of all 6 source tasks to pre-train the target soft prompt. We then update the target soft prompt with the few-shot target data as in prompt tuning.\n\n• ATTEMPT(Asai et al., 2022): ATTEMPT is a sample-specific knowledge fusion method for fulldata prompt tuning. In this method, the soft prompts of source tasks are fused together to construct a new prompt for each target sample, unlike SESoM’s model-level ensemble of source models. To apply ATTEMPT in few-shot settings, we keep everything unchanged for ATTEMPT except that we train their model with the few-shot labelled data, instead of with the full target training set.\n\nThe ensemble baselines are,\n\n• Uniform Ensemble: the simplest ensemble method of the source models. Same as SESoM, we first fine-tune each source model with the target few-shot labelled data. Then we take the average of pre-softmax logits of all source model’s output given a target sample to make the prediction. The difference of Uniform-Ensemble from SESoM is that Uniform-Ensemble simply averages the source logits for all target samples instead of conducting a sample-specific weighted average. • Majority-Vote Ensemble: in this ensemble baseline, we use every individual source model to make predictions first. Then we take the prediction that have the most votes as the final prediction. The rest setting of Majority-Vote Ensmeble is the same as Uniform Ensemble.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: SESoM with top k source models. All results are the average of 20 runs with different random seeds. Standard errors are included in subscripts. “# s.” stands for the number of source models used in SESoM.\n\nT5-Base\n\nModel # s. WNLI MRPC RTE MultiRC BoolQ WiC WSC CB Avgstd 50.61 61.59 66.08 64.83 73.55 74.09\n\n56.87 54.51 47.98 43.48 51.9210.64 67.48 61.43 48.08 64.11 61.847.48 72.96 61.54 49.04 69.46 65.183.40 53.66 56.26 48.76 53.70 58.7214.23 76.36 62.24 56.30 68.04 68.916.93 80.28 66.05 60.19 80.98 72.873.41\n\n53.32 54.15 72.59 62.49 81.91 64.01 72.46 65.14 86.27 72.17 87.43 77.60\n\n54.44 56.97 56.41 54.93 56.34 56.34\n\n1 3\n5 1\n3 5\n\nT5-Large\n\n• Fixed-weight Ensemble:\n\nInstead of simply averaging logits of source models as UniformEnsemble, Fixed-weight Ensemble takes a weighted average of the source logits to make predictions. The voting weight of each source model is fixed for all target samples. The voting weight is calculated according to the F1 score of the source models given the few-shot target data. The better a source model performs given the labelled few-shot data, larger voting weight it would have for all target samples.\n\nMore details about the training of all baseline methods above can be found in Appendix B.2.\n\n4.2 MAIN RESULTS\n\nThe few-shot performance of SESoM and other methods are shown in Table 1. First, on every pretrained model (T5-{base,large,XL}), SESoM outperforms all baselines on the average score by a large margin. SESoM also outperforms baselines on a definite majority of single target tasks on pretrained models with different sizes. Specifically, compared to approaches that provide a fixed knowledge transfer strategy for all target samples (SPoT-t, PPT, Uni Ensemble, FW Ensemble), SESOM’s superior performance indicates the effectiveness of its sample-specific strategy. Compared to existing approaches that conduct sample-specific soft prompt fusion (ATTEMPT), SESoM’s superior performance indicates the effectiveness of its model ensemble strategy in few-shot settings. Moreover, if we compare SESoM on T5-Base with non-ensemble baselines (SPoT-t, PPT, ATTEMPT) on T5-Large, we can see that SESoM on T5-Base even outperforms these baselines with a larger pretrained model. Similarly, SESoM on T5-Large also outperforms non-ensemble baselines on T5-XL. This indicates that SESoM further improves the parameter efficiency of prompt tuning in few-shot settings.\n\nSecond, if we compare the performances of ensemble methods (Uni Ensemble, FW Ensemble and our proposed method SESoM) versus the rest methods, ensemble methods have superior performance against the rest methods in the few-shot setting. The bigger the pre-trained model is, more apparent the performance advantages of ensemble methods are in the few-shot setting. It is likely due to the widely observed strengths of ensemble methods that provide a better generalization and lower variance during inference, as illustrated in Section 1 as our motivation. Finally, compared with all methods, ATTEMPT seems to have the most unsatisfactory results in few-shot settings. ATTEMPT uses the few-shot target data to train a network to output weights to fuse source soft prompts for each target sample. Although it achieves great performances in full-data fine-tuning, few-shot training is not an easy scenario for it due to the overfitting caused by limited training data. While our proposed SESoM takes the logits of source models as input and outputs weights for model ensemble during few-shot training, which takes advantage of the superior generalization and robustness of model ensemble approaches.\n\n4.3 EMPIRICAL ANALYSIS\n\nWould more “ordinary” source tasks help SESoM? In our main experiments, we use 6 source tasks as mentioned earlier, regardless of their actual transferability given the target task. However, if a source task is too different from the target task, it is possible that this source task wouldn’t effect the target task positively in general. In order to investigate how SESoM performs when less preferable source tasks are added, we conduct experiments as follows. First, given each target task, we pick top 1, top 3 and top 5 most generally helpful source tasks for this target task, following SPoT-t (Vu\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Results on target tasks with different number of few-shot training labels. All the scores are the average of 5 runs, with standard errors included in subscripts.\n\nData Size Method\n\n64\n\n128\n\nFull\n\nPPT ATTEMPT FW Ensemble SESoM PPT ATTEMPT FW Ensemble SESoM PPT ATTEMPT FW Ensemble SESoM\n\nAvg\n\nWNLI MRPC RTE MultiRC BoolQ WiC WSC CB 48.51 52.68 53.80 57.69 50.14 54.37 53.55 58.10 50.70 55.56 54.93 57.75\n\n62.04 53.59 62.00 54.29 59.841.83 62.35 57.74 48.46 50.36 57.894.00 72.65 62.66 50.96 58.21 63.334.40 63.71 60.92 77.14 69.581.42 76.09 62.16 51.25 61.81 75.83 62.362.93 66.47 53.89 51.35 69.29 61.445.20 62.65 56.08 55.58 74.29 63.644.49 63.73 61.54 82.86 70.461.06 75.93 79.57 66.61 62.78 78.57 72.240.01 79.37 68.50 59.23 75.36 71.541.89 75.82 66.46 63.46 76.79 71.820.84 80.79 67.54 63.50 86.43 74.690.25\n\n68.53 76.76 77.87 84.56 68.68 77.60 79.80 84.61 87.50 88.14 89.46 89.92\n\n60.54 58.87 64.78 69.18 62.27 58.31 59.78 68.87 71.39 70.86 68.59 72.14\n\n69.22 55.88 65.68 67.33 66.75 60.29 67.36 68.09 80.87 75.31 79.06 79.42\n\nTable 4: Results of different design choices on a pre-trained T5-Base model. All the scores are the average of 20 runs, with standard errors included in subscripts. “Ensemble acc SP” stands for Ensemble acc. source prompts. “PLG” stands for Pseudo Label Generation. Training details can be found in Appx. B.4.\n\nMethod Ensemble acc SP PLG SESoM\n\nWNLI MRPC RTE MultiRC BoolQ WiC WSC 52.34 63.92 52.96 51.39 65.32 50.07 53.56 66.91 56.34\n\n60.97 63.14 63.68\n\n64.15 65.03 66.66\n\n74.46 72.98 74.80\n\n79.69 77.57 84.34\n\nCB 71.25 43.74 74.02\n\nAvg 64.964.29 61.156.37 67.542.16\n\net al., 2022). Then we apply SESoM with the top 1, top 3 and top 5 source tasks separately, and compare with SESoM’s main results with 6 default source tasks. Results are shown in Table 2. From Table 2 we can see that SESoM can achieve better performances when the number of source tasks increases, although the newly added source tasks might be less preferable to the target task in general. It indicates that for the target samples on which the generally less favourable source tasks can actually perform well, SESoM is able to set proper sample-specific weights and lean more on these source models for such specific target samples, which implies the effectiveness of SESoM sample-specific strategy.\n\nThe effect of the number of shots. In main experiments, the number of few-shot labeled data we use for each target task is 32 following existing work. We wonder how SESoM would perform when we increase the number of shots compared to other methods. Therefore we have tried to set the number of shots as 64, and 128 and full. Results are presented in Table 3. We can find that SESoM can consistently achieve better performance compared to existing methods when the number of few-shot labeled target data increases. It suggests that SESoM can be applied to a wide range of few-shot settings. Even in full-data settings, SESoM outperforms existing methods although not as significantly as in few-shot settings.\n\nVerification of other design choices. Before the current SESoM, we have also tried some related design choices that outperform existing baselines to some extent, but turned out to be less effective than SESoM. For example, we have tried different inputs for attention module G. Instead of using source logits as our attention module’s other input given input sample X in SESoM, we have tried using the source prompts as inputs, i.e., replacing lj in Eq.2 with Pj. Other components of this method stay the same as SESoM. We refer to this method as “Ensemble acc. source prompts”. We have also tried directly generating pseudo labels from source models to train the target soft prompt. Specifically, we first prompt tune source prompts on the target data sets with the few-shot target training data. Then we use these prompt-tuned source prompts with the pre-trained language model, to generate pseudo labels for the entire target datasets. The final pseudo label of each target sample is decided by majority voting of the 6 source models. These pseudo labelled samples are used to train the soft prompt of the target task before few-shot prompt tuning with the few-shot target training data. We refer to this method as “Pseudo Label Generation’. Results of these methods are shown in Table 4. We can see that they achieve relatively similar performances with FW Ensemble method in Table 1, while all under-perform SESoM.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n4.4 CASE STUDY\n\nIn this section, we conduct a case study to verify the effectiveness of SESoM’s sample-specific model ensemble strategy. First, we would like to see whether it is true that given one target task, different samples of this target task require different source models. Because it is the basic assumption under which we propose SESoM’s sample-specific ensemble strategy. In Table 5, we show two samples from target task MRPC. Under each sample, in the row of “preds from individual source”, we show each individual source model’s prediction given this sample. We can see that for Example #1, QQP and QNLI makes the correct prediction while for Example #2, MNLI, SST2 and QNLI makes the correct prediction. This sample-specific performance of different source models on target-task samples is not rare for all target tasks. More examples of other target tasks can be found in Appx. E. Moreover, the majority or at least half of source models make wrong predictions for each example. It indicates that universal model ensemble or majority vote model ensemble approaches would fail on these examples.\n\nSecond, we would like to see whether SESoM’s sample-specific contribution weights of different source models generated by its attention module G, are aligned with the performances of different source models. In Table 5 under each example, we show the contribution weights from SESoM for ensembling each source models in the row of “weights from SESoM”. We can see that SESoM generally generates higher weights for source models that make the correct predictions on each example and the weight distribution is different for different examples. More examples of other target tasks can be found in Appx. E. It indicates that SESoM can generate sample-specific weights for model ensemble according to the actual sample-specific performance of different source models.\n\nTable 5: Case study from MRPC target task. “Label” (yellow cells) is ground truth label and “Pred” (yellow cells) is the predicted label obtained by SESoM with the weights (orange cells) shown in the table. “Preds from individual source” (pink cells) shows predictions of each source model.\n\nExample # 1\n\n[MRPC]\n\ns 1: Unable to find a home for him, a judge told mental health authorities they needed to find\n\nsupervised housing and treatment for DeVries somewhere in California.\n\ns 2: The judge had told the state Department of Mental Health to find supervised housing and\n\ntreatment for DeVries somewhere in California.\n\n1 (Equivalent) 1 (Equivalent)\n\nPred Label Preds from individual source Weights from SESoM\n\nSource Models\n\nMNLI 0 ✗ 0.0120\n\nSST2 0 ✗ 0.0708\n\nQNLI 1 ✓ 0.1346\n\nQQP 1 ✓ 0.5857\n\nSQuAD 0 ✗ 0.1266\n\nExample # 2\n\nReCoRD 0 ✗ 0.0702\n\n[MRPC]\n\ns 1: This integrates with Rational PurifyPlus and allows developers to work in supported versions\n\nof Java, Visual C # and Visual Basic.NET.\n\ns 2: IBM said the Rational products were also integrated with Rational PurifyPlus, which allows\n\ndevelopers to work in Java, Visual C # and VisualBasic.Net.\n\n1 (Equivalent) 1 (Equivalent)\n\nPred Label Preds from individual source Weights from SESoM\n\nSource Models\n\nMNLI 1 ✓ 0.2381\n\nSST2 1 ✓ 0.2055\n\nQNLI 1 ✓ 0.2486\n\nQQP 0 ✗ 0.2240\n\nSQuAD 0 ✗ 0.0219\n\nReCoRD 0 ✗ 0.0620\n\n5 CLOSING REMARKS\n\nIn this paper we explored the potentials of prompt tuning methods in few-shot settings. We have found in our exploration that by properly transferring knowledge from trained soft prompts of source tasks, prompt tuning’s few-shot performance can be significantly improved. Specifically, our proposed method, sample-specific ensemble of source models (SESoM) outperforms existing methods by a large margin in every tested few-shot scenario. SESoM adjusts the contribution of each source model for each target sample separately when ensembling source model outputs. Our empirical analysis suggests that the two key components of SESoM, the model-level ensemble instead of prompt-level fusion, and the sample-specific strategy for model ensemble, are both critical to its superior performance. SESoM also consistently achieves superior performance given larger numbers of labelled data and larger numbers of source tasks.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nAkari Asai, Mohammadreza Salehi, Matthew E Peters, and Hannaneh Hajishirzi. Attentional mixtures of soft prompt tuning for parameter-efficient multi-task knowledge sharing. arXiv preprint, 2022.\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n\narXiv:1607.06450, 2016.\n\nLeo Breiman. Bagging predictors, 1994.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n\nYifei Chen. A transfer learning model with multi-source domains for biomedical event trigger\n\nextraction. BMC genomics, 22(1):1–18, 2021.\n\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pp. 107–124, 2019.\n\nDorottya Demszky, Kelvin Guu, and Percy Liang. Transforming question answering datasets into\n\nnatural language inference datasets. arXiv preprint arXiv:1809.02922, 2018.\n\nBill Dolan, Chris Brockett, and Chris Quirk. Microsoft research paraphrase corpus. Retrieved\n\nMarch, 29(2008):63, 2005.\n\nNikita Dvornik, Cordelia Schmid, and Julien Mairal. Diversity with cooperation: Ensemble methods for few-shot classification. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 3723–3731, 2019.\n\nYanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text data. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 11–21, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1002. URL https://aclanthology.org/D18-1002.\n\nYoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning Journal of Computer and System Sciences, 55(1):119–139, ISSN 0022-0000. doi: https://doi.org/10.1006/jcss.1997.1504. URL https://www.\n\nand an application to boosting. 1997. sciencedirect.com/science/article/pii/S002200009791504X.\n\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William B Dolan. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pp. 1–9, 2007.\n\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. PPT: Pre-trained prompt tuning for fewshot learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8410–8423, Dublin, Ireland, May 2022. Association for Computational Linguistics.\n\nL.K. Hansen and P. Salamon. Neural network ensembles. IEEE Transactions on Pattern Analysis\n\nand Machine Intelligence, 12(10):993–1001, 1990. doi: 10.1109/34.58871.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171– 4186, 2019.\n\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252–262, 2018.\n\nYoung-Bum Kim, Karl Stratos, and Dongchan Kim. Domain attention with an ensemble of experts. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 643–653, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1060. URL https://aclanthology.org/ P17-1060.\n\nJoshua Lee, Prasanna Sattigeri, and Gregory Wornell. Learning new tricks from old dogs: Multisource transfer learning from pre-trained networks. Advances in neural information processing systems, 32, 2019.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n\nHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.\n\nJunnan Li, Ziwei Xu, Wong Yongkang, Qi Zhao, and Mohan Kankanhalli. Gradmix: Multi-source transfer across domains and tasks. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3019–3027, 2020.\n\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597, 2021.\n\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021a.\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\n\nunderstands, too. arXiv preprint arXiv:2103.10385, 2021b.\n\nYaoyao Liu, Bernt Schiele, and Qianru Sun. An ensemble of epoch-wise empirical bayes for few-\n\nshot learning. In European Conference on Computer Vision, pp. 404–421. Springer, 2020.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing\n\ninstructional prompts to gptk’s language. arXiv preprint arXiv:2109.07830, 2021.\n\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1267–1273, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\n\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics.\n\nSteven Reich, David Mueller, and Nicholas Andrews. Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast—Choose Three. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5583–5595, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.450. URL https://aclanthology.org/2020.emnlp-main.450.\n\nRemo Sasso, Matthia Sabatelli, and Marco A Wiering. Multi-source transfer learning for deep\n\nmodel-based reinforcement learning. arXiv preprint arXiv:2205.14410, 2022.\n\nRobert E. Schapire. The strength of weak learnability. Mach. Learn., 5(2):197–227, jul 1990. ISSN 0885-6125. doi: 10.1023/A:1022648800760. URL https://doi.org/10.1023/A: 1022648800760.\n\nTimo Schick and Hinrich Sch ̈utze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255–269, Online, April 2021a. Association for Computational Linguistics.\n\nTimo Schick and Hinrich Sch ̈utze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255–269, 2021b.\n\nTimo Schick and Hinrich Sch ̈utze. It’s not just size that matters: Small language models are also In Proceedings of the 2021 Conference of the North American Chapter of few-shot learners. the Association for Computational Linguistics: Human Language Technologies, pp. 2339–2352, 2021c.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013.\n\nJoe Stacey, Pasquale Minervini, Haim Dubossarsky, Sebastian Riedel, and Tim Rockt ̈aschel. Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8281–8291, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.665. URL https://aclanthology.org/2020. emnlp-main.665.\n\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou’, and Daniel Cer. SPoT: Better frozen model adaptation through soft prompt transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5039–5059, Dublin, Ireland, May 2022. Association for Computational Linguistics.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nXinyi Wang, Yulia Tsvetkov, Sebastian Ruder, and Graham Neubig. Efficient test time adapter ensembling for low-resource language varieties. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 730–737, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.63. URL https://aclanthology.org/2021.findings-emnlp.63.\n\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.\n\nDavid H. Wolpert.\n\nStacked generalization. Neural Networks, 5(2):241–259, 1992.\n\nISSN doi: https://doi.org/10.1016/S0893-6080(05)80023-1. URL https://www.\n\n0893-6080. sciencedirect.com/science/article/pii/S0893608005800231.\n\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. Differentiable prompt makes pre-trained language models better few-shot learners. In International Conference on Learning Representations, 2021.\n\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA DATASETS\n\nA.1 SOURCE TASKS\n\nFollowing existing work(Asai et al., 2022), we use six tasks that have been considered generally helpful to other NLP tasks as our source tasks. They are MNLI (Williams et al., 2018), QNLI (Demszky et al., 2018), QQP (Wang et al., 2018), SST2 (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016), and ReCoRD (Zhang et al., 2018). Details are shown in Table 6.\n\nTable 6: The tasks included in source models. NLI is natural language inference, and QA is question answering.\n\nDataset MNLI SST-2 QQP QNLI SQuAD ReCoRD\n\n|Train| Category\n\n393k 67k 364k 105k 88k 101k\n\nGLUE GLUE GLUE GLUE MRQA 2019 QA SuperGLUE QA\n\nDomain misc.\n\nTask NLI Sentiment analysis Movie Reviews Paraphrase QA/NLI\n\nSocial QA questions Wikipedia Wikipedia News (CNN, Daily Mail)\n\nMetric accuracy accuracy accuracy & F1 accuracy F1 & EM F1 & EM\n\nA.2 TARGET TASKS\n\nWe evaluate our model on the following GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) tasks: WNLI (Wang et al., 2018), MRPC (Dolan et al., 2005), BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018), RTE (Giampiccolo et al., 2007), WiC (Pilehvar & Camacho-Collados, 2019), WSC (Levesque et al., 2012), and CB (De Marneffe et al., 2019). Details are shown in Table 7.\n\nTable 7: The tasks included in source models. NLI is natural language inference, and QA is question answering.\n\nTask coreference/NLI Fiction books Paraphrase\n\nDomain\n\nDataset WNLI MRPC RTE MultiRC BoolQ WiC WSC CB\n\n|Train| Category\n\n634 3.7k 2.5k 5.1k 9.4k 6k 554 250\n\nGLUE GLUE SuperGLUE NLI SuperGLUE QA SuperGLUE QA SuperGLUE WSD SuperGLUE coreference SuperGLUE NLI\n\nNews News, Wikipedia various Google queries, Wikipedia WordNet, VerbNet, Wiktionary fiction books various\n\nMetric accuracy accuracy & F1 accuracy F1 & EM accuracy accuracy accuracy accuracy & F1\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nB IMPLEMENTATION DETAILS\n\nB.1 SOURCE PROMPTS TRAINING\n\nWe follow Asai et al. (2022)2 to train the source prompts, [P1; ...; P6] with corresponding source task data. These source prompts are prompt-tuned(Lester et al., 2021) respectively on MNLI (Williams et al., 2018), QNLI (Demszky et al., 2018), QQP (Wang et al., 2018), SST2 (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016), and ReCoRD (Zhang et al., 2018). Details can be found in Table 6. Soft tokens size (m) is 100 for all pre-trained models and embedding dimensions (d) are 768, 1024 and 1024 respectively for T5-{base, large, 3b}. The rest hyper-parameters are shown in Table 8.\n\nTable 8: Parameters of baseline and SESoM training. “Batch size” is different for T5-base, T5-large and T5-XL.\n\nParameters max source length learning rate train epoch optimizer batch size\n\nSource Prompt 512 3e-1 10\n\nSPoT t 256 3e-1 30\n\nPPT 256 3e-1 30 AdamW AdamW AdamW 32/16/4 32/16/4 32/16/4\n\nATTEMPT 256 3e-1 20\n\nSESoM 256 3e-1 20 AdamW AdamW 32/16/4 32/16/8\n\nB.2 BASELINES\n\nB.2.1 SPOT-TARGET (SPOT-T)\n\nWe first train a randomly initialized soft prompt on a target task, which is referred as target prompt, using the target few-shot training data. Then we calculate the cosine similarity between the target prompt Pt and the source prompts, [P1; ...; P6]. The source prompt Pi, which shares the highest similarity with Pt, is used to initialize the soft prompt training. We then prompt-tune this initialized soft prompt ˆPt on the same target few-shot training data. Finally, the source model [ ˆPt; θ] is tested with verbalizer (Appx. B.5) on the original validation set of target task. Hyper-parameter details can be found in Table 8.\n\nB.2.2 PPT\n\nFor a fair comparison, we replicate Gu et al. (2022) only with pre-training a soft prompt on MNLI, QNLI, QQP, SST2, SQuAD, and ReCoRD which we used for training source prompts. Following Gu et al. (2022), we firstly unify six source tasks to a single format—–multiple-choice classification. For example, CB is formatted as,\n\n#premise.\n\n#hypothesis. A. yes. B. maybe. C. no. The correct one is\n\nClassification text labels of CB is A(no), B(maybe) and C(yes). We then prompt-tune a randomly initialized soft prompt Pppt on these unified datasets. We use the same hyperparametes as training SPoT-t. Then for each target, Pppt is tuned on the same Dtrain and Ddev to get target-specific soft prompt ˆPppt. Finally, the source model [ ˆPppt; θ] is tested with verbalizer (Appx. B.5) applied on the same Dtest. See training details in Table 8.\n\nB.2.3 ATTEMPT\n\nFollowing Asai et al. (2022), [P1; ...; P6] (Appx. B.1) are used as source prompts. ATTEMPT first initialize a attention module between source prompts and inputs. ATTEMPT interpolates the source prompts and a newly initialized target-task-specific prompt Ptarget given attention scores generated by the attention module to produce a target soft prompt Pt, then [Pt; θ] is used to produce predictions. We use the same training parameters used in Asai et al. (2022), which is shown in Table 8.\n\n2https://github.com/AkariAsai/ATTEMPT\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nB.3 SESOM\n\nThe hyper-parameters of G are shown in Table 9.\n\nTable 9: Dimensions of G of SESoM used in Table 1.\n\nModel\n\nT5-base\n\nT5-large\n\nT5-XL\n\nl\n\nx\n\nx\n\nMethod d′ d′ d′ G’s dropout % d′ d′ d′ G’s dropout % d′ d′ d′ G’s dropout %\n\nx\n\nl\n\nl\n\nWNLI MRPC RTE MultiRC BoolQ WiC WSC CB 128 128 128 0\n64 64 256 0\n64 64 512 0\n\n100 100 100 0\n64 256 512 0\n32 64 64 50\n\n100 100 100 0\n100 64 100 0\n64 64 512 0\n\n100 100 100 0\n32 32 128 50 100 100 100 0\n\n64 64 64 0\n100 64 100 0\n64 128 256 0\n\n64 100 100 0\n32 32 64 50 64 64 64 50\n\n32 32 32 50 100 100 100 50 100 100 100 50\n\n32 32 128 0\n64 64 64 0\n32 64 64 0\n\nB.4 ABLATION STUDY\n\nBefore SESoM, we also tried some related methods that turned out to be less effective.\n\nEnsemble acc SP. Firstly, we tried consider to use input-prompt attention, where max-pool embedding input ˆx is used as key and source prompts [P1; ...; P6] are values to calculate attention score to computing a linear combination of pre-softmax logits. The architecture design of the attention module is the same with SESoM. The dimensions of the attention module after hyper-parameters tuning is shown in Table 10.\n\nTable 10: Hyper-parameters of G of “Ensemble acc SP” used in Table 4\n\nModel Method\n\nT5-base\n\nl\n\nx\n\nd′ d′ d′ G’s dropout %\n\nWNLI MRPC RTE MultiRC BoolQ WiC WSC CB 100 100 100 0\n\n128 128 128 0\n\n100 100 100 0\n\n64 64 128 0\n\n128 128 128 0\n\n64 64 64 0\n\n32 32 64 0\n\n32 32 32 50\n\nMajority vote. Secondly, we consider whether predictions from each source model [P1; θ], ..., [P6; θ] can provide enough information for final prediction. We get the pre-softmax logits Lx = [lx,1; ...; lx,6] then conduct verbalizer mapping to obtain ˆLx = [ˆlx,1; ...;ˆlx,6]. Then for each input, we obtain 6 predicted labels l1, ..., l6 from ˆLx. The majority vote of l1, ..., l6 is used as final prediction. For tie condition, we uniformly sample one of them as prediction.\n\nB.5 VERBALIZER DETAILS\n\nSource prompts are trained on six source datasets in a text-to-text format. Different source datasets have different verbalizers. For example, CB (De Marneffe et al., 2019) is classified as “entailment”, “neutral” and “contradiction”; and MRPC (Dolan et al., 2005) is classified as “equivalent” and “not equivalent”. We show the verbalizers used for all target task in Table 11. For rare cases in which the source model after few-shot prompt tuning is not able to transfer all the verbalizers from source dataset to target dataset, for all baselines and our method, we conduct a verbalizer mapping for pre-softmax logits.\n\nBased on original verbalizers in Table 11, we develop a mapping dictionary MT to map text to the token for target dataset T . For example, we map “True” to “1”, and “False” to “0” for the prediction of source models with source prompts trained on Squad(Rajpurkar et al., 2016) and Record(Zhang et al., 2018). Formally, for the ith source model, the pre-softmax logit li = [li,1; ...; li,v] obtained\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nfrom source model [Pi; θ] on target dataset T is transformed to ˆli by Algorithm 1, where v is the vocabulary size of tokens. More specifically, all the logits in each pre-softmax logit of li will be swapped to the same position. For example, in source model of ReCoRD, the pre-softmax logit of token 209 in ˆli is replaced with the maximum of the pre-softmax logit of token 209 and 10998 in li, because MT[10998] = 209, where 10998 is the first token representing “True” and 209 is the first token representing “1”.\n\nTable 11: Mapping dictionary details.\n\nTarget\n\nWNLI\n\nMRPC\n\nRTE\n\nBoolQ\n\nMultiRC\n\nWiC\n\nWSC\n\nCB\n\nLabel Text not entailment entailment not equivalent equivalent entailment not entailment False True False True False True False True entailment contradiction neutral\n\n# 0\n1 0\n1 0\n1 0\n1 0\n1 0\n1 0\n1 0\n1 2\n\nAlgorithm 1: Mapping Algorithm Data: MT, li Result: ˆli ˆli ← li; for k ∈ MT do\n\nˆli,k ← min(li,k, li,MT[k]) ; ˆli,MT[k] ← max(li,k, li,M[k]);\n\nend\n\nB.6\n\nINFERENCE TIME\n\nCompared to non-ensemble baselines on the same pre-trained language model, SESoM indeed increases inference time due to the multiple forward passes of source models. In order to achieve a fair comparison, we compare SESoM to prompt fusion model (ATTEMPT(Asai et al., 2022)), which uses the same number of source tasks as SESoM. As a sample-specific knowledge fusion method, ATTEMPT fuses N soft prompts of source tasks together to construct a new prompt for each target sample and foward it together with the target sample to the pre-trained language model, which is one forward pass per sample for inference. While SESoM runs N forward passes for each sample to obtain the source logits.\n\nAlthough SESoM requires more forward passes given one target sample, it requires a smaller pretrained language model (shorter inference time per forward pass) to achieve the same /or even better performance. SESoM with a small pre-trained model (shorter inference time per forward pass) can outperform non-ensemble baseline with a much larger pre-trained model (longer inference time per forward pass). For example, SESoM with T5-base outperforms ATTEMPT with T5-XL on few-shot classification (please see Table 1). The inference time of SESoM with T5-base is 31.38s while the inference of ATTEMPT with T5-XL takes 38.13s, which shows that the inference time of SESoM with T5-base is shorter. The inference time is the average of 20 runs with different random seeds on\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nall 8 target tasks. It indicates that SESoM can achieve a better performance with shorter inference time.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nC ANALYSIS ON ATTENTION\n\nFigure 2 shows the average attention weights of G between source and target tasks on Dtest. The average weights are produced by the SESoM model with T5-base and hyper-parameters are shown in Appx. B.3. G has higher attention score between more similar target and source tasks. G gives significantly higher attentions to QQP for MRPC and WSC or MNLI for RTE and WNLI. MRPC and QQP are both tasks to determine whether a pair of questions/sentences are semantically equivalent. WSC is coreference resolution task to determine the correct referent of the pronoun from the provided noun, which shares similarities with QQP as well. Besides, WNLI, RTE,and MNLI are all textual entailment challenges, which share similarities between each other.\n\nWe also find the attention weights and F1 scores are highly positive correlated on MRPC, RTE, MultiRC, WSC and CB tasks. Pearson product-moment correlation coefficients between attention weights and F1 scores are shown as follows,\n\nFigure 2: Average attention weights of G.\n\nTarget Task WNLI MRPC RTE MultiRC BoolQ WiC WSC CB 0.45 Corr. coeff.\n\n-0.29\n\n0.44\n\n0.42\n\n0.63\n\n0.46\n\n0.03\n\n0.75\n\nHigh F1 scores of source model on target task also indicate underlying task similarities. FixedWeight ensemble model takes advantage of these F1 scores to get a weighted average of all source pre-max logits to make predictions. But as we showed in Table 1 and Section 4.2, these weights are not enough to obatin the best performance on target tasks.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nD MORE RESULTS\n\nD.1 EFFECT OF THE NUMBER OF SHOTS\n\nBesides our main experiments (Table 3), we also reported results on target tasks with 8 training samples in Table 12. The performance of SESoM is better than all the baselines with 32 training samples, which are shown in Table 1.\n\nTable 12: Results on target tasks with 8 samples of few-shot training labels. All the scores are the average of 10 runs, with standard errors included in subscripts.\n\nData Size Method\n\n8\n\nFW Ensemble SESoM\n\nWNLI MRPC RTE MultiRC BoolQ WiC WSC CB 53.73 58.31\n\n72.44 63.17 51.44 61.52 61.903.97 71.27 62.73 54.97 66.61 65.313.14\n\n71.10 78.01\n\n63.17 66.91\n\n58.70 63.65\n\nAvg\n\nD.2 EFFECT OF PROMPT-TUNING SOURCE MODELS\n\nIn main experiments, we train each source model with the few-shot target data in a prompt-tuning manner. We wonder how SESoM would perform when we skip this step. Therefore, we tried to perform SESoM on non-tuned source models. Results are presented in Table 13. We can find that with only 8 and 32 samples of few-shot training samples, SESoM with prompt-tuned source models cannot achieve significantly better performance than SESoM with non-tuned source models. However, when the number of few-shot labeled target data increase, SESoM with prompt-tuned source models achieve better performance, because the source models themselves have a better performance on target datasets.\n\nTable 13: Results on target tasks with different numbers of few-shot training samples. “SESoM w/ PT” represents SESoM with prompt-tuned source models; “SESoM w/o PT” is SESoM with nontuned source models. All the scores of data size of 8, 32 and 128 are the average of 10, 10 and 5 runs, with standard errors included in subscripts.\n\nData Size Method\n\nWNLI MRPC RTE MultiRC BoolQ WiC WSC CB\n\nAvg\n\n8\n\n32\n\n128\n\nSESoM w/o PT 58.24 58.31 SESoM w. PT SESoM w/o PT 55.21 58.31 SESoM w. PT SESoM w/o PT 58.24 58.10 SESoM w. PT\n\n76.34 78.01 82.57 84.22 83.61 84.61\n\n63.16 63.65 65.20 66.86 65.89 68.09\n\n66.45 66.91 63.53 66.72 63.89 68.87\n\n69.50 63.08 54.97 67.41 64.893.46 71.27 62.73 54.97 66.61 65.313.14 74.89 62.74 53.37 75.18 66.593.81 72.87 64.20 53.56 72.86 67.452.41 75.74 63.52 55.29 74.29 67.572.57 75.93 63.73 61.54 82.86 70.461.06\n\nD.3 EFFECT OF ENSEMBLE\n\nWe wonder whether our ensemble method is better than choosing single most appropriate source model’s prediction as final prediction. We implement a hard variant, where after attention weights are computed, we set the argmax position to 1 and all remaining entries to 0s. Results are presented in Table 14. It shows that our ensemble attention is useful compared to selecting the single most appropriate source model.\n\nTable 14: Results on target tasks with 32 samples of few-shot training labels with different ensemble. All the scores are the average of 20 runs, with standard errors included in subscripts.\n\nMethod Hard Variant SESoM\n\nWNLI MRPC RTE MultiRC BoolQ WiC WSC 49.18 53.58 48.52 53.56 66.91 56.34\n\n56.17 63.68\n\n54.49 66.66\n\n60.53 74.80\n\n73.00 84.34\n\nCB 56.52 74.02\n\nAvg 56.507.66 67.542.16\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nE CASE STUDY\n\nTable 15: Case study from WSC and MultiRC target task. “Label” (yellow cells) is ground truth label and “Pred” (yellow cells) is the predicted label obtained by SESoM with the weights (orange cells) shown in the table. “Preds from individual source” (pink cells) shows predictions of each source model [P1,t; θ], ..., [P6,t; θ] obtained by the pre-softmax logits [lx,1; ...; lx,T ].\n\ntext: Alice tried frantically to stop her daughter from barking at the party, leaving us to wonder why\n\nExample # 1\n\n[WSC]\n\nshe was behaving so strangely.\n\ns 1: Alice s 2: She Pred Label Preds from individual source Weights from SESoM\n\n0 (False) 0 (False)\n\nSource Models\n\nMNLI 1 ✗ 0.0480\n\nSST2 0 ✓ 0.3348\n\nQNLI 1 ✗ 0.0912\n\nQQP 0 ✓ 0.3270\n\nSQuAD ReCoRD 1 ✗ 0.0984\n\n1 ✗ 0.1005\n\ntext: Well satisfied with his purchases and feeling very elegant indeed, Babar goes to the photographer\n\nExample # 2\n\n[WSC]\n\nto have his picture taken.\n\ns 1: photographer s 2: his Pred Label Preds from individual source Weights from SESoM\n\n0 (False) 0 (False)\n\nSource Models\n\nMNLI 1 ✗ 0.0732\n\nSST2 1 ✗ 0.0285\n\nQNLI 1 ✗ 0.0972\n\nQQP 0 ✓ 0.5973\n\nSQuAD ReCoRD 1 ✗ 0.1048\n\n1 ✗ 0.0991\n\nquestion: Sarah introduces him to three other guests. Name them. paragraph: The bar was manned by an expensive humanoid robot. It turned toward Sarah’s wave and\n\nExample # 3\n\n[MultiRC]\n\nacknowledged her with a nod, moments later setting a fluted glass of sparkling liquid in front of her. I marveled at the robot’s smoothness and coordination. Clearly, it was a high-end model. Sarah transferred the glass to my free hand and pulled me away from the bar for more introductions, with Alexis trailing after us. I spent the evening listening, mostly. Listening and stuffing my face with all the bits of fine food provided. No one minded; Sarah’s inner circle was content to fill our circle of couches with plenty of chatter. Ray, a plump man who was grey where he wasn’t bald. Zheng, short and dark and lean, with a very intense gaze. He made me a little uncomfortable. Kishori, petite, her hair strung out in a series of braids that reached nearly to her waist.\n\nanswer: Ray, Zheng, and Kishori Pred Label Preds from individual source Weights from SESoM\n\n1 (True) 1 (True)\n\nSource Models\n\nMNLI 1 ✓ 0.2094\n\nSST2 0 ✗ 0.1028\n\nQNLI 0 ✗ 0.1098\n\nQQP 0 ✗ 0.1019\n\nSQuAD ReCoRD 1 ✓ 0.2377\n\n1 ✓ 0.2383\n\nExample # 4\n\n[MultiRC]\n\nquestion: What were Zheng’s traits? paragraph: Same with Example # 3. answer: Humanoid Pred Label Preds from individual source Weights from SESoM\n\n0 (False) 0 (False)\n\nMNLI 0 ✓ 0.2347\n\nSource Models\n\nSST2 0 ✓ 0.1932\n\nQNLI 1 ✗ 0.1891\n\nQQP 1 ✗ 0.1145\n\nSQuAD ReCoRD 1 ✗ 0.1386\n\n1 ✗ 0.1300\n\nWe show more examples of attention weights on samples from different target task in Table 15, Table 16 and Table 17. They show the superior power of SESoM to give higher attention to those pre-softmax logits, which prefers correct label.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nTable 16: Case study from BoolQ and WiC target task. “Label” (yellow cells) is ground truth label and “Pred” (yellow cells) is the predicted label obtained by SESoM with the weights (orange cells) shown in the table. “Preds from individual source” (pink cells) shows predictions of each source model [P1,t; θ], ..., [P6,t; θ] obtained by the pre-softmax logits [lx,1; ...; lx,T ].\n\nquestion: Did the stock market crash of 1929 caused the great depression? passage: Wall Street Crash of 1929 – The Wall Street Crash of 1929, also known as Black Tuesday,\n\nExample # 5\n\n[BoolQ]\n\nthe Great Crash, or the Stock Market Crash of 1929, began on October 24, 1929, and was the most devastating stock market crash in the history of the United States, when taking into consideration the full extent and duration of its after effects. The crash, which followed the London Stock Exchange’s crash of September, signalled the beginning of the 12-year Great Depression that affected all Western industrialized countries.\n\nPred 1 (True) Label 1 (True) Preds from individual source 1 ✓ Weights from SESoM\n\nMNLI\n\n0.0194\n\nSST2 0 ✗ 0.0036\n\nSource Models QQP 0 ✗ 0.0627 0.4285\n\nQNLI 1 ✓ 0.0073\n\nSQuAD ReCoRD 1 ✓\n\n1 ✓ 0.4785\n\nExample # 6\n\n[BoolQ]\n\nquestion: Do house and cuddy get back together in season 8? passage: Lisa Cuddy – The relationship between House and Cuddy is known by the portmanteau term, Huddy. Cuddy has what USA Today’s Peter Johnson terms a cat-and-mouse relationship with House. Edelstein has described it as “a really complicated, adult relationship”, explaining: “These are people who have very full lives and lots of responsibilities that perhaps conflict with their feelings for each other.” The actress “would love for them to have a (romantic) relationship, because it could be as complicated as the rest of their relationship”, however, she is unsure how it would affect the dynamics of the show. Jacobs commented at the end of the show’s third season: “I can’t see them pairing them in a permanent fashion. But they are close; they have gone through a lot together. Might there be a moment of weakness in which the two might explore their chemistry? Maybe.” Questioned at the end of the fourth season on whether Cuddy and House.\n\nPred 0 (False) Label 0 (False) Preds from individual source 1 ✗ Weights from SESoM\n\nMNLI\n\n0.0100\n\nSST2 0 ✓ 0.3151\n\nSource Models QQP 0 ✓ 0.0878 0.3977\n\nQNLI 1 ✗ 0.0176\n\nSQuAD ReCoRD 0 ✓\n\n0 ✓ 0.1717\n\nExample # 7\n\n[WiC]\n\ns 1: An emerging professional class. s 2: Apologizing for losing your temper, even though you were badly provoked, showed real class. Pred 0 (False) Label 0 (False) Preds from individual source 1 ✗ Weights from SESoM\n\nQNLI 1 ✗ 1.6376e−4 2.4063e−4 2.1660e−4 0.2839 0.3430\n\nSource Models QQP 0 ✓\n\nSQuAD ReCoRD 0 ✓\n\n0 ✓ 0.3723\n\nSST2 1 ✗\n\nMNLI\n\nExample # 8 s 1: He could not touch the meaning of the poem. s 2: Helen Keller felt the physical world by touching people and objects around her. Pred 0 (False) Label 0 (False) Preds from individual source 1 ✗ Weights from SESoM\n\nSource Models QQP 0 ✓ 0.1120 0.4110\n\nQNLI 1 ✗ 0.0121\n\nSST2 0 ✓ 0.0904\n\n0.0375\n\nMNLI\n\nSQuAD ReCoRD 0 ✓\n\n0 ✓ 0.3371\n\n[WiC]\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nTable 17: Case study from CB and RTE target task. “Label” (yellow cells) is ground truth label and “Pred” (yellow cells) is the predicted label obtained by SESoM with the weights (orange cells) shown in the table. “Preds from individual source” (pink cells) shows predictions of each source model [P1,t; θ], ..., [P6,t; θ] obtained by the pre-softmax logits [lx,1; ...; lx,T ].\n\nExample # 9\n\n[CB]\n\npremise: Who knows? The point is, do we go with it or not? Do we assume there is a shipment? hypothesis: there is a shipment. Pred 2 (Neutral) Label 2 (Neutral) Preds from individual source Weights from SESoM\n\nQNLI 1 ✗ 1.8867e−4 9.433e−5\n\nMNLI 0 ✗ 0.0310\n\nSST2 0 ✗ 0.0260\n\nSQuAD ReCoRD 2 ✓ 0.4084\n\n2 ✓ 0.5342\n\nSource Models\n\nQQP 0 ✗\n\nExample # 10\n\n[CB]\n\npremise: But what we may not know is just what makes somebody a sucker. What makes people blurt\n\nout their credit-card numbers to a caller they’ve never heard of? Do they really believe that the number is just for verification and is simply a formality on the road to being a grand-prize winner?\n\nhypothesis: The number is just for verification and is simply a formality on the road to being a\n\ngrand-prize winner.\n\nPred 1 (Contradiction) Label 1 (Contradiction) Preds from individual source Weights from SESoM\n\nSource Models\n\nMNLI 1 ✓ 0.7075\n\nSST2 0 ✗ 2.1385e−3 0.1998\n\nQNLI 1 ✓\n\nQQP 0 ✗ 1.2705e−4 0.0234\n\nSQuAD ReCoRD 1 ✓\n\n1 ✓ 0.0668\n\npremise: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\n\nExample # 11\n\n[RTE]\n\naccording to the Christopher Reeve Foundation.\n\nhypothesis: Christopher Reeve had an accident. Pred 1 (not entailment) Label 1 (not entailment) Preds from individual source Weights from SESoM\n\nSST2 0 ✗ 0.0092\n\nMNLI 1 ✓ 0.5337\n\nSource Models\n\nQNLI 1 ✓ 0.2753\n\nQQP 0 ✗ 0.0560\n\nSQuAD ReCoRD 1 ✓ 0.0825\n\n1 ✓ 0.0434\n\npremise: Hands Across the Divide was formed in March 2001, and one of its immediate aims was to press\n\nfor more freedom of contact and communication right away between the two parts of Cyprus, and for early progress towards a solution to “the Cyprus problem”.\n\nExample # 12\n\n[RTE]\n\nhypothesis: Cyprus was divided into two parts in March 2001. Pred 1 (not entailment) Label 1 (not entailment) Preds from individual source Weights from SESoM\n\nSST2 0 ✗ 0.1879\n\nMNLI 0 ✗ 0.0698\n\nQNLI 1 ✓ 0.3535\n\nSource Models\n\nQQP 0 ✗ 0.1364\n\nSQuAD ReCoRD 0 ✗ 0.0259\n\n1 ✓ 0.2265\n\n23",
    "reference": "# Summary Of The Paper\n\nThe paper prose SESoM can adjust the contribution of each model when ensembling the model outputs. SESoM uses an attention module to calculate the attention of each model’s output. The proposed SESoM can achieve SOTA performance in both full-data settings and few-shot settings.\n\n# Strength And Weaknesses\n\nThe main strengths: \n1. The paper is well-written and the idea is well-motivated. \n\n2. The experiments are quite comprehensive. The empirical analysis section answers several interesting questions. \n\n3. The empirical study that compares the prediction ensemble with the prompt ensemble is quite interesting and can inspire many related fields. \n\nThe main weaknesses: \n1. The idea that combining the output of several models using the attention strategy is not novel in deep learning. \n\n2. I can not understand why sample-specific rather than task-specific preference is important for prompt tuning. What if a similar sample exists in a quite different source task? Will the attention model pays more attention to this similar sample resulting in a negative impact on the target task performance since the source task and target task are quite different?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written but the idea is not novel.\n\n# Summary Of The Review\n\nThe paper prose SESoM can adjust the contribution of each model when ensembling the model outputs.  The experiments are comprehensive. The idea is not novel.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nLEARNING TO DECOMPOSE VISUAL FEATURES WITH LATENT TEXTUAL PROMPTS\n\nFeng Wang1, Manling Li2, Xudong Lin3, Hairong Lv1, Alexander G. Schwing2 & Heng Ji2 1Tsinghua University 2University of Illinois at Urbana-Champaign 3Columbia University\n\nABSTRACT\n\nRecent advances in pre-training vision-language models like CLIP (Radford et al., 2021) have shown great potential in learning transferable visual representations. Nonetheless, for downstream inference, CLIP-like models suffer from either 1) degraded accuracy and robustness when inferring by retrieving textual class names (the zero-shot protocol); or 2) breaking the well-established vision-language alignment (linear probing). To combine the best of both worlds, we propose Decomposed Feature Prompting (DeFo). DeFo maintains the dual-model architecture yet leverages learnable embeddings as textual input and performs classification with an additional linear layer. As a result, we find DeFo to be able to extract decomposed visual features with the help of textual prompts and to allow a scalable size of language inputs. Our empirical study shows DeFo’s significance in improving the vision-language models. For example, DeFo obtains 73.2% test accuracy on ImageNet with a ResNet-50 backbone without tuning any pretrained weights of both the vision and language encoder, outperforming zero-shot CLIP by a large margin of 15.0%, and outperforming state-of-the-art vision-language prompt tuning by 7.6%.\n\n1\n\nINTRODUCTION\n\nLanguage-guided visual pretraining has gained a lot of attention and shows great promise in learning transferable image representations. By establishing a connection between images and natural language, recent vision-language models are able to turn visual inference over a restricted number of classes into zero-shot open-vocabulary inference (Radford et al., 2021; Jia et al., 2021; Pham et al., 2021).\n\nOne of the recent successes for zero-shot inference is the contrastive language-image pretraining (CLIP) model (Radford et al., 2021). It uses 400 million image-text pairs to learn an alignment between visual and textual representations obtained from a vision encoder and a language encoder respectively. In downstream applications, CLIP-like models (Radford et al., 2021; Jia et al., 2021; Pham et al., 2021) then perform zero-shot inference by hard-target retrieval, i.e., they directly compute the distance between a vectorial image representation obtained from the vision encoder, and representations of text prompts (e.g., “a photo of an airplane” or “a photo of an automobile”) obtained from the language encoder. The target class (e.g., “airplane” or “automobile”) corresponding to the text prompt with the smallest distance to the vector representing the image constitutes the zero-shot inference result. When annotations are given, simple linear probing (i.e., removing the language encoder, fine-tuning of the vision encoder and training of a classifier on top of the vision encoder) further improves the results (Radford et al., 2021). Moreover, context optimization (CoOp) (Zhou et al., 2021) replaces the hand-crafted prefix or suffix (e.g., “a photo of a”) of the text prompts by trainable embedding vectors.\n\nHowever, the zero-shot CLIP and CoOp infer using hard textual targets, i.e., the class names, which results in two main challenges. First, class names in text prompts (e.g., “airplane” or “automobile”), as used in zero-shot CLIP and CoOp inference, do not permit to accurately summarize the semantic information of an image. Therefore, inference is very sensitive to the words chosen for class names. We refer to this challenge as expressive sensitivity. Empirically, this challenge causes zero-shot CLIP and CoOp to struggle to achieve as competitive results as linear probing with the same image encoder when downstream training data is available (e.g., 58.2% accuracy vs. 72.3%\n\n1\n\nPublished as a conference paper at ICLR 2023\n\non ImageNet (Deng et al., 2009)). Moreover, this sensitivity can be observed by modifying class names. Fore example, for zero-shot inference on CIFAR-10 (Krizhevsky et al., 2009), CLIP obtains an accuracy of 63.7% when the original class names are used. Notably, simply replacing or extending the class names with suitable synonyms1 (e.g., “plane” and “car” rather than “airplane” and “automobile”) can improve accuracy to 79.6%, which highlights the challenge of expressive sensitivity.\n\nSecond, despite the fact that hundreds of millions of pretraining samples cover a large number of concepts that can possibly appear in downstream datasets, zero-shot inference continues to struggle to recognize rare objects. We refer to this as the conceptual sensitivity. For example, zero-shot CLIP is only 38.5% accurate when classifying EuroSAT satellite images (Helber et al., 2019), which is much lower than the result of a supervised ResNet-50 (He et al., 2016) encoder (93.4%). Also, zero-shot CLIP with a ResNet-50 encoder achieves less than 90% accuracy on MNIST (LeCun, 1998), which can even be outperformed by a simple logistic regression model. While linear probing is a straightforward way to improve results, removing of the language encoder breaks the visionlanguage alignment that is learned from the pretraining data, and therefore degrades few-shot and transfer learning performance.\n\nIn this paper, we propose Decomposed Feature Prompting (DeFo), which turns the hard-targetretrieval paradigm of CLIP and CoOp into dual-model feature prompting. Specifically, DeFo 1) provides to the language encoder a set of learnable embedding sequences which are independent of the hard semantic targets; and 2) performs classification by tuning an additional layer. As a result, DeFo does not rely on the textual representations of class names being classification targets, which addresses the issues of expressive sensitivity and conceptual sensitivity. Meanwhile, DeFo maintains the dual-model architecture, which enables the model to leverage the language information, so that few-shot and transfer learning performance can be boosted.\n\nDeFo results show the significance of addressing the sensitivity challenges of CLIP-like models. For example, with a ResNet-50 backbone, DeFo achieves 73.2% test accuracy on ImageNet without modifying any pretrained weight of the image and text encoders, outperforming vanilla CLIP by a large margin of 15.0% and outperforming CoOp by 7.6%. In a variety of visual contexts, DeFo attains an average accuracy of 79.9% over 11 image classification benchmarks, which is 21.0% higher than that of zero-shot CLIP and 6.2% higher than CoOp.\n\n2 RELATED WORK\n\nPretraining-finetuning has long been a dominant paradigm of transfer learning in machine learning, computer vision, and natural language processing. Generally, pretraining a vision encoder by generative objectives (Bao et al., 2021; He et al., 2022) or discriminative objectives (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2021) at the scale of one to ten million images (Deng et al., 2009) is sufficient to yield good visual representations and strong predictive performance in downstream visual tasks. However, without the supervision from other modalities, such pretrained models require task-specific finetuning (Bao et al., 2021; He et al., 2022; O Pinheiro et al., 2020; Wang et al., 2022a; Lin et al., 2022a) or linear probing He et al. (2020); Chen et al. (2020) for reasonably domain-adapted predictions.\n\nThe contrastive language-image pretraining (CLIP) (Radford et al., 2021) method instead jointly pretrains a vision encoder and a text encoder on 400 million curated image-text pairs, with a contrastive objective (Gutmann & Hyv ̈arinen, 2010) that matches the visual and textual representations. In downstream applications, CLIP achieves competitive results in various vision or vision-language tasks such as image classification (Zhou et al., 2021; Gao et al., 2021), dense prediction (Rao et al., 2022), video-language tasks (Luo et al., 2021; Lin et al., 2022b; Wang et al., 2022b), image manipulation (Patashnik et al., 2021), and multimedia event extraction (Li et al., 2022).\n\nFollowing the success of CLIP, the ALIGN (Jia et al., 2021) model leverages a noisy dataset of 1.8 billion image-text pairs to scale up vision-language representation learning, and the BASIC (Pham et al., 2021) model further scales up this approach in terms of data and model size. Based on the success of CLIP-like vision-language pretraining, a series of follow-up inference approaches are proposed to improve classification results. For example, Zhou et al. (2021) propose CoOp to learn\n\n1We use WordNet (Fellbaum, 2010) to find synonyms.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ncontext information in downstream datasets, and Gao et al. (2021) propose CLIP-Adapter to learn domain-adaptation for vision-language models. Further, following CoOp, Zhou et al. (2022) propose CoCoOp to enhance the performance in unseen classes; and similarly, following CLIP-Adapter, Zhang et al. (2021) propose Tip-Adapter to explore non-parametric adaptation layers. Despite the progress these methods (Zhou et al., 2021; Gao et al., 2021; Zhou et al., 2022) have achieved in downstream predictive performance, they do not change CLIP’s inference paradigm of retrieving class names. Hence, the challenges of expressive sensitivity and conceptual sensitivity remain.\n\n3 METHODOLOGY\n\nAs shown in Figure 1, our DeFo follows the dual-model architecture of CLIP, i.e., we use a vision encoder and a language encoder which map the visual inputs and textual inputs into the same latent space. However, in DeFo, the language encoder plays a different role from that in the zeroshot CLIP. Specifically, CLIP directly constructs hard targets for classification by feeding the language encoder with k textual queries (e.g., “a photo of cat”, “a photo of dog”, . . . ), where k is the number of classes and each query corresponds to a specific one. As explained in Section 1, this inference protocol leads to expressive sensitivity and conceptual sensitivity challenges which incurs degradation of accuracy and robustness.\n\nFigure 1: An architectural comparison between our DeFo and CLIP. “V” and “L” denotes vision and language encoder respectively and their weights are fixed. DeFo leverages sequences of trainable embedding vectors ([vj i ]) as textual input and maps decomposed visual features by a linear layer.\n\nIn contrast, in DeFo, we change the existing paradigm of hard-target retrieval while maintaining the vision-language encoder architecture to learn decomposed visual features. Specifically, DeFo aims to utilize the language encoder to construct a projection matrix that maps the visual features from the d-dimensional CLIP latent space to a new n-dimensional feature space. To this end, we feed the language encoder with n trainable text queries and then perform classification by an additional linear layer. By jointly tuning both the text queries and the classification layer, DeFo is able to learn textual prompts of detailed visual features and a robust feature mapping for classification.\n\nOverall, DeFo has two main benefits compared to CLIP-like models. First, compared with hardtarget-based inference protocols such as the zero-shot CLIP and CoOp (Zhou et al., 2021), DeFo removes the expressive and conceptual sensitivity challenges which significantly improves accuracy and robustness of downstream performance (see Table 1 and 4). Next, compared with linear probing which discards textual information, the optimization of the projection matrix in DeFo is bounded by the text encoder which results in the need for much fewer training samples to achieve good performance (see Table 2). Moreover, also note that in DeFo the number of textual queries n is independent of the number of classes k, so the query size is scalable to fit specific downstream tasks. Next, we detail the DeFo and compare it to existing methods.\n\n3.1 DUAL-MODEL INFERENCE\n\nAs shown in Figure 1, DeFo uses a visual encoder gV : Rw×h×3 → Rd and a language encoder gL : Rm×de → Rd to extract image and text representations, respectively. For this, the visual inputs are 3-channel images of shape w × h, and the language inputs are sentences with m words where each word is embedded into a de-dimensional vector. Both the visual and textual features are then mapped into a d-dimensional latent space, i.e., we get an image representation vector fI ∈ Rd and T ∈ Rd, where n denotes the number of query sentences n text representation vectors f 1 used for the encoder gL. By applying the dot product between fI and each of the f i T (note that both\n\nT , . . . , f n\n\nT , f 2\n\n3\n\nLVImage“A photo of cat”“A photo of dog”... (k queries)weightslogitsZero-Shot CLIPLVImageweightslogitsLineargradientDeFo (ours)n×m×den×m×dew×h×3w×h×3n×dn×dddnnkkw×h×3w×h×3k×dk×dddkk[v1] [v1] ... [v1 ]1 2 m[v2] [v2] ... [v2 ]1 2 m... (n queries)Published as a conference paper at ICLR 2023\n\nfI and f i where the i-th element measures the similarity between the image and the i-th text query.\n\nT are l2 normalized vectors, i.e., ∥fI ∥2 = ∥f i\n\nT ∥2 = 1), we get an n-dimensional vector,\n\nCLIP and CoOp directly use this vector to predict the label of the image, because each text query in their settings corresponds to a specific class. Formally, CLIP and CoOp have n = k, where k is the number of classes to be inferred, and the probability of the image belonging to the i-th class is computed by\n\npi =\n\nexp(⟨fI , f i j=1 exp(⟨fI , f j\n\nT ⟩/τ )\n\nT ⟩/τ )\n\n(cid:80)k\n\n,\n\n(1)\n\nwhere ⟨·, ·⟩ denotes the dot product and τ is a temperature coefficient.\n\nInstead, DeFo decouples the text queries from specific classes. Specifically, we use a scalable number of queries, i.e., the number n is not limited to be equal to k, and perform classification by an additional linear layer that maps the n-dimensional feature vectors to k-dimensional vectors. The probabilities are then computed by the softmax of the k-dimensional vector. Note that only this linear classification layer and the textual queries are trainable in DeFo. We fix the weights of both the text encoder and the image encoder to maintain the vision-language alignment.\n\n3.2 TRAINABLE TEXT EMBEDDINGS\n\nThe language encoder gL receives sequences of de-dimensional embedding vectors as input. When natural language is used, each word in the vocabulary first needs to be encoded into a de-dimensional embedding. In DeFo, we skip the process of designing hand-crafted prompts with natural language. Instead, we directly optimize the word embeddings via back-propagation. Specifically, we initialize n independent sequences of text embeddings where each sequence consists of m de-dimensional vectors in the form of “[v1] [v2] . . . [vm]”. The total “textual” input of DeFo can be written as a tensor XL ∈ Rn×m×de. Note that here we assign the same length m to each query for easy comprehension and implementation. In practice, the design of DeFo’s input is more flexible and the length of each query is not required to be identical.\n\nBy optimizing XL, DeFo makes CLIP-like vision-language models free from both hand-crafted prompts and annotations such as class names. In this way we address the issues of expressive and conceptual sensitivity caused by using class names as hard targets.\n\n3.3 COMPARISON TO EXISTING METHODS\n\nAs illustrated in Figure 1, zero-shot CLIP has no trainable parameters. The textual queries are composed by a hand-crafted prompt and class names that describe the semantic targets of the categories. The linear-probing CLIP uses only the vision encoder for classification. Without the assistance of textual representations, this method has to utilize an additional linear layer to map the visual features from the latent space (d-dimensional) to the output space (k-dimensional), which introduces N = d × k additionally trainable parameters. CoOp (Zhou et al., 2021) mostly follows the architecture of zero-shot CLIP, yet replaces CLIP’s hand-crafted prompt by a sequence of trainable text embeddings, with N = k × m × de learnable parameters for class-specific prompts.\n\nIntuitively, both CoOp and our DeFo use trainable text embeddings as inputs of the language encoder. Both methods differ in that the number of textual queries n is independent from the number of classes k for DeFo, and the queries are not composed using class names. Therefore, DeFo has a scalable size of additionally learnable parameters. Specifically, it introduces in total N = n × (m × de + k) trainable parameters, which scales linearly with the number of queries n. For example, with n = 256 and m = 16 in ImageNet (k = 1000), DeFo introduces 2.4M learnable parameters while attaining 72.3% accuracy, which outperforms CoOp (65.6%) who has 8.2M learnable parameters and CLIP-Adapter (63.6%) who has 1M learnable parameters.\n\nIn addition, compared to linear probing which directly maps the d-dimensional latent features to output logits, DeFo also uses a linear layer but maps n-dimensional features. In this way, DeFo is able to first project visual features with the assistance of n textual representation vectors, which provides DeFo with significantly better few-shot performance and interpretability than linear probing.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Test accuracy on ImageNet (%). Results with † are taken from Zhou et al. (2021), and those with ‡ are taken from Zhang et al. (2021). Our results are marked in gray . The best results are bolded. The results without using text encoder are de-emphasized.\n\nMethod Zero-Shot CLIP (Radford et al., 2021) Linear-Probing CLIP Prompt Ensembling CoOp (Zhou et al., 2021) CoCoOp (Zhou et al., 2022) Target Optimization (our ablation) CLIP-Adapter (Gao et al., 2021) Tip-Adapter (Zhang et al., 2021) DeFo (ours)\n\nRN-50 58.2 72.8 60.4† 65.6 65.1 71.4 63.6‡ 62.0‡ 73.2\n\nRN-101 ViT-B/32\n\n61.5 75.5 62.5† 67.8 67.1 73.2 65.4‡ 64.8‡ 75.5\n\n62.0 76.0 63.7† 68.0 -\n74.0 66.2‡ 65.6‡ 76.2\n\nViT-B/16 66.9 79.5 68.7† 72.4 -\n78.1 71.1‡ 70.8‡ 80.2\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETUP\n\n4.1.1 BASELINE MODELS\n\nDeFo is based on CLIP (Radford et al., 2021) for an easy comparison to the other baselines (Zhou et al., 2021; 2022; Gao et al., 2021). For CLIP, we mainly explore two inference protocols, zero-shot and linear probing. Zero-shot CLIP requires no extra training data and it infers by directly matching image representation to the text representation of class names with hand-crafted prompts. Linearprobing CLIP drops the text encoder and instead attaches a randomly initialized linear layer to the image encoder, and then tunes only this linear layer with downstream training data for domainadapted classification.\n\nCoOp (Zhou et al., 2021) and CLIP-Adapter (Gao et al., 2021) succeed in improving CLIP inference performance so they serve as the primary baselines to our DeFo. To give more comprehensive results, we also compare DeFo in ImageNet to the recent baselines of CoCoOp (Zhou et al., 2022) and Tip-Adapter (Zhang et al., 2021), which are direct extensions for CoOp (Zhou et al., 2021) and CLIP-Adapter (Gao et al., 2021). Note that we do not expect CoCoOp and Tip-Adapter to yield better results than their base models CoOp and CLIP-Adapter because they are proposed to address a different problem (discussed in Section 2). We report the results of Tip-Adapter without its further fine-tuning (Zhang et al., 2021) and all the baselines follow the pre-processing of CoOp for a fair comparison. Further, in this paper, we develop another baseline called “Target Optimization”, which uses learnable embedding vectors as class names combined with a hand-crafted prompt prefix. Target Optimization can be regarded as an ablated version of DeFo, which helps to understand the importance of the learnable embeddings.\n\n4.1.2 DATASETS\n\nWe follow prior methods to select 11 publicly available datasets, i.e., ImageNet (Deng et al., 2009), Food101 (Bossard et al., 2014), OxfordPets (Parkhi et al., 2012), Caltech101 (Fei-Fei et al., 2004), SUN397 (Xiao et al., 2010), UCF101 (Soomro et al., 2012), StanfordCars (Krause et al., 2013), FGVCAircraft (Maji et al., 2013), DTD (Cimpoi et al., 2014), Flowers102 (Nilsback & Zisserman, 2008), and EuroSAT (Helber et al., 2019). The categories in these 11 datasets include natural objects, scenes, human actions and fine-grained features such as textures and satellite imagery, which could cover general semantic targets of visual understanding tasks.\n\nFor the domain-generalization study, we also evaluate the models on four ImageNet-variant datasets, namely, ImageNet-v2 (Recht et al., 2019), ImageNet-Adversarial (Hendrycks et al., 2021b), ImageNet-Retention (Hendrycks et al., 2021a), and ImageNet-Sketch (Wang et al., 2019). These four datasets do not have training images and their categories correspond to ImageNet (Deng et al., 2009). We train on ImageNet and test on these variant datasets to evaluate domain-generalization performance.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Few-shot accuracy on ImageNet (%). n-shot denotes training with n samples per class. †: Note that the Zero-Shot CLIP uses no training data of ImageNet. We put this result to the column “Full” for easy comparison. Our results are marked in gray . The best results are bolded.\n\nMethod Zero-Shot CLIP Linear Prob. CLIP CoOp CoCoOp CLIP-Adapter Tip-Adapter DeFo (ours)\n\nL Encoder ✓\n✗ ✓\n✓ ✓\n✓ ✓\n\nFull 58.2† 72.8 65.6 65.1 -\n- 73.2\n\n1-shot -\n23.6 59.2 57.4 58.2 57.1 59.4\n\n2-shot -\n32.2 59.4 57.8 58.6 57.8 59.7\n\n4-shot -\n40.8 59.7 58.2 59.4 58.6 60.3\n\n8-shot -\n48.9 61.0 58.5 60.4 59.9 61.7\n\n16-shot -\n54.3 63.3 59.0 61.3 61.0 64.0\n\n4.1.3 TECHNICAL DETAILS\n\nThe experiments are built upon CLIP pretrained models. During training, the weights of both image and text encoders are frozen. In this paper, we explore both few-shot and full-dataset training. The few-shot setting follows CLIP (Radford et al., 2021) and CoOp (Zhou et al., 2021), i.e., training with 1, 2, 4, 8, and 16 samples per class that are randomly selected from the training set. By default, we use simple data augmentation of random crop and flip, and train with a SGD optimizer with a minibatch size of 32, 2e-3 learning rate, 0.9 momentum, and 0.01 weight decay (following CoOp (Zhou et al., 2021)) for 50 epochs. For full-dataset training on ImageNet, we use a batch size of 256 and a learning rate of 0.01, which yields similar accuracy to the default setting but significantly reduces training time.\n\nThe number of text queries (n) is naturally fixed to the number of classes (k) for zero-shot CLIP, CoOp and Target Optimization. We set the length of learnable prompt to 16 words for CoOp, and set the length of learnable class name to two words for Target Optimization. The query size of DeFo is scalable in terms of both the length and quantity of text, so we have flexible choices. We empirically find that a larger query size (the number of text queries n) generally yields better predictive performance, in particular for large-scale datasets such as ImageNet (Deng et al., 2009). For example, with a similar number of text queries, i.e., n = 1000 for CLIP and n = 1024 for DeFo, DeFo outperforms the zero-shot CLIP by 14.1% (top-1 acc.) on ImageNet, while this improvement can be further boosted to 15.0% by using 2048 queries in DeFo.\n\nWhen training on full ImageNet, we use n = 2048 text queries and m = 16 words (following CoOp (Zhou et al., 2021)) to fully exploit its learning capacity. For few-shot training on ImageNet, we use a smaller query size, i.e., n = 1024 and m = 4, to prevent over-fitting. For the other 10 datasets, the text length is set to 16, and we find that a smaller number of queries could be sufficient to yield good performance. Specifically, considering the scale of each dataset, we set n = 1024 for SUN397 (Xiao et al., 2010), n = 512 for StanfordCars (Krause et al., 2013), Food101 (Bossard et al., 2014), and UCF101 (Soomro et al., 2012), n = 256 for Caltech101 (Fei-Fei et al., 2004), Flowers102 (Nilsback & Zisserman, 2008), and FGVCAircraft (Maji et al., 2013), and n = 128 for OxfordPets (Parkhi et al., 2012), DTD (Cimpoi et al., 2014), and EuroSAT (Helber et al., 2019).\n\nFor CoOp, we follow its default setup to initialize the trainable text embeddings from randomness, as we find that the random initialization and manual initialization (e.g., initialize from “a photo of a”) yield almost the same performance. When training on full datasets, this phenomenon also works for DeFo and Target Optimization, so we randomly initialize the parameters as well. For few-shot training of DeFo, we initialize the first k text queries by the k class names with random prefix, and fix the corresponding weights (W ∈ Rk×k) of the classification layer to an identity matrix. In this way we further reduce the number of trainable parameters and make use of language supervision via the text encoder, which consequently yields robust performance when training data is limited.\n\n4.2 MAIN RESULTS\n\n4.2.1 COMPARISON ON IMAGENET\n\nWe first compare our DeFo with the baselines on ImageNet under both full-dataset training and few-shot settings. As shown in Table 1, by training on the entire ImageNet data, our method ob-\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Domain transfer accuracy on ImageNet variants (%). Our results are marked in gray .\n\nMethod Zero-Shot CLIP Lin-Probe CLIP CoOp DeFo (ours)\n\nL Encoder ✓\n✗ ✓\n✓\n\nImageNet-v2 51.5 52.1 (+0.6) 55.3 (+3.8) 58.4 (+6.9)\n\nImageNet-A ImageNet-R\n\n21.7 13.6 (-8.1) 22.4 (+0.7) 21.7 (+0.0)\n\n56.0 35.5 (-20.5) 55.9 (-0.1) 55.8 (-0.2)\n\nImageNet-S 32.9 21.8 (-11.1) 33.5 (+0.6) 33.2 (+0.3)\n\nTable 4: Average test accuracy (%) on 11 datasets. Results with † are taken from (Gao et al., 2021). Our results are marked in gray . The best results are bolded. The results without using text encoder are de-emphasized.\n\nMethod Zero-Shot CLIP (Radford et al., 2021) Linear-Probing CLIP CoOp (Zhou et al., 2021) Target Optimization CLIP-Adapter (Gao et al., 2021) DeFo (ours)\n\nRN-50 58.9 79.2 73.7 76.1 74.6† 79.9\n\nRN-101 ViT-B/32\n\n59.9 81.9 76.2 78.2 -\n82.5\n\n61.6 74.7 75.5 76.3 -\n80.8\n\nViT-B/16 65.2 80.0 79.7 80.8 -\n82.8\n\ntains the highest test accuracy with both ResNet and Vision Transformer backbones. Notably, with a ResNet-50 image encoder, our DeFo outperforms the zero-shot CLIP by 15.0%. It is also observed that by using better prompts (i.e., Prompt Ensembling and CoOp), the accuracy is improved by a relatively small margin. This result demonstrates the issues of expressive sensitivity, i.e., the human-annotated class names cannot define or well describe the semantic information of the images in each category, even if the prompt has been optimized. Notably, using a simple prompt but optimizing the class names (Target Optimization) yields more competitive performance (e.g., 71.4% vs. 65.6%).\n\nOverall, our DeFo continues to yield superior performance than the baselines for both full-dataset and few-shot training as shown in Table 2. The linear-probing protocol achieves close accuracy to DeFo with sufficient training samples (e.g., 72.8% vs. 73.2%). However, its drawback is obvious when training data is limited. Typically, as reported in Table 2, the linear probing protocol with one sample per class yields only 23.6% accuracy, which is much lower than that of zero-shot CLIP (58.2%), CoOp (59.2%), and our DeFo (59.4%).\n\n4.2.2 GENERALIZED PERFORMANCE\n\nWe evaluate the domain-transfer performance by 16-shot training on ImageNet and testing on ImageNet-v2 (Recht et al., 2019), ImageNet-Adversarial (Hendrycks et al., 2021b), ImageNetRetention (Hendrycks et al., 2021a), and ImageNet-Sketch (Wang et al., 2019). As shown in Table 3, compared with the baseline of zero-shot CLIP, DeFo attains 6.9% higher accuracy on ImageNet-v2. Also, DeFo yields a similar level of transfer performance as zero-shot CLIP and CoOp did on the other three datasets. In contrast, the linear probing protocol incurs significantly degraded performance on ImageNet-A, -R, and -S, as it forgoes assistance of language information.\n\nFor a wider range of classification tasks, we further evaluate DeFo on a total of 11 datasets. As shown in Table 4, our DeFo achieves the highest average test accuracy over the 11 benchmarks with different image encoders. A specific comparison to CLIP and CoOp on each of the datasets is also provided in Figure 2. We note that CLIP favors the common and generic objects such as the images in Food101, OxfordPets, and Caltech101, for which our DeFo outperforms CLIP by < 10% accuracy and CoOp even fails to improve upon CLIP on Food101. However, when it comes to fine-grained feature recognition tasks such as classifying the type of aircraft (Maji et al., 2013), CLIP and CoOp are shown to be very sensitive to the objects. Consequently, DeFo outperforms CLIP by 25.4% accuracy and outperforms CoOp by 11.2% on this dataset. The different robustness between CLIP and DeFo on the 11 datasets indicates the issue of sensitivity challenge for CLIP, and indicates that DeFo successfully addresses this issue by decomposing and then combining the visual features.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Accuracy improvements over zero-shot CLIP. On all the 11 classification benchmarks, our method outperforms the CLIP and CoOp baselines by non-trivial margins.\n\nFigure 3: Interpretation (nearest words) of the learned text embeddings of DeFo. We highlight the key words and replace the symbols and meaningless words by “[s]”. We surprisingly find that our DeFo is able to learn detailed visual features such as color (a), shape (b), texture (c), and context (d). Also, DeFo is able to directly learn a precise semantic target (f, sunflower is a category of Caltech101) or a generalized semantic target (e).\n\n4.3\n\nINTERPRETATION OF TEXT QUERIES\n\nOne benefit of CLIP-like models is that they are able to provide interpretable visual predictions, as the visual features are highly aligned with the representations of natural language. A simple way to interpret the learned word embeddings, i.e., the n sequences of m embedding vectors in XL, is searching the nearest natural words within the vocabulary by measuring their Euclidean distance. However, as this approach directly maps the continuous embedding vectors into discrete codes of words, the interpreted sentences do not necessarily “make sense” and may contain meaningless words or symbols, which is also observed in prior work (Zhou et al., 2021).\n\nNonetheless, we still find very interesting evidence from the interpretation of DeFo. We observe that some of the interpreted query sentences include meaningful key words that describe specific visual features such as color, shape, and texture. As illustrated in Figure 3 (a)-(c), in Caltech-101 dataset (Fei-Fei et al., 2004), DeFo learns the words “red”, “ring”, and “stripe”, while the wellmatched (based on the consine similarity in the latent space) images in the dataset look consistent with human understanding of these features. For example, DeFo matches the word “red” with the objects such as a lotus flower and a bird in this color. For the word “ring”, we can find the ring or circle shapes in the corresponded images. Also, DeFo is able to extract background information such as “snow” (see Figure 3 (d)). And surprisingly, DeFo sometimes directly learns the semantic targets that are closely related to the categories of the dataset. For example, it learns the word “dogs” which is a parent category in OxfordPets (Parkhi et al., 2012), and the word “sunflowers” which is an exact category in Caltech-101 (Fei-Fei et al., 2004).\n\n8\n\n)RRG2[IRUG3HWV&DOWHFK,PDJH1HW6818&)6WIG&DUV)*9&$'7')ORZHUV(XUR6$7$FFXUDF\\RYHU&/,3&R2S'H)2RXUVLearned queries:“toward - red - [s] - love”(Caltech-101)Well-matched images:“ring - [s] - aboard - [s]”(Caltech-101)“an - [s] - stripe - [s]”(Caltech-101)“snow - at - [s] - [s]”(Oxford Pets)“dogs - [s] - ends - [s]”(Oxford Pets)“[s] - sunflowers- with - [s]”(Caltech-101)(a)(b)(c)(d)(e)(f)Learned queries:Well-matched images:Published as a conference paper at ICLR 2023\n\nDespite the fact that this interpretation approach is not rigorous enough, because the text features learned by DeFo possibly exceed the existing vocabulary, it still provides very strong evidence that DeFo features are meaningful. We hope this result will yield greater insights in a follow-up study on interpretable vision-language inference.\n\n4.4 ABLATION STUDY\n\nIn this section, we present additional results to ablate the gains of DeFo. First, the size of the textual input, including the number of queries n and the length m of each query sentence, may affect the performance. We compare the accuracy of a DeFo model with different n and m and summarize the results in Table 5a and 5b, where we use a ResNet-50 image encoder and train our model on the entire ImageNet data. As reported in the two tables, a smaller size of textual input slightly reduces the performance within an acceptable margin. For example, with n = 256 queries it yields 72.5% accuracy on ImageNet, which is only 0.7% lower than that of the default setup of n = 2048. Also, using m = 4 words per query yields only 0.5% lower accuracy than that of m = 16, and further increasing m (e.g., m = 32) cannot obtain clear improvements. These results indicate that DeFo is robust to its hyper-parameters, and DeFo improvements of accuracy do not rely on large-scale trainable parameters.\n\nThere is another concern that DeFo uses an additional classification layer which introduces n × k more parameters, where n and k denote the number of queries and classes, respectively. To ablate the gain of the additional parameters, we add the same layer on top of CLIP and CoOp, and compare their performance with DeFo. Specifically, we attach a k × k-dimensional linear layer to the logits of CLIP and CoOp, so their model size is identical to DeFo if we set n = k. We conduct this experiment on ImageNet as well, and the results are summarized in Table 5c. As is shown, with the help of the linear layer, the accuracy of CLIP is improved to 62.4%, which is still significantly lower than that of DeFo. Notably, the “CLIP + linear” model is equivalent to our DeFo model with n = k and fixing the textual inputs to the queries used in CLIP. This indicates that the classification layer yields a very limited improvement (< 4%), and the superior performance of DeFo mainly comes from it learning decomposed features.\n\nTable 5: Ablation studies of DeFo. Our results with default setup are marked in gray .\n\n(a) Accuracy with m=16.\n\n(b) Accuracy with n=2048.\n\n(c) Gain of linear layer.\n\nQueries (n) Acc. 72.3 72.5 72.9 73.2\n\n256 512 1024 2048\n\nLength (m) 2\n4 8\n16 32\n\nAcc. 72.5 72.7 73.0 73.2 73.2\n\nModel CLIP CLIP + linear CoOp CoOp + linear DeFo (n=1000)\n\nAcc. 58.2 62.0 65.6 69.8 72.9\n\n5 CONCLUSION\n\nIn this paper, we identify two main issues of existing vision-language inference protocols, i.e., the expressive sensitivity and the conceptual sensitivity. To address them, we propose DeFo which maintains the dual-model architecture but infers by decomposed visual features. Specifically, it leverages trainable text prompts and decouples visual features from hard semantic targets. We demonstrate the significance of DeFo by showing its two benefits. First, DeFo gets rid of the textual descriptions of class names and instead infers via a linear classifier, which yields superior performance in the fulldataset scenarios compared with zero-shot CLIP and CoOp. Next, DeFo keeps the language encoder, which we find is able to bound the projection of visual features and therefore achieves competitive results in few-shot learning and domain transfer. Overall, DeFo provides a new vision-language learning and inference paradigm, i.e., prompting the decomposed visual features, which we hope is of practical importance in fully exploiting the learning capacity of vision-language models.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nHangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of image transformers. arXiv\n\npreprint arXiv:2106.08254, 2021. 2\n\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative compo-\n\nnents with random forests. In ECCV, 2014. 5, 6\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 2\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n\ncontrastive learning of visual representations. In ICML, 2020. 2\n\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-\n\nscribing textures in the wild. In CVPR, 2014. 5, 6\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale\n\nhierarchical image database. In CVPR, 2009. 2, 5, 6\n\nLi Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshop, 2004. 5, 6, 8\n\nChristiane Fellbaum. Wordnet.\n\nIn Theory and applications of ontology: computer applications.\n\nSpringer, 2010. 2\n\nPeng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021. 2, 3, 5, 7\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent a new approach to self-supervised learning. In NeurIPS, 2020. 2\n\nMichael Gutmann and Aapo Hyv ̈arinen. Noise-contrastive estimation: A new estimation principle\n\nfor unnormalized statistical models. JMLR, 2010. 2\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In CVPR, 2016. 2\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\n\nunsupervised visual representation learning. In CVPR, 2020. 2\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ́ar, and Ross Girshick. Masked\n\nautoencoders are scalable vision learners. In CVPR, 2022. 2\n\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. 2, 5, 6\n\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical In Proceedings of the IEEE/CVF International analysis of out-of-distribution generalization. Conference on Computer Vision, pp. 8340–8349, 2021a. 5, 7\n\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial\n\nexamples. In CVPR, 2021b. 5, 7\n\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. 1, 2\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained\n\ncategorization. In ICCV Workshop, 2013. 5, 6\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009. 2\n\nYann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.\n\n2\n\nManling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, and Shih-Fu Chang. Clip-event: Connecting text and images with event structures. In CVPR, 2022. 2\n\nXudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang, and Lorenzo Torresani. Learning to recognize procedural activities with distant supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13853–13863, 2022a. 2\n\nXudong Lin, Simran Tiwari, Shiyuan Huang, Manling Li, Mike Zheng Shou, Heng Ji, and ShihFu Chang. Towards fast adaptation of pretrained contrastive models for multi-channel videolanguage retrieval. arXiv preprint arXiv:2206.02082, 2022b. 2\n\nHuaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021. 2\n\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\n\nvisual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 5, 6, 7\n\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number\n\nof classes. In ICVGIP, 2008. 5, 6\n\nPedro O O Pinheiro, Amjad Almahairi, Ryan Benmalek, Florian Golemo, and Aaron C Courville.\n\nUnsupervised learning of dense visual representations. NeurIPS, 2020. 2\n\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR,\n\n2012. 5, 6, 8\n\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Textdriven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 2\n\nHieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for open-vocabulary image classification. arXiv preprint arXiv:2111.10050, 2021. 1, 2\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 5, 6, 7\n\nYongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting. In CVPR, 2022. 2\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers\n\ngeneralize to imagenet? In ICML, 2019. 5, 7\n\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. A dataset of 101 human action classes\n\nfrom videos in the wild. Center for Research in Computer Vision, 2012. 5, 6\n\nFeng Wang, Huiyu Wang, Chen Wei, Alan Yuille, and Wei Shen. Cp2: Copy-paste contrastive\n\npretraining for semantic segmentation. ECCV, 2022a. 2\n\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representa-\n\ntions by penalizing local predictive power. NeurIPS, 2019. 5, 7\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nZhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al. Language models with image descriptors are strong few-shot video-language learners. arXiv preprint arXiv:2205.10747, 2022b. 2\n\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\n\nLarge-scale scene recognition from abbey to zoo. In CVPR, 2010. 5, 6\n\nRenrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930, 2021. 3, 5\n\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\n\nlanguage models. arXiv preprint arXiv:2109.01134, 2021. 1, 2, 3, 4, 5, 6, 7, 8\n\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for\n\nvision-language models. In CVPR, 2022. 3, 5\n\n12",
    "reference": "# Summary Of The Paper\n\nThis paper point out two challenges in the downstream inference of pre-training vision-language models: expressive sensitivity and conceptual sensitivity.  To handle the problems, the paper proposes a new dual-model feature prompting methd, named as Decomposed Feature Prompting (DeFo). By providing an independent set of learnable embedding and tuning an additional layer for classification, authors claim that the model trained by DeFo significantly addresses the sensitivity challenges of CLIP-like models.\n\n# Strength And Weaknesses\n\nStrength：\n1. The two problems pointed out by the paper: do exist and deserve the attention of the community.\n2. The authors take time to implement and evaluate several prominent baselines. Experimental evaluation shows competitive performance.\n3. This paper is well written and easy to follow.\n\nWeakness:\n1. The paper does not discuss the computational complexity of the proposed methods. How to efficiently complete the fine-tuning of the pre-trained model is also a direction worthy of attention. I look forward to seeing the authors discuss a comprehensive comparison of DeFo's training time and other methods, such as CoOp and CLIP-adapter.\n2. Some important ablation study may be missing.  Since the authors point out that using class labels to generate text embeddings may bring challenges with expressive sensitivity. So a very straightforward idea is that we can directly set an independently learnable parameter as the prototype of each category to calculate the cosine similarity with image embeddings. or adopt the exponential-moving-average (EMA) manner [3]. These above-mentioned methods do not use text encode. Therefore, it is not necessary to carry out the forward of the text encoder every iteration during training. If these method can also achieve very good results, then I feel that the novelty and effectiveness of DeFo may be challenged. Therefore, I think it is very necessary to supplement this experiment. Look forward to the author discussing in following version.\n3. The comparison of some other important baseline is missing, such as Tip-adapter [1] and CoCoOp [2].\n4. I look forward to the author's discussion of the additional learnable parameters introduced in addition to CLIP's pre-trained model, and compare the number with other methods. Because if too many parameters are introduced, the performance improvement may come from overfitting of too many parameters. If the authors would like to compare the number of additional parameters of DeFo with CoOp and CLIP-adapter, I think it may be very helpful for us to comprehensively evaluate and compare these methods.\n5. We expect that the model can not only achieve good performance on a single dataset, but also have the potential to transfer beyond a single dataset. I suggest authors to add discussion about the perfomance of DeFo for domain generalization. Maybe the setting in Section 4.2 of [3] is a good formulation. This may strengthen the contribution of the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well written and easy to follow.\n\n# Summary Of The Review\n\nThe proposed approach is shown to be effective, but the lack of some experiments may lead to the limited contribution of the proposed method. If the author adds more meaningful experiments, I think it will make the paper more interesting, and I am very happy to revise my score.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSWEET GRADIENT MATTERS: DESIGNING CONSISTENT AND EFFICIENT ESTIMATOR FOR ZERO-SHOT NEURAL ARCHITECTURE SEARCH\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNeural architecture search (NAS) is one of the core technologies of AutoML for designing high-performance networks. Recently, Zero-Shot NAS has gained growing interest due to its training-free property and super-fast search speed. However, existing Zero-Shot estimators commonly suffer from low consistency, which limits their reliability and applicability. In this paper, we observe that Sweet Gradient of parameters, i.e., the absolute gradient values within a certain interval, brings higher consistency in network performance compared to the overall number of parameters. We further demonstrate a positive correlation between the network depth and the proportion of parameters with sweet gradients in each layer. Based on the analysis, we propose a training-free method to find the Sweet Gradient interval and obtain an estimator, named Sweetimator. Experiments show that Sweetimator has superior consistency compared to existing Zero-Shot estimators in four benchmarks with eight search spaces. Moreover, Sweetimator outperforms state-of-the-art Zero-Shot estimators in NAS-Bench-201 and achieves competitive performance with 2.5x speedup in the DARTS search space.\n\n1\n\nINTRODUCTION\n\nThe computer vision field has witnessed the great success of deep learning. Iconic works such as ResNet (He et al., 2016), MobileNet (Howard et al., 2017; Sandler et al., 2018), and EfficentNet (Tan & Le, 2019) are widely applied for a variety of real-world tasks such as object detection and semantic segmentation. To tackle the trail-and-error shortcomings of handcrafted architectures, Neural Architecture Search (NAS) (Elsken et al., 2019) has been proposed to automatically search powerful networks that even outperform manual designs (Zoph et al., 2018).\n\nA major theme in NAS development is efficiency. From this perspective, NAS can be broadly classified into three categories: All-Shot, One-Shot, and Zero-Shot NAS. All-Shot NAS utilizes approaches such as reinforcement learning (Zoph & Le, 2017) or evolutionary algorithms (Real et al., 2019) to train the sampled architectures one by one during the search process, which costs hundreds or even thousands of GPU days. Based on weight sharing (Pham et al., 2018), One-Shot NAS trains one supernet and utilizes sampling-based (Guo et al., 2020; Chu et al., 2021b; Yu et al., 2020) or gradient-based (Liu et al., 2019; Chen et al., 2019; Xu et al., 2020) approaches, thus reducing the search cost to a few GPU days. Zero-Shot NAS leverages training-free estimators (Mellor et al., 2021; Abdelfattah et al., 2021) to evaluate network performance. As no networks are trained, the search time is reduced to a few GPU hours or even seconds.\n\nHowever, Zero-Shot NAS commonly suffers from low consistency. Figure 1 illustrates the Spearman’s rank between the test accuracy obtained by training the network from scratch and the estimated performance score of mainstream Zero-Shot methods in NAS-Bench-101 (Ying et al., 2019), NASBench-201 (Dong & Yang, 2020; Dong et al., 2022), and NAS-Bench-301 (Zela et al., 2022). The results demonstrate that these methods do not consistently outperform the simple metric of the number of parameters, which limits their reliability and applicability. Moreover, a question is also naturally raised: could we find a Zero-Shot estimator with superior consistency to parameters?\n\nFor the networks in the NAS-Bench-101, the NAS-Bench-201, and the NAS-Bench-301 space, we observe that some specific parameters, whose absolute gradient values are in a certain interval, have a\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The Spearman’s rank correlation coefficient of Zero-Shot estimators on NAS-Bench-101, NAS-Bench-201, and NAS-Bench-301. The dotted line indicates the spearman’s rank of Parameters.\n\nstronger consistency with the network performance than the overall number of parameters (Parameters for short). For the sake of brevity, we named the gradient in the such interval as Sweet Gradient. We found an interesting property of Sweet Gradient that the proportion of parameters with Sweet Gradient in each layer is positively correlated with the depth of the network. Based on this property, we propose Sweetimator, an estimator that computes Sweet Gradient interval without training. Figure 1 shows that Sweetimator outperforms the Parameters estimator and achieves the best consistency in all three benchmarks.\n\nThe contributions of this work are:\n\n• We observe the Sweet Gradient phenomenon, i.e., the number of parameters with absolute gradient values in a certain interval has better performance consistency than Parameters.\n\n• We demonstrate that there is a positive correlation between the network depth and the\n\nproportion of parameters with Sweet Gradient in each layer.\n\n• We propose a simple and effective Zero-Shot estimator, Sweetimator, that can find Sweet\n\nGradient intervals without training.\n\n• In the consistency experiments, Sweetimator outperforms the existing Zero-Shot estimators in four benchmarks with eight search spaces. In the search experiments, Sweetimator has superior performance to state-of-the-art Zero-Shot estimators in NAS-Bench-201 and achieves competitive results with 2.5x speedup in the DARTS search space.\n\n2 RELATED WORK\n\nNeural Architecture Search. Neural architecture search aims at automatically designing the bestperforming network for a specific task. In the early days, Zoph & Le (2017) proposed a reinforcement learning framework to search hyper-parameters of an entire network. Inspired by the modular design paradigm of handcrafted neural networks, NASNet (Zoph et al., 2018) searched cell structures and stacked the searched best normal cell and reduction cell to form a network. Subsequently, Pham et al. (2018) proposed a weight-sharing strategy to reduce the search overhead to a few GPU Days. Afterward, sampling-based approaches (Guo et al., 2020; Chu et al., 2021b; Yu et al., 2020) trained the supernet by path sampling and utilized sub-networks accuracy for evaluation. DARTS (Liu et al., 2019) and its variants (Chen et al., 2019; Xu et al., 2020; Zela et al., 2020; Chu et al., 2021a; Wang et al., 2021; Sun et al., 2022) leveraged differentiable strategies to optimize the supernet and select the final architecture.\n\nNon-Zero-Shot Estimator. To facilitate the performance evaluation process, various estimators have been proposed. It is natural to use the validation loss or accuracy (Zoph & Le, 2017; Real et al., 2019; Liu et al., 2018) as a performance estimator. Subsequently, SPOS (Guo et al., 2020) and similar works (Pham et al., 2018; Yu et al., 2020; Chu et al., 2021b) utilize the accuracy of sub-networks as\n\n2\n\n-0.4-0.200.20.40.60.81FisherSNIPGraSPGradNormSynFlowTE-NASZen-ScoreNASWOTGradSignFLOPsParametersSweetimatorFisherSNIPGraSPGradNormSynFlowTE-NASZen-ScoreNASWOTGradSignFLOPsParametersSweetimatorFisherSNIPGraSPGradNormSynFlowTE-NASZenNASNASWOTGradSignFLOPsParametersSweetimatorSpearman's rankZero-ShotEstimatorsNAS-Bench-101NAS-Bench-201NAS-Bench-301Spearman's rank of ParametersUnder review as a conference paper at ICLR 2023\n\na proxy for efficient evaluation. Another path for estimators is to utilize machine learning models to predict network performance. NAO (Luo et al., 2018) utilizes an encoder and estimator to find a high-performance network. Wen et al. (2020); Chen et al. (2021c) utilized graph convolutions networks to regress network performance. GP-NAS (Li et al., 2020) proposed a gaussian process based NAS method to obtain the correlations between architectures and performances. TNASP (Lu et al., 2021) uses a transformer and self-evolutionary frameworks to predict network performance. Although the above works can effectively evaluate the network performance, a large search overhead still exists with training networks.\n\nZero-Shot Estimator. Zero-Shot estimators are applied to search architectures without training. NASWOT (Mellor et al., 2021) utilizes activations of rectified linear units to evaluate networks with random initialization. Abdelfattah et al. (2021) proposes a series of zero-cost proxies based on pruning literature (Lee et al., 2019b; Wang et al., 2020; Tanaka et al., 2020). TE-NAS (Chen et al., 2021a) combines neural tangent kernel (NTK) (Lee et al., 2019a) and linear region (Raghu et al., 2017) to evaluate the trainability and expressivity of networks. Zen-NAS (Lin et al., 2021) proposes Zen-Score to measure network performance based on network expressivity. KNAS (Xu et al., 2021) finds that the gradient kernel of the initialized network correlated well with training loss and validation performance. Zhang & Jia (2022) analyzes the sample-wise optimization landscape and proposes GradSign for performance evaluation. A remarkably efficient search is facilitated by the above-mentioned zero-shot estimators. However, compared to network parameters, these methods are not competitive in terms of ranking consistency, raising concerns about their applicability.\n\n3 METHOD\n\n3.1 PRELIMINARIES\n\nZero-Shot NAS utilizes network information at initialization for scoring and expects strong rank consistency with the performance. In particular, gradient information is widely adopted in Zero-Shot estimators. For example, SNIP (Abdelfattah et al., 2021) applies gradient values to approximate the change of loss, and GradSign (Zhang & Jia, 2022) calculates the gradient conflict between data samples. Considering that the network parameter is a strong performance estimator as shown in Figure 1, we therefore combine the network parameter with the gradient. Suppose the loss function is J, the network parameters are θ ∈ Rm and corresponding average gradients in a mini batch are ∇θJ ∈ Rm. Then the Zero-Shot estimator to be explored is as follows.\n\nScore(thr1, thr2) =\n\nm (cid:88)\n\nI\n\nk=1\n\n(cid:40)\n\nthr1 ≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂Jn\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:41)\n\n< thr2\n\n(1)\n\nwhere thr1, thr2 ≥ 0 are two thresholds, θ0 is the network initialization parameter. I is the indicator function with a value of 1 when the condition is true and 0 otherwise. Equation (1) describes the number of parameters whose absolute gradient values are within a certain interval. Score(0, +∞) represents the overall network parameters. For the sake of brevity, we utilize [thr1, thr2) to describe a interval. We provide a theoretical analysis for the proposed Zero-Shot estimator in Appendix A.\n\n3.2 SWEET GRADIENT\n\nWe then analyze the rank consistency of Equation (1) with different [thr1, thr2) intervals. Figure 2 illustrates the Spearman’s rank between the test accuracy of one hundred networks and their scores of Equation (1) on NAS-Bench-101 and NAS-Bench-201. We can see the following patterns:\n\n• The Spearman’s rank in the upper right corner of the heatmap can be regarded as the consistency of Parameters which are covered overwhelmingly when thr1 = 0 and thr2 = 5.\n\n• The score with interval [thr1, thr2) in the dark blue area yields better rank consistency than Parameters. We name the gradient in such intervals Sweet Gradient. The Sweet Gradient Interval is defined as: the absolute gradient interval where the number of parameters have better performance consistency than Parameters.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) NAS-Bench-101\n\n(b) NAS-Bench-201\n\nFigure 2: The Spearman’s rank under different intervals on NAS-Bench-101 and NAS-Bench- 201 (CIFAR10). Each rank is calculated with 100 architectures and a batch size of 128.\n\nAlthough Figure 2 only shows the Spearman’s rank of one hundred architectures in two search spaces, Sweet Gradient exists across different search spaces, datasets, the number of architectures, batch size, and initializations (please refer to Appendix B for more details). This observation indicates that a proper threshold setting can obtain an estimator with better rank consistency than the overall network parameters. However, the thresholds derived based on architecture accuracy still need network training. Does there exist a method to obtain two thresholds without training? This motivates us to investigate the underlying reason for the Sweet Gradient phenomenon.\n\nReviewing the development of neural networks, depth is one of the most critical factors affecting performance (Simonyan & Zisserman, 2015). Inspired by He et al. (2016); Balduzzi et al. (2017), we dissect the parameters of the different intervals from the perspective of depth. To eliminate the effect of the parameter magnitude, the parameter proportion is used for analysis:\n\nP roportionl =\n\nScorel(thr1, thr2) Scorel(0, +∞)\n\n(2)\n\nwhere P roportionl is the proportion of parameters with the gradient interval [thr1, thr2) in the l-th layer, whose range is [0, 1]. Figure 3 exhibits the parameter proportion under different layers in NASBench-101 and NAS-Bench-201. More figures can be seen in Appendix C. Surprisingly, the parameter proportion in the Sweet Gradient interval increases as the depth increases, e.g., [1e−7, 5e−7) in NAS-Bench-101 and [0.0005, 0.001) in NAS-Bench-201. In contrast, the parameter proportion in the Non-Sweet Gradient interval decreases with increasing depth or has a large variance, e.g., [0.01, 0.05) in NAS-Bench-101 and [1e−8, 5e−8) in NAS-Bench-201. Another interesting observation is that the parameter proportion tends to increase for small gradients and decrease for large gradients with increasing depth, e.g., [1e−5, 5e−5) and [0.1, 0.5) in NAS-Bench-101.\n\nTherefore, there is a positive correlation between network depth and parameter proportion of Sweet Gradients in each layer. We can seek Sweet Gradient based on the property: Sweet Gradient is more likely to be located in intervals where the parameter proportion increases as network depth increases.\n\nBesides the network depth, we also investigated the gradient and activation distribution but found no correlation between the two factors and Sweet Gradient. Please refer to Appendix D for more details.\n\n3.3 SWEETIMATOR\n\nSweetimator represents the score in Equation (1) when [thr1, thr2) is a Sweet Gradient interval. The discovered property answers the motivational question of how to obtain the Sweet Gradient interval without training, and thus the Sweetimator. Heuristically, we propose sweetness to estimate how sweet the gradient of an interval is:\n\n4\n\n5e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.530.550.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.520.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.600.610.600.600.600.600.600.600.590.510.460.410.400.400.400.400.390.390.600.600.600.600.600.600.600.590.510.460.410.400.400.400.390.390.390.600.600.600.600.600.600.590.510.460.410.400.400.390.390.390.390.600.600.600.600.600.580.510.460.410.400.400.390.390.390.390.600.600.600.600.580.510.460.410.400.390.390.390.390.390.600.600.600.580.510.460.410.400.390.390.390.390.390.600.600.580.500.450.410.400.380.380.380.380.380.590.580.500.450.400.390.370.370.370.370.370.570.460.400.320.320.310.310.310.310.310.400.330.250.250.240.240.240.240.240.090.030.020.010.010.010.010.01-0.02-0.03-0.04-0.05-0.05-0.05-0.05-0.08-0.12-0.13-0.13-0.13-0.13-0.18-0.19-0.20-0.20-0.20-0.35-0.40-0.40-0.40-0.43-0.42-0.43-0.35-0.35-0.320.40.20.00.20.40.65e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr1-0.37-0.35-0.21-0.20-0.14-0.13-0.13-0.13-0.14-0.15-0.13-0.080.300.510.670.700.720.710.710.710.71-0.330.030.080.220.240.270.270.280.280.290.310.650.800.850.860.860.860.850.860.860.030.070.220.240.270.270.280.280.290.310.650.800.850.860.860.860.850.850.85-0.000.220.240.270.270.280.280.290.320.660.800.850.860.860.860.850.850.850.210.240.270.270.280.280.290.320.660.800.860.860.860.860.850.850.850.240.270.270.280.280.300.340.670.810.860.860.860.850.840.850.850.280.280.280.280.310.350.690.810.860.860.860.850.840.840.840.280.290.300.330.400.720.830.870.870.860.840.840.840.840.300.300.360.430.730.840.870.870.850.840.840.840.840.390.440.510.800.870.900.890.860.850.850.850.850.450.550.840.880.900.880.850.850.850.850.850.760.900.910.900.870.860.860.860.860.860.910.920.890.860.850.850.860.860.860.910.830.810.810.810.810.810.810.770.740.730.730.730.730.730.520.540.540.540.540.540.470.450.450.450.450.240.230.230.230.180.180.180.320.320.230.20.00.20.40.60.8Under review as a conference paper at ICLR 2023\n\n(a) NAS-Bench-101\n\n(b) NAS-Bench-201\n\nFigure 3: The parameter proportion along cell depth of Sweet and Non-Sweet Gradient intervals on NAS-Bench-101 and NAS-Bench-201 (CIFAR-10). Each layer represents a cell of the network and there are 9 and 15 layers in NAS-Bench-101 and NAS-Bench-201, respectively.\n\nSweetness(thr1, thr2) =\n\n1 n\n\nn (cid:88)\n\nk\n\n(\n\n1 lk − 1\n\nlk−1 (cid:88)\n\ni\n\nsign(P roportioni+1 − P roportioni))\n\n(3)\n\nwhere n is the number of architectures, lk denotes the depth of the k-th architecture, and the sign function is used to determine whether the parameter proportion is ascending as the depth increases. Equation (3) has a range of [−1, 1], indicating interval incrementality with the depth. Sweetness = 1 indicates monotonically ascending and Sweetness = −1 indicates monotonically descending. Although both Sweetness and GradSign utilize the sign function, the purposes are completely different. GradSign evaluates the gradient conflict between samples, while we evaluate the interval incrementality for Sweetimator.\n\nIn practice, one problem in obtaining the Sweet Gradient is the interval setting, which can be described by the triplet (max, min, split). max means the largest order of magnitude, min means the smallest order of magnitude except zero, and split means each order of magnitude is divided equally into several parts. For example, (1, 0.1, 2) means three interval [0, 0.1), [0.1, 0.5), [0.5, 1.0). We empirically found that max = 10 and min = 1e−10 are sufficient. And the ablation study of split is provided in Section 4.4. Algorithm 1 describes how to obtain Sweetimator.\n\nAlgorithm 1 Sweetimator Input: m initialized architectures; p intervals based on (max, min, split); Output: Best Interval for Sweetimator\n\n1: maxSweetness = −1 2: bestInterval = N one 3: for i ← 1 to p do 4: 5: 6: 7: 8: 9: end for\n\nend if\n\nSweetnessi ← Equation (3) if Sweetnessi > maxSweetness then maxSweetness = Sweetnessi bestInterval = Intervali\n\n10: return bestInterval\n\n5\n\nSweet: [1e-7, 5e-7) ρ=0.60 > 0.39Sweet: [1e-5, 5e-5) ρ=0.59 > 0.39Non-Sweet: [0.01, 0.05) ρ=-0.18 < 0.39Non-Sweet: [0.1, 0.5) ρ=-0.43 < 0.39proportioncell depthcell depthcell depthcell depthSweet: [0.0001, 0.0005) ρ=0.91 > 0.71Sweet: [0.0005, 0.001) ρ=0.91 > 0.71Non-Sweet: [1e-8, 5e-8) ρ=0.21 < 0.71Non-Sweet: [0.01, 0.05) ρ=0.47 < 0.71proportioncell depthcell depthcell depthcell depthUnder review as a conference paper at ICLR 2023\n\n4 EXPERIMENT\n\nIn this section, we first evaluate the ranking consistency of Sweetimator in NAS-Bench-101 (Ying et al., 2019), NAS-Bench-201 (Dong & Yang, 2020; Dong et al., 2022), NAS-Bench-301 (Zela et al., 2022), and NDS (Radosavovic et al., 2019) benchmarks. Then, we use Sweetimator to conduct search experiments in NAS-Bench-201 and DARTS (Liu et al., 2019) search spaces. Finally, we provide ablation experiments for further analysis. Experimental details can be referred to Appendix E.\n\n4.1 CONSISTENCY RESULTS\n\nBenchmarks. We conduct experiments in NAS-Bench-101, NAS-Bench-201, NAS-Bench-301, and NDS search spaces. NAS-Bench-101 is the first large-scale NAS Benchmark with 423k architectures and corresponding accuracy on CIFAR-10. NAS-Bench-201 is an extension to NAS-Bench-101, containing 15625 architectures and CIFAR-10, CIFAR-100, and ImageNet16-120 datasets. NASBench-301 is the first surrogate NAS benchmark, with 1018 architectures on CIFAR-10. NDS statistically analyzes multiple network design spaces, including NAS-Net (Zoph et al., 2018), AmoebaNet (Real et al., 2019), PNAS (Liu et al., 2018), ENAS (Pham et al., 2018), and DARTS (Liu et al., 2019) search space. And each space has thousands of architectures in the NDS benchmark.\n\nBaselines. We compare common Zero-Shot estimators, including SNIP (Lee et al., 2019b), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), Fisher (Turner et al., 2020), GradNorm (Abdelfattah et al., 2021), NASWOT (Mellor et al., 2021), TE-NAS (Chen et al., 2021a), Zen-Score (Lin et al., 2021), GradSign (Zhang & Jia, 2022), and FLOPs and Parameters of networks. A comparison with Non-Zero-Shot estimators is in Appendix F.\n\nSettings. The consistency experiments are divided into two groups. The first group contains NASBench-101, NAS-Bench-201, and NAS-Bench-301 with 4500, 15625, and 5000 architectures to test consistency, respectively. For fairness, the batch size of all Zero-Shot estimators is 64. The evaluation metric is Spearman’s rank. The second group includes NAS-Net, AmoebaNet, PNAS, ENAS, and DARTS spaces of NDS benchmark with 4846, 4983, 4999, 4999, and 5000 architectures for evaluation, respectively. The architectures are trained on CIFAR-10. All Zero-Shot estimators have a batch size of 128. The evaluation metric is Kendall’s Tau.\n\nResults. Tables 1 and 2 show that Sweetimator significantly outperforms existing Zero-Shot estimators. For example, Sweetimator is better than the estimator Parameters by 46% (0.423 vs. 0.618) in NAS-Bench-101 in Table 1. The results verify the effectiveness of the proposed method. Moreover, we observe that existing Zero-Shot estimators do not consistently outperform Parameters in terms of rank consistency, which is also found by Ning et al. (2021). In contrast, Sweetimator significantly outperforms Parameters, suggesting that Sweet Gradient is a direction worth exploring for NAS.\n\nTable 1: Rank Consistency of Zero-Shot estimators in NAS-Bench-101, NAS-Bench-201, and NAS-Bench-301 by Spearman’s rank.\n\nEstimators\n\nFisher SNIP GraSP GradNorm SynFlow TE-NAS Zen-Score NASWOT GradSign FLOPs Parameters Sweetimator\n\nNAS-Bench-101\n\nNAS-Bench-201\n\nNAS-Bench-301\n\nCIFAR-10\n\nCIFAR-10 CIFAR-100\n\nImageNet16-120\n\nCIFAR-10\n\n0.302 0.191 0.329 0.265 0.360 0.065 0.261 0.327 0.422 0.422 0.423 0.618\n\n0.546 0.638 0.551 0.637 0.777 0.376 0.251 0.691 0.808 0.733 0.751 0.888\n\n0.548 0.637 0.549 0.637 0.763 0.350 0.260 0.704 0.792 0.708 0.727 0.859\n\n6\n\n0.491 0.578 0.553 0.578 0.751 0.335 0.319 0.700 0.783 0.673 0.690 0.835\n\n0.392 0.153 0.379 0.155 -0.167 -0.055 -0.119 0.053 0.424 0.423 0.458 0.596\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Rank Consistency of Zero-Shot estimators in five search spaces of NDS by Kendall’s Tau.\n\nEstimators\n\nDARTS ENAS\n\nPNAS NASNet Amoeba\n\nGradNorm SynFlow NASWOT GradSign FLOPs Parameters Sweetimator\n\n0.227 -0.001 0.480 0.537 0.500 0.493 0.568\n\n0.055 -0.092 0.387 0.424 0.413 0.411 0.506\n\n0.109 -0.090 0.363 0.396 0.395 0.387 0.458\n\n-0.080 -0.191 0.299 0.290 0.288 0.289 0.449\n\n-0.116 -0.001 0.208 0.250 0.238 0.241 0.360\n\nTable 3: Mean ± std accuracies in NAS-Bench-201. The upper and lower parts indicate results of Architecture Selection and Assisted NAS, respectively. All searches run for 500 times. For a fair comparison, architectures are searched on CIFAR-10 and evaluated on CIFAR-10, CIFAR-100 and ImageNet16-120. Note that we rerun GradSign-assisted NAS experiments using their released code.\n\nMethods\n\nCIFAR-10\n\nCIFAR-100\n\nImageNet16-120\n\nvalidation\n\ntest\n\nvalidation\n\ntest\n\nvalidation\n\ntest\n\nArchitecture Selection\n\nRandom NASWOT (N=10) Sweetimator (N=10) Optimal (N=10)\n\nSynFlow (N=100) NASWOT (N=100) GradSign (N=100) Sweetimator (N=100) Optimal (N=100)\n\nNASWOT (N=1000) Sweetimator (N=1000) Optimal (N=1000)\n\nREA A-REA G-REA S-REA\n\nREINFORCE G-REINFORCE S-REINFORCE\n\nBOHB G-BOHB S-BOHB\n\n83.20±13.28 86.61±13.46 60.70±12.55 60.83±12.58 33.34±9.39 33.13±9.66 41.09±3.97 41.31±4.11 89.14±1.44 41.54±3.25 41.71±3.34 89.53±1.17 43.11±1.85 43.30±1.87 89.92±0.75\n\n92.44±1.13 92.75±0.93 93.06±0.59\n\n68.62±2.04 69.02±1.96 69.76±1.25\n\n68.50±2.03 68.89±1.97 69.61±1.21\n\n89.83±0.75 89.55±0.89 89.84±0.61 90.47±0.74 91.05±0.28\n\n89.69±0.73 91.00±0.48 91.34±0.18\n\n91.08±0.45 91.20±0.27 91.25±0.57 91.43±0.24\n\n90.32±0.89 90.78±0.68 91.01±0.46\n\n91.84±0.49 91.18±0.29 91.21±0.22\n\n93.12±0.52 92.91±0.99 93.31±0.47 93.45±0.57 93.84±0.23\n\n92.96±0.81 93.83±0.46 94.05±0.22\n\n69.89±1.87 69.35±1.70 70.22±1.32 70.40±1.54 71.45±0.79\n\n69.98±1.22 71.38±1.32 72.15±0.81\n\nAssisted NAS\n\n93.85±0.44 -\n94.10±0.48 94.23±0.23\n\n93.21±0.82 93.61±0.57 93.76±0.46\n\n93.64±0.49 93.96±0.28 93.98±0.23\n\n71.59±1.33 71.95±0.99 72.56±1.53 72.95±1.07\n\n70.03±1.75 70.88±1.46 71.36±1.26\n\n70.82±1.29 71.91±0.96 72.01±0.83\n\n69.94±1.88 69.48±1.70 70.33±1.28 70.54±1.52 71.56±0.78\n\n69.86±1.21 71.62±1.34 72.17±0.83\n\n71.64±1.25 -\n72.62±1.46 72.96±1.01\n\n70.14±1.73 71.02±1.36 71.44±1.20\n\n70.92±1.26 71.96±0.89 72.06±0.76\n\n41.94±4.13 42.26±4.26 42.81±3.05 43.10±3.16 42.07±2.78 42.42±2.81 43.49±2.10 43.71±2.19 45.37±0.61 45.67±0.64\n\n44.44±2.10 43.95±2.05 44.68±1.25 45.05±1.45 45.57±0.73 45.79±0.78\n\n44.90±1.20 45.25±1.41 45.70±1.05 45.62±1.31 45.77±1.28 45.98±0.77 46.10±0.69\n\n-\n\n43.57±2.09 43.64±2.24 44.53±1.46 44.65±1.58 44.76±1.22 45.02±1.37\n\n44.36±1.37 44.50±1.50 45.32±0.89 45.51±0.93 45.38±0.80 45.62±0.85\n\n4.2 SEARCH RESULTS IN NAS-BENCH-201\n\nFollowing Mellor et al. (2021); Zhang & Jia (2022), we conduct architecture selection and assisted NAS experiments to evaluate the ability of Sweetimator to search high-performance architectures.\n\nArchitecture Selection Settings. Architecture selection refers to randomly sampling N candidate architectures from the search space and then evaluating the architecture with the largest Sweetimator score. The comparison methods include NASWOT, GradSign, Random and Optimal, where Random uniformly selects architectures from the search space, and Optimal means picking the network with the highest test accuracy among N architectures. Following Mellor et al. (2021), we conduct Sweetimator for sample size of N = 10, N = 100 and N = 1000.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nSweetimator-Assisted NAS Settings. In the experiment, the NAS algorithms of REA (Real et al., 2019), REINFORCE (Williams, 1992) and BOHB (Falkner et al., 2018) are assisted with Sweetimator to verify the applicability. For a fair comparison, we utilized the assisted algorithm proposed in GradSign (Zhang & Jia, 2022). In particular, the random selection of each NAS algorithm is replaced by Sweetimator-assisted selection. The NASWOT-, GradSign-, Sweetimator-assisted NAS algorithms are abbreviated as A-, G-, S-algorithm, e.g., S-REA, respectively. Following Zhang & Jia (2022), all results are searched with a time budget of 12000s.\n\nResults. Table 3 summarizes the search results. In architecture selection experiments, Sweetimator outperforms compared baselines with a closer distance to Optimal, indicating the ability to search high-performance architectures. In the Sweetimator-assisted NAS experiments, the average accuracy of Sweetimator-assisted was higher than the original algorithm and other estimator-assisted algorithms with lower standard deviation, further validating the efficiency and applicability of Sweetimator.\n\n4.3 SEARCH RESULTS IN DARTS SEARCH SPACE\n\nSettings. DARTS (Liu et al., 2019) is a popular search space to evaluate NAS algorithms. We conduct experiments on CIFAR-10 (Krizhevsky & Hinton, 2009) and ImageNet (Olga et al., 2015) dataset. In the search phase, we utilize 100 architectures and a batch size of 128 to obtain the best interval of Sweetimator. Further, we integrate Sweetimator with REA algorithm (Real et al., 2019) for searching. The hyper-parameters of REA are followed by Dong & Yang (2020) with 200 cycles. Despite small cycles, the score converges rapidly and the search results can be competitive with other Zero-Shot methods, which also bring a fast search speed. In the retraining phase, we follow DARTS settings to build and train searched network for a fair comparison. More details can be referred to Appendix E.\n\nResults. In Table 4, the middle part shows the results on CIFAR-10. Compared to Zero-Shot methods TE-NAS and NASI-ADA, Sweetimator achieves 2.5x speedup (0.01 vs. 0.004) and better performance (2.63 vs. 2.62). When compared to One-Shot methods such as PC-DARTS, Sweetimator has a significantly faster search speed (0.1 vs 0.004). The right part shows the results on ImageNet which also demonstrate that Sweetimator achieves competitive performance and speed compared to other Zero-Shot methods. The searched cells are visualized in Appendix G.\n\nTable 4: Comparison with state-of-the-art NAS methods on CIFAR10 and ImageNet. The Sweetimator results on CIFAR-10 come from four independent searches with different random seeds. Top-1 and Top-5 represent the test error. GDays is GPU days. † Architectures are directly searched on ImageNet. ‡ Time is recorded on one GTX 1080Ti.\n\nMethods\n\nCIFAR-10\n\nImageNet\n\nTest Error (%) Params (M) GDays Top-1 (%) Top-5 (%) Params (M) GDays\n\nAll-Shot NAS\n\nNASNet (Zoph et al., 2018) AmoebaNet (Real et al., 2019) PNAS (Liu et al., 2018)\n\n2.65 3.34±0.06 3.41±0.09\n\n3.3 3.2 3.2\n\n2000 3150 225\n\nOne-Shot NAS\n\nDARTS (2nd) (Liu et al., 2019) SNAS (Xie et al., 2019) GDAS (Dong & Yang, 2019) BayesNAS (Zhou et al., 2019) P-DARTS (Chen et al., 2019) SDARTS (Chen & Hsieh, 2020) PC-DARTS (Xu et al., 2020) † DARTS- (Chu et al., 2021a) † DrNAS (Chen et al., 2021b) † β-DARTS (Ye et al., 2022)\n\n2.76±0.09 2.85±0.02 2.82 2.81±0.04 2.5 2.61±0.02 2.57±0.07 2.59±0.08 2.54±0.03 2.53±0.08\n\n3.3 2.8 2.5 3.4 3.4 3.3 3.6 3.5 4.0 3.8\n\n1.0 1.5 0.17 0.2 0.3 1.3 0.1 0.4 0.4 0.4\n\nTE-NAS (Chen et al., 2021a) † NASI-ADA (Shu et al., 2022) † Sweetimator †\n\n2.63±0.06 2.90±0.13 2.62±0.05\n\nZero-Shot NAS\n\n3.8 3.2 3.4\n\n0.05 0.01 0.004 ‡\n\n26.0 24.3 25.8\n\n26.7 27.3 26.0 26.5 24.4 25.2 24.2 23.8 24.2 24.2\n\n24.5 24.8 24.8\n\n8.4 7.6 8.1\n\n8.7 9.2 8.5 8.9 7.4 7.8 7.3 7.0 7.3 7.1\n\n7.5 7.5 7.6\n\n5.3 6.4 5.1\n\n4.7 4.3 5.3 3.9 4.9 5.4 5.3 4.9 5.2 5.4\n\n5.4 5.2 5.2\n\n2000 3150 225\n\n1.0 1.5 0.21 0.2 0.3 1.3 3.8 4.5 3.9 0.4\n\n0.17 0.01 0.01 ‡\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n4.4 ABLATION STUDY\n\nBatch size, architecture numbers, and split of the interval are three major factors that influence the gradient calculation and threshold acquisition of Sweetimator. For further analysis, we conduct ablation studies in NAS-Bench-201, which are shown in Table 5.\n\nBatch Size. The batch size is an important hyper-parameter that determines the quality of the gradient. The larger the batch size, the more accurate the gradient. Table 5 demonstrates that Spearman’s rank commonly becomes higher with the increasing batch size. Therefore, Sweetimator can be more effective by using a larger batch size.\n\nArchitecture Number. The architecture number affects threshold acquisition. Table 5 shows that a larger architecture number does not change Spearman’s rank. This is because the obtained interval is the same across different architecture numbers. The results suggest that a small number of architectures (e.g., 100) is enough to obtain the best interval.\n\nSplit of Interval. The split influences the granularity of the interval. In Table 5, the Spearman’s ranks are improved on CIFAR-10 (0.888 → 0.913) and CIFAR-100 (0.859 → 0.887) but almost kept on ImageNet16-120 (0.835 → 0.836) when split is larger. Consequently, the interval granularity of different datasets is different, indicating that Sweetimator can be more efficient by designing a flexible interval selection algorithm in the future.\n\nTable 5: Rank Consistency of Zero-Shot estimators in NAS-Bench-201 by Spearman’s rank. The baseline has a batch size of 64, architecture number of 100, and split of 2.\n\nDataset\n\nBaseline\n\nCIFAR-10 CIFAR-100 ImageNet16-120\n\n0.888 0.859 0.835\n\nBatch Size 256 0.917 0.888 0.836\n\n128 0.908 0.881 0.815\n\n512 0.921 0.892 0.838\n\nArchitecture Number 300 200 150 0.888 0.888 0.888 0.859 0.859 0.859 0.835 0.835 0.835\n\nSplit 4\n0.910 0.883 0.824\n\n3 0.913 0.886 0.835\n\n5 0.913 0.887 0.836\n\n5 LIMITATIONS AND FUTURE WORK\n\nWe only consider computer vision classification tasks and cell-based search spaces in this work. And finding the optimal gradient interval is still a process of discrete selection rather than a continuous differentiable optimization process. In future works, we intend to apply Sweetimator to diverse search spaces (e.g., Transformer space) and explore an optimization-friendly method to obtain the best interval. Besides, it is still a mystery that the Sweet Gradient interval exhibits the property of increasing parameter proportion as network depth increases. We believe a more profound reason exists behind this observation which helps us to better understand deep neural networks. Therefore, a theoretical analysis is necessary in the future work.\n\n6 CONCLUSION\n\nThis work observes the Sweet Gradient phenomenon that some specific parameters, whose absolute gradient values are in a certain interval, have a stronger consistency with the network performance than Parameters. To obtain the Sweet Gradient interval without training, we investigated the relationship between network depth and Sweet Gradient. We found that Sweet Gradient tends to exist in intervals with increasing parameter proportion as network depth increases. Based on the property, we utilize Sweetness to obtain the best interval. Experiments demonstrate that Sweetimator can achieve superior rank consistency and excellent search results, verifying the effectiveness of the proposed estimator.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMohamed S Abdelfattah, Abhinav Mehrotra, Łukasz Dudziak, and Nicholas D Lane. Zero-cost proxies for lightweight nas. In International Conference on Learning Representations, 2021.\n\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\n\nparameterization. In International Conference on Machine Learning, 2019.\n\nDavid Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. In\n\nThe shattered gradients problem: If resnets are the answer, then what is the question? International Conference on Machine Learning, 2017.\n\nAmir Beck. First-order methods in optimization. SIAM, 2017.\n\nWuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective. In International Conference on Learning Representations, 2021a.\n\nXiangning Chen and Cho-Jui Hsieh. Stabilizing differentiable architecture search via perturbation-\n\nbased regularization. In International Conference on Machine Learning, 2020.\n\nXiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, and Cho-Jui Hsieh. Drnas: Dirichlet neural architecture search. In International Conference on Learning Representations, 2021b.\n\nXin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. In International Conference on Computer Vision, 2019.\n\nXin Chen, Lingxi Xie, Jun Wu, Longhui Wei, Yuhui Xu, and Qi Tian. Fitting the search space of weight-sharing nas with graph convolutional networks. In Association for the Advancement of Artificial Intelligence, 2021c.\n\nXiangxiang Chu, Xiaoxing Wang, Bo Zhang, Shun Lu, Xiaolin Wei, and Junchi Yan. Darts-: robustly stepping out of performance collapse without indicators. In International Conference on Learning Representations, 2021a.\n\nXiangxiang Chu, Bo Zhang, and Ruijun Xu. Fairnas: Rethinking evaluation fairness of weight\n\nsharing neural architecture search. In International Conference on Computer Vision, 2021b.\n\nXuanyi Dong and Yi Yang. Searching for a robust neural architecture in four gpu hours. In Conference\n\non Computer Vision and Pattern Recognition, 2019.\n\nXuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture\n\nsearch. In International Conference on Learning Representations, 2020.\n\nXuanyi Dong, Lu Liu, Katarzyna Musial, and Bogdan Gabrys. Nats-bench: Benchmarking nas algorithms for architecture topology and size. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The\n\nJournal of Machine Learning Research, 2019.\n\nStefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimiza-\n\ntion at scale. In International Conference on Machine Learning, 2018.\n\nXavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, 2010.\n\nZichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. In European Conference on Computer Vision, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, 2015.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n\nrecognition. In Conference on Computer Vision and Pattern Recognition, 2016.\n\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n\nA. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s thesis,\n\nDepartment of Computer Science, University of Toronto, 2009.\n\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In Advances in Neural Information Processing Systems, 2019a.\n\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. In International Conference on Learning Representations, 2019b.\n\nZhihang Li, Teng Xi, Jiankang Deng, Gang Zhang, Shengzhao Wen, and Ran He. Gp-nas: Gaussian process based neural architecture search. In Conference on Computer Vision and Pattern Recognition, 2020.\n\nMing Lin, Pichao Wang, Zhenhong Sun, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Zen-nas: A zero-shot nas for high-performance image recognition. In International Conference on Computer Vision, 2021.\n\nChenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In European Conference on Computer Vision, 2018.\n\nHanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In\n\nInternational Conference on Learning Representations, 2019.\n\nShun Lu, Jixiang Li, Jianchao Tan, Sen Yang, and Ji Liu. Tnasp: A transformer-based nas predictor with a self-evolution framework. In Advances in Neural Information Processing Systems, 2021.\n\nRenqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization.\n\n2018.\n\nYash Mehta, Colin White, Arber Zela, Arjun Krishnakumar, Guri Zabergja, Shakiba Moradian, Mahmoud Safari, Kaicheng Yu, and Frank Hutter. Nas-bench-suite: Nas evaluation is (now) surprisingly easy. 2022.\n\nJoe Mellor, Jack Turner, Amos Storkey, and Elliot J Crowley. Neural architecture search without\n\ntraining. In International Conference on Machine Learning, 2021.\n\nXuefei Ning, Changcheng Tang, Wenshuo Li, Zixuan Zhou, Shuang Liang, Huazhong Yang, and Yu Wang. Evaluating efficient performance estimators of neural architectures. In Advances in Neural Information Processing Systems, 2021.\n\nRussakovsky Olga, Deng Jia, Su Hao, Krause Jonathan, Satheesh Sanjeev, Ma Sean, Huang Zhiheng, Karpathy Andrej, Aditya Khosla, Bernstein Michael, Berg Alexander C., and Fei-Fei Li. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 2015.\n\nHieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search\n\nvia parameters sharing. In International Conference on Machine Learning, 2018.\n\nIlija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Doll ́ar. On network design\n\nspaces for visual recognition. In International Conference on Computer Vision, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nMaithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. In International Conference on Machine Learning, 2017.\n\nEsteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image\n\nclassifier architecture search. In AAAI Con-ference on Artificial Intelligence, 2019.\n\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Conference on Computer Vision and Pattern Recognition, 2018.\n\nYao Shu, Shaofeng Cai, Zhongxiang Dai, Beng Chin Ooi, and Bryan Kian Hsiang Low. Nasi: Label-and data-agnostic neural architecture search at initialization. In International Conference on Learning Representations, 2022.\n\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.\n\nIn International Conference on Learning Representations, 2015.\n\nZihao Sun, Yu Hu, Shun Lu, Longxing Yang, Jilin Mei, Yinhe Han, and Xiaowei Li. Agnas: Attentionguided micro and macro-architecture search. In International Conference on Machine Learning, 2022.\n\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks.\n\nIn International Conference on Machine Learning, 2019.\n\nHidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. In Advances in Neural Information Processing Systems, 2020.\n\nJack Turner, Elliot J Crowley, Michael O’Boyle, Amos Storkey, and Gavin Gray. Blockswap: Fisherguided block substitution for network compression on a budget. In International Conference on Learning Representations, 2020.\n\nChaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by\n\npreserving gradient flow. In International Conference on Learning Representations, 2020.\n\nRuochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, and Cho-Jui Hsieh. Rethinking architecture selection in differentiable nas. In International Conference on Learning Representations, 2021.\n\nWei Wen, Hanxiao Liu, Yiran Chen, Hai Li, Gabriel Bender, and Pieter-Jan Kindermans. Neural predictor for neural architecture search. In European Conference on Computer Vision, 2020.\n\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. Machine learning, 1992.\n\nSirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. Snas: stochastic neural architecture search. In\n\nInternational Conference on Learning Representations, 2019.\n\nJingjing Xu, Liang Zhao, Junyang Lin, Rundong Gao, Xu Sun, and Hongxia Yang. Knas: green\n\nneural architecture search. In International Conference on Machine Learning, 2021.\n\nYuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong. Pc-darts: Partial channel connections for memory-efficient architecture search. In International Conference on Learning Representations, 2020.\n\nPeng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, and Wanli Ouyang. b-darts: Beta-decay regularization for differentiable architecture search. In Conference on Computer Vision and Pattern Recognition, 2022.\n\nChris Ying, Aaron Klein, Esteban Real, Eric Christiansen, Kevin P. Murphy, and Frank Hutter. Nas-bench-101: Towards reproducible neural architecture search. In International Conference on Machine Learning, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nJiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big single-stage models. In European Conference on Computer Vision, 2020.\n\nArber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter. Understanding and robustifying differentiable architecture search. In International Conference on Learning Representations, 2020.\n\nArber Zela, Julien N. Siems, Lucas Zimmer, Jovita Lukasik, Margret Keuper, and Frank Hutter. Surrogate nas benchmarks: Going beyond the limited search spaces of tabular nas benchmarks. In International Conference on Learning Representations, 2022.\n\nZhihao Zhang and Zhihao Jia. Gradsign: Model performance inference with theoretical insights. In\n\nInternational Conference on Learning Representations, 2022.\n\nHongpeng Zhou, Minghao Yang, Jun Wang, and Wei Pan. Bayesnas: A bayesian approach for neural\n\narchitecture search. In International Conference on Machine Learning, 2019.\n\nBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In International\n\nConference on Learning Representations, 2017.\n\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Conference on Computer Vision and Pattern Recognition, 2018.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA THEORETICAL ANALYSIS\n\nThe dataset is denoted as {xi, yi}N The loss function is\n\ni=1, where the input xi ∈ Rd and the output yi ∈ R.\n\nJN (θ) =\n\nN (cid:88)\n\ni=1\n\nl (yi, f (θ; xi))\n\nwhere f (θ; x) is the network architecture, θ ∈ Rm and l(·, ·) is the loss function. Then the optimal network parameter θ∗ is\n\nScore index\n\nθ∗ ≜ argmin\n\nθ∈Rm\n\nJN (θ).\n\n(cid:40)\n\nm (cid:88)\n\nI\n\nk=1\n\nτ1 ≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n(cid:41)\n\n< τ2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(4)\n\n(5)\n\n(6)\n\nwhere τ2 > τ1 > 0 and θ0 is the initialization parameter. Here τ1 and τ2 correspond to thr1 and thr2 in (1). Regarding the optimal parameter θ∗ and the initial parameter θ0, we provide theoretical analysis from two perspectives.\n\n• Higher performance. The small loss at the optimal parameter JN (θ∗) implies the high network performance. We analyze the upper bound of JN (θ∗) and expect it as small as possible;\n\n• Easier to optimize: The small distance between the initial parameter and the optimal (cid:13)θ0 − θ∗(cid:13) (cid:13) (cid:13)2\n\nparameter means the easy optimization process. We analyze the upper bound of and expect it as small as possible.\n\nA.1 THEORETICAL RESULTS\n\nThe theorems in (Allen-Zhu et al., 2019) reveal that the neighborhood around random initialization has excellent properties that are almost convex and semi-smooth. Based on the work, we introduce a slightly stronger assumption here. Denote the vector l2 norm as ∥·∥2.\n\nAssumption 1 JN (θ) is h-strong convex and H-smooth in the neighborhood Γ (cid:0)θ0, R(cid:1) ≜ (cid:13)θ − θ0(cid:13) (cid:13) (cid:8)θ (cid:12) (cid:13)2 ≤ R (cid:9) of the initialization θ0, where R > 0 is the neighborhood radius. (cid:12)\n\nLemma 1 JN (θ) is h-strong convex and H-smooth which is equivalent to h · I ⪯ ∇2JN (θ) ⪯ H · I, ∀θ ∈ Γ (cid:0)θ0, R(cid:1).\n\nTheorem 1 (The upper bound of the loss in the optimal point.) Assume that JN (θ) is h-strong convex in the neighborhood Γ (cid:0)θ0, R(cid:1) which in Assumption 1 hold. Then\n\nJN (θ∗) ≤ JN (θ0) −\n\n1 2H\n\nε2m −\n\n1 2H\n\n(cid:0)τ 2\n\n1 − ε2(cid:1) ·\n\nm (cid:88)\n\nI\n\nk=1\n\n(cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n(cid:41)\n\n≥ τ1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(7)\n\nwhere 0 ≤ ε ≜ min\n\n(cid:26)(cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN (θ0) ∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN (θ0) ∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:27)\n\n< τ1\n\n< τ1.\n\nRemark 1 The upper bound of JN (θ∗) (cid:80)m\n\n(cid:27)\n\n∂JN (θ0) ∂θk\n\n≥ τ1\n\nk=1\n\nI\n\n(cid:26)(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nis\n\ninfluenced by two items of JN (θ0) and\n\n. Considering network performance ranking, we can ignore the effect of\n\nthe first item since JN (θ0) of different networks is similar. In practice, the network initialization\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\n(Glorot & Bengio, 2010; He et al., 2015) tends to maintain the mean and variance of the output in each layer, so the final output logits of different networks have similar distribution, resulting in similar losses. In NAS-Bench-201, the loss of JN (θ0) of all candidate networks on CIFAR10, CIFAR100 and ImageNet16-120 are 2.343 ± 0.043, 4.651 ± 0.042, and 4.842 ± 0.047, respectively. Small deviation indicates that the JN (θ0) of different networks is indeed similar. Therefore, we only need to focus on (cid:12) (cid:12) the second item, i.e, the larger (cid:80)m (cid:12) (cid:12)\n\n, the smaller the upper bound of JN (θ∗).\n\n∂JN (θ0) ∂θk\n\n(cid:26)(cid:12) (cid:12) (cid:12) (cid:12)\n\n≥ τ1\n\nk=1\n\n(cid:27)\n\nI\n\nTheorem 2 (The upper bound of distance between the initial point and the optimal point.) Assume JN (θ) is H-smooth in the neighborhood Γ (cid:0)θ0, R(cid:1) in Assumption 1 hold. Then\n\n(cid:13)θ0 − θ∗(cid:13) (cid:13)\n\n(cid:13)2 ≤\n\n1 h\n\n(cid:32)\n\n·\n\nM · m − (M − τ2)\n\nm (cid:88)\n\nI\n\nk=1\n\n(cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:41)(cid:33)\n\n< τ2\n\nwhere M ≜ max\n\n(cid:26)(cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN (θ0) ∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN (θ0) ∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:27)\n\n≥ τ2\n\n> τ2 > τ1 > 0.\n\nRemark 2 To make the upper bound of (cid:13)\n\n(cid:13)θ0 − θ∗(cid:13)\n\n(cid:13)2 small, the index (cid:80)m\n\nk=1\n\nneed to be as large as possible.\n\nI\n\n(cid:26)(cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN (θ0) ∂θk\n\n(cid:27)\n\n< τ2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nFor given τ2 > τ1 > 0, considering the two goals of high performance and easy optimization, we expect the upper bound of JN (θ∗) and (cid:13)2 to be as small as possible. Combining Theorem 1 and 2, the index\n\n(cid:13)θ0 − θ∗(cid:13) (cid:13)\n\nm (cid:88)\n\nI\n\nk=1\n\n(cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:41)\n\n≥ τ1\n\n· I\n\n(cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:41)\n\n< τ2\n\n=\n\n(cid:40)\n\nm (cid:88)\n\nI\n\nk=1\n\nτ1 ≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n(cid:41)\n\n< τ2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nneed to be as large as possible.\n\nAccording to Theorems 1 and 2, we can obtain the proposed Zero-Shot estimator.\n\nA.2 PROOF OF LEMMA 1 Let’s just simplify JN (θ) to J(N ) for the sake of the statement. Denote J(θ), ∀θ ∈ Γ (cid:0)θ0, R(cid:1) is h-strong convex and H-smooth as “Left” and h · I ⪯ ∇2JN (θ) ⪯ H · I, ∀θ ∈ Γ (cid:0)θ0, R(cid:1) as “Right” respectively. Left ⇒ Right. Since J(θ) is h-strong convex, F (θ) = J(θ) − 1 we have\n\n2 is convex. Then for the convex function F (θ),\n\nh ∥θ∥2\n\n(∇F (θ1) − ∇F (θ2))⊤ (θ1 − θ2) ≥ 0, ∀θ1, θ2 ∈ Γ (cid:0)θ0, R(cid:1) .\n\n(8)\n\nNotice that ∇F (θ) = ∇J (θ) − hθ and substitute it into Equation (8). Then\n\n∥∇J (θ1) − ∇J (θ2)∥2 ∥θ1 − θ2∥2 ≥ (∇J (θ1) − ∇J (θ2))⊤ (θ1 − θ2) ≥ h ∥θ1 − θ2∥2\n\n2\n\n(9)\n\nwhere the first equality comes from Cauchy-schwartz inequality. Since J(θ) is L-smooth, J(θ) is Lipschitz continuous and the Lipschitz constant is H, i.e., ∥∇J(θ1) − J(θ2)∥2 ≤ H ∥θ1 − θ2∥2 , ∀θ1, θ2 ∈ Γ (cid:0)θ0, R(cid:1). By Cauchy-schwartz inequality, we can obtain that\n\n(∇J (θ1) − ∇J (θ2))⊤ (θ1 − θ2) ≤ ∥∇J (θ1) − ∇J (θ2)∥2 ∥θ1 − θ2∥2 ≤ H ∥θ1 − θ2∥2 2 .\n\n(10)\n\nCombing Equation (9) and (10), we have\n\nh ∥θ1 − θ2∥2 ≤ ∥∇J (θ2 + tv) − ∇J (θ2)∥ ≤ H ∥θ1 − θ2∥2 .\n\nInspired by Theorem 5.12 in (Beck, 2017), assume θ1 = θ2 + tv where t > 0, then\n\n∇J (θ2 + tv) − ∇J (θ2) =\n\n(cid:90) t\n\n0\n\n∇2J(θ2 + zv)vdz\n\n(11)\n\n(12)\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nThus\n\nht∥v∥2 ≤\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:90) t\n\n0\n\n∇2J(θ2 + zv)dz · v\n\n(cid:13) (cid:13) (cid:13) (cid:13)2\n\n= ∥∇J (θ2 + tv) − ∇J (θ2)∥2 ≤ Ht∥v∥2\n\n(13)\n\nDivide both sides by t and let t → 0+. Then\n\nh∥v∥2 ≤ (cid:13)\n\n(cid:13)∇2J(θ2) · v(cid:13)\n\n(cid:13)2 ≤ H∥v∥2\n\nBy the arbitrariness of θ1 and θ2, we have\n\nh · I ⪯ ∇2J(θ) ⪯ H · I.\n\n(14)\n\n(15)\n\nRight ⇒ Left. For ∀θ1, θ2 ∈ Γ (cid:0)θ0, R(cid:1), take J(θ) out of the Taylor expansion at θ2 and substitute θ1 into J(θ).\n\nJ(θ1) = J(θ2) + (∇J(θ2))⊤ (θ1 − θ2) +\n\n1 2\n\n(θ1 − θ2)⊤ ∇2J (βθ1 + (1 − β)θ2) (θ1 − θ2)\n\n(16)\n\nwhere 0 < β < 1. By h · I ⪯ ∇2J(θ) ⪯ H · I, we have\n\nand\n\nJ(θ1) ≥ J(θ2) + (∇J(θ2))⊤ (θ1 − θ2) +\n\n1 2\n\nh ∥θ1 − θ2∥2\n\n2\n\nJ(θ1) ≤ J(θ2) + (∇J(θ2))⊤ (θ1 − θ2) +\n\n1 2\n\nH ∥θ1 − θ2∥2 2 .\n\n(17)\n\n(18)\n\nBy the arbitrariness of θ1, θ2, the following conclusion can be obtained by exchanging the position of θ1 and θ2\n\nand\n\nJ(θ2) ≥ J(θ1) + (∇J(θ1))⊤ (θ2 − θ1) +\n\n1 2\n\nh ∥θ1 − θ2∥2\n\n2\n\nJ(θ2) ≤ J(θ1) + (∇J(θ1))⊤ (θ2 − θ1) +\n\n1 2\n\nH ∥θ1 − θ2∥2 2 .\n\nAdding Equation (17) to Equation (19), we can get\n\n(∇J(θ2) − ∇J(θ1))⊤ (θ2 − θ1) ≥ h ∥θ1 − θ2∥2\n\n2\n\nSimilarly, by adding Equation (18) and (20), we have\n\n(∇J(θ2) − ∇J(θ1))⊤ (θ2 − θ1) ≤ H ∥θ1 − θ2∥2 2 .\n\nThus JN (θ) is h-strong convex and H-smooth.\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\nA.3 PROOF OF THEOREM 1: THE UPPER BOUND OF THE LOSS IN THE OPTIMAL PARAMETER\n\nTake the loss function JN (θ) out of the Taylor expansion at the initialization parameter θ0. For ∀θ ∈ Γ (cid:0)θ0, R(cid:1), we can obtain that\n\nJN (θ) = JN\n\n(cid:0)θ0(cid:1) + (cid:2)∇JN\n\n(cid:0)θ0(cid:1)(cid:3)⊤ (cid:0)θ − θ0(cid:1) +\n\n≤ JN\n\n(cid:0)θ0(cid:1) + (cid:2)∇JN\n\n(cid:0)θ0(cid:1)(cid:3)⊤ (cid:0)θ − θ0(cid:1) +\n\n1 2\n1 2\n\n(cid:0)θ − θ0(cid:1)⊤\n\n∇2JN\n\n(cid:0)αθ + (1 − α) (cid:0)θ − θ0(cid:1)(cid:1) (cid:0)θ − θ0(cid:1)\n\nH · (cid:13)\n\n(cid:13)θ − θ0(cid:13) 2\n(cid:13) 2\n\n= JN\n\n(cid:0)θ0(cid:1) +\n\nm (cid:88)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\nk=1\n\n· (cid:0)θk − θ0\n\nk\n\n(cid:1) +\n\n1 2\n\nH ·\n\nm (cid:88)\n\nk=1\n\n(cid:0)θk − θ0\n\nk\n\n(cid:1)2 ≜ LN (θ)\n\n(23)\n\nwhere 0 < α < 1. Denote θ\n\n∗ ≜ argmin\n\nθ∈Γ(θ0,R)\n\nLN (θ). We assert that\n\nJN (θ∗) ≤ LN\n\n∗(cid:17)\n\n(cid:16)\n\nθ\n\n.\n\n(24)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nOtherwise if JN (θ∗) > LN\n\n∗(cid:17)\n\n(cid:16)\n\nθ\n\n, we have\n\n∗(cid:17)\n\n(cid:16)\n\nθ\n\nJN\n\n≥ JN (θ∗) > LN\n\n∗(cid:17)\n\n(cid:16)\n\nθ\n\n(25)\n\nwhich is in contradiction with Equation (23). Notice that LN (θ) is a quadratic function of θ − θ0. To make the upper bound of J (θ∗) to be as small as possible, we expect LN (θ) to be the global minimum in Γ (cid:0)θ0, R(cid:1). The minimum point of LN (θ) is\n\n∗\n\nk − θ0\n\nk = −\n\nθ\n\n1 H\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n, k ∈ {1, · · · , m}\n\nNext we prove that the optimal point of LN (θ) is in the feasible domain Γ (cid:0)θ0, R(cid:1). Since θ∗ is the optimal point of JN (θ) and JN (θ) is H-smooth, we have\n\n(cid:13) (cid:13)∇JN\n\nThen\n\n(cid:0)θ0(cid:1)(cid:13)\n\n(cid:13)2 = (cid:13)\n\n(cid:13)∇JN\n\n(cid:0)θ0(cid:1) − ∇JN (θ∗)(cid:13)\n\n(cid:13)2 ≤ H∥θ∗ − θ0∥2 ≤ HR.\n\n∗\n\n(cid:13) (cid:13) (cid:13)θ\n\n− θ0(cid:13) (cid:13) (cid:13)2\n\n=\n\n1 H\n\n(cid:13) (cid:13)∇JN\n\n(cid:0)θ0(cid:1)(cid:13)\n\n(cid:13)2 ≤ R,\n\nfurther we have θ Thus the global minimum of LN (θ) is\n\n∈ Γ (cid:0)θ0, R(cid:1).\n\n∗\n\n∗(cid:17)\n\n(cid:16)\n\nθ\n\nLN\n\n= JN\n\n(cid:0)θ0(cid:1) −\n\n1 2H\n\n(cid:32)\n\nm (cid:88)\n\n∂JN\n\n(cid:33)2\n\n(cid:0)θ0(cid:1)\n\nk=1\n\n∂θk\n\n.\n\nFor the upper bound of JN (θ∗), we can obtain that\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\nJN (θ∗) ≤ LN\n\n∗(cid:17)\n\n(cid:16)\n\nθ\n\n= JN\n\n(cid:0)θ0(cid:1) −\n\n1 2H\n\n≤ JN\n\n(cid:0)θ0(cid:1) −\n\n1 2H\n\nτ 2 1 ·\n\nm (cid:88)\n\nI\n\nk=1\n\n(cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:41)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\nm (cid:88)\n\nk=1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂θk\n\n≥ τ1\n\n−\n\n1 2H\n\n· ε2 ·\n\n= JN (θ0) −\n\n1 2H\n\nε2m −\n\n1 2H\n\n(cid:0)τ 2\n\n1 − ε2(cid:1) ·\n\nm (cid:88)\n\nI\n\nk=1\n\n(cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nm (cid:88)\n\nI\n\nk=1\n\n(cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:41)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:41)\n\n< τ1\n\n≥ τ1\n\n(30)\n\nwhere 0 ≤ ε ≜ min\n\n(cid:26)(cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN (θ0) ∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN (θ0) ∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:27)\n\n< τ1\n\n< τ1.\n\nA.4 PROOF OF THEOREM 2: THE UPPER BOUND OF DISTANCE BETWEEN THE INITIAL\n\nPARAMETER AND THE OPTIMAL PARAMETER\n\nDenote the vector l1 norm as ∥·∥1. Take the gradient of loss function ∇JN (θ) out of the Taylor expansion at the optimal parameter θ∗ and substitute the initialization parameter θ0 into the gradient of loss function.\n\n∇JN\n\n(cid:0)θ0(cid:1) = ∇JN (θ∗) + ∇2JN\n\n(cid:0)γθ0 + (1 − γ)θ∗(cid:1) · (cid:0)θ0 − θ∗(cid:1)\n\n= ∇2JN\n\n(cid:0)γθ0 + (1 − γ)θ∗(cid:1) · (cid:0)θ0 − θ∗(cid:1)\n\n(31)\n\nwhere 0 < γ < 1 and the last equality comes from ∇JN (θ∗) = 0. By Assumption 1, we have (cid:13)θ0 − θ∗(cid:13) h · I ⪯ ∇2JN (cid:13)2.\n\n(cid:0)γθ0 + (1 − γ)θ∗(cid:1) ⪯ H · I. Thus h · (cid:13)\n\n(cid:13)2 ≤ H · (cid:13)\n\n(cid:13)θ0 − θ∗(cid:13)\n\n(cid:13)2 ≤ (cid:13)\n\n(cid:0)θ0(cid:1)(cid:13)\n\n(cid:13)∇JN\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFurther we can obtain that\n\n(cid:13)θ0 − θ∗(cid:13) (cid:13)\n\n(cid:13)2 ≤\n\n≤\n\n=\n\n=\n\n1 h\n\n1 h\n\n1 h\n\n1 h\n\n(cid:13) (cid:13)∇JN\n\n(cid:0)θ0(cid:1)(cid:13)\n\n(cid:13)2 ≤\n\n1 h\n\n· (cid:13) (cid:13)∇JN\n\n(cid:0)θ0(cid:1)(cid:13)\n\n(cid:13)1 =\n\nm (cid:88)\n\nk=1\n\nm (cid:88)\n\nk=1\n\nI\n\nI\n\n(cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n· τ2 ·\n\n· τ2 ·\n\n(cid:32)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n∂θk\n\n(cid:41)\n\n(cid:41)\n\n+\n\n+\n\n1 h\n\n1 h\n\n· M ·\n\n· M ·\n\n< τ2\n\n< τ2\n\n·\n\nM · m − (M − τ2)\n\n1 h\n\n·\n\nm (cid:88)\n\nk=1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:0)θ0(cid:1)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂JN\n\n∂θk\n\n(cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) m\n(cid:88)\n\nm (cid:88)\n\nI\n\nk=1 (cid:32)\n\nm −\n\n∂θk (cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nI\n\nk=1\n\n(cid:41)(cid:33)\n\n< τ2\n\n.\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) m\n(cid:88)\n\nk=1\n\n(cid:41)\n\n≥ τ2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:41)(cid:33)\n\n< τ2\n\n(32)\n\nwhere M ≜ max\n\n(cid:26)(cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN (θ0) ∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN (θ0) ∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n∂JN\n\n(cid:0)θ0(cid:1)\n\n∂θk\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nI\n\n(cid:40)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:27)\n\n≥ τ2\n\n> τ2 > τ1 > 0.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nB SWEET GRADIENT PHENOMENON\n\nSweet Gradient widely exists across different search spaces, datasets, the number of architectures, batch size, and initializations. Here we list the heatmaps of Sweet Gradient under various configurations.\n\nB.1 DIFFERENT SEARCH SPACES\n\n(a) NAS-Bench-101\n\n(b) NAS-Bench-201\n\n(c) NAS-Bench-301\n\n(d) NDS-NASNet\n\nFigure 4: Sweet Gradient across different search spaces.\n\n19\n\n5e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.530.550.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.520.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.600.610.600.600.600.600.600.600.590.510.460.410.400.400.400.400.390.390.600.600.600.600.600.600.600.590.510.460.410.400.400.400.390.390.390.600.600.600.600.600.600.590.510.460.410.400.400.390.390.390.390.600.600.600.600.600.580.510.460.410.400.400.390.390.390.390.600.600.600.600.580.510.460.410.400.390.390.390.390.390.600.600.600.580.510.460.410.400.390.390.390.390.390.600.600.580.500.450.410.400.380.380.380.380.380.590.580.500.450.400.390.370.370.370.370.370.570.460.400.320.320.310.310.310.310.310.400.330.250.250.240.240.240.240.240.090.030.020.010.010.010.010.01-0.02-0.03-0.04-0.05-0.05-0.05-0.05-0.08-0.12-0.13-0.13-0.13-0.13-0.18-0.19-0.20-0.20-0.20-0.35-0.40-0.40-0.40-0.43-0.42-0.43-0.35-0.35-0.320.40.20.00.20.40.65e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr1-0.37-0.35-0.21-0.20-0.14-0.13-0.13-0.13-0.14-0.15-0.13-0.080.300.510.670.700.720.710.710.710.71-0.330.030.080.220.240.270.270.280.280.290.310.650.800.850.860.860.860.850.860.860.030.070.220.240.270.270.280.280.290.310.650.800.850.860.860.860.850.850.85-0.000.220.240.270.270.280.280.290.320.660.800.850.860.860.860.850.850.850.210.240.270.270.280.280.290.320.660.800.860.860.860.860.850.850.850.240.270.270.280.280.300.340.670.810.860.860.860.850.840.850.850.280.280.280.280.310.350.690.810.860.860.860.850.840.840.840.280.290.300.330.400.720.830.870.870.860.840.840.840.840.300.300.360.430.730.840.870.870.850.840.840.840.840.390.440.510.800.870.900.890.860.850.850.850.850.450.550.840.880.900.880.850.850.850.850.850.760.900.910.900.870.860.860.860.860.860.910.920.890.860.850.850.860.860.860.910.830.810.810.810.810.810.810.770.740.730.730.730.730.730.520.540.540.540.540.540.470.450.450.450.450.240.230.230.230.180.180.180.320.320.230.20.00.20.40.60.85e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.160.220.380.430.550.620.660.660.660.670.660.660.650.640.610.550.450.440.440.440.440.110.340.400.540.620.660.660.660.670.660.660.650.640.610.550.450.440.440.440.440.330.400.540.620.660.660.660.670.660.660.650.640.610.550.450.440.440.440.440.320.530.620.660.660.660.670.660.660.650.640.610.550.450.440.440.440.440.520.610.660.660.660.670.660.660.650.640.610.550.450.440.440.440.440.550.660.660.660.660.660.660.650.640.610.550.450.440.440.440.440.660.660.660.670.660.660.650.640.610.550.450.440.440.440.440.640.660.670.660.660.650.640.610.550.450.440.440.440.440.660.660.660.660.650.640.610.550.450.440.440.440.440.670.660.660.650.640.610.550.450.440.440.440.440.660.660.650.640.600.550.440.440.440.440.440.650.650.640.600.540.430.420.420.420.420.650.640.590.530.410.400.400.400.400.630.520.390.200.180.180.180.180.390.15-0.08-0.08-0.08-0.08-0.08-0.54-0.54-0.53-0.53-0.53-0.53-0.53-0.51-0.51-0.51-0.51-0.39-0.39-0.39-0.39-0.36-0.36-0.36-0.16-0.16-0.160.40.20.00.20.40.65e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.210.280.560.600.650.660.670.670.670.670.680.670.670.670.640.610.550.540.520.520.510.270.550.600.650.670.670.670.670.670.680.670.670.670.640.610.550.540.520.520.510.530.600.650.670.670.670.670.670.680.670.670.670.640.610.550.540.520.520.510.540.650.660.670.670.670.670.680.670.670.670.640.610.550.540.520.520.510.640.660.670.670.670.680.680.670.670.670.640.610.550.540.520.520.510.650.670.670.680.680.680.670.670.670.640.610.550.540.520.520.510.670.670.670.670.680.670.670.670.640.610.550.540.520.520.510.670.670.670.680.670.670.670.640.610.550.540.520.520.510.680.670.680.670.670.670.640.610.550.540.520.520.510.680.680.670.670.670.640.610.550.540.520.520.510.680.670.670.670.640.610.550.540.520.520.510.670.670.670.640.600.550.530.520.510.510.670.670.630.600.550.530.510.510.510.670.620.580.500.490.470.470.470.600.550.450.440.420.420.410.320.240.220.220.210.210.150.130.120.110.100.030.00-0.00-0.01-0.04-0.04-0.05-0.18-0.19-0.250.20.00.20.40.6Under review as a conference paper at ICLR 2023\n\nB.2 DIFFERENT DATASETS\n\n(a) CIFAR-10\n\n(b) CIFAR-100\n\n(c) ImageNet16-120\n\nFigure 5: Sweet Gradient across different dataset in NAS-Bench-201.\n\n20\n\n5e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr1-0.37-0.35-0.21-0.20-0.14-0.13-0.13-0.13-0.14-0.15-0.13-0.080.300.510.670.700.720.710.710.710.71-0.330.030.080.220.240.270.270.280.280.290.310.650.800.850.860.860.860.850.860.860.030.070.220.240.270.270.280.280.290.310.650.800.850.860.860.860.850.850.85-0.000.220.240.270.270.280.280.290.320.660.800.850.860.860.860.850.850.850.210.240.270.270.280.280.290.320.660.800.860.860.860.860.850.850.850.240.270.270.280.280.300.340.670.810.860.860.860.850.840.850.850.280.280.280.280.310.350.690.810.860.860.860.850.840.840.840.280.290.300.330.400.720.830.870.870.860.840.840.840.840.300.300.360.430.730.840.870.870.850.840.840.840.840.390.440.510.800.870.900.890.860.850.850.850.850.450.550.840.880.900.880.850.850.850.850.850.760.900.910.900.870.860.860.860.860.860.910.920.890.860.850.850.860.860.860.910.830.810.810.810.810.810.810.770.740.730.730.730.730.730.520.540.540.540.540.540.470.450.450.450.450.240.230.230.230.180.180.180.320.320.230.20.00.20.40.60.85e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr1-0.42-0.38-0.23-0.22-0.19-0.19-0.19-0.18-0.20-0.20-0.19-0.140.220.440.600.640.660.660.660.660.67-0.340.030.100.170.170.200.210.210.220.230.250.580.750.800.810.820.820.820.820.82-0.010.080.170.170.200.210.210.220.230.250.580.750.810.810.820.820.820.820.82-0.030.170.170.200.210.210.220.230.250.590.750.810.810.820.820.810.820.820.160.160.200.210.210.220.230.260.600.760.810.810.820.820.810.810.810.140.200.210.210.220.240.280.620.770.810.820.820.810.810.810.810.200.210.220.220.240.290.630.770.820.820.820.810.810.810.810.220.220.230.280.340.660.780.830.820.820.810.810.810.810.220.230.310.380.690.800.830.820.810.800.810.810.810.310.370.450.760.810.850.840.820.830.830.830.830.400.510.800.830.860.840.820.830.830.830.830.730.860.880.850.830.830.830.830.830.830.880.880.850.820.830.830.830.830.830.870.780.770.780.790.790.790.790.730.700.700.700.700.700.700.510.530.530.530.530.530.460.460.460.460.460.360.350.350.350.340.340.340.350.350.260.40.20.00.20.40.60.85e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr1-0.41-0.39-0.29-0.28-0.21-0.19-0.19-0.20-0.21-0.22-0.21-0.180.100.300.530.590.640.630.630.630.64-0.35-0.07-0.040.160.180.180.190.200.200.200.220.460.640.760.780.800.800.800.800.80-0.08-0.040.160.180.180.190.190.200.200.220.460.640.760.780.800.800.800.800.80-0.150.160.180.180.200.190.200.200.220.470.640.770.780.800.800.790.790.790.170.180.180.200.190.200.200.220.470.640.770.780.800.800.790.790.790.150.180.200.200.200.200.230.490.660.770.780.800.800.780.780.780.180.200.200.200.210.240.500.670.770.780.800.800.790.790.790.210.210.210.230.250.540.680.780.790.800.790.790.790.790.210.210.230.290.560.690.790.790.800.780.780.790.790.250.330.370.620.760.810.810.800.800.800.800.800.350.390.670.770.820.810.800.800.800.800.800.580.790.820.820.810.800.800.800.800.800.810.830.820.810.800.800.810.810.810.830.800.770.770.770.780.780.780.760.740.740.740.740.740.740.540.530.540.540.540.540.480.470.470.470.470.350.360.360.360.340.340.340.260.260.290.40.20.00.20.40.60.8Under review as a conference paper at ICLR 2023\n\nB.3 DIFFERENT NUMBER OF ARCHITECTURES\n\n(a) Number = 25\n\n(b) Number = 50\n\n(c) Number = 100\n\nFigure 6: Sweet Gradient across different number of architectures in NAS-Bench-101.\n\n21\n\n5e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.390.370.400.440.460.450.450.450.450.450.430.420.290.240.240.240.230.220.220.220.210.340.380.460.450.450.450.450.450.450.430.420.290.240.240.240.230.220.220.220.200.410.440.460.450.450.450.450.450.430.420.290.240.240.240.230.220.220.220.210.480.450.450.450.450.450.450.430.420.290.240.240.240.230.220.220.210.200.440.450.450.450.450.450.430.420.290.240.240.240.230.220.220.210.200.450.450.450.450.450.430.420.290.240.240.240.230.220.210.210.200.450.450.450.450.430.420.290.240.240.240.230.220.210.210.200.450.450.450.430.420.290.240.240.240.220.200.200.200.200.450.450.430.420.290.230.240.240.220.200.200.200.200.450.430.420.290.220.240.240.200.200.200.200.200.430.420.270.220.240.220.200.200.200.200.200.400.270.220.160.160.130.130.130.130.130.220.130.130.110.120.120.120.120.12-0.03-0.07-0.07-0.05-0.05-0.05-0.05-0.05-0.09-0.09-0.06-0.07-0.07-0.07-0.070.060.080.060.040.040.04-0.01-0.02-0.04-0.04-0.04-0.37-0.41-0.39-0.39-0.41-0.39-0.39-0.09-0.09-0.230.40.30.20.10.00.10.20.30.45e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.460.490.510.520.550.540.540.540.530.530.530.520.450.400.360.350.340.340.340.340.330.440.510.520.540.540.540.540.530.530.530.520.450.400.360.350.340.340.340.340.330.520.520.550.540.540.540.530.530.530.520.450.400.360.350.340.340.340.340.330.540.540.540.540.540.530.530.530.520.450.400.360.350.340.340.340.340.330.540.540.540.540.530.530.530.520.450.400.360.350.340.340.340.340.330.540.530.540.530.530.530.520.450.400.360.350.340.340.340.340.330.530.540.530.530.530.520.450.400.360.350.340.340.340.330.330.530.530.530.530.520.450.400.360.350.340.330.330.330.330.530.530.530.520.450.400.360.350.340.330.330.330.330.530.530.520.450.390.350.340.330.330.330.330.330.520.520.440.390.350.340.320.320.320.320.320.500.410.350.280.270.260.260.260.260.260.360.290.220.220.210.210.200.200.200.060.01-0.01-0.02-0.02-0.02-0.02-0.02-0.04-0.05-0.05-0.06-0.06-0.06-0.06-0.03-0.06-0.07-0.08-0.08-0.08-0.12-0.12-0.13-0.13-0.13-0.38-0.40-0.39-0.39-0.41-0.39-0.39-0.25-0.24-0.290.40.20.00.20.45e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.530.550.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.520.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.600.610.600.600.600.600.600.600.590.510.460.410.400.400.400.400.390.390.600.600.600.600.600.600.600.590.510.460.410.400.400.400.390.390.390.600.600.600.600.600.600.590.510.460.410.400.400.390.390.390.390.600.600.600.600.600.580.510.460.410.400.400.390.390.390.390.600.600.600.600.580.510.460.410.400.390.390.390.390.390.600.600.600.580.510.460.410.400.390.390.390.390.390.600.600.580.500.450.410.400.380.380.380.380.380.590.580.500.450.400.390.370.370.370.370.370.570.460.400.320.320.310.310.310.310.310.400.330.250.250.240.240.240.240.240.090.030.020.010.010.010.010.01-0.02-0.03-0.04-0.05-0.05-0.05-0.05-0.08-0.12-0.13-0.13-0.13-0.13-0.18-0.19-0.20-0.20-0.20-0.35-0.40-0.40-0.40-0.43-0.42-0.43-0.35-0.35-0.320.40.20.00.20.40.6Under review as a conference paper at ICLR 2023\n\nB.4 DIFFERENT BATCH SIZES\n\n(a) Batch Size = 32\n\n(b) Number = 64\n\n(c) Number = 128\n\nFigure 7: Sweet Gradient across different batch size in NAS-Bench-101.\n\n22\n\n5e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.480.530.580.600.600.600.600.600.600.600.600.600.560.510.420.410.400.400.400.400.390.500.580.600.600.600.600.600.600.600.600.600.560.510.420.410.400.400.400.400.390.580.600.600.600.600.600.600.600.600.600.560.510.420.410.400.400.400.400.390.600.600.600.600.600.600.600.600.600.560.510.420.410.400.400.400.400.390.600.600.600.600.600.600.600.600.560.510.420.410.400.400.400.400.390.600.600.600.600.600.600.600.560.510.420.410.400.400.390.390.390.600.600.600.600.600.600.560.510.420.410.400.400.390.390.390.600.600.600.600.600.560.510.420.410.400.400.390.390.390.600.600.600.600.560.510.420.410.400.390.390.390.390.600.600.600.560.510.420.410.400.390.380.380.390.600.600.560.510.410.410.390.390.380.380.380.590.550.480.380.370.340.340.340.340.340.520.450.340.320.310.310.310.310.300.260.130.130.110.110.100.100.100.050.040.020.010.010.010.01-0.02-0.04-0.05-0.06-0.06-0.06-0.09-0.11-0.12-0.12-0.12-0.24-0.29-0.29-0.29-0.37-0.37-0.37-0.36-0.36-0.320.20.00.20.40.65e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.520.560.600.600.600.600.600.610.610.610.600.590.540.480.410.400.400.400.400.400.390.570.600.600.600.600.600.610.610.610.600.590.540.480.410.400.400.400.400.400.390.600.600.600.600.600.610.610.610.600.590.540.480.410.400.400.400.400.400.390.590.600.600.600.610.610.610.600.590.540.480.410.400.400.400.400.400.390.600.600.600.610.610.610.600.590.540.480.410.400.400.400.400.390.390.600.600.610.610.610.600.590.540.480.410.400.400.400.390.390.390.600.610.610.610.600.590.540.480.410.400.400.400.390.390.390.610.600.610.600.590.540.480.410.400.400.390.390.390.390.600.610.600.590.540.480.410.400.400.390.390.390.390.610.600.590.540.480.410.400.390.390.380.380.380.600.590.540.470.410.400.380.380.380.380.380.580.500.430.350.340.320.320.320.320.320.470.390.300.290.270.270.270.270.270.160.080.070.050.050.040.040.040.01-0.00-0.02-0.02-0.03-0.02-0.02-0.04-0.08-0.09-0.10-0.10-0.10-0.14-0.15-0.16-0.16-0.16-0.29-0.34-0.34-0.34-0.40-0.39-0.39-0.35-0.34-0.330.40.20.00.20.40.65e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.530.550.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.520.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.600.610.600.600.600.600.600.600.590.510.460.410.400.400.400.400.390.390.600.600.600.600.600.600.600.590.510.460.410.400.400.400.390.390.390.600.600.600.600.600.600.590.510.460.410.400.400.390.390.390.390.600.600.600.600.600.580.510.460.410.400.400.390.390.390.390.600.600.600.600.580.510.460.410.400.390.390.390.390.390.600.600.600.580.510.460.410.400.390.390.390.390.390.600.600.580.500.450.410.400.380.380.380.380.380.590.580.500.450.400.390.370.370.370.370.370.570.460.400.320.320.310.310.310.310.310.400.330.250.250.240.240.240.240.240.090.030.020.010.010.010.010.01-0.02-0.03-0.04-0.05-0.05-0.05-0.05-0.08-0.12-0.13-0.13-0.13-0.13-0.18-0.19-0.20-0.20-0.20-0.35-0.40-0.40-0.40-0.43-0.42-0.43-0.35-0.35-0.320.40.20.00.20.40.6Under review as a conference paper at ICLR 2023\n\nB.5 DIFFERENT INITIALIZATION\n\n(a) Seed = 0\n\n(b) Seed = 1\n\n(c) Seed = 2\n\nFigure 8: Sweet Gradient across different initialization in NAS-Bench-101.\n\n23\n\n5e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.530.560.580.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.560.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.590.600.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.390.390.600.610.600.600.600.600.600.600.590.510.460.410.400.400.400.400.390.390.600.600.600.600.600.600.600.590.510.460.410.400.400.400.390.390.390.600.600.600.600.600.600.590.510.460.410.400.400.400.390.390.390.600.600.600.600.600.590.510.460.410.400.400.390.390.390.390.600.600.600.600.590.510.460.410.400.390.390.390.390.390.600.600.600.590.510.460.410.400.390.390.390.390.390.600.600.590.500.450.410.390.380.380.380.380.380.600.590.500.450.400.390.370.370.370.370.370.570.450.390.320.320.310.300.300.300.300.400.330.250.240.240.240.240.240.240.090.030.030.010.010.010.010.01-0.01-0.02-0.03-0.03-0.03-0.03-0.03-0.07-0.11-0.11-0.12-0.12-0.12-0.16-0.17-0.18-0.18-0.18-0.34-0.38-0.38-0.38-0.42-0.41-0.41-0.32-0.32-0.290.40.20.00.20.40.65e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.550.570.610.610.600.600.600.600.600.600.600.580.510.460.410.400.400.400.400.400.390.520.610.610.600.600.600.600.600.600.600.580.510.460.410.400.400.400.400.400.390.610.610.600.600.600.600.600.600.600.580.510.460.410.400.400.400.400.390.390.600.600.600.600.600.600.600.600.580.510.460.410.400.400.400.400.390.390.600.600.600.600.600.600.600.580.510.460.410.400.400.400.390.390.390.600.600.610.600.600.600.580.510.460.410.400.400.390.390.390.390.600.610.600.600.600.580.510.460.410.400.400.390.390.390.390.610.600.600.600.580.510.460.410.400.390.390.390.390.390.600.600.600.580.510.460.410.400.390.390.390.390.390.600.600.580.510.450.410.400.390.380.380.380.380.590.580.500.450.400.390.370.370.370.370.370.580.460.390.320.310.310.310.300.310.310.400.330.250.240.240.240.240.240.240.090.040.030.010.010.010.010.01-0.01-0.02-0.03-0.03-0.04-0.04-0.04-0.07-0.11-0.11-0.12-0.12-0.12-0.17-0.18-0.19-0.19-0.19-0.35-0.37-0.37-0.37-0.40-0.40-0.40-0.30-0.30-0.270.40.20.00.20.40.65e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.05.0thr205e-101e-095e-091e-085e-081e-075e-071e-065e-061e-055e-050.00010.00050.0010.0050.010.050.10.51.0thr10.550.570.590.590.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.540.590.590.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.400.390.590.590.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.390.390.580.600.600.600.600.600.600.600.590.510.460.410.400.400.400.400.390.390.600.600.600.600.600.600.600.590.510.460.410.400.400.400.390.390.390.600.600.600.600.600.600.590.510.460.410.400.400.400.390.390.390.600.600.600.600.600.590.510.460.410.400.400.390.390.390.390.600.600.600.600.590.510.460.410.400.390.390.390.390.390.600.600.600.590.510.460.410.400.390.390.390.390.390.600.600.590.500.450.410.400.380.380.380.380.380.590.580.500.450.400.390.370.370.370.370.370.570.450.390.320.310.310.310.310.310.310.390.320.250.240.240.240.240.240.240.080.020.020.000.00-0.00-0.00-0.00-0.02-0.03-0.04-0.05-0.05-0.05-0.05-0.09-0.13-0.14-0.15-0.15-0.15-0.19-0.21-0.21-0.21-0.21-0.39-0.39-0.40-0.40-0.42-0.42-0.41-0.38-0.37-0.320.40.20.00.20.40.6Under review as a conference paper at ICLR 2023\n\nC MORE INTERVALS IN NAS-BENCH-101 AND NAS-BENCH-201\n\nC.1\n\nINTERVALS IN NAS-BENCH-101\n\nFigure 9: More Intervals in NAS-Bench-101.\n\n24\n\nSweet: [5e-10, 1e-9)Sweet: [1e-9, 5e-9)Sweet: [5e-9, 1e-8)Sweet: [1e-8, 5e-8)proportionNon-Sweet: [0.05, 0.1)Non-Sweet: [0.1, 0.5)Non-Sweet: [0.5, 1.0)Non-Sweet: [1.0, 5.0)proportioncell depthcell depthcell depthcell depthSweet: [5e-8, 1e-7)Sweet: [1e-7, 5e-7)Sweet: [5e-7, 1e-6)Sweet: [1e-6, 5e-6)proportionSweet: [5e-6, 1e-5)Sweet: [1e-5, 5e-5)Sweet: [5e-5, 0.0001)Non-Sweet: [0.0001, 0.0005)proportionNon-Sweet: [0.0005, 0.001)Non-Sweet: [0.001, 0.005)Non-Sweet: [0.005, 0.01)Non-Sweet: [0.01, 0.05)proportionUnder review as a conference paper at ICLR 2023\n\nC.2\n\nINTERVALS IN NAS-BENCH-201\n\nFigure 10: More Intervals in NAS-Bench-201.\n\n25\n\nNon-Sweet: [5e-10, 1e-9)Non-Sweet: [1e-9, 5e-9)Non-Sweet: [5e-9, 1e-8)Non-Sweet: [1e-8, 5e-8)proportionNon-Sweet: [0.05, 0.1)Non-Sweet: [0.1, 0.5)Non-Sweet: [0.5, 1.0)Non-Sweet: [1.0, 5.0)proportioncell depthcell depthcell depthcell depthNon-Sweet: [5e-8, 1e-7)Non-Sweet: [1e-7, 5e-7)Non-Sweet: [5e-7, 1e-6)Non-Sweet: [1e-6, 5e-6)proportionNon-Sweet: [5e-6, 1e-5)Non-Sweet: [1e-5, 5e-5)Sweet: [5e-5, 0.0001)Sweet: [0.0001, 0.0005)proportionSweet: [0.0005, 0.001)Sweet: [0.001, 0.005)Non-Sweet: [0.005, 0.01)Non-Sweet: [0.01, 0.05)proportionUnder review as a conference paper at ICLR 2023\n\nD OTHER INVESTIGATIONS\n\nInvestigation 1: Is the Sweet Gradient related to gradient distribution? A natural conjecture is that Sweet Gradient is more likely to exist in intervals with concentrated gradient values. Figure 11(a) and 11(b) show the distribution of the absolute gradients values in NAS-Bench-101 and NAS-Bench201, which disproves the conjecture. One reason is that there are Non-Sweet Gradient intervals where the gradients are concentrated, e.g., [0.001, 0.005) in NAS-Bench-201. Another reason is that there are also Sweet Gradient intervals where the gradients are not concentrated, e.g., [5e−7, 1e−6) in NAS-Bench-101.\n\nInvestigation 2: Is the Sweet Gradient related to activation distribution? The activation reflects the expressibility (Chen et al., 2021a) of the network, so does the activation value distribution affect the sweet gradient? Figure 11(c) and 11(d) show the distribution of absolute activation values in NAS-Bench-101 and NAS-Bench-201. The activation values are mainly distributed in [0.01, 5), and the corresponding interval is almost staggered with Sweet Gradient with a large order of magnitude difference, which reveals that they are not related.\n\n(a) Gradient, NAS-Bench-101\n\n(b) Gradient, NAS-Bench-201\n\n(c) Activation, NAS-Bench-101\n\n(d) Activation, NAS-Bench-201\n\nFigure 11: The gradient and activation distribution under different intervals in NAS-Bench-101 and NAS-Bench-201 (CIFAR-10).\n\n26\n\n0~5e-105e-10~1e-091e-09~5e-095e-09~1e-081e-08~5e-085e-08~1e-071e-07~5e-075e-07~1e-061e-06~5e-065e-06~1e-051e-05~5e-055e-05~0.00010.0001~0.00050.0005~0.0010.001~0.0050.005~0.010.01~0.050.05~0.10.1~0.50.5~1.01.0~5.0thr1~thr20.00.51.01.52.02.53.03.5counts1e80~5e-105e-10~1e-091e-09~5e-095e-09~1e-081e-08~5e-085e-08~1e-071e-07~5e-075e-07~1e-061e-06~5e-065e-06~1e-051e-05~5e-055e-05~0.00010.0001~0.00050.0005~0.0010.001~0.0050.005~0.010.01~0.050.05~0.10.1~0.50.5~1.01.0~5.0thr1~thr20.00.20.40.60.81.01.2counts1e70~5e-105e-10~1e-091e-09~5e-095e-09~1e-081e-08~5e-085e-08~1e-071e-07~5e-075e-07~1e-061e-06~5e-065e-06~1e-051e-05~5e-055e-05~0.00010.0001~0.00050.0005~0.0010.001~0.0050.005~0.010.01~0.050.05~0.10.1~0.50.5~1.01.0~5.05.0~10thr1~thr20.00.20.40.60.81.01.2counts1e80~5e-105e-10~1e-091e-09~5e-095e-09~1e-081e-08~5e-085e-08~1e-071e-07~5e-075e-07~1e-061e-06~5e-065e-06~1e-051e-05~5e-055e-05~0.00010.0001~0.00050.0005~0.0010.001~0.0050.005~0.010.01~0.050.05~0.10.1~0.50.5~1.01.0~5.05.0~10thr1~thr20.000.250.500.751.001.251.501.752.00counts1e7Under review as a conference paper at ICLR 2023\n\nE EXPERIMENTAL DETAILS\n\nE.1 DETAILS OF CONSISTENCY EXPERIMENTS\n\nExperiments are mainly based on the publicly available codes from Mellor et al. (2021); Abdelfattah et al. (2021); Zhang & Jia (2022); Chen et al. (2021a); Lin et al. (2021); Mehta et al. (2022) (MIT or Apache-2.0 licence). Following Zhang & Jia (2022), the seed is 42 which is randomly chosen. The batch size is 64 in NAS-Bench-101, NAS-Bench-201, and NAS-Bench-301, and 128 in NDS space, which is also the same as Zhang & Jia (2022). For Sweetimator, we set the architecture number to 100, and the split to 2.\n\nE.2 DETAILS OF NAS-BENCH-201 EXPERIMENTS\n\nSweetimator has a batch size of 128, architecture numbers of 100, and a split of 2 in architecture selection and estimator-assisted NAS. For a fair comparison, we strictly followed the algorithm proposed in Mellor et al. (2021); Zhang & Jia (2022).\n\nE.3 DETAILS OF DARTS EXPERIMENTS\n\nIn the search phase, Sweetimator is obtained by a batch size of 128, architecture numbers of 100, and a split of 2 on CIFAR-10 and ImageNet. Then we integrate Sweetimator into REA algorithm by replacing training networks with computing the score of Sweetimator. Following Dong & Yang (2020), we set the number of cycles to 200, the population size to 10 and the sample size to 3. Although the cycle is only 200, the scores converge rapidly and the search results can be competitive with other Zero-Shot methods, which also bring a fast search speed. Therefore, we did not do more hyper-parameter tuning. Please note that the search time includes computing the best interval and searching for the optimal architecture.\n\nIn the retraining phase, we follow DARTS settings to train architectures. On CIFAR-10, the network consists of 20 layers with 36 initial channels. We utilized the SGD optimizer to train the network for 600 epochs with a batch size of 96. The learning rate decays from 0.025 to 0 by the cosine scheduler. Other settings like cutout, auxiliary, and path dropout are the same as DARTS. On ImageNet, the network consists of 14 cells with 48 channels, which is restricted to be less than 600M FLOPs. The SGD optimizer is used to train the network with 250 epochs, a learning rate of 0.5, weight decay of 3e−5, and a batch size of 1024. We train the network on eight NVIDIA V100 for around three days.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nF COMPARISON WITH NON-ZERO-SHOT ESTIMATORS\n\nSettings. We compare Sweetimator with Non-Zero-Shot estimators in NAS-Bench-201. The baselines include SPOS (Guo et al., 2020), Neural Predictor (Wen et al., 2020), NAO (Luo et al., 2018), TNASP (Lu et al., 2021). For SPOS, we trained a supernet with 250 epochs and a batch size of 256, then utilized the validation accuracy of sub-networks as the estimator. For Neural Predictor, NAO and TNASP, we directly used the results in Lu et al. (2021). For Sweetimator, we set the batch size to 128, the architecture number to 100, and the split to 2. The compared metric is Kendall’s Tau between the score of estimators and the test accuracy on CIFAR-10 of the benchmark.\n\nResults. Table 7 shows the comparison results. Sweetimator can achieve superior rank consistency to Non-Zero-Shot estimators on NAS-Bench-201, which further demonstrate the effectiveness of the proposed method. Moreover, Sweetimator takes only a few minutes to obtain the best interval, which is more efficient than the process of training networks in Non-Zero-Shot estimators.\n\nTable 6: Rank Consistency of Non-Zero-Shot estimators and Sweetimator in NAS-Bench-201.\n\nEstimators\n\nKendall’s Tau\n\nType\n\nSPOS Neural Predictor NAO TNASP Sweetimator\n\n0.621 0.646 0.526 0.724 0.736\n\nNon-Zero-Shot Non-Zero-Shot Non-Zero-Shot Non-Zero-Shot Zero-Shot\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nG VISUALIZATION OF SEARCH RESULTS\n\nG.1 NAS-BENCH-201\n\nFigure 12: Visualization of model test accuracy versus Sweetimator metric score on CIFAR-10, CIFAR-100, ImageNet16-120.\n\nG.2 DARTS CELLS\n\n(a) CIFAR-10, Normal Cell\n\n(b) CIFAR-10, Reduction Cell\n\n(c) ImageNet, Normal Cell\n\n(d) ImageNet, Reduction Cell\n\nFigure 13: Visualization of the best searched cells on CIFAR-10 and ImageNet.\n\n29\n\n0100000200000300000400000Sweetimator metric score20406080Model accuracyCIFAR-100100000200000300000400000Sweetimator metric score010203040506070Model accuracyCIFAR-100050000100000150000200000250000Sweetimator metric score010203040Model accuracyImageNet16-120c_{k-2}0sep_conv_5x51avg_pool_3x32skip_connect3sep_conv_5x5c_{k-1}sep_conv_5x5skip_connectsep_conv_5x5c_{k}sep_conv_3x3c_{k-2}0skip_connect1max_pool_3x32dil_conv_5x53dil_conv_5x5c_{k-1}avg_pool_3x3avg_pool_3x3sep_conv_5x5c_{k}dil_conv_3x3c_{k-2}0sep_conv_3x31dil_conv_5x52sep_conv_5x53dil_conv_5x5c_{k-1}sep_conv_5x5skip_connectc_{k}dil_conv_5x5dil_conv_5x5c_{k-2}0max_pool_3x31max_pool_3x32skip_connectc_{k-1}sep_conv_3x3sep_conv_5x5dil_conv_5x53sep_conv_3x3c_{k}sep_conv_3x3Under review as a conference paper at ICLR 2023\n\nH MORE DIFFERENT TASKS\n\nWe conduct experiments on NAS-Bench-NLP and NAS-Bench-ASR benchmarks to further validate our method. The compared baselines include SynFlow, GradSign, and Parameters. The results in the following table show that Sweetimator has superior performance consistency than other methods. However, the Spearman’s rank of NAS-Bench-NLP drops significantly compared to that of CV tasks. We conjecture that the text generation tasks and RNN architectures are more complicated than the classification tasks and CNN architectures, and furthermore, the hyper-parameters of the architecture candidates in the NAS-Bench-NLP search space are not well-tuned. Due to the limited time available for rebuttal, we will improve the consistency of Sweetimator for more tasks in the future.\n\nTable 7: Rank Consistency of Non-Zero-Shot estimators and Sweetimator and other Zero-Shot estimators by Spearman’s rank in NAS-Bench-NLP and NAS-Bench-ASR.\n\nBenchmark\n\nSynFlow GradSign\n\nParameters\n\nSweetimator\n\nNAS-Bench-NLP NAS-Bench-ASR\n\n-0.062 0.244\n\n0.134 0.288\n\n-0.046 0.305\n\n0.159 0.311\n\n30",
    "reference": "# Summary Of The Paper\n\nThis paper finds that the absolute gradient values within a certain interval, called Sweet Gradient of parameters can be good zero-cost proxy evaluating candidate models in zero shot NAS. The authors find a positive correlation between the depth and the proportion of sweet gradient parameters, and further propose a way to automatically determine the Sweet Gradient interval. The effectiveness of the proposed method is validated on four benchmarks with eight search spaces.\n\n# Strength And Weaknesses\n\nStrength:\n\n1. Utilizing the absolute gradient values within a certain interval as zero cost proxy evaluating candidate models in NAS is novel and interesting. Building a consistent zero-cost proxy is a very good contribution. \n2. The method is simple and easy to implement. \n3. The paper is clearly written. Experiments and ablation studies are well performed.\n\nWeakness:\n\n1. The proposed method, in its current form, is mostly heuristic. It is still not clear how the authors find that Sweet Gradient of parameters can be good proxies and why it works consistently well. More insightful discussion or theoretical analysis may be required to fully justify the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written, and the proposed method is somewhat novel.\n\n# Summary Of The Review\n\nThe paper is clearly written. The main concern is about insightful discussion and theoretical analysis.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDEEP GENERATIVE MODELING ON LIMITED DATA WITH REGULARIZATION BY NONTRANSFERABLE PRE-TRAINED MODELS\n\nYong Zhong∗12 Hongtao Liu∗12 Xiaodong Liu12 Fan Bao3 Weiran Shen†12 Chongxuan Li†12 1Gaoling School of AI, Renmin University of China, Beijing, China 2Beijing Key Lab of Big Data Management and Analysis Methods, Beijing, China 3Department of Computer Science Technology, Tsinghua University, Beijing, China {yongzhong, ht6, xiaodong.liu}@ruc.edu.cn, bf19@mails.tsinghua.edu.cn, {shenweiran, chongxuanli}@ruc.edu.cn\n\nABSTRACT\n\nDeep generative models (DGMs) are data-eager because learning a complex model on limited data suffers from a large variance and easily overfits. Inspired by the classical perspective of the bias-variance tradeoff, we propose regularized deep generative model (Reg-DGM), which leverages a nontransferable pre-trained model to reduce the variance of generative modeling with limited data. Formally, Reg-DGM optimizes a weighted sum of a certain divergence and the expectation of an energy function, where the divergence is between the data and the model distributions, and the energy function is defined by the pre-trained model w.r.t. the model distribution. We analyze a simple yet representative Gaussian-fitting case to demonstrate how the weighting hyperparameter trades off the bias and the variance. Theoretically, we characterize the existence and the uniqueness of the global minimum of Reg-DGM in a non-parametric setting and prove its convergence with neural networks trained by gradient-based methods. Empirically, with various pretrained feature extractors and a data-dependent energy function, Reg-DGM consistently improves the generation performance of strong DGMs with limited data and achieves competitive results to the state-of-the-art methods. Our implementation is available at https://github.com/ML-GSAI/Reg-ADA-APA.\n\n1\n\nINTRODUCTION\n\nDeep generative models (DGMs) (Kingma & Welling, 2013; Goodfellow et al., 2014; SohlDickstein et al., 2015; Van den Oord et al., 2016; Dinh et al., 2016; Hinton & Salakhutdinov, 2006) employ neural networks to capture the underlying distribution of high-dimensional data and find applications in various learning tasks (Kingma et al., 2014; Zhu et al., 2017; Razavi et al., 2019; Ramesh et al., 2021; 2022; Ho et al., 2022). Such models are often data-eager (Li et al., 2021; Wang et al., 2018) due to the presence of complex function classes. Recent work (Karras et al., 2020a) found that the classical variants of generative adversarial networks (GANs) (Goodfellow et al., 2014; Karras et al., 2020b) produce poor samples with limited data, which is shared by other DGMs in principle. Thus, improving the sample efficiency is a common challenge for DGMs.\n\nThe root cause of the problem is that learning a model in a complex class on limited data suffers from a large variance and easily overfits the training data (Mohri et al., 2018). To relieve the problem, previous work either employed sophisticated data augmentation strategies (Zhao et al., 2020a; Karras et al., 2020a; Jiang et al., 2021), or designed new losses for the discriminator in GANs (Cui et al., 2021; Yang et al., 2021), or transferred a pre-trained DGM (Wang et al., 2018; Noguchi & Harada, 2019; Mo et al., 2020). Although not pointed out in the literature to our knowledge, prior work can be understood as reducing the variance of the estimate implicitly (Mohri et al., 2018). In\n\n∗Equal contribution. †Correspondence to Weiran Shen and Chongxuan Li.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nthis perspective, we propose a complementary framework, named regularized deep generative model (Reg-DGM), which employs a pre-trained model as regularization to achieve a better bias-variance tradeoff when training a DGM with limited data.\n\nIn Sec. 2, we formulate the objective function of Reg-DGM as the sum of a certain divergence and a regularization term weighted by a hyperparameter. The divergence is between the data distribution and model distribution, and the regularization term can be understood as the negative expected loglikelihood of an energy-based model, whose energy function is defined by a pre-trained model, w.r.t. the model distribution. Intuitively, with an appropriate weighting hyperparameter, Reg-DGM balances between the data distribution and the pre-trained model to achieve a better bias-variance tradeoff with limited data, as validated by a simple yet prototypical Gaussian-fitting example.\n\nIn Sec. 3, we characterize the optimization behavior of Reg-DGM in both non-parametric and parametric settings under mild regularity conditions. On one hand, we prove the existence and the uniqueness of the global minimum of the regularized optimization problem with the Kullback–Leibler (KL) and Jensen–Shannon (JS) divergence in the non-parametric setting. On the other hand, we prove that, parameterized by a standard neural network architecture, Reg-DGM converges with a high probability to a global (or local) optimum trained by gradient-based methods.\n\nIn Sec. 4, we specify the components in Reg-DGM. We employ strong variants of GANs (Karras et al., 2020b;a; Jiang et al., 2021) as the base DGM for broader interests. We consider a nontransferable setting where the pre-trained model does not necessarily have to be a generative model. Indeed, we employ several feature extractors, which are trained for non-generative tasks such as image classification, representation learning, and face recognition. Notably, such models cannot be directly used in the fine-tuning approaches (Wang et al., 2018; Mo et al., 2020). With a comprehensive ablation study, we define our default energy function as the expected mean squared error between the features of the generated samples and the training data. Such an energy not only fits our theoretical analyses, but also results in consistent and significant improvements over baselines in all settings.\n\nIn Sec. 5, we present experiments on several benchmarks, including the FFHQ (Karras et al., 2019), LSUN CAT (Yu et al., 2015), and CIFAR-10 (Krizhevsky et al., 2009) datasets. We compare RegDGM with a large family of methods, including the base DGMs (Karras et al., 2020b;a; Jiang et al., 2021), the transfer-based approaches (Wang et al., 2018; Mo et al., 2020), the augmentation-based methods (Zhao et al., 2020a) and others (Cui et al., 2021; Yang et al., 2021). With a classifier pretrained on ImageNet (Deng et al., 2009), or an image encoder pre-trained on the CLIP dataset (Radford et al., 2021), or a face recognizer pre-trained on VGGFace2 (Cao et al., 2018), Reg-DGM consistently improves strong DGMs under commonly used performance measures with limited data and achieves competitive results to the state-of-the-art methods. Our results demonstrate that RegDGM can achieve a good bias-variance tradeoff in practice, which supports our motivation.\n\nIn Sec. 6, we present the related work. In Sec. 7, we conclude the paper and discuss limitations.\n\n2 METHOD\n\nThe goal of generative modeling is to learn a model distribution pg (implicitly or explicitly) from a training set S = {xi}m i=1 of size m on a sample space X . The elements in S are assumed to be drawn i.i.d. according to an unknown data distribution pd ∈ PX , where PX is the set of all valid distributions over X . A general formulation for learning generative models is minimizing a certain statistical divergence D(·||·) between the two distributions as follows:\n\nD(pd||pg),\n\nmin pg∈H\n\n(1)\n\nwhere H ⊂ PX is the hypothesis class, for instance, a set of distributions defined by neural networks in a deep generative model (DGM). Notably, the divergence in Eq. (1) is estimated by the Monte Carlo method over the training set S and its solution has a small bias if not zero. However, learning a DGM with limited data is challenging because solving the problem in Eq. (1) with a small sample size m essentially suffers from a large variance and probably overfits (Mohri et al., 2018).\n\nInspired by the classical perspective of the bias-variance tradeoff, we propose to leverage an external model f pre-trained on a related and large dataset (e.g., ImageNet) as a data-dependent regularization to reduce the variance of training a DGM on limited data (e.g., CIFAR-10), which is\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n(a) Varying β = λ/(λ + 1)\n\n(b) Varying m\n\n(c) Varying ˆμPRE − μ∗\n\nFigure 1: MSE in the Gaussian-fitting example to validate Proposition 2.2. (a) The effect of λ given a fixed m and ˆμPRE − μ∗, where the red circle indicates β = max\n\n(cid:110) σ2−m(ˆμP RE −μ∗)2\n\nand the\n\nσ2+m(ˆμP RE −μ∗)2 , 0\n\n(cid:111)\n\nblack circle indicates β = min{ effect of the bias of the pre-trained model. In both (b) and (c), the hyperparameter λ is optimal.\n\nσ2+m(ˆμP RE −μ∗)2 , 1}. (b) The effect of the sample size m. (c) The\n\n2σ2\n\ncomplementary to prior work (see more details in Sec. 6). In particular, given a pre-trained f , we first introduce an energy function (LeCun et al., 2006) Ef : X → R that satisfies mild regularity conditions (see Assumption A.1) and then define a probabilistic energy-based model (EBM) pf as pf (x) ∝ exp(−Ef (x)). We can treat pf as a special estimate of pd if f is pre-trained on a dataset closely related to pd. In this perspective, pf has a potentially large bias yet a variance of zero. To trade off the bias and variance, we simply optimize a weighted sum of the statistical divergence as in Eq. (1) and the expected log-likelihood of the EBM as follows:\n\nmin pg∈H\n\nD(pd||pg) − λEx∼pg [log pf (x)] ⇔ min\n\npg∈H\n\nD(pd||pg) + λEx∼pg [Ef (x)],\n\n(2)\n\nwhere λ > 0 is the weighting hyperparameter balancing the two terms. The first one encourages pg to fit the data as in the original DGM and the second one encourages pg to produce samples with a high likelihood evaluated by the EBM pf . Naturally, a properly selected λ can hopefully achieve a better bias-variance tradeoff. We refer to our approach as regularized deep generative model (Reg-DGM) when H consists of distributions defined by neural networks.\n\nIn the following, we analyze a simple yet prototypical Gaussian-fitting example to demonstrate how a regularization term defined by a pre-trained model can relieve the bias-variance dilemma in generative modeling. Such an example is helpful to illustrate the motivation of Reg-DGM precisely and provide valuable insights on its practical performance. In the same spirit, representative prior work (Arjovsky et al., 2017; Mescheder et al., 2018) in the literature of DGMs has investigated similar examples with several parameters and closed-form solutions.\n\n2.1 A PROTOTYPICAL GAUSSIAN-FITTING EXAMPLE\n\nExample 2.1 (Gaussian-fitting example). The data distribution is a (univariate) Gaussian pd(x) = N (x|μ∗, σ2), where σ2 is known and μ∗ is the parameter to be estimated. A training sample S = i=1 is drawn i.i.d. according to pd(x). The hypothesis class for pg is H = {N (x|μ, σ2) | μ ∈ {xi}m R}. The regularization term in Eq. (2) is Ef (x) := − log N (ˆμPRE, σ2), i.e., pf (x) = N (x|ˆμPRE, σ2). Note that we let ˆμPRE ̸= μ∗ and their gap |ˆμPRE − μ∗| can be large in general.\n\nFor simplicity, we consider the classical maximum likelihood estimation (MLE) (i.e., using the KL divergence in Eq. (1) as the performance measure), and its solution for Example 2.1 is given m σ2(cid:1). by the sample mean (Bishop & Nasrabadi, 2006): ˆμMLE = 1 The pre-trained model ˆμPRE is another meaningful baseline for our approach. Clearly, it has a bias of ˆμPRE − μ∗ and a zero variance. Formally, the solution of our approach based on MLE is ˆμREG = 1 . We compare all estimators under the mean squared error (MSE)1, which is a common measure in statistics and machine learning. Formally, the MSE of an estimator ˆθ w.r.t. an unknown parameter θ is defined as:\n\ni=1 xi, ˆμMLE ∼ N (cid:0)μ∗, 1\n\n1+λ ˆμPRE, ˆμREG ∼ N\n\n1+λ ˆμMLE + λ\n\n1+λ μ∗ + λ\n\nσ2 m(1+λ)2\n\n1+λ ˆμPRE,\n\n(cid:16) 1\n\n(cid:80)m\n\n(cid:17)\n\nm\n\n1The expected risk coincides with the MSE in the example. See Appendix A.2.2 for generalization analyses.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n(a) Data distribution\n\n(b) Energy function\n\n(c) Varying λ (KLD)\n\n(d) Varying λ (JSD)\n\nFigure 2: Illustration of the optimization analyses. (a) The density of the data distribution. (b) The energy function defined by the pre-trained model. (c) The global minimum of the problem in Eq. (2) with the KL divergence (KLD) and different values of λ. (d) The global minimum of the problem in Eq. (2) with the JS divergence (JSD) and different values of λ. See more details in Appendix B.1.\n\nMSE[ˆθ] := E[(θ − ˆθ)2], which can be decomposed as the sum of the variance of the estimator and the squared bias of the estimator (Bishop & Nasrabadi, 2006). As summarized in Proposition 2.2, our method achieves a better bias-variance tradeoff than the baselines if λ is in an appropriate range. Proposition 2.2. Let β = λ\n\nλ+1 be the normalized weight of the regularization term.\n\nIn the\n\n(cid:110) σ2−m(ˆμPRE−μ∗)2\n\nσ2+m(ˆμPRE−μ∗)2 , 0\n\n(cid:111)\n\n< β < min\n\n(cid:110)\n\n2σ2\n\nσ2+m(ˆμPRE−μ∗)2 , 1\n\n(cid:111)2,\n\nGaussian-fitting example 2.1, if max\n\nthen the following inequalities holds:\n\nMSE[ˆμREG] < min{MSE[ˆμMLE], MSE[ˆμPRE]}.\n\n(3)\n\nDue to space limit, the proof is deferred to Appendix A.1.1. We plot the MSE curves for all estimators w.r.t. the value of λ, m and | ˆμPRE − μ∗| in Fig. 1 for a clearer illustration. Fig. 1 (a) directly validates the results of Proposition 2.2. Fig. 1 (b) and (c) show that with the optimal λ, the regularization gains more w.r.t. MLE as the sample size m decreases and | ˆμPRE − μ∗| increases. See more details of the plot in Appendix B.1.\n\nSince the generalization analysis in deep learning is still largely open (Belkin, 2021; Bartlett et al., 2021), it is difficult to generalize our Proposition 2.2 to the cases with deep models due to lack of appropriate tools. However, the intuition behind the Gaussian example also holds in deep learning. Namely, training a model on limited data suffers from a large variance (overfitting) and using a pretrained model suffers from a large bias (underfitting). Thus, Reg-DGM with a proper hyperparameter can balance between them and potentially achieve a better performance. Further, according to Fig. 1 (c), Reg-DGM benefits if the EBM defined by the pre-trained model is close to the target distribution, which inspires a data-dependent energy function presented in Sec. 4.\n\n3 CONVERGENCE ANALYSES\n\n3.1 ANALYSES IN THE NON-PARAMETRIC SETTING\n\nWe assume that our hypothesis class contains all valid distributions (i.e. H = PX ), and the data distribution pd is accessible. Although the setting is impractical, such analyses characterize the existence and uniqueness of the global minimum in an ideal case and have been widely considered in deep generative models (Goodfellow et al., 2014; Arjovsky et al., 2017). Further, it is insightful to see how the regularization affects the solution of Reg-DGM in the ideal case.\n\nBuilt upon the classical recipe of the calculus of variations and properties of the KL divergence in the topology of weak convergence, we establish our theory on the existence and uniqueness of the global minimum of Eq. (2) with the KL and JS divergence 3. The results are formally characterized in Theorem 3.1 and Theorem 3.2 respectively. We refer the readers to Appendix A.2.1 for the\n\n(cid:110) σ2−m( ˆμPRE−μ∗)2\n\n(cid:111)\n\n(cid:110)\n\n2σ2\n\n2Note that max 3We consider two divergences that are employed in the two representative DGMs: variational auto-encoders\n\nσ2+m( ˆμPRE−μ∗)2 , 0\n\nσ2+m( ˆμPRE−μ∗)2 , 1\n\nalways holds.\n\n< min\n\n(cid:111)\n\n(VAE) (Kingma & Welling, 2013) and generative adversarial networks (GANs) (Goodfellow et al., 2014).\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nX e−Ef (x)dx < ∞, which are common and mild.\n\nproofs. To establish our theory, we assume that (1) X is a nonempty compact set; (2) Ef : X → R is continuous and bounded; (3) (cid:82) Theorem 3.1. Under mild regularity conditions in Assumption A.1, for any λ > 0, there exists a unique global minimum of the problem in Eq. (2) with the KL divergence. Furthermore, the global α∗+λEf (x) , where α∗ ∈ R. minimum is in the form of p∗ Theorem 3.2. Under mild regularity conditions in Assumption A.1, for any λ > 0, there exists a unique global minimum of the problem in Eq. (2) with the JS divergence. Furthermore, the global minimum is in the form of p∗\n\ng(x) = pd(x)\n\n, where α∗ ∈ R.\n\ng(x) =\n\npd(x) eα∗+λEf (x)−1\n\nAs shown in Theorem 3.1 and Theorem 3.2, the global minimum is in the form of a reweighted data distribution and the weights are negatively correlated to the energy function defined by the pre-trained model. Qualitatively, the global minimum assigns high density for a sample x if it has high density under the data distribution (i.e., pd(x)) and low value of the energy function (i.e., Ef (x)). Notably, the weights in Theorem 3.1 and Theorem 3.2 are different because of the different divergences. In particular, the effect of the pre-trained model is enlarged by the exponential term in Theorem 3.2 using JS divergence (JSD). Fig. 2 (c) and (d) show that with the same value of λ, the weighting coefficients of JSD are distributed in a larger range than KL divergence (KLD). Naturally, in both theorems, as λ → 0, the denominator of p∗ g(x) → pd(x), which recovers the solution of pure divergence minimization in Eq. (1). Therefore, Reg-DGM is consistent if the weighting parameter λ is a function of m, and limm→∞ λ(m) → 0.\n\ng(x) tends to a constant and p∗\n\n3.2 ANALYSES IN THE PARAMETRIC SETTINGS\n\nAlthough it provides theoretical insights of Reg-DGM, the non-parametric setting is far from practice. In fact, in our experiments, the hypothesis class is parameterized by neural networks and the training data is finite. In such a case, Reg-DGM can be formulated as a non-convex optimization problem, which is solved by gradient-based methods. Therefore, we also analyze the convergence of Reg-DGM trained by (stochastic) gradient descent in the presence of neural networks upon the general convergence framework (Allen-Zhu et al., 2019).\n\nIn particular, as summarized in Theorem 3.3, we show that Reg-DGM with a standard neural network architecture converges with a high probability under mild smoothness assumptions. The assumptions, result and proof are formally presented in Appendix A.2.2.\n\nTheorem 3.3 (Convergence of Reg-DGM (informal)). Under standard and verifiable smoothness assumptions, with a high probability, Reg-DGM with a sufficiently wide ReLU CNN converges to a global optimum of Eq. (2) trained by GD and converges to a local minimum trained by SGD.\n\n4\n\nIMPLEMENTATION\n\nIn this section, we discuss the base DGM, the pre-trained model and the energy function in practice.\n\n4.1 BASE MODEL\n\nAlthough Reg-DGM applies to variational auto-encoders (VAE) (Kingma & Welling, 2013) and many other DGMs, we focus on GANs (Goodfellow et al., 2014), which are most representative and popular in the scenarios with limited data (Karras et al., 2020a; Mo et al., 2020; Cui et al., 2021). Formally, GANs optimize an estimate of the JS divergence via a minimax formulation as follows:\n\nmin G\n\nmax D\n\nEx∼pd(x)[log D(x)] + Ex∼pg(x)[log(1 − D(x))],\n\n(4)\n\nwhere G is a generator that defines pg(x) and D is a discriminator that estimates the JS divergence by discriminating samples. Both G and D are parameterized by neural networks and Eq. (4) is estimated by the Monte Carlo method over mini-batches sampled from the training set.\n\nFor a broader interest, we adopt three strong GAN variants, StyleGAN2 (Karras et al., 2020b), adaptive discriminator augmentation (ADA) (Karras et al., 2020a), and adaptive pseudo augmentation (APA) (Jiang et al., 2021) as the base DGMs. Please refer to Appendix B.2 for more details.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n4.2 PRE-TRAINED MODEL\n\nAs mentioned in Sec. 2, different from the transfer-based methods (Wang et al., 2018; Mo et al., 2020), Reg-DGM applies to a nontransferable setting where the pre-trained model does not necessarily have the same architecture or the same formulation as pg or does not even have to be a generative model, enjoying the flexibility of choosing the pre-trained models. In our implementation, the pre-trained model is a feature extractor f : X → Rd, which is trained for other tasks (e.g., classification or contrastive representation learning) instead of generation. We choose such models because they are easily available and achieve excellent performance in supervised learning. In particular, we investigate three prototypical pre-trained models: a ResNet (He et al., 2015) trained in a supervised manner on ImageNet, a CLIP image encoder (Radford et al., 2021) trained in a self-supervised manner on a large-scale image-text dataset, and a FaceNet (Schroff et al., 2015) trained on a face recognition dataset. Please refer to Appendix B.2 for more details.\n\nNote that such models are nontransferable in the fine-tuning manner (Hinton et al., 2006). Nevertheless, with such models and a data-dependent energy function presented later, Reg-DGM is still competitive to the transfer-based approaches (Wang et al., 2018; Mo et al., 2020) as shown in Tab. 1.\n\n4.3 ENERGY FUNCTION\n\nAccording to the results in Fig. 1 (c) and our intuition, we should define Ef such that pf is as close to pd as possible. In most of the cases, f is pre-trained on a dataset with richer semantics than pd. Therefore, it is necessary to involve training data (sampled from pd) in the energy function to reducing the distance between pf and pd. As presented above, we specify f as a feature extractor and it is natural to match the features of samples from pg and pd as the energy function.\n\nFormally, the energy function is defined by the expected mean squared error between the features of a generated sample and a training sample as follows:\n\nEf (x) := Ex′∼pd\n\n||f (x) − f (x′)||2\n\n2\n\n(cid:21)\n\n.\n\n(cid:20) 1 d\n\n(5)\n\nNotably, our implementation with the data-dependent energy function in Eq. (5) is a valid instance of the general Reg-DGM framework as formulated in Eq. (2). Furthermore, the convergence results in both the non-parametric and parametric settings (see Sec. 3) hold in this case. The expectation is estimated by the Monte Carlo method of a single sample for efficiency by default and increasing the number of samples will not affect the performance significantly (see results in Appendix C.4).\n\nWe emphasize that our main contribution is not designing a specific energy function but the general framework of Reg-DGM. Many alternative energy functions can be employed in Reg-DGM. Indeed, we perform a systematical ablation study of the energy functions in Sec. 5.3 and find that Eq. (5) is the best among them considering the qualitative and quantitative results together. Moreover, we evaluate the effectiveness of Reg-DGM implemented by Eq. (5), with strong base DGMs (Karras et al., 2020b;a; Jiang et al., 2021), different datasets, different backbones of f and different pretraining datasets in the experiments. We observe a consistent and significant improvement over SOTA baselines across various settings. Based on such a comprehensive empirical study, we believe that our implementation of Reg-DGM based on Eq. (5) would be effective in new settings.\n\n5 EXPERIMENTS\n\nFor a fair comparison to a large family of prior work, we evaluate Reg-DGM on several widely adopted benchmarks with limited data (Karras et al., 2020a), including the FFHQ (Karras et al., 2019), 100-shot Obama (Zhao et al., 2020a), LSUN CAT (Yu et al., 2015) and CIFAR-10 (Krizhevsky et al., 2009) datasets, and the data processing and metric calculating are the same as those of ADA (Karras et al., 2020a). We present the main results and analyses in the section and refer the readers to Appendix B.2 for details and Appendix C for additional results. Throughout the section, we refer to our approach as the name of the base DGM with the prefix “Reg-”. For instance, “Reg-ADA” denotes our approach with ADA (Karras et al., 2020a) as the base DGM.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Median FID ↓ on FFHQ and LSUN CAT and mean FID ↓ on CIFAR-10. † and ‡ indicate the results are taken from the references and Karras et al. (2020a) respectively. Otherwise, the results are reproduced by us upon the official implementation (Karras et al., 2020a; Jiang et al., 2021).\n\nMethod\n\nFFHQ\n\nLSUN CAT\n\nCIFAR-10\n\nTransfer (Wang et al., 2018) Freeze-D (Mo et al., 2020) DA† (Zhao et al., 2020a) InsGen† (Yang et al., 2021) GenCo† (Cui et al., 2021) DA + GenCo† (Cui et al., 2021) ADA + bCR‡ (Zhao et al., 2020b)\n\nRLC ADA + RLC\n\n† (Tseng et al., 2021)\n\n† (Tseng et al., 2021)\n\nAPA† (Jiang et al., 2021)\n\nStyleGAN2 (Karras et al., 2020b) Reg-StyleGAN2 (ours)\n\nADA (Karras et al., 2020a) Reg-ADA (ours)\n\nADA + APA (Jiang et al., 2021) Reg-ADA-APA (ours)\n\n1k\n\n21.42 19.77 25.66 19.58 65.31\n\n22.61 63.16 21.7 45.19\n\n103.66 75.99\n\n22.26 20.05\n\n19.71 17.88\n\n10.58 23.83\n\n13.25\n\n52.71 37.77\n\n12.64 11.95\n\n8.84 8.02\n\n5k\n\n12.34 12.69 10.45\n\n1k\n\n5k\n\n50k\n\n42.26\n\n16.11\n\n8.49\n\n27.96\n\n140.08\n\n40.79\n\n38.82\n\n16.80\n\n8.83 ± 0.04 6.57 ± 0.01\n\n8.31 ± 0.05 2.47 ± 0.01\n\n186.55 107.02\n\n115.16 63.10\n\n41.81 36.17\n\n24.09 21.88\n\n16.76 15.91\n\n11.79 11.27\n\n7.16 ± 0.12 6.56 ± 0.14\n\n3.07 ± 0.08 2.95 ± 0.05\n\n2.64 ± 0.08 2.58 ± 0.04\n\n(a) 100-shot Obama (FID 39.53)\n\n(b) FFHQ-5k (FID 11.69)\n\nFigure 3: Samples from the Reg-ADA, truncated (ψ = 0.7) as in prior work (Karras et al., 2020a).\n\n5.1 BENCHMARK RESULTS WITH LIMITED DATA\n\nWe employ StyleGAN2 (Karras et al., 2020b), ADA (Karras et al., 2020a) and APA (Jiang et al., 2021) as the base DGMs and a ResNet-18 (He et al., 2015) classifier trained on ImageNet (Deng et al., 2009) as the pre-trained model by default. The associated energy function is defined in Eq. (5).\n\nQuantitatively, we compare Reg-DGM with a large family of existing methods, including the base DGMs, the transfer-based approaches, the augmentation-based methods and many others. Following the direct and strong competitor (Karras et al., 2020a), we report the median Fr ́echet inception distance (FID) (Heusel et al., 2017) on FFHQ and LSUN CAT, and the mean FID on CIFAR-10 out of 3 runs for a fair comparison in Tab. 1. For completeness, we also report the mean FID with the standard deviation on FFHQ and LSUN CAT in Appendix C.1.\n\nAs shown in Tab. 1, Reg-StyleGAN2, Reg-ADA and Reg-ADA-APA consistently outperform the corresponding base DGM in five settings, demonstrating that Reg-DGM can achieve a good biasvariance tradeoff in practice. Besides, the superior performance of Reg-ADA over ADA (and RegADA-APA over ADA-APA) shows that our contribution is orthogonal to the augmentation-based approaches. Notably, the improvement of Reg-DGM over the base DGM is larger when the sample size m is smaller. This is as expected because the relative gain of Reg-DGM over the base DGM\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Median FID ↓ and the corresponding KID×103 ↓ using a pre-trained CLIP or FaceNet.\n\nMethod\n\nFFHQ-5k\n\nLSUN CAT-5k\n\nFFHQ-5k\n\nCLIP\n\nFaceNet\n\nStyleGAN2 (Karras et al., 2020b) Reg-StyleGAN2(ours)\n\nADA (Karras et al., 2020a) Reg-ADA(ours)\n\nADA+APA (Jiang et al., 2021) Reg-ADA-APA(ours)\n\nFID\n\nKID\n\nFID\n\nKID\n\nFID\n\nKID\n\n52.71 40.98\n\n12.64 11.09\n\n8.84 8.18\n\n39.52 27.56\n\n115.16 42.04\n\n100.57 26.21\n\n5.17 3.91\n\n2.76 2.26\n\n16.76 14.15\n\n11.79 10.47\n\n8.13 6.72\n\n4.86 4.68\n\n52.71 38.80\n\n12.64 11.37\n\n8.84 8.21\n\n39.52 23.38\n\n5.17 4.01\n\n2.76 2.37\n\nincreases as m decreases given a fixed pre-trained model and the optimal λ (see Fig. 1 (b)). Further, we evaluate the base DGMs and Reg-DGMs under the kernel inception distance (KID) (Bi ́nkowski et al., 2018) metric in Appendix C.4. The conclusion remains the same. As suggested by Proposition 2.2, the value of the weighting hyperparameter λ is crucial and there is an appropriate range of λ such that Reg-DGM is better than the base DGM. We empirically validate the argument on FFHQ-5k with StyleGAN2 as the base model in Appendix C.5.\n\nWe mention that the methods based on fine-tuning (Wang et al., 2018; Mo et al., 2020) in Tab. 1 employ a GAN pre-trained on CelebA-HQ (Karras et al., 2017), which is also a face dataset of the same resolution as FFHQ. In comparison, our approach is built upon a classifier pertained on ImageNet. As highlighted in (Wang et al., 2018), the density of the pre-training dataset is more important than the diversity. Nevertheless, Reg-DGM is competitive with these strong baselines while enjoying the flexibility of choosing the pre-trained model and dataset. Further, we directly adopt the same pre-trained model across all datasets including CIFAR-10, where it takes additional efforts to get a suitable pre-trained generative model to fine-tune. Based on the results, it is safe to emphasize the complementary role of Reg-DGM to the approaches based on fine-tuning.\n\nQualitatively, we show the samples generated from Reg-DGM on FFHQ-5k and 100-shot Obama in Fig. 3. It can be seen that with the regularization, our approach can produce faces of a normal shape with limited data. We present the results from the base DGMs and more samples in other settings in Appendix C.2. For a comprehensive comparison on the visual quality of samples, we perform a human evaluation by the Amazon Mechanical Turk (AMT) as in prior work (Choi et al., 2020). According with the FID results, Reg-StyleGAN2 trained on FFHQ-5k against the baseline StyleGAN2 is chosen in 63.5% of the 3,000 image quality comparison tasks. See details in Appendix C.4.\n\n5.2 ABLATION OF PRE-TRAINED MODELS AND PRE-TRAINING DATASETS\n\nFor simplicity, the main results of Reg-DGM presented in Sec. 5.1 are based on a ResNet-18 model pre-trained on ImageNet. Although its architecture is significantly different from the Inceptionv3 (Szegedy et al., 2016) used in FID and KID calculation, it is worth performing an ablation on the pre-trained models to eliminate the potential bias caused by the pre-training dataset.\n\nIn particular, we test Reg-DGM with the image encoder of the CLIP model (Radford et al., 2021) (an architecture very similar to ResNet-50), which is pre-trained on large-scale noisy text-image pairs instead of ImageNet, and with the face recognizer Inception-ResNet-v1 (Szegedy et al., 2017) of FaceNet (Schroff et al., 2015), which is pre-trained on the large-scale face dataset VGGFace2 (Cao et al., 2018). See details in Appendix B.2. The median FID and corresponding KID results are shown in Tab. 2. Without heavily tuning the hyperparameter λ, Reg-DGM shows consistent improvements over the two baselines under both FID and KID metrics, and achieves comparable results to those presented in Table 1. The results with the CLIP model and the FaceNet model demonstrate that Reg-DGM works well with various backbones and pre-training datasets.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n5.3 ABLATION OF ENERGY FUNCTIONS\n\nAs emphasized in Sec. 4, our main contribution is the general framework instead of a specific energy function. Nevertheless, we still investigate two alternative regularization terms for completeness.\n\nWe first consider the famous entropy-minimization regularization (Grandvalet & Bengio, 2004), which is data-independent. Intuitively, the regularization forces pg that produces samples with clear semantics. We find that the entropy regularization achieves similar FID results of 50.87 on FFHQ-5k to the baseline 52.71, showing the importance of the data dependency in the energy function. We then investigate another data-dependent regularization term, i.e., feature matching (Salimans et al., 2016), which matches the averaged features between the model and data distributions. We find that feature matching can achieve FID 32.65 on FFHQ-5k greatly reducing the FID of StyleGAN2 while it cannot improve the visual quality of the samples. Please refer to Appendix C.4 for details of both energy functions. Therefore, Eq. (5) is the best among them considering the qualitative and quantitative results together and we believe it can be transferred to new settings based on our results in Sec. 5.1 and Sec. 5.2.\n\n6 RELATED WORK\n\nFine-tuning approaches. A milestone of deep learning is that a deep generative model fine-tuned for classification outperforms the classical SVM on recognizing the hand-writing digits (Hinton et al., 2006). Since then, the idea of fine-tuning has a significant impact (Devlin et al., 2018; He et al., 2020) including generative models with limited data (Wang et al., 2018; Mo et al., 2020; Wang et al., 2020; Li et al., 2020; Ojha et al., 2021). However, an inherent restriction of fine-tuning is that the pre-trained model and target model should partially share a common structure. Thus, it may take additional efforts to find a suitable pre-trained model to fine-tune. In comparison, Reg-DGM provides an alternative way to make it possible to exploit a pre-trained classifier to help generative modeling. Notably, the latter is often thought of as much harder than the former.\n\nOther generative adversarial networks with limited data. To relieve the overfitting problem of the discriminator, DA (Zhao et al., 2020a), ADA (Karras et al., 2020a) and APA (Jiang et al., 2021) design sophisticated data augmentation strategies. GenCo (Cui et al., 2021) designs a cotraining framework that introduces multiple complimentary discriminators. InsGen (Yang et al., 2021) improves the data efficiency of GANs via an instance discrimination loss (Wu et al., 2018). We believe that Reg-DGM is orthogonal to these methods based on our results in Tab. 1.\n\nRegularization in probabilistic models. Extensive regularization approaches have been developed in traditional Bayesian inference (Zhu et al., 2014) and probabilistic modeling (Chang et al., 2007; Liang et al., 2009; Ganchev et al., 2010). Among them, posterior regularization (PR) (Ganchev et al., 2010) encodes the human knowledge about the task as linear constraints of the latent representations in generative models for better inference performance. Such methods have been extended to deep generative models (Hu et al., 2018; Du et al., 2018; Shu et al., 2018; Xu et al., 2019) for a similar reason. Technically, PR-based methods regularize the latent space via handcrafted or jointly trained constraints. In comparison, our approach regularizes the data space via a pre-trained model. Besides, PR-based methods are suitable for structured prediction tasks instead of generative modeling with limited data, which is the main focus of this paper.\n\n7 CONCLUSIONS\n\nIn this paper, we propose regularized deep generative model (Reg-DGM), which leverages a pretrained model for regularization to reduce the variance of DGMs with limited data. Theoretically, we analyze the convergence properties of Reg-DGM. Empirically, with various pre-trained feature extractors and a data-dependent energy function, Reg-DGM consistently improves the generation performance of strong DGMs and achieves competitive results to the state-of-the-art methods. An interesting future work is to analyze the generalization behaviour of Reg-DGM in general and inspire new energy functions. Currently, the generalization analysis of deep learning is still largely open (Zhang et al., 2021; Bartlett et al., 2021; Belkin, 2021), and there lacks appropriate tools to formalize our intuition on the bias-variance tradeoff in general.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENT\n\nWe thank Guoqiang Wu for helpful discussions about the generalization analysis. This work was supported by NSF of China (Nos. 62076145, 62106273); Beijing Outstanding Young Scientist Program (No. BJJWZYJH012019100020098); Major Innovation & Planning Interdisciplinary Platform for the “Double-First Class” Initiative, Renmin University of China; the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China (22XNKJ13, 22XNKJ16). Part of the computing resources supporting this work, totaled 720 A100 GPU hours, were provided by High-Flyer AI. (Hangzhou High-Flyer AI Fundamental Research Co., Ltd.). C. Li was also sponsored by Beijing Nova Program.\n\nETHICS STATEMENT\n\nThis work presents a framework to train deep generative models on small data. By improving the data efficiency, it can potentially benefit real-world applications like medicine analysis and automatic drive. However, this work can have negative consequences in the form of “DeepFakes”, as existing GANs. It is worth noting that this work may exacerbate such issues by improving the data efficiency of GANs. How to detect “DeepFakes” is an active research area in machine learning, which aims to relieve the problem.\n\nREPRODUCIBILITY STATEMENT\n\nWe submit the source code in the supplementary material and have released it. Datasets used in experiments are open and publicly available, and experimental details are provided in the Appendix B. In addition, the complete proof of the propositions and theorems is contained in the Appendix A.\n\nREFERENCES\n\nGanesh Ajjanagadde, Anuran Makur, Jason Klusowski, Sheng Xu, et al. Lecture notes on information theory. Lab. Inf. Decis. Syst., Massachusetts Inst. Technol., Cambridge, MA, USA, Tech. Rep, 2017.\n\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019.\n\nMartin Arjovsky, Soumith Chintala, and L ́eon Bottou. Wasserstein generative adversarial networks.\n\nIn International conference on machine learning, pp. 214–223. PMLR, 2017.\n\nPeter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint.\n\nActa numerica, 30:87–201, 2021.\n\nMikhail Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the\n\nprism of interpolation. Acta Numerica, 30:203–248, 2021.\n\nPatrick Billingsley. Convergence of probability measures. John Wiley & Sons, 2013.\n\nMikołaj Bi ́nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd\n\ngans. arXiv preprint arXiv:1801.01401, 2018.\n\nChristopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, vol-\n\nume 4. Springer, 2006.\n\nQiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. Vggface2: A dataset for recognising faces across pose and age. In 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018), pp. 67–74. IEEE, 2018.\n\nMing-Wei Chang, Lev Ratinov, and Dan Roth. Guiding semi-supervision with constraint-driven learning. In Proceedings of the 45th annual meeting of the association of computational linguistics, pp. 280–287, 2007.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8188–8197, 2020.\n\nKaiwen Cui, Jiaxing Huang, Zhipeng Luo, Gongjie Zhang, Fangneng Zhan, and Shijian Lu. Genco: Generative co-training for generative adversarial networks with limited data. arXiv preprint arXiv:2110.01254, 2021.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv\n\npreprint arXiv:1605.08803, 2016.\n\nChao Du, Kun Xu, Chongxuan Li, Jun Zhu, and Bo Zhang. Learning implicit generative models by\n\nteaching explicit ones. arXiv preprint arXiv:1807.03870, 2018.\n\nKuzman Ganchev, Joao Grac ̧a, Jennifer Gillenwater, and Ben Taskar. Posterior regularization for structured latent variable models. The Journal of Machine Learning Research, 11:2001–2049, 2010.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672–2680, 2014.\n\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances\n\nin neural information processing systems, 17, 2004.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. arXiv preprint arXiv:1512.03385, 2015.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on\n\nunsupervised visual representation learning. computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\nGeoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural\n\nnetworks. science, 313(5786):504–507, 2006.\n\nGeoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief\n\nnets. Neural computation, 18(7):1527–1554, 2006.\n\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\n\nFleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022.\n\nZhiting Hu, Zichao Yang, Russ R Salakhutdinov, LIANHUI Qin, Xiaodan Liang, Haoye Dong, and Eric P Xing. Deep generative models with learnable knowledge constraints. Advances in Neural Information Processing Systems, 31, 2018.\n\nLiming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy. Deceive d: Adaptive pseudo augmentation for gan training with limited data. Advances in Neural Information Processing Systems, 34, 2021.\n\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-\n\nproved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.\n\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401–4410, 2019.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in Neural Information Processing Systems, 33:12104–12114, 2020a.\n\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8110–8119, 2020b.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nDurk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in neural information processing systems, pp. 3581–3589, 2014.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nYann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based\n\nlearning. Predicting structured data, 1(0), 2006.\n\nChongxuan Li, Kun Xu, Jun Zhu, Jiashuo Liu, and Bo Zhang. Triple generative adversarial net-\n\nworks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n\nYijun Li, Richard Zhang, Jingwan Lu, and Eli Shechtman. Few-shot image generation with elastic\n\nweight consolidation. arXiv preprint arXiv:2012.02780, 2020.\n\nZiqiang Li, Xintian Wu, Beihao Xia, Jing Zhang, Chaoyue Wang, and Bin Li. A comprehensive\n\nsurvey on data-efficient gans in image generation. arXiv preprint arXiv:2204.08329, 2022.\n\nPercy Liang, Michael I Jordan, and Dan Klein. Learning from measurements in exponential families. In Proceedings of the 26th annual international conference on machine learning, pp. 641–648, 2009.\n\nLars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do In International conference on machine learning, pp. 3481–3490. PMLR,\n\nactually converge? 2018.\n\nSangwoo Mo, Minsu Cho, and Jinwoo Shin. Freeze the discriminator: a simple baseline for fine-\n\ntuning gans. arXiv preprint arXiv:2002.10964, 2020.\n\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.\n\nMIT press, 2018.\n\nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.\n\nIn Icml, 2010.\n\nAtsuhiro Noguchi and Tatsuya Harada. Image generation from small datasets via batch statistics adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2750–2758, 2019.\n\nUtkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A Efros, Yong Jae Lee, Eli Shechtman, and Richard In Proceedings of the\n\nZhang. Few-shot image generation via cross-domain correspondence. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10743–10752, 2021.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821–8831. PMLR, 2021.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nAli Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with\n\nvq-vae-2. Advances in neural information processing systems, 32, 2019.\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in neural information processing systems, pp. 2234–2242, 2016.\n\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 815–823, 2015.\n\nRui Shu, Hung H Bui, Shengjia Zhao, Mykel J Kochenderfer, and Stefano Ermon. Amortized\n\ninference regularization. Advances in Neural Information Processing Systems, 31, 2018.\n\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256–2265. PMLR, 2015.\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. RethinkIn Proceedings of the IEEE conference on\n\ning the inception architecture for computer vision. computer vision and pattern recognition, pp. 2818–2826, 2016.\n\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.\n\ninception-resnet and the impact of residual connections on learning. ference on artificial intelligence, 2017.\n\nInception-v4, In Thirty-first AAAI con-\n\nHung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularizing generaIn Proceedings of the IEEE/CVF Conference on\n\ntive adversarial networks under limited data. Computer Vision and Pattern Recognition, pp. 7921–7931, 2021.\n\nAaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016.\n\nOnno Van Gaans. Probability measures on metric spaces. Lecture notes, 2003.\n\nYaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu. Transferring gans: generating images from limited data. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 218–234, 2018.\n\nYaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, and Joost van de Weijer. Minegan: effective knowledge transfer from gans to target domains with few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9332–9341, 2020.\n\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via nonparametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3733–3742, 2018.\n\nTaufik Xu, Chongxuan Li, Jun Zhu, and Bo Zhang. Multi-objects generation with amortized struc-\n\ntural regularization. Advances in Neural Information Processing Systems, 32, 2019.\n\nCeyuan Yang, Yujun Shen, Yinghao Xu, and Bolei Zhou. Data-efficient instance generation from\n\ninstance discrimination. Advances in Neural Information Processing Systems, 34, 2021.\n\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107– 115, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nShengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient gan training. Advances in Neural Information Processing Systems, 33:7559–7570, 2020a.\n\nZhengli Zhao, Sameer Singh, Honglak Lee, Zizhao Zhang, Augustus Odena, and Han Zhang. Im-\n\nproved consistency regularization for gans. arXiv preprint arXiv:2002.04724, 2020b.\n\nJun Zhu, Ning Chen, and Eric P Xing. Bayesian inference with posterior regularization and applications to infinite latent svms. The Journal of Machine Learning Research, 15(1):1799–1847, 2014.\n\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 2223–2232, 2017.\n\nA PROOFS\n\nA.1 THE GAUSSIAN-FITTING EXAMPLE\n\nWe first derive the solution of Reg-DGM in the main text. In the Gaussian fitting example, the regularized optimization problem can be written as\n\nˆμREG = arg min\n\nμ\n\n−\n\n1 m\n\nm (cid:88)\n\ni=1\n\nlog N (μ, σ2) + λEx∼N (μ,σ2) [− log pf (x)]\n\n= arg min\n\nμ\n\n= arg min\n\nμ\n\n1 m\n\n1 m\n\nm (cid:88)\n\ni=1 m\n(cid:88)\n\ni=1\n\n(μ − xi)2 2σ2\n\n+ λ\n\n(cid:18) (μ − ˆμPRE)2 2σ2\n\n+\n\n1 2\n\nlog(2πσ2) +\n\n(cid:19)\n\n1 2\n\n(μ − xi)2 2σ2\n\n+ λ\n\n(μ − ˆμPRE)2 2σ2\n\n,\n\n(6)\n\n(7)\n\n(8)\n\nwhere the first equality holds by the definition and properties of Gaussian (Bishop & Nasrabadi, 2006) and the second equality holds by omitting a constant irrelevant to the optimization. It is easy to solve the above quadratic programming problem analytically:\n\nˆμREG =\n\n1 m(1 + λ)\n\nm (cid:88)\n\ni=1\n\nxi +\n\nλ 1 + λ\n\nˆμPRE =\n\n1 1 + λ\n\nˆμMLE +\n\nλ 1 + λ\n\nˆμPRE,\n\n(9)\n\nwhere ˆμMLE ∼ N variable and thus is also Gaussian distributed as follows:\n\nm\n\n(cid:16)\n\nμ∗, σ2\n\n(cid:17)\n\n. ˆμREG is obtained by an affine transformation of a Gaussian random\n\nˆμREG ∼ N\n\n(cid:18) 1\n\n1 + λ\n\nμ∗ +\n\nλ 1 + λ\n\nˆμPRE,\n\nσ2 m(1 + λ)2\n\n(cid:19)\n\n.\n\nA.1.1 PROOF OF PROPOSITION 2.2\n\nProof. In the Gaussian-fitting example, we have\n\nˆμMLE ∼ N\n\n(cid:18)\n\nμ∗,\n\n(cid:19)\n\n,\n\nσ2 m\n\nand\n\nˆμREG ∼ N\n\n(cid:18) 1\n\n1 + λ\n\nμ∗ +\n\nλ 1 + λ\n\nˆμPRE,\n\nσ2 m(1 + λ)2\n\n(cid:19)\n\n.\n\nAccording to the bias-variance decomposition of the MSE, we have\n\nMSE[ˆμMLE] =\n\nσ2 m\n\n,\n\n14\n\n(10)\n\n(11)\n\n(12)\n\n(13)\n\nPublished as a conference paper at ICLR 2023\n\nand\n\nMSE[ˆμREG] =\n\nλ2\n\n(1 + λ)2 (ˆμPRE − μ∗)2 +\n\nσ2 m(1 + λ)2 .\n\n(14)\n\nLet β = λ MSE[ˆμREG] as\n\n1+λ ∈ (0, 1) be the normalized weight of the regularization term. Then, we can rewrite\n\nMSE[ˆμREG] = β2(ˆμPRE − μ∗)2 + (1 − β)2 σ2\n\nm\n\n.\n\nTo satisfy MSE[ˆμREG] < MSE[ˆμMLE], we have\n\nβ2(ˆμPRE − μ∗)2 + (1 − β)2 σ2\n\nm\n\n<\n\nσ2 m\n\n⇒ β <\n\n2σ2 σ2 + m(ˆμPRE − μ∗)2 .\n\n(15)\n\n(16)\n\nThe pre-trained model ˆμPRE is another meaningful baseline, which has a bias of ˆμPRE − μ∗ and a zero variance. Its MSE is given by\n\nMSE[ˆμPRE] = (ˆμPRE − μ∗)2.\n\nTo satisfy MSE[ˆμREG] < MSE[ˆμPRE], we have\n\nβ2(ˆμPRE − μ∗)2 + (1 − β)2 σ2\n\nm\n\n< (ˆμPRE − μ∗)2 ⇒ β >\n\nσ2 − m(ˆμPRE − μ∗)2 σ2 + m(ˆμPRE − μ∗)2 ,\n\nwhich completes the proof.\n\n(17)\n\n(18)\n\nWe now computes the optimal MSE[ˆμREG]. problem in Eq. (15) achieves its minimum at β∗ = σ2(ˆμPRE−μ∗)2\n\nm(ˆμPRE−μ∗)2 . The minimum value is\n\nλ∗ =\n\nσ2\n\nIt is easy to see that the quadratic programming σ2+m(ˆμPRE−μ∗)2 with the corresponding\n\nσ2\n\nσ2+m(ˆμPRE−μ∗)2 = MSE[ˆμMLE]MSE[ˆμPRE]\n\nMSE[ˆμMLE]+MSE[ˆμPRE] .\n\nA.1.2 COMPARISON UNDER THE EXPECTED RISK\n\nWe also evaluate all estimators in terms of the expected risk, which is a commonly used measure in statistical learning theory. In a density estimation task for pd, the expected risk of a hypothesis ˆμ, which depends on the training sample S, is R(ˆμ) := Ex∼pd [− log p(x; ˆμ)]. We can show that the expectation of the expected risk w.r.t. S coincides with the corresponding MSE in the Gaussianfitting example and directly obtain the following Corollary A.0.1 from Proposition 2.2.\n\nCorollary A.0.1. In the Gaussian-fitting example 2.1, if λ satisfies the same condition as in Proposition 2.2, then the following inequality holds:\n\nES [R(ˆμREG)] < min{ES [R(ˆμMLE)], ES [R(ˆμPRE)]}.\n\nProof. In the Gaussian-fitting example, the expected risk for a hypothesis ˆμ is\n\nR(ˆμ) = ES∼pm\n\nd\n\n= ES∼pm\n\nd\n\nEx∼N (μ∗,σ2) (cid:20) (ˆμ − μ∗)2 2σ2 1\n2\n\n1\n\n=\n\n2σ2 MSE[ˆμ] +\n\nlog(2πσ2) +\n\n1 2\n\n,\n\n(cid:2)− log N (ˆμ, σ2)(cid:3)\n\n+\n\n1 2\n\nlog(2πσ2) +\n\n(cid:21)\n\n1 2\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\nwhich completes the proof together with Proposition 2.2.\n\nA.2 OPTIMIZATION ANALYSES\n\nA.2.1 PROOF OF THEOREM 3.1 AND THEOREM 3.2\n\nOur results rely on the following regularity conditions.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nAssumption A.1.\n\n1. X is a nonempty compact set.\n\n2. Ef : X → R is continuous and bounded.\n\n3. (cid:82)\n\nX e−Ef (x)dx < ∞.\n\nThe regularity conditions in Assumption A.1 are mild in the sense:\n\n1. The sample space X is often a subset of a n-dimensional Euclidean space. Then, X is compact if and only if X is bounded and closed, which holds for extensive datasets in various types including images, videos, audios, and texts.\n\n2. In this paper, Ef is defined by compositing a neural network and a continuous real-valued function. Then Ef is continuous and bounded on X if the neural network has bounded weights and uses continuous activation functions including ReLU (Nair & Hinton, 2010), Tanh and Sigmoid. X e−Ef (x)dx < ∞ holds following the choice of the sample space and energy function,\n\n3. (cid:82)\n\nWe first prove Theorem 3.1 as follows.\n\nProof. Ignoring some constant irrelevant to the optimization, we rewrite the optimization problem of our approach with the KL divergence as follows:\n\n(cid:90)\n\nX\n\nmin pg\n\n(− ln(pg(x))pd(x)+λEf (x)pg(x))dx,\n\nsubject to\n\n(cid:90)\n\nX\n\npg(x)dx = 1,\n\n(23)\n\n∀x ∈ X , pg(x) ≥ 0.\n\nFor clarity, we denote the functional in problem Eq. (23) to be optimized as\n\nJ (pg) :=\n\n(cid:90)\n\nX\n\n(− ln(pg(x))pd(x)+λEf (x)pg(x))dx.\n\nAccording to Assumption 3.1 that X is a nonempty compact set, the feasible area characterized by the constraints (i.e., PX ) is compact in the topology of weak convergence by the Prokhorov’s Theorem (See Corollary 6.8 in (Van Gaans, 2003)). By Theorem 3.6 in (Ajjanagadde et al., 2017), KL divergence is lower semi-continuous in the topology of weak convergence. According to Assumption 3.2 that Ef is continuous and bounded on X , the regularization term is continuous in the topology of weak convergence. Therefore, by the extreme value theorem, the global minimum of J (pg) exists in the feasible area.\n\nNote that the optimization problem Eq. (23) is convex. To obtain a necessary condition for the global minima, we get the Lagrangian with the equality constraint:\n\nL(pg) :=\n\n(cid:90)\n\nX\n\n(− ln(pg(x))pd(x) + λEf (x)pg(x))dx + α(\n\n(cid:90)\n\npg(x)dx − 1),\n\n(24)\n\nX\n\nwhere α ∈ R. Note that for simplicity, we do not include the inequality constraint, which will be verified shortly. It is easy to check the constraint qualifications for problem Eq. (23). By the calculus of variations, a necessary condition for a global minimum of problem Eq. (23) is\n\nwhich implies that\n\nδL δpg\n\n= −\n\npd(x) pg(x)\n\n+ λEf (x) + α = 0,\n\np∗\n\ng(x) =\n\npd(x) α + λEf (x)\n\n.\n\n16\n\n(25)\n\n(26)\n\nPublished as a conference paper at ICLR 2023\n\nWe define A = {α ∈ R| on X , we have A ̸= ∅. We define a function φ : A → R as\n\npd(x)\n\nα+λEf (x) ≥ 0, ∀x ∈ X }. According to Assumption 3.1 that Ef is bounded\n\nφ(α) =\n\n(cid:90)\n\nX\n\npd(x) α + λEf (x)\n\ndx.\n\n(27)\n\nIt is easy to see φ(α) is monotonically decreasing and there is at most one α∗ ∈ A such that φ(α∗) = 1, which finishes the proof together with the existence of the global minimum.\n\nWe now prove Theorem 3.2 as follows.\n\nProof. The proof here shares the same spirit of Theorem 3.1. Ignoring some constant irrelevant to the optimization, we rewrite the optimization problem of our approach with the JS divergence as follows:\n\n(cid:90)\n\nX\n\nmin pg\n\n(− ln(pg(x) + pd(x))pd(x) − ln(pg(x) + pd(x))pg(x) + ln(pg(x))pg(x)+λEf (x)pg(x))dx,\n\nsubject to\n\n(cid:90)\n\nX\n\npg(x)dx = 1,\n\n∀x ∈ X , pg(x) ≥ 0.\n\n(28)\n\nFor clarity, we denote the functional in problem Eq. (23) to be optimized as\n\nJ (pg) :=\n\n(cid:90)\n\nX\n\n(− ln(pg(x) + pd(x))pd(x) − ln(pg(x) + pd(x))pg(x) + ln(pg(x))pg(x)+ (29)\n\nλEf (x)pg(x))dx.\n\n(30)\n\nSimilarly to the proof of Theorem 3.1, the global minimum of J (pg) exists in the feasible area due to Assumption A.1 and the fact that the JS divergence is lower semi-continuous in the topology of weak convergence.\n\nNotably, the optimization problem Eq. (28) is convex due to the convexity of the JS divergence (Billingsley, 2013). To obtain a necessary condition for the global minima, we get the Lagrangian with the equality constraint:\n\nL(pg) :=\n\n(cid:90)\n\nX\n\n(− ln(pg(x) + pd(x))pd(x) − ln(pg(x) + pd(x))pg(x) + ln(pg(x))pg(x)+\n\nλEf (x)pg(x))dx + α(\n\n(cid:90)\n\nX\n\npg(x)dx − 1),\n\n(31)\n\nwhere α ∈ R. Similarly to the proof of Theorem 3.1, a necessary condition for a global minimum of problem Eq. (23) is\n\nδL δpg\n\nwhich implies that\n\n= − ln(pg(x) + pd(x)) + ln(pg(x)) + λEf (x) + α = 0,\n\np∗\n\ng(x) =\n\npd(x) eα+λEf (x) − 1\n\n.\n\n(32)\n\n(33)\n\nSimilarly to the proof in Theorem 3.1, there is at most one α∗ ∈ R such that (cid:82) which finishes the proof together with the existence of the global minimum.\n\nX\n\npd(x) eα+λEf (x)−1\n\n= 1,\n\nA.2.2 CONVERGENCE WITH NEURAL NETWORKS TRAINED BY (STOCHASTIC) GRADIENT\n\nDESCENT\n\nWe establish the convergence of Reg-DGM with over-parameterized neural networks trained by (stochastic) gradient descent upon the general framework (Allen-Zhu et al., 2019).\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTheorem A.2. (General convergence guarantee (Allen-Zhu et al., 2019)) For an arbitrary Lipschitzsmooth loss function L, with probability at least 1 − e−Ω(log m), a ReLU convolutional neural network with width m and depth l trained by gradient descent with an appropriate learning rate satisfy the following.4\n\n• If L is non-convex, and σ-gradient dominant,\n\nthen GD finds ε-error minimizer in\n\n ̃O(poly(n, l, log 1\n\nε , 1\n\nσ )) iterations as long as m ≥ ̃Ω(poly(n, l, 1\n\nσ )).\n\n• If L is non-convex, then SGD finds a point with ||∇f || ≤ ε in ̃O(poly(m, l, log 1\n\nε )) itera-\n\ntions as long as m ≥ ̃Ω(poly(n, l, 1\n\nε )).\n\nWe assume the following standard smoothness conditions, which can be verified in practice with bounded data and weights. Assumption A.3. (Smoothness conditions)\n\n1. ∃b > 0 such that ∀θ ∈ Θ, ∀x ∈ X , p(θ; x) ≥ b.\n\n2. ∃L > 0 such that ∀x ∈ X , ∀θ ∈ Θ, ∀θ′ ∈ Θ, |p(θ; x) − p(θ′; x)| ≤ L||θ − θ′||. 3. ∃K > 0 such that ∀θ ∈ Θ, ∀θ′ ∈ Θ, (cid:82) |p(θ; x) − p(θ′; x)|dx ≤ K||θ − θ′||.\n\n4. supx∈X supy∈X ||f (x) − f (y)||2 ≤ B.\n\nWe consider the general density estimation setting where LMLE(θ; xi) := − log pθ(xi). Note that the first two conditions in Assumption A.3 directly imply that LMLE(·; x) is L b -Lipschitz. Formally, given a set of samples S = {xi}n\n\ni=1, Reg-DGM optimizes\n\nˆR[θ] :=\n\n1 n\n\nn (cid:88)\n\ni=1\n\nLMLE(θ; xi) + λEx∼pg [Ef (x)].\n\nIf Ef is independent from the training data x, then the overall loss function is also L b -Lipschitz and Theorem A.2 directly applies. Otherwise, we can show that the data-dependent regularization used in our experiments is also Lipschitz-smooth. By the linearity of expectation, we have\n\nˆθREG := arg min\n\nθ∈Θ\n\n= arg min θ∈Θ\n\n1 n\n\n1 n\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\ni=1\n\n[LMLE(θ; xi)] +\n\n(cid:20)\n\nLMLE(θ; xi) +\n\nEy∼pθ\n\n(cid:34)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:35)\n\n||f (y) − f (xi)||2\n\n2\n\nEy∼pθ ||f (y) − f (xi)||2\n\n2\n\n(cid:21)\n\n.\n\nλ d\n\nλ d\n\n(34)\n\n(35)\n\nWe define LREG(θ; xi) := λ\n\nd\n\nEy∼pθ ||f (y) − f (xi)||2\n\n2, which is Lipschitz-smooth:\n\n|LREG(θ; x) − LREG(θ′; x)| =\n\n≤\n\n≤\n\n≤\n\nλ d\nλ d\nλ d\n\n(sup y∈X λBK d\n\n||θ − θ′||.\n\n(cid:12) (cid:12)Ey∼pθ ||f (y) − f (x)||2 (cid:90)\n\n2 − Ey∼pθ′ ||f (y) − f (x)||2\n\n2\n\n(cid:12) (cid:12)\n\n|pθ(y) − pθ′(y)|||f (x) − f (y)||2dy\n\n||f (x) − f (y)||2)\n\n(cid:90)\n\n|pθ(y) − pθ′(y)|dy\n\nTherefore, Theorem A.2 applies to Reg-DGM with the data-dependent energy defined in Sec. 4 of the main text.\n\nB EXPERIMENTAL DETAILS\n\nOur implementation is built upon some publicly available code. Below, we include the links and please refer to the licenses therein.\n\n4See additional assumptions on regularity of the data in (Allen-Zhu et al., 2019).\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nB.1 TOY DATA\n\nIn the toy example for optimization analyses, the data follows a uniform distribution over [0, 1]. The energy function is defined as Ef (x) = 0.7x + 0.9. The optimal β∗ is estimated by numerical integration.\n\nIn the experiments for the Gaussian-fitting example, we set σ2 = 1, m = 150, and ˆμPRE − μ∗ = 0.1 by default.\n\nB.2 GANS WITH LIMITED DATA\n\nDatasets. In our experiments, we use FFHQ (Karras et al., 2019), which consists of 70, 000 human face images of resolution 256 × 256, LSUN CAT (Yu et al., 2015), which consists of 200, 000 cat images of resolution 256 × 256, and CIFAR-10 (Krizhevsky et al., 2009), which consists of 50, 000 natural images of resolution 32 × 32. Specifically, we randomly split training subsets of size 1k and 5k from full FFHQ and LSUN CAT in the same way as ADA (Karras et al., 2020a). For reproducibility, we directly use the random seed provided by the official implementation of ADA 5. In addition, we also experiment on the smaller dataset 100-shot Obama (Zhao et al., 2020a) with only 100 face images of 256 × 256 resolution. In all experiments, we do not use x-flips to amplify training data except for combining with APA (Jiang et al., 2021).\n\nMetrics. To quantitatively evaluate the experimental results, we choose the Fr ́echet inception distance (FID) (Heusel et al., 2017) and the kernel inception distance (KID) (Bi ́nkowski et al., 2018) as our metrics. We compute the FID and KID between 50, 000 generated images and all real images instead of training subsets (Heusel et al., 2017). Following ADA (Karras et al., 2020a), we report the medium FID and corresponding KID on FFHQ and LSUN CAT and the mean FID with standard deviation on CIFAR-10 out of 3 runs. We record the best FID during training in each run as in ADA (Karras et al., 2020a).\n\nBase DGM. In particular, the lighter-weight StyleGAN2 is the backbone for FFHQ and LSUN CAT, and the tuning StyleGAN2 is the backbone for CIFAR-10 following ADA (Karras et al., 2020a). Compared to the official StyleGAN2, the lighter-weight StyleGAN2 has the same performance and less computing cost on the FFHQ and LSUN CAT and the tuning StyleGAN2 is more suitable for CIFAR-10 (Karras et al., 2020a). Adaptive discriminator augmentation (ADA) (Karras et al., 2020a) is a representative way of data augmentation for GANs under limited data, and its combination with adaptive pseudo augmentation (APA) (Jiang et al., 2021) is the state-of-the-art method for few-shot image generation (Li et al., 2022). Our implementation is based on the official code of ADA 5 and APA 6.\n\nPre-trained model. We choose the ResNet-18 7 trained on the ImageNet dataset as the pre-trained model by default due to its excellent performance on ImageNet (Deng et al., 2009) and little computational overhead. We normalize both the real and fake images based on the mean and standard deviation of training data and then feed them into the classifier. We extract the features of the last fully connected layer in the pre-trained model, which is frozen during training of generative models. On CIFAR-10, we interpolate 8 both the fake and real images to a resolution of 256 × 256 after normalization.\n\nOther alternative pre-trained models are the image encoder ResNet-50 of the CLIP model (Radford et al., 2021), which is pre-trained on large-scale noisy text-image pairs instead of ImageNet, and the face recognizer Inception-ResNet-v1 (Szegedy et al., 2017) of FaceNet (Schroff et al., 2015), which is pre-trained on the large-scale face dataset VGGFace2 (Cao et al., 2018). Notably, we replace the last attention pooling layer in the image encoder of CLIP with the global average pooling layer, and we directly pass the image to the face recognizer without operating the face detector in FaceNet\n\n5https://github.com/NVlabs/stylegan2-ada-pytorch 6https://github.com/EndlessSora/DeceiveD 7https://pytorch.org/vision/stable/models.html 8https://pytorch.org/docs/stable/generated/torch.nn.functional.\n\ninterpolate.html\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Hyperparameters in the experiments of GANs. Reg-DGM shares the same hyperparameters as the base DGM if not specified. All models converge with the corresponding training length.\n\nParameter\n\nFFHQ-1k\n\nFFHQ-5k\n\nLSUN CAT-1k\n\nLSUN CAT-5k\n\nCIFAR-10\n\nBase DGM\n\nλ Number of GPUs Training length Minibatch size\n\nStyleGAN2 (Karras et al., 2020b) 4\n4 5M 64\n\nStyleGAN2 (Karras et al., 2020b) 1\n4 5M 64\n\nStyleGAN2 (Karras et al., 2020b) 4\n8 5M 64\n\nStyleGAN2 (Karras et al., 2020b) 1\n8 5M 64\n\nStyleGAN2 (Karras et al., 2020b) 1 × 10−5 8\n25M 64\n\nBase DGM\n\nλ Number of GPUs Training length Minibatch size\n\nADA (Karras et al., 2020a) 0.01 8\n16M 64\n\nADA (Karras et al., 2020a) 0.001 8\n16M 64\n\nADA (Karras et al., 2020a) 0.01 8\n16M 64\n\nADA (Karras et al., 2020a) 0.0005 8\n16M 64\n\nADA (Karras et al., 2020a) 5 × 10−6 8\n60M 64\n\nBase DGM\n\nλ Number of GPUs Training length Minibatch size\n\nADA+APA (Jiang et al., 2021) 0.01 8\n25M 64\n\nADA+APA (Jiang et al., 2021) 0.01 8\n25M 64\n\nADA+APA (Jiang et al., 2021) 0.005 8\n25M 64\n\nADA+APA (Jiang et al., 2021) 0.001 8\n25M 64\n\nADA+APA (Jiang et al., 2021) 0.1 8\n100M 64\n\nTable 4: The hyperparameter λ with pre-trained CLIP and FaceNet.\n\nBase DGM\n\nCLIP\n\nFaceNet\n\nFFHQ-5k LSUN CAT-5k\n\nFFHQ-5k\n\nStyleGAN2 (Karras et al., 2020b) ADA (Karras et al., 2020a) ADA+APA (Jiang et al., 2021)\n\n50 1\n0.5\n\n50 2\n1\n\n4 0.05 0.001\n\nto extract cropped faces. For pre-trained CLIP9 and FaceNet10, we also employ their last layers as feature extractors.\n\nHyperparameters. Some parameters are shown in Tab. 3. The weight parameter λ controls the strength of our regularization term. We choose the weighting hyperparameter λ by performing grid search over [50, 20, 10, 5, 4, 2, 1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005] for FFHQ and LSUN CAT, and [1, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001, 0.000005] for CIFAR-10 according to FID following prior work (Karras et al., 2020a). Other parameters remain the same settings as ADA (Karras et al., 2020a) and APA (Jiang et al., 2021). For pre-trained CLIP and FaceNet, the adopted parameter λ is shown in Tab. 4. In addition, we simply set λ as 0.1 for Reg-ADA trained on 100-shot Obama.\n\nComputing amount. A single experiment can be completed on 8 NVIDIA 2080Ti GPUs. It takes 1 day 6 hours 19 minutes to run our method with ADA on FFHQ or LSUN CAT and 2 days 17 hours 10 minutes on CIFAR-10 at a time.\n\nC ADDITIONAL RESULTS\n\nC.1 STANDARD DEVIATION ON FFHQ AND LSUN CAT\n\nAs shown in Tab. 5, we also provide the mean FID and standard deviation on FFHQ and LSUN CAT. Reg-DGM can reduce the mean FID significantly (compared to the measurement variance) and achieve a similar if not smaller standard deviation.\n\n9https://github.com/openai/CLIP 10https://github.com/timesler/facenet-pytorch\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: The mean FID ↓ and standard deviation on FFHQ and LSUN CAT which is a supplement to reported medium FID.\n\nMethod\n\nFFHQ\n\nLSUN CAT\n\n1k\n\n5k\n\n1k\n\n5k\n\nStyleGAN2 (Karras et al., 2020b) 102.62 ± 5.67 53.37 ± 1.92\n\n189.57 ± 8.13 110.83 ± 6.85 77.80 ± 3.65 38.14 ± 0.97 112.15 ± 8.48 64.11 ± 2.51\n\nReg-StyleGAN2 (ours)\n\nADA (Karras et al., 2020a) Reg-ADA (ours)\n\n16.77 ± 0.74 22.10 ± 0.50 20.16 ± 0.22 11.88 ± 0.13 36.85 ± 1.09 15.85 ± 0.10\n\n12.72 ± 0.13\n\n41.59 ± 1.71\n\n(a) StyleGAN2 (FID 51.41)\n\n(b) Reg-StyleGAN2 (FID 37.17)\n\n(c) ADA (FID 12.62)\n\n(d) Reg-ADA (FID 11.69)\n\nFigure 4: Samples generated for FFHQ-5k, truncated (ψ = 0.7).\n\nC.2 MORE SAMPLES OF GANS\n\nFig. 4 and Fig. 5 respectively show the samples randomly generated by models with best FID trained on FFHQ-5k and CIFAR-10, using slight truncation as in ADA (Karras et al., 2020a). Reg-DGM produces samples of better or comparable quality to the corresponding base DGM.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n(a) StyleGAN2 (FID 7.02)\n\n(b) Reg-StyleGAN2 (FID 6.38)\n\n(c) ADA (FID 2.96)\n\n(d) Reg-ADA (FID 2.88)\n\nFigure 5: Samples generated for CIFAR-10, truncated (ψ = 0.7).\n\nC.3 LEARNING CURVES\n\nWe show the learning curves of GANs on FFHQ, LSUN CAT and CIFAR-10 in Fig. 6, Fig. 7 and Fig. 8 respectively. Reg-DGM consistently improves both baselines in all settings and the improvements increase as the number of the training data decreases. Moreover, the curves of Reg-DGM generally have a smaller fluctuation, which is consistent with our theory that the regularization reduces the variance of the baselines. One exception is Fig. 8 (a), which shows that Reg-DGM is more unstable than the baseline, which is caused by a bad random initialization. We mention that the instability of Reg-ADA in Fig. 6 (b) is due to that of ADA.\n\nC.4 ABLATION OF CLASSIFIER AND RESULTS UNDER MORE EVALUATION METRICS\n\nIn this section, to better understand the influence of classifiers on our method, we try to use pretrained classifiers with different regularization terms, layers, and backbones. We retain the same experimental setting as in Tab. 3.\n\nRegularization form. We first investigate the feature matching objective (Salimans et al., 2016) as an alternative regularization term. Formally, it computes the square of the l2-norm between expected features of real and fake samples from one layer of a feature extractor f , which can be represented\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n(a) StyleGAN2 and Reg-StyleGAN2\n\n(b) ADA and Reg-ADA\n\nFigure 6: Learning Curves on FFHQ.\n\n(a) StyleGAN2 and Reg-StyleGAN2\n\n(b) ADA and Reg-ADA\n\nFigure 7: Learning Curves on LSUN CAT.\n\nas follows:\n\n||Ex′∼pd [f (x′)] − Ex∼pg [f (x)] ||2 2. Note that the feature matching objective cannot be rewritten as the expectation over pg and thus cannot be understood as an energy function. As before, we adopt the last fully-connected layer of ResNet-18, and the results of StyleGAN2 regularized by Eq. (36) are shown in Tab. 6. Feature matching can greatly reduce FID of StyleGAN2 while it cannot improve the visual quality of the samples, as shown in Fig. 9.\n\n(36)\n\nThen, we evaluate the entropy-minimization regularization (Grandvalet & Bengio, 2004) as follows: −H(softmax(f (x))),\n\n(37)\n\nwhere f (x) outputs the logits for the prediction distribution. As shown in Tab. 7, the entropy regularization achieves similar FID results to the baseline within a small search space of λ, showing the importance of the data dependency in the energy function.\n\nLayers in f . To explore the different layers of a pre-trained model, we retrain GANs separately using the first convolution layer of ResNet-18 and the last layers of four modules in ResNet-18. As shown in Tab. 8, our method with features of diverse single layers can all improve the baseline StyleGAN2, and the last layer of ResNet-18 is most beneficial for our regularization strategy.\n\nBackbone of f . We employ ResNet-50 and ResNet-101 as feature extractors to explore the effect of different backbones on Reg-DGM. Tab. 9 shows the results on FFHQ-5k. Even using the default λ without tuning, Reg-DGM with ResNet-50 and ResNet-101 can achieve a similar FID to that of our default setting and outperform the baseline. We believe that Reg-DGM with ResNet-50 and ResNet-101 can get better results if we finely search the hyperparameter λ.\n\nMonte Carlo estimate of f . We evaluate Reg-StyleGAN2 with 8, 16, 32, and 64 (the default batch size) samples to estimate the energy function in Eq. (5) of the main text, as shown in Tab. 10. We do not observe a significant improvement by increasing the number of samples. For instance, when λ = 1, the estimate with 64 samples achieves an FID of 38.10, which is similar to 37.77 of the single sample estimate. Intuitively, the features of faces are likely concentrated in a small area of the feature space of f , which is discriminative to other classes of natural images like cars, making the variance negligible to the training process of the generative model.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\n(a) StyleGAN2 and Reg-StyleGAN2\n\n(b) ADA and Reg-ADA\n\nFigure 8: Learning Curves on CIFAR-10.\n\nFigure 9: Samples generated for FFHQ-5k using feature matching (FID 32.65), truncated (ψ = 0.7).\n\nKID metric. For a comprehensive comparison of Reg-DGM and baselines, we also evaluate the models under KID in Tab. 11. The conclusion remains the same as FID. Namely, Reg-DGM consistently improves baselines in all settings.\n\nHuman evaluation. We compare the quality of generated images from the human perspective by Amazon Mechanical Turk (AMT) as in prior work (Choi et al., 2020). In particular, we randomly generate 1, 000 pairs of images by StyleGAN2 and Reg-StyleGAN2 trained on FFHQ-5k using the truncation trick (ψ = 0.7). For each image pair, three unique workers in AMT choose the image with the more realistic and high-quality face image. Finally, 3, 000 selection tasks are completed by 34 workers within 14 hours. As consistent with the comparison of image quality between Fig. 4 (a) and Fig. 4 (b), Reg-StyleGAN2 is chosen in 63.5% of the 3, 000 tasks, and other statistical information is shown in Tab. 12.\n\nResults on more training sets with different size. To explore the influence of our method on training data with different sizes, we test the Reg-StyleGAN2 with the pre-trained ResNet-18 on different subsets of FFHQ. The results are shown in Tab. 13 and the hyperparameter λ is simply fixed as 1 for new data settings 100, 2k, 7k, 10k, and 15k. Without heavily tuning the hyperparameter λ, Reg-StyleGAN2 shows consistent improvements over the baseline under the FID metric. There seems to be a trend that our method improves more on smaller datasets, which agrees with our Gaussian case illustrated by the Fig. 1 (b).\n\nC.5 SENSITIVITY ANALYSIS OF THE WEIGHTING HYPERPARAMETER\n\nWe empirically analyze the sensitivity of the weighting hyperparameter λ on FFHQ-5k with StyleGAN2 as the base model in Tab. 14. It can be seen that λ affects the performance significantly. Notably, although it is nearly impossible to get the optimal λ via grid search, there is a range of λ (e.g. from 0.01 to 1 in Tab. 14) such that Reg-DGM outperforms the base DGM, which agrees with\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: The median FID ↓ on FFHQ and LSUN CAT with feature matching as the regularization. λ is simply set as the same values in Tab. 3.\n\nMethod\n\nFFHQ\n\nLSUN CAT\n\n1k\n\n5k\n\n1k\n\n5k\n\nStyleGAN2 (Karras et al., 2020b) Reg-StyleGAN2 (ours)\n\n103.66 59.96\n\n52.71 32.65\n\n186.55 66.46\n\n115.16 47.56\n\nTable 7: Median FID on FFHQ-5k for the entropy-minimization regularization.\n\nλ\n\n(Baseline)\n\n1\n\n0.1\n\n0.01\n\n0.001\n\n0.0001\n\n0.00001\n\nFID ↓\n\n52.71\n\n145.55\n\n92.11\n\n56.41\n\n53.37\n\n50.87\n\n55.76\n\nour theoretical analyses. Besides, Reg-DGM is not too sensitive when the value of λ is around the optimum. For instance, the gap between the results of λ = 0.1 and λ = 1 in Tab. 14 is much smaller than their gain compared to the baseline. The performance of Reg-DGM deteriorates with a large λ in Tab. 14 as expected. Proposition 2.2 shows that Reg-DGM is preferable if and only if its value is in a proper interval. Intuitively, a very large value means that we almost ignore the training data, which should lead to inferior performance.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nTable 8: Results with different layers on FFHQ-5k. The “−1 layer” represents the last layer (i.e., our default setting). Note that layers are all indexed by the function named modules in Pytorch. λ is simply set as the same values from Tab. 3.\n\nLayer index\n\n(Baseline)\n\n1\n\n17\n\n33\n\n49\n\n65\n\n−1 (by default)\n\nFID ↓\n\n52.71\n\n46.39\n\n47.65\n\n51.61\n\n45.24\n\n46.01\n\n37.77\n\nTable 9: Results with different backbones on FFHQ-5k. λ is simply set as the same values from Tab. 3.\n\nBackbone\n\n(Baseline) ResNet-18 ResNet-50 ResNet-101\n\nFID ↓\n\n52.71\n\n37.77\n\n40.95\n\n42.63\n\nTable 10: FID ↓ on FFHQ-5k for different number of samples in MC.\n\nMC\n\n1 (by default)\n\n8\n\n16\n\n32\n\n64\n\nλ = 1 λ = 10\n\n37.77 53.09\n\n40.97 53.36\n\n40.78 58.58\n\n37.54 52.93\n\n38.10 48.21\n\nTable 11: KID×103 ↓ on FFHQ, LSUN-CAT, and CIFAR-10.\n\nMethod\n\nFFHQ\n\nLSUN CAT\n\nCIFAR-10\n\n1k\n\n5k\n\n1k\n\n5k\n\n50k\n\nStyleGAN2 (Karras et al., 2020b) Reg-StyleGAN2 (ours)\n\n98.06 47.91\n\n39.52 23.06\n\nADA (Karras et al., 2020a) Reg-ADA (ours)\n\n9.77 9.38\n\n5.17 4.30\n\n161.95 83.71\n\n23.30 20.52\n\n100.57 42.68\n\n3.66 ± 0.07 2.89 ± 0.11\n\n8.13 6.54\n\n0.90 ± 0.12 0.83 ± 0.06\n\nTable 12: Other statistical information about human evaluation. Number-i indicates the number of image pairs that at least i workers think StyleGAN2 or Reg-StyleGAN2 generates more realistic face images.\n\nMethod\n\nNumber-1 Number-2 Number-3\n\nStyleGAN2 (Karras et al., 2020b) Reg-StyleGAN2 (ours)\n\n610 854\n\n339 661\n\n146 390\n\nTable 13: The FID on different subsets of FFHQ.\n\nMethod\n\n100\n\n1k\n\n2k\n\nFFHQ 5k\n\n7k\n\n10k\n\n15k\n\nStyleGAN2 Reg-StyleGAN2\n\n132.68 113.96\n\n103.66 75.99\n\n83.32 60.83\n\n52.71 37.77\n\n37.75 31.58\n\n34.20 25.72\n\n23.07 20.85\n\nTable 14: Sensitivity analysis of λ on FFHQ-5k with StyleGAN2 as the base DGM.\n\nValues of λ\n\nλ = 0 (base DGM)\n\nλ = 0.01\n\nλ = 0.1\n\nλ = 1\n\nλ = 10\n\nλ = 100\n\nFID ↓\n\n52.71\n\n47.49\n\n41.51\n\n37.77\n\n53.09\n\n178.53\n\n26",
    "reference": "# Summary Of The Paper\n\nThis work studies generative modeling under a limited data regime and proposes a regularized generative model by utilizing a pre-trained model from an external dataset. The paper presented some theoretical justification and analysis of the proposed approach from the bias-variance trade-off perspective. Experiments on image generation are conducted in comparison to several baseline methods, using GANs as the backbone generative model, showing that the proposed model can achieve better performance with the help of the proposed regularization approach.\n\n# Strength And Weaknesses\n\n- This paper is generally well-written and the overall structure is clear. The motivation is clearly stated.\n- The authors suggest looking at the problem and proposed model from the bias-variance trade-off perspective. However, the presented theoretical analysis does not seem to be very intuitive in explaining why and how this perspective would help. To put it in another way, bringing another pre-trained model from a larger-scale dataset into the training of a generative model under a limited data regime is not a very surprising idea in the first place, which makes the synthetic study and the theoretical analysis a bit obsolete.\n- Why does the author emphasize the role of an energy-based formulation of $f$ rather than a regularization term, which is pretty much a perceptual loss in previous works? What does the distribution induced by the EBM imply in the context of the proposed model and its training process?\n- The exact description of how the limited data is prepared is not very clear, as well as how the metrics are computed. For example, are the FID scores computed against the original whole training set or in the limited dataset?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nPlease see above comments.\n\n# Summary Of The Review\n\nThis work studies the generative models under a limited data regime which is interesting, some parts including theoretical analysis and evaluation can be improved to make the contributions clearer in terms of soundness and novelty.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCATASTROPHIC OVERFITTING IS A BUG BUT IT IS CAUSED BY FEATURES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nAdversarial training (AT) is the de facto method to build robust neural networks, but it is computationally expensive. To overcome this, fast single-step attacks can be used, but doing so is prone to catastrophic overfitting (CO). This is when networks gain non-trivial robustness during the first stages of AT, but then reach a breaking point where they become vulnerable in just a few iterations. Although some works have succeeded at preventing CO, the different mechanisms that lead to this failure mode are still poorly understood. In this work, we study the onset of CO in singlestep AT methods through controlled modifications of typical datasets of natural images. In particular, we show that CO can be induced when injecting the images with seemingly innocuous features that are very useful for non-robust classification but need to be combined with other features to obtain a robust classifier. This new perspective provides important insights into the mechanisms that lead to CO and improves our understanding of the general dynamics of adversarial training.\n\n1\n\nINTRODUCTION\n\nDeep neural networks are sensitive to imperceptible worst-case perturbations, also known as adversarial perturbations (Szegedy et al., 2014). As a consequence, training neural networks that are robust to such perturbations has been an active area of study in recent years (see Ortiz-Jiménez et al. (2021) for a review). In particular, a prominent line of research, referred to as adversarial training (AT), focuses on online data augmentation with adversarial samples during training. However, it is well known that finding these adversarial samples for deep neural networks is an NP-hard problem (Weng et al., 2018). In practice, this is usually overcome with various methods, referred to as adversarial attacks that find approximate solutions to this hard problem. The most popular attacks are based on projected gradient descent (PGD) (Madry et al., 2018) – a computationally expensive algorithm that requires multiple steps of forward and backward passes through the neural network to approximate the solution. This hinders its use in many large-scale applications motivating the use of alternative efficient single-step attacks (Goodfellow et al., 2015; Shafahi et al., 2019; Wong et al., 2020).\n\nThe use of the computationally efficient single-step attacks within AT, however, comes with concerns regarding its stability. While training, although there is an initial increase in robustness, the networks often reach a breaking point beyond which they lose all gained robustness in just a few iterations (Wong et al., 2020). This phenomenon is known as catastrophic overfitting (CO) (Wong et al., 2020; Andriushchenko & Flammarion, 2020). Nevertheless, given the clear computational advantage of using single-step attacks during AT, a significant body of work has been dedicated to finding ways to circumvent CO via regularization and data augmentation (Andriushchenko & Flammarion, 2020; Vivek & Babu, 2020; Kim et al., 2021; Park & Lee, 2021; Golgooni et al., 2021; de Jorge et al., 2022).\n\nDespite the recent methodological advances in this front, the root cause of CO remains poorly understood. Due to the inherent complexity of this problem, we argue that identifying the causal mechanisms behind CO cannot be done through observations alone and requires active interventions Ilyas et al. (2019). That is, we need to be able to synthetically induce CO in settings where it would not naturally happen otherwise.\n\nIn this work, we identify one such type of intervention that allows to perform abundant experiments to explain multiple aspects of CO. Specifically, the main contributions of our work are: (i) We show that CO can be induced by injecting features that, despite being strongly discriminative (i.e. useful for standard classification), are not sufficient for robust classification (see Fig. 1). (ii) Through\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Left: Depiction of our modified dataset that injects simple, discriminative features. Right: Clean and robust performance after FGSM-AT on injected datasets (cid:101) β. We vary the strength D\nof the synthetic features β (β = 0 corresponds to the original CIFAR-10) and the robustness budget 4/255, 6/255 ε (train and test). We observe that for ε our intervention can induce CO when the synthetic features have strength β slightly larger than ε while training on the original data does not suffer CO. Results are averaged over 3 seeds and shaded areas report minimum and maximum values.\n\n∈ {\n\n}\n\nextensive empirical analysis, we discover that CO is connected to the preference of the network to learn different features in a dataset. (iii) Building upon these insights, we describe and analyse a causal chain of events that can lead to CO. The main message of our paper is:\n\nCatastrophic overfitting is a learning shortcut used by the network to avoid learning complex robust features while achieving high accuracy using easy non-robust ones.\n\nOur findings improve our understanding of CO by focusing on how data influences AT. Moreover, they also provide insights in the dynamics of AT, in which the interaction between robust and non-robust features plays a key role.\n\nOutline In Section 2, we give an overview of the related work on CO. Section 3 presents our main observation: CO can be induced by manipulating the data distribution. In Section 4, we perform an in-depth analysis of this phenomenon to identify the causes of CO. Finally, in Section 5 we use our new perspective to provide new insights on the different ways we can prevent CO.\n\n2 PRELIMINARIES AND RELATED WORK\n\nLet fθ : Rd which maps input samples x is to find the network parameters θ\n\n→ Y\n\n∈\n\nRd to y\n\ndenote a neural network architecture parameterized by a set of weights θ\n\nRn . The objective of adversarial training (AT)\n\n∈\n\n=\n\n1, . . . , c\n\n∈ Y\n\nRn that optimize the following min-max problem:\n\n}\n\n{\n\n∈\n\nE(x,y)∼D\n\nmin θ\n\n(cid:20)\n\n(cid:21)\n\nmax ∥δ∥p≤ε L\n\n(fθ(x + δ), y)\n\n,\n\n(1)\n\n∈\n\nD\n\nis some data distribution, δ\n\nRd represents an adversarial perturbation, and p, ε characterize where the adversary. This is typically solved by alternately minimizing the outer objective and maximizing the inner one via first-order optimization procedures. The outer minimization is tackled via some standard optimizer, e.g., SGD, while the inner maximization problem is approximated with adversarial attacks like Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) and Projected Gradient Descent (PGD) (Madry et al., 2018). Single-step AT methods are built on top of FGSM. In particular, FGSM solves the linearised version of the inner maximization objective. When p =\n\n, this leads to:\n\nδFGSM = argmax\n\n∥δ∥∞≤ε L\n\n(fθ(x), y) + δ⊤\n\nx\n\n∇\n\nL\n\n(fθ(x), y) = ε sign (\n\n∞ (fθ(x), y)) .\n\nx\n\nL\n\n∇\n\n(2)\n\nNote that FGSM is very computationally efficient as it only requires a single forward-backward step. Unfortunately, FGSM-AT generally yields networks that are vulnerable to multi-step attacks such as PGD. In particular, Wong et al. (2020) observed that FGSM-AT presents a characteristic failure mode where the robustness of the model increases during the initial training epochs, but, at a certain point in training, the model loses all its robustness within the span of a few iterations. This is known as catastrophic overfitting (CO). They further observed that augmenting the FGSM attack with random noise seemed to mitigate CO. However, Andriushchenko & Flammarion (2020) showed that this method still leads to CO at larger ε. Therefore, they proposed combining FGSM-AT with\n\n2\n\n=2/255=4255=6255=8255Data interventionInjected features0510152025(/255)0%20%40%60%80%100%Robustaccuracy0510152025(/255)80%85%90%95%100%Cleanaccuracy///Under review as a conference paper at ICLR 2023\n\na smoothness regularizer (GradAlign) that encourages the cross-entropy loss to be locally linear. Although GradAlign succeeds in avoiding CO in all tested scenarios, optimizing it requires the computation of a second-order derivative, which adds a significant computational overhead.\n\nSeveral methods have been proposed that attempt to avoid CO while reducing the cost of AT. However these methods either only move CO to larger ε radii (Golgooni et al., 2021), are more computationally expensive (Shafahi et al., 2019; Li et al., 2020), or achieve sub-optimal robustness (Kang & MoosaviDezfooli, 2021; Kim et al., 2021). Recently, de Jorge et al. (2022) proposed N-FGSM that successfully avoids CO for large ε radii while incurring only a fraction of the computational cost of GradAlign.\n\nOn the more expensive side, multi-step attacks approximate the inner maximization in Eq. (1) with several gradient ascent steps (Kurakin et al., 2017; Madry et al., 2018; Zhang et al., 2019). Provided they use a sufficient number of steps, these methods do not suffer from CO and achieve better robustness. Nevertheless, using multi-step attacks in AT linearly increases the cost of training with the number of steps. Due to their superior performance and extensively validated robustness in the literature (Madry et al., 2018; Tramèr et al., 2018; Zhang et al., 2019; Rice et al., 2020) multi-step methods, such as PGD, are considered the reference in AT.\n\nAside from proposing methods to avoid CO, some works have also studied different aspects of the training dynamics when CO occurs. Wong et al. (2020) initially suggested that CO was a result of the networks overfitting to attacks limited to the corners of the l∞ ball. This conjecture was later dismissed by Andriushchenko & Flammarion (2020) who showed that AT with PGD attacks projected to the corners of the l∞ ball does not suffer from CO. Similarly, while Andriushchenko & Flammarion (2020) suggested that the reason that Wong et al. (2020) avoids CO is the reduced step size of the attack, de Jorge et al. (2022) showed they could prevent CO with noise augmentations while using a larger step-size. On the other hand, it has been consistently reported (Andriushchenko & Flammarion, 2020; Kim et al., 2021) that networks suffering from CO exhibit a highly non-linear loss landscape with respect to the input compared to their CO-free counterparts. As FGSM relies on the local linearity of the loss landscape, this sudden increase in non-linearity of the loss renders FGSM practically ineffective (Kim et al., 2021; Kang & Moosavi-Dezfooli, 2021). This provides a plausible explanation for why models are not fooled by FGSM after CO. However, none of these works have managed to identify what pushes the network to become strongly non-linear. In this work, we address this knowledge gap, and explore a plausible mechanism that can cause single-step AT to develop CO.\n\n3\n\nINDUCING CATASTROPHIC OVERFITTING\n\nOur starting point is a well known observation: while robust solutions can be attained with non-trivial training procedures, e.g., using AT, they are not the default consequence of standard training. That is, robust solutions are harder to learn and are avoided unless explicitly enforced (e.g. via adversarial training). On the other hand, we know robust classification requires leveraging alternative robust features that are not learned in the context of standard training (Ilyas et al., 2019; Sanyal et al., 2021), and, when CO happens, the robust accuracy plummets but the clean and FGSM accuracies do not drop; on the contrary, they keep increasing (Wong et al., 2020; Andriushchenko & Flammarion, 2020).\n\nBearing this in mind, we pose the following question: Can CO be a mechanism to avoid learning the complex robust features? If this is true, then the network could be using CO as a way to favour the learning of some very easy and non-robust features while ignoring the complex robust ones.\n\nDirectly testing this hypothesis, however, requires identifying and characterizing these two sets of features (robust vs non-robust) in a real dataset. This is a challenging task (as it is basically equivalent to solving the problem of adversarial robustness) that is beyond our current capabilities. For this reason, as is standard practice in the field (Arpit et al., 2017; Ilyas et al., 2019; Shah et al., 2020; Ortiz-Jimenez et al., 2020a), we take an alternative approach that relies on controlled modifications of the data. Conducting experiments on the manipulated data, we are able to make claims about CO and the structure of the data. In particular, we discover that if we inject a very simple discriminative feature on standard vision datasets then, under some conditions, we can induce CO at much lower values of ε than those for which it naturally happens without our intervention. This is a clear sign that the structure of the data plays a big role in the onset of CO.\n\nOur injected dataset Let (x, y) . Our intervention modifies the original data x by adding an injected label-dependent feature v(y). We\n\nbe an image-label pair sampled from a distribution\n\n∼ D\n\nD\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nβ where the label-dependent feature is scaled by β:\n\nconstruct a family of injected datasets (cid:101) D\n(cid:101)x = x + β v(y) with (x, y) v(y)\n\n((cid:101)x, y)\n\n(cid:101) D\n\nβ :\n\n∼\n\nMoreover, we design v(y) such that and are linearly separable with respect to y. Since CO has primarily been observed for l∞ perturbations, we mainly use p = but also present some results with p = 2 in Appendix D. We denote the set of all injected features as . The scale parameter β > 0 is fixed for all classes and controls the relative\n\np = 1 for all y\n\nV strength of the original and injected features, i.e., x and v(y), respectively (see Fig. 1 (left)).\n\n∈ Y}\n\nv(y)\n\n∈ Y\n\n∞\n\n=\n\n{\n\n∥\n\n∥\n\ny\n\n|\n\n,\n\n∼ D\n\n(3)\n\nV\n\nThis construction has some interesting properties. Since the injected features are linearly separable can separate (cid:101) and perfectly correlated with the labels, a linear classifier relying only on β for a D\nlarge enough β. Moreover, as β also controls the classification margin, if β ε this classifier is also robust. However, if x has some components in span( ), the interaction between x and v(y) may decrease the robustness (or even clean accuracy) of the classifier for small β. We rigorously illustrate such a behaviour for linear classifiers in Appendix A. In short, although v(y) is easy-to-learn in general (we will empirically see that in Section 4.1), the discriminative power (and robustness) of a classifier that solely relies on the injected features\n\nwill depend on β.\n\nV ≫\n\nV\n\nWith the aim to control the interactions between x and v(y), we design by selecting vectors from the low-frequency components of the 2D Discrete Cosine Transform (DCT) (Ahmed et al., 1974) as these have a large alignment with the space of natural images that we use for our experiments (e.g., CIFAR-10). To ensure the norm constraint, we binarize these vectors so that they only take values ∞ = 1. These two design in constraints also help to visually identify the alignment of adversarial perturbations δ with v(y) as these patterns are visually distinctive (see Fig. 2).\n\n1, ensuring a maximal per-pixel perturbation that satisfies\n\nv(y)\n\n±\n\nV\n\n∥\n\n∥\n\nInjection strength (β) drives CO We train a PreActResNet18 (He et al., 2016) on different intervened versions of CIFAR-10 (Krizhevsky & Hinton, 2009) using FGSM-AT for different robustness budgets ε and different scales β. Fig. 1 (right) shows a summary of these experiments both in terms of clean accuracy and robustness1. For the clean accuracy, Fig. 1 (right) shows two distinct regimes. First, when β < ε, the network achieves roughly the same accuracy by training and testing on (cid:101) β\nD as by training and testing on (corresponding to β = 0). This is expected as FGSM does not suffer from CO in this setting (see Fig. 1 (right)) and effectively ignores the added feature v(y). Meanwhile, when β > ε, the clean test accuracy is almost 100% (which is larger than state-of-the-art accuracy on CIFAR-10) indicating that the network heavily relies on the injected features. We provide further evidence for this in Appendix E\n\nFigure 2: Different samples of the injected dataset β, and FGSM perturbations before and after CO. (cid:101) D\nWhile prior to CO perturbations focus on the synthetic features, after CO they become noisy.\n\nD\n\nThe behaviour with respect to the robust accuracy is more interesting. For small ε (ε = 2/255) the robust accuracy shows the same trend as the clean accuracy, albeit with lower values. For large ε (ε = 8/255), the model incurs CO for most values of β. This is not surprising as CO has already been reported for this value of ε on the original CIFAR-10 dataset (Wong et al., 2020). However, the interesting setting is for intermediate values of ε (ε ). For these settings, Fig. 1 (right) distinguishes between three distinct regimes. The first two regimes are the same as for ε = 2/255: ε), the robust accuracy is similar to that (i) when the strength of the injected features is weak (β trained on the original data (β = 0) and (ii) when it is strong (β ε), the robust accuracy is high as the network can use only v(y) to classify (cid:101)x robustly. Nevertheless, there is a third regime where the injected features are mildly robust, i.e., β ε. Strikingly, in this regime, the training suffers from CO and the robust accuracy drops to zero. This is significant, since training on the original dataset (β = 0) does not suffer from CO for this value of ε; but it does so when β\n\n4/255, 6/255\n\n∈ {\n\n≪\n\n≫\n\n≈\n\nD\n\nε.\n\n}\n\n≈\n\n1Throughout the paper, robustness is measured against strong PGD attacks with 50 iterations and 10 restarts.\n\n4\n\nIntervenedsamples(x,y)∼DβδFGSMbeforeCOδFGSMafterCOUnder review as a conference paper at ICLR 2023\n\n’s and for l2 perturbations with similar findings in Appendix D. We replicate these results for different Results for other datasets and networks, as well as further details of the training protocol are given in Appendices C and D respectively. From these observations, we conclude that there is indeed a link between the structure of the data and CO. In the following section, we delve deeper into these results to better understand the cause of CO.\n\nV\n\n4 ANALYSIS OF INDUCED CATASTROPHIC OVERFITTING\n\nSince we now have a method to intervene in the data using Eq. (3) and induce CO, we can use it to better characterize the mechanisms that lead to CO. In particular, we explore how different features in the dataset influences the likelihood of observing CO in FGSM-AT.\n\n4.1 ROBUST SOLUTIONS COMBINE EASY- AND HARD-TO-LEARN FEATURES\n\nε special? We show that for β\n\nThe previous section showed that when β However, for β β\noriginal dataset dataset (cid:101) D\nthe network only uses the features in\n\nε, our data intervention does not induce CO. ε FGSM-AT consistently experiences CO. This begs the question: what makes ε a network trained using AT uses information from both the to achieve a high robust accuracy on the injected β. However, when trained without any adversarial constraints i.e., for standard training,\n\nand achieves close to perfect clean accuracy.\n\nand the injected features in\n\nε or β\n\n≪\n\n≫\n\n≈\n\n≈\n\n≈\n\nD\n\nV\n\nIn order to demonstrate this empirically, we perform standard, FGSM-AT, and PGD-AT training β (as described in Section 3) with β = 8/255 and of a PreActResNet18 on the injected dataset (cid:101) D\nε = 6/255. First, note that Fig. 1 (right) shows that an FGSM-AT model suffers from CO when trained on this injected dataset. Next, to identify the different features learned by these three models, we construct three different test sets and evaluate the clean and robust accuracy of the networks on β), them in Fig. 3. The three different test sets are: (i) CIFAR-10 test set with injected features ( (cid:101) D\n(ii) original CIFAR-10 test set ( Dπ(β)) where the additive signals are correlated with a permuted set of labels, i.e.,\n\n), and (iii) CIFAR-10 test set with shuffled injected features ( (cid:101)\n\nD\n\nV\n\n((cid:101)x(π), y)\n\n(cid:101)\n\nDπ(β) :\n\n(cid:101)x(π) = x + β v(π(y)) with (x, y)\n\nY → Y\n\n∼ is a fixed permutation operator that shuffles the labels. Note that evaluating these\n\nHere, π : networks (trained on (cid:101) D\nx and v(π(y)) are correlated with different labels in (cid:101) β. Thus, if the classifier only relies on D\nthe performance should be high on (cid:101) Dπ(β), while if it only relies on and (cid:101) D\nperformance should remain constant for all injected datasets.\n\nDπ(β) exposes them to contradictory information, since\n\nβ) on data from (cid:101)\n\nβ and low on\n\nV the\n\n∼ D\n\n∈ V\n\nD\n\nD\n\nand v\n\n.\n\n(4)\n\nV\n\nD\n\nand\n\n. We know it uses\n\nPGD training We can conclude from Fig. 3(left) that the PGD-trained network achieves a robust solution using both (containing no information from the correct label; see Eq. (4)). On the other hand, we know the PGD-trained network uses network achieves higher accuracy on (cid:101) ) than D\ncontains information from from information from both the original and injected features for classification.\n\nDπ(β) (where features from β (containing information from both ). Further, it suffers a drop in performance on (cid:101)\n\nare anti-correlated). This implies that the robust PGD solution effectively combines\n\nD are uncorrelated with as the (only\n\nas it achieves better than trivial accuracy on\n\nDπ(β) (where information\n\nD ) as well as on (cid:101)\n\nV D\n\nand\n\nand\n\nD\n\nD\n\nD\n\nV\n\nV\n\nV\n\nV\n\nStandard training Standard training shows a completely different behaviour than PGD (see β, Fig. 3 (center)). In this case, even though the network achieves excellent clean accuracy on (cid:101) D\nis nearly trivial. This indicates that with standard training the model ignores the its accuracy on information present in for classification. This is D\nfurther supported by the observation that on (cid:101) randomized, its accuracy is almost zero. From these observations we conclude that the injected features are easy to learn i.e. is preferred by our optimiser and neural networks.\n\nDπ(β), where the labels of the injected features are\n\nand only uses the non-robust features from\n\nD\n\nV\n\nFGSM training The behaviour of the FGSM-AT in Fig. 3(right) highlights the preference for the injected features even further. In the studied setting, FGSM-AT undergoes CO around epoch 8 when β. Next, as seen the robust accuracy on (cid:101) D\n\nβ suddenly drops to zero despite a high clean accuracy on (cid:101) D\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Clean (top) and robust (bottom) accuracy on 3 different test sets: (i) the original CIFAR-10 β and (iii) the dataset with shuffled injected features (\nDπ(β). All training runs use β = 8/255 and ε = 6/255 (where FGSM-AT suffers CO). The blue shading\n\n), (ii) the dataset with injected features (cid:101) D\n\nD (cid:101) denotes when the network exploits both\n\nand the yellow shading when it learns only\n\nand\n\n.\n\nD\n\nV\n\nV\n\nin Fig. 3 (top right), FGSM-AT presents two distinct phases during training: (i) Prior to CO, when the robust accuracy on (cid:101) , similar D\nto PGD. (ii) However, with the onset of CO, both the clean and robust accuracy on Dπ(β) drops, exhibiting behavior similar to standard training. This indicates that, post-CO, akin to standard training, the network forgets the information from\n\nβ is non-zero, the network leverages features from both\n\nand solely relies on features in\n\nV and (cid:101)\n\nand\n\nD\n\nD\n\n.\n\nD\n\nV\n\nWhy does FGSM change the learned features after CO? From the behaviour of standard training , i.e., they are easy to learn. we concluded that the network has a preference for the injected features The behaviour of PGD training suggests that when the easy features are not sufficient to classify robustly, the model combines them with other (harder-to-learn) features e.g., in , to become robust. FGSM initially learns a robust solution leveraging both similar to PGD. However, if the FGSM attacks are rendered ineffective, the robustness constraints are essentially removed. This allows the network to revert back to the simple features and the performance on the original dataset drops. This is, exactly, what occurs with the onset of CO around epoch 10 (further discussed in Section 4.2) Yet, why does CO happen in the first place? In the following, we will see that the key to answer this question lies in the way learning each type of feature influences the local geometry of the classifier.\n\nand\n\nD\n\nD\n\nD\n\nV\n\nV\n\n4.2 CURVATURE EXPLOSION DRIVES CATASTROPHIC OVERFITTING\n\nRecent works have shown that after the onset of CO the local geometry around the input x becomes non-linear (Andriushchenko & Flammarion, 2020; Kim et al., 2021; de Jorge et al., 2022). Motivated by these findings, and with the aim to identify how our data intervention causes CO, we now investigate how the curvature of the loss landscape evolves with different types of training. We use the average maximum eigenvalue of the Hessian on N = 100 fixed training points ̄λmax = (fθ((cid:101)xn), yn)(cid:1) to estimate the curvature, as suggested in Moosavi-Dezfooli et al.\n\n1 N\n(2019)), and record it throughout training. Fig. 4(left) shows the result of this experiment for FGSMβ with β = 8/255 and ε = 6/255. Recall that AT (orange line) and PGD-AT (green line) training on (cid:101) D\nthis training regime exhibits CO with FGSM-AT around epoch 8 (see Fig. 3(left)).2\n\nn=1 λmax\n\n(cid:80)N\n\n(cid:101)xL\n\n∇\n\n(cid:0)\n\n2\n\nTwo-phase curvature increase Interestingly, we observe that even before the onset of CO, both FGSM-AT and PGD-AT show a nearly similar steep increase in curvature (the y-axis is in logarithmic scale). While right before the 8th epoch, there is a large increase in the curvature for PGD-AT, it stabilizes soon after and remains controlled for the rest of training. Prior work has observed that PGD-AT acts as a regularizer on the curvature (Moosavi-Dezfooli et al., 2019; Qin et al., 2019) which explains how PGD-AT controls the curvature explosion. However, curvature is a second-order\n\n2Results for other parameters and for the original D are provided in Appendix F.\n\n6\n\n01020300%50%100%CleanaccuracyPGD-10training01020300%50%100%Standardtraining01020300%50%100%COFGSMtraining0102030Epoch0%20%40%60%Robustaccuracy0102030Epoch0%20%40%60%Injected( ̃Dβ)CIFAR10(D)Shuffled( ̃Dπ(β))0102030Epoch0%20%40%60%COUnder review as a conference paper at ICLR 2023\n\nFigure 4: Evolution of different metrics for FGSM-AT and PGD-AT on 2 datasets: (i) with injected feaβ) and (ii) with orthogonally projected features, i.e. with no interaction between the original tures ( (cid:101) D\nβ ). AT is performed for β = 8/255 and ε = 6/255 (where FGSM suffers CO). and injected features ( (cid:101) D\n\n⊥\n\nproperty of the loss surface and unlike PGD-AT, FGSM-AT is based on a coarse linear (first order) approximation of the loss. Therefore, FGSM-AT is not as effective at regularising the curvature. Indeed, we see that FGSM-AT cannot contain the curvature increase, which eventually explodes around the 8th epoch and saturates at a very large value. Quite remarkably, the final curvature of the FGSM-AT model is 100 times that of the PGD-AT model.\n\nHigh curvature leads to meaningless perturbations The fact that the curvature increases rapidly during CO, when the attack accuracy also increases, agrees with the findings of Andriushchenko & Flammarion (2020), that the loss becomes highly non-linear and thus reduces the success rate of FGSM. To show that CO indeed occurs due to the increased curvature breaking FGSM, we visualise the adversarial perturbations before and after CO. As observed in Fig. 2, before CO, the adversarial , albeit with some corruptions originating from x. Nonetheperturbations point in the direction of less, after CO, the new adversarial perturbations point towards meaningless directions; they do not align with even though the network is heavily reliant on this information for classifying the data (cf. Section 4.1). This reinforces the idea that the increase in curvature indeed causes a breaking point after which FGSM is no longer an effective adversarial attack. We would like to highlight that this behaviour of the adversarial perturbations after CO is radically different from the behaviour on standard and robust networks (in the absence of CO) where adversarial perturbations and curvature are strongly aligned with discriminative directions (Fawzi et al., 2018; Jetley et al., 2018; Ilyas et al., 2019).\n\nV\n\nV\n\n4.3 CURVATURE INCREASE IS A RESULT OF INTERACTION BETWEEN FEATURES\n\nBut why does the network increase the curvature in the first place? In Section 4.1, we observed that this is a shared behaviour of PGD-AT and FGSM-AT, at least during the initial stage before CO. Therefore, it should not be a mere “bug\". We conjecture that the curvature increases is a result of the interaction between features of the dataset which forces the network to increase its non-linearity in order to combine them effectively to obtain a robust model.\n\nCurvature does not increase without interaction To demonstrate this, we perform a new experiment again (as in Section 3). However, this time, we ensure that there is no interacin which we modify tion between the synthetic features v(y) and the features from β such that:\n\nD\n\n⊥\n\n. We do so by creating (cid:101) D\n\nD\n\n((cid:101)x⊥, y)\n\n⊥\n\nβ :\n\n(cid:101) D\n\n(cid:101)x⊥ =\n\nPV ⊥ (x) + βv(y) with (x, y)\n\n∼\n\nPV ⊥ denotes the projection operator onto the orthogonal complement of\n\nwhere features v(y) are orthogonal to the data up to a radius that depends solely on β (see the theoretical construction in Appendix A).\n\n∈ V . Since the synthetic , a simple linear classifier relying only on v(y) can robustly separate\n\n∼ D\n\nD\n\nV\n\nand v(y)\n\n(5)\n\nInterestingly, we find that, in this dataset, none of the (β, ε) configurations used in Fig. 5 induce CO. Here, we observe only two regimes: one that ignores (when β > ε). This supports our conjecture that the interaction between the features of x and v(y) is the true β. Moreover, Fig. 4 (left) shows that, when performing either FGSM-AT (light blue) cause of CO in (cid:101) D\nor PGD-AT (dark blue) on (cid:101) β , the curvature is consistently low. This agrees with the fact that in D\nthis case there is no need for the network to combine the injected and the original features to achieve robustness and hence the network does not need to increase its non-linearity to separate the data.\n\n(when β < ε) and one that ignores\n\nD\n\nV\n\n⊥\n\n7\n\n051015202530Epoch10−210−1100101Phase1Phase2Curvature( ̄λmax)FGSM-AT( ̃Dβ)PGD-AT( ̃Dβ)FGSM-AT( ̃D⊥β)PGD-AT( ̃D⊥β)051015202530Epoch50%60%70%COCIFAR-10featureaccuracyUnder review as a conference paper at ICLR 2023\n\nFigure 5: Clean (left) and robust (right) accuracy after FGSM-AT on a dataset with orthogonally injected features (cid:101) β i.e. no interaction between original and injected features. We train while varying D\nthe strength of the injected features and the robustness budget ε.\n\n⊥\n\nV\n\nD\n\nand\n\nNon-linear feature extraction Finally, we study the relationship between the information learned in the features and the network’s curvature. Using a similar methodology as Shi et al. (2022), we (output of the penultimate train multiple logistic classifiers on the the feature representations of β. Note that the accuracy of these classifiers strictly depends on how layer) of networks trained on (cid:101) D\n. We will call this metric feature accuracy. well the network (trained on (cid:101) β) has learned both D\nD Figure 4(right) shows the evolution of the feature accuracy of the networks during training. Observe that, for PGD-AT (green), the feature accuracy on progressively grows during training. The high values of feature accuracy indicate that this network has learned to meaningfully extract information from β. Moreover, we note that the feature accuracy closely matches the curvature trajectory in Fig. 4 (left). Meanwhile, for FGSM-AT the feature accuracy has two phases: first, it grows at the same rate as for the PGD-AT network, but when CO happens, it starts to decrease. Note, however, that the curvature does not decrease. We argue this is because the network is using a shortcut to ignore . Specifically, if the curvature is very high, FGSM is rendered ineffective and allows the network to focus only on the easy non-robust features. On the other hand, if we use the features from networks trained on (cid:101) is always D\nlow. This reinforces the view that the network is increasing the curvature in order to improve its and feature representation. In (cid:101) D\n\nβ the network does not need to combine information from both\n\n, even if it was trained on (cid:101) D\n\nβ we observe that the accuracy on\n\nto become robust, and hence it does not learn to extract information from\n\nD\n\nD\n\nD\n\nD\n\nD\n\n⊥\n\n⊥\n\n.\n\nV 4.4 A MECHANISTIC EXPLANATION OF CO\n\nD\n\nTo summarize, we finally describe the chain of events that leads to CO in our injected datasets:\n\n(i) To learn a robust solution, the network attempts to combine easy, non-robust features with more complex robust features. However, without robustness constraints, the network strongly favors learning only the non-robust features (see Section 4.1).\n\n(ii) When learning both kinds of features simultaneously, the network increases its non-linearity to\n\nimprove its feature extraction ability (see Section 4.3).\n\n(iii) This increase in non-linearity provides a shortcut to break FGSM which triggers CO. This allows the network to avoid learning the complex robust features while still achieving a high accuracy using only the easy non-robust ones (see Section 4.2).\n\nAside from our empirical study, the intuition that a classifier needs to combine features to become robust can be formalized in certain settings. For example, in Appendix B we mathematically prove that there exist many learning problems in which learning a robust classifier requires leveraging additional non-linear features on top of the simple ones used for the clean solution. Moreover, in Section 5 we leverage these intuitions to explore how data interventions on real datasets can prevent CO.\n\n5 FURTHER INSIGHTS AND DISCUSSION\n\nOur proposed dataset intervention, defined in Section 3, allowed us to gain a better understanding of the chain of events that lead to CO. In this section, we focus our attention on methods that can prevent CO and analyze them in the context of our framework to provide further insights.\n\n8\n\n02468101214β(/255)70%80%90%100%Cleanaccuracy(cid:15)=2/255(cid:15)=4/255(cid:15)=6/255(cid:15)=8/25502468101214β(/255)0%20%40%60%80%100%RobustaccuracyUnder review as a conference paper at ICLR 2023\n\nFigure 6: Left: Clean and robust performance after AT with GradAlign, N-FGSM and PGD-10 on β at ε = 6/255. Results averaged over three random seeds and shaded areas report minimum and\n\n(cid:101) D\nmaximum values. Right: Curvature evolution when training on (cid:101) D\n\nβ at ε = 6/255 and β = 8/255.\n\nGradAlign and N-FGSM prevent CO on (cid:101) β In Fig. 6, we show the robust accuracy (left) and D\ncurvature (right) of models trained with GradAlign (Andriushchenko & Flammarion, 2020) and β for varying β. Figure 6 (left) shows that both methods are able N-FGSM (de Jorge et al., 2022) on (cid:101) D\nto prevent CO on our injected dataset for suitable choices of the regularisation parameter i.e., λ for GradAlign and k for N-FGSM. This suggests that the mechanism by which our intervention induces CO is similar to how it occurs in real datasets. However, for certain values of β, both GradAlign and N-FGSM require stronger regularization. Thus, the regularization strength is not only a function of ε, as discussed in their respective manuscripts, but also of the signal strength β. As β increases, v(y) becomes more discriminative creating a stronger incentive for the network to use it. We argue that this increases the chances for CO as the network (based on our observations in Section 4) will likely increase the curvature to combine the discriminative injected features with others in order to become robust. Moreover, Fig. 6 shows that the curvature of N-FGSM and GradAlign AT stabilizes with a trend similar to PGD-AT and stronger regularizations dampens the increase of the curvature even further. This further shows that preventing the curvature from exploding can indeed prevent CO.\n\nTable 1: Clean and robust accuracies of FGSM-AT and PGD-AT trained networks on CIFAR-10 and the low pass version described in Ortiz-Jimenez et al. (2020b) at different ε.\n\nCan data modifications avoid CO? Section 3 shows that it is possible to induce CO through data manipulations. But is the opposite also true that CO can be avoided using data manipulations? We find that this is indeed possible on CIFAR-10. Table 1 shows that removing the high frequency components of consistently prevents CO at ε = 8/255 (where FGSM-AT fails). Interestingly, Grabinski et al. (2022), in concurrent work, applied this idea to the pooling layers and showed they could prevent CO. Surprisingly, though, we have found that applying the same low-pass technique at ε = 16/255 does not work. We conjecture this is because the features which are robust at ε = 8/255 in the low pass version of CIFAR-10 might not be robust at ε = 16/255, therefore forcing the network to combine more complex features. Although this method does not work in all settings, it is an interesting proof of concept that shows that removing some features from the dataset can indeed prevent CO. Generalizing this idea to other kinds of features and datasets can be a promising avenue for future work.\n\nFGSM (16/255) 76.3 PGD (16/255) 68.0\n\nFGSM (8/255) 85.6 PGD (8/255) 80.9\n\nClean Robust Clean Robust\n\nMethod (ε)\n\nLow pass\n\n78.6 66.9\n\n0.0 29.2\n\n0.0 28.4\n\n47.0 49.7\n\n81.1 80.3\n\n0.0 50.6\n\nOriginal\n\nD\n\n6 CONCLUDING REMARKS\n\nIn this work, we have presented a thorough empirical study to establish a causal link between the features of the data and the onset of CO in FGSM-AT. Specifically, using controlled data interventions we have seen that catastrophic overfitting is a learning shortcut used by the network to avoid learning hardto-learn robust features while achieving high accuracy using easy non-robust ones. This new perspective has allowed us to shed new light on the mechanisms that trigger CO, as it shifted our focus towards studying the way the data structure influences the learning algorithm. We believe this opens the door to promising future work focused on understanding the intricacies of these learning mechanisms. In general, we consider that deriving methods for inspecting the data and identifying how different features of a dataset interact with each other as in Ilyas et al. (2019) is another interesting avenue for future work.\n\n9\n\n02468101214β(/255)0%20%40%60%80%RobustaccuracyPGD-10GradAlign(λ=0.112)GradAlign(λ=2)N-FGSM(k=10(cid:15))N-FGSM(k=2(cid:15))051015202530Epoch10−210−1100101Curvature( ̄λmax)Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nN. Ahmed, T. Natarajan, and K. R. Rao. Discrete Cosine Transform. IEEE Transactions on Computers,\n\n1974.\n\nMaksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial\n\ntraining. In Neural Information Processing Systems (NeurIPS), 2020.\n\nDevansh Arpit, Stanisław Jastrz ̨ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In International Conference on Machine Learning (ICML), 2017.\n\nJames Aspnes, Richard Beigel, Merrick Furst, and Steven Rudich. The expressive power of voting\n\npolynomials. Combinatorica, 1994.\n\nFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206–2216. PMLR, 2020.\n\nPau de Jorge, Adel Bibi, Riccardo Volpi, Amartya Sanyal, Philip H. S. Torr, Grégory Rogez, and Puneet K. Dokania. Make some noise: Reliable and efficient single-step adversarial training. arxiv:2202.01181, 2022.\n\nAlhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano Soatto. Empirical study of the topology and geometry of deep networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nZeinab Golgooni, Mehrdad Saberi, Masih Eskandar, and Mohammad Hossein Rohban. Zerograd: Mitigating and explaining catastrophic overfitting in fgsm adversarial training. arXiv:2103.15476, 2021.\n\nIan Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. International Conference on Learning Representations (ICLR), 2015.\n\nJulia Grabinski, Steffen Jung, Janis Keuper, and Margret Keuper. FrequencyLowCut Pooling - Plug & Play against Catastrophic Overfitting. In European Conference on Computer Vision (ECCV), 2022.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\n\nnetworks. In European Conference on Computer Vision (ECCV), 2016.\n\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. Neural Information Processing Systems (NeurIPS), 2019.\n\nSaumya Jetley, Nicholas Lord, and Philip Torr. With friends like these, who needs adversaries?\n\nNeural Information Processing Systems (NeurIPS), 2018.\n\nPeilin Kang and Seyed-Mohsen Moosavi-Dezfooli. Understanding catastrophic overfitting in adver-\n\nsarial training. arXiv:2105.02942, 2021.\n\nHoki Kim, Woojin Lee, and Jaewook Lee. Understanding catastrophic overfitting in single-step\n\nadversarial training. In AAAI Conference on Artificial Intelligence (AAAI), 2021.\n\nKlim Kireev, Maksym Andriushchenko, and Nicolas Flammarion. On the effectiveness of adversarial training against common corruptions. In James Cussens and Kun Zhang (eds.), Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, volume 180 of Proceedings of Machine Learning Research, pp. 1012–1021. PMLR, 01–05 Aug 2022. URL https:// proceedings.mlr.press/v180/kireev22a.html.\n\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.\n\nMaster’s thesis, Department of Computer Science, University of Toronto, 2009.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In\n\nInternational Conference on Learning Representations (ICLR), 2017.\n\nBai Li, Shiqi Wang, Suman Jana, and Lawrence Carin. Towards understanding fast adversarial\n\ntraining. arXiv:2006.03089, 2020.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018.\n\nSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via curvature regularization, and vice versa. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In Neural Information Processing Systems (NeurIPS), Workshops, 2011.\n\nGuillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi, and Pascal Frossard. Neural\n\nanisotropy directions. Neural Information Processing Systems (NeurIPS), 2020a.\n\nGuillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi, and Pascal Frossard. Hold me tight! influence of discriminative features on deep network boundaries. Neural Information Processing Systems (NeurIPS), 2020b.\n\nGuillermo Ortiz-Jiménez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Optimism in the face of adversity: Understanding and improving deep learning through adversarial robustness. Proceedings of the IEEE, 2021.\n\nGeon Yeong Park and Sang Wan Lee. Reliably fast adversarial training via latent adversarial perturbation. In International Conference on Learning Representations (ICLR), Workshops, 2021.\n\nChongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvijotham, Alhussein Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli. Adversarial robustness through local linearization. In Neural Information Processing Systems (NeurIPS), 2019.\n\nLeslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In\n\nInternational Conference on Machine Learning (ICML), 2020.\n\nAmartya Sanyal, Puneet K. Dokania, Varun Kanade, and Philip Torr. How benign is benign overfitting\n\n? In International Conference on Learning Representations (ICLR), 2021.\n\nAli Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! Neural Information Processing Systems (NeurIPS), 2019.\n\nHarshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. Neural Information Processing Systems (NeurIPS), 2020.\n\nYuge Shi, Imant Daunhawer, Julia E Vogt, Philip HS Torr, and Amartya Sanyal. How robust are\n\npre-trained models to distribution shift? arXiv preprint arXiv:2206.08871, 2022.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014.\n\nFlorian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In International Conference on Learning Representations (ICLR), 2018.\n\nBS Vivek and R Venkatesh Babu. Single-step adversarial training with dropout scheduling. In IEEE\n\nConference on Computer Vision and Pattern Recognition (CVPR), 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nLily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In International Conference on Machine Learning (ICML), 2018.\n\nEric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In\n\nInternational Conference on Learning Representations (ICLR), 2020.\n\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. In bmvc, 2016.\n\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning (ICML), 2019.\n\nRichard Zhang. Making convolutionall neural networks shift-invariant again.\n\nIn International\n\nConference on Machine Learning (ICML), 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA ANALYSIS OF THE SEPARABILITY OF THE INJECTED DATASETS\n\n}\n\nV\n\nx\n\nD\n\nand\n\n∼ D\n\n1, +1\n\n∈ {−\n\nwhere y\n\nβ and further assume that v(+1) = u and v(\n\ncan influence the robustness of a and With the aim to illustrate how the interaction between classifier trained on (cid:101) β we now provide a toy theoretical example in which we discuss this interaction. D\nSpecifically, without loss of generality, consider the binary classification setting on the dataset 2 = 1, for ease. Let’s now consider the injected dataset (x, y) 2 = 1, such that , such that and\n\n(cid:101) D\n(cid:101)x = x + βyu. Moreover, let γ γ. −\nWe are interested in characterizing the robustness of a classifier that only uses information in classifying (cid:101) D\nwith the binary setting, we will characterize the robustness of a linear classifier h : Rd that discriminates the data based only on\n\nwhen β depending on the strength of the interaction coefficient. In particular, as we are dealing\n\n∥ −\n[0, 1] denote the interaction coefficient between\n\nu with u\n\n→ {− (cid:101)x). In our setting, we have\n\n, i.e., h((cid:101)x) = sign(u⊤\n\nRd and\n\nx⊤u\n\n1, +1\n\n1) =\n\n∥ D\n\n−\n\n≤\n\n≤\n\nu\n\n∈\n\n∈\n\nV\n\nV\n\nγ\n\n}\n\n∥\n\n∥\n\nV\n\nu⊤ u⊤\n\n(cid:101)x = u⊤x + βu⊤u = u⊤x + β if y = +1 (cid:101)x = u⊤x 1\n\nβu⊤u = u⊤x\n\nβ if y =\n\n−\n\n−\n\n−\n\nProposition 1 (Clean performance). If β > γ, then h achieves perfect classification accuracy on (cid:101) D\n\nβ.\n\n, the dataset is perfectly linearly separable. However, if the data x from\n\nProof. Observe that if γ = 0, i.e. the features from original dataset do not interact with the injected features interacts with the injected signal u, i.e. non zero projection, then the dataset is still perfectly separable but for a sufficiently larger β, such that u⊤x + β > 0 when y = +1 and u⊤x + β < 0 when y = 1. Because\n\nγ this is achieved for β > γ.\n\nx⊤u\n\n−\n\nD\n\nD\n\nV\n\nγ\n\n−\n\n≤\n\n≤\n\nProposition 2 (Robustness). If β > γ, the linear classifier h is perfectly accurate and robust to adversarial perturbations in an l2-ball of radius ε γ. Or, equivalently, for h to be ε-robust, the injected features must have a strength β\n\n≤ ε + γ.\n\n−\n\nβ\n\n≥\n\nProof. Given (cid:101)x, we seek to find the minimum distance to the decision boundary of such a classifier. A minimum distance problem can be cast as solving the following optimization problem:\n\nε⋆((cid:101)x) = min\n\nr∈Rd ∥\n\nr\n\n− (cid:101)x\n\n2 subject to r⊤u = 0, 2\n∥\n\nwhich can be solved in closed form\n\nε⋆((cid:101)x) = |\n\nu⊤ u\n\n∥\n\n(cid:101)x\n\n∥\n\n|\n\n=\n\nu⊤x + yβ |\n\n. |\n\nThe robustness radius of the classifier h will therefore be ε = inf can be bounded by\n\n(cid:101)x∈supp( (cid:101)Dβ ) ε⋆((cid:101)x), which in our case\n\nε =\n\ninf ((cid:101)x,y)∈supp( (cid:101)Dβ )\n\nε⋆((cid:101)x)\n\nmin |u⊤x|≤γ,y=±1 |\n\n≤\n\nu⊤x + yβ\n\n=\n\n|\n\nγ\n\nβ\n\n|\n\n±\n\n| ∓\n\n= β\n\nγ.\n\n−\n\nBased on these propositions, we can clearly see that the interaction coefficient γ reduces the robustness γ, robust classification at a radius ε can only be of the additive features ≥\nV achieved by also leveraging information within\n\n. In this regard, if ε\n\n− .\n\nβ\n\nD\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB ROBUST CLASSIFICATION CAN REQUIRE NON-LINEAR FEATURES\n\nWe now provide a rigorous theoretical example of a learning problem that provably requires additional complex information for robust classification, even though it can achieve good clean performance using only simple features.\n\nGiven some p functions from Rp+1 to degree that can represent h has a degree (largest order polynomial term) of s.\n\nN, let Rp+1 be the input domain. A concept class, defined over Rp+1 is a set of . A hypothesis h is s-non-linear if the polynomial with the smallest\n\n0, 1\n\n∈\n\n{\n\n}\n\nUsing these concepts we now state the main result. Theorem 1. For any p, k Rp+1 and a concept class\n\nN, ε < 0.5 such that k < p, there exits a family of distributions defined over Rp+1 such that\n\n∈ H\n\nk over\n\nD\n\n1.\n\nis PAC learnable (with respect to the clean error) with a linear (degree 1) classifier.\n\nH However,\n\nis not robustly learnable with any linear classifier.\n\nH\n\n2. There exists an efficient learning algorithm, that given a dataset sampled i.i.d. .\n\ndistribution\n\nk robustly learns\n\nfrom a\n\nD ∈ D\n\nH\n\nIn particular, the algorithm returns a k-non-linear classifier and in addition, the returned classifier also exploits the linear features used by the linear non-robust classifier.\n\n4tε : t\n\nProof. We now define the construction of the distributions in family of distribution\n\nk is uniquely defined by three parameters: a threshold parameter ρ D\n∈ (one can think of this as the non-robust, easy-to-learn feature), a p dimensional {\n∈ { bit vector c 1 = k (this is the non-linear but robust feature) and ε. Therefore, given ρ and c (and ε which we discuss when necessary and ignore from the notation for simplicity), we c,ρ. We provide an illustration of this distribution for p = 2 in Figure 7. can define the distribution\n\nk. Every distribution\n\n}} p such that\n\nin the\n\n∈ {\n\n0, 1\n\n· · ·\n\n, k\n\n0,\n\nD\n\nD\n\nc\n\n∥\n\n}\n\n∥\n\nD\n\n{\n\n0, 1\n\np. Let ˆy = (cid:80)p−1 }\n\nc,ρ, first, sample a random bit vector ˆx\n\nSampling the robust non-linear feature To sample a point (x, y)\n\nRp+1 from the distribution Rp from the uniform distribution over the boolean D\nc[i] (mod 2) be the label of the parity function with respect hypercube ·\nto c evaluated on ˆx. The marginal distribution over (cid:98)y, if sampled this way, is equivalent to the Bernoulli distribution with parameter 1 2 . To see why, fix all bits in the input except one (chosen arbitrarily from the variables of the parity function), which is distributed uniformly over . It is easy to see that this forces the output of the parity function to be distributed uniformly over as well. Repeating this process for all dichotomies of p the desired result. Intuitively, ˆx constitutes the robust non-linear feature of this distribution.\n\n} 1 variables of the parity function proves\n\ni=1 ˆx[i]\n\n} 0, 1\n\n0, 1\n\n−\n\n∈\n\n∈\n\n{\n\n{\n\nSampling the non-robust linear feature To ensure that ˆx is not perfectly correlated with the true label, we sample the true label y from a Bernoulli distribution with parameter 1 2 . Then we sample the non-robust feature x1 as follows\n\nx1\n\n∼\n\n \n\n\n\n1\n\nUnif (cid:0)X − Unif (cid:0)X + Unif (cid:0)X − Unif (cid:0)X +\n\n1\n\n2\n\n2\n\n(cid:1) (cid:1) (cid:1) (cid:1)\n\ny = 0 y = 1 y = 0 y = 1\n\n∧ (cid:98)y = 0 ∧ (cid:98)y = 1 ∧ (cid:98)y = 1 ∧ (cid:98)y = 0\n\nwhere X +\n\n1 = [ρ, ρ + ε] and X +\n\n2 = [(ρ + 2ε, ρ + 3ε)] , X − Finally, we return (x, y) where x = (x1; ˆx) is the concatenation of x1 and ˆx.\n\nε, ρ] and X −\n\n1 = [ρ\n\n−\n\n2 = [(ρ\n\n3ε, ρ\n\n−\n\n−\n\n2ε)] .\n\nLinear non-robust solution First, we show that robust solution to this problem. Sm = the first coordinate, of the covariates of the dataset to create S0 where Si [j] indexes the jth coordinate of the ith element of the dataset. Then, sort S0 of the covariates (i.e., the first coordinate). Let ˆρ be the largest element whose label is 0.\n\nthere is a linear, accurate, but nonsample an m-sized dataset c,ρ. Ignore all, except\n\nfrom the distribution m =\n\n(x1 [0] , y1) . . . (xm [0] , ym)\n\nTo obtain this solution,\n\n(x1, y1) , . . . , (xm, ym)\n\n} m on the basis\n\n× {−\n\nRp+1\n\n} ∈\n\n1, 1\n\nD\n\n{\n\n{\n\n}\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nx [1]\n\nx [1]\n\nx [1]\n\n(0, 1)\n\n(0, 1)\n\nε\n\nρ\n\nε\n\nFor |x[0] − ρ| ≤ ε\n\nFor |x[0] − ρ| > ε\n\n(0., 0)\n\n(1, 0)\n\nx [2]\n\n(0., 0)\n\n(1, 0)\n\nx [2]\n\nx [0]\n\n(0, 1)\n\n(0, 0)\n\nc,ρ in three dimensions. The data is linearly Figure 7: Illustration of one possible distribution separable in the direction x[0] but has a very smalll margin in that direction. Leveraging x[1] and x[2] additionally, we see how the data can indeed be separated robustly, albeit non-linearly.\n\nD\n\nDefine flin, ˆρ as the linear threshold function on the first coordinate i.e. flin, ˆρ (x) = I .\n} By construction, flin, ˆρ accurate classifies all points in Sm. The VC dimension of a linear threshold function in 1 dimension is 2. Then, using standard VC sample complexity upper bounds 3 for α log (cid:0) 1 , where κ0 is some universal constant, we consistent learners, if m\n\nα log\n\n+ 1\n\nx[0]\n\n(cid:16) 1\n\n(cid:16) 1\n\n(cid:1)(cid:17)\n\nκ0\n\n≥\n\n(cid:17)\n\nˆρ\n\n{\n\nα\n\nhave that with probability at least 1\n\n≥\n\nβ β,\n\n−\n\nErr (flin, ˆρ;\n\nc,ρ)\n\nD\n\nα.\n\n≤\n\nNon-linear robust solution Next, we propose an algorithm to find a robust solution and show that this solution has a non-linearity of degree k. First, sample the m-sized dataset Sm and use the method described above to find ˆρ. Then, create a modified dataset (cid:98)S by first removing all points x from Sm such that 8 and then removing the first coordinate of the remaining points. Thus, each element in (cid:98)S belongs to Rp\n\ndimensional dataset.\n\nx [0] |\n\n0, 1\n\n| ≥\n\n−\n\nˆρ\n\nε\n\n× {\n\n}\n\nNote, that by construction, there is a consistent (i.e., an accurate) parity classifier on (cid:98)S. Let the parity bit vector consistent with (cid:98)S be ˆc. This can be learned using Gaussian elimination. Consequently, construct the parity classifier fpar,(cid:98)c = (cid:80)p−1 Finally, the algorithm returns the classifier g ˆρ,ˆc, which acts as follows:\n\n· (cid:98)c [i] (mod 2).\n\ni=0 (cid:98)x [i]\n\ng ˆρ,ˆc (x) =\n\n \n\n\n\n1 0\nfpar,(cid:98)c ((cid:101)x)\n\nI (cid:8)x[0] I (cid:8)x[0] o.w.\n\n≥ ≤\n\nˆρ + ε + ε 8\nε ε\nˆρ 8\n\n−\n\n−\n\n(cid:9) (cid:9)\n\n(6)\n\nwhere (cid:101)x = round (x [1, . . . , p]) is obtained by rounding off x starting from the second index till the last. For example, if x = [0.99, 0.4, 0.9, 0.4, 0.8], ε = 0.2, and c = [0, 0, 1, 1] then (cid:101)x = [0, 1, 0, 1] and g0.5,ˆc [(cid:101)x] = 1. Finally, it is easy to verify that the classifier g ˆρ,ˆc is accurate on all training points and as the number of total parity classifiers is less than 2p (hence finite VC dimension), as , where κ1 is some universal constant, we have that with long as m\n\n+ p\n\n(cid:16) 1\n\n(cid:16) 1\n\n(cid:1)(cid:17)\n\nκ1\n\n(cid:17)\n\nα log (cid:0) 1\n\nα\n\nα log\n\nprobability at least 1\n\n≥\n\nβ β,\n\n−\n\nErr (g ˆρ,ˆc;\n\nc,ρ)\n\nD\n\nα.\n\n≤\n\n·\n\n−\n\n| ≤ ≤\n\n4ε Err (flin, ˆρ; D\n1 32 , we have that\n\nRobustness of g ˆρ,ˆc As x [0] is distributed uniformly in the intervals [ρ c,ρ) ˆρ ρ\nthat |\n8 . Intuitively, this guarantees that g ˆρ,ˆc uses the linear such that α ˆρ |\nε] and fpar,(cid:98)c in the threshold function on x [0] for classification in the interval [ρ, ρ + ε] c,ρ), the [ρ + 2ε, ρ + 3ε] D\n∈ classifier g ˆρ,ˆc does not alter its prediction in an l∞-ball of radius ε. We show this by studying four separate cases. First, we prove robustness along all coordinates except the first.\n\n4εα. Therefore, when m is large enough (cid:0)m = poly (cid:0) 1\n\n− 3ε]. A crucial property of g ˆρ,ˆc is that for all x\n\n[ρ, ρ + ε], we have (cid:1)(cid:1)\n\nSupp (\n\n2ε, ρ\n\n[ρ, ρ\n\nε, ρ]\n\n≤ ρ\n\n| ≤\n\n[ρ\n\n−\n\n−\n\n−\n\n−\n\n∪\n\n∪\n\n∪\n\nα\n\nε\n\n1. When\n\nx[0]\n\nˆρ\n\n|\n\n−\n\n| ≥\n\nε + ε\n\nis thus robust against all l∞ perturbations against those coordinates.\n\n8 , as shown above, g ˆρ,ˆc is invariant to all x [i] for all i > 0 and\n\n3https://www.cs.ox.ac.uk/people/varun.kanade/teaching/CLT-MT2018/\n\nlectures/lecture03.pdf\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n2. When\n\n< ε + ε\n\n|\n\n−\n\nˆρ |\n\nx[0]\n\n8 , due to Equation (6), we have that g ˆρ,ˆc = fpar,(cid:98)c ((cid:101)x) where (cid:101)x = round (x [1, . . . , p]) is obtained by rounding off all indices of x except the first. As the rounding operation on the boolean hypercube is robust to any l∞ perturbation of radius less than 0.5, we have that g ˆρ,ˆc is robust to all l∞ perturbations of radius less than 0.5 on the support of the distribution\n\nc,ρ.\n\nD\n\nNext, we prove the robustness along the first coordinate. Let 0 < δ < ε represent an adversarial perturbation. Without loss of generality, assume that x [0] > ˆρ as similar arguments apply for the other case.\n\n1. Consider the case x[0]\n\nx [0] 8 . Then, |\nhence, by construction, g ˆρ,ˆc (x) = g ˆρ,ˆc ([x [0] all δ, we have that g ˆρ,ˆc ([x [0] + δ; [x] [1, . . . , p]]) = 1 if g ˆρ,ˆc (x) = 1.\n\n8 and δ; [x] [1, . . . , p]]). On the other hand, for\n\n8 −\n\n| ≤\n\n−\n\n≤\n\n−\n\n−\n\n≤\n\nˆρ\n\nδ\n\nˆρ + ε + ε\n\nε + ε\n\n(cid:12) (cid:12)ε + ε\n\nδ(cid:12) (cid:12)\n\nˆρ + ε + ε\n\n2. For the case x[0]\n\n8 , the distribution is supported only on the interval [ρ + 2ε, ρ + 3ε]. When a positive δ is added to the first coordinate, the classifier’s prediction does not change and it remains 1. For all δ 2 , when the perturbation is subtracted from the first coordinate, its first coordinate is still larger than ˆρ + ε + ε 8 and hence, the prediction is still 1.\n\n≤\n\n≥\n\nε\n\nThis completes the proof of robustness of g ˆρ,ˆc along all dimensions to l∞ perturbations of radius less than ε. Combining this with its error bound, we have that Advε,∞ (g ˆρ,ˆc;\n\nc,ρ)\n\nα.\n\nD\n\n≤\n\nTo show that the parity function is non-linear, we use a classical result from Aspnes et al. (1994). Theorem 2.2 in Aspnes et al. (1994) shows that approximating the parity function in k bits using (cid:5) mistakes. Therefore, the (cid:0)k a polynomial of degree l incurs at least (cid:80)kl i\nlowest degree polynomial that can do the approximation accurately is at least k.\n\n(cid:1) where kl = (cid:4) k−l−1\n\ni=0\n\n2\n\nThis completes our proof of showing that the robust classifier is of non-linear degree k while the accurate classifier is linear. Next, we prove that no linear classifier can be robust. We show this by contradiction.\n\nZ\n\nof s (to be defined later) points in Rp+1 by No linear classifier can be robust Construct a set sampling the first coordinate from the interval [ρ, ρ + ε] and the remaining p coordinates uniformly from the boolean hypercube. Then, augment the set by subtracting ε from the first coordinate while retaining the rest of the coordinates. Note that this set can be generated, along with its labels, by sampling enough points from the original distribution and discarding points that do not fall in this interval. Now construct adversarial examples of each point in the augmented set by either adding or subtracting ε from the negatively and the positively labelled examples respectively and augment the original set with these adversarial points. For a large enough s,4, this augmented set of points can be decomposed into a multiset of points, where all points in any one set has the same value in the first coordinate but nearly half of their label is zero and the other half one.\n\nc,ρ. Therefore Now, assume that there is a linear classifier that has a low error on the distribution the classifier is also accurate on these sets of points as the classifier is robust, by assumption, and the c,ρ. However, as the first coordinate union of these sets occupy a significant under the distribution of every point within a set is constant despite half the points having label one and the other half zero, the coefficient of the linear classifier can be set to zero without altering the behavior of the classifier. Then, effectively the linear classifier is representing a parity function on the rest of the p coordinates. However, we have just seen that this is not possible as a linear threshold function cannot represent a parity function on k bits where k > 1. This contradicts our initial assumption that there is a robust linear classifier for this problem.\n\nD\n\nD\n\nThis completes the proof.\n\n4There is a slight technicality as we might not obtain points that are exact reflections of each other around ρ\n\nbut that can be overcome by discretising upto a certain precision\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nC EXPERIMENTAL DETAILS\n\nIn this section we provide the experimental details for all results presented in the paper. Adversarial training for all methods and datasets follows the fast training schedules with a cyclic learning rate introduced in Wong et al. (2020). We train for 30 epochs on CIFAR Krizhevsky & Hinton (2009) and 15 epochs for SVHN Netzer et al. (2011) following Andriushchenko & Flammarion (2020). When we perform PGD-AT we use 10 steps and a step size α = 2/255; FGSM uses a step size of α = ε. Regularization parameters for GradAlign Andriushchenko & Flammarion (2020) and N-FGSM de Jorge et al. (2022) will vary and are stated when relevant in the paper. The architecture employed is a PreactResNet18 He et al. (2016). Robust accuracy is evaluated by attacking the trained models with PGD-50-10, i.e. PGD with 50 iterations and 10 restarts. In this case we also employ a step size α = 2/255 as in Wong et al. (2020). All accuracies are averaged after training and evaluating with 3 random seeds.\n\nThe curvature computation is performed following the procedure described in Moosavi-Dezfooli et al. (2019). As they propose, we use finite differences to estimate the directional second derivative of the loss with respect to the input, i.e.,\n\nw⊤\n\n2\n\nxL\n\n∇\n\n(fθ(x), y)\n\nx\n\n∇\n\n≈\n\n(fθ(x + tw), y)\n\nL\n\n(fθ(x\n\nL\n\ntw), y) ,\n\n−\n\nx\n\n− ∇ 2t\n\nwith t > 0 and use the Lanczos algorithm to perform a partial eigendecomposition of the Hessian without the need to compute the full matrix. In particular, we pick t = 0.1.\n\nAll our experiments were performed using a cluster equipped with GPUs of various architectures. The estimated compute budget required to produce all results in this work is around 2, 000 GPU hours (in terms of NVIDIA V100 GPUs).\n\nD INDUCING CATASTROPHIC OVERFITTING WITH OTHER SETTINGS\n\nIn Section 3 we have shown that CO can be induced with data interventions for CIFAR-10 and l∞ perturbations. Here we present similar results when using other datasets (i.e. CIFAR-100 and SVHN) and other types of perturbations (i.e. l2 attacks). Moreover, we also report results when the injected features v(y) follow random directions (as opposed to low-frequency DCT components). Overall, we find similar findings to those reported the main text.\n\nD.1 OTHER DATASETS\n\nSimilarly to Section 3 we modify the SVHN, CIFAR-100 and high resolution Imagenet-100 and TinyImagenet datasets to inject highly discriminative features v(y). Since SVHN also has 10 classes, we use the exact same settings as in CIFAR-10 and we train and evaluate with ε = 4 where training on the original data does not lead to CO (recall β = 0 corresponds to the unmodified dataset). On the other hand, for CIFAR-100 and Imagenet-100 we select v(y) to be the 100 DCT components with lowest frequency and we present results with ε = 5 and ε = 4 respectively. Similarly, for TinyImagenet (which has 200 classes) we use the first 200 DCT components and present results with ε = 6. Moreover, for ImageNet-100 we evaluate robustness with AutoAttack Croce & Hein (2020). Regarding the training settings, for CIFAR10/100 and SVHN datasets we use the same settings as Andriushchenko & Flammarion (2020), for ImageNet-100 we follow Kireev et al. (2022) and for TinyImageNet Li et al. (2020).\n\nIn all datasets we can observe similar trends as with CIFAR-10: For small values of β the injected features are not very discriminative due to their interaction with the dataset images and the model largely ignores them. As we increase β, there is a range in which they become more discriminative but not yet robust and we observe CO. Finally for large values of β the injected features become robust and the models can achieve very good performance focusing only on those.\n\nD.2 OTHER NORMS\n\nCatastrophic overfitting has been mainly studied for l∞ perturbations and thus we presented experiments with l∞ attacks following related work. However, in this section we also present results where\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Clean and robust performance after FGSM-AT on injected datasets (cid:101) β constructed from D\nCIFAR-100. As FGSM-AT already suffers CO on CIFAR-100 at ε = 6/255 we use ε = 5/255 in this experiment where FGSM-AT does not suffer from CO as seen for β = 0. In this setting, we observe CO happening when β is slightly smaller than ε. Results are averaged over 3 seeds and shaded areas report minimum and maximum values.\n\nFigure 9: Clean and robust performance after FGSM-AT on injected datasets (cid:101) β constructed from D\nSVHN. As FGSM-AT already suffers CO on SVHN at ε = 6/255 we use ε = 4/255 in this experiment where FGSM-AT does not suffer from CO as seen for β = 0. In this setting, we observe CO happening when β ε. Results are averaged over 3 seeds and shaded areas report minimum and maximum values.\n\n≈\n\nFigure 10: Clean and robust performance after FGSM-AT on injected datasets (cid:101) β constructed from D\nTinyImageNet. We use ε = 6/255 in this experiment where FGSM-AT does not suffer from CO as seen for β = 0. In this setting, we observe CO happening for β\n\n[7//255, 8//255].\n\n∈\n\nwe induce CO with l2 perturbation which are also widely used in adversarial robustness. In Fig. 11 we show the clean (left) and robust (right) accuracy after FGM-AT5 on our injected dataset from β). Similarly to our results with l∞ attacks, we also observe CO as the injected features CIFAR-10 ( (cid:101) D\n\n5FGM is the l2 version of FGSM where we do not take the sign of the gradient.\n\n18\n\n02468101214β(/255)60%70%80%90%Cleanaccuracy(cid:15)=5/25502468101214β(/255)0%20%40%60%80%Robustaccuracy02468101214β(/255)70%80%90%100%Cleanaccuracy(cid:15)=4/25502468101214β(/255)20%40%60%80%100%Robustaccuracy024681012β(/255)60%80%100%Cleanaccuracy(cid:15)=6/255024681012β(/255)20%40%60%RobustaccuracyUnder review as a conference paper at ICLR 2023\n\nTable 2: Test performance of FGSM-AT trained on different injected versions of ImageNet-100 with ε = 4/255. We observe that for β > ε, FGSM-AT clearly suffers from CO.\n\nClean accuracy\n\nFGSM accuracy Robust accuracy\n\nβ = 0/255 β = 2/255 β = 4/255 β = 5/255 β = 6/255\n\n71.0% 70.9% 74.2% 100% 100%\n\n49.1% 51.1% 52.6% 99.8% 98.0%\n\n45.6% 45.1% 42.1% 0.0% 1.9%\n\nbecome more discriminative (increased β). It is worth mentioning that the l2 norm we use (ε = 1.5) is noticeably larger than typically used in the literature, however, it would roughly match the magnitude of an l∞ perturbation with ε = 7/255. Interestingly, we did not observe CO for this range of β with ε = 1.\n\nFigure 11: Clean and l2 robust performance after FGSM-AT on injected datasets (cid:101) β constructed from D\nCIFAR-10. FGM-AT suffers CO on CIFAR-10 around ε = 2, so we use ε = 1.5 in this experiment where FGM-AT does not suffer from CO as seen for β = 0. In this setting, we observe CO happening when β ε. Results are averaged over 3 seeds and shaded areas report minimum and maximum values.\n\n≈\n\nD.3 OTHER INJECTED FEATURES\n\nWe selected the injected features for our injected dataset from the low frequency components of the DCT to ensure an interaction with the features present on natural images Ahmed et al. (1974). However, this does not mean that other types of features could not induce CO. In order to understand how unique was our choice of features we also created another family of injected datasets but this time using a set of 10 randomly generated vectors as features. As in the main text, we take the sign of each random vector to ensure they take values in and assign one vector per class. In Fig. 12 we observe that using random vectors as injected features we can also induce CO. Note that since our results are averaged over 3 random seeds, each seed uses a different set of random vectors.\n\n1, +1\n\n{−\n\n}\n\nD.4 OTHER ARCHITECTURES\n\nIn all our previous experiments we trainde a PreActResNet18 (He et al., 2016) as it is the standard architecture used in the literature. However, our observations our also robust to the choice of architecture. As we can see in Fig. 13, we can also induce CO when training a WideResNet28x10 Zagoruyko & Komodakis (2016) on an injected version of CIFAR-10.\n\nE LEARNED FEATURES AT DIFFERENT β\n\nIn Section 3 we discussed how, based on the strength of the injected features β, our injected datasets seem to have 3 distinct regimes: (i) When β is small we argued that the network would not use the\n\n19\n\n0.00.51.01.52.02.53.0β(/255)75%80%85%90%95%100%Cleanaccuracy(cid:15)=1.5/2550.00.51.01.52.02.53.0β(/255)0%5%10%15%20%RobustaccuracyUnder review as a conference paper at ICLR 2023\n\nFigure 12: Clean and robust performance after FGSM-AT on injected datasets (cid:101) D\nCIFAR-10 using random signals in injected the dataset with the DCT basis vectors did induce CO. In the random the same behaviour, with CO happening when β areas report minimum and maximum values.\n\nβ constructed from . We perform this experiments at ε = 6/255 where we saw that setting, we observe ε. Results are averaged over 3 seeds and shaded\n\n≈\n\nV\n\nV\n\nβ constructed from Figure 13: Clean and robust performance after FGSM-AT on injected datasets (cid:101) D\nCIFAR-10 when training a WideResNet28x10. We perform this experiments at ε = 4/255. Results are averaged over 3 seeds and shaded areas report minimum and maximum values.\n\ninjected features as these would not be very helpful. (ii) When β would have a very large value then the network would only look at these features since they would be easy-to-learn and provide enough margin to classify robustly. (iii) Finally, there was a middle range of β usually when β ε where the injected features would be strongly discriminative but not enough to provide robustness on their own. This regime is where we observe CO.\n\n∼\n\nβ) and evaluate them on three test sets: (i) The injected test set ( (cid:101) D\n\nIn this section we present an extension of Fig. 3 where we take FGSM trained models on the injected datasets ( (cid:101) β) with the same features D\n) where the images are unmodified. (iii) The shuffled as the training set. (ii) The original dataset ( dataset ( (cid:101) same but the class assignments are shuffled. Therefore, the injected features will provide conflicting information with the features present on the original image.\n\nDπ(β)) where the injected features are permuted. That is, the set of injected features is the\n\nD\n\nIn Fig. 14 we show the performance on the aforementioned datasets for three different values of β. For β = 2/255 we are in regime (i) : we observe that the tree datasets have the same performance, i.e. the information of the injected features does not seem to alter the result. Therefore, we can conclude . When β = 20/255 we are in the network is mainly using the features from the original dataset regime (ii) : the clean and robust performance of the network is almost perfect on the injected test set β while it is close to 0% (note this is worse than random classifier) for the shuffled dataset. So when (cid:101) D\nthe injected and original features present conflicting information the network aligns with the injected features. Moreover, the performance on the original dataset is also very low. Therefore, the network is mainly using the injected features. Lastly, β = 8/255 corresponds to regime (iii) : as discussed in Section 4.1, in this regime the network initially learns to combine information from both the original\n\nD\n\n20\n\n024681012β(/255)40%60%80%100%Cleanaccuracy(cid:15)=6/255024681012β(/255)0%20%40%60%80%100%Robustaccuracy024681012β(/255)88%90%92%95%98%100%Cleanaccuracy(cid:15)=4/255024681012β(/255)0%20%40%60%80%RobustaccuracyUnder review as a conference paper at ICLR 2023\n\nand injected features. However, after CO, the network seems to focus only on the injected features and discards the information from the original features.\n\nFigure 14: Clean (top) and robust (bottom) accuracy of FGSM-AT on (cid:101) D\ndifferent test sets: (i) the original CIFAR-10 ( the dataset with shuffled injected features (cid:101) Center: β = 8/255 Right: β = 20/255.\n\nβ at different β values on 3 β and (iii) Dπ(β). All training runs use ε = 6/255. Left: β = 2/255\n\n), (ii) the dataset with injected features (cid:101) D\n\nD\n\nF ANALYSIS OF CURVATURE IN DIFFERENT SETTINGS\n\n⊥\n\nIn Fig. 4 (left) we track the curvature of the loss surface while training on different injected datasets with either PGD-AT or FGSM-AT. We observe that (i) Curvature rapidly increases both for PGD-AT and FGSM-AT during the initial epochs of training. (ii) In those runs that presented CO, the curvature explodes around the 8th epoch along with the training accuracy. (iii) When training with the dataset with orthogonally injected features ( (cid:101) β ) the curvature does not increase. This is aligned with our D\nproposed mechanisms to induce CO whereby the network increases the curvature in order to combine different features to learn better representations. In this section we extend this analysis to the original CIFAR-10 dataset (as opposed to our injected datasets) and to different values of feature strength β on the injected dataset ( (cid:101) D\n\nβ). For details on how we estimate the curvature refer to Appendix C.\n\nIn Fig. 15 we show the curvature when training on the original CIFAR-10 dataset with ε = 8/255 (where CO happens for FGSM-AT). Similarly to our observations on the injected datasets, the curvature during FGSM-AT explodes along with the training accuracy while for PGD-AT the curvature increases at a very similar rate than FGSM-AT during the first epochs and later stabilizes. This indicates that our described mechanisms may as well apply to induce CO on natural image datasets.\n\nOn the other hand, Fig. 16 presents the curvature for different values of feature strength β on the β). We show three different values of β representative of the three regimes injected dataset ( (cid:101) D\ndiscussed in Appendix E. Recall that when β is small (β = 2/255) we observe that the model seems to focus only on CIFAR-10 features. Thus, we observe a curvature increase aligned with (CIFAR-10) feature combination. However, since for the chosen robustness radii ε = 6/255 there is no CO, we observe that the curvature increase remains stable. When β is quite large (β = 20/255) then the model largely ignores CIFAR-10 information and focuses on the easy-to-learn injected features. Since these features are already robust, there is no need to combine them and the curvature does not need to increase. In the middle range when CO happens (β = 8/255) we observe again the initial curvature increase and then curvature explosion.\n\nG ADVERSARIAL PERTURBATIONS BEFORE AND AFTER CO\n\nQualitative analysis In order to further understand the change in behaviour after CO we presented visualizations of the FGSM perturbations before and after CO in Fig. 2. We observed that while prior\n\n21\n\n01020300%50%100%CleanaccuracyFGSMtraining(β=2/255)01020300%50%100%FGSMtraining(β=8/255)01020300%50%100%FGSMtraining(β=20/255)0102030Epoch0%20%40%60%Robustaccuracy0102030Epoch0%20%40%60%Injected( ̃Dβ)CIFAR10(D)Shuffled( ̃Dπ(β))0102030Epoch0%50%100%Under review as a conference paper at ICLR 2023\n\nFigure 15: Evolution of curvature and training attack accuracy of FGSM-AT and PGD-AT trained on the original CIFAR-10 with ε = 8/255. When CO happens the curvature explodes.\n\nFigure 16: Evolution of curvature and training attack accuracy of FGSM-AT and PGD-AT trained on β at different β and for ε = 6/255. Only when CO happens (for β = 8/255) the curvature explodes. (cid:101) D\nFor the other two interventions the curvature does not increase so much. We argue this is because the network does not need to disentangle\n\n, as it ignores either one of them.\n\nfrom\n\nD\n\nV\n\nto CO, the injected feature components v(y) were clearly identifiable, after CO the perturbations do not seem to point in those directions although the network is strongly relying on them to classify. In Fig. 17 and Fig. 18 we show further visualizations of the perturbations obtained both with FGSM or PGD attacks on networks trained with either PGD-AT or FGSM-AT respectively.\n\nWe observe that when training with PGD-AT, i.e. the training does not suffer from CO, both PGD and FGSM attacks produce qualitatively similar results. In particular, all attacks seem to target the injected features with some noise due to the interaction with the features from CIFAR-10. For FGSM-AT, we observe that at the initial epochs (prior to CO) the pertubations are similar to those of PGD-AT, however, after CO perturbations change dramatically both for FGSM and PGD attacks. This aligns with the fact that the loss landscape of the network has dramatically changed, becoming strongly non-linear. This change yields single-step FGSM ineffective, however, the network remains vulnerable and multi-step attacks such as PGD are still able to find adversarial examples, which in this case do not point in the direction of discriminative features Jetley et al. (2018); Ilyas et al. (2019).\n\nQuantitative analysis Finally, to quantify the radical change of direction of the adversarial perturbations after CO, we compute the evolution of the average alignment (i.e., cosine angle) between the FGSM perturbations δ and the injected features, such that if point x is associated with class y we compute . Figure 19 (left) shows the results of this evaluation, where we can see that before CO there is a non-trivial alignment between the FGSM perturbations and their corresponding injected features, that after CO quickly converges to the same alignment as the one between two random vectors.\n\n⟨δ,v(y)⟩ ∥v(y)∥2∥δ∥2\n\nTo complement this view, we also perform an analysis of the frequency spectrum of the FGSM perturbations. In Fig. 19 (right), we plot the average magnitude of the DCT transform of the FGSM perturbations computed on the test set of an intervened version of CIFAR-10 during training. As we can see, prior to CO, most of the energy of the perturbations is concentrated around the low frequencies (remember that the injected features are low frequency), but after CO happens, around epoch 8, the\n\n22\n\n051015202530Epoch10−210−1100101CurvatureFGSMtrainingPGD-10training051015202530Epoch20%40%60%80%100%Trainingattackaccuracy051015202530Epoch10−210−1100101Curvatureβ=2/255β=8/255β=20/255051015202530Epoch20%40%60%80%100%TrainingattackaccuracyUnder review as a conference paper at ICLR 2023\n\n(a) FGSM adversarial perturbations\n\n(b) PGD adversarial perturbations\n\nFigure 17: Different samples of the injected dataset (cid:101) D\n22 of PGD-AT on (cid:101) D\nperturbations remain qualitatively similar throughout training and align significantly with\n\nβ, and adversarial perturbations at epoch 4 and β at ε = 6/255 and β = 8/255 (where FGSM-AT suffers CO). The adversarial\n\n.\n\nV\n\n(a) FGSM adversarial perturbations\n\n(b) PGD adversarial perturbations\n\nβ, and adversarial perturbations at epoch 4 Figure 18: Different samples of the injected dataset (cid:101) D\nβ at ε = 6/255 and β = 8/255 (where FGSM-AT (before CO) and 22 (after CO) of FGSM-AT on (cid:101) D\nsuffers CO). The adversarial perturbations change completely before and after CO. Prior to CO, they align significantly with\n\n, but after CO they point to meaningless directions.\n\nV\n\nenergy of the perturbations quickly gets concentrated towards higher frequencies. These two plots corroborate, quantitatively, our previous observations, that before CO, FGSM perturbations are pointing towards meaningful predictive features, while after CO, although we know the network still uses the injected features (see Fig. 3) the FGSM perturbations suddenly point in a different direction.\n\nH FURTHER RESULTS WITH N-FGSM, GRADALIGN AND PGD\n\nIn Section 5 we studied different SOTA methods that have been shown to prevent CO. Interestingly, we observed that in order to avoid CO on the injected dataset a stronger level of regularization is needed. Thus, indicating that the intervention is strongly favouring the mechanisms that lead to CO. For completeness, in Fig. 20 we also present results of the clean accuracy (again with the robust accuracy). As expected, for those runs in which we observe CO, clean accuracy quickly saturates. Note that for stronger levels of regularization the clean accuracy is lower. An ablation of the regularization strength might help improve results further, however the purpose of this analysis is not to improve the performance on the injected dataset but rather to show it is indeed possible to prevent CO with the same methods that work for unmodified datasets.\n\nI FURTHER DETAILS OF LOW-PASS EXPERIMENT\n\nWe expand here over the results in Section 5 and provide further details on the experimental settings of Table 1. Specifically, we replicate the same experiment, i.e., training a low-pass version of CIFAR-10\n\n23\n\nIntervenedsamples(x,y)∼DβδFGSMatepoch4δFGSMatepoch25Intervenedsamples(x,y)∼DβδPGDatepoch4δPGDatepoch25Intervenedsamples(x,y)∼DβδFGSMatepoch4δFGSMatepoch25Intervenedsamples(x,y)∼DβδPGDatepoch4δPGDatepoch25Under review as a conference paper at ICLR 2023\n\nFigure 19: Quantitative analysis of the directionality of the FGSM perturbations in the test set during FGSM-AT before and after CO when training on the injected CIFAR-10 with β = 8/255 and ε = 6/255. (Left) Evolution of alignment of FGSM perturbations with their corresponding injected features during training. The red dotted line shows the expected value of alignment between two random vectors of the same dimensionality as CIFAR-10 images. (Right) Evolution of the average magnitude of the DCT spectrum of the same FGSM perturbations during training. The plot shows only the diagonal components of the DCT at every epoch.\n\nFigure 20: Clean (left) and robust (right) accuracy after AT with PGD-10, GradAlign and N-FGSM β at ε = 6/255. Results averaged over three random seeds and shaded areas report minimum and on (cid:101) D\nmaximum values.\n\nusing FGSM-AT at different filtering bandwidths. As indicated in Section 5 we use the low-pass filter introduced in Ortiz-Jimenez et al. (2020b) which only retains the frequency components in the upper left quadrant of the DCT transform of an image. That is, a low-pass filter of bandwidth W would retain the W W upper quadrant of DCT coefficients of all images, setting the rest of the coefficients to 0.\n\n×\n\nFigure 21 shows the robust accuracy obtained by FGSM-AT on CIFAR-10 versions that have been pre-filtered using such a low-pass filter. Interestingly, while training on the unfiltered images does induce CO on FGSM-AT, just removing a few high-frequency components is enough to prevent CO ε = 8255. However, as described before, it seems that at ε = 16/255 no frequency transformation can avoid CO. Clearly, this transformation cannot be used as technique to prevent CO, but it highlights once again that the structure of the data plays a significant role in inducing CO.\n\nRelation with anti-aliasing pooling layers As mentioned in Section 5, our proposed low-passing technique is very similar in spirit to works which propose using anti-aliasing low-pass filters at all pooling layers (Grabinski et al., 2022; Zhang, 2019). Indeed, as shown by Ortiz-Jimenez et al. (2020b), CIFAR10 contains a significant amount of non-robust features on the high-frequency end of the spectrum due to aliasing produced in their down-sampling process. In this regard, it is no surprise that methods like the one proposed in Grabinski et al. (2022) can prevent CO at ε = 8/255 using the same training protocol as in our work (robust accuracy is 45.9%). Interestingly, though, repeating the experiments in Grabinski et al. (2022) work using ε = 16/255 does lead to CO (robust accuracy is 0.0%) in Section 5. This result was not reported in the original paper, but we see it as a corroboration of our observations. Indeed, features play a role in CO, but the problematic features\n\n24\n\n051015202530Epoch0.020.040.06Alignmentof±andinjectedfeaturesFGSMattacksRandombaseline051015202530Epoch051015202530DCTfreq.Evolutionofperturbationspectrum°30°20°10Magnitude(dB)COCO02468101214β(/255)0.70.80.91.0CleanaccuracyPGD-10GradAlign(λ=0.112)GradAlign(λ=2)N-FGSM(k=10(cid:15))N-FGSM(k=2(cid:15))02468101214β(/255)0%20%40%60%80%RobustaccuracyUnder review as a conference paper at ICLR 2023\n\nFigure 21: Robust accuracy of FGSM-AT and PGD-AT on different low-passed versions of CIFAR-10 using the DCT-based low pass filter introduced in Ortiz-Jimenez et al. (2020b). Bandwidth = 32 corresponds to the original CIFAR-10, while smaller bandwidths remove more and more highfrequency components. At ε = 8/255 just removing a few high-frequency components is enough to prevent CO, while at ε = 16/255 no frequency transformation avoids CO.\n\ndo not always come from excessively high-frequencies or aliasing. However, we still consider that preventing aliasing in the downsampling layers is a promising avenue for future work in adversarial robustness.\n\n25\n\n1015202530Bandwidth0%10%20%30%40%50%RobustaccuracyFGSM-AT((cid:15)=8/255)FGSM-AT((cid:15)=16/255)PGD-AT((cid:15)=8/255)PGD-AT((cid:15)=16/255)",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors investigate the phenomenon of catastrophic overfitting (CO) which occurs during FGSM adversarial training (AT). CO describes the “overfitting” of the model to simple adversaries (FGSM) during a certain amount of training epochs while losing robustness against more complex adversaries like PGD.  \n\nThe authors show empirically:\n\n1. that it is possible to induce catastrophic overfitting on smaller \\epsilon values (where \\epsilon is the hyperparameter to determine the strength of the adversarial attack) than before by adding simple discriminative features to the dataset. These injected features are added onto the original image scaled with a hyperparameter $\\beta$\n2. if $\\beta << \\epsilon$ the model behaves normal \n3. if $\\beta >> \\epsilon$ the robust accuracy is high as the model can rely on the simple features\n4. if $\\beta \\approx \\epsilon$ they can induce CO at \\epsilon values at which the model on the clean data would not suffer from CO\n\nThe part where $\\beta \\geq \\epsilon$ is further exploited to analyse the network's training behavior, especially at the point of CO. Further, the authors empirically show:\n\n1. that standard training and FGSM training after CO highly rely on the simple features while PGD training forces the model to learn more robust features\n2. that the curvature of the model’s loss landscape explodes after CO  \n\nThe authors show empirically that models favour easy-to-learn features when no robustness constraints are considered. Further, they claim that a network that needs to learn different features needs to increase its non-linearity and thus opens a shortcut to breaking FGSM with CO.\n\nFurther, the authors analyse two methods which can prevent CO (GradAlign and N-FGSM). They empirically show that these methods can keep low curvature of the loss landscape thus preventing CO. Additionally, the authors try to use a lowpass filter on top of the data but could not prevent CO for large $\\epsilon$ values.\n\n# Strength And Weaknesses\n\n**Strength:**\n- The paper is well-written and supported by adequate graphics to demonstrate the author's findings.\n- The authors provide an empirical analysis of the learning preferences of CNNs under different training settings (standard training, FGSM AT and PGD AT)\n- The approach of injecting simple features for the analysis of CO is interesting and can be extended for future explainability studies on CNNs\n\n**Weakness:**\n\n- Prior work/extensiveness: the authors investigated prior work that prevents CO, however, missed prior work which includes additional insights into why CO happens:                                   \n[0] https://arxiv.org/abs/2204.00491 (ECCV 2022): showing that CO is correlated with a vast increase in aliasing and preventing CO by using an aliasing-free downsampling layer\n\n- The authors conducted their empirical study only on low-resolution datasets and for small networks. It would be interesting to see if their approach also applies to high-resolution data and bigger networks.\n- The authors did not provide any code for reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- **Clarity and Quality:** The paper is well-written and supported by adequate graphics to demonstrate the author's findings.\n-  **Novelty:** The authors missed one prior work which shows a different perspective on CO. Thus, I would suggest the authors to include this method in their analysis part as well as the insights on CO in their discussion.\n- **Reproducibility:** The code for reproducing the results of the paper should be publicly available to encourage future research on the explainability of CNNs.\n\n# Summary Of The Review\n\nThe contribution of this paper is a simple, yet interesting empirical analysis of CO. However, the authors missed a new perspective from previous work. Thus, I would like to encourage the authors to include this perspective in their analysis. Further, it would be interesting to see if the presented results hold for high-resolution datasets and bigger networks. \nAdditionally, the authors should provide their code for reproducibility and enablement of future research on the analysis of the explainability of CNNs.\n\nGenerally, the suggested idea and findings are interesting and worse publishing. However, I can not accept the paper for now due to my concerns mentioned above. I highly encourage the authors to address these concerns such that I can increase my score.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTIERED PRUNING FOR EFFICIENT DIFFERENTIABLE INFERENCE-AWARE NEURAL ARCHITECTURE SEARCH\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe propose three novel pruning techniques to improve the cost and results of Inference-Aware Differentiable Neural Architecture Search (DNAS). First, we introduce Prunode, a stochastic bi-path building block for DNAS, which can search over inner hidden dimensions with O(1) memory and compute complexity. Second, we present an algorithm for pruning blocks within a stochastic layer of the SuperNet during the search. Third, we describe a novel technique for pruning unnecessary stochastic layers during the search. The optimized models resulting from the search are called PRUNET and establishes a new state-of-the-art Pareto frontier for NVIDIA V100 in terms of inference latency for ImageNet Top-1 image classification accuracy. PRUNET as a backbone also outperforms GPUNet and EfficientNet on the COCO object detection task on inference latency relative to mean Average Precision (mAP).\n\n1\n\nINTRODUCTION\n\nNeural Architecture Search (NAS) is a wellestablished technique in Deep Learning (DL); conceptually it is comprised of a search space of permissible neural architectures, a search strategy to sample architectures from this space, and an evaluation method to assess the performance of the selected architectures. Because of practical reasons, Inference-Aware Neural Architecture Search is the cornerstone of the modern Deep Learning application deployment process. Wang et al. (2022); Wu et al. (2019); Yang et al. (2018) use NAS to directly optimize inference specific metrics (e.g., latency) on targeted devices instead of limiting the model’s FLOPs or other proxies. Inference-Aware NAS streamlines the development-to-deployment process. New concepts in NAS succeed with the evergrowing search space, increasing the dimensionality and complexity of the problem. Balancing the search-cost and quality of the search hence is essential for employing NAS in practice.\n\nFigure 1: PRUNET establishes a new state-of-theart Pareto frontier in terms of inference latency for ImageNet Top-1 image classification accuracy.\n\nTraditional NAS methods require evaluating many candidate networks to find optimized ones with respect to the desired metric. This approach can be successfully applied to simple problems like CIFAR-10 Krizhevsky et al. (2010), but for more demanding problems, these methods may turn out to be computationally prohibitive. To minimize this computational cost, recent research has focused on partial training Falkner et al. (2018); Li et al. (2020a); Luo et al. (2018), performing network morphism Cai et al. (2018a); Jin et al. (2019); Molchanov et al. (2021) instead of training from scratch, or training many candidates at the same time by sharing the weights Pham et al. (2018). These approaches can save computational time, but their reliability is questionable Bender et al. (2018); Xiang et al. (2021); Yu et al. (2021); Liang et al. (2019); Chen et al. (2019); Zela et al. (2019), i.e., the final result can still be improved. In our experiments, we focus on a search space based on a state-of-the-art network to showcase the value of our methodology. We aim to revise the\n\n1\n\n0123456TensorRT FP16 bs=1 Latency (ms)7879808182838485Top-1, ImageNetPRUNet distilledGPUNet distilledGPUNetD distilledResNetD distilledPRUNetGPUNetResNetOFARegNetXRegNetYEfficientNet-V1EfficientNet-V2EfficientNet-XFBNet V3Under review as a conference paper at ICLR 2023\n\nweight-sharing approach to save resources and improve the method’s reliability by introducing novel pruning techniques described below.\n\nPrunode: pruning internal structure of the block In the classical SuperNet approach, search space is defined by the initial SuperNet architecture. That means GPU memory capacity significantly limits search space size. In many practical use cases, one would limit themselves to just a few candidates per block. For example, FBNet Wu et al. (2019) defined nine candidates: skip connection and 8 Inverted Residual Blocks (IRB) with expansion, kernel, group ∈ {(1, 3, 1), (1, 3, 2), (1, 5, 1), (1, 5, 2), (3, 3, 1), (3, 5, 1), (6, 3, 1), (6, 5, 1)}. In particular, one can see that the expansion parameter was limited to only three options: 1, 3, and 6, while more options could be considered - not only larger values but also denser sampling using non-integer values. Each additional parameter increases memory and compute costs, while only promising ones can improve the search. Selecting right parameters for a search space requires domain knowledge. To solve this problem, we introduce a special multi-block called Prunode, which optimizes the value of parameters, such as the expansion parameter in the IRB block. The computation and memory cost of Prunode is equal to the cost of calculating two candidates. Essentially, the Prunode in each iteration emulates just two candidates, each with a different number of channels in the internal structure. These candidates are modified based on the current architecture weights. The procedure encourages convergence towards an optimal number of channels.\n\nPruning blocks within a stochastic layer In the classical SuperNet approach, all candidates are trained together throughout the search procedure, but ultimately, one or a few candidates are sampled as a result of the search. Hence, large amounts of resources are devoted to training blocks that are ultimately unused. Moreover, since they are trained together, results can be biased due to co-adaptation among operations Bender et al. (2018). We introduce progressive SuperNet pruning based on trained architecture weights to address this problem. This methodology removes blocks from the search space when the likelihood of the block being sampled is below a linearly changing threshold. Reduction of the size of the search space saves unnecessary computation cost and reduces the co-adoption among operations.\n\nPruning unnecessary stochastic layers By default, layer-wise SuperNet approaches force all networks that can be sampled from the search space to have the same number of layers, which is very limiting. That is why it is common to use a skip connection as an alternative to residual blocks in order to mimic shallower networks. Unfortunately, skip connections blocks’ output provides biased information when averaged with the outputs of other blocks. Because of this, SuperNet may tend to sample networks that are shallower than optimal. To solve this problem, we provide a novel method for skipping whole layers in a SuperNet. It introduces the skip connection to the SuperNet during the procedure. Because of that, the skip connection is not present in the search space at the beginning of the search. Once the skip connection is added to the search space, the outputs of the remaining blocks are multiplied by coefficients.\n\n2 RELATED WORKS\n\nIn NAS literature, a widely known SuperNet approach Liu et al. (2018b); Wu et al. (2019) constructs a stochastic network. At the end of the architecture search, the final non-stochastic network is sampled from the SuperNet using differentiable architecture parameters. The PRUNET algorithm utilizes this scheme – it is based on the weight-sharing design Cai et al. (2019); Wan et al. (2020) and it relies on the Gumbel-Softmax distribution Jang et al. (2016).\n\nThe PRUNET algorithm is agnostic to one-shot NAS Liu et al. (2018b); Pham et al. (2018); Wu et al. (2019) where only one SuperNet is trained or few-shot NAS Zhao et al. (2021) where multiple SuperNets were trained to improve the accuracy. In this work, we evaluate our method on search space based on the state-of-the-art GPUNet model Wang et al. (2022). We follow its structure including the number of channels, the number of layers, and the basic block types.\n\nOther methods that incorporate NAS Dai et al. (2019); Dong et al. (2018); Tan et al. (2019) but remain computationally expensive. Differentiable NAS Cai et al. (2018b); Vahdat et al. (2020); Wu et al. (2019) significantly reduces the training cost. MobileNets Howard et al. (2017); Sandler et al. (2018) started to discuss the importance of model size and latency on embedded systems while\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ninference-aware pruning Cai et al. (2018b); Howard et al. (2017); Sandler et al. (2018); Wang et al. (2022) is usually focused on compressing fixed architectures Chen et al. (2018); Shen et al. (2021); Yang et al. (2018). Molchanov et al. (2021) further combines layer-wise knowledge-distillation (KD) with NAS to speedup the validation.\n\nDNAS and weight-sharing approaches have two major opposing problems, which we address in this article: memory costs that bound the search space and co-adaptation among operation problem for large search spaces. We also compare our methodology to other similar SuperNet pruning techniques.\n\nMemory cost The best-known work on the memory cost problem is FBNetV2 Wan et al. (2020). The authors use different kinds of masking to mimic many candidates with O(1) memory and compute complexity. Unfortunately, masking removes information from the feature maps, which results in bias after averaging with informative feature maps (see Section 3.2.3). Therefore, it may lead to suboptimal sampling. FBNetV2 can extend the search space even 1014 times through this application. Unfortunately, such a large expansion may increase the impact of the co-adaptation among operations problem. Moreover, it can render block benchmarking computationally expensive.\n\nCo-adaptation among operations The weight-sharing approach forces all the operations to be used in multiple contexts, losing the ability to evaluate them individually. This problem, named co-adaptation among operations, posterior fading, or curse of skip connect was noticed by many researchers Bender et al. (2018); Li et al. (2020b); Zhao et al. (2021); Ding et al. (2022). Progressive search space shrinking Li et al. (2020b); Liu et al. (2018a) in different forms was applied to speed up and improve Neural Architecture Search.\n\nSuperNet pruning Ideas similar to our ”Pruning blocks within a stochastic layer” modification (see Section 3.2.2) has already been considered in literature. Xia et al. (2022) removes blocks from the search space which are rarely chosen. In Ci et al. (2021), the authors efficiently search over a large search space with multiple SuperNet trainings. In contrast to both methods, our approach requires a single training of a SuperNet, which makes the procedure much faster. Ding et al. (2022) uses DARTs Liu et al. (2018b) to prune operations in a cell. It assesses the importance of each operation and all operations but the two strongest are removed at the end of the training. In contrast, our methodology removes blocks from the SuperNet during the training, reducing the training time.\n\n3 METHODOLOGY\n\nOur method can be applied both to layer-wise and cell-wise searches, cf. Bender et al. (2018); Liu et al. (2018b); Mei et al. (2019); Xie et al. (2018). Cell-wise search aims to find a cell (building block), which is then repeated to build the whole network. However, as stated in Lin et al. (2020), different building blocks might be optimal in different network parts. That is why we focus on layer-wise search, i.e., each layer can use a different building block.\n\nThe SuperNet is assumed to have a layered structure. Each layer is built from high-level blocks, e.g., IRB in computer vision or self-attention blocks in natural language processing. Only one block in each layer is selected in the final architecture. Residual layers can be replaced by skip connections during the search, effectively making the network shallower.\n\n3.1 SEARCH SPACE\n\nSince layer-wise SuperNets must have a predetermined number of output channels for each layer, selecting the right numbers is critical. Thus, it is worth getting inspired by a good baseline model and defining the search space such that models similar to the baseline model can be sampled from the SuperNet. With a search space defined this way, we can expect to find models uniformly better than the baseline or models with different properties, e.g., optimized accuracy for target latency or optimized latency for a target accuracy. In our main experiments, we chose GPUNet-1 Wang et al. (2022) as a baseline network. In Table 1 we present the structure of our SuperNet defining the search space. Since we sample all possible expansions with channel granularity set to 32 (i.e., the number of channels is forced to be a multiple of 32) and skip connections are considered for all residual layers, our search space is covering 32 ∗ 24 ∗ 65 ∗ 64 ∗ 972 ∗ 96 ∗ 161 ∗ 160 ∗ 2894 ∗ 288 ∗ 4494 ≈ 1.7e39\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: PRUNET search space inspired by GPUNet-1\n\nStage\n\nType\n\nStride Kernel\n\n# Layers Activation\n\nExpansion\n\nFilters\n\nSE\n\n0 1\n\n2 3\n\n4 5\n6\n\n7\n\nConv Conv\n\nFused-IRB Fused-IRB\n\nIRB IRB IRB\n\nConv + Pool + FC\n\n2 1\n\n2 2\n\n2 1\n2\n\n1\n\n3 {3,5}\n\n{3,5} {3,5}\n\n{3,5} {3,5} {3,5}\n\n1\n\n1 2\n\n3 3\n\n2 5\n6\n\n1\n\nSwish RELU\n\nSwish Swish\n\nSwish RELU RELU\n\nRELU\n\n(0, 8] (0, 8]\n\n(0, 8] (0, 8] (0, 8]\n\n24 24\n\n64 96\n\n160 288 448\n\n1280\n\n{0,1} {0,1}\n\n{0,1} {0,1} {0,1}\n\ncandidates. For comparison, similar FBNet SuperNet, which would consume the same amount of GPU memory, would cover only 22 ∗ 817 ≈ 9e15 candidates. So, in this case, our methodology can cover ≈ 1.9e23 times more options with the same GPU memory consumption.\n\n3.2 SEARCH METHOD\n\nOur search algorithm is similar to Wu et al. (2019). We focus on multi-objective optimization\n\nmin ψ\n\nmin θ\n\nL(ψ, θ),\n\n(1)\n\nwhere ψ are the network weights and θ are the architecture weights. The goal is to minimize the following inference-aware loss function L\n\nL(ψ, θ) = CE(ψ, θ) + α log(LAT(θ))β,\n\n(2)\n\nwhere CE(ψ, θ) is the standard cross-entropy loss, and LAT(θ) is the latency of the network. Coefficient α defines the trade-off between accuracy and latency. Higher α results in finding networks with lower latency; lower α results in finding networks with higher accuracy. Coefficient β scales the magnitude of the latency. The loss function is inference-aware, meaning we optimize the latency of the networks for specific hardware.\n\nWe train the SuperNet using continuous relaxation. Output of l-th layer is equal to\n\nxl+1 =\n\n(cid:88)\n\ni\n\nal,i · Bl,i(xl),\n\n(3)\n\nwhere xl is the output of the layer (l − 1), Bl,i(xl) represents the output of the i-th block of the l-th layer. Every block in a layer is assumed to have the same input and output tensor shapes. The coefficients al,i come from the Gumbel-Softmax distributionJang et al. (2016)\n\nal,i = Gumbel-Softmax(θl,i|θl) =\n\nexp[(θl,i + gl,i)/τ ] j exp[(θl,i + gl,i)/τ ]\n\n(cid:80)\n\n,\n\n(4)\n\nwhere θl,i is the architecture weight of the block. gl,i is sampled from Gumbel(0, 1). Parameter τ controls the temperature of the Gumbel-Softmax function. In contrast to Wu et al. (2019), we set τ to have a constant value throughout the training. Architecture weights θ are differentiable and trained using gradient descent alongside ψ weights.\n\nThe total latency LAT(θ) is the sum of latencies of all layers in the SuperNet. Similarly, the latency of a layer is a sum of latencies of its blocks weighted by Gumbel-Softmax coefficients\n\nLAT(θ) =\n\n(cid:88)\n\n(cid:88)\n\nl\n\ni\n\nal,i · LAT(Bl,i).\n\n(5)\n\nOur method allows choosing the target device for inference. On chosen hardware, we pre-compute and store in the lookup table the latency LAT(Bl,i) for every permissible block, the same way as in Wu et al. (2019). Then the search does not need to compute any further latencies and can be run on arbitrary hardware (e.g., for embedded target hardware used in autonomous vehicles, one could still train the network on a supercomputer).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 2: (a) SuperNet trimming procedure: For the first ewarmup epochs, only the regular weights are being trained, and the architecture weights are frozen. After the warmup phase, the threshold increases linearly. If an architecture weight of a block is below the current threshold, it is removed from the layer. If the penultimate is to be removed from the layer, it is replaced by a skip connection block instead. The output of this skip connection block is multiplied by φ, and the output of the other block is multiplied by λ as stated in Section 3.2.3. At the end of the training, every layer contains exactly one block; thus, the result of the training is a non-stochastic network. (b) Procedure for channel pruning in case of an IRB: The inner tensor in IRB has the shape of eC × W × H, where C is the input number of channels of the block, e is the maximal expansion ratio, W and H are the width and the height of the feature map. Small candidate uses seC channels, and large candidate uses leC channels, where s, l ∈ (0; 1] and s < l. The optimal candidate that we try to find (marked with the blue dashed line) uses oC channels, and it is assumed that s ≤ o ≤ l. Both candidates mask out unused channels. The weights are shared between the candidates. The proportion of channels both candidates use, denoted by s and l, dynamically changes throughout the training.\n\nThe training is conducted on a smaller proxy dataset to make the SuperNet training faster. We empirically confirmed that the performance of the found architecture on the proxy problem correlates with the performance on the original problem. Once the final architecture is obtained from the SuperNet, it is evaluated and re-trained from scratch on the full dataset.\n\n3.2.1 PRUNING INTERNAL STRUCTURE OF A BLOCK\n\nWe use a particular procedure for finding the final values of discrete inner hidden dimensions of a block. It is required that the impact of discrete parameters on the objective function (in our case, a function of latency and accuracy) is predictable, e.g., small parameter values mean a negative impact on accuracy but a positive impact on latency, and large parameter values mean a positive impact on accuracy but negative impact on latency. We also require it to be regular – the impacts described above are monotonic with regard to the parameter value. A good example of such a parameter is the expansion ratio in the IRB.\n\nPrunode consists of two copies of the same block. Unlike in Wu et al. (2019), both blocks share the weights Pham et al. (2018), and the only difference between them is masking. In contrast to Wan et al. (2020), masks are not applied to the outputs of blocks but only to a hidden dimension. The larger mask is initialized with all 1s, thus using all the channels. The smaller mask is initialized in a way to mask out half of the channels. Prunodes are benchmarked with all possible inner hidden dimension values. The latency of a prunode is the average of the latencies of the smaller and the larger candidates weighted by Gumbel-Softmax coefficients.\n\nWe try to ensure that the optimal mask value o is between the candidates. More precisely, if a candidate with a larger mask l has a higher likelihood of being chosen, i.e., has a larger architecture weight compared to a candidate with a smaller mask s, then both masks should be expanded. In the\n\n5\n\nEPOCH0ewarmupTHRESHOLD0.150.250.350.450.20.40.50.10.40.40.20.10.20.50.50.60.60.40.7・λ・Φ・Φ・λeCWHCWHCWHSMALL MASKLARGE MASKseCleCUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: Prunode masking Constants c, max_distance, granularity, and momentum were set to 0.8, 0.6, 32, and 0.4 respectively.\n\n// Initialize architecture weight used by Gumbel-Softnax with 0\n\nweight ← 0.0; update ← 0.0; s ← 0.5; l ← 1.0; Procedure update_masks (progress)\n\nif (l − s) × max_channels > granularity then\n\n// Small mask initially masks out half of the channels // Large mask initially contains all of the channels // Called after each training iteration // Update masks until consecutive choices are reached // Momentum speeds up convergence update ← update × momentum + weight ; distance ← max_distance × (1 − progress)2; // Calculate distance between masks; progress ∈ [0, 1] // Update small mask s ← s + update; if s > 0 then\n\nweight ← 0; s ← min(s, 1 − c × distance);\n\n// Reset architecture weight if not a corner case // Prevent premature convergence\n\nelse\n\ns ← 0 ;\n\n// Make small mask non-negative\n\nend // Update large mask l ← s + distance; // Ensure divisibility by granularity small_mask ← round(s × max_channels, granularity); small_mask ← clip(small_mask, granularity, max_channels − granularity); // Ensure small_mask is in bounds // Ensure divisibility by granularity large_mask ← round(l × max_channels, granularity); large_mask ← clip(large_mask, small_mask + granularity, max_channels); // Ensure large_mask is in bounds create_masks(small_mask, large_mask);\n\nend\n\nopposite case, both masks should be reduced. The distance between masks d = l − s should decrease as the training progresses, and at the end of the search, the d should be close to zero. Due to the rules above, by the end of the search, we expect that the final values obtained are close to each other and should tend to an optimized solution. After the search, we sample a single candidate (from those two modified candidates), cf. Wu et al. (2019). Fig. 2b visualizes the masks of two candidates in the case of using IRB. The exact procedure of prunode masking is presented in Algorithm 1.\n\nOur routine extensively searches through discrete inner hidden dimension parameter values while keeping memory usage and computation costs low. Our proposed method has a computation cost and memory usage of O(1) with respect to the number of all possible values, as only two candidates are evaluated every time. Further, as we prune the not promising candidates, the SuperNet architecture tends to the sampled one. As a result, co-adaptation among operations is minimized.\n\n3.2.2 PRUNING BLOCKS WITHIN A STOCHASTIC LAYER\n\nEach block in layer l has its architecture weight initialized as 1/Nl, where Nl is the number of blocks in the layer l. Architecture weights are not modified for the first ewarmup epochs. After the warmup phase is finished, the SuperNet can be pruned, i.e., when a block in a layer has a probability of being chosen below a threshold, it is removed from the layer. This threshold changes linearly during the training and depends only on the value of the current epoch of the training. During epoch e > ewarmup the threshold t is equal to\n\nt(e) = tinitial + (tfinal − tinitial)\n\ne − ewarmup etotal − ewarmup\n\n.\n\n(6)\n\nThe initial threshold should not be higher than 1/max(Nl), where max(Nl) is the highest number of blocks in a layer, so no blocks are immediately removed after the warmup phase. The final threshold should not be lower than 0.5; thus, all blocks but one are removed from each layer during the training. An example of block pruning is presented in Fig. 2a.\n\n3.2.3 PRUNING UNNECESSARY STOCHASTIC LAYERS\n\nSome layers might have the same input and output tensor sizes. For such layers, it is possible to remove them completely to obtain a shorter network. Removing a layer is equivalent to choosing a skip connection block. However, adding a skip connection block to the search space increases memory consumption. Moreover, using this approach, we observed a bias towards finding shallower architectures than optimal. Instead of adding a skip connection block to the set of possible blocks of a layer, we introduce a GPU memory optimization. When the penultimate block is to be removed from\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: PRUNET networks: Image input resolution is set to 288x288 for all networks. Thick border represents block using Squeeze and Excitation. Darker shade of color represents kernel size = 5. Width is proportional to expansion ratio value. Each architecture is followed by Convolution 1x1 with 1280 output channels, RELU activation function, Adaptive Average Pool, and Linear layer.\n\na layer, it is replaced by a skip connection block. From that point, the output of the skip connection block is multiplied by φ, and the output of the other remaining block is multiplied by the parameter λ, where φ and λ are coefficients fixed for the whole training. Each layer uses the same values of these parameters. Once the skip connection block or the other block is removed from the layer, the output is no longer multiplied by any parameter. Selecting different λ and φ values allows to reduce the bias towards shallower networks and thus gives more control to find an even better architecture.\n\n4 EXPERIMENTS\n\nWe test our methodology on Imagenet-1k Deng et al. (2009) as there are many good networks that can potentially be tuned. In particular, recent GPUNet networks Wang et al. (2022) set up a SOTA Pareto frontier in terms of latency and inference accuracy for small networks. Their architecture scheme is perfect to showcase the value of our proposed method. The networks created during these experiments are called PRUNET, and Fig. 3 presents their structure. In this section, we delve into details of how we found PRUNET family and analyze the search cost. We also show that our networks transfer well to COCO object detection task.\n\n4.1\n\nIMAGE CLASSIFICATION ON IMAGENET\n\n4.1.1 FINDING PRUNET NETWORK FAMILY\n\nInspired by GPUNet-1 architecture, we define our SuperNet, as described in section 3.1 and Table 1, with an image resolution of 288x288, 2D convolution with kernel size of 3, stride of 2, and Swish Ramachandran et al. (2017) activation function as the prologue, and then define 6 stages followed by an epilogue. At each stage, we use the same type of building block (convolution, IRB, or Fused Inverted Residual Block denoted by Fused-IRB), activation function, and a number of channels as in GPUNet-1. Within these constraints in stages from 2 to 6, we define stochastic layers that consist of four multi-blocks (kernel size ∈ {3,5}, SE ∈ {True, False}), all with maximum expansion of 8 and granularity of 32 channels. SE stands for Squeeze and Excitation block, cf. Hu et al. (2018). For stage 1, we define only two choices – kernel size ∈ {3,5}. Each stage contains one additional layer\n\n7\n\nk3, s1k3, s1k3, e8, s2k3, e4, s1k3, e8, s2k3, e7, s1k3, e8, s1, SEk5, e8, s2k3, e7.2, s1k3, e8, s1, SEk3, e7.67, s1, SEk5, e8, s1k5, e6.78, s1k3, e5.56, s1k5, e8, s2, SEk5, e7.93, s1k5, e8, s1, SEk3, e8, s1k5, e7.93, s1, SEk3, s2PruNet6k3, s1k3, s1k5, e8, s2k3, e8, s2k3, e4.67, s1k5, e8, s2k3, e5.8, s1k3, e8, s1, SEk5, e6.56, s1k3, e6.78, s1k5, e4.78, s1k3, e4.11, s1k5, e8, s2k5, e8, s1, SEk5, e7.93, s1k5, e7.57, s1, SEk5, e8, s1k3, s2PruNet5Stage 1Conv RELU24 out channelsStage 2Fused-IRB Swish64 out channelsStage 3Fused-IRB Swish96 out channelsStage 4IRB Swish160 out channelsStage 5IRB RELU288 out channelsStage 6IRB RELU448 out channelsk3, s1k3, s1k3, e8, s2k3, e7, s2k3, e4.33 , s1k5, e8, s2k3, e4.4, s1k5, e8, s1k3, e5, s1k3, e4.67, s1k3, e4.56, s1k3, e8, s2k5, e6.43, s1k5, e7.64, s1k3, e5.71, s1k3, e5.93, s1k3, s2PruNet3k3, s1k3, s1k3, e8, s2k3, e7, s2k3, e8, s2k3, e8, s1k3, e5.44, s1k5, e4.44, s1k3, e4.22, s1k3, e8, s2k5, e5.57, s1k5, e5.57, s1k3, e5.36, s1k3, e5.43, s1k3, s2PruNet2k3, s1k3, s1k3, e8, s2k3, e6.5, s2k3, e8, s2k5, e8, s1k3, e4.22, s1k3, e8, s2k5, e5.71, s1k3, e5.71, s1k3, e5.57, s1k3, s2PruNet1k3, s1k3, s1k3, e6.67, s2k3, e6.5, s2k3, e8, s2k3, e8, s1k3, e7.33, s2k5, e5.5, s1k3, e5, s1k3, e5.36, s1k3, s2PruNet0k3, e4, s1k3, s1k3, e4, s2k3, e4, s1k3, e4, s2k3, e4, s1k3, e2s2, SEk3, e5, s1k3, e5, s1k3, e5, s1k3, e5, s1k3, e4, s2, SEk3, e4, s1, SEk3, e4, s1,SEk3, e4, s1,SEk3, s2GPUNet1Stage 0Conv Swish24 out channelstop1(distilled)V100 latency82.29(83.96)1.28ms81.89(83.44)1.07ms81.16(82.64)0.77ms80.68(82.19)0.66ms79.61(80.99)0.56ms78.41(80.01)0.48ms80.49(81.87)0.69msk3, s1k3, s1k3, e8, s2k3, e6.5, s2k3, e4.33, s1k5, e8, s2k3, e4.6, s1k3, e8, s1, SEk5, e5.56, s1k3, e5, s1k5, e4.56, s1k5, e7.78, s2k5, e8, s1, SEk5, e7.71, s1k3, e6.07, s1k5, e7.07, s1k3, s2PruNet481.45(83.13)0.91msk5, e7.71, s1, SEk5, e8, s1k5, e7.57, s1k5, e5.68, s1k5, e5.71, s1k5, e5.71, s1Under review as a conference paper at ICLR 2023\n\nthat can be skipped during the training compared to GPUNet-1. Thanks to such a defined SuperNet, we can sample GPUNet-1; hence the final result is expected to be improved.\n\nTo generate the entire Pareto frontier, we experiment with 7 different values of α ∈ {0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 2.0} . Parameter α is defined in equation 2 and changes the trade-off between latency and accuracy. Since proper layer skipping is crucial for finding a good network, and the last 130 epochs are relatively inexpensive due to the progressive pruning of the search space, we test 3 variants of layer skipping for each α. On the basis of preliminary experiments, we chose λ ∈ {0.4, 0.55, 0.85} and φ = 1.1. For each α value, we decided which λ value was the best based on the final loss from the SuperNet search. The architecture with the best loss was then trained from scratch. Table 2 compares the PRUNET results against other baselines. For all the considered networks of comparable accuracy, PRUNET has significantly lower latency. It is worth noting that it does not necessarily have a lower number of parameters or FLOPs. In particular, comparing PRUNETs and GPUNets (both among distilled and non-distilled networks), we observe that the obtained networks are uniformly better, meaning we get higher accuracy with lower latency.\n\n4.1.2 ABLATION STUDY\n\nEach of the optimizations has a different impact on search performance. Main advantage of the first one (3.2.1) is memory efficiency. Prunode allows significant increase of the size of the search space without increasing the demand for compute and memory resources. The second optimization (3.2.2) can save a lot of computational time without compromising the quality of the results. Last optimization (3.2.3) saves memory that would be needed for skip connection in standard FBNet approach and enhances the quality of the end results by reducing the impact of curse of skip connect. Table 3 visualizes the cost impact of each optimization in a hard memory limit scenario. Appendix B further investigates the impact on the quality of the results.\n\n4.2 OBJECT DETECTION ON COCO\n\nThe experiments were conducted on the COCO 2017 detection datasetLin et al. (2014). We chose EfficientDet Tan et al. (2020) as a baseline model. We replaced the original EfficientNet backbone with GPUNet and PRUNET for broad comparison. All backbones were pretrained without distillation. Figure 4 shows that our architectures can be successfully transferred to other Computer Vision tasks. PRUNET turned out to be faster and more accurate, similarly as it was on the image classification task.\n\n5 CONCLUSIONS\n\nFigure 4: PRUNET as a backbone outperforms GPUNet and EfficientNet on COCO object detection task on inference latency relative to mean Average Precision (mAP)\n\nWe introduced Prunode a stochastic bi-path building block that can be used to search the inner hidden dimension of blocks in any differentiable NAS with O(1) cost. Together with two novel layer pruning techniques, we show that our proposed inference-aware method establishes a new SOTA Pareto frontier (PRUNET) in TensorRT inference latency and ImageNet-1K top-1 accuracy and enables fine granularity sampling of the Pareto frontier to better fit external deployment constraints. Further, our results in Table 2 confirm that FLOP and the number of parameters are not the suitable proxies for latency, highlighting the importance of the correct metric for architectural evaluation. Although we only present results within computer vision tasks, our methods can be generalized to searching models for natural language processing (NLP), speech, and recommendation system tasks.\n\n8\n\n4.04.24.44.64.85.05.25.4TensorRT FP16 bs=1 Latency (ms)0.340.350.360.370.380.390.40mAP, MS COCO 2017EfficientDET with different backbonesPRUNetGPUNetEfficientNet-V1Under review as a conference paper at ICLR 2023\n\nTable 2: PRUNET image classification results. All latency measurements are made using batch size 1.\n\nPRUNET Without Distillation Comparison\n\nTop1 ImageNet\n\nTensorRT Latency\n\nFP16 V100 (ms)\n\n#Params (10e6)\n\n#FLOPS (10e9)\n\nPRUNET Speedup ↑\n\nPRUNET Accuracy ↑\n\nModels\n\nEfficientNet-B0 Tan and Le (2019) EfficientNetX-B0-GPU Li et al. (2021) REGNETY-1.6GF Radosavovic et al. (2020) PRUNET -0\n\nOFA 389 Cai et al. (2019) EfficientNetX-B1-GPU OFA 482 PRUNET -1\n\nFBNetV3-B Dai et al. (2020) EfficientNetX-B2-GPU OFA 595 EfficientNet-B2 ResNet-50 He et al. (2016) GPUNet-1 Wang et al. (2022) PRUNET -2\n\nREGNETY-32GF PRUNET -3\n\nEfficientNetX-B3-GPU PRUNET -4\n\nEfficientNet-B3 PRUNET -5\n\nResNet-101 He et al. (2016) FBNetV3-F GPUNet-2 Wang et al. (2022) PRUNET -6\n\n77.1 77.3 78 78.4\n\n79.1 79.4 79.6 79.6\n\n79.8 80.0 80.0 80.3 80.3 80.5 80.7\n\n81 81.2\n\n81.4 81.5\n\n81.6 81.9\n\n82.0 82.1 82.2 82.3\n\n0.94 0.96 2.55 0.48\n\n0.94 1.37 1.06 0.56\n\n1.5 1.46 1.09 1.65 1.1 0.69 0.66\n\n4.97 0.77\n\n1.9 0.91\n\n2.04 1.07\n\n1.7 1.97 1.57 1.28\n\n5.28 7.6 11.2 11.3\n\n8.4 9.6 9.1 14.8\n\n8.6. 10 9.1 9.2 28.09 12.7 18.6\n\n145 20.8\n\n13.3 24.0\n\n12 27.5\n\n45 13.9 25.8 31.1\n\nPRUNET With Distillation Comparison\n\nPRUNET -0 (distilled)\n\nGPUNet-0 (distilled) PRUNET -1 (distilled)\n\nGPUNet-1 (distilled) PRUNET -2 (distilled)\n\nPRUNET -3 (distilled)\n\nPRUNET -4 (distilled)\n\nPRUNET -5 (distilled)\n\nGPUNet-2 (distilled) EfficientNetV2-S Tan and Le (2021) PRUNET -6 (distilled)\n\n80.0\n\n80.7 81.0\n\n81.9 82.2\n\n82.6\n\n83.1\n\n83.4\n\n83.5 83.9 84.0\n\n0.48\n\n0.61 0.56\n\n0.69 0.66\n\n0.77\n\n0.91\n\n1.07\n\n1.57 2.67 1.28\n\n11.3\n\n11.9 14.8\n\n12.7 18.5\n\n20.8\n\n24.0\n\n27.5\n\n25.8 22 31.1\n\n0.38 0.91 1.6 2.10\n\n0.39 1.58 0.48 2.57\n\n0.46 2.3 0.6 1\n4. 3.3 3.31\n\n32.3 4.05\n\n4.3 4.25\n\n1.8 5.29\n\n7.6 1.18 8.38 7.36\n\n2.10\n\n3.25 2.57\n\n3.3 3.31\n\n4.05\n\n4.25\n\n5.29\n\n8.38 8.8 7.36\n\n1.96x 2x 5.31x\n\n1.68x 2.45x 1.89x\n\n2.27x 2.21x 1.65x 2.5x 1.67x 1.04x\n\n6.45x\n\n2.09x\n\n1.91x\n\n1.33x 1.54x 1.23x\n\n1.09x\n\n1.04x\n\n1.3 1.1 0.4\n\n0.5 0.2 0.0\n\n0.9 0.7 0.7 0.4 0.4 0.2\n\n0.2\n\n0.1\n\n0.3\n\n0.3 0.2 0.1\n\n0.3\n\n0.3\n\n1.23x 2.09x\n\n0.5 0.1\n\nTable 3: Cost analysis in the 80 GB memory limit scenario. Train time considers searching for all PRUNET architectures on a node with 8 NVIDIA Tesla A100 GPUs (DGX-A100). Section 3.1 contains the calculations of the search space sizes.\n\nFBNet\n\n1st opt (3.2.1)\n\n2nd opt (3.2.2)\n\n3rd opt (3.2.3) All opts\n\nMemory consumption [GB]\n\n80\n\n80\n\nSearch space size\n\nTrain time [h]\n\n9e15\n\n140\n\n7.1e38\n\n140\n\n80\n\n9e15\n\n62\n\n80\n\n1.5e17\n\n140\n\n80\n\n1.7e39\n\n62\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nG. Bender, P.-J. Kindermans, B. Zoph, V. Vasudevan, and Q. Le. Understanding and simplifying one-shot\n\narchitecture search. In International Conference on Machine Learning, pages 550–559. PMLR, 2018.\n\nH. Cai, J. Yang, W. Zhang, S. Han, and Y. Yu. Path-level network transformation for efficient architecture search.\n\nIn International Conference on Machine Learning, pages 678–687. PMLR, 2018a.\n\nH. Cai, L. Zhu, and S. Han. Proxylessnas: Direct neural architecture search on target task and hardware. arXiv\n\npreprint arXiv:1812.00332, 2018b.\n\nH. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han. Once-for-all: Train one network and specialize it for efficient\n\ndeployment. arXiv preprint arXiv:1908.09791, 2019.\n\nC. Chen, F. Tung, N. Vedula, and G. Mori. Constraint-aware deep neural network compression. In Proceedings\n\nof the European Conference on Computer Vision (ECCV), pages 400–415, 2018.\n\nX. Chen, L. Xie, J. Wu, and Q. Tian. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1294–1303, 2019.\n\nY. Ci, C. Lin, M. Sun, B. Chen, H. Zhang, and W. Ouyang. Evolving search space for neural architecture search. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6659–6669, 2021.\n\nE. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation policies\n\nfrom data. arXiv preprint arXiv:1805.09501, 2018.\n\nX. Dai, P. Zhang, B. Wu, H. Yin, F. Sun, Y. Wang, M. Dukhan, Y. Hu, Y. Wu, Y. Jia, et al. Chamnet: Towards efficient network design through platform-aware model adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11398–11407, 2019.\n\nX. Dai, A. Wan, P. Zhang, B. Wu, Z. He, Z. Wei, K. Chen, Y. Tian, M. Yu, P. Vajda, et al. Fbnetv3: Joint\n\narchitecture-recipe search using neural acquisition function. 2020.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.\n\nY. Ding, Y. Wu, C. Huang, S. Tang, F. Wu, Y. Yang, W. Zhu, and Y. Zhuang. Nap: Neural architecture search\n\nwith pruning. Neurocomputing, 477:85–95, 2022.\n\nJ.-D. Dong, A.-C. Cheng, D.-C. Juan, W. Wei, and M. Sun. Dpp-net: Device-aware progressive search for pareto-optimal neural architectures. In Proceedings of the European Conference on Computer Vision (ECCV), pages 517–531, 2018.\n\nS. Falkner, A. Klein, and F. Hutter. Bohb: Robust and efficient hyperparameter optimization at scale. In\n\nInternational Conference on Machine Learning, pages 1437–1446. PMLR, 2018.\n\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE\n\nconference on computer vision and pattern recognition, pages 770–778, 2016.\n\nG. Hinton, O. Vinyals, J. Dean, et al. Distilling the knowledge in a neural network. arXiv preprint\n\narXiv:1503.02531, 2(7), 2015.\n\nA. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. arXiv preprint\n\nMobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv:1704.04861, 2017.\n\nJ. Howard. Imagewang. URL https://github.com/fastai/imagenette/.\n\nJ. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on\n\ncomputer vision and pattern recognition, pages 7132–7141, 2018.\n\nE. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax.\n\narXiv preprint\n\narXiv:1611.01144, 2016.\n\nH. Jin, Q. Song, and X. Hu. Auto-keras: An efficient neural architecture search system. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1946–1956, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nA. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian institute for advanced research). URL http://www. cs.\n\ntoronto. edu/kriz/cifar. html, 5(4):1, 2010.\n\nG. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals. arXiv\n\npreprint arXiv:1605.07648, 2016.\n\nL. Li, K. Jamieson, A. Rostamizadeh, E. Gonina, J. Ben-Tzur, M. Hardt, B. Recht, and A. Talwalkar. A system for massively parallel hyperparameter tuning. Proceedings of Machine Learning and Systems, 2:230–246, 2020a.\n\nS. Li, M. Tan, R. Pang, A. Li, L. Cheng, Q. V. Le, and N. P. Jouppi. Searching for fast model families on In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n\ndatacenter accelerators. Recognition, pages 8085–8095, 2021.\n\nX. Li, C. Lin, C. Li, M. Sun, W. Wu, J. Yan, and W. Ouyang. Improving one-shot nas by suppressing the posterior fading. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 13836–13845, 2020b.\n\nH. Liang, S. Zhang, J. Sun, X. He, W. Huang, K. Zhuang, and Z. Li. Darts+: Improved differentiable architecture\n\nsearch with early stopping. arXiv preprint arXiv:1909.06035, 2019.\n\nM. Lin, H. Chen, X. Sun, Q. Qian, H. Li, and R. Jin. Neural architecture design for gpu-efficient networks.\n\narXiv preprint arXiv:2006.14090, 2020.\n\nT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014.\n\nC. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang, and K. Murphy. Progressive neural architecture search. In Proceedings of the European conference on computer vision (ECCV), pages 19–34, 2018a.\n\nH. Liu, K. Simonyan, and Y. Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055,\n\n2018b.\n\nR. Luo, F. Tian, T. Qin, E. Chen, and T.-Y. Liu. Neural architecture optimization. Advances in neural information\n\nprocessing systems, 31, 2018.\n\nJ. Mei, Y. Li, X. Lian, X. Jin, L. Yang, A. Yuille, and J. Yang. Atomnas: Fine-grained end-to-end neural\n\narchitecture search. arXiv preprint arXiv:1912.09640, 2019.\n\nP. Molchanov, J. Hall, H. Yin, J. Kautz, N. Fusi, and A. Vahdat. Hant: Hardware-aware network transformation.\n\narXiv preprint arXiv:2107.10624, 2021.\n\nH. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean. Efficient neural architecture search via parameters sharing. In\n\nInternational conference on machine learning, pages 4095–4104. PMLR, 2018.\n\nI. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Dollár. Designing network design spaces.\n\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10428–10436, 2020.\n\nP. Ramachandran, B. Zoph, and Q. V. Le. Searching for activation functions. arXiv preprint arXiv:1710.05941,\n\n2017.\n\nM. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510–4520, 2018.\n\nM. Shen, H. Yin, P. Molchanov, L. Mao, J. Liu, and J. M. Alvarez. Halp: Hardware-aware latency pruning.\n\narXiv preprint arXiv:2110.10811, 2021.\n\nM. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International\n\nConference on Machine Learning, pages 6105–6114. PMLR, 2019.\n\nM. Tan and Q. Le. Efficientnetv2: Smaller models and faster training. In International Conference on Machine\n\nLearning, pages 10096–10106. PMLR, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nM. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820–2828, 2019.\n\nM. Tan, R. Pang, and Q. V. Le. Efficientdet: Scalable and efficient object detection. In Proceedings of the\n\nIEEE/CVF conference on computer vision and pattern recognition, pages 10781–10790, 2020.\n\nA. Vahdat, A. Mallya, M.-Y. Liu, and J. Kautz. Unas: Differentiable architecture search meets reinforcement learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11266–11275, 2020.\n\nA. Wan, X. Dai, P. Zhang, Z. He, Y. Tian, S. Xie, B. Wu, M. Yu, T. Xu, K. Chen, et al. Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12965–12974, 2020.\n\nL. Wang, C. Yu, S. Salian, S. Kierat, S. Migacz, and A. F. Florea. Gpunet: Searching the deployable convolution\n\nneural networks for gpus. arXiv preprint arXiv:2205.00841, 2022.\n\nR. Wightman.\n\nPytorch\n\nimage models.\n\nhttps://github.com/rwightman/\n\npytorch-image-models, 2019.\n\nB. Wu, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, Y. Jia, and K. Keutzer. Fbnet: Hardwareaware efficient convnet design via differentiable neural architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10734–10742, 2019.\n\nX. Xia, X. Xiao, X. Wang, and M. Zheng. Progressive automatic design of search space for one-shot neural architecture search. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2455–2464, 2022.\n\nL. Xiang, Ł. Dudziak, M. S. Abdelfattah, T. Chau, N. D. Lane, and H. Wen. Zero-cost proxies meet differentiable\n\narchitecture search. arXiv preprint arXiv:2106.06799, 2021.\n\nS. Xie, H. Zheng, C. Liu, and L. Lin. Snas: stochastic neural architecture search.\n\narXiv preprint\n\narXiv:1812.09926, 2018.\n\nT.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, M. Sandler, V. Sze, and H. Adam. Netadapt: Platform-aware neural network adaptation for mobile applications. In Proceedings of the European Conference on Computer Vision (ECCV), pages 285–300, 2018.\n\nK. Yu, R. Ranftl, and M. Salzmann. An analysis of super-net heuristics in weight-sharing nas. IEEE Transactions\n\non Pattern Analysis and Machine Intelligence, 2021.\n\nA. Zela, T. Elsken, T. Saikia, Y. Marrakchi, T. Brox, and F. Hutter. Understanding and robustifying differentiable\n\narchitecture search. arXiv preprint arXiv:1909.09656, 2019.\n\nY. Zhao, L. Wang, Y. Tian, R. Fonseca, and T. Guo. Few-shot neural architecture search. In International\n\nConference on Machine Learning, pages 12707–12718. PMLR, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA EXPERIMENTS DETAILS\n\nA.1 MACHINE AND SOFTWARE SETUP\n\nThe experiments were performed on nodes equipped with 8 NVIDIA Tesla A100 GPUs (DGXA100). Python 3.6.8 and Pytorch Image Models Wightman (2019) v0.4.12 were used inside pytorch/pytorch:1.9.1-cuda11.1-cudnn8-runtime docker container.\n\nBy inference latency we mean Median GPU Compute Time measured via trtexec command with FP16 precision and batch size of 1 using TensorRT version 8.0.1.6 on a single NVIDIA V100 GPU with driver version 450.51.06 inside nvcr.io/nvidia/tensorrt:21.08-py3 docker container.\n\nA.2\n\nIMAGENET EXPERIMENTS\n\nThe experiments were conducted on the Imagenet-1k Deng et al. (2009) image classification dataset. It consists of 1.2 million training samples and 50 thousand validation samples, which span over 1000 classes. For all experiments we used a weight decay of 1e-5 and an AutoAugment Cubuk et al. (2018) with an augmentation magnitude of 9 and standard deviation 0.5 (corresponding to the probability of applying the operation) for both architecture search and training from scratch. The experiments were performed in automatic mixed precision (AMP).\n\nA.2.1 ARCHITECTURE SEARCH DETAILS\n\nAn architecture search was performed on 10% of randomly selected classes from the original dataset. The input images were scaled to a resolution of 288 × 288. The search lasted 200 epochs, with a total batch size of 256 and a cosine learning rate scheduler that had an initial value of 0.1. We used the Adam Kingma and Ba (2014) optimizer for the architecture parameters and the RMSprop optimizer with an initial learning rate of 0.002 for the weights. We divided the search into two phases. In the first phase we train only the regular weights for the first ewarmup = 70 epochs, and we only do it once to save computational time. In the second phase, which lasts the remaining 130 epochs, in each epoch 80% of the training dataset was used to train the regular weights and the remaining 20% was used to train the architecture weights. The second phase has been computed in many variants but always starts from a common checkpoint after ewarmup epochs. During this phase, we progressively prune the search space using a pruning threshold (see Section 3.2.2) that increases linearly from 0.15 to 0.55. At the end of each epoch, we removed blocks below the threshold from the search space. The momentum used in the pruning internal structure of a Prunode was equal to 0.4 . The coefficient α in the loss function varied across different runs with analyzed values of {2.0, 1.2, 1.0, 0.8, 0.6, 0.4, 0.2}, but all runs used the same coefficient β value of 0.6 . The latency term LAT of the loss function was measured in μs.\n\nA.2.2 TRAINING DETAILS\n\nThe hyperparameters’ values were based on GPUNet-1 Wang et al. (2022). For fair comparison we decided to train GPUNet-0, GPUNet-1 and GPUNet-2 using exactly the same hyperparameters (including batch size) as we used for PRUNETs .\n\nAfter the architecture search, the sampled network was again trained from scratch. The training lasted for 450 epochs with a total batch size of 1536 and an initial learning rate of 0.06. The learning rate decays by 0.97 times for every 2.4 epochs. crop_pct was set to 1.0. Exponential Moving Average (EMA) was used with a decay factor of 0.9999. We used the drop path Larsson et al. (2016) with a base drop path rate of 0.2. All of the PRUNET and GPUNet networks have been trained with and without distillation Hinton et al. (2015). Knowledge distillation is a technique that transfers the knowledge from a large pre-trained model to a smaller one which can be deployed under real-world limited constrains. For the training with distillation, we used different teachers and different crop_pct as it is presented in Table 4.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: For each image resolution a different teacher was selected. Additionally crop_pct was changed to match the crop_pct of the teacher.\n\nmodel resolution\n\nteacher architecture\n\nteacher resolution\n\ncrop_pct\n\nGPUNet1 & PRUNET GPUNet0 GPUNet2\n\n288 × 288 320 × 320 384 × 384\n\nEfficientNet-B3 EfficientNet-B4 EfficientNet-B5\n\n300 × 300 380 × 380 456 × 456\n\n0.904 0.922 0.934\n\nTable 5: Search space for B.1 experiment: For stages 2-6 the search space is defined as the sum of RELU and Swish variants\n\nStage\n\nType\n\nStride\n\n# Layers Activation Kernel\n\nExpansion\n\nFilters\n\n0 1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nConv Conv\n\nFused-IRB\n\nFused-IRB\n\nIRB\n\nIRB\n\nIRB\n\nConv+Pool+FC\n\n1 1\n\n2\n\n2\n\n2\n\n1\n\n2\n\n1\n\n1 2\n\n2\n\n3\n\n2\n\n5\n\n6\n\n1\n\nSwish RELU\n\nRELU Swish RELU Swish\n\nRELU Swish RELU Swish RELU Swish\n\nRELU\n\n3 {3,5,7}\n\n{3,5} {3,5,7} {3,5} {3,5,7}\n\n{3,5} {3,5,7} {3,5,7} {3,5} {3,5,7} {3,5}\n\n1\n\n8\n\n8\n\n8\n\n8\n\n8\n\n24 24\n\n64\n\n96\n\n160\n\n288\n\n448\n\n1280\n\nA.3 COCO EXPERIMENTS\n\nThe object detection experiments were conducted on the MS COCO 2017 dataset Lin et al. (2014). We used EfficientDet Tan et al. (2020) as a baseline model and replaced the original EfficientNet Tan and Le (2019) backbone with PRUNET and GPUNet. The training lasted for 300 epochs with batch size of 60. The learning rate was warmed-up for the first 20 epochs with the value set to 1e-4. Then, the cosine learning rate scheduler was used with an initial learning rate of 0.65. The optimizer was SGD with a momentum of 0.9 and a weight decay of 4e-5. Gradient clipping of value 10.0 was introduced. The training was performed in automatic mixed precision (AMP). We used the Exponential Moving Average (EMA) with a decay factor of 0.999.\n\nB ADDITIONAL EXPERIMENTS\n\nB.1 PRUNING BLOCKS WITHIN A STOCHASTIC LAYER\n\nWe can have two different versions of setting the pruning threshold, i.e., constant or linearly increasing. The hypothesis was that they lead to the networks of comparable Pareto frontiers, but the search is significantly faster for the linearly increasing threshold. The experiments supported this hypothesis, and the result Pareto frontiers are on par; however, the processing time for the constant threshold was around 15% – 45% slower.\n\nWe used the Imagewoof Howard image classification dataset both for the search and the training of the obtained architectures. The search space is described in Table 5.\n\nThe search was performed with batch size 256, cosine learning rate scheduler with initial value of 0.1. It lasted 200 epochs and for the first ewarmup = 70 epochs only the regular weights were trained. The image resolution was scaled to 224 × 224 and crop_pct was set to 0.875. Adam optimizer was used for architecture parameters and RMSprop optimizer with initial learning rate of 0.002 for the regular weights. A weight decay of 1e-5 was used and AutoAugment with an augmentation magniture of 9 and standard deviation of 0.5. The search was performed in automatic mixed precision (AMP). In each epoch 80% of the training dataset was used to train the regular weights and the reamaining\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Pareto frontiers\n\n(b) Search time\n\nFigure 5: Linearly increasing pruning threshold leads to a significant search phase speedup while producing a similar Pareto frontier to a constant threshold method. Labels indicate the value of α being used.\n\n20% was used to train the architecture weights. For a constant pruning threshold policy, we use a value of 0.133; for the second setup, we linearly increase the threshold from 0.133 to 0.55 (see Section 3.2.2). The coefficient α in the loss function varied across different runs with analyzed values of {0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 2.0, 3.0}, but all runs used the same coefficient β value of 0.6.\n\nAfter the search has finished, the architectures have been re-trained from scratch for 450 epochs in automatic mixed precision (AMP) using Stochastic Gradient Descent (SGD) with batch size of 32, and initial learning rate of 0.2. The learning rate decays by 0.97 for every 2.4 epochs. The drop path with bsae drop rate of 0.2 was used. The results of those experiments are presented in Figure 5.\n\nB.2 PRUNING UNNECESSARY STOCHASTIC LAYERS\n\nProper selection of parameters φ and λ (which multiply the output of skip connection and the output of a block; for more details, see Section 3.2.3) can influence the final length of the sampled network. In particular Table 6 shows that for α large enough, the number of layers is inversely related to λ for considered cases. To further analyze the impact of our methodology, we run additional searches with (φ, λ) = (1.0, 1.0) – these values correspond to a base approach without our modification. From Table 6 it is clear that for α ≥ 0.8, the base approach samples networks with fewer layers than the searches we performed. To prove that our approach is justified, we train all 28 of the architectures found during the searches described above, and we present Pareto frontiers defined by all four sets of φ and λ. Figure 6 shows that our search technique can find networks with up to 15% better latency with the same accuracy as the base approach. Our methodology uses the final loss of the search as a zero-cost filter to select good candidate architectures to be evaluated. Figure 7 presents the Pareto frontier of Search Loss concerning Final Latency. Looking at both figures, it is directly visible that if search loss is significantly better for similar searches (in this case for similar α values), we can also expect better results in terms of the final accuracy; however, the correlation is not strong. Therefore, without additional compute overhead, our methodology can find almost optimal networks, possibly still leaving room for improvement.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Graph focuses on the architectures being searched using 0.6 ≤ α ≤ 1.2\n\nFigure 7: Graph focuses on the architectures being searched using 0.6 ≤ α ≤ 1.2, which shows the relationship between the final CE of the search and the final latency of the architecture\n\n16\n\n0.550.600.650.700.750.800.850.90TensorRT FP16 bs=1 Latency (ms)79.5079.7580.0080.2580.5080.7581.0081.2581.50Top-1, ImageNetbest Loss (PruNet) = 1.0 = 1.0 = 1.1 = 0.85 = 1.1 = 0.55 = 1.1 = 0.40.550.600.650.700.750.800.850.90TensorRT FP16 bs=1 Latency (ms)0.400.450.500.550.600.65Final CEbest Loss (PruNet) = 1.0 = 1.0 = 1.1 = 0.85 = 1.1 = 0.55 = 1.1 = 0.4Under review as a conference paper at ICLR 2023\n\nTable 6: Search results for different φ and λ. For each combination of (α, φ, λ) we run a single search and for each search the total loss is reported. The total loss is a sum of cross entropy loss and latency loss (Loss = CE + α ∗ LATβ). #IRBs indicates the number of Fused-IRB and IRB in the final architecture. The number of the other layers is the same for all architectures. PRUNET architectures are shown in bold\n\nφ, λ = (1.0, 1.0)\n\nφ, λ = (1.1, 0.85)\n\nφ, λ = (1.1, 0.55)\n\nφ, λ = (1.1, 0.4)\n\nLoss\n\n#IRBs\n\nLoss\n\n#IRBs\n\nLoss\n\n#IRBs\n\nLoss\n\n#IRBs\n\nα = 2.0 α = 1.2 α = 1.0 α = 0.8 α = 0.6 α = 0.4 α = 0.2\n\n6.7149 4.2938 3.6352 3.0027 2.3528 1.7154 1.0573\n\n6 9\n11 13 15 16 18\n\n6.6316 4.1876 3.6015 2.9932 2.3394 1.7201 1.0573\n\n7 10 13 14 15 17 18\n\n6.5968 4.2139 3.6074 2.9762 2.316 1.7296 1.0687\n\n9 12 13 14 15 16 18\n\n6.6241 4.2039 3.6138 2.9692 2.3723 1.6946 1.0531\n\n11 13 14 15 15 16 18\n\n17",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a multi-objective neural architecture search that optimizer for both accuracy and latency. They use GPU-Net1 as the skeleton and search for kernel size and the expansion ratio of the intermediate Fused-IRB and IRB layers. They use masked differentiable architecture search, where the output of a layer is the weighted sum of all the outputs of the blocks. The weights of these blocks are computed using gumbel-softmax of the architecture weights of all the blocks in layer i.\n   The input and output dimensions of all the blocks in a layer are the equal. To search for the expansion ratio, rather than have 1 block for each possible configuration, they use masks similar to FBNetV2. To this end, they create two copies of a block and apply small mask on one and a large mask on another. The small mask masks half of the channels to begin with and the larger mask does not mask any of the channels.  As the search progress, the small mask and the larger mask are updated such that the distance between them reduces and finally becomes close to 0. If the architecture weights of the block with small mask is higher, then both the masks shrink. Similarly, if the architecture weights of the block with the larger mask is higher, then both the masks are increased. Finally when the small mask and the larger mask are less than granularity away, the final candidate has channels between the smaller and the larger one.\n   During the search, they also prune the blocks if their weights are lesser than the threshold. As is the case with other pruning based NAS methods, the networks weights are trained for the first X epochs. After that the bilevel optimization of network weights and architecture weights begins. The threshold keeps increasing as the search progresses.\n\n# Strength And Weaknesses\n\nStrength:\n1. They are able to find networks with higher accuracy and lower latency than all their baselines for both image classification and object detection.\n2. They use a novel search space - GPUNet which is better suited for building architectures with high accuracy and low latency.\n\nWeakness/ Questions:\n 1. Are we using masking for searching for kernels too?\n 2. The algorithm is not very straightforward to understand. \n3. How is this masking algorithm better than the masking technique used in FBNet V2? If we use the masking technique of FBNet V2 with the same search space and setup as this paper, how would it fare? FBNet V2 requires only 1 block per layer while searching for channels. So it seems to be occupying lesser memory.\n\nQuestions about algorithm 1\n1. What does progress variable denote? Is it the ratio of number of epochs done to number of epochs left?\n2. Why are we setting the architecture weights to 0 if s > 0? \n3. f weights are always non-negative, then update variable will also be non-negative. When will s become negative?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper can be written more clearly. They described the setup clearly so the experiments can be reproduced. The novelty of the work is limited.\n\n# Summary Of The Review\n\nThe algorithm1 is not clearly written. It is not evident why this is better than the masking technique in FBNet V2.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nINCORPORATING EXPLICIT UNCERTAINTY ESTIMATES INTO DEEP OFFLINE REINFORCEMENT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nMost theoretically motivated work in the offline reinforcement learning setting requires precise uncertainty estimates. This requirement restricts the algorithms derived in that work to the tabular and linear settings where such estimates exist. In this work, we develop a novel method for incorporating scalable uncertainty estimates into an offline reinforcement learning algorithm called deep-SPIBB that extends the SPIBB family of algorithms to environments with larger state and action spaces. We use recent innovations in uncertainty estimation from the deep learning community to get more scalable uncertainty estimates to plug into deep-SPIBB. While these uncertainty estimates do not allow for the same theoretical guarantees as in the tabular case, we argue that the SPIBB mechanism for incorporating uncertainty is more robust and flexible than pessimistic approaches that incorporate the uncertainty as a value function penalty. We bear this out empirically, showing that deep-SPIBB outperforms pessimism based approaches with access to the same uncertainty estimates and performs at least on par with a variety of other strong baselines across several environments and datasets.\n\n1\n\nINTRODUCTION\n\nIn the study of offline reinforcement learning (OffRL), uncertainty plays a key role (Buckman et al., 2020; Levine et al., 2020). This is because, unlike online RL where an agent receives feedback in the form of low rewards after taking a bad action, an OffRL agent must learn from a fixed dataset without feedback from the environment. As a result, a consistent issue for OffRL algorithms is the overestimation of states and actions that are not seen in the dataset, leading to poor performance when the agent is deployed and finds that those states and actions in fact have low reward (Fujimoto et al., 2019b). To overcome this issue, OffRL algorithms often attempt to incorporate some notion of uncertainty to ensure that the learned policy avoids regions of high uncertainty.\n\nThere are two main issues with this approach: (1) how to define uncertainty and (2) how to incorporate uncertainty estimates into the OffRL algorithm. In tabular and linear MDPs, issue (1) is resolved by using visitation counts and elliptical confidence regions, respectively (Yin et al., 2021; Yin & Wang, 2021; Jin et al., 2021; Laroche et al., 2019). In the large-scale MDPs that we consider, neither of these solutions work, but there is a large literature from the deep learning community on uncertainty quantification that we can leverage for OffRL (Ciosek et al., 2019; Osband et al., 2018; 2021; Burda et al., 2019; Ostrovski et al., 2017; Lakshminarayanan et al., 2017; Blundell et al., 2015; Gal & Ghahramani, 2016). Given these uncertainty estimators, this paper focuses primarily on issue (2), how to incorporate uncertainty for OffRL.\n\nTo understand how to best incorporate uncertainty into an OffRL algorithm, we first provide a high level algorithmic template that captures the majority of related work as instances of modified policy iteration (Scherrer et al., 2012) that alternate between policy evaluation and policy improvement. We then can sort prior work into four categories along two axes: whether the algorithm modifies the evaluation step or the improvement step, and whether the algorithm uses an explicit uncertainty estimator or not. One class of algorithms modifies the evaluation step by introducing value penalties based on explicit uncertainty estimates, which we will call pessimism (Petrik et al., 2016; Buckman et al., 2020; Jin et al., 2021). An alternative modifies the value estimation without using an uncertainty estimate, like in CQL (Kumar et al., 2020). Another family uses behavior constraints that modify the policy improvement step to keep the learned policy near the behavior policy (Fujimoto et al.,\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n2019b;a), but does not use explicit uncertainty. Instead, we propose to use the fourth class of methods that leverages uncertainty-based constraints in the policy improvement step and is inspired by the SPIBB family of algorithms (Laroche et al., 2019; Laroche & Tachet des Combes, 2019; Nadjahi et al., 2019; Sim ̃ao et al., 2020). These algorithms modify the policy improvement step like a behavior constraint, but also reason about state-based uncertainty like the pessimistic algorithms. Explicitly, we define the deep-SPIBB algorithm that effectively incorporates uncertainty estimates into OffRL.\n\nThe main contributions of this paper are as follows:\n\n• We introduce the deep-SPIBB algorithm which provides a principled way to incorporate scalable uncertainty estimates for OffRL. We instantiate this algorithm using ensemblebased uncertainty estimates inspired by Bayesian inference (Ciosek et al., 2019; Osband et al., 2021).\n\n• We provide a detailed comparison of several different mechanisms to incorporate uncertainty by considering how each mechanism operates at the extreme settings of its hyperparameters. This analysis shows that deep-SPIBB provides a flexible and robust mechanism to interpolate between various extremes (greedy RL, behavior cloning, and one-step RL).\n\n• Through experiments on classical environments (cartpole and catch) as well at Atari games, we demonstrate the efficacy of deep-SPIBB. In particular, we find that deep-SPIBB consistently outperforms pessimism when given access to the same imperfect uncertainty estimators.\n\n• When deep-SPIBB has access to better uncertainty estimators (as in the easier cartpole environment) it is able to substantially outperform our other baselines of CQL and BCQ as well. This suggests that as uncertainty estimators improve, deep-SPIBB will provide a useful mechanism for incorporating them for OffRL.\n\n2 PRELIMINARIES\n\nWe consider an OffRL setup with a discrete action space and access to a dataset D = {(sj, aj, rj, s′ j=1 consisting of N transitions collected by some behavior policy β. The goal is to learn a policy π from this data to maximize expected discounted returns J(π) = Eτ ∼π[(cid:80)∞ t=0 γtrt].\n\nj)}N\n\n2.1 ALGORITHMIC TEMPLATE\n\nThe vast majority of prior work on the OffRL problem can be seen through a common algorithmic template of modified policy iteration. Each algorithm alternates between policy evaluation and policy improvement steps. The main difference between algorithms comes in how they modify either the evaluation or the improvement step. Below we first define the generic version of the OffRL algorithmic template and then explain how different OffRL algorithms modify this template.\n\nPolicy improvement by greedy maximization:\n\nπ(i+1)(·|s) = arg max\n\nπ∈Π\n\n(cid:88)\n\na∈A\n\nπ(a|s) ˆQ(i)(s, a).\n\n(1)\n\nValue estimation by fitted Q evaluation given the dataset D = {(sj, aj, rj, s′ man operator from datapoint j with π(i+1), ˆQ(i), as T (j, π, Q) = rj + γ (cid:80) Then the evaluation step is:\n\nj)}N a′∈A π(a′|s′\n\nj=1. Define the Bellj, a′).\n\nj)Q(s′\n\nˆQ(i+1) = arg min\n\n(cid:88)\n\n(cid:16)\n\nQ∈Q\n\nj\n\nQ(sj, aj) − T (j, π(i+1), ˆQ(i))\n\n(cid:17)2\n\n(2)\n\nIn addition to the policy and Q function, some algorithms we consider will also learn an estimated behavior policy ˆβ(a|s) and/or an uncertainty function ˆu(s, a). Generally, ˆβ is learned by maximum likelihood supervised learning. The uncertainty ˆu on the other hand can be learned many different ways. We will discuss ˆu in more detail in Section 3 when we describe our method.\n\nWith this template we can provide a characterization of much prior work that is summarized in Table 1. The essential axes that we consider are (1) whether the algorithm modifies the improvement step or the evaluation step and (2) whether the algorithm uses an uncertainty function u(s, a) or not.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: A characterization of how several baseline methods fit into our template.\n\nModified improvement step Modified evaluation step\n\nUncertainty-free Uncertainty-based deep-SPIBB (ours) Pessimism\n\nBCQ CQL\n\n2.2 MODIFYING THE EVALUATION STEP\n\nOne approach to incorporating uncertainty is to introduce a penalty into the value estimation step that encourages the policy to avoid novel states or actions.\n\nUncertainty-based. The simplest value penalty is to use an explicit estimate of uncertainty, we will can this algorithm pessimism. Variants of pessimism have been examined in a broad range of prior work (Petrik et al., 2016; Buckman et al., 2020; Jin et al., 2021). Given an uncertainty estimator u : S × A → R, the pessimism algorithm modifies the evaluation step to be:\n\nˆQ(i+1) = arg min\n\n(cid:18)\n\n(cid:88)\n\nQ∈Q\n\nj\n\nQ(sj, aj) −\n\n(cid:18)\n\nT (j, π(i+1), ˆQ(i)) − α · u(sj, aj)\n\n(3)\n\n(cid:19)(cid:19)2\n\nThe hyperparameter α controls the amount of pessimism.1\n\nUncertainty-free. Alternatively, the algorithm can modify the evaluation step without use of an explicit uncertainty function. A popular representation of this approach is the CQL algorithm Kumar et al. (2020). In CQL there is no explicit estimate of uncertainty. Instead, the algorithm makes the following update in the evaluation step:\n\nQ(i+1) = arg min\n\n(cid:88)\n\nα(cid:0) log\n\n(cid:88)\n\nQ∈Q\n\nj\n\na\n\nexp(Q(sj, a)) − Q(sj, aj)(cid:1) + (cid:0)Q(sj, aj) − T (j, π(i+1), ˆQ(i))(cid:1)2\n\n(4)\n\nThe first term encourages the Q estimates to underestimate Q values at unobserved actions (via the log-sum-exp term) while remaining large at observed actions (via the Q(sj, aj) term). Again the hyperparameter α controls the penalty.\n\nAs explained in the original paper, this version of CQL can be viewed as a version of pessimism with an entropy regularization term. In analog to pessimism, the implicit uncertainty function would take the form of u(s, a) = π(a|s) β(a|s) − 1 where π is the current policy iterate. Note this function is non-stationary since it depends on π. Moreover, in practice with neural function approximation, the CQL objective may behave differently than trying to implement this function explicitly. The implicit nature of the uncertainty used by CQL makes it different from standard pessimism with explicit uncertainty estimates.\n\n2.3 MODIFYING THE IMPROVEMENT STEP\n\nInstead of modifying the evaluation step, we can alternatively modify the improvement step.\n\nUncertainty-free. The simplest way to modify the improvement step without using an uncertainty estimate is to constrain the learned policy to choose actions that are well-supported under the estimated behavior. The main example of this algorithm that we consider is the BCQ algorithm (Fujimoto et al., 2019b;a). Explicitly, the BCQ algorithm with hyperparameter τ defines\n\nπ(i+1)(a|s) = 1\n\n(cid:34)\n\na = arg max a′∈Aτ (s)\n\n(cid:35)\n\n(cid:40)\n\nˆQ(i)(s, a′)\n\n, Aτ (s) =\n\na ∈ A :\n\nˆβ(a|s)\n\nmaxa′∈A\n\nˆβ(a′|s)\n\n(cid:41)\n\n≥ τ\n\n(5)\n\nwhere ˆβ is a maximum likelihood estimate of the behavior policy. When τ = 1 this is exactly behavior cloning and when τ = 0 the constraint has no effect. Importantly, these methods do not use any notion of uncertainty over states.\n\n1Another instance of an uncertainty-based modification of the evaluation step is from the MBS algorithm of Liu et al. (2020). Instead of using an uncertainty penalty, they threshold an uncertainty function and propagate the minimal possible return in the Bellman backup if the uncertainty is too high.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nUncertainty-based. The final option is to modify the policy improvement step with the use of an explicit estimate of uncertainty. Our propsed deep-SPIBB algorithm falls into this category. In contrast to behavior constraints, an uncertainty-based constraint takes into account the confidence that we have in a given state. And in contrast to a value penalty, the uncertainty is not propagated in the evaluation step. The main example of this style of algorithm is the SPIBB family of algorithms (Laroche et al., 2019; Nadjahi et al., 2019; Sim ̃ao et al., 2020) which we will discuss further in Section 3 when we introduce deep-SPIBB.\n\n2.4\n\n(UN)RELATED WORK\n\nIn this subsection, we promptly acknowledge the existence of algorithm approaches for Offline RL that are not directly related with the uncertainty question that this work is endeavoring to address. A wide range of algorithms rely on actor-critic algorithmic architecture in order to handle MDPs with continuous actions Wang et al. (2020); Wu et al. (2019); Siegel et al. (2020); Kostrikov et al. (2021); Fujimoto & Gu (2021). We focus exclusively on the discrete action case and study different ways of incorporating uncertainty into the algorithm. Another group of algorithms take advantage of an explicit MDP model ˆm, which confers them better out-of-distribution generalization capabilities Yu et al. (2020); Kidambi et al. (2020); Yu et al. (2021); Janner et al. (2021). In our study, all the considered algorithms are model-free in order to guard ourselves against confounding factors of additional implementation details. Finally, there is the return-condition supervised learning approach that has recently been introduced in the Offline RL literature Chen et al. (2021); Emmons et al. (2021). We found the structure of these algorithms to be too distant from our work.\n\n3 DEEP-SPIBB\n\nWe can now explicitly define the deep-SPIBB algorithm. To make the algorithm scale up to high dimensional inputs, we use a neural uncertainty estimator described in Section 3.1. Using this uncertainty estimator, deep-SPIBB uses a variant of the approximate soft-SPIBB (Nadjahi et al., 2019) mechanism to incorporate uncertainty estimates into the policy improvement step, described in Section 3.2.\n\n3.1 NEURAL UNCERTAINTY ESTIMATION\n\nAs presented in prior work, soft-SPIBB relies on count-based uncertainty estimates ˆu(s, a) that have high probability guarantees. In the tabular case, we can derive these estimates from visitation counts n(s, a) and set ˆu(s, a) = c/(cid:112)n(s, a). Unfortunately, this is not a scalable solution in larger domains since it is unknown how to best generalize these precise notions of uncertainty to larger domains that require data-efficient generalization across states. So, we borrow from the literature on neural uncertainty estimation (Ciosek et al., 2019; Osband et al., 2021) and use an uncertainty estimator based on ensembles trained to estimate random targets and regularized by random priors. The objective and estimator are described in full detail in Appendix A. At a high level, the uncertainty is proportional to the variance of an ensemble of models, each trained to predict a random function with high-dimensional outputs.\n\nWe make one key change relative to prior work. Rather than using both state and action as input to our uncertainty estimator, we only use the ensemble-based uncertainty estimator to estimate state-based uncertainty ˆu(s). We then combine this with the behavior policy estimate ˆβ(a|s) to derive ˆu(s, a). In particular, we define our uncertainty estimator as\n\nˆu(s, a) =\n\nˆu(s)\n\n(cid:113)\n\nˆβ(a|s)\n\n.\n\n(6)\n\n(cid:113)\n\nˆβ(a|s) comes from the tabular setting, where if we were to use counts to\n\nand ˆβ(a|s) = n(s,a)\n\nn(s) , then our ˆu(s, a) would be exactly the standard count- . This decision is supported empirically in the experiments section. Intuitively, this\n\nn(s)\n\nThe rationale for using define ˆu(s) = c√\n\nbased\n\nc√\n\nn(s,a)\n\nhelps because it guarantees that the uncertainty estimator is consistent with the estimated behavior used in the SPIBB constraint.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.2 ALGORITHM\n\nThe algorithm consists of three models: ˆβ(a|s) an estimate of the behavior policy, ˆu(s, a) an uncertainty quantification, and ˆQ an estimated Q function. The Q updates are much like those of SPIBB-DQN (Laroche & Tachet des Combes, 2019) except we use approximate soft-SPIBB and scalable neural uncertainty estimates. Since we use target networks to approximate policy iteration, we will use parenthetical superscripts to keep track of the policy iteration step (i.e. the number of times the target network has been updated).\n\nPolicy improvement step. At step i+1, the policy π(i+1) approximates the solution to the following constrained optimization problem:\n\nπ(·|s) = arg max π∈∆|A|\n\n(cid:88)\n\na∈A\n\nˆQ(i)(s, a)π(a|s),\n\ns.t.\n\n(cid:88)\n\na∈A\n\nˆu(s, a)\n\n(cid:12) (cid:12)\n\n(cid:12)π(a|s) − ˆβ(a|s)\n\n(cid:12) (cid:12) (cid:12) ≤ εtrain\n\n(7)\n\nSince this problem is difficult to solve exactly, we use the approximation technique described in Nadjahi et al. (2019).\n\nValue estimation step. For the value estimation step, we use the standard expected SARSA backup from the Equation (2). Since π(i+1) is uncertainty-constrained, it prevents Q values from being propagated from state-action pairs with high uncertainty under ˆu.\n\nEvaluation policy. One modification that we make from prior work on soft-SPIBB is to generalize the algorithm by separating the hyperparameter εtrain that governs the deviation from the behavior during the policy improvement step during training from εeval that governs this deviation during evaluation. So the evaluation policy using the final estimated Q function ˆQ(I) becomes the solution of the optimization:\n\nπeval(·|s) = arg max π∈∆|A|\n\n(cid:88)\n\na∈A\n\nˆQ(I)(s, a)π(a|s),\n\ns.t.\n\n(cid:88)\n\na∈A\n\nˆu(s, a)\n\n(cid:12) (cid:12)\n\n(cid:12)π(a|s) − ˆβ(a|s)\n\n(cid:12) (cid:12) (cid:12) ≤ εeval.\n\n(8)\n\nAs we will see in Section 4, this choice allows us to capture a richer tradeoff between different methods instead of simply interpolating between behavior cloning and greedy RL.\n\nAlgorithmic variants. We will consider two variants of the algorithm:\n\n1. Standard deep-SPIBB where we set εeval = εtrain. 2. Generalized deep-SPIBB where we tune εeval independently of εtrain.\n\nThese variants will be discussed in greater depth in the next section where we compare the tradeoffs made by εtrain and εeval with those made by the baseline offline RL algorithms introduced above.\n\n4 COMPARING ALGORITHMIC TRADEOFFS\n\nEach algorithm introduced above comes with a hyperparameter that governs the tradeoff between acting greedily and restricting the learned policy to be safe (or two hyperparameters in the case of generalized deep-SPIBB). The key difference between the algorithms is how they choose to modulate this tradeoff and which points they choose at the extremes.\n\nExtremal hyperparameter settings. To understand the tradeoff that each hyperparameter governs, it is useful to understand what happens at the extremal values. The results of this analysis are summarized in Table 2. There are a few key takeaways from this analysis:\n\n• All algorithms capture greedy RL at one extreme of the hyperparameters.\n\n• All algorithms except for pessimism capture a variant of BC at another extreme setting of the hyperparameters. Note that due to the greedy nature of the policies defined in BCQ and CQL, they cannot exactly represent BC for a stochastic behavior policy and instead choose the action that has maximum probability under the behavior (which we will denote as argmax BC).\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: A summary of what each algorithm reduces to at extreme settings of their hyperparameters.\n\nAlgorithm BCQ CQL Pessimism Deep-SPIBB Gen Deep-SPIBB\n\nExtreme 1 argmax BC at τ = 1 argmax BC as α → ∞ Minimal uncertainty as α → ∞ BC at εtrain = εeval = 0 BC at εeval = 0 One-step RL at εtrain = 0\n\nExtreme 2 Greedy RL at τ = 0 Greedy RL at α = 0 Greedy RL at α = 0 Greedy RL as εtrain = εeval → ∞ Greedy RL as εtrain = εeval → ∞\n\n• Only generalized deep-SPIBB captures one-step RL (Brandfonbrener et al., 2021; Gulcehre et al., 2021) that performs one step of policy improvement at another extreme setting of the hyperparameters.\n\nWhat happens to pessimism at the extreme. It is worth delving deeper into what happens for the pessimism algorithm as α → ∞. Note that we can view the pessimistic algorithm as optimizing an augmented reward ̃r(s, a) = r(s, a) − α · u(s, a). As we send α → ∞ the impact of r on ̃r tends to zero. As a result, we end up learning a policy that approaches πu(a|s) = 1[a = arg maxa′ Q∗ u(s, a′)] where Q∗ u is the Q function of the optimal policy for the reward function −u(s, a). We will call πu the minimal uncertainty policy. One key difference is that the minimal uncertainty policy can prefer policies that remain in states that are often observed in the dataset over actions chosen by the behavior. This difference is not necessarily bad in all cases, but provides a different inductive bias than the other algorithms considered.\n\nThe issues with pessimism become more problematic when the uncertainty function is poorly estimated (as it may be in high-dimensional state spaces). Since the default is to minimize the uncertainty, if the uncertainty estimate has some accidentally underestimated uncertainties there is no way to remedy the situation by tuning the hyperparameter α. Sending α → ∞ will act greedily with respect to the negative uncertainty and thereby exploit the underestimated uncertainty estimate, yielding an unsafe policy. Alternatively, sending α → 0 will be greedy with respect to the estimated reward and yield a different unsafe policy. This is especially troublesome in realistic datasets where we expect the behavior to already give us a reasonable policy. In contrast, by defaulting to the behavior policy, deep-SPIBB can at least recover the performance of the behavior simply by tuning ε, no matter the quality of the uncertainty estimates2.\n\nThe benefits of deep-SPIBB. From this perspective, our deep-SPIBB algorithm has a few benefits. First, unlike pessimism, SPIBB is able to remain robust to poor quality uncertainty estimates by defaulting to the behavior policy. Second, unlike methods like BCQ and CQL that ignore state-based uncertainty, SPIBB can leverage uncertainty estimates when available.\n\nMoreover, our generalized deep-SPIBB algorithm goes one step further by introducing a different default setting of the hyperparameters. When εtrain is set to 0, then the evaluation step will learn Qβ, as in one-step RL (Brandfonbrener et al., 2021; Gulcehre et al., 2021). Then as we vary εeval from 0 to infinity while fixing εtrain = 0 we interpolate between BC and argmax of ˆQβ, the greedy one-step policy. Capturing this algorithm as a special case (while the other baseline algorithms do not) provides further illustration that generalized deep-SPIBB is capturing a different tradeoff than the baseline algorithms.\n\n5 EXPERIMENTS\n\nWe conduct an empirical analysis to evaluate how well deep-SPIBB incorporates uncertainty estimates and to compare deep-SPIBB to four baselines: BC, BCQ (Fujimoto et al., 2019b;a), pessimism\n\n2Note that there may also be errors in the behavior estimate. However, here we are considering problems with large, high-dimensional state spaces, but finite action spaces. As a result, we expect it to be easier to solve the supervised learning problem of predicting action given state than to solve the uncertainty quantification problem of quantifying uncertainty over state and action, which implicitly requires learning a joint density model over s and a rather than just the model of a conditioned on s\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n(Buckman et al., 2020; Jin et al., 2021), and CQL (Kumar et al., 2020). We run all of these algorithms along with deep-SPIBB on a diverse variety of datasets generated in the classic Cartpole and Catch environments as well as standard Atari benchmark datasets (Gulcehre et al., 2020). These experiments allow us to test how well deep-SPIBB is able to incorporate uncertainty across different domains from low-dimensional observation spaces (Cartpole) to image-based observations with simple dynamics (Catch) to image-based observations with complex dynamics (Atari). Our main finding is that deepSPIBB consistently outperforms pessimism, suggesting that it does a better job of incorporating explicit uncertainty estimates. Our secondary finding is that when uncertainty is easier to estimate (as in Cartpole) deep-SPIBB substantially outperforms all the baselines, while in more challenging environments (as in Atari) it performs about the same as the strongest baseline method, CQL, suggesting a robustness to poor uncertainty estimates.\n\nExperimental setup. Following CQL (Kumar et al., 2020), we build our deep-SPIBB algorithm and each of the baselines on top of QR-DQN (Dabney et al., 2018) using JAX (Bradbury et al., 2018) and the Acme framework (Hoffman et al., 2020). For all experiments, we tune each algorithm (BCQ, CQL, pessimism, and deep-SPIBB) across 4 values of the hyperparameters controlling the deviation from the behavior policy. All other training hyperparameters are held fixed. All algorithms have access to the same behavior and uncertainty estimates. Full details can be found in Appendix B.\n\n5.1 BSUITE ENVIRONMENTS\n\nDatasets. For our first set of experiments we consider two simple environments (cartpole and catch) from bsuite (Osband et al., 2019). In each environment we collect 5 different types of datasets with 10 seeds for each type of data (for a total of 50 datasets per environment). Cartpole has horizon and maximum return of 1000 and catch has horizon of 10 and returns bounded between -1 and 1. Datasets on cartpole have 20k transitions and datasets on catch have 2k transitions. The five dataset types in each environment are collected by as follows: (1) med is data collected by a DQN agent trained to medium performance (200 training episodes), (2) med seed is a mixture of 5 different policies each trained to medium performance, (3) uni is a uniformly random policy, (4) uni med is an equal mixture of data from a uniform policy and a medium policy, (5) uni exp is an equal mixture of data from a uniform policy and an expert policy (trained for 500 episodes on cartpole, 1000 episodes on catch). We report mean and standard error across seeds. Results are shown in Figure 1.\n\nFigure 1: Final performance for OffRL agents trained on five dataset types across two environments. Error bars show standard error across ten seeds for dataset generation.\n\nResults. These results show deep-SPIBB to be consistently the top performer across the suite of experiments and to emphasize our two main findings. First, consider the comparison to pessimism. Across all ten datasets, generalized deep-SPIBB outperforms pessimism and standard deep-SPIBB outperforms pessimism on nine out of ten datasets. The performance gap is particularly large on Catch, where the image-based inputs and smaller dataset size make uncertainty quantification more challenging. By defaulting to the behavior policy instead of the minimal uncertainty policy, deepSPIBB is more robust to poor uncertainty estimates, while still being able to leverage good uncertainty estimates in Cartpole.\n\nSecond, consider the comparison to all baselines. Generalized deep-SPIBB is the top performer on nine out of ten datasets, with the only exception being a slight underperformance relative to BCQ and CQL on the Catch med seed dataset. On Cartpole where the uncertainty estimation task is easier,\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\ndeep-SPIBB dramatically outperforms the uncertainty-free BCQ and CQL methods. On Catch, where uncertainty estimation is more difficult, deep-SPIBB does not outperform BCQ and CQL, but is able to match their performance while pessimism struggles due to the difficulty of uncertainty estimation.\n\n5.2 ATARI ENVIRONMENTS\n\nDatasets. Next we run deep-SPIBB and our four baselines on the atari 1% benchmark (Agarwal et al., 2020). Specifically, we use the data from RL Unplugged (Gulcehre et al., 2020) subsampled down to 1% of the trajectories for each run and report mean and standard deviation across three seeds for each environment. Results are shown in Figure 2.\n\nResults. Again these experiments back up our two main findings. First, compared to pessimism, deep-SPIBB is dramatically better. While deep-SPIBB consistently outperforms BC substantially, pessimism never even recovers the performance of BC. Uncertainty estimation in Atari is very difficult and thus pessimism’s choice to default to the minimal uncertainty policy can cause serious issues as seen here. In contrast, deep-SPIBB is much more robust to poor uncertainty estimates. Second, compared to all baselines, deep-SPIBB generally performs slightly better than BCQ and is competitive with CQL. Improving the performance of deep-SPIBB will likely require improved uncertainty quantification.\n\nFigure 2: Final performance for OffRL agents on atari 1% datasets. Results are normalized so that BC achieves a score of 1 and a randomly initialized policy achieves a score of 0. Error bars show standard deviation across three seeds for dataset generation.\n\n5.3 HYPERPARAMETER ABLATIONS\n\nFigure 3: Sweeps across εtrain and εeval on Seaquest and Pong. Color indicates performance, darker is better. Generally, we find that lower εtrain and higher εeval is beneficial. This setting of hyperparameters places us closer to one-step RL.\n\nTo validate the usefulness of generalized deep-SPIBB over deep-SPIBB with εtrain = εeval, we conduct hyperparameter sweeps over the two different types of epsilon. Results are shown in Figure 3. Essentially, we find that it is often beneficial to set εtrain substantially lower than εeval, especially on more challenging environments. This is consistent with the observations of Nadjahi et al. (2019); Brandfonbrener et al. (2021); Gulcehre et al. (2021) that the one-step algorithm that just performs one step of policy improvement is often a very strong algorithm (generalized soft-SPIBB captures the one-step algorithm for εtrain = 0). Moreover, the more challenging environments and datasets likely yield worse uncertainty estimates, making it more risky to propagate values with low uncertainty since the uncertainty may be erroneously low. Thus, setting a lower εtrain can learn a more robust Q function, while allowing a larger εeval can yield improved performance.\n\nThe plots also show that the hyperparameters are somewhat independent in the sense that the optimal value of εeval is stable across different values of εtrain. This observation can allow for more efficient hyperparameter tuning by avoiding a complete grid search.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n5.4 UNCERTAINTY ABLATIONS\n\nTo validate our choices about how to parameterize the uncertainty estimates, we run deep-SPIBB wih several different uncertainty estimators. The most important decision in our uncertainty estimate is to use separate estimates of u(s) and β(a|s) to derive u(s, a), as described above. This choice is validated by the results in Figure 4.\n\nEssentially, we see major gains of our factored uncertatinty estimator over an uncertainty estimator trained on s and a jointly. Interestingly, just using the behavior component of the uncertainty estimator also outperforms the joint uncertianty estimator suggesting that the statebased part of the uncertainty is either not very important in this task or so poorly estimated in this high dimensional state space that it adds little to the performance.\n\n5.5 CONNECTION TO PRIOR RESULTS\n\nFigure 4: Uncertainty ablations on Seaquest. “BC as unc” uses u(s, a) = , and “unc sa”\n\n1√\n\nˆβ(a|s)\n\ntrains an uncertainty estimator that takes s, a as input rather than using u(s) combined with β(a|s).\n\nThis chunk of empirical analysis builds on the results already reported in the SPIBB papers Nadjahi et al. (2019); Sim ̃ao et al. (2020) where the pessimistic algorithm RaMDP was found to perform as well as Soft-SPIBB under two conditions: (i) the uncertainty estimates are well-estimated, either from count-based statistical concentration bounds such as Hoeffding’s inequality, or from handcrafted uncertainty measures based on states’ similarity computed from their Euclidean distance in a welladapted environment, and (ii) the intrinsic/extrinsic reward balance is fine tuned, meaning that the pessimism hyperparameter setting appeared as more sensitive than Soft-SPIBB’s.\n\nOur novel empirical results bring light on the robustness of these two approaches when the uncertainty estimator is more brittle. In Cartpole, with its low-dimensional observation space, we find that both are able to take a significant advantage over the uncertainty-free methods. In Catch where the state representation is more image-like, we see that pessimism’s performance immediately crashes, while deep-SPIBB is more robust to these less-than-perfect uncertainty estimates and remains on-par with BCQ and CQL. Finally, in the Atari environments where the state is a complex image, pessimism is consistently and significantly worse than behavior cloning, which it cannot even fall back on.\n\nThis set of experiments with increasing difficulty in the uncertainty estimation (and the RL task) tells us that uncertainty-based algorithms are better than uncertainty-free algorithms when the uncertainty estimates are reliable but that out of the uncertainty-based algorithms, only deep-SPIBB is robust to bad estimates. While deep-SPIBB does not surpass CQL in hard environments for now, there is hope that advances in neural uncertainty estimation will allow it to do so in the future.\n\n6 DISCUSSION\n\nHere we have introduced the deep-SPIBB algorithm for inclorporating explicit uncertainty estimates into deep offine RL. We have seen that the deep-SPIBB mechanism of incorporating uncertainty into the policy improvement step is more performant and robust than the pessimism mechanism of incorportating uncertainty as a penalty in the evaluation step. When the uncertainty estimates are good, deep-SPIBB also improves over the uncertainty-free baselines of BCQ and CQL.\n\nOur work does have a few limitations that are worth mentioning to inspire future work in these directions. First, it is not clear how to extend the deep-SPIBB algorithm to continuous action spaces. Second, like most algorithmic work in offline RL, deep-SPIBB has important hyperparameters (εtrain and εeval) that govern the policy constraints. How to practically tune hyperparameters like these offline without interacting with the environment remains an open challenge, although recent work makes some progress (Paine et al., 2020; Zhang & Jiang, 2021). Finally, perhaps the most interesting direction for future work based on deep-SPIBB is to improve the uncertainty estimators. Our results suggest that access to better uncertainty estimators in challenging domains like Atari could dramatically improve deep-SPIBB over the uncertainty-free baselines.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pp. 104–114. PMLR, 2020.\n\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1613–1622, Lille, France, 07–09 Jul 2015. PMLR. URL http://proceedings.mlr. press/v37/blundell15.html.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and JAX: composable transformations of Python+NumPy programs, 2018. URL Qiao Zhang. http://github.com/google/jax.\n\nDavid Brandfonbrener, William F Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without\n\noff-policy evaluation. arXiv preprint arXiv:2106.08909, 2021.\n\nJacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in fixed-dataset\n\npolicy optimization. arXiv preprint arXiv:2009.06799, 2020.\n\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. In Proceedings of the 7th International Conference on Learning Representations (ICLR), 2019. URL https://openreview.net/forum?id=H1lJJnR5Ym.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 2021.\n\nKamil Ciosek, Vincent Fortuin, Ryota Tomioka, Katja Hofmann, and Richard Turner. Conservative uncertainty estimation by fitting prior networks. In International Conference on Learning Representations, 2019.\n\nWill Dabney, Mark Rowland, Marc Bellemare, and R ́emi Munos. Distributional reinforcement learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for\n\noffline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.\n\nScott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.\n\narXiv preprint arXiv:2106.06860, 2021.\n\nScott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch\n\ndeep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019a.\n\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019b.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059. PMLR, 2016.\n\nCaglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio G ́omez, Konrad Zolna, Rishabh Agarwal, Josh S Merel, Daniel J Mankowitz, Cosmin Paduraru, et al. Rl unplugged: A suite of benchmarks for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:7248–7259, 2020.\n\nCaglar Gulcehre, Sergio G ́omez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad Zolna, Yutian Chen, Matthew Hoffman, Razvan Pascanu, and Nando de Freitas. Regularized behavior value estimation. arXiv preprint arXiv:2103.09575, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMatt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alex Novikov, Sergio G ́omez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020. URL https://arxiv.org/ abs/2006.00979.\n\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\n\nmodeling problem. Advances in neural information processing systems, 34, 2021.\n\nYing Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In\n\nInternational Conference on Machine Learning, pp. 5084–5096. PMLR, 2021.\n\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. Advances in neural information processing systems, 33: 21810–21823, 2020.\n\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\n\nq-learning. arXiv preprint arXiv:2110.06169, 2021.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline\n\nreinforcement learning. arXiv preprint arXiv:2006.04779, 2020.\n\nBalaji Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty\n\nestimation using deep ensembles. In NIPS, 2017.\n\nRomain Laroche and R ́emi Tachet des Combes. SPIBB-DQN: Safe batch reinforcement learning with function approximation. In Proceedings of the 4th Reinforcement Learning and Decision Making (RLDM), 2019.\n\nRomain Laroche, Paul Trichelair, and R ́emi Tachet des Combes. Safe policy improvement with baseline bootstrapping. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,\n\nreview, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n\nYao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch reinforce-\n\nment learning without great exploration. arXiv preprint arXiv:2007.08202, 2020.\n\nKimia Nadjahi, Romain Laroche, and R ́emi Tachet des Combes. Safe policy improvement with soft baseline bootstrapping. In Proceedings of the 17th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), 2019.\n\nIan Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement\n\nlearning. arXiv preprint arXiv:1806.03335, 2018.\n\nIan Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, et al. Behaviour suite for reinforcement learning. arXiv preprint arXiv:1908.03568, 2019.\n\nIan Osband, Zheng Wen, Mohammad Asghari, Morteza Ibrahimi, Xiyuan Lu, and Benjamin Van Roy.\n\nEpistemic neural networks. arXiv preprint arXiv:2107.08924, 2021.\n\nGeorg Ostrovski, Marc G. Bellemare, A ̈aron van den Oord, and R ́emi Munos. Count-based exploration with neural density models. In Proceedings of the 34th International Conference on Machine Learning (ICML), pp. 2721–2730, 2017.\n\nTom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for offline reinforcement learning. arXiv preprint arXiv:2007.09055, 2020.\n\nMarek Petrik, Mohammad Ghavamzadeh, and Yinlam Chow. Safe policy improvement by minimizing robust baseline regret. In Proceedings of the 29th Advances in Neural Information Processing Systems (NIPS), 2016.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nBruno Scherrer, Victor Gabillon, Mohammad Ghavamzadeh, and Matthieu Geist. Approximate\n\nmodified policy iteration. arXiv preprint arXiv:1205.3054, 2012.\n\nNoah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint arXiv:2002.08396, 2020.\n\nThiago D. Sim ̃ao, Romain Laroche, and R ́emi Tachet des Combes. Safe policy improvement with estimated baseline bootstrapping. In Proceedings of the 19th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS, in review), 2020.\n\nZiyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. arXiv preprint arXiv:2006.15134, 2020.\n\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.\n\narXiv preprint arXiv:1911.11361, 2019.\n\nMing Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with\n\npessimism. Advances in neural information processing systems, 34, 2021.\n\nMing Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal offline reinforcement learning via double\n\nvariance reduction. Advances in neural information processing systems, 34, 2021.\n\nTianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129–14142, 2020.\n\nTianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. Advances in Neural Information Processing Systems, 34, 2021.\n\nSiyuan Zhang and Nan Jiang. Towards hyperparameter-free policy selection for offline reinforcement\n\nlearning. Advances in Neural Information Processing Systems, 34, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA UNCERTAINTY ESTIMATOR\n\nWe take an approach based off of Bayesian ensembling with prior network from Ciosek et al. (2019). While that work focused on estimating uncertainty in supervised learning problems, we use the same technique with no labels to get a measure of state-based uncertainty. Explicitly, we create an ensemble of B models that each take in a state s and output an M -dimensional vector. Each component fi of the ensemble has a corresponding random prior function pi defined by a randomly initialized network. The fi networks are then trained to predict difference between the prior and some target (in our case, the target is gaussian noise εi sampled from N (0, σε) for each datapoint i). State-based uncertainty is then calculated as\n\n(cid:113)\n\nˆu(s) =\n\nˆσ2\n\nμ(s) + αˆvσ(s),\n\nˆσ2\n\nμ(s) =\n\n1 M B\n\nB (cid:88)\n\ni=1\n\n∥fi(s) − pi(s)∥2,\n\nˆv2\n\nσ(s) =\n\n1 B\n\nB (cid:88)\n\n(cid:18)\n\ni=1\n\nˆσ2\n\nμ(s) −\n\n1 M\n\n∥fi(s) − pi(s)∥2\n\n(9)\n\n(cid:19)2\n\nB EXPERIMENTAL DETAILS\n\nHyperparameters. First we will provide all of the hyperparameters used in the various steps of our training algorithms. Every algorithm is trained with the Adam optimizer.\n\nTable 3: Hyperparameters for behavior estimation in cartpole and catch\n\nHyperparameter Training steps Learning rate Batch size MLP width MLP depth Prior MLP depth\n\nValue 1e4 1e − 3 256 64 2\n1\n\nTable 4: Hyperparameters for behavior estimation in Atari\n\nHyperparameter Training steps Learning rate Batch size Network Architecture\n\nValue 1e5 1e − 4 256 DQN\n\nTable 5: Hyperparameters for uncertainty estimation in cartpole and catch\n\nHyperparameter Value\n\nTraining steps Learning rate Batch size MLP width MLP depth M\nB α\nσε\n\n1e4 1e − 4 256 256 2\n64 5\n1.0 0.1\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Hyperparameters for uncertainty estimation in Atari\n\nValue 1e5 1e − 4 256 DQN\n\nHyperparameter Training steps Learning rate Batch size Network architecture Prior network architecture DQN - 1 dense layer M\nB α\nσε\n\n64 5\n1.0 0.1\n\nTable 7: Shared hyperparameters for RL in cartpole and catch\n\nHyperparameter Training steps Learning rate Batch size Target update period MLP width MLP depth QR quantiles Discount\n\nValue 1e5 3 − 5 256 1000 256 2\n201 0.99\n\nTable 8: Shared hyperparameters for RL in Atari\n\nValue Hyperparameter 1e6 Training steps 3 − 5 Learning rate 256 Batch size 2500 Target update period Network architecture DQN 201 QR quantiles 0.99 Discount\n\nEach RL algorithm also has specific hyperparameters. For each algorithm we choose four values of the hyperparameter. We should note that for the deep-SPIBB and pessimism that rely on our learned uncertainty estimates, we normalize the values of ε and α respectively to the scale of the uncertainty estimator. We estimate the scale of the uncertainty function by just evaluating the mean of the uncertainty function on a batch of data from the training set.\n\nTable 9: Algorithm-specific hyperparameters for cartpole and catch\n\nAlgorithm\n\nBCQ Pessimism CQL deep-SPIBB gen deep-SPIBB\n\nHyperparameter τ\nα α\nεtrain εeval\n\nValue [0.01, 0.03, 0.1, 0.3] [0.3, 1.0, 3.0, 10.0] [0.3, 1.0, 3.0, 10.0] [0.1, 0.3, 1.0, 3.0] [0.001, 0.01, 0.1, 1.0]\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 10: Algorithm-specific hyperparameters for Atari\n\nAlgorithm\n\nBCQ Pessimism CQL deep-SPIBB gen deep-SPIBB\n\nHyperparameter τ\nα α\nεtrain εeval\n\nValue [0.01, 0.03, 0.1, 0.3] [0.1, 1.0, 10.0, 100.0] [0.3, 1.0, 3.0, 10.0] [0.001, 0.01, 0.03, 0.1] [0.0001, 0.001, 0.01, 0.1]\n\nEvaluation. For evaluation we run 50 episodes of each trained RL model and take the mean. Plots in the text then report the mean and standard deviation of this mean value across training seeds. We report the results for the best performing hyperparameter out of those in the table for each algorithm.\n\nCompute. All models are trained on various types of GPU on an internal cluster. Each run for cartpole/catch takes less than 15 minutes and each run on Atari takes less than 1 day.\n\nAsset licenses. For completeness, we also report the licenses of the assets that we used in the paper: JAX Bradbury et al. (2018): Apache-2.0, Acme Hoffman et al. (2020): Apache-2.0, RL-unplugged Gulcehre et al. (2020): Apache-2.0, bsuite Osband et al. (2019): Apache-2.0.\n\n15",
    "reference": "# Summary Of The Paper\n\nThis paper studies how to incorporate uncertainty estimations for offline reinforcement learning to prevent the learner from favoring regions of high uncertainty (which are often over-estimated). The authors extend safe policy improvement with soft baseline bootstrapping (soft-SPIBB) to large state-action space, where count-based uncertainty measures become infeasible, by leveraging a neural uncertainty estimator (using random priors).\n\n# Strength And Weaknesses\n\nPros:\n\n- The authors extend soft-SPIBB to large state-action space and the empirical performance seemed comparable to CQL. \n\n- The design of the estimator $\\hat{u}(s, a) := \\hat{u}(s) / \\sqrt{\\hat{\\beta}(a|s)}$ is sensible.\n\n- The discussion about constrained improvement step versus penalized evaluation step is interesting. \n\nCons:\n\nLimited novelty:\n\n- The authors modified soft-SPIBB (Nadjahi et al., 2019)  by replacing (count-based) error functions $e(s, a)$ with neural uncertainty estimates $\\hat{u}(s, a)$.\n\n- Estimating state uncertainty via random priors is not new in reinforcement learning, e.g. RND (Burda et al., 2019).\n\nRelated works: prior works should be more explicitly explained \n\n- RND is only mentioned in \"there is a large literature from the deep learning community on uncertainty quantification that we can leverage for OffRL (... Burda et al., 2019; ...)\", which does not emphasize that random priors methods have already been applied in reinforcement learning literature, although in an online setting.\n\n- It is encouraged to briefly explain the essence of SPIBB and soft-SPIBB in e.g. preliminaries, so that it is easier to see which parts in Eqn (7) are proposed by the authors, especially for the audiences who are not familiar with soft-SPIBB.\n\nQuestions/additional comments:\n\n- Introducing $\\epsilon_{eval}$ seemed a bit odd to me. Do the authors mean: (a) first train $\\hat{Q}$ until convergence with $\\epsilon_{train}$ and (b) then do one-step policy improvement with $\\epsilon_{test}$?\n\n- Could the authors plot the training curves of experiments in 5.1 and 5.2? As Eqn (7) is now approximated by a greedy heuristic (if my understanding is correct), it is good to see whether it is requiring more iterations to converge.\n\n- The comparison versus Pessimism is not necessarily fair. Deep-SPIBB keeps most essential parts of soft-SPIBB except using a neural quantifier instead of a count-based one. While Pessimism does not keep all key designs of e.g.  (Buckman et al., 2020; Jin et al., 2021), Pessimism should not recover theoretical guarantees of pessimistic approaches while using any count-based uncertainty quantifier because of its simplification. Therefore it does not fully support the claim made in the abstract, \"we argue that the SPIBB mechanism for incorporating uncertainty is more robust and flexible than pessimistic approaches that incorporate the uncertainty as a value function penalty.\"\n\n- I believe pessimistic approaches should be able to recover behavior cloning with proper choice of uncertainty penalty and also proper algorithmic designs. For example, see [1].\n\n[1] Rashidinejad, Paria, et al. \"Bridging offline reinforcement learning and imitation learning: A tale of pessimism.\" Advances in Neural Information Processing Systems 34 (2021): 11702-11716.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: Good, the writing is overall clear but could be improved, for examples\n\n-  The abbr. SPIBB appeared without its full name in the main text.\n\n- Page 5, \"we use the approximation technique described in Nadjahi et al. (2019).\" I would recommend including the optimization pseudo code in the appendix. Otherwise one has to go through Nadjahi et al. (2019) to locate it in their Appendix A.8 (Arxiv version).\n\n- A lot of details are given in a quite verbal way, the readability could be improved.\n\nQuality: Fair, some statements are not well supported and the experiments could be more comprehensive.\n\nNovelty: Fair, novelty is limited because of Nadjahi et al. (2019) and  Burda et al. (2019). The discussion about constrained improvement step versus penalized evaluation step could be important. However, the attempts made in this version are not convincing enough.\n\nReproducibility: Good, code and hyper-params are provided.\n\n# Summary Of The Review\n\nAlthough the proposed algorithm is well-motivated (upon some existing works), its novelty is limited, and the discussions and experiments could use some improvements. I am leaning toward a (weak) reject at this moment.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nAUTOSKDBERT: LEARN TO STOCHASTICALLY DISTILL BERT\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nIn this paper, we propose AutoSKDBERT, a new knowledge distillation paradigm for BERT compression, that stochastically samples a teacher from a predefined teacher team following a categorical distribution in each step, to transfer knowledge into student. AutoSKDBERT aims to discover the optimal categorical distribution which plays an important role to achieve high performance. The optimization procedure of AutoSKDBERT can be divided into two phases: 1) phase-1 optimization distinguishes effective teachers from ineffective teachers, and 2) phase2 optimization further optimizes the sampling weights of the effective teachers to obtain satisfactory categorical distribution. Moreover, after phase-1 optimization completion, AutoSKDBERT adopts teacher selection strategy to discard the ineffective teachers whose sampling weights are assigned to the effective teachers. Particularly, to alleviate the gap between categorical distribution optimization and evaluation, we also propose a stochastic single-weight optimization strategy which only updates the weight of the sampled teacher in each step. Extensive experiments on GLUE benchmark show that the proposed AutoSKDBERT achieves state-of-the-art score compared to previous compression approaches on several downstream tasks, including pushing MRPC F1 and accuracy to 93.2 (0.6 point absolute improvement) and 90.7 (1.2 point absolute improvement), RTE accuracy to 76.9 (2.9 point absolute improvement).\n\n1\n\nINTRODUCTION\n\nBERT (Devlin et al., 2019) has brought about a sea change in the field of Natural Language Processing (NLP). Following BERT, numerous subsequent works focus on various perspectives to further improve its performance, e.g., hyper-parameter (Liu et al., 2019b), pre-training corpus (Liu et al., 2019b; Raffel et al., 2020), learnable embedding paradigm (Raffel et al., 2020), pre-training task (Clark et al., 2020), architecture (Gao et al., 2022) and self-attention (Shi et al., 2021), etc. However, there are massive redundancies in the above BERT-style models w.r.t. attention heads (Michel et al., 2019; Dong et al., 2021), weights (Gordon et al., 2020), and layers (Fan et al., 2020). Consequently, many compact BERT-style language models are proposed via pruning (Fan et al., 2020; Guo et al., 2019), quantization (Shen et al., 2020), parameter sharing (Lan et al., 2020) and Knowledge Distillation (KD) (Iandola et al., 2020; Pan et al., 2021). In this paper, we focus on the KD-based compression approaches.\n\nFrom the point of view of learning procedure, KD is used in the pre-training (Turc et al., 2019; Sanh et al., 2019; Sun et al., 2020; Jiao et al., 2020) and fine-tuning phases (Sun et al., 2019; Jiao et al., 2020; Wu et al., 2021). On the other hand, from the point of view of distillation objective, KD is employed for the outputs of hidden layer (Sun et al., 2020), final layer (Wu et al., 2021), embedding (Sanh et al., 2019) and self-attention (Wang et al., 2020). Wu et al. (2021) employ multiple teachers to achieve better performance than single-teacher KD based approaches on several downstream tasks of GLUE benchmark (Wang et al., 2019). As shown in Table 1, nevertheless, the ensemble of multiple teachers are not always more effective than the single teacher for student distillation. There are two possible reasons: 1) diversity losing (Tran et al., 2020) and 2) capacity gap (Mirzadeh et al., 2020). On the one hand, the ensemble prediction of multi-teacher KD loses the diversity of each teacher. On the other hand, between the large-capacity teacher ensemble and small-capacity student, there is a capacity gap which can be prone to unsatisfactory distillation performance.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Performances of knowledge distillation using single and multiple teachers for a 6-layer BERT-style language model on the development set of GLUE benchmark. In this experiment, we employ five teachers, i.e. T10 to T14 shown in Appendix C.1, for single-teacher distillation and multi-teacher distillation. We introduce the implementation details in Appendix H.\n\nTask Metrics Best Single Teacher† Single-teacher KD Multi-teacher KD Gain † The best teacher for student distillation on each downstream task as shown in Table 15. ‡ Pre-training with whole word masking.\n\nMRPC F1+acc 2\nT13 90.0 89.7 -0.3\n\nCoLA Mcc T10 49.3 50.1 +0.8\n\nSST-2 acc T12 93.1 92.2 -0.9\n\nRTE acc T10 73.3 73.7 +0.4\n\nQQP F1+acc 2\nT11 89.0 88.6 -0.4\n\nQNLI MNLI\n\nacc T12 91.4 91.1 -0.3\n\nm T14‡ 83.5 83.6 +0.1\n\nTo solve the above mentioned issues, we propose AutoSKDBERT which stochastically samples a teacher from a predefined teacher team following a categorical distribution in each step, to transfer knowledge into student. The task of AutoSKDBERT is learning the optimal categorical distribution to achieve high performance. 1) Given a teacher team which consists of multiple teachers with multi-level capacities, AutoSKDBERT optimizes an initialized categorical distribution to distinguish effective teachers from ineffective teachers in phase-1 optimization. 2) The sampling weights of the ineffective teachers are assigned to the effective teachers via teacher selection strategy after phase-1 optimization completion. 3) AutoSKDBERT further optimizes the weights of the effective teachers rather than the ineffective teachers’ in phase-2 optimization. We implement extensive experiments on GLUE benchmark (Wang et al., 2019) to verify the effectiveness of the proposed AutoSKDBERT. Moreover, to show the generalization capacity, we have also distilled deep convolutional neural network (e.g., ResNet (He et al., 2016), Wide ResNet (Zagoruyko & Komodakis, 2016)) by AutoSKDBERT for image classification on CIFAR-100 (Krizhevsky et al., 2009), as shown in Appendix B. Our contributions are summarized as follows1:\n\n• We propose AutoSKDBERT which stochastically samples a teacher from the predefined teacher team following the categorical distribution in each step, to transfer knowledge into the student of BERT-style language model.\n\n• We propose a two-phase optimization framework with teacher selection strategy to select effective teachers and learn the optimal categorical distribution in a differentiable way.\n\n• We propose Stochastic Single-Weight Optimization (SSWO) strategy to alleviate the consistency gap between the categorical distribution optimization and evaluation for performance improvement.\n\n2 THE PROPOSED AUTOSKDBERT\n\n2.1 OVERVIEW\n\nIn each step, AutoSKDBERT samples a teacher ˆT from a teacher team which consists of n multilevel BERT-style teachers T1:n, to transfer knowledge into student S. The objective function of AutoSKDBERT can be expressed as\n\nL(w) =\n\n(cid:88)\n\nx∈X\n\nLd(f ˆT∈T1:n\n\n(x), fS(x; w)),\n\n(1)\n\nwhere Ld represents distilled loss function to compute the difference between the student S with learnable parameter w and the sampled teacher ˆT, X denotes the training data, f ˆT∈T1:n (·) and fS(·) denote the logits from ˆT and S, respectively. In AutoSKDBERT, a categorical distribution Cat(θ) where θ = {θ1:n} and (cid:80)n i=1 θi = 1, is employed to sample the teacher from the teacher team. Particularly, the probability p(Ti) of Ti being sampled is θi. We observe that Cat(θ) plays an important role for obtaining high performance of AutoSKDBERT. As a result, the task of AutoSKDBERT then turns into learning the optimal categorical distribution Cat(θ∗), as illustrated in Figure 1.\n\n1The code will be made publicly available upon publication of the paper.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Two-phase optimization framework with teacher selection strategy for AutoSKDBERT. 1) For a predefined teacher team, AutoSKDBERT optimizes an initialized categorical distribution to distinguish effective teachers from ineffective teachers. 2) After phase-1 optimization completion, the sampling weights of the ineffective teachers are assigned to the effective teachers via teacher selection strategy. 3) AutoSKDBERT further optimizes the weights of the effective teachers rather than the ineffective teachers in phase-2 optimization. Best viewed in color.\n\n2.2 PROBLEM FORMULATION\n\nAutoSKDBERT has two groups of learnable parameter: 1) w of student and 2) θ of categorical distribution. We split original training data into training and validation subsets, and denote Ltrain and Lval as the losses on training and validation subsets, respectively. Both Ltrain and Lval are determined not only by Cat(θ), but also by w. Particularly, AutoSKDBERT aims to learn the best categorical distribution Cat(θ∗) that minimizes the validation loss Lval(w∗, Cat(θ)), where the weights w∗ associated with the categorical distribution Cat(θ) are obtained by argminw Ltrain(w, Cat(θ)). Consequently, AutoSKDBERT can be considered as a bilevel optimization problem (Colson et al., 2007) with upper-level variable Cat(θ) and lower-level variable w:\n\nmin Cat(θ)\n\nLval(w∗(Cat(θ)), Cat(θ)),\n\ns.t. w∗(Cat(θ)) = argmin\n\nw\n\nLtrain(w, Cat(θ)).\n\n(2)\n\nWe optimize w of student (see Section 2.3) and θ of categorical distribution (see Section 2.4) in an alternate and iterative way, and show the optimization algorithm in Algorithm 1.\n\n2.3 STUDENT DISTILLATION\n\nFor student distillation, Cat(θ) is frozen. Similar to Eq. 1, we utilize the following object function: Ld(ˆθf ˆT∈T1:n\n\n(x), fS(x; w)),\n\nL(w) =\n\n(cid:88)\n\n(3)\n\nwhere ˆθ indicates the probability of the teacher ˆT being sampled from T1:n according to Cat(θ).\n\nx∈X\n\n3\n\nStudentTeacher TeamSWeight FreezePhase-1 OptimizationPhase-2 OptimizationTeacher SelectionTrainingSubsetValidationSubsetTraining Data0.1990.2010.1990.2000.2010.0550.0750.4000.1950.275000.4600.2240.316000.3800.2540.366TrainingSubsetValidationSubsetTraining DataStudent DistillationCategoricalDistribution OptimizationTwo-phase OptimizationCategorical Distribution Initialization Student DistillationCategoricalDistribution Optimization0.0550.0750.4000.1950.2750.0550.075SmallestMaskGeneration00111WeightMask000.4000.1950.275WeightNormalization000.4600.2240.316SampleDistillSampleDistillDistillDistillSampleSampleSplitSplit(Sec. 2.3)(Sec. 2.4)(Sec. 2.3)(Sec. 2.4)(Sec. 2.4.2)Under review as a conference paper at ICLR 2023\n\nAlgorithm 1: Two-phase Optimization for AutoSKDBERT Initialize categorical distribution Cat(θ(1)) for phase-1 optimization, weights w of student,\n\nmaximum step N , current step n = 0;\n\nwhile n < N\n\n2 do\n\nUpdate Cat(θ(1)) by descending Eq. 7 ; distribution optimization\n\nUpdate w by descending Eq. 3 ; n = n + 1;\n\n// phase-1 categorical\n\n// phase-1 student distillation\n\nend Select effective teachers to generate Cat(θ(2)) by Eq. 8; while N\n\n2 ≤ n < N do\n\nUpdate Cat(θ(2)) by descending Eq. 7 ; distribution optimization\n\n// teacher selection\n\n// phase-2 categorical\n\nUpdate w by descending Eq. 3 ; n = n + 1;\n\nend\n\n// phase-2 student distillation\n\n2.4 CATEGORICAL DISTRIBUTION OPTIMIZATION\n\nFor categorical distribution optimization, w is frozen. We propose a two-phase optimization framework with teacher selection strategy to learn appropriate categorical distribution:\n\n1. Phase-1 Optimization distinguishes effective teachers from ineffective teachers in the\n\nteacher team according to Cat(θ);\n\n2. Teacher Selection discards the ineffective teachers whose weights are assigned to the ef-\n\nfective teachers;\n\n3. Phase-2 Optimization further optimizes the weights of the effective teachers rather than\n\nthe ineffective teachers;\n\nwhere a Stochastic Single-Weight Optimization (SSWO) strategy is proposed for categorical distribution optimization. Below, categorical distribution optimization and teacher selection strategy are introduced in detail.\n\n2.4.1 CATEGORICAL DISTRIBUTION OPTIMIZATION VIA SSWO\n\nTo optimize Cat(θ) in a differentiable way, Continuous Relaxation (CR) (Liu et al., 2019a) is a common technique to obtain mixture of logits w.r.t. teachers as\n\nf T1:n (x; Cat(θ)) =\n\nn (cid:88)\n\ni=1\n\nθifTi(x).\n\n(4)\n\nSubsequently, Cat(θ) can be optimized by an approximation scheme:\n\n∇Cat(θ)Lval(w∗(Cat(θ)), Cat(θ)) ≈ ∇Cat(θ)Lval(w − α∇wLtrain(w, Cat(θ)), Cat(θ)),\n\n(5)\n\nwhere w and α indicate the current weights of the student and the learning rate of categorIn particular, we employ w with a single-step adapting (i.e., ical distribution, respectively. w − α∇wLtrain(w, Cat(θ)) to appropriate w∗(Cat(θ)) for avoiding the inner optimization in Eq. 2. This appropriation scheme has been widely used in meta-learning (Finn et al., 2017) and neural architecture search (Liu et al., 2019a).\n\nHowever, in the case of CR, there is a consistency gap between the categorical distribution optimization and evaluation in terms of the teacher’s logits. For categorical distribution optimization, f T1:n (x; Cat(θ)) is used to compute the difference between the student’s logits as (cid:80) x∈X Ld(f T1:n (x; Cat(θ)), fS(x; w)). For categorical distribution evaluation, however, only the (x) is used to obtain the difference to the student’s logits as\n\nlogits of the sampled teacher f ˆT∈T1:n (cid:80)\n\n(x), fS(x; w)).\n\nx∈X Ld(f ˆT∈T1:n\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTo alleviate the consistency gap, we propose SSWO whose objective function can be written as\n\nL(w; ˆθ) =\n\n(cid:88)\n\nx∈X\n\nLd(ˆθf ˆT∈T1:n\n\n(x), fS(x; w)),\n\n(6)\n\nwhere ˆθ plays also a role like label smoothing (Szegedy et al., 2016) which aims to reduce the confidence coefficient of the sampled teacher and avoid over fitting (M ̈uller et al., 2019) of the categorical distribution. Moreover, the smaller the sampling weight, the more reduction the confidence coefficient of the sampled teacher. Subsequently, the sampled single-weight ˆθ can be optimized by ∇ˆθ∼Cat(θ)Lval(w∗(ˆθ), ˆθ) ≈ ∇ˆθ∼Cat(θ)Lval(w − α∇wLtrain(w, ˆθ), ˆθ). (7)\n\nIn practice, the proposed SSWO achieves better performance than CR, as shown in Section 4.2.\n\n2.4.2 TEACHER SELECTION\n\nAfter phase-1 optimization completion, m ineffective teachers are separated from the teacher team according to the current categorical distribution Cat(θ(1)), where the smaller the weight, the more ineffective the teacher. For avoiding categorical distribution optimizing from scratch, we present teacher selection strategy which assigns the weights of m ineffective teachers to n − m effective teachers, to deliver the categorical distribution Cat(θ(2)) for phase-2 optimization by\n\nCat(θ(2)) =\n\nCat(θ(1))mask(m smallest(Cat(θ(1)), m)) max((cid:107)Cat(θ(1))mask(m smallest(Cat(θ(1)), m))(cid:107)p, (cid:15))\n\n,\n\n(8)\n\nwhere p (1 in this paper) denotes the exponent value in the norm formulation, (cid:15) is a small value (1e-12 in this paper) to avoid division by zero, m smallest(Cat(θ(1)), m) obtains m indexes of ineffective teachers according to Cat(θ(1)), and mask(·) generates a mask where the values of m ineffective and n − m effective teachers are set to 0 and 1, respectively.\n\n3 EXPERIMENTS AND RESULTS\n\n3.1 DATASETS AND SETTINGS\n\nDatasets. We evaluate the proposed AutoSKDBERT on GLUE benchmark (Wang et al., 2019), including MRPC (Dolan & Brockett, 2005), RTE (Bentivogli et al., 2009), CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), QQP (Chen et al., 2018), QNLI (Rajpurkar et al., 2016) and MNLI (Williams et al., 2017). Moreover, STS-B (Cer et al., 2017) is not selected.\n\nSettings. We employ the development set of GLUE benchmark dubbed as GLUE-dev, for categorical distribution evaluation of AutoSKDBERT. We employ a teacher team which consists of 14 BERT-style teachers, to distill a 6-layer BERT-style student dubbed AutoSKDBERT. The architecture information of the student and the teachers can be found in Appendix C.1. On the one hand, we employ weak T01 to T09 (refer to Table 12) to verify a guess that the diversities of those weak teachers contribute to improve the distillation performance or not. On the other hand, under a conclusion that the extreme strong teacher (i.e., T13 and T14) can not always contribute to improving the distillation performance (see Appendix B.4 and F in the revised manuscript), we employ strong T13 and T14 to verify the effectiveness of the proposed distillation paradigm for capacity gap alleviation. We give a general way to design the teacher team and determine the value of m in Appendix A.\n\n3.2 TWO-PHASE OPTIMIZATION\n\nWe employ identical experimental settings for student distillation and categorical distribution optimization in both phase-1 and phase-2 optimization. The original training set is split fifty-fifty into two subsets, i.e., training subset for student distillation (see Section 2.3) and validation subset for categorical distribution optimization (see Section 2.4).\n\n3.2.1 STUDENT DISTILLATION\n\nWe choose Adam with a weight decay of 1e-4 as the optimizer for student distillation. For various downstream tasks, we employ different batch size, learning rate and epoch number as shown in Table 2. Other hyper-parameters can be found in Appendix C.2.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: The hyper-parameters for student distillation.\n\nHyper-parameter MRPC RTE CoLA SST-2 QQP QNLI MNLI Batch Size Learning Rate Epoch Number\n\n16 1e-5 50\n\n32 3e-5 2\n\n32 1e-5 50\n\n32 1e-5 50\n\n32 2e-5 2\n\n32 2e-5 10\n\n64 1e-5 10\n\n3.2.2 CATEGORICAL DISTRIBUTION OPTIMIZATION\n\nFor categorical distribution optimization, we employ other Adam with a weight decay of 1e-3 as the optimizer. There are two important hyper-parameters: 1) the number of the ineffective teacher and 2) learning rate for categorical distribution optimization. Similarly, for different downstream tasks, the above two parameters are various as shown in Table 9. Other hyper-parameters are identical to student distillation. The impact of each hyper-parameter is discussed in Appendix E.\n\nTable 3: The hyper-parameters for categorical distribution optimization.\n\nHyper-parameter Ineffective Teacher Number Learning Rate\n\nMRPC RTE CoLA SST-2 QQP QNLI MNLI 10 9e-4\n\n9 1e-3\n\n4 7e-4\n\n9 4e-4\n\n1 9e-4\n\n1 6e-4\n\n8 1e-3\n\n3.3 CATEGORICAL DISTRIBUTION EVALUATION\n\n3.3.1\n\nIMPLEMENTATION DETAILS\n\nAutoSKDBERT delivers 25 categorical distribution candidates in phase-2 optimization, and trains the student with 25 candidates from scratch to choose the optimal categorical distribution. In addition to epoch number, other hyper-parameters (e.g., batch size, learning rate, etc.) are identical to student distillation on various downstream tasks as shown in Table 2. The epoch number is set to 15 on MRPC, RTE, CoLA tasks, and 5 on SST-2, QQP, QNLI and MNLI tasks.\n\n3.3.2 LEARNED CATEGORICAL DISTRIBUTION\n\nWe show the categorical distributions learned by AutoSKDBERT on GLUE benchmark in Figure 2. Each teacher model shows various importances on different downstream tasks. 1) The strongest teacher T14 plays a dominant role on CoLA, SST-2 and QNLI tasks. 2) Low-capacity teachers, e.g., T02 to T06, can also provide useful knowledge for student distillation on MRPC, CoLA and QNLI tasks. 3) The capacity of the effective teacher is not always larger than the discard teachers on RTE, SST-2 and QQP tasks. Moreover, the search and evaluation costs with respect to each downstream task are shown in Appendix F.\n\nFigure 2: Categorical distributions learned on GLUE benchmark.\n\n3.4 RESULTS AND ANALYSIS ON GLUE BENCHMARK\n\nTable 4 summarizes the performance of AutoSKDBERT and the comparative approaches on GLUEdev. The proposed AutoSKDBERT achieves state-of-the-art performance on four out of seven tasks. AutoSKDBERT contributes to achieving better performance on those tasks with small data size, e.g., MRPC and RTE. On MRPC, AutoSKDBERT achieves 93.2 F1 score and 90.7 accuracy score which are 0.6 and 1.2 point higher than previous state-of-the-art MoEBERT (Zuo et al., 2022), respectively.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Results of AutoSKDBERT and other popular approaches on GLUE-dev. All comparative approaches have identical architecture, i.e., 6-layer BERT-style language model with 66 million parameters. † and ‡ indicate that the results are cited from Xu et al. (2020) and Zuo et al. (2022), respectively. ∗ means that the comparison between TinyBERT6 and AutoSKDBERT may not be fair since the former employs GloVe word embedding (Pennington et al., 2014) based data augmentation and transformer-layer distillation and embedding-layer distillation. § indicates that the result is obtained by our settings with the distillation loss described in Wu et al. (2021), and the experimental details can be found in Appendix H. Moreover, the stronger teacher can not always contribute to improving the distillation performance of other approaches due to the capacity gap (Mirzadeh et al., 2020) as shown in Appendix G. Besides, we show also the performances of multi-teacher AvgKD and TAKD with T01 to T14 in Appendix C.4.\n\nModel\n\nPoor Man’s BERT6 (Sajjad et al., 2020) DistilBERT6 (Sanh et al., 2019) LayerDrop (Fan et al., 2020)† BERT-PKD (Sun et al., 2019)† BERT-of-Theseus (Xu et al., 2020) MiniLMv1 (Wang et al., 2020) MiniLMv2 (Wang et al., 2020)‡ TinyBERT6 (Jiao et al., 2020)∗ TinyBERT6 (w/o aug) (Jiao et al., 2020)‡ MT-BERT (Wu et al., 2021)§ MoEBERT (Zuo et al., 2022) AutoSKDBERT (Ours)\n\nMRPC RTE CoLA SST-2 QQP QNLI MNLI F1/acc -/80.2 87.5/- 85.9/- 85.7/- 89.0/- 88.4/- 88.9/-\n\nacc Mcc 65.0 59.9 65.2 66.5 68.2 71.5 72.1 90.6/89.3 73.4 72.2 90.8/87.0 72.2 92.6/89.5 74.0 93.2/90.7 76.9\n\n- 51.3 45.4 45.5 51.1 49.2 52.5 54.0 42.8 49.1 55.4 51.8\n\n88.4/-\n\nF1/acc -/90.4 -/88.5 -/88.3 -/88.4 -/89.6 -/91.0 -/91.1 88.0/91.1 -/90.6 87.1/90.4 88.4/91.4 88.0/91.0\n\nm 81.1 82.2 80.7 81.3 82.3 84.0 84.2 84.5 83.5 83.8 84.5 84.3\n\nacc 87.6 89.2 88.4 88.4 89.5 91.0 90.8 91.1 90.5 91.4 91.3 91.6\n\nacc 90.3 92.7 90.7 91.3 91.5 92.0 92.4 93.0 91.6 92.2 93.0 93.0\n\nOn the other hand, compared to TinyBERT (Jiao et al., 2020) and MoEBERT (Zuo et al., 2022) on RTE task, AutoSKDBERT achieves 3.5 and 2.9 point absolute improvement, respectively.\n\nHowever, on CoLA, TinyBERT and MoEBERT achieve 2.2 and 3.6 point absolute improvement compared to AutoSKDBERT, respectively. On the one hand, TinyBERT employs data augmentation and transformer layer distillation to achieve high performance. On the other hand, MoEBERT employs 1) more complex student whose architecture is an ensemble of multiple experts, and 2) extra distillation procedure, i.e., transformer layer distillation, to achieve novel performance.\n\nThe proposed approach is a general KD paradigm for BERT compression. Consequently, we implement also extensive experiments to verify the effectiveness for image classification on CIFAR-100 (see Appendix B) and the orthogonality with other approaches (see Appendix D).\n\n4 ABLATION STUDIES\n\n4.1 TWO-PHASE OPTIMIZATION: PHASE-1 VERSUS PHASE-2\n\nIn this section, AutoSKDBERT delivers also 25 categorical distribution candidates in phase-1 optimization. Subsequently, each categorical distribution candidate is trained from scratch using identical settings described in Section 3.3, and the best-performing one on each task is shown in Table 5.\n\nTable 5: The performance of AutoSKDBERT with the best categorical distribution learned in phase-1 and phase-2 optimization on GLUE-dev.\n\nTask Metrics Phase-1 Phase-2 Gain\n\nMRPC F1/acc 92.9/90.2 93.2/90.7 +0.3/+0.5\n\nRTE acc 73.7 76.9 +1.4\n\nCoLA Mcc 49.2 51.8 +2.7\n\nSST-2 acc 92.9 93.0 +0.1\n\nQQP F1/acc 87.6/90.7 88.0/91.0 +0.4/+0.3\n\nQNLI acc 91.3 91.6 +0.3\n\nMNLI m\n83.0 84.3 +1.3\n\nPhase-2 optimization achieves better performance than phase-1 optimization, e.g., the absolute improvement is more than 1.3 on RTE, CoLA and MNLI, where those teachers weaker than the student are prone to providing useless knowledge even noise disturbance. However, low-capacity teachers contribute to improving the performance of AutoSKDBERT on MRPC, SST-2 and QNLI.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n4.2 CATEGORICAL DISTRIBUTION UPDATE STRATEGY: CR-BASED VERSUS SSWO-BASED\n\nIn Section 2.4.1, we propose SSWO which stochastically samples a single-weight to optimize the categorical distribution, to alleviate the consistency gap between the categorical distribution optimization and evaluation of CR in terms of teachers’ logits. For AutoSKDBERT with CR, the used hyper-parameters of categorical distribution optimization and evaluation are identical to AutoSKDBERT with SSWO, as described in Section 3.2 and Section 3.3.\n\nThe proposed SSWO achieves better performance than CR on all tasks as shown in Table 6. Particularly, the absolute improvement is more than 1.6 point on RTE and CoLA tasks. Compared to Table 5, AutoSKDBERT with CR achieves higher performance than phase-1 AutoSKDBERT on six out of seven tasks. Consequently, useless teachers lead to more performance degradation than the consistency gap issue.\n\nTable 6: The performance of AutoSKDBERT with the best categorical distribution learned by CR and SSWO for categorical distribution optimization on GLUE-dev.\n\nTask Metrics CR SSWO\n\nMRPC F1/acc 92.9/90.2 93.2/90.7\n\nRTE CoLA SST-2 Mcc acc 50.2 74.7 51.8 76.9\n\nacc 92.7 93.0\n\nQQP F1/acc 87.9/91.0 88.0/91.0\n\nQNLI MNLI\n\nacc 91.5 91.6\n\nm 83.9 84.3\n\nIn addition to SSWO, heuristic optimization algorithms like evolutionary algorithm and reinforcement learning can also be used to determine the categorical distribution. In this paper, we choose the most efficient one, i.e., gradient-based SSWO.\n\n4.3 CATEGORICAL DISTRIBUTION GENERATION: RANDOM VERSUS LEARNING\n\nWe compare two groups of implementation of AutoSKDBERT with various algorithms for categorical distribution generation, i.e., random and learning, on GLUE-dev. For random algorithm, 200 categorical distributions are randomly generated for all teacher candidates. For learning algorithm, we employ different learning rates of 3e-4 to 1e-3 with an interval of 1e-4 for categorical distribution optimization. Moreover, the ineffective teacher number is identical to Section 3.3.2 on various tasks. Subsequently, each implementation delivers 25 categorical distributions in phase-2 optimization. Consequently, 200 categorical distributions are obtained. The comparison between random and leaning algorithms is shown in Figure 3.\n\nFigure 3: Comparison of AutoSKDBERT with random and learning algorithm for categorical distribution generation on GLUE-dev. Two types of algorithm are evaluated by 200 categorical distributions. MRPC and QQP tasks are evaluated by the average of F1 score and accuracy score, CoLA task is evaluated by Matthews correlation coefficient, and other tasks are evaluated by accuracy score. Best viewed in color.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nThe categorical distribution generation algorithm aims to achieve more high-performance AutoSKDBERTs. As shown in Figure 3, the proposed learning algorithm contributes to obtaining better categorical distribution than those randomly generated on each downstream task. Particularly, the learning algorithm plays a dominant role on MRPC, CoLA, QQP, QNLI and MNLI tasks. The best accuracy scores of random algorithm are 89.29 on QQP and 83.37 on MNLI, respectively. For the proposed learning algorithm, the worst accuracy scores are 89.11 on QQP and 83.44 on MNLI which rank the top 10 and the best in random algorithm, respectively.\n\n5 RELATED WORK\n\n5.1 PRE-TRAINED LANGUAGE MODEL\n\nBased on the transformer-style architecture (Vaswani et al., 2017), BERT (Devlin et al., 2019) achieves state-of-the-art performance on different natural language understanding benchmarks, e.g., GLUE (Wang et al., 2019), SQuAD (Rajpurkar et al., 2016; 2018). Subsequently, a great number of variants of BERT are proposed, e.g., XLNet (Yang et al., 2019), ELECTRA (Clark et al., 2020) with new pre-training objectives, RoBERTa (Liu et al., 2019b), T5 (Raffel et al., 2020) with larger pre-training corpus, ConvBERT (Jiang et al., 2020) with various architectures and Synthesizer (Tay et al., 2020) with developed transformer-like block w.r.t. the dot-product self-attention mechanism. Besides, previous pre-trained language models often have several hundred million parameters (e.g. 335 million of BERTLARGE (Devlin et al., 2019), even 175 billion of GPT-3 (Brown et al., 2020)) which contribute to delivering amazing performance on downstream tasks while exponentially increasing the difficulty of deployment on resource-constrained device. ALBERT (Lan et al., 2020) adopts parameter sharing strategy to reduce the parameters, and achieves competitive performance.\n\n5.2 KNOWLEDGE DISTILLATION FOR BERT-STYLE LANGUAGE MODEL COMPRESSION\n\nIn order to obtain device-friendly BERT-style language model, many KD-based compression approaches have been proposed. DistilBERT (Sanh et al., 2019) compresses a smaller, faster, cheaper and lighter 6-layer BERT-style language model via learning the soft target probabilities of the teacher in the pre-training stage. Sun et al. (2019) propose patient knowledge distillation which transfers knowledge from the last or every l layers, to compress BERT-style language model in the fine-tuning phase. In MobileBERT (Sun et al., 2020), an inverted-bottleneck BERT-style language model is pretrained to transfer knowledge to task-agnostic MobileBERT in a layer-to-layer way. The student in MiniLM (Wang et al., 2020) imitates not only the attention distribution of the teacher, but also the deep self-attention knowledge which reflects the difference between values. In both the pre-training and the fine-tuning phases, TinyBERT (Jiao et al., 2020) learns various knowledge from hidden layer, final layer, embedding and self-attention to achieve high performance. Moreover, GloVe word embedding (Pennington et al., 2014) based data augmentation technique is employed to further improve the performance of TinyBERT. MT-BERT (Wu et al., 2021) employs multiple teachers to achieve better performance than single-teacher KD based approaches on several downstream tasks.\n\n6 CONCLUSION\n\nThis work proposes AutoSKDBERT, which is a new paradigm of knowledge distillation for BERT model compression. A teacher is stochastically sampled from a predefined multi-level teacher team in each step to distill the student following a categorical distribution. We observe that the categorical distribution plays an important role for obtaining high-performance AutoSKDBERT. Consequently, we propose a two-phase optimization framework to learn the best categorical distribution via SSWO. The first phase distinguishes effective teachers from ineffective teachers. In the second phase, the effective teachers are further optimized. Moreover, before phase-2 optimization beginning, the ineffective teachers are discarded and their weights are assigned to the effective teachers via teacher selection strategy. Extensive experiments on GLUE benchmark show that the proposed AutoSKDBERT achieves state-of-the-art performance compared to popular compression approaches on several downstream tasks.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nSungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Variational information distillation for knowledge transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pp. 9163–9171, 2019.\n\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing\n\ntextual entailment challenge. In TAC, 2009.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 1877–1901, 2020.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. In Proceedings of the 11th Inter- national Workshop on Semantic Evaluation, 2017.\n\nZihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs. University of\n\nWaterloo, pp. 1–7, 2018.\n\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR, 2020.\n\nBenoˆıt Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization. Annals of\n\noperations research, 153(1):235–256, 2007.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, volume 1, pp. 4171–4186, 2019.\n\nBill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In\n\nThird International Workshop on Paraphrasing, IWP, 2005.\n\nYihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning, pp. 2793–2803. PMLR, 2021.\n\nAngela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. In 8th International Conference on Learning Representations, ICLR, 2020.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning (ICML), pp. 1126–1135. PMLR, 2017.\n\nJiahui Gao, Hang Xu, Han Shi, Xiaozhe Ren, LH Philip, Xiaodan Liang, Xin Jiang, and Zhenguo Li. AutoBERT-zero: Evolving bert backbone from scratch. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 10663–10671, 2022.\n\nMitchell A. Gordon, Kevin Duh, and Nicholas Andrews. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, pp. 143–155. Association for Computational Linguistics, 2020.\n\nFuming Guo, Sijia Liu, Finlay S Mungall, Xue Lin, and Yanzhi Wang. Reweighted proximal pruning\n\nfor large-scale language representation. arXiv preprint arXiv:1909.12486, 2019.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recogIn Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,\n\nnition. CVPR, pp. 770–778, 2016.\n\nByeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young Choi. Knowledge transfer via distillation of activation boundaries formed by hidden neurons. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3779–3787, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\n\npreprint arXiv:1503.02531, 2015.\n\nZehao Huang and Naiyan Wang. Like what you like: Knowledge distill via neuron selectivity\n\ntransfer. arXiv preprint arXiv:1707.01219, 2017.\n\nForrest N Iandola, Albert E Shaw, Ravi Krishna, and Kurt W Keutzer. Squeezebert: What can computer vision teach nlp about efficient neural networks? arXiv preprint arXiv:2006.11316, 2020.\n\nZi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. Convbert: Improving bert with span-based dynamic convolution. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 12837–12848, 2020.\n\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP, pp. 4163–4174, 2020.\n\nJangho Kim, SeongUk Park, and Nojun Kwak. Paraphrasing complex network: Network compression via factor transfer. In Advances in Neural Information Processing Systems, NeurIPS, volume 31, 2018.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu SoriIn 8th\n\ncut. ALBERT: A lite BERT for self-supervised learning of language representations. International Conference on Learning Representations, ICLR, 2020.\n\nHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.\n\nHanxiao Liu, Karen Simonyan, Yiming Yang, et al. DARTS: Differentiable architecture search. In\n\nInternational Conference on Learning Representations (ICLR), 2019a.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019b.\n\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In Advances\n\nin Neural Information Processing Systems, NeurIPS, volume 32, 2019.\n\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI Conference on Artificial Intelligence, AAAI, volume 34, pp. 5191–5198, 2020.\n\nRafael M ̈uller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help?\n\nIn\n\nAdvances in Neural Information Processing Systems (NeurIPS), volume 32, 2019.\n\nHaojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang Li, and Jun Huang. Meta-KD: A meta knowledge distillation framework for language model compression across domains. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP, pp. 3026– 3036, 2021.\n\nWonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pp. 3967–3976, 2019.\n\nNikolaos Passalis and Anastasios Tefas. Learning deep representations with probabilistic knowledge transfer. In Proceedings of the European Conference on Computer Vision, ECCV, pp. 268–284, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nBaoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou, and In Proceedings of the\n\nZhaoning Zhang. Correlation congruence for knowledge distillation. IEEE/CVF International Conference on Computer Vision, ICCV, pp. 5007–5016, 2019.\n\nJeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP, pp. 1532–1543, 2014.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP, pp. 2383–2392, 2016.\n\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions In Proceedings of the 56th Annual Meeting of the Association for Computational\n\nfor squad. Linguistics, ACL, pp. 784–789, 2018.\n\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In 3th International Conference on Learning Representations, ICLR, 2015.\n\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. Poor man’s bert: Smaller and faster\n\ntransformer models. arXiv preprint arXiv:2004.03844, 2020.\n\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version\n\nof BERT: Smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-BERT: Hessian based ultra low precision quantization of BERT. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 8815–8821, 2020.\n\nHan Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and James Tin-Yau Kwok. Sparsebert: Rethinking the importance analysis in self-attention. In International Conference on Machine Learning, ICML, pp. 9547–9557. PMLR, 2021.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP, pp. 1631–1642, 2013.\n\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient Knowledge Distillation for BERT Model Compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLPIJCNLP, pp. 4322–4331, 2019.\n\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: A compact task-agnostic BERT for resource-limited devices. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL, pp. 2158–2170, 2020.\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, , pp. 2818–2826, 2016.\n\nY Tay, D Bahri, D Metzler, D Juan, Z Zhao, and C Zheng. Synthesizer: Rethinking self-attention in\n\ntransformer models. arxiv 2020. arXiv preprint arXiv:2005.00743, 2, 2020.\n\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation.\n\nIn 8th\n\nInternational Conference on Learning Representations, ICLR, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nLinh Tran, Bastiaan S Veeling, Kevin Roth, Jakub Swiatkowski, Joshua V Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Sebastian Nowozin, and Rodolphe Jenatton. Hydra: Preserving ensemble diversity for model distillation. In International Conference on Machine Learning Workshop on Uncertainty and Robustness in Deep Learning, 2020.\n\nFrederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In Proceedings of the\n\nIEEE/CVF International Conference on Computer Vision, ICCV, pp. 1365–1374, 2019.\n\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact of student initialization on knowledge distillation. arXiv preprint arXiv:1908.08962, 2019.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. volume 30, 2017.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR, 2019.\n\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. MiniLM: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. In Advances in Neural Information Processing Systems, NeurIPS, volume 33, pp. 5776–5788, 2020.\n\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\n\nTransactions of the Association for Computational Linguistics, 7:625–641, 2019.\n\nAdina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2017 Conference of the North American Chapter of the Association for Computational Linguistics, NAACL, 2017.\n\nChuhan Wu, Fangzhao Wu, and Yongfeng Huang. One teacher is enough? pre-trained language model distillation from multiple teachers. In Findings of the Association for Computational Linguistics: ACL/IJCNLP, pp. 4408–4413, 2021.\n\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. BERT-of-Theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP, pp. 7859–7869, 2020.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems, NeurIPS, volume 32, 2019.\n\nJunho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp. 4133–4141, 2017.\n\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Proceedings of the British\n\nMachine Vision Conference 2016, BMVC, 2016.\n\nSergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the perfor-\n\nmance of convolutional neural networks via attention transfer. 2017.\n\nSimiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao, and Weizhu Chen. MoEBERT: from BERT to Mixture-of-Experts via importance-guided adaptation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL, pp. 1610–1623, 2022.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA INEFFECTIVE TEACHER NUMBER DETERMINATION AND TEACHER TEAM\n\nDESIGN\n\nA.1\n\nINEFFECTIVE TEACHER NUMBER DETERMINATION\n\nA simple way to choose the ineffective teacher number m is elaborately designing the teacher team and setting m to the number of weak teachers whose capacities are weaker than student. Moreover, the student itself can be treated as the above weak teacher.\n\nA.2 TEACHER TEAM DESIGN\n\nFirst, we should determine the strongest teacher and student. Next, we select several teacher assistants whose capacities are stronger than student but weaker than the strongest teacher. Finally, we choose also several weak teachers whose capacities are weaker than student. Above all, the predefined teacher team consists of several weak teachers, several teacher assistants and the strongest teacher.\n\nB AUTOSKD FOR IMAGE CLASSIFICATION\n\nTo verify the effectiveness of the proposed distillation paradigm on computer vision, we conduct three groups of experiment on CIFAR-100 (Krizhevsky et al., 2009) image classification dataset. Following CRD (Tian et al., 2020), we choose three student models: 1)WRN-16-2, 2) WRN-40-1 and 3) ResNet-8×4. WRN-d-w represents Wide ResNet with depth d and width factor w. ResNetd×4 indicates a 4 times wider network (namely, with 64, 128, and 256 channels for each block) with depth d. Moreover, in CRD, the above student models are distilled by WRN-40-2, WRN-40-2 and ResNet-32×4, respectively.\n\nB.1 DATASET\n\nAs a popular dataset for image classification, CIFAR-100 consists of 60000 images (50000 for training and 10000 for test) with 32×32 pixels. Similar to the experiment for BERT compression, the original training set is split fifty-fifty into two subsets, i.e., training subset for student distillation and validation subset for categorical distribution optimization.\n\nB.2 DETAILS OF TEACHER TEAM\n\nFor various student models, we select different teacher teams according to Appendix A.2 as shown in Table 7.\n\nTable 7: Details of teacher team for each student model.\n\nStudent WRN-16-2 WRN-40-1 WRN-40-1, WRN-16-2, WRN-22-2, WRN-28-2, WRN-34-2, WRN-40-2 ResNet-8×4 ResNet-8×4, ResNet-14×4, ResNet-20×4, ResNet-26×4, ResNet-32×4\n\nTeacher Team WRN-16-2, WRN-22-2, WRN-28-2, WRN-34-2, WRN-40-2\n\nMoreover, the performance of each teacher model on CIFAR-100 is shown in Table 8.\n\nTable 8: Performance of each teacher model on CIFAR-100.\n\nTeacher Accuracy Teacher Accuracy\n\nWRN-40-1 71.19\n\nWRN-16-2 73.18\n\nWRN-22-2 76.19 ResNet-8×4 ResNet-14×4 ResNet-20×4 ResNet-26×4 ResNet-32×4 77.96\n\nWRN-28-2 76.21\n\n76.14\n\n79.42\n\n72.85\n\n76.90\n\n78.75\n\nWRN-34-2 WRN-40-2\n\n75.61\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nB.3 CATEGORICAL DISTRIBUTION OPTIMIZATION\n\nSimilar to the experiment for BERT compression, there are two hyper-parameters, i.e., ineffective teacher number and learning rate for categorical distribution optimization. According to Appendix A.1, we fix ineffective teacher number to 1, and choose categorical distribution learning rate from 3e4 to 1e-3 with an interval of 1e-4 for three groups of experiment. Different from BERT compression, we choose SGD as the optimizer with CosineAnnealing learning rate scheduler, initial learning rate of 0.05 and batch size of 64 for student model training. Moreover, the number of epochs is set to 50, and later 25 epochs deliver 25 categorical distribution candidates.\n\nTable 9: The hyper-parameters for categorical distribution optimization.\n\nHyper-parameter Ineffective Teacher Number Learning Rate\n\nWRN-16-2 WRN-40-1 ResNet-8×4 1\n1e-3\n\n1 1e-3\n\n1 4e-4\n\nB.4 CATEGORICAL DISTRIBUTION EVALUATION\n\nFollowing the experimental settings in CRD (Tian et al., 2020), we train the student model with 25 categorical distribution candidates for 240 epochs, and employ SGD as the optimizer with batch size of 64, learning rate of 0.05 which is decayed by a factor of 0.1 when arriving 150-th, 180-th, 210-th epoch and weight decay of 5e-4.\n\nB.5 LEARNED CATEGORICAL DISTRIBUTION\n\nFor various student models, Table 10 shows the learned categorical distributions. Similarly, for each student model, the weakest teacher model, i.e., student itself, is considered as the ineffective teacher model when m = 1.\n\nTable 10: Learned categorical distributions for various student models on CIFAR-100.\n\n0\n\nWRN-16-2\n\nWRN-28-2 0.2513\n\nWRN-22-2 0.2518\n\nStudent Teacher WRN-16-2 Weight Student Teacher WRN-40-1 Weight Student Teacher ResNet-8×4 ResNet-14×4 ResNet-20×4 ResNet-26×4 ResNet-32×4 Weight\n\nWRN-28-2 0.2011\n\nWRN-34-2 0.2458\n\nWRN-16-2 0.1864\n\nWRN-22-2 0.2068\n\nWRN-40-2 0.2511\n\nResNet-8×4\n\nWRN-40-1\n\n0.2663\n\n0.2473\n\n0.2565\n\n0.2034\n\n0.2299\n\n0\n\n0\n\nWRN-34-2 WRN-40-2\n\n0.2024\n\nB.6 RESULTS AND ANALYSIS\n\nFollowing Tian et al. (2020), we show the test accuracy of the last epoch in Table 11 for a fair comparison. The proposed distillation paradigm achieves the best performance for 2 out of 3 student models. Particularly, compared to previous state-of-the-art CRD, the improvements are 0.56% and 0.42% for WRN-16-2 and WRN-40-1 distillation, respectively.\n\nC DETAILS OF STUDENT AND TEACHER TEAM FOR AUTOSKDBERT\n\nC.1 ARCHITECTURE INFORMATION\n\nThe architecture information of student and teachers is shown in Table 12.\n\nC.2 HYPER-PARAMETERS FOR FINE-TUNING AND DISTILLATION\n\nWe utilize the hyper-parameters shown in Table 13 for fine-tuning and distillation.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 11: Test accuracy (%) of the proposed AutoSKD and other popular distillation approaches on CIFAR-100. All experimental results are cited from Tian et al. (2020). Average of the last epoch over 5 runs.\n\nStudent Teacher Student Accuracy Teacher Accuracy KD (Hinton et al., 2015) FitNet (Romero et al., 2015) AT (Zagoruyko & Komodakis, 2017) SP (Tung & Mori, 2019) CC (Peng et al., 2019) VID (Ahn et al., 2019) RKD (Park et al., 2019) PKT (Passalis & Tefas, 2018) AB (Heo et al., 2019) FT (Kim et al., 2018) FSP (Yim et al., 2017) NST (Huang & Wang, 2017) CRD (Tian et al., 2020) AutoSKD (Ours)\n\nWRN-16-2 WRN-40-1 ResNet-8×4 WRN-40-2 WRN-40-2 ResNet-32×4\n\n73.26 75.61 74.92 73.58 74.08 73.83 73.56 74.11 73.35 74.54 72.50 73.25 72.91 73.68 75.48 76.04\n\n71.98 75.61 73.54 72.24 72.77 72.43 72.21 73.30 72.22 73.45 72.38 71.59 -\n72.24 74.14 74.72\n\n72.50 79.42 73.33 73.50 73.44 72.94 72.97 73.09 71.90 73.64 73.17 72.86 72.62 73.30 75.51 75.39\n\nTable 12: The architecture of each student and teacher.\n\nModel Name Student AutoSKDBERT T01 T02 T03 T04 T05 T06 T07 T08 T09 T10 T11 T12 T13 T14†\n\nTeacher\n\nLayer Hidden Size Head 768 128 128 128 256 256 256 512 512 512 768 768 768 1024 1024\n\n12 2\n2 2\n4 4\n4 8\n8 8\n12 12 12 16 16\n\n6 8\n10 12 8\n10 12 8\n10 12 8\n10 12 24 24\n\n#Params (M) 66.0 5.6 6.0 6.4 14.3 15.9 17.5 41.4 47.7 54.0 81.1 95.3 110 335 335\n\n† Pre-training with whole word masking.\n\nTable 13: Hyper-parameters for fine-tuning of student and teacher team.\n\nHyper-parameter Value Adam (cid:15) 1e-6 Adam β1 0.9 Adam β2 0.999 Learning rate decay linear Warmup fraction Attention dropout Dropout Weight decay Batch size\n\nLearning rate\n\n0.1 0.1 0.1 1e-4 32 for fine-tuning, {16, 32} for distillation For T13 and T14, {6e-6, 7e-6, 8e-6, 9e-6} on MRPC and RTE tasks, {2e-5, 3e-5, 4e-5, 5e-5} on other tasks. For student and other teachers, {2e-5, 3e-5, 4e-5, 5e-5} and {1e-5, 2e-5, 3e-5} on all tasks, respectively.\n\nFine-tuning epochs 15 on MRPC, RTE and CoLA tasks, 5 on other tasks\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nC.3 FINE-TUNING PERFORMANCE\n\n2 as the student of AutoSKDOn the one hand, we directly treat the pre-trained model of TinyBERT6 BERT. On the other hand, we choose 14 BERT-style language models with various capabilities as the candidates for teacher team. Moreover, each pre-trained teacher can be downloaded from official implementation of BERT3. Furthermore, the results of the student and the teacher on GLUE-dev are shown in Table 14.\n\nTable 14: The fine-tuning performances of student and teachers on GLUE-dev.\n\nModel\n\nStudent T01 T02 T03 T04 T05 T06 T07 T08 T09 T10 T11 T12 T13 T14\n\nMRPC F1+acc 2\n89.44 81.83 84.75 84.59 85.18 87.84 85.96 87.91 88.17 88.85 89.36 90.10 89.98 90.60 90.15\n\nRTE acc 71.84 66.06 66.06 65.70 64.62 66.06 66.06 70.04 65.70 66.43 68.95 71.12 68.59 62.74 79.06\n\nCoLA SST-2\n\nMcc 45.74 25.92 25.57 27.87 40.35 38.76 41.36 48.14 50.98 53.58 56.30 60.32 60.26 62.74 65.88\n\nacc 91.63 86.24 85.67 86.47 89.33 89.33 89.68 91.28 91.28 92.09 93.00 92.78 92.66 94.50 94.72\n\nQQP F1+acc 2\n86.44 83.95 84.18 85.02 86.36 87.25 87.21 88.69 88.62 89.01 89.27 89.71 89.66 90.26 90.40\n\nQNLI MNLI\n\nacc 90.70 83.80 84.00 84.40 86.80 87.26 87.42 89.27 89.25 90.33 90.79 91.20 91.85 92.70 93.89\n\nm 82.55 72.95 73.75 75.16 78.16 78.75 79.54 80.84 81.41 81.90 83.05 84.00 84.40 86.88 87.06\n\nAvg\n\n85.87 80.00 80.49 81.01 82.46 83.42 83.27 85.21 84.74 85.34 86.21 86.93 86.76 86.83 89.50\n\nC.4 PERFORMANCE OF STUDENT WITH VARIOUS DISTILLATION PARADIGMS\n\nTable 15 summarizes the performance of student using different distillation paradigms with the teacher models described in Appendix C.1. Moreover, experimental settings can be found in Table 13. On the one hand, the student performance using single-teacher distillation with respect to each teacher model is given. On the other hand, two popular multi-teacher KD paradigms, i.e., AvgKD (Hinton et al., 2015) and TAKD (Mirzadeh et al., 2020), are employed to distill the student with two groups of teacher team, i.e., T01 to T14 and T10 to T14.\n\nAccording to Table 15, we can draw several conclusions:\n\n1. For single-teacher KD paradigm, the strongest teacher may not be the best teacher for student distillation. Capacity gap (Mirzadeh et al., 2020) between the strong-capacity teacher and weak-capacity student plays an important role for this phenomenon.\n\n2. For multi-teacher AvgKD, increasing the number of teachers can not always contribute to improving the distillation performance. In AvgKD, the diversity losing issue leads to unsatisfactory performance due to using the ensemble of teacher outputs.\n\n3. For multi-teacher TAKD, weak-capacity teachers dramatically reduce the distillation performance of student. In TAKD, the weakest teacher assistant (e.g., T01 for the teacher team T01-T14, T10 for the teacher team T10-T14) transfers mixture of knowledge which learned from previous stronger teacher assistants (e.g., T02 to T14 for the teacher team T01-T14, T11 to T14 for the teacher team T10-T14) into the student. As a result, the performance of TAKD is very sensitive to the capacity of the weakest teacher assistant.\n\nIn order to verify the effectiveness of weak-capacity teacher for performance improvement, we choose several weak-capacity BERT-style models as teachers, e.g., T01 to T09. Besides, we choose also two strong-capacity teachers, i.e., T13 and T14 in Table 12, to verify the effectiveness of the proposed distillation paradigm for capacity gap alleviation.\n\n2https://huggingface.co/huawei-noah/TinyBERT General 6L 768D 3https://github.com/google-research/bert\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTable 15: Distillation performance of student with various distillation paradigms on GLUE-dev.\n\nKD Paradigm\n\nSingle-teacher\n\nAvgKD (Hinton et al., 2015)\n\nTAKD (Mirzadeh et al., 2020)\n\nAutoSKDBERT (Ours)\n\nTeacher\n\nT01 T02 T03 T04 T05 T06 T07 T08 T09 T10 T11 T12 T13 T14 T01-T14 T10-T14 T01-T14 T10-T14 T01-T14\n\nMRPC RTE CoLA SST-2 QQP QNLI MNLI F1+acc acc 2\n88.8 84.6 89.7 87.9 90.5 87.1 92.4 89.7 91.4 89.7 92.4 88.4 92.8 89.5 92.3 89.6 92.2 89.9 92.0 89.6 92.3 89.7 93.1 89.1 92.1 90.0 92.4 89.5 92.2 90.2 92.2 89.9 88.0 83.7 92.7 89.3 92.0 93.0\n\nF1+acc 2\n84.6 85.1 86.1 87.1 87.8 87.6 88.6 88.8 88.9 88.9 89.0 88.9 88.9 89.0 89.1 89.0 83.2 88.7 89.5\n\nMcc 32.5 35.3 36.7 40.2 46.9 45.8 48.3 46.7 46.2 49.3 48.5 46.9 47.7 48.3 47.2 48.4 29.4 47.8 51.8\n\nacc 86.3 86.2 86.5 89.6 89.8 90.2 91.0 90.9 91.5 91.1 91.3 91.4 91.2 91.3 91.1 91.2 84.6 91.4 91.6\n\nm 74.9 75.3 76.2 79.7 79.8 80.4 81.9 82.3 82.7 82.9 83.2 82.8 83.4 83.5 83.5 83.4 73.6 83.4 84.3\n\nacc 67.9 67.2 70.8 69.0 71.1 69.3 73.7 71.5 72.2 73.3 71.8 71.5 72.9 72.6 71.8 72.9 67.9 71.8 76.9\n\nD ORTHOGONAL EXPERIMENT OF AUTOSKDBERT WITH TRANSFORMER\n\nLAYER DISTILLATION AND DATA AUGMENTATION\n\nThis paper proposes a general distillation paradigm for BERT compression. Consequently, most of other distillation approaches can combine with the proposed distillation paradigm. For instance, transformer layer distillation used in TinyBERT (Jiao et al., 2020) and MoEBERT (Zuo et al., 2022) can be replaced with the stochastic KD paradigm proposed in this paper. Moreover, each teacher in the teacher team should has same hidden size with the student when distilling the transformer layer. Consequently, we can not distill the student with the teacher team used in this paper. In this section, we implement a list of orthogonal experiments to examine the effectiveness of the combination of AutoSKDBERT and TinyBERT, and show the experimental result in Table 16.\n\nTable 16: Results of AutoSKDBERT with Data Augmentation (DA) transformer layer Distillation (TD) on GLUE-dev.\n\nModel\n\nDA TD\n\nAutoSKDBERT\n\n(cid:37) (cid:37) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33)\n\nMRPC RTE CoLA SST-2 QNLI F1+acc Mcc 2\n92.0 51.8 56.8 89.7 41.1 87.8 55.3 91.1\n\nacc 93.0 92.9 92.2 92.7\n\nacc 76.9 73.3 70.0 69.3\n\nacc 91.6 92.0 91.1 92.0\n\nAvg\n\n81.1 80.9 76.4 80.1\n\nDue to the difference of hidden size between the strongest teacher T14 and the student, similar to TinyBERT, we employ also BERTBASE, i.e., T12, as the teacher for transformer layer distillation. TinyBERT employs random search to choose the best batch size and learning rate from {16, 32} and {1e-5, 2e-5, 3e-5}, respectively. Differently, AutoSKDBERT uses also the categorical distributions on vanilla datasets with the batch size and the learning rate shown in Table 2 for each downstream task. Moreover, the epoch number for each downstream task can be found in Table 13.\n\nAs shown in Table 16, the combination of AutoSKDBERT and DA shows better performance compared to vanilla AutoSKDBERT on CoLA and QNLI tasks. Furthermore, the combination of AutoSKDBERT, TD and DA achieves better performance compared to vanilla AutoSKDBERT on QNLI tasks. The main reason in our consideration is that the categorical distributions are learned on the vanilla dataset instead of the augmentation data. In the future, we will directly learn the categorical distribution on the augmentation data.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nHowever, the combination of AutoSKDBERT and TD is prone to obtaining worse performance on each downstream task. We consider that the main cause of the above phenomenon is the knowledge transfer gap between transformer layer distillation and prediction layer distillation, i.e., only using T12 for transformer layer distillation, T01 to T14 for prediction layer distillation. In the future, we will select appropriate teacher team to distill the transformer layer of BERT.\n\nE IMPACT OF HYPER-PARAMETERS FOR CATEGORICAL DISTRIBUTION\n\nOPTIMIZATION\n\nAs above mentioned, there are two important hyper-parameters, i.e., ineffective teacher number m and learning rate, for categorical distribution optimization. In this section, we discuss the impact of the above two hyper-parameters for the best performance of the learned categorical distributions. On the tasks of MRPC, RTE and CoLA, we implement AutoSKDBERT with m from 1 to 10 and learning rate from 3e-4 to 1e-3 with an interval of 1e-4, and show the results in Table 17.\n\nTable 17: Results of AutoSKDBERT with various hyper-parameters for categorical distribution optimization.\n\nTask Metric mmm\n\nMRPC F1+acc\n\n2\n\nMean±Std\n\nRTE\n\nacc\n\nMean±Std\n\nCoLA Mcc\n\n1 2\n3 4\n5 6\n7 8\n9 10\n\n1 2\n3 4\n5 6\n7 8\n9 10\n\n1 2\n3 4\n5 6\n7 8\n9 10\n\n3e-4 90.3 91.7 90.5 90.3 90.7 90.6 90.7 90.8 90.4 90.4\n\n4e-4 90.6 91.6 90.9 90.1 91.1 90.6 90.2 90.7 90.1 89.8\n\n5e-4 91.4 91.6 91.1 90.6 90.7 90.4 91.1 90.2 90.9 89.8\n\nLearning Rate 7e-4 6e-4 91.1 90.8 91.6 91.8 90.9 91.1 91.3 91.0 91.8 90.7 90.8 90.2 90.1 90.6 90.5 90.5 90.1 89.9 89.9 90.4\n\n8e-4 91.0 91.6 91.2 90.4 90.3 90.5 90.4 90.6 90.8 91.1\n\n9e-4 92.0 91.5 90.6 91.0 90.8 89.9 89.8 90.5 90.6 90.7\n\n1e-3 91.6 91.8 90.8 90.4 90.4 90.9 90.5 90.3 90.3 90.4\n\n90.6±0.4 90.6±0.5 90.8±0.5 90.7±0.5 90.8±0.6 90.8±0.4 90.7±0.6 90.7±0.5\n\n73.3 72.9 72.9 72.6 74.4 75.5 74.1 75.8 74.0 72.9\n\n72.6 72.9 76.9 72.9 74.7 76.2 75.1 73.7 73.3 74.7\n\n73.7 73.7 73.7 74.0 74.0 74.0 72.2 74.4 73.7 72.6\n\n72.6 72.9 73.7 73.3 73.3 74.7 74.4 74.4 73.7 75.1\n\n72.6 73.7 72.9 76.2 74.7 75.5 74.4 74.0 74.4 73.7\n\n72.6 73.3 74.4 73.7 74.0 75.1 74.0 74.7 74.4 73.7\n\n72.2 74.7 74.4 75.1 74.7 76.2 74.0 74.4 74.4 74.7\n\n73.3 75.5 74.0 73.3 75.5 76.2 74.4 74.4 76.9 74.0\n\n73.8±1.1 74.3±1.4 73.6±0.6 73.8±0.8 74.2±1.0 74.0±0.7 74.5±0.9 74.8±1.2\n\n50.2 50.0 48.3 49.9 49.2 48.7 49.3 49.6 49.8 49.2\n\n50.0 50.9 48.6 49.3 49.0 49.3 48.7 49.3 49.7 49.2\n\n50.0 49.8 49.4 49.8 49.3 50.0 50.0 49.1 50.1 49.6\n\n50.0 50.3 49.8 50.8 50.0 49.3 49.3 49.4 49.0 48.5\n\n49.7 50.2 50.6 51.8 49.7 49.5 49.7 49.9 49.2 50.6\n\n49.8 49.8 50.2 50.5 49.3 49.7 49.7 50.5 48.7 49.7\n\n50.6 49.2 50.1 49.4 49.4 49.8 49.2 50.6 50.2 49.4\n\n49.6 50.2 49.9 51.5 49.3 49.4 49.0 50.2 49.6 48.6\n\nMean±Std\n\n91.1±0.6 91.7±0.1 90.9±0.2 90.6±0.4 90.8±0.4 90.5±0.3 90.4±0.4 90.5±0.2 90.4±0.3 90.3±0.4\n\n72.9±0.5 73.7±0.9 74.1±1.2 73.9±1.1 74.4±0.6 75.4±0.7 74.1±0.8 74.5±0.6 74.4±1.0 73.9±0.8\n\n50.0±0.3 50.0±0.5 49.6±0.7 50.4±0.9 49.4±0.3 49.5±0.4 49.4±0.4 49.8±0.5 49.5±0.5 49.4±0.6\n\nMean±Std\n\n49.4±0.6 49.4±0.6 49.7±0.3 49.6±0.6 50.1±0.7 49.8±0.5 49.8±0.5 49.7±0.8\n\nWe can draw a conclusion from Table 17 that the proposed AutoSKDBERT is sensitive to the ineffective teacher number rather than the learning rate. For each task, the ineffective teacher number m plays a more important role compared to the learning rate for AutoSKDBERT. For instance, the mean value of m = 2 is 91.7 which is 1.4 higher than the mean value of m = 10. However, the largest difference of mean value with respect to learning rate is only 0.2. There is a similar phenomenon on the tasks of RTE and CoLA. Therefore, the above conclusion can be drawn.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nF COST COMPARISON OF TINYBERT AND AUTOSKDBERT\n\nIn this section, we show the cost of AutoSKDBERT in terms of categorical distribution optimization and evaluation, and compare our approach to TinyBERT with respect to algorithm cost. Experimental results are shown in Table 18 where on five downstream tasks, the cost of AutoSKDBERT is 38.72 hours which is 8.4× less than TinyBERT. Moreover, we obtain the cost on NVIDIA A100 GPU with AMD EPYC 7642 48-Core Processor.\n\nTable 18: The cost (hours) comparison of TinyBERT and AutoSKDBERT on five downstream tasks. These results about TinyBERT are obtained by following the experimental settings described in Jiao et al. (2020) with the code publicly released by the authors at https://github.com/huawei-noah/PretrainedLanguage-Model/tree/master/TinyBERT.\n\nTinyBERT\n\nAutoSKDBERT\n\nMRPC RTE CoLA SST-2 QNLI MRPC RTE CoLA SST-2 QNLI\n\n3.42 2.80 12.72 12.90 61.94\n\n0 0\nTransformer Layer Distillation 1.28 2.45 Categorical Distribution Optimization Prediction Layer Distillation† 6.50 18.25 7.78 20.70 Total Cost † For TinyBERT, the cost is obtained by 6 groups of experiment with various hyper-parameters (i.e., batch sizes of {16, 32} and learning rates of {1e-5, 2e-5, 3e-5}) on augmentation data. For AutoSKDBERT, the cost is obtained by 25 groups of experiment on vanilla data with different categorical distributions learned in the process of categorical distribution optimization.\n\n0 0.26 0.17 1.06 5.04 3.24 4.35 31.71 187.62 1.75 1.25 5.75 8.46 6.04 17.07 44.61 249.56 2.01 1.42 6.81\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\nThe distillation process of TinyBERT can be divided into two phases: 1) transformer layer distillation on augmentation data and 2) prediction layer distillation on augmentation data. The transformer layer distillation of TinyBERT is time-consuming, e.g., it spends about 62 hours on QNLI. Besides, the prediction layer distillation of TinyBERT is also time-consuming due to using large-scale augmentation data.\n\nDifferently, AutoSKDBERT consists of categorical distribution optimization and evaluation (i.e., prediction layer distillation). On the one hand, categorical distribution optimization is efficient, e.g., 2.45 hours on the task of QNLI, due to the gradient-based SSWO. On the other hand, categorical distribution evaluation is also efficient even choosing the best categorical distribution from 25 candidates.\n\nG TINYBERT WITH STRONGER TEACHER MODEL\n\nAutoSKDBERT employs two stronger teacher models, i.e., T13 and T14, compared to most of the comparative methods shown in Table 4. To verify the impact of strong teacher on the distillation performance of other paradigms, we employ T12 and T14 as the teachers to distill TinyBERT on five downstream tasks for a fair comparison. Following TinyBERT (Jiao et al., 2020), we implement the experiments with batch sizes of {16, 32} and learning rates of {1e-5, 2e-5, 3e-5}, and choose the best result to show in Table 19.\n\nTable 19: Results of TinyBERT with the strongest teacher T14 on GLUE-dev. These results are obtained by TinyBERT with the fine-tuned teacher model of AutoSKDBERT using the code publicly released by the authors at https://github.com/huawei-noah/ Pretrained-Language-Model/tree/master/TinyBERT.\n\nStudent\n\nTeacher\n\nTinyBERT (w/o aug)\n\nT12 T14\n\nMRPC RTE CoLA SST-2 QNLI F1+acc Mcc 2\n42.2 87.0 40.9 86.7\n\nacc 91.2 90.9\n\nacc 67.9 70.0\n\nacc 92.0 92.0\n\nAvg\n\n76.1 76.1\n\nWe can observe that the strong teacher T14 contributes to only improving the performance on RTE. For the above phenomenon, the main reason is that a capacity gap (Mirzadeh et al., 2020) exists between T14 and student which is prone to obtaining unsatisfactory performance. As a result, a conclusion can be drawn that the stronger teacher T14 can not always contribute to improving the performance of other distillation paradigms.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nH MT-BERT FOR BERT COMPRESSION\n\nFor BERT-style language model compression, we verify the performance of MT-BERT (Wu et al., 2021) whose objection function can be expressed as:\n\nLMTBERT =\n\nN (cid:88)\n\ni=1\n\nCE(yi/T, ys/T ) 1 + CE(y, yi)\n\n,\n\n(9)\n\nwhere, N indicates the number of teachers, CE(·, ·) is the cross-entropy loss, T denotes the temperature, y represents the ground-truth label, yi and ys refer to the outputs of i-th teacher and the student, respectively.\n\nWe employ T10 to T14 as the teacher team to distill the student via Eq. 9. Particularly, we only use the weighted multi-teacher distillation loss without the multi-teacher hidden loss and the taskspecific loss as in MT-BERT (Wu et al., 2021).\n\nThe hyper-parameters are given as follows:\n\n• Learning Rate: {1e-5, 2e-5, 3e-5} for all tasks.\n\n• Batch Size: {16, 32, 64}.\n\n• Epoch: 10 for MRPC, RTE and CoLA tasks, 3 for other tasks.\n\nOther settings follow AutoSKDBERT.\n\nI DETAILS OF GLUE BENCHMARK\n\nGLUE consists of 9 NLP tasks: Microsoft Research Paraphrase Corpus (MRPC) (Dolan & Brockett, 2005), Recognizing Textual Entailment (RTE) (Bentivogli et al., 2009), Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019), Semantic Textual Similarity Benchmark (STS-B) (Cer et al., 2017), Stanford Sentiment Treebank (SST-2) (Socher et al., 2013), Quora Question Pairs (QQP) (Chen et al., 2018), Question NLI (QNLI) (Rajpurkar et al., 2016), Multi-Genre NLI (MNLI) (Williams et al., 2017), and Winograd NLI (WNLI) (Levesque et al., 2012).\n\nMRPC belongs to a sentence similarity task where system aims to identify the paraphrase/semantic equivalence relationship between two sentences.\n\nRTE belongs to a natural language inference task where system aims to recognize the entailment relationship of given two text fragments.\n\nCoLA belongs to a single-sentence task where system aims to predict the grammatical correctness of an English sentence.\n\nSTS-B belongs to a sentence similarity task where system aims to evaluate the similarity of two pieces of texts by a score from 1 to 5.\n\nSST-2 belongs to a single-sentence task where system aims to predict the sentiment of movie reviews.\n\nQQP belongs to a sentence similarity task where system aims to identify the semantical equivalence of two questions from the website Quora.\n\nQNLI belongs to a natural language inference task where system aims to recognize that for a given pair <question, context>, the answer to the question whether contains in the context.\n\nMNLI belongs to a natural language inference task where system aims to predict the possible relationships (i.e., entailment, contradiction and neutral) of hypothesis w.r.t. premise for a given pair <premise, hypothesis>.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nWNLI of a sentence’s pronoun from a list of choices.\n\nbelongs to a natural language inference task where system aims to determine the referent\n\n22",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a knowledge distillation approach with multiple teachers by introduce a weighting on teachers, and the weights are optimized in a bi-level manner. It achieves state-of-the-art results on several GLUE tasks compared with other model compression methods.\n\n# Strength And Weaknesses\n\nStrength:\n* The authors propose a reasonable bi-level optimization for the weighting of teachers and achieve strong results.\n\nWeakness:\n* The paper writing is comparatively weak, e.g., The figure 1 is even harder to understand than the main text part.\n* Lack of meaningful analysis and discussion towards the result numbers. For example, overall the proposed approach performs on par with MoBERT. Why in some tasks it performs worse? Is the proposed approach orthogonal to the previous methods? Will the performance be better if multiple techniques are applied together?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSome places of the paper writing is unclear. The proposed method is novel, and the experimental setting looks clear.\n\n# Summary Of The Review\n\nThis paper proposes a reasonable approach to weight multiple teacher in knowledge distillation, and gets good results on the GLUE benchmark, but the results are not discussed and analyzed thoroughly, and the paper writing is comparatively weak.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nZERO-SHOT RETRIEVAL WITH SEARCH AGENTS AND HYBRID ENVIRONMENTS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nLearning to search is the task of building artificial agents that learn to autonomously use a search box to find information. So far, it has been shown that current language models can learn symbolic query reformulation policies, in combination with traditional term-based retrieval, but fall short of outperforming neural retrievers. We extend the previous learning to search setup to a hybrid environment, which accepts discrete query refinement operations, after a first-pass retrieval step via a dual encoder. Experiments on the BEIR task show that search agents, trained via behavioral cloning, outperform the underlying search system based on a combined dual encoder retriever and cross encoder reranker. Furthermore, we find that simple heuristic Hybrid Retrieval Environments (HRE) can improve baseline performance by several nDCG points. The search agent based on HRE (HARE) matches state-of-the-art performance, balanced in both zeroshot and in-domain evaluations, via interpretable actions, and at twice the speed.\n\n1\n\nINTRODUCTION\n\nTransformer-based dual encoders for retrieval, and cross encoders for ranking (cf. e.g., Karpukhin et al. (2020); Nogueira & Cho (2019)), have redefined the architecture of choice for information search systems. However, sparse term-based inverted index architectures still hold their ground, especially in out-of-domain, or zero-shot, evaluations. On the one hand, neural encoders are prone to overfitting on training artifacts (Lewis et al., 2021). On the other, sparse methods such as BM25 (Robertson & Zaragoza, 2009) may implicitly benefit from term-overlap bias in common datasets (Ren et al., 2022). Recent work has explored various forms of dense-sparse hybrid combinations, to strike better variance-bias tradeoffs (Khattab & Zaharia, 2020; Formal et al., 2021b; Chen et al., 2021; 2022).\n\nRosa et al. (2022) evaluate a simple hybrid design which takes out the dual encoder altogether and simply applies a cross encoder reranker to the top documents retrieved by BM25. This solution couples the better generalization properties of BM25 and high-capacity cross encoders, setting the current SOTA on BEIR by reranking 1000 documents. However, this is not very practical as reranking is computationally expensive. More fundamentally, it is not easy to get insights on why results are reranked the way they are. Thus, the implicit opacity of neural systems is not addressed.\n\nWe propose a novel hybrid design based on the Learning to Search (L2S) framework (Adolphs et al., 2022). In L2S the goal is to learn a search agent that autonomously interacts with the retrieval environment to improve results. By iteratively leveraging pseudo relevance feedback (Rocchio, 1971), and language models’ understanding, search agents engage in a goal-oriented traversal of the answer space, which aspires to model the ability to ’rabbit hole’ of human searchers (Russell, 2019). The framework is also appealing because of the interpretability of the agent’s actions.\n\nAdolphs et al. (2022) show that search agents based on large language models can learn effective symbolic search policies, in a sparse retrieval environment, but fail to outperform neural retrievers. We extend L2S to a dense-sparse hybrid agent-environment framework structured as follows. The environment relies on both a state-of-the-art dual encoder, GTR (Ni et al., 2021), and BM25 which separately access the document collection. Results are combined and sorted by means of a transformer cross encoder reranker (Jagerman et al., 2022). We call this a Hybrid Retrieval Environment (HRE). Our search agent (HARE) interacts with HRE by iteratively refining the query via search operators, and aggregating the best results. HARE matches state-of-the-art results on the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nBEIR dataset (Thakur et al., 2021) by reranking a one order of magnitude less documents than the SOTA system (Rosa et al., 2022), reducing latency by 50%. Furthermore, HARE does not sacrifice in-domain performance. The agent’s actions are interpretable and dig deep in HRE’s rankings.\n\nFigure 1: Sequential query refinements combining pseudo relevance feedback and search operators.\n\nFigure 1 shows an example of a search session performed by the HARE search agent applying structured query refinement operations. The agent adds two successive filtering actions to the query ’what is the weather like in germany in june’ (data from MS MARCO (Nguyen et al., 2016)). In the first step it restricts results to documents containing the term ’temperatures’, which occurs in the first set of results. In the second step, results are further limited to documents containing the term ’average’. This fully solves the original query by producing an nDCG@10 score of 1.\n\n2 RELATED WORK\n\nClassic retrieval systems such as BM25 (Robertson & Zaragoza, 2009) use term frequency statistics to determine the relevancy of a document for a given query. Recently, neural retrieval models have become more popular and started to outperform classic systems on multiple search tasks. Karpukhin et al. (2020) use a dual-encoder setup based on BERT (Devlin et al., 2019), called DPR, to encode query and documents separately and use maximum inner product search (Shrivastava & Li, 2014) to find a match. They use this model to improve recall and answer quality for multiple open-domain question-answer datasets. Large encoder-decoder models such as T5 (Raffel et al., 2020) are now preferred as the basis for dual encoding as they outperform encoders-only retrievers (Ni et al., 2021).\n\nIt has been observed that dense retrievers can fail to catch trivial query-document syntactic matches involving n-grams or entities (Karpukhin et al., 2020; Xiong et al., 2021; Sciavolino et al., 2021). ColBERT (Khattab & Zaharia, 2020) gives more importance to individual terms by means of a late interaction multi-vector representation framework, in which individual term embeddings are accounted in the computation of the query-document relevance score. This is expensive as many more vectors need to be stored for each indexed object. ColBERTv2 (Santhanam et al., 2022) combines late interaction with more lightweight token representations. SPLADE (Formal et al., 2021b) is another approach that relies on sparse representations, this time induced from a transformer’s masked heads. SPLADEv2 (Formal et al., 2021a; 2022) further improves performance introducing hardnegative mining and distillation. Chen et al. (2021) propose to close the gap with sparse methods on phrase matching and better generalization by combining a dense retriever with a dense lexical model trained to mimic the output of a sparse retriever (BM25). Ma et al. (2021) combine single hybrid vectors and data augmentation via question generation. In Section 3 (Table 2b) we evaluate our search environment and some of the methods above.\n\nThe application of large LMs to retrieval, and ranking, presents significant computational costs for which model distillation (Hinton et al., 2015) is one solution, e.g. DistillBERT (Sanh et al., 2019). The generalization capacity of dual encoders have been scrutinized recently in QA and IR tasks (Lewis et al., 2021; Zhan et al., 2022; Ren et al., 2022). Zhan et al. (2022) claims that dense\n\n2\n\nwhat is the weather like in germany in june\"June, July, and August. Munich Germany weather (especially in the city center) can be very hot. We've had summers with temperatures constantly reaching or even surpassing 30 degrees Celsius (90°F). Enjoy the time as much as you can, because it probably won't last long.\"what is the weather like in germany in june +temperatures\"Average Temperatures for Germany in June. Average temperatures for June at cities throughout Germany are listed below in degrees Celsius and Fahrenheit. The tables give the normals for maximum and minimum monthly temperatures based on three decades of data.\"what is the weather like in germany in june +temperatures +average\"Weather in Hannover in June 2018. Expect 21°C daytime maximum temperatures in the shade with on average 7 hours of sunshine per day in Hannover in June. Check more long-term weather averages for Hannover in June before you book your next holiday to Germany in 2018.\"nDCG=0nDCG=0nDCG=1Under review as a conference paper at ICLR 2023\n\nBEIR subset nDCG@10 SPAR.\n\nColB.\n\nSPL.\n\nColBERTv2 SPAR SPLADE++\n\nHRE, 770M HRE, 11B\n\n0.481 0.475 0.475\n\n0.543 0.529\n\n- 0.482 0.508\n\n0.526 0.530\n\n- -\n0.482\n\n0.507 0.507\n\n(b) Our HRE, compared, at k=10, vs. other dense/sparse methods. The BEIR average score is computed on the subsets of tasks selected by each method. HRE 770M is trained on BM25 top 100 documents, HRE 11B on BM25 top 1000.\n\n(a) Average nDCG@10 of the BM25, GTR and HRE at different retrieval depths.\n\nFigure 2: Preliminary evaluation of our HRE and benchmark environments on BEIR tasks.\n\nrerankers generalize better than dense retrievers. Ni et al. (2021) suggests that increasing the dual encoder model size increases its ability to generalize. Rosa et al. (2022) argue that large rerankers provide the most effective approach, particularly in zero-shot performance and in combination with a sparse retriever. Their best MonoT5 (Nogueira et al., 2020b) model, a pretrained transformer encoder-decoder finetuned for query-document scoring, yields the state-of-the-art results on 12 out of 18, zero-shot tasks on the BEIR task (Rosa et al., 2022). They observe that in-domain performance is not a good indicator of zero-shot performance. Consequently, they regularize the reranker, trading off in-domain performance and improving zero-shot results.\n\nAnother interesting line of research is inspired by large decoder-only language models, where increasing size systematically improves zero-shot performance, as proven by GPT (Brown et al., 2020) and PaLM (Chowdhery et al., 2022). Accordingly, SGPT (Muennighoff, 2022) extends the encoder/decoder-only approach to search to decoder-only modeling, via prompting and finetuning. Also related is the line of work on retrieval-augmented language models (Lewis et al., 2020; Guu et al., 2020) and iterative query reformulation for question answering (Guo et al., 2017; Buck et al., 2018; Qi et al., 2019; 2021; Zhu et al., 2021; Nakano et al., 2021).\n\n3 HYBRID RETRIEVAL ENVIRONMENT (HRE) AND BENCHMARKS\n\nA search environment is composed of one or more retrievers operating over a document collection, whose output is possibly combined, and eventually rescored by a dedicated model, the reranker.\n\n3.1 RETRIEVERS\n\nWe experiment with three types of retrieval methods. The first, BM25, uses Lucene’s implementation of BM251 as the retriever. This is the setting of (Adolphs et al., 2022). The second environment, GTR, uses GTR-XXL (Ni et al., 2021) as the retriever. The last is a hybrid environment that combines the results of the BM25 and GTR retrievers. We call this a Hybrid Retrieval Environment (HRE). After retrieval, HRE simply joins the two k-sized results sets, removing duplicates. Thus, for a fixed value of k, HRE has available a slightly larger pool of documents, at most 2k.\n\n3.2 THE T5 RERANKER (T5-R)\n\nAfter retrieval, and, in the case of HRE, the combination step, the top documents are reranked by the environment’s reranker, which we refer to as T5-R. In contrast to encoder-decoders (Nogueira et al., 2020a) we follow the work of Zhuang et al. (2022) and only train T5-R’s encoder and add a classification layer on top of the encoder output for the first token, similar to how BERT (Devlin et al., 2019) is often used for classification.\n\n1https://lucene.apache.org/.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nInstead of using a point-wise classification loss, we use a list-based loss (Jagerman et al., 2022; Zhuang et al., 2022): for each query, we obtain one positive (y = 1) and m − 1 negative (y = 0) documents to which the model assigns scores ˆy = ˆym 1 . We use a list-wise softmax cross-entropy loss (Bruch et al., 2019):\n\n(cid:96)(y, ˆy) =\n\nm (cid:88)\n\ni=1\n\nyi log\n\neˆyi j=1 eˆyj\n\n(cid:80)m\n\n.\n\n(1)\n\nWe train T5-R on MS MARCO and the output of BM25. We find that a T5-Large trained on the top-100 documents works well on the top results, but a T5-11B model trained on the top-1000 BM25 documents works better in combination with a search agent (Table 2). For HRE we consider the latter reranker. Figure 2 provides a first evaluation of the ranking performance of our environments on the BEIR dataset – see §5 for more details on the task. Figure 2a reports the effect of reranking an increasing number of documents. BM25 provides a baseline performance, and benefits the most from reranking more results with T5-R. GTR starts from a higher point, at k=10, and plateaus around k=300 with an average nDCG@10 (normalized Discounted Cumulative Gain) of 0.507 on the 19 datasets. Despite its simplicity, HRE is effective at combining the best of BM25 and GTR at small values of k. HRE reaches its maximum, 0.509, at k=40 but scores 0.508 at k=20.\n\nIn Figure 2b we situate our environments in a broader context, by comparing zero shot performance against recent dense/sparse combined retrieval proposals: ColBERTv2 (Santhanam et al., 2022), SPAR (Chen et al., 2021) and SPLADE++ (Formal et al., 2022), discussed also in §2. Each of them evaluates on a different subset of the BEIR zero shot tasks, which we select appropriately. HRE produces the best performance by a substantial margin in two configurations. ColBERT and SPLADE do not use a reranker but require more involved training through cross-attention distillation, and rely on token-level retrieval. The best SPAR model needs an additional dense lexical model and relies on a more sophisticated base retriever, Contriever (Izacard et al., 2021). As the results show, a reranker combined with HRE at (k=10) provides a simple and effective hybrid search system.\n\n4 HYBRID AGENT RETRIEVAL ENVIRONMENT (HARE)\n\nA search agent generates a sequence of queries, q0, q1, q2, . . . , qT , to be passed on, one at a time, to the environment. Here, q0=q is the initial query and qT is the last one, in what we also call a search session. At each step, the environment returns a list of documents Dt for qt. We also maintain a list, A, of the best k documents found during the whole session\n\nAt := {di ∈ Dt ∪ At−1 : |{dj ∈ Dt ∪ At−1 : f (q0, di) < f (q0, dj)}| < k}\n\n(2)\n\nwhere f :(q, d) (cid:55)→ R is the score, P (y=1|q, d), predicted by the T5-R model. When no documents can be retrieved after issuing qt+1 or after a maximum number of steps, the search session stops and At is returned as the agent’s output. The agent’s goal is to generate queries such that the output, AT , has a high score under a given ranking quality metric, in our case nDCG@10.\n\n4.1 QUERY REFINEMENT OPERATIONS\n\nAs in (Adolphs et al., 2022), qt+1 is obtained from qt by augmentation. That is, either by adding a single term, which will be interpreted by Lucene as a disjunctive keyword and contribute a component to the BM25 score, or including an additional unary search operator. We experiment with the same three unary operators: ‘+’, which limits results to documents that contain a specific term, ‘-’ which excludes results that contain the term, and ‘∧i’ which boosts a term weight in the BM25 score by a factor i ∈ R. We don’t limit the operators effect to a specific document field, e.g., the content or title, because in the BEIR evaluation there is no such information in the training data (MS MARCO). Formally, a refinement takes the following simplified form:\n\nqt+1 := qt ∆qt, ∆qt := [+| − |∧i] wt, wt ∈ Σt\n\n(3)\n\nwhere Σt is the vocabulary of terms present in At. This latest condition introduces a relevance feedback dimension (Rocchio, 1971). If available, document relevance labels can be used to build an optimal query, e.g., for training. Or, in the absence of human labels, the search results are used for inference purposes – as pseudo relevance feedback.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Schematic view of the HARE search agent. The information flows from the input query q0 to the output AT . In between, retrieval steps (the blue components) and aggregation and refinement steps (the yellow components) alternate in a cycle.\n\n4.2 THE T5 QUERY EXPANDER (T5-Q)\n\nA search agent includes an encoder-decoder transformer based on T5 (Raffel et al., 2020) that generates query refinements. We call this component T5-Q. At each step, an observation ot:=(qt, At) is formed by concatenating qt and At, which is a string with a minimal set of structural identifiers. T5-Q takes ot as input and outputs ∆qt, allowing the composition of qt+1=qt∆qt, as in Eq. (3).\n\n4.3 HARE AND BENCHMARK SEARCH AGENTS\n\nFigure 3 illustrates the HARE search agent. On the first search step only, GTR retrieves the top-1000 documents for q0. These define a sub-collection, Top-K, kept frozen through the search session. The top-k documents from Top-K, D0,2, are combined with the top-k from BM25, D0,1, also retrieved from the full collection. GTR is not used again. Top-K is further accessed only through BM25, i.e., for t > 0. At every step, t, the results from the full corpus, Dt,1, and those from Top-K, Dt,2, are joined to form Dt. Dt, in turn, is joined with the current session results At−1, to form At. At is passed to the query expander model, T5-Q, which compiles the observation ot=(At, qt), and generates ∆qt. The new query, qt+1=qt∆qt, is sent to BM25 for another round of search. When the termination condition is met, the agent returns AT .\n\nBesides HARE we evaluate two simpler search agents, in alignment with the simpler environments, BM25 and GTR. The first agent (BM25) only uses the BM25 components of HARE (the BM25 environment), thus, it has only access to the results Dt,1 in Figure 3. Analogously, the second agent (GTR), only uses the GTR components of HARE (the GTR environment), and has access exclusively to the Dt,2 results.\n\n5 EXPERIMENTS\n\nWe run experiments on the zero-shot retrieval evaluation framework of BEIR (Thakur et al., 2021), which includes 19 datasets on 9 domains. Only MS MARCO is used for training and development. Each dataset has its own document collection which is indexed separately. We use the official TREC eval script2 for our results. Results for the benchmarks are from the corresponding publications.\n\n5.1 DATA\n\nTo generate training data for T5-R we retrieve k∈{100, 1000} documents per query for each query in the MS MARCO training set (532,761 questions) using BM25. To make one example list of length\n\n2https://github.com/usnistgov/trec_eval/archive/refs/heads/master.zip.\n\n5\n\nCorpusTop-Kqt+1Dt=Dt,1∪Dt,2At-1BM25GTRAt T5-RBM25T5-Qqtqt:t>0qtDt,1Dt,2q0ATUnder review as a conference paper at ICLR 2023\n\nm, we take a query and one gold document and sample m−1 negatives from the top-k documents. We skip queries if no gold document occurs within the top-k documents which removes about 20% of the data.\n\nThe training data for T5-Q is generated as follows. Synthetic search sessions are simulated from labeled query documents pairs, (q, d), where d is the relevant document for q, in the MS MARCO training set. We then use the Rocchio Session Algorithm of Adolphs et al. (2022), to search for the optimal expansion. In a nutshell, at step t, terms in At ∩ d are evaluated as candidates for disjunctive term augmentations, or in combination with ’+’ and ‘∧i’ operators. Conversely, terms in At − d are candidates in combination with ’-’. We attempt at most M candidates for each operator using terms in the document set, At, ranked by Lucene’s IDF score. Starting from q0=q, a synthetic session is expanded, one step at a time, with the best scoring augmentation. The procedure stops when the nDCG@10 score of At does not improve, or after five steps. We generate two sets of training data: a high-throughput (HT) data, for faster turnaround in development, which sets M =20 and yields 120,563 training examples (single steps) from 23% of the questions where improvements were found; and a high-quality (HQ) data using M =100 which yields 203,037 training examples from 40% of the questions for which improvements were found. Table 4, in the Appendix, provides an example gold Rocchio session for the query ’what’s the difference between c++ and java’. The nDCG score of the HRE results is 0.0, and by applying two refinements (’+language’ and ’+platform’) the score increases, first to 0.6, then to 1.0. In the training Rocchio sessions, the ‘+’ operator is used for 83% of all refinements. The other operators are each used for only 2-3% of all refinements. Although ’+’ is used for the majority of refinements, when we evaluated the agent’s headroom allowing only the ‘+’ operator, the headroom was lower. We also evaluated the agent’s headroom allowing only ‘∧i’ operators, the result was also worse. The synthetic data is used to finetune T5-Q via Behavioral Cloning, where each step defines an independent generation task.\n\n5.2 MODELS\n\nWe use the published model checkpoint for GTR-XXL (Ni et al., 2021), as the off-the-shelf dual encoder.3 For BM25 we use Lucene’s implementation with default parameters (k=0.9 and b=0.4).\n\nAs detailed in Section 3, the query-document reranker, T5-R, is initialized from the encoder of a pretrained T5 model. The encoder output of the first token is fed into a feed-forward layer to generate a score which we use for ranking query-document pairs. The input is structured as follows: query: {query} document: {document} We experimented with several published checkpoints, including T5-large and T5-11B and found the latter to perform better.4 Note that while T5-11B has 11B parameters we only train roughly half of them (the encoder side). We train with a batch size of 64 and lists of length m = 32, yielding an effective batch size of 2048. To limit memory consumption we truncate our inputs to 256 tokens.\n\nThe query expander, T5-Q, is based on the T5.1.1-XXL model. The input is structured as follows: query: {query} document: {document1} . . . document: {document10} When examining the training data (§5.1), we found multiple examples of that we consider unlearnable, e.g., involving stop words. As we do not want the search agent to concentrate and overfit on these examples, we employ a simple self-supervised training trick. In each batch, we sort the sequences by their negative log-likelihood (NLL) and we mask out the loss for 50% of the training examples with the highest NLL. Examples can be seen in Table 3 in the Appendix. We are essentially training with only a halved batch size while wasting the other half, but given that the model converges quickly, this technique is sufficient. To further avoid overfitting, we use a small constant learning rate of 3 ∗ 10−5. We train for 12,000 steps with a batch size of 128 (forward pass before masking), which is equivalent to around 1.5 epochs. The input sequences have a maximum length of 1024 and the maximum target length is set to 32. All other hyperparameters are the T5 defaults.\n\n5.3 RESULTS\n\nTable 1 holds the detailed BEIR results. We report the average over all datasets (Average), and minus MS MARCO (Avg. Zero-shot). As benchmarks, we compare with MonoT5, the current SOTA,\n\n3https://github.com/google-research/t5x_retrieval. 4https://github.com/google-research/text-to-text-transfer-transformer.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nBenchmarks\n\nEnvironments\n\nAgents\n\nDataset\n\nMonoT5\n\nSGPT\n\nB\n\nM25\n\nG T\nR\n\nH R\nE\n\nB\n\nM25\n\nG T\nR\n\nR M3\n\nH A\nR E\n\nRS\n\nB\n\nM25\n\nMS MARCO\n\n0.398\n\n0.399\n\n0.285\n\n0.470\n\n0.479\n\n0.361\n\n0.480\n\n0.483\n\n0.483\n\n0.557\n\nTrec-Covid BioASQ NFCorpus NQ HotpotQA FiQA-2018 Signal-1M Trec-News Robust04 ArguAna Touche-2020 Quora DBPedia SCIDOCS Fever Climate-Fever SciFact CQADupStack\n\n#docs reranked\n\nAverage Avg. Zero-shot\n\n0.794 0.574 0.383 0.633 0.758 0.513 0.314 0.472 0.540 0.287 0.299 0.840 0.477 0.197 0.849 0.280 0.777 0.415\n\n1000\n\n0.516 0.522\n\n0.873 0.413 0.362 0.524 0.593 0.372 0.276 0.481 0.514 0.514 0.254 0.846 0.399 0.197 0.783 0.305 0.747 0.381\n\n0.579 0.315 0.343 0.419 0.605 0.268 0.371 0.300 0.384 0.318 0.536 0.650 0.290 0.166 0.783 0.222 0.683 0.316\n\n0.537 0.344 0.358 0.637 0.651 0.504 0.273 0.368 0.513 0.352 0.249 0.875 0.428 0.173 0.811 0.282 0.707 0.431\n\n0.666 0.427 0.377 0.655 0.713 0.513 0.320 0.394 0.556 0.327 0.325 0.876 0.453 0.196 0.829 0.296 0.751 0.448\n\n0.778 0.453 0.380 0.528 0.694 0.355 0.355 0.353 0.513 0.246 0.518 0.769 0.383 0.181 0.813 0.258 0.707 0.364\n\n0.703 0.470 0.368 0.664 0.734 0.516 0.313 0.368 0.514 0.362 0.251 0.874 0.432 0.197 0.817 0.287 0.743 0.448\n\n0.744 0.468 0.380 0.661 0.734 0.520 0.310 0.420 0.565 0.237 0.321 0.873 0.463 0.198 0.831 0.300 0.751 0.451\n\n0.765 0.493 0.383 0.669 0.759 0.525 0.318 0.406 0.589 0.260 0.320 0.873 0.476 0.201 0.832 0.300 0.756 0.452\n\n0.921 0.654 0.508 0.724 0.850 0.564 0.476 0.521 0.728 0.389 0.673 0.880 0.549 0.280 0.866 0.408 0.797 0.526\n\n-\n\n10\n\n10\n\n17.5\n\n21.6\n\n23.6\n\n33.4\n\n66.7\n\n15.5\n\n0.485 0.490\n\n0.412 0.419\n\n0.472 0.472\n\n0.505 0.507\n\n0.474 0.480\n\n0.502 0.503\n\n0.511 0.513\n\n0.519 0.521\n\n0.625 0.628\n\nTable 1: Full results on BEIR. MonoT5 refers to the best-performing system in (Rosa et al., 2022), current SOTA performance holder. MonoT5 reranks the top 1000 documents from BM25 with a cross encoder. SGPT refers to the best performing GPT-style system from (Muennighoff, 2022), SGPT-BE 5.8B. As environments, we evaluate BM25, GTR and HRE (§3). The last four columns report the results of the BM25, GTR and HARE agents (§4.3) including a variant (RM3, based on HRE) that replaces T5-Q with RM3 (Pal et al., 2013). As customary on BEIR evals, we report the all datasets average and without MS MARCO (zero shot only). We also report the number of unique documents scored with the reranker, the average value for agents and HRE. The SGPT model retrieves over the full collection. The last column (RS) reports the performance of the (HQ) Rocchio Session algorithm used to generate the training data when run on all BEIR eval sets. Having access to the labeled document(s), it provides an estimate of the headroom for search agents.\n\nwhich reranks the top 1000 documents from BM25, with a cross encoder transformer reranker (Rosa et al., 2022). We also report the results of the best performing GPT-style system from (Muennighoff, 2022). SGPT is intriguing because of its unique performance profile (e.g., see on Trec-Covid and Arguana), reinforcing the suggestion that large decoder-only LMs introduce genuinely novel qualitative dimensions to explore. Next, we discuss the results of our BM25, GTR and HRE and corresponding agents. For all our systems, reranking depth is fixed at k=10.\n\nAll search agents run fixed 5-steps sessions at inference time. They all outperform their environment. One needs to factor in that agents score more documents, because of the multi-step sessions. The BM25 and GTR agents collect on average about 20 documents per session, HARE 66.7. One of the desired features of search agents is deep but efficient exploration, which is observable from the experiments. For the BM25 environment to match the BM25 agent performance (0.474/0.480), one needs to go down to k≈300, cf. Figure 2a. Overall, the BM25 agent outperforms the BM25 environment by more than 6 points. We highlight that the BM25 agent outperforms also the GTR environment, though not in-domain on MS MARCO – consistently with the findings of Adolphs et al. (2022). The GTR agent outperforms the GTR environment by 3 points, with the GTR environment beginning to perform better than the GTR agent only between k=50 and k=100.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Depth of the documents returned by HARE in the original retrieval results, in different depth interval buckets. Results are averages over all BEIR datasets.\n\n(b) Outcome of Rocchio sessions (oracle) compared to HARE’s and RM3’s predictions, as a function of the steps required to achieve the maximum score. Averages over all BEIR datasets.\n\nFigure 4: Analysis of BEIR tasks results.\n\nWith HRE, performance starts to saturate. However, HARE outperforms HRE by 1.4 nDCG points, scoring on average 66.7 documents vs. the 17.5 of HRE. Note that HRE’s maximum performance is 0.509/0.510, at k = 40, thus is never near HARE at any retrieval depth. HARE’s performance is comparable to MonoT5, the current SOTA: better on all datasets average by 0.3 points and worse on zero-shot only, by 0.1 points. However, HARE scores 15X fewer documents. A conservative estimate shows a consequent 50% latency reduction (cf. §A.1). Furthermore, HARE’s in-domain performance (0.483 on MS MARCO) keeps improving and is 8.5 points better than MonoT5. HARE has the best performance on 8 datasets (5 for MonoT5). We also evaluate a variant of the HRE search agent based on RM3 (Jaleel et al., 2004; Pal et al., 2013) a robust pseudo relevance feedback query expansion method (Lv & Zhai, 2009; Miao et al., 2012), which replaces T5-Q as the query expander. At each step, we pick the highest scoring term based on the RM3 score and add the selected term to the previous query with a ’+’ operator. The RM3 search agent is also effective, it improves over HRE but it does not perform as well as HARE, the reason being that it does not pull in enough new documents (33.4).\n\nThe last column in Table 1 reports an oracle headroom estimate. This is obtained by running the same Rocchio Session algorithm used to produce T5-Q’s training data (§5.1) at inference time. As the numbers show, there is substantial room for improvement. In the next section we continue with an in-depth analysis and open questions.\n\n5.4 QUALITATIVE ANALYSIS\n\nFigure 4a plots the average depth of the documents in HARE’s final top-10, over all BEIR tasks in the retrieval results from BM25, and GTR, for the original query. HARE digs deep in the original retrievers rankings, even beyond position 1000: 16.3% of HARE top-10 docs for BM25, and 6.9% for GTR. In this respect, HARE extends the finding of (Adolphs et al., 2022) to neural retrievers.\n\nFigure 4b looks at the outcomes of HARE and HRE-RM3 episodes, compared to oracle sessions. 49% of the time HARE doesn’t change the outcome (RM3, 52.8), 14% of the results are worse for both, 21% of the examples are resolved at the initial query for HARE (18.9 for RM3). However, 16% are improved by HARE (14 for RM3) and 8.3% need two or more steps. Table 5, in Appendix, looks in detail at an example of HARE win at step 2: ’what do you use dtp for +software +publishing’. Another, ’when did rosalind franklin take x ray pictures of dna +52 +taken +franklin +photo’ needs four steps. A single step one, but particularly transparent semantically is ’what is the age for joining aarp +requirements’. Hence, we find evidence of multistep inference and interpretable actions. Compared to HRE-RM3, HARE explores more documents in fewer steps. This is in part due to RM3 term weighting over-relying on the query. For example, RM3 needs three refinements to solve the query ’what make up the pistil in the female flower’, ’+pistil +female +stigma’, while HARE solves it in one with ’+ovary’.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nT5-R BM25-100 Large\n\nT5-R BM25-100 11B\n\nEnvironment\n\nT5-R\n\nT5-Q Large\n\nT5-Q 11B\n\nT5-R T5-Q Large\n\nT5-Q 11B\n\nBM25 GTR HRE\n\n0.437 0.467 0.506\n\n0.450 0.491 0.506\n\n0.447 0.493 0.507\n\n0.439 0.472 0.506\n\n0.449 0.493 0.510\n\n0.456 0.495 0.511\n\nTable 2: Average nDCG@10 on BEIR. ’T5-R BM25-100 Large’ and ’T5-R BM25-100 11B’ are, respectively, T5-Large and T5-v1 1-XXL reranker models trained on the Top 100 BM25 documents from MS MARCO. T5-Q Large and T5-Q 11B are T5-Large and T5-11B agent models trained on data generated via the high-throughput Rocchio Session process (HT, cf. §5.1).\n\nWe find that the HARE learns only to use ’+’, and completely ignores other operators. Part of the problem may be an unbalanced learning task for T5-Q (’+’ covers 83% of training). One way to make the task more expressive and balanced would be defining more operators. Lucene, for example, makes available several other operators including proximity, phrase and fuzzy match, and more. More generally, while interpretable, such operators are also rigid in their strict syntactic implementation in traditional search architectures. An interesting direction to explore is that of implementing such operators in a semantic framework, that is via neural networks, combining transparency and flexibility. SPAR’s approach to modeling phrase matches (Chen et al., 2021), for instance, is one step in this direction. Another possible limitation is the lack of title information in the training data, as more structure in the data may partition the learning space in more interesting ways. The importance of the document title, for one, is well-known in IR.\n\nIn early development phases on MS MARCO we trained several, T5-R and T5-Q, models using different configurations. We first trained T5-R models with the top 100 documents from BM25. We call these models ’T5-R BM25-100’. The T5-R model is used to generate training data for T5-Q, but it is also paired with the trained T5-Q model at inference time. Thus, the T5-R model needs to be robust when faced with documents originating from deeper than at training time. Larger T5-R models seem more robust in this respect, consistently with previous findings (Ni et al., 2021; Rosa et al., 2022). Similarly, larger T5-Q models seem more robust when training on the noisy data generated by the Rocchio Session procedure. Some of those steps are genuinely meaningful, some are spurious actions with little chance of being learnable. Eventually, we settled on the largest models available and trained the T5-R models with the top 1000 documents from BM25. Table 2 provides a sample of these explorations, evaluated on the BEIR dataset as an ablation study.\n\nThere are still open questions on how to properly train these models. For instance, it is apparent that our reranker is not as robust as as we would hope for the document depth that is usually explored by agents. Figure 2a clearly shows performance declining at k>50. This may, at least partially, explain why we don’t squeeze more performance from the existing headroom. A natural option, despite the negative findings of (Adolphs et al., 2022), is joint reinforcement learning of the agent.\n\n6 CONCLUSION\n\nIn this paper we extended the learning to search (L2S) framework to hybrid environments. In our approach, we simply combine dense and sparse retrievers, in what we call a hybrid retrieval environment (HRE), and leave results aggregation to a reranker which operates efficiently, only on the very top retrieved results. Our experiments show that our search environment, while simple, is competitive with respect to other hybrid proposals based on more complex design, or relying on reranking retrieval results very deeply. Our search agent, HARE, learns to explore the indexed document collection deeply, but nimbly, keeping the number of documents to rescore low. HARE leverages discrete query refinement steps which produce SOTA-level retrieval performance in a competitive zero-shot task, without degrading in-domain performance. Furthermore, we find evidence of effective multi-step inference, and the actions of the search agent are often easy to interpret and intuitive. Overall, we are inclined to conclude that search agents can support the investigation of performant information retrieval systems, capable of generalization. At the same time, they provide plenty of unexplored opportunities, and challenges, on the architectural and learning side.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nLeonard Adolphs, Benjamin B ̈orschinger, Christian Buck, Michelle Chen Huebscher, Massimiliano Ciaramita, Lasse Espeholt, Thomas Hofmann, Yannic Kilcher, Sascha Rothe, Pier Giuseppe Sessa, and Lierni Sestorain. Boosting search engines with interactive agents. Transactions on Machine Learning Research, 2022. URL https://openreview.net/forum?id= 0ZbPmmB61g.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901, 2020.\n\nSebastian Bruch, Xuanhui Wang, Mike Bendersky, and Marc Najork. An analysis of the softmax cross entropy loss for learning-to-rank with binary relevance. In Proceedings of the 2019 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR 2019), pp. 75–78, 2019.\n\nChristian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil Houlsby, and Wei Wang. Ask the right questions: Active question reformulation with reIn International Conference on Learning Representations, 2018. URL inforcement learning. https://openreview.net/forum?id=S1CChZ-CZ.\n\nTao Chen, Mingyang Zhang, Jing Lu, Michael Bendersky, and Marc Najork. Out-of-domain seIn Advances in Information Remantics to the rescue! trieval: 44th European Conference on IR Research, ECIR 2022, Stavanger, Norway, April 10–14, 2022, Proceedings, Part I, pp. 95–110, 2022. URL https://doi.org/10.1007/ 978-3-030-99736-6_7.\n\nzero-shot hybrid retrieval models.\n\nXilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen tau Yih. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? arXiv preprint arXiv:2110.06918, 2021.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.\n\nThibault Formal, Carlos Lassance, Benjamin Piwowarski, and St ́ephane Clinchant. Splade v2: Sparse lexical and expansion model for information retrieval, 2021a. URL https://arxiv. org/abs/2109.10086.\n\nThibault Formal, Benjamin Piwowarski, and St ́ephane Clinchant. SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking, pp. 2288–2292. Association for Computing Machinery, New York, NY, USA, 2021b. ISBN 9781450380379. URL https://doi.org/10.1145/ 3404835.3463098.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nThibault Formal, Carlos Lassance, Benjamin Piwowarski, and St ́ephane Clinchant. From distillation In Proceedings of to hard negative sampling: Making sparse neural ir models more effective. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’22, pp. 2353–2359, New York, NY, USA, 2022. Association for Computing ISBN 9781450387323. doi: 10.1145/3477495.3531857. URL https://doi. Machinery. org/10.1145/3477495.3531857.\n\nXiaoxiao Guo, Tim Klinger, Clemens Rosenbaum, Joseph P. Bigus, Murray Campbell, Ban Kawas, Kartik Talamadupula, Gerry Tesauro, and Satinder Singh. Learning to query, reason, and answer questions on ambiguous texts. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=rJ0-tY5xe.\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-\n\naugmented language model pre-training. https://arxiv.org/abs/2002.08909, 2020.\n\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.\n\nURL http://arxiv.org/abs/1503.02531.\n\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning, 2021. URL https://arxiv.org/abs/2112.09118.\n\nRolf Jagerman, Xuanhui Wang, Honglei Zhuang, Zhen Qin, Mike Bendersky, and Marc Najork. Rax: Composable learning-to-rank using jax. In Proceedings of the 28th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2022.\n\nNasreen Jaleel, James Allan, W. Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark Smucker,\n\nand Courtney Wade. Umass at trec 2004: Novelty and hard. In TREC, 01 2004.\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.\n\nOmar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’20, pp. 39–48, New York, NY, USA, ISBN 9781450380164. doi: 10.1145/3397271. 2020. Association for Computing Machinery. 3401075. URL https://doi.org/10.1145/3397271.3401075.\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K ̈uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ̈aschel, Sebastian Riedel, and Douwe In H. Larochelle, Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9459–9474, 2020.\n\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel. Question and answer test-train overlap in open-domain question answering datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, 2021.\n\nYuanhua Lv and ChengXiang Zhai. A comparative study of methods for estimating query language models with pseudo feedback. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM ’09, pp. 1895–1898, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605585123. doi: 10.1145/1645953.1646259. URL https: //doi.org/10.1145/1645953.1646259.\n\nJi Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. Zero-shot neural passage retrieval via domain-targeted synthetic question generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 1075–1088, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.eacl-main.92. URL https://aclanthology.org/2021.eacl-main.92.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJun Miao, Jimmy Xiangji Huang, and Zheng Ye. Proximity-based rocchio’s model for pseudo In Proceedings of the 35th International ACM SIGIR Conference on Research and relevance. Development in Information Retrieval, SIGIR ’12, pp. 535–544, New York, NY, USA, 2012. Association for Computing Machinery. ISBN 9781450314725. doi: 10.1145/2348283.2348356. URL https://doi.org/10.1145/2348283.2348356.\n\nNiklas Muennighoff.\n\nSgpt: Gpt sentence embeddings for semantic search.\n\narXiv preprint\n\narXiv:2202.08904, 2022.\n\nReiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. ArXiv, abs/2112.09332, 2021.\n\nTri Nguyen, Mir Rosenberg, Xia Song,\n\ngan Majumder, chine Reading COmprehension Dataset. https://www.microsoft.com/en-us/research/publication/ ms-marco-human-generated-machine-reading-comprehension-dataset/.\n\nSaurabh Tiwary, RanMS MARCO: A Human Generated MAURL\n\nCoRR, November\n\nand Li Deng.\n\nJianfeng Gao,\n\n2016.\n\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern ́andez ́Abrego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers. CoRR, abs/2112.07899, 2021. URL https://arxiv.org/abs/2112.07899.\n\nRodrigo Nogueira and Kyunghyun Cho.\n\nPassage re-ranking with bert.\n\narXiv preprint\n\narXiv:1901.04085, 2019.\n\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Document ranking with a pretrained sequence-tosequence model. CoRR, abs/2003.06713, 2020a. URL https://arxiv.org/abs/2003. 06713.\n\nRodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. Document ranking with a pretrained sequence-to-sequence model. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 708–718, Online, November 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.63. URL https://aclanthology.org/ 2020.findings-emnlp.63.\n\nDipasree Pal, Mandar Mitra, and Kalyankumar Datta. Query expansion using term distribution and term association. CoRR, abs/1303.0667, 03 2013. URL http://arxiv.org/abs/1303. 0667.\n\nPeng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, and Christopher D. Manning. Answering comIn Proceedings of the 2019 plex open-domain questions through iterative query generation. Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), November 2019. URL https://aclanthology.org/D19-1261.\n\nPeng Qi, Haejun Lee, OghenetegiriTGSido, and Christopher D. Manning. Answering open-domain\n\nquestions of varying reasoning steps from text. In EMNLP, 2021.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.\n\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qifei Wu, Yuchen Ding, Hua Wu, Haifeng Wang, and Ji-Rong Wen. A thorough examination on zero-shot dense retrieval, 2022. URL https://arxiv.org/abs/2204.12755.\n\nStephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333–389, 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL http://dx.doi.org/10.1561/1500000019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nJ. J. Rocchio. Relevance feedback in information retrieval. In G. Salton (ed.), The Smart retrieval system - experiments in automatic document processing, pp. 313–323. Englewood Cliffs, NJ: Prentice-Hall, 1971. URL https://sigir.org/files/museum/pub-08/XXIII-1. pdf.\n\nGuilherme Moraes Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, and Rodrigo Nogueira. No parameter left behind: How distillation and model size affect zero-shot retrieval. arXiv preprint arXiv:2206.02873, 2022.\n\nDaniel M. Russell. The Joy of Search: A Google Insider’s Guide to Going Beyond the Basics. The\n\nMIT Press, 2019.\n\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of\n\nbert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108, 2019.\n\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. ColBERTv2: Effective and efficient retrieval via lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3715–3734, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.272. URL https: //aclanthology.org/2022.naacl-main.272.\n\nChristopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. Simple entity-centric questions challenge dense retrievers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6138–6148, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 496. URL https://aclanthology.org/2021.emnlp-main.496.\n\nAnshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips). In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/ 310ce61c90f3a46e340ee8257bc70e93-Paper.pdf.\n\nNandan Thakur, Nils Reimers, Andreas R ̈uckl ́e, Abhishek Srivastava, and Iryna Gurevych. BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. CoRR, abs/2104.08663, 2021. URL https://arxiv.org/abs/2104.08663.\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=zeFrfgyZln.\n\nJingtao Zhan, Xiaohui Xie, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma.\n\nEvaluating extrapolation performance of dense retrieval. In CIKM, 2022.\n\nYunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information\n\nseeking for open-domain question answering. In EMNLP, 2021.\n\nHonglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. Rankt5: Fine-tuning t5 for text ranking with ranking losses. CoRR, 2022. doi: 10.48550/ARXIV.2210.10634. URL https://arxiv.org/abs/2210.10634.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nquery\n\nquery expansion (target)\n\nwhat is the age for hitting puberty trigeminal definition is rhinitis painful how many glasses of water is required a day how soon do symptoms show up for hiv\n\nthe collection film cast what kind of fossil is made by an imprint? who invented corn flakes what does gelastic mean what is acha\n\naroundˆ8 affectsˆ6 commonˆ6 everyˆ4 acuteˆ4\n\n+collection +fossil +john +laughter +hockey\n\nNLL\n\n75.6 71.2 70.0 67.5 67.5\n\n0.5 1.5 1.7 1.7 1.8\n\nTable 3: Examples from the evaluation set with highest respectively lowest negative log-likelihood (NLL) using the trained HARE search agent.\n\nA.1 LATENCY\n\nWe estimate latency by measuring the wall clock time of each individual step for HaRE, and MonoT5, running in the same setup; i.e., using the same sub-systems and configurations. For both MonoT5 and HARE we don’t include the full collection indexing times which are performed once, and focus on inference.\n\nFor MonoT5, we consider only the BM25 retrieval step, plus the T5-R inference to rerank the top 1000 documents returned. We call this LMonoT5. For HARE, we count: BM25 retrieval + GTR retrieval + T5-R inference + 4×(T5-Q inference + BM25 retrieval + BM25 Top-K retrieval + T5-R inference). We call this LHaRE. The resulting ratio is\n\nLHaRE LMonoT5\n\n= 0.51.\n\nNotice that, while we count the T5-R reranking latency in full for each step, in practice the same documents are often retrieved multiple times and caching can be used effectively.\n\nMore importantly, for simplicity sake, in the current implementation, we don’t create a separate Lucene index for the GTR Top-1K sub-corpus. Instead, to search on GTR’s Top-1K corpus, we search on the full collection but restrict the query to the Top-1k documents by means of Lucene’s ’id’ query operator. Hence, we append 1k document ids to the query, thus constraining retrieval to Top1K only. This turns out to be by far the slowest step for HARE. A conservative back of the envelope calculation shows that by refactoring the search step by indexing separately Top-1K for each episode and executing two separate BM25 retrieval steps, on the the full and Top-1K collections, would cut overall latency by another 50%.\n\nIn general, while more complex, HARE’s architecture offer’s many options for refactoring and optimization.\n\nA.2 EXAMPLES\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nQuery and Results\n\nd2\n\nd2\n\nq0 what’s the difference between c++ and java d1\n\nThe difference between C#/Java and C is too big, but the differences between C#/Java and C++ are easier to pick and the most important, other than said updates to the language, are the adoption of a pure OOP approach to programming. JavaScript is an interpreted computer programming language. The main difference between Java and JavaScript is that while Java is a programming language, JavaScript is scripting language. It is a prototype-based scripting language that is dynamic, weakly typed, and has first-class functions. However, like Java, JavaScript is heavily influenced by the C programming language. ...\n\nq1 what’s the difference between c++ and java +language d1\n\nThe most important difference is that Java is a memory-safe language, whereas C++ is not. This means that errors in Java programs are detected in defined ways; for example, attempting a bad cast or indexing an array out of bounds results in an exception. Java (bottom) is contrasted with C/C++ (top). Unlike C/C++, which is compiled into machine language for a specific hardware platform, Java programs are compiled into an intermediate bytecode language. The bytecode is then compiled into machine language by a runtime program called the Java Virtual Machine (JVM). Unlike C/C++, which is compiled into machine language for a specific hardware platform, Java programs are compiled into an intermediate bytecode language. The bytecode is then compiled into machine language by a runtime program called the Java Virtual Machine (JVM). ...\n\nd2\n\nq2 what’s the difference between c++ and java +language +platform d1 C++ is an evolution to C. Which was a system programming language. C++ Added many features to the language to make it object oriented. It became the mainstream programming language for that reason. Java is an evolution of C++, with different goals ( cross platform for instance ). It remove some of the features that make C++ so hard to learn. Simplify others and remove others. Java (bottom) is contrasted with C/C++ (top). Unlike C/C++, which is compiled into machine language for a specific hardware platform, Java programs are compiled into an intermediate bytecode language. The bytecode is then compiled into machine language by a runtime program called the Java Virtual Machine (JVM). Unlike C/C++, which is compiled into machine language for a specific hardware platform, Java programs are compiled into an intermediate bytecode language. The bytecode is then compiled into machine language by a runtime program called the Java Virtual Machine (JVM). ...\n\nScore\n\n0.0\n\n0.6\n\n1.0\n\nTable 4: Example of multistep gold search session that forms the training data for T5-Q. This is one of few examples from MS MARCO where more than one document is annotated as relevant. The first set of results are on topic but too generic, and slightly off the mark: the first document talks also about c#, the second document is about JavaScript. By restricting document to those containing the term ’language’ a relevant document is in 2nd position in the results from step 1. Here a new term is discovered, ’platform’, which was not present in the results for q0 (it does not occurr in any of the top 10 results, which we omit for simplicity). By further refining the query with +platform, the second-step results contain the two relevant documents at the top and and the session achieves a full score.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nQuery and Results\n\nq0 what do you use dtp for d1\n\nThe Dynamic Trunking Protocol (DTP) is a proprietary networking protocol developed by Cisco Systems for the purpose of negotiating trunking on a link between two VLAN-aware switches, and for negotiating the type of trunking encapsulation to be used. It works on Layer 2 of the OSI model.\n\nd2 DTP (diptheria, tetanus toxoids and pertussis) Vaccine Adsorbed (For Pediatric Use) is a vaccine used for active immunization of children up to age 7 years against diphtheria, tetanus, and pertussis (whooping cough) simultaneously. DTP is available in generic form. ... Page Layout Software (Generally Known as DTP Software). Since page layout software is commonly known as DTP software, this can lead to some confusion but now you know better. These software programs are the workhorses of DTP and they do exactly what you might think they would in accordance with the name. ...\n\nd9\n\nq1 what do you use dtp for +software d1\n\nd2\n\nthe operating systems main interface screen. desktop publishing. DTP; application software and hardware system that involves mixing text and graphics to produce high quality output for commercial printing using a microcomputer and mouse, scanner, digital cameras, laser or ink jet printer, and dtp software. Page Layout Software (Generally Known as DTP Software). Since page layout software is commonly known as DTP software, this can lead to some confusion but now you know better. These software programs are the workhorses of DTP and they do exactly what you might think they would in accordance with the name. ...\n\nq2 what do you use dtp for +software +publishing d1 Desktop publishing. Desktop publishing (abbreviated DTP) is the creation of documents using page layout skills on a personal computer primarily for print. Desktop publishing software can generate layouts and produce typographic quality text and images comparable to traditional typography and printing. Scribus, an open source desktop publishing application. Desktop publishing (abbreviated DTP) is the creation of documents using page layout skills on a personal computer primarily for print. Desktop publishing software can generate layouts and produce typographic quality text and images comparable to traditional typography and printing. ...\n\nd2\n\nScore\n\n0.0\n\n0.0\n\n1.0\n\nTable 5: Example of multistep search session performed by HARE. The first set of results gets an nDCG score of 0.0, and the top 2 seem clearly wrong. Curiously, none of the top-10 documents mentions the word ’publishing’. HARE first selects the refinement ’+software’. The corresponding results, while still scoring 0, lead to the presence of the term ’publishing’ which is used by HARE as the next refinement ’+publishing’, which leads to a full nDCG score on the next round.\n\n16",
    "reference": "# Summary Of The Paper\n\nAdolphs et al., 2022 propose to train a question refinement component as a search agent — a sparse retriever (BM25) first returns a list of results, and then the question refiner edits the original question (add/remove keyword or changes BM25 weight) to get a list of new results, mimicking human agent behavior. \n\nThis paper is an extension of Adolphs et al., by (1) using the combination of a dense retriever (GTR) and a sparse retriever (BM25) for the initial list and (2) using a cross-encoder for reranking. The experiment results show that the hybrid retriever is much better than just using either GTR or BM25 with the reranker. \n\nThen the authors conduct comprehensive experiments towards a series baseline, including the state-of-the-art MonoT5. The proposed HARE is better than other models using different environments or agents, and it achieved comparable performance to MonoT5. The authors also conduct some analysis on the agent behavior; for example, they found that HARE learns to use “adding keywords” the most.\n\n# Strength And Weaknesses\n\n## Strength\n\nThe authors present a finding that using the combination of dense and sparse agents in a retrieve-then-rerank setting leads to much better performance than a single agent (with fewer than 100 retrieved documents). The final model (hybrid search environment + query refinement) leads to a state-of-the-art model. The ablation study and analysis are comprehensive, showing the strength of the proposed hybrid environment.\n\n## Weakness\n\nIf my understanding is correct, the main contribution of the paper is to propose using sparse+dense retrievers +reranking at the first step. Though I don’t think previous works have reported this before, either combing sparse and dense retrievers [1] or using rerankers is not new. \n\nThe authors also sell the hybrid environment as it does not need to retrieve hundreds or thousands of documents for reranking, compared to state-of-the-art models like MonoT5. However, it is not fair to only compare the document numbers — first, sparse and dense retrievers have very different efficiencies; second, the proposed model has a multi-step question refinement procedure, which makes it much slower. To truly show the proposed model’s strength, the authors should really show the retrieval time per query of each model. \n\n[1] Ma et al., 2021. A Replication Study of Dense Passage Retriever.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSome comments/questions\n\n* Figure 1: caption?\n* List-wise softmax cross-entropy: isn’t this just contrastive learning (infoNCE) loss?\n* (Figure 2) Why does the hybrid environment even start to drop after k=50, while BM25 and GTR keeps increasing?\n* Figure 3 is very confusing… just reading the text is much clearer than reading this figure.\n* Since L2S is such an important baseline, the authors should mark it in the main experiment table. Based on my understanding, agents-bm25 is L2S?\n\n# Summary Of The Review\n\nIf my understanding is correct, the main contribution lies in the use of \"hybrid environment\" -- combing dense and sparse retrievers and rerankers. Also, there are some flaws in the experiments -- the authors should really compare the retrieval time of different models instead of the number of retrieved documents. Thus I am leaning towards rejection.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nWHEN AND WHY IS PRETRAINING OBJECT-CENTRIC REPRESENTATIONS GOOD FOR REINFORCEMENT LEARNING?\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nUnsupervised object-centric representation (OCR) learning has recently been drawing a lot of attention as a new paradigm of visual representation. This is because of its potential of being an effective pretraining technique for various downstream tasks in terms of sample efficiency, systematic generalization, and reasoning. Although image-based reinforcement learning (RL) is one of the most important and thus frequently mentioned such downstream tasks, the benefit in RL has surprisingly not been investigated systematically thus far. Instead, most of the evaluations have focused on rather indirect metrics such as segmentation quality and object property prediction accuracy. In this paper, we investigate the effectiveness of OCR pretraining for image-based reinforcement learning via empirical experiments. For systematic evaluation, we introduce a simple object-centric visual RL benchmark and verify a series of hypotheses answering questions such as “Does OCR pretraining provide better sample efficiency?”, “Which types of RL tasks benefit most from OCR pretraining?”, and “Can OCR pretraining help with out-of-distribution generalization?”. The results suggest that OCR pretraining is particularly effective in tasks where the relationship between objects is important, improving both task performance and sample efficiency when compared to singlevector representations. Furthermore, OCR models facilitate generalization to outof-distribution tasks such as changing the number of objects or the appearance of the objects in the scene.\n\n1\n\nINTRODUCTION\n\nMotivated by the natural ability of humans to break down complex scenes into their constituent entities and reason about them, there has been a surge of recent research in learning unsupervised object-centric (OCR) representations (Eslami et al., 2016; Crawford & Pineau, 2019; Kosiorek et al., 2018; Lin et al., 2019; Jiang et al., 2019; Kipf et al., 2019; Veerapaneni et al., 2020; Burgess et al., 2019; Greff et al., 2019; Engelcke et al., 2019; 2021; Locatello et al., 2020; Singh et al., 2021; Kipf et al., 2021; Elsayed et al., 2022; Singh et al., 2022). These approaches learn a structured visual representation of a scene, modeling an image as a composition of objects. By using object-centric representations, downstream tasks can potentially benefit from improved systematic generalization, better sample efficiency, and the ability to do reasoning between the objects in the scene.\n\nSince these representations can be obtained from visual inputs without the need for explicit labels, they have the promise of being an effective pretraining technique for various downstream tasks, including reinforcement learning (RL). However, most previous works in this line of research have evaluated OCRs only in the context of reconstruction loss, segmentation quality, or property prediction accuracy (Dittadi et al., 2021). While several studies have attempted to apply OCR to RL (Goyal et al., 2019; Zadaianchuk et al., 2020; Watters et al., 2019b; Carvalho et al., 2020), OCR pretraining has not been evaluated for RL tasks systematically and thoroughly. Watters et al. (2019b) evaluates OCR pretraining for a synthetic benchmark but a simple search is used rather than policy learning and less complex tasks are evaluated than our benchmark (e.g., the distractors can be ignored while our task requires the agent to avoid distractors).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nIn this study, we investigate when and why OCR pretraining is good for RL. To do this, we propose a new benchmark to cover many object-centric tasks such as object interaction or relational reasoning. Applying OCR pretraining to this benchmark, we empirically verify a series of hypotheses about decomposed representations that have been discussed previously but not systematically investigated (van Steenkiste et al., 2019; Lake et al., 2017; Greff et al., 2020; Diuk et al., 2008; Kansky et al., 2017; Zambaldi et al., 2018; Mambelli et al., 2022; Goyal et al., 2019; Carvalho et al., 2020; Zadaianchuk et al., 2020). For example, our experiments provide answers to questions such as: “Can decomposed representations improve the sample efficiency?”, “Can decomposed representations help with the out-of-distribution generalization?”, and “Can decomposed representations be helpful to solve relational reasoning tasks?”. Furthermore, we thoroughly investigate the important characteristics of applying OCR to RL, such as how number of objects in the scene affects RL performance, which OCR models work best for RL, and what kind of pooling layer is appropriate to aggregate the object representations.\n\nThe main contribution of this paper is to provide empirical evidence about the long-standing belief that object-centric representation learning is useful for reinforcement learning. For this, we have the following more specific contributions: (1) Propose a new simple benchmark to validate OCR pretraining for RL tasks systematically, (2) Evaluate OCR pretraining performance compared with various baselines on this benchmark, and (3) Systematically analyze different aspects of OCR pretraining to develop a better understanding of when and why OCR pretraining is good for RL. Lastly, we will release the benchmark and our experiment framework code to the community.\n\n2 RELATED WORK\n\nObject-Centric Representation Learning. Many recent works have studied the problem of obtaining object-centric representations without supervision (Eslami et al., 2016; Crawford & Pineau, 2019; Kosiorek et al., 2018; Lin et al., 2019; Jiang et al., 2019; Kipf et al., 2019; Veerapaneni et al., 2020; Burgess et al., 2019; Greff et al., 2019; Engelcke et al., 2019; 2021; Locatello et al., 2020; Lin et al., 2020; Singh et al., 2021; Kipf et al., 2021; Elsayed et al., 2022; Singh et al., 2022). These works are motivated by the potential benefits to downstream tasks such as better generalization and relational reasoning (Greff et al., 2020; van Steenkiste et al., 2019).\n\nThere are two main methods for building slot representations; bounding box based methods (Eslami et al., 2016; Crawford & Pineau, 2019; Kosiorek et al., 2018; Lin et al., 2019; Jiang et al., 2019) or segmentation based methods (Kipf et al., 2019; Veerapaneni et al., 2020; Burgess et al., 2019; Greff et al., 2019; Engelcke et al., 2019; 2021; Locatello et al., 2020; Singh et al., 2021; Kipf et al., 2021; Elsayed et al., 2022; Singh et al., 2022). The bounding box based methods infer the latent variables for object presence, object location, and object appearance temporally (Eslami et al., 2016; Kosiorek et al., 2018) or spacially (Crawford & Pineau, 2019; Lin et al., 2019; Jiang et al., 2019). These methods work best for objects of regular shape and size. Segmentation-based methods are more flexible than bounding box based methods and have shown good performance for natural scenes or videos (Singh et al., 2021; 2022; Kipf et al., 2021; Elsayed et al., 2022). In this study, we evaluated the segmentation based models only, because of their possibility to be applied to more natural tasks.\n\nObject-Centric Representations and Reinforcement Learning. RL is one of the most important and frequently mentioned downstream tasks where OCR is thought to be helpful. This is because it has been previously shown that applying decompositional representation to RL can perform better generalization and reasoning and learn more efficiently (Zambaldi et al., 2018; Garnelo et al., 2016; Diuk et al., 2008; Kansky et al., 2017; Stani ́c et al.; Mambelli et al., 2022; Heravi et al., 2022). However, to our knowledge, there have been no studies that systematically and thoroughly show these benefits. Goyal et al. (2019) evaluated OCR for RL by learning end-to-end. Through endto-end learning, OCR learns a task-specific representation, which may be difficult to apply to other tasks and may not have the various strengths obtained through unsupervised OCR learning such as sample efficiency, generalization, and reasoning. Zadaianchuk et al. (2020) investigated OCR pretraining, but applies the bounding box based method (Jiang et al., 2019) and proposes/evaluates a new policy for the limited regime; goal-conditioned RL. Watters et al. (2019b) trained OCR with the exploration policy.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nRL methods using decomposed representations have been previously investigated (Zambaldi et al., 2018; Garnelo et al., 2016; Diuk et al., 2008; Kansky et al., 2017; Stani ́c et al.; Mambelli et al., 2022; Heravi et al., 2022). Many of these works use CNN feature maps (Zambaldi et al., 2018; Stani ́c et al.; Heravi et al., 2022) as the representation or their own encoders (Garnelo et al., 2016). In other studies, ground truth states have been used (Diuk et al., 2008; Kansky et al., 2017; Mambelli et al., 2022), and these representations have been implemented through separate object detectors and encoders (Diuk et al., 2008; Carvalho et al., 2020). The effectiveness of the pretrained representations for out-of-distribution generalization of RL agents is studied in (Tr ̈auble et al., 2021), but only the distributed representation (e.g., VAE) is evaluated. In our work, we use a similar model as a baseline to compare with our pretrained OCR model.\n\n3 EXPERIMENTAL SETUP\n\nIn this section, we provide an overview of our experimental setup. We first discuss the OCR models and baselines we chose to evaluate and explain how the representations are used in a policy for RL. We then introduce the tasks used in our experiments detailing the motivation behind each task.\n\n3.1 MODELS\n\nEach model consists of (1) an encoder that takes as input an image observation and outputs a latent representation and (2) a pooling layer that combines the latent representation into a single vector suitable to be used for the value function and policy network of an RL algorithm. We use PPO (Schulman et al., 2017) for all our experiments. Detailed information about the architecture is in Appendix E.\n\nEncoders. We evaluated three state-of-the art OCR models, IODINE (Greff et al., 2019), Slot-Attention (Locatello et al., 2020), and SLATE (Singh et al., 2021). As non-OCR baselines, we pretrained a β−VAE Higgins et al. (2016) and evaluated both the single-vector representation (VAE) as well as the CNN feature map (CNN(VAE)). These encoders are pretrained on datasets consisting of scenes with randomly distributed objects or observations of trajectories taken by a random policy in the environment. Then, the encoders are frozen and only the pooling layers, policy, and value networks are updated when the agent is trained with RL. Lastly, we also compared these pretrained models with policies trained end-to-end from pixels (E2E CNN) and ground truth state (GT).\n\nFigure 1: The general architecture of the agent\n\nPooling Layers. In order to use the different types of representations for RL, we implement two types of pooling layers that combine the representations into a single vector used for the value function and policy network. The MLP pooling layer combines the representations using a multi-layer perceptron. The transformer pooling layer uses a vanilla Transformer encoder (Vaswani et al., 2017). Similar to Stani ́c et al., the output of a CLS token is used as the input for the policy. For CNN(VAE), the cells of the CNN feature map are used as input tokens and a positional embedding is added. For the OCR and GT models, the positional embedding is not used since those latent representations are order-invariant. For the E2E CNN model, the output of the CNN is used directly in the policy so no pooling layer is used.\n\n3.2 BENCHMARK AND TASKS\n\nTo verify our hypotheses, we created a suite of tasks using objects from Spriteworld (Watters et al., 2019a) (Figures 2a-d). While this environment is visually simple, we wanted to ensure that the OCR models could cleanly segment the objects of the scene into separate slots, reducing the possibility that the downstream RL performance is affected by poor OCR quality.\n\nIn order to evaluate the performance of OCR pretraining on a more visually complex environment, we also implemented a robotic reaching task using the CausalWorld framework (Ahmed et al., 2021) (Figure 2e). Details about the implementation of these benchmarks can be found in Appendix A.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Object-Goal\n\n(b) Object-Inter.\n\n(c) Object-Comp.\n\n(d) Property-Comp.\n\n(e) Object-Reach.\n\nFigure 2: Samples from the five tasks in our benchmark; Object Goal / Object Interaction / Object Comparison / Property Comparison / Object Reaching tasks. In the 2D tasks ((a) - (d)), the red ball is always the agent. See main text for details about each task. In the robotics task (e), the goal is to use the green robotic finger to touch the blue object before touching any of the distractor objects.\n\nObject Goal Task: In this task, the agent (red circle), target object (blue square), and other distractor objects are randomly placed. The goal of the task is for the agent to move to the target object without touching any of the distractor objects. Once the agent reaches the target object, a positive reward is given and the episode ends. If a distractor object is reached, the episode ends without any reward. The discrete action space consists of the four cardinal directions to move the agent. To solve this task, the agent must be able to extract information about the location of the target object as well as the objects in between the agent and the target. Therefore, through this task, we can verify that the agent is able to extract per-object information from the representation.\n\nObject Interaction Task: This task is similar to the object goal task, but requires the agent to push the target to a specific location. In Figure 2b, the bottom left blue square area is the goal area. Since the agent cannot push two objects at once, the agent must plan how to move the target to the goal area while avoiding the other objects. Therefore, through this task, we can verify not only how well the agent can extract per-object information, but also how well the agent can reason about how the objects interact. The action space is the same as above, and the reward is only given when the agent pushes the target to the goal area.\n\nObject Comparison Task: This task is designed to test relational reasoning ability and is motivated from the odd-one-out task in cognitive science Crutch et al. (2009); Stephens & Navarro (2008); Beatty & Vartanian (2015), which has been previously investigated with language-augmented agents Lampinen et al. (2022). To solve this task, the agent must determine which object is different from the other objects and move to it. That is, it must find the object that does not have any duplicates in the scene. Unlike the object goal or object interaction tasks, the characteristics of the target object can change from episode to episode. For example, in Figure 2c, the green box is the target object in the top sample, while the blue box is the target object in the bottom sample. Therefore, in order to know which object is the target, the agent must compare every object with every other object, which requires object-wise reasoning. The action space and reward structure are the same as the Object Goal Task.\n\nProperty Comparison Task: This task is similar to the Object Comparison Task, but requires more complex reasoning. The agent must find the object with a property (i.e. color or shape) that is different from the other objects. For example, in the top sample of Figure 2d, the blue triangle is the target because it is the only triangle in the scene. In the bottom sample, the green triangle is the target because it is the only object that is green. Therefore, this task requires property-level comparison, not just object-level comparison. While OCRs are designed to be disentangled at the object-level, it is not obvious how easily specific properties can be extracted and used for reasoning. Through this task, we can verify how well OCRs can facilitate property-level reasoning. The action space and reward structure are the same as the Object Comparison Task.\n\nObject Reaching Task: Lastly, in order to evaluate the models in a more visually realistic environment, we also created a version of the Object Goal Task using the CausalWorld framework (Ahmed et al., 2021) (Figure 2e). In this environment, a fixed target object and a set of distractor objects are randomly placed in the scene. The agent controls a tri-finger robot and must reach the target object with one of its fingers (the other two fingers are always fixed) to obtain a positive reward and solve\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nthe task. If the finger first touches one of the distractor objects, the episode ends without any reward. The action space in this environment consists of the 3 continuous joint positions of the moveable finger. We do not provide proprioceptive information to the agent, so it must learn from images how to control the finger.\n\n4 EXPERIMENTS\n\nWe first outline three key hypotheses in our study, building upon previous work on relational modeling of entities for reinforcement learning (Diuk et al., 2008; Kansky et al., 2017; Zambaldi et al., 2018; Mambelli et al., 2022; Goyal et al., 2019; Carvalho et al., 2020). We present experimental evidence supporting these hypotheses and answer some interesting questions that arose during our investigations. SLATE (Singh et al., 2021) is used to represent OCR pretraining models, and a comparison with other OCR models is shown in Figure 7. For OCR models, and CNN(VAE), Transformer pooling layer is used. For GT, we used MLP pooling layer for object interaction task and Transformer pooling layer for other tasks, because GT with MLP pooling layer shows better performance for object interaction task. For end-to-end CNN (E2E CNN) and VAE, MLP pooling layer is used. The result is the averaged performance from three random seeds, and the mean or mode of the action distribution is used as the action for evaluation.\n\n4.1 KEY HYPOTHESES\n\nHypothesis 1 (H1): OCR pretraining can improve the sample efficiency of agent learning. Since OCR pretraining provides a representation for each object, if the task is object-centric and OCR pretraining can obtain good representations, it is reasonable to believe an agent using OCR pretraining can learn more efficiently than agents using other representations (Zadaianchuk et al., 2020). We first investigate this general belief that has not yet been investigated systematically.\n\nHypothesis 2 (H2): OCR pretraining can be beneficial to solve relational reasoning tasks. It has been considered by several previous works that a decompositional representation will be useful for reasoning (Greff et al., 2020; van Steenkiste et al., 2019; Lake et al., 2017) and this has been experimentally verified (Zambaldi et al., 2018; Carvalho et al., 2020). Since OCR pretraining provides a well-decomposed representation per object, we can expect that OCR pretraining will be advantageous for the reasoning tasks.\n\nHypothesis 3 (H3): OCR pretraining can help in generalization of agents. Due to its object-wise modular representations, OCR has been shown to generalize well to out-of-distribution data such as unseen number of objects or unseen combination of objects (Dittadi et al., 2021; Locatello et al., 2020; Greff et al., 2019; Singh et al., 2021). Agents using explicit interaction networks like Transformers (Zambaldi et al., 2018) or Linear Relational Networks (Mambelli et al., 2022) have also shown good generalization performance in policy learning. It stands to reason, then, that combining OCR pretraining with a Transformer pooling layer should also have better generalization to out-ofdistribution data in RL.\n\n4.1.1 SAMPLE EFFICIENCY AND RELATIONAL REASONING (H1 AND H2)\n\nAs shown in Figure 3, OCR pretraining outperforms VAE pretraining and its variant CNN (VAE). In fact, the agents using VAE pretraining are not able to solve any of these tasks, pointing to the importance of OCR for this set of tasks. When compared with the end-to-end CNN (E2E CNN), OCR pretraining shows similar sample efficiency for the Object Goal and Object Interaction tasks,\n\nFigure 3: Success rate vs Interaction steps for the Object Goal / Object Interaction / Object Comparison / Property Prediction tasks. There are 2 objects in the Object Interaction task environment, and 4 objects in the other environments.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nbut learns significantly more quickly in the Object Comparison and Property Comparison tasks (E2E CNN is not able to solve the Property Comparison task within 2 million steps). It is very inefficient for the E2E CNN agent to learn the relationships between the objects only from the sparse reward, whereas the OCR pretraining agent can leverage the modular representations of the objects in the scene. This result supports our hypothesis that OCR pretraining can improve sample efficiency, and is especially helpful in the two tasks that require more relational reasoning among objects.\n\nInterestingly, SLATE shows comparable performance with the ground truth state agent (GT) for the Property Comparison task, which requires reasoning at the property level. This is despite the fact that SLATE representations are not necessarily disentangled at the property level. We hypothesize that the transformer pooling layer plays a critical role in correctly extracting the property level information. We also find that in this case, the hyperparameters have an important effect, which we will discuss in Q5.\n\nFigure 4: Generalization performance for the out-of-distribution settings. Top row: The success rate for the unseen number of objects. The in-distribution setting is denoted by “(in)”. To clearly show the performance difference between models, we only include the models that can reasonably solve the task. The same figure with all the models is in Figure 8. Bottom row: The success rate for unseen object colors. The leftmost points in each chart correspond to the in-distribution task.\n\n4.1.2 OUT-OF-DISTRIBUTION GENERALIZATION (H3)\n\nIn order to test the effect of OCR pretraining on generalization, we evaluated three out-ofdistribution cases: unseen number of objects, unseen object colors, and unseen combinations of objects. Additional out-of-distribution results are discussed in Appendix C.1 and C.2.\n\nFirst, we investigate the effect on agent performance when the number of objects differs from that on which the agent was trained. The results are shown in the top row of Figure 4. We see that for the Object Goal, Object Comparison, and Property Comparison tasks, OCR pretraining (SLATE) can generally maintain good performance as the number of objects in the scene changes. The performance on the Object Interaction task, however, degrades significantly as the number of distractor objects increases. Since the agent must learn to avoid the other objects as it pushes the target object to the goal, this task because much harder as the number of objects increases.\n\nInterestingly, we find that the E2E CNN can also maintain good performance as the number of objects changes. This makes sense for the Object Goal task, since the CNN is able to extract more objects in the scene than it was trained on. The target object, then, which is always the same color, can also easily be extracted. For the Object Comparison task, however, increasing the number of objects can cause unseen patterns such as the ones in Figure 2c. To solve the task, each object needs to be compared with all the other objects to find the odd-one-out. The Transformer Pooling layer can handle this pairwise comparison so it makes sense that SLATE and GT perform well when scaling to more objects. The fact that the E2E CNN also scales well to more objects suggests that the multilayered CNN can also select the odd-one-out object; however, the failure of the E2E CNN on the Property Comparison task indicates that this ability is limited as the task becomes more difficult.\n\nNext, we investigate agent performance when evaluated on object colors not seen during training. The experimental details such as which colors are changed are described in Appendix B.1. The\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nresults are shown in the bottom row of Figure 4. In this case, we see the pretrained OCR model (SLATE) generalizes the best when compared to the baselines, although performance does degrade as the number of unseen properties increases. Except for the Object Interaction task, GT is not robust to unseen object types since the object properties are represented as integer indices and the networks are not robust to indices it has not seen during training. For the Object Interaction task, GT generalizes better than other tasks, because MLP pooling uses concatenated GT state in the order of agent, target object, and distractor for this task. We notice that both SLATE and E2E CNN generalize almost perfectly to unseen colors in the Object Goal and Object Interaction tasks. This makes sense because only the distractor object colors change in this scenario and it is not important what colors they are. For the Object Comparison task, however, the colors of the objects are important and there is a sharp decrease in performance for both SLATE and E2E CNN as the number of unseen colors increases. Note, however, that for the Object-Comparison task, when 2 colors are changed in the color set, every scene becomes previously unseen. Yet SLATE still achieved around 80% success rate indicating the robustness of OCR pretraining.\n\nLastly, we investigated the effect of unseen combinations of objects, meaning all the object types are shown in training, but the combinations of object types on which we evaluate are not. This compositional ability is one of the strengths of decompositional representations (Greff et al., 2020), so we wanted to investigate it in the context of RL. We evaluate on the Object Comparison task and the results are shown in Table 1.\n\nSuccess Rate\n\nModel\n\nID\n\nOOD\n\n0.94 ± 0.008 GT E2E CNN 0.96 ± 0.015 0.949 ± 0.011 SLATE\n\n0.44 ± 0.038 0.116 ± 0.04 0.159 ± 0.026\n\nTable 1: Generalization performance of the Object-Comparison Task for unseen combinations.\n\nOCR pretraining is worse than GT, even the success rate is lower than random gussing on 4 objects. Interestingly, this is lower than when all the objects are unseen as shown in Figure 4. In that setting, when the number of unseen colors is 3, every object is previously unseen. We hypothesize that this result is because the difference between the slots is important to solve this task, so even though the objects are seen in training, if the difference is smaller than it has previously seen, the agent cannot solve it well. The detail settings and results are described in B.2 and Table 3.\n\n4.2 ANALYSIS\n\nThe remainder of this section answers several important questions that probe different aspects of OCR pretraining for RL.\n\nQuestion 1: Can OCR pretraining work better than the baselines in environments with more objects? What happens if there are fewer objects? The motivation for this question is related to the binding problem in neural networks (Greff et al., 2020). The non-OCR baselines have distributed representations, so the more objects there are in the scene, the more information needs to be bound together to represent them as entities. On the other hand, OCRs are free from this problem since they scale with the number of objects in a scene.\n\nModels\n\nModels\n\nTask\n\n#Objs\n\nSLATE\n\nE2E CNN\n\nTask\n\n#Objs\n\nSLATE\n\nVAE\n\nObject Goal\n\nObject Comp.\n\n4 6\n\n4 6\n\n0.979 ± 0.01 0.95 ± 0.00\n\n0.985 ± 0.01 0.746 ± 0.00\n\n0.985 ± 0.01 0.823 ± 0.00\n\n0.97 ± 0.01 0.34 ± 0.06\n\nObject Goal\n\nObject Int.\n\n1 3\n\n1 3\n\n0.997 ± 0.01 0.992 ± 0.01\n\n0.999 ± 0.00 0.686 ± 0.02\n\n0.99 ± 0.01 0.859 ± 0.03\n\n0.971 ± 0.01 0.345 ± 0.08\n\n(a) More number of objects\n\n(b) Fewer number of objects\n\nTable 2: Performance comparison when more or fewer objects are in the environments.\n\nTo answer this question empirically, we first evaluated SLATE and E2E CNN for the Object Goal and Object Comparison tasks by increasing the number of objects. Note that VAE is not compared, since it already does not perform well with only 4 objects. The result is shown in Table 2a. As the\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nnumber of objects increases, both models have lower success rates, but the performance degradation of E2E CNN is much greater than that of SLATE. When the number of objects is 6, E2E CNN only has a success rate of around 34%. Interestingly, when learning in an environment with 4 objects and testing in an environment with 6 objects, E2E CNN showed a higher success rate of close to 90% in Figure 4 (first row, third column). One explanation for this is that the E2E CNN learns the rules from the easier tasks and can work to some extent when applied to the more difficult task, but it is not able to learn from the difficult task itself. On the other hand, under the same conditions, SLATE showed a success rate of about 82% without any help from training on an easier task.\n\nWhat happens if there are fewer objects in the environment? In table 2b, we see that with fewer objects in the environment, both VAE and SLATE performed better, and the difference was much greater with VAE. Interestingly, when there is only one object, VAE showed similar performance to SLATE in both Object Goal and Object Interaction tasks. This, together with the above results, clearly shows the binding problem of the distributed representation (Greff et al., 2020).\n\nFigure 5: Success rate comparison vs wall-clock time. Each method is trained to 2 million interaction steps. VAE and CNN (VAE) were not compared because they did not show comparable performance.\n\nQuestion 2: Is OCR pretraining more efficient than end-to-end training in terms of wall-clock time? While we previously discussed the better sample efficiency of OCR, since OCR models typically require more computation than CNN architectures, it is natural to ask this question. As shown in Figure 5, SLATE is slower than E2E CNN or GT in terms of time to reach 2 million steps. In terms of wall clock time to convergence, however, SLATE is comparable to GT except for the Object Interaction task. For this task, the MLP pooling layer is used for GT, which is much faster than the Transformer pooling layer. When comparing with E2E CNN, for the Object Comparison task, the gap between SLATE is lower than when comparing via interaction steps, although SLATE still does train more quickly than E2E CNN. However, for the Object Interaction task, E2E CNN trains more quickly in terms of wall-clock time, even though it takes more interaction steps.\n\nQuestion 3: Does OCR pretraining work well in visually complex environments where segmentation is difficult? To answer this question, we tested SLATE and baselines in the Object Reaching task. As shown in Figure 13, the SLATE segmentation is not perfect, sometimes splitting several objects between slots and not cleanly capturing the robotic finger.\n\nFigure 6 shows the results of this experiment. Although the segmentation is not good, the agent using the SLATE OCR performs best on this task both in terms of sample efficiency and converged success rate. Note that even though this task does not explicitly require reasoning among the objects, it is still important for the agent to learn to avoid touching the distractor objects before the target object, so the distance between the objects as well as the robotic finger and the objects is important to solve the task. This result suggests that the conclusions from the experiments on visually simple environments can potentially generalize to more complex environments.\n\nFigure 6: Success rates for the Object Reaching Task.\n\nQuestion 4: Which OCR model is better for RL? To answer this question, we evaluated three stateof-the-art OCR models, Slot-Attention (Locatello et al., 2020), IODINE (Greff et al., 2019) and SLATE (Singh et al., 2021). For SLATE, we used the version using CNN encoder mentioned as an ablation of that in Singh et al. (2021). The detailed configurations are in Appendix E.\n\nFigure 7 shows that SLATE outperforms Slot-Attention and IODINE in these tasks. Even SlotInterestingly, SLATE and SlotAttention and IODINE fail to solve the Object Interaction task.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Success rates comparison between different OCR models. Attention are very similar architecturally—the main difference is in the decoder and slot size, which is larger for SLATE. To investigate further, we also test Slot-Attention with a larger slot size to match SLATE (Slot-Attention (Large)).\n\nSlot-Attention (Large) shows similar performance to SLATE, and outperforms Slot-Attention in all tasks. When looking at segmentation quality (Table 5) and property prediction accuracy (Figure 11), Slot-Attention and Slot-Attention (Large) do not show significant differences. From this result, at least for Slot-Attention, slot size appears to be important for RL performance regardless of segmentation quality or property prediction accuracy. For those models, we evaluated the correlation of RL performance with segmentation quality, property prediction accuracy, reconstruction loss in Appendix C.3.\n\nQuestion 5: How does the choice of pooling layer affect task performance? We have generally used the Transformer pooling layer for our OCR models since (a) it is permutation invariant, which is an important property since the slots in OCR generally do not have strict ordering, and (b) it explicitly models interactions between the slot which is important for the relational reasoning tasks. As an ablation study, we applied the MLP pooling layer by concatenating all the slots and compared it with the Transformer pooling layer. As shown in Figure 7, the version that applied the MLP pooling layer to SLATE, denoted as SLATE-MLP, performed worse than when the Transformer pooling layer was used for all tasks, failing to solve the interaction and comparison tasks completely. Surprisingly, SLATE-MLP is still able to achieve good performance on the Object Goal task, despite the fact that MLPs are not permutation invariant. This may be because the task is easier than others, and the target object can still be extracted from the MLP and the interaction between objects is not very important to solve that task.\n\n5 CONCLUSION AND DISCUSSION\n\nIn this paper, we investigated when and why OCR pretraining is good for RL. To do this, we designed a new benchmark and empirically evaluated several OCR models and baselines through various conditions. Through this study, we found several conditions that OCR pretraining is good for RL:\n\n• OCR pretraining performs better than distributed representations when the relationships between objects are important to solving the task. However, it can be slower for tasks where object-wise reasoning is not required due to computational overhead.\n\n• Because OCR pretraining provides a separate representation for each object between slots, it performs better than distributed representations in an environment with many objects.\n\n• OCR is better for generalization of agents, especially for objects not seen during training,\n\nshowing better performance than end-to-end learned representations.\n\n• OCR pretraining showed better performance than baselines even in visually complex environments where segmentation is not perfect. This suggests that what we found in visually simple environments can be applied to more complex environments.\n\nAlthough our benchmark covers several important object-centric tasks, such as object interaction and relational reasoning, there are other aspects of agent learning that can benefit from OCR such as partially observable environments or tasks that require exploration. These are good candidates to extend the benchmark in the future. Additionally, while we used several strong OCR baselines in our experiments, there are other OCR models that may be investigated in the future.\n\nWe hope that our benchmark can be useful for evaluating OCR models in the context of agent learning, in addition to the previously standard metrics such as segmentation quality and property prediction accuracy. Further discussion is in Appendix D.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nOssama Ahmed, Frederik Tr ̈auble, Anirudh Goyal, Alexander Neitz, Manuel W ̈uthrich, Yoshua Bengio, Bernhard Sch ̈olkopf, and Stefan Bauer. Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. In International Conference on Learning Representations, 2021.\n\nErin L Beatty and Oshin Vartanian. The prospects of working memory training for improving\n\ndeductive reasoning. Frontiers in human neuroscience, 9:56, 2015.\n\nChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.\n\nWilka Carvalho, Anthony Liang, Kimin Lee, Sungryull Sohn, Honglak Lee, Richard L Lewis, and Satinder Singh. Reinforcement learning for sparse-reward object-interaction tasks in a first-person simulated 3d environment. arXiv preprint arXiv:2010.15195, 2020.\n\nEric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolutional neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3412–3420, 2019.\n\nSebastian J Crutch, Sarah Connell, and Elizabeth K Warrington. The different representational frameworks underpinning abstract and concrete knowledge: Evidence from odd-one-out judgements. Quarterly Journal of Experimental Psychology, 62(7):1377–1390, 2009.\n\nAndrea Dittadi, Samuele Papa, Michele De Vita, Bernhard Sch ̈olkopf, Ole Winther, and Francesco Locatello. Generalization and robustness implications in object-centric learning. arXiv preprint arXiv:2107.00637, 2021.\n\nCarlos Diuk, Andre Cohen, and Michael L Littman. An object-oriented representation for efficient reinforcement learning. In Proceedings of the 25th international conference on Machine learning, pp. 240–247, 2008.\n\nGamaleldin F Elsayed, Aravindh Mahendran, Sjoerd van Steenkiste, Klaus Greff, Michael C Mozer, and Thomas Kipf. Savi++: Towards end-to-end object-centric learning from real-world videos. arXiv preprint arXiv:2206.07764, 2022.\n\nMartin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. arXiv preprint arXiv:1907.13052, 2019.\n\nMartin Engelcke, Oiwi Parker Jones, and Ingmar Posner. Genesis-v2: Inferring unordered object representations without iterative refinement. Advances in Neural Information Processing Systems, 34:8085–8094, 2021.\n\nSM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton, et al. Attend, infer, repeat: Fast scene understanding with generative models. Advances in Neural Information Processing Systems, 29, 2016.\n\nMarta Garnelo, Kai Arulkumaran, and Murray Shanahan. Towards deep symbolic reinforcement\n\nlearning. arXiv preprint arXiv:1609.05518, 2016.\n\nAnirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Sch ̈olkopf. Recurrent independent mechanisms. arXiv preprint arXiv:1909.10893, 2019.\n\nKlaus Greff, Rapha ̈el Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In International Conference on Machine Learning, pp. 2424–2433. PMLR, 2019.\n\nKlaus Greff, Sjoerd Van Steenkiste, and J ̈urgen Schmidhuber. On the binding problem in artificial\n\nneural networks. arXiv preprint arXiv:2012.05208, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDavid Ha and J ̈urgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances\n\nin neural information processing systems, 31, 2018.\n\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning\n\nbehaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.\n\nNegin Heravi, Ayzaan Wahid, Corey Lynch, Pete Florence, Travis Armstrong, Jonathan Tompson, Pierre Sermanet, Jeannette Bohg, and Debidatta Dwibedi. Visuomotor control in multi-object scenes using object-aware representations. arXiv preprint arXiv:2205.06333, 2022.\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. 2016.\n\nLawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classification, 2(1):193–218,\n\n1985.\n\nJindong Jiang, Sepehr Janghorbani, Gerard De Melo, and Sungjin Ahn. Scalor: Generative world models with scalable object representations. In International Conference on Learning Representations, 2019.\n\nDaniel Kahneman. Thinking, fast and slow. Macmillan, 2011.\n\nKen Kansky, Tom Silver, David A M ́ely, Mohamed Eldawy, Miguel L ́azaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zeroshot transfer with a generative causal model of intuitive physics. In International conference on machine learning, pp. 1809–1818. PMLR, 2017.\n\nThomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models.\n\narXiv preprint arXiv:1911.12247, 2019.\n\nThomas Kipf, Gamaleldin F Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional object-centric learning from video. arXiv preprint arXiv:2111.12594, 2021.\n\nAdam Kosiorek, Hyunjik Kim, Yee Whye Teh, and Ingmar Posner. Sequential attend, infer, repeat: Generative modelling of moving objects. Advances in Neural Information Processing Systems, 31, 2018.\n\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building\n\nmachines that learn and think like people. Behavioral and brain sciences, 40, 2017.\n\nAndrew K Lampinen, Nicholas Roy, Ishita Dasgupta, Stephanie CY Chan, Allison Tam, James Mcclelland, Chen Yan, Adam Santoro, Neil C Rabinowitz, Jane Wang, et al. Tell me why! In International Conference on explanations support learning relational and causal structure. Machine Learning, pp. 11868–11890. PMLR, 2022.\n\nZhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, and Sungjin Ahn. Space: Unsupervised object-oriented scene representation via spatial attention and decomposition. In International Conference on Learning Representations, 2019.\n\nZhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Bofeng Fu, Jindong Jiang, and Sungjin Ahn. Improving generative imagination in object-centric world models. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 6140–6149. PMLR, 2020. URL http://proceedings.mlr.press/v119/lin20f.html.\n\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. Advances in Neural Information Processing Systems, 33:11525–11538, 2020.\n\nDavide Mambelli, Frederik Tr ̈auble, Stefan Bauer, Bernhard Sch ̈olkopf, and Francesco Locatello. Compositional multi-object reinforcement learning with linear relation networks. arXiv preprint arXiv:2201.13388, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n\nAntonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dor-\n\nmann. Stable baselines3, 2019.\n\nAdam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. Advances in neural information processing systems, 30, 2017.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nGautam Singh, Fei Deng, and Sungjin Ahn. Illiterate dall-e learns to compose. In International\n\nConference on Learning Representations, 2021.\n\nGautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple unsupervised object-centric learning for com-\n\nplex and naturalistic videos. arXiv preprint arXiv:2205.14065, 2022.\n\nAleksandar Stani ́c, Yujin Tang, David Ha, and J ̈urgen Schmidhuber. An investigation into the open In Decision Awareness in Reinforcement Learning Workshop at\n\nworld survival game crafter. ICML 2022.\n\nR Stephens and D Navarro. One of these greebles is not like the others: Semi-supervised models for\n\nsimilarity structures. Cognitive Science Society, 2008.\n\nFrederik Tr ̈auble, Andrea Dittadi, Manuel Wuthrich, Felix Widmaier, Peter Vincent Gehler, Ole Winther, Francesco Locatello, Olivier Bachem, Bernhard Sch ̈olkopf, and Stefan Bauer. The role of pretrained representations for the ood generalization of rl agents. In International Conference on Learning Representations, 2021.\n\nSjoerd van Steenkiste, Klaus Greff, and J ̈urgen Schmidhuber. A perspective on objects and system-\n\natic generalization in model-based rl. arXiv preprint arXiv:1906.01035, 2019.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nRishi Veerapaneni, John D Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu, Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement learning. In Conference on Robot Learning, pp. 1439–1456. PMLR, 2020.\n\nNicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, and Andrea Tacchetti. Visual interaction networks: Learning a physics simulator from video. Advances in neural information processing systems, 30, 2017.\n\nNicholas Watters, Loic Matthey, Sebastian Borgeaud, Rishabh Kabra,\n\nSpriteworld: A flexible, configurable reinforcement\n\nLerchner. https://github.com/deepmind/spriteworld/, deepmind/spriteworld/.\n\n2019a.\n\nand Alexander learning environment. URL https://github.com/\n\nNicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P Burgess, and Alexander Lerchner. Cobra: Data-efficient model-based rl through unsupervised object discovery and curiosity-driven exploration. arXiv preprint arXiv:1905.09275, 2019b.\n\nAndrii Zadaianchuk, Maximilian Seitzer, and Georg Martius. Self-supervised visual reinforcement\n\nlearning with object-centric representations. arXiv preprint arXiv:2011.14381, 2020.\n\nVinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Deep reinforcement learning with relational inductive biases. In International conference on learning representations, 2018.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA BENCHMARK\n\nOur benchmark consists of 2D tasks from the Spriteworld (Watters et al., 2019a) and a 3D task from the CausalWorld (Ahmed et al., 2021).\n\nA.1\n\n2D TASKS\n\nOn 2D tasks, the observation size and channels are 64 and 3. The object size is represented as the percentage of the observation size. There is no occlusion between objects, and the agent is always the red ball with a size of 0.15. At the beginning of the episode, the agent position is always in the center. The action set is to move up/down/left/right. At each action, the agent moves 0.05 to the direction. The objects are randomly distributed, and their characteristics are sampled from the given set by following the rules of each task. For tasks, The object sizes are 0.15.\n\nFor pretraining, non-task sepcific dataset is used. The dataset for pretraining is consisted of scenes with randomly distributed objects. The number of objects in the scene is 5 and the object color is one of [blue, green, yellow, red] and shape is one of [square, triangle, star 4, circle]. The size is one of [0.15, 0.22]. The minimum distance between object is 0.15, so when object size is 0.22, occlusion could happen. The numbers of scenes for training and validation are 1 million and 100, 000.\n\nA.1.1 OBJECT GOAL TASK\n\nThe sets for shape, color are [square, triangle, star 4], and [blue, green, yellow, red]. The target object is always blue square. At every episode, only one target object is in the environment, and other objects are randomly generated.\n\nA.1.2 OBJECT INTERACTION TASK\n\nThe color set is [blue, green, yellow] and the shape is fixed as square. The target object is always blue square. To make a solvable task always, every object is far from the wall at least the object size (then the agent can push the target in any direction).\n\nA.1.3 OBJECT COMPARISON TASK\n\nThe color set is [blue, green, yellow], and the shape is fixed as a square. There must be a single unique object, and other objects are randomly generated in this rule.\n\nA.1.4 PROPERTY COMPARISON TASK\n\nThe shape and color sets are [square, triangle] and [blue, green]. There must be only one unique property (e.g., only one square in the environment or only one blue object), and other objects are randomly generated in this rule.\n\nA.2\n\n3D OBJECT REACHING TASK\n\nThe tri-finger environment is modified so that two of the fingers are always in the upright positions. The actions only affect the third green finger. At the beginning of each episode, all the fingers are in the upright positions. The observation size and channels are 64 and 3.\n\nThe color set [blue, green, yellow, red] and all objects are cubes.\n\nThe target object is always blue. For each episode, there is only one target object and the other objects are randomly generated. The positions of the objects are randomly chosen and chosen so that the blocks do not overlap with each other.\n\nFor pretraining, the dataset collected through random policy is used. The number of observations to train is 1 million.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: The unseen number of objects test results including VAE and CNN (VAE). There are no GT tests for object interaction tasks. This is because the MLP pooling layer used for GT cannot be applied to the unseen number of objects.\n\n[B,G],[G,Y]→[B,Y]\n\n[B,G],[B,Y]→[G,Y]\n\n[B,Y],[G,Y]→[B,G]\n\nModels\n\nID\n\nOOD\n\nID\n\nOOD\n\nID\n\nOOD\n\n0.95 ± 0.1\n\nGT E2E CNN 0.957 ± 0.06 0.947 ± 0.015 SLATE\n\n0.867 ± 0.05 0.117 ± 0.015 0.177 ± 0.025\n\n0.96 ± 0.01 0.97 ± 0.017 0.96 ± 0.02\n\n0.23 ± 0.035 0.123 ± 0.067 0.147 ± 0.021\n\n0.927 ± 0.006 0.953 ± 0.012 0.94 ± 0.0\n\n0.22 ± 0.03 0.107 ± 0.035 0.153 ± 0.031\n\nTable 3: The detail results of the unseen combination evaluation\n\nB EXPERIMENT DETAILS\n\nB.1 THE UNSEEN COLOR EVALUATION\n\nTo validate the generalization of the agent with the unseen object, we use the unseen color. For Object Goal and Interaction tasks, we change the colors of the distractors only, because the target object of the tasks is always the blue square. For Object Goal task, the in-distribution color set is [blue, green, yellow, red], and it is changed one by one as the follow; [blue, green, yellow, pink] → [blue, green, brown, pink] → [blue, cyan, brown, pink]. For Object Interaction task, the indistribution color set is [blue, green, yellow] and it is changed as [blue, green, pink] → [blue, cyan, pink].\n\nFor Object Comparison and Property Comparison tasks, we change the entire object color. For Object Comparison task, the in-distribution color set is [blue, green, yellow], which is changed as [blue, green, pink] → [blue, cyan, pink] → [brown, cyan, pink]. For Property Comparison task, the in-distribution color set is [blue, green]. It is changed as [blue, pink] → [cyan, pink].\n\nB.2 THE UNSEEN COMBINATION EVALUATION\n\nFor this evaluation, we used Object Comparison task. The task is to select the odd-one from 4 objects and the color set is [blue, green, yellow]. To evaluate the performance on the unseen combination, we train the models for 3 tasks where [blue, green] or [blue, yellow], or [green, yellow] combinations are not given. After training the models on the tasks, we evaluate the models on the unseen combinations. The detail results are in Table 3.\n\nC ADDITIONAL RESULTS\n\nC.1 EVALUATION WHEN THE ENVIRONMENT IS IN-DISTRIBUTION FOR OCR/VAE\n\nPRETRAINING BUT OUT-OF-DISTRIBUTION FOR THE AGENT\n\nIn the paper, we discussed the generalization of the agents when the environment is out-ofdistribution for pretraining and agents. Then, what happens if the agent is tested for the environment that is in-distribution for OCR or VAE (Dittadi et al., 2021)? We evaluate it by training OCR and VAE with a dataset consisting a super set of the task (e.g., if blue and green were used in the task, then training with blue, green, yellow and pink). After that, we trained the agent on property comparison task, and tested it for the unseen shapes or colors for the agent, but seen for OCR or VAE in pretraining.\n\nThe results are in Table 4. The expected result would be that the agent performs better in an environment that is in-distribution to pretraining and out-of-distribution to the agent than in an environment\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: The performance comparison when the agents are trained on the two environments, one of which is in-distribution for OCR and another is not.\n\nFigure 10: The left sample is from in-distribution and the right is from out-of-distribtion. Each column per sample means Observation/Reconstruction through DVAE/Reconstruction through Transformer decoder/Masks of slots (8 slots).\n\nthat is out-of-distribution to both. VAE pretraining shows better performance for the case, ID for pretraining but OOD for agent. However, interestingly, for OCR pretraining, there were no consistent results. For unseen shapes, the result is same to our expectation, but for unseen colors, the result is different from our thought. It is because even though the shape or color is seen in pretraining, the combination is new, so it could be difficult to the agent like the result for the unseen combination in Table 1.\n\nC.2 AGENT TRAINING WHEN THE ENVIRONMENT IS OUT-OF-DISTRIBUTION FOR OCR\n\nPRETRAINING\n\nAs other case, we can imagine, what happens if the agent is trained on the environment where is out-of-distribution for OCR pretraining. To evaluate this, we used OCR pretraining with 6 objects environment for object comparison task. Then, we trained the agents on the two environments, one of which is in-distribution for OCR and other is not. For out-of-distribution case, the every object in the environment is unseen, but the number of object is same to the trained dataset.\n\nAs shown in Figure 11, the performance gap is not too much, even though every object is unseen. However, interestingly, when seeing samples from each agents in Figure 10, we can find OCR not just cannot segment perfectly but also reconstruct poorly. From this result, we can find that OCR encoder can represent the objects even though the objects are unseen, which can be useful for the agent learning. It is quite related with the unseen object type results in Figure 4. The result in Figure 4 shows that the trained agent with OCR pretraining can be generalized better than baselines for the unseen object types. Those results support that we can use the OCR pretraining over the trained distribution for the agent training or the generalization of the agent.\n\nC.3 DO THE STANDARD METRICS OF EVALUATING OCR CORRELATE WITH RL\n\nPERFORMANCE?\n\nMany OCR models are evaluated through segmentation quality, reconstruction loss, or property prediction accuracy(Dittadi et al., 2021; Greff et al., 2019; Locatello et al., 2020; Singh et al., 2021; Burgess et al., 2019). Does performing well on these metrics translate to RL performance?\n\nTo answer this question, we calculated the correlations between the segmentation quality / reconstruction loss / property prediction accuracy and RL performance from SLATE / Slot-Attention / Slot-Attention (Large) / IODINE. For measuring segmentation quality and reconstruction loss, we used foreground Adjusted Rand Index (ARI) (Hubert & Arabie, 1985) and MSE. ARI, MSE, and property prediction accuracy are obtained through a dataset collected from a random policy, which\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nSuccess Rate\n\nSetting\n\nunseen shapes\n\nunseen colors\n\nModel\n\nID for Pretraining, OOD for Agent OOD for Pretraining and Agent\n\nSLATE VAE\n\nSLATE VAE\n\n0.663 ± 0.049 0.297 ± 0.035\n\n0.567 ± 0.021 0.357 ± 0.032\n\n0.487 ± 0.16 0.297 ± 0.379\n\n0.587 ± 0.04 0.31 ± 0.01\n\nTable 4: The generalization performance when the setting is in-distribution for pretraining and outof-distribution for agent learning.\n\nFG-ARI\n\nTasks\n\nSLATE Slot-Attention\n\nSlot-Attention (Large)\n\nIODINE\n\nObject Goal Object-Interaction Object Comparison Property Comparison\n\n0.909 0.932 0.912 0.911\n\n0.928 0.954 0.931 0.929\n\n0.927 0.95 0.93 0.929\n\n0.918 0.943 0.922 0.92\n\nTable 5: Foreground ARI scores.\n\nare shown in Tables 5, and 6, and Figure 11. For RL performance, the average of the success rate in 1000 episodes is used. The correlations are shown in Figure 12. MSE only is shown to have a positive correlation with RL performance for every task, meaning better reconstruction correlates with better RL performance. Other metrics such as ARI or property prediction accuracy are not shown to have correlated through those 4 tasks. We note that the number of validated model types is small, so it must be investigated more in future works.\n\nMSE\n\nTasks\n\nSLATE Slot-Attention\n\nSlot-Attention (Large)\n\nIODINE\n\nObject Goal Object-Interaction Object Comparison Property Comparison\n\n13.325 57.412 14.821 10.583\n\n6.5 35.915 5.898 5.539\n\n6.911 45.961 7.53 6.708\n\n9.345 85.714 9.026 8.5\n\nTable 6: MSE for Object Goal / Object Comparison / Property Comparison tasks\n\nC.4 COMPARISON WITH CNN FEATURE MAPS FROM OTHER MODELS\n\nCNN feature map from SLATE (Heravi et al., 2022): As another pretrained CNN feature map, we used the frozen CNN feature map from the pretrained SLATE encoder. Frozen CNN feature map from pretrained OCR is used in (Heravi et al., 2022). In the paper, the CNN feature map is encoded through another CNN which is trained through reward signal. We follows the architecture, but we evaluated also when applying Transformer on top of another CNN for pooling.\n\nMultiple E2E CNNs (Kipf et al., 2019; Watters et al., 2017): As previous works (Kipf et al., 2019; Watters et al., 2017) did, we evaluated the multiple CNN encoders that are trained through the reward signal. We used 5 encoders, and Transformer pooling is applied on top of the distributed representations from each encoder.\n\nCNN feature map from E2E CNN (CNN(E2E CNN)): In end-to-end learning manner, using CNN feature map not the distributed representation from the MLP on top of CNN. Similar architecture is used for relational reasoning in (Santoro et al., 2017). For pooling, Transformer is used.\n\nCNN feature map from SLATE (CNN(SLATE)): Using CNN feature map from SLATE encoder. It is frozen like OCR pretraining. This CNN feature map size is too large, we used additional CNN\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 11: The property prediction accuracy for four OCR models and VAE. For color and shape, the score is accuracy (higher is better). For x and y coordinates, it is a distance between prediction and grondtruth, so lower is better.\n\nFigure 12: The correlation between saturated / intermediate RL performance and usually used measurements for OCR (e.g., ARI, reconstruction loss or property prediction accuracy)\n\non top of that to collect smaller size of CNN feature map. On top of the smaller CNN feature map, Transformer pooling is used. Similar architecture is used in (Heravi et al., 2022), but they didn’t use Transformer, but use MLP on top of additional CNN.\n\nMultiple E2E CNNs (E2E MultipleCNNs): The model design to use multiple CNN encoders has been used to get object representation earlier in (Kipf et al., 2019; Watters et al., 2017). In the similar way, we evaluate this architecture on our benchmark. We note that the CNNs are trained through reward. We used 5 CNNs, and Transformer pooling is used.\n\nAs shown in Table 7, CNN feature maps with Transformer pooling fail to solve any tasks except for the Object Goal task. This should be investigated more in future work, but one of our hypotheses is that it can be difficult to train the Transformer pooling with a large number of patch representations through the reward signal. One piece of evidence for this is that the model using MLP pooling on the CNN feature map from SLATE works better than the model with Transformer pooling on the same feature map for the Object Interaction task.\n\nMultiple E2E CNNs outperform E2E CNN except for the Object Interaction task. We hypothesize that this is because the encoder model size is much larger than E2E CNN and the reward from the Object Interaction task is much sparser than other tasks. Another interesting result is that Multiple E2E CNNs solve the Property Comparison task somewhat. From this, we can expect that through multiple encoders, the model can represent object-wise information as shown in (Kipf et al., 2019; Watters et al., 2017), but it is not perfect.\n\nD FURTHER DISCUSSION\n\nTo evaluate OCR pretraining in this paper, the benchmark is limited in the scope where a random policy is sufficient to collect enough diverse observations, but there are lots of cases where trajectories from a random policy is not enough to pretrain the encoder. It could be an interesting topic to train OCR with auxiliary loss while learning a policy as shown in (Hafner et al., 2019; Ha & Schmidhuber, 2018). As another future research direction of the OCR community, it is also interesting to investigate non object-centric tasks with OCR or Modulating between System 1 and System 2 modes (Kahneman, 2011).\n\nE MODEL DETAILS\n\nIn this section, the architectural details about used models are introduced.\n\nE.1 ENCODER\n\nAs the encoder, we used several OCR models (Singh et al., 2021; Locatello et al., 2020; Greff et al., 2019), β−VAE (Higgins et al., 2016) and CNN trained through policy loss.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 13: SLATE segmentation on the Object Reaching Task\n\nTasks\n\nE2E CNN CNN Feat from E2E CNN Multiple E2E CNNs\n\nSLATE CNN Feat from SLATE with Trans pooling CNN Feat from SLATE with MLP pooling\n\nSuccess Rate\n\nObj. Goal\n\nObj. Int.\n\nObj. Comp.\n\nProp. Comp.\n\n0.983 ± 0.015 0.973 ± 0.012 0.987 ± 0.005\n\n0.977 ± 0.006 0.972 ± 0.011 0.987 ± 0.008\n\n0.899 ± 0.033 0.307 ± 0.415 0.222 ± 0.209\n\n0.963 ± 0.021 0.01 ± 0.0157 0.647 ± 0.256\n\n0.972 ± 0.007 0.22 ± 0.03 0.975 ± 0.016\n\n0.982 ± 0.011 0.219 ± 0.037 0.216 ± 0.029\n\n0.208 ± 0.037 0.185 ± 0.038 0.599 ± 0.053\n\n0.978 ± 0.013 0.218 ± 0.025 0.224 ± 0.028\n\nTable 7: The average success rate of 3 models with different random seeds after training to 2 million interaction steps\n\nE.1.1 GT\n\n2D tasks: Ground Truth state of this task is given as the matrix of the number of objects × 5. Each object is represented as [COLOR index, SHAPE index, SIZE index, x coordinate, y coordinate]. The color, shape, and size indices are from a pre-specified set. The pre-specified color set is [blue, green, yellow, red, cyan, pink, brown], and the pre-specified shape set is [square, triangle, star 4, circle, pentagon, hexagon, octagon, star 5, star 6, spoke 4, spoke 5, spoke 6]. For the size, [0.15, 0.22] is given. When using the MLP pooling layer, we add some MLPs on top of GT per object state like in Table 8.\n\n3D Object Reaching Task: The ground truth state of this task consists of 37 dimensions for the robot state (9 dimensions for joint positions, 9 dimensions for joint velocities, and 9 dimensions for end effector positions), and 9 dimensions for each object (3 for cartesian position, 3 for size, and 3 for color (RGB)). To represent the ground truth state of both robot state and objects in terms of slots, these 36 dimensions are concatenated together with an additional dimension indicating whether the slot is the robot arm or an object. The final representation consists of 5 slots, each with 37 dimensions.\n\nE.1.2 OCRS\n\nThe SLATE uses the CNN encoder originally used in (Locatello et al., 2020). Two versions of SlotAttention are evaluated, Slot-Attention and Slot-Attention (Large) which architecture is bigger than Slot-Attention. The number of slots is different for different tasks. The hyperparamters are in Table 9 and 10.\n\nE.1.3 VAE\n\nThe encoder and decoder to train VAE mainly consist of multiple blocks of CNN described in Table 11 and 12. The encoder consists of 4 encoder CNN blocks and one CNN layer which channels, kernel size, stride and padding are 64, 1, 1 and 0 without activation function, respectively. The output size is 4 × 4, and CNN-VAE uses it as the representation of the observation. To sampling the latent variable, the CNN feature map is flatten and encoded through the linear layers of mean and variance, input and output sizes of which are 64*4*4 and 256.\n\nTo decode, the latent variable is encoded through the linear layer, which input and output sizes are 256 and 64*4*4. The decoder consists of 4 decoder blocks with pixel shuffle function between\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nType\n\nOutput Size Activation\n\nLinear Linear\n\n32 32\n\nReLU ReLU\n\nTable 8: Hyperparameters for MLP of 2D GT state when using MLP pooling layer\n\nLearning\n\nConfigurations\n\nTemp. Cooldown Temp. Cooldown Steps LR for DVAE LR for CNN Encoder LR for Transformer Decoder LR Warm Up Steps LR Half Time Dropout Clip Batch Size\n\nSLATE\n\n1.0 to 0.1 30000 0.0003 0.0001 0.0003 30000 250000 0.1 0.05 24\n\nDVAE\n\nvocabulary size\n\nCNN Encoder\n\nHidden Size\n\nSlot Attention\n\nTransformer Decoder\n\nSlots Iterations Slot Heads Slot Dim. MLP Hidden Dim. Pos Channels\n\nLayers Heads Hidden Dim.\n\n4\n\n64\n\n- 3\n1 192 192 4\n\n4 4\n192\n\nTable 9: Hyperparameters for SLATE\n\nblocks, and one CNN layer which channels, kernel size, stride and padding are observation channel size, 1, 1 and 0 without activation function, respectively.\n\nThe learning rate is 0.0001, and the weight for KL-term is 5, and the batch size is 128.\n\nE.1.4 CNN\n\nThe CNN architecture for E2E CNN model is following the architecture used in (Mnih et al., 2015) which is also described in Table 13. After encoding the observation through this CNN, the output is flatten and pass a single linear layer with ReLU activation, which output size is 512.\n\nE.2 POOLING\n\nIn this study, we used two pooling layers; Transformer (Vaswani et al., 2017) and MLP which hyperparameters are described in Table 14 and 15.\n\nE.3 POLICY\n\nWe used PPO (Schulman et al., 2017) as the policy algorithm with the configuration in Table 16. The policy is trained through the Stable Baselines3 (Raffin et al., 2019), and the trajectories are collected through 4 environments.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nConfigurations\n\nSlot-Attention\n\nSlot-Attention (Large)\n\nLearning\n\nLR LR Warm Up Steps LR Half Time Clip Batch Size\n\n0.0001 30000 250000 0.05 24\n\nCNN Encoder\n\nHidden Size\n\nSlot Attention\n\nSlots Iterations Slot Heads Slot Dim. MLP Hidden Dim. Pos Channels\n\n64\n\n- 7\n1 64 128 4\n\n0.0001 30000 250000 0.5 24\n\n64\n\n- 3\n1 192 192 4\n\n-\n\nTable 10: Hyperparameters for Slot-Attention and Slot-Attention (Large)\n\nChannels Kernel Size\n\nStride\n\nPadding Activation\n\n64 64 64 64\n\n2 1\n1 1\n\n2 1\n1 1\n\n0 0\n0 0\n\nReLU ReLU ReLU ReLU\n\nTable 11: Hyperparameters for VAE Encoder CNN Block\n\nChannels Kernel Size\n\nStride\n\nPadding Activation\n\n64 64 64 64 *4\n\n3 1\n1 1\n\n2 1\n1 1\n\n1 0\n0 1\n\nReLU ReLU ReLU ReLU\n\nTable 12: Hyperparameters for VAE Decoder CNN Block\n\nChannels Kernel Size\n\nStride\n\nPadding Activation\n\n32 64 64\n\n8 4\n3\n\n4 2\n1\n\n0 0\n0\n\nReLU ReLU ReLU\n\nTable 13: Hyperparameters for E2E CNN\n\nType\n\nOutput Size Activation\n\nLinear Linear\n\n128 128\n\nReLU ReLU\n\nTable 14: Hyperparameters for MLP Pooling Layer\n\nConfigurations\n\nModel Dim heads\n\nlayers\n\n128 8\n3 for GT 1 for others\n\nTable 15: Hyperparameters for Transformer Pooling Layer\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nConfigurations\n\nSteps per training LR coefficient for entropy term\n\n2048 0.0003 0.0\n\nTable 16: Hyperparameters for PPO\n\n21",
    "reference": "# Summary Of The Paper\n\nThe paper provides an empirical evaluation of whether object-centric representation pre-training is useful for RL learning. They find that OCR pre-training generally delivers better and more data-efficient model, also allowing generalization to unseen settings (e.g., an unseen number of objects).\n\n# Strength And Weaknesses\n\nThe paper presents a nice analysis on the environments and tasks it studies. It shows that OCR outperforms end-to-end distributed representations on relational tasks and scenes with many objects but not on simple object goal tasks, which aligns with our expectation of OCR.\n\nHowever, my main concern is that the experiments only concern two simple synthetic environments. I am not sure if this can be considered as a comprehensive study on whether OCR pre-training is effective for reinforcement learning.\n\nThe experiments are als similar to the experiments in COBRA (Watters et al.), which also uses Spriteworld. It would be good if the authors could highlight the difference.\n\nWatters et al. COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration\n\n---\n\nMinor question:\n\nWhy would OCR models outperform the GT model in second figure in Figure 3? I would image GT as an upper-bound.\n\nHow is the GT state embedded? What does the ground truth state include?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is very clear. Most implementation details are provided.\n\n# Summary Of The Review\n\nOverall, while I think the authors provide nice analysis on the studied environment. However, I am not sure if the experiments can be considered as comprehensive enough to support some of the rather general claims the authors made.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPIPS: PATH INTEGRAL STOCHASTIC OPTIMAL CONTROL FOR PATH SAMPLING IN MOLECULAR DYNAMICS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe consider the problem of Sampling Transition Paths: Given two metastable conformational states of a molecular system, e.g. a folded and unfolded protein, we aim to sample the most likely transition path between the two states. Sampling such a transition path is computationally expensive due to the existence of high free energy barriers between the two states. To circumvent this, previous work has focused on simplifying the trajectories to occur along specific molecular descriptors called Collective Variables (CVs). However, finding CVs is non trivial and requires chemical intuition. For larger molecules, where intuition is not sufficient, using these CV-based methods biases the transition along possibly irrelevant dimensions. In this work, we propose a method for sampling transition paths that considers the entire geometry of the molecules. We achieve this by relating the problem to recent works on the Schrödinger bridge problem and stochastic optimal control. Using this relation, we construct a path integral method that incorporates important characteristics of molecular systems such as second-order dynamics and invariance to rotations and translations. We demonstrate our method on commonly studied protein structures like Alanine Dipeptide, and also consider larger proteins such as Polyproline and Chignolin.\n\n1\n\nINTRODUCTION\n\nModeling non-equilibrium systems in natural sciences involves analyzing dynamical behaviour that occur with very low probability known as rare events, i.e. particular instances of the dynamical system that are atypical. The kinetics of many important molecular processes, such as phase transitions, protein folding, conformational changes, and chemical reactions, are all dominated by these rare events. One way to sample these rare events is to follow the time evolution of the underlying dynamical system using Molecular Dynamic (MD) simulations until a reasonable number of events have been observed. However, this is highly inefficient computationally due to the large time-scales involved in MD simulations, which are typically related to the presence of high energy or entropy barriers between the metastable states. Thus, the main problem is: How can we efficiently sample trajectories between metastable states that give rise to these rare but interesting transition events?\n\nNumerous enhanced sampling methods such as steered MD (Jarzynski, 1997), umbrella sampling (Torrie and Valleau, 1977), constrained MD (Carter et al., 1989), transition path sampling (Dellago and Bolhuis, 2009), and many more, have been developed to deal with the problem of rare events in molecular simulation. Most of these methods bias the dynamical system with well-chosen geometric descriptors of the transition (analogous to lower dimensional features), called collective variables (CVs), that allow the system to overcome high-energy transition barriers and sample these rare events. The performance of these enhanced sampling techniques is critically dependent on the choice of these CVs. However, choosing appropriate CVs for all but the simplest molecular systems is fraught with difficulty, as it relies on human intuition, insights about the molecular system, and trial and error.\n\nA key alternative to sampling these rare transition paths is to model an alternate dynamical system that allows sampling these rare trajectories in an optimal manner (Ahamed et al., 2006; Jack, 2020; Todorov, 2009) or by learning an optimal RL policy for such a transition system Rose et al. (2021).\n\nIn this paper, we consider the problem of sampling rare transition paths by developing an alternative dynamical system using path integral stochastic optimal control (Kappen, 2005; 2007; Kappen and Ruiz, 2016; Theodorou et al., 2010). Our method models this alternative dynamics of the system\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nby applying an external control policy to each of the atoms in the molecule. We learn the external control policy such that it minimizes the amount of external work needed to overcome the lowest energy barrier and transition the molecular system from an initial meta-stable state to a final one. The method does not require any knowledge of CVs to sample these rare trajectories. Furthermore, we draw connections between sampling rare transition paths and the Schrödinger bridge problem (Schrödinger, 1931; 1932). Subsequently, we show that stochastic optimal control is well suited to solving these problems by extending the work of Kappen and Ruiz (2016) for molecular systems by incorporating Hamiltonian dynamics and equivariance constraints in our path integral SOC method.\n\nOur main contributions in this paper are:\n\n• We demonstrate the equivalence between the problem of sampling transition paths, the\n\nSchrödinger bridge problem, and path integral stochastic optimal control (SOC) (§2).\n\n• We develop PIPS, a path integral SOC method that incorporates second order Hamiltonian\n\ndynamics with clear physical interpretations of the system (§3).\n\n• In contrast to earlier work, PIPS does not require any knowledge of CVs, which is important for modeling large and complex molecular transitions for which CVs are unknown (§2-3).\n\n• Due to considering second order Hamiltonian dynamics, PIPS seamlessly integrates with\n\ncommon molecular dynamics frameworks such as OpenMM (Eastman et al., 2017).\n\n• We demonstrate the efficacy of PIPS on conformational transitions in three molecular systems of varying complexity, namely Alanine Dipeptide, Polyproline, and Chignolin (§4).\n\n2 PRELIMINARIES AND PROBLEM SETUP\n\nConsider a system evolving over time where π(x) is the distribution of states x and πi(xi|xi−1) a Markovian transition kernel. The distribution of trajectories generated by such a system is given by:\n\nπ(cid:0)x(τ )(cid:1) := π(x0) ·\n\nτ (cid:89)\n\ni=1\n\nπi(xi|xi−1).\n\n(1)\n\nwhere x(τ ) defines a trajectory of states of length τ discretized over time into an ordered sequence of states x(τ ) = {x0, x1, · · · , xτ }.\n\nThe problem of sampling transition paths involves sampling trajectories from this distribution, π(cid:0)x(τ )(cid:1), with the boundary condition that the initial state x0 and terminal state xτ are drawn from pre-specified marginal distributions π0 and πτ , respectively. These marginal distributions describe the stable states of the molecular system located at the local minimas of the free energy surface e.g. these stable states can be reactants and products of chemical reactions, or native and unfolded states of protein. Thus, these marginal distributions defining the stable states can be viewed as Dirac delta distributions. Unfortunately, these stable states are often separated by high free energy barriers making the trajectories, x(τ ), sampled starting from x0 to terminate in the target state xτ unlikely.\n\nIn this paper, we construct a sampling approach that generates trajectories that are still likely under the distribution π(cid:0)x(τ )(cid:1) while also adhering to the boundary conditions by crossing the high free energy barrier by incorporating relevant inductive biases of the system. Formally, we find an alternate dynamical system ˆπ(cid:0)x(τ )(cid:1) with marginals π0 and πτ that is as close to π(cid:0)x(τ )(cid:1) as possible, i.e.\n\nˆπ∗(cid:0)x(τ )(cid:1) :=\n\narg min ˆπ(x(τ ))∈D(π0,πτ )\n\n(cid:16)\n\nˆπ(cid:0)x(τ )(cid:1)∥π(cid:0)x(τ )(cid:1)(cid:17)\n\nDKL\n\n(2)\n\nwhere D(π0, πτ ) is the space of path measures with marginals π0 and πτ . This problem of learning an alternative dynamical system is also known as the Schrödinger Bridge Problem (SBP) (Schrödinger, 1931; 1932). We, thus, take inspiration from recent computational advances for solving SBP (Vargas et al., 2021a; De Bortoli et al., 2021) to develop our solution in §3 to solve the problem of sampling transition paths that can efficiently cross the high free energy barriers. Additionally, in this work, we propose an alternative approach to solving SBP using path integral stochastic optimal control that lends itself well to modelling the chemical nature of our problem.\n\nIn the next section, we will set the stage for this novel approach by first relating the problem of sampling transition paths as a path integral stochastic optimal control problem. Subsequently, we will\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nestablish an equivalence between learning an alternative dynamical system for sampling transition paths, Schrödinger bridge problem, and stochastic optimal control.\n\n2.1 SAMPLING TRANSITION PATHS THROUGH STOCHASTIC OPTIMAL CONTROL\n\nThe original dynamics of the system, as given in eq. (1), can be reformulated as a stochastic process:\n\ndxt = f (xt, t) dt + G(xt, t) · dεt,\n\nt ∈ [0, τ ]\n\n(3)\n\nwhere f : Rd × R+ → Rd and G : Rd × R+ → Rd×d are deterministic functions representing the drift and volatility of the system. The stochastic process εt is a Brownian motion with variance ν.\n\nAs we stated before, the system dynamics in Equation (3) is insufficient for sampling molecular transition paths as they do not adhere to the boundary conditions imposed by the problem. We, thus, add an external bias potential (or control) u(xt, t) ∈ Rd × R+ to the system that pushes the molecule over the transition state barriers. We can write the dynamics of this new system as follows:\n\ndxt = f (xt, t) dt + G(xt, t) ·\n\n(cid:16)\n\nu(xt, t) dt + dεt\n\n(cid:17)\n\n,\n\nt ∈ [0, τ ]\n\n(4)\n\nGiven a trajectory x(τ ) = (x0, · · · , xτ ) ∈ Rτ ×d generated through the SDE in eq. (4), we define the cost of this trajectory under control u following Kappen (2007); Theodorou et al. (2010) as:\n\nC(x(τ ), u, εt) =\n\n(cid:16)\n\n1 λ\n\nφ(xτ ) +\n\nτ (cid:88)\n\nt=0\n\n1 2\n\nu(xt, t)T Ru(xt, t) + u(xt, t)T Rεt\n\n(cid:17)\n\n(5)\n\nwhere φ denotes the terminal cost, λ is a constant and R is the cost of taking action u in the current state and is given as a weight matrix for a quadratic control cost. The goal then becomes to find the optimal control u∗ that minimizes the expected cost in Equation (5): (cid:2)C(x(τ ), u, εt)(cid:3)\n\nu∗ = arg min\n\n(6)\n\nEτ,εt\n\nu\n\nwhere the expectation is taken over trajectories τ sampled using the SDE under control u. Before proceeding further, a couple of remarks are in order:\n\nRemark 1. We note that the control u(xt, t) in Equation (4) does not operate directly on the system dynamics but is controlled through the same control matrix G as the Brownian motion. This formulation is highly crucial for our method, PIPS, to incorporate system specific second order Hamiltonian dynamics as we will show in Section 3.\n\nRemark 2. The last term in the cost function in eq. (5) relating the Brownian motion and the control is unusual and devoid of a clear intuition. However, this term plays an important role when relating the cost to a KL-divergence which we will establish next. Additionally, as discussed in Thijssen and Kappen (2015), the additional cost vanishes under expectation (Eτ,εt [u(xt, t)T Rεt] = 0) and thus, does not influence the optimal control u∗ given by eq. (6)\n\nRelation to sampling transition paths: Interestingly, the objective in Equation (6) is exactly related to the problem of sampling transition paths as given in Equation (2). As Kappen and Ruiz (2016) establish, Equation (4) defines a probability distribution πu\n\n(cid:0)x(τ )(cid:1) over trajectories x(τ ) through:\n\n(cid:0)x(τ )(cid:1) =\n\nπu\n\nτ (cid:89)\n\nt=0\n\nN (xt+1|μt, Σt)\n\n(7)\n\nwith μs = xs + f (xs, s) dt + G(xs, s)(u(xs, s) dt) and Σs = G(xs, s)T νG(xs, s). For different u, these distributions are related through the Girsanov Theorem (Cameron and Martin, 1944). As shown in appendix B, if we make the common assumption that the control cost R and the variance of the Brownian motion ν are inversely correlated as λR−1 = ν, we can obtain:\n\nlog\n\n(cid:0)x(τ )(cid:1) (cid:0)x(τ )(cid:1) =\n\nπu π0\n\n1 λ\n\nτ (cid:88)\n\nt=0\n\n1 2\n\nu(xt, t)T Ru(xt, t) + u(xt, t)T Rεt\n\n(8)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:0)x(τ )(cid:1) denotes the distribution over trajectories with no control i.e. u = 0 (§eq. (1)). This where π0 assumption of relating the control cost and the variance of the Brownian motion is a common trait of control problems referred to as Path Integral Stochastic Optimal Control (Kappen, 2005).\n\nWe observe that the right-hand side of eq. (8) can also be found in the definition of the control cost in eq. (5), including the additional cost term related to the Brownian noise. As Kappen and Ruiz (2016) show, we can thus use eq. (8) to rewrite the objective in Equation (6) as:\n\nπu∗ = arg min\n\nπu\n\nEx(τ )∼πu\n\n(cid:2) 1 λ\n\nφ(xτ )(cid:3) + DKL\n\n(cid:16)\n\n(cid:0)x(τ )(cid:1)∥π0\n\nπu\n\n(cid:0)x(τ )(cid:1)(cid:17)\n\n(9)\n\nThis objective is an approximation of the Schrodinger Bridge formulation in Equation (2) where the constraints on the marginal distributions are replaced by a regularization term in the form of the terminal cost. Therefore, when the terminal cost dominates the KL-divergence term above, it enforces the target boundary constraints of the problem. Before we discuss an algorithm to learn this optimal policy in Equation (9) next, we end this part with a remark:\n\nRemark 3. This connection between the Schrödinger Bridge Problem and stochastic optimal control has been previously established (Chen et al., 2016; Pavon et al., 2021). However, through the formulations in Equations (2) and (9), we also establish the equivalence between sampling transition paths, Schrödinger bridge problem, and stochastic optimal control. This allows us to utilize solutions for finding the optimal control in Equation (9) for the aforementioned problems.\n\nOptimal Control Policy: Kappen and Ruiz (2016) introduced the Path Integral Cross Entropy (PICE) method for solving Equation (9). The PICE method derives an explicit expression for the optimal policy and distribution πu∗ when λ = νR given by:\n\nπu∗ =\n\n1 η(x, t)\n\n(cid:0)x(τ )(cid:1) exp(−C(x(τ ), u))\n\nπu\n\n(10)\n\nwhere η(x, t) = Ex(τ )∼π0[exp(− 1 λ φ(xτ )] is the normalization constant. This establishes the optimal distribution πu∗ as a reweighing of any distribution induced by an arbitrary control u. Similar to importance sampling, depending on the choice of the proposal distribution πu, the estimator variance can greatly differ. Thus, the objective is to find the u that best approximates u∗.\n\nPICE, subsequently, achieves this by minimizing the KL-divergence between the optimal controlled distribution πu∗ and a parameterized distribution πuθ using gradient descent as follows:\n\n∂DKL(πu∗ |πuθ ) ∂θ\n\n= −\n\n1 η\n\nEx(τ )∼πuθ\n\n[exp(−C(x(τ ), uθ))\n\nτ (cid:88)\n\n(Rεt ·\n\nt=0\n\n∂uθ ∂θ\n\n)]\n\n(11)\n\nSimilar to the optimal control in eq. (10), the gradient used to minimize the KL-divergence is found by reweighing for each sampled trajectory, x(τ ), the gradient of the control policy uθ by the cost of said trajectory. Algorithm 1 in the appendix provides a method for finding this gradient and training the policy uθ. Thus, PICE provides an iterative gradient descent method to learn a parameterized policy uθ and subsequently a distribution over paths x(τ ). We can then use this learned control, uθ, to approximate the solution for sampling transition paths as well as the Schrödinger bridge problem.\n\nIn this section, we set up our main problem of sampling transition paths and established its relationship to both the Schrödinger bridge problem and stochastic optimal control. Subsequently, we discussed an iterative gradient descent based method for solving the optimal control problem. In the next section, we will extend this iterative algorithm to consider the entire geometry of the molecular system by incorporating Hamiltonian dynamics using an augmented state space xt, and symmetries by learning a policy network uθ.\n\n3 PATH INTEGRAL OPTIMAL CONTROL FOR SAMPLING TRANSITION PATHS\n\nWe consider a molecule consisting of n atoms with an initial and final configuration r0 ∈ R3×n and rτ ∈ R3×n i.e. we are given a vector defining the 3D positions of each atom in the molecule. Thus, a direct method to sample transition paths r(τ ) for this problem is to learn a control uθ acting directly on the positions r of the molecule using the iterative gradient descent method discussed in Section 2.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nHowever, the collective behaviour of the atoms and molecules are governed by classical molecular dynamics i.e. Newtonian equations of motion:\n\ndr = v(t) dt,\n\nand,\n\ndv = a(t) dt\n\n(12)\n\nwhere v(t) ∈ R3×n is the velocity and a(t) ∈ R3×n is acceleration given by a(t) = ∇r U (r)/m where U (r) is the potential energy of the system and m is the mass. The potential energy of a system is defined by a parameterized sum of pairwise empirical potential functions, such as harmonic bonds, angle potentials, inter-molecular electrostatic and Van der Waals potentials. In our work, we compute this potential energy using the OpenMM framework (Eastman et al., 2017). Therefore, in light of Equation (12), we need to adapt the dynamical system defined in Equation (4) to incorporate these molecular dynamics.\n\nIncorporating second order dynamics: Formally, we incorporate the second order dynamics of the system defined above by considering an augmented state space: Let x0 := (r0, v0) ∈ R3×n × R3×n be the initial configuration of the system defining the initial positions and velocities of each atom and xτ := (rτ , vτ ) be the final configuration. We, thus, model the dynamical system in Equation (4) as: (cid:19)\n\n(cid:19)\n\n(cid:18)\n\n(cid:19)\n\n(cid:17)\n\nu(xt, t) dt + dεt\n\n,\n\nt ∈ [0, τ ]\n\n(13)\n\n(cid:18)drt dvt (cid:124) (cid:123)(cid:122) (cid:125) dxt\n\n=\n\n(cid:124)\n\nvt −∇rtU (rt) (cid:123)(cid:122) f (xt,t)\n\ndt +\n\n(cid:125)\n\n(cid:16) ·\n\n(cid:18)03n I3n (cid:124) (cid:123)(cid:122) (cid:125) G(xt,t)\n\nDue to the choice of G(xt, t) in Equation (13) above, the additional bias force, u(xt, t), applied to the system only influences the acceleration and velocity of the atoms and does not act directly on the positions of the atoms. drt is solely influenced by the velocity vt, thus conforming to the classical molecular dynamics of the system as given in Equation (12).\n\nUnfortunately, this new dynamical system in Equation (13) leads to a singular covariance matrix, Σt in eq. (7) due to the choice of G. However, due to the conditional independence of rt+1 given (rt, vt), we are able to factorize the distribution in Equation (7) which circumvents the singularity of the covariance matrix. Due to space constraint, we provide details and derivations in Appendix B.1.\n\nRemark 4. We note here that second order dynamics have been considered before for stochastic optimal control by Kappen (2007) for a synthetic spring experiment in one dimension and SBP by Vargas et al. (2021a) for modelling motion. Our formulation of incorporating second-order dynamics here is distinct and more practical than these previous works. Courtsey of eq. (13), we have a clear physical interpretation of the control u as an external physical force by limiting it to act linearly on the velocity v. This is interesting for downstream applications of the sampled transition paths such as reconstructing free-energy surfaces. Additionally, it also simplifies incorporating the control with MD simulation software like OpenMM which we will discuss in detail in section 4.\n\nInvariance to rotations and translations: Secondly, the molecules in consideration are invariant w.r.t. translations and 3D rotations i.e. the molecular orientations achieved along a transition path need to incorporate this equivariance w.r.t. the SE(3) group. For this purpose, we need to make the terminal cost function, φ(xτ ), in Equation (5) to be equivariant. We enforce this by defining the terminal cost as the exponentiated pairwise distance between atoms which is commonly used distance metric (Shi (cid:0)dij(rt) − dij(rτ )(cid:1)2 et al., 2021) that is invariant to rotations and translations i.e. φ(rt) = exp (cid:80)n where dij(rt) = ∥(rt)i − (rt)j∥2 2.\n\ni,j\n\nPhysics inspired policy network (uθ): The main learnable component of our PIPS method (as described by eq. (13)) for sampling transition paths is the policy network uθ. Following the discussion above and formalized in Equation (13), we can interpret the control uθ as an additive bias force applied to the system. In this work, we consider two different design approaches to modelling uθ. In our first approach, we model uθ as a neural network that predicts the bias force on the system in which case the velocity evolves as dv = (∇rt U (rt) + uθ,t) dt. Alternatively, in our second approach, we model uθ as a network predicting the bias potential energy. In this case, the corresponding force, F (rt) ,applied to the system is calculated by backpropagating through the network, F (rt) := ∇rtuθ,t. The change in velocity is then given by dv = (cid:0)∇rtU (rt) + F (rt)(cid:1) dt. Additionally, uθ or uθ can be implemented using recent advances in physics inspired equivariant neural networks (Cohen and Welling, 2016; Satorras et al., 2021) that take into account the SE(3) symmetry of the system. We provide details for training the control network uθ in Appendix A.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nForce Prediction Energy Prediction\n\nMD w. fixed timescale\n\nMD w/ fixed timescale\n\nτ fs\n\n500 500\n\n500 500 500 500\n\n34810 48683\n\nTemp. K\n\nEPD (↓) nm × 10−3 %\n\nTHP (↑)\n\nETP (↓) kJ mol−1\n\n300 300\n\n300 1500 4500 9000\n\n1500 4500\n\n2.07 1.25\n\n7.92 7.47 6.33 6.82\n\n1.88 2.01\n\n41.1 % 89.2 %\n\n0.68 -5.21\n\n0% 0% 0% 1.7 %\n\n100% 100%\n\n- -\n- 1019.83\n\n551.51 1647.35\n\nTable 1: Benchmark scores for the proposed method and extended MD baselines. From-left-to-right: Time-horizon τ representing the trajectory length (note that we take one policy step every 1 fs), simulation temperature, Expected Pairwise distance (EPD), Target Hit Percentage (THP), and Energy Transition Point (ETP). ETP can only be calculate when a trajectory reaches the target. All metrics are averaged over 1000 trajectories except for MD w/ fixed timescale which is ran only for 10 trajectories.\n\n4 EXPERIMENTS\n\nWe evaluate our path integral stochastic optimal control method for sampling transition paths with three different molecular systems, namely (i) Alanine Dipeptide, a small amino acid with wellstudied transition paths, (ii) Polyproline, a small protein with two distinct conformations with different helix orientations, and (iii) Chignolin, an artificial mini-protein studied to understand the folding process of proteins. We begin by detailing the experimental setup below.\n\nMolecular Dynamics Simulation: As we discussed in section 3, we use the OpenMM framework to simulate the molecular dynamics following Equation (13). Crucially, by considering the second order dynamics, the control acts linearly on the molecular potential function in this formulation of the molecular dynamics. This allows us to implement the resulting control as a bias potential that is acting on the system in addition to the molecular potential. At every step of the Molecular Dynamics this bias potential is calculated using our PyTorch implementation of the control and then passed to OpenMM as a custom external force. Implementing the control this way thus allows us to use the optimized configuration capabilities of OpenMM, such as forcefield definitions (the potential function description) and integrators (for the time-discretization of our dynamics). We report the molecule specific OpenMM configuration in appendix C. Generally, we run our simulations at 300 K.\n\nPolicy Network, u(xt, t): We implement the policy network as a 6 layer MLP with ReLU activation for all our experiments below. The width of the layers of the policy network is dependent on the number of atoms in the molecule under consideration. We implement all code in Pytorch. We ran the experiments on a single GPU (either an NVIDIA RTX3080 or RTX2080). Our code, including a full stand-alone notebook re-implementation, is available here: https://github. com/pips4anonymous/pips-anonymous.\n\n4.1 ALANINE DIPEPTIDE\n\nAlanine Dipeptide is an extensively studied molecule (Tobias and Brooks III, 1992; Rossky and Karplus, 1979; Head-Gordon et al., 1991; Swenson et al., 2018) for developing and testing enhanced sampling methods due to ready availability of its two CVs (φ, ψ). The conformation transition for Alanine Dipeptide can thus be understood in terms of these two dihedral angles φ and ψ as displayed in Figure 1A. Prior work has, thus, focused on transforming from the initial configuration (see Figure 1A) to the final configuration (Figure 1E) by rotating these CVs. As we discussed previously, a major advantage of our method is that we do not require the knowledge of CVs to sample a transition path. However, in our experiment for Alanine Dipeptide, we will use these CVs to compare the quality of the trajectory sampled by our method.\n\nSetup: For our experiment, we consider both the design choices for the policy network, uθ(xt, t), discussed in Section 3 i.e. directly predicting the force and predicting the energy. We trained the policy networks for 15,000 roll-outs with a time horizon of 500 fs each consisting of 16 samples. A gradient update was made to the policy network after each roll-out with a learning rate of 10−5. The Brownian motion has a standard deviation of 0.1.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Visualization of a trajectory sampled with the proposed method. Left: The sampled trajectory projected on the free energy landscape of Alanine Dipeptide as a function of two CVs Right: Conformations along the sampled trajectory: A) starting conformation showing the CV dihedral angles, B-D) intermediate conformations with C being the highest energy point on the trajectory, and E) final conformation, which closely aligns with the target conformation. Bottom: Potential energy during transition. Letters represent the same configurations in the transition.\n\nBaseline and evaluation metrics: We compare our method to MD simulations with extended timehorizon and increased system temperatures to sample transition paths. To our knowledge, there are no fixed quantitative metrics in the literature to compare different methods that sample transition paths. Thus, we introduce here three metrics to evaluate the quality of transition paths: (i) Expected Pairwise Distance (EPD) measures the euclidean distance between the final conformation in the trajectory and the target conformation, reflecting the goal of the transition to end in the target state, (ii) Target Hit Percentage (THP) assures that the final configuration is also close in terms of CVs by measuring the percentage of trajectories correctly transforming these CVs, and (iii) Energy Transition Point (ETP) which evaluates the capacity of each method to find transition paths that cross the high-energy barrier at a low point by taking the maximum potential energy of the molecule along the trajectory. A good trajectory will be one that passes through the minimal high-energy barrier and ETP aims to measure this. We provide more details in Appendix C.2.1.\n\nResults: We first visualize the trajectory generated by the energy prediction policy in Figure 1 and defer the visualization for the force prediction policy to Appendix C.2.2. The trajectory in Figure 1 demonstrates that the control policy transforms the molecule from the initial position (A) to the final position (E) by transitioning over the barrier with the least energy at (C). Interestingly, the trajectory follows the expected transitions in the CVs without them being explicitly specified e.g. the transition path visualized on the left in Figure 1 shows that the molecule first rotates the dihedral angle associated with CV φ in (A → B), then gradually rotates along both ψ and φ in (B → C → D), and finally rotates ψ in (D → E) to reach the final configuration. As expected, we observe that the potential energy goes up during the transition until it reaches the top of the energy barrier (C). After this point, the molecule settles down in its new low-energy state.\n\nNext, in Table 1, we compare the performance of the trajectories sampled using the force and energy predicting policy networks with MD simulations on the metrics introduced before. We find that the trajectories generated by both the policy networks outperform the MD baselines, but the more physics-aligned energy predicting policy performs best under our metrics. This policy network consistently reaches the target conformation both in terms of full geometry and the CVs orientation. Furthermore, our policy network generates these trajectories in a significantly shorter time than temperature enhanced MD simulations without a fixed timescale. When we do limit MD to run for the same timescale as the proposed method, we found that, in contrast to the proposed method, temperature enhanced MD simulations are unable to generate successful trajectories.\n\n4.2 POLYPROLINE HELIX\n\nPolyproline is a helix-shaped protein structure that consists of repeating proline residues. Polyproline helix can form two different conformations namely Polyproline-I (PP-I) and Polyproline-II helix\n\n7\n\nABDECA:B:C:D:E:φΨABCDEUnder review as a conference paper at ICLR 2023\n\nFigure 2: Visualization of the Polyproline transformation from PP-II to PP-I. From-top-to-bottom 5 stages of the transition, ψ CVs, φ CVs, ω CVs, and Potential Energy. For the CVs multiple instances of the same dihedral angles can be found in a single molecule. Stars indicate target CV states. Colored bonds represent the bonds involved in the ω CV.\n\n(PP-II) (Moradi et al., 2009; 2010). These conformations can be distinguished by their respective helix rotation. PP-I forms a compact right-handed helix due to its peptide bonds having cis-isomers while PP-II has trans-isomer peptide bonds and forms a left-handed helix. Furthermore, the backbone of the polyproline helix also contains two different dihedral angles. We will refer to these peptide bonds and dihedral-angles as the ω, φ and ψ CVs respectively. Polyproline can have varying lengths due to its repeated structure. In our experiment, we consider the polyproline trimer with 3 proline residues transitioning from PP-II to PP-I.\n\nSetup: The policy network was trained over 500 rollouts with 25 samples each using a learning rate of 3 × 10−5 and a standard deviation of 0.1 for the Brownian motion.\n\nResults: We visualize the transformation of the three collective variables (ω, φ, ψ) as well as the corresponding potential energy of the conformation in Figure 2 for a sampled transition path from our trained policy network. The ω CV admits the biggest change for the transition from PP-I going from 180◦ to 0◦. We observe that the transition path sampled by our method aligns with the expected changes in CVs in spite of our method not containing any knowledge about these CVs. Figure 2 shows that the peptide bonds transition from a trans-isomer to a cis-isomer state at steps 450 and 3,000. We notice the biggest changes in CVs at these steps in Figure 2. We also note that in addition to the change in the peptide bonds, the final conformation differs from the initial in one of the ψ-dihedral angles. Technically, PP-I has ψ-dihedral angles similar to PP-II, but as a result of the inherent noise of MD our target conformation was sampled with a slight rotation here as well. Interestingly, our method successfully learned to sample transition paths terminating in a similar perturbed state. This indicates that our proposed method is resilient to target states not having a minimal-energy configuration.\n\n4.3 CHIGNOLIN\n\nChignolin is a small β-hairpin protein constructed artificially to study protein folding mechanisms (Honda et al., 2004; Seibert et al., 2005). Due to its small size, its folding process is easier to study than larger scale proteins while being similar enough to shed light on this complex process. In contrast to Alanine Dipeptide and Polyproline, there is no agreement on the transition mechanism describing the folding of Chignolin. Both the CVs involved (Satoh et al., 2006; Paissoni and Camilloni, 2021), as well as the sequence of steps (Harada and Kitao, 2011; Satoh et al., 2006; Suenaga et al., 2007; Enemark et al., 2012) describing the folding process have multiple different interpretations. Thus, methods that do not require prior knowledge of CVs are particularly useful to study this protein.\n\nSetup: We sample transition paths between the folded and unfolded state of the Chignolin protein using a total time horizon of 5000 fs. Note that the typical folding time of Chignolin is recorded to be\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Visualization of the Chignolin folding process. Top: 5 stages of the folding process, Middle: Pairwise distance wrt to the target conformation of the molecule, Bottom: Potential Energy.\n\n0.6 μs (Lindorff-Larsen et al., 2011). The policy network is trained over 500 rollouts of 16 samples with a learning rate of 1 × 10−4 and standard deviation of 0.05 for the Brownian motion.\n\nResults: In Figure 3, we visualize the transition of Chignolin at 5 different timesteps during the transition path. We observe that to transition the protein from its low energy unfolded state to the folded conformation, the proposed method guides the protein into a region of higher energy. This increase is initially more steep (0→1500) than in the later stages. Additionally, most of the finer-grained folding (2500→4000) occurs with a high potential energy before settling into the lowerenergy folded state. We notice that for the restricted folding time we use in our experiments (5000 fs vs 0.6 μs), the molecule does not end at the final configuration but reaches close to it as shown by the plot on pairwise distance. Furthermore, the learned policy network is able to transition through the high energy transition barrier in this restricted time. We do not encounter this for molecules with a shorter natural transition time (as illustrated by the potential energy of Alanine Dipeptide in fig. 1).\n\n5 DISCUSSIONS, LIMITATIONS, AND FUTURE WORK\n\nIn this work, we proposed a path integral stochastic optimal control method for the problem of sampling rare transition paths for molecular systems that incorporates the Hamiltonian dynamics and equivariance of the system. In passing, we showed an equivalence between the problem of sampling transition paths, stochastic optimal control, and the Schrödinger bridge problem. We empirically tested our method on three different molecular systems of varying sizes and demonstrated that it was able to sample transition paths on the full geometry of the system without biasing along CVs.\n\nOne observed limitation of the proposed method is that for molecules with long natural transition times, we observe the transitions to not converge to the configuration of minimal energy after crossing the high-energy transition barrier. We hypothesize that this is due to the method operating on a reduced time horizon (e.g. 5000 fs instead of 0.6 μs in the case of Chignolin), or due to the terminal control cost function not requiring the velocity to be zero at the end of the transition. Nevertheless, we note that the method is successful in transitioning the molecules over the high energy barriers as exemplified by the known CVs changing appropriately.\n\nThere are many exciting directions for future work. Most importantly, we are excited to see how research from other machine learning fields can be used to develop, possibly more efficient, methods for sampling trajectories between molecular conformations. Given the vast literature on Stochastic Optimal Control theory, Schrodinger Bridge samplers, and other topics such as Covariance Control (Yin et al., 2021; Hotz and Skelton, 1987) and Reinforcement Learning (Das et al., 2021) we hope that our path-integral based method can serve as a starting point for machine learning based solutions for the problem introduced in our work. Following this, these approaches and their sampled trajectories, could be used for solving related problems in chemistry. For example, our experiments showed that the molecules transitioned along the CVs correctly in spite of not having any information about the CVs. It will be interesting to see if we can infer these CVs from the learned policy and dynamics of the systems. Lastly, our method can have implications for training diffusion models within a fixed time-scale by additionally learning the control policy to transform one distribution into another. First explorations of this line of thinking are presented in (Vargas et al., 2021b; Zhang and Chen, 2021).\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nChristopher Jarzynski. Nonequilibrium equality for free energy differences. Physical Review Letters,\n\n78(14):2690, 1997.\n\nGlenn M Torrie and John P Valleau. Nonphysical sampling distributions in Monte Carlo free-energy\n\nestimation: Umbrella sampling. Journal of Computational Physics, 23(2):187–199, 1977.\n\nEA Carter, Giovanni Ciccotti, James T Hynes, and Raymond Kapral. Constrained reaction coordinate dynamics for the simulation of rare events. Chemical Physics Letters, 156(5):472–477, 1989.\n\nChristoph Dellago and Peter G Bolhuis. Transition path sampling and other advanced simulation techniques for rare events. Advanced computer simulation approaches for soft matter sciences III, pages 167–233, 2009.\n\nTP Imthias Ahamed, Vivek S Borkar, and S Juneja. Adaptive importance sampling technique for\n\nMarkov chains using stochastic approximation. Operations Research, 54(3):489–504, 2006.\n\nRobert L Jack. Ergodicity and large deviations in physical systems with stochastic dynamics. The\n\nEuropean Physical Journal B, 93(4):1–22, 2020.\n\nEmanuel Todorov. Efficient computation of optimal actions. Proceedings of the national academy of\n\nsciences, 106(28):11478–11483, 2009.\n\nDominic C Rose, Jamie F Mair, and Juan P Garrahan. A reinforcement learning approach to rare\n\ntrajectory sampling. New Journal of Physics, 23(1):013013, 2021.\n\nHilbert J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of\n\nstatistical mechanics: theory and experiment, 2005(11):P11011, 2005.\n\nHilbert J Kappen. An introduction to stochastic control theory, path integrals and reinforcement learning. In AIP conference proceedings, volume 887, pages 149–181. American Institute of Physics, 2007.\n\nHilbert Johan Kappen and Hans Christian Ruiz. Adaptive importance sampling for control and\n\ninference. Journal of Statistical Physics, 162(5):1244–1266, 2016.\n\nEvangelos Theodorou, Jonas Buchli, and Stefan Schaal. A generalized path integral control approach to reinforcement learning. The Journal of Machine Learning Research, 11:3137–3181, 2010.\n\nErwin Schrödinger. Über die umkehrung der naturgesetze. Verlag der Akademie der Wissenschaften\n\nin Kommission bei Walter De Gruyter u . . . , 1931.\n\nErwin Schrödinger. Sur la théorie relativiste de l’électron et l’interprétation de la mécanique quantique.\n\nIn Annales de l’institut Henri Poincaré, volume 2, pages 269–310, 1932.\n\nPeter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A Beauchamp, Lee-Ping Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, et al. OpenMM 7: Rapid development of high performance algorithms for molecular dynamics. PLoS computational biology, 13(7):e1005659, 2017.\n\nFrancisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving schrödinger\n\nbridges via maximum likelihood. Entropy, 23(9):1134, 2021a.\n\nValentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrödinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34, 2021.\n\nSep Thijssen and H. J. Kappen. Path integral control and state-dependent feedback. Phys. Rev. E, 91:032104, Mar 2015. doi: 10.1103/PhysRevE.91.032104. URL https://link.aps.org/ doi/10.1103/PhysRevE.91.032104.\n\nRobert H Cameron and William T Martin. Transformations of weiner integrals under translations.\n\nAnnals of Mathematics, pages 386–396, 1944.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nYongxin Chen, Tryphon T Georgiou, and Michele Pavon. On the relation between optimal transport and Schrödinger bridges: A stochastic control viewpoint. Journal of Optimization Theory and Applications, 169(2):671–691, 2016.\n\nMichele Pavon, Giulio Trigila, and Esteban G Tabak. The Data-Driven Schrödinger Bridge. Commu-\n\nnications on Pure and Applied Mathematics, 74(7):1545–1573, 2021.\n\nChence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. In International Conference on Machine Learning, pages 9558–9568. PMLR, 2021.\n\nTaco Cohen and Max Welling. Group equivariant convolutional networks. In International conference\n\non machine learning, pages 2990–2999. PMLR, 2016.\n\nVictor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks.\n\narXiv preprint arXiv:2102.09844, 2021.\n\nDouglas J Tobias and Charles L Brooks III. Conformational equilibrium in the alanine dipeptide in the gas phase and aqueous solution: A comparison of theoretical results. The Journal of Physical Chemistry, 96(9):3864–3870, 1992.\n\nPeter J Rossky and Martin Karplus. Solvation. A molecular dynamics study of a dipeptide in water.\n\nJournal of the American Chemical Society, 101(8):1913–1937, 1979.\n\nTeresa Head-Gordon, Martin Head-Gordon, Michael J Frisch, Charles L Brooks III, and John A Pople. Theoretical study of blocked glycine and alanine peptide analogs. Journal of the American chemical society, 113(16):5989–5997, 1991.\n\nDavid WH Swenson, Jan-Hendrik Prinz, Frank Noe, John D Chodera, and Peter G Bolhuis. OpenPathSampling: A Python framework for path sampling simulations. 1. Basics. Journal of chemical theory and computation, 15(2):813–836, 2018.\n\nMahmoud Moradi, Volodymyr Babin, Christopher Roland, Thomas A Darden, and Celeste Sagui. Conformations and free energy landscapes of polyproline peptides. Proceedings of the National Academy of Sciences, 106(49):20746–20751, 2009.\n\nMahmoud Moradi, Volodymyr Babin, Christopher Roland, and Celeste Sagui. A classical molecular dynamics investigation of the free energy and structure of short polyproline conformers. The Journal of chemical physics, 133(12):09B614, 2010.\n\nShinya Honda, Kazuhiko Yamasaki, Yoshito Sawada, and Hisayuki Morii. 10 residue folded peptide\n\ndesigned by segment statistics. Structure, 12(8):1507–1518, 2004.\n\nM Marvin Seibert, Alexandra Patriksson, Berk Hess, and David Van Der Spoel. Reproducible polypeptide folding and structure prediction using molecular dynamics simulations. Journal of molecular biology, 354(1):173–183, 2005.\n\nDaisuke Satoh, Kentaro Shimizu, Shugo Nakamura, and Tohru Terada. Folding free-energy landscape\n\nof a 10-residue mini-protein, chignolin. FEBS letters, 580(14):3422–3426, 2006.\n\nCristina Paissoni and Carlo Camilloni. How to determine accurate conformational ensembles by metadynamics metainference: a chignolin study case. Frontiers in molecular biosciences, 8:491, 2021.\n\nRyuhei Harada and Akio Kitao. Exploring the folding free energy landscape of a β-hairpin miniprotein, chignolin, using multiscale free energy landscape calculation method. The Journal of Physical Chemistry B, 115(27):8806–8812, 2011.\n\nAtsushi Suenaga, Tetsu Narumi, Noriyuki Futatsugi, Ryoko Yanai, Yousuke Ohno, Noriaki Okimoto, and Makoto Taiji. Folding dynamics of 10-residue β-hairpin peptide chignolin. Chemistry–An Asian Journal, 2(5):591–598, 2007.\n\nSøren Enemark, Nicholas A Kurniawan, and Raj Rajagopalan. β-Hairpin forms by rolling up from C-terminal: Topological guidance of early folding dynamics. Scientific Reports, 2(1):1–6, 2012.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nKresten Lindorff-Larsen, Stefano Piana, Ron O Dror, and David E Shaw. How fast-folding proteins\n\nfold. Science, 334(6055):517–520, 2011.\n\nJi Yin, Zhiyuan Zhang, Evangelos Theodorou, and Panagiotis Tsiotras. Improving model predictive\n\npath integral using covariance steering. arXiv preprint arXiv:2109.12147, 2021.\n\nAnthony Hotz and Robert E Skelton. Covariance control theory. International Journal of Control, 46\n\n(1):13–32, 1987.\n\nAvishek Das, Dominic C Rose, Juan P Garrahan, and David T Limmer. Reinforcement learning of\n\nrare diffusive dynamics. The Journal of Chemical Physics, 155(13):134105, 2021.\n\nFrancisco Vargas, Andrius Ovsianas, David Fernandes, Mark Girolami, Neil Lawrence, and Nikolas Nüsken. Bayesian Learning via Neural Schrö odinger-Föllmer Flows. arXiv preprint arXiv:2111.10510, 2021b.\n\nQinsheng Zhang and Yongxin Chen. Path Integral Sampler: a stochastic control approach for\n\nsampling. arXiv preprint arXiv:2111.15141, 2021.\n\nDavid A Sivak, John D Chodera, and Gavin E Crooks. Using nonequilibrium fluctuation theorems to understand and correct errors in equilibrium and nonequilibrium simulations of discrete Langevin dynamics. Physical Review X, 3(1):011007, 2013.\n\nKresten Lindorff-Larsen, Stefano Piana, Kim Palmo, Paul Maragakis, John L Klepeis, Ron O Dror, and David E Shaw. Improved side-chain torsion potentials for the Amber ff99SB protein force field. Proteins: Structure, Function, and Bioinformatics, 78(8):1950–1958, 2010.\n\nUlrich Essmann, Lalith Perera, Max L Berkowitz, Tom Darden, Hsing Lee, and Lee G Pedersen. A smooth particle mesh Ewald method. The Journal of chemical physics, 103(19):8577–8593, 1995.\n\nFerry Hooft, Alberto Pérez de Alba Ortíz, and Bernd Ensing. Discovering collective variables of molecular transitions via genetic algorithms and neural networks. Journal of chemical theory and computation, 17(4):2294–2306, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA ALGORITHMS\n\nA.1 LEARNING\n\nAlgorithm 1: Training Policy uθ Input: r0, rT : Initial and target molecular positions,\n\nU (·): Potential Energy function, φ(·): Terminal cost, uθ(·, ·): Initial parameterized policy, N : Number of trajectories sampled per update, τ : Time horizon, ν: Variance of Brownian noise, R: Control cost matrix, μ: Learning rate, dt: Time discretization step\n\nwhile not converged do\n\n▷ Generate trajectories with current policy uθ λ ← Rν ; n ← 0 ; while n < N do\n\n▷ Initialize initial trajectory state (rn,0, vn,0, t) ← (r0, 0, 0); while t < (τ / dt) do\n\n▷ Sample Brownian noise and action εn,t ∼ N (0, ν); un,t ← uθ(rn,t, t); ▷ Update positions and velocity rn,t+1 ← rn,t + vn,t · dt; vn,t+1 ← vn,t − (cid:0)∇rn,t U (rn,t) + un,t + εn,t t ← t + 1;\n\n(cid:1) · dt;\n\nend ▷ Determine trajectory cost and gradient λ (φ(rn,τ ) + (cid:80)τ Cn ← 1 ∆θn ← exp(−Cn) + (cid:80)τ n ← n + 1 ;\n\nn,iRun,i + uT ∂un,i ∂θ ;\n\ni=0 uT\n\ni=0\n\nn,iRεn,i);\n\nend ▷ Determine gradient normalization and perform policy update η ← (cid:80)N θ ← θ + μ\n\ni=0 exp(−Ci); i=0 ∆θi;\n\n(cid:80)N\n\nη\n\nend\n\nA.2 SAMPLING\n\nAlgorithm 2: Sampling using parameterized control uθ Input: r0: Initial molecular positions,\n\nU (·): Potential Energy function, uθ(·, ·): Trained parameterized policy, τ : Time horizon, dt: Time discretization step\n\n▷ Initialize initial trajectory state (rt, vt, t) ← (r0, 0, 0); while t < (τ / dt) do\n\n▷ Determine action ut ← uθ(rt, t); ▷ Update positions and velocity rt+1 ← rt + vt · dt; vt+1 ← vt − (∇rt U (rt) + ut) · dt; t ← t + 1;\n\nend\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB STOCHASTIC OPTIMAL CONTROL\n\nIn this section we expand on Section 2.1. Specifically, we expand on the derivation of the Stochastic Optimal Control (SOC) objective in terms of a KL-divergence (appendix B.1) and the derivation of the iterative gradient descent procedure (appendix B.2). Note that the derivations presented here are a rephrasing of those given in (Kappen and Ruiz, 2016) using notation similar to the remainder of the paper. One difference to prior work can however be found in the relation between the distribution over uncontrolled and controlled dynamics.\n\nLet us start by restating the objective of Path Integral Stochastic Optimal Control. Given a control u and the Brownian motion εt with variance ν, Equation (4) defines a trajectory x(τ ) = (x0, . . . , xτ ) ∈ Rr×d. We can define the cost for said trajectory as\n\nC(x(τ ), u, εt) =\n\n(cid:16)\n\n1 λ\n\nφ(xτ ) +\n\nτ (cid:88)\n\nt=0\n\n1 2\n\nu(xt, t)T Ru(xt, t) + u(xt, t)T Rεt\n\n(cid:17)\n\n(14)\n\nwhere φ denotes the terminal cost, λ is a constant and R defines a weighted control cost.\n\nThis is a restatement of Equation (5), included here to make future reference easier. We note a number of important observations.\n\n1. Following eq. (4), we observe that the control u acts linearly on the dynamics of the system.\n\n2. The cost of the control itself is quadratic, weighted by the matrix R. 3. Under expectation the final term vanished; Eε[u(xt, t)T Rεt] = 0\n\nThe first two observations are what classify the current control problem in the family of Path Integral Stochastic Optimal Control (Kappen, 2005) and are a requirement to be able to derive the explicit expression for the optimal control policy given in eq. (10). The third observation, while unusual in the context of SOC, is needed to rewrite the SOC objective in terms of the KL-divergence as we will see next. Additionally, if we restate the SOC-objective\n\nu∗ = arg min\n\nu\n\nEτ,εt [C(x(τ ), u, εt)]\n\n(15)\n\nwe observe that observation 3 shows that the additional cost does not change the optimal control u∗.\n\nLastly, we note that the family of Path Integral Stochastic Optimal Control problems assumes that λ = Rν. This assumption is needed both for rewriting the SOC objective as a KL-divergence and to find an explicit expression for the solution.\n\nB.1 SOC OBJECTIVE AS A KL-DIVERGENCE\n\nAs noted in the main body of the paper, an adjustment needs to be made to Equation (7) due to the incorporation of the second-order dynamics. For this purpose we restate eq. (13) as\n\nxt+1 =\n\n(cid:19)\n\n(cid:18)rt+1 vt+1\n\n(cid:19)\n\n(cid:18)\n\n+\n\n(cid:18)rt vt\n\n=\n\n(cid:124)\n\nvt −∇rtU (rt) (cid:123)(cid:122) f (xt,t)\n\n(cid:19)\n\ndt +\n\n(cid:125)\n\nu(xt, t) dt + dεt\n\n(cid:17)\n\n,\n\n(16)\n\n(cid:16)\n\n·\n\n(cid:19)\n\n(cid:18)03n I3n (cid:124) (cid:123)(cid:122) (cid:125) G(xt,t)\n\nwith t ∈ [0, τ ]. We observe here that given rt and vt , rt+1 and vt+1 are conditionally independent. As such we can derive a factorized probability distribution πu\n\n(cid:0)x(τ )(cid:1) over trajectories x(τ ) as\n\nwith\n\nπu\n\n(cid:0)x(τ )(cid:1) = πr\n\nu\n\n(cid:0)x(τ )(cid:1) · πv\n\nu\n\n(cid:0)x(τ )(cid:1)\n\n(cid:0)x(τ )(cid:1) =\n\nπr\n\nu\n\n(cid:0)x(τ )(cid:1) =\n\nπv\n\nu\n\nτ (cid:89)\n\nt=0 τ\n(cid:89)\n\nt=0\n\n1[rt+1=rt+vt](rt+1)\n\nN (vt+1|μt, Σt).\n\n14\n\n(17)\n\n(18)\n\n(19)\n\nUnder review as a conference paper at ICLR 2023\n\nHere, the normal distribution describing the transition probabilities for the velocity component is similar to eq. (4) with μs = vs + f v(xs, s) dt + Gv(xs, s)(u(xs, s) dt) and Σs = Gv(xs, s)T νGv(xs, s). With f v and Gv we denote the components of f and G acting on the velocity, respectively −∇rtU (rt) and I3n.\n\nSimilarly, Equation (3) defines a probability distribution π0 (cid:0)x(τ )(cid:1) · πv\n\n(cid:0)x(τ )(cid:1) = πr\n\nπ0\n\n0\n\n0\n\n(cid:0)x(τ )(cid:1), where now u = 0: (cid:0)x(τ )(cid:1)\n\nwith πr u\n\n(cid:0)x(τ )(cid:1) = πr\n\n0\n\n(cid:0)x(τ )(cid:1) and\n\n(cid:0)x(τ )(cid:1) =\n\nπv\n\n0\n\nτ (cid:89)\n\nt=0\n\nN (xt+1|ˆμt, ˆΣt)\n\nwith ˆμs = vs + f v(xs, s) dt and ˆΣs = Σs.\n\n(20)\n\n(21)\n\nBecause we are only interested in the relation between πu as in (Kappen and Ruiz, 2016) applies with πr u\nGirsanov (Cameron and Martin, 1944), we get:\n\n(cid:0)x(τ )(cid:1) and πr\n\n0\n\n(cid:0)x(τ )(cid:1) and π0\n\n(cid:0)x(τ )(cid:1), the same analysis (cid:0)x(τ )(cid:1) cancelling out. Following\n\nπu(x(τ )) = π0(x(τ )) exp\n\n= π0(x(τ )) exp\n\n= π0(x(τ )) exp\n\n= π0(x(τ )) exp\n\n= π0(x(τ )) exp\n\n(cid:16) τ\n\n(cid:88)\n\nt=0\n\n(cid:16) τ\n\n(cid:88)\n\nt=0\n\n(cid:16) τ\n\n(cid:88)\n\nt=0\n\n(cid:16) τ\n\n(cid:88)\n\nt=0\n\n(cid:16) τ\n\n(cid:88)\n\nt=0\n\n−\n\n−\n\n1 2\n\n1 2\n\nuT\n\nt GT t Gtut Σt\n\nuT\n\nt GT t Gtut Σt\n\n+\n\n+\n\nGtut(f t + vt − vt−1) Σt\n\n(cid:17)\n\nGtut(Gt(ut + εt)) Σt\n\n(cid:17)\n\nuT\n\nt GT t Gtut Σt\n\n+\n\nuT\n\nt GT t Gtεt Σt\n\n(cid:17)\n\nuT\n\nt GT\n\nt Σ−1\n\nt Gtut + uT\n\nt GT\n\nt Σ−1\n\nt Gtεt\n\n(cid:17)\n\nuT\n\nt ν−1ut + uT\n\nt ν−1εt\n\n(cid:17)\n\n1 2\n\n1 2\n\n1 2\n\n= π0(x(τ )) exp\n\n(cid:16) 1 λ\n\nτ (cid:88)\n\nt=0\n\n1 2\n\nuT\n\nt Rut + uT\n\nt Rεt\n\n(cid:17) ,\n\n(22)\n\nwhere we use the assumption λ = Rν in the last step. We use shorthand notation to simplify ut = u(xt, t), Gt = Gv(xt, t), and f t = f v(xt, t). From here we can obtain the relation in Equation (8).\n\nNow, as again show in (Kappen and Ruiz, 2016), we can use this relation to rewrite the cost in eq. (14) as\n\nC(x(τ ), u, εt) =\n\n1 λ\n\nφ(xτ ) + log\n\n(cid:0)x(τ )(cid:1) (cid:0)x(τ )(cid:1) ,\n\nπu π0\n\nand thus, the distribution over trajectories under optimal control u∗ can now be defined as Ex(τ )∼πu[C(x(τ ), u, εt)]\n\nπu∗ = arg min\n\nπu\n\n= arg min\n\nπu\n\nEx(τ )∼πu\n\n= arg min\n\nπu\n\nEx(τ )∼πu\n\n= arg min\n\nπu\n\nEx(τ )∼πu\n\n(cid:104) 1 λ\n\n(cid:104) 1 λ\n(cid:2) 1 λ\n\nφ(xτ ) + log\n\n(cid:0)x(τ )(cid:1) (cid:0)x(τ )(cid:1)\n\nπu π0\n\n(cid:105)\n\n(cid:105)\n\nφ(xτ )\n\n+ Ex(τ )∼πu\n\nφ(xτ )(cid:3) + DKL\n\n(cid:16)\n\nπu\n\n(cid:104)\n\nlog\n\nπu π0 (cid:0)x(τ )(cid:1)∥π0\n\n(cid:105)\n\n(cid:0)x(τ )(cid:1) (cid:0)x(τ )(cid:1) (cid:0)x(τ )(cid:1)(cid:17)\n\n(23)\n\n(24)\n\nThis objective is an approximation of the Schrodinger Bridge formulation in Equation (2) where the constraints on the marginal distributions are replaced by a regularization term in the form of the terminal cost. When the expected terminal cost dominates the KL-divergence the found distribution should be similar.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nB.2\n\nITERATIVE GRADIENT DESCENT\n\nAs mentioned earlier, the specific control problem we are considering here (linear acting control and weighted quadratic control cost) is known as Path Integral Stochastic Optimal Control. Work on this control problem has established that under the additional assumption that λ = Rν there exists an explicit solution describing the optimal control u∗. While there are a number of different papers establishing this result (Kappen, 2005; Theodorou et al., 2010; Kappen, 2007), we note that (Kappen and Ruiz, 2016) is most in line with our work. As such, we refer the interested reader to this work to find the proof for the following statement that defines the distribution over optimal trajectories as a reweighing of the distributions over trajectories under no control:\n\nπu∗ =\n\n1 η\n\n(cid:0)x(τ )(cid:1) exp(−\n\nπ0\n\n1 λ\n\nφ(xτ )),\n\n(25)\n\nwhere η = Ex(τ )∼π0[exp(− 1 λ φ(xτ )] is the normalization constant. Given the previously established relation (eq. (8)) between π0 and πu, we can equivalently express the optimal control u∗ in terms of any control u using importance sampling\n\nπu∗ =\n\n=\n\n=\n\n=\n\n1 η\n\n1 η\n\n1 η\n\n1 η\n\n(cid:0)x(τ )(cid:1) (cid:0)x(τ )(cid:1) πu\n\nπ0 πu\n\n(cid:0)x(τ )(cid:1) exp(−\n\n1 λ\n\nφ(xτ ))\n\n1\n\nexp\n\n(cid:16) 1\n\nλ\n\n(cid:80)τ\n\nt=0\n\n1\n\n2 uT\n\nt Rut + uT\n\nt Rεt\n\n(cid:0)x(τ )(cid:1) exp(−\n\n(cid:17) πu\n\n1 λ\n\nφ(xτ ))\n\n(cid:0)x(τ )(cid:1) exp(−\n\nπu\n\n1 λ\n\nφ(xτ ) −\n\n1 λ\n\nτ (cid:88)\n\nt=0\n\n1 2\n\n(cid:0)x(τ )(cid:1) exp(−C(x(τ ), u, εt))\n\nπu\n\nuT\n\nt Rut − uT\n\nt Rεt)\n\n(26)\n\nWith an explicit expression for the optimal control policy given, the PICE method aims to find a parameterized distribution πuθ∗ that is close to the optimal control in terms of KL-divergence (cid:16)\n\nπu∗\n\nθ\n\n= arg min\n\nDKL\n\nπuθ\n\nπu∗\n\n(cid:0)x(τ )(cid:1)∥πuθ\n\n(cid:0)x(τ )(cid:1)(cid:17)\n\n.\n\n(27)\n\nUsing the explicit expression for the optimal control, the KL-divergence is given as follows:\n\n(cid:16)\n\nDKL\n\nπu∗\n\n(cid:0)x(τ )(cid:1)∥πuθ ∝ −Eπu∗\n\n(cid:0)x(τ )(cid:1)(cid:17) (cid:2) log πuθ (cid:104)\n\n(cid:3)\n\n= −Eπu∗\n\nlog π0(x(τ )) exp\n\n(cid:16) τ\n\n(cid:88)\n\nt=0\n\n−\n\n1 2\n\nuθ(t)T GT t Gtuθ(t) Σt\n\n+\n\nGtuθ(t)(ft + xt − xt−1) Σt\n\n(cid:17)(cid:105)\n\n(cid:104) τ\n\n(cid:88)\n\n∝ −Eπu∗\n\n= Eπu∗\n\nt=0 τ\n(cid:88)\n\nt=0\n\n(cid:104) 1 λ\n\n−\n\n1 2\n\nuθ(t)T GT t Gtuθ(t) Σt\n\n+\n\nGtuθ(t)(Gt(u∗(t) + εt)) Σt\n\n(cid:105)\n\nuθ(t)T Ruθ(t) − uθ(t)T Ru∗(t) − uθ(t)T Rεt\n\n(cid:105)\n\n1 2\n\n=\n\n1 η\n\nEπu\n\n(cid:104)\n\ne−C(x(τ ),u,εt) 1\n\nλ\n\nτ (cid:88)\n\nt=0\n\n1 2\n\nuθ(t)T Ruθ(t) − uθ(t)T Ru(t) − uθ(t)T Rεt\n\n(cid:105)\n\n(28)\n\nWe use shorthand notation to simplify u(t) = u(xt, t), Gt = G(xt, t), and ft = f (xt, t). Line 1 (cid:3). Line 2 we make use of the established relation between we discard the constant term Eπu∗ (cid:3) and note that the πu and π0 for any control u. Line 3 we discard the constant term Eπu∗ expectation is over trajectories sampled from πu∗ . Line 4 we rewrite using the assumption that λ = Rν. Line 5 we use Equation (26) to rewrite the distribution over an arbitrary control u using.\n\n(cid:2) log πu∗\n\n(cid:2) log π0\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nWe can minimize this explicit expression using Gradient Descent, to do this, we derive the gradient of the KL-divergence\n\n∂DKL\n\n(cid:16)\n\nπu∗\n\n(cid:0)x(τ )(cid:1)(cid:17)\n\n(cid:0)x(τ )(cid:1)∥πuθ ∂θ (cid:104)\n\n=\n\nEπu\n\n1 η\n\ne−C(x(τ ),u,εt) 1\n\nτ (cid:88)\n\n(Ruθ(t) − Ru(t) − Rεt)\n\nt=0\n\n∂uθ(t) ∂θ\n\n(cid:105)\n\n.\n\n(29)\n\nλ\n\nFinally, we note that the expectation is over trajectories of any distribution πu, and as such, this distribution can also be chosen to be equal to the parameterized distribution πu = πuθ . This gives us the final gradient\n\n∂DKL\n\n(cid:16)\n\nπu∗\n\n(cid:0)x(τ )(cid:1)(cid:17)\n\n(cid:0)x(τ )(cid:1)∥πuθ ∂θ\n\n=\n\n1 η\n\nEπuθ\n\n(cid:104)\n\ne−C(x(τ ),u,εt) 1\n\nλ\n\nτ (cid:88)\n\n(Ruθ(t) − Ruθ(t) − Rεt)\n\nt=0\n\n(cid:105)\n\n∂uθ(t) ∂θ\n\n= −\n\n1 η\n\nEπuθ\n\n(cid:104)\n\ne−C(x(τ ),uθ,εt) 1\n\nλ\n\nτ (cid:88)\n\nt=0\n\nRεt\n\n∂uθ(t) ∂θ\n\n(cid:105)\n\n.\n\n(30)\n\n(31)\n\nC EXTENSION EXPERIMENTAL SECTION\n\nC.1 OPENMM\n\nGeneral setup: We use the Velocity Verlet with Velocity Randomization (VVVR) integrator (Sivak et al., 2013) within OpenMM at a temperature of 300 K with a collision rate of 1.0 ps-1.\n\nAlanine Dipeptide: We use the amber 99sb-ildn force field (Lindorff-Larsen et al., 2010) without any solvent, a time-step of 1.0 fs for the VVVR integrator and a cutoff of 1 nm for the Particle Mesh Ewald (PME) method (Essmann et al., 1995).\n\nPolyproline Helix: We initialize OpenMM with the amber protein.ff14SBonlysc forcefield and gbn2 as the implicit solvent forcefield. The VVVR integrator had a timestep of 2.0 fs and a cutoff of 5 nm for PME. The proposed method was ran for a total of 10.000 fs (resulting in 5,000 policy steps).\n\nChignolin: To sample transition paths between the folded and unfolded state of the Chignolin protein, we initialize OpenMM using the same forcefield and VVVR integrator as for Polyproline with the exception that we sample a new force from our policy network every 1.0 fs. We do this 5000 times for each rollout for a total time horizon of 5000 fs.\n\nC.2 ALANINE DIPEPTIDE\n\nC.2.1 DISCUSSION BASELINES AND EVALUATION METRICS\n\nMetrics Three different metrics are used for the comparison covering multiple desiderata for the sampled transition trajectories. For each metric we report the score over 1000 trajectories with the exception of the Molecular Dynamics without fixed timescale baseline which is only ran until 10 trajectories are successfully generated.\n\nExpected Pairwise Distance (EPD) The EPD measures the similarity between the final conformation in the trajectory and the target conformation taking into account the full 3D geometry of the molecule. Note that the expected pairwise distance for uncontrolled MD with the target as the starting conformation has a EPD of 2.25 × 10−3. All trajectories with an EPD of less than this can thus be considered to transition the molecule within one standard deviation of the target distribution.\n\nTarget Hit Percentage (THP): The second metric under which we evaluate the proposed Transition Path Sampler measures the similarity of the final and target conformation in terms of the collective\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Visualization of a trajectory sampled with the proposed force prediction method. Left: The sampled trajectory projected on the free energy landscape of Alanine Dipeptide as a function of two CVs Right: Conformations along the sampled trajectory: A) starting conformation showing the CV dihedral angles, B-D) intermediate conformations with D being the highest energy point on the trajectory, and E) final conformation, which closely aligns with the target conformation. Bottom: Potential energy during transition. Letters represent the same configurations in the transition.\n\nvariables. The THP measures the percentage of generated trajectories/paths that reach the target state. As such, higher hit percentages are preferred. We determine a trajectory to have hit the target in CV space when φ and ψ are both within 0.75 of the target.\n\nEnergy Transition Point (ETP): The final metric looks at the potential energy of the transition point—the conformation in the trajectory with the highest potential energy. This directly evaluates the capability of the method to find the transition path that crosses the boundary at the lowest saddle point.\n\nBaselines We compare the proposed Transition Path Sampling method with extended Molecular Dynamics simulation using different time-scales and temperature points. As discussed earlier, there are currently no other methods available for Transition Path Sampling using the full 3D geometry of the molecules.\n\nMolecular Dynamics with fixed timescale: This set of baselines is limited to the same timescale as the proposed Transition Path Sampler, 500 femtoseconds, but uses varying temperatures. With higher temperatures we should have a higher probability of crossing the barrier and hitting the target configuration.\n\nMolecule Dynamics without fixed timescales: In contrast to the other set of baselines, the MD simulation for this set is not limited to 500 femtoseconds, but is instead ran until the target conformation is reached. We consider a trajectory to have reached its target if the following two conditions have been met: 1) the current conformation classifies as having hit the target under the conditions of the metric described above and 2) the current conformation is within one standard deviation of the target distributions mean.\n\nBy running the MD simulations until the target is reached we aim to gain intuition into the speed-up that it achieved by the fixed timescale of the proposed Transition Path Sampler.\n\nC.2.2 ADDITIONAL RESULTS: VISUALIZATION FORCE PREDICTION\n\nWe observe that the force predicting policy has learned a different trajectory then the energy predicting model presented in the main body of the paper. While different, both of the trajectories pass the high energy barrier in a locally low point. Previous work on finding transition path has also observed that multiple viable paths can be found for Alanine Dipeptide (Hooft et al., 2021).\n\n18\n\nABDECA:B:C:D:E:φΨABCDE",
    "reference": "# Summary Of The Paper\n\nThis work considers the problem of sampling transition paths between two metastable states of a molecular system. This problem is difficult as the energy barrier between the states may be large, making it computationally expensive for traditional MD simulation. This work first relates this problem to literature on optimal control and Schrodinger bridges then provides a parameterization in terms of second-order dynamics to learn likely transition paths. This method is demonstrated on three systems using two slightly different parameterizations, either directly modelling the control force, or indirectly modelling the force as $\\nabla E$ and directly parameterizing energy.\n\n# Strength And Weaknesses\n\nStrengths:\n\n- Interesting application of optimal control\n- Neat parameterization in terms of second-order dynamics that allows easy integration with OpenMM.\n- Well explained experiments which are understandable by non MD experts\n\nWeaknesses:\n\n- Limited theoretical novelty as the theory is established in prior work.\n- Limited quantitative experiments on a single (simple) molecule. As a non-expert in MD I can’t evaluate how often good CVs are available and how much they help in this path sampling problem. Do the authors think an experiment with MD using knowledge of the CVs be useful? It would help me understand how this baseline method works. Presumably when the CVs are less accurate (such as in chignolin?) CV-based optimizations would not work as well as PIPS? Could this be confirmed experimentally?\n- Another concern here is overfitting. How are we sure that the parameters are not simply overfit to each molecule making it difficult to apply to new molecule simulations? It would be helpful to state how hyperparameters were tuned and on what data.\n\nComments:\n\nI don’t think I understand remark 3. Is this a claim that equations (2) and (9) are the first to establish the equivalence between sampling transition paths and the SB problem which hasn’t been previously established? I don’t understand how these equations are meant to establish this and would be very surprised if this is the first time this equivalence has been established. \n\nThe notation in the “Physics inspired policy network” is a bit confusing to me with bold $u_\\theta$ and regular $u_\\theta$ having very different (and unspecified) dimensions. Instead of defining F(r_t) could we say instead that bold $u_\\theta$ is either directly parameterized (force prediction) or parameterized as $\\nabla_{r_t} E_\\theta$ (energy prediction)?\n\nI understand the current evaluation is on the quality of the paths. I would also be curious as to the quality in terms of the optimization, i.e. what is the value of $E_{\\tau, \\epsilon_t} C(x(\\tau), u, \\epsilon_t)$ for the force vs. energy models? \n\n$u_\\theta$ with some invariances is mentioned, but the end network is an MLP. Was a more sophisticated network tried? Perhaps it is not helpful for these single-molecule studies.\n\nQuestion: How does Temperature come in to play? Its slightly odd to me that MD without a fixed timescale is evaluated on a different temperature than PIPS. Could PIPS be evaluated on the same temperatures or could the authors explain why 300K is “better”?\n\nMinor remarks:\n\nRemark 3? is this a novelty claim?\n\nmisplaced comma last paragraph of page 5.\n\nCourtsey —> Curtesy Remark 4\n\nIt would be useful to mention Table 1 is on Alanine Dipeptide in the caption. Also it slightly strange that it comes before figure 1, but is referenced afterwards.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI found the manuscript quite easy to read and the experiments clearly described. In terms of novelty, this seems like a slight modification to an existing method for application in an interesting domain. As such I was slightly surprised by the relatively small amount of empirical (quantitative) evaluation. \n\nThis paper could also use some more specificity for additional reproducibility. For example it is stated that “The width of the layers of the policy network is dependent on the number of atoms in the molecule under consideration.” What were these widths and how were they determined? Code (which is not available for review as far as I’m aware) could potentially help here.\n\n# Summary Of The Review\n\nI found this paper interesting and timely with the increased interest in diffusion-based models. For me I would like to see more experimental validation and rigor in terms of quantitative comparisons and experimental setup.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nGRACE-C: Generalized Rate Agnostic Causal Estimation via Constraints\n\nMohammadsajad Abavisani∗ Department of Electrical and Computer Engineering Georgia Institute of Technology Atlanta, GA 30332 s.abavisani@gatech.edu\n\nDavid Danks Halicioglu Data Science Institute and Department of Philosophy University of California San Diego San Diego, CA 92093 ddanks@ucsd.edu\n\nSergey M. Plis TReNDS center Department of Computer Science Georgia State University Atlanta, GA 30302 s.m.plis@gmail.com\n\nAbstract\n\nGraphical structures estimated by causal learning algorithms from time series data can provide misleading causal information if the causal timescale of the generating process fails to match the measurement timescale of the data. Existing algorithms provide limited resources to respond to this challenge, and so researchers must either use models that they know are likely misleading, or else forego causal learning entirely. Existing methods face up-to-four distinct shortfalls, as they might a) require that the difference between causal and measurement timescales is known; b) only handle very small number of random variables when the timescale difference is unknown; c) only apply to pairs of variables; or d) be unable to find a solution given statistical noise in the data. This paper addresses these challenges. Our approach combines constraint programming with both theoretical insights into the problem structure and prior information about admissible causal interactions to achieve multiple orders of magnitude in speed-up. The resulting system maintains theoretical guarantees while scaling to significantly larger sets of random variables (> 100) without knowledge of timescale differences. This method is also robust to edge misidentification and can use parametric connection strengths, while optionally finding the optimal solution among many possible ones.\n\n1\n\nIntroduction\n\nDynamic causal models play a pivotal role in modeling real-world systems in diverse domains, including economics, education, climatology, and neuroscience. Given a sufficiently accurate causal graph over random variables, one can predict, explain, and potentially control some system; more generally, one can understand it. In practice, however, specifying or learning an accurate causal model of a dynamical system can be challenging for both statistical and theoretical reasons.\n\nOne particular challenge arises when data are not measured at the speed of the underlying causal connections. For example, fMRI scanning of the brain indirectly measures dynamical neural activity by measuring the resulting bloodflow and oxygen level changes in different brain regions. However, fMRI measures occur (at most) every second while the brain’s actual dynamics are known to proceed at a faster rate (Oram & Perrett, 1992), though we do not know how much faster. In general, when the measurement timescale is significantly slower than the causal timescale (as with fMRI), learning can output vastly incorrect causal information. For instance, if we only measure every other timestep\n\n∗Corresponding author\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nin Figure 1, then the true graph (top left) would differ from the data graph (top right). We might thus conclude that variable 2 directly influences variable 5, when variable 3 is the actual direct cause. These errors can lead to inefficient or costly attempts at control. More generally, understanding of a system depends on the timescale of the causal relations, not the timescale of measurements.\n\nIn this paper, we consider the problem of learning the causal structure at the causal timescale from data collected at an unknown measurement timescale. This challenge has received significant attention in recent years (Plis et al., 2015b; Gong et al., 2015; Hyttinen et al., 2017; Plis et al., 2015a), but all current algorithms have significant limitations (see Section 2) that make them unusable for many real-world scientific challenges. Current algorithms show the theoretical possibility of causal learning from undersampled data, but their practical applicability is limited to small graph sizes, perhaps only a pair of variables (Gong et al., 2015). In contrast, we present a provably correct and complete solution that can operate on 100-node graphs, and hence is potentially applicable in biological and other domains, for learning causal timescale structure from undersampled data.\n\n2 Related Work And Notation\n\nA directed dynamic causal model is a generalization of “regular” causal models (Pearl et al., 2000; Spirtes et al., 1993): graph G includes n distinct nodes for random variables V = {V1, V2, ..., Vn} at both the current timestep t (Vt), and also previous timesteps (Vt−k) for which there is a direct cause of some V t i . We assume that the “true” underlying causal structure is first-order Markov: the independence Vt ⊥⊥ Vt−k | Vt−1 holds for all k > 1.1 G is thus over 2V, and the only permissible edges are V t−1 j, where possibly i = j. The quantitative component of the dynamic causal model is fully specified by parameters for P(Vt|Vt−1). We assume that these conditional probabilities are stationary over time, but the marginal P(Vt) need not be stationary.\n\ni → V t\n\nWe denote the timepoints of the underlying causal structure as Figure 1: Causal graph G1 and {t0, t1, t2, ..., tk, ...}. The data are said to be undersampled at rate its undersampled version G2: unu if measurements occur at {t0, tu, t2u, ..., tku, ...}. We denote underrolled and compressed versions. sample rate with superscripts: the true causal graph (i.e., undersampled at rate 1) is G1; the same graph undersampled at rate u is Gu. To determine the implied G for u > 1, the graph is first “unrolled” by adding instantiations of G1 at previous timesteps, where Vt−2 bear the same causal relationships to Vt−1 that Vt−1 bear to Vt, and so forth. In this unrolled (time-indexed by t) graph, all V at intermediate timesteps are unmeasured; this lack of measurement is equivalent to marginalizing out (the variables in) those timesteps to yield Gu. The problem of moving from G1 to Gu was structurally addressed by Danks & Plis (2013), and parametrically addressed (for 2-variable systems) by Gong et al. (2015).\n\nVarious representations have been developed for graphs with latent confounders, including partiallyobserved ancestral graphs (PAGs) (Zhang, 2008) and maximal ancestral graphs (MAGs) (Richardson & Spirtes, 2002). However, these graph-types cannot easily capture the types of latents produced by undersampling (Mooij & Claassen, 2020). Instead, we use compressed graphs, along with properties that were previously proven for this representation (Danks & Plis, 2013). A compressed graph includes only V, where temporal information is implicitly encoded in the edges. In particular, a compressed graph G for dynamic causal graph G has Vi → V j in G iff V t−1 j is in G. Undersampling (i.e., marginalizing intermediate timesteps) is a straightforward operation for compressed graphs: (1) Vi → V j in Gu iff there is a length-u directed path from Vi to V j in G1 iff there is a directed j in G1; and (2) Vi ↔ V j in Gu iff there exists length-s < u directed paths from path from V t−u Vk to Vi, and to V j, in G1 (i.e., Vk is an unobserved common cause in G1 fewer than u timesteps back). (See Appendix A for additional lemmas and proofs.) The bottom row of Figure 1 shows compressed graphs for the unrolled ones on the top row; the left shows the causal timescale and the right shows the graphs undersampled at rate 2. (See Appendix B for more examples of graphs through undersampling.)\n\ni → V t\n\nto V t\n\ni\n\n1This assumption is relatively weak, as we do not assume that we measure at this causal timescale. The causal timescale could be arbitrarily fast. This assumption is a form of causal sufficiency (Spirtes et al., 2000).\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nGiven this framework, the overall causal learning challenge can now be stated as: given Gu but not u (alternately: given dataset D at unknown undersample rate), what is the set of possible G1? There will often be many possible G1 that appear the same after undersampling, and so we use ⟦H⟧ to denote the equivalence class of G1 that yield H (the given causal graph inferred from data D) for some u. That is, ⟦H⟧ = {G1 : ∃u(Gu = H)}. There are 2n2 possible G1, so perhaps unsurprisingly, the problem of inferring ⟦H⟧ is NP-complete:\n\nTheorem 1 (Hyttinen et al. (2017)[Theorem 1]). Deciding whether a consistent G1 exists for a given H is NP-complete, for all undersampling rates u ≥ 2.2\n\nSeveral algorithms exist for this problem. Mesochronal Structure Learning (MSL) (Plis et al., 2015b) infers ⟦H⟧ in a non-brute force manner given known u. Every edge in Gu corresponds to one or more paths of length u in G1, and so G1 can be constructed by identifying u − 1 intermediate nodes for each edge in Gu. MSL uses Depth-First Search (DFS) through the state space of possible identifications, where each implies a G1. If Gu = H, then G1 ∈ ⟦H⟧. Otherwise, search continues. MSL backtracks in the DFS whenever some Gu includes an edge that is absent from H, as the candidate G1 and all its supergraphs cannot be in ⟦H⟧.\n\nFigure 2: Comparison of sRASL (red) with previous state-of-the-art RASL (blue).\n\nAlthough Plis et al. (2015b) showed that the concept of causal inference from undersampled data is feasible, MSL is computationally intractable on even moderate-sized graphs. Hyttinen et al. (2017) used the implied constraints to develop an Answer Set Programming (ASP) (Simons et al., 2002; Niemelä, 1999; Gelfond & Lifschitz; Lifschitz, 1988) method that formulated this causal inference challenge as a rule-based constraint satisfaction problem that is well-suited for ASP-type solvers. In essence, the algorithm in Hyttinen et al. (2017) takes as input the measured causal graph H, determines the set of implied constraints on G1, and then uses the general-purpose Answer Set Solver Clingo (Gebser et al., 2011) to determine the set of possible G1 significantly faster than MSL. The same idea of using Boolean satisfiability solvers to integrate (in)dependent data constraints has been used for various other causal learning challenges (Hyttinen et al., 2013; Triantafillou et al., 2010).\n\nAlthough the method in Hyttinen et al. (2017) is significantly faster than MSL, one must specify the undersampling rate u (or else run the method sequentially for all possible u, thereby losing much of the computational advantage). In contrast, the Rate-Agnostic (Causal) Structure Learning (RASL) approach (with several variants) (Plis et al., 2015a) makes no such assumption. RASL algorithms are similar to MSL, but consider each possible u for some G1. RASL reduces computational complexity with two additional stopping rules for given G1: (1) if some Gk has previously been seen, then further undersampling of G1 will not produce new graphs; and (2) if Gk is not an edge-subset of H for all k, then do not consider any edge-superset of G1 (Plis et al., 2015a). Despite these improvements, RASL still faces memory and run-time constraints for even moderate numbers of nodes.\n\nOne key observation from all of these learning algorithms is the importance of strongly connected components (SCCs) (Danks & Plis, 2013), where the variables in a compressed graph H can be fully partitioned based on SCC membership.\n\nDefinition 2.1. An SCC in compressed graph H is a maximal set of nodes S ⊆ V such that, for every X, Y ∈ S there is a directed path from X to Y .\n\nSCCs can be highly stable despite undersampling: the node-membership of an SCC does not change as we undersample, as long as the greatest common divisor (gcd) of the set of lengths of all simple loops (directed cycles without repeated nodes) in the SCC is 1.3\n\n2Proof provided in Hyttinen et al. (2017). In general, we omit previously published proofs. 3The condition easily holds, as it requires only (1) the graph is relatively dense with different loop lengths, or\n\n(2) at least one node in the SCC has a self-loop (i.e., is autocorrelated).\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 2 (Danks & Plis (2013)[Theorem 3]). S is an SCC in Gu ∀u iff gcd(LS) = 1 for SCC S ∈ G1\n\nThe algorithms in this paper all take as input the measurement timescale graph H, perhaps estimated from data at an (unknown) undersampling rate. We do not here develop algorithms to learn H, as there are many existing algorithms for learning graphical structure: at the measurement timescale (Chu et al., 2008; Entner & Hoyer, 2010; Granger, 1969); for time series with latent confounders (Jabbari et al., 2017; Malinsky & Spirtes, 2018; Gerhardus & Runge, 2020); or accounting for structured latents such as those that occur in undersampling (Moneta et al., 2011; Cook et al., 2017).\n\nIn this paper, we develop sRASL (for solver-based RASL), a novel solution to the rate-agnostic structure learning problem that leverages multiple types of insights and constraints (e.g., Theorem 2), and thereby significantly outperforms previous methods. The contributions of this paper are threefold: first, we reformulate the RASL algorithm from a search-based procedure to a constraint satisfaction problem encoded in a declarative language (Fahland et al., 2009). Second, we show how to add additional constraints based on SCC structure. Third, we ensure that sRASL provides a straightforward way to find approximate solutions when H is an unreachable graph (i.e., when ⟦H⟧ = ∅). These advances collectively provide up to three orders of magnitude improvements in speed, thereby enabling causal inference given undersampling data involving over 100 nodes. Figure 2 compares sRASL (red) with the previously-fastest RASL (Plis et al., 2015a) method (blue) on the same graphs. For the example H, RASL required nearly 1000 minutes to compute ⟦H⟧, but only 6 seconds for sRASL. Even the longest-to-compute ⟦H⟧ for sRASL took 20.5 seconds vs. 780 minutes for RASL.\n\n3\n\nsRASL: Optimized ASP-based Causal Discovery\n\nThe sRASL approach takes as input a (potentially) undersampled graph H, whether learned from data D, expert domain knowledge, both of these, or some other source. sRASL’s agnosticism about the source of the input graph enables wider applicability, as we can use whatever information is available (Danks & Plis, 2019). In the asymptotic (data) limit, the sRASL output is the full ⟦H⟧.\n\nsRASL leverages the fact that connections between SCCs in H must form a directed acylic graph. More specifically: if X → Y with X ∈ A, Y ∈ B for SCCs A (cid:44) B, then C ↚ D for all C ∈ A, D ∈ B.4 Theorem 2 provides the (weak) condition under which SCC membership is preserved under undersampling. These two observations imply that structural features provide additional constraints beyond the obvious ones (see Section 4.3). In particular, if H has a roughly modular structure, then sRASL generates many more constraints than the formulation of Hyttinen et al. (2017). (See Appendix D for an ablation study of speed effects of these added constraints.)\n\nListing 1 shows the Clingo (see Appendix F for a brief introduction) code5 of sRASL, which involves exactly representing the conditioning and marginalization operations (from Section 2) in ASP. Line 1 specifies the first-order graph structure of H (e.g., the edge 1 → 10 translates to hdirected(1, 10)). Line 2 encodes the second-order structure of H, including the partition of V into SCCs. Separate code adds these predicates and basic descriptive information (Lines 3, 4, 5) to the Clingo code. maxu on line 3 specifies the maximum undersampling rate; noter that there is provably a u where Gu = Gk for all k > u, under the same condition that leads to stable SCC membership: Theorem 3 (Plis et al. (2015a)[Theorem 3.1]). If gcd(LS) = 1 for all SCCs S ⊆ V, then Gu = Gu+1 for all u > f ≤ nF + γ + d + 1.\n\nwhere γ is the transit number6, d is graph diameter7, and nF is the Frobenius number.8 In practice, the plausible undersampling rate will often be much lower than the theoretical upper bound in Theorem 3, and so maxu could be set by expert knowledge. For example, the underlying rate of brain activity is generally thought to be ∼ 100 milliseconds and fMRI devices measure approximately every two seconds. Hence, u = 20 is a plausible upper bound on undersampling in fMRI studies.9\n\n4If C ← D, then by definition of SCC, there exists π : X ← . . . ← C ← D ← . . . ← Y. X, Y are thus\n\nmutually reachable, so must be in the same SCC, contra A (cid:44) B.\n\n5The full code is available at https://gitlab.com/undersampling/gunfolds 6Length of the “longest shortest path” from a node that touches all simple loops of the SCC. 7Length of the “longest shortest path” between any two graph nodes. 8For set B of positive integers with gcd(B) = 1, nF is the max integer with nF (cid:44) (cid:80)b 9Of course, the actual undersample rate could be much lower than 20. Voxels typically contain 8 − 10 layers\n\ni=1 αiBi for αi ≥ 0\n\nof neurons, so the “causal timescale of a voxel” could easily be as high as 1000 ms (i.e., u = 2).\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n1\n\n2\n\n3\n\n4\n\n%( * input graph edge specifications here * e.g.: hdirected(1,5) ... ) %( * input graph SCC specifications here * e.g.: sccsize(0, 5). scc(1, 0) ...) #const n = 10, maxu = 20 node(1..n). 1 {u(1..maxu)} 1. {edge1(X,Y)} :- node(X), node(Y). directed(X, Y, 1) :- edge1(X, Y). directed(X, Y, L) :- directed(X, Z, L-1), edge1(Z, Y), L <= U, u(U). bidirected(X, Y, U) :- directed(Z, X, L), directed(Z, Y, L), node(X;Y;Z), X < Y, L\n\n< U, u(U).\n\n:- directed(X, Y, L), not hdirected(X, Y), node(X;Y), u(L). :- bidirected(X, Y, L), not hbidirected(X, Y), node(X;Y), u(L), X < Y. :- not directed(X, Y, L), hdirected(X, Y), node(X;Y), u(L). :- not bidirected(X, Y, L), hbidirected(X, Y), node(X;Y), u(L), X < Y. % the following is only used when SCC accounting is enabled :- edge1(X, Y), scc(X, K), scc(Y, L), K != L, sccsize(L, Z), Z > 1, not dag(K,L).\n\nListing 1: sRASL encoding in the clingo ASP-language\n\n:~ directed(X, Y, L), no_hdirected(X, Y, W), node(X;Y), u(L). [W@1,X,Y] :~ bidirected(X, Y, L), no_hbidirected(X, Y, W), node(X;Y), u(L), X < Y.\n\n[W@1,X,Y]\n\n:~ not directed(X, Y, L), hdirected(X, Y, W), node(X;Y), u(L). [W@1,X,Y] :~ not bidirected(X, Y, L), hbidirected(X, Y, W), node(X;Y), u(L), X < Y.\n\n[W@1,X,Y]\n\nListing 2: Integrity constraints to replace Lines 11-14 in Listing 1 to convert sRASL into optimization problem\n\nLine 6 in Listing 1 stipulates that all edges in G1 are possible (by default), and so the output will contain any possible model that does not violate the integrity constraints of lines 11 − 15. Lines 7 and 8 define paths of length L in the graph (i.e., an edge in GL). As described in Section 2, u⇝ is a path of length u. Line 9 similarly defines bidirected X → Y ∈ Gu ⇐⇒ X edges in GL: X ↔ Y ∈ Gu ⇐⇒ ∃Z, l : (X\n\nu⇝ Y ∈ G1 where\n\nl⇝ Y ∈ G1).\n\nl(cid:102) Z\n\nLines 10 − 13 provide the core constraints that ensure sRASL returns only G1 for which there exists u with Gu = H. Line 15 adds the constraint of impermissibility of cycles between SCCs: if each SCC is considered as a super-node, Line 15 ensures that the graph of SCC connections in H is acyclic.\n\nsRASL can return the empty set (i.e., there are no suitable G1), typically because of statistical noise or other errors in estimating or specifying H.10 We can instead run sRASL in an optimization mode to find optimal (though not perfect) outputs (see Section 4.5 for details). In such cases, sRASL finds the set of G1 that are, for some u, closest to H by the objective function:\n\nG1∗, u∗ ∈ arg min\n\n(cid:88)\n\ne∈H\n\nI[e (cid:60) Gu] · w(e ∈ H) +\n\n(cid:88)\n\ne(cid:60)H\n\nI[e ∈ Gu] · w(e (cid:60) H),\n\n(1)\n\nwhere the indicator function I(c) = 1 if c holds and 0 otherwise. w(e ∈ H) indicates the importance (i.e., reliability) of edge e; w(e (cid:60) H) indicates the reliability of the absence of an edge. Since H is an undersampled graph, it consists of directed and bidirected edges. We thus implement both w(e ∈ H) and w(e (cid:60) H) as two pairs of n × n matrices, one pair for existence and absence of directed edges, and one pair for bidirected edges. To learn the optimal graph at the true causal timescale, the corresponding Gu of each G1 in the solution set is compared to H, and penalized for the difference according to the weights.\n\nThe reliability weights may also be based on strength of connection. For example, if H is estimated as a Granger Causality or Structural Vector Autoregressive (SVAR) (Granger, 1969; Lütkepohl, 2005) model, then the edge-weights may enable Clingo to preferentially ignore edges with weaker\n\n10Among all possible graphs that have a combination of both directed (2n2 ) and bidirected (2(n\n\n2)) edges, only a\n\nfraction are possible by undersampling a G1.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nconnection strength. In addition to using observed data to estimate the weights, prior knowledge can play a key role: edges known to exist can be given a higher weight, while those known to not exist could be given reduced (or zero) weight (See also Appendix D). The approach is flexible as it can combine estimates from data and expert knowledge. Applications to fMRI data for causal structure discovery at causal time scale are shown in Appendix E.\n\nIn order to incorporate Equation 1 in Listing 1, we replace its exact integrity constraints (Lines 10 − 13) with the optimization formulation (Gebser et al., 2011) in Listing 2. In Listing 2, we specify a weight for each edge (or lack there of) in H using W, with importance specified using W@i syntax with i being the importance.\n\n3.1\n\nsRASL Completeness and Correctness\n\nsRASL exhibits significant improvements in computation time, so it is important to show that we do not lose generality or theoretical guarantees. We demonstrate correctness and completeness using the notion of a direct encoding of the problem (i.e., the space of solutions is fully characterized, and any non-solution violates a constraint). We first prove (see Appendix A): Theorem 4. Listing 1 is a direct encoding of the undersampling problem.\n\nClingo is a complete solver, based on CDNL (Conflict-Driven Nogood Learning) (Drescher & Walsh, 2011), itself based on CDCL (Conflict-Driven Clause Learning) (Marques Silva & Sakallah, 1996; Marques-Silva & Sakallah, 1999). Hyttinen et al. (2014)[Theorem 2] and Hyttinen et al. (2013)[Section 5.2] show that, if the ASP encoding is the direct encoding of the problem, then ASP will produce the complete set of solutions in the infinite sample space limit. In other words, Theorem 3.1 implies: since our algorithm yields at least one sound solution, Clingo will produce all possible solutions. Therefore, soundness results in completeness. That is, sRASL’s success is not due to heuristics or some incomplete or not-everywhere-correct algorithmic step.11\n\n4 Results\n\nA major virtue of sRASL is its empirical performance, so we now consider a range of simulations (where we have known ground truth) to understand this performance in more detail. These experiments used Clingo in parallel mode using 10 threads computing on AMD EPYC 7551 CPUs. Given computational complexity, all experiments were run on a slurm cluster that submits jobs to one of the 19 machines on the same network, each with 64 cores and 512 GB of RAM.\n\n4.1 Comparing sRASL vs. RASL\n\nWe first compare sRASL with the existing RASL method, which struggles with graphs larger than 6 nodes (Plis et al., 2015a) (Figure 2). We generated 100 6-node SCCs for each density in [0.2, 0.25, 0.3], and then undersampled each graph by 2, 3, and 4. Each column of Figure 2 consists of graphs of approximately same density (increasing density from left-to-right), and subcolumns represent different undersample rates (for that density). As Figure 2 shows, sRASL is typically three orders of magnitude faster than RASL, even on small graphs. A similar comparison with the method of Hyttinen et al. (2017) that iteratively loops through possible values of u can be found in Figure 9 of Appendix C.\n\n4.2 Comparing Graph Size\n\nIt is perhaps unsurprising that sRASL runs much faster than RASL, as sRASL uses an ASP solver (which were previously known to yield faster algorithms (Hyttinen et al., 2017)). We next explored how large graphs could be that sRASL could handle. More generally, we aimed to better understand how sRASL’s computational performance scales with the number of nodes in single-SCC graphs. The focus on single SCCs is motivated by the theoretical need to understand the size-speed tradeoff. Moreover, many real-world systems consist of tightly coupled factors with many feedback loops (i.e., a single SCC). We consider multiple-SCC graphs in the next subsections.\n\n11Simulation testing provides further evidence. We found that sRASL and RASL produced identical outputs for 1000 different input graphs, and RASL is known to be correct and complete (Plis et al., 2015a)[Theorem 3.6].\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Time behavior of graphs of size 8, 16, and 32. Red line shows experimental time-out of 24 hours. Green/Gray dots represent input graphs that were/were not solved within the 24-hours window.\n\nFigure 4: Time behavior of graphs of size 64 with various SCC sizes. The time-out for this experiment was 24 hours (1440 Minutes).\n\nWe generated 50 random single-SCC graphs each of 8, 16, and 32 nodes, all with average degree of 1.4 outgoing edges per node. We then undersampled each graph by 2, 3, and 4, and used each individual undersampled graph as input to sRASL (i.e., 150 different input graphs for each size). We used a 24-hour timeout (i.e., stopped the run if it did not finish in 24 hours). Figure 3 shows the increasing computational costs as both number of nodes and undersample rate increase. Notably, sRASL was able to learn ⟦H⟧ for many 32-node single-SCC graphs, though it reached timeout for all 32-node H at u = 4. That is, for low u, sRASL scales to much larger single-SCC graphs than RASL.\n\n4.3 Comparing SCC Size in Multiple-SCC Graphs\n\nThe other major innovation of sRASL is incorporation of constraints derived from the SCC structure. We thus investigated the performance of sRASL on large, structured, multiple-SCC graphs. Many real-world systems exhibit some degree of modularity, where there are dense or feedback connections within a module or subsystem, and relatively sparser connections between modules or subsystems. In theory, sRASL should perform well on these kinds of structures since it incorporates SCC-based constraints. Please refer to Appendix D for an ablation study on the marginal benefit provided by these additional constraints for SCC structures. We tested the value of SCC-based constraints using graphs with 64 nodes that differed in their SCC structure. Specifically, we randomly generated 50 graphs each of: 32 size-2 SCCs; 16 size-4 SCCs; 8 size-8 SCCs; 4 size-16 SCCs; or 2 size-32 SCCs. We then undersampled each graph by u = 2, 3, or 4, and ran sRASL (again with a 24-hour timeout).\n\nFigure 4 shows the computation time for these graphs, with increasing SCC size (and decreasing number of SCCs) from left to right. The first key observation is that sRASL successfully found ⟦H⟧ for 64-node graphs, at least when there was some internal structure. Second, and relatedly, we observe a wide range of computation times for these graphs, even though all had the same number of nodes (64). We clearly see the impact of SCC structure, as sRASL was dramatically faster when there were many small SCCs, rather than a few large SCCs. The results in Figure 3 might seem to suggest an “upper bound” around 30 nodes for sRASL. But the results in Figure 4 make it clear that any potential “upper bound” is primarily on the number of nodes within the SCCs, rather than the total number of nodes in the graph.\n\n4.4 Comparing Graph Size With Constant SCC Size\n\nThe previous results suggest that sRASL might be able to solve much larger graphs, as long as the SCCs are not overly large. More generally, the previous simulations showed that sRASL’s computational cost scales (at least) exponentially in the size of the SCC, but did not reveal how it scales in the number of SCCs.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Time behaviour of graphs with the same SCCs sizes but with multiple number of SCCs. Top row: graphs of SCC size 7 with 1, 2, ..., 14 number of SCCs. Middle row: graphs of SCC size 8. Bottom row: graphs of SCC size 10. Bottom-right corner shows an example of a structured graph with 98 nodes composed of 14 SCCs of size 7. Each color represents one SCC.\n\nWe again generated 50 different graphs for each of several settings. We used SCCs with 7, 8, and 10 nodes, and varied the number of SCCs within a graph (again for u = 2, 3, and 4). Figure 5 shows the computational cost of sRASL, where each row includes graphs whose SCCs are the same size, and the number of SCCs increases from left-to-right. The critical observation here is that the time complexity grows approximately linearly in the number of SCCs, rather than exponentially (or worse). For example, the graph shown in Figure 5 has 98 nodes, but sRASL successfully computes ⟦H⟧ in approximately 20 minutes. (Recall that RASL took 17 hours to compute a graph with only 6 nodes.)\n\nThis simulation demonstrates that sRASL is usable on relatively large graphs, as long as there is appropriate internal structure. One might worry, though, whether real-world systems have the right structure. For fMRI (brain) data, Sanchez-Romero et al. (2019) recently aggregated a number of simulations of realistic causal graphs for brain processes studied with fMRI, and the largest SCC in these widely-accepted models has only 7 nodes. Moreover, typical brain parcellations contain only 50 − 100 regions (= nodes), and sRASL can easily handle 100-node graphs if SCCs are 8 − 10 nodes. The results in this subsection suggest that we could potentially find ⟦H⟧ for even larger graphs (n > 100), as long as they were composed of reasonably-sized SCCs. However, we found that the Clingo language and solver seems to be limited in the number of atoms that it can handle. In our simulations, graphs of size 100 seem to be the limit for Clingo to handle all the predicates. An open question is whether sRASL can be optimized to produce fewer predicates (or Clingo improved to handle more atoms).\n\n4.5 Optimization\n\nFinally, we explored the optimization capability of Clingo. Recall that sometimes ⟦H⟧ = ∅ due to statistical errors or other noise in learning H. Clingo can solve an optimization problem based on user-specified weights and priorities, and output a single solution with minimum cost function (along with u for this solution). In particular, we can use Clingo to find G1 whose Gu (for some u) are closest (relative to the edge weights) to H.12\n\n12If ⟦H⟧ (cid:44) ∅, then this optimization will simply return a single graph from ⟦H⟧.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: The omission (top) and commission (bottom) error of different graph sizes and undersampling of two, three and four from left to right.\n\nIn this simulation, we first randomly generate G1 and undersample at a random u to get Gu = H such that ⟦H⟧ (cid:44) ∅. We then assign weights to the edges of H and randomly delete one edge in H. We run sRASL on this “broken” H to learn a suitable G1. Red bars in Figure 6 show the edge omission and commission errors for this approach. We see that, except for high undersamplings, the optimization capability of Clingo can be used to frequently retrieve the true G1; that is, this version of sRASL is robust to small errors in H in many settings.\n\nA more complex approach is to first run the optimization method to identify a solution G1 opt and undersample rate uopt. We can then undersample this solution G1 opt. We then use ⟧ (i.e., the full equivalence class of the undersampled graph that is “nearest” to sRASL to obtain ⟦Gu ⟧; that is, we ask H). We then compute the error based on the minimum error among all G1 ∈ ⟦Gu whether the true graph was found somewhere in ⟦Gu ⟧. This approach is motivated by the intended use of sRASL by domain scientists, where they can use domain knowledge to help select graph(s) from the equivalence class. Blue bars in Figure 6 show that this more complex method provides improved performance compared to regular optimization.\n\nopt by uopt to get Gu\n\nopt\n\nopt\n\nopt\n\n5 Conclusion and Discussion\n\nReal-world scientific problems frequently involve measurement processes that operate at a different timescale than the causal structure of the system under study. As causal learning and analysis methods are increasingly used to address societal and policy challenges, it is increasingly critical that we use methods that reveal usable information (while also being clear when we cannot infer some information). Obviously, like any method, sRASL could yield information that is misused, but the aim here is to provide another useful tool in the scientists’ and policy-makers’ toolboxes. If measurements occur at a slower rate than the causal influences, then causal discovery from those undersampled data can yield highly misleading outputs. Multiple methods have been developed to infer aspects of the underlying causal structure from the undersampled data/graph. However, the assumptions or computational complexities of those algorithms make them unusable for most real-world challenges. In this paper, we have developed and tested sRASL, a novel approach that is less subject to those same limitations. More specifically, sRASL provides all consistent solutions (without knowledge of exact undersampling rate) for large (100-node) graphs in a usable amount of time. sRASL also shows reasonable robustness to statistical error in the estimated graph by finding the closest consistent solution. Future research will focus on application of sRASL to actual neuroimaging data, and extensions to situations with multiple measurement modalities.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n6 Acknowledgments\n\nThis work was supported by NIH R01MH129047 and in part by NSF 2112455, and NIH 2R01EB006841. We are grateful to Antti Hyttinen, Matti Järvisalo, and Frederick Eberhardt for discussions on clingo.\n\nReferences\n\nTianjiao Chu, Clark Glymour, and Greg Ridgeway. Search for additive nonlinear time series causal\n\nmodels. Journal of Machine Learning Research, 9(5), 2008.\n\nJohn W Cook, David Danks, and Sergey M Plis. Learning dynamic structure from undersampled\n\ndata. In Proceedings of the UAI Causality Workshop, 2017.\n\nDavid Danks and Sergey Plis. Learning causal structure from undersampled time series. In NIPS\n\nWorkshop on Causality, volume 1, pp. 1–10, 2013.\n\nDavid Danks and Sergey Plis. Amalgamating evidence of dynamics. Synthese, 196(8):3213–3230,\n\n2019.\n\nChristian Drescher and Toby Walsh. Conflict-driven constraint answer set solving with lazy nogood\n\ngeneration. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.\n\nDoris Entner and Patrik O Hoyer. On causal discovery from time series data using FCI. Probabilistic\n\ngraphical models, pp. 121–128, 2010.\n\nDirk Fahland, Daniel Lübke, Jan Mendling, Hajo Reijers, Barbara Weber, Matthias Weidlich, and Stefan Zugal. Declarative versus imperative process modeling languages: The issue of understandability. In Enterprise, Business-Process and Information Systems Modeling, pp. 353–366. Springer, 2009.\n\nMartin Gebser, Benjamin Kaufmann, Roland Kaminski, Max Ostrowski, Torsten Schaub, and Marius Schneider. Potassco: The Potsdam answer set solving collection. Ai Communications, 24(2): 107–124, 2011.\n\nM Gelfond and V Lifschitz. The stable model semantics for logic programming. ICSLP, 1988.\n\nAndreas Gerhardus and Jakob Runge. High-recall causal discovery for autocorrelated time series with latent confounders. Advances in Neural Information Processing Systems, 33:12615–12625, 2020.\n\nMingming Gong, Kun Zhang, Bernhard Schoelkopf, Dacheng Tao, and Philipp Geiger. Discovering temporal causal relations from subsampled data. In International Conference on Machine Learning, pp. 1898–1906. PMLR, 2015.\n\nClive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.\n\nEconometrica: journal of the Econometric Society, pp. 424–438, 1969.\n\nAntti Hyttinen, Patrik O Hoyer, Frederick Eberhardt, and Matti Jarvisalo. Discovering cyclic causal models with latent variables: A general SAT-based procedure. arXiv preprint arXiv:1309.6836, 2013.\n\nAntti Hyttinen, Frederick Eberhardt, and Matti Järvisalo. Constraint-based Causal Discovery: Conflict\n\nResolution with Answer Set Programming. In UAI, pp. 340–349, 2014.\n\nAntti Hyttinen, Sergey Plis, Matti Järvisalo, Frederick Eberhardt, and David Danks. A constraint optimization approach to causal discovery from subsampled time series data. International Journal of Approximate Reasoning, 90:208–225, 2017.\n\nFattaneh Jabbari, Joseph Ramsey, Peter Spirtes, and Gregory Cooper. Discovery of causal models that contain latent variables through bayesian scoring of independence constraints. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 142–157. Springer, 2017.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nV Lifschitz. The stable model semantics for logic programming, 1988.\n\nHelmut Lütkepohl. New introduction to multiple time series analysis. Springer Science & Business\n\nMedia, 2005.\n\nDaniel Malinsky and Peter Spirtes. Causal structure learning from multivariate time series in settings In Proceedings of 2018 ACM SIGKDD workshop on causal\n\nwith unmeasured confounding. discovery, pp. 23–47. PMLR, 2018.\n\nJoao P Marques-Silva and Karem A Sakallah. GRASP: A search algorithm for propositional\n\nsatisfiability. IEEE Transactions on Computers, 48(5):506–521, 1999.\n\nJ.P. Marques Silva and K.A. Sakallah. GRASP-A new search algorithm for satisfiability. In Proceedings of International Conference on Computer Aided Design, pp. 220–227, 1996. doi: 10.1109/ICCAD.1996.569607.\n\nA. Moneta, N. Chlaß, D. Entner, and P. Hoyer. Causal search in structural vector autoregressive models. In Journal of Machine Learning Research: Workshop and Conference Proceedings, Causality in Time Series (Proc. NIPS2009 Mini-Symposium on Causality in Time Series), volume 12, pp. 95–114, 2011.\n\nJoris M Mooij and Tom Claassen. Constraint-based causal discovery using partial ancestral graphs in the presence of cycles. In Conference on Uncertainty in Artificial Intelligence, pp. 1159–1168. PMLR, 2020.\n\nIlkka Niemelä. Logic programs with stable model semantics as a constraint programming paradigm.\n\nAnnals of mathematics and Artificial Intelligence, 25(3):241–273, 1999.\n\nMW Oram and DI Perrett. Time course of neural responses discriminating different views of the face\n\nand head. Journal of neurophysiology, 68(1):70–84, 1992.\n\nJudea Pearl et al. Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress, 19:\n\n2, 2000.\n\nSergey Plis, David Danks, Cynthia Freeman, and Vince Calhoun. Rate-agnostic (causal) structure\n\nlearning. In Advances in neural information processing systems, pp. 3303–3311, 2015a.\n\nSergey Plis, David Danks, and Jianyu Yang. Mesochronal structure learning. In Uncertainty in artificial intelligence: proceedings of the... conference. Conference on Uncertainty in Artificial Intelligence, volume 31. NIH Public Access, 2015b.\n\nThomas Richardson and Peter Spirtes. Ancestral graph Markov models. The Annals of Statistics, 30\n\n(4):962–1030, 2002.\n\nRuben Sanchez-Romero, Joseph D Ramsey, Kun Zhang, Madelyn RK Glymour, Biwei Huang, and Clark Glymour. Estimating feedforward and feedback effective connections from fMRI time series: Assessments of statistical methods. Network Neuroscience, 3(2):274–306, 2019.\n\nPatrik Simons, Ilkka Niemelä, and Timo Soininen. Extending and implementing the stable model\n\nsemantics. Artificial Intelligence, 138(1-2):181–234, 2002.\n\nPeter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search. Springer New York, 1993. doi: 10.1007/978-1-4612-2748-9. URL https://doi.org/10.1007/ 978-1-4612-2748-9.\n\nPeter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, Prediction,\n\nand Search. MIT press, 2000.\n\nSofia Triantafillou, Ioannis Tsamardinos, and Ioannis Tollis. Learning causal structure from overlapping variable sets. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 860–867. JMLR Workshop and Conference Proceedings, 2010.\n\nJiji Zhang. Causal reasoning with ancestral graphs. Journal of Machine Learning Research, 9:\n\n1437–1474, 2008.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nA Appendix\n\nWe start with proving some results used in conversion of the DBN structures to their compressed graph representations.\n\nLemma 1. For all u, Gu contains no directed edges between variables at the same time step.\n\nProof. vvu = 1 holds by assumption for G1. For u > 1, every directed edge corresponds to a directed path of length u in G1. Since all directed edges in G1 are from t − 1 to t (or more generally, from t − k to t − (k + 1)), every directed path in G1 is from an earlier time step to the current one. Hence, no □\ndirected edge in Gu can be from V t\n\ni to V t j.\n\nLemma 2. If the Markov order of G1 is 1, then the Markov order of all Gu is also 1 (relative to measurement at rate u).\n\nProof. The Markov order of a dynamic causal graph is the smallest m such that Vt is independent of Vt−r given Vt−1, . . . , Vt−m for all r > m. If the Markov order of G1 is 1, then all paths from Vt−r to Vt must be blocked by Vt−1 for r > 1. Since graphical structure is replicated across timesteps, it follows that all paths from Vt−r to Vt must be blocked by Vt−u for r > u. Therefore, the Markov order of Gu □\nis u, which corresponds to Markov order 1 for measurements at rate u.\n\nThe following theorem demonstrates correctness of our ASP algorithm.\n\nTheorem 4. Listing 1 is a direct encoding of the undersampling problem.\n\nProof. We will prove this by contradiction. Let us call the undersampled input graph to the algorithm H, considering that is the undersampled version of a graph G1 true at rate utrue. By definition, every directed edge in H corresponds to a path of length utrue in G1 true. Similarly, every bidirected edge in H corresponds to an unobserved common cause fewer than utrue timesteps back(refer to Section 2 for exact definition). Line 7 − 11 in Listing 1 considers all such G1s without exclusion. Let us call the set all the pairs of graphs and corresponding undersampling rates u described by Listing 1 S.\n\na and ua that is in S but if we undersample G1\n\na by ua, let us call it Gu Let us assume there is a pair G1 a, will not be the same as H. If Gu a has an extra directed(bidirected) edge, this will contradict with line 12(13) of Listing 1. Similarly, if H has a directed(bidirected) edge that in not present in Gu a, it will contradict with line 14(16). Therefore, Listing 1 is a direct encoding of the undersampling □\nproblem.\n\nB Examples of changes in graphs through undersampling\n\nIn this section, we provide additional examples to visualize how graphs will change through undersampling.\n\nC Comparing sRASL and a modified version the Hyttinen et al. (2017)\n\nAs mentioned in Section 2, Hyttinen et al. (2017) specifies the undersampling rate u. Therefore, a comparison of their method with ours will not be a fair one. However, one can iterate over undersampling rates to find all the solution at different undersampling rates. In this section, we compare the performance of this modified version of Hyttinen et al. (2017) to our proposed method. Figure 9 summarizes this experiment. As we can see, proposed method in Hyttinen et al. (2017) performs compatibly with our method in small graphs. However, as the graphs grow larger, the advantage of our method gains significance. Specifically, Hyttinen et al. (2017) struggles with large graphs and larger undersampling rates. Most of the test cases on graphs larger than 30 nodes and undersampling greater than 3 did not complete in the dedicated 24-hour period. While our method was able to compute the equivalence class of all the graphs much faster than 24 hours.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7: Example of a 7-node graph undersampled 6 time.\n\nFigure 8: Example of a 12-node graph undersampled 11 time.\n\n13\n\n1234567123456712345671234567123456712345671234567U = 1U = 2U = 3U = 4U = 5U = 6U = 7123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112123456789101112U = 1U = 2U = 3U = 4U = 5U = 6U = 7U = 8U = 9U = 10U = 11U = 12Published as a conference paper at ICLR 2023\n\nFigure 9: Time behavior of the same set of graphs when solved with our proposed method (blue) and modified version of Hyttinen et al. (2017) with iterating over undersampling rates (orange). The time out for this experiment was 24 hours and the numbers in orange indicate number of examples that was not completed in 24-hour period.\n\nD The Effects of Accounting for SCCs In sRASL\n\nIn this section, we show the results of additional experiments on the effects of accounting for strongly connected components (SCCs) when the graph has a modular structure (i.e., consists of several interconnected strongly connected components). For this experiment, we generated 50 random graphs sized 8 to 15 with multiple SCCs as described in Table1. Then on the same set of graphs, we ran sRASL once with using our additional constraints for SCC structures and once without accounting for the modular structure. We limited the computational resources available to each run to 24 hours time cutoff with a RAM limit of 50 GB. The results presented in Figure11 show that using additional constraints to account for SCC structure dramatically reduces the time and memory needed to compute equivalent classes for undersampled graphs. Furthermore, the difference between time and memory requirements to solve for these graphs with and without constraints for SCCs increases for larger graphs as the computational requirements for the latter grow at a much faster pace. This result allows us to handle much larger graphs as shown in Figure 5 of the main paper.\n\nFigure 10: Time behavior of the same set of graphs when solved with and without accounting for additional constraints accounting for the SCC structure. While sRASL most of the 15-node graphs in a 24 hours period without the SCC constraints due to either timeout or Out Of Memory error(OOM), the longest it takes to solve a 15-node graph with SCC constraints is 14 seconds. None of the graphs failed to compute the complete equivalence class within the time and memory allocated when solved accounting for the SCC structure.\n\nTable 1: Number of SCCs and nodes per SCC of the graphs in the benchmark dataset\n\nNum Nodes Num SCCs SCC Sizes\n\n8 2\n4,4\n\n9 3\n3,3,3\n\n10 3\n3,3,4\n\n11 3\n3,4,4\n\n12 3\n4,4,4\n\n13 3\n4,4,5\n\n14 3\n4,5,5\n\n15 3\n5,5,5\n\n14\n\n24 hoursMethodOursModifiedHyttinen et al. 201710474927312193335936381122337109820.30.30.30.30.30.30.30.350G RAM24 hoursPublished as a conference paper at ICLR 2023\n\n4\n\n3 → V t+1\n\nFigure 11: A knowledge of a definite presence of an edge in G1 between, for example, nodes 3 and 4, i.e. V t , can be easily encoded by adding ‘ edge1(3,4).‘ to Listing 1. In this experiment, we have added knowledge about a pair of arbitrary selected edges of G1 to the problem specification (orange dots) and compared the run time with the ASP specification that does not include this additional information about the solution (blue dots). The time out for the new computation was set to 1 hours and the examples were all the same as the ones already shown in Figure 1. The speed up with the additional constraints is clearly visible on the plots.\n\nE sRASL applied to real fMRI data\n\nFigure 12: Estimated graph from fMRI data of resting state left hemisphere medial temporal lobe using sRASL after obtaining measurement time scale graph by applying Granger Causality. Regions of interest include cornu ammonis 1 (CA1), and dentate gyrus together (CA23DG); entorhinal cortex (ERC); perirhinal cortex divided in Brodmann areas (BA35 and BA36); and parahippocampal cortex (PHC)..\n\nIn order to demonstrate the the application of our method, we used publicly available data from (Sanchez-Romero et al., 2019) and applied our method on the resting-state fMRI data. We used the 10 datasets of concatenated recording for 10 individuals, comprising seven regions of interest from medial temporal lobe, each containing 4,210 datapoints.\n\nWe first generated estimated graphs H from the fMRI data using Granger Causality (Granger, 1969; Cook et al., 2017). Note that these estimated graphs are at the measurement timescale (they include bidirected edges) and due to statistical and measurment error are often not reachable from any graph at causal time scale G1. Therefore, we apply sRASL optimization on H to get the closest reachable graph at causal time scale. Figure12 shows the result of our estimated graph at causal time scale. It is important to note that with empirical data, we do not have fully defined ground truth to assess our findings.\n\n15\n\n0.30.30.30.30.30.3SCC Flase (original)SCC False (withdomain knowledge)1234567Published as a conference paper at ICLR 2023\n\nFollowing our approach on Section 4.5, we use the estimated graph G1 opt in Figure 12 and undersample it by the rate that our sRASL optimization has found, i.e. uopt to get Gu opt. We then use sRASL to obtain ⟦Gu ⟧ (i.e., the full equivalence class of the undersampled graph that is “nearest” to H). In the case of resting state fMRI data from left hemisphere medial temporal lobe, the full equivalence class consists of 23 graphs that are shown in Figure 13. From this class of equally possible underlying causal graphs, psychologists and experts can examine and determine with causal graph is the most reasonable one.\n\nopt\n\nF Brief Introduction on clingo and Answer Set Programming (ASP)\n\nclingo (Gebser et al., 2011) combines a grounder gringo and a solver clasp. clingo is a declarative programming system based on logic programs and their answer sets, used to accelerate solutions of computationally involved combinatorial problems. The grounder converts all parts of a clingo program to “atoms,” (grounds the statements) and the solver finds “stable models.” In ASP, the answer set is a model in which all the atoms are derived from the program and each “answer” is a stable model where all the atoms are simultaneously true.\n\nA general clingo program includes three main sections, which we show below using our algorithm as an example:\n\n1. Facts: these are the known elements of the problem. For example, the input to Listing 1 is a graph for which we know the edges. A directed edge from node 1 to node 5 is in H translates to hdirected(1,5) (line 1) or if node 1 is part of the SCC number 2, we state this fact in clingo by scc(1,2) (line 2). 2. Rules: much like an if-else statement, a rule in clingo consists of a body and a head, formatted as head :- body. If all the literals in the body are true, then the head must also be true. Rules can include variables (starting with capital letters), and they are used to derive new facts after grounding. For example:\n\ndirected(X, Y, 1) :- edge1(X, Y). (2) means that for any instantiations of the variables X and Y, if we have an edge from X to Y, there is a directed path from X to Y of length 1. Before this line, if the model contained the fact edge1(2,3), this line would generate a new fact: directed(2,3,1). Another type of rule is the “choice rule” that describes all the possible ways to choose which atoms are included in the model. For example, in line 5 of Listing 1 we used a choice rule to state that the undersampling rate u can be anything from 1 to maxu. The cardinality constraint:\n\n{u(1..20)}. (3) will generate 220 different models (they will not all actually be generated if they conflict with other predicate in each model, or else it would not be possible). In each of these 220 models, one subset of all possible atoms generated with this choice rule exists (φ, {u(1)}, {u(1), u(2)}, . . . ). An example of an unconstrained choice rule is line 6 in Listing 1, where we want to generate one model for each possible way edges can be present in a graph between two nodes X and Y. We can also limit the choice rule. In our problem, only one undersampling rate is present at each solution. We limit the cardinality constraint to have only one member in each model:\n\n1 {u(1..20)} 1.\n\n(4)\n\nthe 1 on the left is the minimum instantiations of this atom in the model and the 1 on the right is the maximum. Therefore, we only generate undersampling rate. Having several choice rules will multiply the number of generated models by each choice rule.\n\n(cid:17) = 20 models with this rule, namely one for each\n\n(cid:16)20\n\n1\n\n3. Integrity Constraints: if choice rules are to generate new models, integrity constraints are there to remove the wrong models from the answers set. More specifically, an integrity constraint is of the form:\n\n(5) where literals L0, L1, .... cannot be simultaneously positive. For example, in line 16 of Listing1, we have:\n\n:- L0, L1, ...\n\n.\n\n:- edge1(X, Y), scc(X, K), scc(Y, L), K != L,\n\n(6)\n\nsccsize(L, Z), Z > 1, not dag(K,L).\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 13: Equivalence class (⟦Gu to and reachGu one\n\n⟧) of all possible graphs at causal time scale (G1s) that can be undersampled opt. Psychologists and experts can examine and determine with causal graph is the most reasonable\n\nopt\n\n17\n\n12345671234567123456712345671234567123456712345671234567123456712345671234567123456712345671234567123456712345671234567123456712345671234567123456712345671234567Published as a conference paper at ICLR 2023\n\nfor cases where the graph consists of several SCCs that are connected using a DAG. If the SCCs are connected by a cyclic directed graph, then the whole graph will become one big Strongly Connected Component. Integrity constraint 6 states that if there is not a directed edge from a node in SCC K to a node in SCC L as part of the initial DAG, there cannot be such edge1(X, Y) from node X to node Y, if node X is in SCC K and node Y is in SCC L.\n\n18",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a new algorithm to solve the problem of recovering, as much as possible, the original graphical causal structure at the causal timescale from the derived graphical structure at a measurement timescale, where measurements are made every u number of time steps for an unknown u. The new algorithm is based on ASP and incorporates some new constraints that were not exploited in previous algorithms. Experiments show that the new algorithm scales much better than the previous methods.\n\n# Strength And Weaknesses\n\nStrengths:\n\n1. The proposed algorithm addresses a commonly encountered challenge to causal discovery and achieves a very significant speedup in comparison to previous attempts. \n2. The experiments demonstrate some nice properties of the new algorithm.\n3. This paper is well written and relatively easy to follow.  \n\nWeaknesses:\n\n1. There is no experiment with real or even simulated data. I am a little puzzled why no empirical demonstration is attempted on streamlining the proposed algorithm with a causal discovery algorithm applied to data at the measurement time scale.\n2. For the optimization version of the algorithm, it is unclear how the weights should be determined. Are they supposed to be entirely user or expert specified? Or is there a data-driven procedure to assign the weights. Conceivably some causal discovery methods may yield interpretable weights on the inferred edges, but this prospect does not seem to be discussed in the paper.    \n3. It is also unclear to me why no empirical comparison is made to the ASP-based algorithm in Hyttinen et al. (2017).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe main idea of the paper seems quite straightforward given the previous work on this problem, so the novelty is limited. However, the improvement achieved by the new proposal is significant. The paper is very clear and readable, and I expect the reproducibility of the empirical results to be very good, especially since they do not involve data.\n\nA clarificatory question: does the soundness of the algorithm rely on the assumption that the condition on gcd(L_s) in Theorem 3 (and Theorem 2) holds?\n\n# Summary Of The Review\n\nThis is a well written paper presenting an improved method to tackle an interesting problem. The improvement demonstrated by experiments is significant, though more experiments on simulated or real data would probably better vindicate the utility of the method.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPAIRWISE CONFIDENCE DIFFERENCE ON UNLABELED DATA IS SUFFICIENT FOR BINARY CLASSIFICATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nLearning with confidence labels is an emerging weakly supervised learning paradigm, where training data are equipped with confidence labels instead of exact labels. Positive-confidence (Pconf) classification is a typical learning problem in this context, where we are given only positive data equipped with confidence. However, pointwise confidence may not be accessible in real-world scenarios. In this paper, we dive into a novel weakly supervised learning problem called confidence-difference (ConfDiff) classification. Instead of pointwise confidence, we are given only unlabeled data pairs equipped with confidence difference specifying the difference in the probabilities of being positive. An unbiased risk estimator is derived to tackle the problem, and we show that the estimation error bound achieves the optimal convergence rate. Extensive experiments on benchmark data sets validate the effectiveness of our proposed approaches in leveraging the supervision information of the confidence difference.\n\n1\n\nINTRODUCTION\n\nRecent years have witnessed the prevalence of deep learning and its successful applications. However, the success is built on the basis of the collection of large amounts of data with unique and In many real-world scenarios, it is often difficult to satisfy such requirements. accurate labels. To circumvent the difficulty, various weakly supervised learning problems have been investigated accordingly, including but not limited to semi-supervised learning (Chapelle et al., 2006; Zhu & Goldberg, 2009; Li & Zhou, 2015; Berthelot et al., 2019), label-noise learning (Patrini et al., 2017; Han et al., 2018; Li et al., 2021; Wang et al., 2021; Wei et al., 2022), positive-unlabeled learning (du Plessis et al., 2014; Su et al., 2021; Yao et al., 2022), partial-label learning (Cour et al., 2011; Wang & Zhang, 2020; Wen et al., 2021; Wang et al., 2022; Wu et al., 2022), unlabeled-unlabeled learning (Lu et al., 2019; 2020) and similarity-based classification (Bao et al., 2018; Cao et al., 2021b; Bao et al., 2022).\n\nLearning with confidence labels (Ishida et al., 2018; Cao et al., 2021a;b) is another weakly supervised learning paradigm, where we are given training examples with confidence labels instead of exact labels. Positive-confidence (Pconf) classification (Ishida et al., 2018) is a problem setting within this scope, which is aimed at learning a binary classifier from only positive data equipped with confidence (the probability of being positive) without negative data. Pconf classification can alleviate the difficulty when negative data cannot be acquired due to privacy or security issues during the data annotation process. The need to learn from such inexact supervision widely exists in real-world scenarios, such as purchase prediction (Ishida et al., 2018), user preservation prediction (Ishida et al., 2018), drivers’ drowsiness prediction (Shinoda et al., 2020), etc.\n\nHowever, the process of collecting large amounts of training examples with pointwise confidence might be actually demanding under many circumstances, since it is tough to describe the probability of being positive for each training example exactly (Shinoda et al., 2020). Feng et al. (2021) showed that learning from pairwise comparisons could serve as an alternative strategy given limited pointwise labeling information. Inspired by it, we investigate a more practical problem setting in this paper, where we are given only unlabeled data pairs with confidence difference indicating the difference in the probabilities of being positive. Compared with pointwise confidence, confidence difference can be collected more easily in many real-world scenarios. Take click-through rate prediction in recommender systems (Zhang et al., 2019) for example. The combinations of users and\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ntheir favorite/disliked items can be regarded as positive/negative data. When collecting training data, it is not easy to distinguish between positive and negative data. Furthermore, the positive confidence of training data may be difficult to be determined due to the extremely sparse and class-imbalance problems (Yao et al., 2021). However, it is much easier to obtain the difference in the preference between a pair of candidate items for a given user. Take the disease risk estimation problem for another example. The goal is to predict the risk of having some disease given a person’s attributes. When asking doctors to annotate the probabilities of having the disease for people, it is not easy to determine the exact values of the probabilities. Furthermore, the probability values given by different doctors may be different due to personally subjective assumptions and will deviate from the ground-truth values. However, it is much easier and less biased to estimate the relative difference in the probabilities of having the disease between two people.\n\nOur contributions are summarized as follows:\n\n• We investigate confidence-difference (ConfDiff) classification, a novel and practical weakly supervised learning problem, which can be solved via empirical risk minimization by constructing an unbiased risk estimator. The proposed approach can be equipped with any model, loss function, and optimizer flexibly.\n\n• The estimation error bound is derived, showing that the proposed approach achieves the optimal parametric convergence rate. The robustness is further demonstrated by probing into the influence of an inaccurate class prior probability and noisy confidence difference.\n\n• To mitigate overfitting issues, a risk correction approach (Lu et al., 2020) with consistency guarantee is further introduced. Extensive experimental results on benchmark data sets validate the effectiveness of the proposed approaches.\n\nRelated works. Learning with pairwise comparisons has been investigated pervasively in the community (Burges et al., 2005; Cao et al., 2007; Jamieson & Nowak, 2011; Park et al., 2015; Kane et al., 2017; Xu et al., 2017; Shah et al., 2019), with applications in information retrieval (Liu, 2011), computer vision (Fu et al., 2015), regression (Xu et al., 2019; 2020), crowdsourcing (Chen et al., 2013; Zeng & Shen, 2022), graph learning (He et al., 2022), etc. It is noteworthy that there exist distinct differences between our work and previous works on learning with pairwise comparisons. Previous works have mainly tried to learn a ranking function which can rank candidate examples according to the relevance or preference. In this paper, we try to learn a pointwise binary classifier by conducting empirical risk minimization under the binary classification setting.\n\nRelationship to Pcomp classification. Feng et al. (2021) elaborated that a binary classifier could be learned from pairwise comparisons, which was termed as Pcomp classification. There are distinct differences between our work and Pcomp classification. First, Pcomp classification is not capable of leveraging the fine-grained confidence difference, which can be incidentally obtained when collecting pairwise comparison data. We will experimentally elucidate the benefit of exploiting the confidence difference in the later section. Second, the assumptions of the data generation process are different. Pcomp classification assumes that the unlabeled data pair is ordered, where the first instance is more likely to be positive than the other. In ConfDiff classification, the instances of the unlabeled data pair are independent, which can be easier to collect.\n\n2 PRELIMINARIES\n\nIn this section, we introduce the notations used in this paper and discuss the background of binary classification, Pconf classification and Pcomp classification. Then, we elucidate the data generation process of confidence-difference classification.\n\n2.1 BINARY CLASSIFICATION\n\nFor binary classification, let X = Rd denote the d-dimensional feature space and Y = {+1, −1} denote the label space. Let p(x, y) denote the unknown joint probability distribution over random variables (x, y) ∈ X × Y. The task of binary classification is to learn a binary classifier g : X → R which minimizes the following classification risk:\n\nR(g) = Ep(x,y)[l(g(x), y)],\n\n(1)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nwhere l(·, ·) is a non-negative binary-class loss function, such as the 0-1 loss and logistic loss. Let π+ = p(y = +1) and π− = p(y = −1) denote the class prior probabilities for the positive and negative classes respectively. Furthermore, let p+(x) = p(x|y = +1) and p−(x) = p(x|y = −1) denote the class-conditional probability densities of positive and negative data respectively. Then the classification risk in Eq. (1) can be equivalently expressed as\n\nR(g) = π+Ep+(x)[l(g(x), +1)] + π−Ep−(x)[l(g(x), −1)].\n\n(2)\n\n2.2 POSITIVE-CONFIDENCE (PCONF) CLASSIFICATION\n\nIn many real-world applications, it may be difficult to collect negative data. Pconf classification (Ishida et al., 2018) is aimed at inducing a binary classifier from only positive data. The additional requirement is that the confidence of being positive should be accessible to the learning algorithm. Given only positive data equipped with confidence {(xi, ri)}n i=1, Ishida et al. (2018) provided an unbiased risk estimator to conduct empirical risk minimization:\n\n(cid:98)RPconf (g) =\n\nπ+ n\n\nn (cid:88)\n\ni=1\n\n(l(g(xi), +1) +\n\n1 − ri ri\n\nl(g(xi), −1)),\n\n(3)\n\nwhere ri = p(yi = +1|xi) is the positive confidence associated with xi. However, pointwise positive confidence may not be easy to obtain in real-world scenarios (Shinoda et al., 2020).\n\n2.3 PAIRWISE-COMPARISON (PCOMP) CLASSIFICATION\n\nPcomp classification is a weakly supervised binary classification problem (Feng et al., 2021). In Pcomp classification, we are given pairs of unlabeled data where we know which one is more likely to be positive than the other. It is assumed that Pcomp data are sampled from labeled data pairs whose labels belong to {(+1, −1), (+1, +1), (−1, −1)}. Based on this assumption, the probability density of Pcomp data (x, x′) is given as\n\n(cid:101)p(x, x′) =\n\nq(x, x′)\n\nπ2\n\n+ + π2\n\n− + π+π−\n\n,\n\n(4)\n\nwhere q(x, x′) = π2 +p+(x)p+(x′) + π2 risk estimator for Pcomp classification is derived as follows:\n\n−p−(x)p−(x′) + π+π−p+(x)p−(x′). Then, an unbiased\n\n(cid:98)RPcomp(g) =\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(l(g(xi), +1) + l(g(x′\n\ni), −1) − π+l(g(xi), −1) − π−l(g(x′\n\ni), +1)).\n\n(5)\n\nIn real-world applications, we may not only know one example is more likely to be positive than the other, but also know how much the difference of confidence is. Next, a novel weakly supervised learning setting named ConfDiff classification is introduced.\n\n2.4 CONFIDENCE-DIFFERENCE (CONFDIFF) CLASSIFICATION\n\nIn this subsection, the formal definition of confidence difference is given firstly. Then, we elaborate the data generation process of ConfDiff data. Definition 1 (Confidence Difference). The confidence difference c(x, x′) between the unlabeled data pair (x, x′) is defined as\n\nc(x, x′) = p(y′ = 1|x′) − p(y = 1|x).\n\n(6)\n\ni), ci)}n\n\ni=1. Here, ci = c(xi, x′ i). Furthermore, the unlabeled data pair (xi, x′\n\nAs shown in the definition above, the confidence difference denotes the difference in the class posterior probabilities between the unlabeled data pair, which can measure how confident the pairwise comparison is. In ConfDiff classification, we are only given n unlabeled data pairs with confidence difference D = {((xi, x′ i) is the confidence difference for the unlabeled data pair (xi, x′ i) is assumed to be drawn from a probability density p(x, x′) = p(x)p(x′). This indicates that xi and x′ i are two i.i.d. instances sampled from p(x). It is worth noting that the confidence difference ci will be positive if the second instance x′ i has a higher probability to be positive than the first instance xi, and will be negative otherwise. During the data collection process, the labeler can first sample two unlabeled data from the marginal distribution p(x), then provide the confidence difference for them. This data generation assumption makes the unlabeled data pairs easier to be collected.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3 THE PROPOSED APPROACH\n\nIn this section, an unbiased risk estimator is presented for ConfDiff classification. Then, we give an estimation error bound to show the convergence property. Besides, we show the influence of an inaccurate class prior probability and noisy confidence difference on the risk estimator. Furthermore, a risk correction approach (Lu et al., 2020) is elaborated to improve the generalization performance of our proposed approach.\n\n3.1 UNBIASED RISK ESTIMATOR\n\nIn this subsection, we show that the classification risk in Eq. (1) can be expressed with ConfDiff data in the equivalent way.\n\nTheorem 1. The classification risk R(g) in Eq. (1) can be equivalently expressed as\n\nRCD(g) = Ep(x,x′)[\n\n1 2\n\n(L(x, x′) + L(x′, x))],\n\n(7)\n\nwhere\n\nL(x, x′) = (π+ − c(x, x′))l(g(x), +1) + (π− − c(x, x′))l(g(x′), −1).\n\nAccordingly, we can derive an unbiased risk estimator for ConfDiff classification:\n\n(cid:98)RCD(g) =\n\n1 2n\n\nn (cid:88)\n\ni=1\n\n((π+ − ci)l(g(xi), +1) + (π− − ci)l(g(x′\n\ni), −1)\n\n+(π+ + ci)l(g(x′\n\ni), +1) + (π− + ci)l(g(xi), −1)).\n\n(8)\n\nTo estimate the class prior probability π+, we can transform ConfDiff data into Pcomp data by ranking the two instances in the unlabeled data pair according to the confidence difference. Then, we can adopt the approach proposed in Feng et al. (2021) to estimate π+. It is worth noting that the risk estimator in Eq. (3) for Pconf classification is very sensitive to small confidence values, while our risk estimator will not be influenced by them.\n\nMinimum-variance risk estimator. Actually, Eq. (8) is one of the candidates of the unbiased risk estimator. We introduce the following lemma:\n\nLemma 1. The following expression is also an unbiased risk estimator:\n\n1 n\n\nn (cid:88)\n\n(αL(xi, x′\n\ni) + (1 − α)L(x′\n\ni, xi)),\n\ni=1\n\n(9)\n\nwhere α ∈ [0, 1] is an arbitrary weight.\n\nThen, we introduce the following theorem:\n\nTheorem 2. The unbiased risk estimator in Eq. (8) has the minimum variance among all the candidate unbiased risk estimators in the form of Eq. (9) w.r.t. α ∈ [0, 1].\n\nTheorem 2 indicates the variance minimality of the proposed unbiased risk estimator in Eq. (8), and we adopt this risk estimator in the following sections.\n\n3.2 ESTIMATION ERROR BOUND\n\nIn this subsection, we elaborate the convergence property of the proposed risk estimator (cid:98)RCD(g) by giving an estimation error bound. Let G = {g : X (cid:55)→ R} denote the model class. It is assumed that there exists some constant Cg such that supg∈G ∥g∥∞ ≤ Cg and some constant Cl such that sup|z|≤Cg l(z, y) ≤ Cl. We also assume that the binary loss function l(z, y) is Lipschitz continuous for z and y with a Lipschitz constant Ll. 1 Let g∗ = arg ming∈G R(g) denote the minimizer of the classification risk in Eq. (1) and (cid:98)gCD = arg ming∈G (cid:98)RCD(g) denote the minimizer of the unbiased risk estimator in Eq. (8). The following theorem can be derived:\n\n1The theoretical analysis in the next subsections is also based on these assumptions. For simplicity, we do\n\nnot restate them in the next subsections.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem 3. For any δ > 0, the following inequality holds with probability at least 1 − δ:\n\nR((cid:98)gCD) − R(g∗) ≤ 8LlRn(G) + 4Cl\n\n(cid:114)\n\nln 2/δ 2n\n\n,\n\n(10)\n\nwhere Rn(G) denotes the Rademacher complexity of G for unlabeled data with size n.\n\nFrom Theorem 3, we can observe that as n → ∞, R((cid:98)gCD) → R(g∗) because Rn(G) → 0 for all parametric models with a bounded norm, such as deep neural networks trained with weight decay n), where Op (Golowich et al., 2018). Furthermore, the estimation error bound converges in Op(1/ denotes the order in probability, which is the optimal parametric rate for empirical risk minimization without making additional assumptions (Mendelson, 2008).\n\n√\n\n3.3 ROBUSTNESS OF RISK ESTIMATOR\n\nIn the previous subsections, it was assumed that the class prior probability is known in advance or estimated accurately. In addition, it was assumed that the ground-truth confidence difference of each unlabeled data pair is accessible. However, these assumptions can rarely be satisfied in real-world In this scenarios, since the collection of confidence difference is inevitably injected with noise. subsection, we theoretically analyze the influence of an inaccurate class prior probability and noisy confidence difference on the learning procedure. Later in subsection 4.4, we will experimentally verify our theoretical findings. Let ̄D = {((xi, x′ i=1 denote n unlabeled data pairs with noisy confidence difference, where ̄ci is generated by corrupting the ground-truth confidence difference ci with noise. Besides, let ̄π+ denote the inaccurate class prior probability accessible to the learning algorithm. Furthermore, let ̄RCD(g) denote the empirical risk calculated based on the inaccurate class prior probability and noisy ̄RCD(g) denote the minimizer of ̄RCD(g). Then, the confidence difference. Let ̄gCD = arg ming∈G theorem demonstrating an estimation error bound is given as follows:\n\ni), ̄ci)}n\n\nTheorem 4. Based on the assumptions above, for any δ > 0, the following inequality holds with probability at least 1 − δ:\n\nR( ̄gCD) − R(g∗) ≤ 16LlRn(G) + 8Cl\n\n(cid:114)\n\nln 2/δ 2n\n\n+\n\n4Cl\n\n(cid:80)n\n\ni=1 | ̄ci − ci|\n\nn\n\n+ 4Cl| ̄π+ − π+|.\n\n(11)\n\nTheorem 4 indicates that the estimation error is bounded by twice the original bound in Theorem 3 with the mean absolute error of the noisy confidence difference and the inaccurate class prior probability. Furthermore, if (cid:80)n i=1 | ̄ci − ci| has a sublinear growth rate with high probability and the class prior probability is estimated consistently, the risk estimator can be even consistent. It elaborates the robustness of the proposed approach.\n\n3.4 RISK CORRECTION APPROACH\n\nIt is worth noting that the empirical risk in Eq. (8) may be negative due to negative terms, which is unreasonable because of the non-negative property of loss functions. This phenomenon will result in severe overfitting problems when complex models are adopted (Lu et al., 2020; Cao et al., 2021b; Feng et al., 2021). To circumvent this difficulty, we wrap the individual loss terms in Eq. (8) with risk correction functions proposed in Lu et al. (2020), such as the rectified linear unit (ReLU) function f (z) = max(0, z) and the absolute value function f (z) = |z|. In this way, the corrected risk estimator for ConfDiff classification can be expressed as follows:\n\n(cid:101)RCD(g) =\n\n1 2n\n\n(f (\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\n(π+ − ci)l(g(xi), +1)) + f (\n\n+f (\n\n(π+ + ci)l(g(x′\n\ni), +1)) + f (\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\n(π− − ci)l(g(x′\n\ni), −1))\n\n(π− + ci)l(g(xi), −1))).\n\n(12)\n\ni=1\n\ni=1\n\nTheoretical analysis. We assume that the risk correction function f (z) is Lipschitz continuous with Lipschitz constant Lf . For ease of notation, let (cid:98)Ag = (cid:80)n i=1(π+ − ci)l(g(xi), +1)/2n, (cid:98)Bg =\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\ni=1(π+ + ci)l(g(x′\n\ni=1(π− − ci)l(g(x′\n\ni), −1)/2n, (cid:98)Cg = (cid:80)n\n\n(cid:80)n i=1(π− + ci)l(g(xi), −1)/2n. From Lemma 3 in Appendix A, the values of E[ (cid:98)Ag], E[ (cid:98)Bg], E[ (cid:98)Cg], and E[ (cid:98)Dg] are non-negative. Therefore, we assume that there exist non-negative constants a, b, c, d such that E[ (cid:98)Ag] ≥ a, E[ (cid:98)Bg] ≥ b, E[ (cid:98)Cg] ≥ c, and E[ (cid:98)Dg] ≥ d. Besides, let (cid:101)gCD = arg ming∈G (cid:101)RCD(g) denote the minimizer of (cid:101)RCD(g). Then, Theorem 5 is provided to elaborate the bias and consistency of (cid:101)RCD(g).\n\ni), +1)/2n, (cid:98)Dg = (cid:80)n\n\nTheorem 5. Based on the assumptions above, the bias of the risk estimator (cid:101)RCD(g) decays exponentially as n → ∞:\n\n0 ≤ E[ (cid:101)RCD(g)] − R(g) ≤ 2(Lf + 1)Cl∆, where ∆ = exp (−2a2n/C 2 l ) + exp (−2c2n/C 2 l ) + exp (−2b2n/C 2 thermore, with probability at least 1 − δ, we have\n\nl ) + exp (−2d2n/C 2\n\nl ). Fur-\n\n(13)\n\n| (cid:101)RCD(g) − R(g)| ≤ 2ClLf\n\n(cid:114)\n\nln 2/δ 2n\n\n+ 2(Lf + 1)Cl∆.\n\n(14)\n\nTheorem 5 demonstrates that (cid:101)RCD(g) → R(g) in Op(1/ consistent. The estimation error bound of (cid:101)gCD is analyzed in Theorem 6. Theorem 6. Based on the assumptions above, for any δ > 0, the following inequality holds with probability at least 1 − δ:\n\nn), which means (cid:101)RCD(g) is biased yet\n\n√\n\nR((cid:101)gCD) − R(g∗) ≤ 8LlRn(G) + 4Cl(Lf + 1)\n\n(cid:114)\n\nln 2/δ 2n\n\n+ 4(Lf + 1)Cl∆.\n\n(15)\n\nTheorem 6 elucidates that as n → ∞, R((cid:101)gCD) → R(g∗), since Rn(G) → 0 for all parametric models with a bounded norm (Mohri et al., 2012) and ∆ → 0. Furthermore, the estimation error bound converges in Op(1/ n), which is the optimal parametric rate for empirical risk minimization without additional assumptions (Mendelson, 2008).\n\n√\n\n4 EXPERIMENTS\n\nIn this section, we verify the effectiveness of our proposed approaches experimentally.\n\n4.1 EXPERIMENTAL SETUP\n\nWe conducted experiments on benchmark data sets, including MNIST (LeCun et al., 1998), Kuzushiji-MNIST (Clanuwat et al., 2018), Fashion-MNIST (Xiao et al., 2017), and CIFAR-10 (Krizhevsky & Hinton, 2009). In addition, four UCI data sets (Dua & Graff, 2017) were used, including Optdigits, USPS, Pendigits, and Letter. Since the data sets were originally designed for multi-class classification, we manually partitioned them into binary classes. The detailed descriptions of data sets is illustrated in Appendix. For CIFAR-10, we used ResNet-34 (He et al., 2016) as the model architecture. For other data sets, we used a multilayer perceptron (MLP) with three hidden layers of width 300 equipped with the ReLU (Nair & Hinton, 2010) activation function and batch normalization (Ioffe & Szegedy, 2015). The logistic loss is utilized to instantiate the loss function l(·, ·). It is worth noting that confidence difference is given by labelers in real-world applications, while it was generated synthetically in this paper to facilitate comprehensive experimental analysis. We firstly trained a probabilistic classifier via logistic regression with ordinarily labeled data and the same neural network architecture. Then, we sampled unlabeled data in pairs at random, and generated the class posterior probabilities by inputting them into the probabilistic classifier. After that, we generated confidence difference for each pair of sampled data according to Definition 1.\n\nIn the experiments, we adopted the following variants of our proposed approaches: 1) ConfDiffUnbiased, which denotes the method working by minimizing the unbiased risk estimator proposed in Eq. (8); 2) ConfDiff-ReLU, which denotes the method working by minimizing the corrected risk estimator proposed in Eq. (12) with the ReLU function as the risk correction function; 3) ConfDiffABS, which denotes the method working by minimizing the corrected risk estimator proposed in Eq. (12) with the absolute value function as the risk correction function. We compared our proposed\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Classification accuracy (mean±std) of each method on benchmark data sets with different class priors, where the best performance is shown in bold.\n\nClass Prior\n\nMethod\n\nπ+ = 0.2\n\nPcomp-Unbiased Pcomp-ReLU Pcomp-ABS Pcomp-Teacher ConfDiff-Unbiased ConfDiff-ReLU ConfDiff-ABS\n\nClass Prior\n\nMethod\n\nπ+ = 0.5\n\nPcomp-Unbiased Pcomp-ReLU Pcomp-ABS Pcomp-Teacher ConfDiff-Unbiased ConfDiff-ReLU ConfDiff-ABS\n\nClass Prior\n\nMethod\n\nπ+ = 0.8\n\nPcomp-Unbiased Pcomp-ReLU Pcomp-ABS Pcomp-Teacher ConfDiff-Unbiased ConfDiff-ReLU ConfDiff-ABS\n\nMNIST 0.761±0.017 0.800±0.000 0.800±0.000 0.965±0.010 0.789±0.041 0.968±0.003 0.975±0.003 MNIST 0.712±0.020 0.502±0.003 0.842±0.012 0.893±0.014 0.911±0.046 0.944±0.011 0.964±0.001 MNIST 0.799±0.005 0.910±0.031 0.854±0.027 0.943±0.026 0.792±0.017 0.970±0.004 0.983±0.002\n\nKuzushiji 0.637±0.052 0.800±0.000 0.800±0.000 0.871±0.046 0.672±0.053 0.860±0.017 0.898±0.003 Kuzushiji 0.578±0.036 0.502±0.004 0.727±0.006 0.782±0.046 0.712±0.046 0.805±0.015 0.867±0.006 Kuzushiji 0.671±0.029 0.775±0.022 0.838±0.026 0.814±0.027 0.758±0.033 0.886±0.009 0.915±0.001\n\nFashion 0.737±0.050 0.800±0.000 0.800±0.000 0.853±0.017 0.855±0.024 0.964±0.004 0.965±0.002 Fashion 0.723±0.042 0.500±0.000 0.851±0.012 0.903±0.016 0.896±0.036 0.960±0.003 0.967±0.001 Fashion 0.813±0.029 0.897±0.023 0.921±0.017 0.936±0.014 0.810±0.035 0.970±0.002 0.975±0.002\n\nCIFAR-10 0.776±0.023 0.800±0.000 0.800±0.000 0.836±0.019 0.789±0.025 0.844±0.020 0.862±0.015 CIFAR-10 0.703±0.042 0.602±0.032 0.583±0.018 0.779±0.016 0.720±0.024 0.830±0.007 0.843±0.004 CIFAR-10 0.737±0.022 0.851±0.010 0.849±0.007 0.821±0.003 0.794±0.012 0.851±0.012 0.874±0.011\n\napproaches with the following approaches: 1) Pcomp-Unbiased, which denotes the method working by minimizing the unbiased risk estimator for Pcomp classification proposed in Feng et al. (2021); 2) Pcomp-ReLU, which denotes the risk correction approach for Pcomp classification with the ReLU function as the risk correction function; 3) Pcomp-ABS, which denotes the risk correction approach for Pcomp classification with the absolute value function as the risk correction function; 4) PcompTeacher, which denotes the state-of-the-art approach improving the label-noise learning approach RankPruning (Northcutt et al., 2017) with consistency regularization.\n\nThe number of training epoches was set to 200 and we obtained the testing accuracy by averaging the results in the last 10 epoches. The detailed hyperparameters can be found in Appendix. To verify the effectiveness of our approaches under different class prior settings, we set π+ ∈ {0.2, 0.5, 0.8} for all the data sets. For ease of implementation, we assumed that the class prior π+ was known for all the compared methods. We repeated the sampling-and-training procedure for five times, and the mean accuracy as well as the standard deviation were recorded.\n\n4.2 EXPERIMENTAL RESULTS\n\nBenchmark data sets. Table 1 reports detailed experimental results for all the compared methods on four benchmark data sets. Based on Table 1, we can draw the following conclusions: a) On all the cases of benchmark data sets, our proposed ConfDiff-ABS method achieves superior performance against all of the other compared approaches significantly, which validates the effectiveness of our approach in utilizing supervision information from confidence difference; b) Pcomp-Teacher achieves superior performance against all of the other Pcomp approaches by a large margin. The excellent performance benefits from the effectiveness of consistency regularization for weakly supervised learning problems (Berthelot et al., 2019; Li et al., 2020; Wu et al., 2022); c) The risk correction methods for ConfDiff classification, i.e. ConfDiff-ReLU and ConfDiff-ABS, achieve better performance against ConfDiff-Unbiased, which elaborates that the risk correction technique is advantageous; d) It is worth noting that the classification results of ConfDiff-ReLU and ConfDiff-ABS have smaller variances than ConfDiff-Unbiased. It demonstrates that the risk correction method can enhance the stability and robustness for ConfDiff classification.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Classification accuracy (mean±std) of each method on UCI data sets with different class priors, where the best performance is shown in bold.\n\nClass Prior\n\nMethod\n\nπ+ = 0.2\n\nPcomp-Unbiased Pcomp-ReLU Pcomp-ABS Pcomp-Teacher ConfDiff-Unbiased ConfDiff-ReLU ConfDiff-ABS\n\nClass Prior\n\nMethod\n\nπ+ = 0.5\n\nPcomp-Unbiased Pcomp-ReLU Pcomp-ABS Pcomp-Teacher ConfDiff-Unbiased ConfDiff-ReLU ConfDiff-ABS\n\nClass Prior\n\nMethod\n\nπ+ = 0.8\n\nPcomp-Unbiased Pcomp-ReLU Pcomp-ABS Pcomp-Teacher ConfDiff-Unbiased ConfDiff-ReLU ConfDiff-ABS\n\nOptdigits 0.771±0.016 0.800±0.000 0.800±0.001 0.901±0.023 0.831±0.078 0.953±0.014 0.963±0.009 Optdigits 0.651±0.112 0.630±0.076 0.787±0.031 0.890±0.009 0.917±0.006 0.921±0.011 0.962±0.006 Optdigits 0.765±0.023 0.902±0.017 0.894±0.019 0.918±0.007 0.886±0.037 0.949±0.007 0.964±0.005\n\nUSPS 0.721±0.046 0.800±0.000 0.800±0.000 0.894±0.023 0.840±0.078 0.957±0.007 0.960±0.005 USPS 0.671±0.090 0.554±0.048 0.814±0.018 0.860±0.012 0.936±0.010 0.945±0.009 0.959±0.004 USPS 0.746±0.012 0.891±0.024 0.879±0.009 0.933±0.023 0.803±0.042 0.958±0.008 0.964±0.003\n\nPendigits 0.743±0.057 0.800±0.000 0.800±0.000 0.928±0.019 0.865±0.079 0.987±0.003 0.988±0.002 Pendigits 0.748±0.038 0.514±0.019 0.793±0.017 0.883±0.018 0.945±0.052 0.981±0.004 0.988±0.003 Pendigits 0.743±0.026 0.913±0.023 0.911±0.009 0.903±0.008 0.892±0.096 0.986±0.003 0.987±0.002\n\nLetter 0.757±0.028 0.800±0.000 0.800±0.000 0.883±0.006 0.732±0.053 0.929±0.008 0.942±0.007 Letter 0.632±0.019 0.525±0.023 0.748±0.031 0.864±0.024 0.755±0.041 0.895±0.006 0.925±0.003 Letter 0.694±0.031 0.827±0.025 0.870±0.006 0.872±0.011 0.748±0.015 0.927±0.008 0.945±0.007\n\n(a) Kuzushiji\n\n(b) Fashion\n\n(c) USPS\n\n(d) Letter\n\nFigure 1: Classification performance of ConfDiff-ReLU and ConfDiff-ABS given a fraction of training data as well as Pcomp-Teacher given 100% of training data (π+ = 0.2).\n\nUCI data sets. Table 2 reports detailed experimental results on four UCI data sets as well. From Table 2, we can observe that: a) On all the UCI data sets under different class prior probability settings, our proposed ConfDiff-ABS method achieves the best performance among all the compared approaches with significant superiority, which verifies the effectiveness of our proposed approaches again; b) The performance of our proposed approaches is more stable than the compared Pcomp approaches under different class prior probability settings, demonstrating the superiority of our methods in dealing with various kinds of data distributions; c) ConfDiff-Unbiased has comparable performance against its risk correction variants on some data sets while has inferior performance on some other data sets. This is mainly because some data sets have simpler patterns and are thus less affected by overfitting issues.\n\n4.3 PERFORMANCE WITH FEWER TRAINING DATA\n\nTo validate the effectiveness of exploiting the confidence difference, we conducted experiments by changing the fraction of training data for ConfDiff-ReLU and ConfDiff-ABS (100% indicated that all the ConfDiff data were used for training). For comparison, we used 100% of training data for Pcomp-Teacher during the training process. Figure 1 shows the results on four data sets with π+ = 0.2, and more experimental results can be found in Appendix. We can observe that the classification\n\n8\n\n20%40%60%80%100%# ConfDiff data0.600.650.700.750.800.850.900.951.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.750.800.850.900.951.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.8000.8250.8500.8750.9000.9250.9500.9751.000accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.8000.8250.8500.8750.9000.9250.9500.9751.000accuracyConfDiff-ReLUConfDiff-ABSPcomp-TeacherUnder review as a conference paper at ICLR 2023\n\n(a) ConfDiff-Unbiased\n\n(b) ConfDiff-ReLU\n\n(c) ConfDiff-ABS\n\n(d) ConfDiff-Unbiased\n\n(e) ConfDiff-ReLU\n\n(f) ConfDiff-ABS\n\nFigure 2: Classification accuracy on MNIST (the first row) and Pendigits (the second row) with π+ = 0.5 given an inaccurate class prior probability and noisy confidence difference.\n\nperformance of our proposed approaches is still advantageous given a fraction of training data. Our approaches can achieve superior or comparable performance even when only 10% of training data are used. It validates the benefit and effectiveness of leveraging the supervision information of the confidence difference.\n\n4.4 ANALYSIS ON ROBUSTNESS\n\nici denote the noisy confidence difference where ε′\n\nIn this subsection, we investigate the influence of an inaccurate class prior probability and noisy confidence difference on the generalization performance of the proposed approaches. Specifically, let ̄π+ = επ+ denote the corrupted class prior probability with ε being a real number around 1. Let ̄ci = ε′ i is sampled from a normal distribution N (1, σ2). Figure 2 shows the classification performance of our proposed approaches on MNIST and Pendigits (π+ = 0.5) with different ε and σ. We can observe that ConfDiff-ABS is more robust against corruptions compared with ConfDiff-Unbiased and ConfDiff-ReLU. It is demonstrated that with ̄π+ and ̄ci varying in a reasonable range, the performance is generally stable and even still superior against compared approaches. However, the performance degenerates with ε = 0.8 or ε = 1.2 on some data sets, which indicates that it is more important to obtain an accurate estimation of the class prior probability to facilitate model training.\n\n5 CONCLUSION\n\nIn this paper, we dived into a novel weakly supervised learning setting where only unlabeled data pairs equipped with confidence difference were given. To solve the problem, an unbiased risk estimator was derived to perform empirical risk minimization. An estimation error bound was established to show that the optimal parametric convergence rate could be achieved. Furthermore, a risk correction approach was introduced to alleviate overfitting issues. Extensive experimental results validated the superiority of our proposed approaches. In future, it would be promising to apply our approaches in real-world scenarios.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nHan Bao, Gang Niu, and Masashi Sugiyama. Classification from pairwise similarity and unlabeled data. In Proceedings of the 35th International Conference on Machine Learning, pp. 461–470, 2018.\n\nHan Bao, Takuya Shimada, Liyuan Xu, Issei Sato, and Masashi Sugiyama. Pairwise supervision can provably elicit a decision boundary. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics, pp. 2618–2640, 2022.\n\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A. In Advances in Neural\n\nRaffel. Mixmatch: A holistic approach to semi-supervised learning. Information Processing Systems 32, pp. 5050–5060, 2019.\n\nChristopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Gregory N. Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd International Conference on Machine Learning, pp. 89–96, 2005.\n\nYuzhou Cao, Lei Feng, Senlin Shu, Yitian Xu, Bo An, Gang Niu, and Masashi Sugiyama. Multi-\n\nclass classification from single-class data with confidences. CoRR, abs/2106.08864, 2021a.\n\nYuzhou Cao, Lei Feng, Yitian Xu, Bo An, Gang Niu, and Masashi Sugiyama. Learning from In Proceedings of the 38th International Conference on Machine\n\nsimilarity-confidence data. Learning, pp. 1272–1282, 2021b.\n\nZhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: From pairwise approach to listwise approach. In Proceedings of the 24th International Conference on Machine Learning, pp. 129–136, 2007.\n\nOlivier Chapelle, Bernhard Sch ̈olkopf, and Alexander Zien. Semi-Supervised Learning. The MIT\n\nPress, 2006.\n\nXi Chen, Paul N. Bennett, Kevyn Collins-Thompson, and Eric Horvitz. Pairwise ranking aggregation in a crowdsourced setting. In Proceedings of the 6th ACM International Conference on Web Search and Data Mining, pp. 193–202, 2013.\n\nTarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David\n\nHa. Deep learning for classical japanese literature. CoRR, abs/1812.01718, 2018.\n\nTimothee Cour, Ben Sapp, and Ben Taskar. Learning from partial labels. Journal of Machine\n\nLearning Research, 12(May):1501–1536, 2011.\n\nMarthinus C. du Plessis, Gang Niu, and Masashi Sugiyama. Analysis of learning from positive and unlabeled data. In Advances in Neural Information Processing Systems 27, pp. 703–711, 2014.\n\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.\n\nics.uci.edu/ml.\n\nLei Feng, Senlin Shu, Nan Lu, Bo Han, Miao Xu, Gang Niu, Bo An, and Masashi Sugiyama. Pointwise binary classification with pairwise confidence comparisons. In Proceedings of the 38th International Conference on Machine Learning, pp. 3252–3262, 2021.\n\nYanwei Fu, Timothy M. Hospedales, Tao Xiang, Jiechao Xiong, Shaogang Gong, Yizhou Wang, and Yuan Yao. Robust subjective visual property prediction from crowdsourced pairwise labels. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(3):563–577, 2015.\n\nNoah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. In Proceedings of the 31st Conference On Learning Theory, pp. 297–299, 2018.\n\nBo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in Neural Information Processing Systems 31, pp. 8536–8546, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n\nYixuan He, Quan Gan, David Wipf, Gesine D. Reinert, Junchi Yan, and Mihai Cucuringu. Gnnrank: Learning global rankings from pairwise comparisons via directed graph neural networks. In Proceedings of the 39th International Conference on Machine Learning, pp. 8581–8612, 2022.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, pp. 448–456, 2015.\n\nTakashi Ishida, Gang Niu, and Masashi Sugiyama. Binary classification from positive-confidence\n\ndata. In Advances in Neural Information Processing Systems 31, pp. 5921–5932, 2018.\n\nKevin G. Jamieson and Robert D. Nowak. Active ranking using pairwise comparisons. In Advances\n\nin Neural Information Processing Systems 24, pp. 2240–2248, 2011.\n\nDaniel M. Kane, Shachar Lovett, Shay Moran, and Jiapeng Zhang. Active classification with comparison queries. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science, pp. 355–366, 2017.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings\n\nof the 3rd International Conference on Learning Representations, 2015.\n\nAlex Krizhevsky and Geoffrey E. Hinton. Learning multiple layers of features from tiny images.\n\nTechnical report, University of Toronto, 2009.\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n\nJunnan Li, Richard Socher, and Steven C. H. Hoi. Dividemix: Learning with noisy labels as semisupervised learning. In Proceedings of the 8th International Conference on Learning Representations, 2020.\n\nJunnan Li, Caiming Xiong, and Steven C. H. Hoi. Mopro: Webly supervised learning with momentum prototypes. In Proceedings of the 9th International Conference on Learning Representations, 2021.\n\nYu-Feng Li and Zhi-Hua Zhou. Towards making unlabeled data never hurt. IEEE Transactions on\n\nPattern Analysis and Machine Intelligence, 37(1):175–188, 2015.\n\nTie-Yan Liu. Learning to Rank for Information Retrieval. Springer, 2011.\n\nNan Lu, Gang Niu, Aditya K. Menon, and Masashi Sugiyama. On the minimal supervision for training any binary classifier from only unlabeled data. In Proceedings of the 7th International Conference on Learning Representations, 2019.\n\nNan Lu, Tianyi Zhang, Gang Niu, and Masashi Sugiyama. Mitigating overfitting in supervised classification from two unlabeled datasets: A consistent risk correction approach. In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics, pp. 1115–1125, 2020.\n\nShahar Mendelson. Lower bounds for the empirical minimization algorithm. IEEE Transactions on\n\nInformation Theory, 54(8):3797–3803, 2008.\n\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.\n\nThe MIT Press, 2012.\n\nVinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning, pp. 807–814, 2010.\n\nCurtis G. Northcutt, Tailin Wu, and Isaac L. Chuang. Learning with confident examples: Rank In Proceedings of the 33rd Conference on\n\npruning for robust classification with noisy labels. Uncertainty in Artificial Intelligence, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDohyung Park, Joe Neeman, Jin Zhang, Sujay Sanghavi, and Inderjit S. Dhillon. Preference completion: Large-scale collaborative ranking from pairwise comparisons. In Proceedings of the 32nd International Conference on Machine Learning, pp. 1907–1916, 2015.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8026–8037, 2019.\n\nGiorgio Patrini, Alessandro Rozza, Aditya K. Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944–1952, 2017.\n\nNihar B. Shah, Sivaraman Balakrishnan, and Martin J. Wainwright. Feeling the bern: Adaptive estimators for bernoulli probabilities of pairwise comparisons. IEEE Transactions on Information Theory, 65(8):4854–4874, 2019.\n\nKazuhiko Shinoda, Hirotaka Kaji, and Masashi Sugiyama. Binary classification from positive data with skewed confidence. In Proceedings of the 29th International Joint Conferences on Artificial Intelligence, pp. 3328–3334, 2020.\n\nGuangxin Su, Weitong Chen, and Miao Xu. Positive-unlabeled learning from imbalanced data. In Proceedings of the 30th International Joint Conference on Artificial Intelligence, pp. 2995–3001, 2021.\n\nDeng-Bao Wang, Yong Wen, Lujia Pan, and Min-Ling Zhang. Learning from noisy labels with complementary loss functions. In Proceedings of the 35th AAAI Conference on Artificial Intelligence, pp. 10111–10119, 2021.\n\nHaobo Wang, Ruixuan Xiao, Sharon Li, Lei Feng, Gang Niu, Gang Chen, and Junbo Zhao. Pico: Contrastive label disambiguation for partial label learning. In Proceedings of the 10th International Conference on Learning Representations, 2022.\n\nWei Wang and Min-Ling Zhang. Semi-supervised partial label learning via confidence-rated margin maximization. In Advances in Neural Information Processing Systems 33, pp. 6982–6993, 2020.\n\nJiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. In Proceedings of the 10th International Conference on Learning Representations, 2022.\n\nHongwei Wen, Jingyi Cui, Hanyuan Hang, Jiabin Liu, Yisen Wang, and Zhouchen Lin. Leveraged weighted loss for partial label learning. In Proceedings of the 38th International Conference on Machine Learning, pp. 11091–11100, 2021.\n\nDong-Dong Wu, Deng-Bao Wang, and Min-Ling Zhang. Revisiting consistency regularization for In Proceedings of the 39th International Conference on Machine\n\ndeep partial label learning. Learning, pp. 24212–24225, 2022.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: A novel image dataset for bench-\n\nmarking machine learning algorithms. CoRR, abs/1708.07747, 2017.\n\nLiyuan Xu, Junya Honda, Gang Niu, and Masashi Sugiyama. Uncoupled regression from pairwise In Advances in Neural Information Processing Systems 32, pp. 3992–4002,\n\ncomparison data. 2019.\n\nYichong Xu, Hongyang Zhang, Kyle Miller, Aarti Singh, and Artur Dubrawski. Noise-tolerant interactive learning using pairwise comparisons. In Advances in Neural Information Processing Systems 30, pp. 2428–2437, 2017.\n\nYichong Xu, Sivaraman Balakrishnan, Aarti Singh, and Artur Dubrawski. Regression with comparisons: Escaping the curse of dimensionality with ordinal information. The Journal of Machine Learning Research, 21(1):6480–6533, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nTiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, Aditya Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi (Jay) Kang, and Evan Ettinger. Self-supervised learning for large-scale item recommendations. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pp. 4321–4330, 2021.\n\nYu Yao, Tongliang Liu, Bo Han, Mingming Gong, Gang Niu, Masashi Sugiyama, and Dacheng Tao. Rethinking class-prior estimation for positive-unlabeled learning. In Proceedings of the 10th International Conference on Learning Representations, 2022.\n\nShiwei Zeng and Jie Shen. Efficient pac learning from the crowd with pairwise comparisons. In Proceedings of the 39th International Conference on Machine Learning, pp. 25973–25993, 2022.\n\nShuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep learning based recommender system: A survey\n\nand new perspectives. ACM Computing Surveys, 52(1):1–38, 2019.\n\nXiaojin Zhu and Andrew B. Goldberg. Introduction to semi-supervised learning. Synthesis Lectures\n\non Artificial Intelligence and Machine Learning, 3(1):1–130, 2009.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOF OF THEOREM 1\n\nBefore giving the proof of Theorem 1, we begin with the following lemmas:\n\nLemma 2. The confidence difference c(x, x′) can be equivalently expressed as\n\nc(x, x′) =\n\n=\n\nπ+p(x)p+(x′) − π+p+(x)p(x′) p(x)p(x′) π−p−(x)p(x′) − π−p(x)p−(x′) p(x)p(x′)\n\n(16)\n\n(17)\n\nProof. On one hand,\n\nc(x, x′) = p(y′ = 1|x′) − p(y = 1|x) p(x′, y′ = 1) p(x′) π+p+(x′) p(x′)\n\nπ+p+(x) p(x)\n\np(x, y = 1) p(x)\n\n=\n\n=\n\n−\n\n−\n\n=\n\nπ+p(x)p+(x′) − π+p+(x)p(x′) p(x)p(x′)\n\n.\n\nOn the other hand,\n\nc(x, x′) = p(y′ = 1|x′) − p(y = 1|x)\n\n= (1 − p(y′ = 0|x′)) − (1 − p(y = 0|x)) = p(y = 0|x) − p(y′ = 0|x′) p(x′, y = 0) p(x′) π−p−(x′) p(x′)\n\np(x, y = 0) p(x)\n\nπ−p−(x) p(x)\n\n−\n\n−\n\n=\n\n=\n\n=\n\nπ−p−(x)p(x′) − π−p(x)p−(x′) p(x)p(x′)\n\n,\n\nwhich concludes the proof.\n\nLemma 3. The following equations hold:\n\nEp(x,x′)[(π+ − c(x, x′))l(g(x), +1)] = π+Ep+(x)[l(g(x), +1)], Ep(x,x′)[(π− + c(x, x′))l(g(x), −1)] = π−Ep−(x)[l(g(x), −1)], Ep(x,x′)[(π+ + c(x, x′))l(g(x′), +1)] = π+Ep+(x′)[l(g(x′), +1)], Ep(x,x′)[(π− − c(x, x′))l(g(x′), −1)] = π−Ep−(x′)[l(g(x′), −1)].\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Firstly, the proof of Eq. (18) is given:\n\nEp(x,x′)[(π+ − c(x, x′))l(g(x), +1)] (cid:90) (cid:90) π+p(x)p(x′) − π+p(x)p+(x′) + π+p+(x)p(x′)\n\np(x)p(x′)\n\nl(g(x), +1)p(x, x′) dx dx′\n\n(cid:90) (cid:90)\n\n(π+p(x)p(x′) − π+p(x)p+(x′) + π+p+(x)p(x′))l(g(x), +1) dx dx′\n\n(cid:90)\n\n+\n\n(cid:90)\n\n(cid:90)\n\nπ+p(x)l(g(x), +1) dx\n\n(cid:90)\n\n(cid:90)\n\np(x′) dx′ −\n\nπ+p(x)l(g(x), +1) dx\n\n(cid:90)\n\np+(x′) dx′\n\n(cid:90)\n\nπ+p+(x)l(g(x), +1) dx\n\n(cid:90)\n\np(x′) dx′\n\nπ+p(x)l(g(x), +1) dx −\n\n(cid:90)\n\nπ+p(x)l(g(x), +1) dx +\n\n(cid:90)\n\nπ+p+(x)l(g(x), +1) dx\n\nπ+p+(x)l(g(x), +1) dx\n\n=\n\n=\n\n=\n\n=\n\n=\n\n=π+Ep+(x)[l(g(x), +1)].\n\nAfter that, the proof of Eq. (19) is given:\n\nEp(x,x′)[(π− + c(x, x′))l(g(x), −1)] (cid:90) (cid:90) π−p(x)p(x′) + π−p−(x)p(x′) − π−p(x)p−(x′)\n\np(x)p(x′)\n\nl(g(x), −1)p(x, x′) dx dx′\n\n(cid:90) (cid:90)\n\n(π−p(x)p(x′) + π−p−(x)p(x′) − π−p(x)p−(x′))l(g(x), −1) dx dx′\n\n(cid:90)\n\n−\n\n(cid:90)\n\n(cid:90)\n\nπ−p(x)l(g(x), −1) dx\n\n(cid:90)\n\n(cid:90)\n\np(x′) dx′ +\n\nπ−p−(x)l(g(x), −1) dx\n\n(cid:90)\n\np(x′) dx′\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\nπ−p(x)l(g(x), −1) dx\n\nπ−p(x)l(g(x), −1) dx +\n\nπ−p−(x)l(g(x), −1) dx\n\np−(x′) dx′\n\nπ−p−(x)l(g(x), −1) dx −\n\n(cid:90)\n\nπ−p(x)l(g(x), −1) dx\n\n=\n\n=\n\n=\n\n=\n\n=\n\n=π−Ep−(x)[l(g(x), −1)].\n\nIt can be noticed that c(x, x′) = −c(x′, x) and p(x, x′) = p(x′, x). Therefore, it can be deduced naturally that Ep(x,x′)[(π+ − c(x, x′))l(g(x), +1)] = Ep(x′,x)[(π+ + c(x′, x))l(g(x), +1)]. Because x and x′ are symmetric, we can swap them and deduce Eq. (20). Eq. (21) can be deduced in the same manner, which concludes the proof.\n\nBased on Lemma 3, the proof of Theorem 1 is given.\n\nProof of Theorem 1. To begin with, =\nEp+(x′)[l(g(x′), +1)] and Ep−(x)[l(g(x), −1)] = Ep−(x′)[l(g(x′), −1)]. Then, by summing up all the equations from Eq. (18) to Eq. (21), we can get the following equation:\n\ncan be noticed that Ep+(x)[l(g(x), +1)]\n\nit\n\nEp(x,x′)[L+(g(x), g(x′)) + L−(g(x), g(x′))] = 2π+Ep+(x)[l(g(x), +1)] + 2π−Ep−(x)[l(g(x), −1)]\n\nAfter dividing each side of the equation above by 2, we can obtain Theorem 1.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nB ANALYSIS ON VARIANCE OF RISK ESTIMATOR\n\nB.1 PROOF OF LEMMA 1\n\nBased on Lemma 3, it can be observed that\n\nEp(x,x′)[L(x, x′)] =Ep(x,x′)[(π+ − c(x, x′))l(g(x), +1) + (π− − c(x, x′))l(g(x′), −1)] =π+Ep+(x)[l(g(x), +1)] + π−Ep−(x′)[l(g(x′), −1)] =π+Ep+(x)[l(g(x), +1)] + π−Ep−(x)[l(g(x), −1)] =R(g)\n\nand\n\nEp(x,x′)[L(x′, x)] =Ep(x,x′)[(π+ + c(x, x′))l(g(x′), +1) + (π− + c(x, x′))l(g(x), −1)] =π−Ep−(x)[l(g(x), −1)] + π+Ep+(x′)[l(g(x′), +1)] =π−Ep−(x)[l(g(x), −1)] + π+Ep+(x)[l(g(x), +1)] =R(g).\n\nTherefore, for an arbitrary weight α ∈ [0, 1],\n\nR(g) =αR(g) + (1 − α)R(g)\n\n=αEp(x,x′)[L(x, x′)] + (1 − α)Ep(x,x′)[L(x′, x)],\n\nwhich indicates that\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(αL(xi, x′\n\ni) + (1 − α)L(x′\n\ni, xi))\n\nis also an unbiased risk estimator and concludes the proof.\n\nB.2 PROOF OF THEOREM 2\n\nIn this subsection, we show that Eq. (8) achieves the minimum variance of\n\nS(g; α) =\n\n1 n\n\nn (cid:88)\n\n(αL(xi, x′\n\ni) + (1 − α)L(x′\n\ni, xi))\n\ni=1\n\nw.r.t. any α ∈ [0, 1]. To begin with, we introduce the following notations: n\n(cid:88)\n\nn (cid:88)\n\nμ1 ≜ Ep(x,x′)[(\n\nL(xi, x′\n\ni))2] = Ep(x,x′)[(\n\nL(x′\n\ni, xi))2],\n\n1 n\n\n1 n\n\nμ2 ≜ Ep(x,x′)[\n\n1 n2\n\ni=1 n\n(cid:88)\n\ni=1\n\nL(xi, x′ i)\n\nn (cid:88)\n\ni=1\n\nL(x′\n\ni, xi)].\n\ni=1\n\n(22)\n\nFurthermore, according to Lemma 1, we have\n\nEp(x,x′)[S(g; α)] = R(g).\n\nThen, we provide the proof of Theorem 2 as follows.\n\nProof of Theorem 2.\n\nVar(S(g; α)) =Ep(x,x′)[(S(g; α) − R(g))2] =Ep(x,x′)[S(g; α)2] − R(g)2\n\n=α2Ep(x,x′)[(\n\n1 n\n\nn (cid:88)\n\ni=1\n\nL(xi, x′\n\ni))2] + (1 − α)2Ep(x,x′)[(\n\n1 n\n\nn (cid:88)\n\ni=1\n\nL(x′\n\ni, xi))2]\n\n+ 2α(1 − α)Ep(x,x′)[\n\n1 n2\n\nn (cid:88)\n\ni=1\n\nL(xi, x′ i)\n\nn (cid:88)\n\ni=1\n\nL(x′\n\ni, xi)] − R(g)2\n\n=μ1α2 + μ1(1 − α)2 + 2μ2α(1 − α) − R(g)2\n\n=(2μ1 − 2μ2)(α −\n\n1 2\n\n)2 +\n\n1 2\n\n(μ1 + μ2) − R(g)2.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nBesides, it can be observed that\n\n2μ1 − 2μ2 = Ep(x,x′)[(\n\n1 n\n\nn (cid:88)\n\n(L(xi, x′\n\ni) − L(x′\n\ni, xi))2] ≥ 0.\n\ni=1\n\nTherefore, Var(S(g; α)) achieves the minimum value when α = 1/2, which concludes the proof.\n\nC PROOF OF THEOREM 3\n\nTo begin with, we give the definition of Rademacher complexity.\n\nDefinition 2 (Rademacher complexity). Let Xn = {x1, · · · xn} denote n i.i.d. random variables drawn from a probability distribution with density p(x), G = {g : X (cid:55)→ R} denote a class of measurable functions, and σ = (σ1, σ2, · · · , σn) denote Rademacher variables taking values from {+1, −1} uniformly. Then, the (expected) Rademacher complexity of G is defined as\n\nRn(G) = EXn\n\nEσ\n\n(cid:34)\n\n1 n\n\nn (cid:88)\n\ni=1\n\nsup g∈G\n\n(cid:35)\n\nσig(xi)\n\n.\n\n(23)\n\nLet Dn L(x′, x))/2, then we introduce the following lemma.\n\ni.i.d.∼ p(x, x′) denote n pairs of ConfDiff data and LCD(g; xi, x′\n\ni) = (L(x, x′) +\n\nLemma 4.\n\n ̄Rn(LCD ◦ G) ≤ 2LlRn(G), where LCD ◦ G = {LCD ◦ g|g ∈ G} and ̄Rn(·) is the Rademacher complexity over ConfDiff data pairs Dn of size n.\n\nProof.\n\n ̄Rn(LCD ◦ G) =EDn\n\nEσ[sup\n\ng∈G\n\n1 n\n\nn (cid:88)\n\nσiLCD(g; xi, x′\n\ni)]\n\n=EDn\n\nEσ[sup\n\ng∈G\n\ni=1 n\n(cid:88)\n\ni=1\n\n1 2n\n\nσi((π+ − ci)l(g(xi), +1) + (π− − ci)l(g(x′\n\ni), −1)\n\n+ (π+ + ci)l(g(x′\n\ni), +1) + (π− + ci)l(g(xi), −1))].\n\nThen, we can induce that\n\n∥∇LCD(g; xi, x′\n\ni)∥2\n\n=∥∇(\n\n(π+ − ci)l(g(xi), +1) + (π− − ci)l(g(x′\n\ni), −1)\n\n2\n\n(π+ + ci)l(g(x′\n\ni), +1) + (π− + ci)l(g(xi), −1)\n\n)∥2\n\n+\n\n≤∥∇(\n\n2 (π+ − ci)l(g(xi), +1) 2\n(π+ + ci)l(g(x′\n\ni), +1)\n\n+ ∥∇(\n\n2\n\n)∥2 + ∥∇(\n\n(π− − ci)l(g(x′\n\ni), −1)\n\n2\n\n)∥2\n\n≤\n\n|π+ − ci|Ll 2\n\n+\n\n|π− − ci|Ll 2\n\n+\n\n|π+ + ci|Ll 2\n\n)∥2 + ∥∇(\n\n(π− + ci)l(g(xi), −1) 2\n|π− + ci|Ll 2\n\n+\n\n.\n\n)∥2\n\n(24)\n\nthe value of RHS of Eq.\n\nSuppose π+ ≥ π−, (24) can be determined as follows: when ci ∈ [−1, −π+), the value is −2ciLl; when ci ∈ [−π+, −π−), the value is (π+ − ci)Ll; when ci ∈ [−π−, π−), the value is Ll; when ci ∈ [π−, π+), the value is (π+ + ci)Ll; when ci ∈ [π+, 1], the value is 2ciLl. To sum up, when π+ ≥ π−, the value of RHS of Eq. (24) is less than 2Ll.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nWhen π+ ≤ π−, we can deduce that the value of RHS of Eq. (24) is less than 2Ll in the same way. Therefore,\n\n ̄Rn(LCD ◦ G) ≤2LlEDn\n\nEσ[sup\n\ng∈G\n\n=2LlEXn\n\nEσ[sup\n\ng∈G\n\n=2LlRn(G),\n\n1 n\n\n1 n\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\ni=1\n\nσig(xi)]\n\nσig(xi)]\n\nwhich concludes the proof.\n\nAfter that, we introduce the following lemma. Lemma 5. The inequality below hold with probability at least 1 − δ: (cid:114)\n\n|R(g) − (cid:98)RCD(g)| ≤ 4LlRn(G) + 2Cl\n\nsup g∈G\n\nln 2/δ 2n\n\n.\n\nProof. To begin with, we introduce Φ = supg∈G(R(g) − (cid:98)RCD(g)) and ̄Φ = supg∈G(R(g) − (cid:98) ̄RCD(g)), where (cid:98)RCD(g) and (cid:98) ̄RCD(g) denote the empirical risk over two sets of training examples i), ci} and {( ̄xi, ̄x′ with exactly one different point {(xi, x′ i)} respectively. Then we have ( (cid:98)RCD(g) − (cid:98) ̄RCD(g))\n\n ̄Φ − Φ ≤ sup\n\ni), c( ̄xi, ̄x′\n\ng∈G\n\nLCD(g; xi, x′\n\ni) − LCD(g; ̄xi, ̄x′ i)\n\nn\n\n)\n\n( ≤ sup g∈G 2Cl n\n\n≤\n\n.\n\nAccordingly, Φ − ̄Φ can be bounded in the same way. The following inequalities holds with probability at least 1 − δ/2 by applying McDiarmid’s inequality:\n\nsup g∈G\n\n(R(g) − (cid:98)RCD(g)) ≤ EDn[sup\n\n(R(g) − (cid:98)RCD(g))] + 2Cl\n\ng∈G\n\n(cid:114)\n\nln 2/δ 2n\n\n,\n\nFurthermore, we can bound EDn [supg∈G(R(g) − (cid:98)RCD(g))] with Rademacher complexity. It is a routine work to show by symmetrization (Mohri et al., 2012) that\n\nEDn [sup\n\n(R(g) − (cid:98)RCD(g))] ≤ 2 ̄Rn(LCD ◦ G) ≤ 4LlRn(G),\n\ng∈G\n\nwhere the second inequality is from Lemma 4. Accordingly, supg∈G( (cid:98)RCD(g) − R(g)) has the same bound. By using the union bound, the following inequality holds with probability at least 1 − δ:\n\nsup g∈G which concludes the proof.\n\n|R(g) − (cid:98)RCD(g)| ≤ 4LlRn(G) + 2Cl\n\n(cid:114)\n\nln 2/δ 2n\n\n,\n\nFinally, the proof of Theorem 3 is provided.\n\nProof of Theorem 3. R((cid:98)gCD) − R(g∗) = (R((cid:98)gCD) − (cid:98)RCD((cid:98)gCD)) + ( (cid:98)RCD((cid:98)gCD) − (cid:98)RCD(g∗)) + ( (cid:98)RCD(g∗) − R(g∗))\n\n≤ (R((cid:98)gCD) − (cid:98)RCD((cid:98)gCD)) + ( (cid:98)RCD(g∗) − R(g∗)) (cid:12) (cid:12) (cid:12) (cid:98)RCD(g∗) − R(g∗) (cid:12) (cid:12) ≤ |R((cid:98)gCD) − (cid:98)RCD((cid:98)gCD)| + (cid:12)\n\n≤ 2 sup g∈G\n\n|R(g) − (cid:98)RCD(g)|\n\n≤ 8LlRn(G) + 4Cl\n\n(cid:114)\n\nln 2/δ 2n\n\n.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nThe first inequality is derived because (cid:98)gCD is the minimizer of (cid:98)RCD(g). The last inequality is derived according to Lemma 5, which concludes the proof.\n\nD PROOF OF THEOREM 4\n\nTo begin with, we provide the following inequality:\n\nsup g∈G\n\n1 2n\n\n|\n\n=\n\n| ̄RCD(g) − (cid:98)RCD(g)|\n\nn (cid:88)\n\ni=1\n\n(( ̄π+ − π+ + ci − ̄ci)l(g(xi), +1) + ( ̄π− − π− + ci − ̄ci)l(g(x′\n\ni), −1)\n\n+ ( ̄π+ − π+ + ̄ci − ci)l(g(x′\n\ni), +1) + ( ̄π− − π− + ̄ci − ci)l(g(xi), −1))|\n\n≤\n\n1 2n\n\nn (cid:88)\n\ni=1\n\n(|( ̄π+ − π+ + ci − ̄ci)l(g(xi), +1)| + |( ̄π− − π− + ci − ̄ci)l(g(x′\n\ni), −1)|\n\n+ |( ̄π+ − π+ + ̄ci − ci)l(g(x′\n\ni), +1)| + |( ̄π− − π− + ̄ci − ci)l(g(xi), −1)|)\n\n=\n\n1 2n\n\nn (cid:88)\n\ni=1\n\n(| ̄π+ − π+ + ci − ̄ci|l(g(xi), +1) + | ̄π− − π− + ci − ̄ci|l(g(x′\n\ni), −1)\n\n+ | ̄π+ − π+ + ̄ci − ci|l(g(x′\n\ni), +1) + | ̄π− − π− + ̄ci − ci|l(g(xi), −1))\n\n≤\n\n1 2n\n\nn (cid:88)\n\ni=1\n\n((| ̄π+ − π+| + |ci − ̄ci|)l(g(xi), +1) + (| ̄π− − π−| + |ci − ̄ci|)l(g(x′\n\ni), −1)\n\n+ (| ̄π+ − π+| + | ̄ci − ci|)l(g(x′\n\ni), +1) + (| ̄π− − π−| + | ̄ci − ci|)l(g(xi), −1))\n\n=\n\n1 2n\n\nn (cid:88)\n\ni=1\n\n((| ̄π+ − π+| + |ci − ̄ci|)l(g(xi), +1) + (|π+ − ̄π+| + |ci − ̄ci|)l(g(x′\n\ni), −1)\n\n+ (| ̄π+ − π+| + | ̄ci − ci|)l(g(x′ 2Cl\n\ni=1 | ̄ci − ci|\n\n(cid:80)n\n\n+ 2Cl| ̄π+ − π+|.\n\n≤\n\nn\n\ni), +1) + (|π+ − ̄π+| + | ̄ci − ci|)l(g(xi), −1))\n\nThen, we deduce the following inequality: R( ̄gCD) − R(g∗) =(R( ̄gCD) − (cid:98)RCD( ̄gCD)) + ( (cid:98)RCD( ̄gCD) − ̄RCD( ̄gCD)) + ( ̄RCD( ̄gCD) − ̄RCD((cid:98)gCD))\n\n+ ( ̄RCD((cid:98)gCD) − (cid:98)RCD((cid:98)gCD)) + ( (cid:98)RCD((cid:98)gCD) − R((cid:98)gCD)) + (R((cid:98)gCD) − R(g∗))\n\n≤2 sup g∈G\n\n|R(g) − (cid:98)RCD(g)| + 2 sup g∈G\n\n| ̄RCD(g) − (cid:98)RCD(g)| + (R((cid:98)gCD) − R(g∗))\n\n≤4 sup g∈G\n\n|R(g) − (cid:98)RCD(g)| + 2 sup g∈G\n\n| ̄RCD(g) − (cid:98)RCD(g)|\n\n≤16LlRn(G) + 8Cl\n\n(cid:114)\n\nln 2/δ 2n\n\n+\n\n4Cl\n\n(cid:80)n\n\ni=1 | ̄ci − ci|\n\nn\n\n+ 4Cl| ̄π+ − π+|.\n\nThe first inequality is derived because ̄gCD is the minimizer of ̄R(g). The second and third inequality are derived according to the proof of Theorem 3 and Lemma 5 respectively.\n\nE PROOF OF THEOREM 5\n\nn (g) = {Dn| (cid:98)A(g) ≥ 0 ∩ (cid:98)B(g) ≥ 0 ∩ (cid:98)C(g) ≥ 0 ∩ (cid:98)D(g) ≥ 0} and D−\n\nTo begin with, let D+ n (g) = {Dn| (cid:98)A(g) ≤ 0 ∪ (cid:98)B(g) ≤ 0 ∪ (cid:98)C(g) ≤ 0 ∪ (cid:98)D(g) ≤ 0}. Before giving the proof of Theorem 5, we give the following lemma based on the assumptions in section 3. Lemma 6. The probability measure of D−\n\nn (g) can be bounded as follows:\n\nP(D−\n\nn (g)) ≤ exp (\n\n−2a2n C 2 l\n\n) + exp (\n\n−2b2n C 2 l\n\n19\n\n) + exp (\n\n−2c2n C 2 l\n\n) + exp (\n\n−2d2n C 2 l\n\n).\n\n(25)\n\nUnder review as a conference paper at ICLR 2023\n\nProof. It can be observed that\n\np(Dn) = p(x1, x′\n\n1) · · · p(xn, x′\n\nn) n)p(x1) · · · p(x′\n\nn).\n\n= p(x1) · · · p(x′\n\nTherefore, the probability measure P(D−\n\nn (g)) can be defined as follows:\n\nP(D−\n\nn (g)) =\n\n=\n\n(cid:90)\n\nDn∈D−\n\nn (g)\n\n(cid:90)\n\nDn∈D−\n\nn (g)\n\np(Dn) dDn\n\np(Dn) dx1 · · · dxn dx′\n\n1 · · · dx′\n\nn.\n\nWhen exactly one ConfDiff data pair in Sn is replaced, the change of (cid:98)A(g), (cid:98)B(g), (cid:98)C(g) and (cid:98)D(g) will be no more than Cl/n. By applying McDiarmid’s inequality, we can obtain the following inequalities:\n\nP(E[ (cid:98)A(g)] − (cid:98)A(g) ≥ a) ≤ exp (\n\nP(E[ (cid:98)B(g)] − (cid:98)B(g) ≥ b) ≤ exp (\n\nP(E[ (cid:98)C(g)] − (cid:98)C(g) ≥ c) ≤ exp (\n\nP(E[ (cid:98)D(g)] − (cid:98)D(g) ≥ d) ≤ exp (\n\n−2a2n C 2 l\n−2b2n C 2 l\n−2c2n C 2 l\n−2d2n C 2 l\n\n),\n\n),\n\n),\n\n).\n\nFurthermore,\n\nP(D−\n\nn (g) ≤P( (cid:98)A(g) ≤ 0) + P( (cid:98)B(g) ≤ 0) + P( (cid:98)C(g) ≤ 0) + P( (cid:98)D(g) ≤ 0)\n\n≤P( (cid:98)A(g) ≤ E[ (cid:98)A(g)] − a) + P( (cid:98)B(g) ≤ E[ (cid:98)B(g)] − b)\n\n+ P( (cid:98)C(g) ≤ E[ (cid:98)C(g)] − c) + P( (cid:98)D(g) ≤ E[ (cid:98)D(g)] − d)\n\n≤P(E[ (cid:98)A(g)] − (cid:98)A(g) ≥ a) + P(E[ (cid:98)B(g)] − (cid:98)B(g) ≥ b)\n\n+ P(E[ (cid:98)C(g)] − (cid:98)C(g) ≥ c) + P(E[ (cid:98)D(g)] − (cid:98)D(g) ≥ d)\n\n≤ exp (\n\n−2a2n C 2 l\n\n) + exp (\n\n−2b2n C 2 l\n\n) + exp (\n\n−2c2n C 2 l\n\n) + exp (\n\n−2d2n C 2 l\n\n),\n\nwhich concludes the proof.\n\nThen, the proof of Theorem 5 is given.\n\nProof of Theorem 5. To begin with, we prove the first inequality in Theorem 5.\n\nE[ (cid:101)RCD(g)] − R(g) =E[ (cid:101)RCD(g) − (cid:98)RCD(g)]\n\n(cid:90)\n\n+\n\n(cid:90)\n\n=\n\n=\n\nDn∈D+\n\nn (g)\n\n(cid:90)\n\nDn∈D−\n\nn (g)\n\nDn∈D−\n\nn (g)\n\n( (cid:101)RCD(g) − (cid:98)RCD(g))p(Dn) dDn\n\n( (cid:101)RCD(g) − (cid:98)RCD(g))p(Dn) dDn\n\n( (cid:101)RCD(g) − (cid:98)RCD(g))p(Dn) dDn ≥ 0,\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nwhere the last inequality is derived because (cid:101)RCD(g) is an upper bound of (cid:98)RCD(g). Furthermore,\n\nE[ (cid:101)RCD(g)] − R(g) (cid:90)\n\n=\n\n( (cid:101)RCD(g) − (cid:98)RCD(g))p(Dn) dDn\n\nDn∈D−\n\nn (g)\n\n≤ sup Dn∈D−\n\nn (g)\n\n( (cid:101)RCD(g) − (cid:98)RCD(g))\n\n(cid:90)\n\nDn∈D−\n\nn (g)\n\np(Dn) dDn\n\n= sup Dn∈D−\n\nn (g)\n\n( (cid:101)RCD(g) − (cid:98)RCD(g))P(D−\n\nn (g))\n\n(f ( (cid:98)A(g)) + f ( (cid:98)B(g)) + f ( (cid:98)C(g)) + f ( (cid:98)D(g))\n\n= sup Dn∈D− − (cid:98)A(g) − (cid:98)B(g) − (cid:98)C(g) − (cid:98)D(g))P(D−\n\nn (g)\n\nn (g))\n\n(Lf | (cid:98)A(g)| + Lf | (cid:98)B(g)| + Lf | (cid:98)C(g)| + Lf | (cid:98)D(g)|\n\n≤ sup Dn∈D− + | (cid:98)A(g)| + | (cid:98)B(g)| + | (cid:98)C(g)| + | (cid:98)D(g)|)P(D−\n\nn (g)\n\nn (g)\n\n= sup Dn∈D−\n\nn (g)\n\nLf + 1 2n\n\n(|\n\nn (cid:88)\n\n(π+ − ci)l(g(xi), +1)| + |\n\nn (cid:88)\n\n(π− − ci)l(g(x′\n\ni), −1)|\n\ni=1\n\ni=1\n\n+ |\n\nn (cid:88)\n\ni=1\n\n(π+ + ci)l(g(x′\n\ni), +1)| + |\n\nn (cid:88)\n\ni=1\n\n(π− + ci)l(g(xi), −1)|)P(D−\n\nn (g))\n\nn (g)\n\n≤ sup Dn∈D− n\n(cid:88)\n\n+\n\ni=1\n\nLf + 1 2n\n\nn (cid:88) (\n\ni=1\n\n|(π+ − ci)l(g(xi), +1)| +\n\nn (cid:88)\n\ni=1\n\n|(π− − ci)l(g(x′\n\ni), −1)|\n\n|(π+ + ci)l(g(x′\n\ni), +1)| +\n\nn (cid:88)\n\ni=1\n\n|(π− + ci)l(g(xi), −1)|)P(D−\n\nn (g))\n\nLf + 1 2n\n\n= sup Dn∈D− + |(π+ + ci)l(g(x′\n\nn (g)\n\nn (cid:88)\n\ni=1\n\n(|(π+ − ci)l(g(xi), +1)| + |(π− − ci)l(g(x′\n\ni), −1)|\n\ni), +1)| + |(π− + ci)l(g(xi), −1)|)P(D−\n\nn (g))\n\n≤ sup Dn∈D−\n\nn (g)\n\n(Lf + 1)Cl 2n\n\nn (cid:88)\n\ni=1\n\n(|π+ − ci| + |π− − ci| + |π+ + ci| + |π− + ci|)P(D−\n\nn (g)).\n\nSimilar to the proof of Theorem 3, we can obtain\n\n|π+ − ci| + |π− − ci| + |π+ + ci| + |π− + ci| ≤ 4.\n\nTherefore, we have\n\nE[ (cid:101)RCD(g)] − R(g) ≤ 2(Lf + 1)Cl∆, which concludes the proof of the first inequality in Theorem 5. Before giving the proof of the second inequality, we give the upper bound of | (cid:101)RCD(g) − E[ (cid:101)RCD(g)]|. When exactly one ConfDiff data pair in Dn is replaced, the change of (cid:101)RCD(g) is no more than 2ClLf /n. By applying McDiarmid’s inequality, we have the following inequalities with probability at least 1 − δ/2:\n\n(cid:101)RCD(g) − E[ (cid:101)RCD(g)] ≤ 2ClLf\n\nE[ (cid:101)RCD(g)] − (cid:101)RCD(g) ≤ 2ClLf\n\n(cid:114)\n\n(cid:114)\n\nln 2/δ 2n\n\nln 2/δ 2n\n\n,\n\n.\n\nTherefore, with probability at least 1 − δ, we have\n\n| (cid:101)RCD(g) − E[ (cid:101)RCD(g)]| ≤ 2ClLf\n\n(cid:114)\n\nln 2/δ 2n\n\n.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Characteristics of experimental data sets.\n\nData Set MNIST Kuzushiji Fashion CIFAR-10 Optdigits USPS Pendigits Letter\n\n# Train 60,000 60,000 60,000 50,000 4,495 7,437 8,793 16,000\n\n# Test 10,000 10,000 10,000 10,000 1,125 1,861 2,199 4,000\n\n# Features 784 784 784 3,072 62 256 16 16\n\n# Class Labels 10 10 10 10 10 10 10 26\n\nModel MLP MLP MLP ResNet-34 MLP MLP MLP MLP\n\nFinally, we have\n\n| (cid:101)RCD(g) − R(g)| = | (cid:101)RCD(g) − E[ (cid:101)RCD(g)] + E[ (cid:101)RCD(g)] − R(g)|\n\n≤ | (cid:101)RCD(g) − E[ (cid:101)RCD(g)]| + |E[ (cid:101)RCD(g)] − R(g)| = | (cid:101)RCD(g) − E[ (cid:101)RCD(g)]| + E[ (cid:101)RCD(g)] − R(g)\n\n≤ 2ClLf\n\n(cid:114)\n\nln 2/δ 2n\n\n+ 2(Lf + 1)Cl∆,\n\n(26)\n\nwith probability at least 1 − δ, which concludes the proof.\n\nF PROOF OF THEOREM 6\n\nWith probability at least 1 − δ, we have\n\nR((cid:101)gCD) − R(g∗) =(R((cid:101)gCD) − (cid:101)RCD((cid:101)gCD)) + ( (cid:101)RCD((cid:101)gCD) − (cid:101)RCD((cid:98)gCD))\n\n+ ( (cid:101)RCD((cid:98)gCD) − R((cid:98)gCD)) + (R((cid:98)gCD) − R(g∗))\n\n≤|R((cid:101)gCD) − (cid:101)RCD((cid:101)gCD)| + | (cid:101)RCD((cid:98)gCD) − R((cid:98)gCD)| + (R((cid:98)gCD) − R(g∗))\n\n(cid:114)\n\n≤4Cl(Lf + 1)\n\nln 2/δ 2n\n\n+ 4(Lf + 1)Cl∆ + 8LlRn(G).\n\nThe first inequality is derived because (cid:101)gCD is the minimizer of (cid:101)RCD(g). The second inequality is derived from Theorem 5 and Theorem 3. The proof is completed.\n\nG ADDITIONAL INFORMATION ON EXPERIMENTS\n\nIn this section, the details of experimental data sets and hyperparameters are provided.\n\nG.1 DETAILS OF EXPERIMENTAL DATA SETS\n\nThe detailed statistics and corresponding model architectures are summarized in Table 3 while the basic information, sources and data split details are elaborated in this subsection.\n\nFor the four benchmark data sets,\n\n• MNIST (LeCun et al., 1998): It is a grayscale handwritten digits recognition data set. It is composed of 60,000 training examples and 10,000 test examples. The original feature dimension is 28*28, and the label space is 0-9. The even digits are regarded as the positive class while the odd digits are regarded as the negative class. We sampled 15,000 unlabeled data pairs as training data. The data set can be downloaded from http://yann.lecun.com/exdb/mnist/.\n\n• Kuzushiji-MNIST (Clanuwat et al., 2018): It is a grayscale Japanese character recognition data set. It is composed of 60,000 training examples and 10,000 test examples. The original feature dimension is 28*28, and the label space is {‘o’, ‘su’,‘na’, ‘ma’, ‘re’, ‘ki’,‘tsu’,‘ha’, ‘ya’,‘wo’}.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nThe positive class is composed of ‘o’, ‘su’,‘na’, ‘ma’, and ‘re’ while the negative class is composed of ‘ki’,‘tsu’,‘ha’, ‘ya’, and ‘wo’. We sampled 15,000 unlabeled data pairs as training data. The data set can be downloaded from https://github.com/rois-codh/kmnist. • Fashion-MNIST (Xiao et al., 2017): It is a grayscale fashion item recognition data set.\n\nIt is composed of 60,000 training examples and 10,000 test examples. The original feature dimension is 28*28, and the label space is {‘T-shirt’, ‘trouser’, ‘pullover’, ‘dress’, ‘sandal’, ‘coat’, ‘shirt’, ‘sneaker’, ‘bag’, ‘ankle boot’}. The positive class is composed of ‘T-shirt’, ‘pullover’, ‘coat’, ‘shirt’, and ‘bag’ while the negative class is composed of ‘trouser’, ‘dress’, ‘sandal’, ‘sneaker’, and ‘ankle boot’. We sampled 15,000 unlabeled data pairs as training data. The data set can be downloaded from https://github.com/zalandoresearch/fashion-mnist.\n\n• CIFAR-10 (Krizhevsky & Hinton, 2009): It is a colorful object recognition data set. It is composed of 50,000 training examples and 10,000 test examples. The original feature dimension is 32*32*3, and the label space is {‘airplane’, ‘bird’, ‘automobile’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’}. The positive class is composed of ‘bird’, ‘deer’, ‘dog’, ‘frog’, ‘cat’, and ‘horse’ while the negative class is composed of ‘airplane’, ‘automobile’, ‘ship’, and ‘truck’. We sampled 10,000 unlabeled data pairs as training data. The data set can be downloaded from https://www.cs.toronto.edu/ ̃kriz/cifar.html.\n\nFor the four UCI data sets, they can be downloaded from Dua & Graff (2017).\n\n• Optdigits, USPS, Pendigits (Dua & Graff, 2017): They are handwritten digit recognition data set. The train-test split can be found in Table 3. The feature dimensions are 62, 256, and 16 respectively and the label space is 0-9. The even digits are regarded as the positive class while the odd digits are regarded as the negative class. We sampled 1,200, 2,000, and 2,500 unlabeled data pairs for training respectively.\n\n• Letter (Dua & Graff, 2017): It is a letter recognition data set. It is composed of 16,000 training examples and 4,000 test examples. The feature dimension is 16 and the label space is the 26 capital letters in the English alphabet. The positive class is composed of the top 13 letters while the negative class is composed of the latter 13 letters. We sampled 4,000 unlabeled data pairs for training.\n\nG.2 DETAILS OF HYPERPARAMETERS\n\nAll the methods were implemented in Pytorch (Paszke et al., 2019). We used the Adam optimizer (Kingma & Ba, 2015). To ensure fair comparisons, We set the same hyperparameter values for all the comparing approaches.\n\nFor MNIST, Kuzushiji-MNIST and Fashion-MNIST, the learning rate was set to 1e-3 and the weight decay was set to 1e-5. The batch size was set to 256 data pairs. For training the probabilistic classifier to generate confidence, the batch size was set to 256 and the epoch number was set to 10.\n\nFor CIFAR10, the learning rate was set to 5e-4 and the weight decay was set to 1e-5. The batch size was set to 128 data pairs. For training the probabilistic classifier to generate confidence, the batch size was set to 128 and the epoch number was set to 10.\n\nFor all the UCI data sets, the learning rate was set to 1e-3 and the weight decay was set to 1e-5. The batch size was set to 128 data pairs. For training the probabilistic classifier to generate confidence, the batch size was set to 128 and the epoch number was set to 10.\n\nThe learning rate and weight decay for training the probabilistic classifier were the same as the setting for each data set correspondingly.\n\nH MORE EXPERIMENTAL RESULTS WITH FEWER TRAINING DATA\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Optdigits\n\n(b) Pendigits\n\n(c) MNIST\n\n(d) CIFAR-10\n\n(e) Kuzushiji\n\n(f) Fashion\n\n(g) USPS\n\n(h) Letter\n\n(i) Optdigits\n\n(j) Pendigits\n\n(k) MNIST\n\n(l) CIFAR-10\n\n(m) Kuzushiji\n\n(n) Fashion\n\n(o) USPS\n\n(p) Letter\n\n(q) Optdigits\n\n(r) Pendigits\n\n(s) MNIST\n\n(t) CIFAR-10\n\nFigure 3: Classification performance of ConfDiff-ReLU and ConfDiff-ABS given a fraction of training data as well as Pcomp-Teacher given 100% of training data with different prior settings (π+ = 0.2 for the first row, π+ = 0.5 for the second and the third row, and π+ = 0.8 for the fourth and the fifth row).\n\n24\n\n20%40%60%80%100%# ConfDiff data0.750.800.850.900.951.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.860.880.900.920.940.960.981.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.860.880.900.920.940.960.981.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.700.750.800.850.900.951.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.600.650.700.750.800.850.900.951.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.8000.8250.8500.8750.9000.9250.9500.9751.000accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.750.800.850.900.951.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.750.800.850.900.951.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.750.800.850.900.951.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.8000.8250.8500.8750.9000.9250.9500.9751.000accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.8000.8250.8500.8750.9000.9250.9500.9751.000accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.50.60.70.80.91.0accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.600.650.700.750.800.850.900.951.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.860.880.900.920.940.960.981.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.860.880.900.920.940.960.981.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.8000.8250.8500.8750.9000.9250.9500.9751.000accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.860.880.900.920.940.960.981.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.860.880.900.920.940.960.981.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.8000.8250.8500.8750.9000.9250.9500.9751.000accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher20%40%60%80%100%# ConfDiff data0.700.750.800.850.900.951.00accuracyConfDiff-ReLUConfDiff-ABSPcomp-Teacher",
    "reference": "# Summary Of The Paper\n\nThis paper studies a weakly supervised learning setting, in which one has limited access to the *confidence labels* of the training examples. Previous work in this line includes the setup with pointwise confidence scores (Pconf), and the setup where pairwise comparisons of the confidence scores are available (Pcomp). This work presumes more fine-grained information than the latter: Unlabeled data pairs with confidence difference (ConfDiff). This learning setup is then formulated as empirical risk minimization and a corresponding unbiased risk estimator is constructed, together with an estimation error bound.\n\n# Strength And Weaknesses\n\n### Strength  \n\n- The paper is technically well formulated. The proposed learning setting was rigorously set up and an unbiased risk estimator is derived for the empirical risk minimization. \n- The paper is well written and easy to follow. Even people outside this particular field should be able to grasp the general idea proposed therein. \n\n### Weaknesses\n\n- In my opinion, the biggest weakness of this paper is its setting. Is the setting realistic? The paper doesn't provide sufficient motivation in the introduction; it also lacks realistic experimental setup to support the ConfDiff setting.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written. As far as I can tell, the reproducibility is high (although I didn't read all the proofs.)\n\nRegarding the novelty, while I do think the ConfDiff setting is new, I am doubtful if it is a realistic or useful setting in practice, as mentioned above.\n\n# Summary Of The Review\n\nAs mentioned earlier, my key issue of this paper is that I don't think the ConfDiff setting is realistic. \n\nIn terms of the level of information required in training, ConfDiff sits in between Pconf and Pcomp, in theory. In practice, however, I don't see a situation where one obtains the exact difference between two confidence scores, **without** first estimating the pointwise confidence scores. Even the experiments in the paper have to first do the point estimation. \n\nIt was mentioned in the paper that *the confidence difference is given by annotators in real-world applications*. Has such annotation procedure ever actually applied in the real world? We have to realize that it is extremely hard for a human annotator to give exact confidence difference between two examples. I would say the annotation settings of Pconf and Pcomp are, comparably speaking, more realistic. In the case of former, each annotation provides more information. (Also I'd say the annotator might have to do pointwise estimates first before giving exact confidence differences). In the case of latter, the annotation is much simpler for the annotator as it is only a qualitative paired comparison.\n\nIn any case, I think the paper should motivate the proposed setting better, ideally with some real-world applications.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nHOW DOES SHARPNESS-AWARE MINIMIZATION MINIMIZE SHARPNESS?\n\nKaiyue Wen Institute for Interdisciplinary Information Sciences Tsinghua University wenky20@mails.tsinghua.edu.cn\n\nTengyu Ma, Zhiyuan Li Computer Science Department Stanford University {tengyuma,zhiyuanli}@stanford.edu\n\nABSTRACT\n\nSharpness-Aware Minimization (SAM) is a highly effective regularization technique for improving the generalization of deep neural networks for various settings. However, the underlying working of SAM remains elusive because of various intriguing approximations in the theoretical characterizations. SAM intends to penalize a notion of sharpness of the model but implements a computationally efficient variant; moreover, a third notion of sharpness was used for proving generalization guarantees. The subtle differences in these notions of sharpness can indeed lead to significantly different empirical results. This paper rigorously nails down the exact sharpness notion that SAM regularizes and clarifies the underlying mechanism. We also show that the two steps of approximations in the original motivation of SAM individually lead to inaccurate local conclusions, but their combination accidentally reveals the correct effect, when full-batch gradients are applied. Furthermore, we also prove that the stochastic version of SAM in fact regularizes the third notion of sharpness mentioned above, which is most likely to be the preferred notion for practical performance. The key mechanism behind this intriguing phenomenon is the alignment between the gradient and the top eigenvector of Hessian when SAM is applied.\n\n1\n\nINTRODUCTION\n\nModern deep nets are often overparametrized and have the capacity to fit even randomly labeled data (Zhang et al., 2016). Thus, a small training loss does not necessarily imply good generalization. Yet, standard gradient-based training algorithms such as SGD are able to find generalizable models. Recent empirical and theoretical studies suggest that generalization is well-correlated with the sharpness of the loss landscape at the learned parameter (Keskar et al., 2016; Dinh et al., 2017; Dziugaite et al., 2017; Neyshabur et al., 2017; Jiang et al., 2019). Partly motivated by these studies, Foret et al. (2021); Wu et al. (2020); Zheng et al. (2021); Norton et al. (2021) propose to penalize the sharpness of the landscape to improve the generalization. We refer this method to Sharpness-Aware Minimization (SAM) and focus on the version of Foret et al. (2021).\n\nDespite its empirical success, the underlying working of SAM remains elusive because of the various intriguing approximations made in its derivation and analysis. There are three different notions of sharpness involved — SAM intends to optimize the first notion, the sharpness along the worst direction, but actually implements a computationally efficient notion, the sharpness along the direction of the gradient. But in the analysis of generalization, a third notion of sharpness is actually used to prove generalization guarantees, which admits the first notion as an upper bound. The subtle difference between the three notions can lead to very different biases (see Figure 1 for demonstration).\n\nMore concretely, let L be the training loss, x be the parameter and ρ be the perturbation radius, a hyperparameter requiring tuning. The first notion corresponds to the following optimization problem (1), where we call RMax (x) − L(x) the worst-direction sharpness at x. SAM intends to minimize the original training loss plus the worst-direction sharpness at x.\n\n(x) = LMax\n\nρ\n\nρ\n\nmin x\n\nLMax ρ\n\n(x), where LMax\n\nρ\n\n(x) = max∥v∥2≤1 L(x + ρv) .\n\n(1)\n\nHowever, even evaluating LMax (x) is computationally expensive, not to mention optimization. Thus Foret et al. (2021); Zheng et al. (2021) have introduced a second notion of sharpness, which approximates the worstcase direction in (1) by the direction of gradient, as defined below in (2). We call RAsc ρ (x) − L(x) the ascent-direction sharpness at x.\n\nρ (x) = LAsc\n\nρ\n\nmin x\n\nLAsc\n\nρ (x), where LAsc\n\nρ (x) = L (x + ρ∇L(x)/ ∥∇L(x)∥2) .\n\n(2)\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nType of Sharpness-Aware Loss Notation\n\nDefinition\n\nBiases (among minimizers)\n\nWorst-direction\n\nAscent-direction\n\nAverage-direction\n\nLMax ρ\nLAsc ρ\nLAvg\n\nρ\n\n(cid:16)\n\nmax∥v∥2≤1 L(x + ρv) minx λ1(∇2L(x)) (Thm G.3) minx λmin(∇2L(x)) (Thm G.4) L\n) minx Tr(∇2L(x)) (Thm G.5)\n\n∥∇L(x)∥2 Eg∼N (0,I)L(x + ρ g\n\nx + ρ ∇L(x)\n\n(cid:17)\n\n∥g∥2\n\nTable 1: Definitions and biases of different notions of sharpness-aware loss. The corresponding sharpness is defined as the difference between sharpness-aware loss and the original loss. Here λ1 denotes the largest eigenvalue and λmin denotes the smallest non-zero eigenvalue.\n\nFor further acceleration, Foret et al. (2021); Zheng et al. (2021) omit the gradient through other occurrence of x and approximate the gradient of ascent-direction sharpness by gradient taken after one-step ascent, i.e., ∇LAsc ρ (x) ≈ ∇L (x + ρ∇L(x)/ ∥∇L(x)∥2) and derive the update rule of SAM, where η is the learning rate. (3)\n\nSharpness-Aware Minimization (SAM): x(t + 1) = x(t) − η∇L (x + ρ∇L(x)/ ∥∇L(x)∥2) .\n\nIntriguingly, the generalization bound of SAM upperbounds the generalization error by the third notion of sharpness, called average-direction sharpness, RAvg ρ (x) = LAvg\n\nρ (x) = Eg∼N (0,I)L (x + ρg/∥g∥2) .\n\nρ (x) − L(x), where LAvg\n\nρ (x) and defined formally below.\n\nRAvg\n\n(4)\n\nThe worst-case sharpness is an upper bound of the average case sharpness and thus it is a looser bound for generalization error. In other words, according to the generalization theory in Foret et al. (2021); Wu et al. (2020) in fact motivates us to directly minimize the average case sharpness (as opposed to the worst-case sharpness that SAM intends to optimize).\n\nIn this paper, we analyze the biases introduced by penalizing these various notions of sharpness as well as the bias of SAM (Equation 3). Our analysis for SAM is performed for small perturbation radius ρ and learning rate η under the setting where the minimizers of loss form a manifold following the setup of Fehrman et al. (2020); Li et al. (2021). In particular, we make the following theoretical contributions.\n\n1. We prove that full-batch SAM indeed minimizes worst-direction sharpness. (Theorem 4.5) 2. Surprisingly, when batch size is 1, SAM minimizes average-direction sharpness. (Theorem 5.4) 3. We provide a characterization (Theorems 4.2 and 5.3) of what a few sharpness regularizers bias towards among the minimizers (including all the three notions of the sharpness in Table 1), when the perturbation radius ρ goes to zero. Surprisingly, both heuristic approximations made for SAM lead to inaccurate conclusions: (1) Minimizing worst-direction sharpness and ascent-direction sharpness induce different biases among minimizers, and (2) SAM doesn’t minimize ascent-direction sharpness.\n\nThe key mechanism behind this bias of SAM is the alignment between gradient and the top eigenspace of Hessian of the original loss in the latter phase of training—the angle between them decreases gradually to the level of O(ρ). It turns out that the worst-direction sharpness starts to decrease once such alignment is established (see Section 4.3). Interestingly, such an alignment is not implied by the minimization problem (2), but rather, it is an implicit property of the specific update rule of SAM. Interestingly, such an alignment property holds for SAM with full batch and SAM with batch size one, but does not necessarily hold for the mini-batch case.\n\n2 RELATED WORKS\n\nSharpness and Generalization. The study on the connection between sharpness and generalization can be traced back to Hochreiter et al. (1997). Keskar et al. (2016) observe a positive correlation between the batch size, the generalization error, and the sharpness of the loss landscape when changing the batch size. Jastrzebski et al. (2017) extend this by finding a correlation between the sharpness and the ratio between learning rate to batch size. Dinh et al. (2017) show that one can easily construct networks with good generalization but with arbitrary large sharpness by reparametrization. Dziugaite et al. (2017); Neyshabur et al. (2017); Wei et al. (2019a;b) give theoretical guarantees on the generalization error using sharpness-related measures. Jiang et al. (2019) perform a large-scale empirical study on various generalization measures and show that sharpness-based measures have the highest correlation with generalization.\n\nBackground on Sharpness-Aware Minimization. Foret et al. (2021); Zheng et al. (2021) concurrently propose to minimize the loss at the perturbed from current parameter towards the worst direction to improve generalization. Wu et al. (2020) propose an almost identical method for a different purpose, robust generalization of adversarial training. Kwon et al. (2021) propose a different metric for SAM to fix the rescaling problem pointed out by Dinh et al. (2017). Liu et al. (2022) propose a more computationally efficient version\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n3 + F2(x1, x2)x2\n\nFigure 1: Visualization of the different biases of different sharpness notions on a 4D-toy example. Let F1, F2 : R2 → R+ be two positive functions satisfying that F1 > F2 on [0, 1]2. For x ∈ R4, consider loss L(x) = F1(x1, x2)x2 4. The loss L has a zero loss manifold {x3 = x4 = 0} of codimension M = 2 and the two non-zero eigenvalues of ∇2L of any point x on the manifold are λ1(∇2L(x)) = F1(x1, x2) and λ2(∇2L(x)) = F2(x1, x2). We test three optimization algorithms on this 4D-toy model with small learning rates. They all quickly converge to zero loss, i.e., x3(t), x4(t) ≈ 0, and after that x1(t), x2(t) still change slowly, i.e., moving along the zero loss manifold. We visualize the loss restricted to (x3, x4) as the 3D shape at various (x1, x2)’s where x1 = x1(t), x2 = x2(t) follows the trajectories of the three algorithms. In other words, each of the 3D surface visualize the function g(x3, x4) = L(x1(t), x2(t), x3, x4). As our theory predicts, (1) Full-batch SAM (Equation 3) finds the minimizer with the smallest top eigenvalue, F1(x1, x2); (2) GD on ascent-direction loss LAsc (Equation 2) finds the minimizer with the smallest bottom eigenvalue, F2(x1, x2); (3) 1-SAM (Equation 13) (with L0(x) = F1(x1, x2)x2 and L1(x) = F2(x1, x2)x2 4) finds the minimizer with the smallest trace of Hessian, F1(x1, x2) + F2(x1, x2). See more details in Appendix B.\n\nρ\n\n3\n\nof SAM. Zhuang et al. (2022) proposes a variant of SAM, which improves generalization by simultaneously optimizing the surrogate gap and the sharpness-aware loss. Zhao et al. (2022) propose to improve generalization by penalizing gradient norm. Their proposed algorithm can be viewed as a generalization of SAM. Andriushchenko et al. (2022) study a variant of SAM where the step size of ascent step is ρ instead of . They show that for a simple model this variant of SAM has a stronger regularization effect when batch size is 1 compared to the full-batch case and argue that this might be the explanation that SAM generalizes better with small batch sizes. More related works are discussed in Appendix A.\n\nρ ∥∇L(x)∥2\n\n3 NOTATIONS AND ASSUMPTIONS\n\nk\n\nFor any natural number k, we say a function is Ck if it is k-times continuously differentiable and is C if its kth order derivatives are locally lipschitz. We say a subset of RD is compact if each of its open covers has a finite subcover. It is well known that a subset of RD is compact if and only if it is closed and bounded. For any positive definite symmetric matrix A ∈ RD×D, define {λi(A), vi(A)}i∈[D] as all its eigenvalues and eigenvectors satisfying λ1(A) ≥ λ2(A)... ≥ λD(A) and ∥vi(A)∥2 = 1. For any mapping F , we define ∂F (x) as the Jacobian where [∂F (x)]ij = ∂jFi(x). Thus the directional derivative of F along the vector u at x can be written as ∂F (x)u. We further define the second order directional derivative of F along the vectors u and v at x, ∂2F (x)[u, v], ∂(∂F · u)(x)v, that is, the directional derivative of ∂F · u along the vector v at x. Given a C1 submanifold (Definition C.1) Γ of RD and a point x ∈ Γ, define Px,Γ as the projection operator onto the manifold of the normal space of Γ at x and P ⊥ x,Γ = ID − Px,Γ. We fix our initialization as xinit and our loss function as L : RD → R. Given the loss function, its gradient flow is denoted by mapping φ : RD × [0, ∞) → RD. Here, φ(x, τ ) denotes the iterate at time τ of a gradient flow starting at x and is defined as the unique solution of φ(x, τ ) = x − (cid:82) τ 0 ∇L(φ(x, t))dt, ∀x ∈ RD. We further define the limiting map Φ as Φ(x) = limτ →∞ φ(x, τ ), that is, Φ(x) denotes the convergent point of the gradient flow starting from x. When L(x) is small, Φ(x) and x are near. Hence in our analysis, we regularly use Φ(x(t)) as a surrogate to analyze the dynamics of x(t). Lemma 3.1 is an important property of Φ from Li et al. (2021) (Lemma C.2), which is repeatedly used in our analysis. The proof is shown in Appendix F.\n\nLemma 3.1. For any x at which Φ is defined and differentiable, we have that ∂Φ(x)∇L(x) = 0.\n\nRecent empirical studies have shown that there are essentially no barriers in loss landscape between different minimizers, that is, the set of minimizers are path-connected (Draxler et al., 2018; Garipov et al., 2018). Motivated by this empirical discovery, we make the assumption below following Fehrman et al. (2020); Li et al. (2021); Arora et al. (2022), which is theoretically justified by Cooper (2018) under a generic setting. Assumption 3.2. Assume loss L : RD → R is C4, and there exists a C2 submanifold Γ of RD that is a (D − M )-dimensional for some integer 1 ≤ M ≤ D, where for all x ∈ Γ, x is a local minimizer of L and rank(∇2L(x)) = M .\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nThough our analysis for the full-batch setting is performed under the general and abstract setting, Assumption 3.2, our analysis for the stochastic setting uses a more concrete one, Setting 5.1, where we can prove that Assumption 3.2 holds. (see Theorem 5.2) Definition 3.3 (Attraction Set). Let U be the attraction set of Γ under gradient flow, that is, a neighborhood of Γ containing all points starting from which gradient flow w.r.t. loss L converges to some point in Γ, or mathematically, U ≜ {x ∈ RD|Φ(x) exists and Φ(x) ∈ Γ}.\n\nIt can be shown that for a minimum loss manifold, the rank of Hessian plus the dimension of the manifold is at most the environmental dimension D, and thus our assumption about Hessian rank essentially says the rank is maximal. Assumption 3.2 implies that U is open and Φ is C on U (Arora et al., 2022, Lemma B.15).\n\n2\n\n4 EXPLICIT AND IMPLICIT BIAS IN THE FULL-BATCH SETTING\n\nIn this section, we present our main results in the full-batch setting. Section 4.1 provides characterization of explicit bias of worst-direction, ascent-dircetion, and average-direction sharpness. In particular, we show that ascent-direction sharpness and worst-direction sharpness have different explicit biases. However, it turns out the explicit bias of ascent-direction sharpness is not the effective bias of SAM (that approximately optimizes the ascent-direction sharpness), because the particular implementation of SAM imposes additional, different biases, which is the main focus of Section 4.2. We provide our main theorem in the full-batch setting, that SAM implicitly minimizes the worst-direction sharpness, via characterizing its limiting dynamics as learning rate ρ and η goes to 0 with a Riemmanian gradient flow with respect to the top eigenvalue of the Hessian of the loss on the manifold of local minimizers. In Section 4.3, we sketch the proof of the implicit bias of SAM and identify a key property behind the implicit bias, which we call the implicit alignment between the gradient and the top eigenvector of the Hessian.\n\n4.1 WORST- AND ASCENT-DIRECTION SHARPNESS HAVE DIFFERENT EXPLICIT BIASES\n\nIn this subsection, we show that the explicit biases of three notions of sharpness are all different under Assumption 3.2. We first recap the heuristic derivation of ascent-direction sharpness RAsc ρ .\n\nThe intuition of approximating RMax comes from the following Taylor expansions (Foret et al., 2021; Wu et al., 2020). Consider any compact set, for sufficiently small ρ, the following holds uniformly for all x in the compact set:\n\nby RAsc\n\nρ\n\nρ\n\nRMax ρ\n\n(x) = sup\n\nL(x + ρv) − L(x) = sup\n\n(cid:0)ρv⊤∇L(x) +\n\n∥v∥2≤1\n\n∥v∥2≤1\n\nρ2 2\n\nv⊤∇2L(x)v + O(ρ3)(cid:1) ,\n\nRAsc\n\nρ (x) = L(cid:0)x + ρ\n\n∇L(x) ∥∇L(x)∥2\n\n(cid:1)−L(x) =ρ ∥∇L(x)∥2 +\n\nρ2 2\n\n∇L(x)⊤∇2L(x)∇L(x) ∥∇L(x)∥2\n\n2\n\n+O(ρ3) .\n\n(5)\n\n(6)\n\nHere, the preference among the local or global minima is what we are mainly concerned with. Since sup∥v∥2≤1 v⊤∇L(x) = ∥∇L(x)∥2 when ∥∇L(x)∥2 > 0, the leading terms in Equations 5 and 6 are both the first order term, ρ ∥∇L(x)∥2, and are the same. However, it is erroneous to think that the first order term decides the explicit bias, as the first order term ∥∇L(x)∥2 vanishes at the local minimizers of the loss L and thus the second order term becomes the leading term. Any global minimizer x of the original loss L is an O(ρ2)-approximate minimizer of the sharpness-aware loss because ∇L(x) = 0. Therefore, the sharpness-aware loss needs to be of order ρ2 so that we can guarantee the second-order terms in Equation 5 and/or Equation 6 to be non-trivially small. Our main result in this subsection (Theorem 4.2) gives an explicit characterization for this phenomenon. The corresponding explicit biases for each type of sharpness is given below in Definition 4.1. As we will see later, they can be derived from a general notion of limiting regularizer (Definition 4.3). Definition 4.1. For x ∈ RD, we define SMax(x) = λ1(∇2L(x))/2, SAsc(x) = λM (∇2L(x))/2 and SAvg(x) = Tr(∇2L(x))/(2D). Theorem 4.2. Under Assumption 3.2, let U ′ be any bounded open set such that its closure U ′ ⊆ U and U ′ ∩ Γ ⊆ U ′ ∩ Γ. For any type ∈ {Max, Asc, Avg} and any optimality gap ∆ > 0, there is a function ε : R+ → R+ with limρ→0 ε(ρ) = 0, such that for all sufficiently small ρ > 0 and all u ∈ U ′ satisfying that\n\nL(u) + Rtype\n\nρ\n\n(u) − inf x∈U ′\n\n(cid:0)L(x) + Rtype\n\n(x)(cid:1) ≤ ∆ρ2, 1\n\nρ\n\nit holds L(u) − inf x∈U ′ L(x) ≤ (∆ + ε(ρ))ρ2 and that Stype(u) − inf x∈U ′∩Γ Stype(x) ∈ [−ε(ρ), ∆ + ε(ρ)].\n\n1We note that RAsc\n\nρ (x) is undefined when ∥∇L(x)∥2 = 0. In such cases, we set RAsc\n\nρ (x) = ∞.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 4.2 suggests a sharp phase transition of the property of the solution of minx L(x) + Rρ(x) when the optimization error drops from ω(ρ2) to O(ρ2). When the optimization error is larger than ω(ρ2), no regularization effect happens and any minimizer satisfies the requirement. When the error becomes O(ρ2), there is a non-trivial restriction on the coefficients in the second-order term.\n\nNext we give a heuristic derivation for the above defined Stype. First, for worst- and average-direction sharpness, the calculations are fairly straightforward and well-known in literature (Keskar et al., 2016; Kaur et al., 2022; Zhuang et al., 2022; Orvieto et al., 2022), and we sketch them here. In the limit of perturbation radius ρ → 0, we know that the minimizer of the sharpness-aware loss will also converges to Γ, the manifold of minimizers of the original loss L. Thus to decide to which x ∈ Γ the minimizers will converge to as ρ → 0, it suffices to take Taylor expansion of LAsc at each x ∈ Γ and compare the second-order coefficients, e.g., we have that RAvg 2 λ1(∇2L(x)) + O(ρ3) by Equation 5.\n\n2D Tr(∇2L(x)) + O(ρ3) and RMax\n\nρ (x) = ρ2\n\n(x) = ρ2\n\nor LAvg\n\nρ\n\nρ\n\nρ\n\nHowever, the analysis for ascent-direction sharpness is more tricky because RAsc ρ (x) = ∞ for any x ∈ Γ and thus is not continuous around such x. Thus we have to aggregate information from neighborhood to capture the explicit bias of Rρ around manifold Γ. This motivates the following definition of limiting regularizer which allows us to compare the regularization strength of Rρ around each point on manifold Γ as ρ → 0. Definition 4.3 (Limiting Regularizer). We define the limiting regularizer of {Rρ} as the function2\n\nS : Γ → R, S(x) = lim\n\nρ→0\n\nlim r→0\n\ninf ∥x′−x∥2≤r\n\nRρ(x′)/ρ2.\n\nρ\n\nρ (x′) ≈ ρ2\n\nTo minimize RAsc\n\naround x, we can pick x′ → x satisfying that ∥∇L(x′)∥2 → 0 yet strictly being nonzero. By Equation 6, we have RAsc . Here the crucial step of the proof is that because of Assumption 3.2, ∇L(x)/ ∥∇L(x)∥2 must almost lie in the column span of ∇2L(x), which implies that inf x′ ∇L(x′)⊤∇2L(x)∇L(x′)/∥∇L(x′)∥2 → λM (∇2L(x)), where rank(∇2L(x)) = M by Assumption 3.2. The above alignment property between the gradient and the column space of Hessian can be checked directly for any non-negative quadratic function. The maximal Hessian rank assumption in Assumption 3.2 ensures that this property extends to general losses.\n\n·∇L(x′)⊤∇2L(x)∇L(x′) ∥∇L(x′)∥2 2\n\nρ→0\n\n2\n\n2\n\nWe defer the proof of Theorem 4.2 into Appendix G.1, where we develop a sufficient condition where the notion of limiting regularizer characterizes the explicit bias of Rρ as ρ → 0.\n\n4.2 SAM PROVABLY DECREASES WORST-DIRECTION SHARPNESS\n\nThough ascent-direction sharpness has different explicit bias from worst-direction sharpness, in this subsection we will show that surprisingly, SAM (Equation 3), a heuristic method designed to minimize ascentdirection sharpness, provably decreases worst-direction sharpness. The main result here is an exact characterization of the trajectory of SAM (Equation 3) via the following ordinary differential equation (ODE) (Equation 7), when learning rate η and perturbation radius ρ are small and the initialization x(0) = xinit is in U , the attraction set of manifold Γ.\n\nX(τ ) = X(0) −\n\n1 2\n\n(cid:90) τ\n\ns=0\n\nP ⊥\n\nX(s),Γ∇λ1(∇2L(X(s)))ds, X(0) = Φ(xinit).\n\n(7)\n\nWe assume ODE (Equation 7) has a solution till time T3, that is, Equation 7 holds for all t ≤ T3. We call the solution of Equation 7 the limiting flow of SAM, which is exactly the Riemannian Gradient Flow on the manifold Γ with respect to the loss λ1(∇2L(·)). In other words, the ODE (Equation 7) is essentially a projected gradient descent algorithm with loss λ1(∇2L(·)) on the constraint set Γ and an infinitesimal learning rate. Note λ1(∇2L(x)) may not be differentiable at x if λ1(∇2L(x)) = λ2(∇2L(x)), thus to ensure Equation 7 is well-defined, we assume there is a positive eigengap for L on Γ.3 Assumption 4.4. For all x ∈ Γ, there exists a positive eigengap, i.e., λ1(∇2L(x)) > λ2(∇2L(x)).\n\nTheorem 4.5 is the main result of this section, which is a direct combination of Theorems I.1 and I.3. The proof is deferred to Appendix I.3.\n\n2Here we implicitly assume the zeroth and first order term varnishes, which holds for all three sharpness notions. 3In fact we only need to assume the positive eigengap along the solution of the ODE. If Γ doesn’t satisfy Assump-\n\ntion 4.4, we can simply perform the same analysis on its submanifold {x ∈ Γ | eigengap is positive at x}.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 4.5 (Main). Let {x(t)} be the iterates of full-batch SAM (Equation 3) with x(0) = xinit ∈ U . Under Assumptions 3.2 and 4.4, for all η, ρ such that η ln(1/ρ) and ρ/η are sufficiently small, the dynamics of SAM can be characterized in the following two phases:\n\n• Phase I: (Theorem I.1) Full-batch SAM (Equation 3) follows Gradient Flow with respect to L until entering\n\nan O(ηρ) neighborhood of the manifold Γ in O(ln(1/ρ)/η) steps;\n\n• Phase II: (Theorem I.3) Under a mild non-degeneracy assumption (Assumption I.2) on the initial point of phase II, full-batch SAM (Equation 3) tracks the solution X of Equation 7, the Riemannian Gradient Flow with respect to the loss λ1(∇2L(·)) in an O(ηρ) neighborhood of manifold Γ. Quantitatively, the approximation error between the iterates x and the corresponding limiting flow X is O(η ln(1/ρ)), that is, ∥x(cid:0)⌈T3/(ηρ2)⌉(cid:1) − X(T3)∥2 = O(η ln(1/ρ)) .\n\nMoreover, the angle between ∇L(cid:0)x(⌈ T3\n\nηρ2 ⌉(cid:1) and the top eigenspace of ∇2L(x(⌈ T3\n\nηρ2 ⌉)) is O(ρ).\n\nTheorem 4.5 shows that SAM decreases the largest eigenvalue of Hessian of loss locally around the manifold of local minimizers. Phase I uses standard approximation analysis as in Hairer et al. (2008). In Phase II, as T3 is arbitrary, the approximation and alignment properties hold simultaneously for all X(t) along the trajectory, provided that η ln(1/ρ) and ρ/η are sufficiently small. The subtlety here is that the threshold of being ”sufficiently small” on η ln(1/ρ) and ρ/η actually depends on T3, which decreases when T3 → 0 or → ∞. We defer the proof of Theorem 4.5 to Appendix I.\n\nAs a corollary of Theorem 4.5, we can also show that the largest eigenvalue of the limiting flow closely tracks the worst-direction sharpness.\n\nCorollary 4.6. In the setting of Theorem 4.5, the difference between the worst-direction sharpness of the iterates and the corresponding scaled largest eigenvalues along the limiting flow is at most O(ηρ2 ln(1/ρ)). That is,\n\n(cid:12) (cid:12)RMax\n\nρ\n\n(x(⌈T3/ηρ2⌉)) − ρ2λ1(∇2L(X(T3))/2(cid:12)\n\n(cid:12) = O(ηρ2 ln(1/ρ)) .\n\n(8)\n\nSince η ln(1/ρ) is assumed to be sufficiently small, the error O(η ln(1/ρ) · ρ2) is only o(ρ2), meaning that penalizing the top eigenvalue on the manifold does lead to non-trivial reduction of worst-direction sharpness, in the sense of Section 4.1.\n\nHence we can show that full-batch SAM (Equation 3) provably minimizes worst-direction sharpness around the manifold if we additionally assume the limiting flow converges to a minimizer of the top eigenvalue of Hessian in the following Corollary 4.7. Corollary 4.7. Under Assumptions 3.2 and 4.4, define U ′ as in Theorem 4.2 and suppose X(∞) = lim t→∞ exists and is a minimizer of λ1(∇2L(x)) in U ′ ∩ Γ. Then for all ε > 0, there exists Tε > 0, such that for all ρ, η such that η ln(1/ρ) and ρ/η are sufficiently small, we have that\n\nX(t)\n\nLMax ρ\n\n(x(⌈Tε/(ηρ2)⌉)) ≤ ερ2 + inf\n\nx∈U ′\n\nLMax ρ\n\n(x) .\n\nWe defer the proof of Corollaries 4.6 and 4.7 to Appendix I.4.\n\n4.3 ANALYSIS OVERVIEW FOR SHARPNESS REDUCTION IN PHASE II OF THEOREM 4.5\n\nNow we give an overview of the analysis for the trajectory of full-batch SAM (Equation 3) in Phase II (in Theorem 4.5). The framework of the analysis is similar to Arora et al. (2022); Lyu et al. (2022); Damian et al. (2021), where the high-level idea is to use Φ(x(t)) as a proxy for x(t) and study the dynamics of Φ(x(t)) via Taylor expansion. Following the analysis in Arora et al. (2022) we can show Equation 9 using Taylor expansion, starting from which we will discuss the key innovation in this paper regarding implicit Hessian-gradient alignment. We defer its intuitive derivation into Appendix I.5.\n\nΦ(x(t + 1))−Φ(x(t))= −\n\nηρ2 2\n\n∂Φ(x(t))∂2(∇L)(x(t))(cid:2) ∇L(x(t))\n\n∥∇L(x(t))∥2\n\n,\n\n∇L(x(t)) ∥∇L(x(t))∥2\n\n(cid:3)+O(η2ρ2 + ηρ3) . (9)\n\nNow, to understand how Φ(x(t)) moves over time, we need to understand what the direction of the RHS of Equation 9 corresponds to—we will prove that it corresponds to the Riemannian gradient of the loss function ∇λ1(∇2L(x)) at x = Φ(x(t)). To achieve this, the key is to understand the direction ∇L(x(t)) .\nIt turns out that we will prove ∇L(x(t))\n\nis close to the top eigenvector of the Hessian up to sign flip, that is\n\n∥∇L(x(t))∥2\n\n∥∇L(x(t))∥2\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n∥ ∇L(x(t)) alignment and will discuss it in more detail at the end of this subsection.\n\n− s · v1(∇2L(x))∥2 ≤ O(ρ) for some s ∈ {−1, 1}. We call this phenomenon Hessian-gradient\n\n∥∇L(x(t))∥2\n\nUsing this property, we can proceed with the derivation (detailed in Appendix I.5): ηρ2 2\n\nΦ(x(t + 1)) − Φ(x(t)) = −\n\n∂Φ(Φ(x(t)))∇λ1(∇2L(Φ(x(t)))) + O(η2ρ2 + ηρ3),\n\n(10)\n\nImplicit Hessian-gradient Alignment. It remains to explain why the gradient implicitly aligns to the top eigenvector of the Hessian, which is the key component of the analysis in Phase II. The proof strategy here is to first show alignment for a quadratic loss function, and then generalize its proof to general loss functions satisfying Assumption 3.2. Below we first give the formal statement of the implicit alignment on quadratic loss, Theorem 4.8 and defer the result for general case (Lemma I.19) to appendix. Note this alignment property is an implicit property of the SAM algorithm as it is not explicitly enforced by the objective that SAM is intended to minimize, LAsc ρ would rather explicitly align gradient to the smallest non-zero eigenvector (See proofs of Theorem G.5)! Theorem 4.8. Suppose A is a positive definite symmetric matrix with unique top eigenvalue. Consider running full-batch SAM (Equation 3) on loss L(x) := 1\n\nρ . Indeed optimizing LAsc\n\n2 xT Ax as in Equation 11 below.\n\n(11) for almost every x(0), we have x(t) converges in direction to v1(A) up to a sign flip and\n\nx(t + 1) = x(t) − ηA(cid:0)x(t) + ρAx(t)/∥Ax(t)∥2\n\n(cid:1) .\n\nThen, limt→∞ ∥x(t)∥2 = ηρλ1(A)\n\n2−ηλ1(A) with ηλ1(A) < 1.\n\nThe proof of Theorem 4.8 relies on a two-phase analysis of the behavior of Equation 11, where we first show that x(t) enters an invariant set from any initialization and in the second phase, we construct a potential function to show alignment. The proof is deferred to Appendix H.\n\nBelow we briefly discuss why the case with general loss is closely related to the quadratic loss case. We claim that, in the general loss function case, the analog of Equation 11 is the update rule for the gradient: (cid:1) + O(ηρ2) .\n\n∇L(x(t + 1))=∇L(x(t))−η∇2L(x(t))(cid:0)∇L(x(t)) + ρ∇2L(x(t))\n\n(12)\n\n∇L(x(t)) ∥∇L(x(t)))∥2\n\nWe first note that indeed in the quadratic case where ∇L(x) = Ax and ∇2L(x) = A, Equation 12 is equivalent to Equation 11 because they only differ by a multiplicative factor A on both sides. We derive its intuitive derivation into Appendix I.5.\n\n5 EXPLICIT AND IMPLICIT BIASES IN THE STOCHASTIC SETTING\n\nIn practice, people usually use SAM in the stochastic mini-batch setting, and the test accuracy improves as the batch size decreases (Foret et al., 2021). Towards explaining this phenomenon, Foret et al. (2021) argue intuitively that stochastic SAM minimizes stochastic worst-direction sharpness. Given our results in Section 4, it is natural to ask if we can justify the above intuition by showing the Hessian-gradient alignment in the stochastic setting. Unfortunately, such alignment is not possible in the most general setting. Yet when the batch size is 1, we can prove rigorously in Section 5.2 that stochastic SAM minimizes stochastic worst-direction sharpness, which is the expectation of the worst-direction sharpness of loss over each data (defined in Section 5.1), which is the main result in this section. We stress that the stochastic worst-direction sharpness has a different explicit bias to the worst-direction sharpness, which full-batch SAM implicitly penalizes. When perturbation radius ρ → 0, the former corresponds to Tr(∇2L(·)), the same as averagedirection sharpness, and the latter corresponds to λ1(∇2L(·)).\n\nBelow we start by introducing our setting for SAM with batch size 1, or 1-SAM. We still need Assumption 3.2 in this section. We first analyze the explicit bias of the stochastic ascent- and worst-direction sharpness in Section 5.1 via the tools developed in Section 4.1. It turns out they are all proportional to the trace of hessian as ρ → 0. In Section 5.2, we show that 1-SAM penalizes the trace of Hessian. Below we formally state our setting for stochastic loss of batch size one (Setting 5.1). Setting 5.1. Let the total number of data be M . Let fk(x) be the model output on the k-th data where fk is a C4-smooth function and yk be the k-th label, for k = 1, . . . , M . We define the loss on the k-th data as Lk(x) = l(fk(x), yk) and the total loss L = (cid:80)M k=1 Lk/M , where function l(y′, y) is C4-smooth in y′. We also assume for any y ∈ R, it holds that arg miny′∈R l(y′, y) = y and that ∂2l(y′,y) (∂y′)2 |y′=y > 0. Finally, we denote the set of global minimizers of L with full-rank Jacobian by Γ and assume that it is non-empty, that is,\n\nΓ ≜ (cid:8)x ∈ RD | fk(x) = yk, ∀k ∈ [M ] and {∇fk(x)}M\n\nk=1 are linearly independent(cid:9) ̸= ∅.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nWe remark that given training data (i.e., {fk}M k=1), Γ defined above is just equal to the set of global minimizers, (cid:8)x ∈ RD | fk(x) = yk, ∀k ∈ [M ](cid:9), except for a zero measure set of labels (yk)M k=1 when fk are C∞ smooth, by Sard’s Theorem. Thus Cooper (2018) argued that the global minimizers form a differentiable manifold generically if we allow perturbation on the labels. In this work we do not make such an assumption for labels. Instead, we consider the subset of the global minimizers with full-rank Jacobian, Γ. A standard application of implicit function theorem implies that Γ defined in Setting 5.1 is indeed a manifold. (See Theorem 5.2, whose proof is deferred into Appendix E.1)\n\nTheorem 5.2. Loss L, set Γ and integer M defined in Setting 5.1 satisfy Assumption 3.2.\n\n1-SAM: We use 1-SAM as a shorthand for SAM on a stochastic loss with batch size 1 as below Equation 13, where kt is sampled i.i.d from uniform distribution on [M ].\n\n1-SAM :\n\nx(t + 1) = x(t) − η∇Lkt\n\n(cid:0)x + ρ∇Lkt(x)/ ∥∇Lkt(x)∥2\n\n(cid:1) .\n\n(13)\n\n5.1 STOCHASTIC WORST-, ASCENT- AND AVERAGE- DIRECTION SHARPNESS HAVE THE SAME\n\nEXPLICIT BIASES AS AVERAGE DIRECTION SHARPNESS\n\nk,ρ , RAsc\n\nSimilar to the full-batch case, we use LMax k,ρ to denote the corresponding sharpness-aware loss for k,ρ, RAvg Lk and RMax k,ρ to denote corresponding sharpness for Lk respectively (defined as Equations 1, 2 and 4 with L replaced by Lk). We further use stochastic worst-, ascent- and average-direction sharpness to denote Ek[RMax k,ρ ]. Unlike the full-batch setting, these three sharpness notions have the same explicit biases, or more precisely, they have the same limiting regularizers (up to some scaling factor).\n\nk,ρ] and Ek[RAvg\n\nk,ρ ], Ek[RAsc\n\nk,ρ , LAsc\n\nk,ρ, LAvg\n\nTheorem 5.3. The limiting regularizers of three notions of stochastic sharpness, denoted by (cid:101)SMax, (cid:101)SAsc, (cid:101)SAvg, satisfy that (cid:101)SMax(x) = (cid:101)SAsc(x) = D · (cid:101)SAvg(x) = Tr(∇2L(x))/2. Furthermore, define U ′ in the same way as in Theorem 4.2 . For any type ∈ {Max, Asc, Avg}, it holds that if for some u ∈ U ′, L(u) + k,ρ (x)](cid:1) + ερ2,4 then we have that L(u) − inf x∈U ′ L(x) ≤ ερ2 + o(ρ2) Ek[Rtype and that (cid:12)\n\n(cid:12) (cid:101)Stype(u) − inf x∈U ′∩Γ (cid:101)Stype(x)(cid:12)\n\n(cid:0)L(x) + Ek[Rtype\n\nk,ρ (u)] ≤ inf\n\n(cid:12) ≤ ε + o(1).\n\nx∈U ′\n\nWe defer the proof of Theorem 5.3 to Appendix G.4. Unlike in the full-batch setting where the implicit regularizer of ascent-direction sharpness and worst-direction sharpness have different explicit bias, here they are the same because there is no difference between the maximum and minimum of its non-zero eigenvalue for rank-1 Hessian of each individual loss Lk, and that the average of limiting regularizers is equal to the limiting regularizer of the average regularizer by definition.\n\n5.2 STOCHASTIC SAM MINIMIZES AVERAGE-DIRECTION SHARPNESS\n\nThis subsection aims to show that the implicit bias of 1-SAM (Equation 13) is minimizing the averagedirection sharpness for small perturbation radius ρ and learning rate η, which has the same implicit bias as all three notions of stochastic sharpness do (Theorem 5.3). As an analog of the analysis in Section 4.3, which shows full-batch SAM minimizes worst-direction sharpness, analysis in this section conceptually shows that 1-SAM minimizes the stochastic worst-direction sharpness.\n\nMathematically, we prove that the trajectory of 1-SAM tracks the following Riemannian gradient flow (Equation 14) with respect to their limiting regularize Tr(∇2L(·)) on the manifold for sufficiently small η and ρ and thus penalizes stochastic worst-direction sharpness (of batch size 1). We assume the ODE (Equation 14) has a solution till time T3.\n\nX(τ ) = X(0) −\n\n1 2\n\n(cid:90) τ\n\ns=0\n\nP ⊥\n\nX(s),Γ∇Tr(∇2L(X(s)))ds, X(0) = Φ(xinit).\n\n(14)\n\nTheorem 5.4. Let {x(t)} be the iterates of 1-SAM (Equation 13) and x(0) = xinit ∈ U , then under Setting 5.1, for almost every xinit, for all η and ρ such that (η + ρ) ln(1/ηρ) is sufficiently small, with probability at least 1 − O(ρ) over the randomness of the algorithm, the dynamics of 1-SAM (Equation 13) can be split into two phases:\n\n• Phase I (Theorem J.1): 1-SAM follows Gradient Flow with respect to L until entering an ̃O(ηρ) neighbor-\n\nhood of the manifold Γ in O(ln(1/ρη)/η) steps;\n\n4We note that RAsc\n\nρ (x) is undefined when ∥∇L(x)∥2 = 0. In such cases, we set RAsc\n\nρ (x) = ∞.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n• Phase II (Theorem J.2): 1-SAM tracks the solution of Equation 14, X, the Riemannian gradient flow with respect to Tr(∇2L(·)) in an ̃O(ηρ) neighborhood of manifold Γ. Quantitatively, the approximation error between the iterates x and the corresponding limiting flow X is ̃O(η1/2 + ρ), that is,\n\n∥x(⌈T3/(ηρ2)⌉) − X(T3)∥2 = ̃O(η1/2 + ρ).\n\nThe high-level intuition for the Phase II result of Theorem 5.4 is that Hessian-gradient alignment holds true for every stochastic loss Lk along the trajectory of 1-SAM and therefore by Taylor expansion (the same argument in Section 4.3), at each step Φ(x(t)) moves towards the negative (Riemannian) gradient of λ1(∇2Lkt) where kt is the index of randomly sampled data, or the limiting regularizer of the worst-direction sharpness of Lkt. Averaging over a long time, the moving direction becomes the negative (Riemmanian) gradient of Ekt[λ1(∇2Lkt)], which is the limiting regularizer of stochastic worst-direction sharpness and equals to Tr(∇2L) by Theorem 5.3.\n\nThe reason that Hessian-gradient alignment holds under Setting 5.1 is that the Hessian of each stochastic loss Lk at minimizers p ∈ Γ, ∇2Lk(p) = ∂2l(y′,yk) |y′=fk(p)∇fk(p)(∇fk(p))⊤(Lemma J.15), is exactly rank-1, which enforces the gradient ∇Lk(x) ≈ ∇2Lk(Φ(x))(x − Φ(x)) to (almost) lie in the top (which is also the unique) eigenspace of ∇2Lk(Φ(x)). Lemma 5.5 formally states this property. Lemma 5.5. Under Setting 5.1, for any p ∈ Γ and k ∈ [M ], it holds that ∇fk(p) ̸= 0 and that there is an open set V containing p, satisfying that\n\n(∂y′)2\n\n∀x ∈ V, ∇Lk (x) ̸= 0 =⇒ ∃s ∈ {−1, 1},\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n= s\n\n∇fk(p) ∥∇fk(p)∥2\n\n+ O(∥x − p∥2).\n\n√\n\n√\n\nCorollaries 5.6 and 5.7 below are stochastic counterparts of Corollaries 4.6 and 4.7, saying that the trace of Hessian are close to the stochastic worst-direction sharpness along the limiting flow (14), and therefore when the limiting flow converges to a local minimizer of trace of Hessian, 1-SAM (Equation 13) minimizes the average-direction sharpness. We defer the proofs of Corollaries 5.6 and 5.7 to Appendix J.4.\n\nρ), the Corollary 5.6. Under the condition of Theorem 5.4, we have that with probability 1 − O( difference between the stochastic worst-direction sharpness of the iterates and the corresponding scaled trace of Hessian along the limiting flow is at most O(cid:0)(η1/4 + ρ1/4)ρ2(cid:1), that is,\n\nη +\n\n(cid:12) (cid:12)Ek[RMax\n\nk,ρ (x(⌈T3/(ηρ2)⌉))] − ρ2Tr(∇2L(X(T3)))/2(cid:12)\n\n(cid:12) = O(cid:0)(η1/4 + ρ1/4)ρ2(cid:1) .\n\nCorollary 5.7. Define U ′ as in Theorem 4.2, suppose X(∞) = lim t→∞ Tr(∇2L(x))) in U ′ ∩ Γ. Then for all ε > 0, there exists a constant Tε > 0, such that for all ρ, η such that (η + ρ) ln(1/ηρ) are sufficiently small, we have that with probability 1 − O( Ek[LMax\n\nk,ρ (x(⌈Tε/(ηρ2)⌉))] ≤ ερ2 + inf\n\nX(t) exists and is a minimizer of\n\nEk[LMax\n\nk,ρ (x)] .\n\nη +\n\nρ),\n\n√\n\n√\n\nx∈U ′\n\n6 CONCLUSION\n\nIn this work, we have performed a rigorous mathematical analysis of the explicit bias of various notions of sharpness when used as regularizers and the implicit bias of the SAM algorithm. In particular, we show the explicit biases of worst-, ascent- and average-direction sharpness around the manifold of minimizers are minimizing the largest eigenvalue, the smallest nonzero eigenvalue, and the trace of Hessian of the loss function. We show that in the full-batch setting, SAM provably decreases the largest eigenvalue of Hessian, while in the stochastic setting when batch size is 1, SAM provably decreases the trace of Hessian.\n\nThe most interesting future work is to generalize the current analysis for stochastic SAM to arbitrary batch size. This is challenging because, without the alignment property which holds automatically with batch size 1, such an analysis essentially requires understanding the stationary distribution of the gradient direction along the SAM trajectory. It is also interesting to incorporate other features of modern deep learning like normalization layers, momentum, and weight decay into the current analysis.\n\nAnother interesting open question is to further bridge the difference between generalization bounds and the implicit bias of the optimizers. Currently, the generalization bounds in Wu et al. (2020); Foret et al. (2020) only work for the randomly perturbed model. Moreover, the bound depends on the average sharpness with finite ρ, whereas the analysis of this paper only works for infinitesimal ρ. It’s an interesting open question whether the generalization error of the model (without perturbation) can be bounded from above by some function of the training loss, norm of the parameters, and the trace of the Hessian.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nWe thank Jingzhao Zhang for helpful discussions. The authors would like to thank the support from NSF IIS 2045685.\n\nREFERENCES\n\nMaksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization.\n\nIn International Conference on Machine Learning, pp. 639–668. PMLR, 2022. 3\n\nSanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on edge of stability in\n\ndeep learning. arXiv preprint arXiv:2205.09745, 2022. 3, 4, 6, 14, 15, 16, 20, 21, 22, 58\n\nPeter L Bartlett, Philip M Long, and Olivier Bousquet. The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima. arXiv preprint arXiv:2210.01513, 2022. 14, 15\n\nGuy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural networks\n\ndriven by an ornstein-uhlenbeck like process. arXiv preprint arXiv:1904.09080, 2019. 14, 16\n\nVivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer, 2009. 70\n\nVivek S Borkar, Jervis Pinto, and Tarun Prabhu. A new learning algorithm for optimal stopping. Discrete\n\nEvent Dynamic Systems, 19(1):91–113, 2009. 15\n\nJeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, and Ameet Talwalkar. Gradient descent on neural\n\nnetworks typically occurs at the edge of stability, 2021. 14\n\nJeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive gradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022. 14\n\nYaim Cooper. The loss landscape of overparameterized neural networks. arXiv preprint arXiv:1804.10200,\n\n2018. 3, 8\n\nAlex Damian, Tengyu Ma, and Jason Lee. Label noise sgd provably prefers flat global minimizers, 2021. 6,\n\n14, 16\n\nAlex Damian, Eshaan Nichani, and Jason D Lee. Self-stabilization: The implicit bias of gradient descent at\n\nthe edge of stability. arXiv preprint arXiv:2209.15594, 2022. 14\n\nChandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM Journal\n\non Numerical Analysis, 7(1):1–46, 1970. 69\n\nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1019– 1028. JMLR. org, 2017. 1, 2\n\nFelix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural In International conference on machine learning, pp. 1309–1318. PMLR,\n\nnetwork energy landscape. 2018. 3\n\nJohn C Duchi and Feng Ruan. Stochastic methods for composite and weakly convex optimization problems.\n\nSIAM Journal on Optimization, 28(4):3229–3259, 2018. 15\n\nGintare Karolina Dziugaite and Daniel M Roy.\n\ndeep (stochastic) neural networks with many more parameters than training data. arXiv:1703.11008, 2017. 1, 2\n\nComputing nonvacuous generalization bounds for arXiv preprint\n\nBenjamin Fehrman, Benjamin Gess, and Arnulf Jentzen. Convergence rates for the stochastic gradient descent method for non-convex objective functions. Journal of Machine Learning Research, 21:136, 2020. 2, 3\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for\n\nefficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020. 9, 16\n\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2021. 1, 2, 4, 7\n\nTimur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. Advances in neural information processing systems, 31, 2018. 3\n\nE. Hairer, S.P. Nørsett, and G. Wanner. Solving Ordinary Differential Equations I: Nonstiff Problems. Springer Series in Computational Mathematics. Springer Berlin Heidelberg, 2008. ISBN 9783540566700. URL https://books.google.com/books?id=F93u7VcSRyYC. 6\n\nThomas P Hayes. A large-deviation inequality for vector-valued martingales. Combinatorics, Probability\n\nand Computing, 2003. 69\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997. 2\n\nRoger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge university press, 2012. 69\n\nStanisław Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and\n\nAmos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623, 2017. 2\n\nYiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generaliza-\n\ntion measures and where to find them. arXiv preprint arXiv:1912.02178, 2019. 1, 2\n\nSimran Kaur, Jeremy Cohen, and Zachary C Lipton. On the maximum hessian eigenvalue and generalization.\n\narXiv preprint arXiv:2206.10654, 2022. 5\n\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016. 1, 2, 5\n\nHarold Kushner and G George Yin. Stochastic approximation and recursive algorithms and applications,\n\nvolume 35. Springer Science & Business Media, 2003. 15\n\nJungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning, pp. 5905–5914. PMLR, 2021. 2\n\nQianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic gradient\n\nalgorithms. In International Conference on Machine Learning, pp. 2101–2110. PMLR, 2017. 15\n\nQianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations. The Journal of Machine Learning Research, 20(1):1474–1520, 2019. 15\n\nZhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after sgd reaches zero loss?–a mathematical\n\nframework. In International Conference on Learning Representations, 2021. 2, 3, 14, 16, 21, 22\n\nZhouzi Li, Zixuan Wang, and Jian Li. Analyzing sharpness along gd trajectory: Progressive sharpening and\n\nedge of stability. arXiv preprint arXiv:2207.12678, 2022. 14\n\nYong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and scalable In Proceedings of the IEEE/CVF Conference on Computer Vision and\n\nsharpness-aware minimization. Pattern Recognition, pp. 12360–12370, 2022. 2\n\nKaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the generalization benefit of normalization\n\nlayers: Sharpness reduction. arXiv preprint arXiv:2206.07085, 2022. 6, 14\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nChao Ma and Lexing Ying. On linear stability of sgd and input-smoothness of neural networks. Advances in\n\nNeural Information Processing Systems, 34:16805–16817, 2021. 14\n\nChao Ma, Lei Wu, and Lexing Ying. The multiscale structure of neural network loss functions: The effect on\n\noptimization and origin. arXiv preprint arXiv:2204.11326, 2022. 14\n\nJan R Magnus. On differentiating eigenvalues and eigenvectors. Econometric theory, 1(2):179–191, 1985.\n\n70\n\nStephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian\n\ninference. Journal of Machine Learning Research, 18:1–35, 2017. 15\n\nBehnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in\n\ndeep learning. In Advances in Neural Information Processing Systems, pp. 5947–5956, 2017. 1, 2\n\nMatthew D Norton and Johannes O Royset. Diametrical risk minimization: Theory and computations. Ma-\n\nchine Learning, pp. 1–19, 2021. 1\n\nAntonio Orvieto, Anant Raj, Hans Kersting, and Francis Bach. Explicit regularization in overparametrized\n\nmodels via noise injection. arXiv preprint arXiv:2206.04613, 2022. 5\n\nWeijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov’s accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pp. 2510– 2518, 2014. 15\n\nColin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via lipschitz aug-\n\nmentation. In Advances in Neural Information Processing Systems, pp. 9722–9733, 2019a. 2\n\nColin Wei and Tengyu Ma. Improved sample complexities for deep networks and robust classification via an\n\nall-layer margin. arXiv preprint arXiv:1910.04284, 2019b. 2\n\nDongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization.\n\nAdvances in Neural Information Processing Systems, 33:2958–2969, 2020. 1, 2, 4, 9, 16\n\nLei Wu, Chao Ma, and Weinan E. How sgd selects the global minima in over-parameterized learning: A\n\ndynamical stability perspective. Advances in Neural Information Processing Systems, 31, 2018. 14\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learn-\n\ning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. 1\n\nYang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving generalization\n\nin deep learning. arXiv preprint arXiv:2202.03599, 2022. 3\n\nYaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial model perturbation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8156–8165, 2021. 1, 2\n\nJuntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware training. arXiv preprint arXiv:2203.08065, 2022. 3, 5\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nCONTENTS\n\n1 Introduction\n\n2 Related Works\n\n3 Notations and Assumptions\n\n4 Explicit and Implicit Bias in the Full-Batch Setting\n\n4.1 Worst- and Ascent-direction Sharpness Have Different Explicit Biases . . . . . . . . . . . .\n\n4.2 SAM Provably Decreases Worst-direction Sharpness\n\n. . . . . . . . . . . . . . . . . . . . .\n\n4.3 Analysis Overview For Sharpness Reduction in Phase II of Theorem 4.5 . . . . . . . . . . .\n\n5 Explicit and Implicit Biases in the Stochastic Setting\n\n5.1 Stochastic Worst-, Ascent- and Average- direction Sharpness Have the Same Explicit Biases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nas Average Direction Sharpness\n\n5.2 Stochastic SAM Minimizes Average-direction Sharpness . . . . . . . . . . . . . . . . . . .\n\n6 Conclusion\n\nA Additional Related Works\n\nB Experimental Details for Figure 1\n\nC Additional Preliminary\n\nD Well-definedness of SAM\n\nE Proof Setups\n\nE.1 Proofs of Theorems 5.2 and E.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nF Properties of Limiting Map of Gradient Flow, Φ\n\nG Analysis for Explicit Bias\n\nG.1 A General Theorem for Explicit Bias in the Limit Case . . . . . . . . . . . . . . . . . . . .\n\nG.2 Bad Limiting Regularizers May Not Capture Explicit Bias\n\n. . . . . . . . . . . . . . . . . .\n\nG.3 Proof of Theorem G.6 .\n\n.\n\nG.4 Proofs of Corollary G.7 .\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nG.5 Limiting Regularizers For Different Notions of Sharpness . . . . . . . . . . . . . . . . . . .\n\nG.6 Proof of Theorems 4.2 and 5.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nH Analysis Full-batch SAM on Quadratic Loss (Proof of Theorem 4.8)\n\nH.1 Entering Invariant Set .\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nH.2 Alignment to Top Eigenvector\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n13\n\n1\n\n2\n\n3\n\n4\n\n4\n\n5\n\n6\n\n7\n\n8\n\n8\n\n9\n\n14\n\n15\n\n15\n\n16\n\n19\n\n20\n\n21\n\n23\n\n23\n\n25\n\n26\n\n28\n\n28\n\n31\n\n31\n\n32\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nI Analysis for Full-batch SAM on General Loss (Proof of Theorem 4.5)\n\nI.1\n\nPhase I (Proof of Theorem I.1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nI.1.1\n\nTracking Gradient Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nI.1.2 Decreasing Loss\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nI.1.3\n\nEntering Invariant Set\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nI.2\n\nPhase II (Proof of Theorem I.3) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nI.2.1 Alignment to Top Eigenvector . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nI.2.2\n\nTracking Riemannian Gradient Flow . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nProof of Theorem 4.5 .\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nProofs of Corollaries 4.6 and 4.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nderivations for Section 4.3 .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nI.3\n\nI.4\n\nI.5\n\nJ Analysis for 1-SAM (Proof of Theorem 5.4)\n\nJ.1\n\nPhase I (Proof of Theorem J.1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nJ.1.1\n\nTracking Gradient Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nJ.1.2 Decreasing Loss\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nJ.2\n\nPhase II (Proof of Theorem J.2)\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nJ.2.1 Convergence Near Manifold . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nJ.2.2\n\nTracking Riemannian Gradient Flow . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nProof of Theorem 5.4 .\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nProofs of Corollaries 5.6 and 5.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nJ.3\n\nJ.4\n\nJ.5 Other Omitted Proofs for 1-SAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nK Technical Lemmas\n\nL Omitted Proofs on Continuous Approximation\n\nA ADDITIONAL RELATED WORKS\n\n38\n\n39\n\n39\n\n40\n\n41\n\n48\n\n48\n\n54\n\n55\n\n55\n\n56\n\n57\n\n58\n\n58\n\n59\n\n66\n\n67\n\n67\n\n68\n\n68\n\n68\n\n69\n\n73\n\nImplicit Bias of Sharpness Minimization. Recent theoretical works (Blanc et al., 2019; Damian et al., 2021; Li et al., 2021) show that SGD with label noise implicitly biased toward local minimizers with a smaller trace of Hessian under the assumption that the minimizers locally connect as a manifold. Arora et al. (2022) show that normalized GD implicitly penalizes the largest eigenvalue of the Hessian. Ma et al. (2022) argues that such sharpness reduction phenomena can also be caused by a multi-scale loss landscape. Lyu et al. (2022) show that GD with weight decay on a scale invariant loss function implicitly decreases penalize the spherical sharpness, i.e., the largest eigenvalue of the Hessian evaluated at the normalized parameter.\n\nAnother line of works study the sharpness minimization effect of large learning rate assuming the (stochastic) gradient descent converges in the end of training, where the analysis is mainly based on linear stability (Wu et al., 2018; Cohen et al., 2021; Ma et al., 2021; Cohen et al., 2022). Recent theoretical analysis (Damian et al., 2022; Li et al., 2022) show that the sharpness minimization effect of large learning rate in gradient descent do not necessarily rely on the convergence assumption and linear stability via a four-phase characterization of the dynamics at the so-called Edge of Stability regime (Cohen et al., 2021).\n\nComparison with concurrent work Bartlett et al. (2022). Bartlett et al. (2022) prove that on quadratic loss, the iterate of SAM (Equation 11) and its gradient converges to the top eigenvector of Hessian, which is almost the same as our Theorem 4.8. Assuming such alignment for a general loss, the work of Bartlett et al.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\n(2022) shows that the largest eigenvalue of Hessian decreases in the next step. This paper also proves such a Hessian-gradient alignment for general loss functions (Lemma I.19) and an end-to-end theorem showing that the largest eigenvalue of Hessian and worst-direction sharpness decrease along the trajectory of SAM (Theorem 4.5), which are not shown in Bartlett et al. (2022). Moreover, this paper also characterize implicit bias of stochastic SAM with batch size 1, which is minimizing the average-direction sharpness, while Bartlett et al. (2022) only considers the deterministic case.\n\nComparison with Arora et al. (2022). Our proof uses a similar framework as Arora et al. (2022). However, our analysis has its own difficulty for the following reasons. First, Arora et al. (2022) only deal with the deterministic case, while our analysis extends to stochastic SAM as well (Section 5). Second, our analysis for the deterministic case is different from that of Arora et al. (2022) in the following two aspects. First, the alignment analysis is more complicated because we have two hyperparameters,learning rate η and perturbation radius ρ, while Arora et al. (2022) only needs to deal with one hyperparameter, learning rate η. Second, the mechanism of penalizing worst-direction sharpness is different, which can be seen from the dependency of the sharpness-reduction rate over learning rate η. In Arora et al. (2022), normalized GD reduces the sharpness via a second-order effect of GD and thus the sharpness is reduced by O(η2) per step. In our analysis, for fixed small perturbation radius ρ, the sharpness is reduced by O(ρ2η) per step, which is linear in η.\n\nAnalyzing Discrete-time Dynamics via Continuous-time Approaches. There is a long line of research that shows the trajectory of stochastic discrete iterations with decaying step size eventually tracks the solution of some ODE (see Kushner et al. (2003); Borkar et al. (2009); Duchi et al. (2018) and the reference therein). However, those results mainly focus on the convergence property of the stochastic iterates (e.g., convergence to stationary points), while we are interested in characterizing the trajectory especially when the process is running for a long time even after the iterate reaches the neighborhood of the manifold of stationary points.\n\nRecently there has been an effort of modeling the discrete-time trajectory of (stochastic) gradient methods by continuous-time approximation (Su et al., 2014; Mandt et al., 2017; Li et al., 2017; 2019). Notably, Li et al. (2019) presents a general and rigorous mathematical framework to prove such continuous-time approximation. More specifically, Li et al. (2019) proves for various stochastic gradient-based methods, the discrete-time weakly converges to the continuous-time one when LR η → 0 in Θ(1/η) steps. The main difference between our results with these results (e.g., Theorem 9 in Li et al. (2019)) is that we focus on a much longer training regime, i.e., T = Θ(η−1ρ−2) steps where the previous continuous-time approximation results no longer holds throughout the entire training. As a result, their continuous approximation is only equivalent to the Phase I dynamics in our Theorems 4.5 and 5.4 and cannot capture the dynamics of SAM in Phase II, when the sharpness-reduction implicit bias happens. The latter requires a more fine-grained analysis to capture the effects of higher-order terms in η and ρ in SAM Equation 3.\n\nB EXPERIMENTAL DETAILS FOR FIGURE 1\n\nIn Figure 1, we choose F1(x) = x2 2 + 8 and F2(x) = 4(1 − x1)2 + (1 − x2)2 + 1. The loss L has a zero loss manifold {x = 0} and the eigenvalues of its Hessian on the manifold are F1(x) and F2(x) with F1(x) ≥ 8 > 6 ≥ F2(x) on [0, 1]2. The loss L has a zero loss manifold {x3 = x4 = 0} of codimension M = 2 and the two non-zero eigenvalues of ∇2L of any point x on the manifold are λ1(∇2L(x)) = F1(x1, x2) and λ2(∇2L(x)) = F2(x1, x2).\n\n1 + 6x2\n\nAs our theory predicts,\n\n1. Full-batch SAM (Equation 3) finds the minimizer with the smallest top eigenvalue F1(x), which is x1 =\n\n0, x2 = 0, x3 = 0, x4 = 0;\n\n2. GD on ascent-direction loss LAsc\n\nρ\n\nwhich is x1 = 1, x2 = 1, x3 = 0, x4 = 0;\n\n(2) finds the minimizer with the smallest bottom eigenvalue, F2(x),\n\n3. Stochastic SAM (Equation 13) (with L0(x, y) = F1(x)y2\n\n0, L1(x, y) = F2(x)y2\n\n1) finds the minimizer with\n\nsmallest trace of Hessian, which is x1 = 4/5, x2 = 1/7, x3 = 0, x4 = 0.\n\nC ADDITIONAL PRELIMINARY\n\nIn this section, we introduce some additional notations and clarification before the proof.\n\nWe will first give the detailed definition of differentiable submanifold.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nDefinition C.1 (Differentiable Submanifold of RD). We call a subset Γ ⊂ RD a Ck submanifold of RD if and only if for every x ∈ Γ, there exists a open neighborhood U of x and an invertible Ck map ψ : U → RD, such that ψ(Γ ∩ U ) = (Rn × {0}) ∩ ψ(U ).\n\nNecessity of Manifold Assumption. The connectivity of the set of local minimizers implied by the manifold assumption above allows us to take limits of perturbation radius ρ → 0 while still yield interesting and insightful implicit bias results in the end-to-end analysis. So far almost all analysis of implicit bias for general model parameterizations relies on Taylor expansion, e.g. Blanc et al. (2019); Damian et al. (2021); Li et al. (2021); Arora et al. (2022), so does the derivation of the SAM algorithm Foret et al. (2020); Wu et al. (2020). Thus it’s crucial to consider small perturbation size ρ. On the contrary, if the set of global minimizers are a set of discrete points, then with small perturbation radius ρ, implicit bias of optimizers is not sufficient to drive the iterate from global minimum to the other one.\n\nImplicit versus Explicit Bias. If an algorithm or optimizer has a bias towards certain type of global/local minima of the loss over other minima of the loss, and this bias is not encoded in the loss function, then we call such bias an implicit bias. On the other hand, a bias emerges as solely a consequence of successfully minimizing certain regularized loss regardless of the optimizers (as long as the optimzers minimize the loss), we say such bias is an explicit bias of the regularized loss (or the regularizer).\n\nAs a concrete example, we will prove that full-batch SAM (Equation 3) prefers local minima with certain sharpness property. The bias stems from the particular update rule of full-batch SAM (Equation 3), and not all optimizers for the intended target loss function LAsc (Equation 2) has this bias. Therefore, it’s considered as an implicit bias. As an example for explicit bias, all optimizers minimizing a loss combined with l2 regularization will prefer model with smaller parameter norm and this is considered as an explicit bias of l2 regularization.\n\nρ\n\nUsage of O(·) Notation: Our analysis assumes small η and ρ while treating all other problem-dependent parameters as constants, such as the dimension of parameter space and the maximum possible value of derivatives (of different orders) of loss function L and the limit map Φ. In O(·), Ω(·), o(·), ω(·), Θ(·), we hide all the dependency related to the problem, e.g., the (unique) initialization xinit, the manifold Γ, compact set U ′ in Theorem 4.2, and the continuous time T3 in Theorems 4.5 and 5.4, and only keep the dependency on ρ and η. For example, O(f (ρ)) is a placeholder for some function g(ρ) such that there exists problem-dependent constant C > 0, ∀ρ > 0, |g(ρ)| ≤ C|f (ρ)|. In informal equations such as Equation 31 in the proof sketch section, we are a bit more sloppy and hide dependency on x(t) in O(·) notation as well. But these will be formally dealt with in the proofs.\n\nIll-definedness of SAM with Zero Gradient. The update rule of SAM (Equations 3 and 13) is ill-defined when the gradient is zero. However, our analysis in Appendix D shows that when the stationary point of loss L, {x | ∇L(x) = 0}, is a zero-measure set, for any perturbation radius ρ, except for countably many learning rates, full-batch SAM is well-defined for almost all initialization and all steps (Theorem D.1). A similar result is shown for stochastic SAM if the stationary points of each stochastic loss form a zero-measure set (Theorem D.2). Thus SAM is generically well-defined. For the sake of rigorousness, when SAM encountering zero gradients, we modify the algorithm via replacing the ill-defined normalized gradient by an arbitrary vector with unit norm and our analysis for implicit bias of SAM still holds.\n\nD WELL-DEFINEDNESS OF SAM\n\n∥∇L(x)∥2\n\nIn this section, we discuss the well-definedness of SAM. When ∇L(x) = 0, SAM (Equation 3) is not welldefined, because the normalized gradient ∇L(x) is not well-defined. The main result of this section are Theorems D.1 and D.2, which say that (stochastic) SAM starting from random initialization only has zero probability to reach points that SAM is undefined (i.e., points with zero gradient), for all except countably many learning rates. These results follow from Theorem D.3, which is a more general theorem also applicable to other discrete update rules as well, like SGD. Note results in this section does not rely on the manifold assumption, i.e., Assumption 3.2. We end this section with a concrete example where SAM is undefined with constant probability, suggesting that the exclusion of countably many learning rates are necessary in Theorems D.1 and D.2. Theorem D.1. Consider any C2 loss L with zero-measure stationary set {x | ∇L(x) = 0}. For every ρ > 0, except countably many learning rates, for almost all initialization and all t, the iterate of full-batch SAM (Equation 3) x(t) has non-zero gradient and is thus well-defined.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTheorem D.2. Consider any C2 losses {Lk}M k=1 with zero-measure stationary set {x | ∇Lk(x) = 0} for each k ∈ [M ]. For every ρ > 0, except countably many learning rates η, for almost all initialization and all t, with probability one of the randomness of the algorithm, the iterate of stochastic SAM (Equation 13) x(t) has non-zero gradient and is thus well-defined. 5\n\nBefore present our main theorem (Theorem D.3), we need to introduce some notations first. For a map F mapping from RD \\ Z → RD, we define that Fη : RD \\ Z → RD as Fη(x) ≜ x − ηF (x) for any η ∈ R+. η (x) ≜ x − ηF n(x), for any x ∈ RD. We further Given a sequence of functions {F n}∞ η (x) ≜ F n\n\nn=1, we define F n (x)) for any n ≥ 1 and that F\n\ndefine that F\n\nη(x) = x.\n\nn−1 η\n\nη (F\n\nn\n\n0\n\nTheorem D.3. Let Z be a closed subset of RD with zero Lebesgue measure and μ be any probability measure on RD that is absolutely continuous to the Lesbegue measure. For any sequence of C1 functions F n : RD \\ Z → RD, n ∈ N+, the following claim holds for all except countably many η ∈ R+:\n\n(cid:16)\n\nμ\n\n{x ∈ RD | ∃n ∈ N, F\n\nn\n\nη (x) ∈ Z and ∀0 ≤ i ≤ n − 1, F\n\ni\n\n(cid:17)\n\nη(x) /∈ Z}\n\n= 0.\n\nIn other words, for almost all η (except countably many positive numbers), iteration x(t + 1) = x(t) − η(x(0)) will not enter Z almost surely, provided that x(0) is sampled from μ.\n\nηF (x(t)) = F\n\nt\n\nTheorem D.1 and Theorem D.2 follows immediately from Theorem D.3.\n\nProof of Theorem D.1. Let F (x) = ∇L(x + ρ ∇L(x) ) and Z = {x ∈ RD | ∇L(x) = 0}. We can easily check F is C1 on RD \\ Z and by assumption Z is a zero-measure set. Applying Theorem D.3 with F n ≡ F for all n ∈ N+, we get the desired results.\n\n∥∇L(x)∥2\n\nProof of Theorem D.2. Let Gk(x) = ∇L(x + ρ ∇Lk(x) k=1{x ∈ RD | ∇Lk(x) = 0}. We ∥∇Lk(x)∥2 can easily check Fk is C1 on RD \\ Z and by assumption Z is a zero-measure set. Applying Theorem D.3 with F n = Gkn for all n ∈ N+ where kn is the nth data/batch sampled by the algorithm, we get the desired results.\n\n) and Z = ∪M\n\nNow we will turn to the proof of Theorem D.3, which is based on the following two lemmas. Lemma D.4. Let Z be a closed subset of RD with zero Lebesgue measure and F : RD \\ Z → RD be a continuously differentiable function. Then except countably many η ∈ R+, {x ∈ Rd \\ Z | det(∂Fη(x)) = 0} is a zero-measure set under Lebesgue measure. Lemma D.5. Let Z be a closed subset of RD with zero Lebesgue measure and H : RD \\ Z → RD be a continuously differentiable function. If {x ∈ Rd \\ Z | det(∂H(x)) = 0} is a zero-measure set, then for any zero-measure set Z ′, H −1(Z ′) is a zero-measure set.\n\nProof of Theorem D.3. It suffices to prove that for every N ∈ N+, at most for countably many η:\n\n(cid:16)\n\nμ\n\n{x ∈ RD | F\n\nN\n\nη (x) ∈ Z and ∀0 ≤ i ≤ N − 1, F\n\ni\n\nη(x) /∈ Z}\n\n(cid:17)\n\n= 0.\n\n(15)\n\nThe desired results is immediately implied by the above claim because the countable union of countable set is still countable, and countable union of zero-measure set is still zero measure. To prove Equation 15, we first introduce some notations. For any η > 0, 0 ≤ n ≤ N − 1, and x ∈ RD, we define F η(x) = x. We extend the definition to set in a natural\n\nη (x)), where F\n\n−(n+1) η\n\n−n\n\nη\n\n0\n\nway, namely F\n\n)−1(F −n\n\n−n\n\n(x) ≜ (F N −n η (S) ≜ ∪x∈SF (cid:16)\n\n(Z) = μ\n\nF −N\n\nη\n\nη (x) for any S ⊆ RD. Under this notation, we have that\n\n{x ∈ RD | F\n\nN\n\nη (x) ∈ Z and ∀0 ≤ i ≤ N − 1, F\n\ni\n\nη(x) /∈ Z}\n\n(cid:17)\n\nWe will prove by induction. We claim that for each 0 ≤ n ≤ N except for countably many η ∈ R+, F η (Z) has zero Lebesgue measure. The base case n = 0 is by trivial as Z is assumed to be zero-measure. Suppose\n\n−n\n\n5Though we call Equation 13 1-SAM, but our result here applies to any batch size where Lk can be regarded as the\n\nloss for k-th possible batch and M is the number of the total number of batches.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nthis holds for n. By Lemma D.4, except countably many η ∈ R+, {x ∈ Rd \\F 0} is a zero-measure set. Next by Lemma D.5 if for some η ∈ R+, {x ∈ Rd\\F\n\n−n\n\nη (Z) | det(∂F N −n−1 η (Z) | det(∂F N −n−1\n\n−n\n\nη\n\nη\n\n(x)) =\n\n(x)) =\n\n0} is a zero-measure set, then F tion, we know that except countably many η ∈ R+, for all integer 0 ≤ n ≤ N , F Since μ is absolutely continuous to Lebesgue measure, μ(F −N\n\nη (Z)) is a zero-measure set. Then by inducη (Z) is zero-measure.\n\n(Z)) = 0.\n\n(Z) = (F N −n−1\n\n)−1(F\n\n−n\n\nη\n\nη\n\n−n−1 η\n\n−n\n\nWe end this section with the proofs of Lemmas D.4 and D.5.\n\nProof of Lemma D.4. We use λi(x) to denote that the real part of the ith eigenvalue of the matrix ∂F (x) in the descending order. Since ∂F (x) is continuous in x, λi(x) is continuous in x as well, for any i ∈ [D], and thus {x ∈ RD \\ Z | λi(x) = 1/η} is a measurable set. Note that for a fixed i ∈ [D], for each positive integer n, let In be the set of η where μ({x ∈ RD \\ Z | λi(x) = 1/η}) > 1/n, then |In| ≤ n, because\n\n|In| n\n\n(cid:88)\n\n≤\n\nη∈In\n\nμ(({x ∈ RD \\ Z | λi(x) = 1/η}) ≤ μ(({x ∈ RD \\ Z | 1/λi(x) ∈ In}) ≤ 1.\n\nTherefore, there are at most countably many η ∈ R+, such that μ({x ∈ RD \\ Z | λi(x) = 1/η}) > 0. Further note that det(∂Fη(x)) = 0 ⇐⇒ ∃i ∈ [D], λi(x) = 1/η, we know that there are at most countably many η ∈ R+, such that μ({x ∈ RD \\ Z | det(∂Fη(x)) = 0}) = 0. This completes the proof.\n\nProof of Lemma D.5. Denote {x ∈ RD \\ Z | det(∂H(x)) = 0} by Z ′′, since det(∂H(x)) is continuous in x as F is C1, Z ′′ is relatively closed in RD \\ Z. Since Z ′ is a closed set, RD \\ (Z ′ ∪ Z ′′) is open. Thus for all x ∈ RD \\ (Z ′ ∪ Z ′′) with det(∂H(x)) ̸= 0, there exists a open neighborhood of x, U , where for all x′ ∈ U , det(∂H(x′)) ̸= 0, since thus det(∂H(x)) is continuous. This further implies H is invertible on U and its inverse (H|U )−1 is differentiable on F (U ). Therefore, (H|U )−1 maps any zero-measure set to a zeromeasure set. In particular, (H|U )−1(Z ′ ∩ H(U )) is zero measure, so is (H)−1(Z ′) ∩ U ⊂ (H|U )−1(Z ′ ∩ H(U )). Now for every x ∈ RD \\ Z we take an open neighborhood Ux ⊆ RD \\ (Z ′ ∪ Z ′′). Since RD is a separable metric space, the open cover of RD, {Ux}x∈RD\\(Z′∪Z′′) has a countable subcover, {Ux}x∈I , where I is a countable set of RD \\ (Z ′ ∪ Z ′′). Therefore we have that H −1(Z ′) \\ (Z ′ ∪ Z ′′) = H −1(Z ′) ∩ (RD \\ (Z ′ ∪ Z ′′)) = ∪x∈I H −1(Z ′) ∩ Ux is a zero-measure set. Thus H −1(Z ′) is also zero-measure since Z ′, Z ′′ are both zero-measure. This completes the proof.\n\nWe end this section with an example where SAM is undefined with constant probability. Theorem D.6. For any η, ρ > 0, there is a C2 loss function L : R → R satisfying that (1) L has a unique stationary point and (2) the set of initialization that makes SAM with learning rate η and perturbation radius ρ to reach the unique stationary point has positive Lebesgue measure.\n\nProof of Theorem D.6. We first consider the case with ρ = η = 1 with\n\nL(x) =\n\n \n\n\n\nx2/2 + x + 1/2, x4/64 + x2/8, x2/2 − x + 1/2,\n\nfor x ∈ (−∞, −2); for x ∈ [−2, 2]; for x ∈ (2, ∞).\n\n(16)\n\nWe first check L is indeed C1: L(2) = L(−2) = 1/2, L′(2) = −L′(−2) = 1 and L′′(2) = L′′(−2) = 1. Now we claim that for all |x(0)| > 2, x(1) = 0, which is a stationary point. Note that L is even and monotone increasing on [0, ∞), we have ∇L(x)/|∇L(x)| = sign(x). Thus for |x(t)| > 1, it holds that |x(t) + sign(x(t)| > 2 and therefore\n\nx(t + 1) =x(t) − ηL′(x(t) + ρ∇L(x)/|∇L(x)|) =x(t) − L′(x(t) + sign(x(t))) =x(t) − (x(t) + sign(x(t)) − sign(x(t) + sign(x(t))))\n\n=x(t) − x(t) = 0.\n\n(17)\n\nNow we turn to the case with arbitrary positive η, ρ. It suffices to consider Lη,ρ(x) ≜ ρ the calculation for ρ = η = 1 to verify for any |x| > 2ρ,\n\nη L( x\n\nρ ). We can use\n\nL′\n\nη,ρ(x + ρ sign(L′\n\nη,ρ(x))) = L′\n\nη,ρ(x + ρ sign(x)) =\n\n1 η\n\nL(x/ρ + sign(x)) =\n\nx η\n\n,\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nnamely x − L′\n\nη,ρ(x + ρ sign(L′\n\nη,ρ(x))) = 0. This completes the proof.\n\nA common (but wrong) intuition here is that, for a continuously differentiable update rule, as long as the points where the update rule is ill-defined (here it means the points with zero gradient) has zero measure, then almost surely for all initialization, gradient-based optimization algorithms like SAM will not reach exactly at any stationary point. However the above example negate this intuition. The issue here is that though a differentiable map (like SAM x (cid:55)→ x − η∇L(x + ρ ∇L(x) )) always maps the zero-measure set to zero-measure set, the preimage of zero-measure set is not necessarily zero-measure, as the map x (cid:55)→ x−η∇L(x+ρ ∇L(x) ) is not necessarily invertible. The update rule of SAM is not invertible at 0 is exactly the reason of why preimage of 0 has a positive measure.\n\n∥∇L(x)∥ 2\n\n∥∇L(x)∥ 2\n\nE PROOF SETUPS\n\nIn this section we provide details of our proof setups, including notations and assumptions/settings. We first introduce some additional notations that will be used in the proofs. For any subset S ∈ RD, we define dist(x, S) ≜ inf y∈S ∥x − y∥2. For any d > 0 and any subset S ∈ RD, we define Sd ≜ {x ∈ RD | dist(x, S) ≤ d}. Our convention is to use K to denote a compact set and U to denote an open set.\n\nBelow we restate our main assumption in the full-batch case and related notations in Section 3. Throughout the analysis, we fix our initialization as xinit, our loss function as L : RD → R. Assumption 3.2. Assume loss L : RD → R is C4, and there exists a C2 submanifold Γ of RD that is a (D − M )-dimensional for some integer 1 ≤ M ≤ D, where for all x ∈ Γ, x is a local minimizer of L and rank(∇2L(x)) = M .\n\nNotations for Full-Batch Setting: Given any point x ∈ Γ, define Px,Γ as the projection operator onto the manifold of the normal space of Γ at x and P ⊥ x,Γ = ID − Px,Γ. Given the loss function L, its gradient flow is denoted by mapping φ : RD × [0, ∞) → RD. Here, φ(x, τ ) denotes the iterate at time τ of a gradient flow starting at x and is defined as the unique solution of φ(x, τ ) = x − (cid:82) τ 0 ∇L(φ(x, t))dt, ∀x ∈ RD. We further define the limiting map of φ(x, ·) as Φ(x) = limτ →∞ φ(x, τ ), that is, Φ(x) denotes the convergent point of the gradient flow starting from x. For convenience, we define λi(x), vi(x) as λi(∇2L(Φ(x))), vi(∇2L(Φ(x))) whenever the latter is well defined. When x(t) and Γ is clear from context, we also use λi(t) := λi(x(t)), vi(t) := vi(x(t)), P ⊥\n\nΦ(x(t)),Γ, Pt,Γ := PΦ(x(t)),Γ.\n\nt,Γ := P ⊥\n\nDefinition 3.3 (Attraction Set). Let U be the attraction set of Γ under gradient flow, that is, a neighborhood of Γ containing all points starting from which gradient flow w.r.t. loss L converges to some point in Γ, or mathematically, U ≜ {x ∈ RD|Φ(x) exists and Φ(x) ∈ Γ}.\n\nBelow we restate the setting for stochastic loss of batch size one in Section 5.\n\nSetting 5.1. Let the total number of data be M . Let fk(x) be the model output on the k-th data where fk is a C4-smooth function and yk be the k-th label, for k = 1, . . . , M . We define the loss on the k-th data as Lk(x) = l(fk(x), yk) and the total loss L = (cid:80)M k=1 Lk/M , where function l(y′, y) is C4-smooth in y′. We also assume for any y ∈ R, it holds that arg miny′∈R l(y′, y) = y and that ∂2l(y′,y) (∂y′)2 |y′=y > 0. Finally, we denote the set of global minimizers of L with full-rank Jacobian by Γ and assume that it is non-empty, that is,\n\nΓ ≜ (cid:8)x ∈ RD | fk(x) = yk, ∀k ∈ [M ] and {∇fk(x)}M\n\nk=1 are linearly independent(cid:9) ̸= ∅.\n\nTheorem 5.2. Loss L, set Γ and integer M defined in Setting 5.1 satisfy Assumption 3.2.\n\nIn our analysis, we prove our main theorems in the stochastic setting under a more general condition than Setting 5.1, which is Condition E.1 (on top of Assumption 3.2). The only usage of Setting 5.1 in the proof is Theorems 5.2 and E.2. Condition E.1. Total loss L = 1 k=1 Lk. For each k ∈ [M ], Lk is C4, and there exists a (D − 1)- dimensional C2-submanifold of RD, Γk, where for all x ∈ Γk, x is a global minimizer of Lk, Lk(x) = 0 and rank(∇2Lk(x)) = 1. Moreover, Γ = ∩M\n\nk=1Γk for Γ defined in Assumption 3.2.\n\n(cid:80)M\n\nM\n\nTheorem E.2. Setting 5.1 implies Condition E.1.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nNotations for Stochastic Setting: Since Lk is rank-1 on Γk for each k ∈ [M ], we can write it as Lk(x) = Λk(x)wk(x)w⊤ k (x) for any x ∈ Γ, where wk is a continuous function on Γ with pointwise unit norm. Given the loss function Lk, its gradient flow is denoted by mapping φk : RD × [0, ∞) → RD. Here, φk(x, τ ) denotes the iterate at time τ of a gradient flow starting at x and is defined as the unique solution of φk(x, τ ) = x − (cid:82) τ 0 ∇Lk(φk(x, t))dt, ∀x ∈ RD. We further define the limiting map Φk as Φk(x) = limτ →∞ φk(x, τ ), that is, Φk(x) denotes the convergent point of the gradient flow starting from x. Similar to Definition 3.3, we define Uk = {x ∈ RD|Φ(x) exists and Φk(x) ∈ Γk} be the attraction set of Γi. We have that each Uk is open and Φk is C Definition E.3. A function L is μ-PL in a set U iff ∀x ∈ U , ∥∇L(x)∥2 Definition E.4. The spectral 2-norm of a k-order tensor Xi1,...,ik ∈ Rd1×...×dk is defined as\n\non Uk by Lemma B.15 in Arora et al. (2022).\n\n2 ≥ 2μ(L(x) − inf x∈U L(x)).\n\n2\n\nLemma E.5 (Arora et al. r(K), μ(K), ∆(K) ∈ R+ such that\n\n(2022) Lemma B.2). Given any compact set K ⊆ Γ,\n\nthere exist\n\n∥X∥2 =\n\nmax xi∈Rdi ,∥xi∥2=1\n\nX[x1, ..., xk].\n\n1. K r(K) ∩ Γ is compact. 2. K r(K) ⊂ U ∩ (∩k∈[M ]Uk). 3. L is μ(K)-PL on K r(K). 4. inf x∈Kr(K)(λ1(∇2L(x)) − λ2(∇2L(x))) ≥ ∆(K) > 0. 5. inf x∈Kr(K) λM (∇2L(x)) ≥ μ(K) > 0. 6. inf x∈Kr(K) λ1(∇2Lk(x)) ≥ μ(K) > 0.\n\nGiven compact set K ⊂ Γ, we further define\n\nζ(K) = sup\n\n∥∇2L(x)∥2, ν(K) = sup\n\n∥∇3L(x)∥2, Υ(K) = sup\n\n∥∇4L(x)∥2,\n\nx∈Kr(K)\n\nx∈Kr(K)\n\nx∈Kr(K)\n\nξ(K) = sup\n\n∥∇2Φ(x)∥2, χ(K) = sup\n\nx∈Kr(K)\n\nx,y∈Kr(K)\n\n∥∇2Φ(x) − ∇2Φ(y)∥2 ∥x − y∥2\n\n.\n\nSimilarly, we use notations like ζk(K), νk(K), Υk(K), ξk(K), χk(K) to denote the counterpart of the above quantities defined for stochastic loss Lk and its limiting map Φk for k ∈ [M ]. Lemma E.6 (Arora et al. (2022), Lemma B.5 and B.7). Given any compact subset K ⊂ Γ, let r(K) be defined in Lemma E.5, there exist 0 < h(K) < r(K) such that\n\n1.\n\nsup x∈Kh(K)\n\nL(x) − inf\n\nx∈Kh(K)\n\nL(x) ≤ μ(K)ρ2(K)\n\n8\n\n.\n\n2. ∀x ∈ K h(K), Φ(x) ∈ K r(K)/2. 3. ∀x ∈ K h(K), ∥x − Φ(x)∥2 ≤ 8μ(K)2 ζ(K)ν(K) . 4. The whole segment xΦ(x) lies in K r(K), so does xΦk(x), for any k ∈ [D].\n\nThe proof of the lemmas above can be found in Arora et al. (2022). Readers should note that although Arora et al. (2022) only prove these lemmas when K is a special compact set (the trajectory of an ODE), all the proof does not use any property of K other than it is a compact subset of Γ, and thus our Lemmas E.5 and E.6 hold for general compact subsets of Γ.\n\nIn the rest part of the appendix, for convenience we will drop the dependency on K in various constants when there is no ambiguity.\n\nE.1 PROOFS OF THEOREMS 5.2 AND E.2\n\nLet Tx ≜ Proof of Theorem 5.2. Define F : RD → RM as [F (x)]k = fk(x), ∀k ∈ [M ]. x be the orthogonal complement of Tx in RD. Now we apply implicit funcspan({∇fk(x)}M tion theorem on F at each x ∈ Γ. Without loss of generality (e.g. by rotating the coordinate system), we can assume that x = 0, Tx = RD−M × {0}, and that T ⊥ x = {0} × RM . Implicit function theorem ensures that there are two open sets 0 ∈ U ⊂ RD−M and 0 ∈ V ⊂ RM and an invertible C4 map g : U → V such that\n\nk=1) and T ⊥\n\nF −1(Y ) ∩ (U × V ) = {(u, g(u)) | u ∈ U },\n\nwhere Y ≜ [y1, . . . , yM ] ∈ RM . Moreover, {∇fk(x)}M k=1 is linearly independent for every x′ ∈ U × V . Thus by definition of Γ, it holds that Γ ∩ (U × V ) = F −1(y) ∩ (U × V ) = {(u, g(u)) | u ∈ U }. Now for\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nx = (u, v) ∈ U × V , we define ψ : U × V → RD by ψ(u, v) ≜ (u, v − g(u)). We can check that ψ is C4 and ψ(Γ ∩ (U × V )) = {(u, v − g(u)) | v = g(u), u ∈ U )} = {(u, 0) | u ∈ U )} = U × {0} = (RD−M × {0}) ∩ ψ(U ). This proves that Γ is a C4 submanifold of RD of dimension D − M . (c.f. Definition C.1) Since arg miny′∈R l(y′, y) = y for any y ∈ R, it is clear that ∀x ∈ Γ, x is a global minimizer of L. Finally we check the rank of Hessian of loss L. Note that for any x ∈ Γ, ∇2Lk(x) = ∂2l(y′,yk) |y′=yk ∇fk(x)(∇fk(x))⊤ and that ∂2l(y′,yk)\n\n|y′=yk > 0, rank(∇2L(x)) = rank(∂F (x)) = M . This completes the proof.\n\n(∂y′)2\n\n(∂y′)2\n\nProof of Theorem E.2.\n\n(cid:80)M\n\nM\n\nk=1 Lk by definition.\n\n1. L = 1 2. ∀k ∈ [M ], Lk(x) = l(fk(x), yk) is C4 as l and fk are both C4. 3. For any x ∈ Γ, by Lemma 5.5, we have ∇fk(x) ̸= 0. Then there exists an open neighborhood Vk such that Γ ⊂ Vk and ∇fk(x) ̸= 0 for any x ∈ Vk, k ∈ [M ]. Then applying implicit function theorem as in the proof of Theorem 5.2, for any k ∈ M there exists a (D − 1)-dimensional C4-manifold Γ′ k ⊂ Vk, such that for any x′ ∈ V , fk(x′) = yk if and only if x′ ∈ Γ′ k. As for any x ∈ Γ ⊂ Vk, fk(x′) = yk, we can infer that Γ ⊂ Γ′\n\nk. Then Γ ⊂ ∪M\n\nk=1Γk.\n\n4. For any x ∈ Γk, we have fk(x) = yk, which implies Lk(x) = 0. Also as x ∈ V ,∇fk(x) ̸= 0.\n\nBy Lemma J.15, we have rank(∇2L(x)) = 1.\n\nF PROPERTIES OF LIMITING MAP OF GRADIENT FLOW, Φ\n\nIn our analysis, the property of Φ will be heavily used. In this section, we will recap some related lemmas from Arora et al. (2022), and then introduce some new lemmas for the stochastic setting with batch size one. Lemma F.1 (Arora et al. (2022) Lemma B.6). Given any compact set K ⊂ Γ, for any x ∈ K h,\n\n∥x − Φ(x)∥2 ≤\n\n(cid:90) ∞\n\n0\n\n∥\n\ndφ(x, t) dt\n\n∥2 ≤\n\n(cid:115)\n\n2(L(x) − L(Φ(x))) μ\n\n≤\n\n∥∇L(x)∥2 μ\n\n.\n\nLemma F.2. Given any compact set K ⊂ Γ, for any x ∈ K h, (cid:115)\n\n∥∇L(x)∥2 ≤ ζ∥x − Φ(x)∥2 ≤ ζ\n\n2(L(x) − L(Φ(x))) μ\n\n.\n\nProof of Lemma F.2. The first inequality is by Lemma E.5 and Taylor Expansion. The second inequality is by Lemma F.1.\n\nLemma F.3 (Arora et al. (2022) Lemmas B.16 and B.22).\n\n∂Φ(x)∇L (x) = 0, x ∈ U ;\n\n∂Φ (x) ∇2L (x) ∇L (x) = −∂2Φ (x) [∇L (x) , ∇L (x)] , x ∈ U ;\n\n∂Φ (x) ∂2(∇L)(x)[v1, v1] = P ⊥\n\nx,Γ∇(λ1(∇2(L(x)))), x ∈ Γ.\n\nLemma F.4 (Arora et al. (2022) Lemmas B.8 and B.9). Given any compact set K ⊂ Γ, for any x ∈ K h, ζν\n\n∥P ⊥\n\nΦ(x),Γ(x − Φ(x))∥2 ≤\n\n∥∇L (x) − ∇2L (Φ(x)) (x − Φ(x))∥2 ≤\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n∥∇L (x) ∥2 ∥∇2L (Φ(x)) (x − Φ(x))∥2\n\n(cid:12) (cid:12) − 1 (cid:12) (cid:12)\n\n∇L (x) ∥∇L (x) ∥\n\n≤\n\n=\n\n4μ2 ∥x − Φ(x)∥2 2; ∥x − Φ(x)∥2 2;\n\n∥x − Φ(x)∥2;\n\nν 2\n2ν μ\n\n∇2L (Φ(x)) (x − Φ(x)) ∥∇2L (Φ(x)) (x − Φ(x))∥2\n\n+ O(\n\nν μ\n\n∥x − Φ(x)∥2).\n\nLemma F.5 (Li et al. (2021), Lemma 4.3). For x ∈ Γ, ∂Φ(x) = P ⊥ the tangent space of Γ at x. Since d ∂Φ(x)∇2L(x) = 0.\n\nx,Γ, the orthogonal projection matrix onto\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nThe proof of above lemmas can be found in Arora et al. (2022); Li et al. (2021). In the following, we will first show the proof of Lemma 3.1\n\nProof of Lemma 3.1. Since Φ is defined the limit map of gradient flow, it holds that for any t ≥ 0, Φ(φ(x, t)) = Φ(x). Differentiating both sides at t = 0, we have ∂Φ(φ(x, 0)) ∂φ(x,t) ∂t = 0. The proof is completed by noting that ∂φ(x,t)\n\n∂t = −∇L(φ(x, t)) by definition of φ.\n\nLemma F.6. Given any compact set K ⊂ Γ, for any x ∈ K h,\n\n∥∂Φ(x)∇Lk(x)∥2 ≤ (νk + ζkξ)∥x − Φ(x)∥2\n\n2\n\n∥∂Φ(x)∇2Lk(x)\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n∥2 ≤ (νk + ζkξ)∥x − Φ(x)∥2\n\nProof of Lemma F.6. By Lemma E.6 and Taylor Expansion,\n\n∥∂Φ(x)∇Lk(x)∥2 ≤ ∥∂Φ(x)∇2Lk(Φ(x))(x − Φ(x))∥2 + νk∥x − Φ(x)∥2\n\n2\n\n≤ ∥∂Φ(Φ(x))∇2Lk(Φ(x))(x − Φ(x))∥2 + νk∥x − Φ(x)∥2 = ∥P ⊥ = (νk + ζkξ)∥x − Φ(x)∥2 2,\n\nx,Γ∂Φ(Φ(x))∇2Lk(Φ(x))(x − Φ(x))∥2 + νk∥x − Φ(x)∥2\n\n2 + ζkξ∥x − Φ(x)∥2\n\n2\n\n2 + ζkξ∥x − Φ(x)∥2\n\n2\n\nthis proves the first claim.\n\nAgain by Lemma E.5 and Taylor Expansion,\n\n∥∂Φ(x)∇2Lk(x)\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n∥2 ≤ ∥∂Φ(x)∇2Lk(Φ(x))\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n∥2 + νk∥x − Φ(x)∥2\n\n≤ ∥∂Φ(Φ(x))∇2Lk(Φ(x))\n\n= (νk + ζkξ)∥x − Φ(x)∥2,\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n∥2 + (νk + ζkξ)∥x − Φ(x)∥2\n\nthis proves the second claim.\n\nLemma F.7. Suppose x ∈ K h and y = x − η∇L\n\n(cid:16)\n\nx + ρ ∇L(x)\n\n∥∇L(x)∥\n\n(cid:17)\n\n,\n\n∥y − x∥2 ≤ η∥∇L (x) ∥2 + ηζρ\n\n∥Φ(x) − Φ(y)∥2 ≤ ξηρ∥∇L (x) ∥2 + νηρ2 + ξη2∥∇L (x) ∥2 ≤ ζξηρ∥x − Φ(x)∥2 + ζ 2ξη2∥x − Φ(x)∥2\n\n2 + ξζ 2η2ρ2 2 + νηρ2 + ξζ 2η2ρ2\n\nProof of Lemma F.7. For sufficient small ρ, x + ρ ∇L(x)\n\n∥y − x∥2 = η∥∇L\n\nx + ρ\n\n(cid:18)\n\n∥∇L(x)∥ ∈ K r. By Taylor Expansion, ∇L (x) ∥∇L (x) ∥\n\n∥2 ≤ η∥∇L (x) ∥2 + ηζρ\n\n(cid:19)\n\nThis further implies that for sufficiently small η and ρ, xy ∈ K r.\n\nAgain by Taylor Expansion,\n\n∥∂Φ(x)(y − x)∥2 ≤ η∥∂Φ(x)∇L (x) + ρ∂Φ(x)∇2L(x)\n\n∇L (x) ∥∇L (x) ∥\n\n∥2 + ηρ2ν/2 .\n\nBy Lemma F.3, ∂Φ(x)∇L (x) = 0 and ∂Φ (x) ∇2L (x) ∇L (x) = −∂2Φ (x) [∇L (x) , ∇L (x)]. Hence,\n\n∥∂Φ(x)(y − x)∥2 ≤ ηρ∥∇L (x) ∥2∥∂2Φ(x)\n\n(cid:20) ∇L (x)\n\n∥∇L (x) ∥\n\n(cid:21)\n\n,\n\n∇L (x) ∥∇L (x) ∥\n\n∥2 + ηρ2ν/2\n\n≤ ξηρ∥∇L (x) ∥2 + ηρ2ν/2 .\n\nAs xy ∈ K r, by Taylor Expansion,\n\n∥Φ(y) − Φ(x)∥2 ≤ ∥∂Φ(x)(y − x)∥2 + ξ∥y − x∥2\n\n2/2\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nPutting together we have\n\n∥Φ(x) − Φ(y)∥2 ≤ ξηρ∥∇L (x) ∥2 + ηρ2ν + ξη2∥∇L (x) ∥2\n\n2 + ξζ 2η2ρ2 .\n\nFinally, by Lemma F.2, we have\n\n∥Φ(x) − Φ(y)∥2 ≤ ξηρ∥∇L (x) ∥2 + νηρ2 + ξη2∥∇L (x) ∥2 ≤ ζξηρ∥x − Φ(x)∥2 + ζ 2ξη2∥x − Φ(x)∥2\n\n2 + ξζ 2η2ρ2 2 + νηρ2 + ξζ 2η2ρ2 .\n\nThis completes the proof.\n\nLemma F.8. Suppose x ∈ K h and y = x − η∇Lk\n\n(cid:16)\n\nx + ρ ∇Lk(x) ∥∇Lk(x)∥\n\n(cid:17)\n\n,\n\n∥y − x∥2 ≤ η∥∇Lk (x) ∥2 + ηζρ ,\n\n∥Φ(x) − Φ(y)∥2 ≤ O(η∥∇L(x)∥2\n\n2 + ηρ∥∇L(x)∥2 + ηρ2) .\n\nProof of Lemma F.8. For sufficient small ρ, x + ρ ∇Lk(x)\n\n∥∇Lk(x)∥ ∈ K r. By Taylor Expansion,\n\n∥y − x∥2 = η∥∇Lk\n\nx + ρ\n\n(cid:18)\n\n(cid:19)\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n∥2 ≤ η∥∇Lk (x) ∥2 + ηζρ .\n\nThis further implies that for sufficiently small η and ρ, xy ∈ K r.\n\nAgain by Taylor Expansion,\n\n∥∂Φ(x)(y − x)∥2 ≤ η∥∂Φ(x)∇Lk (x) + ρ∂Φ(x)∇2Lk(x)\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n∥2 + ηρ2ν/2 .\n\nWe further have by Lemma F.1,\n\n∥∂Φ(x)∇Lk (x) ∥\n\n≤∥∂Φ(Φ(x))∇Lk (x) ∥ + ξ∥∇Lk (x) ∥2∥x − Φ(x)∥ ≤∥∂Φ(Φ(x))∇2Lk(Φ(x))(x − Φ(x))∥ + ν∥x − Φ(x)∥2\n\n2 + ζξ∥x − Φ(x)∥2\n\n2\n\nSimilarly,\n\n≤\n\nν μ\n\n∥∇L(x)∥2\n\n2 +\n\nζξ\n\nμ2 ∥∇L(x)∥2 2 .\n\n∥ρ∂Φ(x)∇2Lk(x)\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n∥2\n\n≤∥ρ∂Φ(Φ(x))∇2Lk(x)\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n∥2 + ρζξ∥x − Φ(x)∥2\n\n≤∥ρ∂Φ(Φ(x))∇2Lk(Φ(x))\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n≤ρ\n\nζξ\n\nμ2 ∥∇L(x)∥2 + ρ\n\nν μ\n\n∥∇L(x)∥2 .\n\nThis completes the proof.\n\nG ANALYSIS FOR EXPLICIT BIAS\n\n∥2 + ρζξ∥x − Φ(x)∥2 + ρν∥x − Φ(x)∥2\n\nThroughout this section, we assume that Assumption 3.2 holds.\n\nG.1 A GENERAL THEOREM FOR EXPLICIT BIAS IN THE LIMIT CASE\n\nIn this subsection we provide the proof details for section 4.1, which shows that the explicit biases of three notions of sharpness are all different, using our new mathematical tool, Theorem G.6. Notation for Regularizers. Let Rρ : RD → R ∪ {∞} be a family of regularizers parameterized by ρ. If Rρ is not well-defined at some x, then we let Rρ(x) = ∞. This convention will be useful when analyzing\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nρ = LAsc\n\nascent-direction sharpness RAsc ρ − L which is not defined when ∇L(x) = 0. This convention will not change the minimizers of the regularized loss. Intuitively, a regularizer should always be non-negative, but however, when far away from manifold, there are regularizers Rρ(x) of our interest that can actually ρ (x) ≈ ρ2 be negative, e.g., RAvg 2D Tr(∇2L(x)). Therefore we make the following assumption to allow the regularizer to be mildly negative.\n\nCondition G.1. Suppose for any bounded closed set B ⊂ U , there exists C > 0, such that for sufficiently small ρ, ∀x ∈ B, Rρ(x) ≥ −Cρ2.\n\nDefinition 4.3 (Limiting Regularizer). We define the limiting regularizer of {Rρ} as the function6\n\nS : Γ → R, S(x) = lim\n\nρ→0\n\nlim r→0\n\ninf ∥x′−x∥2≤r\n\nRρ(x′)/ρ2.\n\nThe high-level intuition is that we want to use the notion of limiting regularizer to capture the explicit bias of Rρ among the manifold of minimizers Γ as ρ → 0, which is decided by the second order term in the Taylor expansion, e.g., Equation 5 and Equation 6. In other words, the hope is that whenever the regularized loss is optimized, the final solution should be in a neighborhood of minimizer x with smallest value of limiting regularizer S(x). However, such hope cannot be true without further assumptions, which motivates the following definition of good limiting regularizer.\n\nDefinition G.2 (Good Limiting Regularizer). We say the limiting regularizer S of {Rρ} is good around some x∗ ∈ Γ, if S is non-negative and continuous at x∗ and that there is an open set Vx∗ containing x∗, such that for any C > 0, inf x′:∥x′−x∥2≤Cρ Rρ(x′)/ρ2 converges uniformly to S(x) in for all x ∈ Γ ∩ Vx∗ as ρ → 0. In other words, a good limiting regularizer satisfy that for any C, ε > 0, there is some ρx∗ > 0,\n\n∀x ∈ Γ ∩ Vx∗ and ρ ≤ ρx∗ ,\n\n(cid:12) (cid:12)S(x) −\n\ninf ∥x′−x∥2≤C·ρ\n\nRρ(x′)/ρ2(cid:12)\n\n(cid:12) < ε.\n\nWe say the limiting regularizer S is good on Γ, if S is good around every point x ∈ Γ. In such case we also say Rρ admits S as a good limiting regularizer on Γ.\n\nThe intuition of the concept of a good limiting regularizer is that, the value of the regularizer should not drop too fast when moving away from a minimizer x in its O(ρ) neighborhood. If so, the minimizer of the regularized loss may be Ω(ρ) away from any minimizer to reduce the regularizer at the cost of increasing the original loss, which makes the limiting regularizer unable to capture the explicit bias of the regularizer. (See Appendix G.2 for a counter example) We emphasize that the conditions of good limiting regularizer is natural and covers a large family of regularizers, including worst-, ascent- and average-direction sharpness. See Theorems G.3 to G.5 below. Theorem G.3. Worst-direction sharpness RMax and satisfies Condition G.1.\n\nadmits λ1(∇2L(·))/2 as a good limiting regularizer on Γ\n\nρ\n\nTheorem G.4. Ascent-direction sharpness RAsc and satisfies Condition G.1.\n\nρ\n\nadmits λM (∇2L(·))/2 as a good limiting regularizer on Γ\n\nTheorem G.5. Average-direction sharpness RAvg on Γ and satisfies Condition G.1.\n\nρ\n\nadmits Tr(∇2L(·))/(2D) as a good limiting regularizer\n\nNext we present the main mathematical tool to analyze the explicit bias of regularizers admitting good limiting regularizers, Theorem G.6. Theorem G.6. Let U ′ be any bounded open set such that its closure U ′ ⊆ U and U ′ ∩ Γ = U ′ ∩ Γ. Then for any family of parametrized regularizers {Rρ} admitting a good limiting regularizer S(x) on Γ and satisfying Condition G.1, for sufficiently small ρ, it holds that\n\n(cid:12) (cid:12) (cid:12) inf\n\nx∈U ′\n\n(cid:0)L(x) + Rρ(x)(cid:1) − inf\n\nx∈U ′\n\nL(x) − ρ2\n\ninf x∈U ′∩Γ\n\n(cid:12) (cid:12) ≤ o(ρ2). (cid:12) S(x)\n\nMoreover, for sufficiently small ρ, it holds uniformly for all u ∈ U ′ that\n\nL(u) + Rρ(u) ≤ inf x∈U ′\n\n(L(x) + Rρ(x)) + O(ρ2) =⇒ Rρ(u)/ρ2 − inf\n\nx∈U ′∩Γ\n\nS(x) ≥ −o(1).\n\n6Here we implicitly assume the zeroth and first order term varnishes, which holds for all three sharpness notions.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nTheorem G.6 says that minimizing the regularized loss L(u) + Rρ(u) is not very different from minimizing the original loss L(u) and the regularizer Rρ(u) respectively. To see this, we define the following optimality gaps\n\nA(u) ≜ L(u) + Rρ(u) − inf\n\nx∈U ′\n\n(L(x) + Rρ(x)) ≥ 0\n\nB(u) ≜ L(u) − inf x∈U ′\n\nL(x) ≥ 0\n\nC(u) ≜ Rρ(u)/ρ2 − inf\n\nS(x),\n\nx∈U ′∩Γ\n\nand Theorem G.6 implies that (cid:12) (cid:12) = o(ρ2). Moreover, A(u), B(u) are non-negative by definition, and C(u) ≥ −o(1) are almost non-negative, whenever A(u) is O(ρ2)-approximately optimized.\n\n(cid:12)A(u) − B(u) − ρ2C(u)(cid:12)\n\nFor the applications we are interested in in this paper, the good limiting regularizer S can be continuously extended to the entire space RD. In such a case, the third optimality gap has an approximate alternative form which doesn’t involve Rρ, namely S(u) − inf x∈U ′∩Γ S(x). Corollary G.7 shows minimizing regularized loss L(u) + Rρ(u) is equivalent to minimizing the limiting regularizer, S(u) around the manifold of local minimizer, Γ. Corollary G.7. Under the setting of Theorem G.6, let S be an continuous extension of S to Rd. For any optimality gap ∆ > 0, there is a function ε : R+ → R+ with limρ→0 ε(ρ) = 0, such that for all sufficiently small ρ > 0 and all u ∈ U ′ satisfying that\n\nL(u) + Rρ(u) − inf x∈U ′\n\n(cid:0)L(x) + Rρ(x)(cid:1) ≤ ∆ρ2,\n\nit holds that L(u) − inf x∈U ′ L(x) ≤ (∆ + ε(ρ))ρ2 and that\n\nS(u) − inf\n\nx∈U ′∩Γ\n\nS(x) ∈ [−ε(ρ), ∆ + ε(ρ)].\n\nG.2 BAD LIMITING REGULARIZERS MAY NOT CAPTURE EXPLICIT BIAS\n\nIn this subsection, we provide an example where a bad limiting regularizer cannot capture the explicit bias of regularizer when ρ → 0, to justify the necessity of Definition G.2. Here a bad limiting regularizer is a limiting regularizer which is not good.\n\nConsider choosing Rρ(x) = L(x + ρe) − L(x) with ∥e∥ = 1 as a fixed unit vector. We will show minimizing the regularized loss L(x) + Rρ(x) does not imply minimizing the limiting regularizer of Rρ(x) on the manifold.\n\nBy Definition 4.3 and the continuity of Rρ, the limiting regularizer S of Rρ is\n\n∀x ∈ Γ, S(x) = lim ρ→0\n\nlim r→0\n\ninf ∥x′−x∥2≤r\n\nRρ(x′)/ρ2 = lim\n\nρ→0\n\nRρ(x)/ρ2 = ∇2L(x)[e, e] ≥ 0.\n\nHowever, for any x ∈ Γ, we can choose x′ = x − ρe, then\n\nL(x′) + Rρ(x′) = L(x′ + ρe) = L(x) = 0. Therefore, no matter how small ρ is, minimizing L(x) + Rρ(x) can return a solution which is ρ-close to any point point of Γ. In other words, the explicit bias of minimizing L(x) + Rρ(x) is trivial and thus is not equivalent to minimizing the limiting regularizer S on the manifold Γ.\n\nThe reason behind the inefficacy of the limiting regularizer S in explaining the explicit bias of Rρ is that S(x) is not a good limiting regularizer for any x ∈ Γ satisfying S(x) > 0. To be more concrete, choose C = 1 and ε = S(x)/2 in Definition G.2. For any x ∈ Γ and sufficiently small ρ > 0, considering x′ = x − ρe1, by Taylor Expansion,\n\nRρ(x′) = L(x′ + ρe) − L(x′)\n\n= ρ⟨∇L(x′), e⟩ + ρ2∇2L(x′)[e, e] + o(ρ2) = ρ⟨∇2L(x)(x′ − x), e⟩ + ρ2∇2L(x′)[e, e] + o(ρ2) = −ρ2∇2L(x)[e, e] + ρ2∇2L(x′)[e, e] + o(ρ2) = ρ2eT (∇2L(x′) − ∇2L(x))e + o(ρ2) = o(ρ2)\n\nThis implies inf ∥x′−x∥2≤Cρ Rρ(x′) ≤ Rρ(x1) = o(ρ2). Hence,\n\nS(x) −\n\ninf ∥x′−x∥2≤Cρ\n\nRρ(x′)/ρ2 ≥ S(x) − o(1) > S(x)/2 = ε.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nG.3 PROOF OF THEOREM G.6\n\nThis subsection aims to prove Theorem G.6. We start with a few lemmas that will be used later.\n\nLemma G.8. Γ = U ∩ Γ.\n\nProof of Lemma G.8. For any point x ∈ U ∩ Γ, there exists {xk}∞ k=1 ∈ Γ such that limk→∞ xk = x. Since x ∈ U and Φ is continuous in U , it holds that Φ is continuous at x, thus limk→∞ Φ(xk) = Φ(x) ∈ Γ. However Φ(xk) = xk because xk ∈ Γ, ∀k. Thus we know x = Φ(x) ∈ Γ. Hence U ∩ Γ ⊂ Γ. The other side is clear because Γ ⊂ U and Γ ⊂ Γ.\n\nLemma G.9. Let U ′ be any bounded open set such that its closure U ′ ⊆ U . If U ′ ∩ Γ ⊆ U ′ ∩ Γ, then U ′ ∩ Γ = U ′ ∩ Γ.\n\nProof of Lemma G.9. By Lemma G.8, it holds that U ′ ∩ Γ = U ′ ∩ U ∩ Γ = U ′ ∩ Γ. Note that U ′ ∩ Γ ⊆ U ′, U ′ ∩ Γ ⊆ Γ, we have that U ′ ∩ Γ ⊆ U ′ ∩ Γ = U ′ ∩ Γ, which completes the proof.\n\nLemma G.10. Let U ′ be any bounded open set such that its closure U ′ ⊆ U and U ′ ∩ Γ ⊆ U ′ ∩ Γ. Then for all h2 > 0,∃ρ0 > 0 if x ∈ U ′, dist(x, Γ) ≤ ρ0 ⇒ dist(x, U ′ ∩ Γ) ≤ h2.\n\nk=1 ∈ U ′, Proof of Lemma G.10. We will prove by contradiction. Suppose there exists h2 > 0 and {xk}∞ such that limk→∞ dist(xk, Γ) = 0 but ∀k > 0, dist(xk, U ′ ∩ Γ) ≥ h2. Since U ′ is bounded, U ′ is compact and thus {xk}∞ k=1 has at least one accumulate point x∗ in U ′ ⊆ U . Since U is the attraction set of Γ under gradient flow, we know that Φ(x∗) ∈ Γ. Now we claim x∗ ∈ Γ. This is because limk→∞ dist(xk, Γ) = 0 and thus there exists a sequence of points on Γ, {yk}∞ k=1, where limk→∞ ∥xk − yk∥ = 0. Thus we have that x∗ = limk→∞ yk = limk→∞ Φ(yk) = Φ(x∗), where the last step we used that x∗ ∈ U and Φ is continuous on U . By the definition of U , x∗ ∈ U ⇐⇒ Φ(x∗) ∈ Γ, thus x∗ ∈ Γ. Then we would have x∗ ∈ U ′ ∩ Γ, which is contradictory to dist(xk, U ′ ∩ Γ) ≥ dist(xk, U ′ ∩ Γ) ≥ h2, ∀k > 0. This completes the proof.\n\nLemma G.11. Let U ′ be any bounded open set such that its closure U ′ ⊆ U and U ′ ∩ Γ ⊆ U ′ ∩ Γ. Then for all h2 > 0,∃ρ1 > 0 if x ∈ U ′, L(x) ≤ inf x∈U ′ L(x) + ρ1 ⇒ dist(x, U ′ ∩ Γ) ≤ h2.\n\nProof of Lemma G.11. We will prove by contradiction. If there exists a list of ρ1, ..., ρk, ..., such that ρk → 0 and there exists xk ∈ U ′, such that L(xk) ≤ inf x∈U ′ L(x) + ρk and dist(xk, U ′ ∩ Γ) ≥ h2. Since U ′ is bounded, U ′ is compact and thus {xk}∞ k=1 has at least one accumulate point x∗ in U ′ ⊆ U . Since L is continuous in U , L(x∗) = limk→∞ L(xk) = inf x∈U ′ L(x). Thus x∗ is a local minimizer of L and thus has zero gradient, which further implies that x∗ = Φ(x∗). Thus x∗ ∈ U ′ ∩ Γ, which is contradictory to dist(xk, U ′ ∩ Γ) ≥ dist(xk, U ′ ∩ Γ) ≥ h2, ∀k > 0. This completes the proof.\n\nLemma G.12. Let U ′ be any bounded open set such that its closure U ′ ⊆ U and U ′ ∩ Γ = U ′ ∩ Γ. Suppose regularizers {Rρ} admits a limiting regularizer S on Γ, then\n\n(L(x) + Rρ(x)) ≤ ρ2\n\ninf x∈U ′\n\ninf x∈U ′∩Γ\n\nS(x) + inf x∈U ′\n\nL(x) + o(ρ2).\n\nProof of Lemma G.12. First choose sufficiently small ρ, such that ρ < h(U ′ ∩ Γ). Choose an approximate minimizer of S(x), x0 ∈ U ′ ∩ Γ, such that S(x0) ≤ inf x∈U ′∩Γ S(x) + ρ2. Then by the definition of limiting regularizers (Definition 4.3) and the assumption that U ′ is open, there exists x1 ∈ U ′ satisfying that ∥x1 − x0∥2 ≤ rρ < ρ2 and Rρ(x1)/ρ2 − S(x0) ≤ ρ2. Thus, Rρ(x1) ≤ ρ2S(x0) + ρ4.\n\nAs ∥x1 − x0∥2 ≤ ρ2 < h and x0 ∈ U ′ ∩ Γ. This further leads to x0x1 ∈ U ′ ∩ Γ L at x0, we would have L(x1) ≤ L(x0) + O(∥x0 − x1∥2\n\n2) = inf x∈U ′∩Γ L(x) + O(ρ4). Thus it holds that\n\n. By Taylor expansion on\n\nh\n\ninf x∈U ′\n\n(L(x) + Rρ(x)) ≤ L(x1) + Rρ(x1) ≤ ρ2\n\ninf x∈U ′∩Γ\n\nS(x) + inf x∈U ′\n\nL(x) + O(ρ4).\n\nThis completes the proof.\n\nLemma G.13. Let U ′ be any bounded open set such that its closure U ′ ⊆ U and U ′ ∩ Γ = U ′ ∩ Γ. Suppose regularizers {Rρ} admits a good limiting regularizer S on Γ, then for all u ∈ U ′,\n\n∥u − Φ(u)∥2 = O(ρ) =⇒ Rρ(u) ≥ ρ2\n\ninf x∈U ′∩Γ\n\nS(x) − o(ρ2) .\n\n26\n\nPublished as a conference paper at ICLR 2023\n\n(18)\n\n(19)\n\nProof of Lemma G.13. Define r = r(K), h = h(K) as the constant in Lemma E.5 with K = U ′ ∩ Γ. Note K is compact and by Lemma G.9, K = U ′ ∩ Γ ⊂ Γ. By Lemma E.5, we have K r ∩ Γ is a compact set, so is K h ∩ Γ. Since S is a good limiting regularizer for {Rρ}, by Definition G.2, for any x∗ ∈ K h ∩ Γ, there exists open neighborhood of x∗, Vx∗ such that for any C, ε1 > 0, there is a ρx∗ such that\n\n∀x ∈ Vx∗ and ρ ≤ ρx∗ ,\n\n(cid:12) (cid:12) S(x) − (cid:12) (cid:12)\n\ninf ∥x′−x∥2≤C·ρ\n\nRρ(x′)/ρ2\n\n< ε1.\n\nNote that K h ∩ Γ is compact, there exists a finite subset of K h ∩ Γ, {xk}k, such that K h ∩ Γ ⊂ ∪kVxk . Hence for any C, ε1 > 0, there is some ρK = mink ρxk > 0, it holds that,\n\n∀x ∈ K h ∩ Γ and ρ ≤ ρK,\n\nS(x) −\n\ninf ∥x′−x∥2≤C·ρ\n\nRρ(x′)/ρ2\n\n< ε1.\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nWe can rewrite Equation 18 as for any C > 0,\n\nsup x∈Kh∩Γ\n\nS(x) −\n\ninf ∥x′−x∥2≤C·ρ\n\nRρ(x′)/ρ2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n= o(1),\n\nas ρ → 0.\n\nAs u ∈ U ′ ⊆ U , we have that Φ(u) ∈ Γ. If ∥u − Φ(u)∥2 = O(ρ), then dist(u, Γ) ≤ O(ρ). By Lemma G.10, we have that dist(u, K) = o(1). This further implies dist(Φ(u), K) ≤ dist(u, K) + dist(Φ(u), u) = o(1). Hence we have that Φ(u) ∈ K h ∩ Γ for sufficiently small ρ. Thus we can pick x = Φ(u) in Equation 19 and C sufficiently large, which yields that\n\nρ2S(Φ(u)) ≤\n\ninf ∥u′−Φ(u)∥2≤O(ρ)\n\nRρ(u′) + o(ρ2) ≤ Rρ(u) + o(ρ2),\n\n(20)\n\nwhere the last step is because ∥u − Φ(u)∥2 = O(ρ). On the other hand, we have that\n\nx∈U ′∩Γ as S is continuous on Γ and dist(U ′ ∩ Γ, Φ(u)) = o(1). Combining Equations 20 and 21, we have Rρ(u) ≥ ρ2 inf x∈U ′∩Γ S(x) − o(ρ2).\n\nS(Φ(u)) ≥ inf\n\nS(x) − o(1) .\n\n(21)\n\nProof of Theorem G.6. We will first lower bound L(x) + Rρ(x) for x ∈ U ′. Suppose CU ′ is the constant in\n\nCondition G.1. Define C1 =\n\nCU ′ +inf\n\n2\n\nx∈U ′ ∩Γ μ\n\nS(x)+1\n\n. We discuss by cases. For sufficiently small ρ,\n\n(cid:113)\n\n1. If x ̸∈ K h, then by Lemma G.11, L(x) is lower bounded by a positive constant. 2. If x ∈ K h and ∥x − Φ(x)∥2 ≥ C1ρ, then by Lemma F.1,\n\nL(x) ≥\n\nμ∥x − Φ(x)∥2 2\n2\n\n≥ (CU ′ + inf\n\nx∈U ′∩Γ\n\nS(x) + 1)ρ2 .\n\nThis implies L(x) + Rρ(x) ≥ (inf x∈U ′∩Γ S(x) + 1)ρ2 + inf x∈U ′ L(x).\n\n3. If ∥x − Φ(x)∥2 ≤ C1ρ, by Lemma G.13, Rρ(x) ≥ ρ2 inf x∈U ′∩Γ S(x) − o(ρ2), hence L(x) + Rρ(x) + o(ρ2) ≥ inf\n\nL(x) .\n\nx∈U ′∩Γ\n\nS(x)ρ2 + inf x∈U ′\n\nConcluding the three cases, we have\n\ninf x∈U ′\n\n(L(x) + Rρ(x)) ≥ inf\n\nx∈U ′∩Γ\n\nL(x) + inf\n\nx∈U ′∩Γ\n\nS(x)ρ2 − o(ρ2) .\n\nBy Lemma G.12, we have that\n\n(L(x) + Rρ(x)) ≤ ρ2\n\ninf x∈U ′\n\ninf x∈U ′∩Γ\n\nS(x) + inf\n\nx∈U ′∩Γ\n\nL(x) + o(ρ2) .\n\nCombining the above two inequalities, we prove the main statement of Theorem G.6.\n\nFurthermore, if L(u) + Rρ(u) ≤ inf x∈U ′(L(x) + Rρ(x)) + O(ρ2), then by the main statement and Condition G.1, we have that\n\nL(u) − inf x∈U ′\n\nL(x) ≤ inf x∈U ′\n\n(L(x) + Rρ(x)) − Rρ(u) − inf x∈U ′\n\nL(x) + O(ρ2)\n\n≤ρ2\n\ninf x∈U ′∩Γ\n\nS(x) + Cρ2 + O(ρ2) = O(ρ2) .\n\nThen by Lemma G.11, we have u ∈ (U ′∩Γ)h for sufficiently small ρ. By Lemma F.1, we have ∥u−Φ(u)∥2 = O(ρ). By Lemma G.13, we have Rρ(u) ≥ ρ2 inf x∈U ′∩Γ S(x) − o(ρ2).\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nG.4 PROOFS OF COROLLARY G.7\n\nProof of Corollary G.7. Since L(u) + Rρ(u) − inf x∈U ′\n\n(cid:0)L(x) + Rρ(x)(cid:1) ≤ ∆ρ2 = O(ρ2), by Theorem G.6, we\n\nhave that\n\nand\n\nL(u) − inf x∈U ′\n\nL(x) ≤ (∆ + o(1))ρ2,\n\nRρ(x) − inf\n\nx∈U ′∩Γ\n\nS(x) ∈ [−o(1), ∆ + o(1)].\n\nThus it suffices to show Rρ(x) − S(x) = o(ρ2). Since L(u) − inf x∈U ′ L(x) ≤ (∆ + ε(ρ))ρ2 = o(1), by Lemma G.11, we know dist(x, U ′ ∩ Γ) = o(1). Thus by Lemma F.1, ∥x − Φ(x)∥ = o(1), which implies that ρ2S(Φ(x))−o(ρ2) ≤ Rρ(u). Since S is an continuous extension, S(x)−S(Φ(x)) = S(x)−S(Φ(x)) = O(∥x − Φ(x)∥2) = o(1). Thus we conclude that S(x) ≤ S(Φ(x)) ≤ inf x∈U ′∩Γ S(x) + ∆ + o(1). On the other hand, S(x) ≥ S(Φ(x)) − o(1) ≥ inf x∈U ′∩Γ S(x) − o(1), where the last step we use the fact that dist(x, U ′ ∩ Γ) = o(1). This completes the proof.\n\nG.5 LIMITING REGULARIZERS FOR DIFFERENT NOTIONS OF SHARPNESS\n\nProof of Theorem G.3.\n\n1. We will first verify Condition G.1. For fixed compact set B ⊂ U , as ∥∇3L(x)∥2 is continuous, there exists\n\nconstant ν, such that ∀x ∈ B1, ∥∇3L(x)∥2 ≤ ν. Then by Taylor Expansion,\n\nRMax ρ\n\n(x) = max ∥v∥2≤1\n\nL(x + ρv) − L(x)\n\n(cid:0)ρ⟨∇L(x), v⟩ + ρ2vT ∇2L(x)v/2(cid:1) − νρ3/6\n\n≥ max ∥v∥2≤1 ≥ −νρ3/6 .\n\n2. Now we verify SMax(x) = λ1(∇2L(·))/2 is the limiting regularizer of RMax\n\nρ\n\n. Let x be any point in Γ, by\n\ncontinuity of RMax\n\nρ\n\n,\n\nlim ρ→0\n\nlim r→0\n\ninf ∥x′−x∥2≤r\n\n(x′)\n\nRMax ρ\nρ2\n\n= lim ρ→0\n\n(x)\n\nRMax ρ\nρ2\n\n= λ1(∇2L(x))/2 .\n\n3. Finally we verify definition of good limiting regularizer, by Assumption 3.2, SMax(x) = λ1(x)/2 is nonnegative and continuous on Γ. For any x∗ ∈ Γ, choose a sufficiently small open convex set V containing x∗ such that ∀x ∈ V 1, ∥∇3L(x)∥2 ≤ ν. For any x ∈ V ∩ Γ, for any x′ satisfying that ∥x′ − x∥2 ≤ Cρ, by Theorem K.3, RMax ρ\n\n(cid:0)ρ⟨∇L(x′), v⟩ + ρ2vT ∇2L(x′)v/2(cid:1) − νρ3/6\n\n(x′) = max ∥v∥2≤1\n\nL(x′ + ρv) − L(x′) ≥ max ∥v∥2≤1\n\n≥ ρ2λ1(∇2L(x′))/2 − νρ3/6 ≥ ρ2λ1(∇2L(x))/2 − O(ρ3) .\n\nThis implies\n\ninf ∥x′−x∥2≤Cρ On the other hand, for any x ∈ V ∩ Γ,\n\nRMax ρ\n\n(x′) ≥ ρ2λ1(∇2L(x))/2 − O(ρ3).\n\nRMax ρ\n\n(x) = max ∥v∥2≤1\n\nL(x + ρv) − L(x) ≤ max ∥v∥2≤1\n\n(cid:0)ρ⟨∇L(x), v⟩ + ρ2vT ∇2L(x)v/2(cid:1) + νρ3\n\n= max ∥v∥2≤1\n\nρ2vT ∇2L(x)v/2 + νρ3 = ρ2λ1(∇2L(x′))/2 + O(ρ3) .\n\nThis implies\n\ninf ∥x′−x∥2≤Cρ\n\nThus, we conclude that\n\nRMax ρ\n\n(x′) ≤ ρ2λ1(∇2L(x))/2 + O(ρ3).\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ninf ∥x′−x∥2≤Cρ\n\nRMax ρ\n\n(cid:12) (cid:12) (x′)/ρ2 − λ1(∇2L(x))/2 (cid:12) (cid:12)\n\nSMax is a good limiting regularizer of RMax\n\nρ\n\non Γ.\n\n= O(ρ), ∀x ∈ V ∩ Γ, indicating\n\nThis completes the proof.\n\nProof of Theorem G.4.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\n1. We will first prove Condition G.1 holds. For any fixed compact set B ⊂ U , as λ1(∇2L) and ∥∇3L∥is continuous, there exists constant C, such that ∀x ∈ B2, λ1(∇2L) > −ζ and ∥∇3L(x)∥ < ν. Then by Taylor Expansion,\n\nRAsc\n\nρ (x) = L(x + ρ\n\n∇L (x) ∥∇L (x) ∥\n\n) − L(x)\n\n(cid:18)\n\n≥\n\nρ∥∇L (x) ∥2 + ρ2(\n\n∇L (x) ∥∇L (x) ∥\n\n)T ∇2L(x)\n\n∇L (x) ∥∇L (x) ∥\n\n(cid:19)\n\n/2\n\n− νρ3/6\n\n≥ −(ζ + ν/6)ρ2.\n\n2. Now we verify SAsc(x) = Tr(∇2L(·))/2 is the limiting regularizer of RAsc K = {x} and choose h = h(K) as in Lemma E.5. For any x′ ∈ K h ∩ U ′,\n\nρ . Let x be any point in Γ. Let\n\nRAsc\n\nρ (x′) = L(x′ + ρ\n\n∇L (x′) ∥∇L (x′) ∥\n\n) − L(x′)\n\n≥ ρ∥∇L (x′) ∥2 + ρ2(\n\n∇L (x′) ∥∇L (x′) ∥\n\n)T ∇2L(x′)\n\n∇L (x′) ∥∇L (x′) ∥\n\n/2 − νρ3/6\n\n≥ ρ2(\n\n∇L (x′) ∥∇L (x′) ∥\n\n)T ∇2L(Φ(x′))\n\n∇L (x′) ∥∇L (x′) ∥\n\n/2 − νρ3/6 .\n\nBy Lemma F.4, we have\n\n∇L(x′) ∥∇L(x′)∥ =\n\n∇2L(Φ(x′))(x′−Φ(x′)) ∥∇2L(Φ(x′))(x′−Φ(x′))∥2\n\n+ O( ν\n\nμ ∥x′ − Φ(x′)∥2). Hence\n\nRAsc\n\nρ (x′) ≥ ρ2λM (∇2L(Φ(x′)))/2 − ζρ2O(∥x′ − Φ(x′)∥2) − νρ3/6 .\n\nThis implies limρ→0 limr→0 inf ∥x′−x∥2≤r We now show the above inequality is in fact equality. Expansion,\n\nRAsc\n\nρ (x′) ρ2 ≥ λM (∇2L(Φ(x′)))/2. If we choose x′′\n\nr = x + rvM , then by Taylor\n\n∇L(x′′\n\nr ) = ∇L(x) + ∇2L(x)(x′′\n\nr − x) + O(∥x′′\n\nr − x∥2)\n\n= rvM + O(r2)\n\nThis implies limr→0 0. Putting together,\n\n∇L(x′′ ∥∇L(x′′\n\nr ) r )∥ = vM . We also have limr→0 ∇2L(x′′\n\nr ) = ∇2L(x) and limr→0 ∇L(x′′\n\nr ) =\n\nlim r→0\n\nRAsc\n\nρ (x′′\n\nr ) = lim\n\nr→0\n\nL(x′′\n\nr + ρ\n\n∇L (x′′ r ) ∥∇L (x′′ r ) ∥\n\n(cid:16)\n\n= lim r→0\n\nρ∥∇L (x′′\n\nr ) ∥2 + ρ2(\n\n) − L(x′′ r )\n\n∇L (x′′ r ) ∥∇L (x′′ r ) ∥\n\n)T ∇2L(x′′ r )\n\n∇L (x′′ r ) ∥∇L (x′′ r ) ∥\n\n/2 + O(νρ3)\n\n(cid:17)\n\n= ρ2λM (∇2L(x))/2 + O(ρ3).\n\nThis implies limρ→0 limr→0 inf ∥x′−x∥2≤r Hence the limiting regularizer S is exactly λM (∇2L(·))/2.\n\nRAsc\n\nρ (x′) ρ2 ≤ limρ→0 limr→0\n\nRAsc\n\nρ (x′′ r ) ρ2 = λM (∇2L(x))/2.\n\n3. Finally we verify definition of good limiting regularizer, by Assumption 3.2, SMax(x) = λM (x)/2 is nonnegative and continuous on Γ. For any x∗ ∈ Γ, choose a sufficiently small open convex set V containing x∗ such that ∀x ∈ V 1, ∥∇3L(x)∥2 ≤ ν. For any x ∈ V ∩ Γ, for any x′ satisfying that ∥x′ − x∥2 ≤ Cρ,\n\nRAsc\n\nρ (x′) = L(x′ + ρ\n\n∇L (x′) ∥∇L (x′) ∥\n\n) − L(x′)\n\n≥ ρ∥∇L (x′) ∥2 + ρ2(\n\n∇L (x′) ∥∇L (x′) ∥\n\n)T ∇2L(x′)\n\n∇L (x′) ∥∇L (x′) ∥\n\n/2 − νρ3/6\n\n≥ ρ2(\n\n∇L (x′) ∥∇L (x′) ∥\n\n)T ∇2L(Φ(x′))\n\n∇L (x′) ∥∇L (x′) ∥\n\n/2 − νρ3/6 .\n\nBy Lemma F.4, we have\n\n∇L(x′) ∥∇L(x′)∥ = ρ (x′) ≥ ρ2λM (∇2L(x))/2 − O(ρ3).\n\n∇2L(Φ(x′))(x′−Φ(x′)) ∥∇2L(Φ(x′))(x′−Φ(x′))∥2\n\nRAsc\n\ninf ∥x′−x∥2≤Cρ\n\n+ O( ν\n\nμ ∥x′ − Φ(x′)∥2). This implies\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nOn the other hand, simillar to the proof in the second part, we have\n\ninf ∥x′−x∥2≤Cρ\n\nRAsc\n\nρ (x′) ≤\n\nρ2λM (∇2L(x))/2 + O(ρ3).\n\nThus, we conclude that\n\n(cid:12) (cid:12) inf (cid:12) (cid:12) ∥x′−x∥2≤Cρ\n\nRMax ρ\n\n(cid:12) (cid:12) (x′)/ρ2 − λ1(∇2L(x))/2 (cid:12) (cid:12)\n\n= O(ρ), ∀x ∈ V ∩ Γ, indicating\n\nSMax is a good limiting regularizer of RMax\n\nρ\n\non Γ.\n\nThis completes the proof.\n\nProof of Theorem G.5.\n\n1. We will first verify Condition G.1. For fixed compact set B ⊂ U , as ∥∇3L(x)∥2 is continuous, there exists\n\nconstant ν, such that ∀x ∈ B1, ∥∇3L(x)∥2 ≤ ν. Then by Taylor Expansion,\n\nRAvg\n\nρ (x) = Eg∼N (0,I)L(x + ρ\n\n) − L(x)\n\ng ∥g∥\n\n(cid:18)\n\nρ⟨∇L(x),\n\ng ∥g∥\n\n⟩ + ρ2(\n\ng ∥g∥\n\n)T ∇2L(x)\n\n(cid:19)\n\ng 2∥g∥\n\n− νρ3/6\n\n≥ Eg∼N (0,I)\n\n≥ −νρ3/6 .\n\n2. Now we verify SMax(x) = Tr(∇2L(·))/2D is the limiting regularizer of RAvg\n\nρ . Let x be any point in Γ, by\n\ncontinuity of RAvg ρ ,\n\nlim ρ→0\n\nlim r→0\n\ninf ∥x′−x∥2≤r\n\nRAvg ρ (x′) ρ2\n\n= lim ρ→0\n\nRAvg ρ (x) ρ2\n\n= Tr(∇2L(x))/2D .\n\n3. Finally we verify definition of good limiting regularizer, by Assumption 3.2, SAvg(x) = Tr(x)/2D is nonnegative and continuous on Γ. For any x∗ ∈ Γ, choose a sufficiently small open convex set V containing x∗ such that ∀x ∈ V 1, ∥∇3L(x)∥2 ≤ ν. For any x ∈ V ∩ Γ, for any x′ satisfying that ∥x′ − x∥2 ≤ Cρ, by Theorem K.3,\n\nRAvg\n\nρ (x′) = Eg∼N (0,I)L(x′ + ρ\n\n) − L(x′)\n\ng ∥g∥\n\n≥ Eg∼N (0,I)\n\n(cid:18)\n\nρ⟨∇L(x′),\n\ng ∥g∥\n\n⟩ + ρ2 g\n\n∥g∥\n\nT\n\n∇2L(x′)\n\n(cid:19)\n\ng ∥2g∥\n\n− νρ3/6\n\n≥ ρ2Tr(∇2L(x′))/2D − νρ3/6 ≥ ρ2Tr(∇2L(x))/2D − O(ρ3) .\n\nThis implies\n\ninf ∥x′−x∥2≤Cρ On the other hand, for any x ∈ V ∩ Γ,\n\nRAvg\n\nρ (x′) ≥ ρ2Tr(∇2L(x))/2D − O(ρ3).\n\nRAvg\n\nρ (x) = Eg∼N (0,I)L(x + ρ\n\ng ∥g∥\n\n) − L(x)\n\n≤ Eg∼N (0,I)\n\n(cid:18)\n\nρ⟨∇L(x),\n\ng ∥g∥\n\n⟩ + ρ2 g\n\n∥g∥\n\nT\n\n∇2L(x)\n\n(cid:19)\n\ng 2∥g∥\n\n+ νρ3\n\n= Eg∼N (0,I)ρ2 g\n\n∥g∥\n\nT\n\n∇2L(x)\n\ng 2∥g∥\n\n+ νρ3 = ρ2Tr(∇2L(x′))/2D + O(ρ3) .\n\nThis implies\n\nρ (x′) ≤ ρ2Tr(∇2L(x))/2D + O(ρ3).\n\nRAvg inf ∥x′−x∥2≤Cρ (cid:12) (cid:12) (cid:12) (cid:12)\n\ninf ∥x′−x∥2≤Cρ\n\nThus, we conclude that\n\nRAvg\n\nρ (x′)/ρ2 − Tr(∇2L(x))/2D\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n= O(ρ), ∀x ∈ V ∩ Γ, indicating\n\nSAvg is a good limiting regularizer of RAvg\n\nρ\n\non Γ.\n\nTheorem G.14. Stochastic worst-direction sharpness Ek[RMax regularizer on Γ and satisfies Condition G.1.\n\nk,ρ ] admits Tr(∇2L(·))/2 as a good limiting\n\nProof of Theorem G.14. By Theorem E.2, Condition E.1 holds.\n\nEasily deducted from Theorem G.3 Λk(x) is a good limiting regularizer for Rmax Λk(x) is a good limiting regularizer for Rmax good limiting regularizer of Ek[RMax k,ρ ](x) on Γ.\n\nk,ρ on Γ. Hence S(x) = (cid:80)\n\nk,ρ on Γk. Then as Γ ⊂ Γk, k Λk(x)/2M = Tr(∇2L(x))/2 is a\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nTheorem G.15. Stochastic ascent-direction sharpness Ek[RAsc regularizer on Γ and satisfies Condition G.1.\n\nk,ρ] admits Tr(∇2L(·))/2 as a good limiting\n\nProof of Theorem G.15. By Theorem E.2, Condition E.1 holds.\n\nEasily deducted from Theorem G.4 Λk(x) is a good limiting regularizer for Rasc mension of Γk is 1. Then as Γ ⊂ Γk, Λk(x) is a good limiting regularizer for Rmax S(x) = (cid:80) k Λk(x)/2M = Tr(∇2L(x))/2 is a good limiting regularizer of Ek[RAsc k,ρ](x) on Γ.\n\nk,ρ on Γk as the codik,ρ on Γ.Hence\n\nTheorem G.16. Stochastic average-direction sharpness Ek[RAvg iting regularizer on Γ and satisfies Condition G.1.\n\nk,ρ ] admits Tr(∇2L(·))/(2D) as a good lim-\n\nProof of Theorem G.16. By definition, we know that Ek[RAvg\n\nk,ρ ] = RAvg\n\nρ . The rest follows from Theorem G.5.\n\nG.6 PROOF OF THEOREMS 4.2 AND 5.3\n\nTo end this section, we prove the two theorems presented in the main text. The readers will find the proof straight forward after we established the framework of good limiting regularizers.\n\nProof of Theorem 4.2. Apply Corollary G.7 on Rtype. The mapping from R to good limiting regularizers Stype are characterized by Theorems G.3 to G.5.\n\nProof of Theorem 5.3. Apply Corollary G.7 on Rtype. The mapping from R to good limiting regularizers ̃Stype are characterized by Theorems G.14 to G.16.\n\nH ANALYSIS FULL-BATCH SAM ON QUADRATIC LOSS (PROOF OF THEOREM 4.8)\n\nThe goal of this section is to prove Theorem 4.8. In this section, we use A ≺ B to indicate B − A is positive semi-definite. Theorem 4.8. Suppose A is a positive definite symmetric matrix with unique top eigenvalue. Consider running full-batch SAM (Equation 3) on loss L(x) := 1\n\n2 xT Ax as in Equation 11 below.\n\n(11) for almost every x(0), we have x(t) converges in direction to v1(A) up to a sign flip and\n\nx(t + 1) = x(t) − ηA(cid:0)x(t) + ρAx(t)/∥Ax(t)∥2\n\n(cid:1) .\n\nThen, limt→∞ ∥x(t)∥2 = ηρλ1(A)\n\n2−ηλ1(A) with ηλ1(A) < 1.\n\nProof of Theorem 4.8. We first rewrite the iterate as\n\nx(t + 1) = x(t) − ηAx(t) − ηρ\n\nDefine ̃x(t) ≜ ∇L(x(t))\n\nρ\n\n= Ax(t)\n\nρ\n\n, and we have\n\n ̃x(t + 1) = ̃x(t) − ηA ̃x(t) − η\n\nA2x(t) ∥Ax(t)∥2\n\n.\n\nA2 ̃x(t) ∥ ̃x(t)∥2\n\n.\n\nWe suppose A ∈ RD×D and use λi, vi to denote λi(A), vi(A).\n\nFurther, we define that\n\n(22)\n\nP (j:D) ≜\n\nD (cid:88)\n\ni=j\n\nvi(A)vi(A)T ,\n\nIj ≜ { ̃x | ∥P (j:D) ̃x∥2 ≤ ηλ2\n\nj } ,\n\n ̃xi(t) ≜ ⟨ ̃x(t), vi⟩ ,\n\nS ≜ {t | ∥ ̃x(t)∥2 ≤\n\nηλ2 1\n2 − ηλ1\n\n, t > T1} .\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nBy Lemma H.1, Ij is an invariant set for update rule Equation 22.\n\nOur proof consists of two steps.\n\n(1) Entering Invariant Set. Lemma H.2 implies that there exists constant T1 > 0, such that ∀t >\n\nT1, ∥P (j:D) ̃x(t)∥2 ≤ ηλ2\n\nj\n\n(2) Alignment to Top Eigenvector. Lemmas H.10 and H.11 show that ∥ ̃x(t)∥2 and | ̃x1(t)| converge to ηλ2\n\n1\n\n,\n\n2−ηλ1\n\nwhich implies our final results.\n\nH.1 ENTERING INVARIANT SET\n\nIn this subsection, we will prove the following three lemmas. 1. Lemma H.1 shows Ij is an invariant set for update rule (Equation 22). 2. Lemma H.2 shows that under the update rule (Equation 22), all iterates not in Ij will shrink exponentially\n\nin l2 norm.\n\n3. Lemma H.3 combines Lemmas H.1 and H.2 to show that for sufficiently large t, x(t) ∈ ∩jIj. Lemma H.1. For t ≥ 0, if ηλ1(A) < 1 and ̃x(t) ∈ Ij, then ̃x(t + 1) ∈ Ij.\n\nProof of Lemma H.1. By (Equation 22), we have that\n\nP (j:D) ̃x(t + 1) = (I − P (j:D)ηA − η\n\nP (j:D)A2 ∥ ̃x(t)∥2\n\n)P (j:D) ̃x(t) .\n\nHence we have that\n\n∥P (j:D) ̃x(t + 1)∥2 = ∥(I − P (j:D)ηA − η\n\n)P (j:D) ̃x(t)∥2\n\n≤ ∥I − P (j:D)ηA − η\n\n∥2∥P (j:D) ̃x(t)∥2 .\n\nP (j:D)A2 ∥ ̃x(t)∥2 P (j:D)A2 ∥ ̃x(t)∥2\n\nBecause ̃x(t) ∈ Ij, ∥ ̃x(t)∥2 ≤\n\nηλ2 1−ηλj\n\nj\n\n. This implies,\n\nI(1 − ηλj − η\n\nλ2 j\n∥P (j:D) ̃x(t)∥2\n\n) ≺ I(1 − ηλj − η\n\nλ2 j\n∥ ̃x(t)∥2\n\n) ≺ I − P (j:D)ηA − η\n\nP (j:D)A2 ∥ ̃x(t)∥2\n\n≺ I .\n\nHence, ∥I − P (j:D)ηA − η P (j:D)A2\n\n∥2 ̃x(t)∥ ∥2 ≤ max(1, ηλj + η ∥P (j:D) ̃x(t + 1)∥2 ≤ max(∥P (j:D) ̃x(t)∥2, ηλ2\n\nλ2 j\n∥P (j:D) ̃x(t)∥2\n\n− 1) . It holds that\n\nj − (1 − ηλj)∥P (j:D) ̃x(t)∥2) ≤ ηλ2 j ,\n\nwhere the last equality is because 1 − ηλj ≥ 0. This above inequality is exactly the definition of ̃x(t + 1) ∈ Ij and thus is proof is completed.\n\nLemma H.2. For t ≥ 0, if ηλ1(A) < 1 and ̃x(t) ̸∈ Ij, then\n\n∥P (j:D) ̃x(t + 1)∥2 ≤ max\n\n(cid:18)\n\n1 − ηλD − η\n\nλ2 D\n∥ ̃x(t)∥2\n\n(cid:19)\n\n, ηλj\n\n∥P (j:D) ̃x(t)∥2\n\n(23)\n\n≤ max (1 − ηλD, ηλj) ∥P (j:D) ̃x(t)∥2 .\n\nProof of Lemma H.2. Note that\n\n∥P (j:D) ̃x(t + 1)∥2 = ∥(I − P (j:D)ηA − η\n\nP (j:D)A2 ∥ ̃x(t)∥2\n\n)P (j:D) ̃x(t)∥2\n\n≤ ∥P (j:D) − P (j:D)ηA − η\n\nP (j:D)A2 ∥ ̃x(t)∥2\n\n∥2∥P (j:D) ̃x(t)∥2 .\n\nAs ̃x(t) ̸∈ Ij, We have ∥ ̃x(t)∥2 ≥ ∥P (j:D) ̃x(t)∥2 > ηλ2\n\nj , hence η P (j:D)A2\n\n∥ ̃x(t)∥2\n\n≺ η P (j:D)A2\n\nηλ2 j\n\n≺ P (j:D).\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nThis implies that\n\nand\n\n−ηλjP (j:D) ≺ −P (j:D)ηA ≺ P (j:D) − P (j:D)ηA − η\n\nP (j:D)A2 ∥ ̃x(t)∥2\n\n,\n\nP (j:D) − P (j:D)ηA − η\n\nP (j:D)A2 ∥ ̃x(t)∥2\n\n≺ P (j:D)(1 − ηλD) − η\n\nλ2 D\n∥ ̃x(t)∥2\n\n.\n\nHence we have that\n\n∥P (j:D) ̃x(t + 1)∥2 ≤ max\n\n(cid:18)\n\n1 − ηλD − η\n\nλ2 D\n∥ ̃x(t)∥2\n\n(cid:19)\n\n, ηλj\n\n∥P (j:D) ̃x(t)∥2.\n\n≤ max (1 − ηλD, ηλj) ∥P (j:D) ̃x(t)∥2\n\nThis completes the proof.\n\nLemma H.3. Choosing T1 = maxj 1, ̃x(t) ∈ Ij\n\n(cid:16)\n\n− logmax(1−ηλD,ηλj ) max( ∥ ̃x(0)∥2\n\nηλ2 j\n\n(cid:17)\n\n, 1)\n\n, then ∀t ≥ T1, D > j ≥\n\nProof of Lemma H.3. We will prove by contradiction. Suppose ∃j ∈ [D] and T > T1, such that ̃x(T ) ̸∈ Ij. By Lemma H.1, it holds that ∀t < T, ̃x(t) ̸∈ Ij. Then by Lemma H.2,\n\n∥P (j:D) ̃x(T )∥2 ≤ max (1 − ηλD, ηλj)T ∥P (j:D) ̃x(0)∥2 ≤ ηλ2 j ,\n\nwhich leads to a contradiction.\n\nH.2 ALIGNMENT TO TOP EIGENVECTOR\n\nIn this subsection, we prove the following lemmas towards showing that ̃x(t) converges in direction to v1(A) up to a proper sign flip.\n\n1. Corollary H.4 show that for almost every learning rate η and initialization xinit, ̃x1(t) ̸= 0, for every t ≥ 0. This condition is important because if ̃x1(t) = 0 at some step t, then for any t′ ≥ t, ̃x1(t′) will also be 0 and thus alignment is impossible.\n\n2. Lemma H.5 shows that under update rule (Equation 22), t ̸∈ S ⇒ t + 1 ∈ S for sufficiently large t, where\n\nthe definition of S is {t|∥ ̃x(t)∥2 ≤ ηλ2\n\n1\n\n, t > T1}.\n\n2−ηλ1\n\n3. Lemma H.9, a combination of Lemmas H.6 and H.7, shows that following update rule (Equation 22),\n\n ̃x1(t) increases for t ∈ S.\n\n4. Lemma H.10 shows that ∥ ̃x(t)∥ converges to ηλ2 5. Lemma H.11 shows that ∥ ̃x1(t)∥2 converges to ηλ2\n\n2−ηλ1\n\n1\n\n1\n\nunder Equation 22.\n\n2−ηλ1\n\nunder Equation 22.\n\nWe will first prove that ∀t, ̃x1(t) ̸= 0 happens for almost every learning rate η and initialization xinit (Corollary H.4), using a much more general result (Theorem D.3). Corollary H.4. Except for countably many η ∈ R+, for almost all initialization xinit = x(0), it holds that for all natural number t, ̃x1(t) ̸= 0.\n\n), ∀n ∈ N+, x ∈ RD and Z = {x ∈ RD | Proof of Corollary H.4. Let Fn(x) ≡ F (x) ≜ A(x + ρ Ax ⟨x, v1⟩ = 0}. We can easily check F is C1 on RD \\ Z and Z is a zero-measure set. Applying Theorem D.3, we have the following corollary.\n\n∥Ax∥2\n\nLemma H.5. For t ≥ 0, if ∥ ̃x(t)∥2 > ηλ2\n\n1\n\n2−ηλ1\n\n, ̃x(t) ∈ ∩Ij, then\n\n∥ ̃x(t + 1)∥2 ≤ max(\n\nηλ2 1\n2 − ηλ1\n\n− η\n\nλ4 D\n2λ2 1\n\n, ηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2)\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nProof of Lemma H.5. Note that\n\n ̃x(t + 1) = (I − ηA − η\n\nA2 ∥ ̃x(t)∥2\n\n) ̃x(t)\n\n=\n\n1 ∥ ̃x(t)∥2\n\nD (cid:88)\n\nj=1\n\n(cid:0)(1 − ηλj)∥ ̃x(t)∥2 − ηλ2\n\nj\n\n(cid:1) ̃xj(t)vj\n\nConsider the following two cases. 1 If for any i, such that (cid:12)\n\n(cid:12)(1 − ηλ1)∥ ̃x(t)∥2 − ηλ2\n\n1\n\n(cid:12)(1 − ηλi)∥ ̃x(t)∥2 − ηλ2\n\ni\n\n(cid:12) ≥ (cid:12) (cid:12)\n\n(cid:12) (cid:12), then we have\n\n2 If there exists i, such that (cid:12)\n\n∥ ̃x(t + 1)∥2 ≤ (cid:12)\n\n(cid:12)(1 − ηλ1)∥ ̃x(t)∥2 − ηλ2 (cid:12) < (cid:12) (cid:12)\n\n(cid:12)(1 − ηλ1)∥ ̃x(t)∥2 − ηλ2\n\n1\n\n1\n\n(cid:12) (cid:12) = ηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2 .\n\n(cid:12)(1 − ηλi)∥ ̃x(t)∥2 − ηλ2\n\ni\n\n(cid:12) (cid:12), then suppose WLOG,\n\ni is the smallest among such index. As\n\nηλ2 We have −ηλ2\n\ni − (1 − ηλi)∥ ̃x(t)∥2 < ηλ2 i + (1 − ηλi)∥ ̃x(t)∥2 > ηλ2\n\nCombining with ̃x(t) ∈ I1 ⇒ ∥ ̃x(t)∥2 ≤ ηλ2 Now consider the following vertors,\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2 = (cid:12) 1 − (1 − ηλ1)∥ ̃x(t)∥2. Equivalently, 1 + ηλ2\n\nηλ2\n\ni\n\n(cid:12)(1 − ηλ1)∥ ̃x(t)∥2 − ηλ2\n\n1\n\n(cid:12) (cid:12)\n\n(24)\n\n∥ ̃x(t)∥2 >\n\n2 − ηλ1 − ηλi 1, we have η < λ1−λi\n\n.\n\nλ2 1\n\nv(1)(t) ≜ (ηλ2 1 − (1 − ηλ1)∥ ̃x(t)∥2) ̃x(t) , v(2)(t) ≜ ((2 − ηλ1 − ηλi)∥ ̃x(t)∥2 − ηλ2 v(2+j)(t) ≜ ((ηλi+j−1 − ηλi+j)∥ ̃x(t)∥2 − ηλ2\n\ni − ηλ2\n\n1)P (i:D) ̃x(t) ,\n\ni+j + ηλ2\n\ni+j−1)P (i+j:D) ̃x(t), 1 ≤ j ≤ D − i .\n\nThen we have\n\n∥ ̃x(t + 1)∥2 =∥\n\n1 ∥ ̃x(t)∥2\n\n≤∥\n\n1 ∥ ̃x(t)∥2\n\n∥\n\n1 ∥ ̃x(t)∥2\n\nD (cid:88)\n\nj=1\n\ni−1 (cid:88)\n\nj=1\n\nD (cid:88)\n\nj=i\n\n(cid:0)(1 − ηλj)∥ ̃x(t)∥2 − ηλ2\n\nj\n\n(cid:1) ̃xj(t)vj∥2\n\n(cid:0)ηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2\n\n(cid:1) ̃xj(t)vj∥+\n\n(cid:0)(1 − ηλj)∥ ̃x(t)∥2 − ηλ2\n\nj\n\n(cid:1) ̃xj(t)vj∥2\n\n≤\n\n1 ∥ ̃x(t)∥2\n\nD+1−i (cid:88)\n\nj=1\n\n∥v(j)∥2\n\nBy assumption, we have ̃x(t) ∈ ∩Ij, hence we have\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2)∥ ̃x(t)∥2 ,\n\n∥v(1)(t)∥2 = (ηλ2 ∥v(2)(t)∥2 ≤ η((2 − ηλ1 − ηλi)∥ ̃x(t)∥2 − ηλ2 ∥v(2+j)(t)∥2 ≤ η((ηλi+j−1 − ηλi+j)∥ ̃x(t)∥2 − ηλ2\n\ni − ηλ2\n\n1)λ2 i , i+j + ηλ2\n\ni+j−1)λ2\n\ni+j, 1 ≤ j ≤ D − i .\n\nUsing AM-GM inequality, we have\n\nλi+j−1λ2\n\ni+j ≤\n\nHence\n\nλ2\n\ni+j−1λ2\n\ni+j ≤\n\ni+j\n\n,\n\nλ3\n\nλ4\n\ni+j−1 + 2λ3 3\ni+j−1 + λ4 2\n\ni+j\n\n.\n\n∥v(2+j)(t)∥2 ≤ η((ηλi+j−1 − ηλi+j)∥ ̃x(t)∥2 − ηλ2\n\ni+j−1)λ2\n\ni+j\n\n≤ η2∥ ̃x(t)∥2\n\nD−i (cid:88)\n\nj=1\n\n∥v(2+j)(t)∥2 ≤ η2∥ ̃x(t)∥2\n\ni+j\n\n+ η2 λ4\n\ni+j + ηλ2 i+j−1 − λ4 2\n\ni+j\n\nλ3\n\nλ3\n\ni+j−1 − λ3 3\ni − λ3 3\n\nD\n\n+ η2 λ4\n\ni − λ4 2\n\nD\n\n.\n\n, 1 ≤ j ≤ D − i\n\n34\n\nPublished as a conference paper at ICLR 2023\n\nPutting together,\n\n∥ ̃x(t + 1)∥2 ≤\n\n1 ∥ ̃x(t)∥2\n\nD+1−i (cid:88)\n\nj=1\n\n∥v(i)∥2\n\n≤ηλ2\n\n1 + ηλ2\n\n− η2λ2\n\ni (λ2\n\ni (2 − ηλ1 − ηλi) + η2 λ3 + η2 λ4\n\ni + λ2 1)\n\n1 ∥ ̃x(t)∥2\n\ni − λ3 3\ni − λ4 2\n\nD\n\nD\n\n1 ∥ ̃x(t)∥2\n\n− (1 − ηλ1)∥ ̃x(t)∥2\n\n≤ηλ2\n\n1 + ηλ2\n\ni (2 − ηλ1 −\n\n≤ηλ2\n\n1 + ηλ2\n\ni (2 − ηλ1 −\n\n2 3\n\n2 3\n\nηλi) − (1 − ηλ1)∥ ̃x(t)∥2 − η2λ2 i (\n\nηλi) − (1 − ηλ1)∥ ̃x(t)∥2 − η2λ2 i (\n\n1 2\n\n1 2\n\nλ2\n\ni + λ2 1)\n\nλ2\n\ni + λ2 1)\n\n1 ∥ ̃x(t)∥2 1\n∥ ̃x(t)∥2\n\n− η2\n\n− η\n\nλ4 D\n2∥ ̃x(t)∥2 λ4 D\n2λ2 1\n\n.\n\nWe further discuss three cases 1. If ηλi\n\n< ηλ2\n\ni +λ2 2 λ2 1\n1−ηλ1\n\n(cid:113) 1\n\n1+ηλ2\n\n2−ηλ1−ηλi\n\ni\n\n∥ ̃x(t + 1)∥2\n\n, we have ∥ ̃x(t)∥2 > ηλ2\n\n1+ηλ2\n\ni\n\n2−ηλ1−ηλi\n\n> ηλi\n\n(cid:113) 1\n\ni +λ2 2 λ2 1\n1−ηλ1\n\n,then\n\n≤ηλ2\n\n1 + ηλ2\n\ni (2 − ηλ1 −\n\n≤ηλ2\n\n1 + ηλ2\n\ni (2 − ηλ1 −\n\n2 3\n\n2 3\n\nηλi) − (1 − ηλ1)∥ ̃x(t)∥2 − η2λ2 i (\n\n1 2\n\nλ2\n\ni + λ2 1)\n\n1 ∥ ̃x(t)∥2\n\n− η\n\nλ4 D\n2λ2 1\n\nηλi) − (1 − ηλ1)\n\nηλ2\n\n1 + ηλ2\n\ni\n\n2 − ηλ1 − ηλi\n\n− η2λ2 i (\n\n1 2\n\nλ2\n\ni + λ2 1)\n\n≤\n\nηλ2 1\n2 − ηλ1\n\n− η\n\nλ4 D\n2λ2 1\n\n.\n\n2 − ηλ1 − ηλi\n\nηλ2\n\n1 + ηλ2\n\ni\n\n− η\n\nλ4 D\n2λ2 1\n\nThe second line is because (1 − ηλ1)∥ ̃x(t)∥2 + η2λ2\n\ni ( 1\n\n2 λ2\n\ni + λ2 1)\n\n1 ∥ ̃x(t)∥2\n\nmonotonously increase w.r.t\n\n∥ ̃x(t)∥2 when ∥ ̃x(t)∥2 > ηλi (cid:113) 1\n\n2. If ηλ2\n\n1 ≥ ηλi\n\n2 λ2 i +λ2 1\n1−ηλ1\n\n≥ ηλ2\n\n1+ηλ2\n\ni\n\n2−ηλ1−ηλi\n\n, then\n\n(cid:113) 1\n\n2 λ2 i +λ2 1\n1−ηλ1\n\n. The last line is due to Lemma K.9.\n\n∥ ̃x(t + 1)∥2\n\n≤ηλ2\n\n1 + ηλ2\n\ni (2 − ηλ1 −\n\n≤ηλ2\n\n1 + ηλ2\n\ni (2 − ηλ1 −\n\n≤\n\nηλ2 1\n2 − ηλ1\n\n− η\n\nλ4 D\n2λ2 1\n\n.\n\n2 3\n\n2 3\n\nηλi) − (1 − ηλ1)∥ ̃x(t)∥2 − η2λ2 i (\n\n(cid:114)\n\nηλi) − 2ηλi\n\n(λ2\n\n1 +\n\n1 2\n\nλ2\n\ni )(1 − ηλ1) − η\n\nλ4 D\n2λ2 1\n\n1 2\n\nλ2\n\ni + λ2 1)\n\n1 ∥ ̃x(t)∥2\n\n− η\n\nλ4 D\n2λ2 1\n\nThe second line is because of AM-GM inequality. The last line is due to Lemma K.11.\n\n3. If ηλ2\n\n1 < ηλi\n\n, we have ∥ ̃x(t)∥2 < ηλ2\n\n1 < ηλi\n\n(cid:113) 1\n\n2 λ2 i +λ2 1\n1−ηλ1\n\n(cid:113) 1\n\n2 λ2 i +λ2 1\n1−ηλ1\n\n, then\n\n∥ ̃x(t + 1)∥2\n\n2 3\n\n2 3\n\n≤ηλ2\n\n1 + ηλ2\n\ni (2 − ηλ1 −\n\n≤ηλ2\n\n1 + ηλ2\n\ni (2 − ηλ1 −\n\n≤\n\nηλ2 1\n2 − ηλ1\n\n− η\n\nλ4 D\n2λ2 1\n\n.\n\nηλi) − (1 − ηλ1)∥ ̃x(t)∥2 − η2λ2 i (\n\n1 2\n\nλ2\n\ni + λ2 1)\n\nηλi) − (1 − ηλ1)ηλ2\n\n1 − ηλ2 i (\n\n1 2\n\nλ2\n\ni + λ2 1)\n\n1 λ2 1\n\n1 ∥ ̃x(t)∥2 λ4 D\n2λ2 1\n\n− η\n\n− η\n\nλ4 D\n2λ2 1\n\nThe second line is because (1 − ηλ1)∥ ̃x(t)∥2 + η2λ2\n\ni ( 1\n\n2 λ2\n\ni + λ2 1)\n\n1 ∥ ̃x(t)∥2\n\nmonotonously decrease w.r.t\n\n∥ ̃x(t)∥2 when ∥ ̃x(t)∥2 < ηλi\n\n(cid:113) 1\n\n2 λ2 i +λ2 1\n1−ηλ1\n\n. The last line is due to Lemma K.10.\n\n35\n\nPublished as a conference paper at ICLR 2023\n\nLemma H.6. if ∥ ̃x(t)∥2 ≤ ηλ2\n\n1\n\n2−ηλ1\n\n, it holds that | ̃x1(t + 1)| ≥ | ̃x1(t)| .\n\nProof of Lemma H.6. Nota that | ̃x1(t + 1)| = |1 − ηλ1 − η\n\nλ2 1\n∥ ̃x(t)∥2\n\n|| ̃x1(t)| and that η\n\nλ2 1\n∥ ̃x(t)∥2\n\n> 2 − ηλ2\n\n1. It\n\nfollows that 1 − ηλ1 − η\n\nλ2 1\n∥ ̃x(t)∥2\n\n< −1. Hence we have that | ̃x1(t + 1)| > | ̃x1(t)|.\n\nLemma H.7. For any t ≥ 0, if ∥ ̃x(t)∥2 ≤ ηλ2\n\n1\n\n2−ηλ1\n\n, ̃x(t) ∈ ∩Ij, it holds that\n\n∥ ̃x(t + 1)∥2 ≤ ηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2 .\n\nProof of Lemma H.7. Note that\n\n∥I − ηA − η\n\nA2 ∥ ̃x(t)∥2\n\n∥2 ≤ max 1≤j≤D\n\n{|1 − ηλj − η\n\nλ2 j\n∥ ̃x(t)∥\n\n|} = η\n\nλ2 1\n∥ ̃x(t)∥\n\n− (1 − ηλj) .\n\nThe proof is completed by noting that ∥ ̃x(t + 1)∥ ≤ ∥I − ηA − η A2\n\n∥ ̃x(t)∥2\n\n∥2 ∥ ̃x(t)∥2.\n\nLemma H.8. For any t ≥ 0, if ∥ ̃x(t)∥2 ≤ ηλ2\n\n1\n\n1−ηλ1\n\n, it holds that\n\n∥ ̃x(t + 1)∥2\n\n1 − (1 + ηλ1)∥ ̃x(t)∥2)×\n\n≤(ηλ2 (cid:118) (cid:117) (cid:117) (cid:116)\n\n| ̃x1(t)|2 ∥ ̃x(t)∥2 +\n\n(cid:16)\n\nmax j∈[2:M ]\n\n(cid:32) |(1 − ηλj)∥ ̃x(t)∥2 − ηλ2 j | 1 − (1 − ηλ1)∥ ̃x(t)∥2\n\nηλ2\n\n(cid:33)\n\n(cid:17)2(cid:0)1 −\n\n| ̃x1(t)|2 ∥ ̃x(t)∥2\n\n(cid:1).\n\nProof of Lemma H.8. We will discuss the movement along v1 and orthogonal to v1. First,\n\n∥P (2:D) ̃x(t + 1)∥2 = ∥(I − P (2:D)ηA − η\n\nP (2:D)A2 ∥ ̃x(t)∥2\n\n)P (2:D) ̃x(t)∥2\n\n≤ ∥P (2:D) − P (2:D)ηA − η\n\nP (2:D)A2 ∥ ̃x(t)∥2\n\n∥2∥P (2:D) ̃x(t)∥2\n\n≤ max\n\n{|1 − ηλj −\n\nj∈[2:M ]\n\nηλ2 j\n\n∥ ̃x(t)∥2\n\n|}∥P (2:D) ̃x(t)∥2 .\n\nSecond, | ̃x1(t + 1)| = ( ηλ2\n\n1\n\n∥ ̃x(t)∥2\n\n− 1 + ηλ1)| ̃x1(t)|. Hence we have that\n\n∥ ̃x(t + 1)∥2\n\n1 − (1 + ηλ1)∥ ̃x(t)∥2)×\n\n≤(ηλ2 (cid:118) (cid:117) (cid:117) (cid:116)\n\n| ̃x1(t)|2 ∥ ̃x(t)∥2 +\n\n(cid:16)\n\nmax j∈[2:M ]\n\n(cid:32) |(1 − ηλj)∥ ̃x(t)∥2 − ηλ2 j | 1 − (1 − ηλ1)∥ ̃x(t)∥2\n\nηλ2\n\n(cid:33)\n\n}\n\n(cid:17)2(cid:0)1 −\n\n| ̃x1(t)|2 ∥ ̃x(t)∥2\n\n(cid:1).\n\nLemma H.9. For t, t′ ∈ S, 0 ≤ t ≤ t′, then | ̃x1(t)| ≤ | ̃x1(t′)|.\n\nProof of Lemma H.9. For t ∈ S, by Lemma H.5, t + 1 ∈ S or t + 1 ̸∈ S, t + 2 ∈ S. We will discuss by case.\n\n1. If t + 1 ∈ S, we can use Lemma H.6 to show | ̃x1(t)| ≤ | ̃x1(t + 1)|. 2. If t + 1 ̸∈ S, t + 2 ∈ S, then\n\n| ̃x1(t + 2)| =\n\n(ηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2)(ηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t + 1)∥2)\n\n∥ ̃x(t)∥2∥ ̃x(t + 1)∥2\n\n36\n\n| ̃x1(t)| .\n\nPublished as a conference paper at ICLR 2023\n\nAs\n\n(ηλ2 ⇐⇒ η2λ4\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2)(ηλ2 1 − ηλ2 ≥ (2ηλ1 − η2λ2 1 − ηλ2\n\n1(1 − ηλ1)∥ ̃x(t)∥2\n\n⇐⇒ η2λ4\n\n1)∥ ̃x(t)∥2∥ ̃x(t + 1)∥2\n\n1(1 − ηλ1)(∥ ̃x(t)∥2 + ∥ ̃x(t + 1)∥2)\n\n1 − (1 − ηλ1)∥ ̃x(t + 1)∥2) ≥ ∥ ̃x(t)∥2∥ ̃x(t + 1)∥2\n\n≥ (cid:0)(2ηλ1 − η2λ2\n\n1)∥ ̃x(t)∥2 + ηλ2\n\n1(1 − ηλ1)(cid:1) ∥ ̃x(t + 1)∥2 ,\n\ncombining with Lemma H.7, we only need to prove, 1 − ηλ2 ≥ (cid:0)(2ηλ1 − η2λ2\n\n1(1 − ηλ1)∥ ̃x(t)∥2\n\n1)∥ ̃x(t)∥2 + ηλ2\n\nη2λ4\n\n1(1 − ηλ1)(cid:1) (cid:0)ηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2\n\n(cid:1) .\n\nThrough some calculation, this is equivalent to\n\n((2 − ηλ1)∥ ̃x(t)∥2 − ηλ2\n\n1)((1 − ηλ1)∥ ̃x(t)∥2 − ηλ2\n\n1) ≥ 0 .\n\nwhich holds for ∥ ̃x(t)∥2 ≤ ηλ2\n\n1\n\n2−ηλ1\n\n.\n\nCombining the two cases and using induction, we can get the desired result.\n\nLemma H.10. ∥ ̃x(t)∥ converges to ηλ2\n\n1\n\n2−ηλ1\n\nwhen t → ∞.\n\nProof of Lemma H.10. By Lemma H.9, | ̃x1(t)| increases monotonously for t ∈ S. By Lemma H.5, S is infinite. By Lemma H.2, for sufficiently large t, | ̃x1(t)| is bounded. Combining the three facts, we know ̃x1(t) for t ∈ S converges. Formally ∀ε > 0, there exists Tε > 0 such that ∀t, t′ ∈ S, t′ > t > Tε, ∥ ̃x1(t′)∥2\n\n< 1 + ε.\n\n∥ ̃x1(t)∥2\n\nThen by Lemma H.5, ∀t ∈ S, t + 1 ∈ S or t + 2 ∈ S, we will discuss by case. For t ≥ Tε,\n\n1. If t + 1 ∈ S, then\n\n1 + ε ≥\n\n∥ ̃x1(t + 1)∥2 ∥ ̃x1(t)∥2\n\n=\n\nηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2\n\n∥ ̃x(t)∥2\n\n.\n\n2. If t + 1 ̸∈ S and t + 2 ∈ S, then\n\n1 + ε ≥\n\n∥ ̃x1(t + 2)∥2 ∥ ̃x1(t)∥2\n\n(ηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2)(ηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t + 1)∥2)\n\n∥ ̃x(t)∥2∥ ̃x(t + 1)∥2\n\n(ηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2) (cid:0)ηλ2 ∥ ̃x(t)∥2 (ηλ2\n\nηλ2\n\n1 − (1 − ηλ1) (cid:0)ηλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2\n\n1 − (1 − ηλ1) (cid:0)ηλ2 1 − (1 − ηλ1)∥ ̃x(t)∥2) (cid:1)\n\n.\n\n=\n\n≥\n\n=\n\n∥ ̃x(t)∥2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2\n\n(cid:1)(cid:1)\n\nHere in the last inequality, we apply Lemma H.7.\n\nConcluding, ∥ ̃x(t)∥2 ≥ min ∥ ̃x(t)∥2 ≥ ηλ2\n\n1+ε , . Hence we have ∀t > Tε, ∥ ̃x(t)∥2 ≥ min\n\n1\n\n1\n\nη2λ3 (2−λ1η)λ1η+ε\n\n(cid:16) ηλ2 1\n2−ηλ2\n\n(cid:17)\n\n2−ηλ2 1\n\n(cid:16) ηλ2 1\n2−ηλ2\n\n1+ε ,\n\nFurther by Lemma H.7, ∀t > Tε + 1, ∥ ̃x(t)∥2 ≤ ηλ2\n\n1 − (1 − ηλ1) min\n\nCombining both bound, we have lim t→∞\n\n∥ ̃x(t)∥2 = ηλ2\n\n1\n\n2−ηλ1\n\n.\n\nLemma H.11. ∥ ̃x1(t)∥2 converges to ηλ2\n\n1\n\n2−ηλ1\n\n, when t → ∞.\n\n37\n\n, ∀t > Tε, t ∈ S. As ∀t ̸∈ S, t > Tε, we have\n\n(cid:17)\n\n.\n\n1\n\nη2λ3 (2−λ1η)λ1η+ε (cid:16) ηλ2 1\n2−ηλ2\n\n1+ε ,\n\nη2λ3 (2−λ1η)λ1η+ε\n\n1\n\n(cid:17)\n\n.\n\nPublished as a conference paper at ICLR 2023\n\nProof of Lemma H.11. Notice that\n\n∥P (2:D) ̃x(t + 1)∥2 ≤ max\n\n(cid:18)\n\n|1 − ηλ2 − η\n\nλ2 2\n∥ ̃x(t)∥2\n\n|, |1 − ηλD − η\n\n(cid:19)\n\nλ2 D\n∥ ̃x(t)∥2\n\n|\n\n∥P (2:D) ̃x(t)∥2 .\n\nWhen ∥ ̃x(t)∥2 > ηλ2\n\n2\n\n2−ηλ2−δ ,\n\n−1 + δ ≤ 1 − ηλ2 − η\n\nλ2 2\n∥ ̃x(t)∥2\n\n≤ 1 − ηλD − η\n\nλ2 D\n∥ ̃x(t)∥2\n\n≤ 1 − ηλD\n\n∥P (2:D) ̃x(t + 1)∥2 ≤ max(1 − ηλD, 1 − δ)∥P (2:D) ̃x(t)∥2\n\nHence for sufficiently large t, ∥P (2:D) ̃x(t)∥2 shrinks exponentially, showing that\n\nlim t→∞\n\n∥ ̃x1(t)∥2 = ηλ2\n\n1\n\n2−ηλ1\n\n.\n\nI ANALYSIS FOR FULL-BATCH SAM ON GENERAL LOSS (PROOF OF THEOREM 4.5)\n\nThe goal of this section is to prove the following theorem. Theorem 4.5 (Main). Let {x(t)} be the iterates of full-batch SAM (Equation 3) with x(0) = xinit ∈ U . Under Assumptions 3.2 and 4.4, for all η, ρ such that η ln(1/ρ) and ρ/η are sufficiently small, the dynamics of SAM can be characterized in the following two phases:\n\n• Phase I: (Theorem I.1) Full-batch SAM (Equation 3) follows Gradient Flow with respect to L until entering\n\nan O(ηρ) neighborhood of the manifold Γ in O(ln(1/ρ)/η) steps;\n\n• Phase II: (Theorem I.3) Under a mild non-degeneracy assumption (Assumption I.2) on the initial point of phase II, full-batch SAM (Equation 3) tracks the solution X of Equation 7, the Riemannian Gradient Flow with respect to the loss λ1(∇2L(·)) in an O(ηρ) neighborhood of manifold Γ. Quantitatively, the approximation error between the iterates x and the corresponding limiting flow X is O(η ln(1/ρ)), that is, ∥x(cid:0)⌈T3/(ηρ2)⌉(cid:1) − X(T3)∥2 = O(η ln(1/ρ)) .\n\nMoreover, the angle between ∇L(cid:0)x(⌈ T3\n\nηρ2 ⌉(cid:1) and the top eigenspace of ∇2L(x(⌈ T3\n\nηρ2 ⌉)) is O(ρ).\n\nReaders may refer to Appendix E for notation.\n\nTo prove the theorem, we will separate the dynamic of SAM on general loss L to two phases.\n\nDefine\n\nRj(x) =\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nM (cid:88)\n\ni=j\n\nλ2\n\ni (x)⟨vi(x), x − Φ(x)⟩2 − ηρλ2\n\nj (x), ∀j ∈ [M ], x ∈ U,\n\nwhich is the length projection of x − Φ(x) on button−k non-zero eigenspace of ∇2L(Φ(x)). We will provide a fine-grained convergence bound on Rj(x). Theorem I.1 (Phase I). Let {x(t)} be the iterates defined by SAM ( Equation 3) and x(t) = xinit ∈ U , then under Assumption 3.2 there exists a positive number T1 independent of η and ρ, such that for any T ′ 1 > T1, it holds for all η, ρ such that (η + ρ) ln(1/ηρ) is sufficiently small, we have\n\nmax\n\nT1 ln(1/ηρ)≤ηt≤T ′\n\n1 ln(1/ηρ)\n\nmax\n\nT1 ln(1/ηρ)≤ηt≤T ′\n\n1 ln(1/ηρ)\n\nmax j∈[M ]\n\nmax{Rj(x(t)), 0} = O(ηρ2)\n\n∥Φ(x(t)) − Φ(xinit)∥ ≤ O((η + ρ) ln(1/ηρ))\n\nTheorem I.1 implies SAM will converge to an O(ηρ) neighbor of Γ. Notice in the time frame defined by Theorem I.1, x(t) effectively operates at a local regime around Φ(⌈T1 ln(1/ηρ)/η⌉), this allows us to approximate L with the quadratic Taylor expansion of L at Φ(⌈T1 ln(1/ηρ)/η⌉) and prove the following theorem Theorem I.3.\n\nTowards proving Theorem I.3, we need to make one assumption about the trajectory of SAM, Assumption I.2. Assumption I.2. There exists step t, satisfying that T1 ln(1/ηρ)/η ≤ t ≤ O(ln(1/ηρ/η)), |⟨x(t) − Φ(x(t)), v1(x(t))⟩| ≥ Ω(ρ2) and that ∥x(t) − Φ(x(t))∥2 ≤ λ1(t)ηρ − Ω(ρ2), where T1 is the constant defined in Theorem I.1.\n\n38\n\nPublished as a conference paper at ICLR 2023\n\nWe remark that the above assumption is very mild as we only need the above two conditions in Assumption I.2 to hold for some step in ̃Θ(1/η) steps after Phase I ends, and since then our analysis for Phase II shows that these two conditions will hold until Phase II ends. Theorem I.3 (Phase II). Let {x(t)} be the iterates defined by SAM (Equation 3) under Assumptions 3.2 and 4.4, for all η, ρ such that η ln(1/ρ) and ρ/η is sufficiently small, further assuming that (1) maxj∈[M ] max{Rj(x(0)), 0} = O(ηρ2), (2) ∥Φ(x(0)) − Φ(xinit)∥ = O((η + ρ) ln(1/ηρ)), (3) |⟨x(0) − Φ(x(0)), v1(x(0))⟩| ≥ Ω(ρ2) and (4) ∥x(0) − Φ(x(t))∥2 ≤ λ1(0)ηρ − Ω(ρ2), the iterates x(t) tracks the solution X of Equation 7. Quantitatively for t = ⌈T3/ηρ2⌉, we have that\n\n∥Φ(x(t)) − X(ηρ2t)∥ = O(η ln(1/ρ)) . Moreover, the angle between ∇L(x(t)) and the top eigenspace of ∇2L(Φ(x(t))) is at most O(ρ). Quantitatively,\n\n|⟨x(t) − Φ(x(t)), v1(x(t))⟩| = Θ(ηρ) . |⟨x(t) − Φ(x(t)), vj(x(t))⟩| = O(ηρ2) .\n\nmax j∈[2:M ]\n\nIn this section we will define K as {X(t) | 0 ≤ t ≤ T3} where X is the solution of Equation 7. To simplify our proof, we assume WLOG L(x) = 0 for x ∈ Γ.\n\nI.1 PHASE I (PROOF OF THEOREM I.1)\n\nProof of Theorem I.1. The proof consists of three major parts.\n\n1. Tracking Gradient Flow. Lemma I.4 shows the existence of step tGF = O(1/η) such that x(tGF) is in a\n\nsubset of K h and Φ(x(tGF)) is O(η + ρ) close to Φ(xinit).\n\n2. Decreasing Loss. Lemma I.6 shows the existence of step tDEC = O(ln(1/ρ)/η) such that x(tDEC) is in\n\nO(ρ) neighbor of Γ and Φ(x(tDEC)) is O((η + ρ) ln(1/ρ)) close to Φ(xinit).\n\n3. Entering Invariant Set. Lemmas I.11 and I.13 shows the existence of step tINV = O(ln(1/ρη)/η) such that for any t satisfying tINV ≤ t ≤ tINV + Θ(ln(1/η)/η), we have that x(t) ∈ ∩k∈[M ]Ik and Φ(x(t)) is O((η + ρ) ln(1/ηρ)) close to Φ(xinit).\n\nI.1.1 TRACKING GRADIENT FLOW\n\nLemma I.4 shows that the iterates x(t) tracks gradient flow to an O(1) neighbor of Γ. Lemma I.4. Under condition of Theorem I.1, there exists tGF = O(1/η), such that the iterate x(tGF) is O(1) close to the manifold Γ and Φ(x(tGF)) is O(η + ρ) is close to Φ(xinit). Quantitatively,\n\nL(x(tGF)) ≤\n\nμh2 32\n\n∥x(tGF) − Φ(x(tGF))∥ ≤ h/4 , ∥Φ(x(tGF)) − Φ(xinit)∥ = O(η + ρ) .\n\nProof of Lemma I.4. Choose C = 1\n\n4\n\n(cid:113) μ\n\nζ . Since Φ(xinit) = limT →∞ φ(xinit, T ), there exists T > 0, such that\n\n∥φ(xinit, T ) − Φ(xinit)∥2 ≤ Ch/2 . Note that\n\nx(t + 1) = x(t) − η∇L(x(t) + ρ\n\n∇L (x(t)) ∥∇L (x(t)) ∥\n\n) = x(t) − η∇L(x(t)) + O(ηρ) .\n\nBy Corollary L.3, let b(x) = −∇L(x), p = η and ε = O(ρ), we have that the iterates x(t) tracks gradient flow φ(xinit, T ) in O(1/η) steps. Quantitatively for tGF = ⌈ T\n\nη ⌉, we have that\n\n∥x(tGF) − φ(xinit, T )∥2 = O(ε + p) = O(η + ρ) .\n\nThis implies x(tGF) ∈ K h, hence by Taylor Expansion on Φ,\n\n∥Φ(x(tGF)) − Φ(xinit)∥2 = ∥Φ(x(tGF)) − Φ(φ(xinit, T ))∥2\n\n≤ O(∥x(tGF) − φ(xinit, T )∥2) ≤ O(η + ρ) .\n\n39\n\nPublished as a conference paper at ICLR 2023\n\nThis implies\n\n∥x(tGF) − Φ(x(tGF))∥2\n\n≤∥x(tGF) − φ(xinit, T0)∥2 + ∥φ(xinit, T0) − Φ(xinit)∥2 + ∥Φ(xinit) − Φ(x(tGF))∥2 ≤Ch/2 + O(η + ρ) ≤ Ch ≤ h/4 .\n\nBy Taylor Expansion, we conclude that L(x(tGF)) ≤ ζ∥x(tGF) − Φ(x(tGF))∥2\n\n2/2 ≤ μh2 32 .\n\nI.1.2 DECREASING LOSS\n\nLemma I.6 shows that the iterates x(t) converges to an O(ρ) neighbor of Γ in O(ln(1/ρ)/η) steps. Lemma I.5. Under condition of Theorem I.1, if x(t) ∈ K h and ∥∇L(x(t))∥ ≥ 4ζρ, then we have that L(x(t + 1)) decreases with respect to L(x(t)), quantitatively, we have that\n\nL(x(t + 1)) ≤ L(x(t))(1 − ημ/8) . Moreover the movement of the projection of the iterates on the manifold is bounded, quantitatively, we have that\n\n∥Φ(x(t + 1)) − Φ(x(t))∥ ≤ O(η2) .\n\nProof of Lemma I.5. As x(t) ∈ K h and L is μ-PL in K h, we have L(x(t)) ≥ 0.\n\nAs x(t) ∈ K h, by Lemma F.7 and Taylor Expansion, we have ∥x(t)x(t + 1)∥ = O(η). hence for sufficiently small η, x(t)x(t + 1) ⊂ K r. Using similar argument, the segment from x(t) to x(t) + ρ ∇L(x(t)) ∥∇L(x(t))∥ is in K r.\n\nThen by Taylor Expansion on L,\n\nL(x(t + 1)) = L(x(t) − η∇L\n\nx(t) + ρ\n\n(cid:18)\n\n∇L (x(t)) ∥∇L (x(t)) ∥\n\n(cid:19) )\n\n≤ L(x(t)) − η\n\n∇L (x(t)) , ∇L\n\nx(t) + ρ\n\n(cid:28)\n\n(cid:18)\n\n(cid:19)(cid:29)\n\n∇L (x(t)) ∥∇L (x(t)) ∥\n\n(cid:16)\n\nζη2∥∇L\n\nx(t) + ρ ∇L(x(t))\n\n∥∇L(x(t))∥\n\n2\n\n(cid:17)\n\n∥2\n\n.\n\n+\n\n(25)\n\nBy Taylor Expansion on ∇L, we have that\n\n(cid:18)\n\n∥∇L\n\nx(t) + ρ\n\n(cid:19)\n\n∇L (x(t)) ∥∇L (x(t)) ∥\n\n− ∇L (x(t)) ∥ ≤ ζρ .\n\nAfter plugging in Equation 25, we have that\n\nL(x(t + 1)) ≤ L(x(t)) − η∥∇L (x(t)) ∥2 + ηζρ∥∇L (x(t)) ∥ + ζη2∥∇L (x(t)) ∥2 + ζ 3η2ρ2 .\n\n(26)\n\nAs ∥∇L(x(t))∥ ≥ 4ζρ, we have that the following term is bounded.\n\nζη2∥∇L (x(t)) ∥2 ≤\n\nηζρ∥∇L (x(t)) ∥ ≤\n\n1 2\n1 4\n\nη∥∇L (x(t)) ∥2 ,\n\nη∥∇L (x(t)) ∥2 ,\n\nζ 3η2ρ ≤ ζ 2ηρ2 ≤\n\n1 16\n\nη∥∇L (x(t)) ∥2 .\n\nAfter plugging in Equation 26, by Lemma F.2,\n\nL(x(t + 1)) ≤ L(x(t)) −\n\n1 16\n\nη∥∇L (x(t)) ∥2\n\n≤ L(x(t))(1 − ημ/8) .\n\nAs x(t) ∈ K h, by Taylor Expansion, we have\n\n∥∇L (x(t)) ∥ ≤ ζh .\n\nHence by Lemma F.7 and Taylor Expansion,\n\n∥Φ(x(t + 1)) − Φ(x(t))∥ ≤ ξηρ∥∇L (x) ∥2 + νηρ2 + ξη2∥∇L (x) ∥2\n\n2 + ξζ 2η2ρ2 ≤ O(η2),\n\nwhich completes the proof.\n\n40\n\nPublished as a conference paper at ICLR 2023\n\nLemma I.6. Under condition of Theorem I.1, assuming there exists tGF such that L(x(tGF)) ≤ μh2 32 and x(tGF) ∈ K h/4, then there exists tDEC = tGF + O(ln(1/ρ)/η), such that x(tDEC) is in O(ρ) neighbor of Γ, quantitatively, we have that\n\nMoreover the movement of the projection of Φ(x(·)) on the manifold is bounded,\n\n∥Φ(x(tGF)) − Φ(x(tDEC))∥2 = O(η ln(1/ρ)) .\n\n∥∇L(x(tDEC))∥2 ≤ 4ζρ .\n\nProof of Lemma I.6. Choose tDEC as the minimal t ≥ tGF such that ∥∇L(x(tDEC))∥2 ≤ 4ζρ. Define C = ⌈ln1− ημ\n\n(64ρ2/h2)⌉ = O(ln(1/ρ)/η).\n\n8\n\nWe will first perform an induction on t ≤ min{tDEC, tGF + C} = tGF + O(ln(1/ρ)/η) to show that\n\nL(x(t)) ≤ (1 − ημ/8)t−tGFL(x(tGF))\n\n∥Φ(x(t)) − Φ(x(tGF))∥ = O(η2(t − tGF))\n\nFor t = tGF, the result holds trivially. Suppose the induction hypothesis holds for t. Then by F.1 and Taylor Expansion,\n\n∥Φ(x(t)) − x(t)∥ ≤\n\n(cid:115)\n\n2L(x(tGF)) μ\n\n≤ h/4 .\n\nThen we have that\n\ndist(K, x(t)) ≤dist(K, x(tGF)) + ∥x(tGF) − Φ(x(tGF))∥2\n\n+ ∥Φ(x(tGF)) − Φ(x(t))∥ + ∥Φ(x(t)) − x(t)∥\n\n≤3h/4 + O(η2(t − tGF)) = 3h/4 + O(η ln(1/ρ)) ≤ h .\n\nThat is x(t) ∈ K h. Then as t ≤ tDEC, ∥∇L(x(t))∥2 ≥ 4ζρ. Then by Lemma I.5, we have that\n\nL(x(t + 1)) ≤ (1 − ημ/8)L(x(t)) ≤ (1 − ημ/8)t+1−tGFL(x(tGF)) , ∥Φ(x(t + 1)) − Φ(x(tGF))∥ ≤ ∥Φ(x(t + 1)) − Φ(x(t))∥ + ∥Φ(x(t)) − Φ(x(tGF))∥\n\nwhich completes the induction.\n\n≤ O(η2(t − tGF)) ,\n\nNow if tDEC ≥ tGF + C = tGF + Ω(ln(1/ρ)/η), As the result of the induction, we have that\n\nL(x(tGF + C)) ≤ (1 −\n\nημ 8\n\n)CL(x(tGF)) ≤\n\n64ρ2 h2 L(x(tGF)) ≤ 8ρ2μ .\n\nBy Lemma F.2, we have that ∥∇L(x(tGF+C))∥2 ≤ ζ\n\n(cid:113) 2L(x(tGF+C))\n\nμ\n\n= 4ζρ, which leads to a contradiction.\n\nHence we have that tDEC ≤ tGF + C = tGF + O(ln(1/ρ)/η). By induction, we have that ∥Φ(x(tDEC)) − Φ(x(tGF))∥ = O(η2(tDEC − tGF)) = O(η ln(1/ρ)) .\n\nThis completes the proof.\n\nI.1.3 ENTERING INVARIANT SET\n\nWe first introduce some notations that is required for the proof in this and following subsection.\n\nDefine\n\nˆx = x − Φ(x) , A(x) = ∇2L (Φ(x)) ,\n\n ̃x = A(x)ˆx , ̃xj = ⟨ ̃x, vj(x)⟩ ,\n\nP (j:D)(x) =\n\nM (cid:88)\n\ni=j\n\nvi(x)vT\n\ni (x) .\n\n41\n\nPublished as a conference paper at ICLR 2023\n\nNote ̃x ≈ ∇L(x) for x near the manifold Γ. We also use ̃x(t), A(t) and ˆx(t) to denote ̃x(t), A(x(t)) and ˆx(t).\n\nRecall the original definition of Rj(x) is\n\nRj(x) =\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nM (cid:88)\n\ni=j\n\nλ2\n\ni (x)⟨vi(x), x − Φ(x)⟩2 − ηρλ2\n\nj (x) ,\n\nBased on the above notions, we can rephrase the notion R as\n\nWe additionally define the approximate invariant set Ij as\n\nRj(x) = ∥P (j:D)(x) ̃x∥ − ηρλ2\n\nj (x) .\n\nLemma I.7. Assuming t satisfy that x(t) ∈ K h, then we have that\n\nIj = {∥P (j:D)(x) ̃x∥ ≤ ηρλ2\n\nj (x) + O(ηρ2)} .\n\nμ 2\n\n∥x(t) − Φ(x(t))∥ ≤ ∥ ̃x(t)∥ ≤ ζ∥x(t) − Φ(x(t))∥\n\nProof of Lemma I.7. First by Lemma F.4, Φ(x(t)) ∈ K r, hence\n\n∥ ̃x(t)∥ = ∥∇2L(Φ(x(t)))(x(t) − Φ(x(t)))∥ ≤ ζ∥x(t) − Φ(x(t))∥ .\n\nAlso\n\n∥ ̃x(t)∥ = ∥∇2L(Φ(x(t)))(x(t) − Φ(x(t)))∥ ≥ μ∥P ⊥\n\nΦ(x(t)),Γ(x(t) − Φ(x(t)))∥ .\n\nBy Lemma F.4 and Lemma E.6, we have\n\n∥x(t) − Φ(x(t))∥ ≤ ∥P ⊥\n\nΦ(x(t)),Γ(x(t) − Φ(x(t)))∥ + ∥PΦ(x(t)),Γ(x(t) − Φ(x(t)))∥\n\n≤\n\n≤\n\nζν\n\n4μ2 ∥x(t) − Φ(x(t))∥2 +\n\n1 μ\n\n∥ ̃x(t)∥\n\n1 2\n\n∥x(t) − Φ(x(t))∥ +\n\n1 μ\n\n∥ ̃x(t)∥.\n\nHence ∥x(t) − Φ(x(t))∥ ≤ 2\n\nμ ∥ ̃x(t)∥.\n\nLemma I.8. Assuming t satisfy that x(t) ∈ K h and ∥ ̃x(t)∥2 = O(ρ), then we have that\n\n∥Φ(x(t + 1)) − Φ(x(t))∥ = O(ηρ2) .\n\nProof of Lemma I.8. By Lemma I.7, we have ∥x(t) − Φ(x(t))∥ = O(ρ). By Lemma F.7, we have that\n\n∥Φ(x(t + 1)) − Φ(x(t))∥ ≤ ζξηρ∥x − Φ(x)∥2 + ζ 2ξη2∥x − Φ(x)∥2\n\n2 + νηρ2 + ξζ 2η2ρ2\n\n≤ O(ηρ2).\n\nLemma I.9. Assuming t satisfy x(t) ∈ K h/2 and ∥x(t) − Φ(x(t))∥2 = O(ρ), define x′ as x′(t) = x(t) and for τ ≥ t,\n\nx′(τ + 1) = x′(τ ) − η∇2L(Φ(x(t)))(x′(τ ) − Φ(x(t)))\n\n−ηρ∇2L(Φ(x(t)))\n\n∇2L(Φ(x(t)))(x′(τ ) − Φ(x(t))) ∥∇2L(Φ(x(t)))(x′(τ ) − Φ(x(t)))∥2\n\nThen\n\n∥x′(t + 1) − x(t + 1)∥2 = O(ηρ2)\n\nand further if ∥x(t + 1) − Φ(x(t + 1))∥2 = Ω(ηρ), then\n\n∥x′(t + 2) − x(t + 2)∥2 = O(ηρ2).\n\n42\n\nPublished as a conference paper at ICLR 2023\n\nProof of Lemma I.9. By ∥x(t) − Φ(x(t))∥ = O(ρ), x(t) ∈ K h/2, and Lemma F.7, we have that ∥x(t + 1) − x(t)∥ = O(ηρ) and hence x(t + 1) ∈ K 3h/4. This also implies ∥x(t + 1) − Φ(x(t + 1))∥2 = O(ρ). Similarly we have x(t + 2) ∈ K 3h/4.\n\nFor k ∈ {1, 2}, by Taylor Expansion,\n\nx(t + k + 1) =x(t + k) − η∇L(x(t + k)) − ηρ∇2L(x(t + k))\n\n∇L (x(t + k)) ∥∇L (x(t + k)) ∥\n\n+ O(ηρ2)\n\n=x(t + k) − η∇2L(Φ(x(t + k)))(x(t + k) − Φ(x(t + k))) + O(ηρ2)\n\n− ηρ∇2L(Φ(x(t + k)))\n\n∇L (x(t + k)) ∥∇L (x(t + k)) ∥\n\n+ O(ηρ2)\n\n=x(t + k) − η∇2L(Φ(x(t + k)))(x(t + k) − Φ(x(t + k)))\n\n− ηρ∇2L(Φ(x(t + k)))\n\n∇L (x(t + k)) ∥∇L (x(t + k)) ∥\n\n+ O(ηρ2).\n\nNow by Lemmas I.7 and I.8, ∥Φ(x(t + k)) − Φ(x(t))∥2 = O(ηρ2),\n\nx(t + k + 1) =x(t + k) − η∇2L(Φ(x))(x(t + k) − Φ(x(t)))\n\n− ηρ∇2L(Φ(x))\n\n∇L (x(t + k)) ∥∇L (x(t + k)) ∥\n\n+ O(ηρ2).\n\n(27)\n\nNow we first prove the first claim, we have for k = 0, ∥x(t + k) − x′(t + k)∥2 = 0, by Lemma F.4 and eq. 27,\n\nx(t + 1) =x(t) − η∇2L(Φ(x))(x(t) − Φ(x)) − ηρ\n\n∇2L(Φ(x))(x(t) − Φ(x)) ∥∇2L(Φ(x))(x(t) − Φ(x))∥2\n\n+ O(ηρ2)\n\n=x′(t + 1) + O(ηρ2).\n\nThe second claim is slightly more complex. By the first claim and Lemma F.4, we have that\n\n∇L (x(t + 1)) ∥∇L (x(t + 1)) ∥\n\n=\n\n∇2L(Φ(x(t + 1)))(x(t + 1) − Φ(x(t + 1))) ∥∇2L(Φ(x(t + 1)))(x(t + 1) − Φ(x(t + 1)))∥2\n\nWe first show ∥∇2L(Φ(x(t + 1)))(x(t + 1) − Φ(x(t + 1)))∥2 is of order ∥x(t + 1) − Φ(x(t + 1))∥2 = Ω(ρ2) to show that the normalized gradient term is stable with respect to small perturbation,\n\n+O(∥x(t + 1) − Φ(x(t + 1))∥2).\n\n(28)\n\n∥∇2L(Φ(x(t + 1)))(x(t + 1) − Φ(x(t + 1)))∥2\n\n≥∥PΦ(x(t+1)),Γ∇2L(Φ(x(t + 1)))(x(t + 1) − Φ(x(t + 1)))∥2 ≥∥∇2L(Φ(x(t + 1)))PΦ(x(t+1)),Γ(x(t + 1) − Φ(x(t + 1)))∥2 ≥μ∥PΦ(x(t+1)),Γ(x(t + 1) − Φ(x(t + 1)))∥2 ≥μ(∥(x(t + 1) − Φ(x(t + 1)))∥2 − ∥P ⊥\n\nΦ(x(t+1)),Γ(x(t + 1) − Φ(x(t + 1)))∥2)\n\n≥μ(∥(x(t + 1) − Φ(x(t + 1)))∥2 −\n\nνζ\n\n4μ2 ∥x(t + 1) − Φ(x(t + 1))∥2 2)\n\n≥\n\nμ 2\n\n∥(x(t + 1) − Φ(x(t + 1)))∥2 = Ω(ηρ).\n\nBased on Lemma F.7, we have\n\nΦ(x(t + 1)) − Φ(x(t)) = O(ηρ2).\n\nWe further have by the first claim and Lemma I.8,\n\n∇2L(Φ(x(t + 1)))(x(t + 1) − Φ(x(t + 1))) − ∇2L(Φ(x))(x′(t + 1) − Φ(x(t)))\n\n=∇2L(Φ(x))(x(t + 1) − Φ(x(t + 1))) − ∇2L(Φ(x))(x′(t + 1) − Φ(x(t))\n\n+ O(∥x(t + 1) − Φ(x(t + 1))∥2∥Φ(x(t + 1)) − Φ(x)∥2)\n\n=∇2L(Φ(x))(x(t + 1) − Φ(x(t + 1))) − ∇2L(Φ(x))(x′(t + 1) − Φ(x(t))) + O(ηρ3) =∇2L(Φ(x))(x(t + 1) − x′(t + 1)) + ∇2L(Φ(x))(Φ(x(t + 1)) − Φ(x(t))) + O(ηρ3) =O(ηρ2)\n\n43\n\nPublished as a conference paper at ICLR 2023\n\nThis implies\n\n∇2L(Φ(x(t + 1)))(x(t + 1) − Φ(x(t + 1))) ∥∇2L(Φ(x(t + 1)))(x(t + 1) − Φ(x(t + 1)))∥2\n\n=\n\n∇2L(Φ(x))(x′(t + 1) − Φ(x)) ∥∇2L(Φ(x))(x′(t + 1) − Φ(x))∥2\n\n+ O(ρ)\n\nCombining with Equation 28, we have\n\n∇L (x(t + 1)) ∥∇L (x(t + 1)) ∥\n\n=\n\n∇2L(Φ(x))(x′(t + 1) − Φ(x)) ∥∇2L(Φ(x))(x′(t + 1) − Φ(x))∥2\n\n+ O(ρ)\n\nBy the above approximation and Equation 27,\n\nx(t + 2) = x′(t + 2) + O(ηρ2) .\n\nLemma I.10. Assuming t satisfy that x(t) ∈ K 3h/4 and ∥ ̃x(t)∥2 = O(ρ), then we have that\n\n∥ ̃x(t + 1) − ̃x(t) + ηA(t) ̃x(t) + ηρA2(t)\n\n ̃x(t) ∥ ̃x(t)∥\n\n∥2 = O(ηρ2) .\n\nProof of Lemma I.10. By Lemma I.9, we know\n\n∥x(t + 1) − x(t) + η ̃x(t) + ηρA(t)\n\n ̃x(t) ∥ ̃x(t)∥\n\n∥ ≤ O(ηρ2) .\n\nThis implies\n\nWe also have\n\n∥A(t)(x(t + 1) − Φ(x(t))) − ̃x(t) + ηA(t) ̃x(t) + ηρA2(t)\n\n ̃x(t) ∥ ̃x(t)∥\n\n∥\n\n=∥A(t)(x(t + 1) − x(t) + η ̃x(t) + ηρA(t)\n\n ̃x(t) ∥ ̃x(t)∥\n\n)∥\n\n≤ζ∥x(t + 1) − x(t) + η ̃x(t) + ηρA(t)\n\n ̃x(t) ∥ ̃x(t)∥\n\n∥ = O(ηρ2) .\n\n(29)\n\n ̃x(t + 1) − A(t)(x(t + 1) − Φ(x(t)))\n\n=(A(t + 1) − A(t))(x(t + 1) − Φ(x(t + 1))) − A(t)(Φ(x(t)) − Φ(x(t + 1))) =O(ηρ2) .\n\nPlugging in Equation 29, we have that\n\n∥ ̃x(t + 1) − ̃x(t) + ηA(t) ̃x(t) + ηρA2(t)\n\n ̃x(t) ∥ ̃x(t)∥\n\n∥2 = O(ηρ2) .\n\nLemma I.11. Under condition of Theorem I.1, assuming there exists tDEC such that x(tDEC) ∈ K h/2 and ∥∇L(x(tDEC))∥ ≤ 4ζρ, then there exists tDEC2 = tDEC + O(ln(1/η)/η), such that x(tDEC2) is in I1 ∩ K 3h/4. Furthermore, for any t satisfying tDEC2 ≤ t ≤ tDEC2 + Θ(ln(1/η)/η), we have that x(t) ∈ I1 ∩ K 3h/4 and ∥Φ(x(t)) − Φ(x(tDEC))∥ = O(ρ2 ln(1/η)).\n\nProof of Lemma I.11. For simplicity, denote C = ⌈ln1−ημ quantity Θ(ln(1/ρ)/η) is the same quantity in the statement of the lemma.\n\nημ3 4ζ2 ⌉ + Θ(ln(1/ρ)/η) = O(ln(1/η)/η). Here the\n\nWe will prove the induction hypothesis for tDEC ≤ t ≤ tDEC + 2C, ∥ ̃x(t − 1)∥ ≥ ηρλ2\n\n\n\n1(t), t > tDEC ⇒ ∥ ̃x(t)∥ ≤ (1 − ημ)∥ ̃x(t − 1)∥,\n\n \n\n∥ ̃x(t − 1)∥ ≤ ηρλ2\n\n1(t − 1), t > tDEC ⇒ ∥ ̃x(t)∥ ≤ ηρλ2\n\n1(t) + O(ηρ2), ∥Φ(x(t)) − Φ(x(tDEC))∥ ≤ O(ηρ2(t − tDEC)),\n\nx(t) ∈ K 3h/4.\n\n44\n\nPublished as a conference paper at ICLR 2023\n\nThe induction hypothesis holds trivially for t = tDEC.\n\nAssume the induction hypothesis holds for t′ ≤ t. By Lemmas F.1 and I.7, ∥ ̃x(tDEC)∥2 ≤ ζ∥x(tDEC) − Φ(x(tDEC))∥ ≤ ζ μ ρ. Combining with the induction hypothesis, we have ∥ ̃x(t)∥ ≤ 4ζ2 μ ρ.\n\nμ ∥∇L(x(tDEC))∥ ≤ 4ζ2\n\nBy x(t) ∈ K 3h/4 and Lemma I.8, we have that\n\n∥Φ(x(t + 1)) − Φ(x(t))∥ ≤ O(ηρ2) .\n\nHence we have that\n\n∥Φ(x(t + 1)) − Φ(x(tDEC))∥ ≤ ∥Φ(x(t + 1)) − Φ(x(t))∥ + ∥Φ(x(t)) − Φ(x(tDEC))∥ ≤ O(ηρ2(t + 1 − tDEC)).\n\n(30)\n\nThis proves the third statement of the induction hypothesis.\n\nBy ∥ ̃x(t)∥ = O(ρ) and Lemma I.10, we have that\n\n∥ ̃x(t + 1) − ̃x(t) + ηA(t) ̃x(t) + ηρA2(t)\n\n ̃x(t) ∥ ̃x(t)∥\n\n∥2 = O(ηρ2) .\n\nAnalogous to the proof of Lemmas H.1 and H.2, we have\n\n1. If ∥ ̃x(t)∥ > ηρλ2\n\n1(t), we would have\n\n∥ ̃x(t) − ηA(t) ̃x(t) − ηρA2(t)\n\n≤∥ ̃x(t)∥∥I − ηA(t) − ηρA2(t)\n\n ̃x(t) ∥ ̃x(t)∥ 1\n∥ ̃x(t)∥\n\n∥\n\n∥\n\n≤∥ ̃x(t)∥ max{ηλ1, 1 − ηλD − ηρλ2\n\nD\n\n1 ∥x(t)∥\n\n}\n\n≤ max{(1 − ηλD)∥ ̃x(t)∥ − ηρλ2 ≤ max{(1 − ημ)∥ ̃x(t)∥ − ηρμ2, ηζ∥ ̃x(t)∥}\n\nD, ηλ1∥ ̃x(t)∥}\n\nHence we have\n\n∥ ̃x(t + 1)∥ ≤ max{(1 − ημ)∥ ̃x(t)∥ − ηρμ2, ηζ∥ ̃x(t)∥} + O(ηρ2) ≤ (1 − ημ)∥ ̃x(t)∥.\n\n2. If ∥ ̃x(t)∥2 ≤ ηρλ2\n\n1(t), then by Lemma H.1, we have that\n\n∥ ̃x(t) − ηA(t) ̃x(t) − ηρA2(t)\n\n ̃x(t) ∥ ̃x(t)∥\n\n∥2 ≤ ηρλ2\n\n1(t) .\n\nHence by Lemma K.1\n\n1(t) + O(ηρ2) ≤ ηρλ2 Concluding the two cases, we have shown the first and second claim of the induction hypothesis holds. Hence we can show that ∥ ̃x(t + 1)∥ ≤ 4ζ2 μ2 ρ.\n\nμ ρ. Then by Lemma I.7, we have that ∥x(t + 1) − Φ(x(t + 1))∥ ≤ 8ζ2\n\n1(t + 1) + O(ηρ2) .\n\n∥ ̃x(t + 1)∥ ≤ ηρλ2\n\nAs t ≤ tDEC + 2C = tDEC + O(ln(1/η)/η), by Equation 30,\n\n∥Φ(x(t + 1)) − Φ(x(tDEC))∥ ≤ O(−ρ2 ln η) .\n\nThis implies\n\ndist(x(t + 1), K) ≤dist(x(tDEC), K) + ∥x(tDEC) − Φ(x(tDEC))∥\n\n+ ∥Φ(x(tDEC)) − Φ(x(t + 1))∥ + ∥x(t + 1) − Φ(x(t + 1))∥\n\n=h/2 + O(ρ2 ln(1/η)) + O(ρ) ≤ 3h/4.\n\nThis proves the fourth claim of the inductive hypothesis.\n\nThe induction is complete.\n\n45\n\nPublished as a conference paper at ICLR 2023\n\nNow define tDEC2 the minimal t ≥ tDEC, such that ∥ ̃x(t)∥ ≤ ηρλ2\n\n1(t).\n\nIf tDEC2 > tDEC + C, then by the induction, Lemmas F.1 and I.7,\n\n≤\n\n∥ ̃x(tDEC + C)∥ ≤ (1 − ημ)C∥ ̃x(tDEC)∥ ημ3 4ζ 2 ∥ ̃x(tDEC)∥ ημ3 4ζ 2 ζ∥x(tDEC) − Φ(x(tDEC)∥ ημ2 4ζ ≤ μ2ηρ ≤ λ2\n\n∥∇L(tDEC)∥\n\n1(tDEC + C)ηρ .\n\n≤\n\n≤\n\nThis is a contradiction. Hence we have tDEC2 ≤ tDEC + C. By the induction hypothesis x(tDEC2) ∈ I1 ∩ K 3h/4.\n\nFurthermore by induction, for any t satisfying tDEC2 ≤ t ≤ tDEC + 2C, we have that\n\n∥ ̃x(t)∥ ≤ ηρλ2\n\n1(t) + O(ηρ2) .\n\nBy the induction hypothesis x(t) ∈ I1 ∩ K 3h/4 and ∥Φ(x(t)) − Φ(x(tDEC))∥ = O(ρ2 ln(1/ρ)).\n\nLemma I.12. Under condition of Theorem I.1, assuming t satisfy that x(t) ∈ I1 ∩ K 3h/4, then we have that\n\n(cid:40)\n\nRk(x(t)) ≥ 0 ⇒ Rk(x(t + 1)) + λ2 Rk(x(t)) ≤ 0 ⇒ Rk(x(t + 1)) ≤ O(ηρ2).\n\nk(t + 1)ηρ ≤ (1 − ημ)(Rk(x(t)) + λ2\n\nk(t)ηρ),\n\nProof of Lemma I.12. As x(t) ∈ I1, ∥ ̃x(t)∥2 ≤ ζηρ + O(ηρ2).\n\nAs ∥ ̃x(t)∥2 = O(ρ), we have x(t)x(t + 1) ⊂ K h and Φ(x(t))Φ(x(t + 1)) ⊂ K r.\n\nWe will begin with a quantization technique separating [M ] into disjoint continuous subset S1, ..., Sp such that ∀i ̸= j,\n\nmin k∈Si,l∈Sj\n\n|λk(t) − λl(t)| ≥ ρ .\n\nBy Lemmas I.8 and K.1, we have that for any n ∈ [M ],\n\n|λk(t) − λk(t + 1)| = O(∥∇2L(Φ(x(t))) − ∇2L(Φ(x(t + 1)))∥)\n\n= O(∥Φ(x(t)) − Φ(x(t + 1))∥) = O(ηρ2).\n\nThis implies\n\nDefine\n\nmin k∈Si,l∈Sj\n\n|λk(t + 1) − λl(t + 1)| ≥ ρ − O(ηρ2) ≥ 0.99ρ .\n\n≜ (cid:88)\n\nP (t)\n\nS(i)\n\nk∈Si\n\nvn(t)vn(t)T .\n\nBy Theorem K.3, for any k,\n\n∥P (t) Sk\n\n− P (t+1) Sk\n\n∥ ≤ O(\n\n∥∇2L(Φ(x(t))) − ∇2L(Φ(x(t + 1)))∥ ρ\n\n) = O(ηρ) .\n\nBy Lemma I.10, we have that\n\n∥ ̃x(t + 1) − ̃x(t) + ηA(t) ̃x(t) + ηρA2(t)\n\n ̃x(t) ∥ ̃x(t)∥\n\n∥2 = O(ηρ2) .\n\nWe will write x′(t + 1) as shorthand of ̃x(t) − ηA(t) ̃x(t) − ηρA2(t) ̃x(t)\n\n∥ ̃x(t)∥ .\n\nNow we discuss by cases,\n\n46\n\nPublished as a conference paper at ICLR 2023\n\n(cid:113)(cid:80)p\n\n1. If\n\ni=j ∥P (t) S(i) ̃x(t)∥2 > maxk∈Sj λ2 (cid:118) (cid:117) (cid:117) (cid:116)\n\nS(i) ̃x(t + 1)∥2 ≤\n\n∥P (t)\n\np (cid:88)\n\ni=j\n\nk(t)ηρ > μ2ηρ, by Lemma H.3,\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\np (cid:88)\n\ni=j\n\n∥P (t)\n\nS(i)x′(t + 1)∥2 + O(ηρ2)\n\n≤ max{(cid:0)1 − ηλD(t + 1)(cid:1)∥\n\np (cid:88)\n\ni=j\n\nP (t)\n\nS(i) ̃x(t)∥ − ηρλD(t + 1)2\n\nη max k∈Sj\n\nλk(t + 1)∥\n\np (cid:88)\n\ni=j\n\nP (t)\n\nS(i) ̃x(t)∥} + O(ηρ2)\n\n∥ (cid:80)p\n\nS(i) ̃x(t)∥\n\ni=j P (t) ∥ ̃x(t)∥\n\n,\n\n≤ max{(cid:0)1 − ημ(cid:1)∥\n\np (cid:88)\n\ni=j\n\nP (t)\n\nS(i) ̃x(t)∥ − ηρ\n\nμ3 2ζ\n\n, ηζ∥\n\np (cid:88)\n\ni=j\n\nP (t)\n\nS(i) ̃x(t)∥} + O(ηρ2) .\n\nThis further implies (cid:118) (cid:117) (cid:117) (cid:116)\n\np (cid:88)\n\ni=j\n\n∥P (t+1)\n\nS(i) ̃x(t + 1)∥2 ≤\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\np (cid:88)\n\ni=j\n\n∥P (t)\n\nS(i) ̃x(t + 1)∥2 + O(ηρ∥ ̃x(t + 1)∥)\n\n≤ max{(cid:0)1 − ημ(cid:1)∥\n\np (cid:88)\n\ni=j\n\nP (t)\n\nS(i) ̃x(t)∥ − ηρ\n\nμ3 2ζ\n\n, ηζ∥\n\np (cid:88)\n\ni=j\n\nP (t)\n\nS(i) ̃x(t)∥} + O(ηρ2)\n\n≤(cid:0)1 − ημ(cid:1)∥\n\np (cid:88)\n\ni=j\n\nP (t)\n\nS(i) ̃x(t)∥ .\n\n(cid:113)(cid:80)p\n\n2. If\n\ni=j ∥P (t)\n\nS(i) ̃x(t)∥2 ≤ maxk∈Sj λ2\n\nk(t)ηρ, then by Lemma H.1, we have that\n\n∥\n\np (cid:88)\n\ni=j\n\nHence we have that\n\nP (t)\n\nS(i)x′(t + 1)∥2 ≤ ηρ max\n\nk∈Sj\n\nλ2\n\nk(t) .\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\np (cid:88)\n\ni=j\n\n∥P (t)\n\nS(i) ̃x(t + 1)∥2 ≤\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\np (cid:88)\n\ni=j\n\n∥P (t)\n\nS(i)x′(t + 1)∥2 + O(ηρ2)\n\n≤ max k∈Sj\n\n≤ max k∈Sj\n\nλ2\n\nk(t)ηρ + O(ηρ2)\n\nλ2\n\nk(t + 1)ηρ + O(ηρ2) .\n\nThis further implies\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\np (cid:88)\n\ni=j\n\n∥P (t+1)\n\nS(i) ̃x(t + 1)∥2 ≤\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\np (cid:88)\n\ni=j\n\n∥P (t)\n\nS(i) ̃x(t + 1)∥2 + O(ηρ∥ ̃x(t + 1)∥)\n\n≤ max k∈Sj\n\n≤ max k∈Sj\n\nλ2\n\nk(t)ηρ + O(ηρ2)\n\nλ2\n\nk(t + 1)ηρ + O(ηρ2) .\n\nFinally taking into quantization error, as all the eigenvalue in the same group at most differ Dρ, for any i ∈ Sj, we have that −λ2\n\nk(t + 1) ≤ 2Dζρ + D2ρ2.\n\ni (t + 1) + maxk∈Sj λ2\n\nHence the previous discussion concludes as\n\n1. If Rk(x(t)) ≥ 0\n\nRk(x(t + 1)) + λ2\n\nk(t + 1)ηρ ≤ (1 − ημ)(Rk(x(t)) + λ2\n\nk(t).ηρ)\n\n47\n\nPublished as a conference paper at ICLR 2023\n\n2. If Rk(x(t)) < 0\n\nRk(x(t + 1)) ≤ O(ηρ2).\n\nLemma I.13. Under condition of Theorem I.1, assuming there exists tDEC2 such that for any t satisfying tDEC2 ≤ t ≤ tDEC2 + Θ(ln(1/η)/η), we have that x(t) ∈ I1 ∩ K 3h/4 . Then there exists tINV = tDEC2 + O(ln(1/η)/η)) such that for any t satisfying tINV ≤ t ≤ tINV + Θ(ln(1/η)/η), we have that\n\nx(t) ∈ (∩k∈[M ]Ik) ∩ K 7h/8 .\n\n∥Φ(x(t)) − Φ(x(tDEC2))∥ = O(ρ2 ln(1/η)) .\n\nProof of Lemma I.13. The proof is almost identical with Lemma I.11 replacing the first two iterative hypothesis to Lemma I.12 and is omitted here.\n\nI.2 PHASE II (PROOF OF THEOREM I.3)\n\nProof of Theorem I.3. Let tALIGN = O(ln(1/ρ)/η) be the quantity defined in Lemma I.19.\n\nWe will inductively prove the following induction hypothesis P(t) holds for tALIGN ≤ t ≤ T3/ηρ2 + 1,\n\nx(t) ∈ K h/2, tALIGN ≤ τ ≤ t |⟨x(τ ) − Φ(x(τ )), v1(x(τ ))⟩| = Θ(ηρ), tALIGN ≤ τ ≤ t |⟨x(τ ) − Φ(x(τ )), vj(x(τ ))⟩| = O(ηρ2), tALIGN ≤ τ ≤ t\n\nmax j∈[2:M ]\n\n∥Φ(x(τ )) − X(ηρ2τ )∥ = O(η ln(1/ρ)), tALIGN ≤ τ ≤ t\n\nP(tALIGN) holds due to Lemma I.19. Now suppose P(t) holds, then x(t + 1) ∈ K h. By Lemma I.19 again, |⟨x(t + 1) − Φ(x(t + 1)), v1(x(t + 1))⟩| = Θ(ηρ) and maxj∈[2:M ] |⟨x(t + 1) − Φ(x(t + 1)), vj(x(t + 1))⟩| = O(ηρ2) holds.\n\nNow by Lemma I.20,\n\n∥Φ(x(τ + 1)) − Φ(x(τ )) + ηρ2P ⊥\n\nΦ(x(τ )),Γ∇λ1(t)/2∥ = O(ηρ3 + η2ρ2) , tALIGN ≤ τ ≤ t.\n\nBy Corollary L.3, let b(x) = −∂Φ(x)∇λ1(∇2L(x))/2, p = ηρ2 and ε = O(η + ρ), it holds that\n\n∥Φ(x(τ )) − X(ηρ2τ )∥\n\n=O(∥Φ(x(tALIGN)) − Φ(xinit)∥ + T3ηρ2 + (ρ + η)T3) =O(η ln(1/ρ)), tALIGN ≤ τ ≤ t + 1\n\nThis implies ∥x(t + 1) − X(ηρ2(t + 1))∥2 ≤ ∥x(t + 1) − Φ(x(t + 1))∥2 + ∥Φ(x(t + 1)) − X(ηρ2(t + 1))∥2 = ̃O(η ln(1/ρ)) < h/2. Hence x(t + 1) ∈ K h/2. Combining with P(t) holds, we have that P(t + 1) holds. The induction is complete.\n\nNow P(⌈T3/ηρ2⌉) is equivalent to our theorem.\n\nI.2.1 ALIGNMENT TO TOP EIGENVECTOR\n\nWe will continue to use the notations introduced in Appendix I.1.3.\n\nWe further define\n\nS = {t|∥ ̃x(t)∥ ≤\n\nT = {t|∥ ̃x(t)∥ ≤\n\nηλ2 1\n2 − ηλ1 1\n2\n\n(cid:18) ηλ2\n\n1\n\n2 − ηλ1 1\n2\n\nρ + O(ηρ2)} ,\n\n+\n\nηλ2 2\n2 − ηλ2\n\n(cid:19)\n\nρ} ,\n\n(cid:18) ηλ2\n\n1\n\n2 − ηλ1\n\n+\n\nηλ2 2\n2 − ηλ2\n\n(cid:19)\n\nρ} .\n\nU = {t|Ω(ρ2) ≤ ∥ ̃x1(t)∥ ≤\n\nHere the constant in O depends on the constant in Ij and will be made clear in Lemma I.16.\n\nFor s ∈ S, define next(s) as the smallest integer greater than s in S.\n\n48\n\nPublished as a conference paper at ICLR 2023\n\nLemma I.14. Under the condition of Theorem I.3, there exist constants C1, C2 < 1 independent of η and ρ, if ∥ ̃x1(t)∥2 ≤ 1\n\nρ and x(t) ∈ (∩j∈[M ]Ij) ∩ K 7h/8, then\n\n(cid:16) ηλ1(t)2\n\n2−ηλ1(t) + ηλ2(t)2\n\n2−ηλ2(t)\n\n(cid:17)\n\n2\n\n∥ ̃x(t)∥2 ≥ C1\n\nηλ2 1\n2 − ηλ1\n\nρ ⇒ ∥ ̃x(t + 1)∥2 ≤ C2\n\nηλ2 1\n2 − ηλ1\n\nρ\n\nProof of Lemma I.14. By Lemma I.10, if we write x′(t+1) as shorthand of ̃x(t)−ηA(t) ̃x(t)−ηρA2(t) ̃x(t) then ∥ ̃x(t + 1) − x′(t + 1)∥ = O(ηρ2).\n\n∥ ̃x(t)∥ ,\n\nj\n\nDefine Iquad and ∥xsur(t) − ̃x(t)∥2 = O(ηρ2). We will write x′ ηρA2(t) xsur(t)\n\nas {x|Rj(x) ≤ 0}. Then we can find a surrogate xsur(t) such that xsur(t) ∈ (∩j∈[M ]Iquad\n\n)∩K h sur(t + 1) as shorthand of xsur(t) − ηA(t)xsur(t) −\n\nj\n\n∥xsur(t)∥ .\n\nLet\n\n(cid:115)\n\nh(t) ≜ (2 − t)\n\n1 2t\n\n(cid:18) (ζ − ∆)2 ζ 2\n\n(cid:19)\n\n+ 1\n\n+ (1 −\n\n1 2t\n\n(cid:18) (ζ − ∆)2 ζ 2\n\n(cid:19)\n\n+ 1\n\n) max{\n\nζ 2 − μ2 ζ 2\n\n,\n\n(ζ − ∆)2 ζ 2\n\n}.\n\nAs h(1) < 1, we can choose C1 < 1, such that h(C1) < 1.\n\nWe can further choose C2 = max{(h(C1) + 1)/2, 1 − μ2\n\n3ζ2 } < 1.\n\nWe will discuss by cases\n\n1. If\n\nThen\n\n∥ ̃x(t)∥2 ≥\n\nηλ4 1\n1(1 − ηλD) + (λ2\n\nλ2\n\n1 − λ2\n\nD)(1 − ηλ1)\n\n∥ ̃x(t)∥2 ηλ2 1\n2−ηλ1\n\nρ\n\n=\n\n=\n\n≥\n\nλ2\n\n1(1 − ηλD) + (λ2\n\nλ2\n\n1(2 − ηλ1) 1 − λ2\n\nD)(1 − ηλ1)\n\nλ2\n\nλ2 1(2 − ηλ1) 1(2 − ηλ1 − ηλD) − λ2 λ2 D\nλ2 1\n\n1 1 − λ2\n\n≥ 1 +\n\n1−ηλ1 2−ηλ1\n\nD λ2 1\n\nD(1 − ηλ1) 1 − ηλ1 2 − ηλ1\n\n≥ 1 +\n\nρ\n\nμ2 3ζ 2 .\n\nIn such case we have\n\n∥\n\n ̃x(t) ∥ ̃x(t)∥\n\n−\n\nxsur(t) ∥xsur(t)∥\n\n∥ = O(ρ) .\n\nThen we have ∥x′\n\nsur(t + 1) − x′(t + 1)∥ = O(ηρ2). By Lemma H.5, we have that\n\n∥ ̃x(t + 1)∥2 ≤ ∥ ̃x(t + 1) − x′(t + 1)∥2 + ∥ ̃x(t + 1) − x′\n\nsur(t + 1)∥ + ∥x′\n\nsur(t + 1)∥\n\n2. If\n\nThen we have\n\n≤ max(\n\nηλ2 1\n2 − ηλ1\n\nρ − ηρ\n\nλ4 D\n2λ2 1\n\n, ηρλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2) + O(ηρ2)\n\n≤ max(1 −\n\nλ4\n\nD(2 − ηλ1) 2λ4 1\n\n, (2 − ηλ1) − (1 − ηλ1)(1 +\n\nμ2 3ζ 2 ))\n\nηλ2 1\n2 − ηλ1\n\nρ\n\n≤ (1 −\n\nμ2 3ζ 2 )\n\nηλ2 1\n2 − ηλ1\n\nρ ≤ C2\n\nηλ2 1\n2 − ηλ1\n\nρ .\n\n∥ ̃x(t)∥2 ≤\n\nηλ4 1\n1(1 − ηλD) + (λ2\n\nλ2\n\n1 − λ2\n\nD)(1 − ηλ1)\n\nρ ≤\n\nηλ2 1\n1 − ηλ1\n\nρ.\n\n| − ηρλ2 ηρλ2\n\nD + (1 − ηλD)∥ ̃x(t)∥2| 1 − (1 − ηλ1)∥ ̃x(t)∥2\n\n|ηρλ2 ηρλ2\n\n2 − (1 − ηλ2)∥ ̃x(t)∥2| 1 − (1 − ηλ1)∥ ̃x(t)∥2\n\n≤\n\n≤\n\n49\n\nλ2\n\n1 − λ2 λ2 1\n\nD\n\n.\n\nλ2 2\nλ2 1\n\n.\n\nPublished as a conference paper at ICLR 2023\n\nBy Lemma H.8,\n\n∥x′(t + 1)∥2\n\n(cid:115)\n\n≤(ηρλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2)\n\n≤(ηρλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥2)\n\n(cid:115)\n\n∥ ̃x2 1(t)∥2 ∥ ̃x(t)∥2 2\n\n∥ ̃x2 1(t)∥2 ∥ ̃x(t)∥2 2\n\n+ (1 −\n\n+ (1 −\n\n∥ ̃x2 1(t)∥2 ∥ ̃x(t)∥2 2\n\n∥ ̃x2 1(t)∥2 ∥ ̃x(t)∥2 2\n\n) max{\n\nλ2\n\n1 − λ2 λ2 1\n\nD\n\n,\n\nλ2 2\nλ2 1\n\n}\n\n) max{\n\nζ 2 − μ2 ζ 2\n\n,\n\n(ζ − ∆)2 ζ 2\n\n} .\n\nAs\n\n∥ ̃x1(t)∥2 ≤\n\n1 2\n\n(cid:18) ηλ2\n\n1\n\n2 − ηλ1\n\n+\n\nηλ2 2\n2 − ηλ2\n\n(cid:19)\n\nρ .\n\nFor ∥ ̃x(t)∥2 ≥ ηλ2\n\n1\n\n2−ηλ1 1\n2\n\n≤\n\nρC1, (cid:18) λ2 λ2\n\n∥ ̃x1(t)∥2 ∥ ̃x(t)∥2\n\n2(2 − ηλ1) 1(2 − ηλ2)\n\n(cid:19)\n\n+ 1\n\n/C1 ≤\n\n1 2\n\n(cid:18) λ2 2\nλ2 1\n\n(cid:19)\n\n+ 1\n\n/C1 ≤\n\n1 2C1\n\n(cid:18) (ζ − ∆)2 ζ 2\n\n(cid:19)\n\n+ 1\n\n.\n\nAfter plugging in, we have that\n\n∥ ̃x(t + 1)∥2 ≤ ∥x′(t + 1)∥2 + O(ηρ2) ≤ h(C1)\n\nηλ2 1\n2 − ηλ1\n\nρ + O(ηρ2) ≤ C2\n\nηλ2 1\n2 − ηλ1\n\nρ.\n\nThis concludes the proof.\n\nLemma I.15. Under the condition of Theorem I.3, for any t ≥ 0 satisfying that (1) x(t) ∈ (∩j∈[M ]Ij) ∩ K h, (2) t ̸∈ S, it holds that t + 1 ∈ S.\n\nMoreover, if | ̃x1(t)| ≥ Ω(ρ2) and ∥ ̃x(t)∥2 ≤ ηρλ2\n\n1 − Ω(ρ2), then it holds that ∥ ̃x1(t + 1)∥ ≥ Ω(ρ2).\n\nProof of Lemma I.15. As t ̸∈ S, it holds that\n\n∥ ̃x(t)∥ ≥\n\nηλ2 1\n2 − ηλ1\n\nρ + Θ(ηρ2).\n\nBy Lemma I.10, if we write x′(t + 1) as shorthand of ̃x(t) − ηA(t + 1) ̃x(t) − ηρA2(t) ̃x(t) 1) − x′(t + 1)∥ = O(ηρ2).\n\n∥ ̃x(t)∥ , then ∥ ̃x(t +\n\nj\n\nDefine Iquad K h, and ∥xsur(t) − ̃x(t)∥2 = O(ηρ2). We will write x′ ηρA2(t) xsur(t)\n\nas {x|Rj(x) ≤ 0}. Then we can find a surrogate xsur(t) such that xsur(t) ∈ (∩j∈[M ]Iquad\n\n) ∩ sur(t + 1) as shorthand of xsur(t) − ηA(t)xsur(t) −\n\nj\n\n∥xsur(t)∥ .\n\nAs ∥ ̃x(t)∥ = Ω(ηρ), we have\n\n∥\n\nxsur(t) ∥xsur(t)∥\n\n−\n\n ̃x(t) ∥ ̃x(t)∥\n\n∥2 = O(ρ) .\n\nHence we have that ∥ ̃x(t + 1) − x′\n\nsur(t + 1)∥ = ∥ ̃x(t + 1) − x′(t + 1)∥ + ∥x′(t + 1) − x′\n\nsur(t + 1)∥ = O(ηρ2)\n\nNotice we have ∥xsur(t)∥2 ≥ ηλ2\n\n1\n\n2−ηλ1\n\nρ for properly chosen function in the definition S, hence, by Lemma H.5\n\n∥x′\n\nsur(t + 1)∥2 ≤\n\nηλ2 1\n2 − ηλ1\n\nρ.\n\nThis further implies t + 1 ∈ S.\n\nWe also have\n\n|⟨x′\n\nsur(t + 1), v1⟩| = |⟨xsur(t), v1⟩ − ηλ1⟨xsur(t), v1⟩ − ηρλ2\n\n1\n\n⟨xsur(t), v1⟩ ∥xsur(t)∥\n\n|\n\n= |⟨xsur(t), v1⟩|(ηλ1 +\n\nηρλ2 1\n∥xsur(t)∥\n\n− 1)\n\nWe will discuss by cases. Let C satisfies that C =\n\n(cid:113) 1\n\n2 (λ4\n\n2 + λ4\n\n1).\n\n50\n\nPublished as a conference paper at ICLR 2023\n\n1. If ∥xsur(t)∥ ≤ Cηρ, then as we have λ2\n\n1\n\nC ≥\n\n√\n\n√\n\n2ζ2 ζ2+(ζ−∆)2)\n\n.\n\n|⟨x′\n\nsur(t + 1), v1⟩| ≥ |⟨xsur(t), v1⟩|(\n\nλ2 1\nC\n\n− 1) ≥ Ω(ρ2).\n\n2. If ∥xsur(t)∥ ≥ Cηρ, then as x(t) ∈ I2, we have that |⟨xsur(t), v1⟩| ≥ Ω(ηρ). Then as ∥xsur(t)∥ ≤\n\n∥ ̃x(t)∥2 + O(ηρ2) ≤ λ2\n\n1ηρ − Ω(ρ2), we have that\n\n|⟨x′\n\nsur(t + 1), v1⟩| ≥ |⟨xsur(t), v1⟩|(\n\nλ2\n\n1ηρ 1ηρ − Ω(ρ2)\n\nλ2\n\n− 1) ≥ Ω(ρ2).\n\nBy previous approximation results, we have that ∥ ̃x1(t + 1)∥ ≥ Ω(ρ2). Lemma I.16. Under the condition of Theorem I.3, for any t ≥ 0 satisfying that (1) x(t) ∈ (∩j∈[M ]Ij) ∩ K 15h/16, (2) t ∈ S, it holds that next(t) is well defined and next(t) ≤ t + 2.\n\nProof of Lemma I.16. Following similar argument in Lemma H.1, we have that x(t + 1) ∈ (∩j∈[M ]Ij) ∩ K h.\n\nIf t + 1 ̸∈ S, then we can apply Lemma I.15 to show that t + 2 ∈ S.\n\nLemma I.17. Under the condition of Theorem I.3, there exists constant C > 0 independent of η and ρ, assuming that (1) x(t) ∈ (∩j∈[M ]Ij) ∩ K 7h/8, (2) t ∈ S, (3) Ω(ρ2) ≤ ∥ ̃x1(t)∥, then\n\n∥ ̃x1(next(t))∥ ≥ ∥ ̃x1(t)∥ − O(ηρ2) .\n\nProof of Lemma I.17. This is by standard approximation as in previous proof and Lemma H.9.\n\nLemma I.18. Under the condition of Theorem I.3, there exists constant C > 0 independent of η and ρ, assuming that (1) x(t) ∈ (∩j∈[M ]Ij) ∩ K 7h/8, (2) t ∈ S (3) Ω(ρ2) ≤ ∥ ̃x1(t)∥ ≤ 1 then\n\n+ ηλ2\n\n(cid:16) ηλ2\n\n2−ηλ1\n\n2−ηλ2\n\nρ,\n\n(cid:17)\n\n2\n\n1\n\n2\n\n∥ ̃x1(next(t))∥ ≥ min{(1 + Cη)∥ ̃x1(t)∥,\n\nor ∥ ̃x1(next(next(t)))∥ ≥ min{(1 + Cη)∥ ̃x1(t)∥,\n\n1 2\n\n1 2\n\n(cid:18) ηλ2\n\n1\n\n2 − ηλ1\n\n(cid:18) ηλ2\n\n1\n\n2 − ηλ1\n\n+\n\n+\n\nηλ2 2\n2 − ηλ2 ηλ2 2\n2 − ηλ2\n\n(cid:19)\n\nρ} ,\n\n(cid:19)\n\nρ}.\n\nProof of Lemma I.18. In this proof, we will sometime drop the t in λk(t) or A(t). Applying Lemma I.16, we (cid:17) have next(t) and next(next(t)) are well-defined. We can suppose ∥ ̃x1(next(t))∥2 ≤ 1 ,\nelse the result holds already.\n\n+ ηλ2\n\n(cid:16) ηλ2\n\n2−ηλ2\n\n2−ηλ1\n\n2\n\n1\n\n2\n\nBy assumption, we have ∥ ̃x1(t)∥ ≥ Ω(ρ2).\n\nUsing Lemma I.10,\n\n∥ ̃x(t + 1) − ̃x(t) + ηA ̃x(t) + ηρA2 ̃x(t)\n\n∥ ̃x(t)∥\n\n∥ ≤ O(ηρ2) .\n\nDenote\n\nas the one step update of SAM on the quadratic approximation of the general loss.\n\nx′(t + 1) = ̃x(t) + ηA ̃x(t) + ηρA2 ̃x(t)\n\n∥ ̃x(t)∥\n\n,\n\nNow using Lemma I.14 and the induction hypothesis, we have for some C1 and C2 smaller than 1, ∥ ̃x(t)∥ ≥ C1\n\nρ ⇒ ∥x′(t + 1)∥ ≤ C2\n\nρ.\n\n1\n\n1\n\nηλ2 2−ηλ1\n\nηλ2 2−ηλ1\n\nWe will discuss by cases,\n\n1 If ∥ ̃x(t)∥ ≤ C1\n\nρ If next(t) = t + 1, then\n\n1\n\nηλ2 2−ηλ1\n\n∥x′\n\n1(t + 1)∥ ∥ ̃x1(t)∥\n\n=\n\nηρλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥\n\n∥ ̃x(t)∥\n\n≥\n\n(2 − C1) − ηλ1 + C1ηλ1 C1\n\n≥\n\n1 C1\n\n51\n\nPublished as a conference paper at ICLR 2023\n\nAs we have ̃x1(t) = Ω(ρ2), we have x′ implies\n\n1(t + 1) = Ω(ρ2), then as ∥ ̃x1(t + 1) − x′\n\n1(t + 1)∥ = O(ηρ2), this\n\n∥ ̃x1(t + 1)∥ ≥ ∥x′\n\n1(t + 1)∥ − O(ηρ2) ≥\n\n1 C1\n\n∥ ̃x1(t + 1)∥ − O(ηρ2) ≥\n\n1 2\n\n(\n\n1 C1\n\n+ 1)∥ ̃x1(t)∥.\n\nIf next(t) = t + 2, define x′(t + 2) = x′(t + 1) − ηAx′(t + 1) − ηρA2 x′(t+1) by Lemma I.9, we have ∥x′(t + 2) − ̃x(t + 2)∥ = O(ηρ2).\n\n∥x′(t+1)∥ , as ∥ ̃x1(t + 1)∥ = Ω(ηρ),\n\n∥x′(t + 2)∥ ∥ ̃x1(t)∥\n\n=\n\n≥\n\n=\n\n(ηρλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥)(ηρλ2\n\n1 − (1 − ηλ1)∥x′(t + 1)∥)\n\n∥ ̃x(t)∥∥x′(t + 1)∥\n\n(ηρλ2\n\n1 − (1 − ηλ1)∥ ̃x(t)∥) (cid:0)ηρλ2 ∥ ̃x(t)∥ (ηρλ2\n\n1 − (1 − ηλ1) (cid:0)ηρλ2 1 − (1 − ηλ1)∥ ̃x(t)∥)\n\nηρλ2\n\n1 − (1 − ηλ1) (cid:0)ηρλ2 ∥ ̃x(t)∥\n\n1 − (1 − ηλ1)∥ ̃x(t)∥(cid:1)\n\n1 − (1 − ηλ1)∥ ̃x(t)∥(cid:1)(cid:1)\n\n≥ (1 − ηλ1)2 +\n\nηλ1 C1\n\n(2 − ηλ1) ≥ 1 + 4Cη.\n\nCombining with | ̃x1(t)| ≥ Ω(ρ2), we have that\n\n∥ ̃x1(next(t))∥ ≥ (1 + Cη)∥ ̃x1(t)∥\n\nηλ2 2−ηλ1\n\n2 Case 2 ∥ ̃x(t)∥ > C1\n\nρ, then ∥ ̃x(t + 1)∥ ≤ C2 By Lemma I.17, ∥ ̃x1(t + 1)∥ ≥ (1 − Cη)∥ ̃x1(t)∥. ηλ2 As ∥ ̃x(next(t))∥ ≤ C2 2−ηλ1\n\n, similar to the first case,\n\n1\n\n1\n\nηλ2 2−ηλ1\n\n1\n\nρ, next(t) = t + 1\n\n∥ ̃x1(next(next(t)))∥ ≥ (1 + 4Cη)∥ ̃x1(next(t))∥ ≥ (1 + Cη)∥ ̃x1(t)∥.\n\nIn conclusion, if ∥ ̃x1(t)∥ ≤ 1\n\n2\n\n(cid:16) ηλ2\n\n1\n\n2−ηλ1\n\n+ ηλ2\n\n2\n\n2−ηλ2\n\n(cid:17)\n\nρ, we would have there exists C > 0\n\n∥ ̃x1(next(t))∥ ≥ (1 + Cη)∥ ̃x1(t)∥ or ∥ ̃x1(next(next(t)))∥ ≥ (1 + Cη)∥ ̃x1(t)∥.\n\nLemma I.19. Under the condition of Theorem I.3, there exists constant T2 > 0 independent of η and ρ, we would have that when t = tALIGN = ⌈T2 ln(1/ρ)/η⌉,\n\n|⟨x(t) − Φ(x(t)), v1(x(t))⟩| = Θ(ηρ) , |⟨x(t) − Φ(x(t)), vj(x(t))⟩| = O(ηρ2) .\n\nmax j∈[2:M ]\n\nFurther if x(t′) ∈ K h holds for t′ = 0, 1, ..., tLOCAL, then for t satisfying tALIGN ≤ t ≤ tLOCAL\n\n|⟨x(t) − Φ(x(t)), v1(x(t))⟩| = Θ(ηρ) , |⟨x(t) − Φ(x(t)), vj(x(t))⟩| = O(ηρ2) .\n\nmax j∈[2:M ]\n\nProof of Lemma I.19. Let C be the constant defined in Lemma I.18.\n\nBy Lemma I.15, we can suppose WLOG ∥ ̃x1(0)∥ ≥ ρ2 and 0 ∈ S. Define\n\nC1 ≜ ⌈log1+Cη(\n\nηλ2 1\n2 − ηλ1\n\n/ρ)⌉\n\nC2 ≜ C1 + ⌈ln\n\nmax{1− μ2\n\n2ζ2 ,1− ∆2\n\n4ζ2 }\n\nρ2 ζ 2 ⌉ = O(log(1/ρ)/η).\n\nWe will choose tALIGNMID as the minimal t ∈ S, such that ∥ ̃x1(t)∥ ≥ 1\n\n2\n\n52\n\n(cid:16) ηλ2\n\n1\n\n2−ηλ1\n\n+ ηλ2\n\n2\n\n2−ηλ2\n\n(cid:17)\n\nρ.\n\nPublished as a conference paper at ICLR 2023\n\nThen by induction and Lemmas I.17 and I.18, we easily have that for t ≤ min{C2 + 1, tALIGNMID} and t ∈ S, we have that\n\nx(t) ∈ K 7h/8 ∩ (∩jIj) ,\n\n∥ ̃x1(t)∥ ≥ min{(1 + Cη)t/4∥ ̃x1(0)∥,\n\nor ∥ ̃x1(next(t))∥ ≥ min{(1 + Cη)t/4∥ ̃x1(0)∥,\n\n1 2\n\n1 2\n\n(cid:18) ηλ2\n\n1\n\n2 − ηλ1\n\n(cid:18) ηλ2\n\n1\n\n2 − ηλ1\n\n+\n\n+\n\n(cid:19)\n\n(cid:19)\n\nηλ2 2\n2 − ηλ2 ηλ2 2\n2 − ηλ2\n\nρ}\n\nρ} .\n\nThe detailed induction is analogous to previous inductive argument and is omitted. If tALIGNMID ≥ C1, then we have for the minimal t ≥ C1 and t ∈ S\n\nThis is a contradiction and we have that tALIGNMID ≤ C1.\n\n∥ ̃x1(t)∥ ≥\n\nηλ2 1\n2 − ηλ1\n\nρ .\n\nBy Lemma I.17, ∥ ̃x1(next(t))∥ ≥ ∥ ̃x1(t)∥ − O(ηρ2) for ∥ ̃x1(t)∥ ≥ 1 then by Lemma I.18,\n\n2\n\n∥ ̃x(t)∥ ≥ ∥ ̃x1(t))∥ ≥\n\n1 4\n\nfor C2 ≥ t ≥ tALIGNMID.\n\n(cid:18) ηλ2\n\n1\n\n2 − ηλ1\n\n+ 3\n\nηλ2 2\n2 − ηλ2\n\n(cid:19)\n\nρ.\n\n(cid:16) ηλ2\n\n1\n\n2−ηλ1\n\n+ ηλ2\n\n2\n\n2−ηλ2\n\n(cid:17)\n\nρ and t ∈ S and\n\nWe will then show that for t ≥ tALIGNMID + C1 iteration, ∥P (2:D) ̄x(t + 1)∥ ≤ O(ηρ2).\n\nFor C1 ≥ t ≥ tALIGNMID,\n\n1 − ηλ2 − ηρ\n\nλ2 2\n∥ ̃x(t)∥\n\n≤ 1 − ηλD − ηρ\n\nλ2 D\n∥ ̃x(t)∥\n\n≤ 1 −\n\nλ2 D\n2λ2 1\n\n≤ 1 −\n\nμ2 ζ 2 .\n\nNotice that,\n\nHence,\n\n1 − ηλ2 − ηρ\n\n≥ 1 − ηλ2 − ηρ\n\n≥1 − ηλ2 −\n\n(2 − ηλ2) ≥ −1 +\n\nλ2 2\n∥ ̃x(t)∥ 4λ2 2\n1 + 3λ2\n\n2\n\nλ2\n\nλ2 2\n∥ ̃x(t)∥ 2(λ2 λ2\n\n1 − λ2 2) 1 + 3λ2\n\n2\n\n≥ −1 +\n\n∆2 2ζ 2\n\n∥P (2:D)(t)x′(t + 1)∥2 ≤ max{1 −\n\nμ2 ζ 2 , 1 −\n\n∆2 2ζ 2 }∥P (2:D)(t) ̃x(t)∥2\n\nNow by Lemma K.1 and Theorem K.3,\n\n∥P (2:D)(t) − P (2:D)(t + 1)∥ ≤ O(ηρ2) ∥v1(t) − v1(t + 1)∥ ≤ O(ηρ2) ∥λ1(t) − λ1(t + 1)∥ ≤ O(ηρ2)\n\nBy Lemma I.10, we have that ∥x′(t + 1) − ̃x(t + 1)∥ = O(ηρ2).\n\nCombining the above, it holds that\n\n∥P (2:D)(t + 1) ̃x(t + 1)∥ ≤ max{1 −\n\nμ2 2ζ 2 , 1 −\n\n∆2 4ζ 2 }∥P (2:D)(t) ̃x(t)∥ + O(ηρ2)\n\nHence when t = tALIGN = tALIGNMID + C2,\n\n∥ ̃x(t)∥ ≥ ∥ ̃x1(t)∥ ≥ Ω(ηρ) ,\n\n∥P (2:D)(t) ̃x(t)∥ ≤ O(ηρ2) .\n\nBy x(t) ∈ I1, we easily have ∥ ̃x1(t)∥ = O(ηρ). Hence we conclude that\n\n∥ ̃x1(t)∥ = Θ(ηρ) , ∥P (2:D)(t) ̃x(t)∥ = O(ηρ2) .\n\nThe second claim is just another induction similar to previous steps and is omitted as well.\n\n53\n\nPublished as a conference paper at ICLR 2023\n\nI.2.2 TRACKING RIEMANNIAN GRADIENT FLOW\n\nWe are now ready to show that Φ(x(t)) will track the solution of Equation 7. The main principal of this proof has been introduced in Section 4.3.\n\nLemma I.20. Under the condition of Theorem I.3, for any t satisfying that\n\nx(t) ∈ K h, ∥ ̃x1(t)∥ = Θ(ηρ),\n\n∥P (2:D)(t) ̃x(t)∥ = O(ηρ2) ,\n\nit holds that\n\n∥Φ(x(t + 1)) − Φ(x(t)) + ηρ2P ⊥\n\nΦ(x(t)),Γ∇λ1(t)/2∥ ≤ O(ηρ3 + η2ρ2) .\n\nProof of Lemma I.20. To begin with, we can approximate Φ(x(t + 1)) − Φ(x(t)) by its first order Taylor Expansion, by Lemma F.7,\n\n∥Φ(x(t + 1)) − Φ(x(t)) − ∂Φ(x(t))(x(t + 1) − x(t))∥ = O(∥x(t + 1) − x(t)∥2) = O(η2ρ2) .\n\nThen by plugging in the update rule and another Taylor Expansion,\n\n∥∂Φ(x(t))(x(t + 1) − x(t))−ηρ∂Φ(x(t))∇2L (x)\n\n∇L (x) ∥∇L (x) ∥\n\n−ηρ2∂Φ(x(t))∂∇2L (x)[\n\n∇L (x) ∥∇L (x) ∥\n\n,\n\n∇L (x) ∥∇L (x) ∥\n\n]/2∥2 = O(ηρ3).\n\n∥ = ηρ∥∇L (x) ∥∥∂2Φ(x(t))\n\n(cid:20) ∇L (x)\n\n∥∇L (x) ∥\n\n(cid:21)\n\n,\n\n∇L (x) ∥∇L (x) ∥\n\n∥ = O(ηρ∥∇L (x) ∥) .\n\nUsing Lemma F.3, we have\n\n∥ηρ∂Φ(x(t))∇2L (x)\n\n∇L (x) ∥∇L (x) ∥\n\nPutting together, we have that\n\n∥Φ(x(t + 1)) − Φ(x(t)) − ηρ2∂Φ(x(t))∂∇2L (Φ(x(t)))[\n\n∇L (x) ∥∇L (x) ∥\n\n,\n\n∇L (x) ∥∇L (x) ∥\n\n]/2∥\n\n≤O(η2ρ2 + ηρ3) + O(ηρ∥∇L (x) ∥) .\n\nAs we have ∥ ̃x(t)∥ = Θ(ηρ), hence by Lemmas F.2 and I.7,\n\n∥Φ(x(t + 1)) − Φ(x(t)) − ηρ2∂Φ(x(t))∂∇2L (Φ(x(t)))[\n\n∇L (x(t)) ∥∇L (x(t)) ∥\n\n,\n\n∇L (x(t)) ∥∇L (x(t)) ∥\n\n]/2∥\n\n≤O(ηρ3 + η2ρ2)\n\nFinally, we have that\n\n∥ηρ2∂Φ(x(t))∂∇2L (Φ(x(t)))[\n\n∇L (x(t)) ∥∇L (x(t)) ∥\n\n,\n\n∇L (x(t)) ∥∇L (x(t)) ∥\n\n]/2\n\n− ηρ2∂Φ(x(t))∂∇2L (Φ(x(t)))[v1(t), v1(t)]/2∥ ≤ O(ηρ3)\n\nas the angle between ∇L(x)\n\n∥∇L(x)∥ and v1(t) is O(ρ).\n\nBy Lemma F.3, it holds that\n\n∂Φ(x(t))∂∇2L (Φ(x(t)))[v1(t), v1(t)] = P ⊥\n\nX,Γ∇(λ1(t))\n\nPutting together we have that,\n\n∥Φ(x(t + 1)) − Φ(x(t)) + ηρ2P ⊥\n\nX,Γ∇λ1(t)/2∥ ≤ O(ηρ3 + η2ρ2).\n\nIt completes the proof.\n\n54\n\nPublished as a conference paper at ICLR 2023\n\nI.3 PROOF OF THEOREM 4.5\n\nProof of Theorem 4.5. By Theorem I.1, there exists constant T1 independent of η, ρ, such that for any T ′ T1 independent of η, ρ, it holds that\n\n1 >\n\nmax\n\nT1 ln(1/ηρ)≤ηt≤T ′\n\n1 ln(1/ηρ)\n\nmax\n\nT1 ln(1/ηρ)≤ηt≤T ′\n\n1 ln(1/ηρ)\n\nmax j∈[M ]\n\nRj(x(t)) = O(ηρ2).\n\n∥Φ(x(t)) − Φ(xinit)∥ = O((η + ρ) ln(1/ηρ)).\n\nBy Assumption I.2, there exists step T1 ln(1/ηρ) ≤ ηtPHASE ≤ T ′\n\n1 ln(1/ηρ), such that\n\nmax j∈[M ]\n\nRj(x(tPHASE)) = O(ηρ2),\n\n∥Φ(x(tPHASE)) − Φ(xinit)∥ = O((η + ρ) ln(1/ηρ)), |⟨x(tPHASE) − Φ(x(tPHASE)), v1(x(tPHASE))⟩| ≥ Ω(ρ2). ∥x(tPHASE)∥2 ≤ λ1(tPHASE)ηρ − Ω(ρ2).\n\nHence by Theorem I.3, if we consider a translated process with x′(t) = x(t + tPHASE), we would have for any T3 such that the solution X of Equation 7 is well defined, we have that for t = ⌈ T3\n\nηρ2 ⌉\n\n∥Φ(x′(t)) − X(ηρ2t)∥2 = O(η ln(1/ρ)) .\n\nThis implies for t satisfying X(ηρ2(t − tPHASE)) is well-defined,\n\n∥Φ(x(t)) − X(ηρ2(t − tPHASE))∥2 = O(η ln(1/ρ)).\n\nFinally, as\n\n∥X(ηρ2(t − tPHASE)) − X(ηρ2t)∥2\n\n= O(ηρ2tPHASE) = O(ρ ln(1/ηρ)) = O(η ln(1/ρ)).\n\nWe have that\n\n∥Φ(x(t)) − X(ηρ2t)∥2 = O(η ln(1/ρ)).\n\nThe alignment result is a direct consequence of Theorem I.3.\n\nI.4 PROOFS OF COROLLARIES 4.6 AND 4.7\n\nProof of Corollary 4.6. We will do a Taylor expansion on LMax . By Theorem I.1 and I.3, we have ∥x(⌈T3/ηρ2⌉)) − X(T3)∥ = ̃O(η + ρ) and ∥x(⌈T3/ηρ2⌉)) − Φ(x(⌈T3/ηρ2⌉)))∥2 = O(ηρ). For convenience, we denote x(⌈T3/ηρ2⌉) by x.\n\nρ\n\nRMax ρ\n\n(x) = max ∥v∥2≤1\n\nρvT ∇L(x) + ρ2vT ∇2L(x)v/2 + O(ρ3)\n\nSince max∥v∥2≤1 ∥vT ∇L(x)∥2 = O(∥x − Φ(x)∥2) = O(ηρ), it holds that\n\nRMax ρ\n\n(x) = ρ2 max ∥v∥2≤1\n\nvT ∇2L(x)v/2 + O(η2ρ2 + ρ3)\n\n= ρ2λ1(∇2L(x)) + O(η2ρ2 + ρ3) = ρ2λ1(∇2L(X(T3))) + ̃O(ηρ2),\n\nwhich completes the proof.\n\nProof of Corollary 4.7. We choose T such that X(Tε) is sufficiently close to X(∞), such that λ1(X(Tε)) ≤ λ1(X(∞)) + ε/2. By Corollary 4.6 (let T3 = Tε), we have that for all ρ, η such that η ln(1/ρ) and (x(⌈Tε/(ηρ2)⌉)) − ρ2λ1(X(Tε))/2∥ ≤ ̃o(1). This further implies ρ/η are sufficiently small, ∥RMax (x(⌈Tε/(ηρ2)⌉))−ρ2λ1(X(∞))/2∥ ≤ ερ2 +o(1). We also have L(x(⌈Tε/(ηρ2)⌉))−inf x∈U ′ L(x) = ∥RMax ρ\no(1). Then we can leverage Theorem G.6 and Theorem G.3 to get the desired bound.\n\nρ\n\n55\n\nPublished as a conference paper at ICLR 2023\n\nI.5 DERIVATIONS FOR SECTION 4.3\n\nWe will first show our derivation of Equation 9.\n\nIn Phase II, x(t) is O(ηρ)-close to the manifold Γ and therefore it can be shown that ∥x(t) − Φ(x(t))∥2 = O(ηρ) holds for every step in Phase II. This also implies that ∥x(t + 1) − x(t)∥2 = O(ηρ) (See Lemma F.7). Using Taylor expansion around x(t), we have that\n\nΦ(x(t + 1)) − Φ(x(t)) =∂Φ(x(t))(x(t + 1) − x(t)) + O(∥x(t + 1) − x(t)∥2 2)\n\nFor any x ∈ RD, applying Taylor expansion on ∇L(cid:0)x + ρ ∇L(x) ∥∇L(x)∥2\n\n= − η∂Φ(x(t))∇L(cid:0)x(t) + ρ\n\n(cid:1) + O(η2ρ2) .\n\n∇L(x(t)) ∥∇L(x(t))∥2 (cid:1) around x, we have that\n\n(31)\n\n∇L(cid:0)x + ρ\n\n∇L(x) ∥∇L(x)∥2\n\n(cid:1)\n\n=∇L(x) + ρ∇2L(x)\n\n∇L(x) ∥∇L(x)∥2\n\n+\n\nρ2 2\n\n∂2(∇L)(x)(cid:2) ∇L(x)\n\n∥∇L(x)∥2\n\n,\n\n∇L(x) ∥∇L(x)∥2\n\n(cid:3) + O(ρ3).\n\n(32)\n\nUsing Equation 32 with x = x(t), plugging in Equation 31 and then rearranging, we have that\n\nΦ(x(t + 1)) − Φ(x(t)) +\n\nηρ2 2\n\n∂Φ(x(t))∂2(∇L)(x(t))(cid:2) ∇L(x(t))\n\n∥∇L(x(t))∥2\n\n,\n\n∇L(x(t)) ∥∇L(x(t))∥2\n\n(cid:3)\n\n= − η∂Φ(x(t))∇L(x(t)) − ηρ∂Φ(x(t))∇2L(x(t))\n\n∇L(x(t)) ∥∇L(x(t))∥2\n\n+ O(η2ρ2 + ηρ3) .\n\nBy Lemma 3.1, we have that ∂Φ(x(t))∇L(x(t)) = 0. Furthermore, by Lemma F.5, we have that ∂Φ(Φ(x(t)))∇2L(Φ(x(t))) = 0. This implies that\n\n∂Φ(x(t))∇2L(x(t)) = ∂Φ(Φ(x(t)))∇2L(Φ(x(t))) + O(∥x(t) − Φ(x(t))∥2) = O(ηρ) .\n\nThus we conclude that\n\nΦ(x(t + 1)) − Φ(x(t)) = −\n\nηρ2 2\n\n∂Φ(x(t))∂2(∇L)(x(t))(cid:2) ∇L(x(t))\n\n∥∇L(x(t))∥2\n\n,\n\n∇L(x(t)) ∥∇L(x(t))∥2\n\n(cid:3)\n\n+O(η2ρ2 + ηρ3) .\n\n(9)\n\nWe will then show our derivation of Equation 10\n\nΦ(x(t + 1)) − Φ(x(t))\n\n= −\n\n= −\n\n= −\n\n= −\n\nηρ2 2\nηρ2 2\nηρ2 2\nηρ2 2\n\n∂Φ(x(t))∂2(∇L)(x(t))(cid:2) ∇L(x(t))\n\n∥∇L(x(t))∥2\n\n,\n\n∇L(x(t)) ∥∇L(x(t))∥2\n\n(cid:3) + O(η2ρ2 + ηρ3)\n\n∂Φ(x(t))∂2(∇L)(x(t))(cid:2)v1(∇2L(x(t))), v1(∇2L(x(t)))(cid:3) + O(η2ρ2 + ηρ3)\n\n∂Φ(x(t))∇λ1(∇2L(x(t))) + O(η2ρ2 + ηρ3)\n\n∂Φ(Φ(x(t)))∇λ1(∇2L(Φ(x(t)))) + O(η2ρ2 + ηρ3),\n\n(10)\n\nwhere the second to last step we use the property of the derivative of eigenvalue (Lemma K.7) and the last step is due to Taylor expansion of ∂Φ(·)∇λ1(∇2L(·)) at Φ(x(t)) and the fact that ∥Φ(x(t)) − x(t)∥ = O(ηρ).\n\nWe will finally show our derivation of Equation 12.\n\nThe update of the gradient (Equation 12) can be viewed as an O(ηρ2)-perturbed version of the update of the iterate in the quadratic case. Note O(ηρ2) is a higher order term comparing to the other two terms, which are on the order of Θ(η2ρ) and Θ(ηρ) respectively. By controlling the error terms, the mechanism and analysis of the implicit alignment between Hessian and gradient still apply to the general case. We can also show that once this alignment happens, it will be kept until the end of our analysis, which is Θ(η−1ρ−2) steps.\n\n56\n\nPublished as a conference paper at ICLR 2023\n\nFinally, we derive Equation 12 by Taylor expansion. We first apply Taylor expansion (Equation 32) on the update rule of the iterate of SAM (Equation 3):\n\nx(t + 1) = x(t) − η∇L(x(t)) − ηρ∇2L(x(t))\n\n∇L(x(t)) ∥∇L(x(t))∥2\n\n+ O(ηρ2).\n\n(33)\n\nSince phase II happens in an O(ηρ)-neighborhood of manifold Γ, we have ∥x(t + 1) − x(t)∥2 = O(ηρ). Then by Equation 33 and Taylor expansion on ∇L(x(t + 1)) at x(t), we have that\n\n∇L(x(t + 1))=∇L(x(t))−∇2L(x(t))(cid:0)x(t + 1) − x(t)(cid:1) + O(η2ρ2)\n\n=∇L(x(t))−η∇2L(x(t))(cid:0)∇L(x(t)) + ρ∇2L(x(t))\n\n∇L(x(t)) ∥∇L(x(t)))∥2\n\n(cid:1) + O(ηρ2) .\n\n(34)\n\nJ ANALYSIS FOR 1-SAM (PROOF OF THEOREM 5.4)\n\nThe goal of this section is to prove the following theorem.\n\nTheorem 5.4. Let {x(t)} be the iterates of 1-SAM (Equation 13) and x(0) = xinit ∈ U , then under Setting 5.1, for almost every xinit, for all η and ρ such that (η + ρ) ln(1/ηρ) is sufficiently small, with probability at least 1 − O(ρ) over the randomness of the algorithm, the dynamics of 1-SAM (Equation 13) can be split into two phases:\n\n• Phase I (Theorem J.1): 1-SAM follows Gradient Flow with respect to L until entering an ̃O(ηρ) neighbor-\n\nhood of the manifold Γ in O(ln(1/ρη)/η) steps;\n\n• Phase II (Theorem J.2): 1-SAM tracks the solution of Equation 14, X, the Riemannian gradient flow with respect to Tr(∇2L(·)) in an ̃O(ηρ) neighborhood of manifold Γ. Quantitatively, the approximation error between the iterates x and the corresponding limiting flow X is ̃O(η1/2 + ρ), that is,\n\n∥x(⌈T3/(ηρ2)⌉) − X(T3)∥2 = ̃O(η1/2 + ρ).\n\nAs mentioned in our proof setups in Appendix E, we will prove Theorem 5.4 under a more general (and weaker) condition, namely Condition E.1 and Assumption 3.2. The only usage of Setting 5.1 in the proof is Theorems 5.2 and E.2, which are restated below.\n\nTheorem 5.2. Loss L, set Γ and integer M defined in Setting 5.1 satisfy Assumption 3.2.\n\nTheorem E.2. Setting 5.1 implies Condition E.1.\n\nCondition E.1. Total loss L = 1 k=1 Lk. For each k ∈ [M ], Lk is C4, and there exists a (D − 1)- dimensional C2-submanifold of RD, Γk, where for all x ∈ Γk, x is a global minimizer of Lk, Lk(x) = 0 and rank(∇2Lk(x)) = 1. Moreover, Γ = ∩M\n\nk=1Γk for Γ defined in Assumption 3.2.\n\nM\n\n(cid:80)M\n\nAnalogous to the full-batch setting, we will split the trajectory into two phases.\n\nTheorem J.1 (Phase I). Let {x(t)} be the iterates defined by SAM (Equation 13) and x(0) = xinit ∈ U , then under Assumption 3.2 and E.1, for almost every xinit, there exists a constant T1, it holds for sufficiently small (η + ρ) ln 1/ηρ, we have with probability 1 − O(ρ), there exists t ≤ T1 ln(1/ηρ)/η, such that ∥x(t) − Φ(x(t))∥2 = O(ηρ) and ∥Φ(xinit) − Φ(x(t))∥2 = ̃O(η1/2 + ρ).\n\nTheorem J.1 shows that SAM will converges to an ̃O(ηρ) neighborhood of the manifold without getting far away from Φ(x(0)), where we can perform a local analysis on the trajectory of Φ(x(t)).\n\nUnder Assumptions 3.2 and E.1, we have Tr(∇2Lk(x)) = λ1(∇2Lk(x)) is differentiable for x ∈ Γi. Hence Tr(∇2L(x)) = (cid:80)M k=1 Tr(∇2Lk(x)) is also differentiable and we have (14) is well defined for some finite time T2.\n\nTheorem J.2 (Phase II). Let {x(t)} be the iterates defined by SAM (Equation 13) under Assumptions 3.2 and E.1, assuming (1) ∥x(0) − Φ(x(0))∥2 = O(ηρ) and (2) ∥Φ(xinit) − Φ(x(0))∥2 = ̃O(η1/2 + ρ), then for almost every x(0), for any T2 > 0 till which solution of (14) X exists, for sufficiently small (η + ρ) ln 1/(ηρ), we have with probability 1 − O(ηρ), for all ηρ2t < T2, ∥Φ(x(t)) − X(ηρ2t)∥2 = ̃O(η1/2 + ρ) and ∥x(t) − Φ(x(t))∥2 = O(ηρ).\n\n57\n\nPublished as a conference paper at ICLR 2023\n\nCombining Theorems E.2, J.1 and J.2, the proof of Theorem 5.4 is clear and we deferred it to Appendix J.3.\n\nNow we recall our notations for stochastic setting with batch size one. Notations for Stochastic Setting: Since Lk is rank-1 on Γk for each k ∈ [M ], we can write it as Lk(x) = Λk(x)wk(x)w⊤ k (x) for any x ∈ Γk, where wk is a continuous function on Γk with pointwise unit norm. Given the loss function Lk, its gradient flow is denoted by mapping φk : RD × [0, ∞) → RD. Here, φk(x, τ ) denotes the iterate at time τ of a gradient flow starting at x and is defined as the unique solution of φk(x, τ ) = x − (cid:82) τ 0 ∇Lk(φk(x, t))dt, ∀x ∈ RD. We further define the limiting map Φk as Φk(x) = limτ →∞ φk(x, τ ), that is, Φk(x) denotes the convergent point of the gradient flow starting from x. Similar to Definition 3.3, we define Uk = {x ∈ RD|Φ(x) exists and Φk(x) ∈ Γk} be the attraction set of Γi. We have that each Uk is open and Φk is C on Uk by Lemma B.15 in Arora et al. (2022).\n\n2\n\nIn this section we will define K as {X(t) | t ∈ [0, T3]} where X is the solution of (14). We will denote h(K) in Lemma E.6 by h. Using Theorem D.3, we will assume the update is always well defined.\n\nJ.1 PHASE I (PROOF OF THEOREM J.1)\n\nProof of Theorem J.1. The proof consists of two steps.\n\n1. Tracking Gradient Flow. By Lemma J.3, with probability 1 − ρ2, there exists step tGF = O(1/η) such\n\nthat\n\n∥x(tGF) − Φ(x(tGF))∥2 ≤ h/4. ∥Φ(x(tGF)) − Φ(xinit)∥2 = ̃O(η1/2 + ρ).\n\n2. Decreasing Loss. By Lemma J.7, with probability 1 − O(ρ),\n\nthere exists step tDEC = tGF +\n\nO(ln(1/ρ)/η) = O(ln(1/ρ)/η) such that\n\n∥∇L(x(tDEC))∥2 = O(ρ).\n\n∥Φ(x(tDEC)) − Φ(xinit)∥2 ≤ ∥Φ(x(tDEC)) − Φ(x(tGF))∥2 + ∥Φ(x(tGF)) − Φ(xinit)∥2\n\n= ̃O(η1/2 + ρ).\n\nThen by Lemma J.12, with probability 1 − O(ρ), there exists step tDEC2 = tDEC + O(ln(1/ηρ)/η) = O(ln(1/ηρ)/η), it holds that\n\n∥x(tDEC2) − Φ(x(tDEC2))∥2 = O(ηρ).\n\n∥Φ(x(tDEC2)) − Φ(xinit)∥2 ≤ ∥Φ(x(tDEC2)) − Φ(x(tDEC))∥2 + ∥Φ(x(tDEC)) − Φ(xinit)∥2\n\n= ̃O(η1/2 + ρ).\n\nConcluding, let T1 be the constant satisfying tDEC2 ≤ T1 ln(1/ηρ)/η, then we have for t = tDEC2 ≤ T1 ln(1/ηρ)/η such that\n\n∥x(t) − Φ(x(t))∥2 = O(ηρ).\n\n∥Φ(x(t)) − Φ(xinit)∥2 = ̃O(η1/2 + ρ).\n\nJ.1.1 TRACKING GRADIENT FLOW\n\nLemma J.3 shows that the iterates x(t) tracks gradient flow to an O(1) neighbor of Γ. Lemma J.3. Under condition of Theorem J.1, with probability 1 − O(ρ2), there exists tGF = O(1/η), such that the iterate x(tGF) is O(1) close to the manifold Γ and Φ(x(tGF)) is ̃O(η1/2 + ρ) is close to Φ(xinit). Quantitatively,\n\nL(x(tGF)) ≤\n\nμh2 32\n\n∥x(tGF) − Φ(x(tGF))∥ ≤ h/4 , ∥Φ(x(tGF)) − Φ(xinit)∥ = ̃O(η1/2 + ρ) .\n\nProof of Lemma J.3. Choose C = 1\n\n4\n\n(cid:113) μ ζ .\n\n58\n\nPublished as a conference paper at ICLR 2023\n\nThere exists T > 0, such that\n\nConsider\n\n∥φ(xinit, T ) − Φ(xinit)∥2 ≤ Ch/2 .\n\nx(t + 1) = x(t) − η∇Lk(x(t) + ρ\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n)\n\n= x(t) − η∇Lk(x(t)) + O(ηρ) .\n\nBy Theorem L.1, let b(x) = −∇L(x),p = η and ε = O(ρ), for sufficiently small η and ρ, the iterates x(t) tracks gradient flow φ(xinit, T ) in O(1/η) steps in expectation, Quantitatively, with probability 1 − ρ2, for tGF = ⌈ T0\n\nη ⌉, we have that\n\n∥x(tGF) − φ(xinit, T0)∥2 = ̃O(\n\n√\n\np + ε) ≤ ̃O(η1/2 + ρ) .\n\nThis implies x(tGF) ∈ K h, hence by Taylor Expansion on Φ,\n\n∥Φ(x(tGF)) − Φ(xinit)∥2 = ∥Φ(x(tGF)) − Φ(φ(xinit, T ))∥2\n\n≤ O(∥x(tGF) − φ(xinit, T )∥2) ≤ ̃O(η1/2 + ρ) .\n\nThis implies\n\n∥x(tGF) − Φ(x(tGF))∥2 ≤∥x(tGF) − φ(xinit, T0)∥2 + ∥φ(xinit, T0) − Φ(xinit)∥2\n\n+ ∥Φ(xinit) − Φ(x(tGF))∥2\n\n≤Ch/2 + ̃O(η1/2 + ρ) ≤ Ch ≤ h/4 .\n\nBy Taylor Expansion,\n\nL(x(tGF)) ≤ ζ∥x(tGF) − Φ(x(tGF))∥2\n\n2/2 ≤\n\nμh2 32\n\n.\n\nJ.1.2 DECREASING LOSS\n\nLemma J.4. Under condition of Theorem J.1, assuming x(t0) ∈ K h/4 and for any t satisfying t0 ≤ t ≤ t0 + O(ln(1/ηρ)/η),\n\nL(x(τ )) ≤ μh2\n\n16 , it holds that\n\nmax t0≤τ ≤t0+O(ln(1/ηρ)/η)\n\nx(τ ) ∈ K h, ∀t0 ≤ τ ≤ t.\n\nMoreover, we have that\n\n∥Φ(x(t)) − Φ(x(t0))∥ = O((η + ρ) ln(1/ηρ)).\n\nProof of Lemma J.4. We will prove by induction. For τ = t0, the result holds trivially. Suppose the result holds for t − 1, then for any τ satisfying t0 ≤ τ ≤ t − 1, by Lemmas F.1 and F.8,\n\n∥Φ(x(τ + 1)) − Φ(x(τ ))∥ ≤ ξηρ∥∇L (x(τ )) ∥2 + νηρ2 + ξη2∥∇L (x(τ )) ∥2\n\n2 + ξζ 2η2ρ2\n\n= O(η2 + ηρ) .\n\nAlso by Lemma F.1, ∥x(t) − Φ(x(t))∥2 ≤ h/2\n\n√\n\n2, this implies,\n\ndist(K, x(t)) ≤dist(K, x(t0)) + ∥x(t0) − Φ(x(t0))∥2\n\n+ ∥Φ(x(t0)) − Φ(x(t))∥ + ∥Φ(x(t)) − x(t)∥\n\n≤0.99h + O(η2(t − tGF)) = 0.99h + O(η ln(1/ηρ)) ≤ h .\n\n59\n\nPublished as a conference paper at ICLR 2023\n\nLemma J.5. Under condition of Theorem J.1, if x(τ ) ∈ K h, then we have that\n\nE[L(x(τ + 1))|x(τ )] ≤ L(x(τ )) −\n\nημ 2\n\nL(x(τ )) .\n\nMoreover it holds that,\n\nE[ln L(x(τ + 1))|x(τ )] ≤ ln E[L(x(τ + 1))|x(τ )] ≤ ln L(x(τ )) −\n\nημ 2\n\n.\n\nProof of Lemma J.5. By Lemma F.8 and Taylor Expansion,\n\nE[L(x(τ + 1))|x(τ )]\n\n=E\n\n≤E\n\n(cid:20)\n\n(cid:18)\n\nL\n\nx(τ ) − η∇Lk[x(τ ) + ρ\n\n∇Lk (x(τ )) ∥∇Lk (x(τ )) ∥\n\n(cid:19) ]\n\n(cid:21)\n\n|x(τ )\n\n(cid:20)\n\n(cid:28)\n\nL(x(τ )) − η\n\n∇L (x(τ )) , ∇Lk\n\n(cid:16)\n\nx(τ ) + ρ\n\n∇Lk (x(τ )) ∥∇Lk (x(τ )) ∥\n\n(cid:17)(cid:29)(cid:21)\n\n(cid:21)\n\n∇Lk (x(τ )) ∥∇Lk (x(τ )) ∥ 2 + ηρζ∥∇L (x(τ )) ∥2 + ζη2E[∥∇Lk(x(τ ))∥2\n\n]∥2\n\n2\n\n2] + ζ 3η2ρ2\n\n+ E\n\n(cid:20) ζη2 2\n\n∥∇Lk[x(τ ) + ρ\n\n≤L(x(τ )) −\n\n≤L(x(τ )) − η∥∇L (x(τ )) ∥2 η\n2 ημ 2\n\n∥∇L (x(τ )) ∥2\n\n≤L(x(τ )) −\n\nL(x(τ )) .\n\n2\n\nLemma J.6. Under condition of Theorem J.1, assuming x(t0) ∈ K h/4 and L(x(t0)) ≤ μh2 32 , then with probability 1 − O(ρ), for any t satisfying t0 ≤ t ≤ t0 + O(ln(1/ηρ)/η), it holds that x(t) ∈ K h. Moreover, we have that\n\n∥Φ(x(t)) − Φ(x(t0))∥ = O((η + ρ) ln(1/ηρ)).\n\nProof of Lemma J.6. By Uniform Bound and Lemma J.4,\n\nP(∃t0 ≤ t ≤ t0 + O(ln(1/ηρ)/η), L(x(t)) ≥\n\nμh2 16\n\n)\n\nt0+O(ln(1/ηρ)/η) (cid:88)\n\nt=t0\n\nt0+O(ln(1/ηρ)/η) (cid:88)\n\n≤\n\n≤\n\nt=t0\n\nP(L(x(t)) ≥\n\nP(L(x(t)) ≥\n\nμh2 16\n\nμh2 16\n\nand L(x(τ )) ≤\n\nμh2 16\n\n, ∀t0 ≤ τ ≤ t − 1)\n\nand x(τ ) ∈ K h, ∀t0 ≤ τ ≤ t − 1)\n\nConsider each term, and applying uniform bound again,\n\nP (L(x(t)) ≥\n\nP (L(x(t)) ≥\n\nμh2 16 μh2 16\n\n≤\n\nt (cid:88)\n\nτ =t0\n\nand x(τ ) ∈ K h, ∀t0 ≤ τ ≤ t − 1)\n\nand L(x(τ )) ≤\n\nμh2 32\n\nand ∀t − 1 ≥ τ ′ ≥ τ + 1,\n\nμh2 16\n\n> L(x(τ ′)) >\n\nand ∀t − 1 ≥ τ ′′ ≥ τ, x(τ ′′) ∈ K h) .\n\nThen if we consider each term, we have that it is bounded by\n\nP (L(x(t)) ≥\n\nμh2 16\n\nand ∀t − 1 ≥ τ ′ ≥ τ + 1, L(x(τ ′)) >\n\nμh2 32\n\nμh2 32\n\nand ∀t − 1 ≥ τ ′′ ≥ τ, x(τ ′′) ∈ K h | L(x(τ )) ≤\n\nμh2 32\n\n) .\n\n60\n\nPublished as a conference paper at ICLR 2023\n\nDefine a coupled process ̃L(τ + 1) = ln L(x(τ + 1)) and\n\n(cid:40)\n\n ̃L(τ ′) =\n\nln L(x(τ ′)), ̃L(τ ′ − 1) − ημ/2,\n\nif ̃L(τ ′ − 1) = ln L(x(τ ′ − 1)) ≥ ln( μh2 if otherwise.\n\n32 ),\n\nThen clearly\n\nP (L(x(t)) ≥\n\nμh2 16\n\nand ∀t ≥ τ ′ ≥ τ + 1, L(x(τ ′)) >\n\nμh2 32\n\nand ∀t ≥ τ ′′ ≥ τ, x(τ ′′) ∈ K h | L(x(τ )) ≤\n\nμh2 32\n\n)\n\n≤ P ( ̃L(t) ≥ ln(\n\nμh2 16\n\n)) .\n\nConsider a fixed τ ′ satisfying τ + 1 ≤ τ ′ ≤ t. By Lemma J.5, we have that\n\n ̃L(x(τ ′ + 1)) − ̃L(x(τ ′)) ≤ −ημ/2.\n\nHence ̃L(t) + ημt/2 is a super martingale.\n\nFurther it holds that if L(x(τ ′ − 1)) ≥ ( μh2\n\n32 ), then\n\nL(x(τ ′ − 1)) − L(x(τ ′)) = O(∥x(τ ′ − 1) − x(τ ′)∥) = O(η) .\n\nUsing the smoothness at log(x) at μh2\n\n32 which is a positive constant, ∥ ̃L(τ ′ + 1) − ̃L(τ ′)∥ ≤ O(η) ≤ Cη .\n\nHere C is a constant independent of η. This implies ̃L(x(τ + 1)) ≤ μh2\n\n√\n\n16\n\n2\n\nNow by Azuma-Hoeffding bound (Lemma K.4), we have that\n\nP ( ̃L(t) − ̃L(τ + 1) + (t − τ − 1)ημ/2 > a) ≤ 2 exp(−\n\na2 8(t − τ − 1)(C + μ)2η2 ).\n\nWith a = ln(\n\nμh2 16 ̃L(τ +1)\n\n) + (t − τ − 1)ημ/2 ≥ (ln 2 + (t − τ − 1)ημ)/2, we have that\n\nHence we have\n\nP ( ̃L(t) > ln(\n\nμh2 16\n\n)) ≤ 2 exp(−\n\n(ln 2 + (t − τ − 1)ημ)2 32(C + μ)2η2\n\n)\n\n≤ 2 exp(−\n\nln 2(t − τ − 1)μ 8(C + μ)2η\n\n)\n\nP(∃t0 ≤ t ≤ t0 + O(ln(1/ηρ)/η), L(x(t)) ≥\n\nμh2 16\n\n)\n\n≤O(2 exp(−\n\nln 2(t − τ − 1)μ 8(C + μ)2η\n\n) ln2(1/ηρ)/η2) ≤ ρ.\n\nHence with probability 1 − ρ, L(x(t)) ≤ μh2 we have completed our proof.\n\n16 , ∀t0 ≤ t ≤ t0 + O(ln(1/ηρ)/η), combining with Lemma J.4,\n\nLemma J.7. Under condition of Theorem J.1, assuming there exists tGF such that L(x(tGF)) ≤ μh2 32 and x(tGF) ∈ K h/4, then with probability 1−O(ρ), there exists tDEC = tGF +O(ln(1/ρ)/η), such that x(tDEC) is in O(ρ) neighbor of Γ, quantitatively, we have that\n\nMoreover the movement of the projection of Φ(x(·)) on the manifold is bounded,\n\n∥Φ(x(tGF)) − Φ(x(tDEC))∥2 = O((η + ρ) ln(1/ρ)) .\n\n∥∇L(x(tDEC))∥2 ≤ 4ζρ .\n\n61\n\nPublished as a conference paper at ICLR 2023\n\nProof of Lemma J.7. For simplicity of writing, define T1 ≜ ⌈\n\n2 ln\n\nh2 256ρ3μ ημ\n\n⌉ = O(ln(1/ρ)/η).\n\nBy Lemma J.6, we may assume x(t) ∈ K h for tGF ≤ t ≤ T1 + tGF.\n\nDefine indicator function as\n\nBy Lemma J.5, we have that,\n\nA(t) = 1[∇L (x(τ )) ≥ 4ζρ, ∀t ≥ τ ≥ tGF] .\n\nE[L(x(t + 1))A(t + 1)] ≤ E[L(x(t + 1))A(t)] ≤ (1 −\n\nημ 2\n\n)E[L(x(t))A(t)].\n\nWe can then conclude that with T2 = T1 + tGF, using Lemma F.2,\n\n8μρ2EA(T2 + 1) ≤ E[L(x(T2 + 1))A(T2 + 1)] ≤ (1 −\n\nημ 2\n\n)T1L(x(tGF)) ≤ 8μρ3.\n\nWe have\n\nEA(T2 + 1) ≤ ρ.\n\nThis implies A(T2 + 1) = 0 with probability 1 − O(ρ), which indicates the existence of tDEC. The second claim is a direct application of Lemma J.6.\n\nLemma J.8 (A general version of Lemma 5.5). Under Assumption 3.2 and Condition E.1, for x ∈ K h and p ∈ C, ∇2Lk(p) = Λk(p)wk(p)wk(p)⊤, there exists s ∈ {1, −1},\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n= swk(p) + O(∥x − p∥2) .\n\nFurther if |w⊤\n\nk (x − p)| ≥ ∥x − p∥3/2\n\n2\n\n, then s = sign(w⊤\n\nk (x − p)). This implies\n\n⊤\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n(x − p) ≥ sw⊤\n\nk (x − p) − O(∥x − p∥2 2)\n\n≥ ∥w⊤\n\nk (x − p)∥2 − O(∥x − p∥3/2\n\n2\n\n) .\n\nProof of Lemma J.8. We will calculate the direction of ∇Lk(x) compare them to get our result.\n\n∥∇Lk(x)∥ using two different approximations and\n\n1. According to Lemma F.4,\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n=\n\n∇2Lk(Φk(x))(x − Φk(x)) ∥∇2Lk(Φk(x))(x − Φk(x))∥2\n\n+ O(∥x − Φk(x)∥2).\n\nSuppose ∇2Lk(Φk(x)) = Λk(Φk(x))wk(Φk(x))wk(Φk(x))⊤, then\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n= wk(Φk(x)) + O(∥x − Φk(x)∥2)\n\nAs ∇2Lk(p) = Λk(p)wk(p)wk(p)⊤, using Davis-Kahan Theorem K.3, we would have ∃s ∈ {−1, 1}, such that ∥wk(Φk(x)) − swk(p)∥2 ≤ ζ∥Φk(x) − p∥2.\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n= swk(p) + O(∥Φk(x) − p∥2 + ∥x − p∥2).\n\nAccording to Lemma F.1, we have ∥x − Φk(x)∥2 ≤ ∥∇Lk(x)∥2\n\nμ\n\n≤ ζ∥x−p∥2\n\nμ\n\n. This implies,\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n= swk(p) + O(∥x − p∥2).\n\n(35)\n\nEquation 35 is our first statement.\n\n62\n\nPublished as a conference paper at ICLR 2023\n\n2. By Taylor expansion at p,\n\nThat being said, when |w⊤\n\n∇Lk (x) = Λk(x)wk(p)wk(p)⊤(x − p) + O(ν∥x − p∥2 k (x − p)| ≥ ∥x − p∥3/2\n\n, we have\n\n2\n\n2).\n\n∥∇Lk (x) − Λkwkw⊤\n\n∥∇Lk (x) ∥ ≥ ∥Λkwkw⊤\n\nk (x − p)∥2 ≤ O(∥x − p∥2\n\n2) . k (x − p)∥2 − O(∥x − p∥2\n\n2) ≥ Ω(∥x − p∥3/2\n\n2\n\n).\n\nConcluding,\n\nHence we have\n\n∥\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n−\n\nΛkwkw⊤ ∥Λkwkw⊤\n\nk (x − p) k (x − p)∥\n\n∥2 ≤ O(∥x − p∥1/2\n\n2\n\n)\n\n∇Lk (x) ∥∇Lk (x) ∥\n\n= sign(w⊤\n\nk (x − p))wk + O(∥x − p∥1/2\n\n2\n\n) .\n\n(36)\n\nComparing (35) and (36), we have s = sign(wk(p)⊤(x − p)) when |w⊤\n\nk (x − p)| ≥ ∥x − p∥3/2\n\n2\n\n.\n\nLemma J.9. Under condition of Theorem J.1, for any constant C > 0 independent of η, ρ, there exists constant C1 > C2 > 0 independent of η, ρ, if x(t) ∈ K h and C1ηρ ≤ ∥x(t) − Φ(x(t))∥ ≤ Cρ, then we have that\n\nEk[∥x(t + 1) − Φ(x(t + 1))∥2 | x(t)] ≤ ∥x(t) − Φ(x(t))∥2 − C2ηρ .\n\nProof of Lemma J.9. By Lemma F.2, ∥x(t) − Φ(x(t))∥ = O(ρ). Hence we have that by Taylor Expansion,\n\nx(t + 1) = x(t) − η∇Lk\n\nx(t) + ρ\n\n(cid:18)\n\n(cid:19)\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n= x(t) − η∇Lk (x(t)) − ηρ∇2Lk(x(t))\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n+ O(ηρ2)\n\n= x(t) − η∇Lk (x(t)) − ηρΛkwkw⊤\n\nk\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n+ O(ηρ2) .\n\nHere Λk, wk indicates Λk(Φ(x(t))), wk(Φ(x(t))).\n\nNotice that given ∥x(t) − Φ(x(t))∥ = O(ρ), by Lemma F.8, we have that ∥Φ(x(t + 1)) − Φ(x(t))∥2 = O(ηρ2), ∥x(t + 1) − x(t)∥2 = O(ηρ).\n\nThis implies x(t + 1) ∈ K r.\n\nFurther by Taylor Expansion, ∇Lk(x(t)) = Λkwkw⊤\n\nk (x(t) − Φ(x(t))) + O(ρ2).\n\nBy Lemma J.8, we have for some sk(t) ∈ {−1, 1}.\n\nw⊤\n\nk\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n= sk(t)wk + O(∥x(t) − Φ(x(t))∥2) .\n\nWe also have\n\nsk(t) ̸= sign(w⊤\n\nk (x(t) − Φ(x(t)))) ⇒ ∥w⊤\n\nk (x(t) − Φ(x(t)))∥2 ≤ ∥x(t) − Φ(x(t))∥3/2\n\n2\n\n.\n\n(37)\n\nConcluding,\n\nx(t + 1) − Φ(x(t + 1))\n\n=(x(t) − Φ(x(t))) − ηΛkwkw⊤\n\nk (x(t) − Φ(x(t))) − ηρΛksk(t)wkw⊤\n\nk wk + O(ηρ2).\n\nAfter we take square and expectation,\n\nE[∥x(t + 1) − Φ(x(t + 1))∥2\n\n2 | x(t)]\n\n≤∥x(t) − Φ(x(t))∥2\n\n2 +\n\n2η2 M\n\nM (cid:88)\n\nk=1\n\nΛ2\n\nk|w⊤\n\nk (x(t) − Φ(x(t)))|2 +\n\n2η2ρ2 M\n\nM (cid:88)\n\nk=1\n\nΛ2\n\nk\n\n− 2\n\nη M\n\nM (cid:88)\n\nk=1\n\nΛk|w⊤\n\nk (x(t) − Φ(x(t)))|2 − 2\n\nηρ M\n\nM (cid:88)\n\nk=1\n\nΛksk(t)w⊤\n\nk (x(t) − Φ(x(t)))\n\n+ O(ηρ2∥x(t) − Φ(x(t))∥ + η2ρ3) .\n\n63\n\nPublished as a conference paper at ICLR 2023\n\nWe will then carefully examine each positive term,\n\n2η2 M\n\nM (cid:88)\n\nk=1\n\nThis implies,\n\nΛ2\n\nk|w⊤\n\nk (x(t) − Φ(x(t)))|2 = 2M η2(x(t) − Φ(x(t)))⊤∇2L(x(t))2(x(t) − Φ(x(t)))\n\n≤ 2M ζη2∥x(t) − Φ(x(t))∥2 = O(η2ρ2) .\n\n2η2ρ2 M\n\nM (cid:88)\n\nk=1\n\nΛ2\n\nk ≤ 2ζ 2η2ρ2 = O(η2ρ2) .\n\nE[∥x(t + 1) − Φ(x(t + 1))∥2\n\n2 | x(t)]\n\n≤∥x(t) − Φ(x(t))∥2\n\n2 − 2\n\nηρ M\n\nM (cid:88)\n\nk=1\n\nΛksk(t)w⊤\n\nk (x(t) − Φ(x(t)))\n\n+ O(ηρ2∥x(t) − Φ(x(t))∥ + η2ρ2) .\n\nWe will now lower bound (cid:80)M\n\nk=1 Λksk(t)w⊤\n\nk (x(t) − Φ(x(t))). By Equation 37,\n\nM (cid:88)\n\nk=1\n\nΛksk(t)w⊤\n\nk (x(t) − Φ(x(t))) ≥\n\n≥\n\nM (cid:88)\n\nk=1\n\nM (cid:88)\n\nk=1\n\nΛk∥w⊤\n\nk (x(t) − Φ(x(t)))∥2 − 2\n\nM (cid:88)\n\nk=1\n\nΛk∥x(t) − Φ(x(t))∥3/2\n\n2\n\nΛk∥w⊤\n\nk (x(t) − Φ(x(t)))∥2 − O(∥x(t) − Φ(x(t))∥3/2\n\n2\n\n) .\n\nFor (cid:80)M\n\nk=1 Λk∥w⊤\n\nk (x(t) − Φ(x(t)))∥2, by Lemma Lemma F.4,\n\nΛk∥w⊤\n\nk (x(t) − Φ(x(t)))∥2 ≥\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nM (cid:88)\n\nΛ2\n\nk∥w⊤\n\nk (x(t) − Φ(x(t)))∥2\n\n2\n\nM (cid:88)\n\nk=1\n\nk=1\n\n(cid:113)\n\n=\n\n(x(t) − Φ(x(t)))⊤∇2L(Φ(x(t)))2(x(t) − Φ(x(t)))\n\n= ∥∇2L(Φ(x(t)))(x(t) − Φ(x(t)))∥2 ≥ μ∥∂Φ(Φ(x(t)))(x(t) − Φ(x(t)))∥2 ≥ μ∥x(t) − Φ(x(t))∥2 − O(∥x(t) − Φ(x(t))∥2\n\n2) .\n\nConcluding, we have that\n\nM (cid:88)\n\nk=1\n\nSo\n\nΛksk(t)w⊤\n\nk (x(t) − Φ(x(t))) ≥ μ∥x(t) − Φ(x(t))∥2/2 .\n\nE[∥x(t + 1) − Φ(x(t + 1))∥2\n\n2 | x(t)]\n\n≤∥x(t) − Φ(x(t))∥2\n\n2 −\n\nμηρ M\n\n∥x(t) − Φ(x(t))∥2\n\n+ O(ηρ2∥x(t) − Φ(x(t))∥ + η2ρ2)\n\n≤(∥x(t) − Φ(x(t))∥2 − C2ηρ)2 .\n\nThe inequality holds if ∥x(t) − Φ(x(t))∥2 > C1ηρ.\n\nFinally by Jenson’s Inequality,\n\nE[∥x(t + 1) − Φ(x(t + 1))∥2|x(t)] ≤ ∥x(t) − Φ(x(t))∥2 − C2ηρ.\n\nLemma J.10. Under condition of Theorem J.1, for any constant C > 0 independent of η, ρ, there exists constant C3 > 0 independent of η, ρ, if x(t) ∈ K h and ∥x(t) − Φ(x(t))∥ ≤ Cρ, then we have that\n\n|∥x(t + 1) − Φ(x(t + 1))∥2 − ∥x(t) − Φ(x(t))∥2| ≤ C3ηρ .\n\n64\n\nPublished as a conference paper at ICLR 2023\n\nProof of Lemma J.10. This is a direct application of Lemma F.8.\n\nLemma J.11. Under condition of Theorem J.1, assuming x(t0) ∈ K h/2 and ∥x(t0) − Φ(x(t0))∥ ≤ f (η, ρ) for some fixed function f and f (η, ρ) ∈ Ω(ηρ ln2(1/ηρ)) ∩ O(ρ), then with probability 1 − O(ρ), for any t satisfying t0 ≤ t ≤ t0 + O(ln(1/ηρ)/η), it holds that ∥x(t) − Φ(x(t)∥ ≤ 2f (η, ρ). Moreover, we have that ∥Φ(x(t)) − Φ(x(t0))∥ = O((η + ρ) ln(1/ηρ)).\n\nProof of Lemma J.11. By Lemma J.6, we have that x(t) ∈ K h for any t satisfying that t0 ≤ t ≤ t0 + O(ln(1/ηρ)/η) and with probability 1 − O(ρ) we will suppose this hold for the following deduction.\n\nBy Uniform Bound,\n\nP(∃t0 ≤ t ≤ t0 + O(ln(1/ηρ)/η), ∥x(t) − Φ(x(t)∥ ≥ 2f (η, ρ))\n\nt0+O( ln(1/ηρ) (cid:88)\n\nη\n\n)\n\n≤\n\nt=t0\n\nP(∥x(t) − Φ(x(t))∥ ≥ 2f (η, ρ))\n\nand ∥x(τ ) − Φ(x(τ ))∥ ≤ 2f (η, ρ), ∀t0 ≤ τ ≤ t − 1).\n\nConsider each term and apply Uniform bound again, P(∥x(t) − Φ(x(t))∥ ≥ 2f (η, ρ))\n\nand ∥x(τ ) − Φ(x(τ ))∥ ≤ 2f (η, ρ), ∀t0 ≤ τ ≤ t − 1)\n\n≤\n\nt (cid:88)\n\nτ =t0\n\nP(∥x(t) − Φ(x(t))∥ ≥ 2f (η, ρ))\n\nand ∥x(τ ) − Φ(x(τ ))∥ ≤ f (η, ρ),\n\nand f (η, ρ) ≤ ∥x(τ ′) − Φ(x(τ ′))∥ ≤ 2f (η, ρ), ∀τ + 1 ≤ τ ′ ≤ t − 1).\n\nThen if we consider each term, it is bounded by\n\nP(∥x(t) − Φ(x(t))∥ ≥ 2f (η, ρ))\n\nand f (η, ρ) ≤ ∥x(τ ′) − Φ(x(τ ′))∥ ≤ 2f (η, ρ), ∀τ + 1 ≤ τ ′ ≤ t − 1\n\n| ∥x(τ ) − Φ(x(τ ))∥ ≤ f (η, ρ)).\n\n(38)\n\nNow let C be the positive constant satisfying 2f (η, ρ) ≤ Cρ, suppose C1, C2 are the constants corresponds to C in Lemma J.9 and C3 is the constant correspond to C in Lemma J.10. By definition C3 > C2.\n\nDefine a coupled process ̃y(τ + 1) = y(τ + 1) and\n\n ̃y(τ ′) =\n\n(cid:26)∥x(τ ′) − Φ(x(τ ′))∥2, ̃y(τ ′ − 1) − C2ηρ,\n\nif ̃y(τ ′ − 1) = ∥x(τ ′ − 1) − Φ(x(τ ′ − 1))∥2 > f (η, ρ) if otherwise.\n\nNow clearly Equation 38 is bounded by P( ̃y(t) ≥ 2f (η, ρ)). As E[ ̃y(τ ′)] ≤ ̃y(τ ′ − 1) − C2ηρ by Lemma J.9 and ∥ ̃y(τ ′) − ̃y(τ ′ − 1)∥ ≤ C3ηρ by Lemma J.10. This implies ∥ ̃y(τ ′)∥ − C2ηρτ ′ is a super martingale. By Azuma-Hoeffding bound(Lemma K.4), we have\n\nP ( ̃y(t) ≥ ̃y(τ + 1) − C2ηρ(t − τ − 1) + h) ≤ 2 exp(−\n\nh2 4(t − τ − 1)(C3 + C2)2η2ρ2 ).\n\nChoosing h = C2ηρ(t − τ − 1) − ∥x(τ + 1) − Φ(x(τ + 1))∥ + 2f (η, ρ)\n\nP ( ̃y(t + 1) ≥ 2f (η, ρ))\n\n≤2 exp(−\n\n(C2ηρ(t − τ ) − ∥x(τ + 1) − Φ(x(τ + 1))∥ + 2f (η, ρ))2 8(t − τ )(C3 + C2)2η2ρ2\n\n)\n\n≤2 exp(−\n\n(C2ηρ(t − τ ) + f (η, ρ)/2)2 4(t − τ )(C3 + C2)2η2ρ2\n\n)\n\n≤2 exp(−\n\nC2f (η, ρ) 2(C3 + C2)2ηρ\n\n) ≤ η10ρ10.\n\nWe then have\n\nP(∃t0 ≤ t ≤ t0 + O(ln(1/ηρ)/η), ∥x(t) − Φ(x(t)∥ ≥ 2f (η, ρ)) ≤ ρ.\n\n65\n\nPublished as a conference paper at ICLR 2023\n\nLemma J.12. Under condition of Theorem J.1, assuming there exists tDEC such that x(tDEC) ∈ K h/2 and ∥∇L(x(tDEC))∥ ≤ 4ζρ, then with probability 1 − O(ρ), there exists tDEC2 = tDEC + O(ln(1/ηρ)/η), such that ∥x(tDEC2) − Φ(tDEC2)∥ ≤ O(ηρ).\n\nFurthermore, for any t satisfying tDEC2 ≤ t ≤ tDEC2 + Θ(ln(1/ηρ)/η), we have that ∥Φ(x(t)) − Φ(x(tDEC))∥ = O(ρ2 ln(1/ηρ)).\n\nProof of Lemma J.12. We have that x(t) ∈ K h (Lemma J.6) and ∥x(t) − Φ(x(t))∥ ≤ Cρ for some constant C (Lemma J.11) for any t satisfying that tDEC ≤ t ≤ tDEC + O(ln(1/ηρ)/η) with probability 1 − O(ρ) and we will suppose this holds for the following deduction. The second statement then follows directly from Lemma F.8. Let C1, C2 be the constant in Lemma J.9 corresponding to C, For simplicity of writing, define T1 ≜\n\nC ln( C\n\nC1ηρ2 ) C2η\n\n⌈\n\n⌉ = O(ln(1/ηρ)/η). Define indicator function as\n\nA(t) = 1[∥x(t) − Φ(x(t))∥ ≥ C1ηρ, ∀t ≥ τ ≥ tGF] .\n\nBy Lemma J.9, we have that,\n\nE[∥x(t + 1) − Φ(x(t + 1))∥A(t + 1)] ≤ E[∥x(t + 1) − Φ(x(t + 1))∥A(t)]\n\n≤ E[∥x(t) − Φ(x(t))∥A(t)] − C2ηρE[A(t)]\n\n≤ E[∥x(t) − Φ(x(t))∥A(t)](1 −\n\nC2η C\n\n)\n\nWe can then conclude that with T2 = T1 + tDEC, using Lemma F.2,\n\nC1ηρEA(T2 + 1) ≤ E[∥x(T2 + 1) − Φ(x(T2 + 1))∥2A(T2 + 1)]\n\nC2η C\nThis implies A(T2 + 1) = 0 with probability 1 − O(ρ), which indicates the existence of tDEC2.\n\n)T1∥x(tDEC) − Φ(x(tDEC))∥ ≤ C1ηρ3.\n\n≤ (1 −\n\nJ.2 PHASE II (PROOF OF THEOREM J.2)\n\nProof of Theorem J.2. We will inductively prove the following induction hypothesis P(t) holds with probability 1 − O(η3ρ3t) for t ≤ T3/ηρ2 + 1,\n\nx(τ ) ∈ K h/2, τ ≤ t ∥x(τ ) − Φ(x(τ ))∥2 ≤ 2∥x(0) − Φ(x(0))∥2 = O(ηρ), τ ≤ t ∥Φ(x(τ )) − X(ηρ2τ )∥ = ̃O(η1/2 + ρ), τ ≤ t\n\nP(0) holds trivially. Now suppose P(t) holds, then x(t + 1) ∈ K h. By Lemma J.13, we have that with probability 1 − O(η3ρ3), ∥x(t + 1) − Φ(x(t + 1))∥ ≤ 2∥x(0) − Φ(x(0))∥2 = O(ηρ).\n\nNow we have\n\n2∥x(0) − Φ(x(0))∥2 = O(ηρ), τ ≤ t + 1. x(τ ) ∈ K h, τ ≤ t + 1\n\nBy Lemma J.14, it holds that\n\n∥Φ(x(τ + 1)) − Φ(x(τ )) + ηρ2P ⊥\n\nΦ(x(τ )),Γ∇λ1\n\n(cid:16)\n\n∇2Lkτ\n\n(cid:0)Φ(x(τ ))(cid:1)(cid:17)\n\n/2∥ ≤ ̃O(ηρ3 + η2ρ2) .\n\nAs\n\nEktP ⊥\n\nΦ(x(t)),Γ∇λ1\n\n(cid:16)\n\n∇2Lkt\n\n(cid:0)Φ(x(t))(cid:1)(cid:17)\n\n= P ⊥\n\nΦ(x(t)),Γ∇Tr(∇2L(Φ(x(t)))).\n\nBy Theorem L.1, let b(x) = −∂Φ(x)∇Tr(∇2L(x)), bk(x) = −∂Φ(x)Tr(∇2Lkt(x)), p = ηρ2 and ε = O(η + ρ), it holds that, with probability 1 − O(η3ρ3), ∥Φ(x(τ )) − X(ηρ2τ )∥\n\n=O(∥Φ(x(0)) − Φ(xinit)∥ + T3ηρ2 + (cid:112)ηρ2T3 log(2eT3/(η2ρ4)) + (ρ + η)T3) = ̃O(η1/2 + ρ), τ ≤ t + 1\n\nThis implies ∥x(t + 1) − X(ηρ2(t + 1))∥2 ≤ ∥x(t + 1) − Φ(x(t + 1))∥2 + ∥Φ(x(t + 1)) − X(ηρ2(t + 1))∥2 = ̃O(η1/2 + ρ) < h/2. Hence x(t + 1) ∈ K h/2. Combining with P(t) holds with probability 1 − O(η3ρ3t), we have that P(t + 1) holds with probability 1 − O(η3ρ3(t + 1)). The induction is complete.\n\nNow P(⌈T3/ηρ2⌉) is equivalent to our theorem.\n\n66\n\nPublished as a conference paper at ICLR 2023\n\nJ.2.1 CONVERGENCE NEAR MANIFOLD\n\nLemma J.13. Under condition of Theorem J.2, assuming x(t) ∈ K h, ∀t0 ≤ t ≤ t0 + O(1/ηρ2) and ∥x(t0) − Φ(x(t0))∥ ≤ f (η, ρ) for some fixed function f and f (η, ρ) ∈ Ω(ηρ ln2(1/ηρ)) ∩ O(ρ), then with probability 1 − O(η3ρ3), for any t satisfying t0 ≤ t ≤ t0 + O(1/ηρ2), it holds that ∥x(t) − Φ(x(t))∥ ≤ 2f (η, ρ).\n\nProof of Lemma J.13. The proof is almost identical to Lemma J.11 and is omitted.\n\nJ.2.2 TRACKING RIEMANNIAN GRADIENT FLOW\n\nLemma J.14. Under the condition of Theorem J.2, for any t satisfying that x(t) ∈ K h and\n\n∥x(t) − Φ(x(t))∥ = O(ηρ ln2(1/ηρ)).\n\nIt holds that\n\n∥Φ(x(t + 1)) − Φ(x(t)) + ηρ2P ⊥\n\nΦ(x(t)),Γ∇λ1\n\n(cid:16)\n\n∇2Lkt\n\n(cid:0)Φ(x(t))(cid:1)(cid:17)\n\n/2∥ ≤ ̃O(ηρ3 + η2ρ2) .\n\nProof of Lemma J.14. We will abbreviate kt by k in this proof.\n\nBy Taylor Expansion,\n\nx(t + 1) = x(t) − η∇Lk\n\nx(t) + ρ\n\n(cid:18)\n\n(cid:19)\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n= x(t) − η∇Lk (x(t)) − ηρ∇2Lk (x(t))\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n− ηρ2∂2(∇Lk)[\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n,\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n]/2 + O(ηρ3).\n\nNow as ∥x(t) − Φ(x(t))∥2 = ̃O(ηρ), by Lemma F.8, it implies\n\n∥x(t + 1) − x(t)∥2 = O(ηρ) .\n\nThen we have\n\n∥Φ(x(t + 1)) − Φ(x(t)) − ∂Φ(x(t))(x(t + 1) − x(t))∥2 ≤ ξ∥x(t + 1) − x(t)∥2\n\n2 = O(η2ρ2).\n\nUsing Lemma F.6, we have\n\n∥η∂Φ(x(t))∇Lk (x(t)) ∥2 = O(η∥x(t) − Φ(x(t))∥2\n\n2) = O(η3ρ2 + ηρ4),\n\n∥ηρ∂Φ(x(t))∇2Lk (x(t))\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n∥2 = O(ηρ∥x(t) − Φ(x(t))∥2) = ̃O(η2ρ2 + ηρ3).\n\nHence\n\n∥Φ(x(t + 1)) − Φ(x(t)) + ηρ2∂Φ(x(t))∂2(∇Lk)[\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n,\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n]/2∥2 = ̃O(η2ρ2 + ηρ3).\n\nNotice finally that by Lemma J.8,\n\n∂Φ(x(t))∂2(∇Lk)[\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥ =∂Φ(Φ(x(t)))∂2(∇Lk)[wk, wk] + O(∥x(t) − Φ(x(t))∥2) =P ⊥\n\nΦ(x(t)),Γ∇(λ1(∇2Lk(Φ(x(t))))) + O(∥x(t) − Φ(x(t))∥2).\n\n∇Lk (x(t)) ∥∇Lk (x(t)) ∥\n\n]\n\n,\n\nHence we have\n\nΦ(x(t + 1)) − Φ(x(t)) = −ηρ2P ⊥\n\nΦ(x(t)),Γ∇λ1\n\n(cid:16)\n\n∇2Lkt\n\n(cid:0)Φ(x(t))(cid:1)(cid:17)\n\n/2 + ̃O(η2ρ2 + ηρ3)\n\nThis completes the proof.\n\n67\n\nPublished as a conference paper at ICLR 2023\n\nJ.3 PROOF OF THEOREM 5.4\n\nProof of Theorem 5.4. By Theorem J.1, there exists constant T1 independent of η, ρ, such that there exists tPHASE ≤ T1 ln(1/ηρ)/η, with probability 1 − O(ρ), it holds that\n\n∥x(tPHASE) − Φ(x(tPHASE))∥2 = O(ηρ). ∥Φ(x(tPHASE)) − Φ(xinit)∥ = ̃O(η1/2 + ρ)\n\nHence by Theorem J.2, if we consider a translated process with x′(t) = x(t + tPHASE), we would have for any T3 such that the solution X of Equation 14 is well defined, we have that for t = ⌈ T3\n\nηρ2 ⌉\n\n∥Φ(x′(t)) − X(ηρ2t)∥2 = O(η ln(1/ρ)) .\n\nThis implies for t satisfying X(ηρ2(t − tPHASE)) is well-defined,\n\n∥Φ(x(t)) − X(ηρ2(t − tPHASE))∥2 = ̃O(η1/2 + ρ).\n\nFinally, as\n\n∥X(ηρ2(t − tPHASE)) − X(ηρ2t)∥2 = O(ηρ2tPHASE) = O(ρ ln(1/ηρ)) = ̃O(ρ).\n\nWe have that\n\nWe also have\n\nby Theorem J.2.\n\n∥Φ(x(t)) − X(ηρ2t)∥2 = ̃O(η1/2 + ρ).\n\n∥x(t) − Φ(x(t))∥2 = O(ηρ).\n\nJ.4 PROOFS OF COROLLARIES 5.6 AND 5.7\n\nProof of Corollary 5.6. We will do Taylor expansion on Ek[LMax k,ρ ](x). By Theorem J.1 and J.2, we have ∥x(⌈T3/ηρ2⌉) − X(T3)∥2 = ̃O(η1/2 + ρ) and ∥Φ(x(⌈T3/ηρ2⌉)) − x(⌈T3/ηρ2⌉)∥2 = ̃O(η1/2 + ρ). For convenience, we denote x(⌈T3/ηρ2⌉) by x.\n\nEk[RMax\n\nk,ρ ](x) = max\n\n∥v∥≤1\n\nEk[ρv⊤∇Lk(x) + ρ2v⊤∇2Lk(x)v/2] + O(ρ3)\n\nSince max∥v∥≤1 |v⊤∇Lk(x)| = O(∥x − Φ(x)∥) = ̃O(η1/2 + ρ), it holds that,\n\nEk[RMax\n\nk,ρ ](x) = ρ2Ek[ max\n\n∥v∥≤1\n\nv⊤∇2L(x)v/2] + O(cid:0)(η1/4 + ρ1/4)ρ2(cid:1)\n\n= ρ2Ek max\n\n[v⊤∇2L(X(T3))v/2] + O(cid:0)(η1/4 + ρ1/4)ρ2(cid:1)\n\n∥v∥≤1\n\n= ρ2Tr(X(T3))/2 + O(cid:0)(η1/4 + ρ1/4)ρ2(cid:1)\n\nProof of Corollary 5.7. We choose Tε such that X(Tε) is sufficiently close to X(∞), such that Tr(X(Tε)) ≤ Tr(X(∞)) + ε/2. By corollary 5.6 (let T3 = Tε), we have for all ρ, η such that (η + ρ) ln(1/ηρ) is sufficiently small, ∥Ek[RMax k,ρ ](x(⌈Tε/(ηρ2)⌉)) − ρ2Tr(X(T ))/2∥2 ≤ o(1). This further implies ∥Ek[RMax k,ρ ](x(⌈Tε/(ηρ2)⌉)) − ρ2Tr(X(∞))/2∥2 ≤ ερ2/2 + o(1). We also have L(x(⌈Tε/(ηρ2)⌉)) − inf x∈U ′ L(x) = o(1). Then we can leverage Theorems G.6 and G.14 to get the desired bound.\n\nJ.5 OTHER OMITTED PROOFS FOR 1-SAM\n\nWe will use l′(y, yk) and l′′(y, yk) to denote dl(y′,yk)\n\ndy′\n\n|y′=y and d2l(y′,yk)\n\ndy′2\n\n|y′=y.\n\nLemma J.15. Under Setting 5.1, fix k ∈ [M ], for any p satisfying l(fk(p), yk) = 0, we have that\n\n∇2Lk(p) = l′′(fk(p), yk)∇fk(p)(∇fk(p))⊤ .\n\n68\n\nPublished as a conference paper at ICLR 2023\n\nProof of Lemma J.15. l(fk(p), yk) = 0 implies l′(fk(p), yk) = 0. Then by Taylor Expansion,\n\n∇2Lk(p) = ∇2\n\npl(fk(p), yk)\n\n= ∂p[l′(fk(p), yk)∇fk(p)] = l′′(fk(p), yk)∇fk(p)∇fk(p)⊤ + l′(fk(p), yk)∇2fk(p) = l′′(fk(p), yk)∇fk(p)∇fk(p)⊤ .\n\nThis concludes the proof.\n\nProof of Lemma 5.5. By Lemma J.15, as L(p) = 1\n\nM\n\n(cid:80)M\n\nk=1 Lk(p), we have\n\n∇2L(p) =\n\n1 M\n\nM (cid:88)\n\nk=1\n\n∂2l(y′, yk) (∂y′)2\n\n|y′=fk(x)∇fk(p)∇fk(p)⊤ .\n\nBy definition of Γ in Setting 5.1, we have for any p ∈ Γ, {∇fk(p)}n implies that ∇fk(p) ̸= 0 for any p ∈ Γ.\n\nk=1 are linearly independent, which\n\nFor any p ∈ Γ, as ∇fk(p) that for any x ∈ V , ∥∇fk(x)∥2 ≥ C1 > 0 and ∥∇[ ∇fk(x)\n\n∥∇fk(p)∥ is well defined and continuous at p, there exists a open ball V containing p such\n\n∥∇fk(x)∥ ]∥2 ≤ C2 for some constants C1 and C2.\n\nSuppose ∇Lk(x) ̸= 0, then as by Taylor Expansion,\n\n∇Lk(x) = l′(fk(x), yk)∇fk(x) .\n\nWe have ∇Lk(x)\n\n∥∇Lk(x)∥ = ∇fk(x)\n\n∥∇fk(x)∥ = ∇fk(p)\n\n∥∇fk(p)∥ + C2∥x − p∥, which completes the proof.\n\nWe note that the alignment result in Lemma 5.5 is not directly used in our proof. Instead, we use its generalized version Lemma J.8 which holds under holds under a more general condition than Setting 5.1, namely Condition E.1.\n\nK TECHNICAL LEMMAS\n\nLemma K.1 (Corollary 4.3.15 in Horn et al. (2012)). Let Σ, ˆΣ ∈ RD×D be symmetric and non-negative with eigenvalues λ1 ≥ ... ≥ λD and ˆλ1 ≥ ... ≥ ˆλD, then for any i,\n\n|ˆλi − λi| ≤ ∥Σ − ˆΣ∥2 Definition K.2 (Unitary invariant norms). A matrix norm ∥ · ∥∗ on the space of matrices in Rp×d is unitary invariant if for any matrix K ∈ Rp×d, ∥U KW ∥∗ = ∥K∥∗ for any unitary matrices U ∈ Rp×p, W ∈ Rd×d. Theorem K.3. [Davis-Kahan sin(θ) theorem (Davis et al., 1970)] Let Σ, ˆΣ ∈ Rp×p be symmetric, with eigenvalues λ1 ≥ . . . ≥ λp and ˆλ1 ≥ . . . ≥ ˆλp respectively. Fix 1 ≤ r ≤ s ≤ p, let d ≜ s−r +1 and let V = (vr, vr+1, . . . , vs) ∈ Rp×d and ˆV = (ˆvr, ˆvr+1, . . . , ˆvs) ∈ Rp×d have orthonormal columns satisfying Σvj = (cid:111) λjvj and ˆΣˆvj = ˆλj ˆvj for j = r, r + 1, . . . , s. Define ∆ ≜ min ,\nwhere ˆλ0 ≜ ∞ and ˆλp+1 ≜ −∞, we have for any unitary invariant norm ∥ · ∥∗,\n\nmax{0, λs − ˆλs+1}, max{0, ˆλr−1 − λr}\n\n(cid:110)\n\n∆ · ∥ sin Θ( ˆV , V )∥∗ ≤ ∥ ˆΣ − Σ∥∗. Here Θ( ˆV , V ) ∈ Rd×d, with Θ( ˆV , V )j,j = arccos σj for any j ∈ [d] and Θ( ˆV , V )i,j = 0 for all i ̸= j ∈ [d]. σ1 ≥ σ2 ≥ · · · ≥ σd denotes the singular values of ˆV ⊤V. [sin Θ]ij is defined as sin(Θij). Lemma K.4 (Azuma-Hoeffding Bound). Suppose {Zn}n∈N is a super-martingale, suppose −α ≤ Zi+1 − Zi ≤ β, then for all n > 0, a > 0, we have\n\nP(Zn − Z0 ≥ a) ≤ 2 exp(−a2/(2n(α + β)2)) Lemma K.5 (Azuma-Hoeffding Bound, Vector Form, Hayes (2003)). Suppose {Zn}n∈N is a RD-valued martingale, suppose ∥Zi+1 − Zi∥2 ≤ σ, then for all n > 0, a > 0, we have\n\nP(∥Zn − Z0∥2 ≥ σ(1 + a)) ≤ 2 exp(1 − a2/2n).\n\nIn other words, for any 0 < δ < 1, with probability at least 1 − δ, we have that\n\n(cid:32)\n\n(cid:114)\n\n∥Zn − Z0∥2 ≤ σ\n\n1 +\n\n2n log\n\n69\n\n(cid:33)\n\n2e δ\n\n(cid:114)\n\n≤ 2σ\n\n2n log\n\n2e δ\n\n.\n\nPublished as a conference paper at ICLR 2023\n\nLemma K.6 (Discrete Gronwall Inequality, Borkar (2009)). Let {x(t)}t∈N be a sequence of nonnegative real numbers, {an}n∈N be a sequence of positive real numbers and C, L > 0 scalars such that for all n,\n\nx(t) ≤ C + L\n\nt−1 (cid:88)\n\nn=0\n\nanx(n).\n\nn=0 an, it holds that x(t + 1) ≤ CeLTt.\n\nThen for Tt = (cid:80)t Lemma K.7 (Magnus (1985)). Let A : RD → RD×D be any C1 symmetric matrix function and x∗ ∈ RD satisfying λ1(A(x∗)) > λ2(A(x∗)) and v1 be the top eigenvector of A(x∗). It holds that ∇λ1(A(x))|x=x∗ = ∇(v⊤\n\n1 A(x)v1)|x=x∗ .\n\nWe then present some of the technical lemmas we required to prove Lemma H.5.\n\nLemma K.8. If 0 < c < b−a\n\nb2 , a\n\n2(1−cb) ≥ a2+b2\n\n2−ca−cb , then a > 1\n\n2 b, cb ≤ 1\n\n2\n\n(cid:113) a2+2b2\n\nProof of Lemma K.8. Notice that\n\n(cid:114) a2 + b2 1 − cb\n\nca\n\n(cid:115)\n\n≥ ca\n\na2 + 2b2 2(1 − cb)\n\n≥\n\ncb2 + ca2 2 − cb − ca\n\n≥\n\ncb2 + ca2 2 − cb\n\n.\n\nSo\n\n√\n\n1 − cb +\n\n√\n\n1\n\n1 − cb\n\n(cid:114)\n\n≥\n\n1 +\n\nb2 a2\n\nAs c < b−a\n\nb2 , we have 1 > 1 − cb > a b .\n\nSo\n\n(cid:114) a b\n\n+\n\n(cid:114)\n\nb a\n\n(cid:114)\n\n≥\n\n1 +\n\nb2 a2\n\nThe above inequality implies a ≥ 1\n\n2 b. As c < b−a\n\nb2 ,cb ≤ 1 2 .\n\nLemma K.9. When 0 < a < b, 0 < c < b−a\n\nb2 , we have\n\ncb2 + ca2(2 − cb −\n\n2 3\n\nca) − (1 − cb)\n\nc(a2 + b2) 2 − ca − cb\n\n− ca2(\n\n1 2\n\na2 + b2)\n\n2 − ca − cb (a2 + b2)\n\n≤\n\ncb2 2 − cb\n\nProof of Lemma K.9. Equivalently, we are going to prove\n\n(1 − cb)b2\n\n(cid:18)\n\n1 2 − ca − cb\n\n−\n\n1 2 − cb\n\n(cid:19)\n\n+ a2\n\n1 − cb 2 − ca − cb\n\n+ a2(\n\n1 2\n\na2 + b2)\n\n2 − ca − cb (a2 + b2)\n\n≥ a2(2 − cb −\n\n2 3\n\nca)\n\nFurther simplifying, we only need to prove\n\n(1 − cb)cab2 (2 − cb)(2 − ca − cb)\n\n+ a2\n\n1 − cb 2 − ca − cb\n\n≥\n\n1 3\n\nca3 +\n\na4 2(a2 + b2)\n\n(2 − ca − cb)\n\nWe have the following auxiliary inequalities,\n\n(1 − cb)b > a\n\n≥\n\n1 b + b−a\n\na\n\na+b\n\n=\n\nab\n\na2 + b2 ≥\n\na2 a2 + b2\n\n1 − cb 2 − ca − cb\n\n≥\n\na+b\n\n1 b + b−a (1−cb)b 2 − ca − cb a2 + b2\n\n1 − cb\n\na2 ≥\n\n70\n\nPublished as a conference paper at ICLR 2023\n\nUsing the above auxiliary inequalities we have\n\n+ a2\n\n1 − cb 2 − ca − cb\n\n≥\n\n1 3\n\nca3 +\n\na4 2(a2 + b2)\n\n(2 − ca − cb)\n\n(cid:18)\n\n+\n\n1 −\n\n1 2\n\n(2 − ca − cb)\n\n(cid:19) a2(1 − cb) 2 − ca − cb\n\n≥\n\n1 3\n\nca3\n\n(1 − cb)cab2 (2 − cb)(2 − ca − cb) ca2b (2 − cb)(2 − ca − cb) ca2b (2 − cb)(2 − ca − cb) ca2b (2 − cb)(2 − ca − cb)\n\n⇐\n\n⇐\n\n⇐\n\n⇐\n\n1\n\n(2 − cb)2 +\n\n1 − cb 2(2 − cb)\n\n≥\n\n⇐3(1 − cb)(2 − cb) + 6 ≥ 2(2 − cb)2 ⇐(cb)2 − cb + 4 ≥ 0\n\n+\n\n+\n\n≥\n\n1 3\n\nca3\n\nca2b\n\nca2(a + b)(1 − cb) 2(2 − ca − cb) ca2b(1 − cb) 2(2 − ca − cb) 1\n3\n\n≥\n\n1 3\n\nLemma K.10. When 0 < a < b, 0 < c < b−a\n\nb2 , a\n\n2−ca−cb , we have\n\ncb2 + ca2(2 − cb −\n\n2 3\n\n(cid:113) a2+2b2\n\n2(1−cb) ≥ a2+b2 1\n2\n\nca) − (1 − cb)cb2 − ca2(\n\na2 + b2)\n\n1\n\nb2 ≤\n\ncb2 2 − cb\n\nProof of Lemma K.10. Equivalently, we are going to prove,\n\ncb3 + a2(2 − cb −\n\n⇐⇒ cb3 + a2(1 − cb −\n\n2 3\n2 3\n\nca) ≤\n\nca) ≤\n\nb2 2 − cb b2 2 − cb\n\n+\n\n+\n\nWe have the auxiliary inequality\n\n1\n\n2−cb > 1\n\n2 + cb 4 .\n\nHence\n\ncb3 + a2(1 − cb −\n\n⇐cb3 + a2(1 − cb −\n\n2 3\n2 3\n\nca) ≤\n\nca) ≤\n\n⇐c(\n\n3b3 4\n\n− ba2 −\n\na3) ≤\n\nb2 2\n\n+\n\na2( 1\n\n2 a2 + b2)\n\nb2\n\na4 2b2\n\n+\n\na4 2b2\n\nb2 2 − cb b2 2\n\n+\n\na4 2b2 + a4 2b2 − a2\n\ncb3 4\n\n1 Case 1, If 3b3\n\n4 − ba2 − 2\n\n2 Case 2, If 3b3\n\n4 − ba2 − 2\n\n3 a3 ≤ 0, then 3b3 4\n\nc(\n\n3 a3 > 0, then 3b3 4\n\nc(\n\n− ba2 −\n\na3) ≤ 0 ≤\n\nb2 2\n\n+\n\na4 2b2 − a2\n\n− ba2 −\n\n2 3\n\na3) ≤\n\nb2 2\n\n+\n\n(\n\n3b3 4\n\n− ba2 −\n\n2 3\n\na3) ≤\n\na4 2b2 − a2 (b2 − a2)2 2b2\n\n− ba2 −\n\n2 3\n\na3) ≤ (b − a)(b + a)2\n\n⇐\n\nb − a b2 3b3 4\n\n⇐2(\n\n⇐2(b3 − ba2) − (b − a)(b + a)2 ≤\n\n⇐(b − a)(2b(a + b) − (a + b)2) ≤\n\nb3 2\nb3 2\n\n+\n\n+\n\n4a3 3\n4a3 3\n\nUsing Lemma K.8,a > b\n\n⇐(b − a)2(b + a) ≤\n\nb3 2\n2 ,(b − a)2(b + a) = (b2 − a2)(b − a) ≤ b2(b − a) ≤ b3\n\n4a3 3\n\n+\n\n2\n\n71\n\n2 3\n\n2 3\n\nPublished as a conference paper at ICLR 2023\n\nLemma K.11. When 0 ≤ a ≤ b, 0 ≤ c ≤ b−a\n\nb2 , b2 ≥ a\n\n(cid:113) a2+2b2\n\n2(1−cb) ≥ a2+b2\n\ncb2 + ca2(2 − cb −\n\n(cid:114)\n\nca) − 2ca\n\n(b2 +\n\n2 3\n\n1 2\n\n2−ca−cb , we have cb2 2 − cb\n\na2)(1 − cb) ≤\n\nProof of Lemma K.11. Define\n\nF (a) ≜ a2(2 − cb −\n\n(cid:114)\n\nca) − 2a\n\n(b2 +\n\n2 3\n\n1 2\n\na2)(1 − cb)\n\nS(c, b) ≜ {a|0 ≤ a ≤ b, 0 < c ≤\n\nb − a b2\n\namin(c, b) ≜ inf S(c, b) amax(c, b) ≜ sup S(c, b) ≤ b − cb2\n\n(cid:115)\n\n, b2 ≥ a\n\na2 + 2b2 2(1 − cb)\n\n≥\n\na2 + b2 2 − ca − cb\n\n}\n\nConsider\n\ndF (a) da\n\n= 2a(2 − cb −\n\n2 3\n\nca) −\n\n2 3\n\n(cid:114)\n\nca2 − 2\n\n(b2 +\n\n1 2\n\na2)(1 − cb) − a2\n\n(cid:115)\n\n1 − cb 2 a2\n\nb2 + 1\n\nd2F (a)\n\nda2 = 2(2 − cb −\n\n2 3\n\nca) −\n\n4 3\n\nca −\n\n4 3\n\n(cid:115)\n\nca − a\n\n1 − cb 2 a2\n\nb2 + 1\n\n(cid:115)\n\n− 2a\n\n1 − cb 2 a2\n\nb2 + 1\n\n+\n\n≥ 4 − 2cb − 4ca − 3a\n\n(cid:115)\n\n1 − cb 2 a2\n\nb2 + 1\n\na3 2(b2 + 1\n\n2 a2) 3\n\n2\n\n√\n\n1 − cb\n\nDefine u ≜ cb, v ≜ a\n\nb , then u + v ≤ 1. d2F (a)\n\nda2 ≥ 4 − 2u − 4uv − 3\n\n√\n\n1 − u\n\n1\n\n(cid:113) 1\n\n2 + 1\n\nv2\n\n≥ 4 − 2u − 4u(1 − u) − 3\n\n√\n\n1 − u\n\n(cid:113) 1\n\n1 2 + 1\n\n(1−u)2\n\n≥ 4u2 − 6u + 4 − 3\n\n√\n\n1 − u\n\n(1 − u)\n\n(cid:113) (1−u)2\n\n2 + 1\n\n(cid:113) (1−u)2\n\n2 + 1 ≥\n\n(cid:113) (1−u)2+1\n\n2\n\n√\n\n≥\n\nAs\n\n1 − u,we have\n\nd2F (a)\n\nda2 ≥ 4u2 − 6u + 4 − 3(1 − u) = 4u2 + 1 − 3u > 0\n\nThe above inequality shows that F (a) is convex w.r.t to a for amin(c, b) ≤ a ≤ amax(c, b). Hence F (a) ≤ max (F (amin(c, b)), F (amax(c, b))). Below we use amin, amax as shorthands for amin(c, b),amax(c, b).\n\nFor F (amin), we have amin\n\n(cid:113) a2\n\nmin+2b2\n\n2(1−cb) = a2\n\nmin+b2\n\n2−camin−cb . This implies\n\n(cid:114)\n\n2amin\n\n(b2 +\n\n1 2\n\na2\n\nmin)(1 − cb) = (1 − cb)\n\n(a2\n\nmin + b2) 2 − camin − cb\n\n+ a2\n\nmin(\n\n1 2\n\nmin + b2) a2\n\n2 − camin − cb min + b2)\n\n(a2\n\nHence using Lemma K.9,\n\nF (amin) = a2\n\nmin(2 − cb −\n\n2 3\n\ncamin) − (1 − cb)\n\nc(a2\n\nmin + b2) 2 − camin − cb\n\n− ca2\n\nmin(\n\n1 2\n\nmin + b2) a2\n\n2 − camin − cb min + b2)\n\n(a2\n\n≤\n\n1 c\n\n(\n\ncb2 2 − cb\n\n− cb2)\n\n72\n\nPublished as a conference paper at ICLR 2023\n\nFor F (amax), we know that amax must satisfy at least of the following three equalities and we discuss three cases one by one.\n\n(cid:113) a2\n\n1. amax\n\n2. b2 = amax\n\nmax+2b2\n\n2(1−cb) = a2 (cid:113) a2\n\nmax+b2\n\nmax+2b2 2(1−cb) . This implies\n\n2−camax−cb , in this case we simply redo the calculation in Part 1.\n\n(cid:114)\n\n2amax\n\n(b2 +\n\n1 2\n\nmax)(1 − cb) = (1 − cb)b2 + a2 a2\n\nmax(\n\n1 2\n\nmax + b2) a2\n\n1 b2\n\nHence using Lemma K.10,\n\nF (amax) = a2\n\nmax(2 − cb −\n\n2 3\n\n≤\n\n1 c\n\n(\n\ncb2 2 − cb\n\n− cb2)\n\ncamax) − (1 − cb)cb2 − ca2\n\nmax(\n\n1 2\n\na2 max + b2)\n\n1 b2\n\n3. cb2 = b − amax. Define v ≜ amax\n\n. max + 2amaxb2 − 2b3 ≤ 0 ⇒ amax < 0.9b. This implies v ≤ 0.9. By Lemma K.8,\n\n, cb = 1 − v. Note that 1 − cb = amax\n\nand b2 ≥ amax\n\nmax+2b2) 2amax\n\nb\n\nb\n\n(cid:113) b(a2\n\nThese imply a3 0.5 ≤ v. As v ∈ [0.5, 0.9], it holds that\n\nv(1 + v) +\n\n1 1 + v\n\n(cid:114)\n\n≤ 2\n\n(1 +\n\nv2 2\n\n)v.\n\nThis implies\n\nFinally,\n\nv2(2 − (1 − v) −\n\n2 3\n\n(cid:114)\n\n(1 − v)v) − 2v\n\n(1 +\n\nv2 2\n\n)v ≤\n\n−v 1 + v\n\n=\n\n1 2 − cb\n\n− 1\n\nF (amax) = a2\n\nmax(2 − cb −\n\n(cid:32)\n\n(cid:114)\n\ncamax) − 2amax\n\n(b2 +\n\n2 3\n\n1 2\n\na2\n\nmax)(1 − cb) (cid:33)\n\n(cid:114)\n\n2 3\n\n(1 − v)v) − 2v\n\n(1 +\n\n)v\n\n≤ b2(\n\nv2 2\n\n1 2 − cb\n\n− 1) .\n\n= b2\n\nv2(2 − (1 − v) −\n\nIn conclusion, it holds that,\n\nF (a) ≤ max (F (amin(c, b)), F (amax(c, b)))\n\n≤\n\n1 c\n\n(\n\ncb2 2 − cb\n\n− cb2).\n\nL OMITTED PROOFS ON CONTINUOUS APPROXIMATION\n\nIn this section we give a general approximation result (Theorem L.1) between a continuous-time flow (Equation 39) and a discrete-time (stochastic) iterates (Equation 40) in some compact subset of RD, denoted by K. This result is used multiple times in our analysis for full-batch SAM and 1-SAM. 7 Let b : K → RD is a C1-lipschitz function, that is, ∀x, x′ ∈ K, it holds that ∥b(x) − b(x′)∥2 ≤ C1 ∥x − x′∥2. Let bk be mappings from K to RD for k ∈ [M ] satisfying that b(x) = 1\n\nk=1 bk(x) for all x ∈ K.\n\n(cid:80)M\n\nM\n\nWe consider the continuous-time flow X : [0, T ] → K, which is the unique solution of\n\nand the discrete-time iterate {x(t)}t∈N which approximately satisfy\n\nx(t + 1) ≈ x(t) + pbkt(x(t)),\n\ndX(τ ) = b(X(τ ))dτ.\n\n(39)\n\n(40)\n\n7Though we believe this approximation result is folklore, we cannot find a reference under the exact setting as ours.\n\nFor completeness, we provide a quick proof in this section.\n\n73\n\nPublished as a conference paper at ICLR 2023\n\nwhere kt is independently sampled from uniform distribution over [M ] for each t ∈ N and x(t) is a deterministic function of k0, . . . , kt−1. We use Ft to denote the σ-algebra generated by k0, . . . , kt−1 and F∗ to denote the filtration (Ft)t∈N. Thus x(t) is adapted to filtration F∗. Note b is undefined outside K, thus in the analysis we only consider the process stopped immediately leaving K, that is, xK(t) ≜ x(min(t, tK)), where tK ≜ {t′ ∈ N | x(t′) /∈ K}. If x(t) is in K for all t ≥ 0, then tK = ∞. It is easy to verify that tK is a stopping time with respect to the filtration F∗. For convenience, we denote XK(τ ) = X(min(τ, ptK)) as the stopped continuous counterpart of xK.\n\nTheorem L.1. Suppose there exist constants C2, ε, ε > 0 satisfying that\n\n1. ∥bk(x)∥2 ≤ C2, for any x ∈ K and k ∈ [M ]; 2. ∥bk(x) − b(x)∥2 ≤ C3, for any x ∈ K and k ∈ [M ]; (cid:13) (cid:13) 3. (cid:13)2\n\n(cid:13)bkt (x(t)) − x(t+1)−x(t)\n\n≤ ε, for all t.\n\n(cid:13) (cid:13)\n\np\n\nThen for any integer 0 ≤ k ≤ T /p and 0 < δ < 1, with probability at least 1 − δ, it holds that\n\nmax 0≤t≤T /p\n\n(cid:13)xK(t) − X K(pt)(cid:13) (cid:13)\n\n(cid:13) ≤ Hp,δeC1T ,\n\nwhere Hp,δ ≜ ∥x(0) − X(0)∥2 + C1C2T p + 2C3\n\n(cid:113)\n\npT log 2eT\n\nδp + εT .\n\nProof of Theorem L.1. Summing up Equation 39 and Equation 40, for any t ≤ tK, we have that\n\nand that\n\nX(pt) − X(0) =\n\n(cid:90) pt\n\nτ =0\n\nb(X(τ ))dτ,\n\nx(t) − x(0) =\n\nt−1 (cid:88)\n\nt′=0\n\nx(t′ + 1) − x(t′)\n\n(41)\n\n(42)\n\nDenote ∥x(t) − X(pt)∥2 by Et, we have that for t ≤ tK,\n\nτ =0\n\n(cid:90) pt\n\nEt − E0 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\n(cid:90) pt\n\nτ =0\n\n≤\n\n≤\n\n+\n\nb(X(τ ))dτ −\n\nb(X(τ ))dτ − p\n\nt−1 (cid:88)\n\n(x(t′ + 1) − x(t′))\n\nt′=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2\n\nt−1 (cid:88)\n\nt′=0\n\nb(X(pt′))\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:125)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\np\n\n(cid:123)(cid:122) (A)\n\nt−1 (cid:88)\n\nt′=0\n\nb(X(pt′)) − p\n\n(cid:123)(cid:122) (B)\n\nt−1 (cid:88)\n\nt′=0\n\nb(x(t′))\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:125)\n\nt−1 (cid:88)\n\nt′=0\n\np\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\nb(x(t′)) − p\n\nt−1 (cid:88)\n\nt′=0\n\n(cid:123)(cid:122) (C)\n\n(cid:13) (cid:13) bkt′ (x(t′)) (cid:13) (cid:13) (cid:13)2 (cid:125)\n\n+\n\nt−1 (cid:88)\n\nt′=0\n\np\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\nbkt′ (x(t′)) −\n\nt−1 (cid:88)\n\nt′=0 (cid:123)(cid:122) (D)\n\n(x(t′ + 1) − x(t′))\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:125)\n\n.\n\n(43)\n\nBelow we will proceed by bounding the four terms (A), (B), (C) and (D) in Equation 43.\n\n1. Note that for any 0 ≤ τ ≤ τ ′ ≤ T , we have that\n\n∥X(τ ) − X(τ ′)∥2 =\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:90) τ ′\n\ns=τ\n\nb(X(s))ds\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2\n\n≤\n\n(cid:90) τ ′\n\ns=τ\n\n∥b(X(s))∥2 ds ≤ (τ ′ − τ )C2.\n\nThus, by C1-lipschitzness of b, (cid:13) (cid:13) (cid:13) (cid:13)\n\n(A) =\n\n(cid:90) pt\n\nτ =0\n\nb(X(τ )) − b(X(⌊τ /p⌋p))dτ\n\n(cid:13) (cid:13) (cid:13) (cid:13)2\n\n≤\n\n(cid:90) pt\n\nτ =0\n\n∥b(X(τ )) − b(X(⌊τ /p⌋p))∥2 dτ\n\n≤C1C2p2t ≤ C1C2pT.\n\n2. By definition of Et and C1-lipschitzness of b, we have that (B) ≤ C1p (cid:80)t−1\n\nt′=0 Et′.\n\n74\n\nPublished as a conference paper at ICLR 2023\n\n3. We claim that for any 0 < δ < 1, we have that for probability at least 1 − δ, it holds that\n\n(cid:115)\n\n(C) ≤ 2C3\n\n2pT log\n\n2eT δp\n\n.\n\nBelow we prove our claim. We denote p (cid:80)min(t,tK )−1 is a martingale with respect to filtration F∗, since tK is a stopping time. Note ∥St − St+1∥2 ≤ max\n\n∥b(x) − bk(x)∥2 ≤ C3,\n\nb(x(t′)) − p (cid:80)min(t,tK )−1\n\nt′=0\n\nt′=0\n\nk∈[M ],x∈K\n\n(44)\n\nbkt′ (x(t′)) by St, which\n\nby Azuma-Hoeffding’s inequality (vector form, Lemma K.5), it holds that for any 0 ≤ t ≤ T /p and 0 ≤ δ ≤ 1, with probability at least 1 − δ,\n\n(cid:114)\n\n∥St∥2 ≤ 2C3p\n\n2t log\n\n2e δ\n\n.\n\nApplying an union bound on the above inequality over t = 0, . . . , ⌊T /p⌋ − 1, we conclude that with\n\nprobability at least 1 − δ, (C) ≤ 2C3p\n\n2T /p log 2eT\n\nδp = 2C3\n\n2T p log 2eT δp .\n\n(cid:113)\n\n(cid:113)\n\n4. We have that\n\n(D) ≤ p\n\nt−1 (cid:88)\n\nt′=0\n\n(cid:13) (cid:13) bkt′ (x(t′)) − (cid:13) (cid:13)\n\nx(t′ + 1) − x(t′) p\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ ptε ≤ εT.\n\nCombining the above upper bounds for (A), (B), (C) and (D), we conclude that for any 0 ≤ t ≤ min(T /p, tK),\n\nEt ≤ Hp,δ + C1p\n\nt−1 (cid:88)\n\nt′=0\n\nEt′.\n\n(45)\n\nApplying the discrete gronwall inequality (Lemma K.6) on Equation 45, we have that\n\nEt ≤ Hp,δeC1pt ≤ Hp,δeC1T ,\n\nwhich completes the proof.\n\nCorollary L.2. If min0≤τ ≤T dist(X(τ ), RD \\ K) > Hp,δeC1T , then with probability at least 1 − δ, tK > ⌊T /p⌋ and therefore\n\nmax 0≤t≤T /p\n\n∥x(t) − X(pt)∥ ≤ Hp,δeC1T .\n\nProof of Corollary L.2. By Theorem L.1, we know with probability at least 1 − δ, we have that\n\nmax 0≤t≤T /p\n\n(cid:13)xK(t) − X K(pt)(cid:13) (cid:13)\n\n(cid:13) ≤ Hp,δeC1T .\n\nTherefore dist(xK(t), RD \\ K) ≥ dist(X K(pt), RD \\ K) − dist(X K(pt), xK(t)) > 0 for any 0 ≤ t ≤ T /p, which implies xK(t) /∈ RD \\ K, or equivalently, xK(t) ∈ K. Thus we conclude that tK ≥ ⌊T /p⌋.\n\nCorollary L.3. Suppose M = 1 and there exist constants C2, ε > 0 satisfying that\n\n1. ∥b(x)∥2 ≤ C2 for any x ∈ K; 2.\n\n(cid:13)b(x) − x(t+1)−x(t)\n\n(cid:13) (cid:13) (cid:13) ≤ ε, for all x ∈ K.\n\n(cid:13) (cid:13)\n\np\n\nThen for any k ∈ N such that kp ≤ T , it holds that\n\nmax 0≤t≤T /p\n\n(cid:13) (cid:13)xK(t) − X K(pt)(cid:13)\n\n(cid:13) ≤ HpeC1T ,\n\nwhere Hp ≜ ∥x(0) − X(0)∥2 + C1C2T p + εT . Therefore, similar to Corollary L.2, if min0≤τ ≤T dist(X(τ ), RD \\ K) > HpeC1T , then it holds that tK > ⌊T /p⌋ and that\n\nmax 0≤t≤T /p\n\n∥x(t) − X(pt)∥ ≤ Hp,δeC1T .\n\n75\n\nPublished as a conference paper at ICLR 2023\n\nProof of Corollary L.3. For any δ ∈ (0, 1], choosing C3 = 0 and by Theorem L.1, we have that\n\nP\n\n(cid:20)\n\nmax 0≤t≤T /p\n\n(cid:13)xK(t) − X K(pt)(cid:13) (cid:13)\n\n(cid:13) ≤ HpeC1T\n\n(cid:21)\n\n≥ 1 − δ .\n\nSince δ can be any number in (0, 1], the above probability is exactly 1.\n\nWe end this section with a summary of applications of Theorem L.1 and corollary L.3 in our proofs (Table 2).\n\nSetting Full-batch SAM, Phase I (Lemma I.4) Full-batch SAM, Phase II (Theorem I.3) 1-SAM, Phase I (Lemma J.3) 1-SAM, Phase II (Theorem J.2)\n\nbk −∇L(·)\n\np η\nηρ2 −∂Φ(·)∇λ1(∇2L(·))/2 η\nηρ2 −∂Φ(·)∇Tr(∇2Lk(·))/2\n\n−∇Lk(·)\n\nε ρ\nρ + η ρ\nρ + η\n\nTable 2: Summary of applications of Theorem L.1 and corollary L.3 in our analysis\n\n76",
    "reference": "# Summary Of The Paper\n\nSharpness-aware training has been widely adopted for its generalization performance.\nThis paper provides the theoretical analysis for sharpness-aware minimization (SAM).\nThis paper categorizes the SAM in three ways, worst-direction, ascent-direction, and average direction.\nBesides, they provide explicit bias for each way.\nThe paper is clear and theoretically sound.\n\n# Strength And Weaknesses\n\nStrength\n1. This paper categorizes the SAM in three ways, and they explain each method in detail.\n2. The paper claims the novel findings and their explicit bias.\n3. There are no experimental results for the real-world dataset, but they provide an intuitive toy experiment to support the claim.\n\nWeakness and Questions\n1. There is no experiment for the real-world datset.\n2. This paper raises a new perspective to analyze the SAM. It would be more interesting if the paper provided a new approach from a new perspective.\n3. This paper claims that batch size=1 SAM corresponds to the average-direction sharpness. In the SAM paper, they introduce the m-sharpness, and m=1 shows a strong performance improvement compared to the full-batch SAM.\nI wonder about the author's opinion about the average-direction sharpness that performs better than worst-direction sharpness in real-world experiments.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is easy to follow.\n\n# Summary Of The Review\n\nI carefully read the paper and the main statement of the theorem.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLEARNING COMBINATORIAL NODE LABELING ALGORITHMS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe present the combinatorial node labeling framework, which generalizes many prior approaches to solving hard graph optimization problems by supporting problems where solutions consist of arbitrarily many node labels, such as graph coloring. We then introduce a neural network architecture to implement this framework. Our architecture builds on a graph attention network with several inductive biases to improve solution quality and is trained using policy gradient reinforcement learning. We demonstrate our approach on both graph coloring and minimum vertex cover. Our learned heuristics match or outperform classical hand-crafted greedy heuristics and machine learning approaches while taking only seconds on large graphs. We conduct a detailed analysis of the learned heuristics and architecture choices and show that they successfully adapt to different graph structures.\n\n1\n\nINTRODUCTION\n\nGraph problems have numerous real-world applications, ranging from scheduling problems (Marx, 2004) and register allocation (Chaitin, 1982; Smith et al., 2004), to computational biology (Abukhzam et al., 2004). However, many useful graph optimizations problems are NP-hard to solve (Karp, 1972). This has spurred a variety of approaches, from greedy heuristics (Brélaz, 1979; Papadimitriou & Steiglitz, 1982; Matula & Beck, 1983; Avis & Imamura, 2007; Delbot & Laforest, 2008) to integer linear programming (Graver, 1975). More recently, machine learning approaches have shown increasing promise (Dai et al., 2017; Kool et al., 2019; Li et al., 2018; Karalias & Loukas, 2020).\n\nFrom a structural point of view, many graph problems fall into one of three classes depending on the type of their solution: Problems that ask for (1) subsets of vertices, (2) permutations of vertices, or (3) partitions of vertices into two or more sets. Most work has focussed on either the first two (Dai et al., 2017), or just one of the three (Bello et al., 2017; Li et al., 2018; Kool et al., 2019; Karalias & Loukas, 2020; Manchanda et al., 2020; Cappart et al., 2020; Drori et al., 2020; Ma et al., 2020). Existing machine learning methods for the first two types of problems, such as S2V-DQN (Dai et al., 2017), do not easily generalize to cases where the number of labels is not known in advance. Many important and challenging problems, such as graph coloring (Marx, 2004; Myszkowski, 2008; Bandh et al., 2009), require that vertices be partitioned into an unkown number of sets.\n\nTo address this, we present the combinatorial node labeling framework (§2), which generalizes prior approaches (Fig. 1), and supports many problems, including minimum vertex cover (Onak et al., 2012; Bhattacharya et al., 2017; Ghaffari et al., 2020), traveling salesman (Dantzig et al., 1954; Garey & Johnson, 1990), maximum cut (Karp, 1972), and list coloring (Jensen et al., 1995). These, and many other (§D), problems can all be framed as iteratively assigning a label to nodes, in some order. We then introduce a neural architecture, GAT-CNL, to learn greedy-inspired heuristics for such problems (§3). We use policy gradient reinforcement learning (Sutton & Barto, 2018; Kool et al., 2019) to learn a node ordering and combine this with a fixed label rule to label each node according to the ordering. We show that for the chosen label rules, there still exists an order that guarantees an optimal solution. By using policy gradients, we can construct both a deterministic greedy policy, as well as a probabilistic policy where sampling boosts the solution quality. To improve performance, we incorporate two inductive biases: spatial locality, where labeling a node only impacts the weights of its neighbors; and temporal locality, where node selection is conditioned only on the previously labeled node, a summary of prior labelings, and a global graph context (Figs. 2 and 3).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Left: Venn diagram of tasks solvable with the set, permutation, and node labeling frameworks. Node labeling generalizes existing frameworks and allows solving additional tasks. Center & right: Comparison of our architecture with S2V-DQN (Dai et al., 2017). We add a label assignment step, allowing us to solve new problems. Further, the average time for picking the next vertex is significantly reduced, such that the total number of arithmetic operations is now linear in the size of the graph.\n\nWe evaluate our approach (§4) and demonstrate significantly improved performance for neural graph coloring (GC) and find near-optimal solutions for minimum vertex cover (MVC). We additionally study the runtime of our models, conduct comprehensive ablation studies, and provide qualitative analyses of the learned heuristics, showing they adapt to the properties of the input graph.\n\nRelated work. We now review key related works. Figure 1 (left) provides a comparison of node labeling with other frameworks.\n\nSupervised learning. The fundamental downsides of supervised learning for combinatorial optimization are twofold: First, it can be difficult to formulate a problem in a supervised manner, since it might have many optimal solutions (e.g., GC). Second, even if the problem admits a direct supervised formulation, we still need labeled data for training, which can be hard to generate and relies on an existing solver. In particular, supervised learning cannot easily be used for problems that have not been studied before. Advantages of supervised learning are its sample efficiency and that it can lead to overall better results. Recent approaches like Joshi et al. (2019) and Manchanda et al. (2020) obtain good results for influence maximization (IM) and the traveling salesman problem (TSP), respectively. Both approaches use supervised learning. For IM, the approach of Manchanda et al. (2020) shows promising results on graphs much larger than those seen in training. For TSP, the approach of Joshi et al. (2019) is very efficient but does not generalize well to graphs larger than those seen in training. Li et al. (2018) also use supervised learning and produce good results on minimum vertex cover (MVC), maximum independent set, and maximal clique.\n\nUnsupervised Learning. To apply unsupervised learning, it is necessary to formulate a differentiable surrogate loss. There have been approaches for several specific combinatorial optimization problems (Nazi et al., 2019; Amizadeh et al., 2019; Tönshoff et al., 2019; Yao et al., 2019) and there has been progress to create a framework for the derivation of trainable losses (Karalias & Loukas, 2020). Still, significant insight into a problem is required to design suitable loss functions.\n\nReinforcement learning (RL). Using RL only requires a way to represent partial solutions and a way to score the cost of a (partial or final) solution. Dai et al. (2017) provide S2V-DQN, a general framework for learning problems like MVC and TSP that is trained with RL. It shows good results across different graph sizes for the covered problems, but is not fast enough to replace existing approaches nor does it handle arbitrary node labels (see Fig. 1). Kool et al. (2019) focus on routing problems like TSP and the vehicle routing problem. They outperform Dai et al. on TSP instances of the training size. Unfortunately, their approach does not seem to generalize to graph sizes that are very different from those used for training. Several other RL approaches have been proposed and evaluated for TSP (Bello et al., 2017; Cappart et al., 2020; Drori et al., 2020; Ma et al., 2020). Barrett et al. (2020) consider the maximum cut (MaxCut) problem. Huang et al. (2019) present a Monte Carlo search tree approach specialized only for graph coloring. These methods do not address the general node labeling framework, but instead model the solution as a permutation of vertices (e.g., TSP, vehicle routing) or a set of nodes or edges (e.g., MVC, MaxCut). Instead, we can represent\n\n2\n\nS2VEncoderGreedyDecoderState EmbeddingVertex embeddingNextvertexGraphS2V-DQNmmmm edgesn verticesn timesGATEncoderProbabilisticDecoderState EmbeddingLabel AssignmentVertex embeddingNext vertex& Next labelNextvertexGraphOurs: GAT-CNLm/nmm/nm/nAverage TimeLearnablem edgesn verticesn timesNolabelsSet[Dai et al. 2017; Li et al. 2018;Manchanda et al. 2020; Barrett et al. 2020]Example tasks:§Max cut§MVC (§4.2)§Max clique‸Permutation[Bello et al. 2017; Dai et al. 2017;Joshi et al. 2019; Kool et al. 2019;Cappartet al. 2020; Droriet al. 2020;Ma et al. 2020]Example tasks:§TSP§Longest pathsNode Labeling[Ours]Example tasks:§Graph coloring (§4.1)§Min k-cut§Clique coverUnder review as a conference paper at ICLR 2023\n\nsolutions where vertices are assigned to an unknown and unbounded number of partitions, which is crucial for solving tasks such as graph coloring.\n\n2 COMBINATORIAL NODE LABELING\n\nMany graph heuristics can be phrased as a greedy process, where vertices get assigned a problemdependent label one after the other. For example, this label could indicate if the vertex is part of the solution set, its position in a permutation, or its membership in one of many sets. We introduce combinatorial node labeling, which frames many hard graph optimization problems, such as graph coloring (see §D for a list), as a greedy process. This generalizes previous work (Kool et al., 2019; Dai et al., 2017; Ma et al., 2020; Drori et al., 2020), to encompass problems where the number of labels is not known in advance and is unbounded (see Fig. 1).\n\nEvery node labeling problem can be formulated as a (finite) Markov decision process (MDP), during which nodes are successively added to a so-called partial node labeling until a termination criterion is met. In §3, we will present a graph learning approach to optimizing such node labeling MDPs.\n\n2.1 PRELIMINARIES\n\nWe consider an undirected, unweighted, and simple graph G = (V, E) with n nodes in V and m edges in E. We denote the neighbors of a node v by N (v). We assume w.l.o.g. that the graph is connected and hence m = Ω(n).\n\nA node labeling is a function c : V → L, where L ⊆ {0, . . . , n}. A partial node labeling is a function c(cid:48) : V (cid:48) → L(cid:48) for a subset of nodes V (cid:48) ⊆ V and labels L(cid:48) ⊆ L. A node labeling problem is subject to a feasibility condition and a real-valued cost function f . The cost function maps a node labeling c to a real-valued cost f (c). We require that the feasibility condition be expressed in terms of an efficient (polynomial-time computable) extensibility test T : P(V × L) × V × L → {0, 1}, where P denotes the powerset. We say the extensibility test passes when it returns 1.\n\nIntuitively, given a partial node labeling c(cid:48), a node v (cid:54)∈ V (cid:48), and label (cid:96), the extensibility test passes if and only if c(cid:48) can be extended by labeling node v with (cid:96) such that c(cid:48) can be extended into a node labeling. Formally, the extensibility test characterizes the set of feasible solutions: Definition 2.1. A node labeling c is feasible if and only if there exists a sequence of nodelabel pairs (v1, (cid:96)1), . . . , (vn, (cid:96)n) such that for all i ≥ 0 the extensibility test T satisfies T ({(v1, (cid:96)1), . . . , (vi, (cid:96)i)}, vi+1, (cid:96)i+1) = 1.\n\nThe goal of the node labeling problem is to minimize the value of the cost function among the feasible node labelings. For consistency, an infeasible node labeling has infinite cost. Next, we present the two node labeling problems on which we focus in our evaluation. Definition 2.2. A k-coloring of a graph G = (V, E) is a node labeling c : V → {1, 2, . . . , k} such that no two neighbors have the same label, i.e., ∀{u, v} ∈ E : c(u) (cid:54)= c(v).\n\nThe cost function for GC is the number of distinct labels (or colors) k. Given a partial node labeling c(cid:48) : V (cid:48) → {1, . . . , k} and any vertex-label pair (v, (cid:96)), the extensibility test passes for (c(cid:48), v, (cid:96)) if and only if the extended partial node labeling c(cid:48) ∪ (v, (cid:96)) is a k- or (k + 1)-coloring of the induced subgraph G[V (cid:48) ∪ {v}]. In particular, the test does not pass when (cid:96) > k + 1. The smallest k for which there is a k-coloring of G is the chromatic number χ(G) of G. Definition 2.3. A vertex cover of a graph G = (V, E) is a node labeling c : V → {0, 1} such that every edge is incident to at least one node with label 1, i.e., ∀{u, v} ∈ E : c(u) = 1 ∨ c(v) = 1.\n\nThe cost function for MVC is the number of nodes with label 1. Given a partial node labeling c(cid:48) : V (cid:48) → {0, 1} the extensibility test passes for (c(cid:48), v, (cid:96)) if and only if the extended partial node labeling c(cid:48) ∪ (v, (cid:96)) is a vertex cover of the induced subgraph G[V (cid:48) ∪ {v}].\n\n2.2 NODE LABELING MDP\n\nWe show how to construct an MDP that models a given combinatorial node labeling problem. Minimizing the cost of the combinatorial node labeling problem is equivalent to maximizing the return of this MDP. In the vast majority of reinforcement learning approaches to solve combinatorial\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ngraph optimization problems (Kool et al., 2019; Dai et al., 2017; Ma et al., 2020; Drori et al., 2020), a state corresponds to a set or sequence of nodes that are already added to a solution set. Instead, in our setting the state represents a partial node labeling. This is why in addition to problems like MVC and TSP, we can also model problems with more than two labels (even when the number of labels is not known in advance). Graph coloring is such a problem. Lemma 2.4. For any node labeling problem, there is an MDP whose terminal states correspond to the feasible solutions with a cost equal to the negative return.\n\nWe embed the cost function f and the extensibility test into the MDP. Note that we do not require a way to measure the cost of partial node labelings. Here, we formulate the state space, action space, transition function, and reward. In §C.1, we finish the proof of Lemma 2.4.\n\nState space. A state S represents a partial node labeling. It is a set of pairs S = V (cid:48) × L for a subset of nodes V (cid:48) ⊆ V and a subset of labels L ⊆ {0, . . . , n}. A state is terminal if V (cid:48) = V . Hence, the set of states is the powerset P(V × {0, . . . , n}) of the Cartesian product of the vertices and labels. Action space. In state S, the set of legal actions are the pairs (v, (cid:96)) for nodes v and labels (cid:96) which pass the extensibility test of the problem for the partial node labeling given by S (i.e., T (S, v, l) = 1). Transition function. In our case, the transition function T is deterministic. That is, given the current state St and an action (v, (cid:96)), T (St, (v, (cid:96))) yields the next state St+1 = St ∪ {(v, (cid:96))}. Reward. For a terminal state S representing the node labeling c, the reward is −f (c). For all other states, the reward is 0.\n\nA policy is a mapping from states to probabilities for each action. Note that we can turn a probabilistic policy into a deterministic greedy policy by choosing the action with largest probability. Next, we present how to train such a policy end-to-end using policy gradients.\n\n3 GRAPH LEARNING APPROACH\n\nWe present a graph learning approach to node labeling, which is inspired by greedy algorithms. Greedy approaches generally trade optimality for improved runtime. A greedy node labeling algorithm assigns a label in {0, . . . , n} to one node after another based on a problem-specific heuristic. Hence, it can be seen as providing (1) an order on the nodes and (2) a rule to label the next selected node.\n\nWe focus on learning an order on the nodes and pick a label that passes the extensibility test according to a fixed rule. The following two lemmas show there exists a label assignment rule that ensures the optimal solution can be found for GC and MVC (see §C.2 for the proofs): Lemma 3.1. For every graph G, there exists an ordering of vertices for which choosing the smallest color that passes the extensibility test colors G optimally. Lemma 3.2. For every graph G, there exists an ordering of vertices for which choosing the label 1 until every edge in G has one of its endpoints labeled with 1 produces a minimum vertex cover of G.\n\nWe expect similar results can be obtained for most other node labeling problems.\n\nInstead of a handcrafted ordering heuristic, we learn to assign weights to each node and choose the nodes according to their weights. To compute these weights, we introduce a novel spatial locality inductive bias inspired by the greedy heuristics: labeling a node should only affect the weights of its neighbors. As we will show in §4.3, this leads to better test scores compared to the alternatives of updating all or none of the weights when a node is labeled. This spatial locality bias is inspired by successful greedy heuristics: The ListRight heuristic for MVC (Delbot & Laforest, 2008) assigns a node to the vertex cover based on the assignment of its neighbors. For GC, the DSATUR strategy selects nodes according to their saturation degree (Brélaz, 1979). If a new node is selected, only the saturation degree of its neighborhood can change; the others remain unchanged.\n\n3.1 POLICY OPTIMIZATION\n\nWe train our node labeling model by policy gradients, specifically REINFORCE (Bello et al., 2017) with a greedy rollout baseline (Kool et al., 2019). The advantage of policy gradients over Q-learning is that is has stronger convergence guarantees (Sutton & Barto, 2018). At a high level, the algorithm works as follows. We begin by initializing two models, the current model and the baseline model. For each graph in the batch, the algorithm performs a probabilistic rollout of the policy. The baseline\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Spatial locality of the decoding. We show how a graph is 2-colored using node order c, e, b, a, d. After labeling a node, only its neighbors’ attention weights change; e.g., when c is colored, only its neighbors b and e receive new attention weights. (We omit the last step where d is colored.)\n\nFigure 3: Temporal locality of the state embedding. We show how the state embedding is updated as nodes are colored. The state embedding focuses on the last labeled node, and contains the graph embedding, and embeddings of the last labeled node and its label, which pools the embeddings of nodes with the same label.\n\nmodel performs a greedy rollout. The difference between the two costs determines the policy gradient update. After every epoch, we perform a (one-sided) paired t-test over the cost on a challenge dataset to check if the baseline model should be replaced with the current model. See §A.2 for more details.\n\n3.2 GAT-CNL ARCHITECTURE\n\nOur architecture, GAT-CNL, consists of an encoder and a decoder to learn a policy specific to the node labeling problem. The encoder learns the local structural information that is important for the problem in the form of a node embedding.\n\nThe state embedding encapsulates information about the graph itself (enabling the network to adapt its actions to the graph), the last node that was labeled and its label, and a summary of prior actions (with pooling). This enables the state embedding to have constant size; adding additional nodes provided no benefit (see §4.3). This also serves as a temporal locality bias; however, note that the decoder is not Markovian, as it depends on more than just the previous decision.\n\nThe decoder uses the node embeddings and the state embedding to select the next node based on attention weights between the node embeddings and the state embedding. After the decoder picks the next node v, the label rule (see Lemmas 3.1 and 3.2) assigns the label (cid:96) for the node. The policy then takes the action (v, (cid:96)). Then, the state embedding is updated and the decoder is invoked again until all nodes are labeled. Figure 1 (right) overviews our architecture.\n\nNode features. Each node v is associated with an input feature vector xv. Our input features consist of a combination of sine and cosine functions of the node degree, similar to positional embeddings (Vaswani et al., 2017). This representation ensures that input features are bounded in magnitude even for larger graphs. We subtract the mean node degree from the degrees on the synthetic dense graph instances.\n\nGAT encoder. We use a hidden dimension of size d (unless stated otherwise, d = 64). The input features are first linearly transformed and then fed into a GNN, which produces, for each node v, a node embedding hv ∈ Rd. We use a three-layer Graph Attention Network (GAT) (Vaswani et al., 2017; Velickovic et al., 2018; Lee et al., 2019), additive multi-head attention with four heads, batch normalization (Ioffe & Szegedy, 2015) with a skip connection (He et al., 2016) at each encoder layer, and leaky ReLU activations (Maas et al., 2013).\n\nState embedding. The state embedding allows the decoder to condition its choice based on the graph instance and the partial node labeling. For computational reasons, we ensure it is of constant size. Denote the node labeled in step t by v(t) and its label by (cid:96)(t). Then the state embedding consists of three components concatenated together: (1) The graph embedding hG, a max-pooling over all node embeddings. (2) The node embedding hv(t−1) of the last labeled node v(t−1). (3) The label embedding h(cid:96)(t−1) of the last labeled node’s label (cid:96)(t−1), a max-pooling over the embeddings of all nodes with that label. In the first iteration, we use a learned parameter h(0) for (2) and (3). We considered including more than just the last labeled node, but found that this led to worse performance (§4.3). Hence, this induces a temporal bias by focusing on the prior node and nodes with the same label as the last labeled node. See Figure 3 for an illustration of the state embedding.\n\n5\n\nAttention Weights ChangeLast Labeled NodeState embeddingGraph embeddingLast labeled node embedding Label embedding Under review as a conference paper at ICLR 2023\n\nLocal attention decoder. The decoder takes as input the node embeddings generated by the encoder In each time step t, an attention and the state embedding and outputs the next node to label. mechanism between the state embedding gt and each node embedding hv produces attention weights a(t) v . Here, we introduce a spatial locality bias: labeling a node can only affect the attention scores of its neighbors in the next time step. Let V (cid:48) be the set of nodes already labeled. The attention weight a(t)\n\nfor node v in time step t is given by the local decoding. For a node v /∈ V (cid:48): v ∈ N (v(t−1)) or t = 0\n\n(cid:16) (Θ1gt)T (Θ2hv)\n\n(cid:40)\n\n(cid:17)\n\n√\n\nv\n\na(t)\n\nv =\n\nC · tanh a(t−1)\n\nv\n\nd\n\nv /∈ N (v(t−1)).\n\nIf v ∈ V (cid:48), then the attention weight is a(t) v = −∞. In the first iteration of the decoder, we calculate the coefficients for each node in the graph. As in Bello et al. (2017), we clip the attention coefficients within a constant range [−C, C]. In our experiments we set C = 10. The learnable parameter matrices are Θ1 ∈ Rd×3d and Θ2 ∈ Rd×d. We use scaled dot-product attention (Vaswani et al., 2017) (instead of additive attention) to speed up the decoding. Finally, for each node v we apply a softmax over all attention weights to obtain the probability pv that node v is labeled next. See Figure 2 for a visualization of the attention weight computation during decoding.\n\nDuring inference, our greedy policy selects the vertices with maximum probability. Our sampling policy (for k samples) runs the greedy policy once, then evaluates the learned probabilistic policy k times (selecting a node v with the learned probability pv), returning the best result.\n\n3.3 NUMBER OF OPERATIONS\n\nWe express the number of operations (arithmetic operations and comparisons) of the model during inference parameterized by the embedding dimension d, the number of nodes n and the number of edges m. The encoder uses O(dm + d2n) arithmetic operations and the decoder uses O(d2m) arithmetic operations, resulting in O(dm + d2n + d2m) arithmetic operations, which is linear in the size of the graph. To select the action of maximum probability (or sample a node), the decoder additionally needs O(n2) comparison operations (although this could be reduced to O(m log n) with an appropriate data structure). We empirically study the runtime in §§4.1 and 4.2; in practice, the d2m term dominates the runtime for the evaluated graphs until 5000 vertices. In contrast, updating all attention weights after every labeling scales as O(n3) (see §B.5).\n\n4 EXPERIMENTS\n\nWe evaluate our approach on established benchmarks for graph coloring and minimum vertex cover, including greedy baselines and machine learning approaches. We focus on other heuristic approaches that return an approximation in polynomial time.\n\nTraining. We use three different synthetic graph distributions to generate instances for training and validation (Albert & Barabási, 2002; Erd ̋os & Rényi, 1960; Watts & Strogatz, 1998). We generate 20,000 graphs for training. The graphs have between 20 and 100 nodes. We use Adam with learning rate α = 10−4 (Kingma & Ba, 2015). The effective batch size is B = 320, which comes from using batches of 64 graphs for each node count n and accumulating their gradients. We clip the L2 norm of the gradient to 1, as done in Bello et al. (2017). We selected these hyperparameters after initial experiments on the validation set. Each model took 15–20 CPU compute node hours to train on a cluster with Intel Xeon E5-2695 v4 and 64 GB memory per node. The overall time spent training was\n\nTable 1: Graph coloring results on the Lemos et al. (2019) subset of the COLOR challenge graphs.\n\nTable 2: Comparison of MVC approaches on dense ER graphs with edge-probability 0.15.\n\nMethod\n\nCost Wins Optimal\n\nMethod\n\nCost Approx. Ratio\n\ni s\ns a\nl\n\nc Largest First DSATUR Smallest Last\n\nC\n\n10.65 50% 9.85 65% 10.8 50%\n\n45% 50% 45%\n\nL M\n\nLemos et al. (2019) Ours — Greedy Ours — Sampling\n\nN/A 45% 10.36±0.01 55%\n\n25% 50% 9.65±0.04 70% 50%\n\n. Maximal Matching\n\nList Right\n\n232.00 225.35\n\n212.296 Li et al. (2018) N/A S2V-DQN 221.52±1.1 Ours — Greedy Ours — 10 samples 220.27±1.2\n\ns s\na l\n\nC\n\nL M\n\n6\n\n1.2486 1.1120\n\n1.0594 1.1208 1.0510 1.0443\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Erd ̋os-Rényi graph.\n\n(b) Watts-Strogatz graph. Figure 4: Example colorings produced by our learned heuristic. Node borders indicate the colors. Numbers on the nodes indicate the order in which the heuristic labels them.\n\n(c) Barabási-Albert graph.\n\nwas less than 2500 CPU node hours and the time spent on validation and testing was less than 300 CPU node hours. We train each model for 200 epochs with five random seeds and report the standard deviation σ of cost w.r.t. the random seeds as ±σ. See §A for more details.\n\nTest Scores. In addition to mean cost, we report the ratio of the solution cost to the optimal solution cost (approximation ratio). For large graphs, this cannot be computed exactly in a timely manner. In this case, we use the best solution found by an ILP solver within a compute time of one hour. To compare with baselines which return infeasible solutions (and hence have ill-defined cost), we report the percentage of wins (ties for first place count as wins) and the percentage of instances solved optimally. We refer to these metrics as “Wins” and “Optimal”, respectively. We use the model with the lowest cost to compute these percentages.\n\nRuntime.We benchmark on a c2d-standard-4 Google Cloud instance with 4 vCPUs and 16 GB RAM.\n\n4.1 GRAPH COLORING\n\nGreedy baselines. Largest-First greedily colors nodes in decreasing order of degree. SmallestLast (Matula & Beck, 1983) colors the nodes in reverse degeneracy order, which guarantees that when a node is colored, it will have the smallest possible number of neighbors that have been already colored. Smallest-Last guantees a constant number of colors for certain families of graphs, such as Barabási-Albert graphs (Albert & Barabási, 2002) and planar graphs (Matula & Beck, 1983). DSATUR (Brélaz, 1979) selects nodes based on the largest number of distinct colors in its neighborhood. DSATUR is exact on certain families of graphs, e.g., bipartite graphs (Brélaz, 1979). We use the implementations from NetworkX (Hagberg et al., 2008).\n\nMachine learning baseline. We compare our approach with the chromatic number estimator of Lemos et al. (2019). It does not guarantee that the solution is feasible, meaning that it can both underand overestimate the chromatic number. We use the values reported by the original paper. Note that S2V-DQN (Dai et al., 2017) cannot solve GC because of the way it embeds the state.\n\nCOLOR benchmark (Table 1). We evaluate on the same subset of the COLOR02/03 benchmark (Col, 2002) as Lemos et al. (2019), consisting of 20 instances of size between 25 and 561 nodes. Our greedy policy outperforms both Largest-First and Smallest-Last and is tied with DSATUR for the most graphs solved optimally. When sampling (100 samples) to evaluate the policy, our model outperforms all baselines in both mean cost and win percentage and is tied for the most graphs solved optimally. The approximation ratio is 1.25 and 1.13 for our greedy and sampling policies, resp.\n\nResults on classic graphs. We also trained our model on four families of sparse graphs: cycles, wheels, random trees, and stars. We trained on graphs up to 400 nodes and evaluated on graphs up to 10,000 nodes. The produced colorings are optimal or extremely close to optimal for all four families (Table 3). As our model works perfectly on cycles and wheels we conclude that the model learns to leverage local graph structure and works even when all nodes have the same degree and are completely symmetrical. Table 4 shows the validation cost for varying instance size on Watts-Strogatz graphs, which grows only slowly with instance size. See §B.2 for additional results.\n\nQualitative results. Figure 4 presents typical examples of the learned coloring heuristic on the training distribution graphs. See §E.1 for more examples. We can observe that the heuristic generally\n\n7\n\n012345678910111213141501234567891011121314150123456789101112131415Under review as a conference paper at ICLR 2023\n\nTable 3: Our approach colors simple families of graphs (near-)optimally.\n\nGraph Family Optimal χ Ours\n\nStars Random trees Even cycles Odd cycles Odd wheels Even wheels\n\n2±0.0 2\n2 2±0.01 2±0.0 2\n3±0.0 3\n3 3±0.01 4±0.0 4\n\n(a) Erd ̋os-Rényi graph.\n\n(b) Barabási-Albert graph.\n\nFigure 5: Example covers from our learned heuristic. Nodes with a bold border are in the cover. Numbers indicate the labeling order. Once a cover is found, the order is irrelevant.\n\npicks higher degree, centrally located nodes first. However, if several nodes have the same degree, it favors coloring neighboring nodes subsequently. This happens in the WS graphs, see Figure 4b. The learned heuristic can consistently color the WS graphs with 4 colors, which matches the Smallest-Last heuristic. We conclude that the learned heuristic captures complex aspects of the graph extending beyond simple degree-based decisions and considers the graph’s local neighborhood structure.\n\nRuntime (Figure 6). We compare the runtime of our approach with the classical baselines. As we did not have access to the code of Lemos et al. (2019), we could not compare the runtime directly. Our approach is faster than DSATUR for graphs larger than 640 nodes and scales much better. As expected from §3.3, the runtime of our algorithm grows linearly with the size of the graph, similar to the simpler baselines such as Largest First and Smallest Last, which have better constant factors.\n\n4.2 MINIMUM VERTEX COVER\n\n√\n\nClassic baselines. We compare with two classic algorithms. First, we use the endpoints of a maximal matching, which produces a 2-approximation Papadimitriou & Steiglitz (1982). Second, we compare 2 approximation algorithm for maximum degree ∆. with list-right Delbot & Laforest (2008), a Machine learning baselines. S2V-DQN is a Q-learning based approach (Dai et al., 2017). We use the values reported in the original paper. Li et al. (2018) present a tree-search based approach trained in a supervised way. In contrast to S2V-DQN, it samples multiple solutions, then verifies if they are feasible. The time to construct a feasible solution varies depending on the instance. We use the publicly available code and pretrained model from the authors. We run Li et al.’s code until it finds a feasible solution, and sample more solutions if it is below the time budget of 30 seconds per graph.\n\n2 + 3\n\n∆\n\nResults on in-distribution graphs. We evaluate and compare our approach for MVC with S2VDQN (Dai et al., 2017) and Li et al. (2018) on the same dataset of generated graphs as Dai et al. (2017). It consists of 16,000 graphs from two distributions, Erd ̋os-Rényi (ER) (Erd ̋os & Rényi, 1960) and Barabási-Albert (BA) (Albert & Barabási, 2002), of sizes varying from 20 to 600 nodes. We use the results reported by Dai et al. (2017) on their model trained on 40–50 nodes, except for the graphs with less than 40 nodes, for which no data is available for this model. Hence we use their model trained on 20–40 nodes on these smaller graphs. See Table 2 for the results on ER graphs and §B.3 for results on additional graphs. On ER graphs, our model achieves the closest average approximation ratio, followed by Li et al. (2018). Li et al. (2018) has the lowest average cost. Note that the lowest approximation ratio and lowest cost need not coincide because the cost grows quickly with graph size, whereas the approximation ratio does not. Our model and Li et al. (2018) outperform the greedy baseline, while S2V-DQN is slightly outperformed by List Right. In Table 4, we show how the approximation ratio and cost depend on the instance size. As shown in §B.3, on the BA graphs, our model is about 2.3% away from optimal. The two machine learning baselines are slightly less than 1% away from optimal. The greedy baselines are 9% − 45% away from optimal.\n\nQualitative Results. Figure 5 shows typical results of our learned MVC heuristics. See §E.2 for more examples. On the ER graphs, we can see that the heuristic does not always start with the highest degree node. In contrast, on the BA graphs, the heuristic has a strong preference to start with the highest degree node. Unlike the classic greedy heuristics (and our learned graph coloring heuristic), the learned MVC heuristics seldomly pick neighboring nodes subsequently.\n\n8\n\n0123456789101112131401234567891011121314Under review as a conference paper at ICLR 2023\n\nTable 4: Mean validation cost of our approach for graph coloring on WS graphs and cost and approximation ratio for minimum vertex cover on ER graphs (10 samples). Shaded columns are for graphs larger than during training.\n\nC Nodes G\nCost\n\n20\n\n50 100 200 400 600 3.92 4.00 4.02 4.01 4.04 4.05\n\nFigure 6: Distribution of graph coloring inference runtime on WS graphs.\n\nC Nodes 8\nCost Approx. 1.012\n\n15–20 40–100 100–300 300–500 500–600 540 178.4 1.054 1.048\n\n43.8 1.042\n\n384.9 1.055\n\nV M\n\nRuntime. As shown in §B.5, our approach takes around 0.5 second to find a cover on the test graphs with 1,000 nodes on a CPU per sample, and around 5 seconds for 10 samples. This is comparable to what Dai et al. (2017) reported on a GPU on a similar graph (11 seconds). Note that the time budget for Li et al. (2018) was 30 seconds and the time budget for the combinatorial solver was 1 hour.\n\n4.3 ABLATION STUDIES\n\nSpatial locality. We test the inductive biases we made regarding locality of the decoder by comparing against a decoder variant that never updates the attention weights (static decoding) and a variant that always updates all of the attention weights (global decoding).\n\nStatic decoding never recomputes the attention weights. For node a node i that is not yet labeled, . Static decoding uses O(d2n + m + n2) operations, its weight is: ai = C · tanh which are fewer than those of local update decoding when m (cid:29) d2n. With static decoding, the model is essentially a GNN with a special node-readout function.\n\n(cid:16) (Θ1g0)T (Θ2hi)\n\n(cid:17)\n\n√\n\nd\n\ni = C · tanh\n\nGlobal decoding recomputes the attention weights in each time step t. For a node i that is not yet labeled, its weight is: a(t) . Global decoding uses O(d2n2) operations, which is at least a d2 factor more than local update decoding for not too dense graphs (m (cid:28) n2/d2). When there are only two labels (as for MVC), global decoding is very similar to the Kool et al. (2019) model. The difference to Kool et al. (2019) is that they use additional attention layer to compute a new state embedding in each step.\n\n(cid:16) (Θ1gt)T (Θ2hi)\n\n(cid:17)\n\n√\n\nd\n\nWe train graph coloring models with both static and global decoding on the Lemos et al. (2019) subset of the COLOR challenge graphs (following §4.1). Static and global decoding achieve a mean cost of 10.74±0.12 and 10.71±0.05, respectively, both worse than when using our inductive bias (Table 1).\n\nArchitecture Parameters. We varied the size of the context embedding (i.e., the number of nodes and their labels that contribute to it). Increasing the context size does not significantly improve the test score on graph coloring. For graph coloring, a context of size two and three results in a mean cost of 10.49±0.12 and 10.42±0.12, respectively, for the greedy policy. We varied the number of attention heads (among 1, 2, 4) with a per-head dimension of 16. For graph coloring, this results in a mean validation cost of 5.29±0.02, 4.98±0.01, and 4.95±0.02, respectively. We therefore use 4 attention heads (hidden dimension 64). In §B.4, we provide additional ablation studies for the encoder.\n\n5 CONCLUSION\n\nWe introduced combinatorial node labeling, a framework that generalizes existing approaches to many hard graph problems, and presented a neural network architecture for it, which demonstrates excellent results on both graph coloring and minimum vertex cover problems. This serves as an important step toward replacing hand-crafted heuristics in graph algorithms with learned heuristics tailored to a particular problem and graph structure.\n\nThere are many avenues for future research. While the nodel labeling framework is very general, other graph problems may require adjustments to the neural architecture or inductive biases for good performance. In particular, handling weighted graphs and edge labeling problems would be valuable.\n\n9\n\n320640128025605120Number of nodes0.00.51.01.52.02.5Runtime [s]DSATUR:min: 3.4median: 3.4max: 8.1DSATUR:min: 13.5median: 13.7max: 32.7Our modelDSATURLargest firstSmallest lastUnder review as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nThe combinatorial node labeling framework and our neural network architecture target a broad class of graph problems, and hence are very general-purpose. Downstream tasks of such problems range from compiler passes to logistics optimization to graph data mining. Hence, it is hard to identify specific cases of benefit or harm from our work, as it depends on the specific application of the downstream tasks. We nevertheless urge careful consideration of the implications of improving performance on tasks using our methods, especially ones with privacy implications (e.g., data mining).\n\nREPRODUCIBILITY STATEMENT\n\nWe detail the combinatorial node labeling framework in §2, and describe the neural network architecture and training process we use in §3. We note that the node labeling framework is very general and alternative architectures could be used to solve it. Proofs of our theoretical claims are provided in §C and we give details of our training setup in §4 and §A. We additionally include our source code in the supplementary material to aid reproducibility.\n\nREFERENCES\n\nComputational symposium on graph coloring and generalizations (COLOR02), Ithaca, NY, 7-8 September 2002,\n\n2002. URL https://mat.tepper.cmu.edu/COLOR02/.\n\nFaisal Abu-khzam, Rebecca Collins, Michael Fellows, Michael Langston, W. Suters, and Christopher Symons.\n\nKernelization algorithms for the vertex cover problem: Theory and experiments. pp. 62–69, 01 2004.\n\nRéka Albert and Albert-László Barabási. Statistical mechanics of complex networks. Reviews of Modern Physics, 74(1):47–97, Jan 2002. ISSN 1539-0756. doi: 10.1103/revmodphys.74.47. URL http://dx. doi.org/10.1103/RevModPhys.74.47.\n\nSaeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit-sat: An unsupervised differentiable approach. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum? id=BJxgz2R9t7.\n\nSanjeev Arora, Satish Rao, and Umesh V. Vazirani. Expander flows, geometric embeddings and graph partitioning. J. ACM, 56(2):5:1–5:37, 2009. doi: 10.1145/1502793.1502794. URL https://doi.org/10.1145/ 1502793.1502794.\n\nDavid Avis and Tomokazu Imamura. A list heuristic for vertex cover. Oper. Res. Lett., 35(2):201–204, March 2007. ISSN 0167-6377. doi: 10.1016/j.orl.2006.03.014. URL https://doi.org/10.1016/j.orl. 2006.03.014.\n\nTobias Bandh, Georg Carle, and Henning Sanneck. Graph coloring based physical-cell-id assignment for LTE networks. In Proceedings of the International Conference on Wireless Communications and Mobile Computing: Connecting the World Wirelessly, IWCMC 2009, Leipzig, Germany, June 21-24, 2009, pp. 116– 120, 2009. doi: 10.1145/1582379.1582406. URL https://doi.org/10.1145/1582379.1582406.\n\nThomas D. Barrett, William R. Clements, Jakob N. Foerster, and Alex Lvovsky. Exploratory combinatorial optimization with reinforcement learning. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 3243–3250, 2020. URL https://aaai.org/ojs/index.php/AAAI/ article/view/5723.\n\nIrwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings, 2017. URL https://openreview. net/forum?id=Bk9mxlSFx.\n\nSayan Bhattacharya, Monika Henzinger, and Danupon Nanongkai. Fully dynamic approximate maximum matching and minimum vertex cover in O(log3 n) worst case update time. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January 16-19, pp. 470–489, 2017. doi: 10.1137/1.9781611974782.30. URL https://doi.org/10. 1137/1.9781611974782.30.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nHans L. Bodlaender. Discovering treewidth. In SOFSEM 2005: Theory and Practice of Computer Science, 31st Conference on Current Trends in Theory and Practice of Computer Science, Liptovský Ján, Slovakia, January 22-28, 2005, Proceedings, pp. 1–16, 2005. doi: 10.1007/978-3-540-30577-4\\_1. URL https: //doi.org/10.1007/978-3-540-30577-4_1.\n\nDaniel Brélaz. New methods to color vertices of a graph. Commun. ACM, 22(4):251–256, 1979. doi:\n\n10.1145/359094.359101. URL https://doi.org/10.1145/359094.359101.\n\nQuentin Cappart, Thierry Moisan, Louis-Martin Rousseau, Isabeau Prémont-Schwarz, and André Augusto Ciré. Combining reinforcement learning and constraint programming for combinatorial optimization. CoRR, abs/2006.01610, 2020. URL https://arxiv.org/abs/2006.01610.\n\nGregory J. Chaitin. Register allocation & spilling via graph coloring. In Proceedings of the SIGPLAN ’82 Symposium on Compiler Construction, Boston, Massachusetts, USA, June 23-25, 1982, pp. 98–105, 1982. doi: 10.1145/800230.806984. URL https://doi.org/10.1145/800230.806984.\n\nLenore J. Cowen, Robert Cowen, and Douglas R. Woodall. Defective colorings of graphs in surfaces: Partitions into subgraphs of bounded valency. J. Graph Theory, 10(2):187–195, 1986. doi: 10.1002/jgt.3190100207. URL https://doi.org/10.1002/jgt.3190100207.\n\nHanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. CoRR, abs/1704.01665, 2017. URL http://arxiv.org/abs/1704.01665.\n\nGeorge B. Dantzig, D. Ray Fulkerson, and Selmer M. Johnson. Solution of a large-scale traveling-salesman problem. Oper. Res., 2(4):393–410, 1954. doi: 10.1287/opre.2.4.393. URL https://doi.org/10. 1287/opre.2.4.393.\n\nFrançois Delbot and Christian Laforest. A better list heuristic for vertex cover. Inf. Process. Lett., 107(3-4): 125–127, 2008. doi: 10.1016/j.ipl.2008.02.004. URL https://doi.org/10.1016/j.ipl.2008. 02.004.\n\nIddo Drori, Anant Kharkar, William R. Sickinger, Brandon Kates, Qiang Ma, Suwen Ge, Eden Dolev, Brenda Dietrich, David P. Williamson, and Madeleine Udell. Learning to solve combinatorial optimization problems on real-world graphs in linear time. In 19th IEEE International Conference on Machine Learning and Applications, ICMLA 2020, Miami, FL, USA, December 14-17, 2020, pp. 19–24, 2020. doi: 10.1109/ ICMLA51294.2020.00013. URL https://doi.org/10.1109/ICMLA51294.2020.00013.\n\nPaul Erd ̋os and Alfréd Rényi. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci, 5(1):17–60,\n\n1960.\n\nMichael R. Garey and David S. Johnson. Computers and Intractability; A Guide to the Theory of NP-\n\nCompleteness. W. H. Freeman & Co., USA, 1990. ISBN 0716710455.\n\nMohsen Ghaffari, Ce Jin, and Daan Nilis. A massively parallel algorithm for minimum weight vertex cover. In SPAA ’20: 32nd ACM Symposium on Parallelism in Algorithms and Architectures, Virtual Event, USA, July 15-17, 2020, pp. 259–268, 2020. doi: 10.1145/3350755.3400260. URL https://doi.org/10.1145/ 3350755.3400260.\n\nJack E. Graver. On the foundations of linear and integer linear programming I. Math. Program., 9(1):207–226,\n\n1975. doi: 10.1007/BF01681344. URL https://doi.org/10.1007/BF01681344.\n\nAric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. Exploring network structure, dynamics, and function using networkx. In Gaël Varoquaux, Travis Vaught, and Jarrod Millman (eds.), Proceedings of the 7th Python in Science Conference, pp. 11 – 15, Pasadena, CA USA, 2008.\n\nFrank Harary and Robert A. Melter. On the metric dimension of a graph. Ars Combinatoria, 2(191-195):1, 1976.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/ CVPR.2016.90.\n\nStephen T. Hedetniemi and Renu C. Laskar. Bibliography on domination in graphs and some basic definitions of domination parameters. Discret. Math., 86(1-3):257–277, 1990. doi: 10.1016/0012-365X(90)90365-O. URL https://doi.org/10.1016/0012-365X(90)90365-O.\n\nJiayi Huang, Md. Mostofa Ali Patwary, and Gregory F. Diamos. Coloring big graphs with alphagozero. CoRR,\n\nabs/1902.10162, 2019. URL http://arxiv.org/abs/1902.10162.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 448–456, 2015. URL http://proceedings.mlr.press/ v37/ioffe15.html.\n\nT.R. Jensen, T.R. Jensen, and B. Toft. Graph Coloring Problems. A Wiley interscience publication. Wiley, 1995.\n\nISBN 9780471028659. URL https://books.google.ch/books?id=YfZQAAAAMAAJ.\n\nChaitanya K. Joshi, Thomas Laurent, and Xavier Bresson. An efficient graph convolutional network technique for the travelling salesman problem. CoRR, abs/1906.01227, 2019. URL http://arxiv.org/abs/ 1906.01227.\n\nNikolaos Karalias and Andreas Loukas. Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 49f85a9ed090b20c8bed85a5923c669f-Abstract.html.\n\nDavid R. Karger and Clifford Stein. A new approach to the minimum cut problem. J. ACM, 43(4):601–640,\n\n1996. doi: 10.1145/234533.234534. URL https://doi.org/10.1145/234533.234534.\n\nDavid R. Karger, Rajeev Motwani, and G. D. S. Ramkumar. On approximating the longest path in a graph. Algorithmica, 18(1):82–98, 1997. doi: 10.1007/BF02523689. URL https://doi.org/10.1007/ BF02523689.\n\nR. Karp. Reducibility among combinatorial problems. In R. Miller and J. Thatcher (eds.), Complexity of\n\nComputer Computations, pp. 85–103. Plenum Press, 1972.\n\nBrian W. Kernighan and Shen Lin. An efficient heuristic procedure for partitioning graphs. Bell Syst. Tech. J., 49(2):291–307, 1970. doi: 10.1002/j.1538-7305.1970.tb01770.x. URL https://doi.org/10.1002/ j.1538-7305.1970.tb01770.x.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n\nIn 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\n\nWouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In 7th International\n\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n\nJohn Boaz Lee, Ryan A. Rossi, Sungchul Kim, Nesreen K. Ahmed, and Eunyee Koh. Attention models in graphs: A survey. ACM Trans. Knowl. Discov. Data, 13(6):62:1–62:25, 2019. doi: 10.1145/3363574. URL https://doi.org/10.1145/3363574.\n\nHenrique Lemos, Marcelo O. R. Prates, Pedro H. C. Avelar, and Luís C. Lamb. Graph colouring meets deep learning: Effective graph neural network models for combinatorial problems. CoRR, abs/1903.04598, 2019. URL http://arxiv.org/abs/1903.04598.\n\nZhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pp. 537–546, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 8d3bba7425e7c98c50f52ca1b52d3735-Abstract.html.\n\nQiang Ma, Suwen Ge, Danyang He, Darshan Thaker, and Iddo Drori. Combinatorial optimization by graph pointer networks and hierarchical reinforcement learning. In AAAI Workshop on Deep Learning on Graphs: Methodologies and Applications, 2020.\n\nAndrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic\n\nmodels. In Proc. icml, volume 30, pp. 3. Citeseer, 2013.\n\nSahil Manchanda, Akash Mittal, Anuj Dhawan, Sourav Medya, Sayan Ranu, and Ambuj Singh. GCOMB: learning budget-constrained combinatorial algorithms over billion-sized graphs. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/ paper/2020/hash/e7532dbeff7ef901f2e70daacb3f452d-Abstract.html.\n\nD. Marx. Graph colouring problems and their applications in scheduling. Periodica Polytechnica Electrical\n\nEngineering, 48:11–16, 2004.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nDavid W. Matula and Leland L. Beck. Smallest-last ordering and clustering and graph coloring algorithms. J. ACM, 30(3):417–427, 1983. doi: 10.1145/2402.322385. URL https://doi.org/10.1145/2402. 322385.\n\nPawel B. Myszkowski. Solving scheduling problems by evolutionary algorithms for graph coloring problem. In Metaheuristics for Scheduling in Industrial and Manufacturing Applications, pp. 145–167. 2008. doi: 10.1007/978-3-540-78985-7\\_7. URL https://doi.org/10.1007/978-3-540-78985-7_7.\n\nAzade Nazi, Will Hang, Anna Goldie, Sujith Ravi, and Azalia Mirhoseini. GAP: generalizable approximate graph partitioning framework. CoRR, abs/1903.00614, 2019. URL http://arxiv.org/abs/1903.00614.\n\nKrzysztof Onak, Dana Ron, Michal Rosen, and Ronitt Rubinfeld. A near-optimal sublinear-time algorithm for approximating the minimum vertex cover size. In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2012, Kyoto, Japan, January 17-19, 2012, pp. 1123–1131, 2012. doi: 10.1137/1.9781611973099.88. URL https://doi.org/10.1137/1.9781611973099.88.\n\nChristos Papadimitriou and Kenneth Steiglitz. Combinatorial Optimization: Algorithms and Complexity,\n\nvolume 32. 01 1982. ISBN 0-13-152462-3. doi: 10.1109/TASSP.1984.1164450.\n\nJ. M. Robson. Algorithms for maximum independent sets. J. Algorithms, 7(3):425–440, 1986. doi: 10.1016/\n\n0196-6774(86)90032-5. URL https://doi.org/10.1016/0196-6774(86)90032-5.\n\nMichael D. Smith, Norman Ramsey, and Glenn H. Holloway. A generalized algorithm for graph-coloring register allocation. In Proceedings of the ACM SIGPLAN 2004 Conference on Programming Language Design and Implementation 2004, Washington, DC, USA, June 9-11, 2004, pp. 277–288, 2004. doi: 10.1145/996841. 996875. URL https://doi.org/10.1145/996841.996875.\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford Book,\n\nCambridge, MA, USA, 2018. ISBN 0262039249.\n\nRobert Endre Tarjan and Anthony E. Trojanowski. Finding a maximum independent set. SIAM J. Comput., 6(3):\n\n537–546, 1977. doi: 10.1137/0206038. URL https://doi.org/10.1137/0206038.\n\nEtsuji Tomita and Tomokazu Seki. An efficient branch-and-bound algorithm for finding a maximum clique. In Discrete Mathematics and Theoretical Computer Science, 4th International Conference, DMTCS 2003, Dijon, France, July 7-12, 2003. Proceedings, pp. 278–289, 2003. doi: 10.1007/3-540-45066-1\\_22. URL https://doi.org/10.1007/3-540-45066-1_22.\n\nJan Tönshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. RUN-CSP: unsupervised learning of message passing networks for binary constraint satisfaction problems. CoRR, abs/1909.08387, 2019. URL http: //arxiv.org/abs/1909.08387.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. In 6th International Conference on Learning Representations, ICLR 2018, Graph attention networks. Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL https: //openreview.net/forum?id=rJXMpikCZ.\n\nDuncan J. Watts and Steven H. Strogatz. Collective dynamics of ’small-world’ networks. Nature, 393(6684):440– 442, June 1998. ISSN 0028-0836. doi: 10.1038/30918. URL http://dx.doi.org/10.1038/30918.\n\nWeichi Yao, Afonso S. Bandeira, and Soledad Villar. Experimental performance of graph neural networks on random instances of max-cut. CoRR, abs/1908.05767, 2019. URL http://arxiv.org/abs/1908. 05767.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA TRAINING\n\nA.1 DATA GENERATION\n\nWe use four different synthetic graph distributions to generate instances for training and validation. All graphs are generated via the Python NETWORKX library (Hagberg et al., 2008).\n\nBarabási-Albert Model (Albert & Barabási, 2002) The Barabási-Albert (BA) Model generates random scale-free networks. Similar to real-world networks BA graphs grow by preferential attachment, i.e., a new node is more likely to link to more connected nodes. The model is parameterized by one parameter δ, which dictates the average degree.\n\nErd ̋os-Rényi Model (Erd ̋os & Rényi, 1960) An Erd ̋os-Rényi (ER) graph G(n, p) has n nodes and each edge exists independently with probability p. The expected number of edges is (cid:0)n Watts-Strogatz Model (Watts & Strogatz, 1998) Watts-Strogatz (WS) graphs were developed to overcome the shortcomings of ER graphs when modeling real world graphs. In real networks we see the formation of local clusters, i.e., the neighbors of a node are more likely to be neighbors. For parameters k and q, a WS graph is built as follows: build a ring of n nodes. Next, connect each node to its k nearest neighbors. Finally, replace each edge {u, v} by a new edge {u, w} (chosen uniformly at random) with probability q.\n\n(cid:1)p.\n\n2\n\nA.1.1 TRAINING SET PARAMETERS\n\nSee Table 5 for the parameters of the graph distributions used during training. Note that for BA and ER graphs, the parameters match those used in the Dai et al. (2017) test set (see Table 2 and Table 10). We also consider sparse ER graphs (S-ER), for we set the edge probability such that graphs have expected average degree close to ∆ = 7.5 when n is small but remain connected with high probability when n is large. This means that\n\n(cid:18)\n\nps−er = min\n\n1, max\n\n(cid:18) ∆ n\n\n, (1 + (cid:15))\n\nln n n\n\n(cid:19)(cid:19)\n\n,\n\n(1)\n\nfor a small (cid:15), which we set to 0.2 in our experiments. The formula is derived from the connectivity threshold of ER graphs (Erd ̋os & Rényi, 1960).\n\nFor graph coloring, we train on a hybrid dataset consisting of an equal proportion of BA, S-ER, and WS graphs. For minimum vertex cut, we train on a dataset consisting of BA graphs, a dataset consisting of ER graphs, and a hybrid dataset consisting on a combination of the two (in equal proportion). We use the in-distribution models for the evaluation on the synthetic test instances and the hybrid model for the memetracker graph. During training, we use an equal proportion of graphs with n ∈ {20, 40, 50, 70, 100} nodes.\n\nthe results on simple graphs\n\n∈ For {10, . . . , 51, 60, 61, 70, 71, 80, 81, 90, 91, 99, 100, 200, 201, 300, 301, 400, 401} and validate on 1000 or 1001 nodes.\n\nin Table 3, we use graphs with sizes n\n\nA.2 POLICY OPTIMIZATION\n\nWe train our model with REINFORCE with a greedy rollout baseline Kool et al. (2019). The details follow. We denote the cost of labeling the graph Gi in the order given by the sequence of nodes π by L(π, Gi). A model M is parameterized by parameters θ. On a graph Gi, the model returns a sequence of nodes π and an associated probability pθ. The probability pθ is the product of all action probabilities that led to the sequence of nodes π. We write pθ, π ← Mθ(Gi) when the policy is evaluated deterministically and pθ, π ∼ Mθ(Gi) when the policy is evaluated probabilistically.\n\nTable 5: The graph parameters for training and validation.\n\nBA\n\nER\n\nS-ER\n\nδ = 4\n\np = 0.15\n\np = ps−er\n\nWS\n\nk = 5, q = 0.1\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Policy Training with Reinforce+Baseline\n\nθ\n\n1: Input: number of epochs E, batch size B, datasets Dtrain 2: Initialize model Mθ and baseline model M BL 3: Dchallenge ← sample new challenge dataset 4: for epoch = 1, . . . , E do for batch in Dtrain do 5: 6: 7:\n\ni ← MθBL(Gi) for Gi in batch ]\n\n[ pθ,i, πi ∼ Mθ(Gi) for Gi in batch ] [ pBL θ,i , πBL ∇θJ(θ) = 1 B\nθ ← ADAM(θ, ∇θJ(θ))\n\ni=1(L(πi | Gi) − L(πBL\n\n(cid:80)B\n\ni\n\nend for if OneSidedPairedTTest(Mθ, M BL\n\nθ\n\nθBL ← θ Dchallenge ← sample new challenge dataset\n\n8: 9: 10: 11: 12: 13: 14: 15: end for\n\nend if\n\n(cid:46) Sample from policy (cid:46) Greedy baseline\n\n| Gi)) ∇θ log(pθ,i)\n\n(cid:46) Policy Gradient\n\n, Dchallenge) < 0.05 then\n\n(cid:46) Challenge the baseline\n\nThe complete training procedure is given in Algorithm 1. Algorithm 1 follows from the textbook REINFORCE with a baseline (Sutton & Barto, 2018) by factoring the probability of reaching a terminal state and using that the rewards are 0 in our MDP except when reaching a terminal state. Unlike Kool et al. (2019), we do not use warmup epochs where training starts out with an exponential moving average baseline.\n\nB ADDITIONAL RESULTS\n\nB.1 VALIDATION RESULTS\n\nWe compare the cost of the learned heuristic for different parameters of the training. The validation set consists of 600 graphs with n nodes for n ∈ {20, 50, 100, 200, 400, 600}.\n\nB.1.1 GRAPH COLORING\n\nTable 6 shows the validation cost on the three training distribution for all evaluated configurations.\n\nWith a larger learning rate of α = 10−3, the mean validation cost for graph coloring is significantly worse, namely 5.22±0.002. A smaller learning rate of α = 10−5 leads to a mean validation cost of 5.02±0.001, which is slightly worse than the cost of 4.95±0.02 for α = 10−4.\n\nB.1.2 MINIMUM VERTEX COVER\n\nTable 7 shows the validation results for training on either only one distribution and evaluating on ER and BA graphs. Training on a mixture ER and BA graphs leads to worse validation cost on BA graphs compared to training only on BA graphs. Training on ER graphs exclusively without BA graphs leads to a slight cost improvement on ER graphs.\n\nB.2 RESULTS BY SIZE\n\nTable 8 shows how the cost and approximation ratio of our MVC approach varies with the instance size (on the ER test graphs). Although the approximation ratio grows slightly with instance size, it remains within ca 5.5% of optimal for graphs with 500-600 nodes.\n\nTable 9 shows how the cost of GC varies on two synthetic distributions of graphs, S-ER and BA. For BA and ER graphs, the cost grows by about one color on the larger graphs.\n\nB.3 ADDITIONAL RESULTS FOR MVC\n\nResults on BA graphs See Table 10 for the MVC results on BA graphs.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Ablation studies for graph coloring. decoders, respectively. Prev. ding, Norm.\n\nL, S, and G are short for local, static, and global denotes the number of previously labeled nodes in the state embedindicates if shortcuts are used.\n\nindicates if batch-normalization is used, and Short.\n\nEncoder\n\nDecoder\n\nValidation cost on distribution\n\nLayers Norm. Short. Type Prev. Heads S-ER\n\nWS\n\nBA\n\nAll\n\n3 2\n1 3\n3 3\n3 3\n3 3\n3\n\n(cid:88) (cid:88) (cid:88) (cid:55) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\n\n(cid:88) (cid:88) (cid:88) (cid:55) (cid:55) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\n\nL L\nL L\nL S\nG L\nL L\nL\n\n1 1\n1 1\n1 1\n1 2\n3 1\n1\n\n4 4\n4 4\n4 4\n4 4\n4 2\n1\n\n5.32±0.05 5.34±0.04 5.4±0.05 5.56±0.02 5.56±0.05 5.59±0.04 5.58±0.02 5.37±0.03 5.36±0.02 5.38±0.03 5.58±0.02\n\n4.01±0.00 4.01±0.01 4.09±0.01 4.29±±0.09 4.12±0.05 4.14±0.00 4.15±0.01 4.02±0.01 4.02±0.01 4.02±0.01 4.01±0.01\n\n5.50±0.04 5.57±0.02 5.55±0.05 6.09±0.04 5.92±0.04 5.52±0.06 5.54±0.03 5.49±0.04 5.48±0.07 5.54±0.03 6.26±0.05\n\n4.95±0.02 4.98±0.02 5.02±0.03 5.32±0.05 5.22±0.04 5.09±0.04 5.1±0.03 4.97±0.03 4.96±0.02 4.98±0.01 5.28±0.02\n\nTable 7: MVC Validation Cost and ing\n\nvalidation\n\ntraining\n\nfor varydistributions.\n\nTrain Graphs Cost ER\n\nCost BA\n\nER BA ER+BA\n\n223.56±0.11 218.07±4.36 223.76±0.43 199.44±12.02 223.98±0.26 208.18±6.24\n\nTable 8: Cost and approximation ratio of our approach for MVC on BA graphs by instance size. Shaded columns are for test instances larger than the training graphs. Approx. denotes the approximation ratio.\n\nNodes 15-20 40-100 100-300 300-500 500-600 113.56 225.16 309.02 Cost 10.53 35.08 1.027 1.025 1.022 Approx. 1.009 1.016\n\nTable 9: Mean validation cost of our greedy approach for GC on S-ER and BA graphs by instance size. Shaded columns are for instances larger than those seen during training.\n\nS-ER Graphs\n\nBA Graphs\n\nNodes 20 Cost\n\n200 400 600 5.30 5.18 5.18 5.28 5.39 5.96\n\n100\n\n50\n\nNodes 20 Cost\n\n200 400 600 4.83 5.08 5.34 5.61 5.96 6.18\n\n100\n\n50\n\nTable 10: Comparison of MVC approaches on BA graphs with average degree 4.\n\n. s\ns a\nl\n\nC\n\nL M\n\nName\n\nMaximal Matching List Right\n\nLi et al. (2018) S2V-DQN Ours - Greedy Ours - 10 Samples\n\nCost\n\n190.84 143.50\n\n131.62 N/A 133.78±0.07 133.39±0.05\n\nApprox. Ratio\n\n1.4516 1.0984\n\n1.0084 1.0099 1.0234 1.0202\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\n(a) GC on S-ER graphs: R2 = 95.2%, S = 1.12 · 10−5s\n\n(b) GC on WS graphs: R2 = 96.0%, S = 1.67 · 10−6s\n\n(c) MVC on S-ER graphs: R2 = 94.4%, S = 3.4 · 10−6s.\n\n(d) MVC on BA graphs: R2 = 94.4%, S = 1.68 · 10−6s\n\nFigure 7: Inference runtime with local decoding. The solid line indicates a linear least squares fit, the dashed orange line the mean. We report the coefficient of determination R2 and standard errror S.\n\nB.4 ABLATION STUDIES FOR THE ENCODER\n\nWe varied the number of layers in the encoder, removed the shortcut connections, and removed the normalization. The results are summarized in Table 6. We can see that removing the shortcut connections has a very strong detrimental effect on the validation cost. Removing both shortcuts and normalization deteriorates the cost further. Using 2 layers results in a small, but noticeable increase in cost, whereas a single layer has a significantly worse cost.\n\nB.5 RUNTIME SCALABILITY\n\nWe evaluated the inference runtime on a c2d-highmem-4 (4 vCPUs, 32 GB RAM) Google Cloud machine (Environment M94 with PyTorch 1.11). Figure 7 show the runtime scaling of our approach on GC and MVC together with a linear least squares fit. In Figure 7b, we see that for graphs up to around 5000 vertices the runtime of GC inference very closely follows a linear trend. For larger graphs, the runtime grows slightly faster than linear, as shown in Figure 7a. Similar results hold for MVC: it takes less than 0.5 seconds to compute a vertex cover for a graph with 1,000 nodes. Figure 7d and Figure 7c show the distribution of MVC runtimes of up to 5120 nodes.\n\nB.6 DISCUSSION.\n\nIt is not surprising that our machine learning approach performs best on in-distribution graphs. Whilst it is desirable to have an approach that generalizes well, if a representative sample of graphs is available for a target applications, this does not pose an issue. Moreover, we have shown that the quality degrades gradually when the test distribution differs from the training distribution.\n\n17\n\n02500500075001000012500150001750020000Number of nodes02468101214Runtime [s]10002000300040005000Number of nodes0.00.51.01.52.02.5Runtime [s]010002000300040005000Number of nodes0.00.51.01.52.02.5Runtime [s]010002000300040005000Number of nodes0.00.51.01.52.02.5Runtime [s]Under review as a conference paper at ICLR 2023\n\nC ADDITIONAL PROOFS\n\nC.1 THE NODE LABELING MDP\n\nProof of Lemma 2.4. Consider a sequence of actions (v1, (cid:96)1), . . . , (vn, (cid:96)n) ending in a terminal state. For all t, the prefix (v1, (cid:96)1), . . . , (vt, (cid:96)t) of this sequence corresponds to a partial node labeling c(cid:48) (by viewing the sequence of node-label pairs as describing a function from nodes to labels). By construction of the MDP, labeling node vi+1 with (cid:96)t+1 passes the extensibility test for c(cid:48). Hence the node labeling c represented by (v1, (cid:96)1), . . . , (vn, (cid:96)n) is feasible. By construction, the return of the episode is −f (c), where f (c) is the cost of node labeling c.\n\nConversely, consider a feasible solution c with cost f (c). Then, by definition of feasibility (§4.3), there is a sequence (v1, (cid:96)1), . . . , (vn, (cid:96)n) of node-label pairs such that for all t ≥ 0 the partial node labeling given by (v1, (cid:96)1), . . . , (vt, (cid:96)t) passes the extensibility test for node vt+1 and label (cid:96)t+1. Hence, the sequence of node-label pairs is also a sequence of actions in the MDP leading to a terminal state. The return for this episode is −f (c).\n\nNote that since our tasks are episodic, the return equals the sum of the rewards (specifically the reward received in the terminal state). In particular, we do not use discounting.\n\nC.2 OPTIMALITY OF THE LABELING RULE\n\nProof of Lemma 3.1. Let G be some graph with chromatic number χ(G) = k and c∗ be a mapping that colors G optimally. We partition V into color classes Ci = {v | c∗(v) = i} such that all nodes with color i are in Ci. Now, we build an ordering by consecutively taking all nodes from C1, then all nodes from C2 and so on. Choosing the smallest color that passes the extensibility test will produce an optimal coloring for such an order of nodes: The proof is by strong induction on the index of the color class i. The induction hypothesis H(i) is that for all nodes v in Cj for j < i, v is colored with a color in {1, . . . , j}. Assume the induction hypothesis H(i) holds. Now, consider a node v in Ci. The color i must be a valid color for v: First, assigning color i does not produce any conflicts with any node u in Cj for j < i because by induction hypothesis node u has a color strictly less than i. Second, assigning color i to v does not produce a conflict with another node w in Ci because then Ci would not be a valid class of colors (nodes in a color class cannot be neighbors.). As we choose the smallest valid color and i is valid, v get a color in {1, . . . i}. Thus, H(i + 1) holds.\n\nNote that this coloring might be different from the one of c∗. This is, because a node in Ci might have no conflicts with some color j < i and therefore this node will be assigned color j.\n\nProof of Lemma 3.2. Let S be the set over nodes with label 1 in a minimum vertex cover of G. Order these nodes first (in an arbitrary relative order), then order the remaining nodes in V − S after these nodes (in an arbitrary relative order). Now, label the nodes with 1 in this order until every edge has an endpoint with label 1. After |S| steps, every node in S has label 1, meaning that the nodes with label 1 form a minimum vertex cover: If the nodes formed a vertex cover after less than |S| steps, we would find a smaller vertex cover, contradicting the minimality of S.\n\nD LIST OF COMBINATORIAL NODE LABELING PROBLEMS\n\nWe provide an extensive list of classic graph optimization problems framed as node labeling problems. Note that there can be multiple equivalent formulations. For some problems, we consider a weighed graph G with weight function w : E (cid:55)→ R+, we write w(u, v) the weight of an edge {u, v}. For a set of nodes S, we denote the subgraph of G induced by S with G[S].\n\nThe problems in Table 11 require a partition of the nodes as their solution. These can be represented as node labeling problems by giving each partition its unique label. For many of the problems, the number of used labels determines the cost function.\n\nThe problems in Table 12 require a path (or a sequences of nodes) as their solution, which we represent as node labeling problems by having the label indicate the position in the path (or sequence).\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 11: Node labeling problems which partition the nodes into 2 or more sets.\n\nProblem\n\nExtensibility Test T (V (cid:48) × L, v, l) Cost function f\n\nBalanced k-partition (Kernighan & Lin, 1970)\n\nBalanced k, 1 + (cid:15) vpartition (Kernighan & Lin, 1970)\n\nMinimum k-cut (Karger & Stein, 1996)\n\nClique cover (Karp, 1972)\n\nk (cid:101)\n\nThere are no more than (cid:100) n nodes with the same label and at most k labels. There are no more than (cid:100) n(1+(cid:15)) nodes with the same label and at most k labels. k − |V | − |V (cid:48)| − 1 ≤ |L ∪ {v}| and |L ∪ {v}| ≤ k\n\nk\n\n(cid:101)\n\n(cid:80)\n\n(cid:80)\n\n(cid:80)\n\n{u,v}∈E,l(u)(cid:54)=l(v) w(u, v)\n\n{u,v}∈E,l(u)(cid:54)=l(v) w(u, v)\n\n{u,v}∈E,l(u)(cid:54)=l(v) w(u, v)\n\nEvery label induces a clique\n\nNumber of labels\n\nDomatic number (Hedetniemi & Laskar, 1990)\n\nEvery label induces a dominating set of G[V (cid:48) ∪ {v}]\n\nNegative number of labels\n\nGraph coloring (Jensen et al., 1995)\n\nGraph co-coloring (Jensen et al., 1995)\n\nk-defective coloring (Cowen et al., 1986)\n\nNo neighbor of v has label l\n\nNumber of labels\n\nThe nodes with label l induce an independent set in G or the complement of G\n\nNo node has more than k neighbors with label l\n\nNumber of labels\n\nNumber of labels\n\nTable 12: Node labeling problems where the labels encode a permutation of nodes.\n\nProblem\n\nExtensibility Test T (V (cid:48) × L, v, l) Cost function f\n\nTraveling salesman problem (Dantzig et al., 1954)\n\nl = max(L) + 1 and v is a neighbor of the node in L with label max(L)\n\nTree decomposition (Bodlaender, 2005)\n\nl = max(L) + 1\n\nLongest path (Karger et al., 1997)\n\nl = max(L) + 1\n\n(cid:80)\n\n(u,v)∈E,l(v)=l(u)+1 w(u, v)\n\nFor a node vi with label i, add edges to G until vi forms a clique with its higher-labeled neighbors. The cost is the largest number of higher-labeled neighbors in the augmented graph (Bodlaender, 2005).\n\nMaximum number of nodes with consecutive labels that induce a path\n\nThe problems in Table 13 require a set of nodes as their solution. These can be represented as node labeling problems by giving the nodes in the solution set the label 1 and the nodes not in the solution set the label 0. The cost function is closely related to the number of nodes with label 1 for most of these problems.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nTable 13: Node labeling problems with binary labels. For all these problems, the extensibility test passes only if the label is 0 or 1 (and the additional requirements listed below are satisfied).\n\nProblem\n\nMaximum cut (Karp, 1972)\n\nSparsest cut (Arora et al., 2009)\n\nExtensibility Test T (V (cid:48) × L, v, l) Cost function f\n\nAt least one node has label 1\n\n−|{{u, v} ∈ E, l(u) (cid:54)= l(v)}|\n\nAt least one node has label 1\n\n|{{u,v}∈E, l(u)(cid:54)=l(v)}| |{v∈V, l(v)=1}|\n\nMaximum independent set (Tarjan & Trojanowski, 1977) (Robson, 1986)\n\nThe subgraph induced by the nodes with label 1 is an independent set\n\nMinimum vertex cover (node cover) (Karp, 1972)\n\nMaximum clique (Tomita & Seki, 2003)\n\nMinimum feedback node set (Karp, 1972)\n\nMetric dimension (Harary & Melter, 1976)\n\nThe subgraph induced by the nodes with label 1 is a vertex cover of G[V (cid:48) ∪ {v}]\n\nThe subgraph induced by the nodes with label 1 is a clique\n\nG[{u ∈ V (cid:48) ∪ {v}, l(u) = 0}] is a forest\n\nThe nodes in V (cid:48) ∪ {v} are uniquely identified by their distances to nodes with label 1\n\n−|{v ∈ V, l(v) = 1}|\n\n|{v ∈ V, l(v) = 1}|\n\n−|{v ∈ V, l(v) = 1}|\n\n|{v ∈ V, l(v) = 1}|\n\n|{v ∈ V, l(v) = 1}|\n\nMinimum dominating set (Hedetniemi & Laskar, 1990)\n\nThe nodes with label 1 form a dominating set of G[V (cid:48) ∪ {v}]\n\n|{v ∈ V, l(v) = 1}|\n\nMinimum connected dominating set (Hedetniemi & Laskar, 1990)\n\nThe nodes with label 1 form a connected dominating set of G[V (cid:48) ∪ {v}]\n\n|{v ∈ V, l(v) = 1}|\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Erd ̋os-Rényi graph, n = 16.\n\n(b) Erd ̋os-Rényi graph, n = 16.\n\n(c) Erd ̋os-Rényi graph, n = 16.\n\n(d) Erd ̋os-Rényi graph, n = 16.\n\n(e) Erd ̋os-Rényi graph, n = 16.\n\n(f) Erd ̋os-Rényi graph, n = 16.\n\n(g) Watts-Strogatz graph, n = 16.\n\n(h) Watts-Strogatz graph, n = 16.\n\n(i) Watts-Strogatz graph, n = 16.\n\n(j) Watts-Strogatz graph, n = 16.\n\n(k) Watts-Strogatz graph, n = 16.\n\n(l) Watts-Strogatz graph, n = 16.\n\nFigure 8: Example colorings produced by our learned heuristic. Node borders indicate the colors. Numbers on the nodes indicate the order in which the heuristic labels them.\n\nE ADDITIONAL EXAMPLES\n\nE.1 GRAPH COLORING\n\nFigure 8 and Figure 9 show additional results of our learned coloring heuristic on in-distribution graphs. For the Erd ̋os-Rényi graphs and Watts-Strogatz, we provide some examples in a circular layout and some with a force-directed layout. The force-directed layout emphasizes the structure of the graph, but for these two graph classes leads to many crossing edges. See Figure 10 and Figure 11 for results on cycles, wheels, and trees. Interestingly, the heuristic picks the highest degree node in a star or wheel sometimes first and sometimes last.\n\n21\n\n012345678910111213141501234567891011121314150123456789101112131415012345678910111213141501234567891011121314150123456789101112131415012345678910111213141501234567891011121314150123456789101112131415012345678910111213141501234567891011121314150123456789101112131415Under review as a conference paper at ICLR 2023\n\n(a) Barabási-Albert graph, n = 16.\n\n(b) Barabási-Albert graph, n = 16.\n\n(c) Barabási-Albert graph, n = 16.\n\n(d) Barabási-Albert graph, n = 16.\n\n(e) Barabási-Albert graph, n = 16.\n\n(f) Barabási-Albert graph, n = 16.\n\n(g) Erd ̋os-Rényi graph, n = 20.\n\n(h) Watts-Strogatz graph, n = 20.\n\n(i) Barabási-Albert graph, n = 20.\n\n(j) Erd ̋os-Rényi graph, n = 20.\n\n(k) Watts-Strogatz graph, n = 20.\n\n(l) Barabási-Albert graph, n = 20.\n\nFigure 9: Example colorings produced by our learned heuristic. Node borders indicate the colors. Numbers on the nodes indicate the order in which the heuristic labels them.\n\n22\n\n012345678910111213141501234567891011121314150123456789101112131415012345678910111213141501234567891011121314150123456789101112131415012345678910111213141516171819012345678910111213141516171819012345678910111213141516171819012345678910111213141516171819012345678910111213141516171819012345678910111213141516171819Under review as a conference paper at ICLR 2023\n\n(a) Cycle graph, n = 12.\n\n(b) Cycle graph, n = 16.\n\n(c) Cycle graph, n = 28.\n\n(d) Cycle graph, n = 13.\n\n(e) Cycle graph, n = 17.\n\n(f) Cycle graph, n = 29.\n\n(g) Wheel graph, n = 12.\n\n(h) Wheel graph, n = 16.\n\n(i) Wheel graph, n = 28.\n\n(j) Wheel graph, n = 13.\n\n(k) Wheel graph, n = 17.\n\n(l) Wheel graph, n = 29.\n\nFigure 10: Example colorings produced by our learned heuristic on cycles and wheels. The learned coloring heuristic visits nodes on the cycles in-order. For the wheels, the center of the wheel is either visited first or last.\n\n23\n\n0123456789101101234567891011121314150123456789101112131415161718192021222324252627012345678910111201234567891011121314151601234567891011121314151617181920212223242526272801234567891011012345678910111213141501234567891011121314151617181920212223242526270123456789101112012345678910111213141516012345678910111213141516171819202122232425262728Under review as a conference paper at ICLR 2023\n\n(a) Star graph, n = 12.\n\n(b) Star graph, n = 16.\n\n(c) Star graph, n = 28.\n\n(d) Star graph, n = 13.\n\n(e) Star graph, n = 17.\n\n(f) Star graph, n = 29.\n\n(g) Tree graph, n = 12.\n\n(h) Tree graph, n = 16.\n\n(i) Tree graph, n = 28.\n\n(j) Tree graph, n = 13.\n\n(k) Tree graph, n = 17.\n\n(l) Tree graph, n = 29.\n\nFigure 11: Example colorings produced by our learned heuristic on stars and random trees. For stars, the heuristic either labels the center first or last. The tree heuristic prefers to start coloring at one of the leaves of the tree and then colors nodes in a search pattern from there, coloring nodes that are neighbors of already colored nodes. It often labels the remaining leaves very late into the coloring.\n\n24\n\n0123456789101101234567891011121314150123456789101112131415161718192021222324252627012345678910111201234567891011121314151601234567891011121314151617181920212223242526272801234567891011012345678910111213141501234567891011121314151617181920212223242526270123456789101112012345678910111213141516012345678910111213141516171819202122232425262728Under review as a conference paper at ICLR 2023\n\n(a) Erd ̋os-Rényi graph, n = 16.\n\n(b) Erd ̋os-Rényi graph, n = 17.\n\n(c) Erd ̋os-Rényi graph, n = 18.\n\n(d) Erd ̋os-Rényi graph, n = 20.\n\n(e) Erd ̋os-Rényi graph, n = 41.\n\n(f) Erd ̋os-Rényi graph, n = 42.\n\n(g) Barabási-Albert graph, n = 16.\n\n(h) Barabási-Albert graph, n = 17.\n\n(i) Barabási-Albert graph, n = 18.\n\n(j) Barabási-Albert graph, n = 20.\n\n(k) Barabási-Albert graph, n = 41.\n\n(l) Barabási-Albert graph, n = 42.\n\nFigure 12: Example covers produced by our learned heuristic on Erd ̋os-Rényi and Barabási-Albert graphs. Black-bordered nodes are in the cover.\n\nE.2 MINIMUM VERTEX COVER\n\nFigure 12 shows additional example covers of our learned heuristic on in-distribution graphs.\n\n25\n\n0123456789101112131415012345678910111213141516012345678910111213141516170123456789101112131415161718190123456789101112131415161718192021222324252627282930313233343536373839400123456789101112131415161718192021222324252627282930313233343536373839404101234567891011121314150123456789101112131415160123456789101112131415161701234567891011121314151617181901234567891011121314151617181920212223242526272829303132333435363738394001234567891011121314151617181920212223242526272829303132333435363738394041",
    "reference": "# Summary Of The Paper\n\nThis work proposes a framework for learning combinatorial problems over graphs in which the solution can be expressed in terms of an ordered labeling of the nodes. Then, the authors propose GAT-CNL, a neural architecture leveraging the framework to learn to solve such class of problems in a greedy fashion. Empirically, the method presents better results when solving graph coloring and minimum vertex cover.\n\n# Strength And Weaknesses\n\n--- Strength ---\na) The node labeling framework is a clear formulation of a wide class of problems and will help guide future contributions.\nb) The graph coloring problem indeed never had, to the best of my knowledge, a more general ML solution.\nc) Their MDP formulation allows for more than two labels and, more importantly, unknown number of labels. This is because a state is not a partial solution, but a partial labeling, which is fundamentally different.\nd) Lemma 2.4 guarantees a solution in the MDP space, which is not a common result in previous literature.\ne) Empirical results, especially for GC, are very promising.\n\n--- Weaknesses ---\na) Throughout the text, the authors argue that node labeling and greedy strategies are intimately related (due to the ordering of labels). I understand that it's a natural relationship when we design heuristics, but we know that relationship does not necessarily exist in optimal solutions. I think it would be good to make this distinction more clear.\nb) I find the proofs not rigorous enough (although the statements seem to be true to me). The authors try to present constructive arguments, which can result incur easily in this (without enough care). From what I see the proofs can be rewritten to use contradiction instead. A good example of this is Lemma 3.2's proof: The authors construct a solution and immediately conclude \" Labeling the nodes in this order produces a minimum vertex cover of G.\". I understand the lemma stands, but it reads sloppy.\nc) I expected to see larger graphs in the dataset. Also, for ML in combinatorial opt, it is vital to test your solution on graphs that are larger at test time (shift in distribution).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is very well written and easy to follow. The entire training procedure, including hyperparams, is described, which results in transparency and good reproducibility. The paper is not very different from what the ML community has been doing in combinatorial optimization, so novelty is a bit limited (although it definitely exists).\n\n# Summary Of The Review\n\nIn my opinion the paper needs a little bit more of work, since the weaknesses currently outweigh the contributions. If the authors are able to test the solution on larger graphs and make the proofs more rigorous, I'm willing to raise my score.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nREPLICABLE BANDITS\n\nHossein Esfandiari Google Research esfandiari@google.com\n\nAlkis Kalavasis National Technical University of Athens kalavasisalkis@mail.ntua.gr\n\nAmin Karbasi Yale University, Google Research amin.karbasi@yale.edu\n\nAndreas Krause ETH Zurich krausea@ethz.ch\n\nVahab Mirrokni Google Research mirrokni@google.com\n\nGrigoris Velegkas Yale University grigoris.velegkas@yale.edu\n\nABSTRACT\n\nIn this paper, we introduce the notion of replicable policies in the context of stochastic bandits, one of the canonical problems in interactive learning. A policy in the bandit environment is called replicable if it pulls, with high probability, the exact same sequence of arms in two different and independent executions (i.e., under independent reward realizations). We show that not only do replicable policies exist, but also they achieve almost the same optimal (non-replicable) regret bounds in terms of the time horizon. More specifically, in the stochastic multi-armed bandits setting, we develop a policy with an optimal problem-dependent regret bound whose dependence on the replicability parameter is also optimal. Similarly, for stochastic linear bandits (with finitely and infinitely many arms) we develop replicable policies that achieve the best-known problem-independent regret bounds with an optimal dependency on the replicability parameter. Our results show that even though randomization is crucial for the exploration-exploitation trade-off, an optimal balance can still be achieved while pulling the exact same arms in two different rounds of executions.\n\n1\n\nINTRODUCTION\n\nIn order for scientific findings to be valid and reliable, the experimental process must be repeatable, and must provide coherent results and conclusions across these repetitions. In fact, lack of reproducibility has been a major issue in many scientific areas; a 2016 survey that appeared in Nature (Baker, 2016a) revealed that more than 70% of researchers failed in their attempt to reproduce another researcher’s experiments. What is even more concerning is that over 50% of them failed to reproduce their own findings. Similar concerns have been raised by the machine learning community, e.g., the ICLR 2019 Reproducibility Challenge (Pineau et al., 2019) and NeurIPS 2019 Reproducibility Program (Pineau et al., 2021), due to the to the exponential increase in the number of publications and the reliability of the findings.\n\nThe aforementioned empirical evidence has recently led to theoretical studies and rigorous definitions of replicability. In particular, the works of Impagliazzo et al. (2022) and Ahn et al. (2022) considered replicability as an algorithmic property through the lens of (offline) learning and convex optimization, respectively. In a similar vein, in the current work, we introduce the notion of replicability in the context of interactive learning and decision making. In particular, we study replicable policy design for the fundamental setting of stochastic bandits.\n\nA multi-armed bandit (MAB) is a one-player game that is played over T rounds where there is a set of different arms/actions A of size |A| = K (in the more general case of linear bandits, we can consider even an infinite number of arms). In each round t = 1, 2, . . . , T , the player pulls an arm at ∈ A and receives a corresponding reward rt. In the stochastic setting, the rewards of each\n\n1\n\nPublished as a conference paper at ICLR 2023\n\narm are sampled in each round independently, from some fixed but unknown, distribution supported on [0, 1]. Crucially, each arm has a potentially different reward distribution, but the distribution of each arm is fixed over time. A bandit algorithm A at every round t takes as input the sequence of arm-reward pairs that it has seen so far, i.e., (a1, r1), . . . , (at−1, rt−1), then uses (potentially) some internal randomness ξ to pull an arm at ∈ A and, finally, observes the associated reward rt ∼ Dat.\n\nWe propose the following natural notion of a replicable bandit algorithm, which is inspired by the definition of Impagliazzo et al. (2022). Intuitively, a bandit algorithm is replicable if two distinct executions of the algorithm, with internal randomness fixed between both runs, but with independent reward realizations, give the exact same sequence of played arms, with high probability. More formally, we have the following definition. Definition 1 (Replicable Bandit Algorithm). Let ρ ∈ [0, 1]. We call a bandit algorithm A ρreplicable in the stochastic setting if for any distribution Daj over [0, 1] of the rewards of the j-th arm aj ∈ A, and for any two executions of A, where the internal randomness ξ is shared across the executions, it holds that\n\nPr ξ,r(1),r(2)\n\n(cid:104)(cid:16)\n\n1 , . . . , a(1) a(1)\n\nT\n\n(cid:17)\n\n(cid:16)\n\n=\n\n1 , . . . , a(2) a(2)\n\nT\n\n(cid:17)(cid:105)\n\n≥ 1 − ρ .\n\nHere, a(i) i ∈ {1, 2}.\n\nt = A(a(i)\n\n1 , r(i)\n\n1 , ..., a(i)\n\nt−1, r(i)\n\nt−1; ξ) is the t-th action taken by the algorithm A in execution\n\nThe reason why we allow for some fixed internal randomness is that the algorithm designer has control over it, e.g., they can use the same seed for their (pseudo)random generator between two executions. Clearly, naively designing a replicable bandit algorithm is not quite challenging. For instance, an algorithm that always pulls the same arm or an algorithm that plays the arms in a particular random sequence determined by the shared random seed ξ are both replicable. The caveat is that the performance of these algorithms in terms of expected regret will be quite poor. In this work, we aim to design bandit algorithms which are replicable and enjoy small expected regret. In the stochastic setting, the (expected) regret after T rounds is defined as\n\nE[RT ] = T max a∈A\n\nμa − E\n\n(cid:35)\n\nμat\n\n,\n\n(cid:34) T\n\n(cid:88)\n\nt=1\n\nwhere μa = Er∼Da [r] is the mean reward for arm a ∈ A. In a similar manner, we can define the regret in the more general setting of linear bandits (see, Section 5) Hence, the overarching question in this work is the following:\n\nIs it possible to design replicable bandit algorithms with small expected regret?\n\nAt a first glance, one might think that this is not possible, since it looks like replicability contradicts the exploratory behavior that a bandit algorithm should possess. However, our main results answer this question in the affirmative and can be summarized in Table 1.\n\nSummary of Results\n\nSetting\n\nStochastic MAB\n\nStochastic MAB\n\nStochastic Linear Bandits\n\nStochastic Linear Bandits Infinite Action Space\n\nAlgorithm\n\nAlgorithm 1\n\nAlgorithm 2\n\nAlgorithm 3\n\nAlgorithm 4\n\nRegret\n\n(cid:16) K2 log3(T )H∆ ρ2\n\n(cid:16) K2 log(T )H∆ ρ2\n\n(cid:17)\n\n√\n\n(cid:16) K2\n\ndT\n\n(cid:17)\n\nρ2\n\n(cid:16) poly(d) ρ2\n\n√\n\nT\n\n(cid:17)\n\n(cid:101)O\n\n(cid:101)O\n\n(cid:101)O\n\n(cid:101)O\n\n(cid:17)\n\nTheorem\n\nTheorem 3\n\nTheorem 4\n\nTheorem 6\n\nTheorem 10\n\nTable 1: Our results for replicable stochastic general multi-armed and linear bandits. In the expected regret column, (cid:101)O(·) subsumes logarithmic factors. H∆ is equal to (cid:80) j:∆j >0 1/∆j, ∆j is the difference between the mean of action j and the optimal action, K is the number of arms, d is the ambient dimension in the linear bandit setting.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n1.1 RELATED WORK\n\nReproducibility/Replicability. In this work, we introduce the notion of replicability in the context of interactive learning and, in particular, in the fundamental setting of stochastic bandits. Close to our work, the notion of a replicable algorithm in the context of learning was proposed by Impagliazzo et al. (2022), where it is shown how any statistical query algorithm can be made replicable with a moderate increase in its sample complexity. Using this result, they provide replicable algorithms for finding approximate heavy-hitters, medians, and the learning of half-spaces. Reproducibility has been also considered in the context of optimization by Ahn et al. (2022). We mention that in Ahn et al. (2022) the notion of a replicable algorithm is different from our work and that of Impagliazzo et al. (2022), in the sense that the outputs of two different executions of the algorithm do not need to be exactly the same. From a more application-oriented perspective, Shamir & Lin (2022) study irreproducibility in recommendation systems and propose the use of smooth activations (instead of ReLUs) to improve recommendation reproducibility. In general, the reproducibility crisis is reported in various scientific disciplines Ioannidis (2005); McNutt (2014); Baker (2016b); Goodman et al. (2016); Lucic et al. (2018); Henderson et al. (2018). For more details we refer to the report of the NeurIPS 2019 Reproducibility Program Pineau et al. (2021) and the ICLR 2019 Reproducibility Challenge Pineau et al. (2019).\n\ni:∆i>0 ∆−1(cid:1); this is achieved, e.g., by the upper confidence bound (UCB) algorithm of\n\nBandit Algorithms. Stochastic multi-armed bandits for the general setting without structure have been studied extensively Slivkins (2019); Lattimore & Szepesv ́ari (2020); Bubeck et al. (2012b); Auer et al. (2002); Cesa-Bianchi & Fischer (1998); Kaufmann et al. (2012a); Audibert et al. (2010); Agrawal & Goyal (2012); Kaufmann et al. (2012b). In this setting, the optimum regret achievable is O (cid:0)log(T ) (cid:80) Auer et al. (2002). The setting of d-dimensional linear stochastic bandits is also well-explored Dani et al. (2008); Abbasi-Yadkori et al. (2011) under the well-specified linear reward model, achieving (near) optimal problem-independent regret of O(d(cid:112)T log(T )) Lattimore & Szepesv ́ari (2020). Note that the best-known lower bound is Ω(d T ) Dani et al. (2008) and that the number of arms can, in principle, be unbounded. For a finite number of arms K, the best known upper bound is O((cid:112)dT log(K)) Bubeck et al. (2012a). Our work focuses on the design of replicable bandit algorithms and we hence consider only stochastic environments. In general, there is also extensive work in adversarial bandits and we refer the interested reader to Lattimore & Szepesv ́ari (2020).\n\n√\n\nBatched Bandits. While sequential bandit problems have been studied for almost a century, there is much interest in the batched setting too. In many settings, like medical trials, one has to take a lot of actions in parallel and observe their rewards later. The works of Auer & Ortner (2010) and CesaBianchi et al. (2013) provided sequential bandit algorithms which can easily work in the batched setting. The works of Gao et al. (2019) and Esfandiari et al. (2021) are focusing exclusively on the batched setting. Our work on replicable bandits builds upon some of the techniques from these two lines of work.\n\n2 STOCHASTIC BANDITS AND REPLICABILITY\n\nIn this section, we first highlight the main challenges in order to guarantee replicability and then discuss how the results of Impagliazzo et al. (2022) can be applied in our setting.\n\n2.1 WARM-UP I: NAIVE REPLICABILITY AND CHALLENGES\n\nLet us consider the stochastic two-arm setting (K = 2) and a bandit algorithm A with two independent executions, A1 and A2. The algorithm Ai plays the sequence 1, 2, 1, 2, . . . until some, potentially random, round Ti ∈ N after which one of the two arms is eliminated and, from that point, the algorithm picks the winning arm ji ∈ {1, 2}. The algorithm A is ρ-replicable if and only if T1 = T2 and j1 = j2 with probability 1 − ρ.\n\nAssume that |μ1 − μ2| = ∆ where μi is the mean of the distribution of the i-th arm. If we assume that ∆ is known, then we can run the algorithm for T1 = T2 = C ∆2 log(1/ρ) for some universal 1 ≈ μ1 and (cid:98)μ(j) constant C > 0 and obtain that, with probability 1 − ρ, it will hold that (cid:98)μ(j) 2 ≈ μ2\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nfor j ∈ {1, 2}, where (cid:98)μ(j) is the estimation of arm’s i mean during execution j. Hence, knowing ∆ implies that the stopping criterion of the algorithm A is deterministic and that, with high probability, the winning arm will be detected at time T1 = T2. This will make the algorithm ρ-replicable.\n\ni\n\nObserve that when K = 2, the only obstacle to replicability is that the algorithm should decide at the same time to select the winning arm and the selection must be the same in the two execution threads. In the presence of multiple arms, there exists the additional constraint that the above conditions must be satisfied during, potentially, multiple arm eliminations. Hence, the two questions arising from the above discussion are (i) how to modify the above approach when ∆ is unknown and (ii) how to deal with K > 2 arms.\n\nA potential solution to the second question (on handling K > 2 arms) is the Execute-Then-Commit (ETC) strategy. Consider the stochastic K-arm bandit setting. For any ρ ∈ (0, 1), the ETC algorithm with known ∆ = mini ∆i and horizon T that uses m = 4 ∆2 log(1/ρ) deterministic exploration phases before commitment is ρ-replicable. The intuition is exactly the same as in the K = 2 case. The caveats of this approach are that it assumes that ∆ is known and that the obtained regret is quite unsatisfying. In particular, it achieves regret bounded by m (cid:80) i∈[k] ∆i.\n\ni∈[K] ∆i + ρ · (T − mK) (cid:80)\n\nNext, we discuss how to improve the regret bound without knowing the gaps ∆i. Before designing new algorithms, we will inspect the guarantees that can be obtained by combining ideas from previous results in the bandits literature and the recent work in replicable learning of Impagliazzo et al. (2022).\n\n2.2 WARM-UP II: BANDIT ALGORITHMS AND REPLICABLE MEAN ESTIMATION\n\nFirst, we remark that we work in the stochastic setting and the distributions of the rewards of the two arms are subgaussian. Thus, the problem of estimating their mean is an instance of a statistical query for which we can use the algorithm of Impagliazzo et al. (2022) to get a replicable mean estimator for the distributions of the rewards of the arms.\n\nProposition 2 (Replicable Mean Estimation (Impagliazzo et al., 2022)). Let τ, δ, ρ ∈ [0, 1]. There\n\nexists a ρ-replicable algorithm ReprMeanEstimation that draws Ω samples from a distribution with mean μ and computes an estimate (cid:98)μ that satisfies |(cid:98)μ − μ| ≤ τ with probability at least 1 − δ.\n\n(cid:16) log(1/δ) τ 2(ρ−δ)2\n\n(cid:17)\n\nNotice that we are working in the regime where δ ≪ ρ, so the sample complexity is Ω\n\n(cid:16) log(1/δ) τ 2ρ2\n\n(cid:17)\n\n.\n\nThe straightforward approach is to try to use an optimal multi-armed algorithm for the stochastic setting, such as UCB or arm-elimination (Even-Dar et al., 2006), combined with the replicable mean estimator. However, it is not hard to see that this approach does not give meaningful results: if we want to achieve replicability ρ we need to call the replicable mean estimator routine with parameter ρ/(KT ), due to the union bound that we need to take. This means that we need to pull every arm at least K 2T 2 times, so the regret guarantee becomes vacuous. This gives us the first key insight to tackle the problem: we need to reduce the number of calls to the mean estimator. Hence, we will draw inspiration from the line of work in stochastic batched bandits (Gao et al., 2019; Esfandiari et al., 2021) to derive replicable bandit algorithms.\n\n3 REPLICABLE MEAN ESTIMATION FOR BATCHED BANDITS\n\nAs a first step, we would like to show how one could combine the existing replicable algorithms of Impagliazzo et al. (2022) with the batched bandits approach of Esfandiari et al. (2021) to get some preliminary non-trivial results. We build an algorithm for the K-arm setting, where the gaps ∆j are unknown to the learner. Let δ be the confidence parameter of the arm elimination algorithm and ρ be the replicability guarantee we want to achieve. Our approach is the following: let us, deterministically, split the time interval into sub-intervals of increasing length. We treat each subinterval as a batch of samples where we pull each active arm the same number of times and use the replicable mean estimation algorithm to, empirically, compute the true mean. At the end of each batch, we decide to eliminate some arm j using the standard UCB estimate. Crucially, if we condition on the event that all the calls to the replicable mean estimator return the same number, then the algorithm we propose is replicable.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Mean-Estimation Based Replicable Algorithm for Stochastic MAB (Theorem 3)\n\nif ⌊qi⌋ · |A| > r then\n\n1: Input: time horizon T , number of arms K, replicability ρ 2: Initialization: B ← log(T ), q ← T 1/B, c0 ← 0, A ← [K], r ← T , (cid:98)μa ← 0, ∀a ∈ A 3: for i = 1 to B − 1 do 4: 5: 6: 7: 8: 9:\n\nci = ci−1 + ⌊qi⌋ Pull every arm a ∈ A for ⌊qi⌋ times for a ∈ A do\n\n(cid:98)μa ← ReprMeanEstimation(δ = 1/(2KT B), τ = 1, (cid:112)log(2KT B)/ci, ρ′ =\n\nbreak\n\n▷ Proposition 2\n\nρ/(KB))\n\nr ← r − |A| · ⌊qi⌋ for a ∈ A do\n\n10: 11: 12: 13: 14: In the last batch play the arm from A with the smallest index\n\nif (cid:98)μa < maxa∈A (cid:98)μa − 2τ then\n\nRemove a from A\n\nTheorem 3. Let T ∈ N, ρ ∈ (0, 1]. There exists a ρ-replicable algorithm (presented in Algorithm 1) for the stochastic bandit problem with K arms and gaps (∆j)j∈[K] whose expected regret is\n\nE[RT ] ≤ C ·\n\nK 2 log2(T ) ρ2\n\n(cid:18)\n\n(cid:88)\n\n∆j +\n\nj:∆j >0\n\nlog(KT log(T )) ∆j\n\n(cid:19)\n\n,\n\nwhere C > 0 is an absolute numerical constant, and its running time is polynomial in K, T and 1/ρ.\n\nThe above result, whose proof can be found in Appendix A, states that, by combining the tools from Impagliazzo et al. (2022) and Esfandiari et al. (2021), we can design a replicable bandit algorithm with (instance-dependent) expected regret O(K 2 log3(T )/ρ2). Notice that the regret guarantee has an extra K 2 log2(T )/ρ2 factor compared to its non-replicable counterpart in Esfandiari et al. (2021) (Theorem 5.1). This is because, due to a union bound over the rounds and the arms, we need to call the replicable mean estimator with parameter ρ/(K log(T )). In the next section, we show how to get rid of the log2(T ) by designing a new algorithm.\n\n4\n\nIMPROVED ALGORITHMS FOR REPLICABLE STOCHASTIC BANDITS\n\nWhile the previous result provides a non-trivial regret bound, it is not optimal with respect to the time horizon T . In this section, we show how to improve it by designing a new algorithm, presented in Algorithm 2, which satisfies the guarantees of Theorem 4 and, essentially, decreases the dependence on the time horizon T from log3(T ) to log(T ). Our main result for replicable stochastic multi-armed bandits with K arms follows. Theorem 4. Let T ∈ N, ρ ∈ (0, 1]. There exists a ρ-replicable algorithm (presented in Algorithm 2) for the stochastic bandit problem with K arms and gaps (∆j)j∈[K] whose expected regret is\n\nE[RT ] ≤ C ·\n\nK 2 ρ2\n\n(cid:18)\n\n(cid:88)\n\n∆j +\n\nj:∆j >0\n\nlog(KT log(T )) ∆j\n\n(cid:19)\n\n,\n\nwhere C > 0 is an absolute numerical constant, and its running time is polynomial in K, T and 1/ρ.\n\nNote that, compared to the non-replicable setting, we incur an extra factor of K 2/ρ2 in the regret. The proof can be found in Appendix B. Let us now describe how Algorithm 2 works. We decompose the time horizon into B = log(T ) batches. Without the replicability constraint, one could draw qi samples in batch i from each arm and estimate the mean reward. With the replicability constraint, we have to boost this: in each batch i, we pull each active arm O(βqi) times, for some q to be determined, where β = O(K 2/ρ2) is the replicability blow-up. Using these samples, we compute\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2 Replicable Algorithm for Stochastic Multi-Armed Bandits (Theorem 4)\n\n1: Input: time horizon T , number of arms K, replicability ρ 2: Initialization: B ← log(T ), q ← T 1/B, c0 ← 0, A0 ← [K], r ← T , (cid:98)μa ← 0, ∀a ∈\n\nPull arm a for β⌊qi⌋ times Compute the empirical mean (cid:98)μ(i)\n\nα\n\nA0\n\nif β⌊qi⌋ · |Ai| > r then\n\n3: β ← ⌊max{K 2/ρ2, 2304}⌋ 4: for i = 1 to B − 1 do 5: 6: 7: 8: 9:\n\nbreak Ai ← Ai−1 for a ∈ Ai do\n\n10:\n\n11: 12:\n\n13:\n\n14:\n\n15: 16: 17:\n\nci ← ci−1 + ⌊qi⌋ (cid:101)ci ← βci (cid:101)Ui ← (cid:112)2 ln(2KT B)/(cid:101)ci Ui ← (cid:112)2 ln(2KT B)/ci U i ← Uni[Ui/2, Ui] r ← r − β · |Ai| · ⌊qi⌋ for a ∈ Ai do if (cid:98)μ(i)\n\n18: 19: 20: In the last batch play the arm from AB−1 with the smallest index\n\na + (cid:101)Ui < maxa∈Ai (cid:98)μ(i) Remove a from Ai\n\na − U i then\n\na + (cid:101)Ui < max (cid:98)μ(i)\n\nthe empirical mean (cid:98)μ(i) α for any active arm α. Note that (cid:101)Ui in Algorithm 2 corresponds to the size of the actual confidence interval of the estimation and Ui corresponds to the confidence interval of an algorithm that does not use the β-blow-up in the number of samples. The novelty of our approach comes from the choice of the interval around the mean of the maximum arm: we pick a threshold uniformly at random from an interval of size Ui/2 around the maximum mean. Then, the algorithm checks whether (cid:98)μ(i) a′ − U i, where max runs over the active arms a′ in batch i, and eliminates arms accordingly. To prove the result we show that there are three regions that some arm j can be in relative to the confidence interval of the best arm in batch i (cf. Appendix B). If it lies in two of these regions, then the decision of whether to keep it or discard it is the same in both executions of the algorithm. However, if it is in the third region, the decision could be different between parallel executions, and since it relies on some external and unknown randomness, it is not clear how to reason about it. To overcome this issue, we use the random threshold to argue about the probability that the decision between two executions differs. The crucial observation that allows us to get rid of the extra log2(T ) factor is that there are correlations between consecutive batches: we prove that if some arm j lies in this “bad” region in some batch i, then it will be outside this region after a constant number of batches.\n\n5 REPLICABLE STOCHASTIC LINEAR BANDITS\n\nWe now investigate replicability in the more general setting of stochastic linear bandits. In this setting, each arm is a vector a ∈ Rd belonging to some action set A ⊆ Rd, and there is a parameter θ⋆ ∈ Rd unknown to the player. In round t, the player chooses some action at ∈ A and receives a reward rt = ⟨θ⋆, at⟩ + ηt, where ηt is a zero-mean 1-subgaussian random variable independent of any other source of randomness. This means that E[ηt] = 0 and satisfies E[exp(ληt)] ≤ exp(λ2/2) for any λ ∈ R. For normalization purposes, it is standard to assume that ∥θ⋆∥2 ≤ 1 and supa∈A ∥a∥2 ≤ 1. In the linear setting, the expected regret after T pulls a1, . . . , aT can be written as\n\n(cid:35)\n\n⟨θ⋆, at⟩\n\n.\n\n(cid:34) T\n\n(cid:88)\n\nt=1\n\nE[RT ] = T sup a∈A\n\n⟨θ⋆, a⟩ − E\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nIn Section 5.1 we provide results for the finite action space case, i.e., when |A| = K. Next, in Section 5.2, we study replicable linear bandit algorithms when dealing with infinite action spaces. In the following, we work in the regime where T ≫ d. We underline that our approach leverages connections of stochastic linear bandits with G-optimal experiment design, core sets constructions, and least-squares estimators. Roughly speaking, the goal of G-optimal design is to find a (small) subset of arms A′, which is called the core set, and define a distribution π over them with the following property: for any ε > 0, δ > 0 pulling only these arms for an appropriate number of times and computing the least-squares estimate (cid:98)θ guarantees that supa∈A⟨a, θ∗ − (cid:98)θ⟩ ≤ ε, with probability 1−δ. For an extensive discussion, we refer to Chapters 21 and 22 of Lattimore & Szepesv ́ari (2020).\n\n5.1 FINITE ACTION SET\n\nWe first introduce a lemma that allows us to reduce the size of the action set that our algorithm has to search over.\n\nLemma 5 (See Chapters 21 and 22 in Lattimore & Szepesv ́ari (2020)). For any finite action set A that spans Rd and any δ, ε > 0, there exists an algorithm that, in time polynomial in d, computes a multi-set of Θ(d log(1/δ)/ε2 + d log log d) actions (possibly with repetitions) such that (i) they span Rd and (ii) if we perform these actions in a batched stochastic d-dimensional linear bandits setting with true parameter θ⋆ ∈ Rd and let (cid:98)θ be the least-squares estimate for θ⋆, then, for any a ∈ A, with probability at least 1 − δ, we have\n\n(cid:68)\n\na, θ⋆ − (cid:98)θ\n\n(cid:69)(cid:12) (cid:12) (cid:12) ≤ ε.\n\n(cid:12) (cid:12) (cid:12)\n\na∈A π(a)aa⊤ ∈ Rd×d and g(π) = supa∈A ∥a∥2\n\nEssentially, the multi-set in Lemma 5 is obtained using an approximate G-optimal design algorithm. Thus, it is crucial to check whether this can be done in a replicable manner. Recall that the above set of distinct actions is called the core set and is the solution of an (approximate) Goptimal design problem. To be more specific, consider a distribution π : A → [0, 1] and define V (π) = (cid:80) V (π)−1. The distribution π is called a design and the goal of G-optimal design is to find a design that minimizes g. Since the number of actions is finite, this problem reduces to an optimization problem which can be solved efficiently using standard optimization methods (e.g., the Frank-Wolfe method). Since the initialization is the same, the algorithm that finds the optimal (or an approximately optimal) design is replicable under the assumption that the gradients and the projections do not have numerical errors. This perspective is orthogonal to the work of Ahn et al. (2022), that defines reproducibility from a different viewpoint.\n\nAlgorithm 3 Replicable Algorithm for Stochastic Linear Bandits (Theorem 6)\n\n1: Input: number of arms K, time horizon T , replicability ρ 2: Initialization: B ← log(T ), q ← (T /c)1/B, A ← [K], r ← T 3: β ← ⌊max{K 2/ρ2, 2304}⌋ 4: for i = 1 to B − 1 do 5:\n\n(cid:101)εi = (cid:112)d log(KT 2)/(βqi) εi = (cid:112)d log(KT 2)/qi ni = 10d log(KT 2)/ε2 a1, . . . , ani ← multi-set given by Lemma 5 with parameters δ = 1/(KT 2) and ε = (cid:101)εi if ni > r then break\n\ni\n\n6: 7: 8: 9: 10: 11:\n\n12:\n\nPull every arm a1, . . . , ani and receive rewards r1, . . . , rni\n\n(cid:16)(cid:80)ni\n\n(cid:17)−1 (cid:16)(cid:80)ni\n\n(cid:17)\n\nj=1 ajrj\n\nj\n\nj=1 ajaT\n\nCompute the LSE (cid:98)θi ← εi ← Uni[εi/2, εi] r ← r − ni for a ∈ A do\n\n13: 14: 15: 16: Remove a from A 17: 18: In the last batch play arg maxa∈A⟨a, (cid:98)θB−1⟩\n\nif ⟨a, (cid:98)θi⟩ + (cid:101)εi < maxa∈A⟨a, (cid:98)θi⟩ − εi then\n\nIn our batched bandit algorithm (Algorithm 3), the multi-set of arms a1, . . . , ani computed in each batch is obtained via a deterministic algorithm with runtime poly(K, d), where |A| = K. Hence, the\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nmulti-set will be the same in two different executions of the algorithm. On the other hand, the LSE will not be since it depends on the stochastic rewards. We apply the techniques that we developed in the replicable stochastic MAB setting in order to design our algorithm. Our main result for replicable d-dimensional stochastic linear bandits with K arms follows. For the proof, we refer to Appendix C. Theorem 6. Let T ∈ N, ρ ∈ (0, 1]. There exists a ρ-replicable algorithm for the stochastic ddimensional linear bandit problem with K arms whose expected regret is\n\nE[RT ] ≤ C ·\n\nK 2 ρ2\n\n(cid:112)dT log(KT ) ,\n\nwhere C > 0 is an absolute numerical constant, and its running time is polynomial in d, K, T and 1/ρ.\n\nNote that the best known non-replicable algorithm achieves an upper bound of (cid:101)O((cid:112)dT log(K)) and, hence, our algorithm incurs a replicability overhead of order K 2/ρ2. The intuition behind the proof is similar to the multi-armed bandit setting in Section 4.\n\n5.2\n\nINFINITE ACTION SET\n\nLet us proceed to the setting where the action set A is unbounded. Unfortunately, even when d = 1, we cannot directly get an algorithm that has satisfactory regret guarantees by discretizing the space and using Algorithm 3. The approach of Esfandiari et al. (2021) is to discretize the action space and use an 1/T -net to cover it, i.e. a set A′ ⊆ A such that for all a ∈ A there exists some a′ ∈ A′ with ||a − a′||2 ≤ 1/T . It is known that there exists such a net of size at most (3T )d (Vershynin, 2018, Corollary 4.2.13). Then, they apply the algorithm for the finite arms setting, increasing their regret d. However, our replicable algorithm for this setting contains an additional guarantee by a factor of factor of K 2 in the regret bound. Thus, even when d = 1, our regret guarantee is greater than T, so the bound is vacuous. One way to fix this issue and get a sublinear regret guarantee is to use a smaller net. We use a 1/T 1/(4d+2)−net that has size at most (3T ) 4d+2 and this yields an expected regret of order O(T 4d+1/(4d+2)(cid:112)d log(T )/ρ2). For further details, we refer to Appendix D.\n\n√\n\nd\n\nEven though the regret guarantee we managed to get using the smaller net of Appendix D is sublinear in T , it is not a satisfactory bound. The next step is to provide an algorithm for the infinite action setting using a replicable LSE subroutine combined with the batching approach of Esfandiari et al. (2021). We will make use of the next lemma. Lemma 7 (Section 21.2 Note 3 of Lattimore & Szepesv ́ari (2020)). There exists a deterministic algorithm that, given an action space A ⊆ Rd, computes a 2-approximate G-optimal design π with a core set of size O(d log log(d)).\n\nWe additionally prove the next useful lemma, which, essentially, states that we can assume without loss of generality that every arm in the support of π has mass at least Ω(1/(d log(d))). We refer to Appendix F.1 for the proof. Lemma 8 (Effective Support). Let π be the distribution that corresponds to the 2-approximate optimal G-design of Lemma 7 with input A. Assume that π(a) ≤ c/(d log(d)), where c > 0 is some absolute numerical constant, for some arm a in the core set. Then, we can construct a distribution (cid:98)π such that, for any arm a in the core set, (cid:98)π(a) ≥ C/(d log(d)), where C > 0 is an absolute constant, so that it holds\n\nsup a′∈A\n\n∥a′∥2\n\nV ((cid:98)π)−1 ≤ 4d .\n\nThe upcoming lemma is a replicable algorithm for the least-squares estimator and, essentially, builds upon Lemma 7 and Lemma 8. Its proof can be found at Appendix F.2. Lemma 9 (Replicable LSE). Let ρ, ε ∈ (0, 1] and 0 < δ ≤ min{ρ, 1/d}1. Consider an environment of d-dimensional stochastic linear bandits with infinite action space A. Assume that π is a 4approximate optimal design with associated core set C as computed by Lemma 7 with input A. There exists a ρ-replicable algorithm that pulls each arm a ∈ C a total of\n\n(cid:18) d4 log(d/δ) log2 log(d) log log log(d) ε2ρ2\n\n(cid:19)\n\nΩ\n\n1We can handle the case of 0 < δ ≤ d by paying an extra log d factor in the sample complexity.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\ntimes and outputs θSQ that satisfies supa∈A |⟨a, θSQ − θ⋆⟩| ≤ ε , with probability at least 1 − δ.\n\nAlgorithm 4 Replicable LSE Algorithm for Stochastic Infinite Action Set (Theorem 10)\n\n1: Input: time horizon T , action set A ⊆ Rd, replicability ρ 2: A′ ← 1/T -net of A 3: Initialization: r ← T, B ← log(T ), q ← (T /c)1/B 4: for i = 1 to B − 1 do 5: 6:\n\nqi denotes the number of pulls of all arms before the replicability blow-up εi = c · d(cid:112)log(T )/qi The blow-up is Mi = qi · d3 log(d) log2 log(d) log log log(d) log2(T )/ρ2 a1, . . . , a|Ci| ← core set Ci of the design given by Lemma 7 with parameter A′ if ⌈Mi⌉ > r then\n\nbreak\n\nPull every arm aj for Ni = ⌈Mi⌉/|Ci| rounds and receive rewards r(j) Si = {(aj, r(j) (cid:98)θi ← ReplicableLSE(Si, ρ′ = ρ/(dB), δ = 1/(2|A′|T 2), τ = min{εi, 1}) r ← r − ⌈Mi⌉ for a ∈ A′ do\n\nt ) : t ∈ [Ni], j ∈ [|Ci|]}\n\n1 , ..., r(j)\n\nNi\n\nfor j ∈ [|Ci|]\n\n7: 8: 9: 10: 11:\n\n12:\n\nif ⟨a, (cid:98)θi⟩ < maxa∈A′⟨a, (cid:98)θi⟩ − 2εi then\n\n13: 14: 15: 16: Remove a from A′ 17: 18: In the last batch play arg maxa∈A′⟨a, (cid:98)θB−1⟩ 19: 20: ReplicableLSE(S, ρ, δ, τ ) 21: for a ∈ C do 22: 23: return ((cid:80)\n\nj )−1 · ((cid:80)\n\na∈C a na v(a))\n\nj∈|S| aja⊤\n\nv(a) ← ReplicableSQ(φ : x ∈ R (cid:55)→ x ∈ R, S, ρ, δ, τ )\n\n▷ Impagliazzo et al. (2022)\n\nThe main result for the infinite actions’ case, obtained by Algorithm 4, follows. Its proof can be found at Appendix E. Theorem 10. Let T ∈ N, ρ ∈ (0, 1]. There exists a ρ-replicable algorithm (Algorithm 4) for the stochastic d-dimensional linear bandit problem with infinite action set whose expected regret is\n\nd4 log(d) log2 log(d) log log log(d) ρ2 where C > 0 is an absolute numerical constant, and its running time is polynomial in T d and 1/ρ.\n\nT log3/2(T ) ,\n\nE[RT ] ≤ C ·\n\n√\n\n√\n\nOur algorithm for the infinite arm linear bandit case enjoys an expected regret of order (cid:101)O(poly(d) T ). We underline that the dependence of the regret on the time horizon is (almost) optimal, and we incur an extra d3 factor in the regret guarantee compared to the non-replicable algorithm of Esfandiari et al. (2021). We now comment on the time complexity of our algorithm. Remark 11. The current implementation of our algorithm requires time exponential in d. However, for a general convex set A, given access to a separation oracle for it and an oracle that computes an (approximate) G-optimal design, we can execute it in polynomial time and with polynomially many calls to the oracle. Notably, when A is a polytope such oracles exist. We underline that computational complexity issues also arise in the traditional setting of linear bandits with an infinite number of arms and the computational overhead that the replicability requirement adds is minimal. For further details, we refer to Appendix G.\n\n6 CONCLUSION AND FUTURE DIRECTIONS\n\nIn this paper, we have provided a formal notion of reproducibility/replicability for stochastic bandits and we have developed algorithms for the multi-armed bandit and the linear bandit settings that satisfy this notion and enjoy a small regret decay compared to their non-replicable counterparts. We hope and believe that our paper will inspire future works in replicable algorithms for more complicated interactive learning settings such as reinforcement learning. We also provide experimental evaluation in Appendix H.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\n7 ACKNOWLEDGEMENTS\n\nAlkis Kalavasis was supported by the Hellenic Foundation for Research and Innovation (H.F.R.I.) under the “First Call for H.F.R.I. Research Projects to support Faculty members and Researchers and the procurement of high-cost research equipment grant”, project BALSAM, HFRIFM17-1424. Amin Karbasi acknowledges funding in direct support of this work from NSF (IIS-1845032), ONR (N00014- 19-1-2406), and the AI Institute for Learning-Enabled Optimization at Scale (TILOS). Andreas Krause was supported by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program grant agreement no. 815943 and the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545. Grigoris Velegkas was supported by NSF (IIS-1845032), an Onassis Foundation PhD Fellowship and a Bodossaki Foundation PhD Fellowship.\n\nREFERENCES\n\nYasin Abbasi-Yadkori, D ́avid P ́al, and Csaba Szepesv ́ari. Improved algorithms for linear stochastic\n\nbandits. Advances in neural information processing systems, 24, 2011. 3\n\nShipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In Conference on learning theory, pp. 39–1. JMLR Workshop and Conference Proceedings, 2012. 3\n\nKwangjun Ahn, Prateek Jain, Ziwei Ji, Satyen Kale, Praneeth Netrapalli, and Gil I Shamir. Reproducibility in optimization: Theoretical framework and limits. arXiv preprint arXiv:2202.04598, 2022. 1, 3, 7\n\nJean-Yves Audibert, S ́ebastien Bubeck, and R ́emi Munos. Best arm identification in multi-armed\n\nbandits. In COLT, pp. 41–53. Citeseer, 2010. 3\n\nPeter Auer and Ronald Ortner. Ucb revisited: Improved regret bounds for the stochastic multi-armed\n\nbandit problem. Periodica Mathematica Hungarica, 61(1-2):55–65, 2010. 3\n\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit\n\nproblem. Machine learning, 47(2):235–256, 2002. 3\n\nMonya Baker. 1,500 scientists lift the lid on reproducibility. Nature, 533(7604), 2016a. 1\n\nMonya Baker. Reproducibility crisis. Nature, 533(26):353–66, 2016b. 3\n\nMihir Bellare and Phillip Rogaway. The complexity of approximating a nonlinear program.\n\nIn\n\nComplexity in numerical optimization, pp. 16–32. World Scientific, 1993. 22\n\nS ́ebastien Bubeck, Nicolo Cesa-Bianchi, and Sham M Kakade. Towards minimax policies for online linear optimization with bandit feedback. In Conference on Learning Theory, pp. 41–1. JMLR Workshop and Conference Proceedings, 2012a. 3\n\nS ́ebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multiarmed bandit problems. Foundations and Trends® in Machine Learning, 5(1):1–122, 2012b. 3\n\nNicolo Cesa-Bianchi and Paul Fischer. Finite-time regret bounds for the multiarmed bandit problem.\n\nIn ICML, volume 98, pp. 100–108. Citeseer, 1998. 3\n\nNicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir. Online learning with switching costs and other\n\nadaptive adversaries. Advances in Neural Information Processing Systems, 26, 2013. 3\n\nVarsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit\n\nfeedback. 2008. 3\n\nHossein Esfandiari, Amin Karbasi, Abbas Mehrabian, and Vahab Mirrokni. Regret bounds for batched bandits. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 7340–7348, 2021. 3, 4, 5, 8, 9\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nEyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6), 2006. 4\n\nValerii Vadimovich Fedorov. Theory of optimal experiments. Elsevier, 2013. 22\n\nRobert M Freund and James B Orlin. On the complexity of four polyhedral set containment prob-\n\nlems. Mathematical programming, 33(2):139–145, 1985. 22\n\nZijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed bandits problem.\n\nAdvances in Neural Information Processing Systems, 32, 2019. 3, 4\n\nSteven N Goodman, Daniele Fanelli, and John PA Ioannidis. What does research reproducibility\n\nmean? Science translational medicine, 8(341):341ps12–341ps12, 2016. 3\n\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. 3\n\nRussell Impagliazzo, Rex Lei, Toniann Pitassi, and Jessica Sorrell. Reproducibility in learning.\n\narXiv preprint arXiv:2201.08430, 2022. 1, 2, 3, 4, 5, 9, 20\n\nJohn PA Ioannidis. Why most published research findings are false. PLoS medicine, 2(8):e124,\n\n2005. 3\n\nEmilie Kaufmann, Olivier Capp ́e, and Aur ́elien Garivier. On bayesian upper confidence bounds for\n\nbandit problems. In Artificial intelligence and statistics, pp. 592–600. PMLR, 2012a. 3\n\nEmilie Kaufmann, Nathaniel Korda, and R ́emi Munos. Thompson sampling: An asymptotically optimal finite-time analysis. In International conference on algorithmic learning theory, pp. 199– 213. Springer, 2012b. 3\n\nPiyush Kumar and E Alper Yildirim. Minimum-volume enclosing ellipsoids and core sets. Journal\n\nof Optimization Theory and applications, 126(1):1–21, 2005. 22\n\nTor Lattimore and Csaba Szepesv ́ari. Bandit algorithms. Cambridge University Press, 2020. 3, 7,\n\n8, 22\n\nTor Lattimore, Csaba Szepesvari, and Gellert Weisz. Learning with good feature representations in bandits and in rl with a generative model. In International Conference on Machine Learning, pp. 5662–5670. PMLR, 2020. 22\n\nMario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal? a large-scale study. Advances in neural information processing systems, 31, 2018. 3\n\nOlvi L Mangasarian and T-H Shiau. A variable-complexity norm maximization problem. SIAM\n\nJournal on Algebraic Discrete Methods, 7(3):455–461, 1986. 22\n\nMarcia McNutt. Reproducibility, 2014. 3\n\nJoelle Pineau, Koustuv Sinha, Genevieve Fried, Rosemary Nan Ke, and Hugo Larochelle.\n\nIclr reproducibility challenge 2019. ReScience C, 5(2), May 2019. doi: 10.5281/zenodo.3158244. URL https://zenodo.org/record/3158244/files/article.pdf. 1, 3\n\nJoelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivi`ere, Alina Beygelzimer, Florence d’Alch ́e Buc, Emily Fox, and Hugo Larochelle. Improving reproducibility in machine learning research: a report from the neurips 2019 reproducibility program. Journal of Machine Learning Research, 22, 2021. 1, 3\n\nGil I Shamir and Dong Lin. Real world large scale recommendation systems reproducibility and\n\nsmooth activations. arXiv preprint arXiv:2202.06499, 2022. 3\n\nAleksandrs Slivkins. Introduction to multi-armed bandits. Foundations and Trends® in Machine\n\nLearning, 12(1-2):1–286, 2019. 3\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nMichael J Todd. Minimum-volume ellipsoids: Theory and algorithms. SIAM, 2016. 22\n\nStephen A Vavasis. Polynomial time weak approximation algorithms for quadratic programming.\n\nIn Complexity in numerical optimization, pp. 490–500. World Scientific, 1993. 22\n\nRoman Vershynin. High-dimensional probability: An introduction with applications in data science,\n\nvolume 47. Cambridge university press, 2018. 8\n\nYinyu Ye. On affine scaling algorithms for nonconvex quadratic programming. Mathematical Pro-\n\ngramming, 56(1):285–300, 1992. 22\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA THE PROOF OF THEOREM 3\n\nTheorem. Let T ∈ N, ρ ∈ (0, 1]. There exists a ρ-replicable algorithm (presented in Algorithm 1) for the stochastic bandit problem with K arms and gaps (∆j)j∈[K] whose expected regret is\n\nE[RT ] ≤ C ·\n\nK 2 log2(T ) ρ2\n\n(cid:88)\n\n(cid:18)\n\n∆j +\n\nj:∆j >0\n\nlog(2KT log(T )) ∆j\n\n(cid:19)\n\n,\n\nwhere C > 0 is an absolute numerical constant, and its running time is polynomial in K, T and 1/ρ.\n\nProof. First, we claim that the algorithm is ρ-replicable: since the elimination decisions are taken in the same iterates and are based solely on the mean estimations, the replicability of the algorithm of Proposition 2 implies the replicability of the whole algorithm. In particular,\n\nPr[(a1, ..., aT ) ̸= (a′\n\n1, ..., a′\n\nT )] = Pr[∃i ∈ [B], ∃j ∈ [K] : (cid:98)μ(i)\n\nj was not replicable] ≤ ρ .\n\nDuring each batch i, we draw for any active arm ⌊qi⌋ fresh samples for a total of ci samples and use the replicable mean estimation algorithm to estimate its mean. For an active arm, at the end of some batch i ∈ [B], we say that its estimation is “correct” if the estimation of its mean is within (cid:112)log(2KT B)/ci from the true mean. Using Proposition 2, the estimation of any active arm at the end of any batch (except possibly the last batch) is correct with probability at least 1 − 1/(2KT B) and so, by the union bound, the probability that the estimation is incorrect for some arm at the end of some batch is bounded by 1/T . We remark that when δ < ρ, the sample complexity of Proposition 2 reduces to O(log(1/δ)/(τ 2ρ2)). Let E denote the event that our estimates are correct. The total expected regret can be bounded as\n\nE[RT ] ≤ T · 1/T + E[RT |E] .\n\nIt suffices to bound the second term of the RHS and hence we can assume that each gap is correctly estimated within an additive factor of (cid:112)log(2KT B)/ci after batch i. First, due to the elimination condition, we get that the best arm is never eliminated. Next, we have that\n\nE[RT |E] =\n\n(cid:88)\n\nj:∆j >0\n\n∆j E[Tj|E] ,\n\nwhere Tj is the total number of pulls of arm j. Fix a sub-optimal arm j and assume that i + 1 was the last batch it was active. Since this arm is not eliminated at the end of batch i, and the estimations are correct, we have that\n\n∆j ≤ (cid:112)log(2KT B)/ci ,\n\nand so ci ≤ log(2KT B)/∆2 sition 2 is (since we need to pull an arm ci/ρ2 (cid:112)log(1/δ)/c2\n\nj . Hence, the number of pulls to get the desired bound due to Propo1 times in order to get an estimate at distance\n\ni with probability 1 − δ in a ρ1-replicable manner when δ < ρ1)\n\nTj ≤ ci+1/ρ2\n\n1 = q/ρ2\n\n1(1 + ci) ≤ q/ρ2\n\n1 · (1 + log(2KT B)/∆2\n\nj ) .\n\nThis implies that the total regret is bounded by\n\nE[RT ] ≤ 1 + q/ρ2\n\n1 ·\n\n(cid:18)\n\n(cid:88)\n\n∆j +\n\nj:∆j >0\n\nlog(2KT B) ∆j\n\n(cid:19)\n\n.\n\nWe finally set q = T 1/B and B = log(T ). Moreover, we have that ρ1 = ρ/(KB). These yield\n\nE[RT ] ≤\n\nK 2 log2(T ) ρ2\n\n(cid:88)\n\n(cid:18)\n\n∆j +\n\nj:∆j >0\n\nlog(2KT log(T )) ∆j\n\n(cid:19)\n\n.\n\nThis completes the proof.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nB THE PROOF OF THEOREM 4\n\nTheorem. Let T ∈ N, ρ ∈ (0, 1]. There exists a ρ-replicable algorithm (presented in Algorithm 2) for the stochastic bandit problem with K arms and gaps (∆j)j∈[K] whose expected regret is\n\nE[RT ] ≤ C ·\n\nK 2 ρ2\n\n(cid:88)\n\nj:∆j >0\n\n(∆j + log(KT log(T ))/∆j) ,\n\nfor some absolute numerical constant C > 0, and its running time is polynomial in K, T and 1/ρ.\n\nTo give some intuition, we begin with a non tight analysis which, however, provides the main ideas behind the actual proof.\n\nNon Tight Analysis Assume that the environment has K arms with unknown means μi and let T be the number of rounds. Consider B to the total number of batches and β > 1. We set q = T 1/B. In each batch i ∈ [B], we pull each arm β⌊ qi⌋ times. Hence, after the i-th batch, we will have drawn (cid:101)ci = (cid:80) 1≤j≤i β⌊qj⌋ independent and identically distributed samples from each arm. Let us also set ci = (cid:80) 1≤j≤i⌊qj⌋.\n\nLet us fix i ∈ [B]. Using Hoeffding’s bound for subgaussian concentration, the length of the confidence bound for arm j ∈ [K] that guarantees 1 − δ probability of success (in the sense that the empirical estimate (cid:98)μj will be close to the true μj) is equal to (cid:101)Ui = (cid:112)2 log(1/δ)/(cid:101)ci ,\n\nwhen the estimator uses (cid:101)ci samples. Also, let\n\nUi = (cid:112)2 log(1/δ)/ci . Assume that the active arms at the batch iteration i lie in the set Ai. Consider the estimates {(cid:98)μ(i) is the empirical mean of arm j using (cid:101)ci samples. We will eliminate an arm j at the end of the batch iteration i if\n\nj }i∈[B],j∈Ai , where (cid:98)μ(i)\n\nj\n\n(cid:98)μ(i) j + (cid:101)Ui ≤ max\n\nt∈Ai (cid:98)μ(i)\n\nt − U i ,\n\nwhere U i ∼ Uni[Ui/2, Ui]. For the remaining of the proof, we condition on the event E that for every arm j ∈ [K] and every batch i ∈ [B] the true mean is within (cid:101)Ui from the empirical one.\n\nj\n\ni⋆\n\n, (cid:98)μ(i)′\n\ni⋆ − (cid:98)μ(i)\n\nthe empirical estimates of arms j, i⋆ after the i-th batch, under some other execution\n\ni⋆ | ≤ 2 (cid:101)Ui. Notice that, since the randomness of U i is shared, if (cid:98)μ(i)\n\nWe first argue about the replicability of our algorithm. Consider a fixed round i (end of i-th batch) and a fixed arm j. Let i⋆ be the optimal empirical arm after the i-th batch. Let (cid:98)μ(i)′ of the algorithm. We condition on the event E ′ for the other execution as well. Notice that |(cid:98)μ(i)′ j − j | ≤ 2 (cid:101)Ui, |(cid:98)μ(i)′ (cid:98)μ(i) j + (cid:101)Ui ≥ (cid:98)μ(i) i⋆ − U i + 4 (cid:101)Ui, then the arm j will not be eliminated after the i-th batch in some other execution of the algorithm as well. Similarly, if (cid:98)μ(i) i⋆ − U i − 4 (cid:101)Ui the the arm j will get eliminated after the i-th batch in some other execution of the algorithm as well. In particular, this means that if (cid:98)μ(i) j − 2 (cid:101)Ui > (cid:98)μ(i) i⋆ + (cid:101)Ui − Ui/2 then the arm j will not get eliminated in some other execution of the algorithm and if (cid:98)μ(i) i⋆ − Ui then the arm j will also get eliminated in some other execution of the algorithm with probability 1 under the event E ∩ E ′. We call the above two cases good since they preserve replicability. Thus, it suffices to bound the probability that the decision about arm j will be different between the two executions when we are in neither of these cases. Then, the worst case bound due to the mass of the uniform probability measure is\n\nj + 5 (cid:101)Ui < (cid:98)μ(i)\n\nj + (cid:101)Ui < (cid:98)μ(i)\n\n16(cid:112)2 log(1/δ)/(cid:101)ci (cid:112)2 log(1/δ)/ci This implies that the probability mass of the bad event is at most 16(cid:112)ci/(cid:101)ci = 16(cid:112)1/β. A union\n\nbound over all arms and batches yields that the probability that two distinct executions differ in at least one pull is\n\n.\n\nPr[(a1, . . . , aT ) ̸= (a′\n\n1, . . . , a′\n\nT )] ≤ 16KB(cid:112)1/β + 2δ ,\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nand since δ ≤ ρ it suffices to pick β = 768K 2B2/ρ2.\n\nWe now focus on the regret of our algorithm. Let us set δ = 1/(KT B). Fix a sub-optimal arm j and assume that batch i + 1 was the last batch that is was active. We obtain that the total number of pulls of this arm is\n\nTj ≤ (cid:101)ci+1 ≤ βq(1 + ci) ≤ βq(1 + 8 log(1/δ)/∆2 j )\n\nFrom the replicability analysis, it suffices to take β of order K 2 log2(T )/ρ2 and so\n\nE[RT ] ≤ T ·1/T +E[RT |E] = 1+\n\n(cid:88)\n\n∆j E[Tj|E] ≤\n\nj:∆j >0\n\nC · K 2 log2(T ) ρ2\n\n(cid:18)\n\n(cid:88)\n\n∆j +\n\nj:∆j >0\n\nlog(KT log(T )) ∆j\n\n(cid:19)\n\n,\n\nfor some absolute constant C > 0.\n\nNotice that the above analysis, which uses a naive union bound, does not yield the desired regret bound. We next provide a more tight analysis of the same algorithm that achieves the regret bound of Theorem 4.\n\n(The Proof of Theorem 4) In the previous analysis, we used a union bound Improved Analysis over all arms and all batches in order to control the probability of the bad event. However, we can obtain an improved regret bound as follows. Fix a sub-optimal arm i ∈ [K] and let t be the first round that it appears in the bad event. We claim that after a constant number of rounds, this arm will be eliminated. This will shave the O(log2(T )) factor from the regret bound. Essentially, as indicated in the previous proof, the bad event corresponds to the case where the randomness of the cut-off threshold U can influence the decision of whether the algorithm eliminates an arm or not. The intuition is that during the rounds t and t + 1, given that the two intervals intersected at round t, we know that the probability that they intersect again is quite small since the interval of the optimal mean is moving upwards, the interval of the sub-optimal mean is concentrating around the guess and the two estimations have been moved by at most a constant times the interval’s length.\n\nSince the bad event occurs at round t, we know that\n\n(cid:104)\n\n(cid:98)μ(t) j ∈\n\nt⋆ − Ut − 5 (cid:101)Ut, (cid:98)μ(t) (cid:98)μ(t)\n\nt⋆ − Ut/2 + 3 (cid:101)Ut\n\n(cid:105)\n\n.\n\nIn the above (cid:98)μt assume that the bad event for arm j also occurs at round t + k. Then, we have that\n\nt⋆ is the estimate of the optimal mean at round t whose index is denoted by t⋆. Now\n\n(cid:98)μ(t+k)\n\nj\n\n∈\n\n(cid:104)\n\n(t+k)⋆ − Ut+k − 5 (cid:101)Ut+k, (cid:98)μ(t+k) (cid:98)μ(t+k)\n\n(t+k)⋆ − Ut+k/2 + 3 (cid:101)Ut+k\n\n(cid:105)\n\n.\n\nFirst, notice that since the concentration inequality under event E holds for rounds t, t + k we have that (cid:98)μ(t+k) (t+k)⋆ − Ut+k − 5 (cid:101)Ut+k ≤ (cid:98)μ(t+k) (cid:98)μ(t+k)\n\nj + (cid:101)Ut + (cid:101)Ut+k. Thus, combining it with the above inequalities gives us\n\nj + (cid:101)Ut + (cid:101)Ut+k ≤ (cid:98)μ(t)\n\nt⋆ − Ut/2 + 4 (cid:101)Ut + (cid:101)Ut+k.\n\n≤ (cid:98)μ(t)\n\n≤ (cid:98)μ(t)\n\nj\n\nj\n\nWe now compare (cid:98)μ(t)\n\nt⋆ , (cid:98)μ(t+k)\n\n(t+k)⋆ . Let o denote the optimal arm. We have that\n\n(cid:98)μ(t+k) (t+k)⋆ ≥ (cid:98)μ(t+k)\n\no\n\n≥ μo − (cid:101)Ut+k ≥ μt⋆ − (cid:101)Ut+k ≥ (cid:98)μ(t)\n\nt⋆ − (cid:101)Ut − (cid:101)Ut+k.\n\nThis gives us that\n\nt⋆ − Ut+k − 6 (cid:101)Ut+k − (cid:101)Ut ≤ (cid:98)μ(t+k) (cid:98)μ(t)\n\n(t+k)⋆ − Ut+k − 5 (cid:101)Ut+k.\n\nThus, we have established that\n\nt⋆ − Ut+k − 6 (cid:101)Ut+k − (cid:101)Ut ≤ (cid:98)μ(t) (cid:98)μ(t)\n\nt⋆ − Ut/2 + 4 (cid:101)Ut + (cid:101)Ut+k =⇒\n\nUt+k ≥ Ut/2 − 7 (cid:101)Ut+k − 5 (cid:101)Ut ≥ Ut/2 − 12 (cid:101)Ut.\n\nSince β ≥ 2304, we get that 12 (cid:101)Ut ≤ Ut/4. Thus, we get that\n\nUt+k ≥ Ut/4.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nNotice that\n\nUt+k Ut\n\n=\n\n(cid:114) ct ct+k\n\n,\n\nthus it immediately follows that\n\nct ct+k\n\n≥\n\n1 16\n\n=⇒\n\nqt+1 − 1 qt+k+1 − 1\n\n≥\n\n(cid:18)\n\n=⇒ 16\n\n1 −\n\n(cid:19)\n\n1 qt+1\n\n≥ qk −\n\n1\n\nqt+1 =⇒\n\n1 16 1\n\nqk ≤ 16 +\n\nqt+1 ≤ 17 =⇒ k log q ≤ log 17 =⇒ k ≤ 5,\n\nwhen we pick B = log(T ) batches. Thus, for every arm the bad event can happen at most 6 times, by taking a union bound over the K arms we see that the probability that our algorithm is not replicable is at most O(K(cid:112)1/β), so picking β = Θ(K 2/ρ2) suffices to get the result.\n\nC THE PROOF OF THEOREM 6\n\nTheorem. Let T ∈ N, ρ ∈ (0, 1]. There exists a ρ-replicable algorithm (presented in Algorithm 3) for the stochastic d-dimensional linear bandit problem with K arms whose expected regret is\n\nE[RT ] ≤ C ·\n\nK 2 ρ2\n\n(cid:112)dT log(KT ) ,\n\nfor some absolute numerical constant C > 0, and its running time is polynomial in d, K, T and 1/ρ.\n\nProof. Let c, C be the numerical constants hidden in Lemma 5, i.e., the size of the multi-set is in the interval [cd log(1/δ)/ε2, Cd log(1/δ)/ε2]. We know that the size of each batch ni ∈ [cqi, Cqi] (see Lemma 5), so by the end of the B − 1 batch we will have less than nB pulls left. Hence, the number of batches is at most B.\n\na, (cid:98)θi − θ⋆(cid:69)(cid:12)\n\nWe first define the event E that the estimates of all arms after the end of each batch are accurate, i.e., for every active arm a at the beginning of the i-th batch, at the end of the batch we have that (cid:12) (cid:68) (cid:12) (cid:12) (cid:12) ≤ (cid:101)εi. Since δ = 1/(KT 2) and there are at most T batches and K active arms in each (cid:12) batch, a simple union bound shows that E happens with probability at least 1 − 1/T. We condition on the event E throughout the rest of the proof.\n\nWe now argue about the regret bound of our algorithm. We first show that any optimal arm a∗ will not get eliminated. Indeed, consider any sub-optimal arm a ∈ [K] and any batch i ∈ [B]. Under the event E we have that\n\n⟨a, (cid:98)θi⟩ − ⟨a∗, (cid:98)θi⟩ ≤ (⟨a, θ∗⟩ + (cid:101)εi) − (⟨a∗, θ∗⟩ − (cid:101)εi) < 2(cid:101)εi < εi + εi. Next, we need to bound the number of times we pull some fixed suboptimal arm a ∈ [K]. We let ∆ = ⟨a∗ − a, θ∗⟩ denote the gap and we let i be the smallest integer such that εi < ∆/4. We claim that this arm will get eliminated by the end of batch i. Indeed,\n\n⟨a∗, (cid:98)θi⟩ − ⟨a, (cid:98)θi⟩ ≥ (⟨a∗, (cid:98)θi⟩ − (cid:101)εi) − (⟨a, (cid:98)θi⟩ + (cid:101)εi) = ∆ − 2(cid:101)εi > 4εi − 2(cid:101)εi > (cid:101)εi + εi. This shows that during any batch i, all the active arms have gap at most 4εi−1. Thus, the regret of the algorithm conditioned on the event E is at most\n\nB (cid:88)\n\ni=1\n\n4niεi−1 ≤ 4βC\n\nB (cid:88)\n\ni=1\n\nqi(cid:112)d log(KT 2)/qi−1 ≤ 6βCq(cid:112)d log(KT )\n\nB−1 (cid:88)\n\ni=0\n\nqi/2 ≤\n\n(cid:16)\n\n(cid:17) βqB/2+1(cid:112)d log(KT )\n\nO\n\n= O\n\n(cid:18) K 2\n\nρ2 qB/2+1(cid:112)d log(KT )\n\n(cid:19)\n\n= O\n\n(cid:18) K 2\n\n(cid:19)\n\nρ2 q(cid:112)dT log(KT ) ρ2 q(cid:112)dT log(KT )\n\n(cid:16) K2\n\n(cid:17)\n\n.\n\n=\n\nThus, (cid:16) K2\n\nO\n\nthe overall\n\nρ2 q(cid:112)dT log(KT )\n\nregret (cid:17)\n\n.\n\nis bounded by δ · T + (1 − δ) · O\n\n16\n\nPublished as a conference paper at ICLR 2023\n\ni⟩| ≤ 2(cid:101)εi, |⟨ai∗ , (cid:98)θi − (cid:98)θ′\n\nWe now argue about the replicability of our algorithm. The analysis follows in a similar fashion as in Theorem 4. Let (cid:98)θi, (cid:98)θ′ i be the LSE after the i-th batch, under two different executions of the algorithm and assume that the set of active arms. We condition on the event E ′ for the other execution as well. Assume that the set of active arms is the same under both executions at the beginning of batch i. Notice that since the set that is guaranteed by Lemma 5 is computed by a deterministic algorithm, both executions will pull the same arms in batch i. Consider a suboptimal arm a and let ai∗ = arg maxa∈A⟨(cid:98)θi, a⟩, a′ i, a⟩. Under the event E ∩ E ′ we have that i∗ , (cid:98)θ′ |⟨a, (cid:98)θi − (cid:98)θ′ i⟩ − ⟨ai∗ , (cid:98)θi⟩| ≤ 2(cid:101)εi. Notice that, since the randomness of εi is shared, if ⟨a, (cid:98)θi⟩ + (cid:101)εi ≥ ⟨ai∗ , (cid:98)θi⟩ − εi + 4(cid:101)εi, then the arm a will not be eliminated after the i-th batch in some other execution of the algorithm as well. Similarly, if ⟨a, (cid:98)θi⟩ + (cid:101)εi < ⟨ai∗ , (cid:98)θi⟩ − εi − 4(cid:101)εi the the arm a will get eliminated after the i-th batch in some other execution of the algorithm as well. In particular, this means that if ⟨a, (cid:98)θi⟩−2(cid:101)εi > ⟨ai∗ , (cid:98)θi⟩+(cid:101)εi−εi/2 then the arm a will not get eliminated in some other execution of the algorithm and if ⟨a, (cid:98)θi⟩ + 5(cid:101)εi < ⟨ai∗ , (cid:98)θi⟩ − εi then the arm j will also get eliminated in some other execution of the algorithm with probability 1 under the event E ∩ E ′. Thus, it suffices to bound the probability that the decision about arm j will be different between the two executions when we are in neither of these cases. Then, the worst case bound due to the mass of the uniform probability measure is\n\ni∗ = arg maxa∈A⟨(cid:98)θ′ i⟩| ≤ 2(cid:101)εi, and |⟨a′\n\n16(cid:112)d log(1/δ)/(cid:101)ci (cid:112)d log(1/δ)/ci This implies that the probability mass of the bad event is at most 16(cid:112)ci/(cid:101)ci = 16(cid:112)1/β. A naive union bound would require us to pick β = Θ(K 2 log2 T /ρ2). We next show to avoid the log2 T factor. Fix a sub-optimal arm a ∈ [K] and let t be the first round that it appears in the bad event.\n\n.\n\nSince the bad event occurs at round t, we know that\n\n(cid:104)\n\n⟨a, (cid:98)θt⟩ ∈\n\n⟨at∗ , (cid:98)θt⟩ − εt − 5(cid:101)εt, ⟨at∗ , (cid:98)θt⟩ − εt/2 + 3(cid:101)εt\n\n(cid:105)\n\n.\n\nIn the above, at∗ is the optimal arm at round t w.r.t. the LSE. Now assume that the bad event for arm a also occurs at round t + k. Then, we have that\n\n(cid:104)\n\n⟨a, (cid:98)θt+k⟩ ∈\n\n⟨a(t+k)∗ , (cid:98)θt+k⟩ − εt+k − 5(cid:101)εt+k, ⟨a(t+k)∗ , (cid:98)θt+k⟩ − εt/2 + 3(cid:101)εt+k\n\n(cid:105)\n\n.\n\nFirst, notice that since the concentration inequality under event E holds for rounds t, t + k we have that ⟨a, (cid:98)θt+k⟩ ≤ ⟨a, (cid:98)θt⟩ + (cid:101)εt + (cid:101)εt+k. Thus, combining it with the above inequalities gives us ⟨a(t+k)∗ , (cid:98)θt+k⟩ − εt+k − 5(cid:101)εt+k ≤ ⟨a, (cid:98)θt+k⟩ ≤ ⟨a, (cid:98)θt⟩ + (cid:101)εt + (cid:101)εt+k ≤ ⟨at∗ , (cid:98)θt⟩ − εt/2 + 4(cid:101)εt + (cid:101)εt+k. We now compare ⟨at∗ , (cid:98)θt⟩, ⟨a(t+k)∗ , (cid:98)θt+k⟩. Let a∗ denote the optimal arm. We have that\n\n⟨a(t+k)∗ , (cid:98)θt+k⟩ ≥ ⟨a∗, (cid:98)θt+k⟩ ≥ ⟨a∗, θ∗⟩ − (cid:101)εt+k ≥ ⟨at∗ , θ∗⟩ − (cid:101)εt+k ≥ ⟨at∗ , (cid:98)θt⟩ − (cid:101)εt+k − (cid:101)εt.\n\nThis gives us that\n\n⟨at∗ , (cid:98)θt⟩ − εt+k − 6(cid:101)εt+k − (cid:101)εt ≤ ⟨a(t+k)∗ , (cid:98)θt+k⟩ − εt+k − 5(cid:101)εt+k.\n\nThus, we have established that\n\n⟨at∗ , (cid:98)θt⟩ − εt+k − 6(cid:101)εt+k − (cid:101)εt ≤ ⟨at∗ , (cid:98)θt⟩ − εt/2 + 4(cid:101)εt + (cid:101)εt+k =⇒ εt+k ≥ εt/2 − 7(cid:101)εt+k − 5(cid:101)εt ≥ εt/2 − 12(cid:101)εt.\n\nSince β ≥ 2304, we get that 12(cid:101)εt ≤ εt/4. Thus, we get that\n\nεt+k ≥ εt/4.\n\nNotice that\n\nεt+k εt\n\n=\n\n(cid:115)\n\nqt qt+k ,\n\nthus it immediately follows that\n\nqt qt+k ≥\n\n1 16\n\n=⇒ qk ≤ 16 =⇒ k log q ≤ log 16 =⇒ k ≤ 4,\n\nwhen we pick B = log(T ) batches. Thus, for every arm the bad event can happen at most 5 times, by taking a union bound over the K arms we see that the probability that our algorithm is not replicable is at most O(K(cid:112)1/β), so picking β = Θ(K 2/ρ2) suffices to get the result.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nD NAIVE APPLICATION OF ALGORITHM 3 WITH INFINITE ACTION SPACE\n\nWe use a 1/T 1/(4d+2)−net that has size at most (3T ) run Algorithm 3 using A′. This gives us the following result, that is proved right after. Corollary 12. Let T ∈ N, ρ ∈ (0, 1]. There is a ρ-replicable algorithm for the stochastic ddimensional linear bandit problem with infinite arms whose expected regret is at most\n\n4d+2 . Let A′ be the new set of arms. We then\n\nd\n\nE[RT ] ≤ C ·\n\n4d+1 4d+2\n\nT\n\nρ2\n\n(cid:112)d log(T ) ,\n\nwhere C > 0 is an absolute numerical constant.\n\nProof. Since K ≤ (3T )\n\nd\n\n4d+2 , we have that\n\n⟨a, θ∗⟩ − E\n\nT sup a∈A′\n\n(cid:34) T\n\n(cid:88)\n\ni=1\n\n(cid:35)\n\n(cid:32)\n\n⟨at, θ∗⟩\n\n≤ O\n\n(cid:114)\n\n2d 4d+2\n\n(3T ) ρ2\n\ndT log\n\n(cid:16)\n\nT (3T )\n\nd 4d+2\n\n(cid:33)\n\n(cid:17)\n\n= O\n\n(cid:32)\n\n4d+1 4d+2\n\nT\n\nρ2\n\n(cid:33)\n\n(cid:112)d log(T )\n\nComparing to the best arm in A, we have that:\n\n⟨a, θ∗⟩ − E\n\nT sup a∈A\n\n⟨at, θ∗⟩\n\n(cid:34) T\n\n(cid:88)\n\ni=1\n\n(cid:35)\n\n(cid:18)\n\n=\n\nT sup a∈A\n\n⟨a, θ∗⟩ − T sup a∈A′\n\n⟨a, θ∗⟩\n\n(cid:19)\n\n+\n\n(cid:32)\n\n⟨a, θ∗⟩ − E\n\nT sup a∈A′\n\n(cid:35)(cid:33)\n\n⟨at, θ∗⟩\n\n(cid:34) T\n\n(cid:88)\n\ni=1\n\nOur choice of the 1/T 1/(4d+2)-net implies that for every a ∈ A there exists some a′ ∈ A′ such that ||a − a′||2 ≤ 1/T 1/(4d+2). Thus, supa∈A⟨a, θ∗⟩ − supa′∈A′⟨a′, θ∗⟩ ≤ ||a − a′||2||θ∗||2 ≤ 1/T 1/(4d+2). Thus, the total regret is at most\n\nT · 1/T 1/(4d+2) + O\n\n(cid:32)\n\n4d+1 4d+2\n\nT\n\nρ2\n\n(cid:33)\n\n(cid:112)d log(T )\n\n= O\n\n(cid:32)\n\n4d+1 4d+2\n\nT\n\nρ2\n\n(cid:112)d log(T )\n\n(cid:33)\n\n.\n\nE THE PROOF OF THEOREM 10\n\nTheorem. Let T ∈ N, ρ ∈ (0, 1]. There exists a ρ-replicable algorithm (presented in Algorithm 4) for the stochastic d-dimensional linear bandit problem with infinite action set whose expected regret is\n\nd4 log(d) log2 log(d) log log log(d) ρ2 for some absolute numerical constant C > 0, and its running time is polynomial in T d and 1/ρ.\n\nT log3/2(T ) ,\n\nE[RT ] ≤ C ·\n\n√\n\nProof. First, the algorithm is ρ-replicable since in each batch we use a replicable LSE sub-routine with parameter ρ′ = ρ/B. This implies that\n\nPr[(a1, ..., aT ) ̸= (a′\n\n1, ..., a′\n\nT )] = Pr[∃i ∈ [B] : (cid:98)θi was not replicable] ≤ ρ .\n\nLet us fix a batch iteration i ∈ [B − 1]. Set Ci be the core set computed by Lemma 7. The algorithm first pulls ni = Cd4 log(d/δ) log2 log(d) log log log(d) times each one of the arms of the i-th core set Ci, as\n\nε2\n\ni ρ′2\n\nindicated by Lemma 9 and computes the LSE (cid:98)θi in a replicable way using the algorithm of Lemma 9. Let E be the event that over all batches the estimations are correct. We pick δ = 1/(2|A′|T 2) so that this good event does hold with probability at least 1 − 1/T . Our goal is to control the expected regret which can be written as\n\nWe have that\n\nE[RT ] = T sup a∈A\n\n⟨a, θ⋆⟩ − E\n\nT (cid:88)\n\n⟨at, θ⋆⟩ .\n\nt=1\n\nT sup a∈A\n\n⟨a, θ⋆⟩ − T sup a′∈A′\n\n⟨a′, θ⋆⟩ ≤ 1 ,\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nsince A′ is a deterministic 1/T -net of A. Also, let us set the expected regret of the bounded action sub-problem as\n\nE[R′\n\nT ] = T sup a′∈A′\n\n⟨a′, θ⋆⟩ − E\n\nT (cid:88)\n\nt=1\n\n⟨at, θ⋆⟩ .\n\nWe can now employ the analysis of the finite arm case. During batch i, any active arm has gap at most 4εi−1, so the instantaneous regret in any round is not more than 4εi−1. The expected regret conditional on the good event E is upper bounded by\n\nE[R′\n\nT |E] ≤\n\nB (cid:88)\n\ni=1\n\n4Miεi−1 ,\n\nwhere Mi is the total number of pulls in batch i (using the replicability blow-up) and εi−1 is the error one would achieve by drawing qi samples (ignoring the blow-up). Then, for some absolute constant C > 0, we have that\n\nE[R′\n\nT |E] ≤\n\n(cid:18)\n\n4\n\nB (cid:88)\n\ni=1\n\nqi d3 log(d) log2 log(d) log log log(d) log2 T\n\nρ2\n\n(cid:19)\n\n· (cid:112)d2 log(T )/qi−1 ,\n\nwhich yields that\n\nE[R′\n\nT |E] ≤ C\n\nd4 log(d) log2 log(d) log log log(d) log(T )(cid:112)log(T ) ρ2\n\n· S ,\n\nwhere we set\n\nS :=\n\nB (cid:88)\n\ni=1\n\nqi q(i−1)/2\n\n= q1/2\n\nB (cid:88)\n\ni=1\n\nWe pick B = log(T ) and get that, if q = T 1/B then S = Θ( is valid since\n\nqi/2 = q(1+B)/2 .\n\n√\n\nT ). We remark that this choice of q\n\nB (cid:88)\n\ni=1\n\nqi =\n\nqB+1 − q q − 1\n\n= Θ(qB) − 1 ≥\n\nT ρ2 d3 log(d) log2 log(d) log log log(d)\n\n.\n\nHence, we have that\n\nE[R′\n\nT |E] ≤ O\n\n(cid:18) d4 log(d) log2 log(d) log log log(d) ρ2\n\n√\n\nT log3/2(T )\n\n(cid:19)\n\n.\n\nNote that when E does not hold, we can bound the expected regret by 1/T · T = 1. This implies that the overall regret E[RT ] ≤ 2 + E[R′ T |E] and so it satisfies the desired bound and the proof is complete.\n\nF DEFERRED LEMMATA\n\nF.1 THE PROOF OF LEMMA 8\n\nProof. Consider the distribution π that is a 2-approximation to the optimal G-design and has support |C| = O(d log log d). Let C′ be the set of arms in the support such that π(a) ≤ c/d log d. We consider (cid:101)π = (1 − x)π + xa, where a ∈ C′ and x will be specified later. Consider now the matrix V ((cid:101)π). Using the Sherman-Morrison formula, we have that\n\nV ((cid:101)π)−1 =\n\n1 1 − x\n\nV (π)−1 −\n\nConsider any arm a′. Then,\n\nxV (π)−1aa⊤V (π)−1\n\n(cid:16)\n\n(1 − x)2\n\n1 + 1\n\n1−x ||a||2\n\nV (π)−1\n\n(cid:17) =\n\n1 1 − x\n\n(cid:32)\n\nV (π)−1 −\n\nxV (π)−1aa⊤V (π)−1\n\n1 − x + ||a||2\n\nV (π)−1\n\n(cid:33)\n\n.\n\n||a′||2\n\nV ((cid:101)π)−1 =\n\n1 1 − x\n\n||a||2\n\nV (π)−1 −\n\nx 1 − x\n\n·\n\n(a⊤V (π)−1a′)2\n\n1 − x + ||a||2\n\nV (π)−1\n\n≤\n\n1 1 − x\n\n||a||2\n\nV (π)−1.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nNote that we apply this transformation at most O(d log log d) times. Let (cid:98)π be the distribution we end up with. We see that\n\n||a′||2\n\nV ((cid:98)π)−1 ≤\n\n(cid:18) 1\n\n(cid:19)cd log log d\n\n1 − x\n\n||a||2\n\nV (π)−1 ≤ 2\n\n(cid:18) 1\n\n(cid:19)cd log log d\n\n1 − x\n\nd.\n\nNotice that there is a constant c′ such that when x = c′/d log d we have that\n\n(cid:17)cd log log d\n\n(cid:16) 1\n\n1−x\n\n≤ 2.\n\nMoreover, notice that the mass of every arm is at least x(1 − x)|C| ≥ x − |C|x2 = c′/(d log(d)) − c′′d log log d/(d2 log2(d)) ≥ c/(d log(d)), for some absolute numerical constant c > 0. This concludes the claim.\n\nF.2 THE PROOF OF LEMMA 9\n\nProof. The proof works when we can treat Ω(⌈d log(1/δ)π(a)/ε2⌉) as Ω(d log(1/δ)π(a)/ε2), i.e., as long as π(a) = Ω(ε2/d log(1/δ)). In the regime we are in, this point is handled thanks to Lemma 8. Combining the following proof with Lemma 8, we can obtain the desired result.\n\nthe arms ai are deterministically choWe underline that we work in the fixed design setting: sen independently of the rewards ri. Assume that the core set of Lemma 7 is the set C. Fix the multi-set S = {(ai, ri) : i ∈ [M ]}, where each arm a lies in the core set and is pulled na = Θ(π(a)d log(d) log(|C|/δ)/ε2) times2. Hence, we have that\n\nM =\n\n(cid:88)\n\na∈C\n\nna = Θ (cid:0)d log(d) log(|C|/δ)/ε2(cid:1) .\n\nLet also V = (cid:80)\n\ni∈[M ] aia⊤\n\ni . The least-squares estimator can be written as LSE = V −1 (cid:88) θ(ε)\n\nairi = V −1 (cid:88)\n\n(cid:88)\n\nri(a) ,\n\na\n\ni∈[M ]\n\na∈C\n\ni∈[na]\n\nwhere each a lies in the core set (deterministically) and ri(a) is the i-th reward generated independently by the linear regression process ⟨θ⋆, a⟩+ξ, where ξ is a fresh zero mean sub-gaussian random variable. Our goal is to reproducibly estimate the value (cid:80) i∈[na] ri(a) for any a. This is sufficient since two independent executions of the algorithm share the set C and na for any a. Note that the above sum is a random variable. In the following, we condition on the high-probability event that the average reward of the arm a is ε-close to the expected one, i.e., the value ⟨θ⋆, a⟩. This happens with probability at least 1 − δ/(2|C|), given Ω(π(a)d log(d) log(|C|/δ)/ε2) samples from arm a ∈ C. In order to guarantee replicability, we will apply a result from Impagliazzo et al. (2022). Since we will union bound over all arms in the core set and |C| = O(d log log(d)) (via Lemma 7), we will make use of a (ρ/|C|)-replicable algorithm that gives an estimate v(a) ∈ R such that\n\n|⟨θ⋆, a⟩ − v(a)| ≤ τ ,\n\nwith probability at least 1 − δ/(2|C|). For δ < ρ, the algorithm uses\n\nSa = Ω (cid:0)d2 log(d/δ) log2 log(d) log log log(d)/(ρ2τ 2)(cid:1)\n\nmany samples from the linear regression with fixed arm a ∈ C. Since we have conditioned on the randomness of ri(a) for any i, we get (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) ri(a) − v(a) (cid:12) (cid:12) (cid:12)\n\n+ |⟨θ∗, a⟩ − v(a)| ≤ ε + τ ,\n\nri(a) − ⟨θ∗, a⟩\n\n1 na\n\n1 na\n\ni∈[na]\n\ni∈[na]\n\n(cid:88)\n\n(cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\nwith probability at least 1 − δ/(2|C|). Hence, by repeating this approach for all arms in the core set, we set θSQ = V −1 (cid:80) LSE. We have that\n\na∈C a na v(a). Let us condition on the randomness of the estimate θ(ε)\n\nsup a′∈A\n\n|⟨a′, θSQ − θ⋆⟩| ≤ sup\n\na′∈A\n\n|⟨a′, θSQ − θ(ε)\n\nLSE⟩| + sup\n\na′∈A\n\n|⟨a′, θ(ε)\n\nLSE − θ⋆⟩| .\n\n2Recall that π(a) ≥ c/(d log(d)), for some constant c > 0, so the previous expression is Ω(log(δ/|C|)/ε2).\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nNote that the second term is ε with probability at least 1 − δ via Lemma 5. Our next goal is to tune the accuracy τ ∈ (0, 1) so that the first term yields another ε error. For the first term, we have that\n\nsup a′∈A\n\n|⟨a′, θSQ − θ(ε)\n\nLSE⟩| ≤ sup\n\na′∈A\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n⟨a′, V −1 (cid:88)\n\na∈C\n\n(cid:12) (cid:12) (cid:12) a na (ε + τ )⟩ (cid:12) (cid:12)\n\nNote that V = Cd log(d) log(|C|/δ) some absolute constant C > 0. This implies that\n\nε2\n\n(cid:80)\n\na∈C π(a)aa⊤ and so V −1 =\n\nCd log(d) log(|C|/δ) V (π)−1, for\n\nε2\n\nsup a′∈A\n\n|⟨a′, θSQ−θ(ε)\n\nLSE⟩| ≤ (ε+τ ) sup\n\na′∈A\n\n(cid:42)\n\na′,\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nε2 Cd log(d) log(|C|/δ)\n\nV (π)−1 (cid:88)\n\na∈C\n\nCd log(d) log(|C|/δ)π(a) ε2\n\na\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\nHence, we get that\n\nsup a′∈A\n\n|⟨a′, θSQ − θ(ε)\n\nLSE⟩| ≤ (ε + τ ) sup\n\na′∈A\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:42)\n\na′, V (π)−1 (cid:88)\n\na∈C\n\nπ(a)a\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\nConsider a fixed arm a′ ∈ A. Then, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\na′, V (π)−1 (cid:88)\n\n(cid:42)\n\na∈C\n\nπ(a)a\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n≤\n\n(cid:88)\n\na∈C (cid:88)\n\na∈C\n\n= 1 +\n\nπ(a) (cid:12)\n\n(cid:12)⟨a′, V (π)−1a⟩(cid:12) (cid:12)\n\n(cid:16)\n\nπ(a)\n\n1 + (cid:12)\n\n(cid:12)⟨a′, V (π)−1a⟩(cid:12) (cid:12)\n\n2(cid:17)\n\nπ(a) (cid:12)\n\n(cid:12)⟨a′, V (π)−1a⟩(cid:12) (cid:12)\n\n2\n\n(cid:88)\n\na∈C\n\n= 1 + ||a′||2\n\nV (π)−1\n\n≤ 4d + 1 ,\n\nwhere the last inequality follows from the fact that π is a 4-approximation of the G-optimal design. Hence, in total, by picking τ = ε, we get that\n\n|⟨a′, θSQ − θ⋆⟩| ≤ 11dε .\n\nsup a′∈A\n\nThus, for any ε > 0, the total number of pulls of each arm is\n\nΩ (cid:0)d4 log(d/δ) log2 log(d) log log log(d)/(ρ2ε2)(cid:1) ,\n\nto get\n\n|⟨a′, θSQ − θ⋆⟩| ≤ ε .\n\nsup a′∈A\n\nG COMPUTATIONAL PERFORMANCE OF ALGORITHM 4\n\nIn this appendix, we discuss the barriers towards computational efficiency regarding Algorithm 4. The reasons why Algorithm 4 is computationally inefficient are the following: (a) we have to compute the arm in the set of active arms that has maximum correlation with the estimate (cid:98)θi, (b) we have to eliminate arms based on this value and (c) we have to run at each batch the Frank-Wolfe algorithm (or some other optimization method needed for Lemma 5) in order to obtain an approximate G-optimal design. As a minimal assumption in what follows, we focus on the case where the action set A is convex and we have access to a separation oracle for it.\n\nNote that executing both (a) and (b) naively requires time exponential in d. However, on the one side arm elimination (issue (b)) reduces to finding the intersection of the current active set with a halfspace H whose normal vector is (cid:98)θi and the threshold is, roughly speaking, the maximum correlation. This maximum correlation can also be computed efficiently. Finding an arm with (almost) maximum correlation relates to the problem of finding a point that maximizes a linear\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nobjective under the constraint that the point lies in the intersection of the active arm set with some linear constraints. Thus, we can use the ellipsoid algorithm to implement this step.\n\nThe above discussion deals with issues (a) and (b) and, essentially, states that even with infinitely many actions, one could implement these steps efficiently. We now focus on issue (c). The FrankWolfe method first requires a proper initialization. As mentioned in Lattimore & Szepesv ́ari (2020), if the starting point is chosen to be the uniform distribution over A′, then the number of iterations before getting a 2-approximate optimal design is roughly (cid:101)O(d). The issue is that since A′ is exponential in d, it is not clear how to work with such an initialization efficiently. Notably there is a different initialization Fedorov (2013); Lattimore et al. (2020) with support O(d) for which the method runs in O(d log log(d)) rounds (see Note 3 at Section 21.2 of Lattimore & Szepesv ́ari (2020) and Lattimore et al. (2020)). There are two issues: first, one requires an oracle to provide this good initialization. Second, each iteration of the Frank-Wolfe method (with current design guess π) requires computing a point in the current active set with maximum V (π)−1-norm. As noted in Todd (2016), a good initialization for finding a G-optimal design, i.e., a minimum volume enclosing ellipsoid (MVEE) should be sufficiently sparse (compared to the number of active arms) and assign positive mass to arms that correspond to extreme points, i.e., points that are close to the border of MVEE. The work of Kumar & Yildirim (2005) provides an initial core set that depends only on d but not on the number of points. The algorithm works as follows: it runs for d iterations and, in each round, it adds 2 arms into the core set. Initially, we set the core set C0 = ∅ and let Ψ = {0}. In each iteration i ∈ [d], the algorithm draws a random direction vi in the orthogonal complement of Ψ (this step is replicable thanks to the shared randomness) and computes the vectors in the active arms’ set with the maximum and the minimum correlation with vi, say a+ i . It then extends C0 ← C0 ∪ {a+ i }). Hence, the runtime of this algorithm corresponds to the runtime of the tasks maxa∈A′⟨a, vi⟩ and mina∈A′⟨a, vi⟩. One can efficiently approximate these values using the ellipsoid algorithm and hence efficiently initialize the Frank-Wolfe algorithm as in Todd (2016) (e.g., set the weights uniformly 1/(2d)).\n\ni } and sets Ψ ← span(Ψ, {a+\n\ni − a−\n\ni , a−\n\ni , a−\n\nOur second challenge deals with finding a point in the active arm set with maximum V (π)−1norm for some current guess π. Even if the current active set is a polytope, finding an exact norm maximizer is NP-hard Freund & Orlin (1985); Mangasarian & Shiau (1986)3. Hence, one should focus on efficient approximation algorithms. We note that even a poly(d)-approximate maximizer T ) regret. Such an algorithm for polytopes, which gets an 1/d2is sufficient to get (cid:101)O(poly(d) approximation, is provided in Ye (1992); Vavasis (1993).\n\n√\n\nAs a general note, if we assume that we have access to an oracle O that computes a 2-approximate G-optimal design in time TO, then our Algorithm 4 runs in time polynomial in TO.\n\nH EXPERIMENTAL EVALUATION\n\nIn this section, we provide some experimental evaluation of our proposed algorithms in the setting of multi-armed stochastic bandits. In particular, we will compare the performance of our algorithm with the standard UCB approach. We first analyze the experimental setup.\n\nExperimental Setup. We consider a multi-armed bandit setting with K = 6 arms that have Bernoulli rewards with bias (0.44, 0.47, 0.5, 0.53, 0.56, 0.59), we run the algorithms for T = 45000 iterations and we execute both algorithms 20 different times. For our algorithm, we set its replicability parameter ρ = 0.3. We are interested in comparing the replicability and the regret of the two algorithms. We underline that our theoretical bound in this setting shows that the regret of our algorithm could be, roughly, at most 360 times worse than the regret of UCB.\n\nObservations. We first observe that our proposed bandit algorithm is indeed replicable, i.e., it satisfies our proposed reproducibility/replicability definition. In particular, for the 10 pairs of consecutive executions we consider, we observe that our algorithm pulls the exact same sequence of arms in all of them. On the other hand, the standard UCB algorithm is far from being replicable. To be more precise, between consecutive executions we see that the algorithm makes a different choice\n\n3In fact, even finding a constant factor approximation, for some appropriate constant, is NP-hard Bellare &\n\nRogaway (1993).\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Cumulative Regret UCB vs. Replicable Batched Algorithm\n\nbetween 26208 to 28002 rounds out of the 45000 rounds, which is more that half of the rounds! However, the regret of our approach is quite competitive compared to the standard UCB algorithm. In particular, the regret of the standard UCB algorithm has the expected polylog(T ) shape, while the regret of our replicable variant incurs a multiplicative overhead that is, roughly, 2.5 times worse than UCB. This is depicted in Figure 1, where we plot the average cumulative regret (across the 20 executions) of the two algorithms.\n\n23",
    "reference": "# Summary Of The Paper\n\nThis paper studies the stochastic multi-armed bandit problem (K-armed as well as the linear version) under \"reproducibility constraints,\" i.e., the policy should play the same sequence of arms in any two i.i.d. instances of the problem (using the same algorithmic random seed) with probability at least $1-\\rho$. The authors propose $\\rho$-reproducible policies with rate-optimal regret (w.r.t. $T$).\n\n# Strength And Weaknesses\n\nThe paper is technically sound. It is insightful to see that while standard bandit algorithms are not reproducible in general, one can with only a slight multiplicative increase in sample complexity ensure reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nIt certainly is an interesting problem to study from a mathematical perspective; the authors point to antecedents in the literature that underscore the importance of reproducibility. The main technical contribution (in my opinion) is showing that one can get rate-optimality of regret w.r.t. $T$ while guaranteeing reproducibility simultaneously. The paper is well written and appears comprehensive in fleshing out connections to extant literature.\n\nQuestion: Is the $1/\\rho^2$-scaling of the upper bounds best possible w.r.t. $\\rho$? Can you please elaborate on this in the paper?\n\n# Summary Of The Review\n\nBased on my assessment of this paper's contributions, I vote for an acceptance.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSCALEFORMER: ITERATIVE MULTI-SCALE REFINING TRANSFORMERS FOR TIME SERIES FORECASTING\n\nMohammad Amin Shabani, Simon Fraser University, Canada & Borealis AI, Canada mshabani@sfu.ca\n\nAmir Abdi, Lili Meng, Tristan Sylvain Borealis AI, Canada {firstname.lastname}@borealisai.com\n\nABSTRACT\n\nThe performance of time series forecasting has recently been greatly improved by the introduction of transformers. In this paper, we propose a general multi-scale framework that can be applied to the state-of-the-art transformer-based time series forecasting models (FEDformer, Autoformer, etc.). By iteratively refining a forecasted time series at multiple scales with shared weights, introducing architecture adaptations, and a specially-designed normalization scheme, we are able to achieve significant performance improvements, from 5.5% to 38.5% across datasets and transformer architectures, with minimal additional computational overhead. Via detailed ablation studies, we demonstrate the effectiveness of each of our contributions across the architecture and methodology. Furthermore, our experiments on various public datasets demonstrate that the proposed improvements outperform their corresponding baseline counterparts. Our code is publicly available in https://github.com/BorealisAI/scaleformer.\n\n1\n\nINTRODUCTION\n\nIntegrating information at different time scales is essential to accurately model and forecast time series (Mozer, 1991; Ferreira et al., 2006). From weather patterns that fluctuate both locally and globally, as well as throughout the day and across seasons and years, to radio carrier waves which contain relevant signals at different frequencies, time series forecasting models need to encourage scale awareness in learnt representations. While transformer-based architectures have become the mainstream and stateof-the-art for time series forecasting in recent years, advances have focused mainly on mitigating the standard quadratic complexity in time and space, e.g., attention (Li et al., 2019; Zhou et al., 2021) or structural changes (Xu et al., 2021; Zhou et al., 2022b), rather than explicit scale-awareness. The essential cross-scale feature relationships are often learnt implicitly, and are not encouraged by architectural priors of any kind beyond the stacked attention blocks that characterize the transformer models. Autoformer (Xu et al., 2021) and Fedformer (Zhou et al., 2022b) introduced some emphasis on scale-awareness by enforcing different computational paths for the trend and seasonal components of the input time series; however, this structural prior only focused on two scales: low- and high-frequency components. Given their importance to forecasting, can we make transformers more scale-aware?\n\nFigure 1: Intermediate forecasts by our model at different time scales. Iterative refinement of a time series forecast is a strong structural prior that benefits time series forecasting.\n\nWe enable this scale-awareness with Scaleformer. In our proposed approach, showcased in Figure 1, time series forecasts are iteratively refined at successive time-steps, allowing the model to better capture the inter-dependencies and specificities of each scale. However, scale itself is not sufficient. Iterative refinement at different scales can cause significant distribution shifts between intermediate\n\n1\n\nCoarse ForecastingFine ForecastingTimeValueValueValueValueGroundTruthPredictionGroundTruthPredictionGroundTruthPredictionGroundTruthPrediction0501001502000204060801000102030405005101520250.50.0-0.5-1.0-1.50.50.0-0.5-1.0-1.50.50.0-0.5-1.0-1.50.0-0.2-0.5-0.6-0.8-1.0-1.2Published as a conference paper at ICLR 2023\n\nforecasts which can lead to runaway error propagation. To mitigate this issue, we introduce cross-scale normalization at each step.\n\nOur approach re-orders model capacity to shift the focus on scale awareness, but does not fundamentally alter the attention-driven paradigm of transformers. As a result, it can be readily adapted to work jointly with multiple recent time series transformer architectures, acting broadly orthogonally to their own contributions. Leveraging this, we chose to operate with various transformer-based backbones (e.g. Fedformer, Autoformer, Informer, Reformer, Performer) to further probe the effect of our multi-scale method on a variety of experimental setups.\n\nOur contributions are as follows: (1) we introduce a novel iterative scale-refinement paradigm that can be readily adapted to a variety of transformer-based time series forecasting architectures. (2) To minimize distribution shifts between scales and windows, we introduce cross-scale normalization on outputs of the Transformer. (3) Using Informer and AutoFormer, two state-of-the-art architectures, as backbones, we demonstrate empirically the effectiveness of our approach on a variety of datasets. Depending on the choice of transformer architecture, our mutli-scale framework results in mean squared error reductions ranging from 5.5% to 38.5%. (4) Via a detailed ablation study of our findings, we demonstrate the validity of our architectural and methodological choices.\n\n2 RELATED WORKS\n\nTime-series forecasting: Time-series forecasting plays an important role in many domains, including: weather forecasting (Murphy, 1993), inventory planning (Syntetos et al., 2009), astronomy (Scargle, 1981), economic and financial forecasting (Krollner et al., 2010). One of the specificities of time series data is the need to capture seasonal trends (Brockwell & Davis, 2009). There exits a vast variety of time-series forecasting models (Box & Jenkins, 1968; Hyndman et al., 2008; Salinas et al., 2020; Rangapuram et al., 2018; Bai et al., 2018; Wu et al., 2020). Early approaches such as ARIMA (Box & Jenkins, 1968) and exponential smoothing models (Hyndman et al., 2008) were followed by the introduction of neural network based approaches involving either Recurrent Neural Netowkrs (RNNs) and their variants (Salinas et al., 2020; Rangapuram et al., 2018; Salinas et al., 2020) or Temporal Convolutional Networks (TCNs) (Bai et al., 2018).\n\nMore recently, time-series Transformers (Wu et al., 2020; Zerveas et al., 2021; Tang & Matteson, 2021) were introduced for the forecasting task by leveraging self-attention mechanisms to learn complex patterns and dynamics from time series data. Informer (Zhou et al., 2021) reduced quadratic complexity in time and memory to O(L log L) by enforcing sparsity in the attention mechanism with the ProbSparse attention. Yformer (Madhusudhanan et al., 2021) proposed a Y-shaped encoderdecoder architecture to take advantage of the multi-resolution embeddings. Autoformer (Xu et al., 2021) used a cross-correlation-based attention mechanism to operate at the level of subsequences. FEDformer (Zhou et al., 2022b) employs frequency transform to decompose the sequence into multiple frequency domain modes to extract the feature, further improving the performance of Autoformer.\n\nMulti-scale neural architectures: Multi-scale and hierarchical processing is useful in many domains, such as computer vision (Fan et al., 2021; Zhang et al., 2021; Liu et al., 2018), natural language processing (Nawrot et al., 2021; Subramanian et al., 2020; Zhao et al., 2021) and time series forecasting (Chen et al., 2022; Ding et al., 2020). Multiscale Vision Transformers (Fan et al., 2021) is proposed for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models, however, it focuses on the spatial domain, specially designed for computer vision tasks. Cui et al. (2016) proposed to use different transformations of a time series such as downsampling and smoothing in parallel to the original signal to better capture temporal patterns and reduce the effect of random noise. Many different architectures have been proposed recently (Chung et al., 2016; Che et al., 2018; Shen et al., 2020; Chen et al., 2021) to improve RNNs in tasks such as language processing, computer vision, time-series analysis, and speech recognition. However, these methods are mainly focused on proposing a new RNN-based module which is not applicable to transformers directly. The same direction has been also investigated in Transformers, TCN, and MLP models. Recent work Du et al. (2022) proposed multi-scale segment-wise correlations as a multi-scale version of the self-attention mechanism. Our work is orthogonal to the above methods\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Overview of the proposed Scaleformer framework. (Left) Representation of a single scale block. In each step, we pass the normalized upsampled output from previous step along with the normalized downsampled encoder as the input. (Right) Representation of the full architecture. We process the input in a multi-scale manner iteratively from the smallest scale to the original scale.\n\nas a model-agnostic framework to utilize multi-scale time-series in transformers while keeping the number of parameters and time complexity roughly the same.\n\n3 METHOD\n\nIn this section, we first introduce the problem setting in Sec. 3.1, then describe the proposed framework in Sec. 3.2, and the normalization scheme in Sec. 3.3. We provide details on the input’s representation in Sec. 3.4, and the loss function in Sec. 3.5.\n\n3.1 PROBLEM SETTING\n\nWe denote X(L) and X(H) the look-back and horizon windows for the forecast, respectively, of corresponding lengths (cid:96)L, (cid:96)H . Given a starting time t0 we can express these time-series of dimension dx, as follows: X(L) = {xt|xt ∈ Rdx, t ∈ [t0, t0 + (cid:96)L]} and X(H) = {xt|xt ∈ Rdx, t ∈ [t0 + (cid:96)L + 1, t0 + (cid:96)L + (cid:96)H ]}. The goal of the forecasting task is to predict the horizon window X(H) given the look-back window X(L).\n\n3.2 MULTI-SCALE FRAMEWORK\n\nOur proposed framework applies successive transformer modules to iteratively refine a time-series forecast, at different temporal scales. The proposed framework is shown in Figure 2.\n\nGiven an input time-series X(L), we iteratively apply the same neural module mutliple times at different temporal scales. Concretely, we consider a set of scales S = {sm, ..., s2, s1, 1} (i.e. for the default scale of s = 2, S is a set of consecutive powers of 2), where m = (cid:98)logs (cid:96)L(cid:99) − 1 and s is a downscaling factor. The input to the encoder at the i-th step (0 ≤ i ≤ m) is the original look-back window X(L), downsampled by a scale factor of si ≡ sm−i via an average pooling operation. The input to the decoder, on the other hand, is Xout i−1 upsampled by a factor of s via a linear interpolation.\n\n3\n\nLoss functionUpsampleCross-scale NormalizationPoolingX\"!\"#$Loss functionUpsampleCross-scale NormalizationPoolingForecastingModuleLoss functionCross-scale NormalizationPoolingForecastingModuleLoss functionUpsampleCross-scale NormalizationForecastingModuleX!\"#,%&’X!\"#$%&Step 0Step iStep mStep iX\"!%\"$X!&’(X!)*&’(X\"#$X!\"#$X!%\"$X!$%&X’$%&X($%&X()*+X*,+Published as a conference paper at ICLR 2023\n\nFinally, Xdec\n\n0\n\nis initialized to an array of 0s. The model performs the following operations:\n\nτ +si(cid:88)\n\nxτ ,\n\nτ = t × si\n\nxt,i =\n\n1 si\n\n(cid:110)\n\nτ +1 (cid:12) (cid:12) (cid:12)\n\nxt,i\n\nX(L)\n\ni =\n\nX(H)\n\ni =\n\n(cid:110)\n\nxt,i\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:111)\n\nt0 + (cid:96)L si\n\n≤ t ≤\n\nt0 si t0 + (cid:96)L + 1 si\n\n≤ t ≤\n\nt0 + (cid:96)L + (cid:96)H si\n\n(cid:111) ,\n\n(1)\n\n(2)\n\n(3)\n\ni\n\ni\n\nand X(H)\n\nwhere X(L) factor of sm−i and with the lengths of (cid:96)L,i and (cid:96)H,i, respectively. Assuming x(cid:48) of the forecasting module at step i − 1 and time t, we can define Xenc normalization:\n\nare the look-back and horizon windows at the ith step at time t with the scale t,i−1 is the output as the inputs to the\n\nand Xdec\n\ni\n\ni\n\nXenc\n\ni = X(L)\n\ni\n\nx(cid:48)(cid:48)\n\nt,i = x(cid:48)\n\n(cid:98)t/s(cid:99),i−1 + (cid:0)x(cid:48)\n\n(cid:100)t/s(cid:101),i−1 − x(cid:48)\n\n(cid:98)t/s(cid:99),i−1\n\n(cid:1) ×\n\nt − (cid:98)t/s(cid:99) s\n\nXdec\n\ni = {x(cid:48)(cid:48)\n\nt,i\n\n(cid:12) (cid:12) (cid:12)\n\nt0 + (cid:96)L + 1 si\n\n≤ t ≤\n\nt0 + (cid:96)L + (cid:96)H si\n\n}.\n\n(4)\n\n(5)\n\n(6)\n\nFinally, we calculate the error between X(H) and Xout refer to Algorithm 1 for details on the sequence of operations performed during the forward pass.\n\nas the loss function to train the model. Please\n\ni\n\ni\n\nAlgorithm 1 Scaleformer: Iterative Multi-scale Refining Transformer Require: input lookback window X(L) ∈ R(cid:96)L×dx, scale factor s,\n\na set of scales S = {sm, ..., s2, s1, 1}, Horizon length (cid:96)H , and Transformer module F .\n\ni ← AvgPool (cid:0)X(L), window size=sm−i(cid:1)\n\nXdec\n\ni ← Upsample (cid:0)Xout\n\ni−1, scale=s(cid:1)\n\n(cid:16)(cid:80)\n\nxenc∈Xenc\n\ni\n\nxenc + (cid:80)\n\nxdec∈Xdec\n\ni\n\nfor i ← 0 to m do\n\nXenc if i = 0 then Xdec\n\ni ←\n\nelse\n\n(cid:104)−→\n\n0 (cid:96)H\n\n(cid:105)\n\n(cid:96)L,i+(cid:96)H,i\n\nend if ̄μXi ← 1 ˆXdec i ← Xdec ˆXenc i ← Xenc i ← F (cid:0)Xenc Xout end for Ensure: Xout\n\ni\n\n:\n\ni − ̄μXi i − ̄μXi\n\n, Xdec i\n\n(cid:46) Equation (1) and (2) of the paper\n\n(cid:46) Equation (5) and (6) of the paper\n\nxdec(cid:17)\n\n(cid:1) + ̄μXi\n\n(cid:46) return the prediction at all scales\n\n3.3 CROSS-SCALE NORMALIZATION\n\nGiven a set of input series (Xenc ), with dimensions (cid:96)Li × dx and (cid:96)Hi × dx, respectively for the encoder and the decoder of the transformer in ith step, we normalize each series based on the temporal average of Xenc\n\n. More formally:\n\nand Xdec\n\n, Xdec i\n\ni\n\ni\n\ni\n\n ̄μXi =\n\n1 (cid:96)L,i + (cid:96)H,i\n\n\n\n\n\n(cid:88)\n\nxenc +\n\n(cid:88)\n\n\n\nxdec\n\n\n\nxenc∈Xenc\n\ni\n\nxdec∈Xdec\n\ni\n\nˆXdec\n\ni = Xdec\n\ni − ̄μXi,\n\nˆXenc\n\ni = Xenc\n\ni − ̄μXi\n\n(7)\n\n(8)\n\nwhere ̄μXi ∈ Rdx is the average over the temporal dimension of the concatenation of both look-back window and the horizon. Here, ˆXenc are the inputs of the ith step to the forecasting module.\n\nand ˆXdec\n\ni\n\ni\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: The figure shows the output results of two series using the same trained multi-scale model with and without shifting the data (left) which demonstrates the importance of normalization. On the right, we can see the distribution changes due to the downsampling of two series compared to the original scales from the Electricity dataset.\n\nDistribution shift is when the distribution of input to a model or its sub-components changes across training to deployment (Shimodaira, 2000; Ioffe & Szegedy, 2015). In our context, two distinct distribution shifts are common. First, there is a natural distribution shift between the look-back window and the forecast window (the covariate shift). Additionally, there is a distribution shift between the predicted forecast windows at two consecutive scales which is a result of the upsampling operation alongside the error accumulation during the intermediate computations. As a result, normalizing the output at a given step by either the look-back window statistics or the previously predicted forecast window statistics result in an accumulation of errors across steps. We mitigate this by considering a moving average of forecast and look-back statistics as the basis for the output normalization. While this change might appear relatively minor, it has a significant impact on the resulting distribution of outputs. The improvement is more evident when compared to the alternative approaches, namely, normalizing by either look-back or previous forecast window statistics.\n\n3.4\n\nINPUT EMBEDDING\n\nFollowing the previous works, we embed our input to have the same number of features as the hidden dimension of the model. The embedding consists of three parts: (1) Value embedding which uses a linear layer to map the input observations of each step xt to the same dimension as the model. We further concatenate an additional value 0, 0.5, or 1 respectively showing if each observation is coming from the look-back window, zero initialization, or the prediction of the previous steps. (2) Temporal Embedding which again uses a linear layer to embed the time stamp related to each observation to the hidden dimension of the model. Here we concatenate an additional value 1/si − 0.5 as the current scale for the network before passing to the linear layer. (3) We also use a fixed positional embedding which is adapted to the different scales si as follows:\n\nP E(pos, 2k, si) = sin\n\n(cid:18) pos × si\n\n(cid:19)\n\n100002k/dmodel\n\n, P E(pos, 2k + 1, si) = cos\n\n(cid:18) pos × si\n\n(cid:19)\n\n100002k/dmodel\n\n(9)\n\n3.5 LOSS FUNCTION\n\nUsing the standard MSE objective to train time-series forecasting models leaves them sensitive to outliers. One possible solution is to use objectives more robust to outliers, such as the Huber loss (Huber, 1964). However, when there are no major outliers, such objectives tend to underperform. Given the heterogeneous nature of the data, we instead utilize the adaptive loss (Barron, 2019):\n\nf (ξ, α, c) =\n\n|α − 2| α\n\n(cid:32)(cid:18) (ξ/c)2 |α − 2|\n\n(cid:19)α/2\n\n(cid:33)\n\n+ 1\n\n− 1\n\n(10)\n\n5\n\nWithout NormalizationWith NormalizationOriginal DitstributionDownsampled DistributionPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison of the MSE and MAE results for our proposed multi-scale framework version of different methods (-MSA) with respective baselines. Results are given in the multi-variate setting, for different lenghts of the horizon window. The best results are shown in Bold. Our method outperforms vanilla version of the baselines over almost all datasets and settings. The average improvement (error reduction) is shown in Green numbers at the bottom with respect the base models.\n\nMethod/ Dataset\n\nExchange\n\nFEDformer FED-MSA Autoformer Auto-MSA MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n\nInfo-MSA\n\nPerformer\n\nRef-MSA\n\nPer-MSA\n\nReformer\n\nInformer\n\n96 0.155 0.285 0.109 0.240 0.154 0.285 0.126 0.259 0.966 0.792 0.168 0.298 1.063 0.826 0.182 0.311 0.667 0.669 0.179 0.305 192 0.274 0.384 0.241 0.353 0.356 0.428 0.253 0.373 1.088 0.842 0.427 0.484 1.597 1.029 0.375 0.446 1.339 0.904 0.439 0.486 336 0.452 0.498 0.471 0.508 0.441 0.495 0.519 0.538 1.598 1.016 0.500 0.535 1.712 1.070 0.605 0.591 1.081 0.844 0.563 0.577 720 1.172 0.839 1.259 0.865 1.118 0.819 0.928 0.751 2.679 1.340 1.017 0.790 1.918 1.160 1.089 0.857 0.867 0.766 1.219 0.882\n\nWeather\n\n96 0.288 0.365 0.220 0.289 0.267 0.334 0.163 0.226 0.388 0.435 0.210 0.279 0.347 0.388 0.199 0.263 0.441 0.479 0.228 0.291 192 0.368 0.425 0.341 0.385 0.323 0.376 0.221 0.290 0.433 0.453 0.289 0.333 0.463 0.469 0.294 0.355 0.475 0.501 0.302 0.357 336 0.447 0.469 0.463 0.455 0.364 0.397 0.282 0.340 0.610 0.551 0.418 0.427 0.734 0.622 0.463 0.464 0.478 0.482 0.441 0.456 720 0.640 0.574 0.682 0.565 0.425 0.434 0.369 0.396 0.978 0.723 0.595 0.532 0.815 0.674 0.493 0.471 0.563 0.552 0.817 0.655\n\nElectricity\n\n96 0.201 0.317 0.182 0.297 0.197 0.312 0.188 0.303 0.344 0.421 0.203 0.315 0.294 0.382 0.183 0.291 0.294 0.387 0.190 0.300 192 0.200 0.314 0.188 0.300 0.219 0.329 0.197 0.310 0.344 0.426 0.219 0.331 0.331 0.409 0.194 0.304 0.305 0.400 0.200 0.310 336 0.214 0.330 0.210 0.324 0.263 0.359 0.224 0.333 0.358 0.440 0.253 0.360 0.361 0.428 0.209 0.321 0.331 0.416 0.209 0.322 720 0.239 0.350 0.232 0.339 0.290 0.380 0.249 0.358 0.386 0.452 0.293 0.390 0.316 0.393 0.234 0.340 0.304 0.386 0.228 0.335\n\nTraffic\n\nILI\n\n96 0.601 0.376 0.564 0.351 0.628 0.393 0.567 0.350 0.748 0.426 0.597 0.369 0.698 0.386 0.615 0.377 0.730 0.405 0.612 0.371 192 0.603 0.379 0.570 0.349 0.634 0.401 0.589 0.360 0.772 0.436 0.655 0.399 0.694 0.378 0.613 0.367 0.698 0.387 0.608 0.368 336 0.602 0.375 0.576 0.349 0.619 0.385 0.609 0.383 0.868 0.493 0.761 0.455 0.695 0.377 0.617 0.360 0.678 0.370 0.604 0.356 720 0.615 0.378 0.602 0.360 0.656 0.403 0.642 0.397 1.074 0.606 0.924 0.521 0.692 0.376 0.638 0.360 0.672 0.364 0.634 0.360\n\n24 3.025 1.189 2.745 1.075 3.862 1.370 3.370 1.213 5.402 1.581 3.742 1.252 3.961 1.289 3.534 1.212 4.806 1.471 3.437 1.148 32 3.034 1.201 2.748 1.072 3.871 1.379 3.088 1.164 5.296 1.587 3.807 1.272 4.022 1.311 3.652 1.235 4.669 1.455 4.055 1.248 48 2.444 1.041 2.793 1.059 2.891 1.138 3.207 1.153 5.226 1.569 3.940 1.272 4.269 1.340 3.506 1.168 4.488 1.371 4.055 1.248 64 2.686 1.112 2.678 1.071 3.164 1.223 2.954 1.112 5.304 1.578 3.670 1.234 4.370 1.385 3.487 1.177 4.607 1.404 3.828 1.224\n\nVs Ours\n\n5.6% 5.9%\n\n13.5% 9.1%\n\n38.5% 26.7%\n\n38.3% 25.2%\n\n23.3% 16.9%\n\ni − X(H)\n\nwith ξ = (Xout ) in step i. The parameters α and c, which modulate the loss sensitivity to outliers, are learnt in an end-to-end fashion during training. To the best of our knowledge, this is the first time this objective has been adapted to the context of time-series forecasting.\n\ni\n\n4 EXPERIMENTS\n\nIn this section, we first showcase the main results of our proposed approach on a variety of forecasting dataset in Sec. 4.1. Then, we provide an ablation study of the different components of our model in Sec. 4.2, and also present qualitative results in Sec. 4.3. Moreover, we discuss a series of additional extensions of our method in Sec. 4.4, shedding light on promising future directions.\n\n4.1 MAIN RESULTS\n\nBaselines: To measure the effectiveness of the proposed framework, we mainly use state-of-the-art transformer-based models FedFormer (Zhou et al., 2022b) Reformer (Kitaev et al., 2020), Performer (Choromanski et al., 2020), Informer (Zhou et al., 2021) and Autoformer (Xu et al., 2021) which are proven to have beaten other transformer-based (e.g. LogTrans (Li et al., 2019), Reformer (Kitaev et al., 2020)), RNN-based (e.g. LSTMNet (Lai et al., 2018), LSTM) and TCN (Bai et al., 2018) models. For brevity, we only keep comparisons with the transformer models in the tables.\n\nDatasets: We consider four public datasets with different characteristics to evaluate our proposed framework. Electricity Consuming Load (ECL)1 corresponds to the electricity consumption (Kwh) of 321 clients. Traffic2 aggregates the hourly occupancy rate of 963 car lanes of San Francisco bay area freeways. Weather3 contains 21 meteorological indicators, such as air temperature, humidity, etc, recorded every 10 minutes for the entirety of 2020. Exchange-Rate (Lai et al., 2018) collects the daily exchange rates of 8 countries (Australia, British, Canada, Switzerland, China, Japan, New Zealand and Singapore) from 1990 to 2016. National Illness (ILI) 4 corresponds to the weekly\n\n1https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 2https://pems.dot.ca.gov 3https://www.bgc-jena.mpg.de/wetter/ 4https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Multi-scale framework without cross-scale normalization. Correctly normalizing across different scales (as per our cross-mean normalization) is essential to obtain good performance when using the multi-scale framework.\n\nDataset\n\nFEDformer\n\nFED-MS (w/o N)\n\nAutoformer\n\nAuto-MS (w/o N)\n\nInformer\n\nInfo-MS (w/o N)\n\nMetric\n\nMSE\n\nMAE\n\nMSE\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\ny\n\nt i\nc i\nr t\nc e\nl\n\nE\n\n96 192 336 720\n\n0.288 0.368 0.447 0.640\n\n0.201 0.200 0.214 0.239\n\n0.365 0.425 0.469 0.574\n\n0.317 0.314 0.330 0.350\n\n0.300 0.424 0.531 0.714\n\n0.258 0.259 0.268 0.285\n\nMAE\n\n0.342 0.422 0.493 0.576\n\n0.356 0.357 0.364 0.368\n\nMSE\n\nMAE\n\nMSE\n\n0.267 0.323 0.364 0.425\n\n0.197 0.219 0.263 0.290\n\n0.334 0.376 0.397 0.434\n\n0.312 0.329 0.359 0.380\n\n0.191 0.281 0.376 0.439\n\n0.221 0.251 0.288 0.309\n\nMAE\n\n0.277 0.360 0.420 0.465\n\n0.337 0.357 0.380 0.397\n\nMSE\n\nMAE\n\nMSE\n\n0.388 0.433 0.610 0.978\n\n0.344 0.344 0.358 0.386\n\n0.435 0.453 0.551 0.723\n\n0.421 0.426 0.440 0.452\n\n0.402 0.393 0.566 1.293\n\n0.407 0.407 0.392 0.391\n\nMAE\n\n0.438 0.434 0.528 0.845\n\n0.465 0.469 0.461 0.453\n\nrecorded influenza-like illness patients from the US Center for Disease Control and Prevention. We consider horizon lengths of 24, 32, 48, and 64 with an input length of 32.\n\nImplementation details: Following previous work (Xu et al., 2021; Zhou et al., 2021), we pass Xenc = X(L) as the input to the encoder. While an array of zero-values would be the default to pass to the decoder, the decoder instead takes as input the second half of the look-back window padded with zeros Xdec = {xt0+(cid:96)L/2, ..., x(cid:96)L, 0, 0, ..., 0} with length (cid:96)L/2 + (cid:96)H . The hidden dimension of models is 512 with a batch size of 32. We use the Adam optimizer with a learning rate of 1e-4. The look-back window size is fixed to 96, and the horizon is varied from 96 to 720. We repeat each experiment 5 times and report average values to reduce randomness. For additional implementation details on our model and baselines please refer to Appendix A.\n\nMain results and comparison with baselines: Table 1 shows the results of the proposed framework compared to the baselines. Our proposed multi-scale framework with the adaptive loss outperforms the baselines in almost all of the experiments with an average improvement of 5.6% over FEDFormer, 13% over Autoformer and 38% over Informer which are the three most recent transformer-based architectures on MSE. We also achieved significant error reduction on MAE. The improvement is statistically significant in all cases, and in certain cases quite substantial. In particular for the exchange-rate dataset, with Informer and Reformer base models, our approach improves upon the respective baselines by over 50% averaged over the different horizon lengths.\n\nTime and memory complexity: The proposed framework uses the same number of parameters for the model as the baselines (except two parameters α and c of the Adaptive loss). Our framework sacrifices a small amount of computation efficiency for the sake of a significant performance improvement. We expand our analysis in Appendix C. As shown in Table 4 in Appendix, if we replace the operation at the final scale by an interpolation of the prior output, we can achieve improved performance over the baselines, at no computational overhead.\n\nFigure 4: Comparison of training with Adaptive loss ”-A”, multi-scale framework with MSE loss ”-MS”, Multi-scale framework and Adaptive loss ”-MSA”. It shows the combination of all our proposed contributions is essential, and results in significantly improved performance.\n\n7\n\nInformerAutoformerFEDformerElectricityExchangeTrafficWeatherElectricityExchangeTrafficWeatherElectricityExchangeTrafficWeatherMSEbaseline-MSbaseline-Abaselinebaseline-MSAPublished as a conference paper at ICLR 2023\n\nTable 3: Single-scale framework with cross scale normalization ”-N”. The cross-scale normalization (which in the single-scale case corresponds to mean-normalization of the output) does not improve the performance of the Autoformer, as it already has an internal trend-cycle normalization component. However, it does improve the results of the Informer and FEDformer.\n\nDataset\n\nFEDformer\n\nFEDformer-N\n\nAutoformer\n\nAutoformer-N\n\nInformer\n\nInformer-N\n\nMetric\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\ny\n\nt i\nc i\nr t\nc e\nl\n\nE\n\n96 192 336 720\n\n0.288 0.368 0.447 0.640\n\n0.201 0.200 0.214 0.239\n\n0.365 0.425 0.469 0.574\n\n0.317 0.314 0.330 0.350\n\n0.234 0.287 0.436 0.545\n\n0.194 0.195 0.200 0.225\n\n0.292 0.337 0.443 0.504\n\n0.307 0.304 0.310 0.332\n\n0.267 0.323 0.364 0.425\n\n0.197 0.219 0.263 0.290\n\n0.334 0.376 0.397 0.434\n\n0.312 0.329 0.359 0.380\n\n0.323 0.531 0.859 1.682\n\n0.251 0.263 0.276 0.280\n\n0.401 0.543 0.708 1.028\n\n0.364 0.372 0.388 0.385\n\n0.388 0.433 0.610 0.978\n\n0.344 0.344 0.358 0.386\n\n0.435 0.453 0.551 0.723\n\n0.421 0.426 0.440 0.452\n\n0.253 0.357 0.459 0.870\n\n0.247 0.291 0.321 0.362\n\n0.333 0.408 0.461 0.676\n\n0.356 0.394 0.416 0.434\n\n4.2 ABLATION STUDY\n\nWe present main ablation studies in this section, more ablation results are shown in Appendix G.\n\nImpact of each component: Two important components of our approach are the multi-scale framework and the use of the adaptive loss. We conduct multiple experiments (1) removing the multi-scale framework and/or (2) replacing the Adaptive loss by the MSE for training, in order to demonstrate the benefits of these two components. Figure 4 shows the effect of multi-scale and the loss function with different base models. Considering the impact of ablating the adaptive loss, we can see that for both the multi-scale and base models, training with the adaptive loss improves performance. Similarly, adding the multi-scale framework improves performance, both with and without the adaptive loss. Overall, combining the adaptive loss with the multi-scale framework results in the best performance.\n\nCross-scale normalization: As we discussed in Section 3.3, having the cross-scale normalization is crucial to avoid the distribution shifts. To confirm that, we conduct two experiments. Firstly, we use the multi-scale framework without the cross-scale normalization to argue that the error accumulation and covariate shift between the scales leads higher error compared to only a single scale. As shown in Table 2, while the multi-scale framework can get better results in a few cases, it mostly have higher errors than baselines.\n\nOn the other hand, adding the normalization with only a single scale can still help to achieve better performance by reducing the effect of covariate shift between the training and the test series. As shown in Table 3, the normalization improves the results of Informer and FEDformer consistently. The decomposition layer of the Autoformer solves a similar problem and replacing that with our normalization harms the capacity of the model.\n\n4.3 QUALITATIVE RESULTS\n\nWe have also shown the qualitative comparisons between the vanilla Informer and FEDformer versus the results of our framework in Figure 5. Most notably, in both cases our approach appears significantly better at forecasting the statistical properties (such as local variance) of the signal. Our scaleformer-based models capture trends (and other human-relevant) information better than their baselines. Despite these interesting findings, we would like to emphasize that these are randomly selected qualitative examples that may have their own limitations. For more qualitative results, please refer to Section I in the Appendix.\n\n4.4 EXTENSIONS AND DISCUSSION\n\nThe Scaleformer structural prior has been shown to be beneficial when applied to transformer-based, deterministic time series forecasting. It is not however limited to those settings. In this section, we show it can be extended to probabilisitc forecasting and non transformer-based encoders, both of which are closely coupled with our primary application. We also aim to highlight potential promising future directions.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Qualitative comparison of base models with our framework. Our multi-scale models can better capture the global trend and local variations of the signal than their baseline equivalents. To keep the figure concise and readable, we only show Informer and FEDformer, please refer to Figure 8 in Appendix for more results.\n\nWe show that our Scaleformer can improve performance in a probabilistic forecasting setting (please refer to Table 9 in Appendix for more details). We adopt the probabilistic output of DeepAR (Salinas et al., 2020), which is the most common probabilistic forecasting treatment. In this setting, instead of a point estimate, we have two prediction heads, predicting the mean μ and standard deviation σ, trained with a negative log likelihood loss (NLL). NLL and continuous ranked probability score (CRPS) are used as evaluation metrics. All other hyperparameters remain unchanged. Here, again, scaleformers continue to outperform the probabilistic Informer.\n\nWhile we have mainly focused on improving transformer-based models, they are not the only encoders. Recent models such as NHits (Challu et al., 2022) and FiLM (Zhou et al., 2022a) attain competitive performance, while assuming a fixed length univariate input/output. They are less flexible compared with variable length of multi-variate input/output, but result in strong performance and faster inference than transformers, making them interesting to consider. The Scaleformer prior demonstrates a statistically significant improvement, on average, when adapted by NHits and FiLM to iteratively refine predictions. For more details please refer to Appendix K.\n\nThe results mentioned above demonstrate that ScaleFormer can adapt to settings distinct from pointwise time-series forecasts with transformers (the primary scope of our paper), such as probabilistic forecasts and non-transformer models. We consider such directions to therefore be promising for future work.\n\n5 CONCLUSION\n\nNoting that introducing structural priors that account for multi-scale information is essential for accurate time-series forecastings, this paper proposes a novel multi-scale framework on top of recent state-the-art methods for time-series forecasting using Transformers. Our framework iteratively refines a forecasted time-series at increasingly fine-grained scales, and introduces a normalization scheme that minimizes distribution shifts between scales. These contributions result in vastly improved performance over baseline transformer architectures, across a variety of settings, and qualitatively result in forecasts that better capture the trend and local variations of the target signal. Additionally, our detailed ablation study shows that the different components synergetically work together to deliver this outcome.\n\nFor future work, it is promising to extend the preliminary work that has been done applying ScaleFormer architectures to both probabilistic forecasting and non-transformer models.\n\n9\n\nInformerInformer-MSAElectricityWeatherExchange-rateTimeTimeValueValueValueFEDformerFEDformer-MSATimeTimePublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nShaojie Bai, J. Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional\n\nand recurrent networks for sequence modeling. arXiv:1803.01271, 2018.\n\nJonathan T Barron. A general and adaptive robust loss function. In Proceedings of the IEEE/CVF\n\nConference on Computer Vision and Pattern Recognition, pp. 4331–4339, 2019.\n\nGeorge EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. Journal of\n\nthe Royal Statistical Society. Series C (Applied Statistics), 17(2):91–109, 1968.\n\nPeter J Brockwell and Richard A Davis. Time series: theory and methods. Springer Science &\n\nBusiness Media, 2009.\n\nCristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. arXiv preprint arXiv:2201.12886, 2022.\n\nZhengping Che, Sanjay Purushotham, Guangyu Li, Bo Jiang, and Yan Liu. Hierarchical deep generative models for multi-rate multivariate time series. In International Conference on Machine Learning, pp. 784–793. PMLR, 2018.\n\nLing Chen, Donghui Chen, Zongjiang Shang, Youdong Zhang, Bo Wen, and Chenghu Yang. Multiscale adaptive graph neural network for multivariate time series forecasting. arXiv preprint arXiv:2201.04828, 2022.\n\nZipeng Chen, Qianli Ma, and Zhenxi Lin. Time-aware multi-scale rnns for time series modeling. In\n\nIJCAI, 2021.\n\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2020.\n\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.\n\narXiv preprint arXiv:1609.01704, 2016.\n\nZhicheng Cui, Wenlin Chen, and Yixin Chen. Multi-scale convolutional neural networks for time\n\nseries classification. arXiv preprint arXiv:1603.06995, 2016.\n\nQianggang Ding, Sifan Wu, Hao Sun, Jiadong Guo, and Jian Guo. Hierarchical multi-scale gaussian\n\ntransformer for stock movement prediction. In IJCAI, pp. 4640–4646, 2020.\n\nDazhao Du, Bing Su, and Zhewei Wei. Preformer: Predictive transformer with multi-scale segmentwise correlations for long-term time series forecasting. arXiv preprint arXiv:2202.11356, 2022.\n\nHaoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and In Proceedings of the IEEE/CVF\n\nChristoph Feichtenhofer. Multiscale vision transformers. International Conference on Computer Vision, pp. 6824–6835, 2021.\n\nJ.Doyne Farmer, Edward Ott, and James A. Yorke. The dimension of chaotic attractors. Physica D: Nonlinear Phenomena, 7(1):153–180, 1983. ISSN 0167-2789. doi: https://doi.org/ 10.1016/0167-2789(83)90125-2. URL https://www.sciencedirect.com/science/ article/pii/0167278983901252.\n\nMarco AR Ferreira, David M Higdon, Herbert KH Lee, and Mike West. Multi-scale and hidden\n\nresolution time series models. Bayesian Analysis, 1(4):947–967, 2006.\n\nPeter J. Huber. Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics, 35(1):73 – 101, 1964. doi: 10.1214/aoms/1177703732. URL https://doi.org/10.1214/ aoms/1177703732.\n\nRob Hyndman, Anne B Koehler, J Keith Ord, and Ralph D Snyder. Forecasting with exponential\n\nsmoothing: the state space approach. Springer Science & Business Media, 2008.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448–456. PMLR, 2015.\n\nM. G. Kendall. Studies in the history of probability and statistics. where shall the history of statistics begin? Biometrika, 47(3/4):447–449, 1960. ISSN 00063444. URL http://www.jstor. org/stable/2333315.\n\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\n\nIn International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=rkgNKkHtvB.\n\nBjoern Krollner, Bruce J Vanstone, Gavin R Finnie, et al. Financial time series forecasting with\n\nmachine learning techniques: a survey. In ESANN, 2010.\n\nGuokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pp. 95–104, 2018.\n\nShiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in Neural Information Processing Systems, 32:5243–5253, 2019.\n\nPengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and Wangmeng Zuo. Multi-level wavelet-cnn for image restoration. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 773–782, 2018.\n\nCH L ́opez-Caraballo, I Salfate, JA Lazz ́us, P Rojas, M Rivera, and L Palma-Chilla. Mackey-glass noisy chaotic time series prediction by a swarm-optimized neural network. In Journal of Physics: Conference Series, volume 720, pp. 012002. IOP Publishing, 2016.\n\nMichael C Mackey and Leon Glass. Oscillation and chaos in physiological control systems. Science,\n\n197(4300):287–289, 1977.\n\nKiran Madhusudhanan, Johannes Burchert, Nghia Duong-Trung, Stefan Born, and Lars SchmidtThieme. Yformer: U-net inspired transformer architecture for far horizon time series forecasting. arXiv preprint arXiv:2110.08255, 2021.\n\nMichael C Mozer. Induction of multiscale temporal structure. Advances in neural information\n\nprocessing systems, 4, 1991.\n\nAllan H Murphy. What is a good forecast? an essay on the nature of goodness in weather forecasting.\n\nWeather and forecasting, 8(2):281–293, 1993.\n\nPiotr Nawrot, Szymon Tworkowski, Michał Tyrolski, Łukasz Kaiser, Yuhuai Wu, Christian Szegedy, and Henryk Michalewski. Hierarchical transformers are more efficient language models. arXiv preprint arXiv:2110.13711, 2021.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style\\ -high-performance-deep-learning-library.pdf.\n\nSyama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting. Advances in neural information processing systems, 31, 2018.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nDavid Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3): 1181–1191, 2020.\n\nJeffery D Scargle. Studies in astronomical time series analysis. i-modeling random processes in the\n\ntime domain. The Astrophysical Journal Supplement Series, 45:1–71, 1981.\n\nLifeng Shen, Zhuocong Li, and James Kwok. Timeseries anomaly detection using temporal hierarchical one-class network. Advances in Neural Information Processing Systems, 33:13016–13026, 2020.\n\nHidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-\n\nlikelihood function. Journal of statistical planning and inference, 90(2):227–244, 2000.\n\nSandeep Subramanian, Ronan Collobert, Marc’Aurelio Ranzato, and Y-Lan Boureau. Multi-scale\n\ntransformer language models. arXiv preprint arXiv:2005.00581, 2020.\n\nAris A Syntetos, John E Boylan, and Stephen M Disney. Forecasting for inventory planning: a\n\n50-year review. Journal of the Operational Research Society, 60(1):S149–S160, 2009.\n\nBinh Tang and David S Matteson. Probabilistic transformer for time series analysis. Advances in\n\nNeural Information Processing Systems, 34:23592–23608, 2021.\n\nNeo Wu, Bradley Green, Xue Ben, and Shawn O’Banion. Deep transformer models for time series\n\nforecasting: The influenza prevalence case. arXiv preprint arXiv:2001.08317, 2020.\n\nJiehui Xu, Jianmin Wang, Mingsheng Long, et al. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34, 2021.\n\nGeorge Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A transformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 2114–2124, 2021.\n\nPengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2998–3008, 2021.\n\nYucheng Zhao, Chong Luo, Zheng-Jun Zha, and Wenjun Zeng. Multi-scale group transformer for long sequence modeling in speech separation. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 3251–3257, 2021.\n\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of AAAI, 2021.\n\nTian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Rong Jin, et al. Film: Frequency improved legendre memory model for long-term time series forecasting. arXiv preprint arXiv:2205.08897, 2022a.\n\nTian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In Proc. 39th International Conference on Machine Learning (ICML 2022), 2022b.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA IMPLEMENTATION DETAILS\n\nOur implementation is based on the Pytorch (Paszke et al., 2019) implementation5 of the Autoformer and Informer (Xu et al., 2021). The hidden dimension of models are fixed to 512 with a batch size of 32, and we train each model for for 10 epochs with early stop enabled. To optimize the models, an Adam optimizer has been used with a learning rate of 1e-4 for the forecasting model and 1e-3 for optimizing the adaptive loss. The forecasting module is fixed to 2 encoder layers and 1 decoder layer. The look-back window size is fixed to 96, and the horizon is varied from 96 to 720. We repeat each experiment 5 times to reduce the effect of randomness in the reported values. In all experiments, the temporal scale factor s is fixed to 2.\n\nFor Informer, we train the model without any changes as the core of our framework. However, Autoformer uses a decomposition layer at the input of the decoder and does not pass the trend series to the network, which makes the model unaware of the previous predictions. To resolve this, we pass zeros as the trend and the series without decomposition as the input to the decoder. For Reformer, we used the available implementation6 of the model from Xu et al. (2021). In addition, we used the pytorch library7 of Performer (Choromanski et al., 2020) for our performer baseline using the same parameters as our Reformer model, and finally, we use the official implementation8 of FEDformer (Zhou et al., 2022b) for the FEDformer model. We fixed the number of modes to 64 following the original paper and Wavelet Enhanced Structure for the core modules. To make it consistent with our other experiments, we use the moving average with the kernel size 25 as in Xu et al. (2021), however, FEDformer (Zhou et al., 2022b) is using moving average with the kernel size of 24. Our random seed is fixed to 2022 in all experiments.\n\nB MORE MOTIVATIONS\n\nThis section aims to provide more motivations for the use of a multi-scale architecture. Let us first consider the following classical example, highlighted in section 2 of Ferreira et al. (2006), corresponding to the monthly flows of the Fraser River from January of 1913 to December of 1990. As shown in the their corresponding plot, the annual averages are strongly inter-related, pointing to the fact that seasonality alone will not suffice to model the variations. In the context of the paper, this showcases a failure mode of an ARMA model, but this failing is more general: models that do not explicitly account for inter-scale dependencies will perform poorly on similar datasets.\n\nDifferent approaches have attempted to introduce multi-scale processing (Ferreira et al., 2006; Mozer, 1991) in ways that differ from our own approach. The multi-scale temporal structure for music composition is introduced in (Mozer, 1991). Ferreira et al. (2006) proposed a time series model with rich autocorrelation structures by coupling processes evolving at different levels of resolution through time. However, their base models are constrained to simple statistical models, e.g. Autoregressive models.\n\nTo conclude, we note the following: (1) the approaches mentioned above have applied multi-scale modeling with success, and (2) we are the first work to explicitly consider a multi-scale prior by construction for transformers.\n\nC REDUCING COMPUTATIONAL COST\n\nTo obtain an estimation of the total running time of Scaleformer, the running time of each scale as the terms of a geometric progression based on the scale factor s which results the total time of 1−sm multiplied by the running time of the baseline method. In this regard, Table 4 shows the running times of different experiments where s = 2 with the batch size is 32 on a GeForce GTX 1080 Ti GPU with 64 cores. While our current code is not optimised, still the scaleformer of each method takes roughly twice of the baselines which is consistent with the mentioned formula. Note that this is the smallest scale which means that our method is bounded to twice of the baseline and considering a larger scale\n\n1−s\n\n5https://github.com/thuml/Autoformer 6https://github.com/thuml/Autoformer/blob/main/models/Reformer.py 7https://github.com/lucidrains/performer-pytorch 8https://github.com/MAZiqing/FEDformer\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Quantitative comparison of training and the test time of different experiments. Using a scale factor 2 the Scaleformer version of the baselines (showed by -MSA) should takes roughly twice of the original baseline time which is almost the same in the quantitative comparisons.\n\nMethod\n\nTraining time (s)\n\nTest time (s)\n\nOutput Length\n\n96\n\n192\n\n336\n\n720\n\nInformer Informer-MSA Autoformer Autoformer-MSA\n\n11.38 28.73 17.34 40.57\n\n13.83 31.88 23.34 51.17\n\n17.38 36.37 31.35 63.53\n\n25.31 51.15 52.34 111.84\n\n96\n\n0.79 2.42 1.94 5.22\n\n192\n\n1.12 2.63 2.29 6.06\n\n336\n\n1.21 2.87 2.88 7.04\n\n720\n\n1.23 2.96 3.38 8.10\n\nfactor can reduce the time overhead. Table 5 shows the impact of replacing the Scaleformer operation at the real scale by an interpolation of values of the previous scale, i.e., at scale s0 we do not apply the transformer but rather compute the results by linear interpolation of Xout m−1, thereby reducing the compute cost. This lower cost alternative results in better performance than the baselines, yet worse results than the full Scaleformer. This shows that adding scale is indeed more efficient, and that there is the possibility of a trade-off between further improved performance (full Scaleformer) or improved computational efficiency (interpolated Scaleformer).\n\nTable 5: Comparison of the MSE and MAE results for our proposed multi-scale framework version of Informer and Autoformer by removing the last step and using an interpolation instead (-MSAr) with the corresponding original models as the baseline. Results are given in the multi-variate setting, for different lenghts of the horizon window. The look-back window size is fixed to 96 for all experiments. The best results are shown in Bold. Our method outperforms vanilla versions of both Informer and Autoformer over almost all datasets and settings.\n\nDataset\n\nMetric\n\ne g\nn a\nh c\nx E\n\n96 192 336 720\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\ny\n\nt i\nc i\nr t\nc e\nl\n\nE\n\n96 192 336 720\n\nfi\n\nc 96 192 336 720\n\nf a\nr\n\nT\n\nAutoformer\n\nAutoformer-MSAr\n\nInformer\n\nInformer-MSAr\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\n0.154±0.01 0.356±0.09 0.441±0.02 1.118±0.04\n\n0.267±0.03 0.323±0.01 0.364±0.02 0.425±0.01\n\n0.197±0.01 0.219±0.01 0.263±0.04 0.290±0.05\n\n0.628±0.02 0.634±0.01 0.619±0.01 0.656±0.01\n\n0.285±0.00 0.428±0.05 0.495±0.01 0.819±0.02\n\n0.334±0.02 0.376±0.00 0.397±0.01 0.434±0.01\n\n0.312±0.01 0.329±0.01 0.359±0.03 0.380±0.02\n\n0.393±0.02 0.401±0.01 0.385±0.01 0.403±0.01\n\n0.132±0.02 0.418±0.22 0.736±0.25 0.773±0.26\n\n0.168±0.01 0.226±0.01 0.298±0.02 0.412±0.04\n\n0.189±0.00 0.207±0.00 0.225±0.01 0.250±0.01\n\n0.585±0.01 0.606±0.01 0.631±0.02 0.660±0.00\n\n0.265±0.02 0.466±0.12 0.629±0.10 0.709±0.13\n\n0.239±0.02 0.296±0.01 0.351±0.02 0.434±0.03\n\n0.305±0.00 0.323±0.00 0.339±0.00 0.361±0.01\n\n0.365±0.01 0.375±0.01 0.400±0.01 0.418±0.01\n\n0.966±0.10 1.088±0.04 1.598±0.08 2.679±0.22\n\n0.388±0.04 0.433±0.05 0.610±0.04 0.978±0.05\n\n0.344±0.00 0.344±0.01 0.358±0.01 0.386±0.00\n\n0.748±0.01 0.772±0.02 0.868±0.04 1.074±0.02\n\n0.792±0.04 0.842±0.01 1.016±0.02 1.340±0.06\n\n0.435±0.03 0.453±0.03 0.551±0.02 0.723±0.02\n\n0.421±0.00 0.426±0.01 0.440±0.01 0.452±0.00\n\n0.426±0.01 0.436±0.01 0.493±0.03 0.606±0.01\n\n0.248±0.07 0.727±0.19 0.643±0.03 1.036±0.08\n\n0.211±0.01 0.288±0.03 0.459±0.02 0.593±0.07\n\n0.194±0.00 0.212±0.00 0.246±0.00 0.283±0.01\n\n0.595±0.00 0.629±0.00 0.692±0.01 0.803±0.01\n\n0.366±0.06 0.637±0.09 0.620±0.02 0.803±0.03\n\n0.278±0.01 0.336±0.02 0.445±0.02 0.528±0.04\n\n0.308±0.00 0.327±0.00 0.357±0.00 0.386±0.01\n\n0.362±0.00 0.381±0.00 0.410±0.01 0.461±0.00\n\nD JUSTIFICATION FOR THE ADAPTIVE LOSS\n\nMathematically, the justification for the adaptive loss is as follows. Considering the ξ term in equation 10, the function is asymptotically close (but not equivalent due to the denominator) to ξα. As a result, for outliers (for which ξ will be large), the loss term will function as a Lα penalty on ξ, which will penalize outliers more for large α. The converse of this is that we would expect a model trained with such a loss to learn lower values of α for settings with fewer outliers.\n\nE INTERPLAY BETWEEN ADAPTIVE LOSS AND MULTI-SCALE ARCHITECTURE\n\nThe main reason for combining the adaptive loss and multi-scale architecture under a unified framework is: they are synergetic. How do we explain this synergy? Compared to other transformer\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Comparison of the MSE and MAE results for our proposed multi-scale framework version of Informer and Autoformer (-MSA) with the iterative refinemenet baseline of keeping the original scale in each iteration (-IA)\n\n.\n\nDataset\n\nMetric\n\nExchange\n\nWeather\n\nElectricity\n\nTraffic\n\n96 192\n\n96 192\n\n96 192\n\n96 192\n\nAutoformer-MSA\n\nAutoformer-IA\n\nInformer-MSA\n\nInformer-IA\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\n0.126±0.01 0.253±0.03\n\n0.259±0.01 0.373±0.02\n\n0.410±0.05 0.809±0.14\n\n0.485±0.03 0.698±0.06\n\n0.168±0.05 0.427±0.12\n\n0.298±0.03 0.484±0.06\n\n0.649±0.08 0.938±0.03\n\n0.632±0.04 0.761±0.01\n\n0.163±0.01 0.221±0.01\n\n0.226±0.01 0.290±0.02\n\n0.233±0.01 0.401±0.02\n\n0.293±0.00 0.429±0.01\n\n0.210±0.02 0.289±0.01\n\n0.279±0.02 0.333±0.01\n\n0.222±0.01 0.357±0.02\n\n0.286±0.02 0.393±0.02\n\n0.188±0.00 0.197±0.00\n\n0.303±0.01 0.310±0.00\n\n0.248±0.01 0.265±0.01\n\n0.360±0.01 0.373±0.01\n\n0.203±0.01 0.219±0.00\n\n0.315±0.01 0.331±0.00\n\n0.237±0.00 0.271±0.01\n\n0.344±0.00 0.374±0.01\n\n0.567±0.00 0.589±0.01\n\n0.350±0.00 0.360±0.01\n\n0.681±0.03 0.699±0.02\n\n0.432±0.01 0.446±0.01\n\n0.597±0.01 0.655±0.01\n\n0.369±0.00 0.399±0.01\n\n0.641±0.01 0.695±0.01\n\n0.367±0.01 0.388±0.01\n\narchitectures (notably the baselines used), ScaleFormer has more iterative steps: the sequential multi-scale operations. Iterative computation tends to accumulate more errors, which will behave like outliers for the purpose of this loss. As a result, the process that leads to the need for the two components can be expressed as: (1) The multi-scale architecture is beneficial for performance as a useful structural prior for time series data. (2) The multi-scale architecture however relies on sequential computation that increases the likelihood of explosive error accumulation. (3) The adaptive loss serves to mitigate this issue, leading to more stable learning and better performance.\n\nF SINGLE-SCALE MODEL WITH MEAN-NORMALIZATION\n\nA single-scale model with mean normalization performs better than the multi-scale version without normalization. The reason for this is that normalization as our proposed scheme is targeting at all forms of internal distribution shift, not only those induced by the multi-scale architecture. In our submission, we make the case for the multi-scale prior as a natural prior to add to transformers. From empirical observations, we found that such a prior requires adapting normalization. When investigating means of normalizing, we observed additional benefits to non-multiscale architectures as well. This means that they also suffer from other forms of distribution shift: we attribute it in the paper to e.g. shifts between lookback and forecast distributions.\n\nG MORE ABLATION STUDIES AND PARAMETER ANALYSIS\n\nG.1\n\nITERATIVE REFINEMENT USING THE SAME SCALE\n\nTo further confirm the effect of the multi-scale framework, we also compare our proposed framework with another baseline by keeping the original scale in each iteration. As Table 6 shows, using multiscale framework outperforms keeping the original scale while having significantly lower memory and time complexity overhead. Following the main experiments, we use the scale factor of 2 which results S = {16, 8, 4, 2, 1} for Scaleformer (showed by -MSA) and a scale factor of 1 with similarly 5 iterations for iterative refinement (showed by -IA).\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nG.2 MORE RESULTS ON DIFFERENT COMPONENTS\n\nTable 7 extends the results of Figure 4 of the paper to all four datasets for both Autoformer and Informer, in the multivariate setting and for the two backbones, Autoformer and Informer.\n\nTable 7: Comparison of training with either only an Adaptive loss ”-A”, only the multi-scale framework with MSE loss ”-MS”, or the whole Multi-scale framework and Adaptive loss ”-MSA”. The results confirm that the combination of all our proposed contributions is essential, and results in significantly improved performance in both Informer and Autoformer. Experiments are done in the multi-variate setting.\n\nDataset\n\nMetric\n\ne g\nn a\nh c\nx E\n\n96 192 336 720\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\ny\n\nt i\nc i\nr t\nc e\nl\n\nE\n\n96 192 336 720\n\nfi\n\nc 96 192 336 720\n\nf a\nr\n\nT\n\nDataset\n\nMetric\n\ne g\nn a\nh c\nx E\n\n96 192 336 720\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\ny\n\nt i\nc i\nr t\nc e\nl\n\nE\n\n96 192 336 720\n\nfi\n\nc 96 192 336 720\n\nf a\nr\n\nT\n\nInformer\n\nInformer-A\n\nInformer-MS\n\nInformer-MSA\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\n0.966±0.10 1.088±0.04 1.598±0.08 2.679±0.22\n\n0.388±0.04 0.433±0.05 0.610±0.04 0.978±0.05\n\n0.344±0.00 0.344±0.01 0.358±0.01 0.386±0.00\n\n0.748±0.01 0.772±0.02 0.868±0.04 1.074±0.02\n\n0.792±0.04 0.842±0.01 1.016±0.02 1.340±0.06\n\n0.435±0.03 0.453±0.03 0.551±0.02 0.723±0.02\n\n0.421±0.00 0.426±0.01 0.440±0.01 0.452±0.00\n\n0.426±0.01 0.436±0.01 0.493±0.03 0.606±0.01\n\n0.962±0.09 1.106±0.03 1.602±0.07 2.719±0.19\n\n0.342±0.03 0.385±0.02 0.656±0.10 0.953±0.04\n\n0.321±0.00 0.341±0.01 0.344±0.00 0.363±0.00\n\n0.744±0.02 0.765±0.01 0.852±0.05 1.030±0.05\n\n0.796±0.04 0.853±0.01 1.019±0.01 1.351±0.06\n\n0.382±0.02 0.404±0.01 0.550±0.04 0.680±0.02\n\n0.402±0.00 0.422±0.01 0.422±0.00 0.432±0.00\n\n0.413±0.01 0.416±0.01 0.461±0.03 0.539±0.03\n\n0.201±0.05 0.446±0.14 0.520±0.05 0.997±0.08\n\n0.249±0.02 0.315±0.02 0.473±0.04 0.664±0.04\n\n0.211±0.00 0.233±0.01 0.279±0.01 0.315±0.01\n\n0.602±0.01 0.669±0.01 0.815±0.03 0.949±0.01\n\n0.325±0.03 0.502±0.07 0.540±0.02 0.783±0.02\n\n0.324±0.01 0.380±0.02 0.478±0.02 0.585±0.02\n\n0.326±0.00 0.348±0.00 0.388±0.01 0.411±0.00\n\n0.375±0.01 0.412±0.00 0.501±0.02 0.556±0.02\n\n0.168±0.05 0.427±0.12 0.500±0.05 1.017±0.05\n\n0.210±0.02 0.289±0.01 0.418±0.04 0.595±0.04\n\n0.203±0.01 0.219±0.00 0.253±0.01 0.293±0.01\n\n0.597±0.01 0.655±0.01 0.761±0.03 0.924±0.02\n\n0.298±0.03 0.484±0.06 0.535±0.02 0.790±0.02\n\n0.279±0.02 0.333±0.01 0.427±0.03 0.532±0.02\n\n0.315±0.01 0.331±0.00 0.360±0.01 0.390±0.01\n\n0.369±0.00 0.399±0.01 0.455±0.03 0.521±0.01\n\nAutoformer\n\nAutoformer-A\n\nAutoformer-MS\n\nAutoformer-MSA\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\n0.154±0.01 0.356±0.09 0.441±0.02 1.118±0.04\n\n0.267±0.03 0.323±0.01 0.364±0.02 0.425±0.01\n\n0.197±0.01 0.219±0.01 0.263±0.04 0.290±0.05\n\n0.628±0.02 0.634±0.01 0.619±0.01 0.656±0.01\n\n0.285±0.00 0.428±0.05 0.495±0.01 0.819±0.02\n\n0.334±0.02 0.376±0.00 0.397±0.01 0.434±0.01\n\n0.312±0.01 0.329±0.01 0.359±0.03 0.380±0.02\n\n0.393±0.02 0.401±0.01 0.385±0.01 0.403±0.01\n\n0.152±0.01 0.342±0.10 0.566±0.22 1.120±0.24\n\n0.229±0.01 0.293±0.02 0.357±0.01 0.419±0.01\n\n0.201±0.01 0.221±0.01 0.232±0.01 0.249±0.01\n\n0.610±0.02 0.633±0.02 0.618±0.00 0.654±0.02\n\n0.282±0.00 0.421±0.06 0.554±0.10 0.811±0.10\n\n0.288±0.01 0.340±0.02 0.387±0.01 0.422±0.01\n\n0.312±0.01 0.328±0.01 0.339±0.01 0.351±0.01\n\n0.381±0.01 0.396±0.02 0.381±0.00 0.397±0.01\n\n0.145±0.02 0.247±0.02 0.456±0.08 0.729±0.16\n\n0.174±0.01 0.250±0.02 0.314±0.02 0.414±0.03\n\n0.196±0.00 0.208±0.00 0.220±0.00 0.252±0.00\n\n0.580±0.01 0.601±0.00 0.639±0.02 0.680±0.02\n\n0.276±0.01 0.366±0.01 0.514±0.05 0.679±0.08\n\n0.254±0.01 0.333±0.02 0.380±0.02 0.457±0.02\n\n0.312±0.01 0.323±0.00 0.336±0.00 0.364±0.00\n\n0.358±0.00 0.369±0.00 0.397±0.01 0.427±0.01\n\n0.126±0.01 0.253±0.03 0.519±0.16 0.928±0.23\n\n0.163±0.01 0.221±0.01 0.282±0.02 0.369±0.04\n\n0.188±0.00 0.197±0.00 0.224±0.02 0.249±0.01\n\n0.567±0.00 0.589±0.01 0.619±0.01 0.642±0.01\n\n0.259±0.01 0.373±0.02 0.538±0.09 0.751±0.09\n\n0.226±0.01 0.290±0.02 0.340±0.03 0.396±0.03\n\n0.303±0.01 0.310±0.00 0.333±0.01 0.358±0.01\n\n0.350±0.00 0.360±0.01 0.383±0.01 0.397±0.01\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nG.3 RESULTS ON DIFFERENT SCALES\n\nTable 8 showcases the results of our model using different values of the scale parameter s. It shows that a scale of 2 results in better performance.\n\nTable 8: Comparison of the baselines with our method using different scales. Reducing the scale factor from s = 16 to s = 2 increases the number of steps but achieves lower error on average.\n\nDataset\n\nMetric\n\ne g\nn a\nh c\nx E\n\n96 192 336 720\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\ny\n\nt i\nc i\nr t\nc e\nl\n\nE\n\n96 192 336 720\n\nfi\n\nc 96 192 336 720\n\nf a\nr\n\nT\n\nDataset\n\nMetric\n\ne g\nn a\nh c\nx E\n\n96 192 336 720\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\ny\n\nt i\nc i\nr t\nc e\nl\n\nE\n\n96 192 336 720\n\nfi\n\nc 96 192 336 720\n\nf a\nr\n\nT\n\nInformer\n\nInformer-MSA(s=16)\n\nInformer-MSA(s=4)\n\nInformer-MSA(s=2)\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\n0.966±0.10 1.088±0.04 1.598±0.08 2.679±0.22\n\n0.388±0.04 0.433±0.05 0.610±0.04 0.978±0.05\n\n0.344±0.00 0.344±0.01 0.358±0.01 0.386±0.00\n\n0.748±0.01 0.772±0.02 0.868±0.04 1.074±0.02\n\n0.792±0.04 0.842±0.01 1.016±0.02 1.340±0.06\n\n0.435±0.03 0.453±0.03 0.551±0.02 0.723±0.02\n\n0.421±0.00 0.426±0.01 0.440±0.01 0.452±0.00\n\n0.426±0.01 0.436±0.01 0.493±0.03 0.606±0.01\n\n0.376±0.08 0.878±0.08 0.899±0.07 1.633±0.14\n\n0.208±0.02 0.302±0.02 0.470±0.06 0.639±0.07\n\n0.215±0.00 0.257±0.01 0.300±0.04 0.334±0.03\n\n0.648±0.02 0.679±0.01 0.811±0.02 1.020±0.07\n\n0.480±0.04 0.721±0.03 0.733±0.03 1.031±0.05\n\n0.275±0.02 0.354±0.02 0.456±0.04 0.544±0.04\n\n0.329±0.00 0.370±0.01 0.400±0.03 0.418±0.02\n\n0.386±0.01 0.391±0.01 0.455±0.01 0.542±0.03\n\n0.246±0.06 0.661±0.20 0.697±0.10 1.457±0.27\n\n0.189±0.01 0.287±0.02 0.420±0.03 0.627±0.07\n\n0.203±0.00 0.235±0.01 0.264±0.01 0.306±0.01\n\n0.616±0.00 0.686±0.01 0.782±0.03 0.965±0.03\n\n0.374±0.04 0.617±0.09 0.648±0.05 0.958±0.09\n\n0.259±0.01 0.333±0.01 0.433±0.02 0.543±0.04\n\n0.315±0.00 0.347±0.01 0.373±0.01 0.401±0.01\n\n0.374±0.01 0.404±0.01 0.451±0.02 0.521±0.01\n\n0.168±0.05 0.427±0.12 0.500±0.05 1.017±0.05\n\n0.210±0.02 0.289±0.01 0.418±0.04 0.595±0.04\n\n0.203±0.01 0.219±0.00 0.253±0.01 0.293±0.01\n\n0.597±0.01 0.655±0.01 0.761±0.03 0.924±0.02\n\n0.298±0.03 0.484±0.06 0.535±0.02 0.790±0.02\n\n0.279±0.02 0.333±0.01 0.427±0.03 0.532±0.02\n\n0.315±0.01 0.331±0.00 0.360±0.01 0.390±0.01\n\n0.369±0.00 0.399±0.01 0.455±0.03 0.521±0.01\n\nAutoformer\n\nAutoformer-MSA(16)\n\nAutoformer-MSA(4)\n\nAutoformer-MSA(2)\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\n0.154±0.01 0.356±0.09 0.441±0.02 1.118±0.04\n\n0.267±0.03 0.323±0.01 0.364±0.02 0.425±0.01\n\n0.197±0.01 0.219±0.01 0.263±0.04 0.290±0.05\n\n0.628±0.02 0.634±0.01 0.619±0.01 0.656±0.01\n\n0.285±0.00 0.428±0.05 0.495±0.01 0.819±0.02\n\n0.334±0.02 0.376±0.00 0.397±0.01 0.434±0.01\n\n0.312±0.01 0.329±0.01 0.359±0.03 0.380±0.02\n\n0.393±0.02 0.401±0.01 0.385±0.01 0.403±0.01\n\n0.182±0.03 0.514±0.18 0.527±0.07 1.019±0.18\n\n0.169±0.00 0.240±0.03 0.304±0.03 0.375±0.01\n\n0.190±0.00 0.206±0.01 0.236±0.02 0.260±0.01\n\n0.605±0.01 0.626±0.01 0.635±0.01 0.697±0.03\n\n0.316±0.03 0.537±0.10 0.570±0.04 0.819±0.08\n\n0.239±0.01 0.310±0.03 0.354±0.03 0.403±0.01\n\n0.306±0.00 0.320±0.01 0.344±0.01 0.368±0.01\n\n0.380±0.01 0.393±0.01 0.400±0.00 0.439±0.01\n\n0.170±0.03 0.359±0.14 0.606±0.18 0.973±0.22\n\n0.164±0.00 0.229±0.01 0.303±0.01 0.382±0.02\n\n0.188±0.00 0.207±0.00 0.237±0.03 0.261±0.01\n\n0.594±0.02 0.600±0.01 0.625±0.01 0.678±0.02\n\n0.304±0.02 0.443±0.08 0.585±0.10 0.809±0.09\n\n0.234±0.01 0.299±0.02 0.350±0.01 0.414±0.01\n\n0.303±0.00 0.320±0.00 0.344±0.02 0.369±0.01\n\n0.367±0.01 0.369±0.00 0.386±0.01 0.422±0.02\n\n0.126±0.01 0.253±0.03 0.519±0.16 0.928±0.23\n\n0.163±0.01 0.221±0.01 0.282±0.02 0.369±0.04\n\n0.188±0.00 0.197±0.00 0.224±0.02 0.249±0.01\n\n0.567±0.00 0.589±0.01 0.619±0.01 0.642±0.01\n\n0.259±0.01 0.373±0.02 0.538±0.09 0.751±0.09\n\n0.226±0.01 0.290±0.02 0.340±0.03 0.396±0.03\n\n0.303±0.01 0.310±0.00 0.333±0.01 0.358±0.01\n\n0.350±0.00 0.360±0.01 0.383±0.01 0.397±0.01\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nG.4 EFFECT OF α ON THE LOSS FUNCTION\n\nFigure 6 shows the impact of α on the shape of the loss function on the left, and example ground truth time-series corresponding to the horizon window on the right. As noted in the main paper, lower values of α tend to result in better robustness to outliers. This is indeed confirmed empirically in the case of the weather dataset, which corresponds to the lowest learnt α value, and has the outliers with the largest relative scales. For simplicity, we have excluded c on the analysis as it does not impact the robustness with regards to the outliers, as shown in Barron (2019).\n\nFigure 6: (a) The loss value based as a function of the ξ (absolute difference between prediction and target) and the learned α for each dataset. Different colors correspond to different datasets (each with specific value of α). (b) Samples taken from the ground-truth horizon window of each dataset. The Weather dataset sample has the outliers with the largest scales compared to the input series. As a consequence, the learnt value of α is the lowest. Different colors correspond to different variables in the multi-variate time-series we are considering.\n\nFigure 7: Qualitative results of each scale. The model can correct its previous mistakes on higher scales, showcasing increased robustness to forecasting artifacts that occur individually at each scale.\n\n18\n\nElectricity (α=1.047)Exchange rate (α=1.986)Traffic (α=1.896)Weather (α=0.367)Standard MSE (α=2.0)252015105002-4-24WeatherElectricityEchange rateTrafficξƒ(ξ, α)(a)(b)Autoformer-MSAElectricityInformer-MSAWeatherTimeTimeTimeTimeTimeFEDformer-MSAExchangeScale 16Scale 8Scale 4Scale 2Scale 1 (Original series)Published as a conference paper at ICLR 2023\n\nH DISCUSSION ON NORMALIZATION FOR AUTOFORMER\n\nThere is limited performance gain for Autoformer from the normalization alone. The reason is that Autoformer benefits from an inner series decomposition module which acts as the normalization by nature. Indeed one benefit of our proposed framework is a simple solution to bring the benefits of these specified designs to other baselines which significantly reduces the gap between for example Informer and Autoformer. AutoFormer already benefits from an internal component that reduces internal distribution shift: the series decomposition module.\n\nI ADDITIONAL QUALITATIVE RESULTS\n\nFigure 7, Figure 8, and Figure 9 provide additional qualitative results respectively showing the intermediate results of each scale and the comparison between the baselines and our approach with the horizon length of 720 and 192. As we can see, Scaleformer is able to better learn local and global time-series variations.\n\nFigure 8: Additional qualitative comparison of the baselines against our framework.\n\n19\n\nInformerInformer-MSAAutoformerAutoformer-MSAElectricityWeatherTrafficExchange-rateTimeTimeTimeValueTimeValueValueValuePublished as a conference paper at ICLR 2023\n\nFigure 9: Additional qualitative comparison of the baselines against our framework.\n\nJ PROBABILISTIC FORECASTING\n\nTable 9 shows the comparison of probabilistic methods for Informer by following the probabilistic output of DeepAR (Salinas et al., 2020), which is the most common probabilistic forecasting treatment. On implementation details, instead of point estimate, we have two prediction heads (one predict μ and one predict σ) with negative log likelihood loss. The other hyper parameters is the same as before. We use NLL loss and continuous ranked probability score (CRPS) which are commonly used in probabilistic forecasting as evaluation metrics.\n\nTable 9: Scaleformer improving probabilistic metrics in Probabilistic forecasting for Informer. By adapting Informer to make probabilistic predictions, we are able to show the Scaleformer prior again brings benefits. While such an analysis excludes dedicated probabilistic approaches for conciseness, it nevertheless shows the generality of our proposed approach.\n\nDataset\n\nMetric\n\n96\n\n192\n\n336\n\n720\n\nCRPS\n\nNLL\n\nCRPS\n\nNLL\n\nCRPS\n\nNLL\n\nCRPS\n\nNLL\n\nExchange\n\nInformer\n\n0.548±0.02 Informer-MSA 0.202±0.01\n\n2.360±0.20 0.452±0.09\n\n0.702±0.05 0.284±0.02\n\n4.350±1.45 0.818±0.11\n\n0.826±0.02 0.414±0.06\n\n4.302±0.49 1.724±0.43\n\n1.268±0.06 0.570±0.03\n\n13.140±1.84 2.862±0.21\n\nWeather\n\nInformer\n\n0.376±0.03 Informer-MSA 0.250±0.02\n\n1.180±0.21 0.392±0.14\n\n0.502±0.03 0.294±0.01\n\n1.752±0.23 0.610±0.04\n\n0.564±0.02 0.308±0.02\n\n1.928±0.27 0.728±0.10\n\n0.684±0.09 0.438±0.04\n\n2.210±0.46 1.270±0.14\n\nElectricity\n\nInformer\n\n0.330±0.01 Informer-MSA 0.238±0.01\n\n1.106±0.05 0.578±0.01\n\n0.338±0.05 0.290±0.00\n\n1.254±0.04 0.776±0.01\n\n0.348±0.01 0.324±0.03\n\n1.244±0.07 0.904±0.10\n\n0.528±0.00 0.358±0.01\n\n1.856±0.06 1.022±0.04\n\nTraffic\n\nInformer\n\n0.372±0.04 Informer-MSA 0.288±0.01\n\n1.376±0.05 1.094±0.03\n\n0.340±0.01 0.312±0.01\n\n1.404±0.04 1.102±0.04\n\n0.372±0.01 0.368±0.02\n\n1.516±0.06 1.194±0.05\n\n0.568±0.01 0.442±0.02\n\n1.658±0.01 1.378±0.06\n\n20\n\nInformerInformer-MSAElectricityWeatherExchange-rateTimeTimeValueValueValueAutoformerAutoformer-MSATimeTimeTrafficValuePublished as a conference paper at ICLR 2023\n\nK EXTENSION TO OTHER METHODS\n\nTable 10 shows the comparison results of NHiTs (Challu et al., 2022) and FiLM (Zhou et al., 2022a) as two baselines. For each method, we copy original model to have model for different scales and we concatenate the input with the output of previous scale for the new scale. The training hyperparameters such as optimizer and learning rate is the same as the previous baselines.\n\nTable 10: The effect of applying our proposed framework to NHits and FiLM as two non-transformer based models. Best results are shown in Bold.\n\nDataset\n\nMetric\n\ne g\nn a\nh c\nx E\n\n96 192 336 720\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\nNHiTS\n\nNHiTS-MSA\n\nFiLM\n\nFiLM-MSA\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\n0.091±0.00 0.200±0.02 0.347±0.03 0.761±0.20\n\n0.169±0.00 0.210±0.00 0.261±0.00 0.333±0.01\n\n0.218±0.01 0.332±0.01 0.442±0.02 0.662±0.08\n\n0.228±0.00 0.268±0.00 0.313±0.00 0.372±0.01\n\n0.087±0.00 0.186±0.01 0.381±0.01 1.124±0.07\n\n0.167±0.00 0.208±0.00 0.261±0.00 0.331±0.00\n\n0.206±0.00 0.306±0.00 0.445±0.01 0.808±0.03\n\n0.211±0.00 0.253±0.00 0.294±0.00 0.348±0.00\n\n0.083±0.00 0.179±0.00 0.341±0.00 0.896±0.01\n\n0.194±0.00 0.238±0.00 0.288±0.00 0.359±0.00\n\n0.201±0.00 0.301±0.00 0.421±0.00 0.714±0.00\n\n0.235±0.00 0.270±0.00 0.305±0.00 0.350±0.00\n\n0.081±0.00 0.156±0.00 0.253±0.01 0.728±0.01\n\n0.195±0.00 0.235±0.00 0.275±0.00 0.337±0.00\n\n0.197±0.00 0.284±0.00 0.378±0.01 0.659±0.00\n\n0.232±0.00 0.269±0.00 0.303±0.00 0.356±0.00\n\nL CROSS-SCALE NORMALIZATION\n\nThe choice of normalization approach is essential to our method, due to the aforementioned distribution shifts. In this section, we showcase multiple experiments that were done to study the impact of different ways of normalizing inputs. In Table 11, we compare using two forms of normalization with Informer-MSA as the baseline: mean-only versus normalization using both mean and standard deviation. We consider that adding normalization to our method is a trade-off between two issues faced during training. One one hand, (internal) distribution shifts hinder performance, as demonstrated in the experiments (See Table 2). On the other hand, normalizing the internal representations results in a form of information loss (during the processing of the input): we lose information about mean and standard deviation of the input data. We believe that the reason mean-normalization outperforms mean and standard deviation normalization is because it results in a better trade-off, losing less information while still managing to address the internal distribution shift.\n\nFig. 10 shows the different dataset distributions. As we can see, mean normalization works better for datasets such as electricity and traffic with almost unimodal distributions. For datasets such as exchange-rate and to some extent weather, which have multi-modal distributions, we find that mean and standandard deviation works better. Our hypothesis is that mean-normalization is too simple an approach for more complex input distributions. We note that, in all cases, either form of normalization results in strong improvements compared to the absence of normalization.\n\nFigure 10: Dataset distribution with histogram and kernel density estimation. Note that different datasets have either mostly uni-modal or multi-modal distributions.\n\nM SYNTHETIC DATASET\n\nTo further evaluate our proposed framework and adaptive loss function, we generate a new dataset based on the Mackey–Glass equations (Mackey & Glass, 1977). Concretely, we use the following\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nTable 11: Comparison of using standard normalization and zero-mean shifting in our cross scale normalization. Zero-mean shifting gets better results in almost all of the Traffic, Electricity, and Illness dataset. While standard normalization gets better results in Weather and Exchange datasets. We use zero-mean shifting in our experiments as it removes less information from the input time-series.\n\nPrediction Length\n\n96 (24)\n\n192 (32)\n\n336 (48)\n\n720 (64)\n\nMetric\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nElectricity\n\nMean Mean+STDEV\n\n0.199±0.00 0.248±0.01\n\n0.312±0.00 0.343±0.01\n\n0.215±0.00 0.271±0.02\n\n0.327±0.00 0.363±0.02\n\n0.250±0.01 0.265±0.01\n\n0.358±0.01 0.359±0.00\n\n0.297±0.01 0.365±0.01\n\n0.391±0.01 0.425±0.01\n\nExchange\n\nMean Mean+STDEV\n\n0.191±0.02 0.165±0.01\n\n0.326±0.02 0.300±0.01\n\n0.374±0.05 0.280±0.02\n\n0.453±0.02 0.387±0.02\n\n0.555±0.03 0.487±0.13\n\n0.567±0.01 0.518±0.06\n\n1.011±0.06 1.406±0.20\n\n0.782±0.02 0.931±0.07\n\nILI\n\nTraffic\n\nMean Mean+STDEV\n\n3.695±0.07 4.582±0.29\n\n1.242±0.01 1.435±0.07\n\n3.798±0.07 4.681±0.15\n\n1.266±0.01 1.470±0.03\n\n3.759±0.09 3.611±0.39\n\n1.238±0.01 1.243±0.07\n\n3.606±0.02 4.156±0.17\n\n1.222±0.00 1.376±0.02\n\nMean Mean+STDEV\n\n0.611±0.01 0.712±0.02\n\n0.377±0.01 0.421±0.01\n\n0.642±0.01 0.777±0.01\n\n0.389±0.01 0.474±0.01\n\n0.760±0.01 0.852±0.01\n\n0.453±0.01 0.501±0.01\n\n0.940±0.00 0.981±0.02\n\n0.514±0.00 0.530±0.01\n\nWeather\n\nMean Mean+STDEV\n\n0.208±0.01 0.197±0.01\n\n0.274±0.01 0.242±0.00\n\n0.303±0.02 0.224±0.01\n\n0.347±0.02 0.276±0.01\n\n0.443±0.03 0.301±0.01\n\n0.435±0.02 0.331±0.01\n\n0.625±0.04 0.392±0.02\n\n0.544±0.02 0.384±0.01\n\nequation:\n\ndx dt\n\n=\n\n0.2 × x(t − τ )\n\n1 + x(t − τ )10 − (0.1 × x(t)),\n\n(11)\n\nwhere we follow L ́opez-Caraballo et al. (2016); Farmer et al. (1983) and we also consider x(0) = 1.2. We create three series of length 10k. First we create a series by only using the above equation and considering τ = 18 where the series has a chaotic behaviour when τ ≥ 17. We also provide two additional series with τ = 12 and τ = 9, and also with added seasonal and trend components. Our synthetic dataset is a combination of these 3 series (See Figure 11 for an example). We compare our method using the new dataset on both Informer and Autoformer in Table 12, showing our proposed framework significantly improves the performance over the baselines.\n\nTable 12: Comparison of our proposed framework and the baselines using a synthetic data with 3 series generated based on Mackey Glass formulation by τ = {18, 12, 9}.\n\nDataset\n\nMetric\n\n96\n\n192\n\n336\n\n720\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nAutoformerbase\n\n0.435±0.05\n\n0.464±0.03\n\n0.399±0.06\n\n0.456±0.03\n\n0.361±0.03\n\n0.449±0.02\n\n0.361±0.02\n\n0.452±0.01\n\nAutoformerMS\n\n0.075±0.01\n\n0.192±0.02\n\n0.113±0.02\n\n0.237±0.02\n\n0.164±0.03\n\n0.298±0.03\n\n0.468±0.22\n\n0.501±0.14\n\nInformerbase\n\n0.265±0.06\n\n0.367±0.03\n\n0.287±0.01\n\n0.396±0.00\n\n0.243±0.02\n\n0.372±0.01\n\n0.246±0.01\n\n0.375±0.01\n\nInformerMS\n\n0.078±0.00\n\n0.192±0.00\n\n0.109±0.01\n\n0.233±0.02\n\n0.164±0.01\n\n0.309±0.01\n\n0.227±0.03\n\n0.356±0.02\n\nFigure 11: Left: an example input of the synthetic dataset. Right: The relative improvement of adaptive loss increases with increasing the percentage of outliers in the dataset.\n\n22\n\nTimeValueOutliers Percentage (%)Relative Improvement (%)00.10.512.55010203040506070InformerInformerMSPublished as a conference paper at ICLR 2023\n\nIn addition, to further analyze the effect of the adaptive loss on noisy datasets with outliers, we randomly replace a percentage of the training data with outliers by defining an outlier as a sample with a distance of larger than 50 times of standard deviation of the series from the median. Figure 11 shows the improvement of using adaptive loss on both Informer baseline and our multi-scale version of Informer. Adaptive loss increases the performance up to roughly 70% when there is a noisy input with extreme outliers (5% of data) while it gets comparable results for a clean dataset, demonstrating it as a strong candidate to replace MSE loss in the time series forecasting tasks.\n\nN STATISTICAL TESTS\n\nTo further validate the strengths of our empirical results, we have conducted two statistical tests. Each test is a Student’s t-test Kendall (1960) to determine whether the two sets of results can be distinguished. The first test, in Table 13, shows that for most settings, the adaptive loss provides gains that are statistically significant (p < 0.05). The second test, in Table 14 shows that for most settings, the multi-scale architectures provides gains that are statistically significant (p < 0.5).\n\nTable 13: Student’s t-test p-values for the test corresponding to whether the Adaptive loss provides an improvement. The base model is FedFormer. While we cannot conclude the adaptive loss provides an improvement for exchange rate and certain window sizes for traffic, for all other datasets the improvement is notable.\n\nWindow size\n\n96\n\n192\n\n336\n\n720\n\nMetric\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nExchange rate\n\n0.7157\n\n0.77286\n\n0.46741\n\n0.57392\n\n0.61317\n\n0.68405\n\n0.97102\n\n0.56755\n\nElectricity\n\n0.05844\n\n0.01267\n\n0.00124\n\n0.00187\n\n0.01294\n\n0.00576\n\n0.00374\n\n0.00597\n\nWeather\n\n0.25509\n\n0.11246\n\n0.00014\n\n8e-05\n\n0.0648\n\n0.00826\n\n0.05881\n\n0.02213\n\nTraffic\n\n0.21029\n\n0.02572\n\n0.78931\n\n0.19641\n\n0.40819\n\n0.248\n\n0.3491\n\n0.17579\n\nTable 14: Student’s t-test p-values for the test corresponding to whether the multi-scale prior provides an improvement. The base model is FedFormer. For certain window sizes of weather and exchange rate the improvement is not statistically provable. For most other settings it is almost always significant.\n\nWindow size\n\n96\n\n192\n\n336\n\n720\n\nMetric\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nMSE\n\nMAE\n\nExchange rate\n\n0.01109\n\n0.01657\n\n0.02263\n\n0.01674\n\n0.14988\n\n0.26543\n\n0.11071\n\n0.13141\n\nElectricity\n\n0.00029\n\n0.00088\n\n2e-05\n\n0.00017\n\n0.28221\n\n0.3316\n\n0.00593\n\n0.07219\n\nWeather\n\n0.02525\n\n0.01987\n\n0.2201\n\n0.05433\n\n0.14593\n\n0.12861\n\n0.00362\n\n0.02396\n\nTraffic\n\n0.01024\n\n0.00241\n\n0.01714\n\n0.00749\n\n0.00058\n\n0.00033\n\n0.00019\n\n0.00012\n\n23",
    "reference": "# Summary Of The Paper\n\nThis paper presents a Transformer-based framework that iteratively forecast time series at different scales with shared weights. In particular, it proposes normalizing the downsampled time series to avoid distribution shift and using adaptive loss to deal with outliers. Experiments show the multi-scale framework can significantly improve the performance of various transformer-based forecasting models.\n\n# Strength And Weaknesses\n\nStrengths:\n+ The paper is well-organized and the essential ideas are easy to follow.\n+ Extensive experiments are conducted with impressive improvement in main results.\n\nWeakness\n- The introduction of multi-scale structure is not well motivated. It would be nicer if a good motivating example of using multi-scale forecasting is provided other than the ablation study.\n- The reason of using average only in cross-scale normalization is not well justified. Why would it be able to address distribution shift without standardization as well?\n- The adaptive loss is introduced for outliers but there is no such demonstration. And the improvement of adaptive loss seems marginal in the ablation study (figure 4)\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe methodology is presented clearly and the empirical studies are thorough, but the motivation is not quite clear and the novelty is marginal. The work is expected to be easy to be reproduce as source code is provided.\n\n# Summary Of The Review\n\nOverall I believe this paper make incremental contribution to forecasting community with a multi-scale framework. The empirical results are inspiring, however, for future research.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nQ-PENSIEVE: BOOSTING SAMPLE EFFICIENCY OF MULTI-OBJECTIVE RL THROUGH MEMORY SHARING OF Q-SNAPSHOTS\n\nWei Hung1,2∗, Bo-Kai Huang1∗, Ping-Chun Hsieh1, Xi Liu3 1Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan 2Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan 3Applied Machine Learning, Meta AI, Menlo Park, CA, USA {hwei1048576.cs08,pinghsieh}@nycu.edu.tw, xiliu.tamu@gmail.com\n\nABSTRACT\n\nMany real-world continuous control problems are in the dilemma of weighing the pros and cons, multi-objective reinforcement learning (MORL) serves as a generic framework of learning control policies for different preferences over objectives. However, the existing MORL methods either rely on multiple passes of explicit search for finding the Pareto front and therefore are not sample-efficient, or utilizes a shared policy network for coarse knowledge sharing among policies. To boost the sample efficiency of MORL, we propose Q-Pensieve, a policy improvement scheme that stores a collection of Q-snapshots to jointly determine the policy update direction and thereby enables data sharing at the policy level. We show that Q-Pensieve can be naturally integrated with soft policy iteration with convergence guarantee. To substantiate this concept, we propose the technique of Q replay buffer, which stores the learned Q-networks from the past iterations, and arrive at a practical actor-critic implementation. Through extensive experiments and an ablation study, we demonstrate that with much fewer samples, the proposed algorithm can outperform the benchmark MORL methods on a variety of MORL benchmark tasks.\n\n1\n\nINTRODUCTION\n\nMany real-world sequential decision-making problems involve the joint optimization of multiple objectives, while some of them may be in conflict. For example, in robot control, it is expected that the robot can run fast while consuming as little energy as possible; nevertheless, we inevitably need to use more energy to make the robot run fast, regardless of how energy-efficient the robot motion is. Moreover, various other real-world continuous control problems are also multi-objective tasks by nature, such as congestion control in communication networks (Ma et al., 2022) and diversified portfolios (Abdolmaleki et al., 2020). Moreover, the relative importance of these objectives could vary over time (Roijers and Whiteson, 2017). For example, the preference over energy and speed in robot locomotion could change with the energy budget; network service providers need to continuously switch service among various networking applications (e.g., on-demand video streaming versus real-time conferencing), each of which could have preferences over latency and throughput.\n\nTo address the above practical challenges, multi-objective reinforcement learning (MORL) serves as one classic and popular formulation for learning optimal control strategies from vector-valued reward signal and achieve favorable trade-off among the objectives. In the MORL framework, the goal is to learn a collection of policies, under which the attained return vectors recover as much of the Pareto front as possible. One popular approach to addressing MORL is to explicitly search for the Pareto front with an aim to maximize the hypervolume associated with the reward vectors, such as evolutionary search (Xu et al., 2020) and search by first-order stationarity (Kyriakis et al., 2022). While being effective, explicit search algorithms are known to be rather sample-inefficient as the data sharing among different passes of explicit search is rather limited. As a result, it is typically\n\n∗Equal contribution.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\ndifficult to maintain a sufficiently diverse set of optimal policies for different preferences within a reasonable number of training samples. Another way to address MORL is to implicitly search for non-dominated policies through linear scalarization, i.e., convert the vector-valued reward signal to a single scalar with the help of a linear preference and thereafter apply a conventional single-objective RL algorithm for iteratively improving the policies (e.g., (Abels et al., 2019; Yang et al., 2019)). To enable implicit search for diverse preferences simultaneously, a single network is typically used to express a whole collection of policies. As a result, some level of data sharing among policies of different preferences is done implicitly through the shared network parameters. However, such sharing is clearly not guaranteed to achieve policy improvement for all preferences. Therefore, there remains one critical open research question to be answered: How to boost the sample efficiency of MORL through better policy-level knowledge sharing?\n\nTo answer this question, we revisit MORL from the perspective of memory sharing among the policies learned across different training iterations and propose Q-Pensieve, where a “Pensieve”, as illustrated in the novel Harry Potter, is a magical device used to store pieces of personal memories, which can later be shared with someone else. By drawing an analogy between the memory sharing among humans and the knowledge sharing among policies, we propose to construct a Q-Pensieve, which stores snapshots of the Q-functions of the policies learned in the past iterations. Upon improving the policy for a specific preference, we expect that these Q-snapshots could help jointly determine the policy update direction. In this way, we explicitly enforce knowledge sharing on the policy level and thereby enhance the sample use in learning optimal policies for various preferences. To substantiate this idea, we start by considering Q-Pensieve memory sharing in the tabular planning setting and integrate Q-Pensieve with the soft policy iteration for entropy-regularized MDPs. Inspired by (Yang et al., 2019), we leverage the envelope operation and propose the Q-Pensieve policy iteration for MORL, which we show would preserve the similar convergence guarantee as the standard singleobjective soft policy iteration. Based on this result, we propose a practical implementation that consists of two major components: (i) We introduce the technique of Q replay buffer. Similar to the standard replay buffer of state transitions, a Q replay buffer is meant to achieve sample reuse and improve sample efficiency, but notably at the policy level. Through the use of Q replay buffer, we can directly obtain a large collection of Q functions, each of which corresponds to a policy in a prior training iteration, without any additional efforts or computation in forming the Q-Pensieve. (ii) We convert the Q-Pensieve policy iteration into an actor-critic off-policy MORL algorithm by adapting the soft actor critic to the multi-objective setting and using it as the base of our implementation.\n\nThe main contributions of this paper can be summarized as:\n\n• We identify the critical sample inefficiency issue in MORL and address this issue by proposing Q-Pensieve, which is a policy improvement scheme for enhancing knowledge sharing on the policy level. We then present Q-Pensieve policy iteration and establish its convergence property.\n\n• We substantiate the concept of Q-Pensieve policy iteration by proposing the technique of Q replay\n\nbuffer and arrive at a practical actor-critic type practical implementation.\n\n• We evaluate the proposed algorithm in various benchmark MORL environments, including Deep Sea Treasure and MuJoCo. Through extensive experiments and an ablation study, we demonstrate the the proposed Q-Pensieve can indeed achieve significantly better empirical sample efficiency than the popular benchmark MORL algorithms, in terms of multiple common MORL performance metrics, including hypervolume and utility.\n\n2 PRELIMINARIES\n\nMulti-Objective Markov Decision Processes (MOMDPs). We consider the formulation of MOMDP defined by the tuple (S, A, P, r, γ, D, Sλ, Λ), where S denotes the state space, A is the action space, P : S × A × S → [0, 1] is the transition kernel of the environment, r : S × A → [−rmax, rmax]d is the vector-valued reward function with d as the number of objectives, γ ∈ (0, 1) is the discount factor, D is the initial state distribution, Sλ : Rd → R is the scalarization function (under some preference vector λ ∈ Rd), and Λ denotes the set of all preference vectors. In this paper, we focus on the linear reward scalarization setting, i.e., Sλ(r) = λ⊤r(s, a), as commonly adopted in the MORL literature (Abels et al., 2019; Yang et al., 2019; Kyriakis et al., 2022). Without loss of generality, we let Λ be the unit simplex. If d = 1, an MOMDP would degenerate to a standard MDP, and we simply use r(s, a) to denote the scalar reward. At each time step t ∈ N ∪{0}, the learner receives the\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nobservation st, takes an action at, and receives a reward vector rt. We use π : S → ∆(A) to denote a stationary randomized policy, where ∆(A) denotes the set of all probability distributions over the action space. Let Π be the set of all such policies.\n\nSingle-Objective Entropy-Regularized RL. In the standard framework of single-objective entropyregularized RL (Haarnoja et al., 2017; 2018; Geist et al., 2019), the goal is to learn an optimal policy for an entropy-regularized MDP, where an entropy regularization term is augmented to the original reward function. For a policy π ∈ Π, the regularized value functions V π : S → R and Qπ : S × A → R can be characterized through the regularized Bellman equations as\n\nQπ(s, a) = r(s, a) + γEs′∼P(·|s,a)[V π(s′)],\n\nV π(s) = Ea∼π(·|s)[Qπ(s, a) − α log π(a|s)],\n\n(1)\n\n(2)\n\nwhere α is a temperature parameter that specifies the relative importance of the entropy regularization term. In this setting, the goal is to learn an optimal policy π∗ such that Qπ∗ (s, a) ≥ Qπ(s, a), for all (s, a), for all π ∈ Π. An optimal policy can be obtained through soft policy iteration, which alternates between soft policy evaluation and soft policy improvement: (i) Soft policy evaluation: For a policy π, the soft Q-function of π can be obtained by iteratively applying the corresponding soft Bellman backup operator T π defined as\n\nT πQ (s, a) = r (s, a) + γEs′∼P(·|s,a) [V (s′)] ,\n\n(3)\n\nwhere V (s′) = Ea′∼π(·|s′) [Q (s′, a′) − α log (π (a′ | s′))]. (ii) Soft policy improvement: In each iteration k, the policy is updated towards an energy-based policy induced by the soft Q-function, i.e.,\n\nπk+1 = arg min π′∈ ̃Π\n\nDKL\n\n(cid:32)\n\nπ′ (· | s)\n\n(cid:13) (cid:13) (cid:13)\n\nexp (cid:0) 1\n\nα Qπk (s, ·)(cid:1) Z πk (s)\n\n(cid:33)\n\n,\n\n(4)\n\nwhere ̃Π is the set of parameterized policies of interest and Z πk is the normalization term.\n\nMulti-Objective Entropy-Regularized RL. We extend the standard single-objective RL with entropy regularization to the multi-objective setting. For each policy π ∈ Π, we define the multi-objective regularized value functions via the following multi-objective version of entropy-regularized Bellman equations as follows:\n\nQπ(s, a) = r(s, a) + γEs′∼P(·|s,a)[V π(s′)],\n\nV π(s) = Ea∼π(·|s)[Qπ(s, a) − α log π(a|s)1d],\n\n(5)\n\n(6)\n\nwhere 1d denotes a d-dimensional vector of all ones.\n\nIn this paper, our goal is to learn a preference-dependent policy π(·|·; λ) such that for any preference λ ∈ Λ, λ⊤Qπ(·|·;λ)(s, a; λ) ≥ λ⊤Qπ′ (s, a), for all (s, a), for all π′ ∈ Π. For ease of notation, we let V π(·|·;λ)(s; λ) ≡ V π(s; λ) and Qπ(·|·;λ)(s, a; λ) ≡ Qπ(s, a; λ) in the sequel.\n\n3 ALGORITHMS\n\nIn this section, we propose our Q-Pensieve learning algorithm for boosting the sample efficiency of multi-objective RL. We first describe the idea of Q-Pensieve in the tabular planning setting by introducing Q-Pensieve soft policy iteration. We then extend the idea to develop a practical deep reinforcement learning algorithm.\n\n3.1 NAIVE MULTI-OBJECTIVE SOFT POLICY ITERATION\n\nTo solve MORL in the entropy-regularized setting, one straightforward approach is to leverage the single-objective soft policy improvement with the help of linear scalarization. That is, in each iteration k, the policy can be updated by\n\nπk+1(·, ·; λ) = arg min π′∈ ̃Π\n\nDKL\n\n(cid:18)\n\nπ′ (· | s)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n3\n\nexp (cid:0) 1\n\nα λ⊤Qπk (s, ·; λ)(cid:1)\n\nZ πk\n\nλ (s)\n\n(cid:19) .\n\n(7)\n\nPublished as a conference paper at ICLR 2023\n\nWhile (7) serves as a reasonable approach, designing a learning algorithm based on the update scheme in (7) could suffer from sample inefficiency due to the lack of policy-level knowledge sharing: In (7), the policy for each preference λ is updated completely separately based solely on the Q-function under λ. Moreover, as the update (7) relies on an accurate estimate of the Q-function, the critic learning for the policy of each individual preference would typically require at least a moderate number of samples. These issues could be particularly critical for a large preference set in practice. While the use of a conditioned policy network (e.g., (Abels et al., 2019)), a commonly-used network architecture in the MORL literature, could somewhat mitigate this issue, it remains unclear whether the knowledge sharing induced by the conditioned network can indeed achieve policy improvement across various preferences. As a result, a systematic approach is needed for boosting the sample efficiency in MORL.\n\n3.2 Q-PENSIEVE SOFT POLICY ITERATION\n\nTo boost the sample efficiency of MORL, we propose to enhance the policy-level knowledge sharing by constructing a Q-Pensieve for memory sharing across iterations. Specifically, a Q-Pensieve is a collection of Q-snapshots obtained from the past iterations, and it is formed to boost the policy improvement update with respect to the Q-function in the current iteration as these Q-snapshots could offer potentially better policy improvement directions under linear scalarization. Moreover, one major computational benefit of Q-Pensieve is that these Q-snapshots are obtained without the need for any updates or additional samples from the environment (and hence are for free) as they already exist during training. We substantiate this idea by first introducing the Q-Pensieve soft policy iteration in the tabular setting (i.e., |S| and |A| are finite) as follows:\n\nQ-Pensieve Policy Improvement. In the policy improvement step of the k-th iteration, for each specific λ, we update the policy as\n\nπk+1(·|·; λ) = arg min π′∈ ̃Π\n\nDKL\n\n(cid:18)\n\nπ′ (· | s; λ)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nexp (cid:0) supλ′∈Wk(λ),Q′∈Qk ZQk (s)\n\n1\n\nα λ⊤Q′(s, ·; λ′)(cid:1)\n\n(cid:19)\n\n,\n\n(8)\n\nwhere ZQk is again the normalization term, Wk(λ) ⊂ Λ is a set of preference vectors, and Qk is a set of Q-snapshots. The two sets Wk(λ) and Qk are to be selected as follows:\n\n• For Wk(λ), the only requirement is that λ ∈ Wk(λ), for all k. The preference sets can be different\n\nin different iterations.\n\n• Similarly, for Qk, the only requirement is that Qπk ∈ Qk, for all k. The set of Q-snapshots can\n\nalso be different in different iterations. Hence, the choice of Qk is rather flexible.\n\nWhen choosing Wk(λ) = {λ} and Qk = {Qπk }, one would recover the update in (7).\n\nPolicy Evaluation. In the policy evaluation step, we evaluate the policy that corresponds to each preference λ by iteratively applying the multi-objective softmax Bellman backup operator T π MO as\n\n(T π\n\nMO Q)(s, a; λ) = r(s, a) + γEs′∼P(·|s,a),a′∼π(·|s′;λ)[Q(s′, a′; λ) − α log π(a′|s′; λ)1d].\n\n(9)\n\nRemark 1 The Q-Pensieve update in (8) is inspired by the envelope Q-learning (EQL) technique (Yang et al., 2019), where in each iteration k, the Q-learning update takes into account the envelope formed by the Q-functions of the current policy πk for different preferences. The fundamental difference between Q-Pensieve and EQL is that Q-Pensieve further achieves memory sharing across training iterations through the use of Q-snapshots from the past iterations, and EQL focuses mainly on the use of the Q-function of the current iteration.\n\nConvergence of Q-Pensieve Soft Policy Iteration. Another nice feature of the Q-Pensieve policy improvement step is that it preserves the similar convergence result as the standard single-objective soft policy iteration, as stated below. The proof of Theorem 3.1 is provided in Appendix A.\n\nTheorem 3.1 Under the Q-Pensieve soft policy iteration given by (8) and (9), the sequence of preference-dependent policies {πk} converges to a policy π∗ such that λ⊤Qπ∗ (s, a; λ) ≥ λ⊤Qπ(s, a) for all π ∈ Π, for all (s, a) ∈ S × A and for all λ ∈ Λ.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n3.3 PRACTICAL IMPLEMENTATION OF Q-PENSIEVE\n\nIn this section, we present the implementation of proposed Q-Pensieve algorithm for learning policies with function approximation for the general state and action spaces.\n\nQ Replay Buffer. Based on (8), we know that the policy update of Q-Pensieve would involve both the current Q-function and the Q-snapshots from the past iterations. To implement this, we introduce Q replay buffer, which could store multiple Q-networks in a predetermined manner (e.g., first-in first-out). Notably, unlike the conventional experience replay buffer (Mnih et al., 2013) of state transitions, Q replay buffer stores the learned Q-networks in past iterations as candidates for forming the Q-Pensieve. On the other hand, while each Q-network would require a moderate amount of memory usage, we found that in practice a rather small Q replay buffer is already effective enough for boosting the sample efficiency. We further illustrate this observation through the experimental results in Section 4.\n\nNext, we convert the Q-Pensieve soft policy iteration into an actor-critic off-policy MORL algorithm. Specifically, we adapt the idea of soft actor critic to Q-Pensieve by minimizing the residual of the multi-objective soft Q-function: Let θ and φ be the parameters of the policy network and the critic network, respectively. Then, the critic network is updated by minimizing the following loss\n\nLQ(φ; λ) = E(s,a)∼μ\n\n(cid:104)\n\nλ⊤ (cid:0)Qφ (s, a; λ) − (cid:0)r (s, a) + γEs′∼P(·|s,a)\n\n(cid:2)V ̄φ (s′)(cid:3)(cid:1)(cid:1)2(cid:105)\n\n,\n\n(10)\n\nwhere ̄φ is the parameter of the target network and μ is the sampling distribution of the state-action pairs (e.g., a distribution induced by a replay buffer of state transitions). On the other hand, based on (8), the policy network is updated by minimizing the following objective\n\nLπ(θ; λ) = Es∼μ\n\n(cid:20) Ea∼πθ\n\n(cid:104)\n\nsup λ′∈W (λ),Q′∈Q\n\n(cid:8)α log (πθ (a | s; λ)) − λ⊺Q′ (s, a; λ′) (cid:9)(cid:105)(cid:21)\n\n.\n\n(11)\n\nThe overall architecture of Q-Pensieve is provided in Figure 1. The pseudo code of the Q-Pensieve algorithm is described in Algorithm 1 in Appendix. The code of our experiments is available 1. Notably, in Section 4 we show that empirically a relatively small Q buffer size (e.g., 4 in our experiments) can already offer a significant performance improvement.\n\nFigure 1: The architecture of Q-Pensieve.\n\n4 EXPERIMENTS\n\nIn this section, we demonstrate the effectiveness of Q-Pensieve on various benchmark RL tasks and discuss how Q-Pensieve boosts the sample efficiency through an extensive ablation study.\n\n1https://github.com/NYCU-RL-Bandits-Lab/Q-Pensieve\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n4.1 EXPERIMENTAL CONFIGURATION\n\nPopular Benchmark Methods. We compare the proposed algorithm against various popular benchmark methods, including the Conditioned Network with Diverse Experience Replay (CN-DER) in (Abels et al., 2019), the Prediction-Guided Multi-Objective RL (PGMORL) in (Xu et al., 2020), the Pareto Following Algorithm (PFA) in (Parisi et al., 2014), and SAC (Haarnoja et al., 2018). For CN-DER, as the original CN-DER is built on deep Q-networks (DQN) for discrete actions, we modify the source code of Abels et al. (2019) for continuous control by implementing CN-DER on top of DDPG. Moreover, we follow the same DER technique, which uses a diverse replay buffer and gives priority according to how much the samples increase the overall diversity of the buffer. For PGMORL and PFA, we use the open-source implementation of (Xu et al., 2020) for the experiments. As these explicit search methods typically require more samples before reaching a comparable performance level, we evaluate the performance PGMORL and PFA under both 1 times and β times (β > 1) of the number of samples used by Q-Pensieve to demonstrate the sample efficiency of Q-Pensieve. For SAC, as the MORL problem reduces a single-objective one under a fixed preference, we train multiple models using single-objective SAC (one model for each fixed preference) as a performance reference for other MORL methods.\n\nPerformance Metrics. In the evaluation, we consider the following three commonly-used performance metrics for MORL:\n\n• HyperVolume (HV): Let R be a set of return vectors attained and r0 ∈ Rd be a reference I{z ∈ H(R)}dz, where H(R) :=\n\npoint. Then, we define the HyperVolume as HV := (cid:82) (cid:8)z ∈ Rd : ∃r ∈ R, r0 ≺ z ≺ r(cid:9)and I is the indicator function.\n\nH(R)\n\n• Utility (UT): To further evaluate the performance under linear scalarization, we define the utility\n\nmetric as UT := Eλ\n\n(cid:104)(cid:80)T\n\nt=0 λ⊤rt\n\n(cid:105)\n\n, where the preference λ is sampled uniformly from Λ.\n\n• Episodic Dominance (ED): To compare the performance of a pair of algorithms, we define Episodic Dominance as ED1,2 := Eλ[I{(cid:80)T1 t are the return vectors, and T1,T2 are the episode lengths of algorithm 1 and 2, respectively. ED serves as a useful metric for pairwise comparison in those problems where the return vectors under different preferences can differ by a lot in magnitude (in this case, HV and UT could be dominated by the return vectors of a few preferences).\n\nt }], where r1\n\nt > (cid:80)T2\n\nt=0 λ⊤r1\n\nt=0 λ⊤r2\n\nt , r2\n\nEvaluation Domains. We evaluate the algorithms in the following domains: (i) Continuous Deep Sea Treasure (DST): a two-objective continuous control task modified from the original DST environment. (ii) Multi-Objective Continuous LunarLander: a four-objective task modified from the classic control task in the OpenAI gym. (iii) Multi-Objective MuJoCo: modified benchmark locomotion tasks with either two or three objectives.\n\nConfiguration of Q-Pensieve. For Q-Pensieve, at each policy update, we set the size of the preference set Wk(λ) to be 5 (including λ and another four preferences drawn randomly) and set the size of the Q replay buffer to be 4, unless stated otherwise.\n\n4.2 EXPERIMENTAL RESULTS\n\nDoes Q-Pensieve achieve better sample efficiency than the MORL benchmark methods? Table 1 shows the performance of Q-Pensieve and the benchmark methods in terms of the three metrics. For each algorithm, we report the mean and the standard deviation over five random seeds. We can observe that Q-Pensieve consistently enjoys higher HV, UT, and ED in almost all the domains. More importantly, Q-Pensieve indeed exhibits superior sample efficiency as it still outperforms the explicit search methods (i.e., PFA and PGMORL) even if these methods are given 10 times of the number of samples used by Q-Pensieve. Moreover, we can observe that the explicit search methods (i.e., PFA and PGMORL) often have larger HV than the implicit search method (such as CN-DER), while implicit search methods tend to have larger UT. This manifests the design principles and the characteristics of the two families of approaches, where explicit search is designed mainly for achieving large HV and implicit search typically aims for larger scalarized return.\n\nHow much improvement in sample efficiency can Q-Pensieve achieve compared to training multiple single-objective SAC models separately? To answer this question, we conduct experiments on 2-objective MuJoCo tasks and consider a whole range of 19 preference vectors\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison of Q-Pensieve and other benchmark algorithms in terms of the three metrics across ten domains. We report the mean and standard deviation over five random seeds. The ED is calculated through comparing each algorithm to a multi-objective version of SAC (equivalent to Q-Pensieve with the size of the preference set equal to 1 and without Q replay buffer). We set β = 10 for HalfCheetah2d, Ant2d, Ant3d, and Hopper3d, set β = 5 for LunarLander4d, LunarLander5d, and Hopper5d, and set β = 3 for DST2d, Hopper2d, and Walker2d.\n\nEnvironments\n\nMetrics\n\nDST2d\n\nLunarLander4d\n\nLunarLander5d\n\nHalfCheetah2d\n\nHopper2d\n\nHopper3d\n\nHopper5d\n\nAnt2d\n\nAnt3d\n\nWalker2d\n\nHV(×102) UT(×100) ED HV(×108) UT(×101) ED HV(×1011) UT(×101) ED HV(×107) UT(×103) ED HV(×106) UT(×102) ED HV(×109) UT(×103) ED HV(×1013) UT(×102) ED HV(×106) UT(×102) ED HV(×108) UT(×103) ED HV(×106) UT(×102) ED\n\nPFA (1.5M steps)\n\n7.43±3.68 -9.27±6.03 0.13±0.11 -\n- -\n- -\n- 0.73±0.19 0.31±0.20 0.08±0.10 0.49±0.46 2.89±1.93 0.31±0.17 -\n- -\n- -\n- 0.17±0.05 -0.06±0.01 0.22±0.03 -\n- -\n0.52 ±0.20 0.23±0.13 0.32±0.06\n\nPFA (1.5×βM steps)\n\n8.67±1.49 -6.86±6.06 0.10±0.08 -\n- -\n- -\n- 1.31±0.26 1.02±0.40 0.10±0.06 1.01±0.62 3.50±1.85 0.41±0.10 -\n- -\n- -\n- 0.77±0.53 0.14±0.14 0.22±0.02 -\n- -\n1.05±0.44 0.95±0.55 0.37±0.09\n\nPGMORL (1.5M steps)\n\n8.10±1.57 4.90±0.44 0.25±0.18 0.32 ±0.11 -0.26 ±0.27 0.02 ±0.01 1.81±0.20 -2.77±0.68 0.05±0.02 0.53±0.17 -0.28±0.94 0.01±0.00 0.63±0.48 1.94±2.46 0.31±0.25 0.29±0.37 0.19±0.16 0.02±0.03 0.63±0.11 1.48±0.28 0.18±0.07 0.14±0.03 -0.21±0.15 0.21±0.02 0.41±0.48 0.18±0.05 0.02±0.02 0.83±0.42 0.38±0.24 0.30±0.10\n\nPGMORL (1.5×βM steps)\n\n8.13±1.61 5.02±0.35 0.28±0.18 0.38±0.11 1.10±0.50 0.04 ±0.04 1.87±0.42 -4.38±1.02 0.05±0.02 0.28±0.29 0.09±0.17 0.11±0.05 1.31±0.48 3.70±1.78 0.31±0.11 0.91±1.39 0.31±0.26 0.03±0.03 0.43±0.09 1.63±0.21 0.14±0.05 0.13±0.04 -0.18±0.38 0.21±0.03 0.68±0.62 0.25±0.05 0.03±0.03 1.28±0.66 1.20±0.67 0.34±0.12\n\nCN-DER (1.5M steps)\n\n5.36±4.71 -5.10±15.73 0.21 ±0.17 1.50±0.60 3.60±2.90 0.21 ±0.12 8.64±0.15 0.56±0.42 0.49 ±0.01 2.08±0.54 5.09±3.57 0.02 ±0.01 0.56±0.16 1.42±1.00 0.04 ±0.03 3.70±0.81 0.72±0.16 0.07±0.03 3.42±0.93 1.76±0.43 0.21 ±0.05 5.03±3.60 3.68±2.34 0.21±0.08 13.00±4.11 0.49±0.23 0.28±0.14 0.42±0.09 3.17±0.53 0.21 ±0.11\n\nQ-Pensieve (1.5M steps)\n\n10.21±1.40 7.31±0.91 0.54±0.11 2.10±0.10 5.10±0.30 0.49 ±0.05 9.48±1.84 1.07±0.24 0.52±0.02 3.82±0.27 5.61±0.31 0.54±0.08 1.33±0.20 4.08±1.10 0.43±0.09 9.56±0.60 1.39±0.15 0.55±0.08 7.24±0.31 3.37±0.65 0.52±0.05 10.01±1.86 14.04±3.03 0.60±0.07 21.87±1.07 1.14±0.22 0.56±0.07 1.12±0.36 6.37±1.42 0.48±0.10\n\n([0.05, 0.95], [0.1, 0.9], [0.15, 0.85], · · · , [0.95, 0.05]). We train 19 models by using single-objective SAC, one model for each individual preference. Each model is trained for 1.5M steps (and hence the total number of steps under SAC is 28.5M steps). By contrast, Q-Pensieve only uses 1.5M steps in total in learning policies for all the preferences. Figure 2 shows the return vectors attained by Q-Pensieve and the collection of 19 SAC models. Q-Pensieve can achieve comparable or better returns than the collection of SAC models with only 1/19 of the samples. This further demonstrate the sample efficiency of Q-Pensieve.\n\n(a) Hopper2d\n\n(b) HalfCheetah2d\n\n(c) DST2d\n\nFigure 2: Return vectors attained by Q-Pensieve and the collection of single-objective SAC models under 19 preferences.\n\n7\n\n-1000-700-400-100200500Control cost030060090012001500Forward rewardQ-PensieveSAC-10000-7000-4000-100020005000Control cost03000600090001200015000Forward rewardQ-PensieveSAC-20-16-12-8-4048Time penalty0481216202428Treasure rewardQ-PensieveSACPublished as a conference paper at ICLR 2023\n\nWhy can Q-Pensieve outperform single-objective SAC in some cases? From Figures 2(a) and (c), we see that Q-Pensieve can attain some return vectors that are strictly better than those of the single-objective SAC models. The reasons behind this phenomenon are minaly two-fold: (i) Under single-objective SAC, despite that we train one model for each individual preference, it could still occur that single-objective SAC gets stuck at a sub-optimal policy under some preferences. (ii) By contrast, Q-Pensieve has a better chance of escaping from these sub-optimal policies with the help of the Q-snapshots in the Q replay buffer.\n\nTo verify the above argument, we design a hybrid SAC algorithm as follows: (a) For the first 105 time steps, this algorithm simply follows the single-objective SAC. (b) At time step 105, it switches to the update rule of Q-Pensieve based on the Q-snapshots stored in the Q replay buffer of another model trained under Q-Pensieve algorithm in parallel. Figure 3 shows the performance of this hybrid algorithm in DST and HalfCheetah. Clearly, the Q-Pensieve update could help the SAC model escape from the sub-optimal policies, under various preferences.\n\n(a) DST2d λ = [0.9, 0.1]\n\n(b) DST2d λ = [0.8, 0.2]\n\n(c) HalfCheetah2d λ = [0.5, 0.5]\n\n(d) HalfCheetah2d λ = [0.6, 0.4]\n\nFigure 3: Comparison of standard single-objective SAC and the hybrid SAC assisted by another Q-Pensieve model trained in parallel.\n\nAn ablation study on Q replay buffer. To verify the effectiveness of the technique of Q replay buffer, we compare the performance of Q-Pensieve with buffer size equal to 4 and that without using Q replay buffer (termed “Vanilla” in Figures 4 and 5). Figure 4 and 5 show the attained return vectors and HV of both methods. We can see that Q replay buffer indeed leads to a better policy improvement behavior, in terms of both HV and the scalarized returns. However, these figures may sometimes oscillate a lot in the end period. It is because our algorithm finds solutions from another Q-vector, and their inner product of Q and preference may be quite close. We can check the points are in the same contour.\n\n(a) Hopper2d\n\n(b) Ant2d\n\n(c) Walker2d\n\nFigure 4: Return vectors attained under preference λ = [0.5, 0.5] at different training stages (We also plot return vectors under others preference in Figure 7 and Figure 8 in Appendix). A number x on the red or blue marker indicates that the model is obtained at 100 · x thousand steps.\n\n8\n\n0123Time Steps105-10-50510152025Total returnSACHybrid SAC0123Time Steps105-1001020Total returnSACHybrid SAC0510Time Steps105-20000200040006000Total returnSACHybrid SAC0510Time Steps105-20000200040006000Total returnSACHybrid SAC-250-200-150-100-500Control cost-500050010001500Forward reward123456789101112131415123456789101112131415VanillaQ-Pensieve-1200-1000-800-600-400-200Control cost01000200030004000Forward reward123456789101112131415123456789101112131415VanillaQ-Pensieve-800-600-400-2000Control cost-50005001000150020002500Forward reward123456789101112131415123456789101112131415VanillaQ-PensievePublished as a conference paper at ICLR 2023\n\n(a) Ant2d\n\n(b) Walker2d\n\nFigure 5: A comparison in HV between Q-Pensieve with buffer size equal to 4 and that without using Q replay buffer at different training stages.\n\n5 RELATED WORK\n\nThe multi-objective RL problems have been extensively studied from two major perspectives:\n\nExplicit Search. A plethora of prior works on MORL updates a policy or a set of policies by explicitly searching for the Pareto front of the reward space. To learn policies under time-varying preferences, (Natarajan and Tadepalli, 2005) presented to store a set of policies, which are to be used in searching for a proper policy for a new preference without learning from scratch. (Lizotte et al., 2012) leveraged linear value function approximation to search for optimal policies. (Van Moffaert and Now ́e, 2014) proposed Pareto Q-learning, which stores the immediate rewards and the non-dominated future return vectors separately and leverage the Pareto dominance for selecting the actions in Q-learning. (Parisi et al., 2014) presented a policy gradient approach to search for non-dominated policies. (Mossalam et al., 2016) solves MORL via scalarized Q-learning along with the concept of prioritizing the corner weights for selecting the preference of the scalarized problem. (Xu et al., 2020) proposed an evolutionary approach to search for the Pareto set of policies, with the help of a prediction model for determining the search direction. (Kyriakis et al., 2022) presented a policy gradient method by approximating the Pareto front via a first-order necessary condition. However, the above explicit search algorithms are known to be rather sample-inefficient as the knowledge sharing among different passes of search is limited.\n\nImplicit Search. Another class of algorithms are designed to improve policies for multiple preferences through implicit search. For example, (Abels et al., 2019) presents Conditioned Network, which extends the standard single-objective DQN to learning preference-dependent multi-objective Q-functions. To achieve scale-invariant MORL, (Abdolmaleki et al., 2020) proposed to first learn the Q-functions for different objectives and encode the preference through constraints. Recently, (Yang et al., 2019) proposes envelope Q-functions to encourage knowledge sharing among the Q functions of different the current multi-objective Q-values that any policy can benefit from other preferences’ experiences, that make training more efficiently, and (Zhou et al., 2020) proposed model-based envelope value iteration base on envelope Q-functions, it provides an efficient way to get optimal multi-objective Q-functions. Despite that our method is inspired by (Yang et al., 2019), the main difference between our work and theirs is that we boost the sample efficiency of MORL via explicit memory sharing among policies learned during training.\n\n6 CONCLUSION\n\nThis paper proposes Q-Pensieve, which significantly enhances the policy-level data sharing through in order to boost the sample efficiency of MORL problems. We substantiate the idea by presenting QPensieve soft policy iteration in the tabular setting and show that it preserves the global convergence property. Then, to implement the Q-Pensieve policy improvement step, we introduce the Q replay buffer technique, which offers a simple yet effective way to maintain Q-snapshot. Our experiments demonstrate that Q-Pensieve is a promising approach in that it can outperform the state-of-the-art MORL methods with much fewer samples in a variety of MORL benchmark tasks.\n\n9\n\n12345Time Steps1050510HyperVolume106VanillaQ-Pensieve12345Time Steps1050510HyperVolume105VanillaQ-PensievePublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis material is based upon work partially supported by the National Science and Technology Council (NSTC), Taiwan under Contract No. 110-2628-E-A49-014 and Contract No. 111-2628-E-A49-019, and based upon work partially supported by the Higher Education Sprout Project of the National Yang Ming Chiao Tung University and Ministry of Education (MOE), Taiwan.\n\nREFERENCES\n\nYiqing Ma, Han Tian, Xudong Liao, Junxue Zhang, Weiyan Wang, Kai Chen, and Xin Jin. Multi-objective congestion control. In Proceedings of the Seventeenth European Conference on Computer Systems, pages 218–235, 2022.\n\nAbbas Abdolmaleki, Sandy Huang, Leonard Hasenclever, Michael Neunert, Francis Song, Martina Zambelli, Murilo Martins, Nicolas Heess, Raia Hadsell, and Martin Riedmiller. A distributional view on multi-objective In Hal Daum ́e III and Aarti Singh, editors, Proceedings of the 37th International policy optimization. Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 11– 22. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/abdolmaleki20a. html.\n\nDiederik M Roijers and Shimon Whiteson. Multi-objective decision making. Synthesis Lectures on Artificial\n\nIntelligence and Machine Learning, 11(1):1–129, 2017.\n\nJie Xu, Yunsheng Tian, Pingchuan Ma, Daniela Rus, Shinjiro Sueda, and Wojciech Matusik. Predictionguided multi-objective reinforcement learning for continuous robot control. In Hal Daum ́e III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 10607–10616. PMLR, 13–18 Jul 2020. URL https: //proceedings.mlr.press/v119/xu20h.html.\n\nPanagiotis Kyriakis, Jyotirmoy Deshmukh, and Paul Bogdan. Pareto policy adaptation.\n\nIn International\n\nConference on Learning Representations, 2022.\n\nAxel Abels, Diederik Roijers, Tom Lenaerts, Ann Now ́e, and Denis Steckelmacher. Dynamic weights in multi-objective deep reinforcement learning. In International Conference on Machine Learning, pages 11–20. PMLR, 2019.\n\nRunzhe Yang, Xingyuan Sun, and Karthik Narasimhan. A generalized algorithm for multi-objective reinforce-\n\nment learning and policy adaptation. Advances in Neural Information Processing Systems, 32, 2019.\n\nTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-\n\nbased policies. In International Conference on Machine Learning, pages 1352–1361, 2017.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861–1870. PMLR, 2018.\n\nMatthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. In\n\nInternational Conference on Machine Learning, pages 2160–2169, 2019.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and\n\nMartin Riedmiller. Playing atari with deep reinforcement learning. arXiv:1312.5602, 2013.\n\nSimone Parisi, Matteo Pirotta, Nicola Smacchia, Luca Bascetta, and Marcello Restelli. Policy gradient approaches In International Joint Conference on Neural Networks\n\nfor multi-objective sequential decision making. (IJCNN), pages 2323–2330, 2014.\n\nSriraam Natarajan and Prasad Tadepalli. Dynamic preferences in multi-criteria reinforcement learning. In\n\nInternational Conference on Machine Learning, pages 601–608, 2005.\n\nDaniel J Lizotte, Michael Bowling, and Susan A Murphy. Linear fitted-q iteration with multiple reward functions.\n\nJournal of Machine Learning Research, 13(1):3253–3295, 2012.\n\nKristof Van Moffaert and Ann Now ́e. Multi-objective reinforcement learning using sets of pareto dominating\n\npolicies. Journal of Machine Learning Research, 15(1):3483–3512, 2014.\n\nHossam Mossalam, Yannis M. Assael, Diederik M. Roijers, and Shimon Whiteson. Multi-objective deep\n\nreinforcement learning, 2016. URL https://arxiv.org/abs/1610.02707.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nDongruo Zhou, Jiahao Chen, and Quanquan Gu. Provable multi-objective reinforcement learning with generative\n\nmodels. arXiv preprint arXiv:2011.10134, 2020.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nArash Tavakoli, Fabio Pardo, and Petar Kormushev. Action branching architectures for deep reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), Apr. 2018. doi: 10.1609/aaai. v32i1.11798. URL https://ojs.aaai.org/index.php/AAAI/article/view/11798.\n\nYunhao Tang and Shipra Agrawal. Discretizing continuous action space for on-policy optimization. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):5981–5988, Apr. 2020. doi: 10.1609/aaai.v34i04. 6059. URL https://ojs.aaai.org/index.php/AAAI/article/view/6059.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA PROOF OF THEOREM 3.1\n\nBefore proving Theorem 3.1, we first present two supporting lemmas as follows. To begin with, we establish the policy improvement property of the Q-Pensieve update. Recall that the Q-Pensieve policy update is that for each preference λ ∈ Λ,\n\nπk+1(·|s; λ) = arg min π′∈Π\n\nDKL\n\n(cid:124)\n\n(cid:18)\n\nπ′ (· | s; λ)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nexp (cid:0) supλ′∈Wk(λ),Q′∈Qk ZQ(s)\n\n(cid:123)(cid:122) =:L(π′;λ)\n\n1\n\nα λ⊤Q′(s, ·; λ′)(cid:1)\n\n(cid:19)\n\n.\n\n(cid:125)\n\n(12)\n\nLemma 1 (Q-Pensieve Policy Improvement) Under the Q-Pensieve policy improvement update, we have λ⊤Qπk (s, a; λ) ≤ λ⊤Qπk+1 (s, a; λ), for all state-action pairs (s, a) ∈ S × A, for all preference vectors λ ∈ Λ, and for all iteration k ∈ N ∪ {0}. Proof (Lemma 1) By the update rule in (12), we know that πk+1 is a minimizer of L(π′; λ) and hence L(πk+1; λ) ≤ L(πk; λ). This implies that for each state s ∈ S, we have\n\nEa∼πk+1(·|s)\n\n(cid:104) λ⊤1d · log πk+1(a|s; λ) −\n\n1 α\n\nsup λ′∈Wk(λ),Q′∈Qk\n\nλ⊤Q′(s, a; λ′) + log ZQk (s)\n\n(cid:105)\n\n≤Ea∼πk(·|s)\n\n(cid:104) λ⊤1d · log πk(a|s; λ) −\n\n1 α\n\nsup λ′∈Wk(λ),Q′∈Qk\n\nλ⊤Q′(s, a; λ′) + log ZQk (s)\n\n(cid:105) .\n\n(13)\n\nSince ZQk only depends on the state, the inequality (13) reduces to\n\nEa∼πk+1(·|s)\n\n(cid:104)\n\nλ⊤1d · log πk+1(a|s; λ) −\n\n1 α\n\nsup λ′∈Wk(λ),Q′∈Qk\n\nλ⊤Q′(s, a; λ′)\n\n(cid:105)\n\n≤Ea∼πk(·|s)\n\n(cid:104)\n\nλ⊤1d · log πk(a|s; λ) −\n\n1 α\n\nsup λ′∈Wk(λ),Q′∈Qk\n\nλ⊤Q′(s, a; λ′)\n\n(cid:105) .\n\nNext, we proceed to consider the multi-objective soft Bellman equation as follows:\n\n(14)\n\n(15)\n\n(16)\n\nλ⊤Qπk (s0, a0; λ) − λ⊤r(s0, a0) = γλ⊤Es1∼P (·|s0,a0),a1∼πk(·|s1;λ) ≤ γEs1∼P (·|s0,a0),a1∼πk(·|s1;λ)\n\n(cid:104)\n\n(cid:2)Qπk (s1, a1; λ) − α · λ⊤1d log(πk(a1|s1; λ))(cid:3)\n\nsup λ′∈Wk(λ),Q′∈Qk\n\nλ⊤Q′(s1, a1; λ′) − α · λ⊤1d log(πk(a1|s1; λ))\n\n(cid:105)\n\n(17)\n\n≤ γEs1∼P (·|s0,a0),a1∼πk+1(·|s1;λ)\n\n(cid:104)\n\nsup λ′∈Wk(λ),Q′∈Qk\n\nλ⊤Q′(s1, a1; λ′) − α · λ⊤1d log(πk+1(a1|s1; λ))\n\n(cid:105)\n\n≤ −γEs1∼P (·|s0,a0),a1∼πk+1(·|s1;λ)\n\nα · λ⊤1d log(πk+1(a1|s1; λ)) (cid:104)\n\n+ γEs1∼P (·|s0,a0),a1∼πk+1(·|s1;λ)\n\nλ⊤r(s1, a1) − γEs2∼P (·|s1,a1),a2∼πk(·|s2)[αλ⊤1d log(πk(a2|s2; λ))]\n\n(cid:105)\n\n(18)\n\n+ γ\n\nsup λ′∈Wk(λ),Q′∈Qk\n\nEs2∼P (·|s1,a1),a2∼πk(·|s2)\n\n(cid:2)λ⊤Q′(s2, a2; λ)(cid:3)(cid:105)\n\n(19)\n\n≤ −γEs1∼P (·|s0,a0),a1∼πk+1(·|s1;λ)\n\nα · λ⊤1d log(πk+1(a1|s1; λ)) (cid:104)\n\n(cid:105)\n\n+ γEs1∼P (·|s0,a0),a1∼πk+1(·|s1;λ)\n\nλ⊤r(s1, a1) − γEs2∼P (·|s1,a1),a2∼πk+1(·|s2)[αλ⊤1d log(πk+1(a2|s2; λ))]\n\n(cid:104)\n\n(cid:104)\n\n+ γ\n\nsup λ′∈Wk(λ),Q′∈Qk\n\nEs2∼P (·|s1,a1),a2∼πk+1(·|s2)\n\n(cid:2)λ⊤Q′(s2, a2; λ)(cid:3)(cid:105)\n\n(20)\n\n12\n\nPublished as a conference paper at ICLR 2023\n\n≤ −γEs1∼P (·|s0,a0),a1∼πk+1(·|s1;λ)\n\n(cid:104)\n\nα · λ⊤1d log(πk+1(a1|s1; λ))\n\n(cid:105)\n\n+ EP,πk+1\n\n(cid:20) (cid:88)\n\n(cid:104) γtE\n\nt≥1\n\nλ⊤r(st, at)\n\n− γEst+1∼P (·|st,at),at+1∼πk+1(·|st+1)\n\n(cid:2)αλ⊤1d log(πk+1(at+1|st+1; λ))|st, at\n\n(cid:3)(cid:105)(cid:21)\n\n= λ⊤Qπk+1 (s0, a0; λ) − λ⊤r(s0, a0),\n\n(21)\n\n(22)\n\n(23)\n\n(24)\n\nwhere (16) follows from the multi-objective soft Bellman equation, (17) holds by the sup operation and the fact that Qπk ∈ Qk, (18) follows from (14), (19) holds by applying the multi-objective soft Bellman equation to Q′(s1, a1; λ), (20) again follows from the inequality in (14), (23) is obtained by □\nunrolling the whole trajectory, and (24) holds by the definition of Qπ.\n\nLemma 2 (Multi-Objective Soft Policy Evaluation) Under the multi-objective soft Bellman MO with respect to a policy π and some Q(0) : S × A → Rd, the sequence backup operator T π of intermediate Q-functions {Q(i)} during policy evaluation is given by Q(i+1) = T π MO Q(i), for all i ∈ N ∪ {0}. Then, Q(i) converges to the soft Q-function of π, as i → ∞.\n\nProof (Lemma 2) This can be directly obtained from the standard convergence property of iterative policy evaluation (Sutton and Barto, 2018) in two steps: (i) Define the entropy-augmented reward as r(s, a; π) := r(s, a) + γEs′∼P(·|s,a),a′∼π(·|s)[α log π(a|s)1d], which is a bounded function. (ii) Then, rewrite the policy evaluation update as\n\nQπ(s, a) ← r(s, a; π) + γEs′∼P(·|s,a),a′∼π(·|s)[Qπ(s′, a′; λ)].\n\nThis completes the proof.\n\n(25)\n\n□\n\nNow we are ready to prove Theorem 3.1. Proof (Theorem 3.1) Note that by Lemma 1, the sequence λ⊤Qπk is monotonically increasing. As each element in Qπ is bounded above for all π ∈ Π given the boundedness of both the reward and the entropy term, the sequence of policies shall converge to some policy π∗. The remaining thing (cid:19)\n\n(cid:18)\n\n(cid:16)\n\n(cid:17)\n\nexp\n\nsupλ∈W (λ),Q′∈Q Q\n\n′ π(s,·;λ)\n\nis to show that π∗ is optimal: (i) Define Lπ′(π) := DKL\n\nπ(· | s)\n\n.\n\nZQ\n\n(cid:13) (cid:13) (cid:13)\n\n(ii) Upon convergence, we shall have Lπ∗ (π∗(·|s)) ≤ Lπ∗ (π(·|s)) for all π ∈ Π. Using the same iterative argument as in the proof of Lemma 1, we get λ⊤ Qπ∗ (s, a; λ) ≥ λ⊤Qπ(s, a; λ) for all □\n(s, a) ∈ S × A and all λ ∈ Λ.\n\nB DETAILED CONFIGURATION OF EXPERIMENTS\n\nB.1 DETAILS ON THE EVALUATION DOMAINS\n\n• Continuous Deep Sea Treasure (DST): DST is a classical multi-objective reinforcement learning environment. We control the agent to find the treasure, while the further the treasure is, the higher its value. In other words, the agent needs to spend more resources (-1 penalty for each action) to get the more precious treasure. To extend DST to continuous space, we modify the simple four direction movement to the movement in a circle, we set the β of DST to 3.\n\n• Multi-Objective Continuous LunarLander: We modify LunarLander to the multi-objective version by dismantling the reward to main engine cost, side engine cost, shaping reward, and result reward. Since the past MORL methods were conducted in environments with 2 or 3 objectives, we created an environment with 4 and 5 objectives to show our method can be used in high dimension objectives environments, we set the β of LunarLander to 5.\n\n• MuJoCo: We divide the scalar reward in MuJoCo environments into vector rewards. What’s more, we amplify the weight of the control cost to make the magnitude of each reward element similar. – HalfCheetah2d: 2 objectives as forward speed, control cost (S ⊆ R17, A ⊆ R6), 1000 times\n\nfor control cost, and β = 10.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n– Hopper2d: 2 objectives: forward speed, control cost (S ⊆ R11, A ⊆ R3), 1500 times for\n\ncontrol cost, and β = 3.\n\n– Hopper3d: 3 objectives: forward speed, jump reward, control cost (S ⊆ R11, A ⊆ R3), 1500 times for control cost. The jump reward is 15 times of the difference between current height and initial height, and β = 10.\n\n– Hopper5d: 5 objectives: forward speed, control cost of each of the 3 joints, and healthy\n\nreward (S ⊆ R11, A ⊆ R3), 1500 times for control cost, and β = 5\n\n– Ant2d: 2 objectives: forward speed, control cost (S ⊆ R111, A ⊆ R8), 1 times for control\n\ncost, and β = 10.\n\n– Ant3d: 3 objectives: forward speed, control cost, healthy reward (S ⊆ R111, A ⊆ R8), 1\n\ntimes for control cost, 1 times for healthy reward, and β = 10.\n\n– Walker2d: 2 objectives: forward speed, control cost (S ⊆ R17, A ⊆ R6), 1000 times for\n\ncontrol cost, and β = 3.\n\nB.2 HYPERPARAMETERS\n\nB.2.1 HYPERPARAMETERS OF Q-PENSIEVE\n\nWe conduct all experiment on baselines with following hyperparameters.\n\nTable 2: Hyperparameters of Q-Pensieve.\n\nParameter Optimizer Learning Rate Discount Factor Replay Buffer Size Depth of Hidden Layers Number of Hidden Units per Layer Number of Samples per Minibatch Nonlinearity Target Smoothing Coefficient Target Update Interval Gradient Steps\n\nValue Adam 0.0003 0.99 1000000 2\n256 256 ReLU 0.005 1\n1\n\nB.2.2 HYPERPARAMETERS OF PGMORL AND PFA\n\nFor PGMORL and PFA, we use the hyperparameters as provided in Table 3:\n\n• n: the number of reinforcement learning tasks.\n\n• total steps: the total number of environment training steps.\n\n• mw: the number of iterations in warm-up stages. • mt: the number of iterations in evolutionary stages. • Pnum: the number of performance buffers. • Psize: the size of each performance buffer. • nweight: the number of sampled weights for each policy. • sparsity: the weight of sparsity metric.\n\nC PSEUDO CODE OF Q-PENSIEVE\n\nWe provide the pseudo code in Algorithm 1 as follows.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Hyperparameters of PGMORL and PFA.\n\nEnvironments DST2d LunarLander4d LunarLander5d HalfCheetah2d Hopper2d Hopper3d Hopper5d Ant2d Ant3d Walker2d\n\nn\n\n5 35 35 5\n5 15 35 5\n15 5\n\ntotal steps mw mt Pnum Psize 1.5 ×106 7.5 ×106 7.5 ×106 1.5 ×107 4.5 ×106 1.5 ×107 7.5 ×106 1.5 ×107 1.5 ×107 4.5 ×106\n\n100 400 400 100 100 200 400 100 200 100\n\n80 40 40 80 200 200 200 200 200 80\n\n20 10 10 20 40 40 40 40 40 20\n\n2 2\n2 2\n2 2\n2 2\n2 2\n\nnweight 7\n7 7\n7 7\n7 7\n7 7\n7\n\nsparsity −1 −106 −106 −1 −1 −106 −106 −1 −106 −1\n\nAlgorithm 1: Q-Pensieve Input\n\n:φ1, φ2, θ, preference sampling distribution Pλ, number of preference vectors Nλ, the soft update coefficient τ , actor learning rates ηπ, critic learning rates ηQ\n\nOutput :φ1, φ2, θ 1 ̄φ1 ← φ1, ̄φ2 ← φ2; 2 M ← ∅ 3 B ← ∅; 4 for each iteration do\n\n▷ Initialize replay buffer;\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\nsample weight λ from Λ according Pλ ; for each environment step do\n\nat∼ πθ(·|st; λ); st+1∼ P (·|st, at); M ← M (cid:83) {(st, at, r(st, at), st+1)};\n\nfor each gradient step do\n\nsample Nλ − 1 preferences and add them to set W; W ← W (cid:83){λ}; φi ← φi − ηQ ˆ∇φi LQ (φi; λ) , B ← B (cid:83){φi; λ} for i ∈ {1, 2}; compute ˆ∇θ with eq. 11 with W; θ ← θ − ηπ ˆ∇θ Lπ(θ; λ); ̄φi ← τ φi + (1 − τ ) ̄φi for i ∈ {1, 2};\n\n(a) Ant2d\n\n(b) Hopper2d\n\nFigure 6: Return vectors attained under different Q replay buffer sizes of Q-Pensieve\n\n15\n\n-3000-5002000Control cost010002000300040005000Forward rewardWithout Q-PensieveBuffer size = 2Buffer size = 4-1000-5000500Control cost050010001500Forward rewardWithout Q-PensieveBuffer size = 2Buffer size = 4Published as a conference paper at ICLR 2023\n\n(a) Hopper2d\n\n(b) Ant2d\n\n(c) Walker2d\n\nFigure 7: Return vectors attained under preference λ = [0.9, 0.1] at different training stages. A number x on the red or blue marker indicates that the model is obtained at 10 · x million steps.\n\n(a) Hopper2d\n\n(b) Ant2d\n\n(c) Walker2d\n\nFigure 8: Return vectors attained under preference λ = [0.1, 0.9] at different training stages. A number x on the red or blue marker indicates that the model is obtained at 10 · x million steps.\n\nD COMPARISON OF LEARNING CURVES\n\nWe demonstrate the learning curves of Q-Pensieve and the benchmark methods. In Figures 9-15, we can find that Q-Pensieve enjoys the fastest learning progress in almost all tasks and preferences. Notably, as PGMORL is an evolutionary method and does explicit search for policies for only a small set of preferences vectors in each generation, the typical learning curve (in terms of expected total reward) under a given preference is not very informative about the overall learning progress. Therefore, regarding the learning curves, we compare Q-Pensieve with CN-DER and PFA. Moreover, as PFA cannot handle tasks with more than two objectives (this fact has also been mentioned in (Xu et al., 2020)), PFA is evaluated only in tasks with two objectives.\n\n(a) λ = [0.1, 0.9]\n\n(b) λ = [0.5, 0.5]\n\n(c) λ = [0.9, 0.1]\n\nFigure 9: Average return in Hopper2d over 5 random seeds (average return is the inner product of the reward vectors and the corresponding preference).\n\n16\n\n-800-600-400-2000Control cost0500100015002000Forward reward123456789101112131415123456789101112131415VanillaQ-Pensieve-2500-2000-1500-1000-500Control cost01000200030004000Forward reward123456789101112131415123456789101112131415VanillaQ-Pensieve-2000-1500-1000-5000Control cost-500050010001500200025003000Forward reward123456789101112131415123456789101112131415VanillaQ-Pensieve-10-8-6-4-20Control cost20406080100120Forward reward12345678910111213141234567891011121314VanillaQ-Pensieve-400-300-200-1000Control cost-1000100200300400Forward reward123456789101112131415123456789101112131415VanillaQ-Pensieve-100-80-60-40-200Control cost-100-50050100150200Forward reward123456789101112131415123456789101112131415VanillaQ-PensievePublished as a conference paper at ICLR 2023\n\n(a) λ = [0.1, 0.9]\n\n(b) λ = [0.5, 0.5]\n\n(c) λ = [0.9, 0.1]\n\nFigure 10: Average return in Ant2d over 5 random seeds (average return is the inner product of the reward vectors and the corresponding preference).\n\n(a) λ = [0.1, 0.9]\n\n(b) λ = [0.5, 0.5]\n\n(c) λ = [0.9, 0.1]\n\nFigure 11: Average return in HalfCheetah2d over 5 random seeds (average return is the inner product of the reward vectors and the corresponding preference).\n\n(a) λ = [0.1, 0.9]\n\n(b) λ = [0.5, 0.5]\n\n(c) λ = [0.9, 0.1]\n\nFigure 12: Average return in Walker2d over 5 random seed (average return is the inner product of the reward vectors and the corresponding preference).\n\nE ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we compare Q-Pensieve with the baseline methods, discuss how the performance of Q-Pensieve can be further improved through hyperparameter tuning, and then demonstrate the model generalization of Q-Pensieve.\n\nE.1 COMPARISON WITH THE ENVELOPE Q-LEARNING ALGORITHM\n\nThe Envelope Q-Learning algorithm and its neural version Envelope DQN (Yang et al., 2019) presume that the action space is discrete. To adapt Envelope DQN to the continuous control tasks\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n(a) λ = [0.33, 0.33, 0.33]\n\n(b) λ = [0.1, 0.1, 0.8]\n\n(c) λ = [0.1, 0.8, 0.1]\n\n(d) λ = [0.1, 0.1, 0.8]\n\nFigure 13: Average return in Ant3d over 5 random seeds (average return is the inner product of the reward vectors and the corresponding preference).\n\n(a) λ = [0.33, 0.33, 0.33]\n\n(b) λ = [0.1, 0.1, 0.8]\n\n(c) λ = [0.1, 0.8, 0.1]\n\n(d) λ = [0.8, 0.1, 0.1]\n\nFigure 14: Average return in Hopper3d over 5 random seeds (average return is the inner product of the reward vectors and the corresponding preference).\n\n(a) λ = [0.25, 0.25, 0.25, 0.25]\n\n(b) λ = [0.05, 0.05, 0.05, 0.85]\n\n(c) λ = [0.05, 0.05, 0.85, 0.05]\n\n(d) λ = [0.05, 0.85, 0.05, 0.05]\n\n(e) λ = [0.85, 0.05, 0.05, 0.05]\n\nFigure 15: Average return in LunarLander4d over 5 random seeds (average return is the inner product of the reward vectors and the corresponding preference).\n\nconsidered in our paper (including MuJoCo and continuous Deep Sea Treasure), we take the opensource implementation in (Yang et al., 2019) and apply action discretization, which has been shown to be also quite effective in MuJoCo control tasks (Tavakoli et al., 2018; Tang and Agrawal, 2020). We compare Envelope DQN with Q-Pensieve in both Hopper3d and DST2d environments. For Envelope DQN, we set the number of bins for each action dimension to be 11 and 5 for DST2d (actions are 2-dimensional) and Hopper3d (actions are 3-dimensional) respectively, based on the suggestions\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nprovided by (Tavakoli et al., 2018). Table 4 shows the performance of Q-Pensieve and Envelope DQN in terms of the two metrics. We can observe that Q-Pensieve outperforms Envelope DQN by a large margin in the above two popular multi-objective tasks.\n\nTable 4: Comparison of Q-Pensieve and Envelope DQN in terms of the two metrics across two domains. We report the mean and standard deviation over five random seeds.\n\nEnvironments\n\nDST2d\n\nHopper3d\n\nMetrics HV(×102) UT(×100) HV(×105) UT(×102)\n\nEnvelope DQN Q-Pensieve\n\n7.02±0.24 4.61±0.08 0.43±0.21 -0.39±0.26\n\n10.21±1.40 7.31±0.91 13.31±2.03 4.08±1.10\n\nE.2 EMPIRICAL STUDY OF Q-PENSIEVE\n\nQ Replay Buffer Size: One could expect that a larger Q buffer size could help provide a more diverse collection of Q snapshots and thereby better boost the policy improvement update in each iteration. On the other hand, in practice, the required memory usage also scales with the Q buffer size. We evaluate Q-Pensieve under buffer sizes = 2, 4, 6 and compare it to that without using a Q replay buffer in Ant2d. Table 5 show that empirically a relatively small Q buffer size already offers a significant performance improvement.\n\nQ Replay Buffer Update Interval: To ensure that the Q snapshots in the Q replay buffer are rather diverse, we would suggest that the update interval shall not be too small (otherwise the Q snapshots in the buffer would be fairly similar). Moreover, as in general this update interval can be viewed as a hyperparameter to be tuned (similar to the update interval of the target networks in many RL algorithms). We further do an empirical study on the performance of Q-Pensieve under different update intervals. We run Q-Pensieve with different update intervals in Ant2d for 1500k steps. Table 6 show that the hypervolume is not sensitive to the update interval, and the performance in UT can potentially be further improved through hyperparameter tuning.\n\nE.3 MODEL GENERALIZATION OF Q-PENSIEVE\n\nTo demonstrate that the critic model with the preference vector can generalize well, we define a metric for the critic as\n\nLcritic = Es∼S,a∼A||Q (s, a; λ) − Qtrue (s, a; λ) ||2,\n\n(26)\n\nwhere Q is the action-value function learned by our critic network and Qtrue is the true Q function calculated by Monte-Carlo method. Table 7 and Table 8 show the Lcritic under various preferences λ at different training stages in Hopper2d and HalfCheetah2d respectively. Note that the true Q values are typically in the range of 1000 to a few thousands. Therefore, we can see that Lcritic under various preferences is indeed pretty low, which indicates that the critic model can generalize well across preferences.\n\nTable 5: Comparison of Q-Pensieve with different Q replay buffer size in terms of the two metrics in Ant2d over five random seeds.\n\nMetrics HV(×107) UT(×102)\n\nWithout Q Buffer 0.93±0.25 8.17±4.83\n\nSize = 2 0.99±0.23 12.45± 2.75\n\nSize = 4 1.00±0.18 14.04±3.03\n\nSize = 6 1.27±0.14 15.31±3.47\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Comparison of Q-Pensieve with different replay buffer update interval in terms of the two metrics in Ant2d over five random seeds.\n\nMetrics HV(×106) UT(×102)\n\nInterval = 500 8.30±0.60 8.94±2.01\n\nInterval = 1000 9.90±2.31 12.45±2.75\n\nInterval = 1500 8.63±1.31 11.83±1.42\n\nInterval = 2000 8.53±1.63 13.33±2.97\n\nTable 7: Lcritic in Hopper2d over five random seeds.\n\nPreferences λ=[0.1,0.9] λ=[0.3,0.7] λ=[0.5,0.5] λ=[0.7,0.3] λ=[0.9,0.1]\n\n100K steps 23.74 22.21 27.59 47.54 87.01\n\n500K steps 43.59 14.82 14.45 10.25 46.95\n\n1000K steps 38.96 15.10 24.91 15.17 84.76\n\n1500K steps 41.47 14.65 11.59 11.52 31.96\n\nTable 8: Lcritic in HalfCheetah2d over five random seeds.\n\nPreferences λ=[0.1,0.9] λ=[0.3,0.7] λ=[0.5,0.5] λ=[0.7,0.3] λ=[0.9,0.1]\n\n100K steps 62.92 134.1 192.47 169.70 146.71\n\n500K steps 139.39 174.81 113.20 87.87 66.92\n\n1000K steps 118.135 132.305 98.18 83.21 65.09\n\n1500K steps 92.35 131.71 90.07 74.69 63.85\n\n20",
    "reference": "# Summary Of The Paper\n\nThe paper presents Q-Pensieve, a method using versions of past Q functions to update a new Q function. This method enables information sharing across policies. The authors instantiate the idea in a soft actor-critic algorithm. Experiment results on DST, LunarLander, and several MuJoCo environments are provided where Q-Pensieve outperforms other baseline methods.\n\n# Strength And Weaknesses\n\nStrength:\nThe Q-Pensieve idea is well-motivated as one can interpret this as an extension of the envelope Q-learning algorithm by Yang et. al. It is good that the Q-Pensieve retains the same convergence policy iteration results. \n\nThe empirical results are good as well, with Q-Pensieve outperforming other baselines in three different performance metrics. My understanding is that this is a fairly straightforward method to plug in so I appreciate its effectiveness.\n\nWeakness:\nThe major weakness is the absence of the envelope Q-learning algorithm by Yang et. al. As the Q-Pensieve algorithm is closely related, the authors should provide a comparison.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is overall clear. I have some clarification questions:\n1. Do the actor and the critic network depend on the preference vector? If so, how is the preference vector handled as an input feature?\n\n2. Can the authors provide more details on maintaining the Q replay buffer? How often did you push a new Q function into the buffer? \n\n3. The practical implementation uses a relatively small Q replay buffer of size 4. Is the algorithm sensitive to this parameter?\n\n# Summary Of The Review\n\nThe proposed Q-Pensieve algorithm appears to be simple and effective. However, an important baseline is missing.\n\n==========================\nI want to thank the authors for writing a detailed response. My main concern, the comparison with envelope Q-learning, is adequately addressed. I would encourage the authors to expand the comparisons to include all the environments in a later revision.\n\nOn the Q replay buffer size study, it seems the performance is monotonically increasing with the buffer size. It would be good to add a discussion on the trade-off of the performance with memory overhead.\n\nOverall, I decided to raise my score from 5 to 6.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nDECOMPOSING TEXTURE AND SEMANTIC FOR OUTOF-DISTRIBUTION DETECTION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nOut-of-distribution (OOD) detection tasks have made significant progress recently since the distribution mismatch between training and testing can severely deteriorate the reliability of AI systems. Nevertheless, the lack of precise interpretation for the in-distribution (ID) limits the application of the OOD detection methods to real-world systems. To tackle this, we decompose the definition of the ID into texture and semantics, motivated by the demands of real-world scenarios. We also design new benchmarks to measure the robustness that OOD detection methods should have. To achieve a good balance between the OOD detection performance and robustness, our method takes a divide-and-conquer approach. Specifically, the proposed model first handles each component of the texture and semantics separately and then fuses these later. This philosophy is empirically proven by a series of benchmarks including both the proposed and the conventional counterpart.\n\n1\n\nINTRODUCTION\n\nOut-of-distribution (OOD) detection is the task that recognizes whether the given data comes from the distribution of training samples, also known as in-distribution (ID), or not. Any machine learning-based system could receive input samples that have a completely disparate distribution from the training environments (e.g., dataset). Since the distribution shift can severely degrade the model performance (Amodei et al., 2016), it is a potential threat to reliable real-world AI systems.\n\nHowever, the ambiguous definition of the ID limits the feasibility of the OOD detection method in real-world applications, considering the various OOD scenarios. For example, subtle corruption is a clear signal of OOD in the machine vision field while a change in semantic information might not be. On the other hand, an autonomous driving system may assume the ID from the semanticoriented perspective; e.g., an unseen traffic sign is OOD. Unfortunately, most of the conventional OOD detection methods and benchmarks (Zhang et al., 2021; Tack et al., 2020; Ren et al., 2019; Chan et al., 2021) assume the ID as a single-mode thus they cannot handle other aspects of OOD properly (Figure 1a).\n\nTo tackle this, we revisit the definition of the ID by decomposing it into two factors: texture and semantics (Figure 1b). For the texture factor, we define OOD as the textural difference between the ID and OOD datasets. On the contrary, the semantic OOD focuses on the class labels that do not exist in the ID environment. Note that the two aspects have a trade-off relationship, thus detecting both problems with a single model is challenging with the (conventional) entangled OOD perspective.\n\nGeirhos et al. (2018) investigated the texture-shape cue conflict in the deep network and a series of subsequent studies (Hermann et al., 2019; Li et al., 2020; Ahmed & Courville, 2020) explored how to achieve a balance between these perspectives. However, they only analyzed the texture-shape bias inherited in deep networks. Instead, we focus on analyzing the texture and semantic characteristics underlying the ID to build a more practically applicable OOD detection method.\n\nUnfortunately, to the best of our knowledge, none of the studies on the OOD detection benchmark have thoroughly analyzed the definition of the ID. It can be problematic when the method judges the image corrupted by negligible distortion as OOD, even when the environment can tolerate the small changes in texture. Because of such a complicated scenario, it is crucial to evaluate the OOD detection method in a comprehensive way that goes beyond the simple benchmark. Thus, in this study, we propose a new approach to measuring the performance of the method according to the decomposed\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Traditional view of the ID\n\n(b) Decomposed view of the ID (ours)\n\nFigure 1: How to define the ID? (a) Traditional OOD detection studies manage the ID in an entangled view. However, this could be na ̈ıve considering the complex nature of the real environments. (b) We decompose the definition of the ID into texture and semantic; this provides the flexibility to handle complicated scenarios by determining which view of the ID is suitable for a given scenario.\n\ndefinition of the ID. One notable observation in our benchmark is that most of the previous OOD detection methods are highly biased to the texture information and ignore the semantic clues.\n\nTo mitigate aforementioned issue, our proposed method tackles the texture and semantic information separately and aggregates these at the final module (Figure 2). To effectively extract texture information, we use a 2D Fourier transform motivated by the recent frequency domain-driven deep method (Xu et al., 2020). For the semantic feature, we design an extraction module upon the Deep support vector data description (Deep-SVDD) (Ruff et al., 2018) with a novel angular distance-based initialization strategy. We then combine two features using the normalizing flowbased method (Dinh et al., 2016), followed by the factor control mechanism. The control system provides the flexibility to handle different OOD scenarios by choosing which decomposed feature is more important in the given surrounding OOD circumstances. The main contributions of this work are as follows:\n\n• We decompose the “unclear” definition of the ID into texture and semantics. To the best of\n\nour knowledge, this is the first attempt to clarify OOD itself in this field.\n\n• Motivated by real-world problems, we create new OOD detection benchmark scenarios.\n\n• We propose a novel OOD detection method that is effective on both texture and semantics as well as the conventional benchmark. Furthermore, our method does not require any auxiliary datasets or labels unlike the previous models.\n\n2 RELATED WORK\n\nClass labels of the ID. Early studies on deep OOD methods rely on class supervision. ODIN and Generalized ODIN (Liang et al., 2017; Hsu et al., 2020) use the uncertainty measure derived by the Softmax output. It determines a given sample as OOD when the output probability of all classes is less than a predefined threshold. (Sastry & Oore, 2020; Lee et al., 2018) utilize the extracted feature map (e.g., gram matrix) from the pre-trained networks to calculate the OOD score. Also, Zhang et al. (2020) employ a flow-based model that is comparable to ours, but they require class information during the training and solely pay attention to semantics.\n\nAuxiliary distribution. Outlier exposure (OE) (Hendrycks et al., 2018) exploits additional datasets that are disjoint from the test dataset to guide the network to better representations for OOD detection. (Papadopoulos et al., 2021) further improves the performance of OE by regularizing the network with the total variation distance of the Softmax output.\n\nData augmentation. Recently, contrastive learning-based methods have shown remarkable success on the tasks related to visual representation (He et al., 2020; Chen et al., 2020). Motivated by this, several studies employ data augmentation methods such as image transformation or additional noise on the OOD detection task or model (Hendrycks et al., 2019; Tack et al., 2020; Kirichenko et al., 2020).\n\n2\n\nTextural discrepancySemantic discrepancyTraining datasetIn-distribution manifoldIn-distribution manifoldsTexture driven in-distributionSemantic driven in-distributionUnder review as a conference paper at ICLR 2023\n\nFigure 2: Model overview. Our framework extracts the texture and semantic information with the corresponding modules, then combines them via the normalizing flow-based method. (a) Texture feature T (x) is distilled by the Fourier spectrum-based module. (b) We use multi-SVDD with a novel angular initialization to extract the semantic information S(x). (c) Output features are merged by the explicit probability inference method, RealNVP. Here, we introduce the user control parameter λ ∈ {0.0, 0.5, 1.0} to determine which feature is more suitable for a given OOD scenario.\n\nUnlike the prior studies that exploit additional information other than the ID dataset, we only utilize the given (ID) training dataset. In addition, we separate and clarify the assumptions of OOD in terms of texture and semantics to improve the real-world practicability.\n\n3 METHOD\n\nWe present an overview of the proposed method (Section 3.1), and each feature extraction module (Section 3.2 and 3.3). Finally, we introduce the normalizing flow-based conditional probabilistic modeling component (Section 3.4). Conventional OOD detection has an assumption that ID data are sampled from the distribution of the training dataset, x ∼ pdata. We decompose the image with two factors and calculate the anomaly score based on each factor’s likelihood. The texture module T (x) uses Fourier analysis to extract frequency-based features from input x. The semantic branch S(x) extracts the content label features such as shape. Our framework calculates the likelihood of these two factors and then combines them. Since we use the normalizing flow model that trains the exact likelihood, the extracted information is adjusted using a user-controllable hyperparameter λ.\n\n3.1 MODEL OVERVIEW\n\nWe aim to train our method with the decomposed ID likelihoods, p(T (x)|x) and p(S(x)|x) as shown in Figure 2. With the given input image x, we extract features for each factor with different modules: texture- and semantic-aided sub components.\n\nThe extracted features are combined by the controllable normalizing flow method. Since the normalizing flow-based model explicitly calculates the negative log-likelihood, we model each extracted information as log pθ(T (x)|x) and log pφ(S(x)|x), where θ and φ are trainable parameters of the networks. In addition, we introduce the control parameter λ ∈ [0.0, 0.5, 1.0] to model the final adjusted log-probability as λ · log p(S(x)|x) + (1 − λ) · log p(T (x)|x). With this control mechanism, users can switch to the appropriate model “mode” by referring to their prior knowledge. For example, when the texture information overwhelms the semantic one for detecting OOD, we can overweight λ for better performance. By default, we use the λ value of 0.5 (no prior knowledge).\n\n3.2 EXTRACTING THE SEMANTIC INFORMATION\n\nMulti-SVDD. Beyond the one-class anomaly detection methods that consider the normal data as a single class (e.g., DeepSVDD (Ruff et al., 2018)), recent studies have viewed the normal data as the union of the multiple hidden semantic information (Ghafoori & Leckie, 2020; Park et al., 2021). Inspired by this idea, we use the multi-SVDD method to extract the semantic information in an unsupervised manner for the OOD detection task.\n\nMulti-SVDD embeds the samples to the multiple center vectors as closely as possible. Suppose a set of center vectors C = {c1, ..., cK} is initialized via K-means and the radius of each center is\n\n3\n\nxESETResNetMulti-SVDD2D DFTS(x)T(x)λlogpθ(T(x)|x)logpφ(S(x)|x)RealNVPRealNVPUnder review as a conference paper at ICLR 2023\n\n(a) K-means based initialization\n\n(b) Angular distance initialization\n\nFigure 3: Comparison of the initialization strategy in multi-SVDD. (a) The model with K-means initialization is effective at the anomaly detection but not for the OOD detection scenario. This is because the definition of the anomaly is when the samples do not belong to the cluster regions (dark shade), while the definition of OOD is when the samples do not lie in the ID manifold (light shade). (b) Our proposed angular distance-based initialization guides the initial center vectors to be positioned following a (virtual) circular line. As a result, it prevents the OOD samples from not belonging to the ID by creating tight cluster layouts without a hole.\n\nr = [r1, ..., rK]. In multi-SVDD, the objective function is defined as follows.\n\nmin W,r\n\nK (cid:88)\n\nk=1\n\nr2 k +\n\n1 νn\n\nn (cid:88)\n\ni=1\n\nmax {0, ∥φ(xi; W) − cj∥2 − r2\n\nj } +\n\n(cid:88)\n\n∥W∥2.\n\nη 2\n\n(1)\n\nHere, φ(xi; W) is the deep network with a set of weight parameters W and cj is assigned to φ(xi; W). As the set r is decreased, the samples are condensed into the center vectors. By using the distance between the center vectors and the samples, we get an anomaly score.\n\nAngular distance initialization. SVDD is proposed for the anomaly detection task. Because of the disparity between OOD and anomaly detection, the direct application of the SVDD-based model to OOD detection causes unexpected performance degradation. In anomaly detection, although the abnormal samples lie in the ID manifold, it is possible to detect them as abnormal unless they are close to the center vectors cj. For example, as in Figure 3a, OOD samples (red) that are located inside of the ID manifold (light blue shade) can be detected as abnormal since they are outside of the tight cluster boundary (dark blue shade). Because of these, a mixture of Gaussian probability density is a reasonable density space for the anomaly detection model.\n\nUnlike the anomaly detection task, the OOD detection is to find the samples that are not ID. In Figure 3a, all the OOD samples placed in the ID manifold (light dark shade) are recognized as the ID. We propose an angular distance-based center vector initialization strategy to tackle this issue. Angular-based embedding has a clear geometric interpretation (Deng et al., 2019), and they can handle the complex and subtle differences in feature space. Our proposed method is as follows:\n\nck = γ\n\nv ∥v∥\n\n, v ∈ Rh ∼ N (0, 1)\n\n(2)\n\nwhere h is the dimension of the embedding space and γ is the hyper-parameter for the radius of the sphere. After φ(xi; W) is trained based on angular initialization, semantic features are extracted through this model; S(xi) = φ(xi; W). By setting the γ value sufficiently large, we ensure that all sample data are within a radius of the sphere as illustrated in Figure 3b. While Equation 1 drives the training samples to be embedded around the center vectors on the sphere, the OOD samples remain near the origin. This embedding space may be weak to recognize the semantic label of a given sample, but it is sufficient to identify whether a sample is OOD or not.\n\n3.3 EXTRACTING THE TEXTURE INFORMATION\n\nTo effectively extract the texture property of the ID, we interpret the image in the frequency space. With a given input image x ∈ R3×h×w, we first convert it into the frequency domain using Discrete Fourier Transform (DFT) F as shown below.\n\nF(fx, fy) =\n\n1 hw\n\nh−1 (cid:88)\n\nw−1 (cid:88)\n\np=0\n\nq=0\n\nI(p, q) · e−i2π(fxp/h + fyq/w),\n\n(3)\n\n4\n\nIn-distributionCenter vectorOOD sampleID sampleAngular InitializationIn-distributionUnder review as a conference paper at ICLR 2023\n\nHere, I(p, q) denotes the pixel value of the image at the (p, q)-coordinate and F(fx, fy) is the output of the DFT at the Cartesian coordinate (fx, fy) in the frequency space. In order to construct a scale and rotation invariant frequency information in 2D image, we modify the coordinate system from Cartesian (fx, fy) to polar (fr, θ), following (Dzanic et al., 2019).\n\nF(fr, θ) = F(fx, fy)\n\n:\n\nfr =\n\n(cid:115)\n\nx + f 2 f 2 4 (m2 + n2)\n\ny\n\n1\n\n,\n\nθ = atan2 (fy, fx).\n\n(4)\n\nSince directly computing the polar coordinate is computationally expensive and tricky, we iteratively calculate the rotation invariant frequency feature. To do that, we only utilize the first channel of the image as x ∈ R1×h×w and assume that the image is square (i.e., h = w). Let T (x) ∈ Rw/2 be the texture feature vector of the image x. Then, the i-th element of Ti(x) is calculated by Equation 5. w\n(cid:88)\n\nw (cid:88)\n\nTi(x) = Ri − Ri−1\n\n: Rw =\n\nF(fx, fy), R0 = F(0, 0)\n\n(5)\n\nfx=−w Note that fr represents the direction of the frequency of an image. Since Ti(x) gathers the boundary region of the rectangular-shape feature map, Ri (as Equation 5), the same frequency power of an image collapses into Ti(x). Consequently, T (x) loses the information of the object’s shape and it only contains the frequency-based texture information.\n\nfy=−w\n\nDiscussion. We compare the power spectrum density (PSD) of CIFAR-10 (C10), CIFAR-100 (C100), and distorted C10 (Figure 4). The PSD discrepancy between the corrupted C10 and the vanilla one indicates that the image feature acquired from the frequency domain is adequate to represent the texture cue. In contrast, C10 and C100 are not distinguishable in the frequency domain since they have a very similar image texture due to the small resolution. These observations support our assumption that the OOD detection model should decouple the texture and semantic information to handle real-world cases properly.\n\n3.4 FEATURE COMPOSITION VIA NORMALIZING FLOW\n\nFigure 4: PSD analysis.\n\nSince we design our framework to directly sample the probability, it does not require any ad-hoc scoring functions. Instead, we use a normalizing flow-based method (Dinh et al., 2014; Rezende & Mohamed, 2015; Dinh et al., 2016) that uses the probability of given samples as a loss function. In the following, we will describe how to get the probability of samples from the prior probability (Normal distribution) using the normalizing flow.\n\nGiven sample x, a normal prior probability distribution pZ on a latent variable z ∈ Z, and a bijection f : X → Z (with g = f −1), the change of variables defines a model distribution on X by\n\npX (x) = pZ\n\n(cid:0)f (x)(cid:1)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ndet\n\n(cid:18) ∂f (x) ∂xT\n\n(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)\n\n,\n\n(6)\n\nwhere ∂f (x) ∂xT is the Jacobian of f at x. The function f can be decomposed into f = f1 ◦ · · · ◦ fk. To compose the features, we use a flow-based RealNVP with coupling layers (Dinh et al., 2016). Here, T (xi) and S(xi) are input of RealNVP. Given the ID training dataset D, the objective of the RealNVP model with the trainable parameters θ and φ is to minimize the following loss function.\n\nLt(D) =\n\n1 N\n\nN (cid:88)\n\ni=1\n\n− log pθ(T (xi)|xi), Ls(D) =\n\n1 N\n\nN (cid:88)\n\ni=1\n\n− log pφ(S(xi)|xi)\n\n(7)\n\nAlthough we extract image features from two different aspects, it is not guaranteed that the features are completely disentangled. To mitigate the unexpected entangling, we propose disentanglement loss using the independence property of log probabilities. If T (xi) and S(xi) are independent given xi, then log p(T (xi)) + log p(S(xi)) = log p(T (xi), S(xi)). Note that log p(T (xi), S(xi)) can be computed by concatenating features S(x) and T (x). The disentanglement loss is calculated as:\n\nLdisentangle =\n\n1 N\n\nN (cid:88)\n\n{(log p(T (xi)) + log p(S(xi))) − log p(T (xi), S(xi))}2.\n\n(8)\n\ni=1\n\n5\n\nCIFAR-10w/ corruptionCIFAR-10001020304050C10C10 w/ corruptionC100Under review as a conference paper at ICLR 2023\n\nID → OOD\n\nUsing Labels Other Dist. Maha Gram OE OEC\n\nSelf-supervised\n\nRot\n\nSSL CSI\n\n0.0\n\nOurs λ = 0.5\n\nN H\nV S\n\n0 1\nC\n\n0 0\n1 C\n\nC10 C100 TinyImgNet LSUN*\n\nSVHN C100 TinyImgNet LSUN*\n\nSVHN C10 TinyImgNet LSUN*\n\n99.3 -\n99.3 99.9\n\n99.1 88.2 99.5 99.3\n\n98.4 77.5 97.4 98.2\n\n97.3 -\n97.3 99.8\n\n99.5 79.0 99.7 99.9\n\n97.3 67.9 99.0 99.3\n\n99.3 99.0 -\n99.9\n\n98.2 92.9 -\n96.4\n\n82.8 77.5 -\n79.5\n\n99.8 99.9 -\n99.9\n\n99.2 93.8 -\n98.9\n\n95.8 77.7 -\n88.8\n\n- -\n- -\n\n97.8 82.3 -\n92.8\n\n- -\n- -\n\n99.8 99.8 -\n99.9\n\n99.2 93.8 -\n98.9\n\n95.8 77.7 -\n88.8\n\n- -\n- -\n\n99.8 89.2 -\n97.5\n\n- -\n- -\n\n93.9 91.1 99.5 99.9\n\n86.1 55.7 65.4 99.9\n\n80.0 49.4 91.7 99.9\n\n99.9 99.8 100. 100.\n\n99.9 93.6 99.9 99.8\n\n99.9 83.1 100. 100.\n\n1.0\n\n100. 99.9 100. 100.\n\n99.9 93.5 99.9 81.9\n\n100. 84.2 100. 100.\n\nTable 1: Conventional OOD detection benchmark. We evaluate the detection performance by AUC (in- vs. out-distribution detection based on confidence/score) in percentage (higher is better). * indicates the high-resolution dataset.\n\nThe final loss is the sum of all the aforementioned losses; L = Lt + Ls + Ldisentangle. One nice side-effect of this probabilistic modeling for texture-semantics decomposition is that we can adjust the contribution of these components by referring to the given OOD environment. Since we designed the features to be separate, we combine them with a linear interpolation as:\n\nλ · log pφ(S(x)|x) + (1 − λ) · log pθ(T (x)|x)\n\n(9)\n\nwhere λ ∈ [0.0, 0.5, 1.0] is the control parameter (default is 0.5). In our experiments, we denote λ = 0.0 as “texture mode” and λ = 1.0 as “semantic mode”.\n\n4 EXPERIMENTS\n\nWe demonstrate the effectiveness of our method on various benchmark setups. In Section 4.1, we report the performance on the conventional OOD detection task and discuss the limitations of the previous OOD detection studies. We use the area under the curve (AUC) for the receiver operating characteristic (ROC) curve to evaluate the performance. Then, we will discuss the factor-aware OOD detection that is proposed to fill the gap between the current benchmark and the real-world environment (Section 4.2). We conclude this part with model analysis (Section 4.3).\n\n4.1 CONVENTIONAL OOD DETECTION\n\nSetups. Here, we evaluate OOD detection methods on the widely used OOD detection benchmark. We select SVHN (Netzer et al., 2011), CIFAR-10 (Krizhevsky et al., 2009) (C10), and CIFAR100 (Krizhevsky et al., 2010) (C100) as the ID dataset. To simulate the OOD samples, LSUN (Yu et al., 2015) and Tiny-ImageNet (Torralba et al., 2008) datasets are additionally used.\n\nBaselines. We compare our approach to the methods belonging to three different OOD detection groups. 1) Methods that use class labels of the training samples; feature-based methods such as Maha (Lee et al., 2018) and Gram (Sastry & Oore, 2020) fall into this category. 2) Methods that utilize an additional distribution (dataset) such as OE (Hendrycks et al., 2018) and OEC (Papadopoulos et al., 2021). 3) Self-supervised based methods; rotation-based (Rot) (Hendrycks et al., 2019), SSL (Mohseni et al., 2020), and CSI (Tack et al., 2020) are in this group.\n\nResults. As shown in Table 1, our proposed method surpasses the competitors on the conventional OOD detection tasks without using any extra information such as class labels, other datasets, or In detail, ours with λ = 1.0 image-transformation techniques that the other methods required. (semantic mode) achieves the best performance in all the cases with the exception of two scenarios.\n\nDiscussion. As we have discussed, it is often risky to assume that the input samples of a real-world system have a single and entangled characteristic. We argue that it is more natural to consider in\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nterms of multiple factors such as texture and semantics when detecting OOD. For example, when a given environment requires detection of the textural discrepancy as OOD, then the detection method should concentrate on the textural side alone, not the semantic counterpart.\n\nUnfortunately, conventional OOD detection benchmarks are not developed to measure the disentangled view of the ID. Here, we dissect the traditional benchmark to quantify the effect of each factor we decompose. We first categorized the datasets into the high- and low-resolution groups by their image resolution: C10, C100, SVHN, and TinyImageNet belong to the low-resolution (LR) group while LSUN is high-resolution (HR). Table 1 shows that previous studies achieve high performance on the LR → HR scenario, and especially the label-based methods produce outstanding results. However, we will show the superior performance of these methods derives from their abuse of texture information (e.g., detect as OOD by highly referring to image resolution) in Section 4.2.\n\nOn the other hand, we argue that the semantic property is the key component to identity the OOD of LR → LR scenarios. For example, C10 ↔ C100 solely requires semantic information to detect OOD since these datasets use very similar images (in terms of the texture and image resolution). This is the reason why ours with λ = 0.0 (texture mode) cannot detect OOD at all (55.7 and 49.4 AUC). Note that other competitors also show inferior performance, especially in the C10 ↔ C100 case, which demonstrates that these methods have weakness at handling semantic information.\n\n4.2 FACTOR-AWARE OOD DETECTION\n\nMotivation. For a more in-depth evaluation of OOD detection performance, we propose a novel OOD detection benchmark in which multiple factors (or environments) are involved. Conventional setups simply define the ID and OOD as different datasets. Unfortunately, this assumption is often invalid in real-world use cases. Imagine a distorted image scenario; would we have to label this corrupted image as OOD? Most previous studies would obviously determine it as OOD, but this heavily depends on the surrounding circumstances; for example, if an OOD detection system is granted a role to send an alert when a new class object appears, then it has to disregard mild corruption.\n\nTherefore, the goal of this benchmark is to evaluate from the OOD detection performance (Figure 5a and b) and the robustness, whether the model can judge the given sample as ID when it differs from the training set while still coming from inside the “interest boundary” (Figure 5c and d). One consecutive question may arise: what kind of factors should be considered? Certainly, in real-world applications, various factors can be derived from given samples or datasets. However, we narrow the scope of the factors to texture (Figure 5a and c) and semantics (Figure 5b and d), since these are the most important aspects considering the decision-making process in the real world.\n\nSetups. To evaluate OOD detection performance, AUC is used; 100.0% AUC is the best. To measure robustness, we define 50% AUC as the best since this indicates that the detection method cannot distinguish test samples as OOD (determined as ID). In this benchmark, we use corrupted C10, SVIRO (Cruz et al., 2020), BTAD (Mishra et al., 2021), MNIST variants, and rotated MNIST datasets as shown in Figure A.1. Detailed descriptions of each dataset are described in the experience parts.\n\nDetecting texture discrepancy of OOD. The goal of this scenario is to detect the texture discrepancy. In real-world application, the factory process requires quality control to find cracks in industrial products. To meet the demand, we use BTAD dataset (Mishra et al., 2021), which is composed of 2,830 industry-oriented images of normal and defective products. We set the normal samples as the ID and the cracked ones as OOD. Furthermore, as discussed earlier, we simulate the severe distortion scenario by applying severe levels of frost, shot noise, haze, and motion blur to C10. In this case, the model is required to detect the distortions as OOD, simulating the circumstances where the system should find the corrupted sensing module.\n\nAs shown in Table 2, our method with texture mode (λ = 0.0) outperforms all the others in BTAD case and achieves comparable performance for severe distortion scenario. On the other hand, the semantic mode (λ = 1.0) shows substantially lower AUC. This is unsurprising because the current benchmark scenario does not require semantic information; distortions and cracks (BTAD) are only related to the texture clue. Gram and Maha also achieve high performance; however, we find that these methods do not have sufficient robustness as will be discussed in the Robustness experiment.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Detect. - Texture\n\n(b) Detect. - Semantics\n\nGoal - Factor\n\nAUC (Oracle)\n\nExperiments\n\n(c) Robust. - Texture\n\n(d) Robust. - Semantics\n\nDetect - Texture\n\nDetect - Seman.\n\nBTAD (Mishra et al., 2021) (T2)\n\n100.\n\nSevere distortion (T2)\n\nRotated MNIST (T3)\n\nMNIST↔F-MNIST (T3)\n\nImage resolution (T4)\n\nMild distortion (T4)\n\nSVIRO (Cruz et al., 2020) (T5)\n\nMNIST↔K-MNIST (T5)\n\nRobust - Texture\n\nRobust - Seman.\n\n50.0\n\nFigure 5: Taxonomy of our proposed benchmark. Left: (a, b) OOD detection scenario on two factors: texture, and semantics. In this case, the training set is ID, while the test set is OOD. (c, d) Evaluating the robustness on factor discrepancy of ID by providing ID samples at test time. Here, an oracle produces 50.0 AUC (cannot distinguish it as OOD at all) since the given samples are from ID. Right: A list of the corresponding experiments in each scenario.\n\nMethod\n\nSevere distortion C10→\n\nBTAD Normal→ Frost Haze Motion Shot Anomaly\n\nMethod\n\n15◦\n\nRotated MNIST 0◦→ 45◦\n\n60◦\n\n30◦\n\nMNISTs MNIST→\n\n75◦ FMNIST\n\nODIN 79.9 Maha 97.2 Gram 99.9 CSI 84.3\n\n0.0 83.4 λ = 0.5 80.4 1.0 67.5\n\n69.5 99.8 99.9 81.6\n\n94.4 73.5 67.1\n\n80.9 97.4 99.9 87.6\n\n91.0 90.8 63.1\n\n84.2 99.8 99.8 97.1\n\n96.1 82.5 74.1\n\n52.2 79.1 81.1 61.1\n\n92.1 90.0 61.0\n\nODIN 68.1 84.4 91.9 96.3 97.7 Maha 80.0 86.1 88.1 87.9 89.9 Gram 88.8 91.0 96.6 98.8 99.1 CSI 59.2 60.0 60.8 61.8 61.9\n\n0.0 51.1 51.1 52.3 54.6 59.1 λ = 0.5 65.5 84.1 90.1 95.5. 91.1 1.0 67.1 89.6 96.8 98.5 99.8\n\n74.4 96.8 99.9 56.1\n\n91.8 100. 99.9.\n\nTable 2: Comparison of the texture discrepancy detection of OOD.\n\nTable 3: Comparison of the semantic discrepancy detection of OOD.\n\nDetecting semantic discrepancy of OOD. With this benchmark, we evaluate the method’s sensitivity to the semantic difference between the training and test samples. Catching semantic discrepancies is important in many applications such as autonomous systems. For example, this should judge whether a given traffic symbol is geometrically distorted for a stable recognition system (some recognition models are vulnerable to rotation). Here, we use two testbeds: MNIST→F-MNIST and rotated MNIST scenarios. For the rotated MNIST, we train the model on MNIST and evaluate the rotated version. In most cases, our method with semantic mode surpasses the others (Table 3).\n\nRobustness on Texture Discrepancy of ID. We conduct the benchmark on two scenarios: mild distortion and varying image resolution. Both cases assume that the definition of OOD is in the disparity of semantic information. Particularly, a subtle change of image should not be considered as a signal of OOD, but rather recognized as ID. For example, the system that is requested to detect abnormal events should be robust on the mild image distortions. To implement the distortion case, we corrupt C10 with a mild level. For the varying resolution, we center-crop and resize back to the original resolution. We carefully adjust the crop operation so as not to harm the image information.\n\nTable 4 shows the AUC score of this benchmark. Since this experiment focuses on detecting the test samples as the ID, 50.0% AUC is the best. We observe that the deep feature-based methods (e.g., Maha and Gram) have little robustness against the texture discrepancies. They are not potentially appropriate for applications that do not treat a mild corruption as OOD. Furthermore, all the methods except ours with semantic mode are extremely sensitive to image resolution change, although no other information is modified. In contrast, our method with λ = 1.0 (semantic mode) achieves the best performance because this focuses on the semantic information alone.\n\nRobustness on Semantic Discrepancy of ID. Unlike the above, we focus on the robustness against the semantic discrepancy presented in ID. To evaluate this, we use MNIST→KMNIST and\n\n8\n\nTest dataset (OOD)Interest boundaryTraining dataset (ID)Textural discrepancySemantic discrepancyTraining dataset (ID)Test dataset (OOD)Training dataset (ID)Test dataset (ID)Training dataset (ID)Test dataset (ID)Under review as a conference paper at ICLR 2023\n\nMethod\n\nMild distortion C10→\n\nImage resolution 322 →\n\nFrost Haze Motion Shot 362 402 442 482\n\nMethod\n\nSVIRO A-class→\n\nMNISTs MNIST→ Tesla Escape Tiguan i3 KMNIST\n\nODIN 72.6 75.2 Maha 88.3 99.3 Gram 99.8 99.6 CSI 73.1 60.1\n\n0.0 68.8 59.3 λ = 0.5 64.3 58.0 1.0 56.6 55.0\n\n80.9 97.4 99.9 87.6\n\n79.0 71.8 54.1\n\n84.2 55.7 62.7 67.5 91.2 99.8 69.0 71.1 79.9 80.0 99.8 66.5 74.8 82.4 92.9 97.1 81.5 86.1 89.9 93.5\n\nODIN 67.3 Maha 90.0 Gram 91.1 CSI 65.5\n\n91.1 59.9 67.7 75.8 82.8 82.5 50.2 55.8 71.2 72.8 74.1 50.1 55.0 60.9 66.0\n\n0.0 60.0 λ = 0.5 90.0 1.0 96.5\n\n53.4 76.0 91.9 54.4\n\n59.4 79.2 84.1\n\n56.2 88.1 81.1 53.2\n\n55.8 79.0 87.6\n\n58.9 77.2 81.2 57.2\n\n55.3 76.6 90.9\n\n94.4 91.6 97.1 72.2\n\n50.6 100. 100.\n\nTable 4: Comparison of the robustness on the texture discrepancy of ID.\n\nTable 5: Comparison of the robustness on the semantic discrepancy of ID.\n\nMethod\n\nC10→\n\nC100→ SVHN→ C100 SVHN C10 SVHN C10 C100\n\nMulti-SVDD 53.7 + Ldisentangle 69.5 + Angular init. 93.5\n\n99.7 67.5 84.2 87.5 84.2 99.8 70.9 91.0 99.9 98.9 99.9 84.2 100. 100. 99.9\n\nMethod\n\nODIN Gram CSI Ours\n\nC10\n\n56.2 79.1 61.9 50.1\n\nC100\n\nSVHN\n\n53.3 69.1 52.2 49.8\n\n52.2 71.1 51.1 50.0\n\nTable 6: Ablation study on the semantic module. Multi-SVDD without our proposed components cannot properly handle the semantics information.\n\nTable 7: ID→ID scenario. All the models are trained on the “training set” of the ID dataset and evaluated on its “test set”.\n\nSVIRO (Cruz et al., 2020) scenarios. SVIRO is the collection of images from diverse vehicle interior rear seats. Among them, we set Mercedes A-class as ID and other car brands as the OOD datasets. With this, we simulate the following scenario: the machine learning system utilizing the camera installed inside of the vehicle should be robust to the vehicle type (semantics discrepancy).\n\nFor MNIST → K-MNIST (Table 5), our method with λ = 0.0 (texture mode) successfully determines that K-MNIST is the ID (AUC is 50.6). In contrast, our model with other λ values (0.5 and 1.0) cannot, since these mostly depend on the semantic clue which is meaningless in this case. Similarly, our method with λ = 0.0 (texture mode) achieves good performance on SIVRO.\n\n4.3 MODEL ANALYSIS\n\nSemantic module. We validate the proposed component of the semantic module (Table 6). The disentangle loss plays an important role in the model performance and the angular initialization also improves the conventional initialization method in a huge margin, implying the superiority of our method when extracting the semantic information. To further present the effectiveness of our strategy, we plot the distribution of the embedding results in Figure B.2.\n\nID → ID scenario. If we test an OOD detection model using the test set of the ID dataset, then ideally the prediction for it should be ID. On the other hand, when the OOD detection model overfits the training set, then the AUC score for the test set would go to 100.0%. As shown in Table 7, our method determines the ID almost perfectly, and CSI and ODIN also show reasonable performance. However, the Gram-based method is rather sensitive to this and we argue that it might be due to abuse of features of the seen (training) dataset.\n\n5 CONCLUSION\n\nIn this work, we introduce a novel viewpoint of the ID for the practically applicable OOD detection. We separate the definition of the “single-mode” ID into “texture” and “semantics” factors by following the requirements of the real-world applications. To effectively handle both aspects, we take a divide-and-conquer strategy that extracts the features using the appropriate method in each factor and then combines these with the normalizing flow-based model. By doing so, our method outperforms previous models on both our newly proposed benchmark scenarios and the conventional OOD detection cases. We hope that our work can provide useful guidance for future OOD detection work.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nFaruk Ahmed and Aaron Courville. Detecting semantic anomalies.\n\nIn Proceedings of the AAAI\n\nConference on Artificial Intelligence, volume 34, pp. 3154–3162, 2020.\n\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man ́e. Con-\n\ncrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.\n\nRobin Chan, Krzysztof Lis, Svenja Uhlemeyer, Hermann Blum, Sina Honari, Roland Siegwart, Mathieu Salzmann, Pascal Fua, and Matthias Rottmann. Segmentmeifyoucan: A benchmark for anomaly segmentation. arXiv preprint arXiv:2104.14812, 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nSteve Dias Da Cruz, Oliver Wasenmuller, Hans-Peter Beise, Thomas Stifter, and Didier Stricker. Sviro: Synthetic vehicle interior rear seat occupancy dataset and benchmark. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 973–982, 2020.\n\nJiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4690–4699, 2019.\n\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-\n\nmation. arXiv preprint arXiv:1410.8516, 2014.\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv\n\npreprint arXiv:1605.08803, 2016.\n\nTarik Dzanic, Karan Shah, and Freddie Witherden. Fourier spectrum discrepancies in deep network\n\ngenerated images. arXiv preprint arXiv:1911.06465, 2019.\n\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.\n\nZahra Ghafoori and Christopher Leckie. Deep multi-sphere support vector data description.\n\nIn Proceedings of the 2020 SIAM International Conference on Data Mining, pp. 109–117. SIAM, 2020.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF Conference on\n\nunsupervised visual representation learning. Computer Vision and Pattern Recognition, pp. 9729–9738, 2020.\n\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-\n\nruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.\n\nDan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier\n\nexposure. arXiv preprint arXiv:1812.04606, 2018.\n\nDan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning\n\ncan improve model robustness and uncertainty. arXiv preprint arXiv:1906.12340, 2019.\n\nKatherine L Hermann, Ting Chen, and Simon Kornblith. The origins and prevalence of texture bias\n\nin convolutional neural networks. arXiv preprint arXiv:1911.09071, 2019.\n\nYen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting outIn Proceedings of the\n\nof-distribution image without learning from out-of-distribution data. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10951–10960, 2020.\n\nPavel Izmailov, Polina Kirichenko, Marc Finzi, and Andrew Gordon Wilson. Semi-supervised learning with normalizing flows. In International Conference on Machine Learning, pp. 4615–4630. PMLR, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nPolina Kirichenko, Pavel Izmailov, and Andrew G Wilson. Why normalizing flows fail to detect out-of-distribution data. Advances in neural information processing systems, 33:20578–20589, 2020.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-\n\nsearch). URL http://www. cs. toronto. edu/kriz/cifar. html, 5:4, 2010.\n\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.\n\nYingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, and Cihang Xie. Shape-texture debiased neural network training. arXiv preprint arXiv:2010.05981, 2020.\n\nShiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution\n\nimage detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.\n\nZiqian Lin, Sreya Dutta Roy, and Yixuan Li. Mood: Multi-level out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15313– 15323, 2021.\n\nPankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, and Gian Luca Foresti. Vt-adl: A vision transformer network for image anomaly detection and localization. In 2021 IEEE 30th International Symposium on Industrial Electronics (ISIE), pp. 01–06. IEEE, 2021.\n\nSina Mohseni, Mandar Pitale, JBS Yadawa, and Zhangyang Wang. Self-supervised learning for generalizable out-of-distribution detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 5216–5223, 2020.\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading\n\ndigits in natural images with unsupervised feature learning. 2011.\n\nAristotelis-Angelos Papadopoulos, Mohammad Reza Rajati, Nazim Shaikh, and Jiamian Wang. Outlier exposure with confidence control for out-of-distribution detection. Neurocomputing, 441: 138–150, 2021.\n\nJuneKyu Park, Jeong-Hyeon Moon, Namhyuk Ahn, and Kyung-Ah Sohn. What is wrong with\n\none-class anomaly detection? arXiv preprint arXiv:2104.09793, 2021.\n\nJie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. Advances in Neural Information Processing Systems, 32:14707–14718, 2019.\n\nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-\n\ntional conference on machine learning, pp. 1530–1538. PMLR, 2015.\n\nLukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel M ̈uller, and Marius Kloft. Deep one-class classification. In International conference on machine learning, pp. 4393–4402. PMLR, 2018.\n\nChandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with gram\n\nmatrices. In International Conference on Machine Learning, pp. 8491–8501. PMLR, 2020.\n\nJihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive\n\nlearning on distributionally shifted instances. arXiv preprint arXiv:2007.08176, 2020.\n\nAntonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence, 30(11):1958–1970, 2008.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nKai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, and Fengbo Ren. Learning in the frequency domain. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1740–1749, 2020.\n\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n\nAlireza Zaeemzadeh, Niccol`o Bisagno, Zeno Sambugaro, Nicola Conci, Nazanin Rahnavard, and Mubarak Shah. Out-of-distribution detection using union of 1-dimensional subspaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9452– 9461, 2021.\n\nHongjie Zhang, Ang Li, Jie Guo, and Yanwen Guo. Hybrid models for open set recognition. In\n\nEuropean Conference on Computer Vision, pp. 102–117. Springer, 2020.\n\nMingtian Zhang, Andi Zhang, and Steven McDonagh. On the out-of-distribution generalization of\n\nprobabilistic image modelling. arXiv preprint arXiv:2109.02639, 2021.\n\nEv Zisselman and Aviv Tamar. Deep residual flow for out of distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13994–14003, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA EXPERIMENT SETUPS\n\nA.1 MODEL TRAINING\n\nWe use Adam (Kingma & Ba, 2014) with a weight decay of 5e-5 and a batch size of 256 for 100 epochs to train all the modules in our framework. Extracted features by the ET and ES have the same 112 dimensions. We train and test with a single NVIDIA 1080TI GPU.\n\nA.2 MODEL ARCHITECTURE\n\nMulti-SVDD. We use ResNet-18 as the encoder network and modify the dimension of the last layer to 512. All the hidden dimensions of semantic modules have the same as the original setup. The γ parameter for the angular distance is set as 250 for all in-distribution datasets.\n\nRealNVP. We use the RealNVP implementation following (Izmailov et al., 2020)1. In detail, each perspective model has 8 blocks of 8 flows and we use an affine coupling that is defined by the fully connected shift and scale networks each of which 16 dimensions of hidden layers.\n\nA.3 OOD BENCHMARK\n\nDatasets. Figure A.1 shows examples of the datasets in the proposed OOD detection benchmark. To quantify the role of texture information, we use corrupted C10 dataset (Figure A.1a). We distort images of the C10 dataset with frost, haze, motion blur, and shot noises (following C10-C settings (Hendrycks & Dietterich, 2019)). With such corrupted C10 scenarios, we try to simulate the weather change or small sensing error of the vision system. Note that we separate the distortion scenario into mild and severe corruption cases to cover the various demands from the real applications. Specifically, the mild scenario is utilized to evaluate the textural robustness of the method. It simulates the environments where slight textural deviation should be ignored and the algorithm should concentrate on the semantics changes. For instance, the vision-based surveillance system requires such capability since it should be robust to minor changes (e.g., weather, dust, etc.) and detect the environment change regarding the semantics (e.g., trespassing). The severe distortion case, on the other hand, is used to measure the OOD detection performance with a given textural change. A system that should notice the deviation of textural dissimilarity falls in this category.\n\nFigure A.1: Examples of the datasets used in the proposed benchmark. The corresponding experiment for each dataset is illustrated in Figure 5.\n\nFigure A.1b displays the SVIRO dataset (Cruz et al., 2020), which is a collection of vehicle interior images. Each car dataset is consist of images taken on the rear seats with six car brands (as shown in figure) and among them, we set the ID as whole directions of Mercedes A-class rear seat and OOD\n\n1https://github.com/izmailovpavel/flowgmm\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nas other car datasets. With this dataset, we measure the robustness of the semantics change for the following potential scenario: The machine learning system utilizing the camera installed inside of the vehicle should not behave differently although the vehicle interior is changed. In other words, the system should be robust to the vehicle type (semantics discrepancy).\n\nMNIST variants such as K-MNIST and F-MNIST are used to measure semantics-aware benchmarks (Figure A.1c). In addition, we generate a rotated MNIST dataset by rotating images of the MNIST dataset as illustrated in Figure A.1d. Since MNIST-based datasets are composed of grayscale images, we replicate the color channel by three times before feeding into the detection method.\n\nTo evaluate the ability to detect the products’ crack from the real environment, we use the BTAD dataset (Mishra et al., 2021) as shown in Figure A.1e. In detail, we selected the cracked image from the test dataset of BTAD and reorganize it as the ID dataset contains only non-cracked (normal) product images and the OOD dataset has cracked images only.\n\nImage resolution. When we are able to acquire the official pre-trained network or samples, we provide the 32×32 resolution images as input, which is de facto setup in this field. If not the case (such as our benchmark), we prepossess the image as to have 32×32 resolution following the official setup. In our method, except for the resolution change experiment, we resize the images to 112×112 resolution to give enough information to the texture extraction module.\n\nB MODEL ANALYSIS\n\nFigure B.1: Power spectrum density (PSD) analysis of the blending and Jigsaw augmentations.\n\nTexture information analysis. Our texture extraction module T (x) is induced to lose the shape information so that we can ensure that the texture and semantic features are disentangled. To validate that T (x) concentrates on the texture information, we analyze the power spectrum density (PSD) on the transformed CIFAR dataset (Figure B.1). For transformation of CIFAR, we blend the C10 and C100 in half ratios with alpha-blending (denoted as Blend). Also, we cut the image into four parts and randomly arrange them as Jigsaw augmentation (denoted as Jigsaw). We observe slight differences in the mid-level frequency for Jigsaw due to the hard crossing border at the center of the image but no significant change. Moreover, even though the sample’s shape is mixed in the alphablend case, it can be seen that PSD is almost identical. These inspections support that T (.) forgets the semantics (especially the shape clue) as expected when embedding the features.\n\nSemantic embedding results. In Figure B.2, we plot the embedding results of the proposed semantics extraction module to verify the effectiveness of using semantic information. To visualize, we apply principal component analysis (PCA) to the extracted feature and make the dimension three. Figure B.2a depicts the scenario where MNIST is used as the ID dataset and other MNIST variants are the OOD. Figure B.2b shows the case where C10 is used as the ID dataset while C100 and SVHN are OOD. For both results, we observe that the embedding manifolds of the ID datasets are easily separable from the OODs’ counterparts, and our semantics extraction module S(x) concentrates on semantics information, such as labels of C10 and C100.\n\nOn the other hand, as presented in Figure B.2c and d, the semantics module S(x) cannot identify the textural change such as the image distortions or resolution changes; all the embeddings lie in similar manifolds. These results are natural and expected since the semantics extraction module is designed to solely focus on the semantics side, not the texture.\n\nPower Spectrum Density (PSD) analysis. To validate our assumption that texture information could be handled via Fourier transform, we visualize the PSD for all the datasets of the proposed\n\n14\n\n01020304050C10C100BlendJigsawUnder review as a conference paper at ICLR 2023\n\n(a) ID: MNIST OOD:{K, F}-MNIST\n\n(b) ID: C10 OOD: C100, SVHN\n\n(c) ID: C10 OOD: C10 with motion blur\n\n(d) ID: C10 OOD: C10 with resolution change\n\nFigure B.2: Embedding results of the semantics extraction module. The proposed semantics module S(x) focuses on the semantics information. (a, b) The embedding outputs between ID and OOD are easily separable since the ID and OOD datasets have distinct semantic information. (c, d) It fails to disentangle the textural difference cases such as motion blur or image resolution change. This is a natural outcome since S(x) is not guided to follow the textural cues.\n\nOOD benchmark. In the case of the MNIST variant datasets (Figure B.3a), MNIST and K-MNIST show a similar spectrum because they have identical textures but only the semantics (letters) are different as illustrated in Figure A.1. On the other hand, F-MNIST has a different spectrum trend due to the disparate texture (letters vs. cloths). In Figure B.3b, we compare C10 to the C100 and SVHN datasets. Here, although C10 and C100 are separable in the embedding space (as shown in Figure B.2b), which has semantic information, the frequency domain-based approach, in which only texture information is extracted, cannot distinguish them. The SVHN dataset, however, shows disparate PSD, implying that the texture extraction module can identify this effectively.\n\nWe visualize PSD analysis results when the image distortions are applied to the C10 dataset (Figure B.3c-f). In many cases, the C10 dataset (blue) and mild corruptions (orange) appear as aligned spectrum, thus the texture-only mode may struggle in detecting mild corruptions as OOD. However, based on the real-world applications, we assumed that the mild distortion would be treated as ID, in other words, to be the textural robustness (Table 4). In contrast, severe corruptions (green) are distinctive compared to original C10, indicating that texture information is useful for detecting the severe corruptions as OOD.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n(a) ID: MNIST OOD: {K, F}-MNIST\n\n(b) ID: C10 OOD: C100, SVHN\n\n(c) ID: C10 OOD: C10 with frost\n\n(d) ID: C10 OOD: C10 with shot noise\n\n(e) ID: C10 OOD: C10 with haze\n\n(f) ID: C10 OOD: C10 with motion blur\n\nFigure B.3: Power spectrum density (PSD) analysis. We extend the PSD analysis conducted on Figure 4 to all the benchmark datasets we proposed.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nC MODEL COMPARISON\n\nTable C.1 presents the OOD detection performance of various methods when C10 and C100 datasets are set to the ID. We compare with R-Flow (Zisselman & Tamar, 2020), MOOD (Lin et al., 2021), 1-Dim (Zaeemzadeh et al., 2021) and G-ODIN (Hsu et al., 2020) algorithms. In the table, (c) and (r) depict the center-cropped and resized, respectively. To get a standard image resolution (32×32), we downsize high-resolution datasets by described downsizing procedure. Note that the resized datasets are similar to the one presented in this work. We would like to emphasize that we mainly compare the competitors to our method with the λ = 0.5 setting. This makes the comparison fairer because λ = 0.5 does not require prior knowledge of which factor is important. Our method outperforms the previous algorithms in many cases and the proposed framework does not utilize any information other than the given ID dataset (C10 or C100) in the training phase. Unlike ours, the other methods use external information such as class labels or additional datasets to boost the performance.\n\nID → OOD\n\nR-Flow MOOD 1-Dim G-ODIN\n\n0 1\nC\n\n0 0\n1 C\n\nSVHN TinyImgNet LSUN (c) LSUN (r) ImgNet (c) ImgNet (r)\n\nSVHN TinyImgNet LSUN (c) LSUN (r) ImgNet (c) ImgNet (r)\n\n98.2 99.6 -\n99.6 -\n-\n\n95.1 98.1 -\n98.9 -\n-\n\n96.4 -\n99.2 93.2 -\n-\n\n85.8 -\n96.8 77.6 -\n-\n\n- -\n99.4 99.3 98.1 98.5\n\n- -\n93.8 95.7 88.6 93.7\n\n98.8 -\n98.3 99.4 98.7 99.1\n\n95.9 -\n95.3 98.7 97.6 98.6\n\nOurs λ = 0.5\n\n99.9 99.9 90.9 99.4 98.1 98.1\n\n99.9 100. 91.9 99.8 100. 97.7\n\n1.0\n\n99.9 99.9 98.6 99.6 98.5 98.9\n\n100. 100. 92.2 99.6 100. 98.2\n\n0.0\n\n86.1 65.4 68.3 97.0 85.2 85.0\n\n80.9 91.7 65.6 97.0 94.1 94.7\n\nTable C.1: Comparison on the conventional OOD detection benchmark. We compare to other representative methods with various settings: R-Flow (Zisselman & Tamar, 2020), MOOD (Lin et al., 2021), 1-Dim (Zaeemzadeh et al., 2021) and G-ODIN (Hsu et al., 2020). Note that (c) and (r) indicate center-cropped and resized, respectively.\n\n17",
    "reference": "# Summary Of The Paper\n\nThe paper outlines an approach to out of distribution [OOD] example identification. The approach makes use of the idea of explicitly separating high-level semantic content and low-level image frequency content and training separate classifiers whose outputs are combined to make the OOD decision. The semantic content extraction network uses a novel approach to data pre-processing intended to make it easier to identify OOD elements.\n\n# Strength And Weaknesses\n\nThe overall analysis of the problem is interesting and provides a justification for the design choices and dual-pathway classifier. The angular projection of the training data to enhance OOD detection is also an interesting approach to capturing the concept of the training set boundaries.\n\nThe use of rectangular frequency binning, and discarding orientation, as a texture descriptor is very rough. Empirically, it seems to work well enough, but a more sophisticated set of features may improve performance. Does the process make use of an integral image to do the computation of rectangular sums? It's also not clear if the computation is being done on the base DFT or the power spectrum. The text for figure 4 implies it is the power spectrum.\n\nThe acronym SVDD does not seem to be defined in the text.\n\nThe balance of the two systems is evaluated at only lambda = {0, 0.5, and 1}.  It's not clear from the results that equal weighting will necessarily be the best option.\n\nThe new benchmark data set seems to be needed, given the performance on the prior data sets. It seems tailored to demonstrating this particular differentiation of the task (high level versus low level features).  My question is whether it has broader applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe explicit separation of high and low level features seems to be novel and enables a specific architecture. The new benchmark data set is a novel contribution.\n\n# Summary Of The Review\n\nThe conceptual approach leads to a specific architecture that shows good performance on both the existing data sets and a new proposed benchmark data set. The concept of thinking about OOD detection as having separable problem characteristics enables new approaches to the problem.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nn/a"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTHE SURPRISING EFFECTIVENESS OF EQUIVARIANT MODELS IN DOMAINS WITH LATENT SYMMETRY\n\nDian Wang, Jung Yeon Park, Neel Sortur, Lawson L.S. Wong, Robin Walters∗, Robert Platt∗ Northeastern University {wang.dian,park.jungy,sortur.n,l.wong,r.walters,r.platt}@northeastern.edu\n\nABSTRACT\n\nExtensive work has demonstrated that equivariant neural networks can significantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the domain symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the input. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surprisingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model’s performance, imposing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems.\n\n1\n\nINTRODUCTION\n\nRecently, equivariant learning has shown great success in various machine learning domains like trajectory prediction (Walters et al., 2020), robotics (Simeonov et al., 2022), and reinforcement learning (Wang et al., 2022c). Equivariant networks (Cohen & Welling, 2016; 2017) can improve generalization and sample efficiency during learning by encoding task symmetries directly into the model structure. However, this requires problem symmetries to be perfectly known and modeled at design time – something that is sometimes problematic. It is often the case that the designer knows that a latent symmetry is present in the problem but cannot easily express how that symmetry acts in the input space. For example, Figure 1b is a rotation of Figure 1a. However, this is not a rotation of the image – it is a rotation of the objects present in the image when they are viewed from an oblique angle. In order to model this rotational symmetry, the designer must know the viewing angle and somehow transform the data or encode projective geometry into the model. This is difficult and it makes the entire approach less attractive. In this situation, the conventional wisdom would be to discard the model structure altogether since it is not fully known and to use an unconstrained model. Instead, we explore whether it is possible to benefit from equivariant models even when the way a symmetry acts on the problem input is not precisely known. We show empirically that this is indeed the case and that an inaccurate equivariant model is often better than a completely unstructured model. For\n\nFigure 1: Object vs image transforms. Object transform rotates the object itself (b), while image transform rotates the image (c). We propose to use the image transform to help model the object transform.\n\n∗Equal Advising\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nexample, suppose we want to model a function with the object-wise rotation symmetry expressed in Figure 1a and b. Notice that whereas it is difficult to encode the object-wise symmetry, it is easy to encode an image-wise symmetry because it involves simple image rotations. Although the imagewise symmetry model is imprecise in this situation, our experiments indicate that this imprecise model is still a much better choice than a completely unstructured model.\n\nThis paper makes three contributions. First, we define three different relationships between problem symmetry and model symmetry: correct equivariance, incorrect equivariance, and extrinsic equivariance. Correct equivariance means the model correctly models the problem symmeincorrect equivariance is when the model symmetry interferes with the problem symmetry; try; and extrinsic equivariance is when the model symmetry transforms the input data to outof-distribution data. We theoretically demonstrate the upper bound performance for an incorrectly constrained equivariant model. Second, we empirically compare extrinsic and incorrect equivariance in a supervised learning task and show that a model with extrinsic equivariance can improve performance compared with an unconstrained model. Finally, we explore this idea in a reinforcement learning context and show that an extrinsically constrained model can outperform state-of-the-art conventional CNN baselines. Supplementary video and code are available at https://pointw.github.io/extrinsic_page/.\n\n2 RELATED WORK\n\nEquivariant Neural Networks. Equivariant networks are first introduced as G-Convolution (Cohen & Welling, 2016) and Steerable CNN (Cohen & Welling, 2017; Weiler & Cesa, 2019; Cesa et al., 2021). Equivariant learning has been applied to various types of data including images (Weiler & Cesa, 2019), spherical data (Cohen et al., 2018), point clouds (Dym & Maron, 2020), sets Maron et al. (2020), and meshes (De Haan et al., 2020), and has shown great success in tasks including molecular dynamics (Anderson et al., 2019), particle physics (Bogatskiy et al., 2020), fluid dynamics (Wang et al., 2020), trajectory prediction (Walters et al., 2020), robotics (Simeonov et al., 2022; Zhu et al., 2022; Huang et al., 2022) and reinforcement learning (Wang et al., 2021; 2022c). Compared with the prior works that assume the domain symmetry is perfectly known, this work studies the effectiveness of equivariant networks in domains with latent symmetries.\n\nSymmetric Representation Learning. Since latent symmetry is not expressable as a simple transformation of the input, equivariant networks can not be used in the standard way. Thus several works have turned to learning equivariant features which can be easily transformed. Park et al. (2022) learn an encoder which maps inputs to equivariant features which can be used by downstream equivariant layers. Quessard et al. (2020), Klee et al. (2022), and Marchetti et al. (2022) map 2D image inputs to elements of various groups including SO(3), allowing for disentanglement and equivariance constraints. Falorsi et al. (2018) use a homeomorphic VAE to perform the same task in an unsupervised manner. Dangovski et al. (2021) consider equivariant representations learned in a self-supervised manner using losses to encourage sensitivity or insensitivity to various symmetries. Our method may be considered as an example of symmetric representation learning which, unlike any of the above methods, uses an equivariant neural network as an encoder. Zhou et al. (2020) and Dehmamy et al. (2021) assume no prior knowledge of the structure of symmetry in the domain and learn the symmetry transformations on inputs and latent features end-to-end with the task function. In comparison, our work assumes that the latent symmetry is known but how it acts on the input is unknown.\n\nSample Efficient Reinforcement Learning. One traditional solution for improving sample efficiency is to create additional samples using data augmentation (Krizhevsky et al., 2017). Recent works discover that simple image augmentations like random crop (Laskin et al., 2020b; Yarats et al., 2022) or random shift (Yarats et al., 2021) can improve the performance of reinforcement learning. Such image augmentation can be combined with contrastive learning (Oord et al., 2018) to achieve better performance (Laskin et al., 2020a; Zhan et al., 2020). Recently, many prior works have shown that equivariant methods can achieve tremendously high sample efficiency in reinforcement learning (van der Pol et al., 2020; Mondal et al., 2020; Wang et al., 2021; 2022c), and realize on-robot reinforcement learning (Zhu et al., 2022; Wang et al., 2022a). However, recent equivariant reinforcement learning works are limited in fully equivariant domains. This paper extends the prior works by applying equivariant reinforcement learning to tasks with latent symmetries.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n3 BACKGROUND\n\nEquivariant Neural Networks. A function is equivariant if it respects symmetries of its input and output spaces. Specifically, a function f : X → Y is equivariant with respect to a symmetry group G if it commutes with all transformations g ∈ G, f (ρx(g)x) = ρy(g)f (x), where ρx and ρy are the representations of the group G that define how the group element g ∈ G acts on x ∈ X and y ∈ Y , respectively. An equivariant function is a mathematical way of expressing that f is symmetric with respect to G: if we evaluate f for differently transformed versions of the same input, we should obtain transformed versions of the same output.\n\nIn order to use an equivariant model, we generally require the symmetry group G and representation ρx to be known at design time. For example, in a convolutional model, this can be accomplished by tying the kernel weights together so as to satisfy K(gy) = ρout(g)K(y)ρin(g)−1, where ρin and ρout denote the representation of the group operator at the input and the output of the layer (Cohen et al., 2019). End-to-end equivariant models can be constructed by combining equivariant convolutional layers and equivariant activation functions. In order to leverage symmetry in this way, it is common to transform the input so that standard group representations work correctly, e.g., to transform an image to a top-down view so that image rotations correspond to object rotations.\n\nEquivariant SAC. Equivariant SAC (Wang et al., 2022c) is a variation of SAC (Haarnoja et al., 2018) that constrains the actor to an equivariant function and the critic to an invariant function with respect to a group G. The policy is a network π : S → A×Aσ, where Aσ is the space of action standard deviations (SAC models a stochastic policy). It defines the group action on the output space of the policy network network ̄a ∈ A × Aσ as: g ̄a = g(aequiv, ainv, aσ) = (ρequiv(g)aequiv, ainv, aσ), where aequiv ∈ Aequiv is the equivariant component in the action space, ainv ∈ Ainv is the invariant component in the action space, aσ ∈ Aσ, g ∈ G. The actor network π is then defined to be a mapping s (cid:55)→ ̄a that satisfies the following equivariance constraint: π(gs) = g(π(s)) = g ̄a. The critic is a Q-network q : S × A → R that satisfies an invariant constraint: q(gs, ga) = q(s, a).\n\n4 LEARNING SYMMETRY USING OTHER SYMMETRIES\n\n4.1 MODEL SYMMETRY VERSUS TRUE SYMMETRY\n\nThis paper focuses on tasks where the way in which the symmetry group operates on the input space is unknown. In this case the ground truth function f : X → Y is equivariant with respect to a group G which acts on X and Y by ρx and ρy respectively. However, the action ρx on the input space is not known and may not be a simple or explicit map. Since ρx is unknown, we cannot pursue the strategy of learning f using an equivariant model class fφ constrained by ρx. As an alternative, we propose restricting to a model class fφ which satisfies equivariance with respect to a different group action ˆρx, i.e., fφ(ˆρx(g)x) = ρy(g)fφ(x). This paper tests the hypothesis that if the model is constrained to a symmetry class ˆρx which is related to the true symmetry ρx, then it may help learn a model satisfying the true symmetry. For example, if x is an image viewed from an oblique angle and ρx is the rotation of the objects in the image, ˆρx can be the rotation of the whole image (which is different from ρx because of the tilted view angle). Section 4.4 will describe this example in detail.\n\nFigure 2: An example classification task for correct, incorrect, and extrinsic equivariance. The grey ring shows the input distribution. Circles are the training data in the distribution where the color shows the ground truth label. Crosses show the group transformed data.\n\n4.2 CORRECT, INCORRECT, AND EXTRINSIC EQUIVARIANCE\n\nOur findings show that the success of this strategy depends on how ˆρx relates to the ground truth function f and its symmetry. We classify the model symmetry as correct equivariance, incorrect\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nequivariance, or extrinsic equivariance with respect to f . Correct symmetry means that the model symmetry correctly reflects a symmetry present in the ground truth function f . An extrinsic symmetry may still aid learning whereas an incorrect symmetry is necessarily detrimental to learning. We illustrate the distinction with a classification example shown in Figure 2a. (See Appendix B for a more in-depth description.) Let D ⊆ X be the support of the input distribution for f . Definition 4.1. The action ˆρx has correct equivariance with respect to f if ˆρx(g)x ∈ D for all x ∈ D, g ∈ G and f (ˆρx(g)x) = ρy(g)f (x).\n\nThat is, the model symmetry preserves the input space D and f is equivariant with respect to it. For example, consider the action ˆρx of the group G1 = C2 acting on R2 by reflection across the horizontal axis and ρy = 1, the trivial action fixing labels. Figure 2b shows the untransformed data x ∈ D as circles along the unit circle. The transformed data ˆρx(g)x (shown as crosses) also lie on the unit circle, and hence the support D is reflection invariant. Moreover, the ground truth labels f (x) (shown as orange or blue) are preserved by this action. Definition 4.2. The action ˆρx has incorrect equivariance with respect to f if there exist x ∈ D and g ∈ G such that ˆρx(g)x ∈ D but f (ˆρx(g)x) ̸= ρy(g)f (x).\n\nIn this case, the model symmetry partially preserves the input distribution, but does not correctly preserve labels. In Figure 2c, the rotation group G2 = ⟨Rotπ⟩ maps the unit circle to itself, but the transformed data does not have the correct label. Thus, constraining the model fφ by fφ(ˆρx(g)x) = fφ(x) will force fφ to mislabel data. In this example, for a = 2/2, f (a, a) = ORANGE and f (−a, −a) = BLUE, however, fφ(a, a) = fφ(Rotπ(a, a)) = fφ(−a, −a). Definition 4.3. The action ˆρx has extrinsic equivariance with respect to f if for x ∈ D, ˆρx(g)x ̸∈ D.\n\n√\n\nExtrinsic equivariance is when the equivariant constraint in the equivariant network fφ enforces equivariance to out-of-distribution data. Since ˆρx(g)x ̸∈ D, the ground truth f (ˆρx(g)x) is undefined. An example of extrinsic equivariance is given by the scaling group G3 shown in Figure 2d. For the data x ∈ D, enforcing scaling invariance fφ(ˆρx(g)x) = fφ(x) where g ∈ G3 will not increase error, because the group transformed data (in crosses) are out of the distribution D of the input data shown in the grey ring. In fact, we hypothesize that such extrinsic equivariance may even be helpful for the network to learn the ground truth function. For example, in Figure 2d, the network can learn to classify all points on the left as blue and all points on the right as orange.\n\n4.3 THEORETICAL UPPER BOUND ON ACCURACY FOR INCORRECT EQUIVARIANT MODELS\n\nConsider a classification problem over the set X with finitely many classes Y . Let G be a finite group acting on X. Consider a model fφ : X → Y with incorrect equivariance constrained to be invariant to G. In this case the points in a single orbit {gx : g ∈ G} must all be assigned the same label fφ(gx) = y. However these points may have different ground truth labels. We classify how bad this situation is by measuring p(x), the proportion of ground truth labels in the orbit of x which are equal to the majority label. Let cp be the fraction of points x ∈ X which have consensus proportion p(x) = p. Proposition 4.1. The accuracy of fφ has upper bound acc(fφ) ≤ (cid:80)\n\np cpp\n\nSee the complete version of the proposition and its proof in Appendix A. In the example in Figure 2c, we have p ∈ {0.5} and c0.5 = 1, thus acc(fφ) ≤ 0.5. In contrast, an unconstrained model with a universal approximation property and proper hyperparameters can achieve arbitrarily good accuracy.\n\n4.4 OBJECT TRANSFORMATION AND IMAGE TRANSFORMATION\n\nIn tasks with visual inputs (X = Rc×h×w), incorrect or extrinsic equivariance will exist when the transformation of the image does not match the transformation of the latent state of the task. In such case, we call ρx the object transform and ˆρx the image transform. For an image input x ∈ X, the image transform ˆρx(g)x is defined as a simple transformation of pixel locations (e.g., Figure 1a-c where g = π/2 ∈ SO(2)), while the object transform ρx(g)x is an implicit map transforming the objects in the image (e.g., Figure 1a-b where g = π/2 ∈ SO(2)). The distinction between object transform and image transform is often caused by some symmetry-breaking factors such as camera angle, occlusion, backgrounds, and so on (e.g., Figure 1). We refer to such symmetry-breaking factors as symmetry corruptions.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 3: (a) The rotation estimation task requires the network to estimate the relative rotation between the two input states. (b) Different symmetry corruptions in the rotation estimation experiment.\n\n5 EVALUATING EQUIVARIANT NETWORK WITH SYMMETRY CORRUPTIONS\n\nAlthough it is preferable to use an equivariant model to enforce correct equivariance, real-world problems often contain some symmetry corruptions, such as oblique viewing angles, which mean the symmetry is latent. In this experiment, we evaluate the effect of different corruptions on an equivariant model and show that enforcing extrinsic equivariance can actually improve performance. We experiment with a simple supervised learning task where the scene contains three ducks of different colors. The data samples are pairs of images where all ducks in the first image are rotated by some g ∈ C8 to produce the second image within each pair. Given the correct g, the goal is to train a network fφ : R2×4×h×w → R|C8| to classify the rotation (Figure 3a). If we have a perfect top-down image observation, then the object transform and image transform are equal, and we can enforce the correct equivariance by modeling the ground truth function f as an invariant network fφ(ρx(g)x) = fφ(x) where g ∈ SO(2) (because the rotation of the two images will not change the relative rotation between the objects in the two images). To mimic symmetry corruptions in real-world applications, we apply seven different transformations to both pairs of images shown in Figure 3b (more corruptions are considered in Appendix E.1). In particular, for invert-label, the ground truth label g is inverted to −g when the yellow duck is on the left of the orange duck in the world frame in the first input image. Notice that enforcing SO(2)-invariance in fφ under invert-label is an incorrect equivariant constraint because a rotation on the ducks might change their relative position in the world frame and break the invariance of the task: f (gx) ̸= f (x), ∃g ∈ SO(2). However, in all other corruptions, enforcing SO(2)-invariance is an extrinsic equivariance because gx will be out of the input distribution. We evaluate the equivariant network defined in group C8 implemented using e2cnn (Weiler & Cesa, 2019). See Appendix D.1 for the training details.\n\nComparing Equivariant Networks with CNNs. We first compare the performance of an equivariant network (Equi) and a conventional CNN model (CNN) with a similar number of trainable parameters. The network architectures are relatively simple (see Appendix C.1) as our goal is to evaluate the performance difference between an equivariant network and an unconstrained CNN model rather than achieving the best performance in this task. In both models, we apply a random crop after sampling each data batch to improve the sample efficiency. See Appendix E.1 for the effects of random crop augmentation on learning. Figure 4 (blue vs green) shows the test accuracy of both models after convergence when trained with varying dataset sizes. For all corruptions with extrinsic equivariance constraints, the equivariant network performs better than the CNN model, especially in low data regimes. However, for invert-label which gives an incorrect equivariance constraint, the CNN outperforms the equivariant model, demonstrating that enforcing incorrect equivariance negatively impacts accuracy. In fact, based on Proposition 4.1, the equivariant network here has a theoretical upper bound performance of 62.5%. First, p ∈ {1, 0.5}. Then p = 1 when f (x) ∈ {0, π} ⊆ C8 where f (x) = −f (x) (i.e., negating the label won’t change it), and c1 = 2/8 = 0.25. The consensus proportion p = 0.5 when f (x) ∈ {π/4, π/2, 3π/4, 5π/4, 3π/2, 7π/4} ⊆ C8, where half of the\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Comparison of an equivariant network (blue), a conventional network (green), and CNN equipped with image transformation augmentation using C8 rotations (red). The plots show the prediction accuracy in the test set of the model trained with different number of training data. In all of our experiments, we take the average over four random seeds. Shading denotes standard error.\n\nlabels in the orbit of x will be the negation of the labels of the other half (because half of g ∈ C8 will change the relative position between the yellow and orange duck), thus c0.5 = 6/8 = 0.75. acc(fφ) ≤ 1 × 0.25 + 0.5 × 0.75 = 0.625. This theoretical upper bound matches the result in Figure 4. Figure 4 suggests that even in the presence of symmetry corruptions, enforcing extrinsic equivariance can improve the sample efficiency while incorrect equivariance is detrimental.\n\nExtrinsic Image Augmentation Helps in Learning Correct Symmetry. In these experiments, we further illustrate that enforcing extrinsic equivariance helps the model learn the latent equivariance of the task for in-distribution data. As an alternative to equivariant networks, we consider an older alternative for symmetry learning, data augmentation, to see whether extrinsic symmetry augmentations can improve the performance of an unconstrained CNN by helping it learn latent symmetry. Specifically, we augment each training sample with C8 image rotations while keeping the validation and test set unchanged. As is shown in Figure 4, adding such extrinsic data augmentation (CNN + Img Trans, red) significantly improves the performance of CNN (green), and nearly matches the performance of the equivariant network (blue). Notice that in invert-label, adding such augmentation hurts the performance of CNN because of incorrect equivariance.\n\n6 EXTRINSIC EQUIVARIANCE IN REINFORCEMENT LEARNING\n\nThe results in Section 5 suggest that enforcing extrinsic equivariance can help the model better learn the latent symmetry in the task. In this section, we apply this methodology in reinforcement learning and demonstrate that extrinsic equivariance can significantly improve sample efficiency.\n\n6.1 REINFORCEMENT LEARNING IN ROBOTIC MANIPULATION\n\nWe first experiment in five robotic manipulation environments shown in Figure 6. The state space S = R4×h×w is a 4-channel RGBD image captured from a fixed camera pointed at the workspace (Figure 5). The action space A = R5 is the change in gripper pose (x, y, z, θ), where θ is the rotation along the z-axis, and the gripper open width λ. The task has latent O(2) symmetry: when a rotation or reflection is applied to the poses of the gripper and the objects, the action should rotate and reflect accordingly. However, such symmetry does not exist in image space because the image perspective is skewed instead of top-down (we also perform experiments with another symmetry corruption caused by sensor occlusion in\n\n6\n\nFigure 5: The image state in the Block Picking task. Left image shows the RGB channels and right image shows the depth channel.\n\nPublished as a conference paper at ICLR 2023\n\n(a) Block Pulling\n\n(b) Block Pushing\n\n(c) Block Picking\n\n(d) Drawer Opening\n\n(e) Block in Bowl\n\nFigure 6: The manipulation environments from BulletArm benchmark Wang et al. (2022b) implemented in PyBullet Coumans & Bai (2016). The top-left shows the goal for each task.\n\nFigure 7: Comparison of Equivariant SAC (blue) with baselines. The plots show the performance of the evaluation policy. The evaluation is performed every 200 training steps.\n\nAppendix E.3). We enforce such extrinsic symmetry (group D4) using Equivariant SAC (Wang et al., 2022c;a) equipped with random crop augmentation using RAD (Laskin et al., 2020b) (Equi SAC + RAD) and compare it with the following baselines: 1) CNN SAC + RAD: same as our method but with an unconstrained CNN instead of an equivariant model; 2) CNN SAC + DrQ: same as 1), but with DrQ (Yarats et al., 2021) for the random crop augmentation; 3) FERM (Zhan et al., 2020): a combination of 1) and contrastive learning; and 4) SEN + RAD: Symmetric Embedding Network (Park et al., 2022) that uses a conventional network for the encoder and an equivariant network for the output head. All baselines are implemented such that they have a similar number of parameters as Equivariant SAC. See Appendix C.2 for the network architectures and Appendix F for the architecture hyperparameter search for the baselines. All methods use Prioritized Experience Replay (PER) (Schaul et al., 2015) with pre-loaded expert demonstrations (20 episodes for Block Pulling and Block Pushing, 50 for Block Picking and Drawer Opening, and 100 for Block in Bowl). We also add an L2 loss towards the expert action in the actor to encourage expert actions. More details about training are provided in Appendix D.2.\n\nFigure 7 shows that Equivariant SAC (blue) outperforms all baselines. Note that the performance of Equivariant SAC in Figure 7 does not match that reported in Wang et al. (2022c) because we have a harder task setting: we do not have a top-down observation centered at the gripper position as in the prior work. Such top-down observations would not only provide correct equivariance but also help learn a translation-invariant policy. Even in the harder task setting without top-down observations, Figure 7 suggests that Equivariant SAC can still achieve higher performance compared to baselines.\n\n6.2\n\nINCREASING CORRUPTION LEVELS\n\nIn this experiment, we vary the camera angle by tilting to see how increasing the gap between the image transform and the object transform affects the performance of extrinsically equivariant networks. When the view angle is at 90 degrees (i.e., the image is top-down), the object and image transformation exactly match. As the view angle is decreased, the gap increases. Figure 8 shows the observation at 90 and 15 degree view angles. We remove the robot arm except for the gripper and the blue/white grid on the ground to remove the other symmetry-breaking components in the environment so that the camera angle\n\n7\n\nFigure 8: Left: view angle at 90 degrees. Right: view angle at 15 degrees.\n\nPublished as a conference paper at ICLR 2023\n\nFigure 9: Comparison between Equivariant SAC (blue) and CNN SAC (green) as the view angle decreases. The plots show the evaluation performance of Equivariant SAC and CNN SAC at the end of training in different view angles.\n\nFigure 11: Comparison between Equivariant SAC (blue) and CNN SAC (green) in an environment that will make Equivariant SAC encode incorrect equivariance. The plots show the performance of the evaluation policy. The evaluation is performed every 200 training steps.\n\nis the only symmetry corruption. We compare Equi SAC + RAD against CNN SAC + RAD. We evaluate the performance of each method at the end of training for different view angles in Figure 9. As expected, the performance of Equivariant SAC decreases as the camera angle is decreased, especially from 30 degrees to 15 degrees. On the other hand, CNN generally has similar performance for all view angles, with the exception of Block Pulling and Block Pushing, where decreasing the view angle leads to higher performance. This may be because decreasing the view angle helps the network to better understand the height of the gripper, which is useful for pulling and pushing actions.\n\n6.3 EXAMPLE OF INCORRECT EQUIVARIANCE\n\nWe demonstrate an example where incorrect equivariance can harm the performance of Equivariant SAC compared to an unconstrained model. We modify the environments so that the image state will be reflected across the vertical axis with 50% probability and then also reflected across the horizontal axis with 50% probability (see Figure 10). As these random reflections are contained in D4, the transformed state reflect(s), s ∈ S is affected by Equivariant SAC’s symmetry constraint. In particular, as the actor produces a transformed action for reflect when the optimal action should actually be invariant, the extrinsic equivariance constraint now becomes an incorrect equivariance for these reflected states. As shown in Figure 11, Equivariant SAC can barely learn under random reflections, while CNN can still learn a useful policy.\n\nFigure 10: The environment conducts a random reflection on the state image at every step. The four images show the four possible reflections, each has 25% probability.\n\n6.4 REINFORCEMENT LEARNING IN DEEPMIND CONTROL SUITE\n\nWe further apply extrinsically equivariant networks to continuous control tasks in the DeepMind Control Suite (DMC) (Tunyasuvunakool et al., 2020). We use a subset of the domains in DMC that have clear object-level symmetry and use the D1 group for cartpole, cup catch, pendulum, acrobot domains, and D2 for reacher domains. This leads to a total of 7 tasks, with 4 easy and 3 medium\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 12: Comparison between Equivariant DrQv2 and Non-equivariant DrQv2 on easy tasks (top) and medium tasks (bottom). The evaluation is performed every 10000 environment steps.\n\nlevel tasks as defined in (Yarats et al., 2022). Note that all of these domains are not fully equivariant as they include a checkered grid for the floor and random stars as the background.\n\nWe use DrQv2 Yarats et al. (2022), a SOTA model-free RL algorithm for image-based control, as our base RL algorithm. We create an equivariant version of DrQv2, with an equivariant actor and invariant critic with respect to the environment’s symmetry group. We follow closely the architecture and training hyperparameters used in the original paper except in the image encoder, where two max-pooling layers are added to further reduce the representation dimension for faster training. Furthermore, DrQv2 uses convolution layers in the image encoder and then flattens its output to feed it into linear layers in the actor and the critic. In order to preserve this design choice for the equivariant model, we do not reduce the spatial dimensions to 1 × 1 by downsampling/pooling or stride as commonly done in practice. Rather we flatten the image using a process we term action restriction since the symmetry group is restricted from Z2 ⋉ Dk to Dk. Let I ∈ Rh×w×c denote the image feature where Dk acts on both the spatial domain and channels. Then we add a new axis corresponding to Dk by ̃I = (gI)g∈Dk ∈ Rh×w×c×2k. We then flatten to ̄I = (gI)g∈Dk ∈ R1×1×hwc×2k. The intermediate step ̃I is necessary to encode both the spatial and channel actions into a single axis which ensures the action restriction is Dk-equivariant. We now map back down to the original dimension with a Dk-equivariant 1 × 1 convolution. To the best of our knowledge, this is the first equivariant version of DrQv2.\n\nWe compare the equivariant vs. the non-equivariant (original) DrQv2 algorithm to evaluate whether extrinsic equivariance can still improve training in the original domains (with symmetry corruptions). In figures 12, equivariant DrQv2 consistently learns faster than the non-equivariant version on all tasks, where the performance improvement is largest on the more difficult medium tasks. In pendulum swingup, both methods have 1 failed run each, leading to a large standard error, see Figure 27 in Appendix E.4 for a plot of all runs. These results highlight that even with some symmetry corruptions, equivariant policies can outperform non-equivariant ones. See Appendix E.4.1 for an additional experiment where we vary the level of symmetry corruptions as in Section 6.2.\n\n7 DISCUSSION\n\nThis paper defines correct equivariance, incorrect equivariance, and extrinsic equivariance, and identifies that enforcing extrinsic equivariance does not necessarily increase error. This paper further demonstrates experimentally that extrinsic equivariance can provide significant performance improvements in reinforcement learning. A limitation of this work is that we mainly experiment in reinforcement learning and a simple supervised setting but not in other domains where equivariant learning is widely used. The experimental results of our work suggest that an extrinsic equivariance should also be beneficial in those domains, but we leave this demonstration to future work. Another limitation is that we focus on planar equivariant networks. In future work, we are interested in evaluating extrinsic equivariance in network architectures that process different types of data.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work is supported in part by NSF 1724257, NSF 1724191, NSF 1763878, NSF 1750649, NSF 2107256, and NASA 80NSSC19K1474. R. Walters is supported by the Roux Institute and the Harold Alfond Foundation and NSF 2134178.\n\nETHIC STATEMENT\n\nEquivariant models allow us to train robots faster and more accurately in many different tasks. Our work shows this advantage can be applied even more broadly to tasks in real-world conditions. Our method is agnostic to the morality of the actions which robots are trained for and, in that sense, can make it easier for robots to be used for either societally beneficial or detrimental tasks.\n\nREFERENCES\n\nBrandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural\n\nnetworks. Advances in neural information processing systems, 32, 2019.\n\nAlexander Bogatskiy, Brandon Anderson, Jan Offermann, Marwah Roussi, David Miller, and Risi Kondor. Lorentz group equivariant neural network for particle physics. In International Conference on Machine Learning, pp. 992–1002. PMLR, 2020.\n\nGabriele Cesa, Leon Lang, and Maurice Weiler. A program to build e (n)-equivariant steerable cnns.\n\nIn International Conference on Learning Representations, 2021.\n\nTaco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-\n\nence on machine learning, pp. 2990–2999. PMLR, 2016.\n\nTaco S. Cohen and Max Welling. Steerable CNNs. In International Conference on Learning Rep-\n\nresentations, 2017. URL https://openreview.net/forum?id=rJQKYt5ll.\n\nTaco S Cohen, Mario Geiger, Jonas K ̈ohler, and Max Welling. Spherical cnns. In International\n\nConference on Learning Representations, 2018.\n\nTaco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on homo-\n\ngeneous spaces. Advances in neural information processing systems, 32, 2019.\n\nErwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,\n\nrobotics and machine learning. GitHub repository, 2016.\n\nRumen Dangovski, Li Jing, Charlotte Loh, Seungwook Han, Akash Srivastava, Brian Cheung, Pulkit Agrawal, and Marin Soljacic. Equivariant self-supervised learning: Encouraging equivariance in representations. In International Conference on Learning Representations, 2021.\n\nPim De Haan, Maurice Weiler, Taco Cohen, and Max Welling. Gauge equivariant mesh cnns: Anisotropic convolutions on geometric graphs. In International Conference on Learning Representations, 2020.\n\nNima Dehmamy, Robin Walters, Yanchen Liu, Dashun Wang, and Rose Yu. Automatic symmetry discovery with lie algebra convolutional network. Advances in Neural Information Processing Systems, 34:2503–2515, 2021.\n\nNadav Dym and Haggai Maron. On the universality of rotation equivariant point cloud networks. In\n\nInternational Conference on Learning Representations, 2020.\n\nLuca Falorsi, Pim De Haan, Tim R Davidson, Nicola De Cao, Maurice Weiler, Patrick Forr ́e, and Taco S Cohen. Explorations in homeomorphic variational auto-encoding. arXiv preprint arXiv:1807.04689, 2018.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861–1870. PMLR, 2018.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nHaojie Huang, Dian Wang, Robin Walters, and Robert Platt. Equivariant transporter network. In\n\nRobotics: Science and Systems, 2022.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nDavid Klee, Ondrej Biza, Robert Platt, and Robin Walters. I2i: Image to icosahedral projection for\n\nSO(3) object reasoning from single-view images. arXiv preprint arXiv:2207.08925, 2022.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\n\nlutional neural networks. Communications of the ACM, 60(6):84–90, 2017.\n\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pp. 5639– 5650. PMLR, 2020a.\n\nMisha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. Advances in neural information processing systems, 33: 19884–19895, 2020b.\n\nGiovanni Luca Marchetti, Gustaf Tegn ́er, Anastasiia Varava, and Danica Kragic. Equivariant repre-\n\nsentation learning via class-pose decomposition. arXiv preprint arXiv:2207.03116, 2022.\n\nHaggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements.\n\nIn International Conference on Machine Learning, pp. 6734–6744. PMLR, 2020.\n\nMirgahney Mohamed, Gabriele Cesa, Taco S Cohen, and Max Welling. A data and compute efficient\n\ndesign for limited-resources deep learning. arXiv preprint arXiv:2004.09691, 2020.\n\nArnab Kumar Mondal, Pratheeksha Nair, and Kaleem Siddiqi. Group equivariant deep reinforce-\n\nment learning. arXiv preprint arXiv:2007.03437, 2020.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nJung Yeon Park, Ondrej Biza, Linfeng Zhao, Jan-Willem Van De Meent, and Robin Walters. Learning symmetric embeddings for equivariant world models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 17372–17389. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/ v162/park22a.html.\n\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop, 2017.\n\nRobin Quessard, Thomas Barrett, and William Clements. Learning disentangled representations and group structure of dynamical environments. Advances in Neural Information Processing Systems, 33:19727–19737, 2020.\n\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv\n\npreprint arXiv:1511.05952, 2015.\n\nAnthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, and Vincent Sitzmann. Neural descriptor fields: Se (3)-equivariant object repIn 2022 International Conference on Robotics and Automation resentations for manipulation. (ICRA), pp. 6394–6400. IEEE, 2022.\n\nSaran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, dm control: Software and Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. tasks for continuous control. doi: https://doi.org/10.1016/j.simpa.2020.100022. URL https://www.sciencedirect.com/ science/article/pii/S2665963820300099.\n\nSoftware Impacts, 6:100022, 2020.\n\nISSN 2665-9638.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nElise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max Welling. Mdp homomorphic networks: Group symmetries in reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020.\n\nRobin Walters, Jinxi Li, and Rose Yu. Trajectory prediction using equivariant continuous convolu-\n\ntion. arXiv preprint arXiv:2010.11344, 2020.\n\nDian Wang, Robin Walters, Xupeng Zhu, and Robert Platt. Equivariant Q learning in spatial action spaces. In 5th Annual Conference on Robot Learning, 2021. URL https://openreview. net/forum?id=IScz42A3iCI.\n\nDian Wang, Mingxi Jia, Xupeng Zhu, Robin Walters, and Robert Platt. On-robot learning with In 6th Annual Conference on Robot Learning, 2022a. URL https://\n\nequivariant models. openreview.net/forum?id=K8W6ObPZQyh.\n\nDian Wang, Colin Kohler, Xupeng Zhu, Mingxi Jia, and Robert Platt. Bulletarm: An open-source robotic manipulation benchmark and learning framework. arXiv preprint arXiv:2205.14292, 2022b.\n\nDian Wang, Robin Walters, and Robert Platt. SO(2)-equivariant reinforcement learning. In International Conference on Learning Representations, 2022c. URL https://openreview.net/ forum?id=7F9cOhdvfk_.\n\nRui Wang, Robin Walters, and Rose Yu. Incorporating symmetry into deep dynamics models for\n\nimproved generalization. arXiv preprint arXiv:2002.03061, 2020.\n\nMaurice Weiler and Gabriele Cesa. General e (2)-equivariant steerable cnns. Advances in Neural\n\nInformation Processing Systems, 32, 2019.\n\nDenis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=GY6-6sTvGaf.\n\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=_SJ-_yyes8.\n\nAlbert Zhan, Philip Zhao, Lerrel Pinto, Pieter Abbeel, and Michael Laskin. A framework for effi-\n\ncient robotic manipulation. arXiv preprint arXiv:2012.07975, 2020.\n\nAllan Zhou, Tom Knowles, and Chelsea Finn. Meta-learning symmetries by reparameterization.\n\narXiv preprint arXiv:2007.02933, 2020.\n\nXupeng Zhu, Dian Wang, Ondrej Biza, Guanang Su, Robin Walters, and Robert Platt. Sample\n\nefficient grasp learning using equivariant models. In Robotics: Science and Systems, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA THEORETICAL UPPER BOUND ON ACCURACY FOR MODELS WITH\n\nINCORRECT SYMMETRY\n\nWe consider a classification problem over the set X with finitely many classes Y . Let m = |Y | be the number of classes. Let l : X → Y be the true labels. Let G be a finite group acting on X. We assume the action of G on X is density preserving. That is, if pX is the density function corresponding to the input domain, then pX (gx) = pX (x). Denote the orbit of a point x ∈ X by Gx = {gx : g ∈ G} and the stabilizer by Gx = {g ∈ G : gx = x}. By the orbit-stabilizer theorem |G| = |Gx||Gx|.\n\nNow consider a model f : X → Y with incorrect equivariance constrained to be invariant to G. We partition the input set into subsets X = (cid:96)m k=1 Xk where\n\nXk = {x ∈ X : |l(Gx)| = k}.\n\nIf f has correct equivariance then X = X1. Incorrect equivariance implies that there are orbits Gx which are assigned more than one label. Since f is constrained to be equivariant such orbits will necessarily result in some errors. We give an upper bound on that error. Define ck = P(x ∈ Xk). Note that since Xk give a partition, (cid:80)m k=1 ck = 1. Also, Xk is empty for k > |G| since the number of labels assigned to an orbit is also upper bounded by the number of points in the orbit which is at most |G|. Letting K = min(|Y |, |G|), we have X = (cid:96)K Proposition A.1. The accuracy of f has upper bound acc(f ) ≤ 1 − (cid:80)K\n\nk=1 Xk.\n\nk=1 ck(k − 1)/|G|.\n\nIn contrast, we can choose an unconstrained model from a model class with a universal approximation property and given properly chosen hyperparameters find a model with arbitrarily good accuracy.\n\nProof. Let y = l(x). Then acc(f ) = Ex∈X [δ(f (x) = y)]. Since the action of G is density preserving, applying an element of G before sampling does not affect the expectation, Ex∈X [δ(f (x) = y)] = Ex∈X [δ(f (gx) = y)] and so\n\nacc(f ) =\n\n1 |G|\n\n(cid:88)\n\ng∈G\n\nEx∈X [δ(f (gx) = y)].\n\nIf we split the expectation over the partition X = (cid:96)K\n\nk=1 Xk we get\n\nInterchanging sums gives\n\n1 |G|\n\n(cid:88)\n\nK (cid:88)\n\ng∈G\n\nk=1\n\nckEx∈Xk [δ(f (gx) = y)].\n\n Ex∈Xk\n\n\n\n\n\nck\n\nK (cid:88)\n\nk=1\n\n1 |G|\n\n(cid:88)\n\ng∈G\n\n\n\n\n\nδ(f (gx) = y)\n\n\n\n .\n\nBy the orbit-stabilizer theorem,\n\n1 |G|\n\n(cid:88)\n\ng∈G\n\nδ(f (gx) = y) =\n\n|Gx| |G|\n\n(cid:88)\n\nx′∈Gx\n\nδ(f (x′) = y) =\n\n|1| |Gx|\n\n(cid:88)\n\nx′∈Gx\n\nδ(f (x′) = y)\n\nwhich is the average accuracy over the orbit Gx. Since f is constrained to a single value of the orbit, and k different true labels appear, the highest accuracy attainable is when |G| = |Gx| and the true labels are maximally unequally distributed such that 1 point in the orbit takes each of k − 1 labels and all the other |G| − (k − 1) points receive a single label. In this case accuracy can be maximized by choosing f (x′) to be this majority label, and\n\n|1| |Gx|\n\n(cid:88)\n\nx′∈Gx\n\nδ(f (x′) = y) ≤ 1 −\n\nk − 1 |G|\n\n.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nFigure 13: Demonstration of the upper bound of an equivariant model under invert label corruption in our supervised learning experiment. The number on each partition shows the ground truth label.\n\nSubstituting back in,\n\nacc(f ) ≤\n\nK (cid:88)\n\nk=1\n\n(cid:18)\n\nck\n\nEx∈Xk\n\n(cid:20)\n\n1 −\n\n(cid:21)(cid:19)\n\nk − 1 |G|\n\n= 1 −\n\nK (cid:88)\n\nk=1\n\n(cid:19)\n\n(cid:18) k − 1 |G|\n\nck\n\nsince k−1\n\n|G| is constant over Xk and (cid:80)K\n\nk=1 ck = 1.\n\nNote that the assumption that |G| = |Gx| and that the labels on a given orbit are maximally unequally distributed need not hold in general and thus this bound is not tight. In order to produce a tight upper bound, consider a partition X = (cid:96) p Xp where Xp = {x ∈ X : (maxy|f −1(y) ∩ Gx|)/|Gx| = p} and define cp = P(x ∈ Xp). The set Xp contains points in orbits where the majority label covers a fraction p of the points. Note that although p is a fraction between 0 and 1, there are only finitely many possible values of p since the numerator and denominator and bounded natural numbers. We may thus sum over the values of p. Proposition A.2. The accuracy of f has upper bound acc(f ) ≤ (cid:80)\n\np cpp.\n\nProof. The proof is similar to the proof of Proposition A.1 replace Xk and ck with Xp and cp respectively. For x ∈ Xp, the term |1| x′∈Gx δ(f (x′) = y) can be upper bounded by choosing the majority label yielding |1|\n\nx′∈Gx δ(f (x′) = y) ≤ p. The bound then follows as before.\n\n|Gx|\n\n(cid:80)\n\n(cid:80)\n\n|Gx|\n\nThis is a tight upper bound since assigning any but the majority label would result in lower accuracy.\n\nFigure 13 demonstrates the upper bound of an incorrectly constrained equivariant network with the invert label corruption in Section 5, where acc(f ) ≤ 0.25 × 1 + 0.75 × 0.5 = 0.625.\n\nB CORRECT, INCORRECT, AND EXTRINSIC EQUIVARIANCE EXAMPLES\n\nIn this section, we describe how the model symmetry transforms data under correct, incorrect, and extrinsic equivariance and how such transformations relate to the true symmetry present in the task using the example of Section 4.2. The ground truth function f : X → Y is a mapping from X = R2 to Y = {ORANGE, BLUE}. Let (a, a), (−a, −a), (−a, a), (b, c) be the coordinates of four points in the data distribution on the unit circle (Figure 14a). The ground truth labels for these points are: f (a, a) = ORANGE, f (−a, −a) = BLUE, f (−a, a) = BLUE, f (b, c) = ORANGE.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nFigure 14: An example classification task for correct, incorrect, and extrinsic equivariance. The input distribution is shown as a gray ring. The training data samples are shown as circles, where the color is the ground truth label. Crosses represent the group transformed data. The opaque points highlight the example points while other points are semitransparent.\n\nB.1 CORRECT EQUIVARIANCE\n\nDefinition 4.1. The action ˆρx has correct equivariance with respect to f if ˆρx(g)x ∈ D for all x ∈ D, g ∈ G and f (ˆρx(g)x) = ρy(g)f (x).\n\n0 −1\n\nConsider the reflection group G = C2 = {1, r} (where r is the reflection along the horizontal (cid:1) and Y via ρy = Id, the trivial action fixaxis) acting on X by ˆρx(1) = Id or ˆρx(r) = (cid:0) 1 0 If we define an equivariant model with respect to ˆρx and ρy, then ing the labels (Figure 14b). the model’s symmetry preserves the problem symmetry. For example, consider the point (−a, a), r ∈ G1 is the reflection so that ˆρx(r) = (cid:0) 1 0 (cid:1) and ˆρx(r)(−a, a) = (−a, −a). Since the 0 −1 model fφ is G1-equivariant, fφ(ˆρx(r)x) = ρy(r)fφ(x). Substituting ρy = Id and x = (−a, a), we obtain fφ(−a, −a) = fφ(−a, a), meaning that the output of fφ(−a, −a) and fφ(−a, a) are constrained to be equal. Thus the invariance property in the ground truth function f where f (−a, −a) = f (−a, a) = BLUE is preserved (notice that this applies to all x ∈ X). We call this correct equivariance.\n\nB.2\n\nINCORRECT EQUIVARIANCE\n\nDefinition 4.2. The action ˆρx has incorrect equivariance with respect to f if there exist x ∈ D and g ∈ G such that ˆρx(g)x ∈ D but f (ˆρx(g)x) ̸= ρy(g)f (x).\n\nConsider the rotation group G2 = ⟨Rotπ⟩ (Figure 14c) which acts via ˆρx on X via a rotation matrix of π and acts on Y via ρy = Id. If we define an equivariant model with respect to ˆρx and ρy, the network’s symmetry will conflict with the problem’s symmetry. For example, consider the point (a, a) and let g ∈ G2 be the rotation action so that ˆρx(g) = (cid:0) −1 0 (cid:1) and ˆρx(g)(a, a) = (−a, −a). As the model fφ is G2-equivariant, fφ(ˆρx(g)x) = ρy(g)fφ(x). Substituting ρy = Id and x = (a, a), we get fφ(−a, −a) = fφ(a, a). However, this constraint interferes with the ground truth function f as f (−a, −a) = BLUE and f (a, a) = ORANGE. We call this incorrect equivariance.\n\n0 −1\n\nB.3 EXTRINSIC EQUIVARIANCE\n\nDefinition 4.3. The action ˆρx has extrinsic equivariance with respect to f if for x ∈ D, ˆρx(g)x ̸∈ D.\n\nConsider the scaling group G3 acting on X by scaling the vector and on Y via ρy = Id (Figure 14d). If we define an equivariant model with respect to ˆρx and ρy, the group-transformed data will be\n\n15\n\nPublished as a conference paper at ICLR 2023\n\noutside the input distribution. Consider the point (b, c) and let g ∈ G3 be the scaling action so that ˆρx(g)(b, c) = (b′, c′). Since the model fφ is G3-equivariant, fφ(ˆρx(g)x) = ρy(g)fφ(x). Substituting ρy = Id and x = (b, c) we have fφ(b′, c′) = fφ(b, c) meaning that the output of fφ(b′, c′) and fφ(b, c) are constrained to be equal. However, (b′, c′) is outside of the input distribution (gray ring) and thus the ground truth f (b′, c′) is undefined. We call this extrinsic equivariance.\n\nIntuitively, it is easy to see in this example how extrinsic equivariance would help the model learn f . If the model fφ is equivariant to the scale group G3, then it can generalize to “scaled” up or down versions of the input distribution and “covers” more of the input space R2. As such, the model may learn the decision boundary (the vertical axis) more easily because of its equivariance compared to a non-equivariant model, even if the equivariance is extrinsic.\n\nC NETWORK ARCHITECTURE\n\nFigure 15: Network architecture of the equivariant network in the supervised learning experiment.\n\nFigure 16: Network architecture of the CNN network in the supervised learning experiment.\n\nTable 1: Number of trainable parameters of the equivariant network (Equi) and conventional CNN network (CNN) in the supervised learning task.\n\nNetwork\n\nEqui\n\nCNN\n\nNumber of Parameters\n\n1.11 million\n\n1.28 million\n\nC.1 SUPERVISED LEARNING\n\nFigure 15 shows the network architecture of the equivariant network and Figure 16 shows the network architecture of the CNN network in Section 5. Both networks are 8-layer convolutional neural networks. The equivariant network is implemented using the e2cnn (Weiler & Cesa, 2019) library, where the hidden layers are defined using the regular representation and the output layer is defined using the trivial representation. Table 1 shows the numbers of trainable parameters in both networks, where both networks have a similar number with a slight advantage in the CNN.\n\nC.2 REINFORCEMENT LEARNING IN ROBOTIC MANIPULATION\n\nFigure 17 shows the network architecture of Equivariant SAC used in manipulation tasks in Section 6.1. All hidden layers are implemented using the regular representation. For the actor (top), the output is a mixed representation containing one standard representation for the (x, y) actions, one signed representation for the θ action, and seven trivial representations for the (z, λ) actions and the standard deviations of all action components. Figure 18 shows the network architecture of CNN SAC for both RAD and DrQ. Figure 19 shows the network architecture of FERM. Figure 20 shows the network architecture of SEN.\n\nTable 2 shows the number of trainable parameters for each model. All baselines have slightly more parameters compared with Equivariant SAC.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 17: Network architecture of Equivariant SAC in robotic manipulation tasks.\n\nFigure 18: Network architecture of CNN SAC in robotic manipulation tasks.\n\nD TRAINING DETAILS\n\nD.1 SUPERVISED LEARNING\n\nWe implement the environment in the PyBullet simulator (Coumans & Bai, 2016). The ducks are located in a workspace with a size of 0.3m × 0.3m. The pixel size of the image is 152 × 152 (and will be cropped to 128 × 128 during training). We implement the training in PyTorch (Paszke et al., 2017) using a cross-entropy loss. The output of the model is the score for each g ∈ C8. We use the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 10−4. The batch size is 64. In all training, we perform a three-way data split with N training data, 200 holdout validation data, and 200 holdout test data. The training is terminated either when the validation prediction success rate does not improve for 100 epochs or when the maximum epoch (1000) is reached.\n\nD.2 REINFORCEMENT LEARNING IN ROBOTIC MANIPULATION\n\nWe use the environments provided by the BulletArm benchmark (Wang et al., 2022b) implemented in the PyBullet simulator (Coumans & Bai, 2016). The workspace’s size is 0.4m × 0.4m × 0.24m. The pixel size of the image observation is 152 × 152 (and will be cropped to 128 × 128 during training). The action space is Ax, Ay, Az = [−0.05m, 0.05m] for the change of (x, y, z) position of the gripper; Aθ = [− π 4 ] for the change of top-down rotation of the gripper; and Aλ = [0, 1] for the open width of the gripper where 0 means fully close and 1 means fully open. All environ-\n\n4 , π\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 19: Network architecture of FERM in robotic manipulation tasks.\n\nFigure 20: Network architecture of SEN in robotic manipulation tasks.\n\nments have a sparse reward: +1 for reaching the goal and 0 otherwise. During training, we use 5 parallel environments where a training step is performed after all 5 parallel environments perform an action step. The evaluation is performed every 200 training steps. We implement the training in PyTorch (Paszke et al., 2017). We use the Adam optimizer (Kingma & Ba, 2014) with a learning rate of 10−3. The batch size is 128. The entropy temperature for SAC is initialized at 10−2. The target entropy is −5. The discount factor γ = 0.99. The Prioritized Experience Replay (PER) (Schaul et al., 2015) has a capacity of 100,000 transitions with prioritized replay exponent of α = 0.6 and prioritized importance sampling exponent β0 = 0.4 as in Schaul et al. (2015). The expert transitions are given a priority bonus of εd = 1.\n\nThe contrastive encoder of the FERM baseline has an encoding size of 50 as in Zhan et al. (2020). The FERM baseline’s contrastive encoder is pre-trained for 1.6k steps using the expert data as in Zhan et al. (2020). In DrQ, the number of augmentations for calculating the target K and the number of augmentations for calculating the loss M are both 2 as in Yarats et al. (2021).\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Number of trainable parameters of Equivariant SAC, CNN SAC, FERM, and SEN in the reinforcement learning task in robotic manipulation. Notice that FERM has a shared encoder between the actor and the critic so the total number of parameters is smaller than the sum of the actor parameter and the critic parameter.\n\nNetwork\n\nEqui SAC\n\nCNN SAC\n\nFERM\n\nSEN\n\nNumber of Actor Parameters\n\n1.11 million\n\n1.13 million\n\n1.79 million\n\n1.22 million\n\nNumber of Critic Parameters\n\n1.18 million\n\n1.27 million\n\n1.90 million\n\n1.24 million\n\nNumber of Total Parameters\n\n2.29 million\n\n2.40 million\n\n2.34 million\n\n2.46 million\n\nD.3 REINFORCEMENT LEARNING IN DEEPMIND CONTROL SUITE\n\nSample images of each environment are shown in Figure 21. Environment observations are 3 consecutive frames of RGB images of size 85 × 85, in order to infer velocity and acceleration. Note that we use odd-sized image sizes instead of 84 × 84 used in Yarats et al. (2022), as the DrQv2 architecture contains a convolutional layer with stride 2 and this breaks equivariance for even-sized spatial inputs (Mohamed et al., 2020). For each environment, an episode lasts 1000 steps where each step has a reward between 0 and 1.\n\n(a) Cartpole Balance\n\n(b) Cartpole Swingup\n\n(c) Pendulum Swingup\n\n(d) Cup Catch\n\n(e) Acrobot Swingup\n\n(f) Reacher easy\n\n(g) Reacher hard\n\nFigure 21: DeepMind Control Suite: images of easy (top) and medium (bottom) tasks.\n\nWe modify the original DrQv2 by making the encoder map down to a smaller spatial output, leading to faster training. The second and third convolutional blocks have an added max-pooling layer, leading to a spatial output size of 7 × 7. As the equivariant version of DrQv2 has an additional convolutional layer after the action restriction, the non-equivariant version also has an additional convolutional layer at the end of the encoder. We also scale the number of channels by (cid:112)|G| in order to preserve roughly the same number of parameters as the non-equivariant version.\n\nThe policy is evaluated by averaging the return of 10 episodes every 10000 environment steps. In all DMC experiments, we plot the mean and the standard error over 4 seeds. All other training details and hyperparameters are kept the same as in Yarats et al. (2022).\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFigure 22: All symmetry corruptions in the rotation estimation experiment.\n\nE ADDITIONAL EXPERIMENTS\n\nE.1 SUPERVISED LEARNING WITH MORE SYMMETRY CORRUPTION TYPES\n\nIn this section, we demonstrate the experiment in Section 5 in more symmetry corruptions. Figure 22 shows the 15 different corruptions. We also show the performance of ‘Equi’, ‘CNN’, and ‘CNN + Img Trans’ without the random crop augmentation used in Section Section 5 (labeled as ‘no Crop’ variations). The result is shown in Figure 23. First, comparing blue vs green, and purple vs orange, the equivariant network always outperforms the CNN with or without random crop augmentation, especially with fewer data. Second, comparing blue vs purple, and green vs orange, random crop generally helps both the equivariant network and the CNN network. Third, comparing red vs green, and cyan vs orange, adding the image transformation augmentation improves the performance of CNN. Notice that the condition reverse is an outlier because the equivariant network has incorrect equivariance, where the CNN methods (green and orange) without image transformation augmentation have the best performance.\n\nE.2 RL IN MANIPULATION WITHOUT RANDOM CROP\n\nIn this section, we demonstrate the performance of Equivariant SAC and CNN SAC without random crop augmentation using RAD. As is shown in Figure 24, both methods work poorly without the random crop augmentation.\n\nE.3 RL IN MANIPULATION WITH OCCLUSION CORRUPTION\n\nIn this section, we perform the same experiment as in Section 6.1 with a different type of symmetry corruption: occlusion due to orthographic projection using a single camera. Instead of using an RGBD image observation as in Section 6.1, we take the depth channel from the RGBD image and perform an orthographic projection at the gripper’s position (Figure 25). This is the same process as in Wang et al. (2022a) to generate a top-down image for equivariant learning, however, since we only have one camera instead of two as in the prior work, this orthographic projection will have missing depth values due to occlusion and thus leads to an extrinsic equivariant constraint. Figure 26 shows the results. Similar as in Section 6.1, Equivariant SAC outperforms all baselines with a significant margin.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nFigure 23: Comparison of an equivariant network (blue), a conventional network (green), CNN equipped with image transformation augmentation (red), and their variation without random crop augmentation (purple, orange, cyan). The plots show the prediction accuracy in the test of the model trained with different number of training data. Results are averaged over four runs. Shading denotes standard error.\n\nFigure 24: Comparison between Equivariant SAC and CNN SAC without data augmentation using RAD. The plots show the performance (in terms of discounted reward) of the evaluation policy. The evaluation is performed every 200 training steps. Results are averaged over four runs. Shading denotes standard error.\n\nE.4 RL IN DEEPMIND CONTROL SUITE\n\nFigure 27 is another visualization of equivariant vs non-equivariant DrQv2 on the original pendulum swingup environment. As each method has 1 failed seed, we plot all runs with slightly different color shades. If we exclude the failed run from each method, it can easily be seen that equivariant DrQv2 learns faster than the non-equivariant version.\n\nE.4.1\n\nINCREASING SYMMETRY CORRUPTIONS\n\nIn these experiments, we modify some domains to have different levels of symmetry-breaking corruptions. For cartpole and cup catch, we either remove the gridded floor and background (None) to make the observation perfectly equivariant or keep the floor and background and further change the camera angle by rolling (30◦ − 75◦), increasing the level of corruption. For reacher, we use the same modifications but tilt the camera instead of rolling. See Figure 3 for sample images. In order to see the effects of increasing corruption on learning, we plot the mean discounted reward when both methods have converged (30k frames for cartpole and cup catch, 1.5M frames for reacher). Figure 28 shows that both the equivariant and non-equivariant DrQv2 surprisingly perform quite well across all corruption levels, with the exception of 75◦ on reacher. The equivariant policy seems to converge to a slightly higher discounted reward than the non-equivariant version, though the dif-\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nFigure 25: Left: the depth image taken from a depth camera. Right: the orthographic projection centered at the gripper position generated from the left image, where the black areas are missing depth values due to occlusions.\n\nFigure 26: Comparison of Equivariant SAC (blue) with baselines in environments with occlusion corruption. The plots show the performance (in terms of discounted reward) of the evaluation policy. The evaluation is performed every 200 training steps. Results are averaged over four runs. Shading denotes standard error.\n\nference is not significant. On reacher, changing the camera angle may have affected both methods by making the task more difficult for both an equivariant and regular CNN encoder.\n\nF BASELINE ARCHITECTURE SEARCH\n\nF.1 CNN SAC ARCHITECTURE SEARCH\n\nThis section demonstrates the architecture search for CNN SAC. We consider three different architectures (all with a similar amount of trainable parameters): 1) conv (Figure 18): a CNN network with the same structure as Equivariant SAC, where all layers are implemented using convolutional layers. 2) fc1 (Figure 29): a CNN network that replaces some layers in 1) with fully connected layers. 3) fc2 (Figure 30): similar as 2), but with fewer convolutional layers and more weights in the FC layer. We evaluate the three network architectures with SAC equipped random crop augmentation using RAD (Laskin et al., 2020b).\n\nFigure 31 shows the result, where all three variations have a similar performance. We use conv in the main paper since it has a similar structure as Equivariant SAC.\n\nF.2 FERM ARCHITECTURE SEARCH\n\nThis section demonstrates the architecture search for FERM. We consider four different architectures: 1) sim total 1 (Figure 33) and 2) sim total 2 (Figure 19) are two different architectures with the similar amount of total trainable parameters as Equivariant SAC. 3) sim enc (Figure 32) has similar amount of trainable parameters in the encoder as Equivariant SAC’s encoder. Notice that since FERM share an encoder between the actor and the critic while Equivariant SAC has separate encoders, having the similar amount of parameters in the encoder will lead to fewer total parameter in FERM compared with Equivariant SAC. 4) ferm ori (Figure 34) is the same network architecture used in the FERM paper (Zhan et al., 2020).\n\nFigure 35 shows the comparison across the four architectures. ‘sim total 2’ has a marginal advantage compared with the other three variations, so we use it in the main paper.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nFigure 27: All runs of equivariant and non-equivariant DrQv2 on the DMC pendulum swingup task. Each method has 1 failed seed - the failed equivariant policy (blue) run is consistently near zero reward and the failed non-equivariant policy run (red) is around 200. Overall, the equivariant DrQv2 learns faster than the non-equivariant version when it succeeds.\n\nCartpole Swingup\n\nCup Catch\n\nReacher hard\n\nNone\n\nOrig\n\n30◦\n\n45◦\n\n60◦\n\n75◦\n\nTable 3: Modifications to DMC domains for varying symmetry corruption levels. The gridded floor and background are removed to be fully equivariant (None) or the camera angle is modified to increase the level of symmetry corruption (roll for cartpole and cup catch, tilt for reacher).\n\nF.3 SEN ARCHITECTURE SEARCH\n\nThis section shows the architecture search for SEN. We consider three variations (all with similar amount of trainable parameters): 1) SEN conv (Figure 36): all layers are implemented using convolutional layers. 2) SEN fc1 (Figure 37) and SEN fc2 (Figure 20) replaces some layers in 1) with fully connected layers.\n\nFigure 38 shows the comparison across the three variations. ‘SEN fc2’ shows the best performance.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFigure 28: DMC performance comparison on various levels of symmetry corruptions. Both the equivariant and non-equivariant DrQv2 perform quite well even with increasing levels of corruption.\n\nFigure 29: Network architecture of the ‘fc1’ variation for CNN SAC.\n\nFigure 30: Network architecture of the ‘fc2’ variation for CNN SAC.\n\nFigure 31: Architecture search for CNN SAC. The plots show the performance (in terms of discounted reward) of the evaluation policy. The evaluation is performed every 200 training steps. Results are averaged over four runs. Shading denotes standard error.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nFigure 32: Network architecture of the ‘sim enc’ variation for FERM.\n\nFigure 33: Network architecture of the ‘sim total 1’ variation for FERM.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nFigure 34: Network architecture of the ‘ferm ori’ variation for FERM.\n\nFigure 35: Architecture search for FERM. The plots show the performance (in terms of discounted reward) of the evaluation policy. The evaluation is performed every 200 training steps. Results are averaged over four runs. Shading denotes standard error.\n\nFigure 36: Network architecture of ‘SEN conv’ variation of SEN.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nFigure 37: Network architecture of ‘SEN fc1’ variation of SEN.\n\nFigure 38: Architecture search for SEN. The plots show the performance (in terms of discounted reward) of the evaluation policy. The evaluation is performed every 200 training steps. Results are averaged over four runs. Shading denotes standard error.\n\n27",
    "reference": "# Summary Of The Paper\n\nThis work defines correct equivariance, incorrect equivariance, and extrinsic equivariance. Correct symmetry means that the model symmetry correctly reflects a symmetry present in the ground truth function, for which it is correct to enforce equivariance constraints. Extrinsic equivariance is when the equivariant constraint in the equivariant network enforces equivariance to out-of-distribution data. The authors theoretically demonstrate the upper bound performance for an incorrectly constrained equivariant model. In a supervised classification setting, they empirically show that a model with extrinsic equivariance can aid learning compared with an unconstrained model, especially in a low-data regime. Then, they explore this idea in a RL context and show that an extrinsically constrained model can outperform state-of-the-art conventional CNN baselines.\n\n# Strength And Weaknesses\n\nStrength\n* This work empirically shows that extrinsic equivariance can still aid learning, which is very well-motivated contribution to model architecture with image transformation augmentation. \n* Empirical evaluations are strong and comprehensive, covering many kinds of transformations, supervised learning, and reinforcement learning.\n\nWeaknesses\n* The scale of the model and data used in these experiments are still considered small. Given the results in slightly larger data-regime, it's not clear how much it could be useful with large model and large data.\n* When the term correct, incorrect, and extrinsic equivariance/symmetry appear the first time in the paper, they are not well-explained. Even in the following, it's still unclear what the definition is and examples are somewhat unclear. I wonder if it would be useful to at least have section in appendix to clarify is the main paper space is too constrained.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI consider the work novel. However, as pointed out in the weaknesses part, there are still rooms to improve clarity.\n\n# Summary Of The Review\n\nI lean to acceptance but would like the authors to improve the clarify and comment on effectiveness in large-data regime.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCONDITIONAL PERMUTATION INVARIANT FLOWS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe present a novel, conditional generative probabilistic model of set-valued data with a tractable log density. This model is a continuous normalizing flow governed by permutation equivariant dynamics. These dynamics are driven by a learnable per-set-element term and pairwise interactions, both parametrized by deep neural networks. We illustrate the utility of this model via applications including (1) complex traffic scene generation conditioned on visually specified map information, and (2) object bounding box generation conditioned directly on images. We train our model by maximizing the expected likelihood of labeled conditional data under our flow, with the aid of a penalty that ensures the dynamics are smooth and hence efficiently solvable. Our method significantly outperforms non-permutation invariant baselines in terms of log likelihood and domain-specific metrics (offroad, collision, and combined infractions), yielding realistic samples that are difficult to distinguish from real data.\n\n1\n\nINTRODUCTION\n\nInvariances built into neural network architectures can exploit symmetries to create more data efficient models. While these principles have long been known in discriminative modelling (Lecun et al., 1998; Cohen & Welling, 2015; 2016; Finzi et al., 2021), in particular permutation invariance has only recently become a topic of interest in generative models (Greff et al., 2019; Locatello et al., 2020). When learning a density that should be invariant to permutations we can either incorporate permutation invariance into the architecture of our deep generative model or we can factorially augment our observations and hope that the generative model architecture is sufficiently flexible to at least approximately learn a distribution that assigns the same mass to known equivalents. The former is vastly more data efficient but places restrictions on the kinds of architectures that can be utilized, which might lead one to worry about performance limitations. While the latter does allow unrestricted architectures it is often is so data-inefficient that, despite the advantage of fewer limitations, achieving good performance is extremely challenging, to the point of being impossible.\n\nIn this work we describe a new approach to permutation invariant conditional density estimation that, while architecturally restricted to achieve invariance, is demonstrably flexible enough to achieve high performance on a number of non-trivial density estimation tasks.\n\nPermutation invariant distributions, where the likelihood of a collection of objects does not change if they are re-ordered, appear widely. The joint distribution of independent and identically distributed observations is permutation invariant, while in more complex examples the observations are no longer independent, but still exchangeable. Practical examples include the distribution of non-overlapping physical object locations in a scene, the set of potentially overlapping object bounding boxes given an image, and so forth (see Fig. 1). In all of these we know that the probability assigned to a set of such objects (i.e. locations, bounding boxes) should be invariant to the order of the objects in the joint distribution function argument list.\n\nRecent work has addressed this problem by introducing equivariant normalizing flows (Köhler et al., 2020; Satorras et al., 2021; Biloš & Günnemann, 2021). Our work builds on theirs but differs in subtle but key ways that increase the flexibility of our models. More substantially this body of prior art focuses on non-conditional density estimation. The work of Satorras et al. (2021) does consider a form of implicit conditioning, where the flow is evaluated for different graph sizes. In this work we go beyond that by making the dynamics that constitute our flow dependent on a conditional input. To this end, we believe we are the first to develop conditional permutation invariant flows, that are explicitly dependent on external input.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Realistic vehicle placement as a permutation invariant modeling problem. At every moment in time vehicles in the real world exhibit a characteristic spatial distribution of position, orientation, and size; notably vehicles (green rectangles) do not overlap, usually are correctly oriented (red lines indicate forward direction), and almost exclusively are conditionally distributed so as to be present only in driving lanes (shown in grey). The likelihood of each such arrangement does not depend on the ordering of the vehicles (permutation invariance). Each column shows a particular map with vehicle positions from real training data and from infraction free samples drawn from our permutation invariant flow conditioned on the map image. Note that because the image indicates lanes, not drivable area, the training data includes examples of vehicles that hang over into the black. We invite the reader to guess which image in each column is real and which is generated by our model. The answer appears in a footnote at the end of the paper.1\n\nWe demonstrate our conditional permutation invariant flow on two difficult conditional density estimation tasks: realistic traffic scene generation (Fig. 1) given a map and bounding box prediction given an image. In both the set of permutation invariant objects is a set of oriented bounding boxes with additional associated semantic information such as heading. We show that our method significantly outperforms baselines and meaningful ablations of our model.\n\n1.1 BACKGROUND\n\n1.1.1 NORMALIZING FLOWS\n\nNormalizing Flows (Tabak & Vanden-Eijnden, 2010; Tabak & Turner, 2013; Rezende & Mohamed, 2015) are probability distributions that are constructed by combining a simple base distribution pz(z) (e.g., a standard normal) and a differentiable transformation T with differentiable inverse, that maps z to a variable x\n\nWe can then express the density using the change of variables formula\n\nx = T −1(z).\n\npx(x) = pz(T (x))\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ndet\n\n∂T −1(z) ∂z\n\n(cid:12) (cid:12) (cid:12)z=T (x)\n\n−1\n\n,\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(1)\n\n(2)\n\nwhere p denotes the respective densities over variables x and z connected by transformation T with inverse T −1. The transformation T can be parametrized and used to approximate some distribution over data x ∼ π by maximizing the likelihood of this data under the approximate distribution using gradient descent. An important feature distinguishing normalizing flows from other models is that in addition to a method to generate samples they provide a tractable log density, enabling maximum likelihood training and outlier detection among others. This formulation, while powerful, has two noteworthy design challenges: the right hand side of Eq. (2) has to be efficiently evaluable and the aforementioned requirement that T be invertible. The approach in the field generally is to define a chain of transformations T0 ◦ · · · ◦ Tn, each of which satisfy both conditions. In this manner, they can be comparatively simple, yet when joined together provide a flexible approximating family.\n\n2\n\n402002040402002040402002040402002040402002040402002040402002040Under review as a conference paper at ICLR 2023\n\n1.1.2 CONTINUOUS NORMALIZING FLOWS\n\nContinuous normalizing flows were first introduced in Chen et al. (2018), and then further developed in Grathwohl et al. (2019). The concept is to use a continuous transformation of variables, described by dynamics function v parametrized by t in the form of an ordinary differential equation (ODE)\n\nx(t1) = x(t0) +\n\n(cid:90) t1\n\nt0\n\nvθ(x(t), t)dt.\n\n(3)\n\nWe set x(t0) = z and x(t1) = x, so that Eq. (3) provides our definition of T −1 as defined in Eq. (1). Similarly, if we integrate backward in time from t1 to t0 we obtain T . The dynamics vθ(x(t), t) can be represented by a flexible function. As long as the dynamics function is uniformly Lipschitz continuous in x and uniformly continuous in t, the solution to the ODE is unique, and the transformation is invertible (Coddington & Levinson, 1955). In this case, we can write the probability density as another ODE (Grathwohl et al., 2019)\n\nd log pt (x (t)) dt\n\n= −∇x · vθ(x(t), t).\n\n(4)\n\nThe term on the right hand side is the divergence (not gradient) of the dynamics (sometimes equivalently written as the trace of the Jacobian, note that vθ is vector valued in Eq. (4)). Integrating this ODE from the probability density at t0 gives the density at t1\n\nlog pt1(x(t1)) = log pt0 (x(t0)) −\n\n(cid:90) t1\n\nt0\n\n∇x · vθ(x(t), t)dt.\n\n(5)\n\nEq. (5) is the equivalent of Eq. (2) for continuous normalizing flows. Together with a suitable base distribution (e.g. a standard normal), the above transformation constitutes a distribution with a tractable likelihood and generative mechanism, which we will exploit to construct our flows.\n\n1.1.3\n\nINVARIANCE AND EQUIVARIANCE\n\nWe seek to construct distributions that have a permutation invariant density via permutation equivariant transformations. We state here the definition of permutation invariance and equivariance we adopt. Definition 1. Let x = (x1 . . . xN ) where each xn ∈ RD, and let permutations σ act on x via\n\nσx = (xσ1 . . . xσN ) .\n\nA function G : RN ×D → R is permutation invariant if for any permutation σ,\n\n∀ x ∈ RN ×D, G(σx) = G(x).\n\nA function F : RN ×D → RN ×D is permutation equivariant if for any permutation σ,\n\n∀ x ∈ RN ×D, F (σx) = σF (x).\n\n(6)\n\n(7)\n\n(8)\n\n1.2 RELATED WORK\n\nPermutation invariant models have been studied in the literature for some time. Examples include models of sets (Zaheer et al., 2017; Lee et al., 2019), and graphs (Duvenaud et al., 2015; Kipf & Welling, 2017; Kipf et al., 2018). Recently, also generative models for sets have made an appearance, (Zhang et al., 2019; Burgess et al., 2019; Greff et al., 2019; Locatello et al., 2020; Zhang et al., 2020). Our conditional permutation invariant flows belong to the larger class of generative models, such as variational autoencoders (Kingma & Welling, 2014), generative adversarial networks (Goodfellow et al., 2014), and normalizing flows (Rezende & Mohamed, 2015). Among these, normalizing flows are the only class of models that enables likelihood evaluation.\n\nOther work belonging to the generative category is “Equivariant Hamiltonian flows” (Rezende et al., 2019), which relates to our work since it models interactions elements of a set using Hamiltonian dynamics. The choice of these dynamics allows the use of a symplectic integrator, and the transformation is volume conserving, eliminating the need to integrate a divergence term. However, this requires the introduction of a set of momentum variables that preclude the exact calculation of a density. In Liu et al. (2019), the authors present a flow for graphs with tractable density. Although\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nthe target domain is similar, their flow differs from ours as it is based on the mechanism developed in RealNVP (Dinh et al., 2017), whereas our flow uses continuous normalizing flows.\n\nOur work is strongly related to, and draws inspiration from recent work that uses continuous normalizing flows with permutation invariant dynamics (Köhler et al., 2020; Satorras et al., 2021; Biloš & Günnemann, 2021). However Köhler et al. (2020) and Satorras et al. (2021) focus also on rotation and translation invariance, in order to model molecular graphs. Our work is also related to PointFlow, a continuous normalizing flow for point clouds (Li et al., 2021). We focus on sets like in Biloš & Günnemann (2021) and Li et al. (2021), however these studies focus on reducing evaluation cost for large set sizes. Our dynamics function on the other hand focuses on strong interactions between set elements, more akin to Satorras et al. (2021) and Köhler et al. (2020). Importantly, none of this previous work considers the problem of conditioning on external inputs and learning a distribution that is able to deal with a varying conditional input distribution.\n\n2 CONDITIONAL PERMUTATION INVARIANT FLOWS\n\n2.1 PERMUTATION INVARIANT FLOWS\n\nIn this work, we will construct normalizing flows that are characterized by a permutation equivariant transformation T (σx) = σT (x); we will demonstrate these flows produce a permutation invariant density p(x) = p(σx). We construct our permutation invariant flows using a dynamics function that is based on a global force term and pairwise interaction terms\n\nvθ,i(x) =\n\n(cid:88)\n\nj,j̸=i\n\nfθ(xi, xj) + gθ(xi).\n\n(9)\n\nHere, vθ,i(x) ∈ RD denotes the ith element of vθ, x ∈ RN ×D, gθ : RD → RD, and fθ : R2D → RD. This construction can be interpreted as objects xi moving in a global potential with corresponding force field gθ(xi), and interacting with other objects through the pairwise interaction fθ(xi, xj). We proceed to construct a continuous normalizing flow using the function in Eq. (9) as the dynamics. If we use a permutation invariant base distribution p(x(t0)) = p(z) we obtain the following:\n\nTheorem 1. If the transformation z = T (x) defined in Eq. (3) has dynamics vθ(x) defined in Eq. (9), then T is permutation equivariant. If in addition p(z) is permutation invariant, then the density p(x) is permutation invariant.\n\nThe proof of Theorem 1 is given in Appendix A.1. We note that this theorem can be seen as a special case of theorems presented in previous work (Köhler et al., 2020; Zaheer et al., 2017), but add it here for a complete exposition. The dynamical system this theorem represents is similar to an interacting set of particles in a global potential. The dynamics as presented in Eq. (9) are independent of time; in a few cases, however, we have found it useful to make the dynamics time-dependent, i.e. vθ(x, t), by passing time to both gθ and fθ as an input. A complete overview of when time dependence is used is given in the supplementary information Appendix A.2.3. In practice we represent fθ and gθ by neural networks, which satisfy the criterion of uniform Lipschitz continuity if activation functions are chosen appropriately, guaranteeing invertibility. Implementation details can be found in Appendix A.2.1.\n\n2.2 DIVERGENCE\n\nGiven the dynamics in Eq. (9), we compute the density at time t in Eq. (4) using the divergence\n\n∇x · vθ(x) =\n\n(cid:88)\n\ni,j,j̸=i\n\n∇xi · fθ(xi, xj) +\n\n(cid:88)\n\ni\n\n∇xi · gθ(xi).\n\n(10)\n\nA naïve computation of the divergence in Eq. (4) using automatic differentiation is expensive, as computing the Jacobian requires N D evaluations, one for each of the N D terms in vθ (Chen et al., 2018; Grathwohl et al., 2019). Since the cost of evaluating Eq. (9) is quadratic in N , this would result in an asymptotic computational cost of N 3D2 for the forward and backward pass. Earlier work has suggested the use of the Hutchinson’s trace estimator (Chen et al., 2018) for the divergence, which reduces the cost of the divergence to that of v, but suffers from high variance (Chen & Duvenaud,\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n2019). Instead we opt to re-express the divergence of vθ in terms of derivatives of f (xi, xj) and g(xi), resulting in Eq. (10). The form in Eq. (10) is quadratic in N , and therefore same cost in N as the evaluation of vθ itself, both in the forward and backward pass.\n\n2.3 REGULARIZATION\n\nContinuous normalizing flows have no inherent mechanism that penalizes very complex dynamics. While in theory there is no reason to prefer simple dynamics, in practice, the numerical integration of complex dynamics can result in long computation when using an adaptive scheme. This effect has been previously observed in the literature, and suggestions to regularize the dynamics have been proposed in previous work (Finlay et al., 2020; Kelly et al., 2020). While the work of Kelly et al. (2020) is more comprehensive, we find that an adaptation of the solution proposed in Finlay et al. (2020) works well for our purposes. The proposed solution in Finlay et al. (2020) is to add a term proportional to the squared Frobenius norm of the Jacobian, and the l2-norm of the dynamics. We use the l2-norm for the dynamics; however, since we do not estimate the full Jacobian we calculate\n\nl2 div =\n\n(cid:88)\n\ni̸=j,d,d′\n\n(cid:18) ∂fd(xid, xjd) ∂xid′\n\n(cid:19)2\n\n(cid:88)\n\n+\n\ni,d,d′\n\n(cid:18) ∂gd(xid) ∂xid′\n\n(cid:19)2\n\n.\n\n(11)\n\nWe find that this penalty significantly reduces the number of evaluations of our trained flows. We visualize the effect this penalty has on the dynamics in some examples in Appendix A.3.4.\n\n2.4 CONDITIONAL PERMUTATION INVARIANT FLOWS\n\nWhen performing amortized inference, in which a family of posterior distributions is learned, a requirement is a flexible variational family that can be made to depend (i.e. conditioned) on external input. An example would be to produce a valid distribution of a set of bounding box locations and sizes x for objects in an image y selected from a distribution of images. We will denote the conditioning input as y, coming from some data distribution π(y). To model such cases we construct a dynamics function that depends on y by modifying the pair forces, and the global force to fθ(xi, xj, y) and gθ(xi, y). The dynamics then become:\n\nvθ,i(x, y) =\n\n(cid:88)\n\nj\\i\n\nfθ(xi, xj, y) + gθ(xi, y).\n\n(12)\n\nNote that here too, the dynamics can be made time dependent by passing time t as an argument.\n\nWe will train our flows by minimizing the Kullback-Leibler divergence to the joint distribution π(x, y) = π(x|y)π(y) over data x and condition y:\n\narg min θ\n\nDKL (π(x, y)||pθ(x|y)π(y)) = arg min\n\nθ\n\nE y∼π(y)\n\nDKL (π(x|y)||pθ(x|y))) .\n\n(13)\n\nIn other words, we optimize our flow to match the distribution of x in expectation over π(y).\n\n3 EXPERIMENTS\n\n3.1 SYNTHETIC EXAMPLES\n\nWe start our experiments with two pedagogical examples that demonstrate the capabilities and mechanisms of our flows. The first example task is to model the spatial distribution of five non-overlapping squares of width w = 1, that furthermore do not overlap with the prohibited regions shown in blue. This example is representative of placing assets into a physically realizable configuration in accordance with constraints imposed by an environment. We fit our conditional flow to a dataset generated by first sampling a prohibited region—three boxes of width w = 1.5 from a standard normal prior—and then sampling box locations independently from a standard normal prior, with rejection for overlap with previous boxes or the prohibited region, until a total of five boxes are sampled. The prohibited region is input to our conditional flow as an image tensor. Since the dataset was generated via rejection sampling, we can compare our sample efficiency against it. The conditional flow provides a substantial sampling efficiency improvement (77% valid) over rejection sampling (0.02% valid), in addition to a tractable density.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Two pedagogical permutation invariant modeling tasks. The left two panels illustrate the first task; conditionally modeling non-overlapping squares (green), which also do not overlap with the blue boxes whose arrangement varies between datapoints. The right two panels illustrate the second task; modeling boxes that are conditionally distributed so as to bound the underlying blue boxes. Samples from the base p(z) and final distribution pθ(x|y) are plotted in dashed grey and green lines respectively. The conditional input is plotted as a blue on white image. Red lines indicate the trajectories the objects follow by integrating the dynamics function v(x(t), y).\n\nThe second example task is bounding box prediction, or conditionally generating object bounding boxes x directly from an image y. Here the objects are monochrome blue squares. Data is generated in a similar manner as in the first experiment: squares are sampled indepenently from a standard normal prior, and rejected if they overlap. The conditional input is an image of the generated boxes. Sampled bounding boxes from our trained flow achieve an average intersection over union (IOU) of 0.85 with the ground truth bounding boxes.\n\nWe display representative samples and their trajectories through time in Fig. 2 for both experiments. In the left two panels it can be seen that initial samples are transported around the space to avoid one another; in the right two, the boxes coordinate through the pairwise interactions to each surround exactly one of the objects in the scene. Further details for these experiments appear in Appendix A.3.\n\n3.2 TRAFFIC SCENES\n\nModeling and being able to sample realistic traffic scenes is an essential task related to autonomous driving simulation and control. Referring back to Fig. 1, the problem—similar to the first pedagogical task above—is one of modeling the physical configuration a collection of agents conditioned on a representation of the environment. Until recently, the predominant methods for generating realistic vehicle configurations were rule-based (Yang & Koutsopoulos, 1996; Lopez et al., 2018). Rule-based systems can be tailored to have desirable properties such as avoiding occurences of offroad and vehicle overlap, but they produce vehicle arrangements that are distributionally dissimilar to real data.\n\nRecent work addressing this problem uses non-rule-based autoregressive model (Tan et al., 2021) that enables sequential generation of vehicle and agent positions conditioned on a visual representation of a map. While this model-based approach closes the gap between simulation and reality, modeling sets autoregressively introduces a factorial data augmentation requirement, as there is no intrinsic ordering of actors. The authors of Tan et al. (2021) avoid this by imposing an arbitrary order, sampling agents from left to right. Our experiments indicate that, at least for this specific task, directly addressing permutation invariance is preferred, and avoids the need to arbitrarily fix the order of elements.\n\nTo test the performance of our flows on this task, we train them to generate a scene of cars in the INTERACTION dataset (Zhan et al., 2019), conditioned on a rendered image of the drivable area y. The properties that our flows predict are two-dimensional position, size, aspect ratio and orientation for each of N agents, i.e. x ∈ RN ×5. An advantage of the formulation of the dynamics in Eq. (12) is that they can be applied with the same f and g regardless of the number of agents N . We make use of this property, and train a single model on a varying amount of agents N . At test time, generating N agents is accomplished simply by initializing the flow appropriately.\n\nWe train our flows until the likelihood of the data stops increasing, or the likelihood of a held out validation sets starts decreasing, whichever comes fist. Examples of the data we train on, and representative samples from our trained flow are shown in Fig. 1.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Results for scene generation and bounding box prediction.\n\n(a) Quantitative results for traffic scene generation. NLL indicates negative loglikelihood in nats, while the other metrics indicate the fraction of samples exhibiting offroad, collision, or either (lower is better).\n\nMethod\n\nGaussian RealNVP CNF Autoregr.\n\nPIF Single PIF Pair PIF Pair MF PIF (ours)\n\nCond. single Cond. pair Cond. base\n\nNLL\n\nOffroad\n\nCollision\n\nInfraction\n\n46.3 ± 0.0 30.4 ± 1.4 20.8 ± 0.9 11.0 ± 1.7\n\n7.3 ± 0.2 6.3 ± 0.3 7.0 ± 0.1 6.5 ± 0.3\n\n7.1 ± 0.3 7.2 ± 0.1 16.5 ± 2.2\n\n0.99 ± 0.00 0.96 ± 0.01 0.86 ± 0.01 0.72 ± 0.02\n\n0.09 ± 0.01 0.17 ± 0.01 0.11 ± 0.02 0.12 ± 0.01\n\n0.11 ± 0.01 0.18 ± 0.02 0.91 ± 0.01\n\n0.27 ± 0.00 0.26 ± 0.00 0.33 ± 0.00 0.14 ± 0.01\n\n0.57 ± 0.00 0.18 ± 0.02 0.56 ± 0.01 0.09 ± 0.01\n\n0.12 ± 0.00 0.17 ± 0.03 0.05 ± 0.01\n\n0.99 ± 0.00 0.98 ± 0.00 0.92 ± 0.01 0.76 ± 0.02\n\n0.61 ± 0.01 0.32 ± 0.02 0.61 ± 0.00 0.20 ± 0.01\n\n0.22 ± 0.01 0.32 ± 0.03 0.92 ± 0.01\n\n(b) Quantitative results for bounding box prediction. IOU refers to the intersection over union of object area covered by the samples (higher is better).\n\nData\n\nIOU\n\n3\n\n6-3 6-4 6-5 6-6 6\n\n0.732\n\n0.759 0.711 0.644 0.596 0.679\n\n3.2.1 BASELINES\n\nWe compare our conditional flows against several baselines. Three are non-permutation invariant flows: a unimodal Gaussian model, a RealNVP based model (Dinh et al., 2017), and a “vanilla” continuous normalizing flow (“CNF”). We also implement an autoregressive model consisting of a convolutional neural net paired with a recurrent neural network and a 10 component Gaussian mixture for every prediction component, and adopt the canonical ordering discussed in Tan et al. (2021). We also test two ablations of our model: one where the dynamics are restricted only to single particle terms gθ (“PIF Single”), and one the dynamics only include the pair term fθ (“PIF Pair”). The flow where the dynamics are restricted to single particle terms only corresponds to a conditional version of PointFlow (Li et al., 2021). We also compare against a conditional version of the model presented in Biloš & Günnemann (2021) (“PF Pair MF”), in which interactions are addressed, but only in terms of a mean field (being cheaper to calculate). Both these methods result in a significantly worse collision and resulting infraction rate.\n\nTo compare to non-permutation invariant methods, we have to fix the number of agents, as these architectures cannot straightforwardly be provisioned to generate and score sets that differ in cardinality. We exclude all data with less than seven agents, and cases with more than seven agents are pruned to retain only the agents closest to the center. The cardinality of seven was chosen to retain as much data as possible while not making each individual scene too small. Furthermore, we restricted the INTERACTION dataset to the roundabout scenes in order to better match the seven agent target while still maintaining a semantically similar set of possible y. We compare the negative log likelihood (NLL) of the various models on a held-out test dataset. For these traffic scenarios there are two other useful metrics we can compare: the fraction of offroad (i.e. an agent is on the undrivable area), collision (i.e., an agent overlaps with another agent) and total infractions (offroad or collision) the model makes. We note that these metrics are sensitive indicators of model fitness, in the sense that the training data contains no offroad or colliding data examples. Samples that exhibit these infractions are evidence of model error, and additionally inform whether modeling mistakes are made globally (i.e. offroad) or through interactions (i.e. collisions). We report our results in Table 1.\n\nComparing the likelihood and infraction metrics demonstrates the clear advantage of using a permutation invariant model. The non-permutation invariant version of our flow does not converge to a competitive likelihood, and struggles to generate infraction-free examples. While the canonically ordered autoregressive model is much more capable than the non-permutation invariant flows, it still underperforms compared to ours. The two ablations of our model provide an insightful result: the function of gθ is to define the collective behavior of the agents (all of them need to stay on the road, independently of one another), whereas fθ provides the necessary interaction between them (agents should not collide with one-another). These functions are evident from the respective infraction metrics. Furthermore, there appears to be a certain amount of competition between the pair and single\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Bounding box prediction on CLEVR3 images. Each image shows 50 samples from our conditional flow (green, ground truth in blue), conditioned on the background image. The bottom row shows the trajectories of the boxes with time along the trajectory encoded by the color of the box.\n\nterms: as the agents are steered onto the road, they have a higher density and thus a higher chance to collide. The opposite is equally true, as repelling agents can push each other off-road. As such it is not terribly surprising that the “single-only” (gθ-based) flow performs better when only considering offroad infractions. Nevertheless, the combined flow has the best performance overall, both in overall infraction rate and negative log likelihood.\n\nTo better understand the effects of our conditioning inputs, we report the performance of two ablations, one where the conditional input is only used in gθ (“Cond. Single”), and one where it is only used in fθ (“Cond. Pair”). In the former, the interactions between actors are independent of the scene, which one may expect to be a reasonable approximation. However, the results in Table 1 indicate that this input is in fact important, which may be understood from the fact that different locations lead to different traffic configurations. In the case where conditioning on gθ is ignored, we also do worse than conditioning both, while surprisingly maintaining a fairly competitive result. Finally, we show a variant of our model where we condition the base distribution (i.e. pθ(z|y) vs. p(z)), but otherwise remove the conditional dependence from fθ and gθ. Although in this case the collision rate is lowest, this can be attributed to the poor performance with respect to offroad infractions. Overall, this variation fails to provide a competitive result.\n\n3.2.2 VARIABLE SET SIZE Since our model is trained on a variety of different set sizes, and performs well for each of the different set sizes, we can investigate whether the it generalizes beyond the number of agents it has seen during training. We therefore generate samples in our roundabouts model with a previously unseen number of agents. Such samples are presented in the last column of Fig. 1, which have 28 agents, while the maximum number of agents present in the training data is 22. These results indicate the the inductive bias of the representation in Eq. (12) not only performs well in sample with respect to set size, but generalizes to larger set sizes too.\n\n3.3 BOUNDING BOX PREDICTION\n\nOur final experiment considers bounding box prediction. The gold standard in this sub-problem of object detection remains so-called “non-max-suppression,” in which a large number of putative bounding boxes are scene-conditionally generated and then “pruned” by a greedy selection algorithm (Girshick et al., 2014; Ren et al., 2015). Some very recent studies (Hu et al., 2018; Zhang et al., 2019; Carion et al., 2020) have proposed the use of conditional set generators for object detection. We build on this idea by illustrating how our flow can be used to conditionally generate and score sets of object bounding boxes. Being able to compute the log density of a configuration of bounding boxes eliminates the need for a differentiable matching algorithm (Hu et al., 2018; Carion et al., 2020). Having a tractable density also opens up uncertainty-preserving approaches to downstream tasks such as outlier detection and object counting.\n\nWe assess the viability of our approach through the CLEVR (Johnson et al., 2017) dataset, which is a standard benchmark for set-generation models (Greff et al., 2019; Zhang et al., 2019; Locatello et al., 2020). While this former work combines localization with classification (i.e., predicting object\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Bounding box prediction on CLEVR6 images. Each image shows 50 samples (green, ground truth blue) from our flow conditioned on the corresponding background image.\n\nposition and type), we focus on the task of bounding box prediction (i.e predicting object position and size). The CLEVR dataset does not provide bounding boxes, so we generate ground truth bounding boxes from object metadata. The full details on how to create bounding boxes for the CLEVR dataset are described in Appendix A.4.1.\n\nWe begin with a subset of the CLEVR dataset only containing three objects. We find that our flows perform well on this task, and show example predictions in Fig. 3. For each conditional image (displayed in the background), 50 samples from the conditional distribution are shown, graphically illustrating the variance of the conditional density. For each column, the bottom row displays the trajectory taken to generate a single one of these samples. The trajectories show the interactions between the bounding boxes over time, as they coordinate through the use of repulsive forces. Note that by sorting out which of the base distribution samples goes to which of the objects, the flow solves the assignment problem along the way. It is worth pointing out that the third sample has an occluded object, and the variance of the object position is clearly higher than that for objects where there is no occlusion, which we take to be evidence that these flows for bounding box prediction should be useful in uncertainty aware downstream applications. Importantly, the variance of the size is not significantly increased, which is correct behavior for this example. Additional samples are presented in the appendix. The averaged IOU with the ground truth is reported in Table 1b, and corresponds approximately to an average mismatch of 15% in each spatial dimension.\n\nWe continue our exploration with a larger subset of the CLEVR dataset, including images that have between three and six objects. This subset has also been used in Locatello et al. (2020) for object detection. We assume that the number of objects is given, and only predict the bounding box locations and sizes given the number of bounding boxes and the image. Some example samples are displayed in Fig. 4, with green boxes displaying samples from the distribution that is conditioned on the image in the background. We provide baseline results of Deep Set Prediction Networks (Zhang et al., 2019), which does not provide a tractable log density, in Appendix A.4.2. More samples are presented in Appendix A.3.5. The flow generalizes well over set cardinalities, hinting that some generalizing principles are learned by the flow about these bounding boxes interact, even with different set size. We moreover see that the more crowded the image becomes, the more spread there is in the predicted bounding boxes, representing increased uncertainty about object sizes and positions, also representing more occlusion. The overall IOU (“3” and “6”), as well as the IOU’s separated by set cardinality (“6-3”, etc.) are given in Table 1b. These results show that the flow trained on data with variable set size performs marginally better on the CLEVR3 subset than a flow trained only on that data, which we speculate is due to the larger amount of available data. A modest decrease in IOU can be observed as the sets become larger, resulting in an overall performance that is slightly lower.\n\n4 DISCUSSION AND CONCLUSIONS\n\nThis work introduced conditional permutation invariant flows, a framework built on continuous normalizing flows that enables conditional set generation with a tractable density. We have applied our flows to two problems: realistic traffic scene generation, and bounding box prediction. For traffic scene generation, we significantly outperform baselines, and are the first to present a permutation invariant solution. Ablations to our flows highlight their intuitive mechanism, that can be understood as objects moving jointly in a field, augmented with pairwise interaction potentials. We have moreover shown that bounding box prediction can be enhanced with a tractable log density, opening an avenue to develop downstream vision algorithms that deal with uncertainty in a more principled way.\n\n1By column from left to right, the flow samples are: top, top, top, bottom, both. For the last column in\n\nparticular, our flow is able to produce more vehicles than ever appeared on that map in the training data.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMarin Biloš and Stephan Günnemann. Scalable normalizing flows for permutation invariant densities. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 957–967. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/bilos21a.html.\n\nChristopher P. Burgess, Loïc Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matthew Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. CoRR, abs/1901.11390, 2019. URL http://arxiv.org/abs/1901.11390.\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 213–229, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58452-8.\n\nRicky T. Q. Chen and David K Duvenaud. Neural networks with cheap differential operators. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/770f8e448d07586afbf77bb59f698587-Paper.pdf.\n\nRicky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/ 69386f6bb1dfed68692a24c8686939b9-Paper.pdf.\n\nEarl A. Coddington and Norman Levinson. Theory of Ordinary Differential Equations. McGraw-Hill,\n\n1955. ISBN 978-0-07-099256-6.\n\nTaco Cohen and Max Welling. Group equivariant convolutional networks. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 2990–2999, New York, New York, USA, 20–22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/cohenc16.html.\n\nTaco S. Cohen and Max Welling. Transformation properties of learned visual representations. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.7659.\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum? id=HkpbnH9lx.\n\nJ.R. Dormand and P.J. Prince. A family of embedded runge-kutta formulae. Journal of Computational and Applied Mathematics, 6(1):19–26, 1980. ISSN 0377-0427. doi: https://doi. org/10.1016/0771-050X(80)90013-3. URL https://www.sciencedirect.com/science/article/pii/ 0771050X80900133.\n\nDavid K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf.\n\nChris Finlay, Jörn-Henrik Jacobsen, Levon Nurbekyan, and Adam M. Oberman. How to train your neural ode: the world of jacobian and kinetic regularization. In ICML, pp. 3154–3164, 2020. URL http://proceedings.mlr.press/v119/finlay20a.html.\n\nMarc Finzi, Max Welling, and Andrew Gordon Gordon Wilson. A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nProceedings of Machine Learning Research, pp. 3318–3328. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/finzi21a.html.\n\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2014.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper/ 2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.\n\nWill Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. Scalable reversible generative models with free-form continuous dynamics. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJxgknCcK7.\n\nKlaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2424–2433. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/greff19a.html.\n\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Towards non-autoregressive language models. CoRR, abs/2102.05379, 2021.\n\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n\nJustin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\n\nJacob Kelly, Jesse Bettencourt, Matthew James Johnson, and David Duvenaud. Learning differential equations that are easy to solve. In Neural Information Processing Systems, 2020. URL https: //arxiv.org/abs/2007.04504.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/ 1312.6114.\n\nThomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2688–2697. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/ v80/kipf18a.html.\n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview. net/forum?id=SJU4ayYgl.\n\nJonas Köhler, Leon Klein, and Frank Noe. Equivariant flows: Exact likelihood generative learning for symmetric densities. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5361–5370. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/kohler20a.html.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document\n\nrecognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791.\n\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 3744–3753. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/lee19d.html.\n\nXiangtai Li, Hao He, Xia Li, Duo Li, Guangliang Cheng, Jianping Shi, Lubin Weng, Yunhai Tong, and Zhouchen Lin. Pointflow: Flowing semantics through points for aerial image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4217–4226, June 2021.\n\nJenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/1e44fdf9c44d7328fecc02d677ed704d-Paper.pdf.\n\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 11525–11538. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/8511df98c02ab60aea1b2356c013bc0f-Paper.pdf.\n\nPablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob Erdmann, Yun-Pang Flötteröd, Robert Hilbrich, Leonhard Lücken, Johannes Rummel, Peter Wagner, and Evamarie Wiessner. Microscopic traffic simulation using sumo. In 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pp. 2575–2582, 2018. doi: 10.1109/ITSC.2018.8569938.\n\nChris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations, 2017.\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\n\nFaster r-cnn: Towards realtime object detection with region proposal networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/ 14bfa6bb14875e45bba028a21ed38046-Paper.pdf.\n\nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1530–1538, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/rezende15.html.\n\nDanilo Jimenez Rezende, Sébastien Racanière, Irina Higgins, and Peter Toth. Equivariant hamiltonian\n\nflows, 2019.\n\nVictor Garcia Satorras, Emiel Hoogeboom, Fabian Bernd Fuchs, Ingmar Posner, and Max Welling. E(n) equivariant normalizing flows. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=N5hQI_RowVA.\n\nE. G. Tabak and Cristina V. Turner. A family of nonparametric density estimation algorithms. Communications on Pure and Applied Mathematics, 66(2):145–164, 2013. doi: https://doi.org/10. 1002/cpa.21423. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423.\n\nEsteban G. Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood. Communications in Mathematical Sciences, 8(1):217 – 233, 2010. doi: cms/1266935020. URL https://doi.org/.\n\nShuhan Tan, Kelvin Wong, Shenlong Wang, Sivabalan Manivasagam, Mengye Ren, and Raquel Urtasun. Scenegen: Learning to generate realistic traffic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 892–901, June 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\n\nwith linear complexity, 2020.\n\nQI Yang and Haris N. Koutsopoulos. A microscopic traffic simulator for evaluation of dynamic traffic management systems. Transportation Research Part C: Emerging Technologies, 4(3): 113–129, 1996. ISSN 0968-090X. doi: https://doi.org/10.1016/S0968-090X(96)00006-X. URL https://www.sciencedirect.com/science/article/pii/S0968090X9600006X.\n\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ f22e4747da1aa27e363d86d40ff442fe-Paper.pdf.\n\nWei Zhan, Liting Sun, Di Wang, Haojie Shi, Aubrey Clausse, Maximilian Naumann, Julius Kümmerle, Hendrik Königshof, Christoph Stiller, Arnaud de La Fortelle, and Masayoshi Tomizuka. INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps. arXiv:1910.03088 [cs, eess], 2019.\n\nYan Zhang, Jonathon Hare, and Adam Prugel-Bennett. Deep set prediction networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https: //proceedings.neurips.cc/paper/2019/file/6e79ed05baec2754e25b4eac73a332d2-Paper.pdf.\n\nYan Zhang, Jonathon Hare, and Adam Prügel-Bennett. Fspool: Learning set representations with featurewise sort pooling. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJgBA2VYwH.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 PROOF\n\nWe provide here the proof for Theorem 1.\n\nProof. We will consider equivariance of T to a transposition of elements i and j, denoted σi,j. A transposition σi,j is a permutation for which σi = j, σj = i, and σk = k for all k ∈ {1 . . . N }\\{i, j}. Since any permutation can be constructed from a series of transpositions, proving that T is equivariant to a transposition, trivially extends to equivariance to all permutations. In the following we have dropped the dependence on y and t for notational clarity. We have\n\nT (σi,jx) = σi,jx +\n\n(cid:90) t1\n\nt0\n\nvθ(σi,jx)dt.\n\n(14)\n\nThe first term σi,jx trivially satisfies the equivariance condition. Focusing on the ith term of the dynamics function vθ,i\n\n(cid:88)\n\nk\\{i} (cid:88)\n\nfθ ((σi,jx)i, (σi,jx)k) + gθ ((σi,jx)i)\n\n(15)\n\nfθ ((σi,jx)i, (σi,jx)k) + fθ ((σi,jx)iv, (σi,jx)j) + gθ (xj)\n\n(16)\n\nvθ,i(σi,jx) =\n\n=\n\n=\n\n=\n\nk\\{i,j} (cid:88)\n\nfθ (xj, xk) + fθ (xj, xi) + gθ (xj)\n\nk\\{i,j} (cid:88)\n\nfθ (xj, xk) + gθ (xj)\n\nk\\{j}\n\n= vθ,j (x) = (σi,jvθ(x))i,\n\nthus demonstrating the dynamics are equivariant and\n\nT (σi,jx) = σi,jx +\n\n(cid:90) t1\n\nt0\n\nσi,jvθ(x)dt = σi,jT (x).\n\n(17)\n\n(18)\n\n(19)\n\n(20)\n\nFor continuous normalizing flows, the inverse transformation T −1(x) is obtained by reversing the integration limits, and an identical derivation can be made to show T −1(x) is also equivariant. For the density, we have\n\nlog pt1(σi,jx(t1)) = log pt0\n\n(cid:0)T −1 (σi,jx(t1))(cid:1) −\n\n(cid:90) t1\n\nt0\n\n∇x · v(σi,jx)dt.\n\n(21)\n\nHere, the divergence ∇x · vθ (σi,jx) denotes the divergence with respect to the argument of v, evaluated at σi,jx. Since the base distribution pt0 is permutation invariant, and T −1 is equivariant\n\nlog pt0\n\n(cid:0)T −1 (σi,jx(t1))(cid:1) = log pt0 = log pt0\n\n(cid:0)σi,jT −1 (x(t1))(cid:1) (cid:0)T −1 (x(t1))(cid:1) .\n\n(22)\n\n(23)\n\nThe divergence in the second term ∇x · vθ (σi,jx) is a sum over derivatives with respect to all its arguments, so is invariant to σi,j\n\nWe therefore have\n\n∇x · vθ (σi,jx) = ∇x · vθ (x) .\n\nlog pt1 (σi,jx(t1)) = log pt1(x(t1)),\n\n(24)\n\n(25)\n\nthus showing that the dynamics are equivariant, and the density is invariant.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Hyperparameters used for experiments. Abbreviations are defined in the appendix text.\n\nExperiment\n\ngθ\n\nfθ\n\nyemb\n\nExample conditional Example bounding box\n\nTraffic baseline\n\nCLEVR3 CLEVR6\n\nn\n\n5 5\n\n5\n\n4 5\n\nh\n\n200 200\n\n200\n\n188 100\n\nn\n\n5 5\n\n4\n\n5 5\n\nh\n\n200 200\n\n100\n\n196 200\n\nn\n\n3 3\n\n3\n\n5 5\n\nc\n\n16 16\n\n32\n\n18 28\n\nh\n\n200 200\n\n500\n\n409 478\n\nt\n\nbatch\n\nyes yes\n\nno\n\nno no\n\n100 100\n\n100\n\n100 100\n\nA.2 EXPERIMENTAL DETAILS\n\nA.2.1 FORCE FUNCTION IMPLEMENTATION\n\nWe model the force functions fθ(xi, xj) and gθ(xi) by feed forward neural networks. For the pair force fθ(xi, xj), we concatenate the inputs xi and xj. In the case a time variable (t) is used, it is also concatenated. For the conditional input, we construct an embedding vector yemb, which we concatenate to the second layer inputs of gθ and fθ. The embedding vector yemb is generated using a separate neural network, here chosen to be a convolutional network, since all our conditional distributions have images as inputs.\n\nA.2.2 SOLVING THE ODE\n\nWe use the adaptive solver of Dormand and Prince of order 4 to solve the ODE (Dormand & Prince, 1980). To calculate the gradients of the ODE with respect to its parameters we use the adjoint method (Coddington & Levinson, 1955). This enables calculation of the gradients without back propagating through the computational graph. This functionality is all available in the torchdiffeq package (Chen et al., 2018), which is the implementation we use in our experiments.\n\nA.2.3 TABLE OF EXPERIMENTAL HYPERPARAMETERS\n\nIn all experiments gθ, fθ are implemented as neural networks of n layers and h neurons per layer. The convolutional embedding network has n layers of c channels, followed by a single feed forward layer of h neurons. We use sigmoid-linear units in all our dynamics functions, which satisfy the requirement of Lipschitz continuity, provided the networks are evaluated on a finite domain. The use of a time variable in the dynamics is indicated with t ∈ {yes, no}. For each experiment, these parameters are presented in Table 2.\n\nA.2.4 COMPUTATIONAL COST\n\nSumming over all pairs of interactions is necessary to make the transformation permutation equivariant, but it comes at a quadratic cost in N . While not problematic for the set sizes in this study, this is clearly a limited approach for large numbers of objects. In these cases, it would be possible to set a boundary on the interaction range, or use a fixed set of M < N inducing points, for a total cost of M N . Such approximations have been studied for example in transformers (which are also quadratic in the sequence length) (Vaswani et al., 2017; Wang et al., 2020). Furthermore, the divergences with respect to xi are still quadratic with respect to D. This has been addressed in recent work by using functions that have divergences that can be easily evaluated using automatic differentiation (Chen & Duvenaud, 2019; Biloš & Günnemann, 2021). Although these types of functions are compatible with our framework, the current work only considers cases where D ≤ 5, and therefore we do not implement it. Our overall algorithm therefore is of cost N 2D2. If it is necessary to construct distributions with larger D (in for example object detection, rather than bounding box prediction), it is possible to use the methodologies from the aforementioned work to end up with a total computational cost of M N D.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Conditional samples. The condition is an image, which is plotted as a blue on white background. The distribution is trained on samples that do not overlap with the blue regions, or with oneanother. The grey boxes are samples from the base distribution, the green boxes are samples from the flow. The red curves indicate the traveled trajectory for each box.\n\nTable 3: Acceptance rates for conditional sampling. Results presented are the acceptance rate (AR) for Prior samples and the Conditional Permutation Invariant Flows (PIF)\n\nSet Size\n\nPrior AR\n\nPIF AR\n\n2 3\n5\n\n0.01 2.01 · 10−3 1.76 · 10−4\n\n0.83 0.79 0.77\n\nA.2.5 COMPUTATIONAL RESOURCES\n\nAll our experiments were performed on a single GPU, all permutation invariant models were trained between 2 and 7 days of wall-clock time. The “vanilla” continuous normalizing flow, realNVP, and autoregressive model were trained over 14 days of wall-clock time.\n\nA.2.6 REJECTION SAMPLING\n\nAll samples presenting traffic scenarios in the main text of this manuscript have been checked by an automated procedure, and in a small amount of cases were rejected if an infraction occurred based on actors being offroad or vehicle overlap.\n\nA.2.7 DATASETS\n\nThe datasets used in this study are the INTERACTION datset (Zhan et al., 2019) (available for research purposes), and the CLEVR dataset (Johnson et al., 2017) (available under the Creative Commons CC BY 4.0 license).\n\nA.3 ADDITIONAL EXPERIMENTAL RESULTS\n\nA.3.1 CONDITIONAL NON-OVERLAPPING BOXES\n\nAdditional samples of of the conditional generation of non-overlapping boxes (Section 3.1) are presented in Fig. 5, for cardinalities 2, 3 and 5. Performance in terms of acceptance rate against an independent prior are reported in Table 3.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Additional samples of the example bounding box prediction problem.\n\nFigure 7: Sample and its log probability (left panel) and three corrupted variations where actors in blue have been turned around (last three panels).\n\nA.3.2 BOUNDING BOX PREDICTION\n\nAdditional samples for the example bounding box prediction task (Section 3.1) are presented in Fig. 6.\n\nA.3.3 OUTLIER DETECTION\n\nSince our model has a tractable density, we can use it for outlier detection. In the traffic scene task, we study the case of mislabelled examples, which we artificially generate by rotating one of the actors in the scene by π. The original and corrupted scenes and corresponding log probabilities are shown in the last three panels of Fig. 7. It is clear that reversing one of the actors substantially decreases the probability. Moreover the model correctly captures the severity of the resulting infraction, which is less when an actor is going the wrong way on a two-way road, without the presence of other surrounding actors.\n\nA.3.4 REGULARIZATION\n\nWe present the effect of our regularization term on the bounding box prediction task, in which the effect is most pronounced. The proportionality constants of the l2 and l2 div penalty terms are denoted as λ and λdiv respectively. Results for various λ and λdiv are presented in Fig. 8. The increase of the parameters λ and λdiv evidently creates more direct trajectories. We further find empirically that it drastically reduces the number of calls to the dynamics made by the ODE solver.\n\n17\n\n402002040402002040LP: -1.4402002040LP: -11.5402002040LP: -28.3402002040LP: -15.6Under review as a conference paper at ICLR 2023\n\nFigure 8: The effect of regularization on the dynamics. The penalty proportionality constants are reported per column in the top panel.\n\nTable 4: Results for DSPN (Zhang et al., 2019)\n\nSet size\n\nIOU\n\n3 4\n5 6\n\n0.89 0.88 0.84 0.83\n\nA.3.5 CLEVR\n\nAdditional samples for the CLEVR3 dataset are presented in Fig. 9.\n\nAdditional samples for the CLEVR6 dataset are presented in Fig. 10.\n\nA.4 BOUNDING BOX PREDICTION\n\nA.4.1 CLEVR SIZE\n\nThe CLEVR dataset contains the pixel positions of objects, but not the bounding box sizes. The dataset does however provide the center locations of the objects in the global frame {xi, yi, zi}. Since the objects are sitting on a flat plane at z = 0, its z coordinate is equal to half its size. Furthermore, the dataset provides the distance to the camera along the viewpoint axis, dz. Using these quantities, we approximate the bounding box size ∆ as:\n\n∆ ≈\n\nzi√ dz\n\n.\n\n(26)\n\nWe find empirically that this results in reasonable bounding boxes.\n\nA.4.2 CLEVR DSPN\n\nWe use Deep Set Prediction Networks (Zhang et al., 2019) to provide some experimental context for the CLEVR results. We note that this method does not provide a tractable log density, and therefore comparison between this method and ours is limited. The network was trained for 100 epochs and results are provided in Table 4 for set sizes reported in the main text.\n\n18\n\n=0div=0=0.1div=0.001=0.1div=0.01=0.5div=0.05Under review as a conference paper at ICLR 2023\n\nFigure 9: Additional examples for the CLEVR3 dataset. The blue boxes show ground truth bounding boxes, while the green boxes are all samples from the learned conditional distribution.\n\nA.4.3 DETECTION\n\nIn order to turn our bounding box detector in an object detection system, the objects need to be classified as well, which requires the use of categorical variables. While a full treatment of these is beyond the scope of this work, we provide some pointers to ways in which these could be included. Categorical variables are not trivially reparameterizable, but there has been a significant body of work that presents techniques to do this (Maddison et al., 2017; Hoogeboom et al., 2021), none of which provides an exact likelihood. Alternatively, the object classes could be treated as conditionally independent given the image and bounding box, in which case the likelihood is available, but this would neglect correlations between object classes other than those through the image and their bounding boxes.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Additional examples for the CLEVR6 dataset. The blue boxes show ground truth bounding boxes, while the green boxes are all samples from the learned conditional distribution.\n\n20",
    "reference": "# Summary Of The Paper\n\nThis work proposes a continuous normalizing flow model, an exact likelihood generative modeling method, which is conditional and permutation invariant.  The flow is driven by dynamics computed from a shared global force term and a pairwise interaction term which have shared weights across all elements thus achieving permutation equivariance in a similar manner to DeepSets or a message-passing NN over a fully connected graph.    The model is tested over two toy datasets involving box placement and more realistic datasets involving traffic scene generation and bounding box placement.  The dataset outperforms non-permutation invariant baselines in the traffic scene generation experiment in both NLL and domain relevant metrics.\n\n# Strength And Weaknesses\n\n## Strengths \n- The proposed method is simple and well-explained.  The need for permutation-invariance is well motived in the examples (allows generalization to different numbers of objects, avoids matching loss) and mathematically reasonable as inductive bias (it avoid makes arbitrary ordering decisions).  The addition of conditioning is also well motivated in the examples.  For example, in the traffic generation case, it is clear that map information will be relevant to the car location distribution.\n- The traffic scene experiment is pretty thorough including several non-equivariant baselines.  The results are convincing that permutation-equivariance is useful for the task.  Moreover, the results show good generalization over the number of cars.  The ablation shows the usefulness of the different global force term and pairwise interaction term and the analysis is insightful in noting the utility of each for staying on the road and not colliding.  I think the domain-relevant metrics are useful here. \n\n## Weaknesses\n- The effect of the efficient divergence computation in 2.2 and regularization in 2.3 are not evaluated in the experiments. \n- It would nice to have a permutation-equivariant baseline for the traffic scene generation task.  For example a non-flow based method. \n- It's not clear if the non-equivariant baselines in traffic scene generation are trained with ample data augmentation. \n- The bounding box prediction experiment has some potential issues.  There are no baseline comparisons here.  Only IOU is presented and with no variance.  Also, it is a bit of a strange task given that the target distribution is a point and so the fit distribution only represents uncertainty.  However, no serious evaluation of how well the model capture uncertainty (calibration, e.g.) is done.  On the plus side, this experiment does show the method can work with real world data. \n- It would be good to have some quantitative / qualitative evaluation on sample diversity and coverage.\n\n## Questions \n- The velocity term is simple, but seems quite constrained.  Are there potential issues with expressivity given this simple form imposed by equivariance and the fact the flow dynamics give a diffeomorphism of the space?  Even if theoretically expressive, could the flow dynamics have trouble fitting certain kinds of distributions from certain priors?\n- Can you say more the about the motivation to generate traffic scenes?   It's not completely clear to me how it is useful. \n- Can this method be applied to high-dimensional modeling with many objects or high-dimensional object features?  For example, could it be used for trajectory prediction with 100 objects? \n- Although this method is not specialized for it, could it be used for and compared to methods that generate point clouds or graphs? \n- For table 1 (b), I didn't fully understand the Data column.  What does \"6-4\" mean? \n\n## Minor Points \n- I would write $v_{\\theta,i}(x)$ on the line after equation (9). \n- Caption in Fig. 2 described the right two panels as \"to avoid the\". Should it be \"to bound the\"?\n- Page 6, Para 2, Line 5, typo \"flow flow\"\n- I liked the guessing game in figure 1.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nVery clear paper.  The method appears to be novel based on comparison to related work (I may not be aware of other related work.)  While most of the elements have appeared before (or very similar) the combination is new.  No code is provided, although the method and experiments are clearly described and most experiments are performed on publicly available data.\n\n# Summary Of The Review\n\nI felt the method was well-described, straightforward, and wholly suited to the a range of tasks where conditioning and permutation-invariance are useful properties.  The experimental results support the value of incorporating these features.  Overall, the weaknesses are largely minor and addressable.  (I'd probably opt for 7 if available, so I'll be conservative for now, but would be happy to upgrade my score if no serious concerns are raised.)\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nUNIVERSAL SPEECH ENHANCEMENT WITH SCOREBASED DIFFUSION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRemoving background noise from speech audio has been the subject of considerable effort, especially in recent years due to the rise of virtual communication and amateur recordings. Yet background noise is not the only unpleasant disturbance that can prevent intelligibility: reverb, clipping, codec artifacts, problematic equalization, limited bandwidth, or inconsistent loudness are equally disturbing and ubiquitous. In this work, we propose to consider the task of speech enhancement as a holistic endeavor, and present a universal speech enhancement system that tackles 55 different distortions at the same time. Our approach consists of a generative model that employs score-based diffusion, together with a multi-resolution conditioning network that performs enhancement with mixture density networks. We show that this approach significantly outperforms the state of the art in a subjective test performed by expert listeners. We also show that it achieves competitive objective scores with just 4–8 diffusion steps, despite not considering any particular strategy for fast sampling. We hope that both our methodology and technical contributions encourage researchers and practitioners to adopt a universal approach to speech enhancement, possibly framing it as a generative task.\n\n1\n\nINTRODUCTION\n\nReal-world recorded speech almost inevitably contains background noise, which can be unpleasant and prevent intelligibility. Removing background noise has traditionally been the objective of speech enhancement algorithms (Loizou, 2013). Since the 1940s (Kolmogorov, 1941; Wiener, 1949), a myriad of denoising approaches based on filtering have been proposed, with a focus on stationary noises. With the advent of deep learning, the task has been dominated by neural networks, often outperforming more classical algorithms and generalizing to multiple noise types (Lu et al., 2013; Pascual et al., 2017; Rethage et al., 2018; Défossez et al., 2020; Fu et al., 2021). Besides recent progress, speech denoising still presents room for improvement, especially when dealing with distribution shift or real-world recordings.\n\nNoise however is only one of the many potential disturbances that can be present in speech recordings. If recordings are performed in a closed room, reverberation is ubiquitous. With this in mind, a number of works have recently started to zoom out the focus in order to embrace more realistic situations and tackle noise and reverberation at the same time (Su et al., 2021; 2019; Polyak et al., 2021). Some of these works adopt a generation or re-generation strategy (Maiti & Mandel, 2019), in which a two-stage approach is employed to first enhance and then synthesize speech signals. Despite the relative success of this strategy, it is still an open question whether such approaches can perceptually outperform the purely supervised ones, especially in terms of realism and lack of voice artifacts.\n\nBesides noise and reverberation, a few works propose to go one step further by considering additional distortions. Pascual et al. (2019) introduce a broader notion of speech enhancement by considering whispered speech, bandwidth reduction, silent gaps, and clipping. More recently, Nair & Koishida (2021) consider silent gaps, clipping, and codec artifacts, and Zhang et al. (2021a) consider clipping and codec artifacts. In concurrent work, Liu et al. (2021) deal with bandwidth reduction and clipping in addition to noise and reverberation. Despite the recent efforts to go beyond pure denoising, we are not aware of any speech enhancement system that tackles more than 2–4 distortions at the same time.\n\nIn this work, we take a holistic approach and regard the task of speech enhancement as a universal endeavor. We believe that, for realistic speech enhancement situations, algorithms need not only face\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nand improve upon background noise and possibly reverberation, but also to correct a large number of typical but usually neglected distortions that are present in everyday recordings or amateur-produced audio, such as bandwidth reduction, clipping, codec artifacts, silent gaps, excessive dynamics compression/expansion, sub-optimal equalization, noise gating, and others (in total, we deal with 55 distortions, which can be grouped into 10 different families).\n\nOur solution relies on an end-to-end approach, in which a generator network synthesizes clean speech and a conditioner network informs of what to generate. The idea is that the generator learns from clean speech and both generator and conditioner have the capability of enhancing representations, with the latter undertaking the core part of this task. For the generator, we put together a number of known and less known advances in score-based diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020). For the conditioner, we develop a number of improved architectural choices, and further propose the usage of auxiliary, out-of-path mixture density networks for enhancement in both the feature and the waveform domains. We quantify the relative importance of these main development steps using objective metrics, and show how the final solution outperforms the state of the art in all considered distortions using a subjective test with expert listeners (objective metrics for the denoising task are also reported in the Appendix). Finally, we also study the number of diffusion steps needed for performing high-quality universal speech enhancement, and find it to be on par with the fastest diffusion-based neural vocoders without the need for any specific tuning.\n\n2 RELATED WORK\n\nOur approach is based on diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020). While diffusion models have been more extensively studied for unconditional or weaklyconditioned image generation, our work presents a number of techniques for strongly-conditioned speech re-generation or enhancement. Diffusion-based models achieve state-of-the-art quality on multiple generative tasks, in different domains. In the audio domain, they have been particularly successful in speech synthesis (Chen et al., 2021; Kong et al., 2021), text-to-speech (Jeong et al., 2021; Popov et al., 2021), bandwidth extension (Lee & Han, 2021), or drum sound synthesis (Rouard & Hadjeres, 2021). An introduction to diffusion models is given in Appendix A\n\nDiffusion-based models have also recently been used for speech denoising. Zhang et al. (2021a) expand the DiffWave vocoder (Kong et al., 2021) with a convolutional conditioner, and train that separately with an L1 loss for matching latent representations. Lu et al. (2021) study the potential of DiffWave with noisy mel band inputs for speech denoising and, later, Lu et al. (2022) and Welker et al. (2022) propose formulations of the diffusion process that can adapt to (non-Gaussian) real audio noises. These studies with speech denoising show improvement over the considered baselines, but do not reach the objective scores achieved by state-of-the-art approaches (see also Appendix E). Our work stems from the WaveGrad architecture (Chen et al., 2021), introduces a number of crucial modifications and additional concepts, and pioneers universal enhancement by tackling an unprecedented amount of distortions.\n\nThe state of the art for speech denoising and dereverberation is dominated by regression and adversarial approaches (Défossez et al., 2020; Fu et al., 2021; Su et al., 2021; Isik et al., 2020; Hao et al., 2021; Kim & Seo, 2021; Zheng et al., 2021; Kataria et al., 2021). However, if one considers further degradations of the signal like clipping, bandwidth removal, or silent gaps, it is intuitive to think that generative approaches have great potential (Polyak et al., 2021; Pascual et al., 2019; Zhang et al., 2021b), as such degradations require generating signal where, simply, there is none. Yet, to the best of our knowledge, this intuition has not been convincingly demonstrated through subjective tests involving human judgment. Our work sets a milestone in showing that a generative approach can outperform existing supervised and adversarial approaches when evaluated by expert listeners.\n\n3 DIFFUSION-BASED UNIVERSAL SPEECH ENHANCEMENT\n\n3.1 METHODOLOGY\n\nData — To train our model, we use a data set of clean and programmatically-distorted pairs of speech recordings. To obtain the clean speech, we sample 1,500 h of audio from an internal pool of data sets and convert it to 16 kHz mono. The speech sample consists of about 1.2 M utterances\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nof between 3.5 and 5.5 s, from thousands of speakers, in more than 10 languages, and with over 50 different recording conditions (clean speech is chosen to be the most dry possible and of high quality). To validate our model, we use 1.5 h of clean utterances sampled from VCTK (Yamagishi et al., 2019) and Harvard sentences (Henter et al., 2014), together with noises/backgrounds from DEMAND (Thiemann et al., 2013) and FSDnoisy18k (Fonseca et al., 2019). Train data does not overlap with the validation partition nor with other data used for evaluation or subjective testing.\n\nTo programmatically generate distorted speech, we consider 10 distortion families: band limiting, codec artifacts, signal distortion, loudness dynamics, equalization, recorded noise, reverb, spectral manipulation, synthetic noise, and transmission. Each family includes a variety of distortion algorithms, which we generically call ‘types’. For instance, types of synthetic noise include colored noise, electricity tones, non-stationary noise bursts, etc., types of codecs include OPUS, Vorbis, MP3, EAC3, etc., types of reverb include algorithmic reverb, delays, and both real and simulated room impulse responses, and so on. In total, we leverage 55 distortion types. Distortion type parameters such as strength, frequency, or gain are set randomly within reasonable bounds. A more comprehensive list of distortion families, types, and parameters can be found in Appendix C.\n\nEvaluation — To measure relative improvement, we use objective speech quality metrics reflecting different criteria. On the one hand, we employ speech enhancement metrics COVL (Loizou, 2013) and STOI (Taal et al., 2010), which are widely used for denoising or dereverberation tasks. On the other hand, we employ the codec quality metric WARP-Q (Jassim et al., 2021) and an internal semi-supervised metric imitating SESQA (Serrà et al., 2021b), which should perform well with generative algorithms with perceptually-valid outputs that do not necessarily perfectly align with the target signal. We also consider the composite measure COMP that results from normalizing the previous four metrics between 0 and 10 and taking the average.\n\nTo compare with the state of the art, we perform a subjective preference test. In it, listeners are presented with a reference distorted signal plus two enhanced versions of it: one by an existing approach and one by the proposed approach. Then, they are asked to choose which of the two enhanced signals they prefer, based on the presence of the original nuisance, voice distortion, audible artifacts, etc. The test was voluntarily performed by 22 expert listeners, each of them listening to randomly-chosen pairs of distorted and enhanced signals, taken from the online demo/example pages of 12 existing approaches, plus the corresponding version enhanced by our approach. Further details of our evaluation methodology are provided in Appendix D.\n\n3.2 BASE APPROACH\n\nScore-based diffusion — In this work, we use a variance exploding (VE) diffusion approach (Song et al., 2021). We train our score network S following a denoising score matching paradigm (Song & Ermon, 2019; 2020), using\n\nLSCORE = EtEzt\n\nEx0\n\n(cid:20) 1 2\n\n(cid:107)σtS(x0 + σtzt, c, σt) + zt(cid:107)2\n\n2\n\n(cid:21)\n\n,\n\n(1)\n\nwhere t ∼ U(0, 1), zt ∼ N (0, I), x0 ∼ pdata, c is the conditioning signal, and values σt follow a geometric noise schedule (Song & Ermon, 2019). In all our experiments we use σ0 = 5 · 10−4 and σ1 = 5, which we find sufficient for audio signals between −1 and 1 (cf. Song & Ermon, 2020). We consider different approaches to obtain c, but all of them have ̃x0, the distorted version of x0, as the main and only input (x0 and ̃x0 are both in the waveform domain).\n\nTo sample, we follow noise-consistent Langevin dynamics (Jolicoeur-Martineau et al., 2021), which corresponds to the recursion\n\nxtn−1 = xtn + ησ2\n\ntn\n\nS(xtn , c, σtn ) + βσtn−1ztn−1\n\nover N uniformly discretized time steps tn ∈ [0, 1], where we set η and β with the help of a hyperparameter (cid:15) ∈ [1, ∞). An extensive derivation of our approach to both training and sampling can be found in Appendix A, including an introduction to score-based diffusion.\n\nGeneral model description — We use convolutional blocks and a couple of bi-directional recurrent layers. Convolutional blocks are formed by three 1D convolutional layers, each one preceded by a multi-parametric ReLU (PReLU) activation, and all of them under a residual connection. If needed,\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Block diagram of the proposed approach. Individual blocks are depicted in Appendix B.\n\nup- or down-sampling is applied before or after the residual link, respectively (Appendix B). We perform up-/down-sampling with transposed/strided convolutions, halving/doubling the number of channels at every step. The down-sampling factors are {2,4,4,5}, which yield a 100 Hz latent representation for a 16 kHz input.\n\nThe model consists of a generator and a conditioning network (Fig. 1). The generator network is formed by a UNet-like structure with skip connections v and a gated recurrent unit (GRU) in the middle (Fig. 1-A). Convolutional blocks in the generator receive adaptor signals g, which inform the network about the noise level σ, and conditioning signals c, which provide the necessary speech cues for synthesis. Signals g and c are mixed with the UNet activations using FiLM (Perez et al., 2018) and summation, respectively. To obtain g, we process the logarithm of σ with random Fourier feature embeddings and an MLP as in Rouard & Hadjeres (2021) (see also Appendix B). The conditioning network processes the distorted signal ̃x with convolutional blocks featuring skip connections to a down-sampled latent that further exploits log-mel features extracted from ̃x (Fig. 1-B). The middle and decoding parts of the network are formed by a two-layer GRU and multiple convolutional blocks, with the decoder up-sampling the latent representation to provide multi-resolution conditionings c to the generator. Multiple heads and target information are exploited to improve the latent representation and provide a better c (see below). We call our approach UNIVERSE, for universal speech enhancer.\n\n3.3 DEVELOPING UNIVERSE\n\nIn the following, we explain all the steps we took to arrive at the above-mentioned structure, motivating each decision and quantifying its impact with objective metrics (schematic diagrams are depicted in Fig. 2). Unless stated otherwise, all models are trained with Adam and weight decay for 1 M iterations, with two-second long audio frames and a batch size of 32. We use a cosine learning rate schedule with a linear warm-up that spans the first 5 % of the iterations. Under these specifications,\n\n4\n\nConvConvBlockSigmaBlockConvBlock...GRU(1)ConvBlockConvBlock...PReLULinearConvConvBlockConvBlock...ConvBlockConvBlock...ConvBlockGRU(2)ConvBlockMDNConvBlockMel bandsConvBlockPReLUStConv......PReLUStConvA: Generator networkB: Conditioning networkLin...LinLin...LinLin...LinMDN...MDN...ConvUnder review as a conference paper at ICLR 2023\n\nFigure 2: Diagrams of the followed steps: L indicates auxiliary losses and + capacity increase.\n\ntraining a UNIVERSE model with 49 M parameters takes less than 5 days using two Tesla V100 GPUs and PyTorch’s native automatic mixed precision (Paszke et al., 2019).\n\nInitial baselines (B1–B3) — We start with a structure inspired by WaveGrad (Chen et al., 2021), featuring a UNet-like architecture with the aforementioned convolutional blocks, each with 64, 128, 256, and 512 channels (Sec. 3.2 and Appendix B). To condition we use the distorted speech ̃x, from which we compute a 100 Hz log-mel representation with 80 bands that is summed to the UNet latent after a linear projection (Fig. 2-B1). This approach turns out to lack enhancement capabilities, probably hampered by the coarse mel representation of ̃x. It also presents considerable speech distortion, probably due to the distortions in the mel representation (Table 1, B1).\n\nWe next consider an approach inspired by NU-Wave (Lee & Han, 2021). We employ the same convolutional UNet structure as before, but inject the conditioning signal ̃x at the input (Fig. 2-B2), concatenated with x + σz (see Eq. 1). We find that this approach yields less speech distortion and better scores than the previous one (Table 1, B2). However, after listening to some excepts, we notice that a lot of the audio is ‘bypassed’ from input to output. This leads to, for example, input noises present at the output, or silent gaps not being reconstructed. We hypothesize that the UNet’s skip connections are the culprit for this behavior (and removing them affected training in a dramatic way).\n\nAnother baseline approach we consider is inspired by ModDW (Zhang et al., 2021a). In it, following DiffWave (Kong et al., 2021), an auxiliary encoder and loss are used to learn a conditioning latent (Fig. 2-B3). This encoder uses the same blocks as the generative one, but with progressively downsampled skip connections. Also different from ModDW, we use mean squared error (MSE) instead of an L1 loss, and compare directly with the clean mel-band representation instead of a pre-learnt latent. With these two modifications, we find no problems in training the model end-to-end, in contrast to the original ModDW, which had to be trained using separate stages (Zhang et al., 2021a). The scores for this approach are in the middle of the previous baselines, closer to the second one (Table 1, B3).\n\nCapacity and losses (S1–S7) — With the three baselines above, we decide to add processing capacity in order to obtain a better conditioning signal. This decision was motivated by preliminary analysis showing that B1 and B3 were capable to synthesize high-quality speech in a vocoder setting (that is, when a clean mel-band conditioning was provided). Hence, with more capacity in the conditioning network, one should come closer to such clean mel-band scenario (an observation also made by Zhang et al. (2021a)). Unfortunately, increasing the capacity for the best scoring model, B2, could not avoid the bypass problems outlined above. Therefore, we focus on improving B3, which had the best scores after B2 and a similar listening quality, with less noise but more speech distortion.\n\nWe first add an extra log-mel input to the conditioner’s encoder, with a convolutional block after it, and sum its output to the skip connections coming from the distorted waveform processing (mels are extracted from the same distorted signal ̃x that is input to the waveform encoder, see Fig. 1-B). This\n\n5\n\nEncoderDecoderEncDecEncEncoderDecoderEnc+DecEnc+B1 B2 B3 S1,S2 S3 Enc+DecEnc+DecEnc+DecEnc+DecCNNCNNEnc+DecEnc+DecEnc+DecEnc+DecMDNMDNS4 S5 S6,S7,F,A1,A2 A3Enc+DecEnc+DecUnder review as a conference paper at ICLR 2023\n\nTable 1: Test set objective scores for the steps in developing UNIVERSE. Increments ∆ are calculated with respect to the COMP value of the previous row, except for A1–A3, which take S7 as reference.\n\nID Description\n\nCOVL ↑\n\nSTOI ↑ WARP-Q ↓\n\nSESQA ↑ COMP ↑\n\n∆ ↑\n\nB1 B2 B3\n\nInspired by WaveGrad Inspired by NU-Wave Inspired by ModDW\n\nExtra mel input S1 S2 Latent RNN+CNN S3 Auxiliary decoder & loss S4 Multi-resolution cond. S5 Out-of-path losses S6 Mixture density S7\n\nExtra latent targets\n\nF\n\nScaling parameters & iterations\n\nA1 Two-stage training A2 No SigmaBlock A3 No auxiliary losses\n\n1.55 2.02 1.76\n\n1.86 1.99 2.37 2.53 2.58 2.75 2.73\n\n3.12\n\n2.63 2.53 2.55\n\n0.755 0.829 0.811\n\n0.820 0.844 0.880 0.888 0.893 0.909 0.909\n\n0.930\n\n0.904 0.897 0.872\n\n0.952 0.873 0.902\n\n0.872 0.834 0.830 0.811 0.787 0.748 0.751\n\n0.679\n\n0.783 0.816 0.767\n\n4.90 5.57 5.35\n\n5.51 5.41 5.81 6.18 6.29 6.47 6.52\n\n6.82\n\n6.35 6.42 6.27\n\n3.77 4.69 4.34\n\n4.54 4.75 5.25 5.54 5.66 5.95 5.96\n\n6.50\n\n5.76 5.64 5.59\n\n+0.20 +0.21 +0.50 +0.29 +0.12 +0.29 +0.01\n\n+0.54\n\n−0.20 −0.32 −0.37\n\nalready results in an improvement with respect to B3, with objective scores that are very close to B2 (Table 1, S1). Next, we add a two-layer GRU and two convolutional blocks after the skip summation. We also add a one-layer GRU to the generator latent. Both GRUs should provide the model with a larger receptive field and better sequential processing capabilities. With this additions, the model increases the COMP score to 4.75 (Table 1, S2).\n\nAfter improving the encoders’ capacity, we decide to add a waveform loss to the existing MSE on log-mel latents. To do so, we also need to include additional decoder blocks for up-sampling the latent (Fig. 2-S3). For the waveform loss, we use MSE together with a multi-resolution STFT loss (Yamamoto et al., 2020). The result is a noticeable improvement (Table 1, S3). Although this is the largest improvement in objective scores, we do not observe such a large difference throughout informal listening. Better low-level details are present, but we suspect a large part of the score improvement is due to the objective measures paying too much attention to (sometimes irrelevant) low-level detail, which is now induced by the losses in the waveform domain.\n\nNow that we have parallel up-sampling blocks in the generator and conditioning decoders, we can condition at multiple resolutions between 100 Hz and 16 kHz (Fig. 2-S4). This provides an improvement (Table 1, S4). However, after careful listening, we have the impression that loss errors have a strong effect on the conditioning, provoking alterations that difficult the task of the generator (for instance, we hear a bit of muffled speech, presumably resulting from using MSE in both mel-band and waveform domains). To try to alleviate these issues, we decouple the loss calculations from the main signal path of the conditioning network. To do so, we use two convolutional heads (Fig. 2-S5), which include layer normalization, PReLUs, and a convolution with a kernel width of 3. With that, we observe an improvement not only in objective metrics (Table 1, S5), but also in subjective quality.\n\nDecoupled auxiliary heads allow us to further think of alternative loss functions that might work better for latent and waveform representations. Interestingly, such loss functions do not need to be restricted to regression or adversarial approaches, as used in the literature, but can be probabilistic and model the representations’ distribution explicitly. Now that losses are out of the signal path, nothing prevents us from using approaches that otherwise would require to sample from a probabilistic prediction in order to proceed with the signal flow. With the idea of better modeling the distribution of both log-mels and waveforms, we employ a mixture density network (MDN) approach (Bishop, 1994), with the same head architecture as before, but with 3 Gaussian components, each calculated from different convolutional layers (Fig. 2-S6). Assuming independent time steps, the negative log-likelihood of each time step using k components of dimensionality d and diagonal covariance is\n\nLMDN = − ln\n\n\n\n\n\nk (cid:88)\n\ni=1\n\nα(i) (2π)d/2 (cid:81)d\n\nj=1 s(i)\n\nj\n\nexp\n\n \n\n\n\n−\n\n1 2\n\nd (cid:88)\n\nj=1\n\n6\n\n(cid:32) yj − m(i) s(i)\n\nj\n\nj\n\n\n\n ,\n\n(cid:33)2 \n\n\n\nUnder review as a conference paper at ICLR 2023\n\nwhere y is the target representation and α(i), m(i), and s(i) denote the output of the convolutional layer corresponding to the mixing probability, the mean, and the standard deviation of the i-th Gaussian, respectively. Replacing standard losses by MDNs has a positive effect on the objective scores (Table 1, S6). Together with moving the loss computation out of the signal path (S5), they provide a COMP increment of 0.41, the largest one besides using an auxiliary decoder and loss (S3).\n\nAfter introducing the MDNs, our last step consists of including more targets in the latent predictions. These additional targets are pitch and harmonicity, as provided by crepe (Kim et al., 2018), voice activity detection (VAD) and loudness, provided by simple internal algorithms, and the deltas of all of them. We consider a separate MDN for each target type. That is, one for mel bands and their deltas, one for pitch/harmonicity and their deltas, and one for VAD/loudness and their deltas. The result only provides a marginal improvement in objective scores (Table 1, S7), but we find such improvement to be consistent after listening to a number of enhanced signals.\n\nScaling up (F) — Finally, after defining our base model, we can train a larger model for a longer time. We increase the model size by doubling the number of channels and we reduce the learning rate by two. The resulting model has 189 M parameters, and is trained for 2.5 M iterations using a batch size of 64. This takes less than 14 days using 8 Tesla V100 GPUs. The result of scaling up parameters and training is a large improvement, both objectively and subjectively (Table 1, F). This is the model we will use in our final evaluation, with 50 diffusion steps (see Sec. 4.3).\n\nFurther ablations (A1–A3) — Starting with S7, we can further assess the effect of some design choices. For instance, it is common in speech enhancement to train multi-part models using multiple stages, or taking some pre-trained (frozen) parts (Polyak et al., 2021; Maiti & Mandel, 2019; Nair & Koishida, 2021; Liu et al., 2021). The equivalent of this strategy for our two-part model would be to first train the conditioner using all LMDN losses, freeze it, and then train the generator with LSCORE. An intuition for this could be that, this way, the generator network is ‘decoupled’ from the enhancement/conditioner one, and that therefore the two networks can fully concentrate in their commitment (generating and cleaning, respectively). Nonetheless, this intuition seems to be at least partially wrong, as we obtain worse scores (Table 1, A1). In fact, this result points towards a ‘coupling’ situation, in which part of the generator performs some enhancement and part of the conditioner shapes the generator input.\n\nAnother design choice we can question is the strategy to provide the diffusion model with the noise level σ (SigmaBlock, Fig. 1). This strategy is used by some audio generation models (like CRASH by Rouard & Hadjeres (2021), from which we borrow it), but other models employ other strategies or none. We observe that the use of this strategy is important for our generator network (Table 1, A2). Finally, we can also quantify the effect of the additional losses LMDN. By removing them and training the whole model only with LSCORE, we observe a clear decrease in objective scores (Table 1, A3). Thus, we conclude that both auxiliary noise levels and losses are important.\n\n4 RESULTS\n\n4.1 COMPARISON WITH EXISTING APPROACHES\n\nTo compare with the state of the art, a common approach is to use objective metrics and wellestablished test sets. However, since the task of universal speech enhancement has not been formally addressed before, an established test set does not exist. Furthermore, it is not yet clear if common objective metrics provide a reasonable measurement for the enhancement of other distortions beyond additive noise and reverberation. An alternative is to evaluate on separate, individual test sets, each of them focused on a particular task (for example denoising, declipping, codec artifact removal, and so on). However, to the best of our knowledge, there do not exist well-established test sets nor metrics for other tasks beyond denoising. Therefore, to be the most fair possible to existing approaches, and in order to skip potentially flawed objective metrics, we think the best approach is to conduct a subjective test with expert listeners (Sec. 3.1 and Appendix D). Nonetheless, in Appendix E we also report objective scores for the two most common denoising test sets and show that the proposed approach achieves competitive results despite being generative and not favored by objective metrics.\n\nIn our subjective test, we compare against 12 existing approaches on different combinations of enhancement tasks (Table 2). We find that UNIVERSE outperforms all existing approaches by a\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Preference test results, including an indication of the model class (regression, adversarial, generative) and the considered distortions (noise, reverb, bandwidth reduction, clipping, codec artifacts, and others). Subjects’ preference (other/existing approach or UNIVERSE) is shown on the right, together with statistical significance (cid:63) (binomial test, p < 0.05, Holm-Bonferroni adjustment).\n\nApproach\n\nClass\n\nDistortions\n\nPreference (%)\n\nNoise Rev BW Clip Codec Others\n\nOther\n\nUNIVERSE\n\nDemucs (Défossez et al., 2020) MetricGAN+ (Fu et al., 2021) PERL-AE (Kataria et al., 2021) Speech Reg. (Polyak et al., 2021) SPEC-GAN (Su et al., 2019) HiFi-GAN-2 (Su et al., 2021) WSRGlow (Zhang et al., 2021b) SEANet (Li et al., 2021b) GSEGAN (Pascual et al., 2019) DNN-S (Mack & Habets, 2019) CT+TF-UNet (Nair & Koishida, 2021) VoiceFixer (Liu et al., 2021)\n\nUNIVERSE UNIVERSE-Regress UNIVERSE-Denoise Ground truth oracle\n\nReg Adv Reg Gen Adv Adv Gen Adv Gen Reg Reg Gen\n\nGen Reg Gen n/a\n\n(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\n\n(cid:88)\n\n(cid:88) (cid:88) (cid:88) (cid:88)\n\n(cid:88) (cid:88) (cid:88)\n\n(cid:88)\n\n(cid:88) (cid:88)\n\n(cid:88)\n\n(cid:88) (cid:88) (cid:88)\n\n(cid:88)\n\n(cid:88) (cid:88)\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\n(cid:88) (cid:88)\n\n(cid:88)\n\n(cid:88) (cid:88) (cid:88) (cid:88)\n\n(cid:88) (cid:88)\n\n(cid:88)\n\n(cid:88)\n\n(cid:88) (cid:88)\n\n(cid:88)\n\n2.3 2.3 4.5 4.5 6.8 22.7 6.8 27.3 0.0 2.3 4.5 29.5\n\nn/a 11.4 43.2 86.4 (cid:63)\n\n97.7 (cid:63) 97.7 (cid:63) 95.5 (cid:63) 95.5 (cid:63) 93.2 (cid:63) 77.3 (cid:63) 93.2 (cid:63) 72.7 (cid:63) 100.0 (cid:63) 92.7 (cid:63) 95.5 (cid:63) 70.5 (cid:63)\n\nn/a 88.6 (cid:63) 56.8 13.6\n\nsignificant margin (Table 2, top). In all considered distortions, UNIVERSE is preferred by expert listeners when compared to the corresponding competitor. The closest competitors are HiFi-GAN-2, which considers denoising, dereverb, and equalization, SEANet, which only considers bandwidth extension, and VoiceFixer, which considers denoising, dereverb, bandwidth extension, and declipping. We encourage the reader to listen to the UNIVERSE-enhanced examples in the companion website1.\n\n4.2 VARIATIONS AND INSIGHTS\n\nAnother interesting set of results stems from comparing against ablations or ground truth data (Table 2, bottom). In particular, we try to answer the following questions:\n\n1. Do we find a clear gain from using a generative, diffusion-based approach compared to using classical regression losses? To answer this question, we performed several preliminary experiments with different regression-based alternatives, and concluded that the best candidate for the subjective test was a version of UNIVERSE with exactly the same configuration and capacity which, instead of a score matching loss for the generator network and diffusion-based sampling, uses MSE and STFT losses for direct waveform prediction (the conditioner network was still found to be superior with the MDN losses). The result of this approach was not at the level of the generative version (Table 2, UNIVERSE-Regress), especially with regard to speech distortion and artifacts. Listeners preferred the generative version 88 % of the time.\n\n2. Can it be a problem for the model to consider more distortions beyond additive noise? To assess this, we trained exactly the same version of UNIVERSE with a 500 h train set that consisted of only additive noise mixtures, and evaluated only in the additive noise case. The reason for using 1/3 of the hours used for the universal enhancement case is that we estimate that this number is not far from the amount of real-world additive noise in the full multi-distortion train set. In this case, the results indicate that adding extra data and extra distortions does not affect performance (Table 2, UNIVERSE-Denoise). In fact, there seems to be a slight advantage in doing so (43 vs. 57 % preference), albeit not a significant one.\n\n3. How far are we from the ideal targets? To gain intuition about this question, we included recordings from the target speech to the subjective test (with input references featuring all considered distortions). In this case, listeners preferred the ideal targets 86 % of the time (Table 2, Ground truth oracle). This, beyond confirming that listeners were able to spot the clean\n\n1Link removed to preserve anonymity. Examples are attached to the current submission in a ZIP file. To foster future comparison, we will also provide the first (random) 100 pairs of our validation set on the website.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Speed-quality trade-off when varying sampling parameters. Speed is measured by the real-time factor on a Tesla V100 GPU (RTF; gray dashed line) and quality is measured by COMP (colored solid lines). Sampling parameters are the number of iterations N and the constant (cid:15) (Sec. 3.2). Results for the other objective metrics are in Appendix E.\n\nreferences, indicates that there is still some room for improvement for UNIVERSE. Informal listening by the authors indicates that two of the most problematic cases are with very loud noises and strong (and usually long) reverbs. The former tends to yield some babbling while the latter yields noticeable speech distortions and, in some cases, also babbling.\n\n4.3 SPEED-QUALITY TRADEOFF\n\nScore-based diffusion models require performing multiple denoising steps for high-quality sampling, and efficient strategies to tackle this issue have been the subject of recent research. In the literature, we find that high-quality sampling can be obtained with relatively few steps for tasks that have a rich conditioning signal such as speech vocoding (for example, less than 10 steps as in Chen et al. (2021); Kong et al. (2021)). Speech enhancement, as formulated here, should also be considered a task with a rich conditioning signal (essentially, low-level speech details are contained at the input, except for the distorted parts). Therefore, we hypothesize that high-quality synthesis can also be achieved with relatively few steps. Our results confirm this hypothesis and show that we can obtain good quality synthesis with as few as 4–8 denoising steps, with a speed above 10 times real-time on GPU (Fig. 3 and Appendix E). Importantly, this holds for a variety of values of (cid:15), and without the use of any specific strategy nor any search for an appropriate schedule of σt (we use plain geometric scheduling).\n\n5 CONCLUSION\n\nIn this work, we consider the task of speech enhancement as a holistic endeavor, and propose a universal speech enhancer that makes use of score-based diffusion for generation and MDN auxiliary heads for conditioning. We show that this approach, UNIVERSE, outperforms 12 state-of-the-art approaches as evaluated by expert listeners in a subjective test, and that it can achieve high-quality enhancement with only 4–8 diffusion steps. Regarding the potential societal impact, we do not foresee any serious implications at this initial research stage. Despite being a generative model, the nature of the task enforces to enhance what is being input to the system without changing major characteristics, keeping content and intent absolutely unaltered. Thus, for example, the system should not change the speaker’s identity or words unless attacked by a third party (we explicitly ask expert listeners to consider identity/word changes in their judgment). Finally, it is also worth noting that data-driven models highly depend on the training data characteristics. Accordingly, before deploying such models, one should ensure that target languages and use cases are sufficiently represented in the train set (we explicitly include multiple languages and recording conditions in our training set).\n\n9\n\n248163264128256512N234567COMP=1.3=1.7=2.3=3.10.050.10.20.512510RTFUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nB. D. O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their\n\nApplications, 12:313–323, 1982. 15\n\nC. M. Bishop. Mixture density networks. Technical report, Aston University, UK, 1994. 6, 18\n\nO. Chang, D. N. Tran, and K. Koishida. Single-channel speech enhancement using learnable loss mixup. In Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 2696–2700, 2021. 23\n\nN. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan. WaveGrad: estimating gradients for waveform generation. In Proc. of the Int. Conf. on Learning Representations (ICLR), 2021. 2, 5, 9, 16\n\nF. Dang, H. Chen, and P. Zhang. DPT-FSNet: dual-path transformer based full-band and sub-band fusion network for speech enhancement. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2022. 23\n\nA. Défossez, G. Synnaeve, and Y. Adi. Real time speech enhancement in the waveform domain. In Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 3291–3295, 2020. 1, 2, 8, 23\n\nR. Fejgin, J. Klejsa, L. Villemoes, and C. Zhou. Source coding of audio signals with a generative model. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 341–345, 2020. 23\n\nE. Fonseca, M. Plakal, D. P. W. E. Ellis, F. Font, X. Favory, and X. Serra. Learning sound event classifiers from web audio with noisy labels. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 21–25, 2019. URL http://www.eduardofonseca.net/ FSDnoisy18k/. 3\n\nS.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, X. Lu, and Y. Tsao. MetricGAN+: an improved version of MetricGAN for speech enhancement. In Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 201–205, 2021. 1, 2, 8, 23\n\nX. Hao, X. Su, R. Horaud, and X. Li. FullSubNet: a full-band and sub-band fusion model for real-time single-channel speech enhancement. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 6633–6637, 2021. 2, 23\n\nG. E. Henter, T. Merritt, M. Shannon, C. Mayo, and S. King. Repeated Harvard sentence prompts corpus version 0.5. Technical report, University of Edinburgh, The Centre for Speech and Technology Research (CSTR), 2014. URL https://doi.org/10.7488/ds/39. 3\n\nJ. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 6840–6851. Curran Associates, Inc., 2020. 2, 15\n\nA. Hyvärinen. Estimation of non-normalized statistical models by score matching. Journal of\n\nMachine Learning Research, 6(24):695–709, 2005. ISSN 1532-4435. 15\n\nU. Isik, R. Giri, N. Phansalkar, J.-M. Valin, K. Helwani, and A. Krishnaswamy. PoCoNet: better speech enhancement with frequency-positional embeddings, semi-supervised conversational data, and biased loss. In Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 2487–2491, 2020. 2, 23\n\nW. A. Jassim, J. Skoglund, M. Chinen, and A. Hines. Warp-Q: quality prediction for generative neural speech codecs. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 401–405, 2021. 3, 21, 23\n\nM. Jeong, H. Kim, S. J. Cheon, B. J. Choi, and N. S. Kim. Diff-TTS: a denoising diffusion model for text-to-speech. In Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 3605–3609, 2021. 2\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nA. Jolicoeur-Martineau, R. Piché-Taillefer, R. T. des Combes, and I. Mitliagkas. Adversarial score matching and improved sampling for image generation. In Proc. of the Int. Conf. on Learning Representations (ICLR), 2021. 3, 15, 16\n\nS. Kataria, J. Villalba, and N. Dehak. Perceptual loss based speech denoising with an ensemble of audio pattern recognition and self-supervised models. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 7118–7122, 2021. 2, 8, 23\n\nE. Kim and H. Seo. SE-Conformer: time-domain speech enhancement using conformer. In Proc. of\n\nthe Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 2736–2740, 2021. 2, 23\n\nJ. W. Kim, J. Salamon, P. Li, and J. P. Bello. Crepe: a convolutional representation for pitch estimation. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 161–165, 2018. 7\n\nA. Kolmogorov. Interpolation and extrapolation of stationary random sequences. Izv. Akad. Nauk\n\nSSSR Ser. Mat., 5:3–14, 1941. 1\n\nZ. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro. DiffWave: a versatile diffusion model for audio synthesis. In Proc. of the Int. Conf. on Learning Representations (ICLR), 2021. 2, 5, 9\n\nJ. Lee and S. Han. NU-Wave: a diffusion probabilistic model for neural audio upsampling.\n\narXiv:2104.02321, 2021. 2, 5\n\nA. Li, W. Liu, C. Zheng, C. Fan, and X. Li. Two heads are better than one: a two-stage complex spectral mapping approach for monaural speech enhancement. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:1829–1843, 2021a. 23\n\nY. Li, M. Tagliasacchi, O. Rybakov, V. Ungureanu, and D. Roblek. Real-time speech frequency bandwidth extension. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 691–695, 2021b. 8\n\nH. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and Y. Wang. VoiceFixer: toward general\n\nspeech restoration with neural vocoder. ArXiv: 2109.13731, 2021. 1, 7, 8\n\nP. C. Loizou. Speech enhancement. CRC Press, 2013. 1, 3, 21\n\nX. Lu, Y. Tsao, S. Matsuda, and C. Hori. Speech enhancement based on deep denoising autoencoder.\n\nIn Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 436–440, 2013. 1\n\nY.-J. Lu, Y. Tsao, and S. Watanabe. A study on speech enhancement based on diffusion probabilistic model. In Proc. of Asia Pacific Signal and Information Proc. Assoc. Annual Submit and Conf. (APSIPA), 2021. 2, 23\n\nY.-J. Lu, Z.-Q. Wang, S. Watanabe, A. Richard, C. Yu, and Y. Tsao. Conditional diffusion probabilistic\n\nmodel for speech enhancement. ArXiv: 2202.05256, 2022. 2, 23\n\nS. Lv, Y. Hu, S. Zhang, and L. Xie. DCCRN+: channel-wise subband DCCRN with SNR estimation for speech enhancement. In Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 2816–2820, 2021. 23\n\nW. Mack and E. A. P. Habets. Declipping speech using deep filtering. In Proc. of the IEEE Workshop\n\non Appl. of Signal Proc. to Audio and Acoustics (WASPAA), pp. 200–204, 2019. 8\n\nS. Maiti and M. I. Mandel. Parametric resynthesis with neural vocoders. In Proc. of the IEEE Workshop on Appl. of Signal Proc. to Audio and Acoustics (WASPAA), pp. 303–307, 2019. 1, 7\n\nS. Maiti and M. I. Mandel. Speaker independence of neural vocoders and their effect on parametric resynthesis speech enhancement. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 206–210, 2020. 23\n\nA. A. Nair and K. Koishida. Cascaded time + time-frequency UNet for speech enhancement: jointly addressing clipping, codec distortions, and gaps. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 7153–7157, 2021. 1, 7, 8\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nS. Pascual, A. Bonafonte, and J. Serrà. SEGAN: speech enhancement generative adversarial network. In Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 3642–3646, 2017. 1, 23\n\nS. Pascual, J. Serrà, and A. Bonafonte. Towards generalized speech enhancement with generative adversarial networks. In Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 161–165, 2019. 1, 2, 8\n\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: an imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pp. 8024–8035. Curran Associates, Inc., 2019. 5, 17\n\nE. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. Courville. FiLM: visual reasoning with a general conditioning layer. In Proc. of the AAAI Conf. on Artificial Intelligence (AAAI), volume 32, pp. 3942–3951, 2018. 4, 18\n\nH. Phan, I. V. McLoughlin, L. Pham, O. Y. Chén, P. Koch, M. de Vos, and A. Mertins. Improving\n\nGANs for speech enhancement. IEEE Signal Processing Letters, 27:1700–1704, 2020. 23\n\nA. Polyak, L. Wolf, Y. Adi, O. Kabeli, and Y. Taigman. High fidelity speech regeneration with application to speech enhancement. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 7143–7147, 2021. 1, 2, 7, 8\n\nJ. Pons, S. Pascual, G. Cengarle, and J. Serrà. Upsampling artifacts in neural audio synthesis. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 3005–3009, 2021. 17\n\nV. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov. Grad-TTS: a diffusion probabilistic\n\nmodel for text-to-speech. arXiv:2105.06337, 2021. 2\n\nC. K. A. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, P. Rana, S. Srinivasan, and J. Gehrke. The INTERSPEECH 2020 deep noise suppression challenge: datasets, subjective testing framework, and challenge results. In Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 2492–2496, 2020. URL https:// github.com/microsoft/DNS-Challenge/tree/interspeech2020/master. 22\n\nD. Rethage, J. Pons, and X. Serra. A WaveNet for speech denoising. In Proc. of the IEEE Int. Conf.\n\non Acoustics, Speech and Signal Processing (ICASSP), pp. 5069–5073, 2018. 1\n\nA. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra. Perceptual evaluation of speech quality (PESQ) – A new method for speech quality assessment of telephone networks and codecs. In IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), volume 2, pp. 749–752, 2001. 21\n\nS. Rouard and G. Hadjeres. CRASH: raw audio score-based generative modeling for controllable high-resolution drum sound synthesis. In Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR), pp. 579–585, 2021. 2, 4, 7\n\nS. Routray and Q. Mao. Phase sensitive masking-based single channel speech enhancement using conditional generative adversarial network. Computer Speech & Language, 71:101270, 2022. 23\n\nJ. Serrà, S. Pascual, and J. Pons. On tuning consistent annealed sampling for denoising score matching.\n\narXiv: 2104.03725, 2021a. 16\n\nJ. Serrà, J. Pons, and S. Pascual. SESQA: semi-supervised learning for speech quality assessment. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 381–385, 2021b. 3, 21\n\nJ. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proc. of the Int. Conf. on Machine learning (ICML), pp. 2256–2265, 2015. 2\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nY. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pp. 11895–11907. Curran Associates, Inc., 2019. 2, 3, 15, 16\n\nY. Song and S. Ermon. Improved techniques for training score-based generative models. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 12438–12448. Curran Associates, Inc., 2020. 3, 15, 16\n\nY. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In Proc. of the Int. Conf. on Learning Representations (ICLR), 2021. 3, 15, 16\n\nM. Strauss and B. Edler. A flow-based neural network for time domain speech enhancement. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 5754–5758, 2021. 23\n\nJ. Su, A. Finkelstein, and Z. Jin. Perceptually-motivated environment-specific speech enhancement. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 7015–7019, 2019. 1, 8\n\nJ. Su, Z. Jin, and A. Finkelstein. HiFi-GAN-2: studio-quality speech enhancement via generative adversarial networks conditioned on acoustic features. In Proc. of the IEEE Workshop on Appl. of Signal Proc. to Audio and Acoustics (WASPAA), 2021. 1, 2, 8, 23\n\nS. Särkkä and A. Solin. Applied stochastic differential equations, volume 10. Cambridge University\n\nPress, 2019. 15\n\nC. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen. A short-time objective intelligibility measure for time-frequency weighted noisy speech. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 4214–4217, 2010. 3, 21\n\nJ. Thiemann, N. Ito, and E. Vincent. DEMAND: a collection of multi-channel recordings of acoustic noise in diverse environments (1.0). In Proc. of the Int. Congress on Acoustics (ICA), 2013. URL https://zenodo.org/record/1227121. 3\n\nC. Valentini-Botinhao. Noisy speech database for training speech enhancement algorithms and TTS models. Technical report, University of Edinburgh, The Centre for Speech and Technology Research (CSTR), 2017. URL https://datashare.ed.ac.uk/handle/10283/2791. 22\n\nP. Vincent. A connection between score matching and denoising autoencoders. Neural Computation,\n\n23(7):1661–1674, 2011. 15, 16\n\nS. Welker, J. Richter, and T. Gerkmann. Speech enhancement with score-based generative models in\n\nthe complex STFT domain. ArXiv: 2203.17004, 2022. 2\n\nN. Wiener.\n\nInterpolation, extrapolation, and smoothing of stationary time series. MIT Press,\n\nCambridge: USA, 1949. 1\n\nY. Yamagishi, C. Veaux, and K. MacDonald. CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice cloning toolkit (version 0.92). Technical report, University of Edinburgh, The Centre for Speech and Technology Research (CSTR), 2019. URL https://doi.org/10.7488/ds/ 2645. 3\n\nR. Yamamoto, E. Song, and J.-M. Kim. Parallel WaveGAN: a fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 6199–6203, 2020. 6\n\nG. Yu, Y. Wang, H. Wang, Q. Zhang, and C. Zheng. A two-stage complex network using cycleconsistent generative adversarial networks for speech enhancement. Speech Communication, 134: 42–54, 2021. 23\n\nJ. Zhang, S. Jayasuriya, and V. Berisha. Restoring degraded speech via a modified diffusion model. In Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 221–225, 2021a. 1, 2, 5\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nK. Zhang, Y. Ren, C. Xu, and Z. Zhao. WSRGlow: a Glow-based waveform generative model for audio super-resolution. In Proc. of the Int. Speech Comm. Assoc. Conf. (INTERSPEECH), pp. 1649–1653, 2021b. 2, 8\n\nC. Zheng, X. Peng, Y. Zhang, S. Srinivasan, and Y. Lu. Interactive speech and noise modeling for speech enhancement. In Proc. of the AAAI Conf. on Artificial Intelligence (AAAI), pp. 14549–14557, 2021. 2, 23\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA SCORE-BASED DIFFUSION MODELS\n\nA.1 THEORY\n\nDiffusion-based models are defined through a forward process where we progressively add noise to samples from the data distribution, x0 ∼ pdata, until we obtain a result that is indistinguishable from a prior tractable distribution, x1 ∼ pknown. In the case of Gaussian noise, pknown also becomes approximately Gaussian, and this process can be modeled (Song et al., 2021) through a stochastic differential equation (SDE):\n\ndx = f (x, t)dt + g(t)dw, (2) where f, g : R → R and w is the standard Wiener process (Brownian motion) indexed by a continuous time variable t ∈ [0, 1]. Different definitions of f and g yield to different but equivalent processes (Song et al., 2021). To model the backward process where we go from pknown to pdata, we can then employ the reverse-time SDE (Anderson, 1982)\n\ndx = (cid:2)f (x, t) − g(t)2∇x log pt(x)(cid:3) dt + g(t)d ̄w, where ̄w is the standard Wiener process in which time flows backward, and ∇x log pt(x) corresponds to the score of pt, the marginal distribution at time t (Hyvärinen, 2005). Thus, once f and g are defined, in order to obtain x0 ∼ pdata, we only need to know the score function ∇x log pt(x) to sample x1 ∼ pknown and simulate the process of Eq. 3.\n\n(3)\n\nSince one does not typically have direct access to neither pt nor ∇x log pt(x), the solution is to approximate the latter with a neural network S(x, t). In order to train S, Vincent (2011) showed that, for a given t, minimizing the score matching objective\n\nL = Ext\n\n(cid:20) 1 2\n\n(cid:107)S(xt, t) − ∇x log pt(x)(cid:107)2\n\n2\n\n(cid:21)\n\nis equivalent to minimizing the denoising objective\n\nL = Ext|x0\n\nEx0\n\n(cid:20) 1 2\n\n(cid:107)S(xt, t) − ∇x log pt(xt|x0)(cid:107)2\n\n2\n\n(cid:21)\n\n,\n\nwhere pt(xt|x0) corresponds to a Gaussian kernel (Song et al., 2021; Särkkä & Solin, 2019), the transition kernel for the forward SDE (Eq. 2). For all t, one can use the continuous generalization (Song & Ermon, 2019)\n\nL = EtExt|x0\n\nEx0\n\n(cid:107)S(xt, t) − ∇x log pt(xt|x0)(cid:107)2\n\n2\n\n(4)\n\n(cid:21)\n\n(cid:20) λt 2\n\nwith t ∼ U(0, 1), where λt is an appropriately chosen weight that depends on t (Ho et al., 2020; Song & Ermon, 2020).\n\nSampling with score-based diffusion models is done by simulating or solving the reverse-time SDE (Eq. 3) with a finite (discrete) time schedule. This can be done in many ways, for instance by using numerical SDE and ODE solvers, as introduced by Song et al. (2021). Other schemes that have shown competitive performance include predictor-corrector schemes (Song et al., 2021), ancestral sampling (Ho et al., 2020), and variations of Langevin dynamics (Song & Ermon, 2020; Jolicoeur-Martineau et al., 2021).\n\nA.2\n\nIN PRACTICE\n\nIn our work, we employ the so-called variance exploding schedule (VE; Song et al., 2021), which corresponds to choosing\n\nf (x, t) = 0\n\nand\n\ng(t) =\n\n(cid:114)\n\ndσ2(t) dt\n\n(5)\n\nin Eq. 2. Then, the associated transition kernel for the forward process (Särkkä & Solin, 2019) is pt(xt|x0) = N (cid:0)xt; x0, [σ2\n\n0]I(cid:1), which in practice is approximated by\n\nt − σ2\n\npt(xt|x0) ≈ N (cid:0)xt; x0, σ2\n\nt I(cid:1)\n\n(6)\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nsince σ0 → 0 (see also Song et al., 2021). The intuition is that pt=0 becomes indistinguishable from pdata, and that pt=1 becomes indistinguishable from pknown (a Gaussian distribution). In other words, perturbation and signal should become imperceptible at t = 0 and t = 1, respectively. In the VE schedule, the scale of the signal x0 is kept intact, and then it corresponds to g(t) to fulfill that notion through a variance schedule (see Eq. 5). Therefore, σ0 should be negligible while σ1 should be large, compared to the variability of x0. That is, σ2\n\n0 (cid:28) E[(x0 − E[x0])2] (cid:28) σ2 1.\n\nThe use of the approximated transition kernel (Eq. 6) implies that xt = x0 + σtzt, zt ∼ N (0, I), and that\n\n−∇x0 log pt(xt|x0) ≈ −∇x0\n\nC −\n\n(cid:20)\n\n(cid:21)\n\n(xt − x0)2 2σ2 t\n\n=\n\nxt − x0 σ2 t\n\n,\n\nwhere C is a constant (see also Vincent, 2011)). Substituting these into Eq. 4 and operating yields\n\nL = EtEzt\n\nEx0\n\n(cid:34)\n\nλt 2\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nS(x0 + σtzt, t) +\n\n(cid:35)\n\n.\n\nzt σt\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) 2\n\nFrom Eq. 5, it now remains to set how σ evolves. Song & Ermon (2019; 2020) choose a geometric progression for σt,\n\nσt = σmin\n\n(cid:18) σmax σmin\n\n(cid:19)t\n\n,\n\nand provide some generic guidance on how to select σmin and σmax. They also justify setting λ proportional to the noise variance at time t: λt = σ2\n\nt . Using this weighting yields\n\nL = EtEzt\n\nEx0\n\n(cid:20) 1 2\n\n(cid:107)σtS(x0 + σtzt, t) + zt(cid:107)2\n\n2\n\n(cid:21)\n\n.\n\nFinally, instead of using t directly as input for S, we use σt and train S with a continuum of noise scales (Chen et al., 2021). In addition, we need to use some conditioning information c as an indication of what to generate (in our case, a signal derived from the distorted speech ̃x). With that, our training loss becomes\n\nL = EtEzt\n\nEx0\n\n(cid:20) 1 2\n\n(cid:107)σtS(x0 + σtzt, c, σt) + zt(cid:107)2\n\n2\n\n(cid:21)\n\n.\n\nThis is the loss denoted as LSCORE in the main paper.\n\nFor sampling with the VE schedule, we resort to consistent annealed sampling (Jolicoeur-Martineau et al., 2021). We use the discretization of t ∈ [0, 1] into N uniform steps tn = (n − 1)/(N − 1), n = {1, . . . N }, which implies discretized progressions for xt and σt. Starting at n = N , we initialize xtN = σtN ztN , and then recursively compute\n\nxtn−1 = xtn + ησ2\n\ntn\n\nS(xtn , c, σtn ) + βσtn−1 ztn−1,\n\nfor n = N, N − 1, . . . 2. Finally, at n = 1 (t1 = 0), we take\n\n ̄x0 = x0 + σ2\n\n0S(x0, c, σ0),\n\nwhich corresponds to the empirically denoised sample (Jolicoeur-Martineau et al., 2021). The values of η and β are determined through the parameterization (Serrà et al., 2021a)\n\nη = 1 − γ(cid:15),\n\nβ =\n\n1 −\n\n(cid:115)\n\n(cid:18) 1 − η γ\n\n(cid:19)2\n\n,\n\nwhere γ ∈ (0, 1) is the ratio of the geometric progression of the noise, γ = σtn /σtn+1, and the constant (cid:15) ∈ [1, ∞) is left as a hyper-parameter. Note that the value of γ changes with the chosen number of discretization steps N , thus facilitating hyper-parameter search for continuous noise scales at different N (Serrà et al., 2021a).\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nB IMPLEMENTATION DETAILS\n\nArchitecture overview — The architecture contains two main parts that are jointly trained: the generator network, which is tasked with estimating the score of the perturbed speech distributions, and the conditioner network, which creates the conditioning signal required for synthesizing speech with preserved speaker characteristics. Both the generator and the conditioner networks are essentially encoder-decoder architectures, enhanced with various conditioning signals and skip connections (see figure in main paper). The encoding and decoding is done by down- and up-sampling, respectively, using convolutional blocks, which are the main building blocks of UNIVERSE. Throughout the architecture, the multi-parametric rectified linear unit (PReLU) activation function is used prior to all the layers (except where we specify otherwise), and all convolutional layers are one dimensional. Unless stated otherwise, we use PyTorch’s defaults (Paszke et al., 2019, version 1.10.1).\n\nConvolutional block — The convolutional block consists of a core part which is common between all blocks, and an optional prior or posterior part that can exist depending on the block’s functionality (Fig. 4-B). The core part contains a convolutional layer of kernel width 5, followed by two convolutional layers of kernel width 3, and we use a residual sum between its input and output. All residual sums and skip connections are weighted by 1/\n\nr, where r is the number of elements in the sum.\n\n√\n\nIf the convolutional block will be used for up-sampling, the core’s input is processed initially by a transposed convolutional layer of kernel width and stride equal to the up-sampling ratio, without padding (Pons et al., 2021). On the contrary, if the block will be used for down-sampling, the output of the core is processed by a strided convolutional layer of kernel width and stride equal to the down-sampling ratio, also without padding. Every down-sampling operation is accompanied by a channel expansion of factor 2, and up-sampling by a reduction of 2. We start with 32 channels in the encoder and have 512 in the latent. Depending on a block’s position in the architecture we use skip connections and a maximum of two types of conditioning signals, all injected to a dedicated part in the convolutional block (Fig. 4-B). If a signal will be extracted with a skip connection, we take it from the output of the residual sum, and insert it to the target block before the residual signal.\n\nFigure 4: Diagram of the individual blocks not depicted in the main paper. Dashed connections and blocks are optional, depending on the functionality of the block.\n\n17\n\nPReLUTrConvPReLUConvFiLMPReLUConvPReLUConvPReLUStConvLinearPReLULinearPReLULinearPReLUA: SigmaBlockB: ConvBlockLNPReLUConvC: MDNRFFPReLUUnder review as a conference paper at ICLR 2023\n\nIn order to preserve speaker characteristics that are consistent throughout a frame, we use conditioning signals between the generator and the conditioner. These conditioning signals are taken from the output of the first convolutional layer of the core, and inserted to the same position in the target block’s core. Furthermore, a secondary conditioning signal containing information about the noise level is injected to all the convolutional blocks of the generator. This conditioning is done with FiLM (Perez et al., 2018).\n\nConditioner network — The conditioner network contains a stem cell, a down-sampling encoder, a recursive middle part, and an up-sampling decoder. Both the encoder and decoder are made up of convolutional blocks that perform only down-sampling and up-sampling, respectively. The distorted signal is down-sampled to a 100 Hz sampling rate, with the encoder following a {2,4,4,5} factor progression, recursively processed in the middle part, and up-sampled to the original sampling rate with the decoder, mirroring the encoder’s progression. At each encoder block, we use skip connections, process them with adaptor networks, and sum the adapted signals with the output of the last encoder block. The adaptor networks consist of strided convolutions in order to match the sampling rate of the summation inputs.\n\nAlongside this main path, we extract and normalize 80 mel band features from the distorted signal, process them with a convolutional layer followed by a convolutional block, and add the result to the summation of the down-sampled signals. The hop size is equal to the total down-sampling rate, preserving the sampling rate of the features during processing. We process the feature-enriched signal with the conditioner’s middle part, consisting of a convolutional block, a two-layer bi-directional GRU with a residual sum, and a final convolutional block, all of them preserving the sampling rate. We finally up-sample the latent representation to the original sampling rate with the decoder, using an initial convolutional block preserving the sampling rate and numerous blocks performing up-sampling. We obtain a hierarchical conditioning for the generator network by taking conditioning signals from each block of the decoder, corresponding to all the sampling rates. In order to adapt these representations to the ones of the generator network, we process each conditioning signal with linear layers.\n\nGenerator network — The generator network is also an encoder-decoder architecture with UNetlike skip connections, where there is a direct connection between each encoder and decoder block pair sharing the same sampling rate. We use the same down- and up-sampling progression as in the conditioner network, hence achieving parallel up-sampling blocks between the generator and conditioner decoders. However, we limit the middle part to only a single bi-directional GRU layer without residual sums. The output of the final block is processed by a convolutional layer to reduce its dimension back to the score’s dimension.\n\nThe generator is conditioned on two signals: Fourier features depending on the current noise level and the hierarchical conditioning extracted from the distorted speech. For the former, we embed the noise level with logarithmic compression and use it to modulate frequencies of random Fourier features (RFFs) sampled from a standard normal distribution. Following the extraction of RFFs, we process them with a fully connected network of 3 times repeated linear layers followed by PReLUs to obtain noise level embeddings. This block, which we call SigmaBlock, expands 32 pairs of Fourier coefficients into 256 channels. (Fig. 4-A). After the embeddings are extracted, they are adapted to each block’s frame rate and dimension with a linear projection layer. The conditioning signals taken from the conditioner network’s decoder are injected to the generator network’s decoder at each block with the same sampling rate, using summation. This way, the estimated score is conditioned with multiple sampling rates starting from the lowest (100 Hz), until the original (16 kHz).\n\nTraining objective — The main loss is the denoising score matching loss outlined in the main paper and derived in Appendix A.2. Additionally, we introduce auxiliary losses in order to structure the conditioner network’s latent representation and prime its final block’s output to perform speech enhancement. We use separate mixture density networks (MDN; Bishop, 1994) to model the distributions of the clean signal waveform and features. Each feature and its delta distribution is jointly estimated with an MDN that models the distribution using 3 multivariate Gaussian components with diagonal covariance. The parameters of the mixture components are learned with convolutional layers of kernel width 3 and layer normalization (Fig. 4-C). We calculate the negative log-likelihood (NLL) of the feature MDNs and average them. Then, the total objective is to minimize the sum of the denoising score matching loss, waveform NLL, and feature NLLs. Here, we would like to underline\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nthat auxiliary losses are generative, as they model continuous data distributions, making UNIVERSE a fully-generative model (that is, not using any classification or regression losses). Moreover, as the auxiliary losses are taken out of the main signal path, we do not need any sampling procedure neither in training nor in validation stages.\n\nOptimization — The resulting model contains about 49 M parameters. We train it with a batch size of 32, where each sample is an approximately two-seconds long frame with 16 kHz sampling rate, and using automatic mixed precision. We apply 1 M parameter updates, which takes approximately four and a half days using 2 Tesla V100 GPUs in parallel. The large/final model has 189 M parameters, is trained for 2.5 M iterations using a batch size of 64, and takes less than 14 days using 8 Tesla V100 GPUs. We use the Adam optimizer with a maximum learning rate of 2 · 10−4 and schedule the learning rate with a cosine scheduler. The schedule includes a warm-up of 50 k iterations, increasing the learning rate from a starting point 1.6 · 10−6. We additionally implement a manual weight decay, excluding biases and the PReLU weights, and set its coefficient to 0.01.\n\nC DATA AND DISTORTIONS\n\nTo create our training data, we sample chunks of 3.5–5.5 seconds from speech recordings of an internal pool of data sets, featuring multiple speakers, languages, prosody/emotions, and diverse recording conditions. Speech is pre-selected to be clean, but nonetheless can contain minimal background noise and reverb. We down-sample to 16 kHz mono and follow two different signal paths to create input and target pairs. With distortions that introduce delays we automatically time-align at the sample level using a custom correlation function.\n\nInput — To programmatically generate distorted versions, we randomly choose the number of distortions between {1, 2, 3, 4, 5}, with probabilities {0.35, 0.45, 0.15, 0.04, 0.01}, respectively. Next, we cascade distortion types with random parameters (bounds for distortion parameters are selected such that distortions are perceptually noticeable by expert listeners; random selection is typically uniform, except for parameters that have an intuitive logarithmic behavior, such as frequency, in which case we sample uniformly in logarithmic space). Table 3 summarizes the distortion families and types we consider, together with their weights (distortion type weights will define the probability of selecting each distortion type). In total, we consider 55 distortions, which we group into 10 families.\n\nThe ranges we consider for the distortion parameters include: SNRs between −5 and 25 dB for all added noises, slopes between 0.01 and 0.7 for colored noise, DC components between 10−6 and 10−1, non-stationary noise lengths between 20 and 350 ms, frequency ranges between 100 and 7500 Hz for filters and tones (except low and high pass, with a minimum and maximum of 1 kHz, respectively), Qs between 0.1 and 2 for filters, gains between −12 and 6 dB for filters and random equalization, between 2 and 20 mel bands for random equalization, gap lengths between 20 and 80 ms, temporal probabilities between 0.5 and 3 times/sec for transmission distortions, percentiles between 0.5 and 50 for clipping, resample frequencies of 4, 6, 8, 11, 12, and 14 kHz, wet proportions between 0.2 and 1 for reverbs, augmentation of early and tail reflections for room impulse responses, room sizes between 3 and 1000 m2 for algorithmic reverbs, bit rates between 2 and 96 kbps for codecs, compression ratios between 2 and 10, window lengths between 29 and 212 for spectral manipulations, amounts between 0.5 and 0.8 for spectral manipulation probability, wet proportions between 0.1 and 0.75 for other effects, and linear gains between 0 and 1 for plosive and sibilance addition.\n\nTarget — Given that the design of UNIVERSE does not rely on assumptions regarding the nature of the distortions (for instance, a good example would be the assumption that noise is additive), nothing prevents us from using a different target than the original signal that was used as input to the distortion chain. In addition, we want to ensure that the generated signal is of the top/highest quality, which is a characteristic that is not shared by some of the original clean speech signals that we sample. Therefore, we decide to apply a small enhancement chain to clean the signals to be used as target. In particular, we apply four consecutive steps2:\n\n1. Denoising — We employ a denoiser to remove any (minimal) amount of noise that might be present in the recording. Since the pre-selected speech is already of good quality, we expect that\n\n2All of these algorithms are available at https://dolby.io/\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Considered distortion types, grouped by family.\n\nID Family\n\nType\n\nWeight Randomized parameters\n\nBand limiting\n\nCodec\n\n1 2\n3 4\n\n5 6\n7 8\n9 10 11 12 13\n\n14 Distortion 15 16 17\n\nLoudness dynamics\n\nEqualization\n\n18 19 20 21 22 23\n\n24 25 26\n\nBand pass filter High pass filter Low pass filter Down-sample\n\nAC3 codec EAC3 codec MDCT codec MP2 codec MP3 codec Mu-law quantization OGG/Vorbis codec OPUS codec 1 OPUS codec 2\n\nMore plosiveness More sibilance Overdrive Threshold clipping\n\nCompressor Destroy levels Noise gating Simple compressor Simple expansor Tremolo\n\nBand reject filter Random equalizer Two-pole filter\n\n27 Recorded noise\n\nAdditive noise\n\n28\n\n29 Reverb/delay 30 31 32 33 34\n\nImpulsional additive noise\n\nAlgorithmic reverb 1 Algorithmic reverb 2 Chorus Phaser RIR convolution Very short delay\n\n35 36 37 38 39 40\n\n41 42 43 44 45 46 47 48\n\n49 50 51 52 53 54 55\n\nSpectral manipulation Convolved spectrogram\n\nSynthetic noise\n\nTransmission\n\nGriffin-Lim Phase randomization Phase shuffle Spectral holes Spectral noise\n\nColored noise DC component Electricity tone Non-stationary colored noise Non-stationary DC component Non-stationary electricity tone Non-stationary random tone Random tone\n\nFrame shuffle Insert attenuation Insert noise Perturb amplitude Sample duplicate Silent gap (packet loss) Telephonic speech\n\n5 5\n20 30\n\nFrequencies, filter characteristics. Frequency, filter characteristics. Frequency, filter characteristics. Frequency, method.\n\n2 Bit rate, codec configuration. 3 Bit rate, codec configuration. 15 Bit rate, codec configuration. 5 Bit rate, codec configuration. 20 Bit rate, codec configuration.\n\n3 Mu. 3 Bit rate, codec configuration. 15 Bit rate, codec configuration. 2 Bit rate, codec configuration.\n\n10 Gain. 10 Gain.\n\n5 Gain, harmonicity. 8 Gain.\n\n10 Ratio, compressor characteristics. 20 Gains, durations, temporal probability. 10 Gate characteristics.\n\n3 Ratio. 2 Ratio. 2 Tremolo characteristics.\n\n5\n\nFrequencies, filter characteristics.\n\n15 Number of bands, gains. 10\n\nFrequency, filter characteristics.\n\n150\n\n30\n\nSNR, noise type (cafeteria, traffic, nature, classroom, keyboard, plane, music, ...). SNR, noise type, temporal probability.\n\n30 Gain, reverb characteristics. 5 Gain, reverb characteristics. 1 Gain, chorus characteristics. 1 Gain, phaser characteristics.\n\n120 Gain, room impulse response (RIR), augment.\n\n3 Gain, delay time.\n\n1 Window, amount. 3 Window, amount. 1 Window, amount. 1 Window, amount. 1 Window, amount. 1 Window, amount.\n\n15\n\nSNR, slope. 1 Amplitude. SNR, frequency, type of waveform. 6\n5 SNR, slope, duration, temporal probability. 1 Amplitude, duration, temporal probability. 3\n1 2\n\nSNR, frequency, duration, temporal prob. SNR, frequency, duration, temporal prob. SNR, frequency, type of waveform.\n\n10 Length, temporal probability.\n\n3 Length, temporal probability, gain. 5 Length, temporal probability, SNR. 1 Length, temporal probability, gain. 2 Length, temporal probability. 15 Length, temporal probability. 10\n\nFrequencies, compression ratio, filter type.\n\nany decent denoiser will have no problems and will not introduce any noticeable distortion. We carefully verified that by listening to several examples.\n\n2. Deplosive — We employ a deplosive algorithm, which is composed of detection and processing\n\nstages. The processing acts only on the level of the plosive bands.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n3. Deesser — We employ a basic deesser algorithm, which is composed of detection and processing\n\nstages. The processing acts only on the level of the sibilant bands.\n\n4. Dynamic EQ — We also employ a signal processing tool that aims at bringing the speech to a predefined equalization target. The target is level-dependent, so that the algorithm can act as a compressor and/or expander depending on the characteristics of the input speech.\n\nInterestingly, passing the original clean recordings through this chain forces the model to not rely on bypassing input characteristics (especially low-level characteristics), which we find particularly relevant to remove other non-desirable input characteristics. In addition, the chain provides a more homogeneous character to the target speech, which should translate into some characteristic ‘imprint’ or ‘signature sound’ of UNIVERSE.\n\nD EVALUATION\n\nD.1 OBJECTIVE MEASURES\n\nWe report results with COVL (Loizou, 2013) and STOI (Taal et al., 2010) as computed by the pysepm3 package using default parameters (version June 30, 2021). WARP-Q (Jassim et al., 2021) is used also with default parameters4 (version March 14, 2021) and SESQA is a reference-based version of the reference-free speech quality measure presented in Serrà et al. (2021b), trained on comparable data and losses as explained by the authors. These four objective measures are only used to aid in the assessment of the development steps presented in the main paper. PESQ (Rix et al., 2001), COVL, and STOI are also used to compare with the state of the art in the task of speech denoising in Table 4 below. Calculation of PESQ is also done using pysepm5.\n\nD.2 SUBJECTIVE TEST\n\nWe perform a subjective test considering 15 competitor approaches (12 existing ones plus 3 ablations of the proposed approach, as reported in the main paper). For each existing approach, we download distorted and enhanced pairs of speech recordings from the corresponding demo websites, and use those as the main materials for the test. We randomly select at least 25 pairs per system, and remove a few ones for which we consider the distorted signal is too simple/easy to enhance (for instance, signals that have a very high SNR when evaluating a denoising system, almost no clipping when evaluating a de-cplipping system, and so forth). This is done to reduce potentially ambivalent results in the test, as the two systems would presumably perform very well and the listener would be confused on which to choose (removed pairs were never more than 5 per approach). For a few systems that do not have enough material on their demo page, we download the code and enhance a random selection of distorted input signals from other systems. That is the case of Demucs, MetricGAN+, and VoiceFixer. For the ablations of the proposed approach, we take the first (random) 25 pairs of our validation set. In total, we have over (12+3)×25 enhanced signals from which to judge preference. All distorted signals are enhanced by UNIVERSE using N = 64 diffusion steps and hyper-parameter (cid:15) = 2.3.\n\nA total of 22 expert listeners voluntarily participated in the test. The test features a set of instructions and then shows a list of triplets from which to perform judgment (Fig. 5). Every time a listener lands on the test page, triplets of signals are uniformly randomly selected for each approach: input-distorted signal (Input), competitor-enhanced signal (A or B), and UNIVERSE-enhanced signal (A or B). The order of A or B, as well as the order of the triplets is also uniformly randomly selected when landing to the web page. Preference for an enhanced signal can only be A or B (forced-choice test). Every subject listens to two triplets per competitor system, performing a total of 30 preference choices for 15 approaches (two per approach, yielding a total of 22×2 preference judgments per system).\n\nThe results of the test are based on counting preference choices per competitor approach (% of preference). Statistical significance is determined with a binomial test using p < 0.05 and the Holm-Bonferroni adjustment to compensate for multiple comparisons. In the results table, we also\n\n3https://github.com/schmiph2/pysepm 4https://github.com/wjassim/WARP-Q 5We always use wide-band PESQ (also for computing COVL). The only exception are the results of Table 4,\n\nwhere we report both narrow- and wide-band PESQ (COVL is still computed with wide-band PESQ).\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Screenshot of the subjective test interface. On the top, instructions are given to expert listeners. On the bottom, random test triplets are provided to the listeners. For every test, subjects listen to two triplets comparing UNIVERSE with an existing approach, and enter preference for 15 of such approaches.\n\ndisplay which distortions were tackled by the competitor approach. Note that, since testing materials have different distortions and contents per approach, one cannot infer a ranking of systems based on preference % (that is, one cannot compare preferences between rows of the table; actually they even correspond to different tasks: denoising, de-clipping, restoring codec artifacts, etc.). Preference % only represents a pairwise comparison between UNIVERSE and the corresponding competitor approach listed on the left of the table.\n\nE ADDITIONAL RESULTS\n\nE.1 DENOISING TASK WITH OBJECTIVE METRICS\n\nTo have a comparison on a well-established benchmark, we can consider the task of speech denoising, which has a long tradition in the speech community and, in the last years, has seen a consolidation of test data sets and objective metrics (this is something that, to the best of our knowledge, has not yet happened with other tasks like dereverberation, declipping, bandwidth extension, and so on). Two widely-used data sets with an established test partition are Voicebank-DEMAND (Valentini-Botinhao, 2017) and IS20 DNS-Challenge (Reddy et al., 2020). Since the standard objective metrics are PESQ, COVL, and STOI, and given that UNIVERSE produces audio with further enhancements like leveling/dynamics or a specific equalization, we need to train a new version of UNIVERSE specifically for the denoising task. This is important because, otherwise, UNIVERSE would be performing additional enhancements besides denoising and, more importantly, the standard objective metrics would not find an agreement between the ground truth and the estimated clean speech, what would result in the whole evaluation being unfair to UNIVERSE.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Comparison with the state of the art on the speech denoising task using objective metrics (for all metrics, the higher the better). The first block of the table contains results for existing generative approaches (upper part), while the second block of the table contains results for regression/adversarial approaches (middle part). Bottom rows correspond to the proposed approach UNIVERSE.\n\nApproach\n\nVoiceBank-DEMAND\n\nIS20 DNS Challenge (no rev)\n\nPESQNB\n\nPESQ COVL STOI\n\nPESQNB\n\nPESQ COVL STOI\n\nSEGAN (Pascual et al., 2017) DSEGAN (Phan et al., 2020) SE-Flow (Strauss & Edler, 2021) DiffuSE (Lu et al., 2021) CDiffuSE (Lu et al., 2022) PR-WaveGlow (Maiti & Mandel, 2020) CycleGAN-DCD (Yu et al., 2021) PSMGAN (Routray & Mao, 2022)\n\nPoCoNet (Isik et al., 2020) FullSubNet (Hao et al., 2021) DCCRN+ (Lv et al., 2021) CTS-Net (Li et al., 2021a) Demucs (Défossez et al., 2020) SN-Net (Zheng et al., 2021) SE-Conformer (Kim & Seo, 2021) MetricGAN+ (Fu et al., 2021) PERL-AE (Kataria et al., 2021) HiFi-GAN-2 (Su et al., 2021) Loss Mixup (Chang et al., 2021) DPT-FSNet (Dang et al., 2022)\n\nUNIVERSE-Denoise UNIVERSE-Denoise-E\n\n2.16 2.39 2.43 2.44 2.52\n\n2.90 2.92\n\n2.84 2.92 3.07 3.12 3.12 3.15 3.17 3.18 3.26 3.33\n\n3.21 3.33\n\n2.80 2.90 3.09 3.03 3.10 3.10 3.49 3.52\n\n3.59 3.63 3.60 3.82 3.64 3.83 3.84 3.91 4.00\n\n3.68 3.82\n\n0.93\n\n0.91 0.94\n\n0.95\n\n0.95\n\n0.95\n\n0.96\n\n0.95 0.96\n\n3.85 3.94\n\n3.42\n\n2.75 2.78\n\n2.94\n\n0.96\n\n0.97\n\n3.31 3.33 3.42\n\n3.39\n\n3.26\n\n3.01 3.17\n\n0.98\n\n0.97 0.98\n\n3.60 3.75\n\n3.58 3.73\n\nIn addition, because these standard metrics are known to disfavor generative approaches due to lack of waveform alignment or minor spectral nuances (see Jassim et al., 2021, for a discussion and further pointers), we need to devise a method to produce outputs that are ‘less generative’ and can therefore better coincide with what standard metrics measure. Inspired by Fejgin et al. (2020), we decide to use an expectation of the enhanced waveform. Intuitively, that expectation should minimize to a certain extent different nuances introduced by the generative model, especially regarding small misalignments and minor spectral nuances. To compute such expectation, we sample 10 times using the same conditioning (distorted) signal, and take the sample average of the resulting waveforms. While this has some audible effect such as lowering the presence of high frequencies, it clearly shows a boost in the standard objective metrics, probably because they focus more on small misalignments and low frequencies. We use E to denote this version of the approach. We also want to stress that we solely use this expectation version for the result in the corresponding row of Table 4.\n\nTable 4 shows the results for the denoising task. In it, we can observe the difference in standard metrics between generative and regression/adversarial approaches (first two blocks of the table). The bestperforming approaches in the generative block struggle to get the numbers of the worse-performing approaches in the regression/adversarial block (by listening to some examples from both blocks, we believe this is a metrics issue since we could not find a clear perceptual difference). Interestingly, we observe that even the generative version of the proposed approach (UNIVERSE-Denoise) clearly surpasses all existing generative approaches in terms of standard metrics. In addition, the ‘less generative’ variant using an expectation over multiple realizations (UNIVERSE-Denoise-E) shows a clear improvement for those metrics, with values that become competitive with the non-generative state-of-the-art. The results of our subjective test, which include some of the competing approaches in Table 4, suggest that UNIVERSE outperforms those in such more realistic evaluation.\n\nE.2 SPEED-QUALITY TRADE-OFF\n\nFor completeness, we provide the speed-quality plots for every considered objective metric in Fig. 6. As in the main paper, synthesis parameters are the number of denoising iterations N and the hyperparameter (cid:15) (Sec. A.2). The real-time factor (RTF) is defined as the time to process a recording using\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Speed-quality trade-off when varying synthesis parameters. Speed is measured by the real-time factor on a Tesla V100 GPU (RTF; gray dashed line) and quality is measured with the considered objective metrics on the validation set (colored solid lines).\n\na single Tesla V100 GPU divided by the duration of that recording (for instance, if UNIVERSE takes 2 seconds for enhancing a 20-second recording, RTF=0.1 and we say it is 10 times faster than real time).\n\n24\n\n248163264128256512N1.01.52.02.53.0COVL=1.3=1.7=2.3=3.10.050.10.20.512510RTF248163264128256512N0.8000.8250.8500.8750.9000.925STOI=1.3=1.7=2.3=3.10.050.10.20.512510RTF248163264128256512N0.70.80.91.01.11.21.3WARP-Q=1.3=1.7=2.3=3.10.050.10.20.512510RTF248163264128256512N34567SESQA=1.3=1.7=2.3=3.10.050.10.20.512510RTF",
    "reference": "# Summary Of The Paper\n\nThe paper proposes diffusion models based speech enhancement. In particular, it extends enhancement to a large number of degradations beyond additive noise. The degradation includes common speech degradations such as codec artifacts, bandwidth reduction, reverb etc. The proposed approach puts together different pieces of diffusion-based learning in the context of speech enhancement and then different variations on the top of the base approach are applied to achieve improved performances.\n\n# Strength And Weaknesses\n\nStrengths\n\n1. Diffusion based approaches have been very successful in generative tasks in other domains. While their applications to the audio domain are starting to show up, the success is perhaps not yet at the level in other domains like images. To this end, the authors' attempt on using diffusion based learning for speech enhancement is a good idea. Extending it to degradations beyond additive noise also makes sense. \n\n2. I liked that authors did subjective tests as well. \n\nWeaknesses\n\n1. The experimental section leaves quite a few things unanswered. More on it in the review summary below - for which I would like to see the author's response. \n\n2. I think the novelty of the diffusion-learning process is limited. It largely follows prior works. \n\n3. Some questions below are part of the weaknesses.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is largely clear and the writing quality is good. The novelty is limited to some extent. I did not see any comments on reproducibility like code, model release etc.\n\n# Summary Of The Review\n\nThis work proposes speech enhancement through diffusion based approaches. It trains a single model to learn from a large number of speech degradations. \n\n1. There is a waveform direct waveform based loss too - the significance of which seems to be very high. Doesn’t that default the whole process to so many other speech enhancement models where MSE  on waveform or spectrograms is common. Moreover, there are other latent targets which are also added like pitch, VAD, loudness. One of the versions uses noise level information too. I wonder how much some of the other prior methods can be improved just by adding these additional latent targets and  these auxiliary information. \n\n2. In Section 4.2.1 authors discuss a regression based approach with Universe architecture. Along with waveform and STFT losses are other losses  (as in UNIVERSE like latent targets etc, ) used here ? \n\n3. I am not sure how fair the comparison is with prior works here. \nFirst of all the proposed model is likely orders of magnitude bigger than several of the prior works. \nIn fact, some of them might be causal as well (which proposed model is not) and causality can have significant impact on performance.\nMoreover, the training data is different for different algorithms whereas here a large corpus of private data is used. But importantly, several of these prior works were never trained on such degradations.  I wonder what performance would look like if we just take them and train them with all these other degradations. \nI think decoupling the point above is necessary because it will tell us whether variety in speech degradations is important or diffusion-based learning is bringing something to the table. \n\n3. It would be good to show how this model performs in really difficult conditions. For example, a large number of state of the art methods do a really good job on enhancement but struggle in low-SNRs.  A model like the current one (very large, non-causal) are clearly not the most practical speech enhancement systems which often have real-time uses. Nevertheless, this work puts together a whole bunch of pieces to build the enhancement system. Considering this, stronger results would have been expected, and especially in situations where current state of the art struggles. How does this model compare against prior works in really difficult quality conditions?\n\n4. While I like subjective tests in the paper, I think adding MOS subjective scores would add a lot of value. It will clarify a lot of different aspects. Comparative tests do not give an idea of absolute performance of UNIVERSE or others. \n\n-- updates after rebuttal --\nupdated the score and comments.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nWHAT DO WE MAXIMIZE IN SELF-SUPERVISED LEARNING AND WHY DOES GENERALIZATION EMERGE?\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nIn this paper, we provide an information-theoretic (IT) understanding of selfsupervised learning methods, their construction, and optimality. As a first step, we demonstrate how IT quantities can be obtained for deterministic networks as an alternative to the commonly used unrealistic stochastic networks assumption. Secondly, we demonstrate how different SSL models can be (re)discovered based on first principles and highlight the underlying assumptions of different SSL variants. Based on this understanding, we present new SSL methods that are superior to existing methods in terms of performance. Third, we derive a novel generalization bound based on our IT understanding of SSL methods, providing generalization guarantees for the downstream supervised learning task. As a result of this bound, along with our unified view of SSL, we can compare the different approaches and provide general guidelines to practitioners. Consequently, our derivation and insights contribute to a better understanding of SSL and transfer learning from a theoretical and practical perspective.\n\n1\n\nINTRODUCTION\n\nSelf-Supervised Learning methods (SSL) learn representations using a surrogate objective between inputs and self-defined signals. In SimCLR (Chen et al., 2020), for example, a contrastive loss is defined that makes representations for different versions of the same image similar, while making the representations for different images different. After optimizing the surrogate objective, the pre-trained model is used as a feature extractor for a downstream supervised task, such as image classification, object detection, instance segmentation and transfer learning (Caron et al., 2021; Chen et al., 2020; Misra & Maaten, 2020; Shwartz-Ziv et al., 2022). However, despite success in practice, only a few number of authors (Arora et al., 2019; Lee et al., 2021a) have sought to provide theoretical insights about the effectiveness of SSL.\n\nIn recent years, information theory methods have played a key role in several deep learning achievements, from practical applications in representation learning (Alemi et al., 2016), to theoretical investigations (Xu & Raginsky, 2017; Steinke & Zakynthinou, 2020; Shwartz-Ziv, 2022). Moreover, different deep learning problems have been successfully approached by developing and applying novel estimators and learning principles derived from information-theoretic quantities. Specifically, many works have attempted to analyze SSL from an information theory perspective. An example is the use of the renowned information maximization (InfoMax) principle (Linsker, 1988) in SSL (Bachman et al., 2019). However, looking at these works may be confusing. Numerous objective functions are presented without a rigorous justification, some contradicting each other, as well as many implicit assumptions (Kahana & Hoshen, 2022; Wang et al., 2022; Lee et al., 2021b) Moreover, these works rely on a crucial assumption: a stochastic DN mapping, which is rarely the case nowadays.\n\nThis paper presents a unified framework for SSL methods from an information theory perspective, which can be applied to deterministic DN training. We summarize our contributions into four points: (i) First, in order to study deterministic DNs from an information theory perspective, we shift stochasticity to the DN input, which is a much more faithful assumption for current training techniques. (ii) Second, based on this formulation, we analyze how current SSL methods that use deterministic networks optimize information-theoretic quantities. (iii) Third, we present new SSL\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nmethods based on our analysis and empirically validate their superior performance. (iv) Fourth, we study how the optimization of information-theoretic quantities is related to the final performance in the downstream task using a new generalization bound.\n\n2 BACKGROUND\n\nContinuous Piecewise Affine (CPA) Mappings. A rich class of functions emerges from piecewise polynomials: spline operators. In short, given a partition Ω of a domain RD, a spline of order k is a mapping defined by a polynomial of order k on each region ω ∈ Ω with continuity constraints on the entire domain for the derivatives of order 0,. . . ,k − 1. As we will focus on affine splines (k = 1), we define this case only for concreteness. An K-dimensional affine spline f produces its output via\n\nf (z) =\n\n(cid:88)\n\n(Aωz + bω)1{z∈ω},\n\nω∈Ω\n\n(1)\n\nwith input z ∈ RD and Aω ∈ RK×D, bω ∈ RK, ∀ω ∈ Ω the per-region slope and offset parameters respectively, with the key constraint that the entire mapping is continuous over the domain f ∈ C0(RD). Spline operators and especially affine spline operators have been widely used in function approximation theory (Cheney & Light, 2009), optimal control (Egerstedt & Martin, 2009), statistics (Fantuzzi et al., 2002), and related fields. Deep Networks. A deep network (DN) is a (non-linear) operator fΘ with parameters Θ that map a input x ∈ RD to a prediction y ∈ RK. The precise definitions of DNs operators can be found in Goodfellow et al. (2016). We will omit the Θ notation for clarity unless needed. The only assumption we require for our study is that the non-linearities present in the DN are CPA, as is the case with (leaky-) ReLU, absolute value, and max-pooling. In that case, the entire input-output mapping becomes a CPA spline with an implicit partition Ω, the function of the weights and architecture of the network (Montufar et al., 2014; Balestriero & Baraniuk, 2018). For smooth nonlinearities, our results hold from a first-order Taylor approximation argument. Self-Supervised Learning. Joint embedding methods learn the DN parameters Θ without supervision and input reconstruction. The difficulty of SSL is to produce a good representation for downstream tasks whose labels are not available during training —while avoiding a trivially simple solution where the model maps all inputs to constant output. Many methods have been proposed to solve this problem, see Balestriero & LeCun (2022) for a summary and connections between methods. Contrastive methods learn representations by contrasting positive and negative examples, e.g. SimCLR (Chen et al., 2020) and its InfoNCE criterion (Oord et al., 2018). Other recent work introduced non-contrastive methods that employ different regularization methods to prevent collapsing of the representation. Several papers used stop-gradients and extra predictors to avoid collapse (Chen & He, 2021; Grill et al., 2020) while Caron et al. (2020) uses an additional clustering step. As opposed to contrastive methods, noncontrastive methods do not explicitly rely on negative samples. Of particular interest to us is the VICReg method (Bardes et al., 2021) that considers two embedding batches Z = [f (x1), . . . , f (xN )] and Z′ = [f (x′ N )] each of size (N × K). Denoting by C the (K × K) covariance matrix obtained from [Z, Z′] we obtain the VICReg triplet loss\n\n1), . . . , f (x′\n\nL=\n\n1 K\n\n\n\nK (cid:88)\n\nα max\n\n(cid:16)\n\n0, γ − (cid:112)Ck,k + ε\n\n(cid:17)\n\n+β\n\n(cid:88)\n\n  + γ∥Z − Z′∥2\n\nF /N.\n\n(Ck,k′)2\n\nk=1\n\nk′̸=k\n\nDeep Networks and Information-Theory. Recently, information-theoretic methods have played a key role in several remarkable deep learning achievements (Alemi et al., 2016; Xu & Raginsky, 2017; Steinke & Zakynthinou, 2020; Shwartz-Ziv & Tishby, 2017). Moreover, different deep learning problems have been successfully approached by developing and applying informationtheoretic estimators and learning principles (Hjelm et al., 2018; Belghazi et al., 2018; Piran et al., 2020; Shwartz-Ziv et al., 2018). There is, however, a major problem when it comes to analyzing information-theoretic objectives in deterministic deep neural networks: the source of randomness. The mutual information between the input and the representation in such networks is infinite, resulting in ill-posed optimization problems or piecewise constant, making gradient-based optimization methods ineffective (Amjad & Geiger, 2019). To solve these problems, researchers have proposed several solutions. For SSL, stochastic deep networks with variational bounds could be used, where the output of the deterministic network is used as parameters of the conditional distribution (Lee et al.,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2021b; Shwartz-Ziv & Alemi, 2020). Dubois et al. (2021) suggested another option, which assumed that the randomness of data augmentation among the two views is the source of stochasticity in the network. Another line of works assume a random input, but not using any properties of the distribution of the newtork’s output in order to analysis the netwokr’s objective, which rely on general lower bounds (Wang & Isola, 2020; Zimmermann et al., 2021). For supervised learning, Goldfeld et al. (2018) introduced an auxiliary (noisy) DN framework by injecting additive noise into the model and demonstrated that it is a good proxy for the original (deterministic) DN in terms of both performance and representation. Finally, Achille & Soatto (2018) found that minimizing a stochastic network with a regularizer is equivalent to minimizing cross-entropy over deterministic DNs with multiplicative noise. However, all of these methods assume that the noise comes from the model itself, which contradicts current training methods. In this work, we explicitly assume that the stochasticity comes from the data, which is a less restrictive assumption and does not require changing current algorithms.\n\n3\n\nINFORMATION THEORY FOR DETERMINISTIC DEEP NETWORKS\n\nThis section first sets up notation and assumption on the information-theoretic challenges in SSL (section 3.1) and on our assumptions regarding the data distribution (section 3.2) so that any training sample x can be seen as coming from a single Gaussian distribution as in x ∼ N (μx, Σx). From this we obtain that the output of any deep network f (x) corresponds to a mixture of truncated Gaussian (section 3.3). In particular, it can fall back to a single Gaussian under small noise (det(Σ) → ε) assumptions. These results will enable information measures to be applied to deterministic DNs. We then recover known SSL methods (Bardes et al., 2021; Chen et al., 2020) by making different assumptions about the data distribution and estimating their information.\n\n3.1 SSL AS AN INFORMATION-THEORETIC PROBLEM\n\nTo better grasp the difference between key SSL methods, we first formulate the general SSL goal from an information-theoretical perspective. We start with the MultiView InfoMax principle, i.e., maximizing the mutual information between the representations. Let X and X ′ be two different views and Z and Z ′ their corresponding representations. As shown in Federici et al. (2020), to maximize their information, we maximize I(Z; X ′) and I(Z ′; X) using the lower bound\n\nI(Z, X ′) = H(Z) − H(Z|X ′) ≥ H(Z) + Ex′[log q(z|x′)]\n\n(2)\n\nwhere H(Z) is the entropy of Z. In supervised learning, where we need to maximize I(Z; Y ), the labels (Y ) are fixed, the entropy term H(Y ) is constant, and you only need to optimize the log-loss Ex′[log q(z|x)] (cross-entropy or square loss). However, it is well known that for Siamese networks there exists a degenerate solution, in which all outputs ”collapse” into an undesired value (Chen et al., 2020). Looking at eq. (2) we can see that the entropies are not constant and can be optimized throughout the learning process. Therefore, only minimizing the log loss will cause it to collapse to the trivial solution of making the representations constant (where the entropy goes to zero). To regularize these entropies, that is, prevent collapse, different methods utilize different approaches to implicit regularizing information. To recover them in section 4, we must first introduce the results around the data distribution (section 3.2) and how a DN transforms that distribution (section 3.3).\n\n3.2 DATA DISTRIBUTION HYPOTHESIS\n\nOur first step is to assess how the output random variables of the network are represented, assuming a distribution on the data itself. Under the manifold hypothesis, any point can be seen as a Gaussian random variable with a low-rank covariance matrix in the direction of the manifold tangent space of the data (Fefferman et al., 2016). Therefore, we will consider throughout this study the conditioning of a latent representation with respect to the mean of the observation, i.e., X|x∗ ∼ N (x∗, Σx∗ ) where the eigenvectors of Σx∗ are in the same linear subspace than the tangent space of the data manifold at x∗, which varies with the position of x∗ in space.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nHence a dataset is considered to be a collection of {x∗ to be a sum of low-rank covariance Gaussian densities, as in\n\nn, n = 1, . . . , N } and the full data distribution\n\nX ∼\n\nN (cid:88)\n\nn=1\n\nN (x∗\n\nn, Σx∗\n\nn\n\n)1{T =n} , T ∼ Cat(N ),\n\n(3)\n\nwith T the uniform Categorical random variable. For simplicity, we consider that the effective support of N (x∗ ) do not overlap, where the effective support is defined as {x ∈ RD : p(x) > ε} This keeps things general, as it is enough to cover the domain of the data manifold overall, without overlap between different Gaussians. Therefore, we have that.\n\n) and N (x∗\n\nj , Σx∗\n\ni , Σx∗\n\nj\n\ni\n\np(x) ≈ N\n\n(cid:16)\n\nx; x∗\n\nn(x), Σx∗\n\nn(x)\n\n(cid:17)\n\n/N,\n\n(4)\n\nwhere N (x; ., .) is the Gaussian density at x and with n(x) = arg minn(x − x∗ n). This assumption, that a dataset is a mixture of Gaussians with non-overlapping support, will simplify our derivations below, and could be extended to the general case if needed.\n\nn)T Σx∗\n\n(x − x∗\n\nn\n\n3.3 DATA DISTRIBUTION AFTER DEEP NETWORK TRANSFORMATION\n\nConsider an affine spline operator f (Eq. 1) that goes from a space of dimension D to a space of dimension K with K ≥ D. The span, that we denote as image, of this mapping is given by\n\nIm(f ) ≜ {f (x) : x ∈ RD} =\n\n(cid:91)\n\nω∈Ω\n\nAff(ω; Aω, bω)\n\n(5)\n\nwith Aff(ω; Aω, bω) = {Aωx + bω : x ∈ ω} the affine transformation of region ω by the per-region parameters Aω, bω, and with Ω the partition of the input space in which x lives in. The practical computation of the per-region affine mapping can be obtained by setting Aω to the Jacobian matrix of the network at the corresponding input x, and b to be defined as f (x) − Aωx.\n\nTherefore, the DN mapping consists of affine transformations on each input space partition region ω ∈ Ω based on the coordinate change induced by Aω and the shift induced by bω.\n\nWhen the input space is equipped with a density distribution, this density is transformed by the mapping f . In general, finding the density of f (X) is an intractable task. However, given our disjoint support assumption provided in section 3.2, we can arbitrarily increase the representation power of the density by increasing the number of prototypes N . In doing so, the support of each Gaussian is included with the region ω in which its means lie in, leading to the following result.\n\nTheorem 1. Given the setting of eq. (4) the unconditional DN output density denoted as Z is approximately a mixture of the affinely transformed distributions x|x∗ n(x) e.g. for the Gaussian case\n\nN (cid:88)\n\nN\n\nZ∼\n\nn=1\n\n(cid:16) Aω(x∗\n\nn)x∗\n\nn + bω(x∗\n\nn), AT\n\nω(x∗\n\nn)Σx∗\n\nn\n\n(cid:17)1{T =n}\n\nAω(x∗ n)\n\n,\n\n(6)\n\nwhere ω(x∗\n\nn) = ω ∈ Ω ⇐⇒ x∗\n\nn ∈ ω is the partition region in which the prototype x∗\n\nn lives in.\n\nProof. The proof of of Theorem 1 is presented in Appendix A.\n\n4\n\nINFORMATION OPTIMIZATION AND OPTIMALITY\n\nNext, we will show how SSL algorithms for deterministic networks can be derived. According to Section 3.1, we want to maximize I(Z; X ′) and I(Z ′; X). Although this mutual information is intractable in general, we can obtain a tractable variational estimation using the expected loss. First, when our input noise is small, namely that the effective support of the Gaussian centered at x is contained within the region w of the DN’s input space partition, we can reduce the conditional output density to a single Gaussian: (Z ′|X ′ = xn) ∼ N (μ(xn), Σ(xn)) , where μ(xn) = Aω(xn)xn + bω(xn) and Σ(xn) = AT ω(xn)ΣxnAω(xn). Second, In order to compute the expected loss, we need\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Left: The network output SSL training is more Gaussian for small input noise. The P-value of the normality test for different SSL models trained on CIFAR-10 for different input noise levels. The dashed line represents the point at which the null hypothesis (Gaussian distribution) can be rejected with 99% confidence. Right: Smaller learning rates prevent collapsing. GMM points where in black is the entropy, in blue and red are the data points and GMM centroids respectively, with the corresponding learning rate\n\nto marginalize out the stochasticity in the output of the network. In general, training with squared loss is equivalent to assuming a Gaussian observation model p(z|z′) ∼ N (z′, Σr), where Σr = I. To compute the expected loss over samples of x′, we need to marginalize out the stochasticity in Z ′: which means that the conditional decoder is a Gaussian - (Z|X ′ = xn) ∼ N (μ(xn), Σr + Σ(xn)). However, the expected log loss over samples of Z is hard to compute, and therefore we focused on its lower bound, the expected log loss over samples of Z ′. For simplicity, we set Σr = I which gives us:\n\nEx′ [log q(z|x′)] ≥ Ez′|x′ [log q(z|z′)] =\n\nand now we can take the expectation over Z:\n\nEz|x\n\n(cid:2)Ez′|x′ [log q(z|z′)](cid:3) =\n\nd 2\n\nlog 2π −\n\nd 2\n\n1 2\n\nlog 2π −\n\n1 2\n\n(z − μ(x′))2 −\n\n1 2\n\nT r log Σ(x′)\n\n(7)\n\n(μ(x) − μ(x′))2 −\n\n1 2\n\nlog (|Σ(x)| · |Σ(x′)|)\n\n(8)\n\nFull derivations of eq. (7) and eq. (8) are presented in Appendix B. Combine all the above give us\n\nI(Z; X ′) ≥ H(Z) + Ex,z|x,x′,z′|x′ [log q(z|z′)] d\n2\n\n= H(Z) +\n\nlog 2π −\n\nEx,x′\n\n1 2\n\n(cid:104)\n\n(μ(x) − μ(x′))2 + log (|Σ(x)| · |Σ(x′)|)\n\n(cid:105)\n\n(9)\n\n(10)\n\nTo optimize it in practice, we can approximate p(x, x′) using the empirical data distribution:\n\nL ≈\n\n1 N\n\nN (cid:88)\n\ni=1\n\nH(Z) −\n\n1 2\n\n(μ(xi) − μ(x′\n\ni))2 −\n\n1 2\n\nlog (|Σ(xi)| · |Σ(x′\n\ni)|)\n\n(11)\n\nNext, we will discuses how the estimation of the intractable entropy H(Z) effect our objective.\n\n4.1 DERIVING VICREG FROM FIRST PRINCIPLES\n\nAs a result of eq. (11), we can reconstruct VICReg from first principles. Unfortunately, H(Z) cannot be determined explicitly. However, there are several approximations in the literature (Kolchinsky & Tracey, 2017; Huber et al., 2008). For a detailed discussion about the different entropy estimator, see appendix C. A simpler solution is to approximate the entire mixture by capturing the first two moments of the distribution, which provides an upper bound on the entropy. Note that we are optimizing an upper bound, which means we do not have a formal guarantee, and could lead to an arbitrary increase in our estimator. In practice, there are cases where we can achieve good results by maximizing a lower bound (Martinez et al., 2021; Nowozin et al., 2016), even though this may cause instability in the training process. Using ΣZ as the covariance matrix of Z, we will maximize:\n\n5\n\n0.0001H(Z)=450.0001H(Z)=450.02505H(Z)=430.050.02505H(Z)=45H(Z)=-20H(Z)=-2150.05H(Z)=45H(Z)=-175H(Z)=-264datacentroidsLearning Rate DataLearning Rate CentroidsUnder review as a conference paper at ICLR 2023\n\nL ≈\n\nN (cid:88)\n\nn=1\n\nlog\n\n|ΣZ| |Σ(xi)| · |Σ(x′\n\ni)|\n\n−\n\n1 2\n\n(μ(x) − μ(x′))2\n\n(12)\n\nOptimizing the log determinate of Z means maximizing its log eigenvalues. Although it is theoretically possible to differentiate eigendecomposition, this leads to numerical instability (Dang et al., 2018). While many works have attempted to address this issue (Giles, 2008; Ionescu et al., 2015), VICReg is using a straightforward approach. Because the eigenvalues of a diagonal matrix are the diagonal, increasing the sum of the log-diagonal terms is equivalent to increasing the sum of the log eigenvalues. One approach is to set the off-diagonal terms of ΣZ to zero. However, VICReg maximizes the sum of the diagonal term instead of the log of diagonal terms, which is an upper. An exciting research direction is to maximize the eigenvalues of Z using more sophisticated methods, such as using a differential expression for eigendecomposition.\n\n4.2 EMPIRICAL EVALUATION\n\n4.2.1 VALIDATION OF OUR ASSUMPTIONS\n\nBased on the theory presented in Section 3.3, the conditional output density pz|x=i reduces to a single Gaussian with decreasing input noise. We validated it using a ResNet-18 model trained with SimCLR or VICReg on the CIFAR-10 dataset (Krizhevsky, 2009). From the test dataset, we sample 512 Gaussian samples for each image and analyzed whether each sample remains Gaussian in the penultimate layer of the DN. Then, we employ the D’Agostino and Pearson’s test (D’Agostino, 1971). Figure 1 (left) shows the p-value as a function of the normalized standard deviation. For small noise, we can reject the hypothesis that the conditional output density of the network is not Gaussian (85% for VICReg). Increasing the input noise causes the network’s output to become less Gaussian. Although the results indicate that the output of the network is Gaussian, even for the small noise regime, there is a 15% of Type I error.\n\nThe next step is to try to confirm our assumption that the model of the data distribution has nonoverlapping effective support. We calculate the distribution of pairwise l2 distances between images for seven datasets: MNIST, CIFAR10, CIFAR100, Flowers102, Food101, FGVAircaft. In Figure appendix D, we can see that even for raw pixels, the pairwise distances are far from zero, which means you can use a small Gaussian around each point without overlapping. Therefore, the effective support of these datasets are not-overlapping, and our assumption is realistic.\n\n4.2.2 OPTIMIZING THE MUTUAL INFORMATION OBJECTIVE\n\nImplementing Eq 9 in practice requires many ”design choices”. In section 4.1, we discuss how VICReg uses an approximation of the entropy that is both loose and an upper bound on the true entropy. Next, we suggest combining the VICReg invariance term with different methods for optimizing the entropy.\n\nEstimators. The VICReg objective aims to approximate the log determinate of the empirical covariance matrix by using diagonal terms. However, this estimator can be problematic Huber et al. (2008). Instead, we use the LogDet Entropy Estimator Zhouyin & Liu (2021), which provides a tighter upper bound. This estimator is still an upper bound on entropy, which does not provide any guarantee. To address this problem, we also use a lower bound, based on the pairwise distances of the individual Gaussians (Kolchinsky & Tracey, 2017). These proposed methods are compared with recent SSL methods - SimCLR (Chen et al., 2020) and Barlow Twin (Zbontar et al., 2021).\n\nSetup Our experiments are conducted on CIFAR-10 Krizhevsky et al. (2009). We use ResNet-18 (He et al., 2016) as our backbone. We use linear evaluation for the quality of the representation. For full details see Appendix E.\n\nResults. It can be seen from Table E that the proposed estimators outperform both the original VICReg and SimCLR as well as Barlow Twin. By estimating the entropy with a more accurate estimator, we can improve the results of VICReg, and the pairwise distance estimator, which is a lower bound, achieves the best results. This aligns with the theory that we want to maximize a lower bound on true entropy. The results of our study suggest that a smart selection of entropy estimators, inspired by our framework, leads to better results.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n5 SELF SUPERVISED LEARNING, EM AND INFORMATION\n\nSeveral SSL methods employ the stop gradient operator and only train with positive pairs of data (Grill et al., 2020; Chen & He, 2021). According to (Chen & He, 2021), presetting the stop gradient operation implicitly involves presenting two sets of variables where the algorithm alternates between optimizing each set. Next, we formalize these SSL methods as generalized EM optimization problems, link them to information theory, and analyze how specific design choices affect their collapse.\n\n5.1 THE EM ALGORITHM AND SELF SUPERVISED LEARNING\n\nThe classical approach to learning with hidden variables is based on the Expectation Maximization (EM) algorithm (Dempster et al., 1977). Neal & Hinton (1998) showed that we can view it as a dual optimization where both steps are seen as maximizing the same function, F ( ̃P , θ) = E ̃P [P (Z, Z ′|θ)] + H( ̃P ) where H( ̃P ) = −E ̃P is the entropy of the empirical distribution ̃P and E ̃P [P (Z, Z ′|θ)] is the regular likelihood. Using this formulation, Neal & Hinton (1998) showed that the (G)EM algorithm maximizes a variational lower bound on the log likelihood. However, as discussed in section 3.1, optimize the likelihood can be problematic when both variables are changing. Unlike the classic EM algorithm, for SSL, our input variable Z changes in each iteration, and the optimization is with respect to both Z and Z ′.\n\n(cid:105) (cid:104) log ̃P (z′)\n\n5.2 PREVENTING POINT COLLAPSE UNDER THE EM ALGORITHM\n\nFor Gaussian mixture models (GMMs), clustering consists of estimating the parameters that maximize its likelihood function, followed by assigning to each data point the cluster corresponding to its most likely multivariate Gaussian distribution. Chen & He (2021) suggested that the SimSiam method can be viewed as the K-means algorithm, which can be derived by reducing the GMMs\n\nLet us examine a toy dataset on the pattern of two intertwining moons to illustrate the collapse phenomenon under GMM (Figure 1 - right). We begin by training a classical GMM with maximum likelihood, where the means are initialized based on random samples, and the covariance is used as the identity matrix. A red dot represents the Gaussian’s mean after training, while a blue dot represents the data points. In the presence of fixed input samples, we observe that there is no collapsing and that the entropy of the centers is high (Figure 4 - left, in the Appendix). However, when we make the input samples trainable and optimize their location, all the points collapse into a single point, resulting in a sharp decrease in entropy (Figure 4 - right, in the Appendix).\n\nTo prevent collapse, we follow the K-means algorithm in enforcing sparse posteriors, i.e. using small initial standard deviations and learning only the mean. This forces a one-to-one mapping which leads all points to be closest to the mean without collapsing, resulting in high entropy (Figure 4 - middle, in the Appendix). Another option to prevent collapse is to use different learning rates for input and parameters. Using this setting, the collapsing of the parameters does not maximize the likelihood. Figure 1 (right) shows the results of GMM with different learning rates for learned inputs and parameters. When the parameter learning rate is sufficiently high in comparison to the input learning rate, the entropy decreases much more slowly and no collapse occurs.\n\n6 BENEFITS OF INFORMATION MAXIMIZATION FOR GENERALIZATION\n\nThe purpose of this section is to further connect the invariance loss, the covariance matrix, and the information with the input to the generalization ability of the model by deriving a novel generalization bound. Together with the results from the previous sections, this provides a mathematical understanding of the benefits of SSL through maximization of information with implicit regularization.\n\n6.1 NOTATION\n\nLet x be our input and y ∈ Rr the output. We are given a labeled training data S = ((xi, yi))n of size n and an unlabeled training data ̄S = ((x+ i and x++ share the same (unknown) label. With the unlabeled training data, we define the invariance loss I ̄S(fθ) = 1 )∥ where fθ is the trained representation on the unlabeled\n\ni=1 of size m, where x+\n\ni ) − fθ(x++\n\ni=1 ∥fθ(x+\n\ni , x++\n\n(cid:80)m\n\n))m\n\ni=1\n\ni\n\ni\n\ni\n\nm\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n1 n\n\n(cid:80)n\n\ndata ̄S. We define a labeled loss lx,y(w) = ∥W fθ(x) − y∥ where w = vec[W ] ∈ Rdr is the vectorization of the matrix W ∈ Rr×d. Let wS = vec[WS] be the minimum norm solution as i=1 ∥Wfθ(xi) − yi∥2. We also define the WS = minimizeW ′ ∥W ′∥F s.t. W ′ ∈ arg minW m)] ∈ Rd×m, representation matrices ZS = [f (x1), . . . , f (xn)] ∈ Rd×n and Z ̄S = [f (x+ ⊤)†Z ̄S. ⊤(ZSZS and the projection matrices PZS = I − ZS We define the label matrix YS = [y1, . . . , yn]⊤ ∈ Rn×r and the unknown label matrix Y ̄S = is the unknown label of x+ [y+ i . Let F be a hypothesis space of fθ. For a given hypothesis space F, we define the normalized Rademacher complexity ̃Rm(F) = 1√ )∥], where , ξ1, . . . , ξm are independent uniform random variables taking values in {−1, 1}. It is normalized such that ̃Rm(F) = O(1) as m → ∞ for typical choices of hypothesis spaces F, including DNs (Bartlett et al., 2017; Kawaguchi et al., 2018).\n\n⊤)†ZS and PZ ̄S = I − Z ̄S\n\nm]⊤ ∈ Rm×r, where y+\n\n1 ), . . . , f (x+\n\nE ̄S,ξ[supf ∈F\n\ni=1 ξi∥f (x+\n\ni ) − f (x++\n\n1 , . . . , y+\n\n⊤(Z ̄SZ ̄S\n\n(cid:80)m\n\nm\n\ni\n\ni\n\n6.2 GENERALIZATION BOUND FOR VICREG\n\nWe now show that SSL via VICReg can be understood to improve the generalization ability for the supervised downstream task. Namely, Theorem 2 shows that the expected labeled loss Ex,y[lx,y(wS)] is minimized when we minimize the unlabeled invariance loss I ̄S(fθ) while controlling the covariance Z ̄SZ ̄S\n\n⊤ and the complexity of representations ̃Rm(F):\n\nTheorem 2. (Informal version). For any δ > 0, with probability at least 1 − δ, the following holds:\n\nEx,y[lx,y(wS)] ≤ I ̄S(fθ) +\n\n∥PZ ̄S Y ̄S∥F +\n\n1 √\nn\n\n∥PZS YS∥F +\n\n2 ̃Rm(F) √\nm\n\n+ Qm,n,\n\n(13)\n\nwhere Qm,n = O(G decaying at the rate 1/ at the rate 1/\n\n√\n\n(cid:113) ln(1/δ) √\n\n) → 0 as m, n → ∞. In Qm,n, the value of G for the term m + m depends on the hypothesis space of fθ and w whereas the term decaying\n\nn\n\nn is independent of any hypothesis space.\n\n2 √\nm (cid:113) ln(1/δ)\n\nProof. The complete version of Theorem 2 and its proof are presented in Appendix G.\n\nNote that our framework holds for a classification with a linear layer and the l2 norm as the loss. Also, in order that this bounds will not become vacuous we should imposed that the class F has a finite norm range and that the class of matrices for the linear layer W is of finite norm.\n\nThe term ∥PZ ̄S Y ̄S∥F in Theorem 2 contains the unobservable label matrix Y ̄S. However, we can minimize this term by using ∥PZ ̄S Y ̄S∥F ≤ ∥PZ ̄S ∥F ∥Y ̄S∥F and by minimizing ∥PZ ̄S ∥F . The ⊤ is maximized. Since a factor ∥PZ ̄S ∥F is minimized when the rank of the covariance Z ̄SZ ̄S strictly diagonally dominant matrix is non-singular, this can be enforced by maximizing the diagonal entries while minimizing the off-diagonal entries, as is done in VICReg. For example, if d ≥ n, ⊤ is of full rank. The term ∥PZS YS∥F contains then ∥PZ ̄S ∥F = 0 when the covariance Z ̄SZ ̄S only observable variables and we can directly measure the value of this term using training data. ⊤ is In addition, the term ∥PZS YS∥F is also minimized when the rank of the covariance ZSZS ⊤ concentrate to each other via concentration ⊤ and Z ̄SZ ̄S maximized. Since the covariances ZSZS inequalities with the error in the order of O((cid:112)(ln(1/δ))/n + ̃Rm(F)(cid:112)(ln(1/δ))/m), we can also ⊤ while minimize the upper bound on ∥PZS YS∥F by maximizing the diagonal entries of Z ̄SZ ̄S minimizing its off-diagonal entries, as is done in VICReg.\n\nThus, VICReg can be understood as a method to minimize the generalization bound in Theorem 2 ⊤ to minimize the labelby minimizing the invariance loss while controlling the covariance Z ̄SZ ̄S agnostic upper bounds on ∥PZ ̄S Y ̄S∥F and ∥PZS YS∥F . If we know partial information about the label Y ̄S of the unlabeled data, we can use it to minimize ∥PZ ̄S Y ̄S∥F and ∥PZS YS∥F directly. This direction can be used to improve VICReg in future work for the partial observable setting.\n\n6.3 UNDERSTANDING VIA MUTUAL INFORMATION\n\nTheorem 2 together with the result of the previous section shows that, for generalization in the downstream task, it is helpful to maximize the mutual information I(Z; X ′) in SSL via minimizing\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n⊤. The term 2 ̃Rm(F )\n\n√\n\nthe invariance loss I ̄S(fθ) while controlling the covariance Z ̄SZ ̄S m captures the importance of controlling the complexity of the representations fθ. To understand this term further in terms of mutual information, let us consider a discretization of the parameter space of F to have finite |F| < ∞ (indeed, a computer always implements some discretization of continuous variables). Then, by Massart’s Finite Class Lemma, we have that ̃Rm(F) ≤ C(cid:112)ln |F| for some constant C > 0. Moreover, Shwartz-Ziv (2022) shows that we can approximate ln |F| by 2I(Z;X). Thus, in Theorem 2, the term I ̄S(fθ) + 2√ n ∥PZS YS∥F corresponds to I(Z; X ′) while the term of 2 ̃Rm(F ) √\n\nm corresponds to I(Z; X). Recall that the information can be decomposed as\n\nm ∥PZ ̄S Y ̄S∥F + 1√\n\nI(Z; X) = I(Z; X ′) + I(Z; X|X ′).\n\n(14)\n\nwhere we want to maximize the predictive information I(Z; X ′), while minimizing I(Z; X) (??). Thus, in order to improve generalization, we also need to control 2 ̃Rm(F ) m to restrict the superfluous information I(Z; X|X ′), in addition to minimize I ̄S(fθ) + 2√ n ∥PZS YS∥F that corresponded to maximize the predictive information I(Z; X ′). Although we can explicitly add regularization on I(Z; X|X ′) to control 2 ̃Rm(F ) m are implicitly regularized via implicit bias through e design choises (Gunasekar et al., 2017; Soudry et al., 2018; Gunasekar et al., 2018). Thus, Theorem 2 connects the information-theoretic understanding of SSL with the probabilistic guarantee on the generalization ability.\n\nm , it is possible that I(Z; X|X ′) and 2 ̃Rm(F )\n\nm ∥PZ ̄S Y ̄S∥F + 1√\n\n√\n\n√\n\n√\n\n6.4 COMPARING GENERALIZATION BOUNDS\n\nThe generalization bound of SimCLR (Saunshi et al., 2019) requires the number of label classes to go infinity to make the generalization gap decrease towards zero. In contrast, the bound on VICReg in Theorem 2 does not require the number of label classes to approach infinity to let the generalization gap go to zero. This reflects the fact that, unlike SimCLR, VICReg does not use negative pairs and thus does not use a loss function that is based on the implicit expectation that the labels of a negative pair (y+, y−) are different. Another difference is that our VICReg bound improves as n increases, while the previous bound of SimCLR (Saunshi et al., 2019) does not depend on n. This is because the previous work assumes partial access to the true distribution of x given y per class for setting W , which removes the importance of labeled data size n and is not assumed in our study. Consequently, our bound provides a new insight for VICReg regarding the ratio of the effects of m v.s. n through G(cid:112)ln(1/δ)/m + (cid:112)ln(1/δ)/n. Finally, Theorem 2 also illuminates the advantages of VICReg over standard supervised training. That is, with standard training, the generalization bound via the Rademacher complexity requires the complexities of hypothesis spaces, ̃Rn(W)/ n and ̃Rn(F)/ n, with respect to the size of labeled data n, instead of the size of unlabeled data m. Here, ̃Rn(W) is the the normalized Rademacher complexity for the hypothesis space of w. Thus, Theorem 2 shows that using SSL, we can replace all the complexities of hypothesis spaces in terms of n with those in terms of m. Since m is typically much larger than n, this illuminates the benefit of SSL.\n\n√\n\n√\n\n7 CONCLUSIONS\n\nIn this study, we examine SSL’s objective function from an information-theoretic perspective. Based on transfering of the required stochasticity to the input distribution, we show how SSL objectives can be derived. Thus, even when using deterministic DNs, it is possible to perform an informationtheoretic analysis. The second part of the paper rediscovered SSL loss functions from first principles and demonstrated their implicit assumptions. We empirically validated our analysis and confirmed the validity of our novel understanding. As a result of our analysis, we have proposed new SSL algorithms that perform better than existing ones. Furthermore, we derived a generalization bound on the downstream task, tight it to known information objeective terms and demonstrate that VICReg In addition, our work opens many new avenues for future research, including a minimizes it. better estimation of information-theoretic quantities that are consistent with our assumptions and identifying which SSL method is the most appropriate according to data characteristics. In addition, our probabilistic guarantee suggests that VICReg can be further improved for the setting of partial label information by aligning the covariance matrix with the partially observable label matrix.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through noisy computation. IEEE transactions on pattern analysis and machine intelligence, 40 (12):2897–2905, 2018.\n\nN.A. Ahmed and D.V. Gokhale. Entropy expressions and their estimators for multivariate distributions.\n\nIEEE Transactions on Information Theory, 35(3):688–692, 1989. doi: 10.1109/18.30996.\n\nAlexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information\n\nbottleneck. arXiv preprint arXiv:1612.00410, 2016.\n\nRana Ali Amjad and Bernhard Claus Geiger. Learning representations for neural network-based classification using the information bottleneck principle. IEEE transactions on pattern analysis and machine intelligence, 2019.\n\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint arXiv:1902.09229, 2019.\n\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. Advances in neural information processing systems, 32, 2019.\n\nRandall Balestriero and Richard Baraniuk. A spline theory of deep networks.\n\nIn Proc. ICML,\n\nvolume 80, pp. 374–383, Jul. 2018.\n\nRandall Balestriero and Yann LeCun. Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods, 2022. URL https://arxiv.org/abs/2205.11508.\n\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization\n\nfor self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.\n\nPeter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and\n\nstructural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.\n\nPeter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for\n\nneural networks. Advances in neural information processing systems, 30, 2017.\n\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.\n\nBrendon J Brewer. Computing entropies with nested sampling. Entropy, 19(8):422, 2017.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912–9924, 2020.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ́e J ́egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9650–9660, 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021.\n\nElliott Ward Cheney and William Allan Light. A course in approximation theory, volume 101.\n\nAmerican Mathematical Soc., 2009.\n\nRalph B. D’Agostino. An omnibus test of normality for moderate and large size samples. Biometrika,\n\n58(2):341–348, 1971. ISSN 00063444. URL http://www.jstor.org/stable/2334522.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nZheng Dang, Kwang Moo Yi, Yinlin Hu, Fei Wang, Pascal Fua, and Mathieu Salzmann. Eigendecomposition-free training of deep networks with zero eigenvalue-based losses. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 768–783, 2018.\n\nArthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1): 1–22, 1977.\n\nYann Dubois, Benjamin Bloem-Reddy, Karen Ullrich, and Chris J Maddison. Lossy compression for\n\nlossless prediction. Advances in Neural Information Processing Systems, 34, 2021.\n\nMagnus Egerstedt and Clyde Martin. Control theoretic splines: optimal control, statistics, and path\n\nplanning. Princeton University Press, 2009.\n\nCesare Fantuzzi, Silvio Simani, Sergio Beghelli, and Riccardo Rovatti. Identification of piecewise affine models in noisy environment. International Journal of Control, 75(18):1472–1485, 2002.\n\nMarco Federici, Anjan Dutta, Patrick Forr ́e, Nate Kushman, and Zeynep Akata. Learning robust representations via multi-view information bottleneck. arXiv preprint arXiv:2002.07017, 2020.\n\nCharles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis.\n\nJournal of the American Mathematical Society, 29(4):983–1049, 2016.\n\nMike B. Giles. Collected matrix derivative results for forward and reverse mode algorithmic differentiation. In Christian H. Bischof, H. Martin B ̈ucker, Paul Hovland, Uwe Naumann, and Jean Utke (eds.), Advances in Automatic Differentiation, pp. 35–44, Berlin, Heidelberg, 2008. Springer Berlin Heidelberg.\n\nZ. Goldfeld, E. van den Berg, K. Greenewald, I. Melnyk, N. Nguyen, B. Kingsbury, and Y. Polyanskiy.\n\nEstimating Information Flow in Neural Networks. ArXiv e-prints, 2018.\n\nNoah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of\n\nneural networks. In Conference On Learning Theory, pp. 297–299. PMLR, 2018.\n\nI. Goodfellow, Y. Bengio, and A. Courville. Deep Learning, volume 1. MIT Press, 2016.\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020.\n\nSuriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems, pp. 6151–6159, 2017.\n\nSuriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. In Advances in Neural Information Processing Systems, pp. 9461–9471, 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.\n\nMarco Huber, Tim Bailey, Hugh Durrant-Whyte, and Uwe Hanebeck. On entropy approximation for gaussian mixture random vectors. In IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, pp. 181 – 188, 09 2008. doi: 10.1109/MFI.2008.4648062.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nCatalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Matrix backpropagation for deep networks with structured layers. In 2015 IEEE International Conference on Computer Vision (ICCV), pp. 2965–2973, 2015. doi: 10.1109/ICCV.2015.339.\n\nJonathan Kahana and Yedid Hoshen. A contrastive objective for learning disentangled representations.\n\narXiv preprint arXiv:2203.11284, 2022.\n\nKenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.\n\nMIT-CSAIL-TR-2018-014, Massachusetts Institute of Technology, 2018.\n\nKenji Kawaguchi, Zhun Deng, Kyle Luh, and Jiaoyang Huang. Robustness Implies Generalization via Data-Dependent Generalization Bounds. In International Conference on Machine Learning (ICML), 2022.\n\nArtemy Kolchinsky and Brendan D Tracey. Estimating mixture entropy with pairwise distances.\n\nEntropy, 19(7):361, 2017.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nJason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps: Provable self-supervised learning. Advances in Neural Information Processing Systems, 34, 2021a.\n\nKuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive\n\nvisual representations. Advances in Neural Information Processing Systems, 34, 2021b.\n\nRalph Linsker. Self-organization in a perceptual network. Computer, 21(3):105–117, 1988.\n\nJulieta Martinez, Jashan Shewakramani, Ting Wei Liu, Ioan Andrei Bˆarsan, Wenyuan Zeng, and Raquel Urtasun. Permute, quantize, and fine-tune: Efficient compression of neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15699–15708, 2021.\n\nIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6707–6717, 2020.\n\nNeeraj Misra, Harshinder Singh, and Eugene Demchuk.\n\na multivariate normal distribution. 2005. https://www.sciencedirect.com/science/article/pii/S0047259X03001787.\n\nISSN 0047-259X.\n\nEstimation of the entropy of Journal of Multivariate Analysis, 92(2):324–342, URL https://doi.org/10.1016/j.jmva.2003.10.003.\n\ndoi:\n\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT\n\npress, 2012.\n\nGuido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear\n\nregions of deep neural networks. In Proc. NeurIPS, pp. 2924–2932, 2014.\n\nRadford M Neal and Geoffrey E Hinton. A view of the em algorithm that justifies incremental, sparse,\n\nand other variants. In Learning in graphical models, pp. 355–368. Springer, 1998.\n\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing systems, pp. 271–279, 2016.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\n\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n\nZoe Piran, Ravid Shwartz-Ziv, and Naftali Tishby. The dual information bottleneck. arXiv preprint\n\narXiv:2006.04641, 2020.\n\nNikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In International Conference on Machine Learning, pp. 5628–5637. PMLR, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nShai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to\n\nalgorithms. Cambridge university press, 2014.\n\nRavid Shwartz-Ziv. Information flow in deep neural networks. arXiv preprint arXiv:2202.06749,\n\n2022.\n\nRavid Shwartz-Ziv and Alexander A Alemi. Information in infinite ensembles of infinitely-wide neural networks. In Symposium on Advances in Approximate Bayesian Inference, pp. 1–17. PMLR, 2020.\n\nRavid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information.\n\narXiv preprint arXiv:1703.00810, 2017.\n\nRavid Shwartz-Ziv, Amichai Painsky, and Naftali Tishby. Representation compression and general-\n\nization in deep neural networks, 2018.\n\nRavid Shwartz-Ziv, Micah Goldblum, Hossein Souri, Sanyam Kapoor, Chen Zhu, Yann LeCun, and Andrew Gordon Wilson. Pre-train your loss: Easy bayesian transfer learning with informative priors. arXiv preprint arXiv:2205.10279, 2022.\n\nDaniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1): 2822–2878, 2018.\n\nThomas Steinke and Lydia Zakynthinou. Reasoning about generalization via conditional mutual\n\ninformation. In Conference on Learning Theory, pp. 3437–3452. PMLR, 2020.\n\nHaoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. Rethinking minimal sufficient representation in contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16041–16050, 2022.\n\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pp. 9929–9939. PMLR, 2020.\n\nAolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning\n\nalgorithms. Advances in Neural Information Processing Systems, 30, 2017.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St ́ephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning, pp. 12310– 12320. PMLR, 2021.\n\nZhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant entropy\n\nestimator. arXiv preprint arXiv:2105.03705, 2021.\n\nRoland S Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel. Contrastive learning inverts the data generating process. In International Conference on Machine Learning, pp. 12979–12990. PMLR, 2021.\n\nA DATA DISTRIBUTION AFTER DEEP NETWORK\n\nTRANSFORMATION\n\nTheorem 3. Given the setting of eq. (4) the unconditional DN output density denoted as Z approximates (given the truncation of the Gaussian on its effective support that is included within a single region ω of the DN’s input space partition) a mixture of the affinely transformed distributions x|x∗ e.g. for the Gaussian case\n\nn(x)\n\nN (cid:88)\n\nN\n\nZ∼\n\nn=1\n\n(cid:16) Aω(x∗\n\nn)x∗\n\nn + bω(x∗\n\nn), AT\n\nω(x∗\n\nn)Σx∗\n\nn\n\n(cid:17)T =n\n\nAω(x∗ n)\n\n,\n\nwhere ω(x∗\n\nn) = ω ∈ Ω ⇐⇒ x∗\n\nn ∈ ω is the partition region in which the prototype x∗\n\nn lives in.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nProof. We know that If (cid:82) n(x))dx ≈ 1 then f is linear within the effective support of p. Therefore, any sample from p will almost surely lie within a single region ω ∈ Ω and therefore the entire mapping can be considered linear with respect to p. Thus, the output distribution is a linear transformation of the input distribution based on the per-region affine mapping.\n\nω p(x|x∗\n\nB LOWER BOUNDS ON Ex′ [log q(z|x′)]\n\nIn this appendix we present the full derivation of the lower bound on Ex′ [log q(z|x′)]. Because Z ′|X ′ is a Gaussian, we can write it as Z ′ = μ(x′) + L(x′)ε where ε ∼ N (0, 1) and L(x′)T L(x′) = Σ(x′). Now, setting Σr = I, will give us:\n\nEx′ [log q(z|x′)] ≥\n\nEz′|x′ [log q(z|z′)] = 1\n2 (cid:104)\n\nlog 2π −\n\n(cid:20) d 2\n\nEz′|x′\n\nlog 2π −\n\nEz′|x′,\n\nd 2\nd 2\nd 2\n\nlog 2π −\n\nlog 2π −\n\n1 2\n1 2\n1 2\n\nEε\n\nEε\n\n(z − z′)T (I))−1 (z − z′)\n\n(z − z′)2(cid:105)\n\n=\n\n(cid:104)\n\n(z − μ(x′) − L(x′)ε)2(cid:105)\n\n=\n\n(cid:21)\n\n=\n\n(15)\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\n(cid:104)\n\n(z − μ(x′))2 − 2 (z − μ(x′) ∗ L(x′)ε) +\n\n(cid:16)\n\n(L(x′)ε)T (L(x′)ε)\n\n(cid:17)(cid:105)\n\n=\n\nd 2\n\nd 2\n\nlog 2π −\n\nlog 2π −\n\n1 2\n\n1 2\n\n(cid:104)\n\n(z − μ(x′))2(cid:105)\n\nEε\n\n+ (z − μ(x′)L(x′)) Eε [ε] −\n\n1 2\n\n(z − μ(x′))2 −\n\n1 2\n\nT r log Σ(x′)\n\n(20)\n\nEε\n\n(cid:2)εT L(x′)T L(x′)ε(cid:3) =\n\n(21)\n\n(22)\n\nwhere Ex′ [log q(z|x′)] = Ex′ Eε[ε] = 0 and Eε\n\n(cid:2)log Ez′|x′ [q(z|z′)](cid:3) ≥ Ez′ [log q(z|z′)] by Jensen’s inequality,\n\n(cid:2)ε (cid:0)L(x′)T L(x′(cid:1) ε(cid:3) = T r log Σ(x′) by the Hutchinson’s estimator.\n\nEz|x\n\n(cid:2)Ez′|x′ [log q(z|z′)](cid:3) =\n\nEz|x\n\n(cid:20) d 2\n\nlog 2π −\n\n1 2\n(cid:104)\n\n(z − μ(x′))2 −\n\n(z − μ(x′))2(cid:105)\n\n−\n\n1 2\n1 2\n\nEz|x\n\n(cid:21)\n\nT r log Σ(x′)\n\n=\n\nT r log Σ(x′) =\n\n(cid:104)\n\n(μ(x) + L(x)ε − μ(x′))2(cid:105)\n\n−\n\nEε\n\n1 2\n\nT r log Σ(x′) =\n\n(cid:104)\n\n(μ(x) − μ(x′))2(cid:105)\n\nEε\n\n+ Eε [(μ(x) − μ(x′)) L(x)ε]\n\nd 2\nd 2\n\nd 2\n\nlog 2π −\n\nlog 2π −\n\nlog 2π −\n\n1 2\n1 2\n\n1 2\n\n−\n\n1 2\n\nEε\n\n(cid:2)εT L(x)T L(x)ε(cid:3) −\n\n1 2\n\nT r log Σ(x′) =\n\nd 2\n\nd 2\n\nlog 2π −\n\nlog 2π −\n\n1 2\n\n1 2\n\n(μ(x) − μ(x′))2 −\n\n(μ(x) − μ(x′))2 −\n\n1 2\n\n1 2\n\n14\n\nT r log Σ(x) −\n\n1 2\n\nT r log Σ(x′) =\n\nlog (|Σ(x)| · |Σ(x′)|)\n\n(23)\n\n(24)\n\n(25)\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\nUnder review as a conference paper at ICLR 2023\n\nC ENTROPY ESTIMATORS\n\nThe estimation of entropy is one of the classic problems in information theory, where Gaussian mixture density is one of the most popular representations. With a sufficient number of components, they can approximate any smooth function with arbitrary accuracy. For Gaussian mixtures, there is, however, no closed-form solution to differential entropy. There exist several approximations in the literature, including loose upper and lower bounds (Huber et al., 2008). Monte Carlo (MC) sampling is one way to approximate Gaussian mixture entropy. With sufficient MC samples, an unbiased estimate of entropy with an arbitrarily accurate can be obtained. Unfortunately, MC sampling is a very computationally expensive and typically requires a large number of samples, especially in high dimensions (Brewer, 2017). Using the first two moments of the empirical distribution, VIGCreg used one of the most straightforward approaches for approximating the entropy. Despite this, previous studies have found that this method is a poor approximation of the entropy in many cases Huber et al. (2008). Another options is to use the LogDet function. Several estimators have been proposed to implement it, including uniformly minimum variance unbiased (UMVU) (Ahmed & Gokhale, 1989), and bayesian methods Misra et al. (2005). These methods, however, often require complex optimizations. The LogDet estimator presented in Zhouyin & Liu (2021) used the differential entropy α order entropy using scaled noise. They demonstrated that it can be applied to high-dimensional features and is robust to random noise. Based on Taylor-series expansions, Huber et al. (2008) presented a lower bound for the entropy of Gaussian mixture random vectors. They use Taylor-series expansions of the logarithm of each Gaussian mixture component to get an analytical evaluation of the entropy measure. In addition, they present a technique for splitting Gaussian densities to avoid components with high variance, which would require computationally expensive calculations. Kolchinsky & Tracey (2017) introduce a novel family of estimators for the mixture entropy. For this family, a pairwise-distance function between component densities defined for each member. These estimators are computationally efficient, as long as the pairwise-distance function and the entropy of each component distribution are easy to compute. Moreover, the estimator is continuous and smooth and is therefore useful for optimization problems. In addition, they presented both lower bound (using Chernoff distance) and an upper bound (using the KL divergence) on the entropy, which are are exact when the component distributions are grouped into well-separated clusters,\n\nD EMPIRICAL VALIDATION OF OUR ASSUMPTION\n\nWe will try to verify empirically our assumptions on different datasets We compute the pairwise l2 distances between images for seven datasets: MNIST, CIFAR10, CIFAR100, Flowers102, Food101, and FGVAircaft. We found that even for raw pixels, the pairwise distances are far from zero, which means you can use a small Gaussian around each point without overlapping. Consequently, the effective supports of these high-dimensional datasets are not overlapping, and our assumption is realistic even for current popular SSL datasets..\n\nE EXPERIMENTAL VERIFICATION OF INFORMATION-BASED BOUND\n\nOPTIMIZATION\n\nSetup Our experiments are conducted on CIFAR-10 Krizhevsky et al. (2009). We use ResNet-18 (He et al., 2016) as our backbone. Each model is trained with 512 batch size for 800 epochs. We use linear evaluation to assess the quality of the representation. Once the model has been pre-trained, we follow the same fine-tuning procedures as for the baseline methods (Caron et al., 2020).\n\nF EXPECTATION MAXIMIZATION AND COLLAPSING\n\nG ON BENEFITS OF INFORMATION MAXIMIZATION FOR GENERALIZATION\n\nIn this Appendix, we present the complete version of Theorem 2 along with its proof and additional discussions.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The Gaussians around each point are not overlapping The l2 distances between raw images for different datasets\n\nMethod SimCLR Barlow Twins VICReg VICReg + Pairwise Distances Estimator (ours) VICReg + Log Derminate Estimator (ours)\n\nAccuracy 89.72 ± 0.05 88.81 ± 0.1 89.32 ± 0.09 90.09 ± 0.09 89.77 ± 0.08\n\nTable 1: Entropy estimator achieved better results on SSL - CIFAR10 accuracy on linear evaluation of SSL for different entropy estimators. The best results achieved by pairwise distances lower bound\n\n16\n\nMNISTCIFAR10CIFAR100Flowers102Food101FGVCAircraft0255075100125150175l2 DistancesUnder review as a conference paper at ICLR 2023\n\nFigure 3: Evolution of the entropy for each of the learning rate configurations showing that the impact of picking the incorrect learning rate for the data and/or centroids lead to a collapse of the samples.\n\nG.1 ADDITIONAL NOTATION AND DETAILS\n\ni=1\n\n(cid:80)n\n\nWe start to introduce additional notation and details. We use the notation of x ∈ X for an input and y ∈ Y ⊆ Rr for an output. Define p(y) = P(Y = y) to be the probability of getting label y and 1{yi = y} to be the empirical estimate of p(y). Let ζ be an upper bound on the ˆp(y) = 1 n\nnorm of the label as ∥y∥2 ≤ ζ for all y ∈ Y. Define the minimum norm solution W ̄S of the unlabeled i )∥2. Let κS data as W ̄S = minimizeW ′ ∥W ′∥F s.t. W ′ ∈ arg minW be a data-dependent upper bound on the per-sample Euclidian norm loss with the trained model as ∥WSfθ(x) − y∥ ≤ κS for all (x, y) ∈ X × Y. Similarly, let κ ̄S be a data-dependent upper bound on the per-sample Euclidian norm loss as ∥W ̄Sfθ(x) − y∥ ≤ κ ̄S for all (x, y) ∈ X × Y. Define the difference between WS and W ̄S by c = ∥WS − W ̄S∥2. Let W be a hypothesis space of W such that W ̄S ∈ W. We denote by ̃Rm(W ◦ F) = 1√ i )∥] the normalized Rademacher complexity of the set {x+ (cid:55)→ ∥g∗(x+) − Wf (x+)∥ : W ∈ W, f ∈ F}. we denote by κ a upper bound on the per-sample Euclidian norm loss as ∥W f (x) − y∥ ≤ κ for all (x, y, W, f ) ∈ X × Y × W × F.\n\nE ̄S,ξ[supW ∈W,f ∈F\n\ni=1 ∥Wfθ(x+\n\ni=1 ξi∥g∗(x+\n\ni ) − Wf (x+\n\ni ) − g∗(x+\n\n(cid:80)m\n\n(cid:80)m\n\n1 m\n\nm\n\nWe adopt the following data-generating process model that is used in the previous paper on analyzing contrastive learning (Saunshi et al., 2019). For the labeled data, first, y is drawn from the distritbuion\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Evolution of GMM training when enforcing a one-to-one mapping between the data and centroids akin to K-means i.e. using a small and fixed covariance matrix. We see that collapse does not occur. Left - In the presence of fixed input samples, we observe that there is no collapsing and that the entropy of the centers is high. Right - when we make the input samples trainable and optimize their location, all the points collapse into a single point, resulting in a sharp decrease in entropy.\n\nρ on Y, and then x is drawn from the conditional distribution Dy conditioned on the label y. That is, we have the join distribution D(x, y) = Dy(x)ρ(y) with ((xi, yi))n i=1 ∼ Dn. For the unlabeled data, first, each of the unknown labels y+ and y− is drawn from the distritbuion ρ, and then each of the positive examples x+ and x++ is drawn from the conditional distribution Dy+ while the negative example x− is drawn from the Dy− . Unlike the analysis of contrastive learning, we do not require the negative samples. Let τ ̄S be a data-dependent upper bound on the invariance loss with the trained representation as ∥fθ( ̄x) − fθ(x)∥ ≤ τ ̄S for all ( ̄x, x) ∼ D2 y and y ∈ Y. Let τ be a data-independent upper bound on the invariance loss with the trained representation as∥f ( ̄x) − f (x)∥ ≤ τ for all ( ̄x, x) ∼ D2 y, y ∈ Y, and f ∈ F. For the simplicity, we assume that there exists a function g∗ such that y = g∗(x) ∈ Rr for all (x, y) ∈ X × Y. Discarding this assumption adds the average of label noises to the final result, which goes to zero as the sample sizes n and m increase, assuming that the mean of the label noise is zero.\n\nG.2 RESULT\n\nThe following theorem is the complete version of Theorem 2: Theorem 4. For any δ > 0, with probability at least 1 − δ, the following holds:\n\nEx,y[lx,y(wS)] ≤ cI ̄S(fθ) +\n\n2 √\nm\n\n∥PZ ̄S Y ̄S∥F +\n\n1 √\nn\n\n∥PZS YS∥F + Qm,n,\n\n(31)\n\nwhere\n\nQm,n = c\n\n(cid:32)\n\n2 ̃Rm(F) √\nm\n\n(cid:114)\n\n+ τ\n\nln(3/δ) 2m\n\n+ τ ̄S\n\n(cid:114)\n\nln(3/δ) 2n\n\n(cid:33)\n\n(cid:114)\n\n+ κS\n\n2 ln(6|Y|/δ) 2n\n\n(cid:88)\n\n(cid:16)(cid:112)ˆp(y) + (cid:112)p(y)\n\n(cid:17)\n\n+\n\n4Rm(W ◦ F) m\n\n√\n\n+ 2κ\n\nln(4/δ) 2m\n\n+ 2κ ̄S\n\n(cid:114)\n\nln(4/δ) 2n\n\n.\n\ny∈Y (cid:114)\n\nProof. The complete proof is presented in Appendix G.3.\n\n18\n\nH(Z)=45datacentroidsH(Z)=40datacentroidsH(Z)=34datacentroidsUnder review as a conference paper at ICLR 2023\n\nThe bound in the complete version of Theorem 4 is better than the one in the informal version of Theorem 2, because of the factor c. The factor c measures the difference between the minimum norm solution WS of the labeled training data and the minimum norm solution W ̄S of the unlabeled training data. Thus, the factor c also decreases towards zero as n and m increase. Moreover, if the labeled and unlabeled training data are similar, the value of c is small, decreasing the generalization bound further, which makes sense. Thus, we can view the factor c as a measure on the distance between the labeled training data and the unlabeled training data.\n\nWe obtain the informal version from the complete version of Theorem 2 by the following reasoning to simplify the notation in the main text. We have that cI ̄S(fθ) + c 2 ̃Rm(F ) m + Q, where Q = (c − 1)(I ̄S(fθ) + 2 ̃Rm(F ) m ) ≤ ς → 0 as as m, n → ∞, since c → 0 as m, n → ∞. However, this reasoning is used only to simplify the notation in the main text. The bound in the complete version of Theorem 2 is more accurate and indeed tighter than the one in the informal version.\n\nm = I ̄S(fθ) + 2 ̃Rm(F )\n\n√\n\n√\n\n√\n\nIn Theorem 2, Qm,n → 0 as m, n → ∞ if Indeed, this typically holds because ̃Rm(F) = O(1) as m → ∞ for typical choices of F, including deep neural networks (Bartlett et al., 2017; Kawaguchi et al., 2018; Golowich et al., 2018) as well as other common machine learning models (Bartlett & Mendelson, 2002; Mohri et al., 2012; Shalev-Shwartz & Ben-David, 2014).\n\nm → 0 as m → ∞.\n\n ̃Rm(F ) √\n\nG.3 PROOF OF THEOREM 2\n\nProof of Theorem 2. Let W = WS where WS is the the minimum norm solution as WS = (cid:80)n minimizeW ′ ∥W ′∥F s.t. W ′ ∈ arg minW i=1 ∥W fθ(xi) − yi∥2. Let W ∗ = W ̄S where W ̄S is the minimum norm solution as W ∗ = W ̄S = minimizeW ′ ∥W ′∥F s.t. W ′ ∈ arg minW\n\ni )∥2. Since y = g∗(x),\n\ni=1 ∥W fθ(x+\n\ni ) − g∗(x+\n\n(cid:80)m\n\n1 n\n\n1 m\n\ny = g∗(x) ± W ∗fθ(x) = W ∗fθ(x) + (g∗(x) − W ∗fθ(x)) = W ∗fθ(x) + φ(x)\n\nwhere φ(x) = g∗(x) − W ∗fθ(x). Define LS(w) = 1\n\nn\n\n(cid:80)n\n\ni=1 ∥W fθ(xi) − yi∥. Using these,\n\nLS(w) =\n\n=\n\n≥\n\n=\n\n1 n\n\n1 n\n\n1 n\n\n1 n\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\ni=1 n\n(cid:88)\n\ni=1 n\n(cid:88)\n\ni=1\n\n∥W fθ(xi) − yi∥\n\n∥W fθ(xi) − W ∗fθ(xi) − φ(xi)∥\n\n∥W fθ(xi) − W ∗fθ(xi)∥ −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥\n\n∥ ̃W fθ(xi)∥ −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥\n\nwhere ̃W = W − W ∗. We now consider new fresh samples ̄xi ∼ Dyi for i = 1, . . . , n to rewrite the above further as:\n\nLS(w) ≥\n\n=\n\n≥\n\n=\n\n1 n\n\n1 n\n\n1 n\n\n1 n\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\ni=1 n\n(cid:88)\n\ni=1 n\n(cid:88)\n\ni=1\n\n∥ ̃W fθ(xi) ± ̃W fθ( ̄xi)∥ −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥\n\n∥ ̃W fθ( ̄xi) − ( ̃W fθ( ̄xi) − ̃W fθ(xi))∥ −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥\n\n∥ ̃W fθ( ̄xi)∥ −\n\n∥ ̃W fθ( ̄xi)∥ −\n\n1 n\n\n1 n\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\ni=1\n\nn (cid:88)\n\n∥φ(xi)∥\n\n1 n\n\ni=1 n\n(cid:88)\n\ni=1\n\n∥φ(xi)∥\n\n∥ ̃W fθ( ̄xi) − ̃W fθ(xi)∥ −\n\n∥ ̃W (fθ( ̄xi) − fθ(xi))∥ −\n\n1 n\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nThis implies that\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥ ̃W fθ( ̄xi)∥ ≤ LS(w) +\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥ ̃W (fθ( ̄xi) − fθ(xi))∥ +\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥.\n\nFurthermore, since y = W ∗fθ(x) + φ(x), by writing ̄yi = W ∗fθ( ̄xi) + φ( ̄xi) (where ̄yi = yi since ̄xi ∼ Dyi for i = 1, . . . , n),\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥ ̃W fθ( ̄xi)∥ =\n\n=\n\n≥\n\n1 n\n\n1 n\n\n1 n\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\ni=1 n\n(cid:88)\n\ni=1\n\n∥W fθ( ̄xi) − W ∗fθ( ̄xi)∥\n\n∥W fθ( ̄xi) − ̄yi + φ( ̄xi)∥\n\n∥W fθ( ̄xi) − ̄yi∥ −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ( ̄xi)∥\n\nCombining these, we have that\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥W fθ( ̄xi) − ̄yi∥ ≤ LS(w) +\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥ ̃W (fθ( ̄xi) − fθ(xi))∥\n\n(32)\n\n+\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥ +\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ( ̄xi)∥.\n\nTo bound the left-hand side of equation 32, we now analyze the following random variable:\n\nEX,Y [∥WSfθ(X) − Y ∥] −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥WSfθ( ̄xi) − ̄yi∥,\n\n(33)\n\n(cid:80)n\n\ni=1 ∥WSfθ( ̄xi) − ̄yi∥ ≤ supW ∈W\n\nwhere ̄yi = yi since ̄xi ∼ Dyi for i = 1, . . . , n. Importantly, this means that as WS depends on yi, WS depends on ̄yi. Thus, the collection of random variables ∥WSfθ( ̄x1) − ̄y1∥, . . . , ∥WSfθ(nn) − ̄yn∥ is not independent. Accordingly, we cannot apply standard concentration inequality to bound equation 33. A standard approach in learning theory is to first bound equation 33 by Ex,y∥WSfθ(x)− y∥ − 1 i=1 ∥W fθ( ̄xi) − ̄yi∥ for n\nsome hypothesis space W (that is independent of S) and realize that the right-hand side now contains the collection of independent random variables ∥Wfθ( ̄x1) − ̄y1∥, . . . , ∥Wfθ(nn) − ̄yn∥ , for which we can utilize standard concentration inequalities. This reasoning leads to the Rademacher complexity of the hypothesis space W. However, the complexity of the hypothesis space W can be very large, resulting into a loose bound. In this proof, we show that we can avoid the dependency on hypothesis space W by using a very different approach with conditional expectations to take care the dependent random variables ∥WSfθ( ̄x1) − ̄y1∥, . . . , ∥WSfθ(nn) − ̄yn∥. Intuitively, we utilize the fact that for these dependent random variables, there are a structure of conditional independence, conditioned on each y ∈ Y.\n\nEx,y∥W fθ(x) − y∥ − 1\n\n(cid:80)n\n\nn\n\nWe first write the expected loss as the sum of the conditional expected loss:\n\nEX,Y [∥WSfθ(X) − Y ∥] =\n\n=\n\n(cid:88)\n\ny∈Y (cid:88)\n\ny∈Y\n\nEX,Y [∥WSfθ(X) − Y ∥ | Y = y]P(Y = y)\n\nEXy [∥WSfθ(Xy) − y∥]P(Y = y),\n\nwhere Xy is the random variable for the conditional with Y = y. Using this, we decompose equation 33 into two terms:\n\nEX,Y [∥WSfθ(X) − Y ∥] −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥WSfθ( ̄xi) − ̄yi∥\n\n(34)\n\n\n\n=\n\n\n\n+\n\n(cid:88)\n\ny∈Y\n\n(cid:88)\n\ny∈Y\n\nEXy [∥WSfθ(Xy) − y∥]\n\n|Iy| n\n\n−\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥WSfθ( ̄xi) − ̄yi∥\n\n\n\n\n\nEXy [∥WSfθ(Xy) − y∥]\n\n(cid:18)\n\nP(Y = y) −\n\n(cid:19)\n\n,\n\n|Iy| n\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nwhere\n\nThe first term in the right-hand side of equation 34 is further simplified by using\n\nIy = {i ∈ [n] : yi = y}.\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥WSfθ( ̄xi) − ̄yi∥ =\n\n1 n\n\n(cid:88)\n\n(cid:88)\n\ny∈Y\n\ni∈Iy\n\n∥WSfθ( ̄xi) − y∥,\n\nas\n\nEXy [∥WSfθ(Xy) − y∥]\n\n(cid:88)\n\ny∈Y\n\n|Iy| n\n\n−\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥WSfθ( ̄xi) − ̄yi∥\n\n=\n\n1 n\n\n(cid:88)\n\ny∈ ̃Y\n\n EXy [∥WSfθ(Xy) − y∥] −\n\n|Iy|\n\n\n\n∥WSfθ( ̄xi) − y∥\n\n ,\n\n1 |Iy|\n\n(cid:88)\n\ni∈Iy\n\nwhere ̃Y = {y ∈ Y : |Iy| ̸= 0}. Substituting these into equation equation 34 yields\n\nEX,Y [∥WSfθ(X) − Y ∥] −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥WSfθ( ̄xi) − ̄yi∥\n\n(35)\n\n EXy [∥WSfθ(Xy) − y∥] −\n\n|Iy|\n\n\n\n∥WSfθ( ̄xi) − y∥\n\n\n\n1 |Iy|\n\n(cid:88)\n\ni∈Iy\n\nEXy [∥WSfθ(Xy) − y∥]\n\n(cid:18)\n\nP(Y = y) −\n\n(cid:19)\n\n|Iy| n\n\n=\n\n1 n\n\n+\n\n(cid:88)\n\ny∈ ̃Y\n\n(cid:88)\n\ny∈Y\n\nImportantly, while ∥WSfθ( ̄x1) − ̄y1∥, . . . , ∥WSfθ( ̄xn) − ̄yn∥ on the right-hand side of equation 35 are dependent random variables, ∥WSfθ( ̄x1) − y∥, . . . , ∥WSfθ( ̄xn) − y∥ are independent random variables since WS and ̄xi are independent and y is fixed here. Thus, by using Hoeffding’s inequality (Lemma 1), and taking union bounds over y ∈ ̃Y, we have that with probability at least 1 − δ, the following holds for all y ∈ ̃Y:\n\nEXy [∥WSfθ(Xy) − y∥] −\n\n1 |Iy|\n\n(cid:88)\n\ni∈Iy\n\n∥WSfθ( ̄xi) − y∥ ≤ κS\n\n(cid:115)\n\nln(| ̃Y|/δ) 2|Iy|\n\n.\n\nThis implies that with probability at least 1 − δ,\n\n EXy [∥WSfθ(Xy) − y∥] −\n\n|Iy|\n\n∥WSfθ( ̄xi) − y∥\n\n\n\n\n\n1 |Iy|\n\n(cid:88)\n\ni∈Iy\n\n1 n\n\n(cid:88)\n\ny∈ ̃Y\n\n≤\n\nκS n\n\n(cid:88)\n\ny∈ ̃Y \n\n(cid:115)\n\n|Iy|\n\nln(| ̃Y|/δ) 2|Iy|\n\n(cid:114)\n\n\n\n(cid:115)\n\n\n\n|Iy| n\n\nln(| ̃Y|/δ) 2n\n\n.\n\n(cid:88)\n\n= κS\n\n\n\ny∈ ̃Y\n\nSubstituting this bound into equation 35, we have that with probability at least 1 − δ,\n\nEX,Y [∥WSfθ(X) − Y ∥] −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥WSfθ( ̄xi) − ̄yi∥\n\n\n\n\n\n(cid:115)\n\n≤ κS\n\n\n\n(cid:88)\n\n(cid:112)ˆp(y)\n\n\n\ny∈ ̃Y\n\nln(| ̃Y|/δ) 2n\n\n+\n\n(cid:88)\n\ny∈Y\n\nEXy [∥WSfθ(Xy) − y∥]\n\n(cid:18)\n\nP(Y = y) −\n\n(cid:19)\n\n|Iy| n\n\n(36)\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nwhere\n\nˆp(y) =\n\n|Iy| n\n\n.\n\nMoreover, for the second term on the right-hand side of equation 36, by using Lemma 1 of (Kawaguchi et al., 2022), we have that with probability at least 1 − δ,\n\nEXy [∥WSfθ(Xy) − y∥]\n\n(cid:18)\n\nP(Y = y) −\n\n(cid:19)\n\n|Iy| n\n\n(cid:88)\n\ny∈Y\n\n(cid:112)p(y)EXy [∥WSfθ(Xy) − y∥\n\n\n\n\n\n(cid:114)\n\n2 ln(|Y|/δ) 2n\n\n\n\n≤\n\n\n\n(cid:88)\n\ny∈Y \n\n≤ κS\n\n\n\n(cid:88)\n\n(cid:112)p(y)\n\n\n\ny∈Y\n\n\n\n(cid:114)\n\n2 ln(|Y|/δ) 2n\n\nwhere p(y) = P(Y = y). Substituting this bound into equation 36 with the union bound, we have that with probability at least 1 − δ,\n\nEX,Y [∥WSfθ(X) − Y ∥] −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥WSfθ( ̄xi) − ̄yi∥\n\n(37)\n\n\n\n≤ κS\n\n\n\n\n\n(cid:115)\n\n(cid:112)ˆp(y)\n\n\n\n(cid:88)\n\ny∈ ̃Y\n\nln(2| ̃Y|/δ) 2n\n\n\n\n\n\n≤\n\n\n\n(cid:88)\n\n(cid:112)ˆp(y)\n\n κS\n\n(cid:114)\n\n2 ln(2|Y|/δ) 2n\n\n\n\n+ κS\n\n\n\n(cid:88)\n\n(cid:112)p(y)\n\n(cid:114)\n\n\n\n\n\ny∈Y\n\n\n\n\n\n+\n\n\n\n(cid:88)\n\n(cid:112)p(y)\n\n κS\n\ny∈Y\n\n2 ln(2|Y|/δ) 2n\n\n(cid:114)\n\n2 ln(2|Y|/δ) 2n\n\ny∈Y (cid:114)\n\n≤ κS\n\n2 ln(2|Y|/δ) 2n\n\n(cid:88)\n\n(cid:16)(cid:112)ˆp(y) + (cid:112)p(y)\n\n(cid:17)\n\ny∈Y\n\nCombining equation 32 and equation 37 implies that with probability at least 1 − δ,\n\nEX,Y [∥WSfθ(X) − Y ∥]\n\n≤\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥WSfθ( ̄xi) − ̄yi∥ + κS\n\n(cid:114)\n\n2 ln(2|Y|/δ) 2n\n\n(cid:88)\n\n(cid:16)(cid:112)ˆp(y) + (cid:112)p(y)\n\n(cid:17)\n\ny∈Y\n\n(38)\n\n≤ LS(wS) +\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥ ̃W (fθ( ̄xi) − fθ(xi))∥\n\n+\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥ +\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ( ̄xi)∥ + κS\n\n(cid:114)\n\n2 ln(2|Y|/δ) 2n\n\n(cid:88)\n\n(cid:16)(cid:112)ˆp(y) + (cid:112)p(y)\n\n(cid:17)\n\n.\n\ny∈Y\n\nWe will now analyze the term 1 tion 38. Since W ∗ = W ̄S,\n\nn\n\n(cid:80)n\n\ni=1 ∥φ(xi)∥ + 1\n\nn\n\n(cid:80)n\n\ni=1 ∥φ( ̄xi)∥ on the right-hand side of equa-\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥ =\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥g∗(xi) − W ̄Sfθ(xi)∥.\n\nBy using Hoeffding’s inequality (Lemma 1), we have that for any δ > 0, with probability at least 1 − δ,\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥ ≤\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥g∗(xi) − W ̄Sfθ(xi)∥ ≤ Ex+[∥g∗(x+) − W ̄Sfθ(x+)∥] + κ ̄S\n\n(cid:114)\n\nln(1/δ) 2n\n\n.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nMoreover, by using (Mohri et al., 2012, Theorem 3.1) with the loss function x+ (cid:55)→ ∥g∗(x+) − W f (x+)∥ (i.e., Lemma 2), we have that for any δ > 0, with probability at least 1 − δ,\n\nEx+[∥g∗(x+) − W ̄Sfθ(x+)∥] ≤\n\n1 m\n\nm (cid:88)\n\ni=1\n\n∥g∗(x+\n\ni ) − W ̄Sfθ(x+\n\ni )∥ +\n\n2 ̃Rm(W ◦ F) √\nm\n\n(cid:114)\n\n+ κ\n\nln(1/δ) 2m\n\n(39)\n\nm\n\nE ̄S,ξ[supW ∈W,f ∈F\n\nwhere ̃Rm(W ◦ F) = 1√ i )∥] is the normalized Rademacher complexity of the set {x+ (cid:55)→ ∥g∗(x+) − Wf (x+)∥ : W ∈ W, f ∈ F} (it is normalized such that ̃Rm(F) = O(1) as m → ∞ for typical choices of F), and ξ1, . . . , ξm are independent uniform random variables taking values in {−1, 1}. Takinng union bounds, we have that for any δ > 0, with probability at least 1 − δ,\n\ni ) − Wf (x+\n\ni=1 ξi∥g∗(x+\n\n(cid:80)m\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥ ≤\n\n1 m\n\nm (cid:88)\n\ni=1\n\n∥g∗(x+\n\ni ) − W ̄Sfθ(x+\n\ni )∥ +\n\n2 ̃Rm(W ◦ F) √\nm\n\n(cid:114)\n\n+ κ\n\nln(2/δ) 2m\n\n+ κ ̄S\n\n(cid:114)\n\nln(2/δ) 2n\n\nSimilarly, for any δ > 0, with probability at least 1 − δ,\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ( ̄xi)∥ ≤\n\n1 m\n\nm (cid:88)\n\ni=1\n\n∥g∗(x+\n\ni ) − W ̄Sfθ(x+\n\ni )∥ +\n\n2 ̃Rm(W ◦ F) √\nm\n\n(cid:114)\n\n+ κ\n\nln(2/δ) 2m\n\n+ κ ̄S\n\n(cid:114)\n\nln(2/δ) 2n\n\n.\n\nThus, by taking union bounds, we have that for any δ > 0, with probability at least 1 − δ,\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥ +\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ( ̄xi)∥\n\n≤\n\n2 m\n\nm (cid:88)\n\ni=1\n\n∥g∗(x+\n\ni ) − W ̄Sfθ(x+\n\ni )∥ +\n\n4Rm(W ◦ F) m\n\n√\n\n(cid:114)\n\n+ 2κ\n\nln(4/δ) 2m\n\n+ 2κ ̄S\n\n(cid:114)\n\nln(4/δ) 2n\n\nTo analyze the first term on the right-hand side of equation 40, recall that\n\nW ̄S = minimize\n\nW ′\n\n∥W ′∥F s.t. W ′ ∈ arg min\n\nW\n\n1 m\n\nm (cid:88)\n\ni=1\n\nHere, since W fθ(x+\n\ni ) ∈ Rr, we have that\n\n∥W fθ(x+\n\ni ) − g∗(x+\n\ni )∥2.\n\nW fθ(x+\n\ni ) = vec[W fθ(x+\n\ni )] = [fθ(x+\n\ni )⊤ ⊗ Ir] vec[W ] ∈ Rr,\n\n(40)\n\n(41)\n\nwhere Ir ∈ Rr×r is the identity matrix, and [fθ(x+ i )⊤ ⊗ Ir] ∈ Rr×dr is the Kronecker product of the two matrices, and vec[W ] ∈ Rdr is the vectorization of the matrix W ∈ Rr×d. Thus, by i )⊤ ⊗ Ir] ∈ Rr×dr and using the notation of w = vec[W ] and its inverse defining Ai = [fθ(x+ W = vec−1[w] (i.e., the inverse of the vectorization from Rr×d to Rdr with a fixed ordering), we can rewrite equation 41 by\n\nW ̄S = vec−1[w ̄S] where w ̄S = minimize\n\nw′\n\n∥w′∥F s.t. w′ ∈ arg min\n\nw\n\nm (cid:88)\n\ni=1\n\n∥gi − Aiw∥2,\n\nwith gi = g∗(x+ sufficient condition of the minimizer of this function is obtained by\n\ni ) ∈ Rr. Since the function w (cid:55)→ (cid:80)m\n\ni=1 ∥gi − Aiw∥2 is convex, a necessary and\n\n0 = ∇w\n\nm (cid:88)\n\ni=1\n\n∥gi − Aiw∥2 = 2\n\nThis implies that\n\nm (cid:88)\n\ni=1\n\nA⊤\n\ni Aiw =\n\n23\n\nm (cid:88)\n\ni=1\n\nm (cid:88)\n\ni=1\n\nA⊤\n\ni (gi − Aiw) ∈ Rdr\n\nA⊤\n\ni gi.\n\nUnder review as a conference paper at ICLR 2023\n\nIn other words,\n\nA⊤Aw = A⊤g where A =\n\n\n\n \n\n\nA1 A2 ... Am\n\n\n\n \n\n\n∈ Rmr×dr and g =\n\n\n\n \n\n\n∈ Rmr\n\n\n\n \n\n\ng1 g2 ... gm\n\nThus,\n\nw′ ∈ arg min\n\nw\n\nm (cid:88)\n\ni=1\n\n∥gi − Aiw∥2 = {(A⊤A)†A⊤g + v : v ∈ Null(A)}\n\nwhere (A⊤A)† is the Moore–Penrose inverse of the matrix A⊤A and Null(A) is the null space of the matrix A. Thus, the minimum norm solution is obtained by\n\nThus, by using this W ̄S, we have that\n\nvec[W ̄S] = w ̄S = (A⊤A)†A⊤g.\n\n1 m\n\nm (cid:88)\n\ni=1\n\n∥g∗(x+\n\ni ) − W ̄Sfθ(x+\n\ni )∥ =\n\n≤\n\n=\n\n=\n\n=\n\n1 m\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nm (cid:88)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nr (cid:88)\n\n((gi − Aiw ̄S)k)2\n\ni=1\n\nk=1\n\n1 m\n\nm (cid:88)\n\nr (cid:88)\n\n((gi − Aiw ̄S)k)2\n\ni=1\n\nk=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nm (cid:88)\n\nr (cid:88)\n\ni=1\n\nk=1\n\n((gi − Aiw ̄S)k)2\n\n∥g − Aw ̄S∥2\n\n∥g − A(A⊤A)†A⊤g∥2 =\n\n1 √\nm\n\n∥(I − A(A⊤A)†A⊤)g∥2\n\n1 √\nm\n\n1 √\nm 1\n√ m\n\nwhere the inequality follows from the Jensen’s inequality and the concavity of the square root function. Thus, we have that\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ(xi)∥ +\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥φ( ̄xi)∥\n\n≤\n\n2 √\nm\n\n∥(I − A(A⊤A)†A⊤)g∥2 +\n\n4Rm(W ◦ F) m\n\n√\n\n+ 2κ\n\n(cid:114)\n\nln(4/δ) 2m\n\n+ 2κ ̄S\n\n(cid:114)\n\nln(4/δ) 2n\n\nBy combining equation 38 and equation 42 with union bound, we have that\n\nEX,Y [∥WSfθ(X) − Y ∥]\n\n≤ LS(wS) +\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥ ̃W (fθ( ̄xi) − fθ(xi))∥ +\n\n2 √\nm\n\n∥PAg∥2\n\n+\n\n4Rm(W ◦ F) m\n\n√\n\n(cid:114)\n\n+ 2κ\n\nln(8/δ) 2m\n\n+ 2κ ̄S\n\n(cid:114)\n\nln(8/δ) 2n\n\n(cid:114)\n\n+ κS\n\n2 ln(4|Y|/δ) 2n\n\n(cid:88)\n\n(cid:16)(cid:112)ˆp(y) + (cid:112)p(y)\n\n(cid:17)\n\n.\n\ny∈Y\n\n(42)\n\n(43)\n\nwhere ̃W = WS − W ∗ and PA = I − A(A⊤A)†A⊤.\n\nWe will now analyze the second term on the right-hand side of equation 43:\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥ ̃W (fθ( ̄xi) − fθ(xi))∥ ≤ ∥ ̃W ∥2\n\n(cid:32)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:33)\n\n∥fθ( ̄xi) − fθ(xi)∥\n\n,\n\n(44)\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nwhere ∥ ̃W ∥2 is the spectral norm of ̃W . Since ̄xi shares the same label with xi as ̄xi ∼ Dyi (and xi ∼ Dyi), and because fθ is trained with the unlabeled data ̄S, using Hoeffding’s inequality (Lemma 1) implies that with probability at least 1 − δ,\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥fθ( ̄xi) − fθ(xi)∥ ≤ Ey∼ρE ̄x,x∼D2\n\ny\n\n[∥fθ( ̄x) − fθ(x)∥] + τ ̄S\n\n(cid:114)\n\nln(1/δ) 2n\n\n.\n\n(45)\n\nMoreover, by using (Mohri et al., 2012, Theorem 3.1) with the loss function (x, ̄x) (cid:55)→ ∥fθ( ̄x)−fθ(x)∥ (i.e., Lemma 2), we have that with probability at least 1 − δ,\n\nEy∼ρE ̄x,x∼D2\n\ny\n\n[∥fθ( ̄x) − fθ(x)∥] ≤\n\n1 m\n\nm (cid:88)\n\ni=1\n\n∥fθ(x+\n\ni ) − fθ(x++\n\ni\n\n)∥ +\n\n2 ̃Rm(F) √\nm\n\n(cid:114)\n\n+ τ\n\nln(1/δ) 2m\n\n(46)\n\n(cid:80)m\n\nm\n\nE ̄S,ξ[supf ∈F\n\nwhere ̃Rm(F) = 1√ )∥] is the normalized Rademacher complexity of the set {(x+, x++) (cid:55)→ ∥f (x+) − f (x++)∥ : f ∈ F} (it is normalized such that ̃Rm(F) = O(1) as m → ∞ for typical choices of F), and ξ1, . . . , ξm are independent uniform random variables taking values in {−1, 1}. Thus, taking union bound, we have that for any δ > 0, with probability at least 1 − δ,\n\ni ) − f (x++\n\ni=1 ξi∥f (x+\n\ni\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥ ̃W (fθ( ̄xi) − fθ(xi))∥\n\n≤ ∥ ̃W ∥2\n\n(cid:32)\n\n1 m\n\nm (cid:88)\n\ni=1\n\n∥fθ(x+\n\ni ) − fθ(x++\n\ni\n\n)∥ +\n\n2 ̃Rm(F) √\nm\n\n(cid:114)\n\n+ τ\n\nln(2/δ) 2m\n\n+ +τ ̄S\n\n(cid:114)\n\n(cid:33)\n\n.\n\nln(2/δ) 2n\n\n(47)\n\nBy combining equation 43 and equation 47 using the union bound, we have that with probability at least 1 − δ, EX,Y [∥WSfθ(X) − Y ∥]\n\n≤ LS(wS) + ∥ ̃W ∥2\n\n(cid:32)\n\n1 m\n\nm (cid:88)\n\ni=1\n\n∥fθ(x+\n\ni ) − fθ(x++\n\ni\n\n)∥ +\n\n2 ̃Rm(F) √\nm\n\n(cid:114)\n\n+ τ\n\nln(4/δ) 2m\n\n+ τ ̄S\n\n(cid:114)\n\nln(4/δ) 2n\n\n(48) (cid:33)\n\n4Rm(W ◦ F) m\n\n√\n\n(cid:114)\n\n+ 2κ\n\nln(16/δ) 2m\n\n+ 2κ ̄S\n\n(cid:114)\n\nln(16/δ) 2n\n\n+\n\n∥PAg∥2 +\n\n2 √\nm (cid:114)\n\n+ κS\n\n2 ln(8|Y|/δ) 2n\n\n(cid:88)\n\n(cid:16)(cid:112)ˆp(y) + (cid:112)p(y)\n\n(cid:17)\n\ny∈Y\n\n= LS(wS) + ∥ ̃W ∥2\n\n(cid:32)\n\n1 m\n\nm (cid:88)\n\ni=1\n\n∥fθ(x+\n\ni ) − fθ(x++\n\ni\n\n(cid:33)\n\n)∥\n\n+\n\n2 √\nm\n\n∥PAg∥2 + Qm,n\n\nwhere\n\nQm,n = ∥ ̃W ∥2\n\n(cid:32)\n\n2 ̃Rm(F) √\nm\n\n(cid:114)\n\n+ τ\n\nln(3/δ) 2m\n\n+ τ ̄S\n\n(cid:114)\n\nln(3/δ) 2n\n\n(cid:33)\n\n(cid:114)\n\n+ κS\n\n2 ln(6|Y|/δ) 2n\n\n(cid:88)\n\n(cid:16)(cid:112)ˆp(y) + (cid:112)p(y)\n\n(cid:17)\n\ny∈Y (cid:114)\n\n+\n\n4Rm(W ◦ F) m\n\n√\n\n+ 2κ\n\nln(4/δ) 2m\n\n+ 2κ ̄S\n\n(cid:114)\n\n.\n\nln(4/δ) 2n ⊤ ⊗ Ir]. Thus, ⊤)†Z ̄S ⊗ Ir] = [PZ ̄S ⊗ Ir]\n\nDefine Z ̄S = [f (x+ PA = I − [Z ̄S\n\n1 ), . . . , f (x+ ⊤ ⊗ Ir][Z ̄SZ ̄S\n\nm)] ∈ Rd×m. Then, we have A = [Z ̄S ⊤(Z ̄SZ ̄S ⊤ ⊗ Ir]†[Z ̄S ⊗ Ir] = I − [Z ̄S\n\nwhere PZ ̄S = Im − Z ̄S Rm×r, since g = vec[Y ⊤\n\n ̄S ],\n\n⊤(Z ̄SZ ̄S\n\n⊤)†Z ̄S ∈ Rm×m. By defining Y ̄S = [g∗(x+\n\n1 ), . . . , g∗(x+\n\nm)]⊤ ∈\n\n∥PAg∥2 = ∥[PZ ̄S ⊗ Ir] vec[Y ⊤\n\n ̄S ]∥2 = ∥ vec[Y ⊤\n\n ̄S PZ ̄S ]∥2 = ∥PZ ̄S Y ̄S∥F\n\n(49)\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nOn the other hand, recall that WS is the minimum norm solution as\n\nWS = minimize\n\nW ′\n\n∥W ′∥F s.t. W ′ ∈ arg min\n\nW\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥W fθ(xi) − yi∥2.\n\nBy solving this, we have\n\nWS = Y ⊤ZS\n\n⊤(ZSZS\n\n⊤)†,\n\nwhere ZS = [f (x1), . . . , f (xn)] ∈ Rd×n and YS = [y1, . . . , yn]⊤ ∈ Rn×r. Then,\n\nLS(wS) =\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∥WSfθ(xi) − yi∥ =\n\n1 n\n\nn (cid:88)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nr (cid:88)\n\n((WSfθ(xi) − yi)k)2\n\ni=1\n\nk=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n1 n\n\nn (cid:88)\n\nr (cid:88)\n\n((WSfθ(xi) − yi)k)2\n\ni=1\n\nk=1\n\n1 √\nn 1\n√ n\n1 √\nn\n\n∥WSZS − Y ⊤∥F\n\n∥Y ⊤(ZS\n\n⊤(ZSZS\n\n⊤)†ZS − I)∥F\n\n∥(I − ZS\n\n⊤(ZSZS\n\n⊤)†ZS)Y ∥F\n\n≤\n\n=\n\n=\n\n=\n\nThus,\n\nLS(wS) =\n\n1 √\nn\n\n∥PZS Y ∥F\n\n(50)\n\nwhere PZS = I − ZS\n\n⊤(ZSZS\n\n⊤)†ZS.\n\nBy combining equation 48–equation 50 and using 1 ≤ 1 − δ,\n\n√\n\n2, we have that with probability at least\n\nEX,Y [∥WSfθ(X) − Y ∥] ≤ cI ̄S(fθ) +\n\n2 √\nm\n\n∥PZ ̄S Y ̄S∥F +\n\n1 √\nn\n\n∥PZS YS∥F + Qm,n,\n\n(51)\n\nwhere\n\nQm,n = c\n\n(cid:32)\n\n2 ̃Rm(F) √\nm\n\n(cid:114)\n\n+ τ\n\nln(3/δ) 2m\n\n+ τ ̄S\n\n(cid:114)\n\nln(3/δ) 2n\n\n(cid:33)\n\n(cid:114)\n\n+ κS\n\n2 ln(6|Y|/δ) 2n\n\n(cid:88)\n\n(cid:16)(cid:112)ˆp(y) + (cid:112)p(y)\n\n(cid:17)\n\n+\n\n4Rm(W ◦ F) m\n\n√\n\n+ 2κ\n\nln(4/δ) 2m\n\n+ 2κ ̄S\n\n(cid:114)\n\nln(4/δ) 2n\n\n.\n\ny∈Y (cid:114)\n\nH KNOWN LEMMAS\n\nWe use the following well-known theorems as lemmas in our proof. We put these below for the completeness. These are classical results and not our results.\n\nLemma 1. (Hoeffding’s such that a ≤ Xi ≤ b almost surely.\n\ninequality) Let X1, ..., Xn be independent\n\nConsider the average of\n\nrandom variables these random variables,\n\nSn =\n\n1 n\n\n(X1 + · · · + Xn). Then, for all t > 0,\n\n(cid:32)\n\n(cid:114)\n\nPS\n\nE [Sn] − Sn ≥ (b − a)\n\n(cid:33)\n\nln(1/δ) 2n\n\n≤ δ,\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nand\n\n(cid:32)\n\n(cid:114)\n\nPS\n\nSn − E [Sn] ≥ (b − a)\n\n(cid:33)\n\nln(1/δ) 2n\n\n≤ δ.\n\nProof. By using Hoeffding’s inequality, we have that for all t > 0,\n\nand\n\nPS (E [Sn] − Sn ≥ t) ≤ exp\n\nPS (Sn − E [Sn] ≥ t) ≤ exp\n\n(cid:18)\n\n−\n\n2nt2 (b − a)2\n\n(cid:19)\n\n(cid:18)\n\n−\n\n2nt2 (b − a)2\n\n(cid:19)\n\n,\n\n,\n\nSetting δ = exp\n\n(cid:16)\n\n− 2nt2\n\n(b−a)2\n\n(cid:17)\n\nand solving for t > 0,\n\n1/δ = exp\n\n(cid:18) 2nt2\n\n(cid:19)\n\n(b − a)2\n\n=⇒ ln(1/δ) =\n\n2nt2 (b − a)2\n\n=⇒\n\n(b − a)2 ln(1/δ) 2n\n\n= t2\n\n=⇒ t = (b − a)\n\n(cid:114)\n\nln(1/δ) 2n\n\nIt has been shown that generalization bounds can be obtained via Rademacher complexity (Bartlett & Mendelson, 2002; Mohri et al., 2012; Shalev-Shwartz & Ben-David, 2014). The following is a trivial modification of (Mohri et al., 2012, Theorem 3.1) for a one-sided bound on the nonnegative general loss functions:\n\nLemma 2. Let G be a set of functions with the codomain [0, M ]. Then, for any δ > 0, with probability at least 1 − δ over an i.i.d. draw of m samples S = (qi)m\n\ni=1, the following holds for all ψ ∈ G:\n\nEq[ψ(q)] ≤\n\n1 m\n\nm (cid:88)\n\ni=1\n\nψ(qi) + 2Rm(G) + M\n\n(cid:114)\n\nln(1/δ) 2m\n\n,\n\n(52)\n\nwhere Rm(G) := ES,ξ[supψ∈G variables taking values in {−1, 1}.\n\n1 m\n\n(cid:80)m\n\ni=1 ξiψ(qi)] and ξ1, . . . , ξm are independent uniform random\n\nProof. Let S = (qi)m\n\ni=1 and S′ = (q′\n\ni)m\n\ni=1. Define\n\nφ(S) = sup ψ∈G\n\nEx,y[ψ(q)] −\n\n1 m\n\nm (cid:88)\n\ni=1\n\nψ(qi).\n\n(53)\n\nTo apply McDiarmid’s inequality to φ(S), we compute an upper bound on |φ(S) − φ(S′)| where S and S′ be two test datasets differing by exactly one point of an arbitrary index i0; i.e., Si = S′ i for all i ̸= i0 and Si0 ̸= S′\n\ni0. Then,\n\nφ(S′) − φ(S) ≤ sup\n\nψ∈G\n\nψ(qi0) − ψ(q′ m\n\ni0\n\n)\n\n≤\n\nM m\n\n.\n\n(54)\n\nSimilarly, φ(S) − φ(S′) ≤ M least 1 − δ,\n\nm . Thus, by McDiarmid’s inequality, for any δ > 0, with probability at\n\nφ(S) ≤ ES[φ(S)] + M\n\n(cid:114)\n\nln(1/δ) 2m\n\n.\n\n(55)\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nMoreover,\n\nES[φ(S)]\n\n(cid:34)\n\n= ES\n\nsup ψ∈G (cid:34)\n\nES′\n\n(cid:34)\n\n1 m\n\nm (cid:88)\n\ni=1\n\n(cid:35)\n\nψ(q′ i)\n\n−\n\n1 m\n\nm (cid:88)\n\n(cid:35)\n\nψ(qi)\n\ni=1 (cid:35)\n\n≤ ES,S′\n\nsup ψ∈G (cid:34)\n\n≤ Eξ,S,S′\n\nsup ψ∈G\n\n1 m\n\nm (cid:88)\n\n(ψ(q′\n\ni) − ψ(qi))\n\ni=1\n\n(cid:35)\n\nξi(ψ(q′\n\ni) − ψ(qi))\n\n1 m\n\nm (cid:88)\n\ni=1\n\n≤ 2Eξ,S\n\n(cid:34)\n\n1 m\n\nm (cid:88)\n\ni=1\n\nsup ψ∈G\n\n(cid:35)\n\nξiψ(qi)\n\n= 2Rm(G)\n\n(56)\n\n(57)\n\n(58)\n\n(59)\n\n(60)\n\nwhere the fist line follows the definitions of each term, the second line uses the Jensen’s inequality and the convexity of the supremum, and the third line follows that for each ξi ∈ {−1, +1}, the distribution i) − l(f (xi), yi)) is the distribution of (l(f(x′ of each term ξi(l(f(x′ i) − l(f (xi), yi)) since S and S′ are drawn iid with the same distribution. The forth line uses the subadditivity of supremum.\n\ni), y′\n\ni), y′\n\nI SIMCLR\n\nIn contrastive learning, different augmented views of the same image are attracted (positive pairs), while different augmented views are repelled (negative pairs). MoCo (He et al., 2020) and SimCLR (Chen et al., 2020) are recent examples of self-supervised visual representation learning that reduce the gap between self-supervised and fully-supervised learning. SimCLR applies randomized augmentations to an image to create two different views, x and y, and encodes both of them with a shared encoder, producing representations rx and ry. Both rx and ry are l2-normalized. The SimCLR version of the InfoNCE objective is:\n\n(cid:34)\n\n(cid:32)\n\nEx,y\n\n− log\n\n1\n\nη rT\n\ny rx\n\n(cid:33)(cid:35)\n\ne (cid:80)K\n\nk=1 e\n\n1\n\nη rT yk\n\nrx\n\nwhere η is a temperature term and K is the number of views in a minibatch\n\n28",
    "reference": "# Summary Of The Paper\n\nThe submission explores information-theoretically motivated objectives for self-supervised learning (SSL). \nThe submission derives a bound on an objective to maximize the mutual information between neural network inputs and outputs using assumptions about Gaussianity of the input distribution and a spline framing of neural network functions as in Montufar et al. (2014) and Balestriero & Baraniuk (2018). \nA small experiment tests deviations from the normality of the distribution of outputs of a neural network, given inputs from a KDE-like expansion of the CIFAR-10 test dataset to validate the theory's assumptions.\nA generalization bound and relationships to prior criteria for SSL optimization (spec. VICReg) are also derived.\n\n# Strength And Weaknesses\n\n### Strengths\n\n1. **Clarity.** The paper is extremely well-written. I had no trouble understanding the ideas presented in the work and their relationship to prior work.\n\n1. **Reasonable motivation.** The paper aims to derive SSL algorithms from an information-theoretic perspective, given that naive formulations thereof are ill-defined. This is reasonable and of interest to the community.\n\n1. **Novelty.** Using a neural-networks-as-splines argument to derive an information-theoretic SSL optimization criterion appears novel.\n\n### Weaknesses\n\n1. **Assumptions are insufficiently related to practice.**\nBroadly, the manuscript aims to explain SSL in practice (judging from the introduction).\n\n    1. In Section 3.2, two assumptions are made regarding a model of the data distribution\n    (non-overlapping effective support) and sufficient Gaussian modes.\n    In practical settings (i.e., not in the limit of modes), how well does this represent data\n    distributions of interest to SSL?\n\n    1. The mapping $f$ is assumed *not* to be a dimensionality reduction operator (i.e., $f : D \\to K$ with $K \\ge D$).\n    Is this not false of the neural networks used as standard in SSL?\n\n1. **Unconvincing result.** The assumption of non-overlapping effective support in Theorem 1 appears to allow the output distribution of the neural network to depend quite simply as the result of a single affine spline (with parameters $A_\\omega$, $b_\\omega$). \nThis is in contrast to the more complicated expressions in Balestriero & Baraniuk (2018), who define splines per layer (e.g., their Eq. (6)).\nCould you explain this discrepancy?\nHow could your setting \"be extended to the general case,\" as is claimed in Section 3.2?\n\n1. **No practical experiments.** Despite aiming to address SSL in practice, the paper does not attempt to optimize the bound on the MI objective derived in (7) nor empirically verify the generalization bound derived in Theorem 2.\n\n### Minor\n\n1. Please number all equations, etc. in the manuscript.\n\n1. Around Eq. (2), $X$, $X'$, $Z$, $Z'$ are not explicitly defined, and it is not given which distribution to take the expectation with respect to (for completeness, though this is a standard expression)\n\n1. The plot in Figure 1 looks stretched rather than scaled.\n\n1. Typo (both \"first\" & \"second\"): \"VICReg ... estimates the entropy of Z solely from the first second moment\"\n\n1. The \"Theorem\" counters in the appendix are off.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity:** The motivation of the paper is clear, but the details of the contribution need to be more clearly communicated.\n\n**Novelty:** The contribution appears novel.\n\n**Reproducibility:** I did not see any overt effort towards reproducibility, and the experimental setup seems to lack details.\n\n# Summary Of The Review\n\nThe idea appears novel, but the practical significance of the results for SSL in general remains unclear.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLARGE LANGUAGE MODELS CAN SELF-IMPROVE\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nLarge Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate “high-confidence” rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach im82.1% proves the general reasoning ability of a 540B-parameter LLM (74.4% on GSM8K, 78.2% 94.4% on OpenBookQA, and 83.0% on DROP, 90.0% 63.4% 67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that finetuning on reasoning is critical for self-improvement.\n\n→\n\n→\n\n→\n\n→\n\n1\n\nINTRODUCTION\n\nScaling has enabled Large Language Models (LLMs) to achieve state-of-the-art performance on a range of Natural Language Processing (NLP) tasks (Wang et al., 2018; 2019; Rajpurkar et al., 2016). More importantly, new capabilities have emerged from LLMs as they are scaled to hundreds of billions of parameters (Wei et al., 2022a): in-context few-shot learning (Brown et al., 2020) makes it possible for an LLM to perform well on a task it never trained on with only a handful of examples; Chain-of-Thought (CoT) prompting (Wei et al., 2022b; Kojima et al., 2022) demonstrates strong reasoning ability of LLMs across diverse tasks with or without few-shot examples; self-consistency (Wang et al., 2022b) further improves the performance via self-evaluating multiple reasoning paths.\n\nDespite these incredible capabilities of models trained on large text corpus (Brown et al., 2020; Chowdhery et al., 2022), fundamentally improving the model performances beyond few-shot baselines still requires finetuning on an extensive amount of high-quality supervised datasets. FLAN (Wei et al., 2021; Chung et al., 2022) and T0 (Sanh et al., 2022) curated tens of benchmark NLP datasets to boost zero-shot task performances on unseen tasks; InstructGPT (Ouyang et al., 2022) crowd-sourced many human answers for diverse sets of text instructions to better align their model to human instructions. While significant efforts were committed on collecting high-quality supervised datasets, human brain, on the contrary, is capable of the metacognition process (Dunlosky & Metcalfe, 2008), where we can refine our own reasoning ability without external inputs.\n\nIn this paper, we study how an LLM capable of in-context few-shot learning and chain-of-thought reasoning, is able to self-improve its reasoning ability without supervised data. We show that using only input sequences (without ground truth output sequences) from multiple NLP task datasets, a pre-trained LLM is able to improve performances for both in-domain and out-of-domain tasks. Our method is shown in Figure 1: we first sample multiple predictions using few-shot Chain-ofThought (CoT) (Wei et al., 2022b) as prompts, filter “high-confidence” predictions using majority voting (Wang et al., 2022b), and finally finetune the LLM on these high-confidence predictions. The resulting model shows improved reasoning in both greedy and multi-path evaluations. We call the model fine-tuned in this way as Language Model Self-Improved (LMSI). Note that LMSI depends on in-context few-shot learning and chain-of-thought reasoning abilities which small language models do not necessarily have. We empirically verify LMSI using a pre-trained 540B LLM, where our method not only improves training task performances (74.4% 83.0% on DROP, 90.0% 67.9% on ANLI-A3), but also enhances out-of-domain (OOD) test tasks (AQUA, StrategyQA, MNLI), achieving state-of-the-art perfor-\n\n94.4% on OpenBookQA, and 63.4%\n\n82.1% on GSM8K, 78.2%\n\n→\n\n→\n\n→\n\n→\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Overview of our method. With Chain-of-Thought (CoT) examples as demonstration (Wei et al., 2022b), the language model generates multiple CoT reasoning paths and answers (temperature T > 0) for each question. The most consistent answer is selected by majority voting (Wang et al., 2022b). The “high-confidence” CoT reasoning paths that lead to the majority answer are augmented by mixed formats as the final training samples to be fed back to the model for fine-tuning.\n\nmances in many tasks without relying on supervised ground truth answers. Lastly, we conduct preliminary studies on self-generating additional input questions and few-shot CoT prompts, which could further reduce the amount of human effort required for model self-improving, and ablation studies on important hyperparameters of our approach. We hope our simple approach and strong empirical results could encourage more future work by the community to investigate optimal performances of pretrained LLMs without additional human supervision.\n\nOur contributions are summarized as follows:\n\n• We demonstrate that a large language model can self-improve by taking datasets without ground truth outputs, by leveraging CoT reasoning (Wei et al., 2022b) and selfconsistency (Wang et al., 2022b), achieving competitive in-domain multi-task performances as well as out-of-domain generalization. We achieve state-of-the-art-level results on ARC, OpenBookQA, and ANLI datasets.\n\n• We provide detailed ablation studies on training sample formatting and sampling temperature after fine-tuning, and identify critical design choices for most successful selfimprovement by LLMs.\n\n• We study two other approaches for self-improvements, where the model generates additional questions from finite input questions and generates few-shot CoT prompt templates itself. The latter achieves 74.2% on GSM8K, which is the state-of-the-art zero-shot performance, against 43.0% by Kojima et al. (2022) or 70.1% through its naive extension with Wang et al. (2022b).\n\n2 RELATED WORK\n\nLearning from explanations. Augmenting a machine learning model with explanations has been studied in existing literature extensively. For example, in the supervised learning setting, a model can be fine-tuned using human-annotated rationales (Zaidan et al., 2007; Ling et al., 2017b; Narang et al., 2020; Camburu et al., 2018; Cobbe et al., 2021; Chung et al., 2022). A few works have also looked at how explanations can help the models in various settings, e.g., in-context learning (Lampinen et al., 2022) and in distillation (Pruthi et al., 2022). In this paper, we focus more on the unsupervised learning setting, where we do not assume we have a rationale-augmented training dataset available, since human-annotated rationales can be expensive.\n\nFew-shot explanations improves reasoning in LLMs. Recently, a lot of progress has been made towards improving LLMs’ reasoning abilities via prompting or in-context learning. Wei et al.\n\n2\n\n2Alex is 10-8 = 2 years old.Alex’s age is in the middle of 8 and 10. Alex is 9 years old.(8+10)/2 = 9. The answer is 9....Language ModelQ: John buys 20 cards and 1/4 are uncommon. How many uncommon cards did he get?A: John gets 20 * 1/4 = 5 uncommon cards. The answer is 5....Q: Amy is 10. Jake is 8. Alex’s age is right in the middle. How old is Alex?A:CoT examplesQ:A:Q: ... How old is Alex?A: ... (8+10)/2=9 ... answer is 9.Mixed formats of selected reasoning pathsSelf-trainingTraining-set questions or self-generated questionsMultiple path decodingMajority Votingby answer<latexit sha1_base64=\"HOCrR75PTdXMU4JpjxYtpz9BBcY=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkVI8FLx4r2A9oQ9lsNu3aTTbsToRS+h+8eFDEq//Hm//GbZuDtj4YeLw3w8y8IJXCoOt+O4WNza3tneJuaW//4PCofHzSNirTjLeYkkp3A2q4FAlvoUDJu6nmNA4k7wTj27nfeeLaCJU84CTlfkyHiYgEo2ildp+FCs2gXHGr7gJknXg5qUCO5qD81Q8Vy2KeIJPUmJ7npuhPqUbBJJ+V+pnhKWVjOuQ9SxMac+NPF9fOyIVVQhIpbStBslB/T0xpbMwkDmxnTHFkVr25+J/XyzC68aciSTPkCVsuijJJUJH56yQUmjOUE0so08LeStiIasrQBlSyIXirL6+T9lXVq1dr97VKo57HUYQzOIdL8OAaGnAHTWgBg0d4hld4c5Tz4rw7H8vWgpPPnMIfOJ8/ramPLA==</latexit>···<latexit sha1_base64=\"HOCrR75PTdXMU4JpjxYtpz9BBcY=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkVI8FLx4r2A9oQ9lsNu3aTTbsToRS+h+8eFDEq//Hm//GbZuDtj4YeLw3w8y8IJXCoOt+O4WNza3tneJuaW//4PCofHzSNirTjLeYkkp3A2q4FAlvoUDJu6nmNA4k7wTj27nfeeLaCJU84CTlfkyHiYgEo2ildp+FCs2gXHGr7gJknXg5qUCO5qD81Q8Vy2KeIJPUmJ7npuhPqUbBJJ+V+pnhKWVjOuQ9SxMac+NPF9fOyIVVQhIpbStBslB/T0xpbMwkDmxnTHFkVr25+J/XyzC68aciSTPkCVsuijJJUJH56yQUmjOUE0so08LeStiIasrQBlSyIXirL6+T9lXVq1dr97VKo57HUYQzOIdL8OAaGnAHTWgBg0d4hld4c5Tz4rw7H8vWgpPPnMIfOJ8/ramPLA==</latexit>···9... Alex is 9 years old ... ...Input:Output:CoT examplesQ: ... How old is Alex?A: Let’s think step-by-step.Under review as a conference paper at ICLR 2023\n\n(2022b) propose Chain-of-Thought prompting, which prompts the language model to generate a series of natural-language-based intermediate steps, and show it can help language models better solve complex and multi-step reasoning tasks. Wang et al. (2022b) improve Chain-of-Thought prompting by sampling multiple diverse reasoning paths and finding the most consistent answers via majority voting. Kojima et al. (2022) propose to prompt the language model with “Let’s think step by step” to generate reasoning in a zero-shot fashion. Zhou et al. (2022a) further decompose the questions into multiple sub-questions, and ask the language model to solve each sub-question sequentially.\n\nRefining explanations. More recent work proposes to further refine the generated reasoning paths as some of them could be unreliable. For example, Ye & Durrett (2022) calibrate model predictions based on the reliability of the explanations, Jung et al. (2022) show that inducing a tree of explanations and inferring the satisfiability of each explanation can further help judge the correctness of explanations. Li et al. (2022b) show that sampling a diverse set of prompts from the training data, and a voting verifier can be used to improve model’s reasoning performance. Zelikman et al. (2022) proposes better rationale generation by augmenting ground truth answers as hints when predicted answers are incorrect. Our work is orthogonal to these lines of work, as we utilize refined explanations for model self-improvement, and could readily incorporate these other refinement techniques for generating higher-quality self-training data. Our work is similar to Zelikman et al. (2022) where we both propose to fine-tune a model on self-generated CoT data, but our method does not require ground truth labels and shows stronger empirical results with multi-task generalization.\n\nSelf-training models. One related line of work is self-training (see a survey from Amini et al. (2022)). The key idea is to assign pseudo labels from a learned classifier to unlabeled data, and use these pseudo-labeled examples to further improve the original model training, e.g., (RoyChowdhury et al., 2019; Xie et al., 2020; He et al., 2020; Chen et al., 2021). Different from such prior work, our proposed self-improvement framework uses CoT prompting plus self-consistency to obtain highconfidence solutions on a large set of unlabeled data to augment the fine-tuning process.\n\nDistillation and dark knowledge. Our method also tangentially relates to rich literature on distillation (Ba & Caruana, 2014; Hinton et al., 2015). A key detail is to learn from soft targets instead of hard predicted labels, as softmax outputs with a high temperature reveal more detailed relative class likelihoods, colloquially known as dark knowledge (Hinton et al., 2015; Korattikara Balan et al., 2015). Recent studies (Zelikman et al., 2022; Snell et al., 2022; Eisenstein et al., 2022) show that dark knowledge within LLMs can be retrieved with more computation at inference time, such as adding informative instructions into the input sequence, and output CoT generation (Wei et al., 2022b; Kojima et al., 2022). In our work, we explicitly show that imperfect CoT reasoning (which may lead to incorrect answer) can be used directly for self-improving language models as evidenced in our experiments in Sections 5.2 and 5.3.\n\n3 METHOD\n\ntrain =\n\nThe overview of our method is illustrated in Fig. 1: We are given a pre-trained Large Language Model (LLM) M and a question-only training dataset i=1 with few-shot Chain-ofThought (CoT) examples (Wei et al., 2022b). We apply multiple path decoding with a sampling temperature T > 0 for generating m reasoning paths and answers for each questrain, and use majority voting (self-consistency) to select the most consistent, highest tion xi in confidence answer (Wang et al., 2022b). We then keep all reasoning paths that lead to the most consistent answer, apply mixed formats of prompts and answers for augmentation, and fine-tune the model on these self-generated reasoning-answer data. We consider our approach as making the model self-improve. In the following sections, we detail important designs within our method, along with additional approaches for the model to self-improve without supervised data.\n\nri1, ri2 , . . . , rim }\n\nxi\n\nD\n\nD\n\n{\n\n}\n\n{\n\nD\n\n3.1 GENERATING AND FILTERING MULTIPLE REASONING PATHS\n\nSelf-consistency (Wang et al., 2022b) brings large improvements on reasoning tasks (e.g., 56.5% →\n74.4% on GSM8K test set), and the gap between greedy decoding and diverse decoding shows there is a potential for further improving the reasoning ability of M , using the self-selected highconfidence reasoning paths as training data.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nQuestion: Stefan goes to a restaurant with his family. They order an appetizer that costs $10 and 4 entrees that are $20 each. If they tip 20% of the total, what is the total amount of money that they spend?\n\nMultiple Path Decoding: Output 1: The appetizer costs $10. The entrees cost $20 each so in total 4 * $20 = $80. This means the total cost is $10 + $80 = $90. They tip 20% of it, so the total amount they spend is $90 * 1.2 = $108. The answer is 108.\n\nOutput 2: The appetizer costs $10 and the entrees are $20 each. There are 4 entrees so the sum is $20 * 4 = $80. The waiter gets 20% of the total. 20% of $80 is $80 * .2 = $16. The answer is $80 + $16 = $96. (Incorrect reasoning path)\n\nOutput 3: The appetizer costs $10. The entrees cost 4 * $20 = $80. The tip is 20% of the total, so it is 20% of the $90 they have spent. The tip is 0.2 * 90 = $18. The total they spent is $90 + $18 = $108. The answer is 108.\n\nTable 1: Examples of 3 self-generated CoT reasoning paths given a question. Output 1 and 3 are the most consistent reasoning paths based on majority voting and kept as self-training data.\n\nFor each training question xi, we sample m CoT reasoning paths, denoted as (see Table 1 for examples). Since M is prompted with the CoT examples from Wei et al. (2022b), we apply the same output parsing with “The answer is” to generate their predicted an- . The most consistent answer, which is not necessarily a correct answer, swers (cid:80)m I(yij = yik ). For all the trainis selected by majority voting, denoted as ̃yi = arg maxyij ing questions, we filter the CoT reasoning paths that reach ̃y as the final answer to be put into the .\nself-training data, denoted as\n\nyi1 , yi2, . . . , yim}\n\nri1, ri2 , . . . , rim }\n\nself−consistent =\n\n, where ̃ri =\n\nm, yij = ̃yi\n\nxi, ̃ri\n\nk=1\n\n{\n\n1\n\n{\n\nj\n\nrij |\n\n{\n\n≤\n\n≤\n\n}\n\n{\n\nD\n\n} Since we do not use any ground truth labels to filter out cases where ̃yi = yi, it is important that the self-generated CoT reasoning paths are mostly reliable and incorrect answers do not hurt the self-improvement of the model. We plot the relation between the accuracy and confidence of self-generated CoT paths for each question in GSM8K training set in Fig. 2. The confidence is the number of CoT paths leading to ̃y divided by the total path number m. The y-axis shows the accuracy of ̃y under a certain confidence. The circle area and the color darkness shows the number of questions under a certain confidence. We can observe that confident answers are more likely to be correct, which means that when a question has many consistent CoT paths, then the corresponding ̃y is more likely to be correct. On the other hand, when ̃y is wrong, it is likely to be supported by fewer CoT paths, and brings little noise to the training samples.\n\nFigure 2: The relation of accuracy and confidence of the majorityvoted answer after multiple path decoding on GSM8K training-set questions. Predicted confidence from selfconsistency (Wang et al., 2022b) is well calibrated (Guo et al., 2017).\n\n3.2 TRAINING WITH MIXED FORMATS\n\nTo prevent the language model from overfitting to specific prompts or answer styles, we create four different formats for each reasoning path to be mixed in the self-training data, shown in Table 2. In the first format, a few Chain-of-Thought examples (questions followed by reasoning paths leading to the correct final answers) are prepended to the new question, while the language model output is trained to be the same with the filtered CoT reasoning paths. In the second format, we use examples of questions and their direct answers as standard prompting, and the language model output is supposed to also only contain the direct answer. The third and fourth format are similar to the first and second format, except that no example of question-answer pairs are given, so that the model will learn to think on its own in an in-context zero-shot manner. In the third format, where we want the model to output CoT reasoning without prepending examples containing CoT reasonings, we append “Let’s think step by step.” at the end of the input sequence, to guide the language model\n\n4\n\n0.00.20.40.60.81.0Confidence0.00.20.40.60.81.0Accuracy0200400# of Questions̸ Under review as a conference paper at ICLR 2023\n\nto generate step-by-step CoT reasoning paths (Kojima et al., 2022). The mixed formats of training samples are then used to fine-tune the pre-trained language model M .\n\nQuestion: Amy is 10 years old. Jake is 8 years old. Alex’s age is right in the middle. How old is Alex? Selected Chain-of-Thought: Amy is 10 years old. Jake is 8 years old. Alex’s age is in the middle of Amy and Jake, so Alex is ( 8 + 10 ) / 2 = 9 years old. The answer is 9.\n\nMixed-formats of training data: Format 1: Input: [CoT prompting examples] + ‘ Output: Amy is 10 years old. Jake is 8 years old. Alex’s age is in the middle of Amy and Jake, so Alex is ( 8 + 10 ) / 2 = 9 years old. The answer is 9.\n\nn’ + [Question] + ‘\n\nn’ + ‘A:’\n\n\\\n\n\\\n\nFormat 2: Input: [Standard prompting examples] + ‘ Output: The answer is 9.\n\n\\\n\nn’ + [Question] + ‘\n\nn’ + ‘A:’\n\n\\\n\nFormat 3: Input: [Question] + ‘ n’ + ‘A: Let’s think step by step.’ Output: Amy is 10 years old. Jake is 8 years old. Alex’s age is in the middle of Amy and Jake, so Alex is ( 8 + 10 ) / 2 = 9 years old. The answer is 9.\n\n\\\n\nFormat 4: Input: [Question] + ‘ Output: The answer is 9.\n\n\\\n\nn’ + ‘A:’\n\nTable 2: An example of how a reasoning path is augmented into four formats of training data with different prompts (in input) and answer styles (in output). Specifically, the CoT prompting examples used for each tasks are listed in Appendix A.2. The Standard prompting examples are the same question-answer pairs with CoT prompting examples, except that reasoning is removed.\n\n3.3 GENERATING QUESTIONS AND PROMPTS\n\nGiven a set of training questions and a few human-written Chain-of-Thought (CoT) examples as prompts, our proposed approach enables model self-improvement. However, when the amount of training questions or CoT examples is limited, our method may not generate sufficient training samples for language model self-training. Collecting questions from the web requires human engineering. To further reduce human effort, we investigate how to self-generate more training questions as well as example prompts.\n\nQuestion Generation Previous work (Yoo et al., 2021; Meng et al., 2022) discuss few-shot data augmentation by generating diverse training samples using LLMs. However, those methods are designed for classification tasks and require ground truth label for each few-shot example. We use a simple yet effective approach to generate diverse questions (without ground truth answers) for in-domain questions. Specifically, we randomly select several existing questions, concatenate them in a random order as input prompt, and let the language model generate consecutive sequences as new questions. We repeat the process to obtain a large set of new questions, then use selfconsistency (Wang et al., 2022b) to only keep the questions that have a highly confident answer. Those questions are then used as self-generated training questions.\n\nPrompt Generation Given a set of questions, humans can write CoT examples as reasoning paths leading to the final answer. In zero-shot setting without manual prompts, we can generate these CoT paths using the model itself. Following Kojima et al. (2022), we start the answer with “A: Let’s think step by step.” and let the language model generate the consecutive reasoning paths. We then use those generated reasoning paths as examples for few-shot CoT prompting.\n\n4 EXPERIMENTAL SETUP\n\nTasks and Datasets. We demonstrate the effectiveness of our method on three types of tasks1:\n\n1We evaluate on the test sets of GSM8K, ARC, OpenBookQA, and ANLI, and the dev set of DROP (ground\n\ntruth labels of the test set are not publicly available).\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n• Arithmetic reasoning: We use the math problem set GSM8K (Cobbe et al., 2021), and a reading comprehension benchmark DROP (Dua et al., 2019) which requires numerical reasoning. We follow Zhou et al. (2022a) to partition the DROP dataset into football related and non-football related subsets for training.\n\n• Commonsense reasoning: We use the OpenBookQA (Mihaylov et al., 2018) dataset, and the AI2 Reasoning Challenge (ARC) (Clark et al., 2018) dataset. Note that for ARC, we only use the Challenge sub-set (ARC-c) in our experiments. Both datasets contain multiple-choice questions.\n\n• Natural Language Inference: We use the Adversarial NLI (ANLI) (Mihaylov et al., 2018) subsets, ANLI-A2 and ANLI-A3, which are the more challenging subsets compared to ANLI-A1. These datasets contain pairs of sentences with relations of entailment, neutral, or contradiction.\n\ntrain\n\n× |D\n\nModels, Training settings and Hyperparameters We follow previous studies (Wei et al., 2022b; Wang et al., 2022b) and conduct our experiments on an autoregressive Transformer-based language model with 540 billion parameters. The CoT examples for each dataset are listed in Appendix A.2. We generate m = 32 reasoning paths for each question in a training set. Since each reasoning path is augmented into four formats in Sec. 3.2, the final training samples are up to the size of 128 being the size of the corresponding training set. For all datasets except DROP, we use the whole training set; To reduce the training burden, we sample 5k examples from the non-football and football partition of the DROP dataset, and sample 5k examples from ANLI-A2 and ANLI-A3. For each dataset, we fine-tune the model for 10k steps with a learning rate of 5e 5\nand a batch size of 32. For multiple path decoding, we use a sampling temperature of T = 0.7 with the pre-trained model as suggested by Wang et al. (2022b). We use T = 1.2 for the language model after self-improvement (LMSI). We set the maximum number of decoded steps to 256 for all experiments.\n\n, with |\n\ntrain\n\n|D\n\n−\n\n|\n\n5 RESULTS\n\nWe conduct a series of experiments to demonstrate the effectiveness of our proposed self-improving method. First, we apply our method on each individual dataset (task) and report the results. We then merge the generated data from all datasets and train one model to study the generalization ability of the model on unseen datasets as in (Wei et al., 2021). In addition to the results of using generated CoT reasoning paths, we show studies on generating input questions and few-shot prompts. We end with ablation studies on model sizes and hyperparameters.\n\n5.1 MAIN RESULTS\n\nPrompting Method GSM8K DROP ARC-c OpenBookQA ANLI-A2 ANLI-A3\n\nw/o LMSI\n\nLMSI\n\nPrevious SOTA\n\n82.3a\n\n84.9b\n\n88.7c\n\nStandard-Prompting CoT-Prompting Self-Consistency\n\nStandard-Prompting CoT-Prompting Self-Consistency\n\n17.9 56.5 74.4\n\n32.2 73.5 82.1\n\n60.0 70.6 78.2\n\n71.7 76.2 83.0\n\n87.1 85.2 88.7\n\n87.2 88.3 89.8\n\n91.0d\n\n84.4 86.4 90.0\n\n92.0 93.0 94.4\n\n64.9d\n\n66.0d\n\n55.8 58.9 64.5\n\n64.8 65.3 66.5\n\n55.8 60.6 63.4\n\n66.9 67.3 67.9\n\nTable 3: Accuracy results on six reasoning benchmarks. The previous SOTA results are from: (a) Li et al. (2022a), (b) Zhou et al. (2022b), (c) Wang et al. (2022b), (d) Wang et al. (2022a).\n\nWe list the results of using the 540B model before and after LMSI in Table 3. For each model, during test time, we apply three separate prompting methods on all six datasets: standard-prompting, CoT-Prompting, and Self-Consistency. We observe that after LMSI, the performance of all three prompting methods increase by a large margin. We observe significant improvement, comparing self-consistency versus LMSI with self-consistency: +7.7% on GSM8K, +4.8% on DROP, +4.4% on OpenBookQA, and +4.5% on ANLI-A3. This shows that our proposed method is quite effective. Furthermore, the single path CoT-Prompting performance of LMSI is close to or even better than\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nthe multiple path Self-Consistency performance of the model without LMSI, showing that LMSI truly helps the language model learn from the multiple consistent reasoning paths. We also compare our results with previous SOTA, achieved by different methods on different datasets, listed in Table 3. On ARC-c, OpenBookQA, ANLI-A2 and ANLI-A3, LMSI outperforms previous SOTA. On GSM8K dataset, LMSI is close to the DiVeRSe approach (Li et al., 2022a) which uses diverse prompts and a voting verifier to ensemble 100 output paths. On the contrary, we only use 32 output paths for self-generating training samples and for self-consistency with LMSI. On the DROP dataset, LMSI is close to the OPERA approach (Zhou et al., 2022b) which uses ground truth labels for training. On the other hand, our method only leverages the questions in the training set, without using ground truth labels.\n\nSelf-training data\n\nAQUA SVAMP StrategyQA ANLI-A1 RTE MNLI-M/MM\n\nw/o LMSI\n\n-\n\nLMSI\n\nGSM8K + DROP + ...\n\n35.8\n\n39.0\n\n79.0\n\n82.8\n\n75.3\n\n77.8\n\n68.8\n\n79.2\n\n79.1\n\n80.1\n\n72.0/74.0\n\n81.8/82.2\n\nTable 4: Comparison of CoT-prompting accuracy results on six Out-Of-Domain benchmarks with or without training on six In-Domain (GSM8K, DROP, ARC-c, OpenBookQA, ANLI-A2, ANLI-A3) training-set questions.\n\nMulti-task self-training for unseen tasks To demonstrate the generalization ability of LMSI, we conduct experiments of self-training on a mixture of the training-set questions from the above six datasets (denoted as In-Domain tasks), then use the same model checkpoint for the evaluation on six Out-Of-Domain (OOD) tasks, as shown in Table 4. Of all the OOD tasks: (1) AQUA (Ling et al., 2017a) and SVAMP (Patel et al., 2021) are arithmetic reasoning tasks; (2) StrategyQA (Geva et al., 2021) is a commonsense reasoning task; (3) ANLI-A1 (Mihaylov et al., 2018), RTE (Dagan et al., 2005) and MNLI-M/MM (Williams et al., 2018) are natural language inference tasks.2 Among these tasks, AQUA, StrategyQA, and RTE are significantly different from any In-Domain task. These three tasks have their own few-shot prompts. From Table 4, we can observe that LMSI achieves higher accuracy results on all OOD tasks, showing that the overall reasoning ability of the language model is improved.\n\nResults on GSM8K Standard Prompting CoT Prompting\n\nw/o LMSI\n\nLMSI w/o CoT formats\n\nLMSI\n\n17.9\n\n23.6\n\n32.2\n\n56.5\n\n61.6\n\n73.5\n\nTable 5: Ablation study: w/ or w/o CoT reasoning paths as training format on GSM8K dataset.\n\nImportance of training with Chain-of-Thought formats We demonstrate the importance of training language models with Chain-of-Thoughts compared to training with only direct answers. In Table 5, we list the results of LMSI with all four formats, and the results of LMSI with only direct answer formats. The results clearly show that without the CoT formats, the language model can still self-improve, but the performance gain drops by a large amount compared to using all four formats.\n\n5.2 PUSHING THE LIMIT OF SELF-IMPROVEMENTS\n\nSelf-Generating Questions We further explore the few-shot setting where there are only limited training questions in the target domain. On GSM8K, we sample 10 real questions as few-shot\n\n2We evaluate on the test set of SVAMP and ANLI, the dev set of MNLI and RTE (ground truth labels of the test sets are not publicly available). For StrategyQA we use the question-only set from bench collaboration (2022).\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nQuestions used for Self-Training\n\nResults on GSM8K\n\nCoT-Prompting\n\nSelf-Consistency\n\nw/o LMSI\n\n-\n\nLMSI\n\nLMSI\n\nGenerated Questions\n\nTraining-set Questions\n\n56.5\n\n66.2\n\n73.5\n\n74.4\n\n78.1\n\n82.1\n\nTable 6: Accuracy on GSM8K test set after self-training on self-generated or training set questions.\n\nsamples, and use the language model to generate more training questions using the method in Section 3.3. We then self-train the language model with these generated questions and list the results in Table 6. The results show that using self-generated questions still improves the reasoning ability of language models, but using the real training-set questions leads to better results.\n\nFigure 3: Accuracy results on GSM8K test set using 540B model with multi-path sampling and self-consistency (Wang et al., 2022b). “Step-by-Step” is the baseline performance of Kojima et al. (2022) plus self-consistency (Wang et al., 2022b), while our “Few-Shot w/ Step-by-Step” uses exemplers self-generated from Step-by-Step (greedy decoding) for few-shot prompting the LLM.\n\nSelf-Generating Few-Shot CoT Prompts We explore the situation where no in-domain CoT examples are provided for a task. We apply the Step-by-Step method (Kojima et al., 2022) to generate CoT examples using the language model as described in Section 3.3, and show the results in Figure 3. We observe that few-shot prompting with self-generated Step-by-Step CoT examples substantially outperforms the Step-by-Step (Kojima et al., 2022) baseline (66.2% vs 53.8% at 10 paths, 74.2% vs 70.1% at 40 paths), and nearly matches the performance of human-written few-shot CoT (Wei et al., 2021) (74.4% at 40 paths (Wang et al., 2022b)). The strong performance of “Few-Shot w/ Stepby-Step” despite the limited accuracy of prompt examples (43.0% for greedy Step-by-Step) likely comes from leveraging more diverse CoT prompts for multi-path decoding (Li et al., 2022a), where at 40 paths it uses 20 generate prompt-templates, each with 4-shot CoT examples, i.e. a total of 80 generated CoT examples compared to 8 human-written examples use in Wei et al. (2022b). Since we did not use training questions or few-shot CoT examples, 74.2% also marks the new state-of-the-art zero-shot performance on GSM8K.\n\n5.3 DISTILLATION TO SMALLER MODELS\n\nw/o LMSI\n\nDistilled from LMSI 540 billion\n\nResults on GSM8K\n\n8 billion\n\n62 billion\n\n540 billion\n\n5.0\n\n33.4\n\n29.7\n\n57.4\n\n56.5\n\n-\n\nTable 7: Distillation from 540B model to small models. We see that distilled smaller models outperform models that are one-tier larger.\n\n8\n\n1020304050607080Total Sample Paths0.550.600.650.700.75AccuracyStep-by-StepFew-Shot w/ Step-by-StepUnder review as a conference paper at ICLR 2023\n\nWe also explore whether the knowledge can be distilled to smaller models, such as in distillation (Hinton et al., 2015) and in Zelikman et al. (2022). We use the same set of training samples generated by the 540B model, but fine-tune on models with smaller sizes (8B and 62B respectively), and show the results of CoT-prompting in Table 7. It is interesting to point out that after distillation from LMSI, the 62 billion model can outperform the pre-trained 540 billion model, and the 8 billion model can outperform the pre-trained 62 billion model. This implies that for downstream applications with limited computing resources, the reasoning knowledge from large models can be used to largely enhance small models to achieve competitive performance.\n\n5.4 HYPERPARAMETER STUDY\n\n(a) Accuracy results of LMSI on GSM8K and DROP test set when different sampling temperatures are applied for Self-Consistency.\n\n(b) Accuracy results with or without LMSI on GSM8K test set using different numbers of sampled reasoning path for SelfConsistency.\n\nFigure 4: Hyperparameter study results.\n\nSampling Temperature after Self-Improvement We study the effect of varying the temperature T for multiple path decoding after LMSI is applied. Specifically, we vary T between [0.7, 1.0, 1.2, 1.5] and show the results on GSM8K and DROP dataset respectively in Fig. 4(a). As shown in the figure, T = 1.2 benefits both datasets the most, and is used in the Self-Consistency method for LMSI on all datasets. We notice that the optimal T after model self-improvement is larger than the optimal T = 0.7 (Wang et al., 2022b) before self-improvement. We believe the reason is that after training the model, the entropy of the output distribution is reduced.\n\nNumber of Sampled Reasoning Paths We study whether the number of sampled reasoning paths m for Self-Consistency largely affects the accuracy after LMSI is applied. We show the accuracy on GSM8K test set for models both with or without LMSI in Fig. 4(b). For both cases, setting m = 15 already achieves a reasonably good accuracy, and using a larger m only brings marginal improvements. We also notice that after Self-Improvement, using 5 paths for Self-Consistency can already surpass the performance of using 32 paths for model without Self-Improvement. Thus, with a well-improved model, huge computing resources can be saved when applied to real applications.\n\n6 CONCLUSIONS\n\nWe demonstrated that a Large Language Model (LLM) is capable of improving its performance on reasoning datasets by training on its own generated labels, given input questions only. Experiments using an LLM with 540 billion parameters show that our approach improves the accuracy scores on the six datasets by 1.1% to 7.7%, achieving new state-of-the-art results on ARC, OpenBookQA, and ANLI, without training on ground truth labels. Furthermore, we show that it is possible for the LLM to self-improve even on its own generated questions and few-shot Chain-of-Thought prompts. As part of our future work, we plan to combine large-scale generated data from our approach and existing supervised data, to further improve the performance of LLMs.\n\n9\n\n0.71.01.21.5Sampling Temperature80818283AccuracyDROPGSM8K151015202532# Sampled Reasoning Path50607080AccuracyLMSIw/o LMSIUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMassih-Reza Amini, Vasilii Feofanov, Loic Pauletto, Emilie Devijver, and Yury Maximov. Self-\n\ntraining: A survey, 2022. URL https://arxiv.org/abs/2202.12040.\n\nJimmy Ba and Rich Caruana. Do deep nets really need to be deep? Advances in neural information\n\nprocessing systems, 27, 2014.\n\nBIG bench collaboration. Beyond the imitation game: Quantifying and extrapolating the capabilities\n\nof language models. ArXiv, abs/2206.04615, 2022.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Neurips, 2020.\n\nlanguage inference with natural\n\nOana-Maria Camburu, Tim Rockt ̈aschel, Thomas Lukasiewicz, and Phil Blunsom.\n\nsnli: Natural gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, (eds.), Advances Information Processing Systems 31, Curran Associates, 8163-e-snli-natural-language-inference-with-natural-language-explanations. pdf.\n\neIn S. Benand R. Garnett pp. 9539–9549. URL http://papers.nips.cc/paper/\n\nlanguage explanations.\n\nin Neural Inc.,\n\n2018.\n\nXiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang. Semi-supervised semantic segmentation with cross pseudo supervision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc ́ıa, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D ́ıaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. MeierHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Adams Yu, Albert Webson, Xinyun Chen, Gaurav Mishra, Zhuyun Dai, Shixiang Shane Gu, Mirac Suzgun, Vincent Zhao, Aakanksha Chowdhery, Sharan Narang, Yanping Huang, Andrew Dai, Hongkun Yu, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. In arxiv, 2022.\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021.\n\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment\n\nchallenge. In MLCW, 2005.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL, 2019.\n\nJohn Dunlosky and Janet Metcalfe. Metacognition. Sage Publications, 2008.\n\nJacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael Collins, and David Mimno. Honest students from untrusted teachers: Learning an interpretable question-answering pipeline from a pretrained language model. arXiv preprint arXiv:2210.02498, 2022.\n\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346–361, 2021.\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural\n\nnetworks. In International conference on machine learning, pp. 1321–1330. PMLR, 2017.\n\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio Ranzato. Revisiting self-training for neural In International Conference on Learning Representations, 2020. URL\n\nsequence generation. https://openreview.net/forum?id=SJgdnAVKDH.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv\n\npreprint arXiv:1503.02531, 2(7), 2015.\n\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations, 2022. URL https://arxiv.org/abs/2205.11822.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Neural Information Processing Systems (NeurIPS), 2022.\n\nAnoop Korattikara Balan, Vivek Rathod, Kevin P Murphy, and Max Welling. Bayesian dark knowl-\n\nedge. Advances in neural information processing systems, 28, 2015.\n\nAndrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. Can language models learn from explanations in context?, 2022. URL https://arxiv.org/abs/2204. 02329.\n\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the\n\nadvance of making language models better reasoners. ArXiv, abs/2206.02336, 2022a.\n\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners, 2022b. URL https://arxiv.org/ abs/2206.02336.\n\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gener-\n\nation: Learning to solve and explain algebraic word problems. In ACL, 2017a.\n\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017b. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.\n\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating training data with language models:\n\nTowards zero-shot language understanding. ArXiv, abs/2202.04538, 2022.\n\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\n\nelectricity? a new dataset for open book question answering. In EMNLP, 2018.\n\nSharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. Wt5?! training text-to-text models to explain their predictions, 2020. URL https://arxiv. org/abs/2004.14546.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\n\nArkil Patel, S. Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math\n\nword problems? In NAACL, 2021.\n\nDanish Pruthi, Rachit Bansal, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C. Lipton, Graham Neubig, and William W. Cohen. Evaluating Explanations: How Much Do Explanations from the Teacher Aid Students? Transactions of the Association for Computational Linguistics, 10:359–375, 04 2022. ISSN 2307-387X. doi: 10.1162/tacl a 00465. URL https://doi.org/10.1162/tacl_a_00465.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\n\nfor machine comprehension of text. In EMNLP, 2016.\n\nAruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Automatic adaptation of object detecIn CVPR, pp. 780–790, 2019. URL http:\n\nLiangliang Cao, and Erik G. Learned-Miller. tors to new domains using self-training. //openaccess.thecvf.com/content_CVPR_2019/html/RoyChowdhury_ Automatic_Adaptation_of_Object_Detectors_to_New_Domains_Using_ Self-Training_CVPR_2019_paper.html.\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In ICLR, 2022.\n\nCharlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context.\n\narXiv preprint\n\narXiv:2209.15189, 2022.\n\nYi Tay, Mostafa Dehghani, Vinh Quang Tran, Xavier Garc ́ıa, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. Ul2: Unifying language learning paradigms. 2022.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. ArXiv, abs/1905.00537, 2019.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-\n\naugmented ensembles in language models. ArXiv, abs/2207.00747, 2022a.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency\n\nimproves chain of thought reasoning in language models. ArXiv, abs/2203.11171, 2022b.\n\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Brian Ichter, Fei Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35, 2022b.\n\nAdina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for\n\nsentence understanding through inference. In NAACL, 2018.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves imagenet classification. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10684–10695, 2020. doi: 10.1109/CVPR42600.2020.01070.\n\nXi Ye and Greg Durrett. The unreliability of explanations in few-shot in-context learning, 2022.\n\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyeong Park. Gpt3mix:\n\nLeveraging large-scale language models for text augmentation. In EMNLP Findings, 2021.\n\nOmar Zaidan, Jason Eisner, and Christine Piatko. Using “annotator rationales” to improve machine learning for text categorization. NAACL, 2007. URL https://aclanthology.org/ N07-1033.\n\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with\n\nreasoning, 2022. URL https://arxiv.org/abs/2203.14465.\n\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. ArXiv, abs/2205.10625, 2022a.\n\nYongwei Zhou, Junwei Bao, Chaoqun Duan, Haipeng Sun, Jiahui Liang, Yifan Wang, Jing Zhao, Youzheng Wu, Xiaodong He, and Tiejun Zhao. Opera: Operation-pivoted discrete reasoning over text. In NAACL, 2022b.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 RESULTS ON UL2 MODEL\n\nWe also apply LMSI on a recently proposed public language model, UL2 (Tay et al., 2022), using the pre-trained model at step 2,650,0003. We use a fixed set of hyperparameters for fine-tuning on each dataset. Specifically, we generate m = 40 reasoning paths for each question in a training set for majority voting. We fine-tune the model for 10k steps with a learning rate of 5e 5 and a batch size of 32. For multiple path decoding, we use a sampling temperature of T = 0.5 with the pre-trained UL2 model following Tay et al. (2022), and set T = 0.7 for the language model after LMSI. We set the maximum number of decode steps to 256 for all experiments.\n\n−\n\nThe results are shown in Table 8. For arithmetic reasoning datasets, we follow (Tay et al., 2022) to provide both exact matching accuracy scores as well as accuracy scores after an equation-correction postprocessing step. We observe that for most datasets, LMSI still improves the reasoning accuracy (+1.6% on DROP, +1.2% on OpenBookQA, and +0.7% on ANLI-A2), but the improvement on UL2 is not as large as that on 540B. We think the reason is that, since LMSI exploits the implicit rationale of language models, and the capacity of a language model is determined by its size, larger models can capture more high-order semantics and are more likely to benefit from LMSI. For example, on the adversarial entailment tasks of ANLI (which is a three-class classification problem with labels “yes”, “no”, or “it is not possible to tell”), the UL2 model w/o LMSI only achieves an accuracy of marginally above 1/3, implying that the model is slightly better than doing random guess on this challenging task without any training. Our proposed LMSI can still improve the performance under this hard case by training on its implicit knowledge from self-generated paths.\n\nPrompting Method GSM8K\n\nDROP\n\nARC-c OpenBookQA ANLI-A2 ANLI-A3\n\nw/o LMSI\n\nLMSI\n\nCoT-Prompting Self-Consistency\n\nCoT-Prompting Self-Consistency\n\n5.4/7.1 6.4/9.9\n\n6.1/8.6 7.9/10.2\n\n11.1/16.8 16.8/26.5\n\n11.4/17.1 18.1/28.1\n\n49.9 54.9\n\n50.9 54.7\n\n53.6 54.0\n\n53.8 55.2\n\n35.9 37.4\n\n35.4 38.1\n\n33.8 36.8\n\n34.4 37.4\n\nTable 8: Accuracy results on six reasoning benchmarks with LMSI on UL2. On GSM8K and DROP, we also include accuracy scores after an equation-correction postprocessing step.\n\nA.2 CHAIN-OF-THOUGHT PROMPTS FOR EACH DATASET\n\nWe list the Chain-of-Thought Prompts for each dataset for “CoT-Prompting” experiments and selfgenerated training samples.\n\n3UL2: https://github.com/google-research/google-research/tree/master/ul2\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nQ: Poison causes harm to which of the following? (a) a Tree (b) a robot (c) a house (d) a car\n\nA: Poison will harm living things, only a tree is a living thing. The answer is (a).\n\nQ: As you look deeper into a Marbel you can see (a) the future (b) minut defects (c) colors (d) the other side\n\nA: Marbel is not transparent, so you can not see the other side. Marbel does not necessarily have multiple colors. You will see minut defects. The answer is (b).\n\nQ: When food is reduced in the stomach (a) the mind needs time to digest (b) take a second to digest what I said (c) nutrients are being deconstructed (d) reader’s digest is a body of works\n\nA: The food is being deconstructed in the stomach during digestion. The answer is (c).\n\nQ: The sun is responsible for (a) puppies learning new tricks (b) children growing up and getting old (c) flowers wilting in a vase (d) plants sprouting, blooming and wilting\n\nA: The sun can affect the growing of living things, like plants. The answer is (d).\n\nTable 9: Few-shot CoT prompts for OpenBookQA, from Wang et al. (2022a).\n\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n\nA: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n\nA: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n\nA: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74 chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n\nA: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n\nA: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in total he has 7 + 2 = 9 toys. The answer is 9.\n\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n\nA: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29.\n\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n\nA: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n\nA: She bought 5 bagels for $3 each. This means she spent 5 * $3 = $15 on the bagels. She had $23 in beginning, so now she has $23 - $15 = $8. The answer is 8.\n\nTable 10: Few-shot CoT prompts for GSM8K and SVAMP, from Wei et al. (2022b).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nQ: Since the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states. How many years did the European Coal and Steel Community exist?\n\nA: According to the passage, the European Coal and Steel Community was established in 1951 and became the EEC in 1958. 1958 - 1951 = 7. So the answer is 7.\n\nQ: In the county, the population was spread out with 23.50% under the age of 18, 8.70% from 18 to 24, 29.70% from 25 to 44, 24.70% from 45 to 64, and 13.30% who were 65 years of age or older. How many more percent are under the age of 18 compared to the 18 to 24 group?\n\nA: According to the passage, 23.5% are under the age of 18, and 8.7% are from ages 18 to 24. 23.5% - 8.7% = 14.8%. So the answer is 14.8.\n\nQ: Playing in their second straight Thanksgiving game, the Eagles struggled especially on defense, where they were unable to stop the much-hyped Lions offense. The worst of it all was how unproven rookie Eric Rowe was tasked with covering wide receiver Calvin Johnson, leading to Johnson catching 3 touchdowns. Stafford’s five passing touchdowns, including three of them to Johnson was too much for the Eagles to overcome and for the second consecutive time this season, the Eagles gave up 45 points in a game. With the loss, the Eagles drop to 4-7 on the season and 6-1 when playing on Thanksgiving. How many TD passes did Stafford throw other than to Johnson?\n\nA: According to the passage, Stafford threw 5 TD passes, 3 of which were to Johnson. 5 - 3 = 2. So the answer is 2.\n\nTable 11: Few-shot CoT prompts for DROP (nonfootball), from Zhou et al. (2022a).\n\nQ: The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD. How many yards do the shortest touchdown run and the longest touchdown pass combine for?\n\nA: All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown run. The smallest number among 27, 9, 11 is 9. So the shortest touchdown run was 9 yards. All the touchdown passes are: a 12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the shortest touchdown run and the longest touchdown pass combine for 9 + 12 = 21 yards. So the answer is 21 yards.\n\nQ: The Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening punch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens would make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. The Steelers brought the game into overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a winning 33-yard field goal in overtime. How many points did the Ravens have at halftime?\n\nA: The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. So the Ravens had 10 points at halftime. So the answer is 10 points.\n\nQ: The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard touchdown pass to wide receiver Steve Smith. How many scoreless quarters were there?\n\nA: The first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is 2.\n\nTable 12: Few-shot CoT prompts for DROP (football), from Zhou et al. (2022a).\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nPremise: ”Conceptually cream skimming has two basic dimensions - product and geography.” Based on this premise, can we conclude the hypothesis ”Product and geography are what make cream skimming work.” is true? OPTIONS: - yes - no - it is not possible to tell\n\nA: Based on ”cream skimming has two basic dimensions” we can’t infer that these two dimensions are what make cream skimming work. The answer is it is not possible to tell.\n\nPremise: ”One of our member will carry out your instructions minutely.” Based on this premise, can we conclude the hypothesis ”A member of my team will execute your orders with immense precision.” is true? OPTIONS: - yes - no - it is not possible to tell\n\nA: ”one of” means the same as ”a member of”, ”carry out” means the same as ”execute”, and ”minutely” means the same as ”immense precision”. The answer is yes.\n\nPremise: ”Fun for adults and children.” Based on this premise, can we conclude the hypothesis ”Fun for only children.” is true? OPTIONS: - yes - no - it is not possible to tell\n\nA: ”adults and children” contradicts ”only children”. The answer is no.\n\nPremise: ”He turned and smiled at Vrenna.” Based on this premise, can we conclude the hypothesis ”He smiled at Vrenna who was walking slowly behind him with her mother.” is true? OPTIONS: - yes - no - it is not possible to tell\n\nA: the premise does not say anything about ”Vrenna was walking”. The answer is it is not possible to tell.\n\nPremise: ”well you see that on television also” Based on this premise, can we conclude the hypothesis ”You can see that on television, as well.” is true? OPTIONS: - yes - no - it is not possible to tell\n\nA: ”also” and ”as well” mean the same thing. The answer is yes.\n\nPremise: ”Vrenna and I both fought him and he nearly took us.” Based on this premise, can we conclude the hypothesis ”Neither Vrenna nor myself have ever fought him.” is true? OPTIONS: - yes - no - it is not possible to tell\n\nA: ”Vrenna and I both” contradicts ”neither Vrenna nor myself”. The answer is no.\n\nTable 13: Few-shot CoT prompts for NLI tasks, including ANLI and MNLI, from Wang et al. (2022a).\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nQ: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat? (a) dry palms. (b) wet palms. (c) palms covered with oil. (d) palms covered with lotion.\n\nA: Dry surfaces will more likely cause more friction via rubbing than other smoother surfaces, hence dry palms will produce the most heat. The answer is (a).\n\nQ: Which factor will most likely cause a person to develop a fever? (a) a leg muscle relaxing after exercise. (b) a bacterial population in the bloodstream. (c) several viral particles on the skin. (d) carbohydrates being digested in the stomach.\n\nA: Option (b), bacterial population is the most likely cause for a person developing fever. The answer is (b).\n\nQ: Which change in the state of water particles causes the particles to become arranged in a fixed position? (a) boiling. (b) melting. (c) freezing. (d) evaporating.\n\nA: When water is freezed, the particles are arranged in a fixed position; the particles are still moving for all other options. The answer is (c).\n\nQ: When a switch is used in an electrical circuit, the switch can (a) cause the charge to build. (b) increase and decrease the voltage. (c) cause the current to change direction. (d) stop and start the flow of current.\n\nA: The function of a switch is to start and stop the flow of a current. The answer is (d).\n\nTable 14: Few-shot CoT prompts for ARC-challenge, from Wang et al. (2022b).\n\nQ: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\n\nA: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. The answer is (a).\n\nQ: If a / b = 3/4 and 8a + 5b = 22,then find the value of a. Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2\n\nA: If a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simplifies to 8a + 20a / 3 = 22, which means 44a / 3 = 22. So a is equal to 3/2. The answer is (b).\n\nQ: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance? Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km\n\nA: The distance that the person traveled would have been 20 km/hr * 2.5 hrs = 50 km. The answer is (e).\n\nQ: How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788\n\nA: There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401 three-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. The answer is (b).\n\nTable 15: Few-shot CoT prompts for AQUA, from Wang et al. (2022b).\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nQ: Do hamsters provide food for any animals?\n\nA: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. The answer is yes.\n\nQ: Could Brooke Shields succeed at University of Pennsylvania?\n\nA: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. The answer is yes.\n\nQ: Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\n\nA: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. The answer is no.\n\nQ: Yes or no: Is it common to see frost during some college commencements?\n\nA: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. The answer is yes.\n\nQ: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\n\nA: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. The answer is no.\n\nQ: Yes or no: Would a pear sink in water?\n\nA: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. The answer is no.\n\nTable 16: Few-shot CoT prompts for StrategyQA, from Wei et al. (2022b).\n\nPremise: ”No Weapons of Mass Destruction Found in Iraq Yet.” Based on this premise, can we conclude the hypothesis ”Weapons of Mass Destruction Found in Iraq.” is true?\n\nA: ”No Weapons of Mass Destruction Found” contradicts ”Weapons of Mass Destruction Found”. The answer is no.\n\nPremise: ”A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.” Based on this premise, can we conclude the hypothesis ”Pope Benedict XVI is the new leader of the Roman Catholic Church.” is true?’\n\nA: ”installation of new Pope Benedict XVI.” means ”Pope Benedict XVI is the new leader”. The answer is yes.\n\nPremise: ”A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to be featured on BBC One’s Crimewatch. Colette Aram, 16, was walking to her boyfriend’s house in Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later.” Based on this premise, can we conclude the hypothesis ”Paul Stewart Hutchinson is accused of having stabbed a girl.” is true?\n\nA: The premise does not say Paul Stewart Hutchinson ”stabbed” this girl. The answer is no.\n\nPremise: ”Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.” Based on this premise, can we conclude the hypothesis ”Herceptin can be used to treat breast cancer.” is true?\n\nA: ”Herceptin was approved to treat breast cancer” implies that ”Herceptin can be used to treat breast cancer”. The answer is yes.\n\nTable 17: Few-shot CoT prompts for RTE, from Wang et al. (2022a).\n\n19",
    "reference": "# Summary Of The Paper\n\nThe authors propose an avenue to self-improve large language models without explicit supervision. They first generate candidates (in this case high-confidence rationale-augmented answers in their words) for unlabeled questions and further fine-tune the model on those unlabeled and self-generated solutions. They are able to improve performance on a very large 540B model on datasets that the community evaluates on such as GSM8K, DROP, OpenBookQA, and ANLI.\n\n# Strength And Weaknesses\n\nStrengths: \n1) The empirical gains indicate that the method is effective.\n2) The approach is reasonably straight forward, although difficult to estimate the impact of a variety of hyperparameters and choices (some of which are varied and shown in later in the paper) at each stage due to the complexity of fine-tuning such a large model.\n3) One-type of distillation, fine-tuning a smaller model from the generations of the 540B model, is effective.\n\nWeaknesses:\n1) The method is only broadly applied to the 540B model that almost no one else has access to/can train or fine-tune. The distillation approaches show that the method is effective when using the samples from the 540B model. What happens if you don't have access to such a large model? Can you apply the entire method to a smaller model say < 10B? \n2) It'd be great to see if these approaches can be used with language models that are available to everyone and at scales that most of the community can use.\n3) Replicating this is nearly impossible.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: The paper reads well and understanding most of the paper is relatively easy.\n\nQuality/Novelty: The authors integrate chain-of-thought prompting and self-consistency as means for self-improving large pretrained language models. Technically this basically just takes tools from other work and integrates them, but the evaluation and the utilization is reasonably novel.\n\nOriginality: I think many people in the community over the years have approached a self-supervised, self-improvement goal for AI systems and NLP systems, but this specific approach is relatively original and to my knowledge has not been tried before.\n\nReproducibility: Access to the pretrained model is not available, and the sheer size of the models make reproducibility impossible for the vast majority of labs, universities, and companies alike.\n\n# Summary Of The Review\n\nThe paper is good. The method is relatively straight forward and empirically the method performs well. The paper is clear and has additional experiments such as distillation that make it more compelling. \n\nThere are limitations in that the models here are of squillions of parameters and not publicly available, so its nearly impossible to replicate. The method has not been tested on models that are of reasonable size or ones that are publicly available, which is a significant limitation. Given the compute resources at the authors' disposal, running a few additional experiments on relatively much smaller and publicly available models would strengthen the paper greatly and would be of significant interest to the community.\n\nIn general this paper is of broad interest to the NLP subcommunity at ICLR, so I vote that it be accepted.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nEVERYONE’S PREFERENCE CHANGES DIFFERENTLY: WEIGHTED MULTI-INTEREST RETRIEVAL MODEL\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nUser embeddings (vectorized representations of a user) are essential in recommendation systems. Numerous approaches have been proposed to construct a representation for the user in order to find similar items for retrieval tasks, and they have been proven effective in industrial recommendation systems. Recently people have discovered the power of using multiple embeddings to represent a user, with the hope that each embedding represents the user’s interest in a certain topic. With multi-interest representation, it’s important to model the user’s preference over the different topics and how the preference change with time. However, existing approaches either fail to estimate the user’s affinity to each interest or unreasonably assume every interest of every user fades with an equal rate with time, thus hurting the performance of candidate retrieval. In this paper, we propose the Multi-Interest Preference (MIP) model, an approach that not only produces multi-interest for users by using the user’s sequential engagement more effectively but also automatically learns a set of weights to represent the preference over each embedding so that the candidates can be retrieved from each interest proportionally. Extensive experiments have been done on various industrial-scale datasets to demonstrate the effectiveness of our approach. 1\n\n1\n\nINTRODUCTION\n\nToday, the recommendation system is widely used in online platforms to help users discover relevant items and deliver a positive user experience. In the industrial recommendation systems, there are usually billions of entries in the item catalog, which make it impossible to calculate the similarity between a user and every item. The common approach is, illustrated in Figure 1, retrieving only hundreds or thousands of candidate items based on their similarity to the user embedding on an approximate level (e.g. inverted indexes, location sensitive hashing) without consuming too much computational power, and then sending the retrieved candidates to the more nuanced ranking models. Thus, finding effective user embedding is fundamental to the recommendation quality.\n\nThe user representations learned from the neural networks are proven to work well on large-scale online platforms, such as Google (Cheng et al., 2016), YouTube (Covington et al., 2016), and Alibaba (Wang et al., 2018). Mostly, the user embeddings are learned by aggregating the item embeddings from the user engagement history, via sequential models (?Hidasi et al., 2015; Quadrana et al., 2017; Kang & McAuley, 2018; You et al., 2019). These works usually rely on the sequential model, e.g. a recurrent neural network (RNN) model or an attention mechanism, to produce a single embedding that summarizes the user’s one or more interests from recent and former actions.\n\nRecently researchers (Epasto & Perozzi, 2019; Weston et al., 2013; Pal et al., 2020; Li et al., 2005) have discovered the importance of having multiple embeddings for an individual, especially in the retrieval phase, with the hope that they can capture a user’s multiple interests. The intuition is quite clear: if multiple interests of a user are collapsed into a single embedding, though this embedding could be similar to and can be decoded to all the true interests of the user, directly using the single collapsed embedding to retrieve the closest items might result in items that the user is not quite interested in, as illustrated in Figure 1.\n\n1The code is available at https://anonymous.4open.science/r/MIP-802B\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Mis-representation with single user embedding in the retrieve-then-rank framework.\n\nThough, conventional sequential models like RNN or the Transformer network do not naturally produce multiple sequence-level embeddings as desired in the multi-interest user representation. Existing solutions fall into two directions: 1) split-by-cluster approaches first cluster the items in the user engagement history by category labels (Li et al., 2019) or item embedding vectors (Pal et al., 2020) and then compute a representation embedding per cluster; 2) split-by-attention models adopt transformer-like architecture with two modifications. The query vectors in the attention are learnable vectors instead of the projections from the input and the results of each attention head are directly taken as multiple embeddings (Zhuang et al., 2020; Cen et al., 2020). The limitation of the two approaches is obvious: the split-by-cluster method works best with dense the item feature (Xue et al., 2005); and split-by-attention models bias towards the popular categories owing to its shared query vector among all the users and are inflexible to adjust the number of interests, which is fixed in the training phase as the number of attention heads.\n\nMoreover, the existing multi-interest works ignore one important aspect: the weights for each embedding. In the retrieval stage, given the limited number of items to return, retrieving items from each embedding uniformly will cause a recall problem when the user clearly indicates a high affinity towards one or two categories. Some existing approaches, e.g. PinnerSage (Pal et al., 2020), use exponentially decayed weights to assign a higher score to interests that have more frequent and recent engagements. However, the methods still assume that in the same period, regardless of the interest is enduring or ephemeral, the level of interest decays equally for any user. Furthermore, these works also assume the number of embeddings to be fixed across all users. Not only is this hyperparameter costly to find, but also the assumption that all users have the same number of interests is questionable. Some dormant users can be well represented using one or two vectors, while others might have a far more diverse set of niche interests that requires tens of embeddings to represent.\n\nIn this paper, we propose Multi-Interest Preference (MIP) model that learns user embeddings on multiple interest dimensions with personalized and context-aware interest weights. The MIP model is consist of a clustering-enhanced multi-head attention module to compute multiple interests and a feed-forward network to predict the weights for each embedding from the interest embedding as well as the temporal patterns of the interest. The clustering-enhanced attention overcomes the aforementioned shortcomings from two aspects: the query, key, and value vectors are projected from the user’s engaged items, thus the output of the attention is personalized and minimized the bias toward globally popular categories; moreover, the clustering module can be applied before or after the multi-head attention, releasing the assumption that item features are pre-computed or the item-category labels are available. The main contribution of this paper and the experimental evidence can be summarized as follows:\n\n• We propose a multi-interest user representation model that minimizes the bias towards popular categories and is applicable no matter if the item embeddings are pre-computed. MIP is successful in various industry-scale datasets (Section 4.1, 4.2); Appendix A.1 reveals the bias in global query vector and the error from fixed number of clusters in the split-byattention approaches, comparing to MIP.\n\n• In addition to the multi-facet vector representations of a user, MIP will assign weights to each embedding, which is automatically customized for each user interest, and improve the recall of candidate generation by retrieving more candidates from the most representative embedding (Section 4.3).\n\n• Although if the cluster algorithms require, MIP still asks for a number of clusters during the training phase, the number of clusters in MIP in the inference phase can be trivially increased\n\n2\n\nEmbedding modelUser behavioral datalarge-scale item catalog (~billions)RetrievalRankingRecommended items+++++Other categories in corpusUser interested categories++Single user representationCategories retrievedUser embeddingCandidates (hundreds or thousands)Under review as a conference paper at ICLR 2023\n\nor decreased without re-training of the model. And the experimental results (Appendix A.3) show that re-configuring the number of clusters has an insignificant impact on the retrieval performance, thus allowing the system to trade off the storage and computation cost for better performance. Thus, MIP does not require prior knowledge of the number of interests of users during the model training phase.\n\n2 RELATED WORK\n\nThis work relates to two important aspects of existing recommendation systems: sequential models and the multi-interest framework.\n\nSequential models. A basic consensus in the recommendation system is that user embeddings should be inferred from the user’s historical behavior, and thus the sequential models have been at the heart of recommendation models. A typical and classical sequential model is the Markov Chain (Rendle et al., 2010; He & McAuley, 2016). While Markov Chain captures short-term patterns of engagement sequence well, it fails to make the recommendation that requires memorizing long sequences. With stronger representation power on long sequences, Recurrent neural networks (RNNs), have been adopted for learning user embedding from arbitrarily long sequences, e.g. GRU4Rec (Hidasi et al., 2015) and etc. (Xu et al., 2019; Devooght & Bersini, 2017). Besides the standard RNN models, specialized recurrent units are proposed to meet the special need of incorporating certain information, e.g. user demographic information (Donkers et al., 2017), global context (Xia et al., 2017), interest drifts with time (Chen et al., 2019), and interaction session (Hidasi et al., 2015). Recently, the success of the Transformer network (Vaswani et al., 2017) has brought evolution to sequential modeling tasks and has been soon adapted to the recommendation models, e.g. ComiRec (Cen et al., 2020), BERT4Rec (Sun et al., 2019), TiSASRec (Li et al., 2020), SASRec (Kang & McAuley, 2018), MIND (Li et al., 2019), PinText2 (Zhuang et al., 2020), and also our MIP.\n\nMulti-interest user representation. Representing users by multiple embeddings greatly improved the recommendation quality, but not every existing recommendation model can easily extend to a multi-interest framework. Classical collaborative filtering and matrix factorization methods do not naturally produce multiple user embeddings, and so do RNNs and attention-based models. To discover multiple interests from user engagement history, heuristic methods (Jiang et al., 2020; Yue & Xiang, 2012) and unsupervised learning methods like clustering (Pal et al., 2020; Wandabwa et al., 2020) and community mining (Wang, 2007; Yu, 2008) have been adopted. Besides, researchers have made efforts to modify the existing neural networks to produce multiple results, for instance, the capsule network (Li et al., 2019; Sabour et al., 2017; Cen et al., 2020) and multi-head attention models (Li et al., 2020; Cen et al., 2020; Zhou et al., 2018). However, they require an estimation of the number of interests of users as a hyperparameter and do not learn the weight of interests. Therefore, unlike MIP, they produce an equal number of clusters for every user and treat each interest with uniform importance.\n\nRelation to previous works. The motivation of MIP is to acquire weighted multiple user embeddings with standard self-attention but without explicit item-category labels. ComiRec and PinText2 use global-query attention to produce multiple embeddings, which introduces a bias toward frequent items or popular categories and the phenomenon is shown in Appendix A.1. Furthermore, they also predefine a number of interests that is uniform for all the users. Tisas and BERT4Rec adopt self-attention but can not learn multiple embeddings. MIND relies on the category labels to produce multiple embeddings from self-attention and capsule networks, while the category labels are sometimes unavailable or vague in other applications, e.g. YouTube and Pinterest. PinnerSage produces multiple embeddings without category labels, but requires pre-computed item embeddings. Lastly, MIP learns the weights of the multiple embeddings. The comparison are summarized in Table 1.\n\n3 METHODOLOGY\n\nIn this section, we formulate the recommendation problem and the neural architecture to model the multiple user interests with preference weights in detail.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nName\n\nGRU4Rec TiSASRec BERT4Rec MIND ComiRec PinText2 PinnerSage MIP\n\nUser Embedding single single single multiple multiple multiple multiple multiple\n\nSequential Model RNN time-aware self-attention self-attention label-aware self-attention global-query attention shared global-query attention N/A time-aware SA\n\nAdditional Input interaction session timestamps –\ncategory labels –\n– –\ntimestamps\n\nPreference Weight N/A N/A N/A ✗\n✗ ✗\nheuristic learned\n\nTable 1: Comparison of MIP to existing recommendation models.\n\n3.1 PROBLEM STATEMENT\n\nLet I denote the collection of items (with the ID v or the feature vector p) in the data store, and the U represents the set of users. For each of user u ∈ U, we observe the historical sequence of items that the user has engaged with, S u = (pu , ..., pu ), and the timestamps t1 i ∈ Rd of the engagement T u = (tu (i = 1, ..., k) and their weights wi. Since the user representation is learned only from the history of that user, hereinafter, we omit the superscript for u in both the input and output sides for simplicity. Notations as summarized in Table 2.\n\n) or (vt1, vt2 , ..., vtu |lu|). The objective is to learn a set of user embedding zu\n\n2 , ..., tu\n\n1 , tu\n\n, pu t2\n\nt|lu|\n\n|lu |\n\nWe distinguish two cases of the I: 1) Dense feature p. In industrial recommendation systems, the learning of item features is often decoupled from the recommendation model. For instance, the item feature may contain semantic information extracted from natural language models and visionary features from computer vision models. Given those feature vectors, the recommendation object mainly focuses on learning the user embeddings from the history of user behaviors. 2) Sparse feature v. On the contrary, in many public datasets, the items have no attainable feature except item IDs (encoded as one-hot vectors), and the recommendation model must learn the dense representation of items through only the user-item interactions. The proposed model is capable of handling both cases.\n\n3.2 MULTI-INTEREST USER REPRESENTATION\n\nItem Encoding. We distinguish two conditions of an item. In most practical systems, every item has a pre-computed feature vector (denote as pj) defined in the space where the distance can capture the similarity between the items. In other cases where the recommendation system knows nothing but unique ids (denoted by an one-hot vector vj) about the item, the system needs to learn the dense item feature through an embedding layer:\n\npj = Wembvj\n\n(1)\n\nAdditionally, how the engaged items reflect the user’s interest may be implied by the sequential order and the time when they interact. Thus, we concatenate the item features with both its positional and\n\nNotations Description I, U l\nd p\npi, ti M\nS zi, Z 1[cond] k\n\nItem set and user set Abbreviation of lu, length of S The item embedding dimension An item embedding, p ∈ Rd ti and tu Short form of the pu Attention mask matrix Abbreviation of S u, Engagement history of user u User embedding vector(s), zi ∈ Rd, Z ∈ Rd×k Indicator function, 1 if cond is true, 0 otherwise Maximum number of embedding vectors per user\n\nNotations Description [; ] C\nh q , bh W h r , bh W h r\ndmodel\n\nq\n\ni\n\nVector concatenation operator Cluster assignment, C ∈ Rl. Attention head superscript Query projection weights and bias Key projection weights and bias Projected key/query vector size\n\nTable 2: Notations\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: An overview of MIP architecture. The input to the model is the user engagement history containing item embeddings ([v1...vl] or [p1...pl]) and their timestamps ([t1...tl]). The multi-interest user embedding module produces k embeddings, where k is decided by the clustering method or as a hyperparameter. With clustering and multi-interest representation, the cluster weight module will then estimate the cluster weights for each cluster. Finally, the multi-interest embeddings with corresponding weights are combined to predict the user’s interest in an unseen item p. The processes with dense and sparse features are shown with different arrows.\n\ntemporal information to produce the action encoding:\n\nej = [pj; τ (tj); ρ(j)]\n\nand the positional (ρ) and temporal (τ ) encodings are given by Vaswani et al. (2017):\n\nτ2j(ti) = sin(cid:0)ti/(τmax)2j/mt (cid:1), τ2j+1(ti) = cos(cid:0)ti/(τmax)2j/mt (cid:1) ρ2j(i) = sin(cid:0)i/(ρmax)2j/mp (cid:1), ρ2j+1(i) = cos(cid:0)i/(ρmax)2j/mp (cid:1)\n\n(2)\n\n(3)\n\nThe hyper-parameters are set as τmax = 104, mt = 1, and mp = 1. The unit of timestamps is day. In the Section A.2, other forms of encodings are compared, and the Equation 3 has a weak advantage.\n\nUser Representation Candidates To find only a few vectors to represent the entire user-engaged items, the model has to consider the similarity within the engaged items. The process is achieved through the self-attention layer: i,j = (cid:0)(W h sh\n\nr )(cid:1)/ (4) We constrain the attention scores a with a mask matrix M . In the sparse feature case, let Mi,j = 1 for any i, j; while in the dense feature case, M indicate the intra-cluster relationship, i.e. Mi,j = 1[Ci=Cj ], where the C denotes the cluster-ID. Thus, M requires the attention model to only aggregate items within the same cluster. The generation of M in the dense feature case is detailed in Section 3.6. With M and attention scores a, each attention head aggregates the sequences as:\n\ni,j = sof tmaxi(sh\n\nq )⊤ · (W h\n\nq ej + bh\n\nr ei + bh\n\ndmodel;\n\ni,j)\n\nah\n\n(cid:112)\n\nTo process the aggregated vector from all attention heads, the dropout layer and a feed-forward network (FFN) are applied, and compute the output vectors as\n\n(cid:88)\n\nφh\n\nj =\n\nah\n\ni,jMi,jpi\n\ni\n\n(5)\n\n(6) where j = 1, ..., l. The F F N () has two fully-connected layers with a hyperbolic tangent activation function after the first layer, i.e. F F N (x) = W (tanh(W ′x + b′)) + b.\n\nφj = F F N (Dropout([φ1\n\nj ; ...; φH\n\nj ]))\n\nNow, each φj summarizes the item features pj, its adjacency with other items, and neighbor item features. The remaining question is which vectors from the {φj} are the most representative.\n\nCluster Representations So far, the multi-head attention module has produced l output vectors φ1, ..., φl, and each φi uses pi as the (unprojected) query. To find representative vectors, the model utilizes the clustering results. Denote the last item in each cluster as pμ1, ..., pμk , we take the φμ∗ that uses the last item as query, to represent the corresponding clusters. Then let the user embeddings be zi = φμi, and let the user representation be Z ∈ Rk×d, then\n\nZ = [z⊤\n\n1 ; ...; z⊤ k ]\n\n5\n\n(7)\n\nItem Encoding[p#...p%][e#...e%]Multi-Head AttentionSelect by index[φ#...φ%][z#=φ+,...z-=φ+.]Clustering[v#...v%]M[μ#...μ-] [t#...t%]Mask φ +5 [C#...C%][t#,...,t%] [w#...w-]dot productpyLinearMultiplyMaxdense feature processsparse feature processcommon processtrainable modulesnon-trainable operatorsMulti-interest User Embedding (Sec.3.2)Cluster Weight (Sec.3.3)Engagement Prediction (Sec.3.4)unseen itemw:FFN1,...,kpredictionClustering(Sec.3.6)Under review as a conference paper at ICLR 2023\n\n3.3 CLUSTER WEIGHT MODULE\n\nBesides the multi-interest assumption, it’s also likely that the user favors each interest unequally. As mentioned in the earlier section of this paper, ranking these interests can greatly benefit the candidate generation task given its limited budget. In general, a higher weight should be assigned to an interest cluster if the user engages more with items that belong to it. In order to utilize both the contextual cluster information and the user’s engagement sequence in that cluster, we build a two-layer feed-forward network on top of the cluster representation zj and the temporal encoding of items τ that belong to the cluster j. We will mask those items that belong to other clusters as zero to make the input dimension consistent. The cluster weight can be written as\n\nwj = F F N ([zj; 1[C1∈Lj ] · τ1; ...; 1[Cl∈Lj ] · τl])\n\n(8)\n\nConcretely, the F F N () consists of two fully-connected layers with a sigmoid function in between, e.g. F F N (x) = W (sigmoid(W ′x + b′)) + b. Physically, the clusters learned from Section 3.2 are topics that the user likes, so the preference weights of the clusters should be positive. Therefore, the output of the second layer are normalized to the range of [0, +∞] by softplus function2.\n\n3.4 USER-ITEM ENGAGEMENT PREDICTION\n\nIntuitively, a user will engage with an item as long as the item match one of his/her interests (not all). In other words, it’s important that the item embedding is very close to one of the user embeddings, rather than being as close as possible to all of them. Therefore, the user-item affinity should depend on the maximum of item embedding and user embedding on each interest dimension. Furthermore, user’s affinity to each interest cluster should be embodied in the likelihood as well: a higher weight should be assigned to an interest cluster when the user engages more with items that belong to it. Therefore, we propose the likelihood that a user will engage with an item as follow:\n\ny = max{wjLinear(zj · p)}k\n\nj=1 = max{wj(Wo(zj · p) + bo)}k\n\nj=1\n\n(9)\n\nwhere Z = [z⊤ 1 ; · · · ; z⊤ sions), and p is the item embedding.\n\nk ] is the aforementioned user multi-interest embedding (on k interest dimen-\n\n3.5 LOSS FUNCTION\n\nGiven the set of items with a positive label (I+) and a negative label (I−), the negative log-likelihood (NLL) loss of our model can be written as:\n\nL = −\n\n(cid:88)\n\n(cid:0) (cid:88)\n\nlog(yu\n\ni ) +\n\n(cid:88)\n\nlog(1 − yu\n\ni )(cid:1)/(cid:0)(cid:88)\n\nu∈U\n\npi∈Iu +\n\npi∈Iu −\n\nu∈U\n\n(cid:0)|I u\n\n+| + |I u\n\n−|(cid:1)(cid:1)\n\n(10)\n\n3.6 CLUSTERING\n\nWe have referred to a clustering step in producing attention mask (M in Eq. 5), finding cluster representation through last item in each cluster (μk in Eq. 7), and learning the cluster weight (C in Eq. 8). Generically, the clustering algorithm takes the set of vectors and produces the clusters L1, L2, ..., Lk which is a partition of S. From the partition, the following items are computed: cluster assignment C, a list of the same length as S that Ci = r if pi ∈ Lr, i = 1, ..., l and r = 1, ..., k; attention mask M , a symmetric binary matrix that Mi,j = 1[Ci=Cj ]; and last item index μ that μr denotes the index of the latest item if cluster r.\n\nThe clustering is applied differently with the dense and sparse features. With dense features, the exogenous item embeddings (pj) are clustered, and the cluster results lead to a non-constant M mask, while with the sparse feature, the clustering is applied to the user representation candidates (Eq. 6). When unspecified, the model uses the Ward Jr (1963)’s clustering method.3\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nPinnerSage TiSASRec ComiRec MIP\n\nP@20 R@20 AUC 81.45 29.61 74.02 84.96 –\n– 87.50 34.54 86.35 89.26 35.28 88.20\n\nNLL 103.3 47.8 40.7 37.7\n\n# Items # Interactions # Training # Test # Validation\n\nAmazon MovieLens 425,582 51M 57,165 5,000 5,000\n\n15,243 20M 127,212 5,000 5,000\n\nTaobao 823,971 100M 343,171 10,000 10,000\n\nTable 3: Performance on the Pinterest dataset.\n\nTable 4: Dataset statistics.\n\n4 EXPERIMENTS\n\nWe conduct an exhaustive analysis to demonstrate the effectiveness of MIP on the data from Pinterest, one of the largest online content discovery platforms, and on public datasets.\n\n4.1 WITH SPARSE FEATURE\n\nWe first evaluate MIP on learning from collaborative filtering datasets, where the item features are absent and will be learned from the user-item interactions.\n\nDataset: Three public datasets are used: Amazon-book 4 (hereinafter, Amazon), Taobao5, and MovieLens6. We adopted a 10-core setting as previous works (Li et al., 2020; Wang et al., 2019) and filtered out rare items that appear less than 10 times in the whole dataset, and the inactive users who interact with less than 100 items. We split each user’s engagement history to non-overlapping sequences of length 100, and use the first 50 items to learn the user embedding(s) and the last 50 items as positive samples to rank. For each sequence, another 50 negative samples are uniformly randomly selected from the items that the user does not interact with. The numbers of sequences in the datasets are listed in Table 4.\n\nModels configurations: We compare the open-sourced baseline models with MIP. The configurations follow the principles: 1) all the item and user embedding vectors have the same size (d = 32); 2) if the models adopt a multi-head attention module, the number of attention heads is the same (H = 8); 3) the baseline models should have comparable or more parameters than MIP. We let the hidden size in GRU4Rrec be 128, the key and query projected dimension (dmodel in Eq. 4) is labeled in place with the results, and if the model contains a position-wise FFN (Eq 6), the FFN is two-layered fully-connected with a hidden size of 32. The BERT4Rec model is originally proposed to predict the item directly as a classification task, which is unrealistic in the retrieval setting, so we take its last BERT output as the user embedding to compute the similarity between user and item, and train with the NLL loss. We disabled the session-parallel mini-batch in GRU4Rec since the session information is absent. We also replace the text encoder in the PinText2 with an item embedding layer since the input in our experiments is items instead of sequences of words.\n\nMetrics: The models are evaluated in the retrieval scenario, where the recommendation system needs to recommend a batch of items to the user. The following metrics are used: area under the ROC curve (AUC), negative log-likelihood (NLL), precision at top-K (P@K), and recall at top-K (R@K). The metrics are reported in 10−2.\n\nTraining: All the models are trained for 100 epochs on Tesla T4 GPU with an early stop strategy that ceases the training when validation AUC does not improve for 20 epochs. The MIP is trained with two-phase. In the first phase, the parameters in the cluster weight module are frozen, and the weights for any cluster are set as 1. After the model converges in the first phase, the parameter freeze is removed, and then all parameters are trained until converge. Appendix A.4 compared and analyzed this training strategy of MIP.\n\nβ log(1 + exp(βx)). We set β = 1\n\n2y = 1 3We adopted the scikit-learn implementation. https://scikit-learn.org/stable/modules/ generated/sklearn.cluster.AgglomerativeClustering.html, with n_cluster=5, and other default arguments. In Appendix A.3, other clustering methods are compared.\n\n4https://jmcauley.ucsd.edu/data/amazon/ 5https://tianchi.aliyun.com/dataset/dataDetail?dataId=649 6https://www.kaggle.com/grouplens/movielens-20m-dataset\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nParams.\n\nGRU4Rec BERT4Rec BERT4Rec PinText2 TiSASRec ComiRec MIP\n\n66338 50242 55426 69634 67586 67586 49347\n\nConfig\n\nAmazon\n\nTaobao\n\nMovieLens\n\nlayers 1\n1 2\n1 2\n1 1\n\ndmodel AUC R@50 AUC R@50 AUC R@50 90.31 81.55 90.11 81.47 90.24 81.46 81.68 71.58 90.16 81.46 90.65 70.92 93.34 88.49\n\n74.48 74.52 74.47 66.88 74.43 65.61 88.43\n\n68.62 68.11 72.10 55.83 72.11 71.72 80.47\n\n63.5 63.15 66.52 54.13 66.67 67.36 78.85\n\n96.13 95.95 96.02 88.27 96.02 96.25 93.04\n\n– 64 32 256 64 256 32\n\nTable 5: Performance on public datasets. Params excludes the parameters in the embedding matrix.\n\nResults: The performance is summarized in Table 5. MIP has stronger performance on Amazon and Taobao datasets and is trivially worse than GRU4Rec and ComiRec in AUC on MovieLens. Intuitively, the results might be because the shopping scenario fits more to our multi-interest assumption, users purchase only items from their interested categories, like a type of sports or habits, while the majority of people would watch all types of movies, and if they like the movie largely depends on the movie quality, instead of movie category. In addition, since all models have very close performance, MIP is still a competitive approach in applications that do not support the strong multi-interest assumption.\n\n4.2 WITH DENSE FEATURE\n\nDataset: The dataset contains user engagement history collected from Pinterest, an image-sharing and social media service that allows users to share and discover visual content (images and videos). The interactions between a user and an item (also referred to as a pin) are categorized into impression (pin is shown to the user), clickthrough (user clicks the pin), re-pin (user saves the pin into their board collection), and hide (user manually hides the pin). In total, there are 38 million interactions from 510 thousand users during three weeks period of time. Each pin is represented as a 256-dimension feature extracted by the PinSage model (Ying et al., 2018).\n\nActive users could have very long engagement sequences in a day. Let the engagement sequence of a user be {j1, j2, · · · , jn}, then for each training sample, we use the first l engagements to predict the l future engagements. We also enforce a one-day gap between these two segments, because nearby user engagements are usually very consistent with each other (falling into the same category) and thus make the prediction task easy. l is set to 50, which is the same as the setting in Cen et al. (2020). We treat click-through and re-pin as the positive label and hide (which is less often), and impression (without click or re-pin) as the negative label. Since these negative data have also been recommended to the user at some point, they are likely to be still relevant to the user, and thus correlate with the positive data. In order to alleviate the bias, we introduce the random negative data where pins are sampled from the whole set of pins. The entire negative dataset will consist of 50% observed negative data (impression and hide), and 50% random negative data.\n\nBaselines and Model Configuration: We compare the multi-interest models PinnerSage and ComiRec, and the single-embedding model Tisas with the same setting as in Section 4.1.\n\nResults and Analysis. As shown in Table 3, MIP outperforms all the state-of-the-art models that are published very recently. The detailed comparison of the model and explanation of the improvement are as follows:\n\n• PinnerSage shares the clustering algorithm with MIP, but differs in 1) the cluster is represented by the medoid, 2) and the cluster weight uses the heuristic model. Instead, MIP learns the cluster representations and weights from data end-to-end from data, which aligns with our intuition.\n\n• TiSASRec has a similar attention model structure as MIP, except only using the last output of the attention model as the user embedding. The comparison confirms the necessity of multi-interest representation, as in ComiRec and MIP.\n\n• Compared to ComiRec, MIP interestingly shows that self-attention has stronger representation power than attention with global-query. In Appendix A.1, we use synthetic data to illustrate the internal difference between the two types of models.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(1) On public dataset.\n\n(2) On Pinterest dataset.\n\nFigure 3: Cluster weight variants performance (difference to the best baseline models)\n\n4.3 ABLATION STUDY\n\nThis section focus on the claim that the user’s preference should be dynamically learned from the temporal pattern by demonstrating the effectiveness of the learned personalized cluster weights module of MIP. We refer the reader to Appendix for a comprehensive ablation study on attention mechanism (A.1), positional and temporal encoding (A.2), re-configuration and comparison of the clustering method post-training (A.3).\n\nConfigurations. To validate the assumption that the preference trends (weights of multiple interests) change from user to user, we compare the MIP with two variants that disable the cluster weight module. The first one (referred to as MIP (Equal Weight)) constantly assigns 1 as cluster weight, thus leading to an equally weighted multi-interest user representation. Another one (referred as MIP (Exp Decay)) uses the heuristic formula, an exponential decay weights given by wj = (cid:80) Ci∈j exp(−λ(tnow − ti))(Pal et al., 2020). We let the tnow be the last user engagement time tl, since in practice it’s unrealistic to update the weights in real-time according to the current timestamp. According to Pal et al. (2020), we also set λ = 0.01, which balances well between emphasizing recently engaged items and accentuating frequently engaged categories. We let the number of clusters k = 5.\n\nThe variants on the cluster weights are evaluated on Pinterest and the public dataset.\n\nResults and Analysis The performances are shown in Figure 3. In most experiments, the AUC and recall can benefit from learned cluster weights (our proposal). From Fig. 3, the equal weights and exponential decay have an advantage on AUC and recall respectively, but learned weights are overall superior to other options. Note that the exponential decay rate is global to any user and any cluster, therefore, the results demonstrated that personalized and context-aware cluster weight estimation is a more reasonable solution to the recommendation.\n\n5 CONCLUSIONS\n\nIn this paper, we study the problem of multi-interest user embedding for recommendation systems. We follow the recent findings on representing users with multiple embeddings, which has been proven helpful over the single user representation. However, we find that in industrial recommendation systems, it is important to have a set of weights for these multiple embeddings for a more efficient candidate generation process due to its budget on the number of items returned. More specifically, we define the likelihood of an engagement based on the closest user embedding to the item embedding and update the weight for the corresponding cluster. We also illustrate the different numbers of interests (embeddings) that users could have, which is fundamentally different from the assumption of similar works. In addition, an attention mechanism is applied in the model architecture, and we have done extensive studies on different model design choices. Finally, case studies on multiple real-world datasets have demonstrated our advantage over state-of-the-art approaches.\n\n9\n\nAmazonTaobaoMovieLens5.02.50.02.55.07.5AUCMIP(Exp Decay)MIP(Equal Weight)MIPAmazonTaobaoMovieLens0.02.55.07.510.0Recall@50P@20R@20AUCNLL321012MIP(Equal Weight)MIPUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nYukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang. Controllable multi-interest framework for recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2942–2951, 2020.\n\nXu Chen, Yongfeng Zhang, and Zheng Qin. Dynamic explainable recommendation based on neural attentive models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 53–60, 2019.\n\nHeng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems, pp. 7–10, 2016.\n\nPaul Covington, Jay Adkan, and Emre Sargin. Deep neural networks for youtube recommendations.\n\nIn\n\nProceedings of the 10th ACM conference on recommender systems, pp. 191–198, 2016.\n\nRobin Devooght and Hugues Bersini. Long and short-term recommendations with recurrent neural networks. In Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization, pp. 13–21, 2017.\n\nTim Donkers, Benedikt Loepp, and Jürgen Ziegler. Sequential user-based recurrent neural network recommendations. In Proceedings of the Eleventh ACM Conference on Recommender Systems, pp. 152–160, 2017.\n\nAlessandro Epasto and Bryan Perozzi. Is a single embedding enough? learning node representations that capture\n\nmultiple social contexts. In The World Wide Web Conference, pp. 394–404, 2019.\n\nMartin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering\n\nclusters in large spatial databases with noise. In kdd, volume 96, pp. 226–231, 1996.\n\nRuining He and Julian McAuley. Fusing similarity models with markov chains for sparse sequential recommendation. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pp. 191–200. IEEE, 2016.\n\nBalázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations\n\nwith recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015.\n\nHao Jiang, Wenjie Wang, Yinwei Wei, Zan Gao, Yinglong Wang, and Liqiang Nie. What aspect do you like: Multi-scale time-aware user interest modeling for micro-video recommendation. In Proceedings of the 28th ACM International Conference on Multimedia, pp. 3487–3495, 2020.\n\nWang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018 IEEE International\n\nConference on Data Mining (ICDM), pp. 197–206. IEEE, 2018.\n\nChao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang, Guoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. Multi-interest network with dynamic routing for recommendation at tmall. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pp. 2615–2623, 2019.\n\nJiacheng Li, Yujie Wang, and Julian McAuley. Time interval aware self-attention for sequential recommendation. In Proceedings of the 13th International Conference on Web Search and Data Mining, pp. 322–330, 2020.\n\nYu Li, Liu Lu, and Li Xuefeng. A hybrid collaborative filtering method for multiple-interests and multiple-content\n\nrecommendation in e-commerce. Expert systems with applications, 28(1):67–77, 2005.\n\nAditya Pal, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles Rosenberg, and Jure Leskovec. Pinnersage: Multi-modal user embedding framework for recommendations at pinterest. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2311–2320, 2020.\n\nMassimo Quadrana, Alexandros Karatzoglou, Balázs Hidasi, and Paolo Cremonesi. Personalizing sessionbased recommendations with hierarchical recurrent neural networks. In proceedings of the Eleventh ACM Conference on Recommender Systems, pp. 130–137, 2017.\n\nSteffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. Factorizing personalized markov chains for next-basket recommendation. In Proceedings of the 19th international conference on World wide web, pp. 811–820, 2010.\n\nSara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. arXiv preprint\n\narXiv:1710.09829, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nHui Shi, Yang Zhang, Hao Wu, Shiyu Chang, Kaizhi Qian, Mark Hasegawa-Johnson, and Jishen Zhao. Continuous cnn for nonuniform time series. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3550–3554. IEEE, 2021.\n\nJianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis\n\nand machine intelligence, 22(8):888–905, 2000.\n\nFei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management, pp. 1441–1450, 2019.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\n\nand Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n\nHerman Wandabwa, M Asif Naeem, Farhaan Mirza, Russel Pears, and Andy Nguyen. Multi-interest user profiling in short text microblogs. In International Conference on Design Science Research in Information Systems and Technology, pp. 154–168. Springer, 2020.\n\nFang Wang. Multi-interest communities and community-based recommendation. 2007.\n\nJizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. Billion-scale commodity embedding for e-commerce recommendation in alibaba. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 839–848, 2018.\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval, pp. 165–174, 2019.\n\nJoe H Ward Jr. Hierarchical grouping to optimize an objective function. Journal of the American statistical\n\nassociation, 58(301):236–244, 1963.\n\nJason Weston, Ron J Weiss, and Hector Yee. Nonlinear latent factorization by embedding multiple user interests.\n\nIn Proceedings of the 7th ACM conference on Recommender systems, pp. 65–68, 2013.\n\nBin Xia, Yun Li, Qianmu Li, and Tao Li. Attention-based recurrent neural network for location recommendation. In 2017 12th International Conference on Intelligent Systems and Knowledge Engineering (ISKE), pp. 1–6. IEEE, 2017.\n\nChengfeng Xu, Pengpeng Zhao, Yanchi Liu, Jiajie Xu, Victor S Sheng S. Sheng, Zhiming Cui, Xiaofang Zhou, and Hui Xiong. Recurrent convolutional neural network for sequential recommendation. In The World Wide Web Conference, pp. 3398–3404, 2019.\n\nGui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, Hua-Jun Zeng, Yong Yu, and Zheng Chen. Scalable collaborative filtering using cluster-based smoothing. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 114–121, 2005.\n\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974–983, 2018.\n\nJiaxuan You, Yichen Wang, Aditya Pal, Pong Eksombatchai, Chuck Rosenburg, and Jure Leskovec. Hierarchical temporal convolutional networks for dynamic recommender systems. In The world wide web conference, pp. 2236–2246, 2019.\n\nLi Yu. Using ontology to enhance collaborative recommendation based on community. In 2008 The Ninth\n\nInternational Conference on Web-Age Information Management, pp. 45–49. IEEE, 2008.\n\nWu Yue and Chen Xiang. A multi-interests model of recommendation system based on customer life cycle. In 2012 Fifth International Conference on Intelligent Computation Technology and Automation, pp. 22–25. IEEE, 2012.\n\nTian Zhang, Raghu Ramakrishnan, and Miron Livny. Birch: an efficient data clustering method for very large\n\ndatabases. ACM sigmod record, 25(2):103–114, 1996.\n\nChang Zhou, Jinze Bai, Junshuai Song, Xiaofei Liu, Zhengchao Zhao, Xiusi Chen, and Jun Gao. Atrank: An attention-based user behavior modeling framework for recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\nJinfeng Zhuang, Jennifer Zhao, Anant Subramanian, Srinivas, Yun Lin, Balaji Krishnapuram, and Roelof van\n\nZwol. Pintext 2: Attentive bag of annotations embedding. In Proceedings of DLP-KDD 2020, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 VARIANTS OF ATTENTION MECHANISM\n\nTo interpret the performance improvement of our models against other attention models that have been applied in the recommendation system. We further construct a synthetic dataset and visualize the internal attentions and the user representations aggregated from different attentions.\n\nSynthetic dataset. Without loss of generality, we assume there’s G global clusters in the corpus, representing different global categories, each of which is a d-dimensional Gaussian distribution. Each user is interested in up to k (< G) categories, referred as user clusters. We generate the oracle user interest model by sampling no more than k clusters from G global clusters following a multinomial distribution. Then each of the item in the user engagement history is sampled by two steps: uniformly sample one cluster from user clusters, then sample from the d-dimensional Gaussian distribution. Note that in the synthetic model, the item-to-cluster affinity is measured in Euclidean distance, while in the recommendation model, the affinity is decided by cosine distance. To eliminate this discrepancy, we force the Gaussian distributions to center on the unit sphere, so that the rankings by cosine distance and Euclidean distance are consistent.\n\nWe use a 2D dataset (d = 2) for visualization purpose and another high-dimensional dataset for quantitative evaluation. For 2D dataset, we set a relatively small G = 8 and k = 4 in order to have a clear boundary between clusters. Since there are only 162 distinct subsets7 with G = 8, k = 4, we use 100 of them for training, 31 for validation and the remaining 31 for testing. For high-dimensional dataset, we set G = 1024 and k = 8, and let d = 16, 32, 64, 128. We generate 10000 users for training, 1000 for validation and another 1000 for testing.\n\nAttention models. We focus on comparing our attention model (i.e. self-attention), the attention model utilized in ComiRec (i.e. Non-shared query), and the one used in PinText2 (i.e. Shared query). The comparison of the attentions are in Table 6.\n\nFor simplicity, we remove the temporal and positional encoding from the computation of attentions, skip the Ward clustering step from MIP, and directly represent user as Equation 7. Also, the dropout layer is removed in order to eliminate randomness in visualization.\n\nMetrics. We visualize the intermediate results and user representations learned from the 2D dataset for qualitative evaluation. For high-dimensional data, we evaluate the performance by AUC and normalized discounted cumulative gain (nDCG).\n\nQualitative results and interpretations. Figure 4 shows the learned user representations given the engagement history. There are three observations. 1) When H = 1, global query attention fails to capture all the user interests, while the self attention model is free from the limitation. 2) Viewing from the third row, self-attention model is more accurate in learning cluster representations than global query models. The later is systematically bias due to the global query as shown in global query models Figure 6. 3) All the models learn super-clusters, depending on the bias in the dataset. For the example show in Figure 4, the two adjacent clusters on the top side of the unit circle is often represented to be a super-cluster.\n\nWe also visualize the internal attention scores and self attention models (Figure 5). Some attention heads show highly similar attention patterns because their queries are close to each other, which\n\nModel\n\nGlobal query\n\nNon-shared\n\nShared\n\nSelf-attention\n\nKey vector kj Query vector q shared between sequences q shared between att. heads Dot-product\n\n(W h\n\nr pj + bh)\n\nq\n\nqh\n\nYes\n\nNo j = qh⊤ · kh eh\n\nj\n\nYes j = q⊤ · kh eh\n\nj\n\nTable 6: Variations of multi-head attention.\n\nqh\n\ni = W h q pi + bh No No\n\nq\n\ni,j = qh⊤ eh\n\ni\n\n· kh j\n\n7Number of ways to select no more than 4 clusters from a pool of 8 clusters: 162 = (cid:0)8\n\n1\n\n12\n\n(cid:1) + (cid:0)8\n\n(cid:1) + (cid:0)8\n\n(cid:1) + (cid:0)8\n\n4\n\n(cid:1)\n\n3\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Learned user representations.\n\nFigure 5: Learned attention scores in self attention model. Darker color represents (a) the indicator function higher values. 1[Ci=Cj ], (b-d) ai,j. The input sequence is re-ordered for better visualization.\n\nFigure 6: Learned global interests in global query models. The query vector is reversely projected and normalized.\n\nFigure 7: Performance comparison on high dimensional synthetic dataset. d denotes the feature dimension and H is the number of attention heads.\n\ncan be verified from Figure 6. Figure 5 compares the ground truth attention model with the learned attention. The learned attention shows clear boundaries between clusters in the heatmap. Note that the ground truth ignores the adjacency of clusters but the self attention model considers the similarity between clusters, so Figure 5(a) is block diagonal while Figure 5(b-d) has dark blocks off the diagonal.\n\nQuantitative results. Previous results show intuitive comparison between global query models and the self attention model, and the quantitative results further confirm the consistency of performance gain of self attention. Experiments are repeated on dataset for feature dimension d = 16, 32, 64, 128 and number of attention heads H = 4, 8. Figure 7 shows that the MIP model constantly and significantly outperforms global query models. As illustrated in the 2D dataset, the performance gain benefits from the personalized user representation, rather than matching to the globally popular clusters. Another observation from the result is that for global query models, H = 4 under-performs H = 8 models, as the number of attention head decides the number of global clusters the model can learn; however for self attention model, H = 4 performs even better than H = 8. The explanation is that self-attention model does not require growing number of attention heads with respect to the number of global clusters, and H = 4 could be already enough for capturing user interest but easier than H = 8 to train.\n\n13\n\nH=1non-shared queryshared queryself attentionH=4H=8observed itemsnegative samplesuser embeddingsprojected user embeddings(a) ground truth(b) H=1(c) H=4(d) H=8non-shared queryH=1H=4H=8shared querycluster 0cluster 1cluster 2cluster 3cluster 4cluster 5cluster 6cluster 7global queryd=16 H=4d=16 H=8d=32 H=4d=32 H=8d=64 H=8d=128 H=80.900.951.00nDCGd=16 H=4d=16 H=8d=32 H=4d=32 H=8d=64 H=8d=128 H=80.80.91.0AUCnon-shared queryshared queryself attentionUnder review as a conference paper at ICLR 2023\n\nDataset\n\nAmazon\n\nTaobao\n\nMovieLens\n\nConfiguration item embedding +positional +temporal +both item embedding +positional +temporal +both item embedding +positional +temporal + both\n\nAUC Recall 73.75 76.34 74.28 76.19 76.94 78.59 78.22 79.31 81.75 82.06 86.22 86.54 86.72 86.56 85.83 86.59 94.45 95.26 94.31 95.01 94.19 94.96 94.12 94.61\n\nTable 7: Ablation study on the positional and temporal encoding on public datasets. (in 10−2)\n\nA.2 POSITIONAL AND TEMPORAL ENCODING\n\nConfiguration item embedding + positional + temporal (one-hot) + temporal (two-hot) + temporal (sinusoid) + both (one-hot) + both (two-hot) + both (sinusoid)\n\nAUC 0.8923 0.8846 0.8850 0.8846 0.8921 0.8861 0.8852 0.8926\n\nNLL 0.377 0.386 0.388 0.385 0.377 0.382 0.387 0.377\n\nTable 8: Influence of temporal and positional encoding in attention on the performance in MIP\n\nIn Eq. 2, the sequential (positional) and temporal information are encoded and included in the self-attention module to produce the multi-interest representations. The motivation is that given items from the same category, the recent ones might better represent the user’s current interest than the obsolete items. We verify 1) if the incorporation of positional and temporal encoding is critical to the performance; 2) how the encoding (Eq. 3) method affects the performance.\n\nConfiguration. The MIP are configured on the two set of choices: the Eq. 2 can be configured alternatively:\n\n• item embedding only: ej = pj. • + positional: ej = [pj; ρ(j)], where ρ(j) is given by Eq. 3. • + temporal: ej = [pj; τ (tj)], where τ (tj) is given by Eq. 3. • + positional and temporal: Eq. 2 and Eq. 3\n\nand there are several other choices of temporal encodings other than the sinusoidal form in the Eq. 3:\n\n• One-hot (Zhou et al., 2018): Create exponential buckets [0, b), [b, b2), · · · , [bk−1, ∞) with\n\nbase b, and encode the timestamp as an one-hot vector, i.e. τi = lookup(buckets(t)).\n\n• Two-hot (Shi et al., 2021): Similarly create exponential boundaries {0, b, b2, · · · , bk−1, ∞}, and encode the timestamp as τi = logb(t) − i and τi+1 = i + 1 − logb(t), where bi ≤ t < bi+1.\n\nDataset and Results. The options to include positional and temporal information are evaluated on all the dataset (Tab. 7), and the encodings methods are compared exhaustively on Pinterest dataset (Tab. 8).\n\nAnalysis. Two conclusions can be made from Tab. 7 and Tab. 8: 1) including both temporal and positional information is a safe option, which has the best performance on Amazon and Pinterest and marginally (< 0.01) worse performance on Taobao and MovieLens; and 2) the model is insensitive to encoding methods.\n\nA.3 CLUSTERING OPTIONS\n\nThe Ward’s algorithm is applied to MIP considering its success in PinnerSage(Pal et al., 2020), it’s beneficial to explore the selection of clustering algorithm and the number of clusters on the collaborative filtering dataset. To illustrate the impact, we evaluate MIP with a wide range of clustering algorithms.\n\nModel Configuration and training: MIP models are configured with an attention module that takes both positional and temporal encoding. For unweighted MIP, no clustering method is applied to the encoded user engagement history {z∗} (computed from Eq. 6) in the training stage. For weighted MIP, Ward’s algorithm is applied to {z∗} and the number of clusters is set to 5. To keep the MIP\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nClustering Method None\n\nWard\n\nK-Means\n\nSpectral\n\nBIRCH\n\nDBSCAN\n\nInference Clusters -\n5 8\n10 5\n8 10 5\n8 10 5\n8 10 -\n\nUnweighted MIP\n\nWeighted MIP\n\nAmazon Taobao MovieLens Amazon Taobao MovieLens\n\n73.11 71.56 71.99 72.16 71.58 71.95 72.14 72.28 72.37 72.64 71.98 72.03 72.44 71.98\n\n82.09 80.58 80.99 81.20 80.62 81.03 81.22 80.72 81.08 81.26 80.63 81.02 81.21 80.63\n\n95.97 95.53 95.72 95.78 95.53 95.71 95.77 95.54 95.73 95.78 95.52 95.71 95.78 95.52\n\n- 79.31 80.47 80.84 79.26 80.66 80.62 78.99 80.79 81.19 79.39 80.65 80.91 70.05\n\n- 86.49 87.85 88.42 86.18 88.02 88.61 85.84 87.61 88.40 86.29 88.03 88.53 75.58\n\n- 94.61 95.25 95.25 94.86 95.17 95.10 94.46 94.81 95.07 94.61 95.25 95.25 89.63\n\nTable 9: Comparison of clustering options in AUC (in 10−2). Note that the number of inference clusters is independent of training, i.e. changing the number of inference clusters does not require the re-train of the model.\n\nfully differentiable, the cluster embedding is the encoding of the last item in each cluster, instead of the medoid.\n\nInference: The choice of the clustering in the inference phase is independent of its configuration during the training. We explore the inference options on the pre-trained models. Different types of clustering methods are compared:\n\n• Ward: hierarchical clustering method that minimizes the sum of squared distances within all\n\nclusters.\n\n• K-Means: an iterative method also minimized the sum of in-cluster summed squared\n\ndistances.\n\n• Spectral(Shi & Malik, 2000): performs clustering on the projection of the normalized\n\nLaplacian computed from the affinity matrix.\n\n• BIRCH(Zhang et al., 1996): another hierarchical method that clusters the points by building\n\nthe Clustering Feature Tree.\n\n• DBSCAN(Ester et al., 1996): a density-based clustering method that does not require\n\nspecifying the number of clusters.\n\nThe number of clusters is set to 5, 8, and 10 when required. Note that during training, the number of clusters is fixed to 5, however, after training, MIP can produce other numbers of embeddings per user, which gives the system huge flexibility to trade-off between storage/computation cost and recommendation performance.\n\nResult and analysis: There are two observations from Table 9. 1) the different clustering algorithm has a marginal impact on the performance. While PinnerSage reported that Ward’s algorithm outperforms the K-Means, their result does not conflict with our observation here. Recall that for PinnerSage and our experiment on the Pinterest dataset, the clustering method is applied to the exogenous item embeddings, thus the clustering methods can be influenced by the non-flat geometry and outliers. However, with the collaborative filtering dataset, the clustering method is applied to the encodings produced by multi-head self-attention layers which average the embedding of the items and all other items (Eq. 4). The encodings after the multi-head self-attention should be smoothly distributed, and as a result, any clustering methods work almost equally well on that. 2) Selecting the number of clusters is a non-trivial trade-off. The motivation to decrease the number of clusters is the storage and computation cost which grow linearly as the number of clusters increases. For unweighted MIP, though the non-clustering (each item is a cluster) settings have the best AUC, decreasing the number of user embedding from 50 (non-clustering) to 10 is still acceptable. For weighted MIP, since it’s impossible to learn the clustering weights without applying a clustering method, the trade-off can be more complicated: besides the storage concern, when the number of clusters increases the\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nMIP Model equal weight two-phase direct-train\n\nAmazon Epoch AUC R@50 66.67 73.11 78.85 80.47 57.47 57.61\n\n22 6\n18\n\nTaobao Epoch AUC R@50 74.86 82.09 88.43 88.49 80.23 80.33\n\n4 4\n28\n\nMovieLens Epoch AUC R@50 90.06 95.97 93.34 93.04 93.68 92.43\n\n34 6\n14\n\nTable 10: Comparison of training strategy and the performance of weighted MIP. The columns Epoch shows the training epochs when the best validation AUC is achieved.\n\nLatency/ Recall Train (std) Inference (std) R@50\n\nGRU4Rec\n\n218.57 (0.81) 1.15 (0.20) 63.50\n\nBERT4Rec (L = 1) 925.36 (108.96) 38.34 (56.60) 63.15\n\nBERT4Rec (l = 2) 1058.34 (733.33) 57.53 (56.23) 66.52\n\nPinText2\n\nTiSASRec ComiRec\n\n555.79 (76.96) 14.46 (54.99) 54.13\n\n452.94 (67.07) 14.54 (56.34) 66.67\n\n479.59 (67.50) 14.61 (55.70) 67.36\n\nMIP (total) 998.62 (56.31) 40.05 (27.08) 78.85\n\nMIP (clustering) 5.86 (0.76) 5.93 (0.72) -\n\nTable 11: Latency and performance comparison of the models. Training and inference latencies are measured in ms, and brackets show the standard deviations.\n\naverage information to learn the weights of each cluster decreases and consequently may hurt the overall performance; on the other hand, 10-cluster settings are better than the 5-cluster settings for all the dataset.\n\nA.4 TRAINING WEIGHTED MIP WITH SPARSE FEATURE DATASET\n\nAs mentioned in Section 4.1, when training MIP on a collaborative filtering dataset, we first train the equally weighted MIP until converging, then lift the parameter freezing to train the cluster weight module together. We detailed the reason and explanation of the strategy here.\n\nConfigurations. The model is trained twice with the same configuration in Section 4.1. The first training adopts a two-phase strategy. The first phase model is denoted as MIP (Equal Weight), and the final model is denoted as MIP (two-phase). In the second round, the model is directly trained from random initialization and is denoted as MIP (direct-train).\n\nResults: Table 10 shows the performance and number of training epochs to achieve the best performance. The difference between the two-phase and direct-train is significant. Two-phase training not only shortens the training process but also achieves remarkably better results.\n\nAnalysis: It’s unsurprising that two-phase can accelerate the training, but we need to understand why random-initialized MIP under-performs equally weighted MIP, while the MIP is a generalization to equally weighted MIP. The clue is lying in the experiment on the Pinterest dataset. On the Pinterest dataset, the weighted model is randomly initialized and outperforms the equally weighted one. The difference between the two experiments is two-fold: 1) the Pinterest dataset contains dense item features thus no item embedding matrix needs to be learned; 2) the clustering algorithm is applied to the item embedding in the Pinterest dataset but to the output of multi-head self-attention (MHA) in the collaborative filter dataset. As a result, in the Pinterest experiments, the clustering assignment is reasonably reliable throughout the training, since the item embeddings are always meaningful. On the contrary, on the collaborative filtering dataset, with the random initialization, the item encodings after MHA are much noisier at the beginning of the training, and thus the cluster assignment can be fairly random. Then the input and output of the cluster weight module may be meaningless, which causes hardship in learning the weighted MIP.\n\nA.5 MODEL LATENCY COMPARISON\n\nSeeing the performance gain, another prominent question will be what is the time cost of the performance increase. In this section, we profiled the model latency on a desktop computer with a 12-core Intel i7-8700k CPU, and a single Nvidia GeForce RTX 2080 Ti GPU. The neural network training and inference is on the GPU with vanilla PyTorch framework (version 1.12) without any further optimization on the computation. The clustering algorithm in MIP is performed by CPU with\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nNLL\n\nAUC R@50 AUC 88.46 92.97 92.32\n\nTriplet (α =0.2) R@50 88.20\n\nTriplet (α =0.5) R@50 AUC 90.08 90.33\n\nTriplet (α =0.8) R@50 AUC 90.61 90.60\n\nTable 12: MIP Performance on MovieLens with difference loss functions.\n\nPython’s scikit-learn package. We set the batch size to 1 and the dataset to Amazon, then measure and summarize the training and inference latency in Table 11.\n\nThere are a few observations from Table 11. First, comparing to the neural network inference latency, the clustering step time cost is trivial. PinText2, ComiRec, and TiSASRec has similar training and inference latency, while the performance of them are worse than MIP. BERT4Rec has similar latency as MIP since our sequential model architecture are similar, while the BERT4Rec has worse performance. GRU4Rec has the least inference time. Notice that the standard deviations of the inference latency of PinText2, TiSASRec, and ComiRec are large. It indicates, though on average the three models are faster in inference, MIP inference latency is less possible to be very large while the other latency might be several times longer than average.\n\nConclusively, MIP, as well as other baselines compared, can all satisfy the latency requirement when applying online, even without further optimization on the computation and serving. MIP has higher time cost compared to some of the baselines, but the performance increase is also appealing.\n\nA.6 LOSS FUNCTION\n\nBeside the NLL loss function in Equation 10, triplet loss is also widely used in contrastive learning and recommendation system. Let the anchor vector to be pa, the positive vector is p+ and the negative one is p−, the triplet loss is given by:\n\nLα =\n\n(cid:88)\n\nμ∈U\n\n(cid:0)sim(pa, p+) − sim(pa, p−) + α(cid:1)\n\n(11)\n\nIn the single user embedding models, the pa is the single user embedding, and p+, p− are positive and negative vectors. While in our multiple embedding assumption, to train with the triplet loss, we have to modify the form. Similarly to Equation 9, we introduce the maximum operator in the similarity measure. Define the triplet loss for multiple embedding {zj}k\n\nj=1 as:\n\nLα =\n\n(cid:88)\n\nμ∈U\n\n(cid:0)max{sim(zj, p+)}k\n\nj=1 − max{sim(zj, p−)}k\n\nj=1 + α(cid:1) =\n\n(y+ − y− + α)\n\n(12)\n\n(cid:88)\n\nμ∈U\n\nwhere α is a hyperparameter of the positive-negative margin. We let the MIP to train on MovieLens dataset to investigate the impact to the performance of loss function choice. From the observation in Appendix A.4, the MovieLens dataset can be learnt well without two phase training, so we directly train from scratch.\n\nThe results in Table 12 illustrates that the triplet loss marginally performs worse than the NLL loss we used. A possible reason might be the migration from single-anchor to muli-anchor, but the further investigation is left as future work.\n\n17",
    "reference": "# Summary Of The Paper\n\nThis paper investigates the multi-interest for user embeddings in recommender retrievers. The authors consider the different weights of interests as well as time-varying interests and integrate a multi-head attention module and a cluster strategy with their weights. The proposed MIP model is then validated using publicly available datasets.\n\n# Strength And Weaknesses\n\nStrengths\n1. This work focuses on the encoding of user embeddings, and it is an interesting and important in recommender retrievers. \n2. Incorporating time information into the multi-interest task is intriguing because user preferences change over time.\n3. The authors run several experiments on various public datasets to demonstrate the effectiveness of the proposed method.\n\nWeakness\n+ The authors point that this work focuses on the retrieval in recommender systems. The models, however, are optimized using the binary cross entropy loss and tested using the metric AUC, NLL, which are rarely used in retrievers.\n+ In general, the sparse and dense features can be combined with the item embedding to allow the item embeddings to be treated as a whole in models. The authors create various operations and run separate experiments on these two types of features. This means that the proposed method cannot be used in cases where there are both sparse and dense features.\n+ The authors claim that \"propose a multi-interest user representation model that minimizes the bias towards popular categories\". However, \"a higher weight is assigned to an interest cluster if the user engages more with items that belong to it\". How do the authors deal with the popularity bias?\n+ In Eq 9, each user chooses one interest, which is inconsistent with the goal of multi-interest.\n+ The experiments are insufficiently convincing.\n    + The adopted metrics AUC and NLL are more appropriate for ranking models, rather than retrievers.\n    + The authors filter users with less than 100 items and thus there are only a significant limited number of users are observed. Experiments should be conducted on larger datasets. The data statistics after filtering should be more detailed. Furthermore, the authors do not provide the details about the test data.\n    + Several important baseline methods in multi-interest task are missing, such as MIND [Multi-Interest Network with Dynamic Routing for Recommendation at Tmall].\n    + In appendix A.1, the experiments are conducted on a synthetic dataset which is not convincing. \n+ The notions, such as k in 3.1 and [;] in equations, are not well defined.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper should be carefully revised to improve clarify. The contribution is limited to introducing users' multiple interests to sequential retrievers. This work can be somewhat reproducible.\n\n# Summary Of The Review\n\nThis work focuses on an important and interesting problem. However, the general solved task is not clear where the unusual objective function and metrics are adopted. There are some misleading notions and statements which makes it very confusing. The experiments are not convincing where the pre-processing is not appropriate and important baselines are missing.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nNEURAL EPDOS: SPATIALLY ADAPTIVE EQUIVARIANT PARTIAL DIFFERENTIAL OPERATOR BASED NETWORKS\n\nLingshen He1, Yuxuan Chen2∗, Zhengyang Shen3, Yibo Yang2, Zhouchen Lin1,4,5† 1 National Key Lab of General AI, School of Intelligence Science and Technology, Peking University 2 JD Explore Academy, Beijing, China 3 Department of Computer Vision Technology (VIS), Baidu Inc. 4 Institute for Artificial Intelligence, Peking University 5 Peng Cheng Laboratory\n\nABSTRACT\n\nEndowing deep learning models with symmetry priors can lead to a considerable performance improvement. As an interesting bridge between physics and deep learning, the equivariant partial differential operators (PDOs) have drawn much researchers’ attention recently. However, to ensure the PDOs translation equivariance, previous works have to require coefficient matrices to be constant and spatially shared for their linearity, which could lead to the sub-optimal feature learning at each position. In this work, we propose a novel nonlinear PDOs scheme that is both spatially adaptive and translation equivariant. The coefficient matrices are obtained by local features through a generator rather than spatially shared. Besides, we establish a new theory on incorporating more equivariance like rotations for such PDOs. Based on our theoretical results, we efficiently implement the generator with an equivariant multilayer perceptron (EMLP). As such equivariant PDOs are generated by neural networks, we call them Neural ePDOs. In experiments, we show that our method can significantly improve previous works with smaller model size in various datasets. Especially, we achieve the state-ofthe-art performance on the MNIST-rot dataset with only tenth of parameters of the previous best model.\n\n1\n\nINTRODUCTION\n\nIn recent years, convolutional neural networks (CNNs) have achieved superior performance on various vision tasks (Szegedy et al., 2015; He et al., 2016; Chen et al., 2017). It is acknowledged that the success of CNNs is attributed to their ability to exploit the intrinsic translation-invariance symmetry of data to help downstream vision tasks. To incorporate other symmetries like rotation-invariance, various CNNs-based equivariant networks have been studied and carried out to enhance the performance of vision tasks (Cohen & Welling, 2016a;b; Weiler & Cesa, 2019). In another branch, some works (Osher & Rudin, 1990; Perona & Malik, 1990) adopted partial differential operators (PDOs) to process images in the early period. Recently, PDOs with learnable coefficients are adopted by Shen et al. (2020) to design equivariant networks which achieve competitive performance compared to previous equivariant networks. Jenner & Weiler (2021) further generalized this work to a unified framework on the equivariant linear PDOs on Euclidean spaces of various representation types.\n\nActually, the coefficient matrices of the current PDOs works are spatially shared, e.g. the same PDOs are applied to process features at each position (see Figure.1(a)). However, such a coefficient sharing scheme of the PDOs is not the optimal pattern to extract features from input images (Wu et al., 2018; Su et al., 2019; Zhou et al., 2021; He et al., 2021a). To be specific, the contents of the input images vary according to positions, e.g. some pixels cover the background while some express texture, which would make coefficient-sharing PDOs inefficient to extract features at each position.\n\n∗Equal first authorship †Corresponding author\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nIn fact, Jenner & Weiler (2021) have proved that the linear PDOs layer is translation equivariant if and only if its coefficient matrices are spatially shared, so it seems impossible to ensure both the spatial adaptivity and translation equivariance for PDOs.\n\nIn this work, to deal with the above issue, we think outside the box of the linear limitation and propose brand new nonlinear PDOs that are both spatially adaptive and translation equivariant. Compared with spatially shared PDOs, we construct a coefficient generator that inputs local features and outputs the coefficient matrices. Since different positions produce different coefficient matrices, the PDOs are essentially position-specific and can extract individual features according to the local content (see Figure 1(b)). In addition, the coefficient matrices generated by local features guarantee the translation equivariance for such PDOs naturally. However, such a nonlinear PDOs scheme is not intrinsically equivariant to rotations or reflections. To incorporate equivariance of these transformations, we establish a theory on the equivariant formulation of this nonlinear PDOs scheme under any given symmetry group. Specifically, the theory reveals that this type of PDOs is equivariant if and only if the coefficient generators are exactly equivariant maps of particular transformations. In practice, we choose a two-layer EMLP (Finzi et al., 2021) as the coefficient generator to satisfy the equivariance condition and provide an efficient implementation scheme. We name our model Neural ePDOs and evaluate its performance on MNIST-rot and ImageNet datasets. Extensive experiments show that our model can significantly improve accuracy with fewer parameters. Especially, we achieve the state-of-the-art results on MNIST-rot dataset with only a tenth of the parameters compared to previous best models.\n\n(a)\n\n(b)\n\nFigure 1: Illustration of two different designs for PDOs. Here, we use the 2-dimensional vector field to represent the feature map. (a)For linear PDOs, the coefficient matrices are shared to process features across different positions. (b) For nonlinear PDOs we propose in this paper, the coefficient matrices are generated by the local features through neural networks.\n\nWe summarize the main contributions as follows:\n\n• To our knowledge, we are the first one to propose the nonlinear form of PDOs that are both spatially adaptive and translation equivariant. The coefficient matrices of the novel PDOs are adaptive to local features, which could alleviate the sub-optimal feature learning problem at each position.\n\n• We develop a theory for such nonlinear PDOs that precisely characterize when it is equivariant under any given symmetry group. The theory reveals that the nonlinear PDOs are equivariant if and only if the coefficient generators are exactly equivariant maps of particular transformations.\n\n• We provide an efficient implementation which adopts a two-layer EMLP as the coefficient\n\ngenerator and could largely save parameters and computations.\n\n• Extensive experiments show that our method can significantly improve the results on MNIST-rot and ImageNet datasets with significantly fewer parameters. Especially, we achieve state-of-the-art results on the MNIST-rot dataset.\n\n2 RELATED WORKS\n\nSo far, there are two mainstream approaches to constructing group equivariant networks. One is first developed by Cohen & Welling (2016a) which views the feature maps as maps defined on a\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ngroup, and they proposed group convolution operation to process these feature maps equivariantly for image recognition. The method is further applied to designing equivariant networks for 3D space (Worrall & Brostow, 2018), sphere (Cohen et al., 2018), video tracking (Gupta et al., 2021) and lie groups (Finzi et al., 2020a; Bekkers, 2019), etc. This approach is further developed to design attentive convolution layer (Romero et al., 2020) and self-attention layer (Romero & Cordonnier, 2020; Hutchinson et al., 2021; He et al., 2021b). The other one follows the approach of steerable CNNs (Cohen & Welling, 2016b; Weiler & Cesa, 2019; Weiler et al., 2018; Jenner & Weiler, 2021), which is a generalization of the first approach. Analogous to physics, the feature map here is viewed as a field, which is transformed according to a specified group representation under the act of transformation. In comparison, the feature map in the first approach is simply the field with regular representation. Works (Cohen & Welling, 2016b; Weiler et al., 2018; Weiler & Cesa, 2019) are devoted to finding out all the equivariant convolution operations as a map between any two fields. Later works further generalize the approach to design equivariant transformer (Fuchs et al., 2020) and graph network (Brandstetter et al., 2021). Our work follows these approaches. Recently, some works focus on utilizing PDOs to design equivariant neural networks, as they can build an interesting bridge between physics and deep learning (Jenner & Weiler, 2021). In addition, PDOs are very suitable for processing continuous data (Finzi et al., 2020b) and non-Euclidean structure data. The work most closely related to ours is Jenner & Weiler (2021) which derives steerable PDOs as a linear map between any two fields in the language of group representation theory. It is an extension of PDO-eConv (Shen et al., 2020) which employs rotated PDOs to design linear equivariant layers similar to the approach of Cohen & Welling (2016a). In our work, to alleviate the spatial-agnostic problem in linear PDO-based equivariant layers, we propose a nonlinear PDO scheme and develop an equivariant theory that generalizes the Jenner & Weiler (2021). A more detailed comparison between our work and steerable PDOs can be found in supplementary material.\n\n3 PRELIMINARY\n\n3.1 EQUIVARIANCE\n\nEquivariance measures how the output of a network layer transforms in a predictable way with respect to the transformation of the input. In mathematics, a map Ψ is group equivariant if it satisfies: ∀h ∈ H, Ψ [π(h)[f ]] = π′(h)[Ψ[f ]],\n\n(1)\n\nwhere H is a transformation group, π(h) and π′(h) are group actions, and f is the input. In CNNs, f is the feature map which can be seen as a vector-valued function f : R2 → Rn, where Rn is the n-dimensional vector space. If we choose H to be the translation group, it is easy to prove that the convolution layer satisfies this requirement. In the following, we mainly consider feature maps defined on R2, and the conclusions can be readily extended to feature maps defined on any dimension.\n\nFollowing the standard practice of equivariant deep learning, the feature map f is modeled as a vector field composed of fiber f (x) located at every point x ∈ R2. For transformation group H, we mainly consider affine group of the form H = (R2, +) ⋊ G, for some G ≤ GL(2, R). Here, H is constructed by the semi-direct product between translation group (R2, +) and a linear invertible transformation group G performed on R2, e.g. rotations and mirrorings. The group action π(h) acts on field f as:\n\n∀x ∈ R2,\n\nπ(h)f (x) = ρ(g)f (g−1(x − t)),\n\n(2)\n\nwhere t ∈ R2 is a translation, g ∈ G is a linear transformation, h := (t, g) ∈ H, and ρ(g) is a group representation of g. Formally, a group representation ρ of the group G is a group homomorphism: G → Rn×n, i.e., ∀g1, g2 ∈ G, ρ(g1g2) = ρ(g1)ρ(g2). It describes how each fiber transforms under the group action. When ρ is given, its corresponding feature map is called a ρ-field. See supplementary material for more introduction to group representations.\n\n3.2 PARTIAL DIFFERENTIAL OPERATORS\n\nPartial differential operators (PDOs) are commonly used in physics areas, such as gradient, curl or Laplacian. They can be seen as a kind of maps between smooth functions. Given a smooth cin-\n\n3\n\nPublished as a conference paper at ICLR 2023\n\ndimensional feature map f = (f1, ..., fcin )T , the PDO ∂x acts on f as ∂xf := (∂xf1, ..., ∂xfcin )T . In general, the PDOs can be formalized as the linear combination of various orders of elementary PDO which is denoted as ∂α := ∂α1 0. Here, we adopt the x1 multi-index notation on elementary PDOs as Jenner & Weiler (2021). For simplicity, we utilize ΓN = {(i, j)|i, j ∈ N0, 0 ≤ i + j ≤ N } to index elementary PDOs with their order less than N as we have to set a truncation order to implement PDOs in the computer. For example, the PDOs from C∞(R2, Rcin ) to C∞(R2, Rcout ) with truncation order N = 3 can be formalized as\n\nx2 , α = (α1, α2) ∈ N2 ∂α2\n\nˆD(3)f :=W(0,0)f + W(1,0)∂(1,0)f + W(0,1)∂(0,1)f + W(2,0)∂(2,0)f + W(1,1)∂(1,1)f\n\n+ W(0,2)∂(0,2)f + W(3,0)∂(3,0)f + W(2,1)∂(2,1)f + W(1,2)∂(1,2)f + W(0,3)∂(0,3)f ,\n\n(3)\n\nwhere W(i,j) : R2 → Rcout×cin is the coefficient matrix corresponding to ∂(i,j).\n\nTo study the equivariance of PDOs, we give a description of the transformation property of elementary PDOs. Here, we assume the input f to be scalar field and take ∂(2,0) as an example. When input of the PDO go through a affine transformation (g, t), resulting in ̃f (x) := f (g−1(x − t)). According to the chain rule, we get:\n\n[∂(2,0) ̃f ](x) = (cid:0)g−1 11 and g−1\n\n11\n\nwhere g−1 PDO:\n\n(cid:1)2\n\n[∂(2,0)f ]( ̃x) + 2g−1\n\n11 g−1\n\n21 [∂(1,1)f ]( ̃x) + (cid:0)g−1\n\n21\n\n(cid:1)2\n\n[∂(0,2)f ]( ̃x),\n\n(4)\n\n21 are matrix elements of g−1 and ̃x := g−1(x−t). In general, for each elementary\n\n∀α ∈ ΓN , x ∈ R2, g ∈ G,\n\n∂α ̃f (x) =\n\n(cid:88)\n\nβ∈ΓN\n\nˆρα,β(g)∂βf ( ̃x),\n\n(5)\n\nwhere ˆρα,β(g) denotes the transformation coefficient in front of elementary PDO ∂β on the right side of the above equation of a given α. All these transformation coefficients of a given group element g constitute a matrix ˆρ(g). We have the following result: Lemma 1 ˆρ(g) defined in Eq.(5) is a group representation of g on R|ΓN |.\n\nProof of the lemma can be found in supplementary material. We also give a procedure in the supplementary material to automatically compute ˆρ(g) for any given N .\n\n4 THE NEURAL EPDOS FRAMEWORK\n\nIn this section, we first propose the new scheme of PDOs, in which the coefficient matrices are generated by features. Then, we propose the general theory that gives a necessary and sufficient condition to ensure equivariance for the operator for any given symmetry. As coefficient generators will induce heavy parameters and computation costs, we propose to require coefficient matrices to be diagonal and characterize the equivariant space for it.\n\n4.1 A NONLINEAR PDOS SCHEME\n\nAs introduced in Section 3.2, PDOs as maps from C∞(R2, Rcin ) to C∞(R2, Rcout) are formulated as :\n\n∀x ∈ R2, Ψ[f ](x) =\n\n(cid:88)\n\nWα(x)∂α[f ](x),\n\n(6)\n\nα∈ΓN\n\nwhere f ∈ C∞(R2, Rcin ), Wα : R2 → Rcout×cin. Jenner & Weiler has proved that the necessary and sufficient condition for Eq.(6) to be translation equivariant is to require the coefficients Wα to be spatially shared, that is, ∀x, x′ ∈ R2, α ∈ ΓN , Wα(x) = Wα(x′). However, it is not efficient for the spatially shared PDOs to learn diverse patterns in the feature map, which may lead to the redundancy of learnable parameters.\n\nTo alleviate this problem, we propose a nonlinear PDOs scheme that adjusts PDOs according to features at different positions. Furthermore, our newly proposed module still keeps translation equivariance as the spatially shared PDOs. Specifically, we adopt Wα as the coefficient generators to generate coefficient matrices from local input features, which can be formulated as:\n\n∀x ∈ R2, Ψ[f ](x) =\n\n(cid:88)\n\nα∈ΓN\n\n4\n\nWα(f (x))∂α[f ](x),\n\n(7)\n\nPublished as a conference paper at ICLR 2023\n\nwhere Wα : Rcin → Rcout×cin , α ∈ ΓN are coefficient generators with local features as input. It is easy to check the translation equivariance of Eq.(7). We could adopt MLP as the structure of the coefficient generators as they are the universal approximator of any continuous function. Then, the neural network and local input features decide the specific PDOs applied at each position.\n\n4.2 EQUIVARIANCE THEORY\n\nAlthough the nonlinear PDOs formulated in Eq.(7) are equivariant to translation, they are not intrinsically equivariant to common transformations such as rotation or reflection. We now derive a complete characterization of their equivariant space of such symmetry.\n\nThe equivariant requirement on the operators (7), in the sense defined by Eq.(1), can be reduced to the requirement on the coefficient generators Wα. Supposing the input and output of the operator are any ρ-field and ρ′-field, respectively, we have, Proposition 1 The nonlinear PDOs in Eq.(7) are equivariant to affine transformation H if and only if the coefficient generators satisfy the following constraint:\n\n∀α ∈ ΓN , ∀g ∈ G, ∀y ∈ Rcin ,\n\n(cid:88)\n\nβ∈ΓN\n\nˆρβ,α(g)Wβ(ρ(g)y)ρ(g) = ρ′(g)Wα(y).\n\n(8)\n\nThe proof of this proposition can be found in supplementary material. The proof makes use of the fact that elementary PDOs are independent of each other. It is remarkable that the above equation constraints for coefficient generators are imposed for each α ∈ ΓN . To uncover the intrinsic structure of Wα, we further concatenate all the coefficient generators side by side as a whole W such that ∀y ∈ Rcin, W(y) = [W(0,0)(y), ..., W(0,N )(y)]. Then, the constraint Eq.(8) reduces to the following form: Proposition 2 Eq.(8) is equivalent to the following form:\n\n∀g ∈ G, ∀y ∈ Rcin , W(ρ(g)y)( ˆρ(g) ⊗ ρ(g)) = ρ′(g)W(y).\n\n(9)\n\nHere, ⊗ is the tensor product of two group representations.\n\nWe show the proof at supplementary material. Applying the vec-operator1 on Eq.(9), it can be rewritten as:\n\n∀g ∈ G, ∀y ∈ Rcin ,\n\nvec[W(ρ(g)y)] = (ρ′(g) ⊗ ˆρ(g−1)⊤ ⊗ ρ(g−1)⊤)vec[W(y)],\n\n(10)\n\nwhere vec[·] operator flattens the matrix into a vector by concatenating the rows of a matrix one by one. Eq.(10) reveals that vec[W] is an equivariant function with input and output vector transform according to ρ(g) and ρ′(g) ⊗ ˆρ(g−1)⊤ ⊗ ρ(g−1)⊤, respectively. It is easy to check that ρ′(g) ⊗ ˆρ(g−1)⊤ ⊗ ρ(g−1)⊤ is also a representation of G. As we adopt coefficient generators as MLP, vec[W] is an EMLP and can be efficiently constructed by Finzi et al. (2021).\n\nAs coefficient matrices in such operators are generated via MLP, it brings much extra computational burden compared to steerable PDOs. To alleviate the problem, we propose a novel structure for coefficient generators and give its equivariant characterization in the following.\n\n4.3 EFFICIENT COEFFICIENT GENERATORS\n\nIn practice, regular representation and quotient representation (see supplementary material) are mostly adopted for equivariant networks (Weiler & Cesa, 2019) due to their superior performance. Therefore, we attempt to propose an efficient coefficient generator for these representation types in this subsection. In Eq.(7), directly generating the whole coefficient matrix W would make it suffer heavy parameter and computation costs. To alleviate this issue, we assume that the coefficient matrices in Eq.(7) are diagonal matrices. Then we can formulate the operator (7) as:\n\n∀x ∈ R2, Ψ[f ](x) =\n\n(cid:88)\n\nα∈ΓN\n\nwα(f (x)) ◦ ∂α[f ](x),\n\n(11)\n\n1For matrices A, X, B, we have vec(AXB) = (A ⊗ B⊤)vec(X).\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nwhere wα : Rcin → Rcin and ◦ is used to denote element-wise product between two vectors. Here, we have assumed output and input to be of the same field type and, if necessary, we can follow it with a linear projection to transform it into another field. Such design greatly reduces the computational burden for generating coefficients and also works well in the experiment in Section 7.\n\nAccording to Proposition 1, the above operator (11) is equivariant if and only if:\n\n∀α ∈ ΓN , ∀g ∈ G, ∀y ∈ Rcin,\n\n(cid:88)\n\nβ∈ΓN\n\nˆρβ,α(g)diag[wβ(ρ(g)y)]ρ(g) = ρ(g)diag[wα(y)].\n\n(12)\n\nWe use diag[·] to denote converting an n-dimensional vector to an n-dimensional diagonal matrix with the vector as diagonal. Because of the diagonal operation, directly using results in Proposition 2 cannot uncover the structure of wα satisfying the above constraint. By utilizing the special structure of regular representation and quotient representation, we have the following result.\n\nProposition 3 Suppose the input and output of operator (11) are both ρ(g)-field. regular or quotient representation of G, the constraint in Eq.(12) is equivalent to:\n\nIf the ρ is a\n\n∀g ∈ G, ∀y ∈ Rcin,\n\n ̄w(ρ(g)y) = (ρ(g) ⊗ ˆρ(g−1)⊤) ̄w(y),\n\n(13)\n\nwhere ∀y ∈ Rcin , ̄w(y) = vec([w(0,0)(y), ..., w(0,N )(y)]) is a large vector concatenated from all generated vectors (see supplementary material for proof). Similar to the above general case, ̄w is an equivariant function with input and output vector transform according to ρ(g) and (ρ(g) ⊗ ˆρ(g−1)⊤), respectively. So far, we have fully characterized the structure of the coefficient generators which ensures operator (11) is equivariant. As the coefficient generator is based on an equivariant neural network, we name our model Neural ePDOs. In the next section, we will show a detailed implementation that is both parameters efficient and computationally efficient.\n\n5\n\nIMPLEMENTATION OF NEURAL EPDOS\n\n5.1 DESIGN OF COEFFICIENT GENERATOR\n\nAs is shown in Proposition 3, coefficient generator ̄w can be viewed as an equivariant vector-valued function. In practice, we implement it as an EMLP which can be constructed as Finzi et al. (2021). In this paper, we adopt a two-layer EMLP as ̄w. To reduce parameter and computation costs, we choose a bottleneck design here, a relatively small size of the hidden layer. Specifically, ̄w(x) = W2ReLu(W1x), where W1 ∈ Rcmid×cin, W2 ∈ R|ΓN |cin×cmid , cmid = cin r , where r is the reduction ratio, and ReLu(·) is a element-wise ReLu activation function (Nair & Hinton, 2010). Here, we assume ρ = pρ0, in other words, representation ρ can be decomposed into multiple identical representations ρ0, which is very common in practice. Then the output representation of ̄w(x) = W2ReLu(W1x) can be decomposed in a similar way, i.e., (ρ ⊗ ( ˆρ−1)⊤) = p(ρ0 ⊗ ( ˆρ−1)⊤). We reduce both computations and parameters by requiring the outputs of the p partitions to be the same.\n\n5.2 DISCRETIZATION OF PDOS\n\nOur theory for Neural ePDOs is developed in the continuous space. In practice, in order to process digital images which are defined on two-dimensional grids, we need to discretize our PDOs. In our paper, we mainly consider the finite difference (FD) method and the Gaussian derivatives method (GA) (Jenner & Weiler, 2021). In principle, we only need to consider the discretization of the elementary PDO because the discretization of the whole PDOs can be obtained by a linear combination of them. FD: Finite difference method is widely used in numerical analysis to approximate PDO by a linear combination of function values on finite grids. For example, ∂xf = (f (x + 1) − f (x − 1))/2. On the regular grids, a PDO can be approximated by a convolution operation (Shen et al., 2020), i.e.,\n\n∂α[f ] ≈ uα ∗ F,\n\n(14)\n\nwhere ui is a convolution filter. Corresponding filters for elementary PDOs in P are provided in the supplementary material.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nGA: PDO can also be estimated by taking derivatives of Gaussian function (Jenner & Weiler, 2021), i.e., given grid points xn ∈ R2, ∀α ∈ ΓN ,\n\n∂α[f ] ≈ ∂α[G(xn; σ)]f (xn),\n\n(15)\n\nwhere G(x; σ) is a Gaussian kernel with standard deviation σ around 0.\n\n6 COMPLEXITY ANALYSIS\n\nIn this section, we give a complexity analysis for both steerable PDOs and Neural ePDOs. For simplicity, we assume the feature field to be a regular field (Results for other feature fields are similar). We especially consider the complexity for Neural ePDOs with full coefficient matrices (denoted as full) and diagonal coefficient matrices (denoted as diag), respectively. Here, we assume the EMLP of Neural ePDOs (full) to have the same design as Neural ePDOs (diag). Suppose the representations type of the input feature map and output feature map to be cρreg. The width and height of the feature map are h and w, respectively. n is the number of elements in the group G and k is the discretization kernel size. Both parameters and flops complexity are listed in Table (1).\n\nWe first make a comparison of parameters. For both the Neural ePDOs (full) and Neural ePDOs (diag), the first term is the number of parameters in the first layer of EMLP (coefficient generator) and the second term is for the second layer. It is obvious that Neural ePDOs (full) have significantly more parameters than Neural ePDOs (diag) (O(c3) vs O(c2)). For comparison of parameters between steerable PDOs and Neural ePDOs (diag), both the first term and second term in Neural ePDOs (diag) are much less than the parameters of steerable PDOs (In our paper, we set N = 4), hence Neural ePDOs (diag) is much parameter efficient than steerable PDOs.\n\nFor flops, the first two terms in the Neural ePDOs (both full and diag) are used for coefficient generation and the last term is used for the action of PDOs. As the last term in Neural ePDOs (full) is equal to steerable PDOs, the latter one surely requires less computational burden. It is also easy to check all three terms in the Neural ePDOs (diag) are much less than flops of steerable PDOs, hence Neural ePDOs (diag) is much more computationally efficient than steerable PDOs.\n\nFrom the comparison above, both the diagonal restriction and EMLP design (bottleneck structure (r) and partition (p) operation) help to make our Neural ePDOs more efficient than steerable PDOs.\n\nTable 1: Complexity Analysis of each PDO layer.\n\nMethod Steerable PDO Neural ePDOs (full) Neural ePDOs (diag)\n\nParameters c2(N + 1)(N + 2)n/2 c2n/r + c3(N + 1)(N + 2)n/2rp c2n/r + c2(N + 1)(N + 2)n/2rp\n\nFlops c2n2k2hw (c2n2/r + c3n3k2/rp)hw + c2n2k2hw (c2n2/r + c2n2k2/rp)hw + cnk2hw\n\n7 EXPERIMENTS\n\n7.1 MNIST-ROT\n\nWe first test our model on MNIST-rot dataset (Larochelle et al., 2007), which is a standard benchmark to test the equivariant models. The dataset contains 62k 28 × 28 randomly rotated gray-scale handwritten digits. Images in the dataset are split into 12k for training and 50k for testing.\n\nAs the images in the MNIST-rot dataset are orientation-unknown, we choose the group as C16 for our model. Following the architecture in the Jenner & Weiler (2021) that consists of 6 steerable PDOs layers followed by two fully connected layers, we construct our model by replacing the last 5 steerable PDOs layers with our Neural ePDOs layers. More details about the model, training and hyperparameters analysis can be found in the supplementary material.\n\nOur results can be found in Table 2. Some of our models use the regular field as intermediate feature fields and others use quotient representations (which are denoted by quotient in Table 2). Under the setting of using both finite difference and Gaussian derivatives, our model achieves significant\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Results in MNIST-rot. The test error with standard deviations are averaged over 5 runs.\n\nMethod Vanilla CNN Steerable PDOs Neural ePDOs E2CNN(quotient) Steerable PDOs (quotient) Neural ePDOs (quotient) E2CNN Steerable PDOs Neural ePDOs E2CNN (D16|5C16) Steerable PDOs (D16|5C16) Neural ePDOs (D16|5C16)\n\nDiscretization Test error (%)\n\n– FD FD –\nGauss Gauss –\nGauss Gauss –\nGauss Gauss\n\n1.96±0.06 1.54±0.32 0.77±0.05 0.70±0.03 0.74±0.04 0.70±0.06 0.72±0.03 0.75±0.02 0.65±0.04 0.68±0.02 0.78±0.05 0.59±0.03\n\nParams 1.1M 941K 234K 2.75M 951K 229K 2.69M 941K 234K 5.36M 947K 525K\n\nTable 3: Results for ImageNet100. DA is short for data augmentation. The test error with standard deviations are averaged over 5 runs.\n\nMethod ResNet26 Res26 Steerable PDOs Res26 Steerable PDOs Res26 Neural ePDOs Res26 Neural ePDOs ResNet26 Res26 Steerable PDOs Res26 Steerable PDOs Res26 Neural ePDOs Res26 Neural ePDOs\n\nDiscretization DA Test error (%) 23.50± 0.46 ×\n19.92± 0.24 ×\n18.64± 0.29 ×\n17.76± 0.26 ×\n17.44± 0.25 ×\n✓ 15.11± 0.25 ✓\n13.94± 0.15 ✓\n12.54± 0.18 ✓\n12.08± 0.13 ✓\n11.84± 0.14\n\n– FD Gauss FD Gauss –\nFD Gauss FD Gauss\n\nParams 13.12M 14.4M 14.4M 8.24M 8.24M 13.12M 14.4M 14.4M 8.24M 8.24M\n\nimprovement over the steerable PDOs based model with only 1/4 parameters. It is noteworthy that the Gaussian derivatives method tends to achieve a better performance than the finite difference method for both steerable PDOs and our model, which is consistent to experiment results in Jenner & Weiler (2021). However, the performance of our quotient field model is inferior to the regular field, which is different from the results of steerable PDOs in Jenner & Weiler (2021). As is shown in Weiler & Cesa (2019), quotient fields can help to reduce redundancy in the regular fields in some cases. In our models, the redundancy is already less than steerable PDOs, hence, applying quotient fields in our model may hurt performance. To further demonstrate the potential capacity of our model, we enlarge the model size and employ D16 regular field in the first five layers and restrict it to C16 regular field in the last layer (results are denoted as D16|5C16 in Table 2). Our model improves the current SOTA model (Weiler & Cesa, 2019) while consuming only 9.8% of parameters.\n\n7.2 EVALUATION ON NATURAL IMAGES\n\nIn general, the objects in real-world images are not always in a uniform orientation. So we believe that the models with rotation symmetry can generalize better on real-world images. ImageNet (Deng et al., 2009) is a large-scale dataset that consists of 1000 classes with roughly 1000 images per class, which is a common benchmark for image recognition. It contains 1.2 million training images and 50k validation images. Following Hou et al. (2019), we consider two experimental settings which correspond to different data scales. In the first setting, we conduct the experiments on a subset of ImageNet which randomly select 100 classes, and denoted it as ImageNet100. In the other one, we evaluate our model on the whole 1000 classes. We choose ResNet-26 as the baseline model and construct equivariant PDO-based networks that are equivariant to C8 group by taking place the convolution layer with C8 equivariant module. We adopt regular representation for all the equivariant modules throughout the model. To make classification results rotation-invariant to the inputs, we add an orientation pooling (Cohen & Welling, 2016a)\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Results for ImageNet1k. The test error with standard deviations are averaged over 5 runs.\n\nMethod ResNet26 Res26 Steerable PDOs Res26 Steerable PDOs Res26 Neural ePDOs Res26 Neural ePDOs\n\nDiscretization Test error (%)\n\n– FD Gauss FD Gauss\n\n26.4± 0.52 25.39± 0.35 23.55± 0.31 24.02±0.27 23.15± 0.33\n\nParams 13.7M 15.02M 15.02M 8.82M 8.82M\n\nbefore global average pooling. Notice that the 1 × 1 convolutions in ResNet26 are simply replaced with equivariant linear projection layers rather than the PDOs layer. Other convolution layers are replaced by steerable PDOs or Neural ePDOs which we denote as Res26 Steerable PDOs or Res26 Neural ePDOs. The output dimensions of each layer for the two models are scaled with the same factor to make its learnable parameters in Res26 Steerable PDOs comparable with the baseline. More detailed training settings can be found in the supplementary material.\n\nResults on ImageNet100 and ImageNet1k are shown in the Table 3 and Table 4 respectively. In all the settings, our models significantly improve Steerable PDOs based models with fewer parameters (8.2M vs 14.4M) and computational costs (flops=24.1G vs 56.6G). For ImageNet100, training with/without data augmentation is adopted. It is observed that equivariance can help improve the performance of the model and data augmentation can further enhance the performance of the equivariant model, which may be attributed to the approximate equivariance of the equivariant model incurred by discretization. In addition, we observe that performance improvement of Neural ePDOs over Steerable PDOs under no data augmentation setting is more significant. We conjecture our Neural ePDOs can help our model easier adapt to unseen patterns and hence tend to be more data efficient. It is also noticeable that the Gaussian discretization method still outperforms the finite difference method on the natural images.\n\n8 CONCLUSION AND FUTURE WORKS\n\nIn this work, we propose a new nonlinear PDOs scheme that is both spatially adaptive and translation equivariance. A new equivariant theory is developed for our nonlinear PDOs scheme which gives a general equivariant formulation of it under any given symmetry group. The theory systematically characterizes the space of coefficient generators of our equivariant nonlinear PDOs for any given equivariance. Based on this theory, we efficiently implement it by adopting two-layer EMLP as the coefficient generators and, hence, name our model Neural ePDOs. Extensive experiments demonstrate our Neural ePDOs can significantly improve performance on MNIST-rot and ImageNet datasets. Particularly, we achieve new SOTA performance on MNIST-rot dataset with only 9.8% parameters compared to the previous best model. To reduce the parameters and computational costs of generating coefficients, we have proposed efficient coefficient generators which are designed for features of regular field and quotient field. Efficient coefficient generators for other representation fields, e.g., irreducible representation field, could be further explored. It is worth emphasizing that the nonlinearity introduced in our adaptive PDOs has greatly improved their performance compared to linear PDOs even with smaller model sizes. There is still a large space to explore how to introduce nonlinearity into PDOs more efficiently. Although we mainly focus on the two-dimensional plane, our framework can be readily extended to other homogeneous spaces such as spheres and 3D spaces. As our Neural ePDOs can achieve significant improvement on the two-dimensional image tasks, it is worth believing applications of Neural ePDOs in these domains are promising.\n\nACKNOWLEDGMENT\n\nZ. Lin was supported by National Key RD Program of China (2022ZD0160302), the major key project of PCL, China (No. PCL2021A12), the NSF China (No. 62276004), Qualcomm, and Project 2020BD006 supported by PKU-Baidu Fund.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nReproducibility Statement\n\nThe complete proof of the theorems is provided in supplementary material, and all the experimental details are provided in Section 7 and supplementary material.\n\nEthics Statement\n\nThe research in this paper does NOT involve any human subject, and our dataset is not related to any issue of privacy and can be used publicly. All authors of this paper follow the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics).\n\nREFERENCES\n\nErik J Bekkers. B-spline CNNs on lie groups. arXiv preprint arXiv:1909.12057, 2019.\n\nJohannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. GeIn International\n\nometric and physical quantities improve E (3) equivariant message passing. Conference on Learning Representations, 2021.\n\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE transactions on pattern analysis and machine intelligence, 40(4): 834–848, 2017.\n\nTaco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-\n\nence on machine learning, pp. 2990–2999. PMLR, 2016a.\n\nTaco S Cohen and Max Welling. Steerable CNNs. arXiv preprint arXiv:1612.08498, 2016b.\n\nTaco S Cohen, Mario Geiger, Jonas K ̈ohler, and Max Welling. Spherical CNNs. arXiv preprint\n\narXiv:1801.10130, 2018.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nMarc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In International Conference on Machine Learning, pp. 3165–3176. PMLR, 2020a.\n\nMarc Finzi, Max Welling, and Andrew Gordon Wilson. A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups. In International Conference on Machine Learning, pp. 3318–3328. PMLR, 2021.\n\nMarc Anton Finzi, Roberto Bondesan, and Max Welling. Probabilistic numeric convolutional neural\n\nnetworks. In International Conference on Learning Representations, 2020b.\n\nFabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE (3)-transformers: 3D rototranslation equivariant attention networks. Advances in Neural Information Processing Systems, 33:1970–1981, 2020.\n\nDeepak K Gupta, Devanshu Arya, and Efstratios Gavves. Rotation equivariant siamese networks for tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12362–12371, 2021.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nLingshen He, Yuxuan Chen, Yiming Dong, Yisen Wang, Zhouchen Lin, et al. Efficient equivariant\n\nnetwork. Advances in Neural Information Processing Systems, 34:5290–5302, 2021a.\n\nLingshen He, Yiming Dong, Yisen Wang, Dacheng Tao, and Zhouchen Lin. Gauge equivariant\n\ntransformer. Advances in Neural Information Processing Systems, 34:27331–27343, 2021b.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nSaihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 831–839, 2019.\n\nMichael J Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh, and Hyunjik Kim. Lietransformer: Equivariant self-attention for lie groups. In International Conference on Machine Learning, pp. 4533–4543. PMLR, 2021.\n\nErik Jenner and Maurice Weiler. Steerable partial differential operators for equivariant neural net-\n\nworks. arXiv preprint arXiv:2106.10163, 2021.\n\nHugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pp. 473–480, 2007.\n\nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.\n\nIn Icml, 2010.\n\nStanley Osher and Leonid I Rudin. Feature-oriented image enhancement using shock filters. SIAM\n\nJournal on numerical analysis, 27(4):919–940, 1990.\n\nPietro Perona and Jitendra Malik. Scale-space and edge detection using anisotropic diffusion. IEEE\n\nTransactions on pattern analysis and machine intelligence, 12(7):629–639, 1990.\n\nDavid Romero, Erik Bekkers, Jakub Tomczak, and Mark Hoogendoorn. Attentive group equivariant convolutional networks. In International Conference on Machine Learning, pp. 8188–8199. PMLR, 2020.\n\nDavid W Romero and Jean-Baptiste Cordonnier. Group equivariant stand-alone self-attention for\n\nvision. In International Conference on Learning Representations, 2020.\n\nZhengyang Shen, Lingshen He, Zhouchen Lin, and Jinwen Ma. PDO-eConvs: Partial differential operator based equivariant convolutions. In International Conference on Machine Learning, pp. 8697–8706. PMLR, 2020.\n\nHang Su, Varun Jampani, Deqing Sun, Orazio Gallo, Erik Learned-Miller, and Jan Kautz. Pixeladaptive convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11166–11175, 2019.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015.\n\nMaurice Weiler and Gabriele Cesa. General E (2)-equivariant steerable CNNs. Advances in Neural\n\nInformation Processing Systems, 32, 2019.\n\nMaurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3D steerable CNNs: Learning rotationally equivariant features in volumetric data. Advances in Neural Information Processing Systems, 31, 2018.\n\nDaniel Worrall and Gabriel Brostow. Cubenet: Equivariance to 3d rotation and translation. Proceedings of the European Conference on Computer Vision (ECCV), pp. 567–584, 2018.\n\nIn\n\nJialin Wu, Dai Li, Yu Yang, Chandrajit Bajaj, and Xiangyang Ji. Dynamic filtering with large In Proceedings of the European Conference on Computer Vision\n\nsampling field for convnets. (ECCV), pp. 185–200, 2018.\n\nJingkai Zhou, Varun Jampani, Zhixiong Pi, Qiong Liu, and Ming-Hsuan Yang. Decoupled dynamic filter networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6647–6656, 2021.\n\n11",
    "reference": "# Summary Of The Paper\n\nThe paper describes an equivariant implementation of non-linear partial differential operator (PDO) based networks. A key contribution of the paper is in the non-linear adaption of equivariant PDOs (ePDOs), which makes them more parameter efficient and expressive. The idea is intuitive and sensible, based on the idea that the operators should be locally adapted to local patterns. This is achieved by letting a local feature vector determine the PDO coefficients locally. This coefficient predictor then has to be equivariant as well, and this is achieved by equivariant MLPs. The paper then provides a clear recipe/theory for the conditions under which the PDO is indeed equivariant, which guides the construction of an efficient implementation. The proposed method is extensively validated on rotated MNIST as well as on ImageNet, and confirm the added benefit of working with non-linear ePDOs as opposed to linear ePDOs.\n\n# Strength And Weaknesses\n\n**Strengths**\n\n1. The paper implements an intuitive and appealing idea: making the main layers of NN non-linear and adaptive to the present features locally. In some sense this also happens in transformers via attention, or via deformable convolutions [Dai] and possibly other works, however, what makes this approach interesting is that is efficient in that the operations (PDOs) are strictly local. Their non-linear adaption boilos down to a point-wise (equivariant) MLP.\n2. The proposed work is timely, and builds up recent advances in PDO-based deep learning.\n3. The experiments are well done and clearly underpin the benefit of non-linear ePDOs vs linear ePDOs and normal resnets.\n4. The paper is theoreticall sound.\n5. The paper is overall well written and precise, but could occasionally benefit form added details and intuition.\n\nDai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., & Wei, Y. (2017). Deformable convolutional networks. In Proceedings of the IEEE international conference on computer vision (pp. 764-773).\n\n**Weaknesses**\n\n1. Upon first reading, it is hard to distill the differences between the ePDO of Jenner and Weiler, from the contributions in this paper. It is clear though that it is the modification of the PDO coefficient being dependent on the input (which is an important contribution in itself!), however, the kernel constraint is very hard to digest. It would have been nice to put this in perspective relative to the original constraint posed in the paper by Jenner and Weiler.\n2. The paper is a bit untransparent when it comes to the \"steerabble\" aspect of things. Parts are clear, vector that collects derivatives can in principle be steered by some representation of G, like in Eq.5, but it is unclear what the types of the input and output feature fields are, and how to set them. It may help a lot of some intuition is provided on how to pick those reps, also in relation to the kernel constraint. Overall, I think an intuitive breakdown of Eq 8 would be helpful.\n3. Some minor details are missing: \n    1. to which sub-group is the quotient representation defined?\n    2. The method is overall more efficient (in memory and flops), could it be made more precise where this effiency comes from? The operations themselves are more expensive than the original ePDOs, right? Is it mainly due to working with less independent feature channels (e.g. choosing a lower p in the non-linear case compared to the linear ePDO setting)?\n    3. In proposition 3, why is it important to consider regular or quotient representations (which are also regular), and not irreducible representations? I.e., why not formulate it for any representation of G?\n    4. Might this have to do with sec 5.1 where the equivariant MLPs use ReLUs. If working with irreps, not all activation functions may be allowed (though this seems to have nothing to do with proposition 3 otherwise).\n    5. What sigma did you pick for the Gaussian derivatives?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nAs mentioned above the paper could benefit from additional details and intuitive explanations, though overall the paper is sound.\n\nThe work is of high quality and relies on state-of-the-art techniques, the experiments are appropriate.\n\nThe work is novel.\n\nIn regards to reproducibility, all theoretical details as well as an appendix with details are provided, but releasing code with this submission would greatly improve reproducibility (translating the work to code may be a challenge!)\n\n# Summary Of The Review\n\nThe paper is a great submission to ICLR; it is novel and sound, but could still benefit from additional details and intuitive explanations. I judge these improvement to be possible within the rebuttal period/before the cam-ready as they can mostly be fixed with textual improvements. I therefore recommend accept.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nNo concerns"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTRADING INFORMATION BETWEEN LATENTS IN HIERARCHICAL VARIATIONAL AUTOENCODERS\n\nTim Z. Xiao University of T ̈ubingen & IMPRS-IS zhenzhong.xiao@uni-tuebingen.de\n\nRobert Bamler University of T ̈ubingen robert.bamler@uni-tuebingen.de\n\nABSTRACT\n\nVariational Autoencoders (VAEs) were originally motivated (Kingma & Welling, 2014) as probabilistic generative models in which one performs approximate Bayesian inference. The proposal of β-VAEs (Higgins et al., 2017) breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (“bit rate”) of the latent representation and the distortion of reconstructed data (Alemi et al., 2018). In this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We identify a general class of inference models for which one can split the rate into contributions from each layer, which can then be tuned independently. We derive theoretical bounds on the performance of downstream tasks as functions of the individual layers’ rates and verify our theoretical findings in large-scale experiments. Our results provide guidance for practitioners on which region in rate-space to target for a given application.\n\n1\n\nINTRODUCTION\n\nVariational autoencoders (VAEs) (Kingma & Welling, 2014; Rezende et al., 2014) are a class of deep generative models that are used, e.g., for density modeling (Takahashi et al., 2018), clustering (Jiang et al., 2017), nonlinear dimensionality reduction of scientific measurements (Laloy et al., 2017), data compression (Ball ́e et al., 2017), anomaly detection (Xu et al., 2018), and image generation (Razavi et al., 2019). VAEs (more precisely, β-VAEs (Higgins et al., 2017)) span such a diverse set of application domains in part because they can be tuned to a specific task without changing the network architecture, in a way that is well understood from information theory (Alemi et al., 2018).\n\nThe original proposal of VAEs (Kingma & Welling, 2014) motivates them from the perspective of generative probabilistic modeling and approximate Bayesian inference. However, the generalization to β-VAEs breaks this interpretation as they are no longer trained by maximizing a lower bound on the marginal data likelihood. These models are better described as neural networks that are trained to learn the identity function, i.e., to make their output resemble the input as closely as possible. This task is made nontrivial by introducing a so-called (variational) information bottleneck (Alemi et al., 2017; Tishby & Zaslavsky, 2015) at one or more layers, which restricts the information content that passes through these layers. The network activations at the information bottleneck are called latent representations (or simply “latents”), and they split the network into an encoder part (from input to latents) and a decoder part (from latents to output). This separation of the model into an encoder and a decoder allows us to categorize the wide variety of applications of VAEs into three domains:\n\n1. data reconstruction tasks, i.e., applications that involve both the encoder and the decoder; these include various nonlinear inter- and extrapolations (e.g., image upscaling, denoising, or inpainting), and VAE-based methods for lossy data compression;\n\n2. representation learning tasks, i.e., applications that involve only the encoder; they serve a downstream task that operates on the (typically lower dimensional) latent representation, e.g., classification, regression, visualization, clustering, or anomaly detection; and\n\n3. generative modeling tasks, i.e., applications that involve only the decoder are less com-\n\nmon but include generating new samples that resemble training data.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Left: trade-off between performance in the three applications domains of VAEs, using GHVAE trained on the SVHN data set (details: Section 5); higher is better for all three metrics; gray dots on walls show 2d-projections. Right: color code, corresponding layer-wise rates (Eq. 7), and performance). The hyperparameters β2 and β1 individual performance landscapes (size of dots allow us to tune the HVAE for best data reconstruction ( ), or ). Note that performance landscapes differ strongly across the three best generative modeling ( applications, and neither a standard VAE (β2 = β1 = 1; marked “ ” in right panels) nor a conventional β-VAE (β2 = β1; dashed red lines) result in optimal models for any of the three applications.\n\n), best representation learning (\n\n△\n\n∝\n\n(cid:68)\n\n⋄\n\n•\n\nThe information bottleneck incentivizes the VAE to encode information into the latents efficiently by removing any redundancies from the input. How agressively this is done can be controlled by tuning the strength β of the information bottleneck (Alemi et al., 2018). Unfortunately, information theory distinguishes relevant from redundant information only in a quantitative way that is agnostic to the qualitative features that each piece of information represents about some data point. In practice, many VAE-architectures (Deng et al., 2017; Yingzhen & Mandt, 2018; Ball ́e et al., 2018) try to separate qualitatively different features into different parts of the latent representation by making the model architecture reflect some prior assumptions about the semantic structure of the data. This allows downstream applications from the three domains discussed above to more precisely target specific qualitative aspects of the data by using or manipulating only the corresponding part of the latent representation. However, in this approach, the degree of detail to which each qualitative aspect is encoded in the latents can be controlled at most indirectly by tuning network layer sizes.\n\nIn this paper, we argue both theoretically and empirically that the three different application domains of VAEs identified above require different trade-offs in the amount of information that is encoded in each part of the latent representation. We propose a method to independently control the information content (or “rate”) of each layer of latent representations, generalizing the rate/distortion theory of β-VAEs (Alemi et al., 2018) for VAEs with more than one layer of latents (“hierarchical VAEs” or HVAEs for short). We identify the most general model architecture that is compatible with our proposal and analyze how both theoretical performance bounds and empirically measured performances in each of the above three application domains depend on how rate is distributed across layers.\n\nOur approach is summarized in Figure 1. The 3d-plot shows empirically measured performance metrics (discussed in detail in Section 5.2) for the three application domains identified above. Each point on the colored surface corresponds to different layer-wise rates in an HVAE with two layers of latents. Crucially, the rates that lead to optimal performance are different for each of the three application domains (see markers in Figure 1), and none of these three optimal models coincide with a conventional β-VAE (dashed red lines in right panels). Thus, being able to control each layer’s individual rate allows practitioners to train VAEs that target a specific application.\n\n, and\n\n△\n\n(cid:68)\n\n⋄\n\n,\n\nThe paper is structured as follows. Section 2 summarizes related work. Section 3 introduces the proposed information-trading method. We then analyze how controlling individual layers’ rates can be used to tune HVAEs for specific tasks, i.e., how performance in each of the three application domains identified above depends on the allocation of rates across layers. This analysis is done theoretically in Section 4 and empirically in Section 5. Section 6 provides concluding remarks.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n(a) bottom-up\n\n(b) implicit top-down (e.g., LVAE)\n\n(c) generalized (explicit) top-down\n\nFigure 2: Inference (dashed arrows) and generative (solid arrows) models for hierarchical VAEs (HVAEs) with two layers of latent variables. White/gray circles denote latent/observed random variables, respectively; the diamond d1 in (b) is the result of a deterministic transformation of x.\n\n2 RELATED WORK\n\nWe group related work into work on model architectures for hierarchical VAEs, and on β-VAEs.\n\nModel Design for Hierarchical VAEs. The original VAE design (Kingma & Welling, 2014; Rezende et al., 2014) has a single layer of latent variables, but recent works (Vahdat & Kautz, 2020; Child, 2021), found that increasing the number of stochastic layers in hierarchical VAEs (HVAEs) improves performance. HVAEs have various designs for their inference models. Sønderby et al. (2016) introduced Ladder VAE (LVAE) with a top-down inference path rather than the naive bottomup inference (see Section 3), whereas the Bidirectional-Inference VAE (BIVA) (Maaløe et al., 2019) uses a combination of top-down and bottom-up. Our proposed framework applies to a large class of inference models (see Section 3) that includes the popular LVAE (Sønderby et al., 2016).\n\nβ-VAEs And Their Information-Theoretical Interpretations. Higgins et al. (2017) introduced an extra hyperparameter β in the objective of VAEs that tunes the strength of the information bottleneck, and they observed that large β leads to a disentangled latent representation. An informationtheoretical interpretation of β-VAEs was provided in (Alemi et al., 2018) by applying the concept of a (variational) bottleneck (Tishby & Zaslavsky, 2015; Alemi et al., 2017) to autoencoders. Due to this information-theoretical interetation, β-VAEs are popular models for data compression (Ball ́e et al., 2017; Minnen et al., 2018; Yang et al., 2020), where tuning β allows trading off between the bit rate of compressed data and data distortion. In the present work, we generalize β-VAEs when applied to HVAEs, and we introduce a framework for tuning the rate of each latent layer individually.\n\n3 A HIERARCHICAL INFORMATION TRADING FRAMEWORK\n\nWe propose a refinement of the rate/distortion theory of β-VAEs (Alemi et al., 2018) that admits controlling individual layers’ rates in VAEs with more than one layers of latents (hierarchical VAEs).\n\n3.1 CONVENTIONAL β-VAE WITH HIERARCHICAL LATENT REPRESENTATIONS\n\nWe consider a hierarchical VAE (HVAE) for data x with L layers of latent representations l=1. Figure 2, discussed further in Section 3.2 below, illustrates various model architectures for the example of L = 2. Solid arrows depict the generative model pθ( , x), where θ are model parameters (neural network weights). We assume that the implementation factorizes pθ(\n\n, x) as follows,\n\nzl}\n\nzl}\n\n{\n\n{\n\nL\n\npθ(\n\nzl}\n\n, x) = pθ(zL) pθ(zL−1|\n\nzL) pθ(zL−2|\n\nzL−1, zL)\n\n· · ·\n\n{\n\nwhere the notation z≥n for any n is short for the collection of latents are synonymous), and the numbering of latents from L down to 1 follows the common convention in the literature (Sønderby et al., 2016; Gulrajani et al., 2017; Child, 2021). The loss function of a\n\nl=n (thus, z≥1 and\n\nzl}\n\n{\n\nzl} {\nz≥2) pθ(x |\n\nz≥1)\n\n(1)\n\npθ(z1| zl}\n\n{\n\nL\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nnormal β-VAE (Higgins et al., 2017) with this generic architecture would be\n\nLβ(θ, φ) = Ex∼Xtrain\n\n(cid:2) E (cid:124)\n\nqφ({zl}|x)\n\n(cid:2)\n\nlog pθ(x (cid:123)(cid:122) = “distortion” D\n\n−\n\nzl}\n\n|{\n\n)(cid:3) (cid:125)\n\n+β DKL (cid:124)\n\n(cid:2)qφ( {\n\nzl} |\n\nx)\n\n(cid:12) (cid:12) (cid:12) pθ( (cid:12)\n\n(cid:123)(cid:122) = “rate” R\n\n(cid:3).\n\n(2)\n\nzl}\n\n{\n\n)(cid:3) (cid:125)\n\n{ · || ·\n\nzl} |\n\nx) is the inference (or “encoder”) model with parameteres φ, Xtrain is the training Here, qφ( set, DKL[ ] denotes Kullback-Leibler divergence, and the Lagrange parameter β > 0 trades off between a (total) rate R and a distortion D (Alemi et al., 2018). Setting β = 1 turns Eq. 2 into the negative ELBO objective of a regular VAE (Kingma & Welling, 2014). The rate R obtains its name as it measures the (total) information content that qφ encodes into the latent representations ,\nwhich would manifest itself in the expected bit rate when one optimally encodes a random draw ) as an entropy model (Agustsson & Theis, 2020; Bennett et al., {\n2002). An important observation pointed out in (Alemi et al., 2017) is that, regardless how rate R is traded off against distortion D by tuning β, their sum R + D is—in expectation under any data distribution pdata(x)—always lower bounded by the entropy H[pdata(x)] := E log pdata(x)], (3)\n\nx) using pθ(\n\nzl} ∼\n\nzl} |\n\npdata(x)[\n\nzl}\n\nzl}\n\nH[pdata(x)]\n\npdata(x)[R + D]\n\npdata.\n\nqφ(\n\n−\n\nE\n\n{\n\n{\n\n{\n\n≥\n\n∀\n\nzl}\n\nLimitations. The rate R in Eq. 2 is a property of the collection of all latents, which can limit its interpretability for some inference models. For example, the common convention of enumerating layers zl from l = L down to 1 in Eq. 1 is reminiscent of a naive architecture for the inference model that factorizes in reverse order compared to Eq. 1 (“bottom up”, see dashed arrows in Figure 2(a)), zL−1). Using a HVAE with such a “bottom-up” i.e., qφ( x) = qφ(z1| qφ(zL| · · · x) and then inference model to reconstruct some given data point x would map x to z1 using qφ(z1| z1), thus ignoring all latents zl with l > 1. Yet, the map z1 back to the data space using pθ(x |\nrate term in Eq. 2 still depends on all latents, including the ones not needed to reconstruct any data (practical VAE-based compression methods using bits-back coding (Frey & Hinton, 1997) would, however, indeed use zl with l > 1 as auxiliary variables for computational efficiency).\n\nx) qφ(z2|\n\nzl} |\n\nz1)\n\n{\n\n{\n\n3.2 TRADING INFORMATION BETWEEN LATENTS\n\nqφ(\n\nMany HVAEs used in the literature allow us to resolve the limitations identified in Section 3.1. For example, the popular LVAE architecture (Sønderby et al., 2016), (Figure 2(b)), uses an inference in the same order as the generative model model (dashed arrows) that traverses the latents (solid arrows). We consider the following generalization of this architecture (see Figure 2(c)), z≥2, x).\n\n(4) Formally, Eq. 4 is just the product rule of probability theory and therefore holds for arbitrary inx). More practically, however, we make the assumption that the actual ference models qφ( implementation of qφ( x) follows the structure in Eq. 4. This means that, using the trained model, the most efficient way to map a given data point x to its reconstruction ˆx now involves all latents zl (either drawing a sample or taking the mode at each step):\n\nx) = qφ(zL| zl} |\n\nzL, x) qφ(zL−2 |\n\nx) qφ(zL−1 |\n\nzL−1, zL, x)\n\nqφ(z1 |\n\nzl} |\n\nzl} |\n\nzl}\n\n· · ·\n\n{\n\n{\n\n{\n\n{\n\nx\n\nqφ(zL|x)\n\n−−−−−−→\n\nzL\n\nqφ(zL−1|zL,x)\n\n−−−−−−−−−−→\n\nzL−1 −→ · · · −→\n\nz2\n\nqφ(z1|z≥2,x)\n\n−−−−−−−−−→\n\nz1\n\npθ(x|{zl})\n\n−−−−−−−−→\n\nˆx.\n\n(5)\n\nLayer-wise Rates. We can interpret Eq. 5 in that it first maps x to a “crude” representation zL, which gets iteratively refined to z1, and finally to a reconstruction ˆx. Note that each factor z≥l+1, x) of the inference model in Eq. 4 is conditioned not only on the previous layqφ(zl | ers z≥l+1 but also on the original data x. This allows the inference model to target each refinement step in Eq. 5 such that the reconstruction ˆx becomes close to x. More formally, we chose the inference architecture in Eq. 4 such that it factorizes over in the same order as the generative model (Eq. 1). This allows us to split the total rate R into a sum of layer-wise rates as follows,\n\n{\n\nR = E\n\nqφ({zl}|x)\n\nlog\n\n(cid:20)\n\n= R(zL) + R(zL−1|\n\nHere,\n\nR(zL) = DKL z≥l+1) = E\n\nR(zl|\n\nx)\n\n+ log\n\nzL, x) zL)\n\nzl} qφ(zL| qφ(zL−1| pθ(zL) pθ(zL−1| zL) + R(zL−2 | zL−1, zL) + . . . + R(z1| (cid:12) x) (cid:12) (cid:12) pθ(zL)(cid:3) (cid:2)qφ(zL| (cid:12) (cid:2)qφ(zl | (cid:2)DKL\n\nz≥l+1, x)\n\nq(z≥l+1|x)\n\n+ . . . + log\n\n(cid:12) pθ(zl |\n\nand\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:21)\n\nqφ(z1| pθ(z1|\n\nz≥2, x) z≥2)\n\nz≥2).\n\nz≥l+1)(cid:3)(cid:3)\n\n(6)\n\n(7)\n\nquantify the information content of the highest-order latent representation zL and the (expected) increase in information content in each refinement step zl+1 →\n\nzl in Eq. 5, respectively.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nControlling Each Layer’s Rate. Using Eqs. 6-7, we generalize the rate/distortion trade-off from Section 3.1 by introducing L individual Lagrange multipliers βL, βL−1, . . . , β1, collectively denoted as boldface β. This leads to a new loss function that generalizes Eq. 2 as follows,\n\nβ(θ, φ) = Ex∼Xtrain\n\nL\n\n(cid:2)D + βLR(zL) + βL−1R(zL−1|\n\nzL) + . . . + β1R(z1|\n\nz≥2)(cid:3).\n\n(8)\n\nSetting all βs to the same value recovers the conventional β-VAE (Eq. 2), which trades off distortion against total information content in . Tuning each β-hyperparameter individually allows trading off information content across latents. (In a very deep HVAE (i.e., large L) it may be more practical to group layers into only few bins and to use the same β-value for all layers within a bin.) We analyze how to tune βs for various applications theoretically in Section 4 and empirically in Section 5.\n\nzl}\n\n{\n\n4\n\nINFORMATION-THEORETICAL PERFORMANCE BOUNDS FOR HVAES\n\nby the individual layers’ rates R(zL) and R(zl|\n\nIn this section, we analyze theoretically how various performance metrics for HVAEs are restricted z≥l+1) identified in Eq. 7 for a HVAE with “topdown” inference model. Our analysis motivates the use of the information-trading loss function in Eq. 8 for training HVAEs, following the argument from the introduction that VAEs are commonly used for a vast variety of tasks. As we show, different tasks require different trade-offs that can be targeted by tuning the Lagrange multipliers β in Eq. 8. We group tasks into the application domains of (i) data reconstruction and manipulation, (ii) representation learning, and (iii) data generation.\n\nData Reconstruction and Manipulation. The most obvious class of application domains of VAEs includes tasks that combine encoder and decoder to map some data point x to representations and then back to the data space. The simplest performance metric for such data reconstruction tasks is the expected distortion Epdata(x)[D], which we can bound by combining Eq. 3 with Eqs. 6-7,\n\nzl}\n\n{\n\nE\n\npdata(x)[D]\n\nH[pdata(x)]\n\n≥\n\nE\n\npdata(x)\n\n(cid:2)R(zL) + R(zL−1|\n\n−\n\nzL) +\n\n+ R(z1|\n\n· · ·\n\nz≥2)(cid:3).\n\n(9)\n\nEq. 9 would suggest that higher rates (i.e., lower β’s) are always better for data reconstruction tasks. However, in many practical tasks (e.g., image upscaling, denoising, or inpainting) the goal is not solely to reconstruct the original data but also to manipulate the latent representations in a meaningful way. Here, lower rates can lead to more semantically meaningful representation spaces (see, e.g., Section 5.6 below). Controlling how rate is distributed across layers via Eq. 8 may allow practitioners to have a semantically meaningful high-level representation zL with low rate R(zL) while still retaining a high total rate R, thus allowing for low distortion D without violating Eq. 9.\n\nzl}\n\n{\n\nRepresentation Learning. In many practical applications, VAEs are used as nonlinear dimensionality reduction methods to prepare some complicated high-dimensional data x for downstream tasks such as classification, regression, visualization, clustering, or anomaly detection. We consider zl) operating on the latents zl at some level l. We assume that the (unknown) a classifier pcls.(y y) generates data x conditioned on some true data generative process pdata(y, x) = pdata(y) pdata(x |\ntrue label y, thus defining a Markov chain y pdata zl zl). ˆy where ˆy := arg maxy pcls.(y x\n| −−→ Classification accuracy is bounded (Meyen, 2016) by a function of the mutual information Iq(y; zl), (cid:20)\n\npcls. −−→\n\n−→\n\n(cid:21)(cid:21)\n\nqφ\n\n(cid:20)\n\n|\n\nIq(y; zl)\n\n≤\n\nIq(x; zl)\n\n≡\n\nE\n\npdata(x)\n\nqφ(zl|x)\n\nE\n\n(10)\n\nDKL\n\n(cid:12) (cid:2)qφ(zl) (cid:12) (cid:12) pθ(zl)(cid:3) (cid:12)\n\nlog\n\n(cid:20)\n\nlog\n\n(cid:20)\n\nx)\n\nx)\n\nqφ(zl| qφ(zl) qφ(zl| pθ(zl) qφ(z≥l| pθ(z≥l)\n\n(cid:21)(cid:21)\n\n− (cid:21)\n\nx)\n\n(cid:20)\n\nE\n\nqφ(zl|x)\n\n(cid:20)\n\nE\n\nqφ(z≥l|x)\n\nlog\n\nE\n\nqφ(zl|x)\n\n(cid:104)\n\nDKL\n\n−\n\n(cid:2) R(zL) + R(zL−1|\n\n(cid:124)\n\n(cid:12) x, zl) (cid:12) (cid:12)\n\n(cid:2)qφ(z≥l+1 | zL) + . . . + R(zl |\n\n(cid:12) pθ(z≥l+1| (cid:3). z≥l+1) (cid:125)\n\n(cid:123)(cid:122) =:R(z≥l) (≤R)\n\nzl)(cid:3)(cid:105)(cid:21)\n\n= E\n\npdata(x)\n\nE\n\npdata(x)\n\n≤\n\nE\n\npdata(x)\n\n≤\n\nHere, qφ(zl) := E x)] and we identify R(z≥l) as the rate accumulated in all layers from zL to zl. The first inequality in Eq. 10 comes from the data processing inequality (MacKay,\n\npdata(x)[qφ(zl|\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n2003), and the other two inequalities result from discarding the (nonnegative) KL-terms. The classification accuracy is thus bounded by (Meyen, 2016) (see also proof in Appendix B)\n\nclass. accuracy\n\n≤\n\nf −1(cid:0)Iq(y; zl)(cid:1)\n\nf −1(cid:0)E\n\npdata(x)[R(z≥l)](cid:1)\n\n(cid:0)\n\nf −1(cid:0)E\n\npdata(x)[R](cid:1)(cid:1)\n\n(11)\n\n≤\n\n≤\n\nα) log 1−α where f −1 is the inverse of the monotonic function f (α) = H[pdata(y)]+α log α+(1 M −1 with M being the number of classes and H[pdata(y)] log M the marginal label entropy. Eq. 11 suggests that the accuracy of an optimal classifier on zl would increase as the rate R(z≥l) accu0), and that the rate added in downstream layers z<l mulated from zL to zl grows (i.e., as β≥l → would be irrelevant. Practical classifiers, however, have a limited expressiveness, which a very high rate R(z≥l) might exceed by encoding too many details into zl that are not necessary for classification. We observe in Section 5.6 that, in such cases, increasing the rates of downstream layers z<l improves classification accuracy as it allows keeping zl simpler by deferring details to z<l.\n\n−\n\n≤\n\nData Generation. The original proposal of VAEs (Kingma & Welling, 2014) motivated them from a generative modeling perspective using that, for β = 1, the negative of the loss function in Eq. 2 is a lower bound on the log marginal data likelihood. This suggests setting all β-hyperparameters in Eq. 8 to values close to 1 if a HVAE is used primarily for its generative model pθ.\n\nIn summary, our theoretical analysis suggests that optimally tuned layer-wise rates depend on whether a HVAE is used for data reconstruction, representation learning, or data generation. The next section tests our theoretical predictions empirically for the same three application domains.\n\n5 EXPERIMENTS\n\nTo demonstrate the features of our hierarchical information trading framework, we run large-scale grid searches over a two-dimensional rate space using two different implementations of HVAEs and three different data sets. Although the proposed framework is applicable for HVAEs with L 2, we only use HVAEs with L = 2 in our experiments for simplicity and visualization purpose.\n\n≥\n\n5.1 EXPERIMENTAL SETUP\n\n×\n\n32 pixel color images), and MNIST (LeCun et al., 1998) (28\n\nData sets. We used the SVHN (Netzer et al., 2011) and CIFAR-10 (Krizhevsky, 2009) data sets (both 32 28 binary pixel images). SVHN consists of photographed house numbers from 0 to 9, which are geometrically simpler than the 10 classes of objects from CIFAR-10 but more complex than MNIST digits. Most results shown in the main paper use SVHN; comprehensive results for CIFAR-10 and MNIST are shown in Appendix A.2 and tell a similar story except where explicitly discussed.\n\n×\n\nN\n\n(0, I), and we use diagonal Gaussian models for pθ(z1|\n\nModel Architectures. For the generative model (Eq. 1), we assume a (fixed) standard Gaussian (gμ(z2), gσ(z2)2) prior p(z2) = N\nand (for SVHN and CIFAR-10) pθ(x xI) (this is similar to, e.g., (Minnen et al., 2018)). Here, gμ, gσ, and gμ′, denote neural networks (see details below). Since MNIST has binary z1) = Bern(gμ′(z1)). For the pixel values, we model it with a Bernoulli distribution for pθ(x |\n(fμ(x), fσ(x)2) and for x) = inference model, we also use diagonal Gaussian models for qφ(z2| (fμ′(x, z2), fσ′(x, z2)2), where fμ, fσ, fμ′, and fσ′ are again neural networks. qφ(z1|\n\n(gμ′(z1), σ2\n\nz1) = |\n\nx, z2) =\n\nz2) =\n\nWe examine both LVAE (Figure 2(b)) and our generalized top-down HVAEs (GHVAEs; see Figure 2(c)), using simple network architectures with only 2 to 3 convolutional and 1 fully connected layers (see Appendix A.1 for details) so that we can scan a large rate-space efficiently. Note that we are not trying to find the new state-of-the-art HVAEs. Results for LVAE are in Appendix A.2.2.\n\nN\n\nN\n\nN\n\nWe trained 441 different HVAEs for each data set/model combination, scanning the ratehyperparameters (β2, β1) over a 21 21 grid ranging from 0.1 to 10 on a log scale in both directions (see Figure 1 on page 2, right panels). Each model took about 2 hours to train on an RTX-2080Ti 27 hours in total for each data set/model combination using 32 GPUs in parallel). GPU (\n\n×\n\n∼\n\nBaselines. Our proposed framework (Eq. 8) generalizes over both VAEs and β-VAEs (Eq. 2), which we obtain in the cases β2 = β1 = 1 and β2 = β1, respectively. These baselines are indicated as black “ ” and red “ ” circles, respectively, in Figures 3, 5, 6, and 7, discussed below.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n(a) Rate/rate/distortion surface for SVHN.\n\n(b) PSNR-rates comparison in 2d.\n\nFigure 3: PSNR-rate trade-off for GHVAEs trained on SVHN and CIFAR-10. Figure (a) visualizes the same data as the left panel of (b) in 3d. Black circles “ ” mark standard VAEs (β2 = β1 = 1), red circles “ ” mark β-VAEs (β2 = β1), and purple circles “ ” mark optimal models along constant total rate (dashed diagonal lines) as defined in Section 5.3. Crosses point to columns in Figure 4.\n\nMetrics. Performance metrics for the three application domains of VAEs mentioned in the introIn addition, we duction are introduced at the beginnings of the corresponding Sections 5.4-5.6.\n\nevaluate the individual rates R(z2) and R(z1|\n\nz2) (Eq. 7), which we report in nats (i.e., to base e).\n\n5.2 THERE IS NO “ONE HVAE FITS ALL”\n\nFigure 1 on page 2 summarizes our results. The 21 21 GHVAEs trained with the grid of hyperparameters β2 and β1 map out a surface in a 3d-space spanned by suitable metrics for the three application domains (metrics defined in Sections 5.4-5.6 below). The two upper right panels map colors on this surface to βs used for training and to the resulting layer-wise rates, respectively. The lower right panels show performance landscapes and identify the optimal models for the three appli- ). cation domains of data reconstruction (\n\n), and generative modeling (\n\n), representation learning (\n\n×\n\nThe figure shows that moving away from a conventional β-VAE (β2 = β1; dashed red lines in Figure 1) allows us to find better models for a given application domain as the three application domains favor vastly different regions in β-space. Thus, there is no single HVAE that is optimal for all tasks, and a HVAE that has been optimized for one task can perform poorly on a different task.\n\n(cid:68)\n\n△\n\n⋄\n\n5.3 DEFINITION OF THE OPTIMAL MODEL FOR A GIVEN TOTAL RATE\n\nOne of the questions we study in Sections 5.4-5.6 below is: “Which allocation of rates across layers results in best model performance if we keep the total rate R fixed”. Unfortunately, it is difficult to keep R fixed at training time since we control rates only indirectly via their Lagrange multipliers β2 and β1. We instead use the following definition, illustrated in Figure 6 for a performance metric introduced in Section 5.6 below. The figure plots the performance metric over R for all 21 21 βsettings and highlights with purple circles “ ” all points on the upper convex hull. These highlighted models are optimal for a small interval of total rates in the following sense: if we use the total rates R of all “ ” to partition the horizontal axis into intervals then, by definition of the convex hull, each “ ” represents the model with highest performance in either the interval to its left or the one to its right.\n\n×\n\n5.4 PERFORMANCE ON DATA RECONSTRUCTION\n\nlog D] up to rescaling and shifting. Higher PSNR means better reconstruction.\n\nReconstruction is a popular task for VAEs, e.g., in the area of lossy compression (Ball ́e et al., 2017). We measure reconstruction quality using the common peak signal-to-noise ratio (PSNR), which is equal to Ex∼Xtest[ Figure 3(a) shows a 3d-plot of PSNR as a function of both R(z1|\n\nz2) and R(z2) for SVHN, thus generalizing the rate/distortion curve of a conventional β-VAE to a rate/rate/distortion surface. Figure 3(b) introduces a more compact 2d-representation of the same data that we use for all remaining metrics in the rest of this section and in Appendix A.2, and it also shows results for CIFAR-10.\n\n−\n\n7\n\nR(z2)01020304050R(z1|z2)020406080PSNR18202224262818202224262802040R(z2)020406080R(z1|z2)123SVHN20250204060R(z2)0255075100125150CIFAR-101520Published as a conference paper at ICLR 2023\n\nFigure 4: Samples (top) and reconstructions (bottom) from 3 different models (blue column labels “1”, “2”, and “3” from left to right correspond to crosses “1”, “2”, and “3” in Figures 3(b) & 5). Consistent with PSNR and IS metrics, model “1” produces poorest samples but best reconstructions.\n\nFigure 5: Sample generation performance, measured in Inception Score (IS, see Eq. 12) and its factorization into diversity and sharpness as a function of layer-wise rates for GHVAEs trained using SVHN data. Crosses in left panel correspond to samples shown in Figure 4. Markers “ ”, “ ”, and “ ” same as in Figure 3.\n\nFigure 6: RBF-SVM classification accuracies on μ2. Dashed line shows theoretical bound (Eq. 11). Other markers as in Figure 3.\n\nUnsurprisingly and consistent with Eq. 9, reconstruction performance improves as total rate grows. However, minimizing distortion without any constraints is not useful in practice as we can simply use the original data, which has no distortion. To simulate a practical constraint in, e.g., a datacompression application, we consider models with optimal PSNR for a given total rate R (as defined in Section 5.3) which are marked as purple circles “ ” in Figure 3(b). We see for both SVHN and CIFAR-10 that conventional β-VAEs (β2 = β1; red circles) perform somewhat suboptimal for a given total rate and can be improved by trading some rate in z2 for some rate in z1. Reconstruction examples for the three models marked with crosses in Figure 3(b) are shown in Figure 4 (bottom). Visual reconstruction quality improves from “3” to “2” to “1”, consistent with reported PSNRs.\n\n5.5 PERFORMANCE ON SAMPLE GENERATION\n\nWe next evaluate how tuning layer-wise rates affects the quality of samples from the generative model. We measure sample quality by the widely used Inception Score (IS) (Salimans et al., 2016),\n\nIS = exp (cid:8)E\n\npθ(x)\n\n(cid:2)DKL[pcls.(y\n\npcls.(y)](cid:3)(cid:9) = eH[pcls.(y)]\n\ne−Epθ (x)[H[pcls.(y|x)]]\n\n(12)\n\nx) |\n\n||\n\n×\n\nx) is the predictive distribution of a classifier Here, pθ is the trained generative model (Eq. 1), pcls.(y trained on the same training set, and pcls.(y) := E x)]. The second equality in Eq. 12 |\nfollows Barratt & Sharma (2018) to split IS into a product of a diversity score and a sharpness score. Higher is better for all scores. The classifier is a ResNet-18 (He et al., 2016) for SVHN (test accuracy 95.02 %) and a DenseNet-121 (Huang et al., 2017) for CIFAR-10 (test accuracy 94.34 %).\n\npθ(x)[pcls.(y\n\n|\n\nFigure 5 (left) shows IS for GHVAEs trained on SVHN. Unlike the results for PSNR, here, higher rate does not always lead to better sample quality: for very high R(z2) and low R(z1| z2), IS eventually drops. The region of high IS is in the area where β2 < β1, i.e., where R(z2) is higher than in a comparable conventional β-VAE. The center and right panels of Figure 5 show diversity and sharpness, indicating that IS is mainly driven here by sharpness, which depends mostly on R(z2), possibly because z2 captures higher-level concepts than z1 that may be more important to the classifier in Eq. 12. Samples from the the three models marked with crosses in Figure 5 are shown in Figure 4 (top). Visual sample quality improves from “1” to “3” to “2”, consistent with reported IS.\n\n8\n\n02040R(z2)020406080R(z1|z2)123InceptionScore(IS)34502040R(z2)ISDiversity5.07.502040R(z2)ISSharpness0.50.60255075TotalRate0.20.40.60.81.0RBF-SVMAccuracy0.20.40.6Published as a conference paper at ICLR 2023\n\nFigure 7: Mutual information (MI) Iq(y; z2) and classification accuracies of four classifiers (see z2). Classifiers are conditioned on column labels) as a function of layer-wise rates R(z2) and R(z1| x) learned from GHVAEs trained with SVHN (top) and CIFAR-10 (bottom). μ2 := arg maxz2 q(z2|\n\nMarkers “ ”, “ ”, and “ ” same as in Figure 3.\n\n5.6 PERFORMANCE ON REPRESENTATION LEARNING FOR DOWNSTREAM CLASSIFICATION\n\nVAEs are very popular for representation learning as they map complicated high dimensional data x to typically lower dimensional representations . To measure the quality of learned representations, we train two sets of classifiers on a labeled test set for each trained HVAE, each consisting of: logistic regression, a Support Vector Machine (SVM) (Boser et al., 1992) with linear kernel, an SVM with RBF kernel, and k-nearest neighbors (kNN) with k = 5. One set of classifiers is conditioned on x).\n\nx) and the other one on the mode μ1 of qφ(z1|\n\nWe use the implementations from scikit-learn (Pedregosa et al., 2011) for all classifiers.\n\nthe mode μ2 of qφ(z2|\n\nz2, x), where z2 ∼\n\nqφ(z2|\n\nzl}\n\n{\n\nFigure 7 shows the classification accuracies (columns 2-5) for all classifiers trained on μ2. The first column shows the mutual information Iq(y; z2), which depends mainly on R(z2) as expected from Eq. 10. is expressive As long as the classifier enough (e.g., RBF-SVM or kNN) and the data set is simple (SVHN; top row), higher higher R(z2)) cormutual information ( ≈\nresponds to higher classification accuracies, consistent with Eq. 11. But for less expres-\n\nTable 1: Optimal classification accuracies (across all (β2, β1)-settings) using either μ2 or μ1.\n\nData Set\n\nlog. reg. lin. SVM RBF SVM kNN\n\nSVHN (μ2) SVHN (μ1)\n\n28.43 % 27.87 % 77.60 % 64.25 % 45.77 % 49.81 % 59.28 % 56.49 %\n\nCIFAR-10 (μ2) 47.36 % 46.95 % 53.15 % 44.20 % CIFAR-10 (μ1) 43.27 % 42.55 % 45.60 % 39.25 %\n\nsive (e.g., linear) classifiers or more complex data (CIFAR-10; bottom row), increasing R(z1|\n\nz2) improves classification accuracy (see purple circles “ ” in corresponding panels), consistent with the discussion below Eq. 11. We see a similar effect (Table 1) for most classifier/data set combinations when replacing μ2 by μ1, which has more information about x but is also higher dimensional.\n\n6 CONCLUSIONS\n\nWe classified the various tasks that can be performed with Variational Autoencoders (VAEs) into three application domains and argued that each domain has different trade-offs, such that a good VAE for one domain is not necessarily good for another. This observation motivated us to propose a refinement of the rate/distortion theory of VAEs that allows trading off rates across individual layers of latents in hierarchical VAEs. We showed both theoretically and empirically that the proposal indeed provides practitioners better control for tuning VAEs for the three application domains. In the future, it would be interesting to explore adaptive schedules for the Lagrange parameters β that would make it possible to target a specific given rate for each layer in a single training run, for example by using the method proposed by Rezende & Viola (2018).\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThe authors would like to thank Johannes Zenn, Zicong Fan, Zhen Liu for their helpful discussion. Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645. This work was supported by the German Federal Ministry of Education and Research (BMBF): T ̈ubingen AI Center, FKZ: 01IS18039A. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Tim Z. Xiao.\n\nReproducibility Statement. All code necessary to reproduce the results in this paper is available at https://github.com/timxzz/HIT/.\n\nREFERENCES\n\nEirikur Agustsson and Lucas Theis. Universally quantized neural compression. Advances in Neural\n\nInformation Processing Systems, 33:12367–12376, 2020. 4\n\nAlexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A Saurous, and Kevin Murphy. Fixing a broken elbo. In International Conference on Machine Learning, pp. 159–168. PMLR, 2018. 1, 2, 3, 4\n\nAlexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information\n\nbottleneck. In International Conference on Learning Representations, 2017. 1, 3, 4\n\nJohannes Ball ́e, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression.\n\nIn International Conference on Learning Representations, 2017. 1, 3, 7\n\nJohannes Ball ́e, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. In International Conference on Learning Representations, 2018. 2\n\nShane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973,\n\n2018. 8\n\nCharles H Bennett, Peter W Shor, John A Smolin, and Ashish V Thapliyal. Entanglement-assisted capacity of a quantum channel and the reverse shannon theorem. IEEE transactions on Information Theory, 48(10):2637–2655, 2002. 4\n\nBernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal margin classifiers. In Proceedings of the fifth annual workshop on Computational learning theory, pp. 144–152, 1992. 9\n\nRewon Child. Very deep vaes generalize autoregressive models and can outperform them on images.\n\nIn International Conference on Learning Representations, 2021. 3\n\nZhiwei Deng, Rajitha Navarathna, Peter Carr, Stephan Mandt, Yisong Yue, Iain Matthews, and Greg Mori. Factorized variational autoencoders for modeling audience reactions to movies. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2577–2586, 2017. 2\n\nBrendan J Frey and GE Hinton. E cient stochastic source coding and an application to a bayesian\n\nnetwork source model. Computer Journal, 1997. 4\n\nIshaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, In International\n\nand Aaron Courville. Pixelvae: A latent variable model for natural images. Conference on Learning Representations, 2017. 3\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. 8\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a In International Conference on Learning Representations, constrained variational framework. 2017. 1, 3, 4\n\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017. 8\n\nZhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep embedding: an unsupervised and generative approach to clustering. In International Joint Conference on Artificial Intelligence, 2017. 1\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference\n\non Learning Representations, 2014. 1, 3, 4, 6\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009. 6\n\nEric Laloy, Romain H ́erault, John Lee, Diederik Jacques, and Niklas Linde.\n\nInversion using a new low-dimensional representation of complex binary geological media based on a deep neural network. Advances in water resources, 110:387–405, 2017. 1\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 6\n\nLars Maaløe, Marco Fraccaro, Valentin Li ́evin, and Ole Winther. Biva: A very deep hierarchy of latent variables for generative modeling. Advances in Neural Information Processing Systems, 32, 2019. 3\n\nDavid JC MacKay. Information theory, inference and learning algorithms. Cambridge university\n\npress, 2003. 5\n\nSascha Meyen. Relation between classification accuracy and mutual information in equally weighted\n\nclassification tasks. Master’s thesis, Universit ̈at Hamburg, 2016. 5, 6, 17\n\nDavid Minnen, Johannes Ball ́e, and George D Toderici. Joint autoregressive and hierarchical priors for learned image compression. Advances in Neural Information Processing Systems, 31, 2018. 3, 6\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. 6\n\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 9\n\nAli Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with\n\nvq-vae-2. Advances in Neural Information Processing Systems, 32, 2019. 1\n\nDanilo Jimenez Rezende and Fabio Viola. Taming vaes. arXiv preprint arXiv:1810.00597, 2018. 9\n\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pp. 1278–1286. PMLR, 2014. 1, 3\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in Neural Information Processing Systems, 29, 2016. 8\n\nCasper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder variational autoencoders. Advances in Neural Information Processing Systems, 29, 2016. 3, 4\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nHiroshi Takahashi, Tomoharu Iwata, Yuki Yamanaka, Masanori Yamada, and Satoshi Yagi. StudentIn International Joint Conference on\n\nt variational autoencoder for robust density estimation. Artificial Intelligence, 2018. 1\n\nNaftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle.\n\nIn\n\n2015 ieee information theory workshop (itw), pp. 1–5. IEEE, 2015. 1, 3\n\nArash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. Advances in\n\nNeural Information Processing Systems, 33:19667–19679, 2020. 3\n\nHaowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying Liu, Youjian Zhao, Dan Pei, Yang Feng, et al. Unsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications. In Proceedings of the 2018 world wide web conference, pp. 187–196, 2018. 1\n\nYibo Yang, Robert Bamler, and Stephan Mandt. Improving inference for neural image compression.\n\nAdvances in Neural Information Processing Systems, 33:573–584, 2020. 3\n\nLi Yingzhen and Stephan Mandt. Disentangled sequential autoencoder. In International Conference\n\non Machine Learning, 2018. 2\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA EXPERIMENT SUPPLEMENTARIES\n\nA.1\n\nIMPLEMENTATION DETAILS\n\nTable 2: Model architecture details for generalized top-down HVAEs (GHVAEs) used in Section 5. Conv and ConvTransp denote the convolutional and transposed convolutional layer, which has the corresponding input: input channel, output channel, kernel size, stride, padding. FC represents fully connected layer.\n\nData set\n\nq(z2|\n\nx)\n\nSVHN/ CIFAR-10\n\nShare: Conv(3, 32, 4, 2, 1), Conv(32, 32, 4, 2, 1), Conv(32, 32, 4, 2, 1)\n\nFor mean: FC(In=512, Out=32) For variance: FC(In=512, Out=32)\n\nq(z1|\n\nz2, x)\n\nFor x: Conv(3, 32, 4, 2, 1), Conv(32, 32, 4, 2, 1)\n\np(z1|\n\nz2)\n\nz1)\n\np(x |\n\nShare: FC(In=32, Out=256)\n\nFor mean: FC(In=256, Out=512) For variance: FC(In=256, Out=512)\n\nFor z2: FC(In=32, Out=512)\n\nShare: ConvTransp(64, 32, 4, 2, 1)\n\nFor mean: Conv(32, 32, 3, 1, 1) For variance: Conv(32, 32, 3, 1, 1)\n\nConvTransp(32, 32, 4, 2, 1), ConvTransp(32, 32, 4, 2, 1), ConvTransp(32, 3, 4, 2, 1)\n\nz1 dims: 512\n\nz2 dims: 32\n\nσx = 0.71\n\nTotal params: 475811\n\nMNIST (Binary)\n\nShare: Conv(1, 16, 4, 2, 1), Conv(16, 16, 4, 2, 1), Conv(16, 16, 4, 1, 0)\n\nFor mean: FC(In=256, Out=20) For variance: FC(In=256, Out=20)\n\nFor x: Conv(1, 16, 4, 2, 1), Conv(16, 16, 4, 1, 0)\n\nFor z2: FC(In=20, Out=256)\n\nShare: ConvTransp(32, 16, 4, 1, 0)\n\nFor mean: Conv(16, 16, 3, 1, 1) For variance: Conv(16, 16, 3, 1, 1)\n\nShare: FC(In=20, Out=128)\n\nFor mean: FC(In=128, Out=256) For variance: FC(In=128, Out=256)\n\nConvTransp(16, 16, 4, 1, 0), ConvTransp(16, 16, 4, 2, 1), ConvTransp(16, 1, 4, 2, 1)\n\nz1 dims: 256\n\nz2 dims: 20\n\nσx: N/A\n\nTotal params: 122713\n\nTable 3: Model architecture details for LVAEs used in Section 5. Conv and ConvTransp denote the convolutional and transposed convolutional layer, which has the corresponding input: input channel, output channel, kernel size, stride, padding. FC represents fully connected layer.\n\nData set\n\nSVHN/ CIFAR-10\n\nq(z2|\n\nx)\n\nShare: Conv(3, 32, 4, 2, 1), Conv(32, 32, 4, 2, 1), Conv(32, 32, 4, 2, 1)\n\nFor mean: FC(In=512, Out=32) For variance: FC(In=512, Out=32)\n\nq(z1|\n\nz2, x)\n\np(z1|\n\nz2)\n\np(x\n\nz1) |\n\nInvolve d: Conv(32, 32, 4, 2, 1)\n\nShare: FC(In=32, Out=256)\n\nFor mean: Conv(32, 32, 3, 1, 1) For variance: Conv(32, 32, 3, 1, 1)\n\nFor mean: FC(In=256, Out=512) For variance: FC(In=256, Out=512)\n\nConvTransp(32, 32, 4, 2, 1), ConvTransp(32, 32, 4, 2, 1), ConvTransp(32, 3, 4, 2, 1)\n\nz1 dims: 512\n\nz2 dims: 32\n\nσx = 0.71\n\nTotal params: 408131\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA.2 ADDITIONAL RESULTS\n\nHere we attached the results for MNIST, as well as the full results for LVAE on SVHN and generalized top-down HVAEs on CIFAR-10.\n\nA.2.1 RESULTS FOR GENERALIZED TOP-DOWN HVAES ON MNIST\n\nWe also evaluate our proposed framework using generalized top-down HVAEs trained on binary MNIST data (i.e., black and white images rather than grayscale).\n\n(see Figure 5) in that optimal IS in MNIST occurs for high R(z1|\n\nWe note that the inception score (IS) behaves different in our MNIST models compared to SVHN z2) rather than high R(z2). This indicates that semantically low-level properties (hand-writing style) of MNIST might have more variation than high level properties (the digit), whereas SVHN images show variation in additional high-level properties such as the background color.\n\nFigure 8: Trade-offs between rates and all metrics we used in Section 5 from the generalized topdown HVAEs trained with MNIST. The results from the standard VAE (i.e. β2 = β1 = 1) and the β-VAE (i.e. β2 = β1) are marked with “ ” and “ ”. The markers “ ” highlight the optimal models selected using convex hull (see Figure 6 for details). The diagonal grid lines are references for equivalent total rates, i.e. points on the same line have the same total rates.\n\n14\n\n0204060020406080100120140R(z1|z2)PSNR1520250204060InceptionScore(IS)2.55.00204060MI020204060R(z2)020406080100120140R(z1|z2)Logistic0.100.520.950204060R(z2)LinearSVM0.090.530.960204060R(z2)RBFSVM0.380.680.980204060R(z2)kNN0.500.740.97Published as a conference paper at ICLR 2023\n\nA.2.2 RESULTS FOR LVAE ON SVHN\n\nFigure 9: Trade-offs between rates and all metrics we used in Section 5 from LVAE trained with SVHN. The results from the standard VAE (i.e. β2 = β1 = 1) and the β-VAE (i.e. β2 = β1) are marked with “ ” and “ ”. The markers “ ” highlight the optimal models selected using convex hull (see Figure 6 for details). The diagonal grid lines are references for equivalent total rates, i.e. points on the same line have the same total rates.\n\n15\n\n02040020406080R(z1|z2)PSNR202502040InceptionScore(IS)4602040MI0202040R(z2)020406080R(z1|z2)Logistic0.180.240.2902040R(z2)LinearSVM0.180.230.2802040R(z2)RBFSVM0.180.480.7702040R(z2)kNN0.120.380.64Published as a conference paper at ICLR 2023\n\nA.2.3 RESULTS FOR GENERALIZED TOP-DOWN HVAES ON CIFAR-10\n\nFigure 10: Trade-offs between rates and all metrics we used in Section 5 from the generalized topdown HVAEs trained with CIFAR-10. The results from the standard VAE (i.e. β2 = β1 = 1) and the β-VAE (i.e. β2 = β1) are marked with “ ” and “ ”. The markers “ ” highlight the optimal models selected using convex hull (see Figure 6 for details). The diagonal grid lines are references for equivalent total rates, i.e. points on the same line have the same total rates.\n\n16\n\n02040600255075100125150R(z1|z2)PSNR15200204060InceptionScore(IS)230204060MI020204060R(z2)0255075100125150R(z1|z2)Logistic0.080.270.470204060R(z2)LinearSVM0.080.270.460204060R(z2)RBFSVM0.110.320.530204060R(z2)kNN0.160.300.44Published as a conference paper at ICLR 2023\n\nB PROOF OF THE BOUND ON CLASSIFICATION ACCURACY\n\nThis section provides a proof of Eq. 11 by reformulating the proof of Proposition 5 in the thesis by Meyen (2016) into the notation used in the present paper. We stress that this section contains no original contribution and is provided only as a convenience to the reader, motivated by reviewer feedback. All credits for this section belong to Meyen (2016).\n\nWe consider an (unknown) true data generative distribution pdata(y, x) for data x with (unobserved) true labels y, and a hierarchical VAE with an inference model qφ( x) of the form of Eq. 4. Focusing on a single layer l of latents, we denote the joint probability over y, x, and zl as\n\nzl} |\n\n{\n\nq(y, x, zl) := pdata(y, x) qφ(zl|\n\nzl) that operates on zl. Denoting its top prediction as ˆy := arg maxy pcls.(y\n\nwhere the marginal qφ(zl| pcls.(y cation accuracy is α := Eq[δy,ˆy], where δ is the Kronecker delta. Theorem 1. The mutual information Iq(y; zl) between the latent representation zl and the true label y under the distribution q defined in Eq. 13 is lower bounded as follows,\n\n(13) x) is defined as usual. We further consider a classifier zl), the classifi- |\n\nx) of qφ(\n\nzl} |\n\nx)\n\n{\n\n|\n\nIq(y; zl) where H2(α) = Hpdata[y]\n\n(14) −\nα) is the entropy of a Bernoulli distribution, log M is the marginal entropy of the true labels, and M denotes the number of classes.\n\nf (α) α log α\n\nf (α) = Hpdata[y]\n\nα) log(M\n\nα) log(1\n\nH2(α)\n\nwith\n\n(1\n\n1)\n\n(1\n\n−\n\n−\n\n−\n\n−\n\n−\n\n−\n\n−\n\n≥\n\n≤\n\nBefore we prove Theorem 1, we note that the function f is strictly monotonically increasing on the relevant interval [maxy pdata(y), 1]. Thus, f is invertible and we obtain the following corollary: Corollary 1. The classification accuracy α is upper bounded as in Eq. 11 of the main text, i.e.,\n\nf −1(Iq(y; zl))\n\nα\n\n≤\n\n≤\n\nf −1(cid:0)E\n\npdata(x)[R(z≥l)](cid:1).\n\n(15)\n\nThe second inequality in Eq. 15 results from the bound Iq(y; zl) ≤\nEq. 10, using the fact that f −1 is monotonically increasing (since f is).\n\nE\n\npdata(x)[R(z≥l)] derived in\n\n|\n\nProof of Theorem 1. We split the mutual information into two contributions,\n\nIq(y; zl) = Hpdata[y]\n\nzl] = Hpdata[y] |\nwhere, as clarified in the second equality, Hq[y entropy of y given zl, and q(zl) and q(y\n\nHq[y\n\n−\n\n− −\nzl] is the expectation over zl of the conditional zl) are marginals and conditionals of q (Eq. 13) as usual.\n\n|\n\nEzl∼q(zl)\n\n(cid:2)E\n\ny∼q(y|zl)[\n\nlog q(y\n\nzl)](cid:3) |\n\n(16)\n\n|\n\ny∼q(y|zl)[\n\nzl] = Ezl∼q(zl)[E\n\nSince Hpdata[y] is fixed by the problem at hand, finding a lower bound on Iq(y; zl) for a given classification accuracy α is equivalent to finding an upper bound on the second term on the right-hand zl)]], with the constraint Eq[δy,ˆy] = α. side of Eq. 16, Hq[y We do this by upper bounding the conditional entropy E zl)] of y given zl for |\nall zl independently, and then taking the expectation over zl ∼ For a fixed latent representation zl, we first split off the contribution to E from y = ˆy, where ˆy = arg maxy pcls.(y\n\nzl)] log q(y |\nzl) is the label that our classifier would predict for zl, |\n(cid:88) q(y = ˆy |\n\nlog q(y\n\nzl)] =\n\n|\n\n−\n\nzl) log q(y = ˆy |\n\nzl) log q(y |\n\ny∼q(y|zl)[\n\ny∼q(y|zl)[\n\ny∼q(y|zl)[\n\n− q(zl).\n\nzl). |\n\nlog q(y\n\nlog q(y\n\n(17)\n\nzl)\n\nq(y\n\n−\n\n−\n\n−\n\n−\n\nE\n\n|\n\ny̸=ˆy\n\n1) labels (y\n\nHere, the second term on the right-hand side resembles the entropy of a distribution over the rezl)) rather than maining (M q(y = ˆy = ˆy), except that the probabilities sum to (1 |\nzl) distributes the zl), this term is maximized if q(y one. Thus, regardless of the value of q(y = ˆy |\nzl)) uniformly over the remaining (M 1) labels, i.e., remaining probability mass (1 1\n\nq(y = ˆy |\n\n−\n\n−\n\nzl)\n\nE\n\ny∼q(y|zl)[\n\nlog q(y\n\nzl)]\n\n|\n\n−\n\n≤ − zl)) + (1 = H2(q(y = ˆy |\n− Plugging Eq. 18 back into Eq. 16, we obtain the bound zl))(cid:3) (cid:2)H2(q(y = ˆy |\n\nEzl∼q(zl)\n\nIq(y; zl)\n\n− zl) log q(y = ˆy q(y = ˆy |\n| zl)) log(M q(y = ˆy |\n\nzl)\n\n(1\n\n−\n\n−\n\n≥\n\nHpdata[y]\n\n1). (19) We arrive at the proposition (Eq. 14) by pulling the concave function H2 out of the expectation using Jensen’s inequality, and by then identifying Ezl∼q(zl)[q(y = ˆy zl)] = q(y = ˆy) = α. |\n\nzl)(cid:3) log(M q(y = ˆy |\n\nEzl∼q(zl)\n\n(cid:2)1\n\n−\n\n−\n\n−\n\n−\n\n| −\nzl)) log q(y = ˆy |\n\n−\n\nq(y = ˆy |\nM 1\n\n−\n\n(18)\n\n1).\n\n−\n\n17\n\n̸",
    "reference": "# Summary Of The Paper\n\nThe authors extend an existing method and develop an approach to control each layer’s contribution to the rate independently in the hierarchy VAE. They identify the most general class of inference models to which their proposed method is applicable. In the experiments, they demonstrate that the proposed method better tunes hierarchical VAEs.\n\n# Strength And Weaknesses\n\nStrength:\n\nThe paper was well written with good structures. \n\nWeakness\n\nThe paper is a simple extension of existing works, and the significance and novelty of the paper are not strong enough for publication at the moment. The paper could be stronger with additional formal theoretical study.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing of the paper is clear and easy to follow.\n\n# Summary Of The Review\n\nThe author could add more theoretical and experimental studies to make the paper stronger. For example, how does the distribution of $\\beta$s affect the performance? How many layers of H-VAEs do we need given specific datasets?\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nIMPLICIT NEURAL SPATIAL REPRESENTATIONS FOR TIME-DEPENDENT PDES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNumerically solving partial differential equations (PDEs) often entails spatial and temporal discretizations. Traditional methods (e.g., finite difference, finite element, smoothed-particle hydrodynamics) frequently adopt explicit spatial discretizations, such as grids, meshes, and point clouds, where each degree-offreedom corresponds to a location in space. While these explicit spatial correspondences are intuitive to model and understand, these representations are not necessarily optimal for accuracy, memory-usage, or adaptivity. In this work, we explore implicit neural representation as an alternative spatial discretization, where spatial information is implicitly stored in the neural network weights. With implicit neural spatial representation, PDE-constrained time-stepping translates into updating neural network weights, which naturally integrates with commonly adopted optimization time integrators. Our approach requires neither training data nor training/testing separation. Our method is the solver itself, just like the classical PDE solver. We validate our approach on a variety of classic PDEs with examples involving large elastic deformations, turbulent fluids, and multi-scale phenomena. While slower to compute than traditional representations, our approach exhibits higher accuracy, lower memory consumption, and dynamically adaptive allocation of degrees of freedom without complex remeshing.\n\n1\n\nINTRODUCTION\n\nMany science and engineering problems can be formulated as spatiotemporal partial differential equations (PDEs),\n\nF (f , ∇f , ∇2f , . . . , ̇f , ̈f , . . .) = 0,\n\nf (x, t) : Ω × T → Rd .\n\n(1)\n\nwhere Ω ∈ Rm and T ∈ R are the spatial and temporal domains, respectively. Examples include the inviscid Navier-Stokes equations for fluid dynamics and the elastodynamics equation for solid mechanics.\n\nTo numerically solve these PDEs, we oftentimes introduce temporal discretizations, {tn}T n=0, where T is the number of temporal discretization samples and ∆t = tn+1 − tn is the time step size. The solution to Equation (1) then becomes a list of spatially dependent vector fields: {f n(x)}T\n\nn=0.\n\nTraditional approaches represent these spatially dependent vector fields using grids, meshes, or point clouds. For example, the grid-based linear finite element method (Hughes, 2012) defines a shape function N i on each grid node and represents the spatially dependent vector field as f n(x) = (cid:80)P\n\ni N i, where P is the number of spatial samples.\n\ni=1 f n\n\nWhile widely adopted in scientific computing applications, these traditional spatial representations are not without drawbacks:\n\n1. Spatial discretization errors abound in fluid simulations as artificial numerical diffusion (Lantz, 1971), dissipation (Fedkiw et al., 2001), and viscosity (Roache, 1998). These errors also appear in solid simulations as inaccurate collision resolution (M ̈uller et al., 2015) and numerical fractures (Sadeghirad et al., 2011).\n\n2. Memory usage spikes with the number of spatial samples P (Museth, 2013).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: 1D advection example: A Gaussian-shaped wave initially centered at x = −1.5 moves rightward with a constant velocity of 0.25. From left to right, we show mean absolute error plot over time and solutions at t = 0s, t = 3s and t = 12s, respectively. The solution from grid-based finite difference method (green) tends to diffuse over time. PINN (yellow), trained within temporal range 0 ∼ 3s, fails to generalize for t = 12s. Our solution (blue) approximates the ground truth (grey) the best over time. All three representations have the same memory footprint: our approach and PINN (Raissi et al., 2019) both use α = 2 hidden layers of width β = 20, and the finite difference grid resolution is 901.\n\n3. Adaptive meshing (Narain et al., 2012) and data structures (Setaluri et al., 2014) can reduce memory footprints but are often computationally expensive and challenging to implement.\n\nWe alleviate these limitations by exploring implicit neural representation (Park et al., 2019; Chen & Zhang, 2019; Mescheder et al., 2019) as an alternative spatial representation for PDE solvers. Unlike traditional representations that explicitly discretize the spatial vector via spatial primitives (e.g., points), neural spatial representations implicitly encode the field through neural network weights. In other words, the field is parameterized by a neural network (typically multilayer perceptrons), i.e., f n(x) = f θn (x) with θn being the network weights. As such, the memory usage for storing the spatial field is independent of the number of spatial samples, but rather it is determined by the number of neural network weights. We show that under the same memory constraint, implicit neural representations indeed achieve higher accuracies than traditional discrete representations. Furthermore, implicit neural representations are adaptive by construction (Xie et al., 2021), allocating the network weights to resolve field details at any spatial location without changing the network architecture.\n\nViewed from the lens of optimization-based time integrators, our PDE solver seeks neural network weights that optimize an incremental potential over time (Kane et al., 2000b). Our solver does not employ the so-called training/testing split commonly appearing in many neural-network-based PDE approaches (Sanchez-Gonzalez et al., 2020; Li et al., 2020b). Our approach is the solver itself and does not require training in the machine learning sense. As such, we avoid using the word “training” in the exposition but rather use “optimizing”. We employ exactly the same “optimization” integrator formulation as the classical solvers (e.g., finite element method (Bouaziz et al., 2014)).\n\nWe compare the proposed solver to grid, mesh, and point cloud representations on time-dependent PDEs from various disciplines, and find that our approach trades wall-clock runtime in favor of three benefits: lower discretization error, lower memory usage, and built-in adaptivity.\n\n2 RELATED WORKS\n\nMany prior works have explored representing continuous vector fields with neural networks. Here we highlight two lines of work: implicit neural representation and physics informed neural network.\n\nImplicit Neural Representation uses neural networks to parameterize spatially-dependent functions. It has successfully captured the radiance fields (Mildenhall et al., 2020) and the signed distance fields (Park et al., 2019) in computer vision and graphics settings. It has also captured the solutions of strictly spatially dependent PDEs from elastostatics (Zehnder et al., 2021), elliptic PDEs (Chiaramonte et al., 2013), and geometry processing (Yang et al., 2021). Chen et al. (2021), Pan et al. (2022), and Chen et al. (2022) also explore neural networks as spatial representations for dimension reduction. Dupont et al. (2022) develops a machine learning technique operating on data presented as implicit neural representations. Memory consumptions of traditional representations,\n\n2\n\n1.0t = 12st = 3st = 0sxu-2.00-1.000.0xu-1.25-0.250.01.0xu1.002.000.01.0OursGridPINNGTMean Absolute Error0.000.050.100.15Time (s)036912OursGridPINN0.50.50.5Under review as a conference paper at ICLR 2023\n\nsuch as grids and point clouds, scale poorly with spatial resolutions. Adaptive discretizations can reduce memory but their generations are expensive. By contrast, neural representations are adaptive by construction and can use their representation capacities at arbitrary locations of interest without memory increases or data structures alterations. We refer to the recent review by Xie et al. (2021) for additional contexts.\n\nPhysics Informed Neural Network (PINN) is a powerful tool for solving PDEs. Traditional numerical methods, such as finite difference and finite element methods, represent tensor fields with well-studied polynomial basis functions (Hughes, 2012) constructed on meshes. Instead, PINN represents tensor fields with neural networks and converts the PDE solution process into finding network weights via PDE-based loss functions. Since the pioneering works by Raissi et al. (2019); Sirignano & Spiliopoulos (2018); Lagaris et al. (1998); Dissanayake & Phan-Thien (1994), PINN has been shown to excellently model forward simulation (Shin et al., 2020; Hennigh et al., 2021; Lu et al., 2021a; Krishnapriyan et al., 2021), inverse design (Raissi et al., 2020; Mao et al., 2020; Mishra & Molinaro, 2022), optimal control (Mowlavi & Nabi, 2021), and uncertain quantification (Lye et al., 2020). PINN has found success in a wide range of application domains, including turbulence (Hennigh et al., 2021), elasticity (Rao et al., 2020), acoustics (Sitzmann et al., 2020), and topology optimization (Zehnder et al., 2021). Due to its mesh-free nature, PINN can robustly handle high-dimensional PDEs. The recent review by Karniadakis et al. (2021) offers more details.\n\nWhen it comes to using neural networks to represent vector fields, we face an important design choice as which dimension of the vector field is represented through the network. In the case of spatiotemporal vector fields, we have the choice of representing both the spatial and temporal dimensions via neural networks; we can also represent just the spatial dimension or just the temporal dimension with a network.\n\nWe observe that the spatial variable x is oftentimes bounded, e.g., a fixed geometry with well-defined boundaries. However, the temporal variable t can be unbounded, e.g., in a virtual reality application where the user interacts with a physical environment indefinitely (Sun et al., 2018). Modeling the additional temporal dimension also puts extra burden on the network. Motivated by these observations, we opt to treat the spatial and the temporal dimensions differently. In particular, we use the neural network strictly as a spatial representation and do not consider the temporal dimension as an input to the network. We then evolve this spatial representation by updating the network weights θn (See Figure 2), potentially for an indefinite amount of time. Such an approach is different from standard PINN that takes both the spatial dimension x and the temporal dimension t as an input to the network (Raissi et al., 2019; Karniadakis et al., 2021) which cannot resolve PDE solution outside a pre-defined temporal range (Kim et al., 2021) (See Figure 1).\n\nDu & Zaki (2021); Bruna et al. (2022); Krishnapriyan et al. (2021) also explore evolution of neural network weights over time, with the goal of resolving PINN’s limited time range as well as solving high-dimensional problems that classical solvers often suffer. Our work differs from these works by focusing on low-dimensional settings (1D-3D) that heavily rely on classical solvers (e.g., finite element method). Our primary goal is to understand if we only replace classical solver’s spatial representation with a neural network, while keeping the rest unchanged (e.g., time integrator, boundary condition), what tradeoffs do we get?\n\nOptimization Time Integrators. Since our approach only replaces the spatial representations of traditional numerical solvers with neural networks while keeping the rest of the solver intact, it is compatible with any classical time integration schemes (e.g., implicit Euler). In particular, we formulate time integration as an energy minimization problem (Radovitzky & Ortiz, 1999; Kane et al., 2000b; Marsden & West, 2001; Kharevych et al., 2006). These integrators find wide applications in PDE solvers on traditional representations, such as grids (Batty et al., 2007), tetrahedral meshes (Bouaziz et al., 2014), and point clouds (Gast et al., 2015). In the case of neural spatial representations, time integration translates into optimizing the neural network weights at every time step.\n\nMachine Learning for PDEs is an emerging field with exciting techniques, such as graph neural network (Sanchez-Gonzalez et al., 2020), neural operator (Li et al., 2020b;c), and DeepONet (Lu et al., 2019). These techniques usually train on a dataset and are then validated on a test dataset. However, due to the machine learning nature, these methods’s time-stepping schemes neither enforce PDE constraints at test time (Pfaff et al., 2020) nor generalize to scenarios (e.g., initial conditions, boundary conditions) drastically different from the training cases (Wang & Perdikaris, 2021). As a\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nmajor point of departure, our approach does not employ any training data. There is not a so-called training/inference separation in our approach. Our method is the solver itself, just like the classical solvers (e.g., FEM). As such, we enjoy classical solver’s unparalleled generalizability and explicit PDE constraints. See Table 1 for a comparision of these techniques. Relatedly, Wandel et al. (2020) also proposes a data-free approach but still employs a training / testing split.\n\n3 METHOD: TIME-STEPPING ON NEURAL SPATIAL REPRESENTATIONS\n\nOur goal is to solve time-dependent PDEs on neural-network-based spatial representations. In Section 3.1, we first discuss representing spatial vector fields with neural networks. Afterward, we will describe our time-stepping technique that evolves from one neural spatial representation to another.\n\n3.1 NEURAL NETWORKS AS SPATIAL REPRESENTATIONS\n\nWe parameterize each of the time-discretized spatial vector fields with a neural network: f n = f θn, where θn are the neural network weights at time tn. Specifically, the field quantity at an arbitrary spatial location x ∈ Ω can be queried via network inference f θn (x).\n\nTraditional representations explicitly discretize the spatial vector field using primitives such as points, tetrahedra, or voxels. These primitives explicitly correspond to spatial locations due to their compactly supported basis functions (Hughes, 2012). By contrast, neural spatial representations implicitly encode the vector field via neural network weights. These weights do not directly correspond to specific spatial locations. Instead, each weight affects the vector field globally. Such global support is also an attribute of spectral methods (Canuto et al., 2007a;b). Compared to spectral methods, our approach does not need to know the required complexity ahead of time in order to determine the ideal basis functions (Xie et al., 2021). Our neural representation automatically optimizes its parameters to where field detail is present.\n\nWhereas memory consumption of traditional explicit representations scales poorly with the number of spatial samples, memory consumption for implicit neural representations is independent of the number of spatial samples (Xie et al., 2021). Rather, memory use is determined by the number of neural network weights.\n\nNetwork Architecture Following the implicit neural representation literature, we adopt a multilayer perceptron (MLP) architecture with SIREN activation function for its accuracy and quick convergence speed advantages (Sitzmann et al., 2020). Each MLP has a total of α hidden layers, each layer of width β. The specific choice of these hyper-parameters will be described in Section 4.\n\nSpatial Gradients Traditional spatial representations (e.g., the finite element method) compute spatial gradients via basis functions. Higher-order gradients require higher-order basis functions. By contrast, a neural spatial representation is C∞ by construction. We evaluate their gradients via computation-graph-based auto-differentiation with respect to the input (not the weights).\n\n3.2 TEMPORAL EVOLUTION\n\nGiven previous-time spatial vector fields {f n(x)}n the next time-step (tn+1) vector field by optimizing\n\nk=0, optimization-based time integrators compute\n\nf n+1 = argmin\n\n(cid:88)\n\nf n+1\n\nx∈M⊂Ω\n\nI(∆t, {f k}n+1\n\nk=0 , {∇f k}n+1\n\nk=0 , {∇2f k}n+1\n\nk=0 , . . .) .\n\n(2)\n\nTraditional time integrators, whether explicit and implicit, can be expressed in optimization forms (Kharevych et al., 2006). Furthermore, this optimization formulation applies to any spatial representation, and has been explored thoroughly for traditional discretizations (Batty et al., 2007; Bouaziz et al., 2014; Gast et al., 2015), which is defined over a finite number of the spatial integration samples M := {xj ∈ Ω | 1 ≤ j ≤ |M|}, e.g., grids or meshes. Applying this formulation to a neural spatial representation, we optimize for\n\nθn+1 = argmin\n\n(cid:88)\n\nθn+1\n\nx∈M⊂Ω\n\nI(∆t, {f θk }n+1\n\nk=0 , {∇f θk }n+1\n\nk=0 , {∇2f θk }n+1\n\nk=0 , . . .)\n\n(3)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nwhere {θk}n k=0 are the (fixed, not variable) neural network weights from previous time steps. Figure 2 illustrates our time integration process. The particular choice of the objective function I depends on the PDE of interest. In all the examples presented in this work, we solve this time-integration optimization problem via Adam (Kingma & Ba, 2014), a firstorder stochastic gradient descent method.\n\nFigure 2: Time integration. We represent the field of interest using a neural network f θn, whose weights θn are updated at each timestep In via an optimization problem (Equation (3)). this case, the spatial domain Ω is the interior of the initial object and the represented field f is the deformation map. The governing PDE is the elastodynamics equation (see Section 4.3).\n\nSpatial Sampling Explicit spatial representations (e.g., tetrahedra mesh) are often tied to a particular spatial sampling; remeshing is sometimes possible, but can also have drawbacks, especially in higher dimensions (Alliez et al., 2002; Narain et al., 2012). By contrast, implicit spatial representations allow for arbitrary spatial sampling by construction (Equation (3)). Following Sitzmann et al. (2020), we dynamically sample M during optimization. For every gradient descent iteration in every time step, we use a stochastic sample set M from the spatial domain Ω; M corresponds to the “mini-batch” in stochastic gradient descent, with batch size |M|. By directly drawing samples from the entire spatial domain Ω, our approach is reminiscent of mesh-free Monte Carlo methods (Sawhney & Crane, 2020).\n\nBoundary Condition PDEs are typically accompanied by spatial (e.g., Dirichlet or Neumann) boundary conditions, which we formulate as additional penalty terms in the objective Equation (3),\n\nθn+1 = argmin\n\n(cid:88)\n\nθn+1\n\n+λ\n\nx∈M⊂Ω (cid:88)\n\nI(∆t, {f θk }n+1\n\nk=0 , {∇f θk }n+1\n\nk=0 , {∇2f θk }n+1\n\nk=0 , . . .)\n\nC(f θn+1 , ∇f θn+1 , ∇2f θn+1 , . . .) ,\n\n(4)\n\nxb∈Mb⊂∂Ω\n\nwhere λ is the weighting factor and ∂Ω is the boundary of the spatial domain. The particular choice of the boundary constraint function C depends on the problem of interest.\n\nInitial Condition The neural network is initialized using the given initial condition, i.e., the field value at time t = 0, by optimizing\n\nθ0 = argmin\n\n(cid:88)\n\nθ0\n\nx∈M⊂Ω\n\n||f θ0(x) − ˆf 0(x)||2 2 ,\n\n(5)\n\nwhere ˆf 0 is the given initial condition. Similar to Equation (3), we solve this optimization problem using Adam (Kingma & Ba, 2014) and stochastically sample M at each gradient descent iteration.\n\n4 EXPERIMENTS\n\nIn this section, we evaluate our method on three classic time-dependent PDEs: the advection equation, the inviscid Navier-Stokes equation and, the elastodynamics equation. For each problem, we first discuss the continuous PDE and the specific objective function I for temporal evolution (recall Equation (3)). Then we demonstrate the advantages of our approach by comparing with baselines using discrete spatial representations (i.e. a grid, tetrahedral mesh, or point cloud). We refer readers to Appendices B and C for other implementation details (e.g., initial and boundary conditions) and additional results. The temporal evolutions of the PDEs are best illustrated by the supplementary video.\n\n5\n\nEqn. (3)forcespatial samplefield value: network weights at stepUnder review as a conference paper at ICLR 2023\n\nFigure 3: 2D Taylor-Green vortex simulation. Left: mean squared error of the velocity field for 100 timesteps. Right: velocity magnitude of solutions from the ground truth, ours, and the grid-based method at timestep n = 100. Under the same memory usage (for storing the spatial representation), our solution has a significantly smaller error than the grid-based method.\n\n4.1 ADVECTION EQUATION\n\nConsider the classic 1D advection equation,\n\n∂u ∂t\n\n+ (a · ∇)u = 0 ,\n\n(6)\n\nwhere a is the advection velocity, and the vector field of interest is the advected quantity f = u. It is well known that traditional spatial representations, such as grid-based finite differences, exhibit numerical dissipation for the advection equation (Courant et al., 1952; Selle et al., 2008).\n\nTime Integration We adopt the same time integration scheme in both the traditional representation and ours. Choosing the energy-preserving midpoint method (Mullen et al., 2009) yields the time integration operator\n\nI = ∥\n\nun+1(x) − un(x) ∆t\n\n+ (a · ∇)(\n\nun+1(x) + un(x) 2\n\n)∥2\n\n2 .\n\n(7)\n\nResults Figure 1 compares our results with those of grid-based finite differences and PINN (Raissi et al., 2019), subject to equal memory usage of the three methods. A Gaussian-shaped wave moves with constant velocity a = 0.25. Our approach uses α = 2 hidden layers of width β = 20, and the finite difference grid resolution is 901. We set PINN to use the same network architecture (with SIREN activation) as ours. For ours and grid-based methods, we set ∆t = 0.05. PINN does not require ∆t but needs a pre-specified temporal range for training. For this temporal range, we use [0, 3]. As shown in Figure 1, the solution from the grid-based method diffuses over time due to its spatial discretization. While PINN can accurately capture the result up to t = 3s, it fails to produce meaningful solutions beyond its trained temporal range (see t = 12s) (Kim et al., 2021). By contrast, our solution does not suffer from numerical dissipation and agrees well with the ground truth at all frames (see the error plot in Figure 1).\n\n4.2\n\nINVISCID NAVIER-STOKES EQUATIONS\n\nIn the incompressible and inviscid Navier-Stokes Equations\n\nρf (\n\n∂u ∂t\n\n+ u · ∇u) = −∇p + ρf g,\n\n∇ · u = 0,\n\n(8)\n\nthe vector field of interest is the fluid velocity field f = u; p is the pressure, g is the external force, and ρf is the fluid density. In our experiments, we consider ρf = 1 and g = 0. The pressure field p is represented with another MLP network.\n\nTime Integration We apply the Chorin-style operator splitting scheme (Chorin, 1968; Stam, 1999) to both the neural spatial and finite-difference grid representations. The scheme involves three sequential steps: advection (adv), pressure projection (pro), and velocity correction (cor).\n\nAdvection uses a semi-Lagrangian method, encoded by the operator (Staniforth & Cˆot ́e, 1991)\n\nI adv = ∥uadv\n\nn+1(x) − un(xbacktrack)∥2\n\n2 ,\n\n(9)\n\n6\n\n1.00.00.5OursGround truthGrid-basedMean Squared Error00.0040.0070.0110.014Timesteps020406080100OursGrid-basedUnder review as a conference paper at ICLR 2023\n\nn+1. The backtracked location is given by whose optimization yields the advected velocity uadv xbacktrack = x − ∆tun(x). While traditional spatial representations compute the backtracked velocity using interpolation (e.g., linear basis function), our approach requires no interpolation, only direct evaluation via network inference at the location xbacktrack.\n\nPressure projection is encapsulated by the operator\n\nI pro = ∥∇2pn+1(x) − ∇ · uadv\n\nn+1(x)∥2 2.\n\n(10)\n\nPlugging I pro into the optimization solver, we obtain the pressure pn+1 that enforces incompressibility. Note that the MLP that represents the velocity field uadv is kept fixed in this step.\n\nVelocity correction is formulated by the operator I cor = ∥un+1 − (uadv\n\nn+1(x) − ∇pn+1(x))∥2\n\n2 ,\n\n(11)\n\nwhich adds the pressure gradient to the advected velocity yielding the incompressible velocity un+1.\n\nResults We first test our method on the 2D Taylor-Green vortex with zero viscosity (Taylor & Green, 1937; Brachet et al., 1983). The closed-form analytical solution is given by: u(x, t) = (sin x cos y, − cos x sin y) for x ∈ [0, 2π] × [0, 2π]. To compare under the same memory usage (for storing the velocity field), we use α = 3 hidden layers of width β = 32 for our MLP and set grid resolution to 48 for the grid-based projection method. We set ∆t = 0.05 and execute both methods for 100 timesteps. In Figure 3, we show the mean squared error of the solved velocity field over time. This example demonstrates that our method excellently preserves a stationary solution. Compared to the grid-based method, our method has less diffusion and achieves higher accuracy.\n\nFor discrete grid representation, efficiently capturing multi-scale details usually requires difficultto-implement adaptive data structures (Setaluri et al., 2014). Instead, implicit neural representations are adaptive by construction (Xie et al., 2021) and enable us to capture more details under the same memory storage. We setup an example where the initial velocity field is composed by two Taylor-Green vortices of different scales (see Figure 8 for illustration). We compare our approach with PINN and the grid-based projection method under the same memory constraint for storing the spatial representations. Specifically, our approach and PINN uses a MLP with α = 3 hidden layers of width β = 32 and the grid-based projection method uses resolution 48. We execute our approach and the grid-based method for 50 timesteps with ∆t = 0.05, and train PINN with the same temporal range of 2.5 seconds. Using the solved velocity field, we advect a density field to visualize the amount of fine details captured by different representations. As shown in Figure 4, we are able to capture the fine details of the smaller vortex and best approximate the reference solution. The grid-based method (resolution 48) suffers from severe dissipation and fails to capture the vorticity. PINN is unable to correctly capture this two-vortices field and we found its training loss remains high (∼ 1e−3) after convergence. This is in agreement with previous findings (Chuang & Barba, 2022) that suggest PINN approaches have difficulty solving inviscid Navier-Stokes equations for non-trivial examples involving turbulence.\n\n4.3 ELASTODYNAMICS EQUATION\n\nIn the third experiment, we study the Elastodynamics equations\n\n ̈φ = ∇ · P (F ) + ρ0b that describe the motions of deformable solids (Gonzalez & Stuart, 2008). The vector field of interest is the deformation map f = φ. Here ρ0 is the density in the reference space, P is the first PiolaKirchhoff stress, F = ∇φ is the deformation gradient, ̇φ and ̈φ are the velocity and acceleration, and b is the body force.\n\n(12)\n\nρ0\n\nWe assume a hyper-elasticity constitutive law, i.e., P = ∂Ψ In particular, we assume a variant of the stable Neo-Hookean energy (Smith et al., 2018)\n\n∂F , where Ψ is the energy density function.\n\nΨ =\n\nλ 2\n\ntr2(Σ − I) + μ(det(F ) − 1)2,\n\n(13)\n\nwhere λ and μ are the first and second lame parameters, Σ are the singular values of the deformation gradient F , and det(F ) is the determinant of the deformation gradient F . When μ = 0, the elastic energy recovers the As-Rigid-As-Possible energy (Sorkine & Alexa, 2007).\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Two vortices of different scales. We show the advected density field after 2.5 seconds from the reference (top-left), our method (top-right), the grid-based method of resolution 48 (bottom-left) and PINN (bottom-right). The reference is obtained by running the high-resolution grid-based method (we use resolution 1024). Our MLP (α = 3, β = 32) has the same memory footprint as grids of resolution 48. PINN uses the same MLP network as ours. Under the same memory constraint, our approach suffers from less dissipation, captures more vorticity, and best resembles the reference solution, whose grids takes ∼ 450× memory compared to our network. See Figure 8 for the initial condition of this example.\n\nFigure 5: Elastic tension test. We compare the quasi-static simulation using our implicit neural representation (left) with the mesh representation (middle), and the point cloud representation (right). Our method handles large deformations matching the mesh-based finite element method (FEM), while the point cloud based material point method (MPM) suffers from incorrect numerical fracture.\n\nTime Integration We apply the implicit Euler time integration scheme (Gast et al., 2015; Kane et al., 2000a) to the (1) tetrahedral finite element, (2) material point method, and (3) our neural representation, using the operator\n\nI =\n\n1 2\n(cid:124)\n\nn+1\n\nρ0( ̇φ\n\nn\n\n− ̇φ\n\nn+1\n\n)T ( ̇φ\n\nn\n\n− ̇φ\n\n+ Ψ(φn+1)\n\n)\n\n(cid:123)(cid:122) kinematic energy\n\n(cid:125)\n\n(cid:124)\n\n(cid:125)\n\n(cid:123)(cid:122) elastic energy\n\n,\n\n− ρ0bT φn+1 (cid:125) (cid:123)(cid:122) (cid:124) external force potential\n\n(14)\n\nn+1\n\nwhere ̇φ = (φn+1 − φn)/∆t, ρ0 is the density, b is the external force. We can also incorporate boundary conditions, e.g., positional and contact constraints, by introducing additional energy terms (Bouaziz et al., 2014; Li et al., 2020a) (see Appendix B.4).\n\nResults We first compare our implicit neural representation to the traditional tetraheral mesh representation (Finite Element Method, FEM (Hughes, 2012; Reddy, 2019)) and the point cloud rep-\n\n8\n\nReferenceGrid-based (resolution 48)OursPINNDeformedUndeformedmesh-based (FEM)Ourspoint-cloud-based (MPM)Under review as a conference paper at ICLR 2023\n\nFigure 6: When an elastic square collides with a circle, the finite element mesh (top) conforms poorly at the interface compared to the neural representation (bottom), for equal memory usage.\n\nFigure 7: A bunny collides with the ground in 3D. Our neural representation (green, left) captures more intricate geometry details and complex dynamics compared to the traditional tetrahedral mesh representation (blue, right) under the same memory usage.\n\nresentation (Material Point Method, MPM (Sulsky et al., 1995; Jiang et al., 2016)). We use α = 3 hidden layers of width β = 68 for our MLP, which takes the same memory as the FEM mesh (0.8K vertices, 1.5K faces) and MPM point cloud (1.7K points). As shown in Figure 5 and Figure 11, our method is capable of handling the large elastic deformations and matches the result of the traditional mesh-based method (FEM), while the point-cloud-based method (MPM) suffers from incorrect numerical fracture due to its meshless nature. To avoid these fractures, meshless methods require sophisticated modifications of the underlying kernel and basis functions (Gray et al., 2001; Su et al., 2022).\n\nBy virtue of its implicit nature, our representation is able to represent more intricate details compared to the traditional explicit representations under the same memory usage. In Figure 6, we show that our implicit neural representation allows the deformed square to gracefully fit the boundary of the sphere during the non-trivial collision. In contrast, the traditional mesh-based representation struggles to produce smooth result due to its insufficient mesh resolution. To alleviate such artifacts, the traditional mesh-based representation either needs to increase resolutions thusly inducing higher memory cost or conducts complex remeshing (Narain et al., 2012). In Figure 7, our implicit neural representation allows for more complex dynamics and fine geometry details compared to the traditional tetrahedral mesh representation.\n\nNote that we adopt the same collision detection and handling strategy for both the neural representation and the mesh-based representation (FEM). Specifically, we use a spring-like penalty force and the corresponding energy to move the collided point out of its collision surface, similar to (McAdams et al., 2011; Xian et al., 2019). Since our approach and FEM share the same time integration scheme and the same collision handling method, the difference reported in Figure 6 and Figure 7 strictly stems from the underlying spatial representations.\n\nThese advantages extend to other complex 3D simulations. Figure 2 and Figure 13 depict a cow and statue deforming as they collide with the ground, exhibiting complex geometry, and rich contactinduced deformations.\n\n9\n\n0 sec0.2 sec0.6 sec0.8 secmesh-based(FEM)Ours......forceforce0 sec0.2 sec0.4 secforceUnder review as a conference paper at ICLR 2023\n\n5 DISCUSSION AND CONCLUSION\n\nIn this work, we explore implicit neural representations as spatial representations for numerically modeling time-dependent PDEs. This representation naturally integrates with widely adopted optimization-based time integrators. PDE solvers with neural spatial representation offers improved accuracy, reduced memory, and automatic adaptivity compared to traditional explicit representations such as a mesh, grid, or point cloud.\n\nWhile offering important benefits, neural-spatial-representation-based PDE time-stepping requires longer wall-clock computation time than existing methods (see also Table 1 by Zehnder et al. (2021) and Section 7 by Yang et al. (2021)). Optimizing neural networks weights takes longer than optimizing grid values even if there are fewer number of neural network weights than the number of grid nodes. For instance, for the bunny example (Figure 7), our neural network optimization takes around 30 minutes per timestep while the corresponding FEM simulation takes less than 1 minute. Future work therefore lies in exploring advanced training techniques that reduce training time (Liu et al., 2020; Martel et al., 2021; Takikawa et al., 2021). In particular, M ̈uller et al. (2022) offer a promising direction where they show that we can reduce implicit neural representation training time from hours to seconds via advanced data structures and optimized implementation.\n\nOur work demonstrates the effectiveness of neural spatial representations in solving time-dependent PDEs and observes empirical convergence under refinement (see Figure 12). Future work should consider theoretical analysis (Mishra & Molinaro, 2022) on convergence and stability. More challenging physical phenomena, such as turbulence (Wilcox et al., 1998), intricate contacts (Johnson & Johnson, 1987), and thin shells (Pfaff et al., 2020), are also important future directions. Currently, our work enforces “soft” boundary conditions. Enforcing “hard” boundary conditions on a neural architecture is another exciting direction (Lu et al., 2021b).\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nPierre Alliez, Mark Meyer, and Mathieu Desbrun. Interactive geometry remeshing. ACM Transac-\n\ntions on Graphics (TOG), 21(3):347–354, 2002.\n\nChristopher Batty, Florence Bertails, and Robert Bridson. A fast variational framework for accurate\n\nsolid-fluid coupling. ACM Transactions on Graphics (TOG), 26(3):100–es, 2007.\n\nSofien Bouaziz, Sebastian Martin, Tiantian Liu, Ladislav Kavan, and Mark Pauly. Projective dynamics: Fusing constraint projections for fast simulation. ACM transactions on graphics (TOG), 33(4):1–11, 2014.\n\nMarc E Brachet, Daniel I Meiron, Steven A Orszag, BG Nickel, Rudolf H Morf, and Uriel Frisch. Small-scale structure of the taylor–green vortex. Journal of Fluid Mechanics, 130:411–452, 1983.\n\nJoan Bruna, Benjamin Peherstorfer, and Eric Vanden-Eijnden. Neural galerkin scheme with active\n\nlearning for high-dimensional evolution equations. arXiv preprint arXiv:2203.01360, 2022.\n\nClaudio Canuto, M Yousuff Hussaini, Alfio Quarteroni, and Thomas A Zang. Spectral methods:\n\nfundamentals in single domains. Springer Science & Business Media, 2007a.\n\nClaudio Canuto, M Yousuff Hussaini, Alfio Quarteroni, and Thomas A Zang. Spectral methods: evolution to complex geometries and applications to fluid dynamics. Springer Science & Business Media, 2007b.\n\nPeter Yichen Chen, Maurizio Chiaramonte, Eitan Grinspun, and Kevin Carlberg. Model reduction for the material point method via an implicit neural representation of the deformation map. arXiv preprint arXiv:2109.12390, 2021.\n\nPeter Yichen Chen, Jinxu Xiang, Dong Heon Cho, GA Pershing, Henrique Teles Maia, Maurizio Chiaramonte, Kevin Carlberg, and Eitan Grinspun. CROM: Continuous reduced-order modeling of PDEs using implicit neural representations. arXiv preprint arXiv:2206.02607, 2022.\n\nZhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5939–5948, 2019.\n\nM Chiaramonte, M Kiener, et al. Solving differential equations using neural networks. Machine\n\nLearning Project, 1, 2013.\n\nAlexandre Joel Chorin. Numerical solution of the navier-stokes equations. Mathematics of compu-\n\ntation, 22(104):745–762, 1968.\n\nPi-Yueh Chuang and Lorena A Barba. Experience report of physics-informed neural networks in\n\nfluid simulations: pitfalls and frustration. arXiv preprint arXiv:2205.14249, 2022.\n\nRichard Courant, Eugene Isaacson, and Mina Rees. On the solution of nonlinear hyperbolic differential equations by finite differences. Communications on pure and applied mathematics, 5(3): 243–255, 1952.\n\nMWMG Dissanayake and Nhan Phan-Thien. Neural-network-based approximations for solving partial differential equations. communications in Numerical Methods in Engineering, 10(3):195– 201, 1994.\n\nYifan Du and Tamer A Zaki. Evolutional deep neural network. Physical Review E, 104(4):045303,\n\n2021.\n\nEmilien Dupont, Hyunjik Kim, SM Eslami, Danilo Rezende, and Dan Rosenbaum. From data arXiv preprint\n\nto functa: Your data point is a function and you should treat it like one. arXiv:2201.12204, 2022.\n\nRonald Fedkiw, Jos Stam, and Henrik Wann Jensen. Visual simulation of smoke. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pp. 15–22, 2001.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nTheodore F Gast, Craig Schroeder, Alexey Stomakhin, Chenfanfu Jiang, and Joseph M Teran. Optimization integrator for large time steps. IEEE transactions on visualization and computer graphics, 21(10):1103–1115, 2015.\n\nOscar Gonzalez and Andrew M Stuart. A first course in continuum mechanics, volume 42. Cam-\n\nbridge University Press, 2008.\n\nJames P Gray, Joseph J Monaghan, and RP1021 Swift. Sph elastic dynamics. Computer methods in\n\napplied mechanics and engineering, 190(49-50):6641–6662, 2001.\n\nOliver Hennigh, Susheela Narasimhan, Mohammad Amin Nabian, Akshay Subramaniam, Kaustubh Tangsali, Zhiwei Fang, Max Rietmann, Wonmin Byeon, and Sanjay Choudhry. Nvidia simnetTM: An ai-accelerated multi-physics simulation framework. In International Conference on Computational Science, pp. 447–461. Springer, 2021.\n\nYuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, and Fr ́edo Durand. Taichi: a language for high-performance computation on spatially sparse data structures. ACM Transactions on Graphics (TOG), 38(6):1–16, 2019.\n\nThomas JR Hughes. The finite element method: linear static and dynamic finite element analysis.\n\nCourier Corporation, 2012.\n\nChenfanfu Jiang, Craig Schroeder, Joseph Teran, Alexey Stomakhin, and Andrew Selle. The maIn ACM SIGGRAPH 2016 Courses,\n\nterial point method for simulating continuum materials. SIGGRAPH ’16. Association for Computing Machinery, 2016.\n\nKenneth Langstreth Johnson and Kenneth Langstreth Johnson. Contact mechanics. Cambridge\n\nuniversity press, 1987.\n\nC. Kane, J. E. Marsden, M. Ortiz, and M. West. Variational integrators and the newmark algorithm for conservative and dissipative mechanical systems. International Journal for Numerical Methods in Engineering, 49(10):1295–1325, 2000a.\n\nCouro Kane, Jerrold E Marsden, Michael Ortiz, and Matthew West. Variational integrators and the newmark algorithm for conservative and dissipative mechanical systems. International Journal for numerical methods in engineering, 49(10):1295–1325, 2000b.\n\nGeorge Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang.\n\nPhysics-informed machine learning. Nature Reviews Physics, 3(6):422–440, 2021.\n\nLiliya Kharevych, W Wei, Yiying Tong, Eva Kanso, Jerrold E Marsden, Peter Schr ̈oder, and Matthieu Desbrun. Geometric, variational integrators for computer animation. Eurographics Association, 2006.\n\nJungeun Kim, Kookjin Lee, Dongeun Lee, Sheo Yon Jhin, and Noseong Park. Dpm: a novel training method for physics-informed neural networks in extrapolation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 8146–8154, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nAditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks. Advances in Neural Information Processing Systems, 34:26548–26560, 2021.\n\nIsaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks, 9(5):987–1000, 1998.\n\nRB Lantz. Quantitative evaluation of numerical diffusion (truncation error). Society of Petroleum\n\nEngineers Journal, 11(03):315–320, 1971.\n\nDavid I.W. Levin. Bartels: A lightweight collection of routines for physics simulation, 2020.\n\nhttps://github.com/dilevin/Bartels.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nMinchen Li, Zachary Ferguson, Teseo Schneider, Timothy R Langlois, Denis Zorin, Daniele Panozzo, Chenfanfu Jiang, and Danny M Kaufman. Incremental potential contact: intersectionand inversion-free, large-deformation dynamics. ACM Trans. Graph., 39(4):49, 2020a.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020b.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXiv preprint arXiv:2003.03485, 2020c.\n\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel\n\nfields. Advances in Neural Information Processing Systems, 33:15651–15663, 2020.\n\nLu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. arXiv preprint arXiv:1910.03193, 2019.\n\nLu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library\n\nfor solving differential equations. SIAM Review, 63(1):208–228, 2021a.\n\nLu Lu, Raphael Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G Johnson. Physics-informed neural networks with hard constraints for inverse design. SIAM Journal on Scientific Computing, 43(6):B1105–B1132, 2021b.\n\nKjetil O Lye, Siddhartha Mishra, and Deep Ray. Deep learning observables in computational fluid\n\ndynamics. Journal of Computational Physics, 410:109339, 2020.\n\nZhiping Mao, Ameya D Jagtap, and George Em Karniadakis. Physics-informed neural networks for high-speed flows. Computer Methods in Applied Mechanics and Engineering, 360:112789, 2020.\n\nJerrold E Marsden and Matthew West. Discrete mechanics and variational integrators. Acta Numer-\n\nica, 10:357–514, 2001.\n\nJulien NP Martel, David B Lindell, Connor Z Lin, Eric R Chan, Marco Monteiro, and Gordon Wetzstein. Acorn: Adaptive coordinate networks for neural scene representation. arXiv preprint arXiv:2105.02788, 2021.\n\nAleka McAdams, Yongning Zhu, Andrew Selle, Mark Empey, Rasmus Tamstorf, Joseph Teran, and Eftychios Sifakis. Efficient elasticity for character skinning with contact and collisions. In ACM SIGGRAPH 2011 papers, pp. 1–12. 2011.\n\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4460–4470, 2019.\n\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pp. 405–421. Springer, 2020.\n\nSiddhartha Mishra and Roberto Molinaro. Estimates on the generalization error of physics-informed neural networks for approximating a class of inverse problems for pdes. IMA Journal of Numerical Analysis, 42(2):981–1022, 2022.\n\nSaviz Mowlavi and Saleh Nabi. Optimal control of pdes using physics-informed neural networks.\n\narXiv preprint arXiv:2111.09880, 2021.\n\nPatrick Mullen, Keenan Crane, Dmitry Pavlov, Yiying Tong, and Mathieu Desbrun. Energypreserving integrators for fluid animation. ACM Transactions on Graphics (TOG), 28(3):1–8, 2009.\n\nMatthias M ̈uller, Nuttapong Chentanez, Tae-Yong Kim, and Miles Macklin. Air meshes for robust\n\ncollision handling. ACM Transactions on Graphics (TOG), 34(4):1–9, 2015.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nThomas M ̈uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\n\nitives with a multiresolution hash encoding. arXiv preprint arXiv:2201.05989, 2022.\n\nKen Museth. Vdb: High-resolution sparse volumes with dynamic topology. ACM transactions on\n\ngraphics (TOG), 32(3):1–22, 2013.\n\nRahul Narain, Armin Samii, and James F O’brien. Adaptive anisotropic remeshing for cloth simu-\n\nlation. ACM transactions on graphics (TOG), 31(6):1–10, 2012.\n\nShaowu Pan, Steven L Brunton, and J Nathan Kutz. Neural implicit flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data. arXiv preprint arXiv:2204.03216, 2022.\n\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 165–174, 2019.\n\nTobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning mesh-\n\nbased simulation with graph networks. arXiv preprint arXiv:2010.03409, 2020.\n\nRaul Radovitzky and Michael Ortiz. Error estimation and adaptive meshing in strongly nonlinear dynamic problems. Computer Methods in Applied Mechanics and Engineering, 172(1-4):203– 240, 1999.\n\nMaziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686–707, 2019.\n\nMaziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning\n\nvelocity and pressure fields from flow visualizations. Science, 367(6481):1026–1030, 2020.\n\nChengping Rao, Hao Sun, and Yang Liu. Physics informed deep learning for computational elasto-\n\ndynamics without labeled data. arXiv preprint arXiv:2006.08472, 2020.\n\nJ. N. Reddy. Introduction to the Finite Element Method. McGraw-Hill Education, New York, 4th\n\nedition edition, 2019.\n\nPatrick J Roache. Fundamentals of computational fluid dynamics. Hermosa Publishers, 1998.\n\nAlireza Sadeghirad, Rebecca M Brannon, and Jeff Burghardt. A convected particle domain interpolation technique to extend applicability of the material point method for problems involving massive deformations. International Journal for numerical methods in Engineering, 86(12): 1435–1456, 2011.\n\nAlvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to simulate complex physics with graph networks. In International Conference on Machine Learning, pp. 8459–8468. PMLR, 2020.\n\nRohan Sawhney and Keenan Crane. Monte carlo geometry processing: A grid-free approach to\n\npde-based methods on volumetric domains. ACM Transactions on Graphics, 39(4), 2020.\n\nAndrew Selle, Ronald Fedkiw, Byungmoon Kim, Yingjie Liu, and Jarek Rossignac. An uncondi-\n\ntionally stable maccormack method. Journal of Scientific Computing, 35(2):350–371, 2008.\n\nRajsekhar Setaluri, Mridul Aanjaneya, Sean Bauer, and Eftychios Sifakis. Spgrid: A sparse paged grid structure applied to adaptive smoke simulation. ACM Transactions on Graphics (TOG), 33 (6):1–12, 2014.\n\nYeonjong Shin, Jerome Darbon, and George Em Karniadakis. On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type pdes. arXiv preprint arXiv:2004.01806, 2020.\n\nJustin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial\n\ndifferential equations. Journal of computational physics, 375:1339–1364, 2018.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nVincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in Neural Information Processing Systems, 33, 2020.\n\nBreannan Smith, Fernando De Goes, and Theodore Kim. Stable neo-hookean flesh simulation. ACM\n\nTrans. Graph., mar 2018.\n\nOlga Sorkine and Marc Alexa. As-rigid-as-possible surface modeling. In Symposium on Geometry\n\nprocessing, volume 4, pp. 109–116, 2007.\n\nJos Stam. Stable fluids. In Proceedings of the 26th annual conference on Computer graphics and\n\ninteractive techniques, pp. 121–128, 1999.\n\nAndrew Staniforth and Jean Cˆot ́e. Semi-lagrangian integration schemes for atmospheric models—a\n\nreview. Monthly weather review, 119(9):2206–2223, 1991.\n\nHaozhe Su, Tao Xue, Chengguizi Han, and Mridul Aanjaneya. A-ulmpm: An adaptively updated lagrangian material point method for efficient physics simulation without numerical fracture. In Computer Graphics Forum, volume 41, pp. 325–341. Wiley Online Library, 2022.\n\nDeborah Sulsky, Shi-Jian Zhou, and Howard L Schreyer. Application of a particle-in-cell method to\n\nsolid mechanics. Computer physics communications, 87(1-2):236–252, 1995.\n\nQi Sun, Anjul Patney, Li-Yi Wei, Omer Shapira, Jingwan Lu, Paul Asente, Suwen Zhu, Morgan McGuire, David Luebke, and Arie Kaufman. Towards virtual reality infinite walking: dynamic saccadic redirection. ACM Transactions on Graphics (TOG), 37(4):1–13, 2018.\n\nTowaki Takikawa,\n\nJoey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11358–11367, 2021.\n\nGeoffrey Ingram Taylor and Albert Edward Green. Mechanism of the production of small eddies from large ones. Proceedings of the Royal Society of London. Series A-Mathematical and Physical Sciences, 158(895):499–521, 1937.\n\nNils Wandel, Michael Weinmann, and Reinhard Klein. Learning incompressible fluid dynamarXiv preprint\n\nics from scratch–towards fast, differentiable fluid models that generalize. arXiv:2006.08762, 2020.\n\nSifan Wang and Paris Perdikaris. Long-time integration of parametric evolution equations with\n\nphysics-informed deeponets. arXiv preprint arXiv:2106.05384, 2021.\n\nDavid C Wilcox et al. Turbulence modeling for CFD, volume 2. DCW industries La Canada, CA,\n\n1998.\n\nZangyueyang Xian, Xin Tong, and Tiantian Liu. A scalable galerkin multigrid method for real-time\n\nsimulation of deformable objects. ACM Trans. Graph., 38(6), nov 2019.\n\nYiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond. arXiv preprint arXiv:2111.11426, 2021.\n\nGuandao Yang, Serge Belongie, Bharath Hariharan, and Vladlen Koltun. Geometry processing with\n\nneural fields. Advances in Neural Information Processing Systems, 34, 2021.\n\nJonas Zehnder, Yue Li, Stelian Coros, and Bernhard Thomaszewski. Ntopo: Mesh-free topology optimization using implicit neural representations. Advances in Neural Information Processing Systems, 34:10368–10381, 2021.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nA COMPARISON OF DIFFERENT PDE SOLVERS\n\nIn the table below, we compare other ML-PDE solvers including MeshGraphNet (Pfaff et al. (2020)), GraphNetworkSim (Sanchez-Gonzalez et al. (2020)), DeepOnet (Lu et al. (2019)), Fourier Neural Operator (Li et al. (2020b)).\n\nTable 1: Comparison with other ML-PDE solvers.\n\nMethods\n\nRequire training data (from classical solvers or real-world data)\n\nMeshGraphNet GraphNetworkSim DeepOnet Fourier Neural Operator Ours Classical solvers (e.g., FEM)\n\nYes Yes Yes Yes No No\n\nB IMPLEMENTATION DETAILS\n\nB.1 OPTIMIZATION\n\nGeneralize to outof-distribution initial/boundary conditions, material parameters, and time spans\n\nLimited Limited Limited Limited Good Very good\n\nEnforce PDE constraints\n\nNone None Good Good Good Very good\n\nWe solve our time-integration optimization problem (Equation (3)) with the Adam optimizer (Kingma & Ba, 2014). For all examples in our experiments, we set an initial learning rate lr0 and reduce it by a factor of 0.1 if the loss value does not decrease for iterp iterations. We stop the optimization process when the learning rate is smaller than lrmin or until it reaches a maximum of itermax iterations. Specific values of these hyper-parameters are described for each example below. We implement our method using PyTorch library and performed our experiments on an NVIDIA GeForce RTX 3090 GPU.\n\nAlgorithm 1: Temporal Evolution\n\nInput: network weights θ0, time step size ∆t, time integrator I, spatial domain Ω\n\n1 n ← 0; 2 while true do 3\n\nθn+1 ← θn; while not converged do\n\n4\n\n5\n\n6\n\n7\n\nend while n ← n + 1;\n\n8 9 end while\n\n// network training with Adam optimizer\n\nrandomly sample M ⊂ Ω; θn+1 ← θn+1 − α∇ (cid:80)\n\nx∈M\n\nI(∆t, {f θk }n+1\n\nk=0 , {∇f θk }n+1\n\nk=0 , . . .);\n\n// eq. (3)\n\nB.2 ADVECTION EQUATION\n\nFor our advection example in Figure 1, the 1D spatial domain is Ω = [−2, 2]. We consider the Dirichlet boundary condition, i.e., the advected quantity at boundaries equals zero. Hence we set the boundary constraint term in Equation (4) as\n\nwith the weighting factor λ = 1. The initial condition for this example is\n\nC = ||un+1(x)||2 2,\n\nˆu0(x) = e−\n\n(x − μ)2 2σ2\n\n,\n\n16\n\n(15)\n\n(16)\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Initial condition for the example in Figure 4. Left: velocity field (Equation (19)). Right: density field (Equation (20)).\n\nwith μ = −1.5 and σ = 0.1. We set the optimization hyper-parameters lr0 = 1e−4, lrmin = 1e−8, iterp = 500 and itermax = 20000. For each gradient descent iteration, we randomly sample |M| = 5000 points within the spatial domain [−2, 2]. For this example, our method takes ∼ 80s to compute per timestep, while the grid-based method (using the same memory) takes ∼ 4e−3s.\n\nB.3\n\nINVISCID NAVIER-STOKES EQUATIONS\n\nFor our 2D fluid examples, the spatial domain is Ω = [−1, 1] × [−1, 1]. We consider solid boundary conditions, i.e., the fluid cannot go through the boundaries. Recall that we adopt the operator splitting scheme. Therefore, the boundary constraint terms for the three sequential steps are\n\nCadv = ||un+1\n\nadv⊥(x)||2 Cpro = ||∇⊥pn+1(x)||2 ⊥ (x)||2\n\n2\n\n2\n\n(17)\n\nCcor = ||un+1 where ⊥ indicates the perpendicular direction against the boundary. The weighting factor λ = 1.\n\n2\n\n2D Taylor-Green vortex Standard 2D Taylor-Green is originally defined in domain [0, 2π] × [0, 2π]. We translate and scale the domain to [−1, 1] × [−1, 1] such that the input range fits our MLP with the SIREN activation (Sitzmann et al., 2020). Therefore, the initial condition for the velocity field becomes\n\nˆu0(x) = (\n\n1 π\n\nsin[π(x + 1)] cos[π(y + 1)], −\n\n1 π\n\ncos[π(x + 1)] sin[π[y + 1]]).\n\n(18)\n\nAfter the simulation, we convert it back to domain [0, 2π] × [0, 2π] for evaluation and comparison. We set the optimization hyper-parameters lr0 = 1e−5, lrmin = 1e−8, iterp = 500 and itermax = 20000. The size of sample set |M| = 2562. For this example, our method takes ∼ 10min to compute per timestep, while the grid-based method (using the same memory) takes ∼ 0.03s.\n\nTwo vortices of different scale For the example shown in Figure 4, the initial condition for the velocity field is\n\nˆu0(x) =\n\n \n\n\n\n(sin[2π(x + 1)] cos[2π(y + 1)], − cos[2π(x + 1)] sin[2π(y + 1)]) (sin[8π(x − 7 (0, 0)\n\n4 )], − cos[8π(x − 7\n\n4 )] cos[8π(y − 7\n\n4 )] sin[8π(y − 7\n\n4 )]) x ∈ [ 7\n\n4 , 1]2 otherwise.\n\nx ∈ [−1, 0]2\n\nThe density field that we advect is initialized as\n\nˆd0(x) =\n\n(cid:26)1 ||2x + 1|| ≤ 0.5 or ||8x + 7|| ≤ 0.5\n\n0 otherwise.\n\n(19)\n\n(20)\n\nFigure 8 visually illustrates the above initial conditions. After the simulation, we convert it back to domain [0, 2π] × [0, 2π] for evaluation and comparison. We set the optimization hyper-parameters lr0 = 1e−5, lrmin = 1e−8, iterp = 500 and itermax = 20000. The size of sample set |M| = 1282. For this example, our method takes ∼ 10min to compute per timestep.\n\n17\n\n1.00.00.5Initial velocity fieldInitial density fieldUnder review as a conference paper at ICLR 2023\n\nTable 2: Experiment setup for the elasticity examples. ρ0 is the density. λ and μ are the first and second lame parameters. α and β are the number of hidden layers and the dimension of the hidden features. lr0 and itermax are the initial learning rate and the maximum number of iterations. tavg is average training time per time step. Note that the density ρ0 and timestep size dt are reported as N/A for the quasistatic example Stretch (2D) (Figure 5).\n\nExample Collision (2D) (Figure 6) Stretch (2D) (Figure 5) Bunny (Figure 7) Spot (Figure 2) Lucy (Figure 13)\n\nDim |M|\n\ndt\n\nρ0\n\nλ\n\nμ α β\n\nlr0\n\n2 2\n3 3\n3\n\n1002 0.1 1e1 2e1 1e3 3 68 1e−5 1002 N/A N/A 1e0 1e3 3 68 1e−4 203 0.1 1e0 1e2 1e3 3 66 1e−5 203 0.1 1e0 1e2 1e3 3 66 1e−3 203 0.1 1e0 1e3 1e3 3 128 1e−4\n\nitermax 1e4 5e4 2e4 5e3 2e4\n\ntavg(s) 1.38e2 2.30e3 1.70e3 1.74e3 1.16e3\n\nFigure 9: Elasticity patch test. Quasistatic simulation in 2D (undeformed, stretched, compressed).\n\nFigure 10: Twisting test. Quasistatic simulation in 3D. The right end is twisted 45 degrees.\n\nB.4 ELASTODYNAMICS EQUATION\n\nInitial and Boundary Conditions For our 2D elasticity examples in Figure 5, Figure 9 and Figure 6, the 2D spatial domain is Ω = [−1, 1] × [−1, 1]. For our 3D elasticity examples in Figure 12, the 3D spatial domain is Ω = [−1, 1] × [−1, 1] × [−1, 1]. For our 2D and 3D examples involving nonregular geometry (Figure 7, Figure 2 and Figure 13), the spatial domain is the interior of the shape, including the boundary. The initial condition for all the elasticity examples is\n\nˆφ0(x) = (0, 0) (2D), ˆφ0(x) = (0, 0, 0) (3D)\n\n(21)\n\nThe boundary constraint for elasticity examples involves positional constraints or collision constraints. Positional constraints, or Dirichlet boundary conditions, can be realized by defining the position of the constraint set ∂Ω as the desired goal positions φ∂Ω:\n\nI pos = ∥φn+1\n\n∂Ω − φ∂Ω∥2 2.\n\n(22)\n\nCollision constraints can be handled by adding unilateral constraints dynamically and viewing the collision penalty force as external force. Specifically, for a colliding point qc, we first find the closest surface point bc with normal nc, and define our spring-like collision penalty force as:\n\nwhere kcol is the ratio for the collision penalty force.\n\nf col = kcol((bc − qc)⊤nc)nc.\n\nThe corresponding collision energy can be defined as the work exerted by the collision force:\n\nI col = ρ0f T\n\ncolφn+1.\n\n(23)\n\n(24)\n\nExperiment Setup For all the 2D comparison under the same memory usage, we use α = 3 hidden layers of width β = 68 with SIREN activation function (Sitzmann et al., 2020) for our MLP, which takes the same memory (57 KB) as the FEM mesh (0.8K vertices, 1.5K faces) and MPM point cloud (1.7K points) in use. We initialize the 2D deformation field of the network to be zero\n\n18\n\nUndeformedStretchedCompressedUndeformedTwistedUnder review as a conference paper at ICLR 2023\n\nFigure 11: Error of the elastic tension test. We visualize the L2 position error e of our result and the low-resolution FEM result (0.8K vertices) in Fig. 5 with respect to high-resolution FEM ground truth (3.2K vertices). Under the same memory footprint (for storing the spatial representation), our method (∥e∥∞ = 8.89e-2) is closer to the high-resolution ground truth than the mesh-based method (∥e∥∞ = 1.99e-1).\n\nFigure 12: Sampling convergence test. Quasi-static elasticity simulation using different number of samples. We trained the implicit neural representation using different number of samples and visualized the result using samples |M| = 503 (left). We further compute the error with respect to |M| = 503 when using different number of samples (right). As we increase the number of the training samples in use, the deformation field converges to the result trained on the highest resolution.\n\nusing |M| = 10002 uniform and random samples. Then we train the network using |M| = 1002 uniform and random samples at each training iteration. We use Bartels (Levin, 2020) and Taichi (Hu et al., 2019) to perform the FEM and MPM simulation, respectively. We run our FEM and MPM comparison on CPU using a MacBook Pro with Apple M2 processor and 24GB of RAM.\n\nFor the 3D comparison under the same memory usage, for the bunny example (Figure 7), we use α = 3 hidden layers of width β = 66 with SIREN activation function for our MLP, which takes the same memory (53 KB) as the FEM mesh (0.5K vertices, 1.5K tetrahedra) in use. For the statue example (Figure 13), we use α = 3 hidden layers of width β = 128 with SIREN activation function for our MLP, which takes the same memory (197 KB) as the FEM mesh (2.0K vertices, 7.0K tetrahedra) in use. We initialize the 3D deformation field of the network to be zero using |M| = 1003 uniform and random samples. Then we train the network using |M| = 203 uniform and random samples at each training iteration. Here for simplicity we use the mesh vertices as the uniform samples. We further report all the parameters and experiment setup in Table 2. In addition, we set the hyper-parameters iterp = 800 and lrmin = 1e−8 for all elasticity examples.\n\nFor sampling of the shapes involving nonregular geometry, for simplicity we choose to use a triangle or tetrahedral mesh and perform sampling within it. An ideal alternative would be adopting the implicit representation of the surface and performing rejection sampling based on it.\n\nFor rendering, we simply sample sufficient number of points from the undeformed shape and evaluate the trained model at time t on the sample positions to predict their deformation. Here we only sample the surface of the shape in 3D cases. Then we render the shape as a dense point cloud.\n\nC ADDITIONAL RESULTS\n\nC.1 ELASTODYNAMICS EQUATION\n\nWe validate the physical plausibility of our method using a small 2D patch test (Figure 9) and a 3D twisting example (Figure 10). We show that our implicit neural representation can exhibit volumepreserving property under both stretching, compression and twisting.\n\n19\n\nOursmesh-based (FEM)Ground Truth0.20.00.1203#samples30340350310353#samples Error531032033034035030.00.10.20.30.40.5Under review as a conference paper at ICLR 2023\n\nFigure 13: The statue collides with the ground and deforms. Our implicit neural representation (green, right) is capable of capturing more fine geometry details compared to the traditional tetrahedral mesh representation (blue, left) under the same memory footprint.\n\nWe demonstrate qualitative and quatitative convergence of our method when increasing the number of training samples in use. In Figure 12, we compare the quasi-static stretching results visualized using |M| = 503 uniform samples when using different number of training samples (|M| = 53, 103, 203, 303, 403, 503), and report the error with respect to the high-resolution trained result (trained and visualized both on |M| = 503).\n\nFinally, we provide an additional example for elasticity involving complex contact-induced deformations in Figure 13. Our implicit neural representation is able to maintain more intricate geometry details compared to the traditional tetrahedral mesh representation under the same memory usage.\n\nD QUANTITATIVE RESULTS\n\nWe present the quantitative results (error, runtime and memory usage) for each of our tested examples in separate tables.\n\nTable 3: Quantitative results for 1D advection example (Figure 1). Error: mean absolute error over total 240 time steps, compared to the ground truth analytical solution. Time: runtime for total 240 time steps. Memory: memory usage for storing the spatial representations.\n\nMethods\n\nError\n\nTime Memory\n\nOurs Grid PINN\n\n0.0030 0.0146 0.0564\n\n3.520KB 5.33h 1.13s 3.520KB 9.73m 3.598KB\n\nTable 4: Quantitative results for 2D Taylor-Green fluid example (Figure 3). Error: mean squared error of velocity field over total 100 time steps, compared to the ground truth analytical solution. Time: runtime for total 100 time steps. Memory: memory usage for storing the spatial representations.\n\nMethods\n\nError\n\nTime\n\nMemory\n\nOurs Grid-based\n\n3.35e-4 4.83e-3\n\n14.02h 2.91s\n\n25.887KB 27.00KB\n\nTable 5: Quantitative results for two-vortices fluid example (Figure 4). Error: mean absolute error of kinetic energy over total 50 time steps, compared to the reference solution (obtained by high resolution grid-based method). Kinetic energy is computed using 10242 uniform samples. Time: runtime for total 50 time steps. Memory: memory usage for storing the spatial representations.\n\nMethods\n\nError\n\nTime\n\nMemory\n\nOurs Grid-based PINN PINN-seq\n\n2.24e2 1.07e4 1.92e4 3.21e4\n\n10.81h 1.78s 2.2h 20.83h\n\n25.887KB 27.00KB 26.137KB 26.137KB\n\n20\n\n0 sec0.2 sec0.4 sec0.6 sec0.8 sec1.0 secTetrehedral MeshforceUnder review as a conference paper at ICLR 2023\n\nTable 6: Quantitative results for 2d tension example (Figure 5 and Figure 11). Error: Infinity norm of L2 distance w.r.t. high-resolution ground truth. Time: Total runtime until convergence. Memory: memory usage for storing the spatial representations.\n\nMethods\n\nError\n\nTime Memory\n\nOurs Mesh-based\n\n8.82e-2 1.99e-1\n\n38.33m 56.32KB 54.00KB 22.04s\n\nTable 7: Elastic square collides with circle (Figure 6). Error: Infinity norm of L2 intersection distance w.r.t. the circle boundary. Time: Average runtime for 1 time step. Memory: memory usage for storing the spatial representations.\n\nMethods\n\n0.2s\n\n0.4s\n\n0.6s\n\n0.8s\n\nError\n\nTime Memory\n\nOurs Mesh-based\n\n1.62e-2 3.45e-2\n\n1.10e-2 1.39e-2\n\n1.04e-2 5.47e-2\n\n1.06e-2 4.23e-2\n\n2.30m 56.32KB 54.00KB 9.82s\n\n21",
    "reference": "# Summary Of The Paper\n\nIn this work the authors propose to implicit neural representations to model the spatial representation, while approaching the temporal representation with classical optimization-based approaches to model time-dependent partial differential equations.\n\n# Strength And Weaknesses\n\nStrengths:\n- Clarity of the mathematical exposition of the approach\n- Clarity in the embedding of the presented approaches intersection and background in PDE- and fluid dynamics theory\n\nWeaknesses:\n- Poor integration into the current state-of-the-art by omitting many similar approaches, such as Graph Network Simulations [2] which effectively encode the spatial representation with graph neural networks, and similarly use classical time-integrators for the temporal representation. The same applies for the even more modern MeshGraph [3], and Fourier Neural Operator [4], both would need to be compared to the herein presented approach to properly assess the capabilities of the presented approach. The usage of implicit representations for functions has also been explored by Dupont et al. [1] before. Said work would need to be put in context to the presented literature.\n- Improperly chosen comparisons to current state-of-the-art models due to an incomplete representation of the literature, see preceding point\n- No ablation analyses\n- Performance of the approach induces a 30x overhead, hence making it intractable for any practical problem at the current moment\n\n[1] Dupont, Emilien, Hyunjik Kim, SM Ali Eslami, Danilo Jimenez Rezende, and Dan Rosenbaum. \"From data to functa: Your data point is a function and you can treat it like one.\" In International Conference on Machine Learning, pp. 5694-5725. PMLR, 2022.\n[2] Pfaff, Tobias, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. \"Learning Mesh-Based Simulation with Graph Networks.\" In International Conference on Learning Representations. 2020.\n[3] Sanchez-Gonzalez, Alvaro, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. \"Learning to simulate complex physics with graph networks.\" In International Conference on Machine Learning, pp. 8459-8468. PMLR, 2020.\n[4] Li, Zongyi, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. \"Fourier Neural Operator for Parametric Partial Differential Equations.\" In International Conference on Learning Representations. 2020.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nWhile the exposition is written with a lot of clarity, and does have a very high quality at first glance, it displays a severe lack of nuance in its relation to current work to properly assess its originality. While the present paper still has novelty in aspects, the novel arises from the implicit spatial representations, which are also viewed from the viewpoint of implicit neural representations. Previous approaches such as Graph Network Simulations, and MeshGraphNets have already followed a highly similar approach, where one can surmise that under certain conditions the Graph Network Simulations approach of spatial graph network representations, can be equivalent to the presented spatial implicit representations. As such some of the claims such as \"To our best knowledge, computing neural spatial representations on time-dependent PDEs for long horizon tasks with multiple time steps has not been explored, and our work aims to fill this gap.\" are not supported in literature.\n\nThere do in addition exist a multitude of sentence structure errors, and typos which I would dearly recommend to address.\n\n# Summary Of The Review\n\nThis work present a new approach for implicit neural spatial representations for time-dependent partial differential equations. With the temporal representation modeled classically, there exist a number of similar approaches in existing literature, hence rendering the use of implicit representations the main novelty of the paper. In addition the paper severely lacks adequately chosen comparisons, only comparing to the original physics-informed neural network (PINN) approach from 2019.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nRPM: GENERALIZABLE MULTI-AGENT POLICIES FOR MULTI-AGENT REINFORCEMENT LEARNING\n\nWei Qiu‡∗ Xiao Ma† Bo An‡ Svetlana Obraztsova‡ Shuicheng Yan† Zhongwen Xu†(cid:0) ‡Nanyang Technological University qiuw0008@e.ntu.edu.sg\n\n†Sea AI Lab zhongwen.s.xu@gmail.com\n\nABSTRACT\n\nDespite the recent advancement in multi-agent reinforcement learning (MARL), the MARL agents easily overfit the training environment and perform poorly in evaluation scenarios where other agents behave differently. Obtaining generalizable policies for MARL agents is thus necessary but challenging mainly due to complex multi-agent interactions. In this work, we model the MARL problem with Markov Games and propose a simple yet effective method, called ranked policy memory (RPM), i.e., to maintain a look-up memory of policies to achieve good generalizability. The main idea of RPM is to train MARL policies via gathering massive multi-agent interaction data. In particular, we first rank each agent’s policies by its training episode return, i.e., the episode return of each agent in the training environment; we then save the ranked policies in the memory; when an episode starts, each agent can randomly select a policy from the RPM as the behavior policy. Each agent uses the behavior policy to gather multi-agent interaction data for MARL training. This innovative self-play framework guarantees the diversity of multi-agent interaction in the training data. Experimental results on Melting Pot demonstrate that RPM enables MARL agents to interact with unseen agents in multi-agent generalization evaluation scenarios and complete given tasks. It significantly boosts the performance up to 818% on average.\n\n1\n\nINTRODUCTION\n\nIn Multi-Agent Reinforcement Learning (MARL) (Yang & Wang, 2020), each agent acts decentrally and interacts with other agents to complete given tasks or achieve specified goals via reinforcement learning (RL) (Sutton & Barto, 2018). In recent years, much progress has been achieved in MARL research (Vinyals et al., 2019; Jaderberg et al., 2019; Perolat et al., 2022). However, the MARL agents trained with current methods tend to suffer poor generalizability (Hupkes et al., 2020) in the new environments. The generalizability issue is critical to real-world MARL applications (Leibo et al., 2021), but is mostly neglected in current research.\n\nIn this work, we aim to train MARL agents that can adapt to new scenarios where other agents’ policies are unseen during training. We illustrate a two-agent hunting game as an example in Fig. 1. The game’s objective for two agents is to catch the stag together, as one agent acting alone cannot catch the stag and risks being killed. They may perform well in evaluation scenarios similar to the training environment, as shown in Fig. 1 (a) and (b), respectively, but when evaluated in scenarios different from the training ones, these agents often fail. As shown in Fig. 1 (c), the learning agent (called the focal agent following (Leibo et al., 2021)) is supposed to work together with the other agent (called the background agent also following (Leibo et al., 2021)) that is pre-trained and can capture the hare and the stag. In this case, the focal agent would fail to capture the stag without help from its teammate. The teammate of the focal agent may be tempted to catch the hare alone and not cooperate, or may only choose to cooperate with the focal agent after capturing the hare. Thus, the focal agent should adapt to their teammate’s behavior to catch the stag. However, the policy of the background agent is unseen to the focal agent during training. Therefore, without generalization, the agents trained as Fig. 1 (left) cannot achieve an optimal policy in the new evaluation scenario.\n\n∗Wei Qiu did the work while interning at Sea AI Lab. (cid:0) Corresponding author.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Two-Agent Hunting Game. (a) Training environment. Two agents (hunters) hunt in the environment. (b) After training in the training environment, all agents behave cooperatively to capture the stag. (c) In the new evaluation scenario, one agent is picked as the focal agent (in the magenta circle) and paired with a pre-trained agent (in the brown circle) that behaves in different ways to evaluate the performance of the selected agent. In conclusion, the conventional evaluation protocol fails to evaluate such behavior and current MARL methods easily fail to learn the optimal policy due to the lack of diversified multi-agent interaction data during training.\n\nInspired by the fact that human learning is often accelerated by interacting with individuals of diverse skills and experiences (Meltzoff et al., 2009; Tomasello, 2010), we propose a novel method aimed at improving the generalization of MARL through the collection of diverse multi-agent interactions. Concretely, we first model the MARL problem with Markov Games (Littman, 1994) and then propose a simple yet effective method called ranked policy memory (RPM) to attain generalizable policies. The core idea of RPM is to maintain a look-up memory of policies during training for the agents. In particular, we first evaluate the trained agents’ policies after each training update. We then rank the trained agents’ policies by the training episode returns and save them in the memory. In this way, we obtain various levels, i.e., the performance of the policies. When starting an episode, the agent can access the memory and load a randomly sampled policy to replace the current behavior policy. The new ensemble of policies enables the agents in self-play to collect diversified experiences in the training environment. These diversified experiences contain many novel multi-agent interactions that can enhance the extrapolation capacity of MARL, thus boosting the generalization performance. We note that an easy extension by incorporating different behavior properties as the keys in RPM could potentially further enrich the generalization but it is left for future work.\n\nWe implement RPM on top of the state-of-the-art MARL algorithm MAPPO (Yu et al., 2021). To verify its effectiveness, we conduct large-scale experiments with the Melting Pot (Leibo et al., 2021), which is a well-recognized benchmark for MARL generalization evaluation. The experiment results demonstrate that RPM significantly boosts the performance of generalized social behaviors up to 818% on average and outperforms many baselines in a variety of multi-agent generalization evaluation scenarios. Our code, pictorial examples, videos and experimental results are available at this link: https://sites.google.com/view/rpm-iclr2023/.\n\n2 PRELIMINARIES\n\ni=1Ai is a set of joint actions with Ai denoting the set of actions for an agent i; O = ×N\n\nMarkov Games. We consider the Markov Games (Littman, 1994) represented by a tuple G = ⟨N , S, A, O, P, R, γ, ρ⟩. N is a set of agents with the size |N | = N ; S is a set of states; A = ×N i=1Oi is the observation set, with Oi denoting the observation set of the agent i; P : S × A → S is the transition i=1ri is the reward function where ri : S × A → R specifies the reward for the function and R = ×N agent i given the state and the joint action; γ is the discount factor; the initial states are determined by a distribution ρ : S → [0, 1]. Given a state s ∈ S, each agent i ∈ N chooses its action ui and obtains the reward r(s, u) with the private observation oi ∈ Oi, where u = {ui}N i=1 is the joint action. The joint policy of agents is denoted as πθ = {πθi}N i=1 where πθi : S × Ai → [0, 1] is the policy for the agent i. The objective of each agent is to maximize its total expected return Ri = (cid:80)∞ Multi-Agent RL. In MARL, multiple agents act in the multi-agent systems to maximize their respective returns with RL. Each agent’s policy πi is optimized by maximizing the following objective:\n\nt=0 γtrt i.\n\nJ (πi) ≜ E\n\ns0:∞∼ρ0:∞\n\nG ,ai\n\n0:∞∼πi\n\n(cid:35)\n\nγtri\n\nt\n\n,\n\n(cid:34) ∞ (cid:88)\n\nt=0\n\nwhere J (πi) is a performance measure for policy gradient RL methods (Williams, 1992; Lillicrap et al., 2016; Fujimoto et al., 2018). Each policy’s Q value Qi is optimized by minimizing the following regression loss (Mnih et al., 2015) with TD-learning (Sutton, 1984):\n\nL(θi) ≜ ED′∼D\n\n(cid:20)(cid:16)\n\nt − Qi yi\n\nθi\n\n(cid:16)\n\nst, ut, si\n\nt, ui\n\nt\n\n(cid:17)(cid:17)2(cid:21)\n\n,\n\n2\n\nTrainingenvironmentTwo-agentHuntingGameBothagentscatchthestagFocalagentBackgroundagentFocalagentbehavespoorlyConventionalevaluationprotocol:evaluatingtwoagentstogetherNewevaluationprotocol:evaluatethethefocalagentonly(a)(b)(c)Published as a conference paper at ICLR 2023\n\nFigure 2: An example of our formulation. Left: All six agents’ policies are trained with the MARL. Right: Two agents with policies πφ1 and πφ2 are picked as background agents, and the rest of the four agents (with new indices) are focal agents to be evaluated. The focal and the background agents constitute the evaluation scenario.\n\nt, ui,′(cid:1). θi are the parameters of the agents. ̄θi is the where yi parameter of the target Qi and periodically copied from θ. D′ is a sample from the replay buffer D.\n\nt + γ maxu′ Qi ̄θi\n\n(cid:0)st+1, u′, si\n\nt = ri\n\n3 PROBLEM FORMULATION\n\nWe introduce the formulation of MARL for training and evaluation in our problem. Our goal is to improve generalizabiliby of MARL policies in scenarios where policies of agents or opponents are unseen during training while the physical environment is unchanged. Following Leibo et al. (2021), the training environment is defined as substrate. Each substrate is an N -agent partially observable Markov game G. Each agent optimizes its policy πθi via the following protocol.\n\nDefinition 1 (Multi-Agent Training). There are N agents act in the substrate, which is denoted as G. Each agent receives partial environmental observation not known to other agents and aims to optimizes its policy πθi by optimizing its accumulated rewards: (cid:80)∞ t. The performance of the joint (cid:80)N policy πθ = {πθi}N i=1 R(πθi; G). R(πθi ; G) measures the episode return of policy πθi in game G for agent i.\n\ni=1 is measured by the mean individual return: ̄R(πθ) = 1\n\nt=0 γtri\n\nN\n\nIn order to evaluate the trained MARL policies in evaluation scenario G′, we follow the evaluation protocol defined by Leibo et al. (2021):\n\nDefinition 2 (Multi-Agent Evaluation). There are M (1 ≤ M ≤ N − 1) focal agents that are selected from N agents. The focal agents are agents to be evaluated in evaluation scenarios. They are paired with N − M background agents whose policies πφ = {πφj }N −M j=1 were pre-trained with pseudo rewards in the same physical environment where the policies πθ are trained. To measure the generalized performance in evaluation scenarios, we use the mean individual return of focal agents as the performance measure: ̄R({πθ}M\n\ni=1 R(πθi; G′).\n\ni=1) = 1\n\n(cid:80)M\n\nM\n\nWe show an example of our formulation in Fig. 2. Note that the focal agents cannot utilise the interaction data collected during evaluation to train or finetune their policies. Without training the policies of focal agents with the collected trajectories during evaluation, the focal agents should behave adaptively to interact with the background agents to complete challenging multi-agent tasks. It is also worth noting that the ad-hoc team building (Stone & Kraus, 2010; Gu et al., 2021) is different from our formulation both in the training and evaluation. We discuss the differences in the related works section (Paragraph 3, Sec. 7).\n\nIn MARL, the focal agents need adaptively interact with background agents to complete given tasks. Formally, we define the objective for optimizing performance of the focal agents without exploiting their trajectories in the evaluation scenario for training the policies {πθj }M\n\nj=1:\n\nmax J ({πθj }M\n\nj=1) ≜ max E\n\ns0:∞∼ρ0:∞\n\nG′\n\n,aj\n\n0:∞∼{πθj\n\n}M\n\nj=1\n\n(cid:34) ∞ (cid:88)\n\nt=0\n\nγt 1 M\n\n(cid:35)\n\nG′\n\n.\n\nM (cid:88)\n\nj=1\n\nrj\n\nt\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(1)\n\n4 RANKED POLICY MEMORY\n\nTo improve the generalization of MARL, agents in the substrate must cover as much as multi-agent interactions, i.e., data, that resemble the unseen multi-agent interactions in the evaluation scenario. However, current training paradigms, like independent learning (Tampuu et al., 2017) and centralized\n\n3\n\nπ!!π!\"π!#π!$π!%π!&πθSubstrateMARLTrainingInteractionCollectdataUpdateπ!!π!\"π!#π!$π\"! #$%&SubstrateDataBufferInteractionCollectdataπ\"!π\"\"ScenarioMulti-AgentTrainingMulti-AgentEvaluationAgent policiesPolicies of 4 focal agentsPolicies of 2 background agentsPublished as a conference paper at ICLR 2023\n\nFigure 3: The workflow of RPM for a three-agent substrate. In the workflow, there are three agents in the substrate. Agent 3 is the background agent. Agents 1 and 2 are focal agents.\n\ntraining and decentralized execution (CTDE) (Oliehoek et al., 2008), cannot give diversified multiagent interactions, as the agents’ policies are trained at the same pace. To this end, we propose a Ranked Policy Memory (RPM) method to provide diversified multi-agent behaviors.\n\nθ}N\n\ni=1. Then {πi\n\nRPM Building & Updating. We denote an RPM with Ψ, which consists of |Rmax| entries, i.e., ranks, where |Rmax| is the maximum training episode return (the episode return in the substrate). When an agent is acting in the substrate, it will receive the training episode return R of all agents with policies {πi i=1 are saved into Ψ by appending agents’ policies into the corresponding memory slot, Ψ[re].add({πi i=1). To avoid there being too many entries in the policy memory caused by continuous episode return values, we discretize the training episode return. Each discretized entry κ covers a range of [κ, κ + ψ), where ψ > 0 and it can be either an integer or a float number. For the training episode return R, the corresponding entry κ can be calculated by:\n\nθ}N\n\ne}N\n\nκ =\n\n(cid:26)⌊R/ψ⌋ × 1{(R mod ψ) ̸= 0} × ψ,\n\n⌊R/ψ⌋ × ψ,\n\nif R ≥ 0, otherwise.\n\n(2)\n\nwhere 1{·} is the indicator function, and ⌊·⌋ is the floor function. Intuitively, discretizing R saves memory and memorize policies of similar performance in to the same rank. Therefore, diversified policies can be saved to be sampled for agents.\n\nRPM Sampling. The memory Ψ stores diversified policies with different levels of performance. We can sample various policies of different ranks and assign each policy to each agent in the substrate to collect multi-agent trajectories for training. These diversified multi-agent trajectories can resemble trajectories generated by the interaction with agents possessing unknown policies in the evaluation scenario. At the beginning of an episode, we first randomly sample N keys with replacement and then randomly sample one policy for each key from the corresponding list. All agents’ policies will be replaced with the newly sampled policies for multi-agent interactions in the substrate, thus generating diversified multi-agent trajectories.\n\nAlgorithm 1: MARL with RPM 1 Input: Initialize πθ, Ψ, D, G and G′; 2 Input: Initialize behavior policy πθb ← πθ; 3 for each update do 4\n\nThe Workflow of RPM. We showcase an example of the workflow of RPM in Fig. 3. There are three agents in training. Agents sample policies from RPM. Then all agents collect data in the substrate for training. The training episode return is then used to update RPM. During evaluation, agents 1 and 2 are selected as focal agents and agent 3 is selected as the background agent. We present the pseudo-code of MARL training with RPM in Algorithm 1. In Lines 4-5, the πθb is updated by sampling policies from RPM. Then, new trajectories of D are collected in Line 6. πθ is trained in Line 7 with MARL method by using the newly collected trajecotries and πθb is updated with the newly updated πθ. RPM is updated in Line 8. After that, the performance of πθ is evaluated in the evaluation scenario G′ and the evaluation score ̄R is returned in Line 9.\n\nD ← GatherTrajectories(πθb , G); πθ ← MARLTrainig(πθ, D); Ψ ← UpdateRPM(πθ, Ψ, G); ̄R ← Evaluate(πθ, G′); πθb ← πθ;\n\nπθb ← SamplingRPM(Ψ);\n\n10 11 Output: πθ.\n\nif RPM sampling then\n\n8\n\n5\n\n6\n\n7\n\n9\n\nDiscussion. RPM leverages agents’ previously trained models in substrates to cover as many patterns of multi-agent interactions as possible to achieve generalization of MARL agents when paired with agents with unseen policies in evaluation scenarios. It uses the self-play framework for data collection. Self-play (Brown, 1951; Heinrich et al., 2015; Silver et al., 2018; Baker et al., 2019) maintains a memory of the opponent’s previous policies for acquiring equilibria. RPM differs from other self-play methods in four aspects: (i) self-play utilizes agent’s previous policies to create fictitious opponents\n\n4\n\nSubstrateFocal Agents 1 & 2MARL LearnerRank1Rank2Rank3.........Agent 1Agent 2Agent 3...RPMSave policies of π!!\",π!\"\"andπ!#\"given mean individual returnπ!!\"#\"$Sampled policies π!\"\"#\"%π$#Pretrained policyBackground agentdataUpdate RPMMARL TrainingMulti-Agent Evaluationπ!\"\"#\"&θ%, θ&and θ’(1) Training episode return; (2) θ%, θ&and θ’Published as a conference paper at ICLR 2023\n\nwhen the real opponents are not available. By playing with the fictitious opponents, many fictitious data are generated for training the agents. In RPM, agents load their previous policies to diversify the multi-agent interactions, such as multi-agent coordination and social dilemmas, and all agents’ policies are trained by utilizing the diversified multi-agent data. (ii) Self-play does not maintain explicit ranks for policies while RPM maintains ranks of policies. (iii) Self-play was not introduced for generalization of MARL while RPM aims to improve the generalization of MARL. In Sec. 6, we also present the evaluation results of a self-play method.\n\n5 MARL TRAINING\n\nWe incorporate RPM into the MARL training pipeline. We take MAPPO (Yu et al., 2021) for instantiating our method, which is a multi-agent variant of PPO (Schulman et al., 2017) and outperforms many MARL methods (Rashid et al., 2018; 2020; Wang et al., 2021a) in various complex multi-agent domains. In MAPPO, a central critic is maintained for utilizing the concealed information of agents to boost multi-agent learning due to non-stationarity. RPM introduces a novel method for agents to collect experiences/trajectories τ = {τi}N\n\ni=1. Each agent optimizes the following objective: (cid:1) · At\n\n(cid:1) , 1 − ε, 1 + ε(cid:1) · At\n\ni, clip (cid:0)ηt\n\n(cid:1)(cid:3) ,\n\n(cid:0)θt\n\ni\n\ni\n\ni\n\n(3)\n\nJ (θi) = E (cid:2)min (cid:0)ηt\n\ni\n\n(cid:0)θt\n\ni\n\nπθt πθold\n\n(ut\n\ni|τ t i ) i|τ t\n\ni\n\ni\n\ni (θt\n\nwhere ηt\n\n(ut\n\ni) =\n\ni ) denotes the important sampling weight. The clip (·) clips the values of θi that are outside the range [1 − ε, 1 + ε] and ε is a hyperparameter. At i is a generalized advantage estimator (GAE) (Schulman et al., 2015). To optimize the central critic Vψ({ot i}N i=1), we mix agents’ observation-action pairs and output an N -head vector where each value corresponds to the agent’s value:\n\ni, ut\n\nL(ψ) := ED′∼D\n\n(cid:104)(cid:0)yt − V ̄ψ({ot\n\ni, ut\n\ni}N\n\ni=1)(cid:1)2(cid:105)\n\n,\n\n(4)\n\n(cid:104)(cid:80)k−1\n\nl=0 γlrt+l\n\ni + γkV ̄ψ({ot+k\n\nwhere yt = is a sample from the replay buffer D. In complex scenarios, e.g., Melting Pot, with an agent’s observation as input, its action would not impact other agents’ return, since the global states contain redundant information that deteriorates multi-agent learning. We present the whole training process, the network architectures of the agent and the central critic in Appx. D.\n\nis a vector of k-step returns, and D′\n\ni=1)[i]\n\n, ut+k\n\n}N\n\ni=1\n\ni\n\ni\n\n(cid:105)N\n\n6 EXPERIMENTS\n\nIn this section, to verify the effectiveness of RPM in improving the generalization of MARL, we conduct extensive experiments on Melting Pot and present the empirical results. We first introduce Melting Pot, baselines and experiment setups. Then we present the main results of RPM. To demonstrate that ψ is important for RPM, we conducted ablation studies. We finally showcase a case study to visualize RPM. To sum up, we answer the following questions: Q1: Is RPM effective in boosting the generalization performance of MARL agents? Q2: How does the value of ψ impact RPM training? Q3: Does RPM gather diversified policies and trajectories?\n\n6.1 EXPERIMENTAL SETUP\n\nMelting Pot. To demonstrate that RPM enables MARL agents to learn generalizable behaviors, we carry out extensive experiments on DeepMind’s Melting Pot (Leibo et al., 2021). Melting Pot is a suite of testbeds for the generalization of MARL methods. It proposes a novel evaluation pipeline for the evaluation of the MARL method in various domains. That is, all MARL agents are trained in the substrate; during evaluation, some agents are selected as the focal agents and the rest agents become the background agents (pre-trained policies of MARL models will be loaded); the evaluation scenarios share the same physical properties as the substrates. Melting Pot environments possess many properties, such as temporal coordination and\n\n5\n\nFigure 4: The green box to the lower left shows the agent’s observation.\n\nobsAgentsCNNCNNGRUMLPπ!\"+featuresMLPH×W×CH: HeightW:WidthC:ChannelMLPPublished as a conference paper at ICLR 2023\n\nFigure 5: Melting Pot environments. More information can be found in Appx. A.\n\nTable 1: Properties of Melting Pot environments. The first column shows the properties and the first row lists environments. ✓ mark indicates the environment possessing the corresponding property while ✗ mark stands for the environment that does not own the corresponding property. Refer to Appx. A for more information.\n\nStag Hunt\n\nPure\n\nCoordination\n\nClean Up\n\nPrisoners’\n\nRational\n\nDilemma\n\nCoordination\n\nChicken Game\n\nTemporal Coordination\n\nReciprocity\n\nDeception\n\nFair Resource Sharing\n\nConvention Following\n\nTask Partitioning\n\nTrust & Partnership\n\nFree Riding\n\n✗ ✓\n✓ ✗\n✓ ✗\n✓ ✗\n\n✗ ✓\n✗ ✗\n✓ ✗\n✗ ✗\n\n✓ ✓\n✓ ✓\n✓ ✓\n✗ ✓\n\n✗ ✓\n✓ ✗\n✗ ✓\n✗ ✗\n\n✗ ✗\n✗ ✗\n✓ ✗\n✗ ✗\n\n✗ ✓\n✓ ✗\n✓ ✗\n✓ ✗\n\nfree riding as depicted in Table 1. An agent performing well in such environments indicates that its behaviors exhibit these properties. In Fig. 4, the agent’s observation is shown in the green box to the lower left of the state (i.e., the whole image). The agent is in the lower middle of the observation. The deep neural network architecture of the agent’s policy is shown on the left. More information about substrates, scenarios, neural network architectures and training details can be found in Appx. D.\n\nBaselines. Our baselines are MAPPO (Yu et al., 2021), MAA2C (Papoudakis et al., 2021), OPRE (Vezhnevets et al., 2020), heuristic fictitious self-play (HFSP) (Heinrich, 2017; Berner et al., 2019) and RandNet (Lee et al., 2019). MAPPO and MAA2C are MARL methods that achieved outstanding performance in various multi-agent scenarios (Papoudakis et al., 2021). OPRE was proposed for the generalization of MARL. RandNet is a general method for the generalization of RL by introducing a novel component in the convolutional neural network. HFSP is a general self-play method for obtaining equilibria in competitive games, we use it by using the policies saved by RPM.\n\nTraining setup. We use 6 representative substrates (Fig. 5) to train MARL policies and choose some evaluation scenarios from each substrate as our evaluation testbed. The properties of the environments are listed in Table 1. We train agents in Melting Pot substrates for 200 million frames with 3 random seeds for all methods. Our training framework is distributed with 30 CPU actors to collect experiences and 1 GPU for the learner to learn policies. We implement our actors with Ray (Moritz et al., 2018) and the learner with EPyMARL (Papoudakis et al., 2021). We use mean-std to measure the performance of all methods. The bold lines in all figures are mean values, and the shades stand for the standard deviation. Due to a limited computation budget, it is redundant to compare our method with other methods, such as QMIX (Rashid et al., 2018) and MADDPG (Lowe et al., 2017) as MAPPO outperforms them. All experiments are conducted on NVIDIA A100 GPUs.\n\n6.2 EXPERIMENT RESULTS\n\nTo answer Q1, we present the evaluation results of 17 Melting Pot evaluation scenarios in Fig. 6. Our method can boost MARL in various evaluation scenarios, which have different properties, as shown in Table 1. In Chicken Game (CG) 1-2 (the number stands for the number of the evaluation scenario of Chicken Game), RPM outperforms its counterparts by a convincing margin. HFSP performs no better than RPM. RandNet gets around 15 evaluation mean returns on Chicken Game (CG) 1. MAA2C and OPRE perform nearly random (the red dash lines indicate the random result) in the two scenarios. In Pure Coordination (PC) 1-3, Rational Coordination (PC) 1-3 and Prisoners’ Dilemma (PD) 1-3, most baselines perform poorly. In Stag Hunt (SH) 1-3 and Clean Up (CU) 1-2, MAPPO and MAA2C perform unsatisfactorily. We can also find that HFSP even gets competitive performance in Stag Hunt (SH) 1-3. However, HFSP performs poorly in Pure Coordination (PC) 1-3, Rational Coordination (RC) 1-3 and Prisoners’ Dilemma (PD) 1-3. Therefore, the vanilla self-play method cannot directly be applied to improve the generalization of MARL methods. In summary, RPM boosts the performance up to around 818% on average compared with MAPPO on 6 evaluation scenarios. To answer Q2, we present experimental results of the impact of ψ and the sampling ratio in HFSP in the following.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: Evaluation results of RPM and baselines in 17 scenarios. The red dash horizontal lines indicate the results of random policy. The optimal (opt) values are shown in each sub-figure and were gathered from (Leibo et al., 2021), which an exploiter generated. The exploiter was trained in the evaluation scenarios with RL methods, and the training time steps were 1,000 M.\n\nFigure 7: Histograms of training episode returns.\n\n6.3 ABLATION STUDY\n\nThe Impact of ψ. To investigate which value of ψ has the greatest impact on RPM performance, we conduct ablation studies by (i) removing ranks and sampling from the checkpoint directly; (ii) reducing the number of ranks by changing the value of ψ. As shown in Fig. 8, without ranks (sampling policies without ranks randomly), RPM cannot attain stable performance in some evaluation scenarios. Especially in Pure Coordination (PC) 1-3, the result is low and has a large variance. In RPM, choosing the right interval ψ can improve the performance, as shown in the results of Pure Coordination (PC) 13 and Prisoners’ Dilemma (PD) 1-3, showing that the value of ψ is important for RPM. We summarize the results and values of ψ in Table 2 and Table 3.\n\nFigure 8: Ablation Studies: the performance of RPM with 3 types of ψ and Random sampling (without ranks).\n\nTable 2: Ablation study: the averaged value of the last three evaluation episode returns. Curves are in Fig. 8.\n\nTable 3: ψ values. ψ∗ indicates the values of ψ used to get results in Fig. 6.\n\nEval Scenarios\n\nRPM Random\n\nTypes of ψ\n\n1\n\n2\n\n3\n\nEval Scenarios\n\nψ∗\n\nTypes of ψ\n\nPure Coordination 1\n\nPure Coordination 2\n\nPure Coordination 3\n\nPrisoners’ Dilemma 1\n\nPrisoners’ Dilemma 2\n\nPrisoners’ Dilemma 3\n\n0.78\n\n0.23\n\n0.70\n\n13.90\n\n19.60\n\n22.31\n\n0.18\n\n0.16\n\n0.19\n\n10.11\n\n10.41\n\n10.28\n\n0.33\n\n0.24\n\n0.37\n\n10.70\n\n13.76\n\n19.80\n\n0.39\n\n0.17\n\n0.33\n\n8.70\n\n17.74\n\n11.74\n\n0.42\n\n0.27\n\n0.42\n\n3.20\n\n14.96\n\n9.76\n\nPure Coordination 1\n\nPure Coordination 2\n\nPure Coordination 3\n\nPrisoners’ Dilemma 1\n\nPrisoners’ Dilemma 2\n\nPrisoners’ Dilemma 3\n\n0.01\n\n0.01\n\n0.01\n\n0.02\n\n0.02\n\n0.02\n\n1\n\n0.1\n\n0.1\n\n0.1\n\n0.2\n\n0.2\n\n0.2\n\n2\n\n0.5\n\n0.5\n\n0.5\n\n1\n\n1\n\n1\n\n3\n\n1\n\n1\n\n1\n\n5\n\n5\n\n5\n\nThe Sampling Ratio in HFSP HFSP shows comparable results in some scenarios in Figure 6. In Figure 6, the sampling ratio of HFSP is 0.3. We are interested in studying the impact of the sampling\n\n7\n\n0501001502000204060opt: 98.9CG 10501001502000510opt: 14.3CG 2050100150200010opt: 36.4CG 3050100150200051015opt: 65.9SH 1050100150200010opt: 54.4SH 205010015020001020opt: 53.8SH 30501001502000200400opt: 722.6CU 10501001502000100opt: 385.9CU 20501001502000.000.250.500.75opt: 4.4PC 10501001502000.00.20.4opt: 3.2PC 20501001502000.00.51.0opt: 3.2PC 3050100150200010opt: 55.7PD 10501001502000102030opt: 60.8PD 205010015020001020opt: 36.8PD 30501001502000123opt: 11.9RC 105010015020001opt: 7.7RC 2050100150200024opt: 13.1RC 3RPM (ours)MAPPOMAA2CRandNetOPREHFSPRandom PolicyTraining Steps (million)Eval Return Mean0100250500750CountsChicken Game0100250500750Stag Hunt0100020004000Clean Up01050010001500Pure Coordination0100250500750Prisoners Dilemma02010002000Rational CoordianationTraining Episode Returns0501001502000.000.250.500.75PC 10501001502000.00.20.40.6PC 20501001502000.000.250.500.751.00PC 3050100150200051015PD 10501001502000102030PD 20501001502000102030PD 3Training Steps (million)Eval Return MeanType 1Type 2Type 3RPMRPM_RandomRandom PolicyPublished as a conference paper at ICLR 2023\n\nFigure 9: Ablation Studies: the results of HFSP with different sampling ratios.\n\nFigure 10: Results analysis. (a) The evaluation results of RPM on Stag Hunt (SH) 1; (b) The number of RPM keys during training; (c) The distribution of the keys of RPM during training; (d) The histogram of the keys of RPM at timestep 200M during training.\n\nratio in HFSP on evaluation performance. We conduct experiments in CU 1 and 2, PC 1 and 3 and PD 1 and 3. The sampling ratio list is [0.9, 0.7, 0.5, 0.3, 0.1]. We use the default training setup and use 3 random seeds. HFSP shows comparable results in PC 2 and 3, but its performances are poor in CU 1 and 2 and PD 2 and 3. As shown in Figure 9, HFSP heavily relies on the sampling ratio. HFSP should be carefully tuned on each substrate to attain good performance, which is not feasible. In contrast, RPM is stable (the sampling ratio is 0.5) on all substrates. HFSP can also perform well in substrates such as PC and PD, where the return-checkpoint count distribution is more uniform. The absence of ranks leads to the frequent sampling of policies with high count values in substrates that have skewed return-checkpoint count distribution, thereby reducing the diversity of training data. Such distributions typically comprise a large number of policies with suboptimal performance.\n\n6.4 CASE STUDY\n\nWe showcase how RPM helps to train the focal agents to choose the right behaviors in the evaluation scenario after training in the substrate. To illustrate the trained performance of RPM agents, we use the RPM agent trained on Stag Hunt and run the evaluation on Stag Hunt 1. In Stag Hunt, there are 8 agents. Each agent collects resources that represent ‘hare’ (red) or ‘stag’ (green) and compares inventories in an interaction, i.e., encounter. The results of solving the encounter are the same as the classic Stag Hunt matrix game. In this environment, agents are facing tension between the reward for the team and the risk for the individual. In Stag Hunt 1, One focal agent interacts with seven pretrained background agents. All background agents were trained to play the ‘stag’ strategy during the interaction1. The optimal policy for the focal agent is also to play ‘stag’. However, it is challenging for agents to detect other agents’ strategy since such a behavior may not persist in the substrate. Luckily, RPM enables focal agents to behave correctly in this scenario.\n\nTo answer Q3, we present the analysis of RPM on the substrate Stag Hunt and its evaluation scenario SH 1 in Fig. 10. We can find that in Fig. 10 (b), the number of the keys in RPM is growing monotonically during training and the maximum number of the keys in RPM is over 20, showing that agents trained with RPM discover many novel patterns of multi-agent interaction and new keys are created and the trained models are saved in RPM. Meanwhile, the evaluation performance is also increasing in SH 1 as depicted in Fig. 10 (a). In Fig. 10 (c), it is interesting to see that the distribution of the keys of RPM is expanding during training. In the last 25 million training steps, the last distribution of RPM keys covers all policies of different performance levels, ranging from 0 to 14. By utilizing RPM, agents can collect diversified multi-agent trajectories for multi-agent training. Fig. 10 (d) demonstrates the final histogram of RPM keys after training. There are over 600 trained policies that have a small value of keys. Since agents should explore the environment at the early\n\n1This preference was trained with pseudo rewards by Leibo et al. (2021) and the trained models are available\n\nat this link: https://github.com/deepmind/meltingpot\n\n8\n\n0501001502000250CU 10501001502000100CU 20501001502000.00.5PC 10501001502000.00.5PC 205010015020001PC 3050100150200010PD 1050100150200020PD 2050100150200020PD 3Training Steps (million)Eval Return MeanRPM (ours)HFSP-0.9HFSP-0.7HFSP-0.5HFSP-0.3HFSP-0.1Random Policy(a)(b)(c)(d)Published as a conference paper at ICLR 2023\n\nstage of training, it is reasonable to find that many trained policies of RPM keys have low training episode returns. After 50 million training steps, RPM has more policies with higher training episode returns. Note that the maximum training episode return of RPM keys is over 14 while the maximum mean evaluation return of RPM shown in Fig. 10 (a) is around 14.\n\nOur experiments show that training policies with good performance in the substrate is crucial for improving generalization performance in the evaluation scenarios. When MARL agents perform poorly in the substrate, the evaluation performance will also be inferior or random, making it hard to have diversified policies. We show the results in Appx. E.\n\n7 RELATED WORKS\n\nRecent advances in MARL (Yang & Wang, 2020; Zhang et al., 2021) have demonstrated its success in various complex multi-agent domains, including multi-agent coordination (Lowe et al., 2017; Rashid et al., 2018; Wang et al., 2021b), real-time strategy (RTS) games (Jaderberg et al., 2019; Berner et al., 2019; Vinyals et al., 2019), social dilemma (Leibo et al., 2017; Wang et al., 2018; Jaques et al., 2019; Vezhnevets et al., 2020), multi-agent communication (Foerster et al., 2016; Yuan et al., 2022), asynchronous multi-agent learning (Amato et al., 2019; Qiu et al., 2022), open-ended environment (Stooke et al., 2021), autonomous systems (Hüttenrauch et al., 2017; Peng et al., 2021) and game theory equilibrium solving (Lanctot et al., 2017; Perolat et al., 2022). Despite strides made in MARL, training generalizable behaviors in MARL is yet to be investigated.\n\nRecently, generalization in RL (Packer et al., 2018; Song et al., 2019; Ghosh et al., 2021; Lyle et al., 2022) has achieved much progress in domain adaptation (Higgins et al., 2017) and procedurally generated environments (Lee et al., 2019; Igl et al., 2020; Zha et al., 2020). However, there are few works of generalization in MARL domains (Carion et al., 2019; Vezhnevets et al., 2020; Mahajan et al., 2022; McKee et al., 2022). Recently, Vezhnevets et al. (2020) propose a hierarchical MARL method for agents to play against opponents it hasn’t seen during training. However, the evaluation scenarios are only limited to simple competitive scenarios. Mahajan et al. (2022) studied the generalization in MARL empirically and proposed theoretical findings based on successor features (Dayan, 1993). However, no method to achieve generalization in MARL was proposed in (Mahajan et al., 2022).\n\nAd-hoc team building (Stone & Kraus, 2010; Gu et al., 2021) models the multi-agent problem as a single-agent learning task. In ad-hoc team building, one ad-hoc agent is trained by interacting with agents that have fixed pretrained policies and the non-stationarity issue is not severe. However, in our formulation, non-stationarity is the main obstacle to MARL training. In addition, there is only one ad-hoc agent evaluated by interacting agents that are unseen during training, while there can be more than one focal agent in our formulation as defined in Definition 2, thus making our formulation general and challenging. There has been a growing interest in applying self-play to solve complex games (Heinrich et al., 2015; Silver et al., 2018; Hernandez et al., 2019; Baker et al., 2019); however, its value in enhancing the generalization of MARL agents has yet to be examined. Due to space constraints, we discuss meta-learning (Al-Shedivat et al., 2018; Kim et al., 2021) and population-based training (Strouse et al., 2021; Lupu et al., 2021; Tang et al., 2021) works in Appx. F.\n\n8 CONCLUSION, LIMITATIONS AND FUTURE WORK\n\nIn this paper, we consider the problem of achieving generalizable behaviors in MARL. We first model the problem with Markov Game. To train agents that can interact with agents that possess unseen policies. We propose a simple yet effective method, RPM, to collect diversified multi-agent interaction data. We save policies in RPM by ranking the training episode return. Empirically, RPM significantly boosts the performance of MARL agents in various Melting Pot evaluation scenarios.\n\nRPM’s performance is dependent on the appropriate value of ψ. Several attempts may be needed to determine the correct value of ψ for RPM. We are interested in discovering broader measures for ranking policies that do not explicitly consider the training episode return. Recently, there has been a growing interest in planning in RL, especially with model-based RL. We are interested in exploring the direction of applying planning and opponent/teammate modelling for attaining generalized MARL policies for future work. Agents are engaged in complex interactions in multi-agent scenarios. Devising novel self-play methods is our future direction.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nWe addressed the relevant aspects in our conclusion and have no conflicts of interest to declare.\n\nREPRODUCIBILITY STATEMENT\n\nWe provide detailed descriptions of our experiments in the appendix and list all relevant parameters in Table 4 and Table 5 in Appx. D. The code can be found at this link: https://sites.google. com/view/rpm-iclr2023/.\n\nACKNOWLEDGMENTS\n\nWe would like to thank the anonymous reviewers for their suggestions. We thank the support from Xinyi Wan, Jiahao Ji and Xiangfan Li of the infrastructure team at Sea AI Lab. Wei Qiu and Bo An are supported by the National Research Foundation, Singapore under its Industry Alignment Fund – Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nMaruan Al-Shedivat, Trapit Bansal, Yura Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. In International Conference on Learning Representations, 2018.\n\nChristopher Amato, George Konidaris, Leslie P Kaelbling, and Jonathan P How. Modeling and planning with macro-actions in decentralized pomdps. Journal of Artificial Intelligence Research, 64:817–859, 2019.\n\nPierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of\n\nthe AAAI Conference on Artificial Intelligence, volume 31, 2017.\n\nBowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. In International Conference on Learning Representations, 2019.\n\nAndre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In International Conference on Machine Learning, pp. 501– 510. PMLR, 2018.\n\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D ̨ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n\nGeorge W Brown. Iterative solution of games by fictitious play. Act. Anal. Prod Allocation, 13(1):\n\n374, 1951.\n\nNicolas Carion, Nicolas Usunier, Gabriel Synnaeve, and Alessandro Lazaric. A structured prediction approach for generalization in cooperative multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019.\n\nKyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.\n\nPeter Dayan. Improving generalization for temporal difference learning: The successor representation.\n\nNeural Computation, 5(4):613–624, 1993.\n\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance In International Conference on Machine Learning, pp. weighted actor-learner architectures. 1407–1416. PMLR, 2018.\n\nJakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2137–2145, 2016.\n\nScott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In International conference on machine learning, pp. 1587–1596. PMLR, 2018.\n\nDibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine. Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability. Advances in Neural Information Processing Systems, 34, 2021.\n\nPengjie Gu, Mengchen Zhao, Jianye Hao, and Bo An. Online ad hoc teamwork under partial\n\nobservability. In International Conference on Learning Representations, 2021.\n\nJohannes Heinrich. Reinforcement learning from self-play in imperfect-information games. PhD\n\nthesis, UCL (University College London), 2017.\n\nJohannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games. In\n\nInternational conference on machine learning, pp. 805–813. PMLR, 2015.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nDaniel Hernandez, Kevin Denamganaï, Yuan Gao, Peter York, Sam Devlin, Spyridon Samothrakis, and James Alfred Walker. A generalized framework for self-play training. In 2019 IEEE Conference on Games (CoG), pp. 1–8. IEEE, 2019.\n\nIrina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in reinforcement learning. In International Conference on Machine Learning, pp. 1480–1490. PMLR, 2017.\n\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: how do neural networks generalise? Journal of Artificial Intelligence Research, 67:757–795, 2020.\n\nMaximilian Hüttenrauch, Adrian Šoši ́c, and Gerhard Neumann. Guided deep reinforcement learning for swarm systems. In AAMAS 2017 Autonomous Robots and Multirobot Systems (ARMS) Workshop, 2017.\n\nMaximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. Transient non-stationarity and generalisation in deep reinforcement learning. In International Conference on Learning Representations, 2020.\n\nMax Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Humanlevel performance in 3D multiplayer games with population-based reinforcement learning. Science, 364(6443):859–865, 2019.\n\nNatasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In International Conference on Machine Learning, pp. 3040–3049. PMLR, 2019.\n\nDong Ki Kim, Miao Liu, Matthew D Riemer, Chuangchuang Sun, Marwa Abdulhai, Golnaz Habibi, Sebastian Lopez-Cot, Gerald Tesauro, and Jonathan How. A policy gradient algorithm for learning to learn in multiagent reinforcement learning. In International Conference on Machine Learning, pp. 5541–5550. PMLR, 2021.\n\nMarc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Pérolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. Advances in Neural Information Processing Systems, 30, 2017.\n\nKimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network randomization: A simple technique In International Conference on Learning\n\nfor generalization in deep reinforcement learning. Representations, 2019.\n\nJoel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multi-agent reinforcement learning in sequential social dilemmas. arXiv preprint arXiv:1702.03037, 2017.\n\nJoel Z Leibo, Edgar A Dueñez-Guzman, Alexander Vezhnevets, John P Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charlie Beattie, Igor Mordatch, and Thore Graepel. Scalable evaluation of multi-agent reinforcement learning with melting pot. In International Conference on Machine Learning, pp. 6187–6199. PMLR, 2021.\n\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations, 2016.\n\nMichael L Littman. Markov games as a framework for multi-agent reinforcement learning. In\n\nMachine learning proceedings 1994, pp. 157–163. Elsevier, 1994.\n\nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pp. 6379–6390, 2017.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAndrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. Trajectory diversity for zero-shot coordination. In International Conference on Machine Learning, pp. 7204–7213. PMLR, 2021.\n\nClare Lyle, Mark Rowland, Will Dabney, Marta Kwiatkowska, and Yarin Gal. Learning dynamics\n\nand generalization in reinforcement learning. arXiv preprint arXiv:2206.02126, 2022.\n\nAnuj Mahajan, Mikayel Samvelyan, Tarun Gupta, Benjamin Ellis, Mingfei Sun, Tim Rocktäschel, and Shimon Whiteson. Generalization in cooperative multi-agent systems. arXiv preprint arXiv:2202.00104, 2022.\n\nKevin R McKee, Joel Z Leibo, Charlie Beattie, and Richard Everett. Quantifying the effects of environment and population diversity in multi-agent reinforcement learning. Autonomous Agents and Multi-Agent Systems, 36(1):1–16, 2022.\n\nAndrew N Meltzoff, Patricia K Kuhl, Javier Movellan, and Terrence J Sejnowski. Foundations for a\n\nnew science of learning. science, 325(5938):284–288, 2009.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928–1937. PMLR, 2016.\n\nPhilipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. Ray: A distributed framework for emerging {AI} applications. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pp. 561–577, 2018.\n\nFrans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value functions\n\nfor decentralized POMDPs. Journal of Artificial Intelligence Research, 32:289–353, 2008.\n\nCharles Packer, Katelyn Gao, Jernej Kos, Philipp Krähenbühl, Vladlen Koltun, and Dawn Song. Assessing generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282, 2018.\n\nGeorgios Papoudakis, Filippos Christianos, Lukas Schäfer, and Stefano V Albrecht. Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.\n\nZhenghao Peng, Quanyi Li, Ka Ming Hui, Chunxiao Liu, and Bolei Zhou. Learning to simulate self-driven particles system with coordinated policy optimization. Advances in Neural Information Processing Systems, 34:10784–10797, 2021.\n\nJulien Perolat, Bart de Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T Connor, Neil Burch, Thomas Anthony, et al. Mastering the game of stratego with model-free multiagent reinforcement learning. Science, 2022.\n\nWei Qiu, Weixun Wang, Rundong Wang, Bo An, Yujing Hu, Svetlana Obraztsova, Zinovi Rabinovich, Jianye Hao, Yingfeng Chen, and Changjie Fan. Off-beat multi-agent reinforcement learning. arXiv preprint arXiv:2205.13718, 2022.\n\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295–4304, 2018.\n\nTabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 33:10199–10210, 2020.\n\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419): 1140–1144, 2018.\n\nXingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, and Behnam Neyshabur. Observational overfitting\n\nin reinforcement learning. In International Conference on Learning Representations, 2019.\n\nPeter Stone and Sarit Kraus. To teach or not to teach?: decision making under uncertainty in ad hoc\n\nteams. In AAMAS, pp. 117–124, 2010.\n\nAdam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. Open-ended learning leads to generally capable agents. arXiv preprint arXiv:2107.12808, 2021.\n\nDJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett. Collaborating with humans without human data. Advances in Neural Information Processing Systems, 34: 14502–14515, 2021.\n\nRobert Sugden. Rights, co-operation and welfare. In The Economics of Rights, Co-operation and\n\nWelfare, pp. 170–182. Springer, 2005.\n\nRichard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n\nRichard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181–211, 1999.\n\nRichard Stuart Sutton. Temporal credit assignment in reinforcement learning. PhD thesis, University\n\nof Massachusetts Amherst, 1984.\n\nArdi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning. PLoS ONE, 12(4), 2017.\n\nZhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon Du, Yu Wang, and Yi Wu. Discovering diverse multi-agent strategic behavior via reward randomization. arXiv preprint arXiv:2103.04564, 2021.\n\nMichael Tomasello. Origins of human communication. MIT press, 2010.\n\nAlexander Vezhnevets, Yuhuai Wu, Maria Eckstein, Rémi Leblond, and Joel Z Leibo. Options as responses: Grounding behavioural hierarchies in multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 9733–9742. PMLR, 2020.\n\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n\nJianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: Duplex dueling\n\nmulti-agent q-learning. In International Conference on Learning Representations, 2021a.\n\nWeixun Wang, Jianye Hao, Yixi Wang, and Matthew Taylor. Towards cooperation in sequential prisoner’s dilemmas: a deep multiagent reinforcement learning approach. arXiv preprint arXiv:1803.00162, 2018.\n\nYihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. DOP: Off-policy multiagent decomposed policy gradients. In International Conference on Learning Representations, 2021b.\n\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. Machine learning, 8(3):229–256, 1992.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nYaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game\n\ntheoretical perspective. arXiv preprint arXiv:2011.00583, 2020.\n\nChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.\n\nLei Yuan, Jianhao Wang, Fuxiang Zhang, Chenghe Wang, ZongZhang Zhang, Yang Yu, and Chongjie Zhang. Multi-agent incentive communication via decentralized teammate modeling. Proceedings of the AAAI Conference on Artificial Intelligence, 36(9):9466–9474, Jun. 2022.\n\nDaochen Zha, Wenye Ma, Lei Yuan, Xia Hu, and Ji Liu. Rank the episodes: A simple approach for exploration in procedurally-generated environments. In International Conference on Learning Representations, 2020.\n\nKaiqing Zhang, Zhuoran Yang, and Tamer Ba ̧sar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of Reinforcement Learning and Control, pp. 321–384, 2021.\n\n15",
    "reference": "# Summary Of The Paper\n\nThe authors propose a method for containing multiple distinct behaviours (or policies) in a single agent. To do this they store a dictionary of policies observed during training, where the key relates to returns observed during training (this is discretised to keep the buffer size reasonable). \n\nThis is actually very similar to simply holding a population of agents during training. At the start of training the dictionary (size N)  is initialised with N random policies which are all selected randomly and trained with. RPM uses self-play from its buffer to train more agents. Unlike self-play or population-play their is no risk of degeneracy as values are only overwritten for agents which produce the same return.\n\n# Strength And Weaknesses\n\n### Strengths\n\n* Idea is simple and good first step in the melting pot game\n* Good ablations\n\n### Weaknesses\n* No explanation of the RPM update rule.\n* Method makes a major assumption that return is enough to distinguish behaviour. A toy example of where this fails would be that grim trigger and tit-for-tat would get similar returns in IPD against a defective agent but intuitively have some variance in behaviour (e.g.  tit-for-tat is forgiving but grim trigger is not)\n* Unclear how large a RPM buffer should be\n* This method is computationally much more intensive than baselines, some analysis to count the different number of timesteps used in training or when convergence is achieved would also be reasonable.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n### Clarity\n* It is unclear how agents are chosen at evaluation time to be entered into the substrate.\n* The focal agent's return is clearly dependent on the co-player trained with, how do you handle this instability during training?\n\n### Quality\n* Typos in \"newly collected trajecotries and πθb\"\n* In the ablations diagrams you refer to  \\phi type III but do not explain what this is.\n\n### Novelty\nMethod is sufficiently novel - however it is very similar to [Fictitious Co-Play](https://arxiv.org/abs/2110.08176), the agents stored in the buffer are very similar to a population of agents and thus explaining the differences in related work could be useful. \n\n### Reproducibility\nNo code is provided, nor is method clear enough for reproducibility.\n\n# Summary Of The Review\n\nSimple method but paper lacks clarity.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "SIMPLICIAL EMBEDDINGS IN SELF-SUPERVISED LEARNING AND DOWNSTREAM CLASSIFICATION\n\nSamuel Lavoie(cid:5)∗, Christos Tsirigotis(cid:5), Max Schwarzer(cid:5), Ankit Vani(cid:5), Michael Noukhovitch(cid:5), Kenji Kawaguchi‡, Aaron Courville(cid:5)♣ (cid:5) Mila, Université de Montréal, ‡ National University of Singapore, ♣ CIFAR Fellow\n\nABSTRACT\n\nSimplicial Embeddings (SEM) are representations learned through self-supervised learning (SSL), wherein a representation is projected into L simplices of V dimensions each using a softmax operation. This procedure conditions the representation onto a constrained space during pre-training and imparts an inductive bias for discrete representations. For downstream classification, we provide an upper bound and argue that using SEM leads to a better expected error than the unnormalized representation. Furthermore, we empirically demonstrate that SSL methods trained with SEMs have improved generalization on natural image datasets such as CIFAR100 and ImageNet. Finally, when used in a downstream classification task, we show that SEM features exhibit emergent semantic coherence where small groups of learned features are distinctly predictive of semantically-relevant classes.\n\n1\n\nINTRODUCTION\n\nSelf-supervised learning (SSL) is an emerging family of methods that aim to learn representations of data without manual supervision, such as class labels. Recent works (Hjelm et al., 2019; Grill et al., 2020; Saeed et al., 2020; You et al., 2020) learn dense representations that can solve complex tasks by simply fitting a linear model on top of the learned representation. While SSL is already highly effective, we show that changing the type of representation learned can improve both the performance and interpretability of these methods.\n\nFor this we draw inspiration from overcomplete representations: representations of an input that are non-unique combinations of a number of basis vectors greater than the input’s dimensionality (Lewicki & Sejnowski, 2000). Mostly studied in the context of the sparse coding literature (Gregor & LeCun, 2010; Goodfellow et al., 2012; Olshausen, 2013), sparse overcomplete representations have been shown to increase stability in the presence of noise (Donoho et al., 2006), have applications in neuroscience (Olshausen & Field, 1996; Lee et al., 2007), and lead to more interpretable representations (Murphy et al., 2012; Fyshe et al., 2015; Faruqui et al., 2015). But, the basis vector is learned using linear models (Lewicki & Sejnowski, 2000; Teh et al., 2003).\n\nFigure 1: Linear probe accuracy of BYOL and BYOL + SEM on ImageNet trained for 200 epochs with a ResNet-50 architecture.\n\nIn this work, we show that SSL may be used to learn discrete, sparse and overcomplete representations. Prior work has considered sparse representation but not sparse and overcomplete representation learning with SSL; for example, Dessì et al. (2021) propose to discretize the output of the encoder in a SSL model using Gumbel-Softmax (Jang et al., 2017). However, we show that discretization during pre-training is not necessary to achieve a sparse representation. Instead, we propose to project the encoder’s output into L vectors of V dimensions onto which we apply a softmax function to impart an inductive bias toward sparse one-hot vectors (Correia et al., 2019; Goyal et al., 2022), also alleviating the need to use high-variance gradient estimators to train the encoder. We refer to this embedding as Simplicial Embeddings (SEM), as the softmax functions map the unnormalized representations onto L simplices. The procedure to induce SEM is simple, efficient, and generally applicable.\n\n*Correspondence to: samuel.lavoie.m@gmail.com\n\n1\n\n0.41.13.7FLOPs (109)727374757677Top-1 accuracyBYOLBYOL (x2)BYOL (x4)BYOL+SEMBYOL+SEM (x2)BYOL+SEM (x4)The SSL pre-training phase, used with SEM, learns a set of L approximately one-hot vectors. Key to controlling the inductive bias of SEM during pre-training is the softmax temperature parameter: the lower the temperature, the stronger the bias toward sparsity. Consistent with earlier attempts at sparse representation learning (Coates & Ng, 2011), we find that the optimal sparsity for pre-training need not match the optimal level for downstream learning.\n\nFor downstream classification, we may discretize the learned representation by, for example, taking the argmax for each simplex. But, we can also use SEM to control the representation’s expressivity via the softmax’s temperature. We provide a theoretical bound showing that the expected error follows a trade-off between the training error and the representations’ expressivity that can be controlled by the softmax’s temperature used to normalize the representation for downstream classification. Our bound also shows improved expected error as we increase L and V for SEM.\n\nSEM is generally applicable to recent SSL methods. Applying it to seven different SSL methods (Chen et al., 2020b; He et al., 2020; Grill et al., 2020; Caron et al., 2020; 2021; Zbontar et al., 2021; Bardes et al., 2022), we find accuracy increases of 2% to 4% on CIFAR-100. We observe monotonic improvement as we increase the number of vectors L, showing the benefit of the overcomplete representations learned by SEM, while this improvement is absent when we do not use softmax normalization. When training a SSL method with SEM on ImageNet we also observe improvements on in-distribution compared to the baseline (Figure 1). We also observe improvement on out-of-distribution test sets, semi-supervised learning benchmark and transfer learning datasets, demonstrating the potential of SEM for large scale applications. Finally, we find that SEM learns features that are closely aligned to the semantic categories in the data. This demonstrates that SEM learns disentangled and interpretable representations, as previously observed in overcomplete representations (Faruqui et al., 2015).\n\n2 RELATED WORK\n\nThe softmax operation has been used in other contexts, notably as an architectural component for models to attend to context-dependent queries via, for example, an attention mechanism (Bahdanau et al., 2016; Vaswani et al., 2017; Correia et al., 2019; Goyal et al., 2022), a mixture of experts (Jordan & Jacobs, 1993) or memory augmented networks (Graves et al., 2014). This operation is also used for the computation of several SSL objectives such as InfoNCE (van den Oord et al., 2018; Hjelm et al., 2019), and as a normalization of the output to compute the objective in DINO and SWaV (Caron et al., 2020; 2021). Different from these, our method places the softmax at the output of an encoder to constrain the representation into a set of L sparse vectors.\n\nSimilar to our approach, other architectural constraints such as Dropout (Srivastava et al., 2014), BatchNorm (Ioffe & Szegedy, 2015) and LayerNorm (Ba et al., 2016) also improve the training of large neural networks. However, contrary to SEMs, they are not used to induce sparsity on the representation or control its expressivity for downstream tasks. Closer to our work, Liu et al. (2021) propose to constrain the expressivity of the representation of a neural network with a set of discrete-valued symbols obtained using a set of Vector Quantized (Oord et al., 2018) bottlenecks. Similarly, Dessi et al. (2021) propose a communication game with a discrete bottleneck. The idea of discretizing the encoder’s output is similar to using SEM vectors that are one-hot (e.g. temperature = 0) and only one symbol (e.g. L = 1, V = 2048). In our work, we find success in removing the hard-discretization and having L > 1, which can be interepreted as combining several symbols.\n\n3 SIMPLICIAL EMBEDDINGS\n\nSimplicial Embeddings (SEM) are representations that can be integrated easily into a contrastive learning model (Hjelm et al., 2019; Chen et al., 2020b), the BYOL method (Grill et al., 2020), and other SSL methods (Caron et al., 2020; 2021; Zbontar et al., 2021). For example, in BYOL, we insert the SEM after the encoder and before the projector and the rest is unchanged as shown in Figure 2c. In this figure, t and t(cid:48) are augmentations defined by the practitioner, ξ are parameters of the target network that are updated as moving average of the parameters θ of the online networks trained with SGD. So, ξ are updated as follow: ξ ← αξ + (1 − α)θ, with α ∈ [0, 1]. To produce SEM representation, the encoder’s output e is embedded into L vectors zi ∈ RV . A temperature parameter τ scales zi, and then a softmax re-normalizes each vector zi to produce ̄zi.\n\n2\n\n(a)\n\n(b)\n\n(c)\n\nFigure 2: (a) Procedure to obtain Simplicial Embeddings (SEM). A matrix z ∈ RL×V contains L vectors zi ∈ RV . The vectors zi are normalized with στ , the softmax operation with temperature τ . The normalized vectors are concatenated into the vector ˆz. (b) Normalized histogram of the entropies H( ̄zi) of each simplex ̄zi for the sample in CIFAR’s training dataset at the end of pre-training with various τ . The peak at ln(2) for τ = 0.01 and τ = 0.1 are a large number of simplices with two elements close to 0.5. (c) Integration of SEM with BYOL (Grill et al., 2020). The encoder outputs a latent vector which is embedded into the matrix z ∈ RL×V and then transformed into SEM.\n\nFinally, the normalized vectors ̄zi are concatenated to produce the vector ˆz of length L · V . We illustrate SEM in Figure 2a. Formally, the re-normalization is as follows:\n\n ̄zi := στ (zi),\n\nστ (zi)j =\n\nezij /τ k=1 ezik/τ\n\n(cid:80)V\n\n,\n\nˆz := Concat( ̄z1, . . . , ̄zL),\n\n∀i ∈ [L], ∀j ∈ [V ].\n\n(1)\n\n3.1\n\nINDUCTIVE BIAS TOWARDS SPARSITY DURING PRE-TRAINING\n\nIn SEM, L controls the numbers of simplices and V controls the dimensionality of each simplex. As such, the higher V is, the sparser the representation can be. During pre-training, the constraint induced by embedding the representation into a simplex biases each vector towards sparse vectors by creating a zero-sum competition between the components of the vector. In order for a component to increase by α, then the other elements must decrease by α, and all elements are bounded by 0. For networks to learn useful features and minimize their objective, they must prioritize some components at the expense of others. The strength of this bias is controlled via the pretraining temperature τp of the softmax, and the size of the vectors V as it was noted in the context of attention (Vaswani et al., 2017; Wang et al., 2021b). For SSL methods with a target network, the temperature for the target network can be different to the online network’s as no gradient is back-propagated through it.\n\nTo visualize the effect of the temperature on SEM after pre-training, we interpret each simplex as a probability mass function p( ̄zij) where, for all i ∈ [L], (cid:80)V j=1 p( ̄zij) = 1 and p( ̄zij) ≥ 0 ∀j. The entropy of a simplex ̄zi, defined as H( ̄zi) := − (cid:80)V j=1 p( ̄zij) log p( ̄zij), informs whether the simplex is a sparse or a dense vector. That is, if H( ̄z(x) ) = 0 then the vector is one-hot. On the other hand, if H( ̄z(x) ) = ln(V ) then the vector is dense and uniform. While the temperature τp is merely a scaling of the logits, it has an important control over the learned representation’s entropy and resulting SEM sparsity. We demonstrate this by learning a representation on CIFAR-100 using BYOL, and analyze the entropies of the resulting simplices. In Figure 2b, we plot the histogram of the entropies H( ̄zi), for a given τp, of each simplex for each sample in the training set of CIFAR-100. We observe that\n\ni\n\ni\n\n3\n\nSEMnormalization...Concat0.00.51.01.52.02.5Entropy H(zi)01234: 10.0: 1.0: 0.5: 0.1: 0.01stop-gradSSL losseven after pre-training, small temperatures (τp = 0.01) yields representations that are close to one-hot vectors while high temperatures yields vectors that are close to uniform vectors.\n\nBy pre-training using a softmax, SEMs create representations that are conditioned to fit onto simplices. In pre-training, we select τp for optimal inductive bias: τp too small yields vanishing gradients (Wang et al., 2021b) and τp too large yields a bias that is too weak. We may select a different optimal τd for downstream performance as discussed formally in the next subsection.\n\n3.2 SEM IMPROVEMENT ON THE GENERALIZATION OF THE DOWNSTREAM CLASSIFIER\n\nIn this subsection, we theoretically demonstrate the benefit of training a downstream classifier with SEM normalized input compared to a baseline classifier with unnormalized input. We show that: (1) there is a trade-off between the training loss and the generalization gap, which is controlled by the value of τd (denoted τ := τd in this subsection), (2) SEM can improve the base model performance when we attain good balance in this trade-off, and (3) the improvement due to SEM is expected to increase or stay constant as L and V increase. In the remainder of this subsection, we introduce the notation and assumptions needed to understand and derive the result, then present our theoretical claim and discuss its implications.\n\nNotation. We use a training dataset S = (z(i), y(i))n i=1 of n samples for supervised training of a classifier, using the representation z extracted from the pre-trained model* and the corresponding label y ∈ Y where Y is the space of possible labels. Assume that z ∈ Z = [−1, +1]L×V , which means that z is a matrix with L rows and V columns. We denote the element of z at row i and column j as zij. Let g represent the downstream classifier. We refer to the baseline downstream model with unnormalized input as fbase, and fbase(z) = g(z). The corresponding downstream model trained with the SEM normalization is fSEM(τ )(z) = (g ◦ στ )(z), where στ is applied element-wise along each row of z such that στ (zij) = ezij /τ base and f S SEM(τ ) the base and the SEM normalized models obtained by fitting the dataset S. Finally, let H be the union of the hypothesis spaces of fSEM(τ ) and fbase.\n\nt=1 ezit/τ for j = 1, . . . , V . Moreover, we define f S\n\n(cid:80)V\n\nTo compare the quality of the base model and the model with SEM normalization, we analyze (cid:80)n the generalization gap Ez,y[l(fS(z), y)] − 1 base}, where l : R × Y → R≥0 is the per-sample loss.\n\ni=1 l(fS(z(i)), y(i)) for each fS ∈ {f S\n\nSEM(τ ), f S\n\nn\n\nThe key insight that we exploit for the theorem is that the softmax operation στ controls the expressivity of the input’s representation to g via the temperature τ . We denote φfbase as an upper bound on the expressivity of zi for the baseline model fbase, and φfSEM(τ ) as the upper bound on the expressivity of στ (zi) for the model with SEM normalization fSEM(τ ). The formal definition of φfbase and φfSEM(τ ) requires proof devices that will hinder the readability of this section, so we refer the reader to Appendix A for a detailed definition. Let φf ∈ {φfbase, φfSEM(τ )}. Intuitively, φfS measures the largest possible distance that two embeddings can have such that the largest component remains the same for both embeddings. We note that this measure depends only on V for fbase, and on both V and τ for fSEM(τ ). We use φfS (V, τ ) to denote the measure given by either model and note that τ has no effect for fbase.\n\nAssumptions. We assume that the per-sample loss is bounded such that l(f (z), y) ≤ B for all f ∈ H and for all (z, y) ∈ Z × Y. For example, B = 1 for the 0-1 loss. Next, let ly be the per-sample loss given y. We assume that ly ◦g are uniformly Lipschitz functions for all y ∈ Y and g ∈ GS, where GS is the set of classifiers g returned by the training algorithm using the dataset S. Let R be such a uniform Lipschitz constant. This means that |(ly ◦ g)(σf (z)) − (ly ◦ g)(σf (z(cid:48)))| ≤ R(cid:107)σf (z) − σf (z(cid:48))(cid:107)F , where ly(g ◦ σf (z)) = l(g ◦ σf (z), y), and σf = στ when f = fSEM(τ ) and σf is identity when f = fbase. Finally, we assume that there exists ∆ > 0 such that for all representations z of the underlying distribution we have that for any i ∈ [L], if k = arg maxj∈[V ] zij, then zik ≥ zij + ∆ for any j (cid:54)= k. Since ∆ can be arbitrarily small (e.g. as small as machine precision), this assumption typically holds in practice. We are now ready to state our theoretical claim.\n\nTheorem 1 illuminates the advantage of SEM and the effect of the hyper-parameter τ on the performance of the downstream classifier. We present the proof in Appendix A.\n\n*In this subsection, we refer to the extracted representation as z, the embedder’s output\n\n4\n\nTheorem 1. Let V ≥ 2. For any 1 ≥ δ > 0, with probability at least 1 − δ, the following holds for any fS ∈ {f S\n\nSEM(τ ), f S\n\nbase}:\n\nEz,y[l(fS(z), y)] ≤\n\n1 n\n\nn (cid:88)\n\ni=1\n\nl(fS(z(i)), y(i)) + R\n\nL φfS (V, τ ) + c\n\n(cid:113)\n\n(cid:114)\n\nln(2/δ) n\n\n,\n\nwhere c > 0 is a constant in (n, f, H, δ, τ, S). Moreover,\n\nφf S\n\nSEM(τ )\n\n→ 0 as τ → 0\n\nand φf S\n\nSEM(τ )\n\n− φf S\n\nbase\n\n≤\n\n3 4\n\n(1 − V ) < 0 ∀τ > 0.\n\nn\n\n(cid:80)n\n\ni=1 l(fS(z(i)), y(i)), the second term R(cid:112)LφfS , and the third term c\n\nThe first statement of Theorem 1 shows that the expected loss is bounded by the three terms: the (cid:113) ln(2/δ) training loss 1 .\nSince c is a constant in (n, f, H, δ, τ, S), the third term goes to zero as n → ∞ and is the same with and without SEM. Thus, for the purpose of assessing the impact of SEM, we can focus on the second term, where a difference arises. Theorem 1 shows that R(cid:112)LφfS goes to zero with SEM; i.e., φ(f S SEM(τ )) → 0 as τ → 0. Also, for any τ > 0, the second term with SEM is strictly smaller than 4 (1 − V ) < 0 and demonstrates that the improvement that without SEM as φf S due to SEM is expected to asymptotically increase as V increases. Moreover, L is a multiplicative constant of φ which shows that, as L increases, the improvement due to SEM is also expected to be higher. Overall, Theorem 1 shows the benefit of SEM as well as the trade-off with τ . When τ → 0, the second term goes to zero, but the training loss (the first term) can increase due to underfitting resulting from the reduction in representation expressivity. Thus, τ should be chosen to optimally balance this trade-off.\n\n− φf S\n\n≤ 3\n\nSEM(τ )\n\nbase\n\nn\n\n4 EMPIRICAL ANALYSIS\n\nWe empirically study the effect of SEM on the representation of SSL methods and demonstrate that SEM improves the test set accuracy on CIFAR-100 (Krizhevsky, 2009). We compare SEM with other methods for inducing sparse representations during pretraining and demonstrate that SEM lead to better downstream accuracy. On IMAGENET (Deng et al., 2009), we study the effect of SEM on robustness, semi-supervised learning and transfer learning datasets, demonstrating consistent improvement attributed to SEM. Finally, we present evidences that features produced by SEMs are more naturally aligned with the semantic categories of the data. The code for reproducing the results is available at: https://github.com/lavoiems/simplicial-embeddings/.\n\nTraining setup. For all experiments, we build off the implementation of the baseline models from the Solo-Learn library (da Costa et al., 2021). We probe the encoder’s output for the baseline methods, as typically done in the literature. For models with SEM, we probe the SEM normalized representation (i.e. ˆz). In our experiments, the embedder is a linear layer followed by BatchNorm (Ioffe & Szegedy, 2015). Unless mentioned otherwise, we use L = 5000 and V = 13 for the SEM representation. We do not perform any search for the non-SEM hyper-parameters. The SEM hyper-parameters are selected by using a validation set of 10% of the training set of CIFAR-100 and 10 samples per class on the in distribution dataset for IMAGENET. The test accuracy is obtained by retraining the model with all of the training data using the parameters found with the validation set. We pre-train the SSL models for 200 epochs on IMAGENET and 1000 epochs on CIFAR-100.\n\nTable 1: Linear probe top-1 accuracy on CIFAR-100 trained for 1000 epochs with a ResNet-18/50 encoder. We compare the test accuracy of several SSL models with and without SEM. Boldface indicates highest accuracy. Green rows indicate a SSL method + SEM.\n\nSIMCLR MOCO BYOL BARLOW-TWINS SWAV DINO VICREG\n\nResNet-18: 66.8 Baseline With SEM 69.5 ResNet-50: 70.5 Baseline With SEM 73.2\n\n69.3 71.0\n\n73.24 75.8\n\n70.7 73.9\n\n74.2 77.5\n\n70.7 73.0\n\n72.0 73.3\n\n64.6 67.7\n\n66.8 69.2\n\n− −\n\n− −\n\n68.5 71.4\n\n70.8 73.3\n\n5\n\n4.1 SEM IMPROVES ON DOWNSTREAM CLASSIFICATION\n\nBaseline comparison. We evaluate the effect of adding SEMs in seven modern SSL approaches. We take standard SimCLR (Chen et al., 2020b), MoCo-v2 (He et al., 2020), BYOL (Grill et al., 2020) Barlow-Twins (Zbontar et al., 2021), SwAV (Caron et al., 2020), DINO (Caron et al., 2021) and VicReg (Bardes et al., 2022) models and implement SEM after the encoder. We compare our approach on CIFAR-100 with a ResNet-18 and ResNet-50 in Table 1. We found SWaV and DINO to be unstable with ResNet-50 thus have decided not to compare them with SEM. For every SSL methods, using SEMs improves the baseline methods by 2% to 4% demonstrating that SEM is a general approach that improves in-distribution generalization for SSL methods.\n\nIncreasing the representation’s size of SEM increases the performance. We find that increasing L (the number of simplices of SEM) beyond the over-complete regime increases the downstream accuracy. This increased performance is not observed when we abstain from using the softmax normalization of SEM. In Figure 3, using a ResNet-50 encoder, we compare BYOL + SEM, with an identical model without the Softmax normalization which we call BYOL + Embed and BYOL to which we increase the representation’s size before the meanpooling using the method proposed in (Dubois et al., 2022) and described in their Appendix F. To be clear, the extracted representation of BYOL + Embed is the embedder’s output zθ and the extracted representation for BYOL is the encoder’s output eθ. We fix V = 13 and scale L ∈ [10, 10000] to get a range of representation sizes.\n\nFigure 3: Effect of the Softmax when scaling up L on the linear probe accuracy. Using a RN-50.\n\nTable 2: Comparing SEM with hard discretization using Gumbel Softmax (G.S.) and Vector Quantization (V.Q.). RN-18 base on CIFAR-100. ˆzθ Accuracy BYOL -\nBYOL+G.S. BYOL+V.Q. BYOL+SEM (τd = 0) BYOL+SEM (τd = 0.1)\n\nComparison of SEM with hard discretization approaches. Several other methods can be used to induce a sparse and overcomplete representation during pre-training and downstream classification. For example, we may sample L discrete one-hot codes of V dimensions using Gumbel Softmax (Jang et al., 2017) as done in Dessì et al. (2021). We can also use Vector Quantization (VQ) (Oord et al., 2018) and consider L latent embedding spaces with V embedding vectors each, wherein the vectors are in Rd. In contrast to SEM, it is not possible to propagate the gradient through the bottleneck trivially and VQ uses straight-through estimation in the embedding space to backpropagate the gradient to the encoder. Here, we observe that these alternative approaches exhibit a considerable decrease in performance in comparison to the baseline as demonstrated in Table 2. In this table, we reproduce the same setup as SEM but we replace the Softmax with hard discretization baselines methods. For discretization with Gumbel Straight-Through estimation, we use the same setup as SEM with L = 5000 and V = 13, that is 5000 one-hot vectors of 13 dimensions and τ = 2†. For VQ, we found that L = 512 and V = 128 led to the best performance. That is, we have 512 latent embedding spaces, each with 128 possible embedding vectors that are in R32.\n\neθ 70.7 63.3 54.5 65.6 60.3 - 73.2 - 73.9\n\nWe note that while we have not found hard-discretization to be successful during pre-training, we may hard-discretize a SEM representation for downstream task. In Table 2, we also present SEM with τDS = 0, which correspond to using the discretized representation for downstream classification. We obtain the discrete representation by taking the argmax for each simplex. This result demonstrating that SEM with pre-training can be used to learn meaningful discrete codes for downstream applications and yields better performance than the baselines, implying that pre-training with SEM could be be used in applications that require discretization.\n\nMemory and computational efficiency of SEM. SEM’s performance improvements come at a cost of increased memory allocation (VRAM) due to additional parameters needed to perform the matrix multiplication, and slightly more computation (FLOPs/sample). For very large over-complete representation the increased memory requirement can impede practical application. We propose a more efficient version of SEM by sparsifying the matrix multiplication of the embedder and of the projector and detail this procedure in Appendix D.1. As shown in Table 17, SEM with sparse matrix\n\n†A hyper-parameter search was performed to select the best performing hyper-parameter.\n\n6\n\n13065013K130KEmbedding size68707274767880Top-1 accuracyBYOL + SEMBYOLBYOL + Embedmultiplication use only slightly more memory and compute but outperforms the BYOL baseline on CIFAR-100 though underperforming the regular SEM. We also note that SEM’s memory cost becomes relatively minor as we scale up the encoder. As well, the computational cost of SEM is small compared to the total cost of pre-training and achieves higher accuracy using fewer FLOPs compared to scaling the encoder as shown in Figure 1.\n\n4.2 ANALYZING THE PARAMETERS OF SEM\n\nWe present two figures in this section to better understand the effect of the parameters of SEM on the downstream accuracy. In Figure 4, we evaluate the effect of changing τp and τd on the downstream accuracy. In Figure 5, we evaluate the effect of L and V on the downstream accuracy and also contrast fbase and fSEM(τ = 1) by using the same encoder pre-trained with SEM. This allows us to relate some observations to the theorey presented in Section 3.2. Now, we discuss the effect of each of SEM’s parameter on the resulting downstream classification.\n\nIncreasing V yields a steep performance increase for small V but quickly plateau. In Figure 5b, we observe a steep increase of the accuracy for V < 13 followed by a plateau for V > 13. In Figure 4a, we observe that the optimal accuracy obtained for V = 1024 and L = 64 is similar to the one obtained for L = 50 (Embedding size=650) in Figure 3.\n\nIncreasing L yields monotonical improvement for downstream classification. In the regime that we can test it, increasing L lead to consistent improvement on the downstream accuracy as observed in Figure 3 and Figure 5a. Using SEM in pre-training only is not enough and using it in the downstream classifier is necessary for the improved performance as demonstrated in Figure 5a.\n\nThe optimal τp depends on V . As previously noted in the context of Attention (Vaswani et al., 2017; Wang et al., 2021a), the optimal attention’s temperature is proportional to attention’s vector size. We also observe this in SEM. As presented in Figure 4a, the optimal τp for larger V is higher.\n\nModels with larger L are more robust to smaller τd. In Figure 4, we observe that SSL models are more robust to smaller τd as L increase. We speculate that the information can be scattered across the simplices for large L, allowing to reduce the expressivity of each vector with minimal impact on the downstream accuracy.\n\n(a)\n\n(b)\n\n(a)\n\n(b)\n\nFigure 4: Effect of τp and τd on a RN-50.\n\nFigure 5: Comparing fSEM and fbase on a RN-18.\n\n4.3 SEM IMPROVEMENT ON LARGE-SCALE DATASETS WITH IMAGENET\n\nFigure 1 in the introduction demonstrates that using SEM leads to better in distribution generalization for IMAGENET and is a more efficient method of scaling up the model as compared to scaling up the width of the ResNet-50 encoder. Here, we demonstrate that SEM generally improves the accuracy on several robustness test sets, a semi-supervised learning benchmark and transfer learning datasets. We use BYOL+SEM with an embedding size of 105 000 features (L = 5000 and V = 21) for these experiments. The embedding is pre-trained for 200 epochs using the BYOL SSL procedure.\n\nRobustness to out-of-distribution test sets. We perform a comparative study using several test sets: (IN) the in-distribution test set provided in IMAGENET; (IN-C) IMAGENET-C, which exhibits a set of common image corruptions (Hendrycks & Dietterich, 2019); (IN-R) IMAGENET-R (Hendrycks et al., 2021) which consists of different renderings for several IMAGENET classes; and (IN-V2) IMAGENETV2 (Recht et al., 2019), a distinct test set for IMAGENET collected using the same process; (IN-A) IMAGENET-A (Chen et al., 2020a) contains a set of samples that are miclassifier by a IMAGENET ResNet-50 classifier. We use the methodology and software proposed in Djolonga et al. (2020; 2021)\n\n7\n\n0.010.10.512510Pretrain tau6568717477AccuracyL=5000, V=13L=64,V=10240.010.10.51510Downstream tau6568717477AccuracyL=100L=1000L=50005010050010005000L62646668707274AccuracyfbasefSEM(=1)2581334V62646668707274AccuracyfbasefSEM(=1)Table 3: Robustness via linear probe top1 test accuracies on IMAGENET variant datasets, using representations pre-trained for 200 epochs. * Taken from (Chen & He, 2020)\n\nIN IN-V2 IN-R IN-C IN-A 70.6 -\nBYOL* 71.9 59.2 18.8 39.5 1.65 BYOL BYOL+SEM 74.1 61.2 22.1 43.4 2.53\n\n-\n\n-\n\n-\n\nTable 4: Top-1 transfer learning accuracy from IMAGENET pre-trained representation.\n\nFOOD101 C10 C100 SUN DTD FLOWER\n\nLinear probe: 74.2 BYOL BYOL+SEM 74.7 Fine-tuned: 83.1 BYOL BYOL+SEM 84.7\n\n91.8 74.9 60.9 72.2 93.5 78.6 62.1 71.9\n\n88.9 91.5\n\n97.2 83.6 59.1 69.2 85.4 97.2 85.6 63.3 71.3 91.7\n\nto perform our experiments. We observe that BYOL + SEM outperforms BYOL on every robustness datasets probed, demonstrating that SEM also improves generalization to out-of-distribution test sets.\n\nTransfer learning. We probe the effectiveness of SEM in BYOL and MoCo when transferring representations trained on IMAGENET to other classification tasks. We follow the linear evaluation and fine-tuning methodologies described in previous works (Grill et al., 2020; Lee et al., 2021), which entails training a linear classifier with logistic regression using sklearn (Pedregosa et al., 2011) on the embeddings of the samples and fine-tuning the encoder respectively. To avoid out-of-memory issues that may occur in the linear probe experiment with the sklearn solver when the number of features, we discretize our features and use sparse matrix to fit the logistic regression. This is equivalent to forcing τd = 0 for all the experiments. For the fine-tuning experiments, we fix τd = 1 since the evaluation method allows for mini-batch gradient descent. We perform our transfer learning experiments on the following datasets: FOOD (Bossard et al., 2014), CIFAR-10 (C-10) (Krizhevsky, 2009), CIFAR-100 (C-100) (Krizhevsky, 2009), SUN (Xiao et al., 2010), DTD (Cimpoi et al., 2014) and FLOWER (Nilsback & Zisserman, 2008).\n\nThis task evaluates the generality of the encoder as it has to encode samples from various out-ofdistribution domains with categories that it may not have seen during training. We present our results in Table 4 and observe that SEM improves the transfer accuracy over the baseline for every datasets but DTD for the linear probe experiment. For DTD, we hypothesize that the drop in performance is due to the fact that we use a temperature that is too small. Since this is a texture dataset with higher frequency, it might be the case that we need more expressivity to correctly fit the data. We support the conjecture with the fine-tuning experiment where BYOL + SEM out-performs the baseline.\n\nSemi-supervised learning. We evaluate the effect of using SEM when fine-tuning on a classification task with a small subset of IMAGENET’s training set. We follow the semi-supervised learning procedure of Chen et al. (2020b); Grill et al. (2020) and use the same fixed splits of 1% and 10% of ImageNet labelled training set. In Table 5, we demonstrate that using SEM lead to an important increased performance, especially in the low supervised data regime.\n\nTable 5: Semi-supervised learning accuracy by fine-tuning on IMAGENET.\n\nTop-1\n\nTop-5\n\n1% 10% 1% 10% 51.6 67.5 78.0 88.9 BYOL BYOL+SEM 56.7 69.9 81.0 90.0\n\n4.4 SEMANTIC COHERENCE OF SEM FEATURES\n\nHere we demonstrate that SEM features are coherently aligned with the semantics present in the training data. Qualitatively, we visualize the most predictive features of a downstream linear classifier trained on CIFAR-100 and see that the classes with similar predictive features are semantically related. Quantitatively we propose a metric that returns the ratio of features mostly predictive for a classes that are in the same super class to total number of class predictive for this feature.\n\nFor both our analysis, we use a linear classifier trained on the features extracted from BYOL with and without SEM. Consider the trained linear classifier with a weight matrix W ∈ RN ×C, with N features, and C classes. By preserving the top K parameters of the weight matrix W for each class and pruning the features predictive for only one class, we create a bipartite graph between two set of nodes: the CIFAR-100 classes and the features of the representation. We denote this graph WK.\n\nThe qualitative analysis is given by plotting the subset W5, obtained by taking the top 5 features for each class. We present a subset of the graph for BYOL+SEM in Figure 6a and for BYOL in Figure 6b. The full graphs are presented in the Appendix. In the SEM plot, a set of connected components emerge, and the connected components of the graph are semantically related. For example, the\n\n8\n\n(a) BYOL + SEM\n\n(b) BYOL\n\n(c)\n\nFigure 6: Semantic coherence of the features. (a) and (b) Subset of W5, the bipartite graph of the most 5 highest magnitude features on BYOL + SEM features (a) and BYOL on the encoded features (b). (c) Coherence of the top K features to the semantics of the super-class of the categories of CIFAR-100. It is taken as the number of pairwise categories in the same super-class for which a feature is among its top K most predictive features over the total number of pairwise categories.\n\nfirst set of connected components are flowers, and the last set of connected components are aquatic mammals.‡. The same class coherence is not observed with either the BYOL baseline or with BYOL augmented with a large representation. In particular, we do not see a small number of semantically related connected components. Instead, we see a large fully connected graphs.\n\nNext, we describe how we quantitatively measure the semantic coherence of the features. Notice that two classes share a common predictive feature on WK if they are 2-neighbour. Let N (ci) returns all pairs (ci, cj) for all j 2-neighbour of ci. Moreover, define the operation is_super(ci, cj) which returns 1 if ci and cj are from the same CIFAR-100 superclass and 0 otherwise. We reproduce the superclass of CIFAR-100 in Table 22 in the Appendix. We measure semantic coherence as follows: (cid:80)\n\n(ci,cj )∈N (ci) is_super(ci, cj)\n\n,\n\n(2)\n\nCoherence(WK) :=\n\nC (cid:88)\n\n1 C\n\ni=1\n\n|N (ci)|\n\nwhere C = 100 for CIFAR-100 and | · | is the cardinality of a set.\n\nWe compare the semantic coherence of BYOL+SEM with the control experiments on BYOL: regular BYOL, BYOL with an embedding of the same size as BYOL+SEM but without the normalization and BYOL to which we applied linear ICA (Hyvärinen & Oja, 2000) in an attempt to disentangle the features. In Figure 10, we plot the full graph W5 for BYOL+SEM and the baselines. We observe that using the SEM yields semantically coherent features for all the classes of CIFAR-100. This observation is consistent with the qualitative and quantitative experiments presented earlier and demonstrates that SEM’s inductive bias during pre-training leads to features that are semantically coherent with the semantic categories extant in the data. This arguably have important implications for improving the interpretability of SSL representations.\n\n5 CONCLUSION\n\nSEM is a simple, drop-in module that induces discrete sparse overcomplete representations for standard SSL methods using a softmax operation. This simple modification leads to improved generalization on downstream classification across several state-of-the-art SSL methods. Furthermore, SEM improves performance on out-of-distribution, semi-supervised, and transfer learning tasks across the board and also scales with encoder size. By analyzing semantic coherence, we find that SEMs naturally disentangle data into semantic categories without any explicit training objectives.\n\n‡Although \"flatfish\" may seem out of place in the third set, manually checking CIFAR images showed that\n\nmany images labelled \"flatfish\" are often humans holding flatfish.\n\n9\n\ntuliproseorchidpoppyplatecupbowlbottlecanmanbabywomanboyflatfishgirlwhaleturtledolphinraysharktablefoxchimpanzeemountaincattlewardrobeplatebabywormcaterpillartelevisiontigerleopardbridgecastleplainraccooncloudmanbedcouchtelephonecamelseaskunkmushroomwolforangechairskyscraperbutterfly01020304050Top K features0.10.20.30.40.50.60.70.8Semantic coherenceBYOLBYOL + embed largeBYOL + SEMBYOL + ICAACKNOWLEDGEMENTS\n\nThe authors are grateful for the insightful discussions with Xavier Bouthillier, Hattie Zhou, Sébastien Lachapelle, Tristan Deleu, Yuchen Lu, Eeshan Dhekane, Maude Lizaire, Julien Roy and David Dobre. We acknowledge funding support from Samsung and Hitachi, as well as support from Aaron Courville’s CIFAR CCAI chair. We also wish to acknowledge Mila and Compute Canada for providing the computing infrastructure that enabled this project. Finally, this project would not have been possible without the contribution of the following open source projects: Pytorch (Paszke et al., 2019), Orion (Bouthillier et al., 2022), Solo-Learn (da Costa et al., 2021), Scikit-Learn (Pedregosa et al., 2011), and Numpy (Harris et al., 2020).\n\nREFERENCES\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. URL\n\nhttps://arxiv.org/abs/1607.06450.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. arXiv:1409.0473 [cs, stat], May 2016. URL http://arxiv. org/abs/1409.0473. arXiv: 1409.0473.\n\nAdrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for self-supervised learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=xm6YD62D1Ub.\n\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision – ECCV 2014, pp. 446–461, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10599-4.\n\nXavier Bouthillier, Christos Tsirigotis, François Corneau-Tremblay, Thomas Schweizer, Lin Dong, Pierre Delaunay, Fabrice Normandin, Mirko Bronzi, Dendi Suhubdy, Reyhane Askari, Michael Noukhovitch, Chao Xue, Satya Ortiz-Gagné, Olivier Breuleux, Arnaud Bergeron, Olexa Bilaniuk, Steven Bocco, Hadrien Bertrand, Guillaume Alain, Dmitriy Serdyuk, Peter Henderson, Pascal Lamblin, and Christopher Beckham. Epistimio/orion: Asynchronous Distributed Hyperparameter Optimization, March 2022. URL https://doi.org/10.5281/zenodo.3478592.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9912–9924. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 70feb62b69f16e0238f741fab228fec2-Paper.pdf.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging Properties in Self-Supervised Vision Transformers. arXiv:2104.14294 [cs], May 2021. URL http://arxiv.org/abs/2104.14294. arXiv: 2104.14294.\n\nTianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial\n\nrobustness: From self-supervised pre-training to fine-tuning. In CVPR 2020, June 2020a.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 1597–1607. PMLR, 13–18 Jul 2020b.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint\n\narXiv:2011.10566, 2020.\n\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3606–3613, 2014.\n\n10\n\nAdam Coates and Andrew Y. Ng. The importance of encoding versus training with sparse coding and vector quantization. In ICML, pp. 921–928, 2011. URL https://icml.cc/2011/papers/ 485_icmlpaper.pdf.\n\nGonçalo M. Correia, Vlad Niculae, and André F. T. Martins. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2174–2184, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1223. URL https://aclanthology.org/D19-1223.\n\nVictor G. Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. Solo-learn: A library of self-supervised methods for visual representation learning, 2021. URL https://github. com/vturrisi/solo-learn.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nRoberto Dessì, Eugene Kharitonov, and Marco Baroni. Interpretable agent communication from scratch(with a generic visual processor emerging on the side). CoRR, abs/2106.04258, 2021. URL https://arxiv.org/abs/2106.04258.\n\nRoberto Dessi, Eugene Kharitonov, and Marco Baroni. Interpretable agent communication from scratch (with a generic visual processor emerging on the side). In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=1AvtkM4H-y7.\n\nJosip Djolonga, Frances Hubis, Matthias Minderer, Zachary Nado, Jeremy Nixon, Rob Romijnders, Dustin Tran, and Mario Lucic. Robustness Metrics, 2020. URL https://github.com/ google-research/robustness_metrics.\n\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvain Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On Robustness and Transferability of Convolutional Neural Networks. arXiv:2007.08558 [cs], March 2021. URL http: //arxiv.org/abs/2007.08558. arXiv: 2007.08558.\n\nD.L. Donoho, M. Elad, and V.N. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Information Theory, 52(1):6–18, 2006. doi: 10.1109/TIT.2005.860430.\n\nYann Dubois, Stefano Ermon, Tatsunori Hashimoto, and Percy Liang. Improving self-supervised learning by characterizing idealized representations. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=agQGDz6gPOo.\n\nManaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah A. Smith. Sparse overcomplete word vector representations. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1491–1500, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1144. URL https: //aclanthology.org/P15-1144.\n\nAlona Fyshe, Leila Wehbe, Partha P. Talukdar, Brian Murphy, and Tom M. Mitchell. A compositional and interpretable semantic space. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 32–41, Denver, Colorado, May–June 2015. Association for Computational Linguistics. doi: 10.3115/v1/N15-1004. URL https://aclanthology.org/N15-1004.\n\nIan J. Goodfellow, Aaron Courville, and Yoshua Bengio. Large-scale feature learning with spike-andslab sparse coding. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML’12, pp. 1387–1394, Madison, WI, USA, 2012. Omnipress. ISBN 9781450312851.\n\n11\n\nAnirudh Goyal, Aniket Rajiv Didolkar, Alex Lamb, Kartikeya Badola, Nan Rosemary Ke, Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Curtis Mozer, and Yoshua Bengio. Coordination among neural modules through a shared global workspace. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=XzTtHjgPDsT.\n\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv:1410.5401 [cs],\n\nDecember 2014. URL http://arxiv.org/abs/1410.5401. arXiv: 1410.5401.\n\nKarol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML’10, pp. 399–406, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077.\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21271–21284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.\n\nCharles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised Visual Representation Learning. arXiv:1911.05722 [cs], March 2020. URL http: //arxiv.org/abs/1911.05722. arXiv: 1911.05722.\n\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HJz6tiCqYm.\n\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. arXiv:2006.16241 [cs, stat], July 2021. URL http://arxiv.org/abs/2006.16241. arXiv: 2006.16241.\n\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=Bklr3j0cKX.\n\nAapo Hyvärinen and Erkki Oja. Independent component analysis: algorithms and applications.\n\nNeural Networks, 13:411–430, 2000.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 448–456, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings. mlr.press/v37/ioffe15.html.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=rkE3y85ee.\n\nLi Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse in contrastive self-supervised learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=YevsQ05DEN7.\n\n12\n\nM.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the em algorithm. In Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan), volume 2, pp. 1339–1344 vol.2, 1993. doi: 10.1109/IJCNN.1993.716791.\n\nAlexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation learning. CoRR, abs/1901.09005, 2019. URL http://arxiv.org/abs/1901.09005.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images, 2009. URL https:\n\n//www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.\n\nHonglak Lee, Chaitanya Ekanadham, and Andrew Ng. Sparse deep belief net model for visual area v2. In J. Platt, D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips. cc/paper/2007/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf.\n\nKuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive Visual Representations. arXiv:2109.12909 [cs, math], September 2021. URL http://arxiv. org/abs/2109.12909. arXiv: 2109.12909.\n\nMichael S. Lewicki and Terrence J. Sejnowski. Learning Overcomplete Representations. Neural Computation, 12(2):337–365, 02 2000. ISSN 0899-7667. doi: 10.1162/089976600300015826. URL https://doi.org/10.1162/089976600300015826.\n\nDianbo Liu, Alex M Lamb, Kenji Kawaguchi, Anirudh Goyal ALIAS PARTH GOYAL, Chen In Sun, Michael C Mozer, and Yoshua Bengio. Discrete-valued neural communication. M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 2109–2121. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 10907813b97e249163587e6246612e21-Paper.pdf.\n\nBrian Murphy, Partha Pratim Talukdar, and Tom Michael Mitchell. Learning effective and inter-\n\npretable semantic models using non-negative sparse embedding. In COLING, 2012.\n\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing, pp. 722–729, 2008. doi: 10.1109/ICVGIP.2008.47.\n\nB.A. Olshausen and D.J. Field. Emergence of simple-cell receptive field properties by learning a\n\nsparse code for natural images. Nature, 381:607–609, June 1996.\n\nBruno A. Olshausen. Highly overcomplete sparse coding. In Bernice E. Rogowitz, Thrasyvoulos N. Pappas, and Huib de Ridder (eds.), Human Vision and Electronic Imaging XVIII, volume 8651 of Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, pp. 86510S, March 2013. doi: 10.1117/12.2013504.\n\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural Discrete Representation Learning. arXiv:1711.00937 [cs], May 2018. URL http://arxiv.org/abs/1711.00937. arXiv: 1711.00937.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf.\n\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.\n\n13\n\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet Classifiers In Proceedings of the 36th International Conference on Machine Generalize to ImageNet? Learning, pp. 5389–5400. PMLR, May 2019. URL https://proceedings.mlr.press/ v97/recht19a.html. ISSN: 2640-3498.\n\nAaqib Saeed, David Grangier, and Neil Zeghidour. Contrastive Learning of General-Purpose Audio Representations. arXiv:2010.10915 [cs, eess], October 2020. URL http://arxiv.org/ abs/2010.10915. arXiv: 2010.10915.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Journal of Machine Dropout: A simple way to prevent neural networks from overfitting. Learning Research, 15(56):1929–1958, 2014. URL http://jmlr.org/papers/v15/ srivastava14a.html.\n\nYee Whye Teh, Max Welling, Simon Osindero, and Geoffrey E. Hinton. Energy-based models for sparse overcomplete representations. J. Mach. Learn. Res., 4(null):1235–1260, dec 2003. ISSN 1532-4435.\n\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\n\ncoding. CoRR, abs/1807.03748, 2018. URL http://arxiv.org/abs/1807.03748.\n\nAad W. van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes. Springer New York, 1996. doi: 10.1007/978-1-4757-2545-2. URL https://doi.org/10.1007% 2F978-1-4757-2545-2.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762 [cs], December 2017. URL http://arxiv.org/abs/1706.03762. arXiv: 1706.03762 version: 5.\n\nJindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip S. Yu. Generalizing to Unseen Domains: A Survey on Domain Generalization. arXiv:2103.03097 [cs], December 2021a. URL http://arxiv.org/abs/2103.03097. arXiv: 2103.03097.\n\nShulun Wang, Bin Liu, and Feng Liu. Escaping the gradient vanishing: Periodic alternatives of softmax in attention mechanism, 2021b. URL https://arxiv.org/abs/2108.07153.\n\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 3485–3492. IEEE, 2010.\n\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. CoRR, abs/2010.13902, 2020. URL https://arxiv. org/abs/2010.13902.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\n\nlearning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.\n\n14\n\nA PROOF OF THEOREM 1\n\nLet us introduce additional notations used in the proofs. Define r = (z, y) ∈ R, (cid:96)(f, r) = l(f (z), y),\n\n ̃Cy,k1,...,kL = {(z, ˆy) ∈ Z × Y : ˆy = y, kj = arg max\n\nt∈[V ]\n\nzj,t ∀j ∈ [L]},\n\nand\n\n ̃Zk1,...,kL = {z ∈ Z : kj = arg max\n\nt∈[V ]\n\nzj,t ∀j ∈ [L]}.\n\nto be\n\nthen define Ck\n\nthe flatten version of\n\n= We { ̃Cy,k1,...,kL,y}y∈Y,k1,...,kL∈[V ] with C1 = ̃C1,1,...,1, C2 = ̃C2,1,...,1, C|Y| = ̃C|Y|,1,...,1, C|Y|+1 = ̃C1,2,1,...,1, C2|Y| = ̃C|Y|,2,1,...,1, and so on. Similarly, define Zk to be the flatten version of ̃Zk1,...,kL. We also use Qi = {q ∈ [−1, +1]V : i = arg maxj∈[V ] qj}, Ik := I S k := {i ∈ [n] : ri ∈ Ck}, and αk(h) := Er[(cid:96)(h, r)|r ∈ Ck]. Moreover, we define φ(f S base) = supi∈[V ] supq,q(cid:48)∈Qi (cid:107)q − q(cid:48)(cid:107)2 2, SEM(τ )) = supi∈[V ] supq,q(cid:48)∈Qi (cid:107)στ (q) − στ (q(cid:48))(cid:107)2 and φ(f S for j = 1, . . . , V .\n\n2 where στ (q)j =\n\neqj /τ t=1 eqt/τ\n\ni.e.,\n\nk=1\n\n(cid:80)V\n\n{Ck}K\n\n ̃Cy,k1,...,kL;\n\nWe first decompose the generalization gap into two terms using the following lemma: Lemma 1. For any δ > 0, with probability at least 1 − δ,the following holds for all h ∈ H:\n\nEr[(cid:96)(h, r)] −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:96)(h, ri) ≤\n\n1 n\n\nK (cid:88)\n\nk=1\n\n\n\n|Ik|\n\nαk(h) −\n\n\n\n(cid:114)\n\n(cid:96)(h, ri)\n\n + c\n\nln(2/δ) n\n\n.\n\n1 |Ik|\n\n(cid:88)\n\ni∈Ik\n\nProof. We first write the expected error as the sum of the conditional expected error:\n\nEr[(cid:96)(h, r)] =\n\nK (cid:88)\n\nk=1\n\nEr[(cid:96)(h, r)|r ∈ Ck] Pr(r ∈ Ck) =\n\nK (cid:88)\n\nk=1\n\nErk [(cid:96)(h, rk)] Pr(r ∈ Ck),\n\nwhere rk is the random variable for the conditional with r ∈ Ck. Using this, we decompose the generalization error into two terms:\n\nEr[(cid:96)(h, r)] −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:96)(h, ri)\n\n(3)\n\n(cid:18)\n\nErk [(cid:96)(h, rk)]\n\nPr(r ∈ Ck) −\n\n=\n\nK (cid:88)\n\nk=1\n\n(cid:19)\n\n|Ik| n\n\n\n\n+\n\n\n\nK (cid:88)\n\nk=1\n\nErk [(cid:96)(h, rk)]\n\n|Ik| n\n\n−\n\n1 n\n\nn (cid:88)\n\ni=1\n\n\n\n(cid:96)(h, ri)\n\n .\n\nThe second term in the right-hand side of (3) is further simplified by using\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:96)(h, ri) =\n\n1 n\n\nK (cid:88)\n\n(cid:88)\n\nk=1\n\ni∈Ik\n\n(cid:96)(h, ri),\n\nas\n\nK (cid:88)\n\nk=1\n\nErk [(cid:96)(h, rk)]\n\n|Ik| n\n\n−\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:96)(h, ri) =\n\n1 n\n\nK (cid:88)\n\nk=1\n\n Erk [(cid:96)(h, rk)] −\n\n|Ik|\n\n\n\n(cid:96)(h, ri)\n\n\n\n1 |Ik|\n\n(cid:88)\n\ni∈Ik\n\nSubstituting these into equation (3) yields n\n(cid:88)\n\nEr[(cid:96)(h, r)] −\n\n(cid:96)(h, ri)\n\n1 n\n\ni=1\n\nErk [(cid:96)(h, rk)]\n\n(cid:18)\n\nPr(r ∈ Ck) −\n\n=\n\nK (cid:88)\n\nk=1\n\n(cid:19)\n\n+\n\n|Ik| n\n\n1 n\n\nK (cid:88)\n\nk=1\n\n Erk [(cid:96)(h, rk)] −\n\n|Ik|\n\n1 |Ik|\n\n≤ B\n\nK (cid:88)\n\nk=1\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nPr(r ∈ Ck) −\n\n|Ik| n\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n+\n\n1 n\n\nK (cid:88)\n\nk=1\n\n Erk [(cid:96)(h, rk)] −\n\n|Ik|\n\n1 |Ik|\n\n(cid:88)\n\ni∈Ik\n\n15\n\ni∈Ik \n\n(cid:96)(h, ri)\n\n\n\n(4)\n\n\n\n(cid:88)\n\n(cid:96)(h, ri)\n\n\n\nBy using the Bretagnolle-Huber-Carol inequality (van der Vaart & Wellner, 1996, A6.6 Proposition), we have that for any δ > 0, with probability at least 1 − δ, (cid:114)\n\nK (cid:88)\n\nk=1\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nPr(r ∈ Ck) −\n\n|Ik| n\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n2K ln(2/δ) n\n\n.\n\n(5)\n\n(cid:12) Here, notice that the term of (cid:80)K (cid:12) (cid:12) does not depend on h ∈ H. Moreover, note that for any (f, h, M ) such that M > 0 and B ≥ 0 for all X, we have that P(f (X) ≥ M ) ≥ P(f (X) > M ) ≥ P(Bf (X) + h(X) > BM + h(X)), where the probability is with respect to the randomness of X. Thus, by combining (4) and (5), we have that for any h ∈ H, for any δ > 0, with probability at least 1 − δ, the following holds for all h ∈ H,\n\n(cid:12)Pr(r ∈ Ck) − |Ik|\n\nk=1\n\n(cid:12) (cid:12)\n\nn\n\nEr[(cid:96)(h, r)] −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:96)(h, ri) ≤\n\n1 n\n\nK (cid:88)\n\nk=1\n\n\n\n|Ik|\n\nαk(h) −\n\n\n\n(cid:114)\n\n(cid:96)(h, ri)\n\n + c\n\nln(2/δ) n\n\n.\n\n1 |Ik|\n\n(cid:88)\n\ni∈Ik\n\nIn particular, the first term from the previous lemma will be bounded with the following lemma: Lemma 2. For any f ∈ {f S\n\nSEM(τ ), f S \n\nbase},\n\n|Ik|\n\nαk(f ) −\n\n1 n\n\nK (cid:88)\n\nk=1\n\n\n\n(cid:96)(f, ri)\n\n ≤ R(cid:112)Lφ(f ).\n\n1 |Ik|\n\n(cid:88)\n\ni∈Ik\n\nProof. By using the triangle inequality,\n\n1 n\n\nK (cid:88)\n\nk=1\n\n Er[(cid:96)(f, r)|r ∈ Ck] −\n\n|Ik|\n\n\n\n(cid:96)(f, ri)\n\n\n\n1 |Ik|\n\n(cid:88)\n\ni∈Ik\n\n≤\n\n1 n\n\nK (cid:88)\n\nk=1\n\n|Ik|\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nEr[(cid:96)(f, r)|r ∈ Ck] −\n\n1 |Ik|\n\n(cid:88)\n\ni∈Ik\n\n(cid:96)(f, ri)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\nFurthermore, by using the triangle inequality,\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nEr[(cid:96)(f, r)|r ∈ Ck] −\n\n1 |Ik|\n\n(cid:88)\n\ni∈Ik\n\n(cid:12) (cid:12) (cid:12) (cid:96)(f, ri) (cid:12) (cid:12) (cid:12)\n\n=\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 |Ik|\n\n(cid:88)\n\ni∈Ik\n\nEr[(cid:96)(f, r)|r ∈ Ck] −\n\n1 |Ik|\n\n(cid:88)\n\ni∈Ik\n\n(cid:12) (cid:12) (cid:12) (cid:96)(f, ri) (cid:12) (cid:12) (cid:12)\n\n1 |Ik|\n\n≤ sup\n\nr,r(cid:48)∈Ck\n\n(cid:88)\n\n(cid:12)Er[(cid:96)(f, r)|r ∈ Ck] − (cid:96)(f, ri)(cid:12) (cid:12) (cid:12)\n\ni∈Ik (cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12) (cid:12) (cid:12) .\n\nIf f = f S and non-negativity,\n\nSEM(τ ) = gS\n\nSEM(τ ) ◦στ , since gS\n\nSEM(τ ) ∈ GS, by using the Lipschitz continuity, boundedness,\n\nsup r,r(cid:48)∈Ck\n\n(cid:12) (cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\n\n(cid:12) = sup y∈Y\n\nsup z,z(cid:48)∈Zk\n\n|(ly ◦ gS\n\nSEM(τ ))(στ (z)) − (ly ◦ gS\n\nSEM(τ ))(στ (z(cid:48)))|\n\n≤ R sup\n\nz,z(cid:48)∈Zk\n\n(cid:107)στ (z) − στ (z(cid:48))(cid:107)F\n\n= R sup\n\nz,z(cid:48)∈Zk\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nL (cid:88)\n\nV (cid:88)\n\nt=1\n\nj=1\n\n(στ (zt,j) − στ (z(cid:48)\n\nt,j))2\n\n2\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nL (cid:88)\n\nt=1\n\nsup i∈[V ]\n\nsup q,q(cid:48)∈Qi\n\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n\n2\n\n(cid:113)\n\nLφ(f S\n\nSEM(τ ))\n\n≤ R\n\n= R\n\n16\n\nSimilarly, if f = f S and non-negativity,\n\nbase = gS\n\nbase, since gS\n\nbase ∈ GS, by using the Lipschitz continuity, boundedness,\n\nsup r,r(cid:48)∈Ck\n\n(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12) (cid:12)\n\n(cid:12) = sup y∈Y\n\nsup z,z(cid:48)∈Zk\n\n|(ly ◦ gS\n\nbase)(z) − (ly ◦ gS\n\nbase)(z(cid:48))|\n\n≤ R sup\n\n(cid:107)z − z(cid:48)(cid:107)F\n\nz,z(cid:48)∈Zk (cid:113)\n\nLφ(f S\n\nbase).\n\nTherefore, for any f ∈ {f S\n\nSEM(τ ), f S\n\nbase},\n\n≤ R\n\n1 n\n\nK (cid:88)\n\nk=1\n\n\n\n|Ik|\n\nαk(f ) −\n\n1 |Ik|\n\n(cid:88)\n\ni∈Ik\n\n\n\n(cid:96)(f, ri)\n\n ≤\n\n1 n\n\nK (cid:88)\n\nk=1\n\n|Ik|R(cid:112)Lφ(f ) = R(cid:112)Lφ(f ).\n\nCombining Lemma 1 and Lemma 2, we obtain the following upper bound on the gap:\n\nLemma 3. For any δ > 0, with probability at least 1 − δ, the following holds for any f ∈ {f S\n\nSEM(τ ), f S\n\nbase}:\n\nEr[(cid:96)(f, r)] −\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:96)(f, ri) ≤ R(cid:112)Lφ(f ) + c\n\n(cid:114)\n\nln(2/δ) n\n\n.\n\nProof. This follows directly from combining Lemma 1 and Lemma 2.\n\nWe now provide an upper bound on φ(f S\n\nSEM(τ )) in the following lemma:\n\nLemma 4. For any τ > 0,\n\nφ(f S\n\nSEM(τ )) ≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + (V − 1)e−2/τ\n\n−\n\n1 1 + (V − 1)e−∆/τ\n\n(cid:12) 2\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n+ (V − 1)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + e∆/τ (1 + (V − 2)e−2/τ )\n\n−\n\n1 1 + e2/τ (1 + (V − 2)e−∆/τ )\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\nProof. Recall the definition:\n\nφ(f S\n\nSEM(τ )) = sup\n\ni∈[V ]\n\nsup q,q(cid:48)∈Qi\n\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2 2.\n\nwhere\n\nστ (q)j =\n\neqj /τ t=1 eqt/τ\n\n(cid:80)V\n\n,\n\nfor j = 1, . . . , V . By the symmetry and independence over i ∈ [V ] inside of the first supremum, we have\n\nφ(f S\n\nSEM(τ )) = sup\n\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2 2.\n\nq,q(cid:48)∈Q1\n\nFor any q, q(cid:48) ∈ Q1 and i ∈ {2, . . . , V } (with q = (q1, . . . , qV ) and q(cid:48) = (q(cid:48) δi, δ(cid:48)\n\ni > 0 such that\n\n1, . . . , q(cid:48)\n\nV )), there exists\n\nand\n\nqi = q1 − δi\n\ni = q(cid:48) q(cid:48)\n\n1 − δ(cid:48) i.\n\nHere, since zik − ∆ ≥ zij from the assumption, we have that for all i ∈ {2, . . . , V },\n\nδi, δ(cid:48)\n\ni ≥ ∆ > 0.\n\n17\n\nThus, we can rewrite\n\nV (cid:88)\n\nt=1\n\neqt/τ = eq1/τ +\n\nV (cid:88)\n\ni=2\n\ne(q1−δi)/τ\n\n= eq1/τ + eq1/τ\n\nV (cid:88)\n\ni=2\n\ne−δi/τ\n\n\n\n= eq1/τ\n\n1 +\n\n\n\n\n\ne−δi/τ\n\nV (cid:88)\n\ni=2\n\nSimilarly,\n\nUsing these,\n\nV (cid:88)\n\nt=1\n\neq(cid:48)\n\nt/τ = eq(cid:48)\n\n1/τ\n\n\n\n1 +\n\nV (cid:88)\n\ni=2\n\n\n\ne−δ(cid:48)\n\ni/τ\n\n .\n\nστ (q)1 =\n\neq1/τ t=1 eqt/τ\n\n(cid:80)V\n\n=\n\n(cid:16)\n\neq1/τ\n\neq1/τ 1 + (cid:80)V\n\ni=2 e−δi/τ\n\n(cid:17) =\n\n1 1 + (cid:80)V i=2 e−δi/τ\n\nand for all j ∈ {2, . . . , V },\n\nστ (q)j =\n\neqj /τ t=1 eqt/τ\n\n(cid:80)V\n\n=\n\n=\n\n=\n\n(cid:16)\n\neq1/τ\n\ne(q1−δj )/τ 1 + (cid:80)V\n\ni=2 e−δi/τ\n\n(cid:17)\n\ne−δj /τ\n\n1 + (cid:80)V\n\ni=2 e−δi/τ\n\n1 1 + eδj /τ + (cid:80)V\n\ni∈Ij\n\ne(δj −δi)/τ\n\nwhere Ij := {2, . . . , V } \\ {j}. Similarly,\n\nστ (q(cid:48))1 =\n\n1 1 + (cid:80)V i=2 e−δ(cid:48)\n\ni/τ\n\n,\n\nand for all j ∈ {2, . . . , V },\n\nστ (q(cid:48))j =\n\nUsing these, for any q, q(cid:48) ∈ Q1,\n\n|στ (q)1 − στ (q(cid:48))1| =\n\n≤\n\n=\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 j /τ + (cid:80)V\n\n1 + eδ(cid:48)\n\ni∈Ij\n\n.\n\ne(δ(cid:48)\n\nj −δ(cid:48)\n\ni)/τ\n\n1 1 + (cid:80)V i=2 e−δi/τ\n\n−\n\n1 1 + (cid:80)V i=2 e−2/τ\n\n−\n\n1 1 + (cid:80)V i=2 e−δ(cid:48)\n\n1 1 + (cid:80)V i=2 e−∆/τ\n\n(cid:12) (cid:12) (cid:12) (cid:12) i/τ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + (V − 1)e−2/τ\n\n−\n\n1 1 + (V − 1)e−∆/τ\n\n18\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n,\n\nand for all j ∈ {2, . . . , V },\n\n|στ (q)j − στ (q(cid:48))j| =\n\n≤\n\n=\n\n=\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + eδj /τ + (cid:80)V\n\ni∈Ij\n\n−\n\ne(δj −δi)/τ\n\n1 j /τ + (cid:80)V\n\n1 + eδ(cid:48)\n\ne(δ(cid:48)\n\nj −δ(cid:48)\n\ni)/τ\n\ni∈Ij\n\n1 1 + e∆/τ + (cid:80)V\n\ni∈Ij\n\ne(∆−2)/τ\n\n−\n\n1 1 + e2/τ + (cid:80)V\n\ni∈Ij\n\ne(2−∆)/τ\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + e∆/τ + (V − 2)e(∆−2)/τ\n\n1 1 + e∆/τ (1 + (V − 2)e−2/τ )\n\n−\n\n−\n\n1 1 + e2/τ + (V − 2)e(2−∆)/τ\n\n1 1 + e2/τ (1 + (V − 2)e−∆/τ )\n\n.\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nBy combining these,\n\nsup q,q(cid:48)∈Q1\n\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n\n2\n\n= sup\n\nq,q(cid:48)∈Q1\n\nV (cid:88)\n\nj=1\n\n|στ (q)j − στ (q(cid:48))j|2\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + (V − 1)e−2/τ\n\n−\n\n1 1 + (V − 1)e−∆/τ\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n+ (V − 1)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + e∆/τ (1 + (V − 2)e−2/τ )\n\n−\n\n1 1 + e2/τ (1 + (V − 2)e−∆/τ )\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\nUsing the previous lemma, we will conclude the asymptotic behavior of φ(f S lemma:\n\nSEM(τ )) in the following\n\nLemma 5. It holds that\n\nProof. Using Lemma 4,\n\nlim τ →0\n\nφ(f S\n\nSEM(τ )) ≤ lim\n\nτ →0\n\nφ(f S\n\nSEM(τ )) → 0 as τ → 0.\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + (V − 1)e−2/τ\n\n−\n\n1 1 + (V − 1)e−∆/τ\n\n(cid:12) 2\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n+ n(V − 1) lim τ →0\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + e∆/τ (1 + (V − 2)e−2/τ )\n\n−\n\n1 1 + e2/τ (1 + (V − 2)e−∆/τ )\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n.\n\nMoreover,\n\nand\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nlim τ →0\n\nTherefore,\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nlim τ →0\n\n1 1 + (V − 1)e−2/τ\n\n−\n\n1 1 + (V − 1)e−∆/τ\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n=\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1\n\n−\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1\n\n= 0,\n\n1 1 + e∆/τ (1 + (V − 2)e−2/τ )\n\n−\n\n1 1 + e2/τ (1 + (V − 2)e−∆/τ )\n\n(cid:12) 2\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n= |0 − 0|2 = 0.\n\nlim τ →0\n\nφ(f S\n\nSEM(τ )) ≤ 0.\n\nSince φ(f S\n\nSEM(τ )) ≥ 0, this implies the statement of this lemma.\n\n19\n\nAs we have analyzed φ(f S φ(f S\n\nSEM(τ )) and φ(f S Lemma 6. For any τ > 0,\n\nSEM(τ )) in the previous two lemmas, we are now ready to compare\n\nbase), which is done in the following lemma:\n\nφ(f S\n\nSEM(τ )) − φ(f S\n\nbase) ≤\n\n3 4\n\n(1 − V ) < 0.\n\nProof. From Lemma 4, for any τ > 0,\n\nφ(f S\n\nSEM(τ )) ≤\n\n≤\n\n=\n\n≤\n\n=\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1\n(cid:12) (cid:12) 1\n(cid:12) (cid:18) 1 1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + (V − 1)e−2/τ\n\n−\n\n1 1 + (V − 1)e−∆/τ\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n+ n(V − 1)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + e∆/τ (1 + (V − 2)e−2/τ )\n\n−\n\n1 1 + e2/τ (1 + (V − 2)e−∆/τ )\n\n1 1 + (V − 1)e−2/τ\n\n−\n\n1 1 + (V − 1)\n\n(cid:12) 2\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n+ (V − 1)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + (1 + (V − 2)e−2/τ )\n\n−\n\n1 1 + e2/τ (1 + (V − 2))\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 1 + (V − 1)e−2/τ\n\n−\n\n1 V\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n+ (V − 1)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n1 2 + (V − 2)e−2/τ\n\n−\n\n1 1 + e2/τ (V − 1)\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n−\n\n1 V\n\n2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n+ (V − 1)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 2\n\n2\n\n(cid:12) (cid:12) − 0 (cid:12) (cid:12)\n\n(cid:19)2\n\n−\n\n1 V\n\n+ (V − 1)\n\n1 4\n\n.\n\nRecall the definition of\n\nφ(f S\n\nbase) = sup\n\ni∈[V ]\n\nsup q,q(cid:48)∈Qi\n\n(cid:107)q − q(cid:48)(cid:107)2 2.\n\nBy choosing an element in the set over which the supremum is taken, for any δ ≥ ∆ > 0,\n\nφ(f S\n\nbase) ≥ sup\n\nq,q(cid:48)∈Q1\n\n(cid:107)q − q(cid:48)(cid:107)2\n\n2 ≥ (cid:107)ˆq − ˆq(cid:48)(cid:107)2\n\n2 =\n\nV (cid:88)\n\nj=1\n\n(ˆqj − ˆq(cid:48)\n\nj)2\n\n2 = (2 − δ)2V,\n\nwhere ˆq1 = 1, ˆqj = 1 − δ for j ∈ {2, . . . , V }, ˆq(cid:48)\n\n1 = δ − 1, and ˆq(cid:48)\n\nj = −1 for j ∈ {2, . . . , V }.\n\nBy combining those, for for any τ > 0 and δ ≥ ∆ > 0,\n\nφ(f S\n\nSEM(τ )) − φ(f S\n\nbase) ≤\n\n(cid:19)2\n\n(cid:18) 1 1\n\n−\n\n1 V\n\n+ (V − 1)\n\n1 4\n\n− (2 − δ)2V\n\n≤ 1 +\n\nV −\n\n1 4\n\n− (2 − δ)2V\n\n+\n\nV − (2 − δ)2V\n\n(2 − δ)2 −\n\n(cid:19)\n\n1 4\n\n(cid:18)\n\n(cid:18)\n\n(cid:19)\n\n1 4\n\n1 4\n1 4\n\n− V\n\n=\n\n=\n\n≤\n\n=\n\n3 4\n3 4\n\n3 4\n3 4\n\n− V\n\n1 −\n\n(1 − V )\n\n20\n\nWe combine the lemmas above to prove Theorem 1, which is restated below with its proof:\n\nTheorem 1. Let V ≥ 2. For any 1 ≥ δ > 0, with probability at least 1 − δ, the following holds for any fS ∈ {f S\n\nSEM(τ ), f S\n\nbase}:\n\nEz,y[l(fS(z), y)] ≤\n\nl(fS(z(i)), y(i)) + R\n\nL φfS (V, τ ) + c\n\n(cid:113)\n\n(cid:114)\n\nln(2/δ) n\n\n,\n\n1 n\n\nn (cid:88)\n\ni=1\n\nwhere c > 0 is a constant in (n, f, H, δ, τ, S). Moreover,\n\nφf S\n\nSEM(τ )\n\n→ 0 as τ → 0\n\nand φf S\n\nSEM(τ )\n\n− φf S\n\nbase\n\n≤\n\n3 4\n\n(1 − V ) < 0 ∀τ > 0.\n\nProof. The first statement directly follows from Lemma 3. The second statement is proven by Lemma 5 and Lemma 6.\n\nB EXPERIMENT DETAILS FOR IMAGENET\n\nB.1\n\nIMAGE AUGMENTATION\n\nThe augmentation applied in order during training are:\n\n• Random Resize crop to a 224 × 224 image. A random patch of the image is selected and\n\nresized to a 224 × 224 image.\n\n• Random color jitter. Modifying the brightness, the contrast, the saturation and the hue.\n\n• Random gray scale. Randomly applying a gray scale filter to the image\n\n• Random Gaussian blur. Randomly applying a Gaussian bluer filter.\n\n• Random solarization. Randomly applying a solarization filter.\n\nThe parameters of the augmentations are presented in Table 16. At validation and test time, we resize the images to 256 × 256 and then center crop a patch of 224 × 224.\n\nFor both training and evaluation, we re-normalize the image using the statistic of the training set.g\n\nB.2 LINEAR EVALUATION\n\nWe follow the evaluation protocol from (Chen et al., 2020b). The linear evaluation is done by training a linear classifier on the frozen representation of the ImageNet training samples. We train a linear classifier with a cross-entropy objective for 100 epochs using SGD with nesterov, a momentum of 0.9 and a batch size of 256. We perform learning rate scheduling at epoch 60 and epoch 80 where we divide the learning rate by a factor of 10. During training, we apply random resized crop to 224 × 224 pixels and random horizontal flip. We sweep over a set of 4 learning rates: {0.5, 0.1, 0.05, 0.01}, 3 l1 weight decays: {0, 1e − 6, 1e − 5} and 3 τd for SEM: {0.01, 0.1, 1}, using a validation set of 10 images per class and re-traing using the full training set. We report the results on the test set.\n\nB.3 ROBUSTNESS EXPERIMENTS\n\nWe follow the evaluation procedure from (Lee et al., 2021). We treated the robustness datasets as additional \"test sets\" in that we simply evaluated them using the evaluation procedure described above. The images were resized to a 256 × 256 before being center cropped to a 224 × 224 image. The evaluation procedure was performed using the public robustness benchmark evaluation code of (Djolonga et al., 2020)§.\n\nB.4 TRANSFER LEARNING LINEAR PROBE\n\nWe follow the linear evaluation protocol of (Kolesnikov et al., 2019; Chen et al., 2020b) We train a linear classifier using a regularized multinomial logistic regression from the scikit-learn package (Pedregosa et al., 2011). The representation is frozen, so that we do not train the encoder backbone nor\n\n§https://github.com/google-research/robustness_metrics\n\n21\n\nthe batch-normalization statistics. We do not perform any augmentations and the images are resized to 224 pixels using bicubic resampling and the normalized using the statistics on ImageNet’s training set. We tune the regularizer term from a range of 45 logarithmically-spaced values between 10−6 and 105 using a small validation set and re-train using the full training set. For SEM, we set τd = 0 for all experiments.\n\nB.5 TRANSFER LEARNING FINE-TUNING\n\nWe follow the same fine-tuning protocol of (Chen et al., 2020b; Grill et al., 2020). We initialize the encoder with the pre-trained model and a classifier head with random initialization. We train for 20,000 steps with a batch size of 256 using SGD with a Nesterov momentum of 0.9. We set the momentum parameter for the batch normalization to be max(1 − 10/s, 0.9) where s is the number of steps per epoch. During pre-training, we use random resize to 224 × 224 pixels and random horizontal flipping. At test time, we resize the images along the shortest size to 256 pixels using cubic resampling following by a center resize to 224 × 224 pixels. Due to computational constraint, we only tune the learning rate using a search of 7 values spaces on logarithmic scales between 0.0001 and 0.1. For SEM, we set τd = 1. for all experiments After choosing the best learning rate of a validation set, we re-run the models using the full training set and evaluate it on the test set, which we use to report the numbers.\n\nB.6 SEMI-SUPERVISED LEARNING\n\nWe follow the semi-supervised learning protocol of (Chen et al., 2020b; Grill et al., 2020). We initialize the network using the pre-trained representation and initialize a classification head using random initialization. We fine-tune the encoder while training the classification head using a small subset of ImageNet. We choose the same subset used in prior works which is defined in the TensorFlow-Dataset software. During training, we random resize the images to 224 × 224 pixels along the shorter size using bicubic resampling followed by a center crop and random horizontal flipping. At test time, we resize the image to 224 × 224. We optimize the cross entropy loss with nestorov and a momentum of 0.9 using batch sizes of 224. We train models for {30, 50} and take the best performing on the validation set. The learning rate used is chosen among a set of 5 learning rates: {0.01, 0.02, 0.05, 0.1, 0.005}. For SEM, we also search τd ∈ {0.01, 0.1, 1}. We perform the search on the best performing one on the validation set and the number are returned are obtained using the test set after re-training using the full training set.\n\nC HYPERPARAMETERS\n\nThe implementation of the SSL methods used in this work are taken from Solo-Learn (da Costa et al., 2021) to which we added the SEM module. The pre-training hyper-parameters of every SSL methods trained on CIFAR-100 with ResNet-18 used in this work are the default provided in the companion repository of Solo-Learn. The hyper-parameters are also provided in the launch scripts accompanying this work. Due to the large number of SSL methods probed in this work and the amount of space it would require to exhaustively detail all of the hyper-parameters, we refer the reader to the code.\n\nFor the CIFAR-100 results obtained with BYOL and a ResNet-50, we have slightly modified the default parameters. Otherwise, the baseline BYOL model would not obtain competitive results. The hyper-parameters were tuned using the BYOL baseline and the SEM module was not considered in the selection of the SSL hyper-parameters. The BYOL hyper-parameters are presented in the launch script accompanying this work and presented below for completeness.\n\nFor the ImageNet experiments, we took the hyper-parameters proposed in the launch scripts of Solo-Learn to which we only modified the amount of epochs (100 epochs to 200 epochs.)\n\nHere, we present all of the SEM hyper-parameters used in every experiments. These hyper-parameters can also be found in the launch scripts accompanying this work.\n\nWe present the hype-parameters used to train for BYOL+SEM and MoCo+SEM on CIFAR100. Unless mentioned otherwise, these are the parameters used.\n\n22\n\nTable 6: BYOL with ResNet-50 for CIFAR-100.\n\nprecision Learning rate Weight-decay Optimizer LR scheduler eta lars exclude bias n norm (lars) batch size base ema momentum final ema momentum proj output dim proj hidden dim pred hidden dim augmentations: solarization_prob crop size hue saturation contrast brightness\n\n16 0.5 1e-4 sgd + lars warmup + cosine 0.001 True 256 0.99 1.0 256 4096 4096\n\nview 1: 0 view 2: 0.2 32 0.1 0.2 0.4 0.4\n\nTable 7: SEM SimCLR RN-18 for CIFAR-100\n\nL 5000\n\nV 13\n\nτp 0.17\n\nτ (cid:48) p\n0.78\n\nTable 8: SEM MoCo RN-18 for CIFAR100\n\nL 5000\n\nV 13\n\nτp 0.04\n\nτ (cid:48) p\n0.01\n\nTable 9: SEM BYOL RN-18 for CIFAR100\n\nL 5000\n\nV 13\n\nτp 1.0\n\n1.0\n\nTable 10: SEM SwAV RN-18 for CIFAR-100\n\nL 5000\n\nV 13\n\nτp 0.85\n\nτ (cid:48) p\n1.5\n\nTable 11: SEM DINO RN-18 for CIFAR100\n\nL 5000\n\nV 13\n\nτp 1.0\n\nτ (cid:48) p\n1.0\n\nTable 12: SEM Barlow RN-18 for CIFAR-100\n\nL 5000\n\nV 13\n\nτp 1.0\n\nτ (cid:48) p\n0.99\n\nTable 13: SEM VicREG RN-18 for CIFAR-100\n\nL 5000\n\nV 13\n\nτp 1.0\n\nτ (cid:48) p\n1.0\n\nTable 14: SEM BYOL RN-50 for CIFAR-100\n\nTable 15: SEM BYOL all ResNets for ImageNet\n\nL 5000\n\nV τp 1\n13\n\nτ (cid:48) p\n1\n\nL 5000\n\nV 21\n\nτp 0.16\n\nτ (cid:48) p\n0.04\n\nC.1 COMPUTATIONAL RESOURCES\n\nFor all our CIFAR-100 training, we used 1 RTX-8000 per experiment. For our ImageNet experiments, we used parallel training with 2 40GB A100 for the training with ResNet50 and ResNet50-x2 and 4 40GB A100 for the training with ResNet50-x4. With this setup, the training takes about a week for the ResNet50 experiments and about 10 days for the ResNet50-x2 and ResNet50-x4 experiments.\n\nD ADDITIONAL STUDIES OF SEM\n\nIn Section 4.2, we discussed the effect of scaling L and V as well as changing the Softmax temperature during pre-training of the online network and changing the Softmax temperature for the downstream task. Here, we propose additional studies of SEM to provide a better mastery of the method. We provide a method for reducing the memory overhead of SEM and experiments demonstrating that despite this version still largely outperform the baseline. We additionally present the effect of modifying the embedder contributing to the insight on how to get the most out of SEM. Next, we have discussion with a study of the spectrum of the covariance matrix of the SEM representation and the BYOL representation, showing insight how SEM can particularly improve the training signal\n\n23\n\nTable 16: BYOL with all ResNet-50 architectures for ImageNet.\n\nprecision Learning rate Weight-decay Optimizer LR scheduler eta lars exclude bias n norm (lars) batch size base ema momentum final ema momentum proj output dim proj hidden dim pred hidden dim augmentations: solarization_prob gaussian_prob crop size hue saturation contrast brightness\n\n16 0.4 1e-6 sgd + lars warmup + cosine 0.001 True 256 0.99 1.0 256 4096 4096\n\nview 1: 0 view 2: 0.2 view 1: 1.0 view 2: 0.1 224 0.1 0.2 0.4 0.4\n\nTable 17: # of parameters, # of activations, allocated memory, computation efficiency (FLOPs/sample) and CIFAR-100 accuracy of BYOL, BYOL with SEM and its memory-efficient variant with 8 blocks (denoted BYOL + SEM/8).\n\n# params\n\n# activations\n\nvRAM (GiB)\n\nFLOPs Accuracy\n\nResnet-18: BYOL BYOL+SEM BYOL+SEM/8 Resnet-50: BYOL BYOL+SEM BYOL+SEM/8\n\n16.5M 313.7M 51.9M\n\n35M 425.6M 76.7M\n\n0.731M 0.797M 0.796M\n\n4.05M 4.12M 4.12M\n\n4.0 13.1 5.3\n\n11.1 21.9 11.8\n\n7.20e8 1.01e9 7.46e8\n\n1.65e9 2.04e9 1.69e9\n\n70.7 73.9 73.3\n\n74.3 77.4 76.6\n\nduring pre-training. We provide a scaling analysis of BYOL and BYOL + SEM on CIFAR-100. We end with an experiment showing that pre-training with SEM is necessary to get the best performance.\n\nD.1 AN EFFICIENT VARIANT OF SEM\n\nA large over-complete representation may induce a significant memory footprint due to the additional parameters of the fully connected linear layer used to map to and from the representation. For SEM we require two such mappings as depicted in Figure 2c for BYOL. To reduce the amount of parameters, we propose to sparsify the weight matrix of the fully connected linear layer. We propose to do so by taking the block diagonal of the parameters of the matrix multiplication and setting the parameters outside the block diagonal to 0. Formally, let v ∈ Rb×m, w ∈ Rm×o and y = v · w be the fully connected matrix multiplication. Instead, we partition v into n blocks with vi ∈ Rb× m n and define n smaller wi ∈ R m n , where i ∈ [L] is the ith block. Then, we perform a batch matrix multiplication of vi and wi that we concatenate as follows: yi = vi · wi and ̄yi = Concat([y1, . . . , yn]). Thus, the amount of parameters of this matrix multiplication scales in O( m·o n ), allowing us to reduce the memory consumption by increasing n, the number of blocks.\n\nn × o\n\nWe perform an experiment where we partition the embedder and the first linear layer of the projector into 8 blocks. We present the results in Table 17 in which we compare the # of parameters, the\n\n24\n\n# of activations, the allocated vRAM by pytorch, the FLOPs/sample and the accuracy of BYOL, BYOL+SEM and BYOL+SEM/8 representing the model with 8 blocks obtained following the method described above. We observe that partitioning the matrix multiplications of SEM allows to vastly reduce the computation parameters while still yielding an important improvement over the baseline. This result demosntrate that SEM can be beneficial while inducing minimal computational overhead.\n\nAttentive readers may notice that this performance is better compared to the ablation presented in Figure 3. The difference in performance is due to probing the embedder’s output (i.e. zθ) in Figure 3 and probing the encoder’s output (i.e. eθ) in Table 17. Using the each ablation’s representation for probing to the other recovers the performance observed by each.\n\nD.2 ADDITIONAL ABLATION OF THE SEM PARAMETERS\n\nAblating the embedder In the main text, we mentioned that we use batch normalization)) at the output of the embedder. The reason we use batch normalization is mostly due to the fact that we wanted to avoid tuning any hyper-parameters that were not related to SEM to emphasize its contribution. Using BatchNorm gave the best performance without tuning the hyper-parameters of the baseline models.\n\nHere, we want to emphasize that SEM can be used without batch norm, but more hyper-parameters might need to be tuned for it to perform as well as the model with batch norm in the encoder. For example, we found that using no weight decay was important to get better performance when we did not have batch normalization as illustrated in Table table 18. We leave the full study of the interaction of SEM with the SSL related parameters for future work.\n\nTable 18: Understanding the relationship between the use of BatchNorm in the embedder and the weight decay hyper-parameter.\n\nBatchNorm weight decay Accuracy\n\n(cid:88) (cid:88)\n\n0 1e-5 0\n1e-5\n\n67.2 57.9 68.3 73.9\n\nAnother decision is to use a linear layer as the embedder. Other alternative may include using the Identidy function (i.e. the output of the encoder is used for SEM). However, if we want to systematically use the same encoder as the SSL model, then we are constrained to a representation size that is the one of the ResNet encoder (i.e. 512 for a ResNet-18).\n\nFinally, we showcase that using a more expressive embedder leads to exacerbated performance and recommend practitioner to limit the expressivity of their embedder.\n\nTable 19: Comparing alternative embedders.\n\nIdentity Linear 1 hidden layer MLP\n\nAccuracy 63.0 73.9 65.0\n\nA very very large embedding Using a ResNet-18 encoder and the method proposed in Section D.1, we further scale the embedding size of SEM to see where the performance saturates for classification. In Figure 7 we observe that the performance saturates for L = 10000 for the classification task. We conjecture that the optimal L might be different for other tasks, but we leave that study for future work.\n\nD.3 ANALYZE OF THE SPECTRUM OF THE COVARIANCE MATRIX OF THE REPRESENTATION\n\nTo obtain a better insight on why the SEM representation leads to better downstream performance, we analyze the spectrum of the covariance matrix of the representation using the methodology presented\n\n25\n\nFigure 7: Study of very very large L using a ResNet-18 backbone and 8 SEM/8 blocks using the method described in Section D.1.\n\nin Jing et al. (2022). That is, we collect the embedding vectors of the test set of CIFAR-100 using a pre-trained model using ResNet-50. For BYOL, we have an additional embedder without softmax normalization (as done in Figure 3). For BYOL and BYOL+SEM we use the embedder’s output (zθ) to perform the evaluation. To compute the covariance matrix C ∈ RL·V ×L·V of the embedding layer z, we define ̄z := (cid:80)N i=1 zi/N the average representation over the N samples and compute the covariance as follows:\n\nC :=\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(zi − ̄z)(zi − ̄z)(cid:62).\n\n(6)\n\nTo plot the spectrum of the covariance matrix, we take the singularalue decomposition of the matrix (C = U SV (cid:62)) with S the diagonal of the singular values, which we plot in sorted order and logarithm scale in Figure 8.\n\nThis experiment demonstrates that the softmax normalization counters the dimensionality collapse that was discussed in Jing et al. (2022). Interestingly, the drop observed with SEM with L ≥ 500 occurs at the index 2048 which is the dimensionality output of the ResNet-50 encoder.\n\nFigure 8: Spectrum of the covariance matrix of the represention for BYOL and BYOL + SEM obtained with a ResNet-50 encoder.\n\nD.4 SCALING THE RESNET ENCODER FOR CIFAR-100\n\nWe perform a scaling experiment on CIFAR-100 where we compare the scaling behaviour of BYOL and BYOL + SEM. We evaluate the computational cost of the methods and the resulting downstream accuracy for a range of four resnets: ResNet-18, ResNet-50, ResNet-50 x2 and ResNet-50 x4. In Figure 9, we observe that SEM has a better scaling behaviour than the baseline, especially as we increase the width of the ResNet-50. For BYOL, we observe that the performance decays for\n\n26\n\n10005000100002000050000L71.572.072.573.073.5Top-1 Accuracy020004000600080001000012000eigenvalue index201510505log eigenvalueSEM L=1000SEM L=500SEM L=100BYOL L=500Figure 9: Scaling the ResNet encoder for CIFAR-100.\n\nTable 20: Downstream accuracy of training a classifier with SEM normalization of the representation while using unormalized representation during pretraining. Experiments performed with a ResNet-50 encoder.\n\nPre-train model BYOL + Embed BYOL + Embed BYOL + SEM\n\nProbe location Embedder Embedder Embedder\n\nSEM(τ = 0.1) Accuracy\n\nNo Yes Yes\n\n69.8 72.3 77.3\n\nResNet-50 with width x2 and x4. This is not unprecendented, as prior works as demonstrated other methods where scaling up the capacity of a model led to decrease in performance. When comparing the discrepancy with Figure 1, we attribute that to the fact that CIFAR-100 is a small dataset. In fact, we observe that the training accuracy stays constant to about 79% for all the ResNet-50 scales demonstrating overfitting for the baseline BYOL. Nevertheless, SEM prevents the decrease in performance and even lead to further improved performance as we increase the scale of the ResNet-50.\n\nD.5 THE ROLE OF PRE-TRAINING WITH SEM\n\nWe probe the downstream accuracy obtained of a model pre-trained without SEM and add SEM normalization only for the downstream classification. For this experiment, we take a pre-trained model with embedder (i.e. BYOL + embed) with L = 5000 and V = 13 and add the softmax normalization only for the downstream classification. We do not use SEM during pre-training. We observe that using SEM for downstream classification leads to an improvement even when the model is not pre-trained with SEM, demonstrating the utility of SEM downstream classification. However, we note that the performance of the model pre-trained without SEM is much weaker and thus demonstrates the imprtance of also pre-training using SEM.\n\nE CIFAR-10 RESULTS\n\nWe confirm that our method also yield improvement on simpler datasets such as CIFAR-10. Here, we compare BYOL and BYOL + SEM on a ResNet-50 and observe and improvement of 1.6%.\n\nTable 21: Downstream accuracy of training a classifier with SEM normalization of the representation while using unormalized representation during pretraining. Experiments performed with a ResNet-50 encoder.\n\nPre-train model TOP-1 Accuracy BYOL BYOL + SEM\n\n94.2 95.8\n\n27\n\n7.2e+081.65e+094.65e+091.52e+10FLOPs6466687072747678Top-1 AccuracyRN18RN50RN50-X2RN50-X4RN18RN50RN50-X2RN50-X4BYOL + SEMBYOLF CIFAR100 SUPERCLASS\n\nThe 100 classes of CIFAR-100 (Krizhevsky, 2009) are grouped into 20 superclasses. The list of superclass for each class in Table 22\n\nTable 22: Set of classes for each superclass on CIFAR-100.\n\nSuperclass aquatic mammals fish flowers food containers fruit and vegetables household electrical devices household furniture insects large carnivores large man-made outdoor things large natural outdoor scenes large omnivores and herbivores medium-sized mammals non-insect invertebrates people reptiles small mammals trees vehicles 1 vehicles 2\n\nClasses beaver, dolphin, otter, seal, whale aquarium fish, flatfish, ray, shark, trout orchids, poppies, roses, sunflowers, tulips bottles, bowls, cans, cups, plates apples, mushrooms, oranges, pears, sweet peppers clock, computer keyboard, lamp, telephone, television bed, chair, couch, table, wardrobe bee, beetle, butterfly, caterpillar, cockroach bear, leopard, lion, tiger, wolf bridge, castle, house, road, skyscraper cloud, forest, mountain, plain, sea camel, cattle, chimpanzee, elephant, kangaroo fox, porcupine, possum, raccoon, skunk crab, lobster, snail, spider, worm baby, boy, girl, man, woman crocodile, dinosaur, lizard, snake, turtle hamster, mouse, rabbit, shrew, squirrel maple, oak, palm, pine, willow bicycle, bus, motorcycle, pickup truck, train lawn-mower, rocket, streetcar, tank, tractor\n\n28\n\nG ADDITIONAL CIFAR-100 COHERENCE GRAPHS\n\n29\n\n(a) BYOL baseline\n\n(b) BYOL baseline with a large representation\n\n(c) BYOL + SEM\n\nFigure 10: Comparison of the full semantic coherence graph W5 between BYOL and BYOL + SEM.\n\n30\n\nS363otterkangaroobottleS382lampS225beartankS144streetcarroadS249S167trainS94lobsterS273boygirlhousewomanS29S106cockroachS6appledolphinsealwhaleaquarium_fishflatfishraysharkorchidpoppyrosesunflowertulipbowlcancupplatemushroomorangepearsweet_pepperclockcomputer_keyboardtelephonetelevisionbedchaircouchtablewardrobebeebeetlebutterflycaterpillarleopardtigerwolfbridgecastleskyscrapercloudforestmountainplainseacamelcattlechimpanzeefoxpossumraccoonskunkcrabsnailwormbabymanlizardsnaketurtlehamstermouserabbitshrewsquirrelmaple_treeoak_treepalm_treewillow_treebicyclebusmotorcyclepickup_trucklawn_mowerrockettractorS55S63S47S279S162S69S134S242S34S241S0S98S158S361S91S66S12S95S388S53S374S190S22S59S183S261S1S375S27S35S130S110S127S43S111S258S380S214S30S85S292S193S238S336S128S132S284S14S99S93S332S236S298S260S281S68S204S161S182S177S381S64S227S320S318S266S230S196S126S181S184S114S187flatfishtableS16troutcaterpillarS194cattleS276orchidcomputer_keyboardS229palm_treeS184cloudrocketskunkchimpanzeeS256S187pickup_trucksnailcameltigerspiderS307dolphinsweet_pepperS85lionS434S415lobstersunflowermushroomS66orangesquirrelS189S319manS190snakeS195babywardrobewolfsealwillow_treecanS323maple_treeS252S404turtleleopardhamsterrabbitcockroachS353clockforestS87S61pine_treeS369couchS397tankotterwomanbicycleS365S148bedbeavertulipbeelawn_mowercrocodilelamptelephoneS435tractorS422oak_treeS233motorcycleshrewS284S33S146S13plainS78whalebusbeeS248spiderbeetlecockroachS178S240bearkangarootigerS393leopardhousecastleS40roadS107plainS403motorcyclelawn_mowertankS350tractorsealS61beaverS284porcupinebedS168couchS360chairlionfoxS216wolfS272skyscraperS227mountainS78seasnakeS98S49S219S2lizardwormorangeS185S30sweet_pepperpearS198S193applebusS18S133streetcarS370S106pickup_trucktrainroseS306poppyS289S329tulipS334S135orchidS210cancupS31S259S321S291bowlplatebottlewillow_treepine_treemaple_treeoak_treeforestS71palm_treeS138S327S405dolphinrayS151S347S17S379turtlesharkS176S189S355whaleS381S221S200S22boyS157girlwomanbabyS204flatfishmanmouseraccoonS336S271S300possumsquirrelS239S128shrewhamsterrabbit",
    "reference": "# Summary Of The Paper\n\nIn self-supervised learning (SSL), a representation is learned and it is usually one multi-dimensional vector. This paper proposes an effective representation using multiple sparsified embeddings (SEM) inspired by over-complete representation. It is obtained by projecting the SSL embedding into multiple smaller dimensional vectors each through a softmax operation. The sparsity of the smaller vectors can be controlled via the temperature parameter in the softmax function. The author gives theoretical analysis on the benefit of using SEM and also conducts extensive empirical experiments. The results show the superiority of the proposed method.\n\n# Strength And Weaknesses\n\nStrength: \n\nClarity and quality of writing. Extensive experiments.\nThe paper is well written with a pretty clear introduction of background and related work. I also like how the author draws inspirations from the over-complete representation literature to motivate the proposed method. The experimental results show both quality improvement (with reasonable computation increase) and interpretability improvement.\n\nOriginality. The work seems pretty novel to me. \nPractical application: Being aware of the increased memory usage from SEM, the author also studies how to efficiently reduce memory usage at inference time. \n\nThough the findings in the proposed work is largely based on prior work in SSL, the SEM idea is pretty interesting and effective (observation from the empirical results).\n\nWeakness:\nQuestions to the author(s):\n\n1. Regarding Figure 5 (a), or effects of increasing $L$. Have you investigated when the improvement of accuracy starts to diminish if we further increases $L$ (e.g, increase $L$ to 20K, 50K)?  A larger number of basic vectors could be very helpful in defining finer granularities in some applications.\n\n2. Could we increase the embedding dimension in BYOL to achieve the similar quality improvement as achieved by SEM?\n\n3. Is the BYOL metric in Figure 1 a strong baseline? This is not something I am familiar with. Please clarify.\n\n4. After SSL embedding is obtained, one usage of the SSL embedding is to perform clustering to obtain needed level of granularity and interpretability. Let's say we cluster the embeddings into 5K groups. And also on the other hand, we have the SEM embedding (when L= 5K), what would be the pros and cons of the two approaches ?\n\nI will be willing to increase my score once the questions above are well clarified/explained.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSee clarity/quality/novelty above.\nReproducibility:\nThe author includes very extensive descriptions of their experiment setup in the appendix and provides code in the supplemental maternal. I believe it will be sufficient to reproduce their work.\n\nMinor comment:\nAppendix D.1: the last line at page 24, \"the $ of parameters\" should probably be change to \"the # of parameters\".\n\n# Summary Of The Review\n\nThe author proposes a simple yet effective component to build more interpretable and more effective representation. The experiments are solid and extensively demonstrate the effectiveness of the method. There, I think it is a good paper - \"accept\".\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nn/a"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nREVISITING ACTIVATION FUNCTION DESIGN FOR IMPROVING ADVERSARIAL ROBUSTNESS AT SCALE\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nModern ConvNets typically use ReLU activation function. Recently smooth activation functions have been used to improve their accuracy. Here we study the role of smooth activation function from the perspective of adversarial robustness. We find that ReLU activation function significantly weakens adversarial training due to its non-smooth nature. Replacing ReLU with its smooth alternatives allows adversarial training to find harder adversarial training examples and to compute better gradient updates for network optimization.\n\nWe focus our study on the large-scale ImageNet dataset. On ResNet-50, switching from ReLU to the smooth activation function SILU improves adversarial robustness from 33.0% to 42.3%, while also improving accuracy by 0.9% on ImageNet. Smooth activation functions also scale well with larger networks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6% robustness, largely outperforming the previous state-of-the-art defense by 9.5% for accuracy and 11.6% for robustness. Models are available at https://rb.gy/qt8jya.\n\n1\n\nINTRODUCTION\n\nIt is known that convolutional neural networks can be easily fooled by adversarial examples (Szegedy et al., 2014). To improve robustness, many efforts have been made (Papernot et al., 2016; Guo et al., 2018; Xie et al., 2018; Liu et al., 2018; Pang et al., 2019; Schott et al., 2019); while adversarial training (Goodfellow et al., 2015; Kurakin et al., 2017; Madry et al., 2018), which trains networks with adversarial examples on-the-fly, stands as one of the most effective methods. Later studies further improve adversarial training by feeding networks with harder adversarial examples (Wang et al., 2019b), maximizing the margin of networks (Ding et al., 2020), optimizing a regularized surrogate loss (Zhang et al., 2019), etc. While these methods achieve stronger adversarial robustness, they sacrifice accuracy on clean inputs. It is generally believed such trade-off between accuracy and robustness might be inevitable (Tsipras et al., 2019), except for enlarging network capacities, e.g., making wider or deeper networks (Madry et al., 2018; Xie & Yuille, 2020).\n\nAnother popular direction for increasing robustness against adversarial attacks is gradient masking (Papernot et al., 2017; Xie et al., 2018; Samangouei et al., 2018; Song et al., 2018; Ma et al., 2018; Guo et al., 2018). With the degenerated gradient quality, attackers cannot successfully optimize the targeted loss and therefore fail to circumvent such defenses. Nonetheless, the gradient masking operation will be ineffective to offer robustness if its differentiable approximation is used for generating adversarial examples (Athalye et al., 2018).\n\nTo effectively build robust models, we hereby rethink the relationship between gradient quality and adversarial robustness, especially in the context of adversarial training where gradients are applied more frequently than standard training. In addition to computing gradients to update network parameters, adversarial training also requires gradient computation for generating training samples. Guided by this principle, we identify ReLU, a widely-used activation function in modern ConvNets, significantly weakens adversarial training due to its non-smooth nature, e.g., ReLU’s gradient gets an abrupt change (from 0 to 1) when its input is close to zero (see Figure 1).\n\nIn this paper, we revisit the activation function design for improving adversarial robustness, with a special focus on the large-scale ImageNet dataset (Russakovsky et al., 2015). To fix the issue\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The visualization of ReLU and Parametric Softplus. Left panel: the forward pass for ReLU (blue curve) and Parametric Softplus (red curve). Right panel: the first derivatives for ReLU (blue curve) and Parametric Softplus (red curve). Different from ReLU, Parametric Softplus is smooth with continuous derivatives.\n\ninduced by ReLU as aforementioned, we propose to apply its smooth approximations1 for improving the gradient quality in adversarial training (Figure 1 shows Parametric Softplus, an example of smooth approximations for ReLU). Our experiment results show that switching from ReLU to its smooth approximations in adversarial training can substantially improves adversarial robustness. For instance, by training with the computationally cheap single-step PGD attacker2 on ImageNet, the smooth activation function SILU significantly improves the robustness of the ResNet-50 baseline (which uses ReLU) by 9.3%, from 33.0% to 42.3%, meanwhile increasing standard accuracy by 0.9%. In addition, we note this performance improvement in both robustness and accuracy comes for “free”, as the change of activation function does not incur additional computational cost.\n\nWe next explore the limits of adversarial training with smooth activation function using larger networks. We obtain the best result by using EfficientNet-L1 (Tan & Le, 2019; Xie et al., 2020), which achieves 82.2% accuracy and 58.6% robustness on ImageNet, significantly outperforming the previous state-of-the-art defense (Qin et al., 2019) by 9.5% for accuracy and 11.6% for robustness.\n\n2 RELATED WORKS\n\nAdversarial training. Adversarial training improves robustness by training models on adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015; Kurakin et al., 2017; Madry et al., 2018). Existing works suggest that, to further adversarial robustness, we need to either sacrifice accuracy on clean inputs (Wang et al., 2019b; 2020; Zhang et al., 2019; Ding et al., 2020), or incur additional computational cost (Madry et al., 2018; Xie & Yuille, 2020; Xie & et al., 2019). This phenomenon is referred to as no free lunch in adversarial robustness (Tsipras et al., 2019; Nakkiran, 2019; Su et al., 2018). In this paper, we show that, using smooth activation function in adversarial training, adversarial robustness can be improved for “free”—no accuracy degradation on clean images and no additional computational cost incurred.\n\nOur work is also related to the theoretical study (Sinha et al., 2018), which shows replacing ReLU with smooth alternatives can help networks get a tractable bound when certifying distributional robustness. In this paper, we empirically corroborate the benefits of utilizing smooth activations is also observable in the practical adversarial training on the real-world dataset using large networks.\n\nGradient masking. Besides training models on adversarial data, other ways for improving adversarial robustness include defensive distillation (Papernot et al., 2016), gradient discretization (Buckman et al., 2018; Rozsa & Boult, 2019; Xiao et al., 2019), dynamic network architectures (Dhillon et al., 2018; Wang et al., 2018; 2019a; Liu et al., 2018; Lee et al., 2020; Luo et al., 2020), randomized transformations (Xie et al., 2018; Bhagoji et al., 2018; Xiao & Zheng, 2020; Raff et al., 2019; Kettunen et al., 2019; AprilPyone & Kiya, 2020), adversarial input denoising/purification (Guo et al., 2018; Prakash et al., 2018; Meng & Chen, 2017; Song et al., 2018; Samangouei et al., 2018; Liao et al., 2018; Bhagoji et al., 2018; Pang et al., 2020; Xu et al., 2017; Dziugaite et al., 2016), etc. Nonetheless, most of these defense methods degenerate the gradient quality of the protected models, therefore could induce the gradient masking issue (Papernot et al., 2017). As argued in (Athalye et al., 2018), defense methods with gradient masking may offer a false sense of adversarial robustness. In contrast to these works, we aim to improve adversarial robustness by providing networks with better gradients, but in the context of adversarial training.\n\n1The term “smooth” hereby refer to this function satisfies the property of being C1 smooth, i.e., its first\n\nderivative is continuous everywhere.\n\n2In practice, we note that single-step PGD adversarial training is only ∼1.5× slower than standard training.\n\n2\n\n-0.10.00.10.20.30.40.50.6-0.6-0.4-0.20.00.20.40.6Forward ReLU Parametric Soft(cid:83)lus-0.20.00.20.40.60.81.01.2-0.6-0.4-0.20.00.20.40.6Backward ReLU Parametric Soft(cid:83)lus-0.10.00.10.20.30.40.50.6-0.6-0.4-0.20.00.20.40.6Forward ReLU Parametric Soft(cid:83)lus-0.20.00.20.40.60.81.01.2-0.6-0.4-0.20.00.20.40.6Backward ReLU Parametric Soft(cid:83)lusUnder review as a conference paper at ICLR 2023\n\nTable 1: ReLU significantly weakens adversarial training. By improving gradient quality for either the adversarial attacker or the network optimizer, resulted models obtains better robustness than the ReLU baseline. The best robustness is achieved by adopting better gradients for both the attacker and the network optimizer. Note that, for all these model, ReLU is always used at their forward pass, albeit their backward pass may get probed in this ablation.\n\nNetwork\n\nResNet-50\n\nImproving Gradient Quality for the Adversarial Attacker ✗\n✓ ✗\n✓\n\nImproving Gradient Quality for the Network Optimizer ✗\n✗ ✓\n✓\n\nAccuracy (%) Robustness (%)\n\n68.8 68.3 (-0.5) 69.4 (+0.6) 68.9 (+0.1)\n\n33.0 34.5 (+1.5) 35.8 (+2.8) 36.9 (+3.9)\n\n3 RELU WEAKENS ADVERSARIAL TRAINING\n\nIn this section, we perform a series of controlled experiments, specifically in the backward pass of gradient computations, to investigate how ReLU weakens, and how its smooth approximation strengthens adversarial training.\n\n3.1 ADVERSARIAL TRAINING\n\nAdversarial training (Szegedy et al., 2014; Goodfellow et al., 2015; Madry et al., 2018), which trains networks with adversarial examples on-the-fly, aims to optimize the following framework:\n\narg min θ\n\nE(x,y)∼D\n\n(cid:104)\n\nmax ε∈S\n\nL(θ, x + ε, y)\n\n(cid:105) ,\n\n(1)\n\nwhere D is the underlying data distribution, L(·, ·, ·) is the loss function, θ is the network parameter, x is a training sample with the ground-truth label y, ε is the added adversarial perturbation, and S is the allowed perturbation range. To ensure the generated adversarial perturbation ε is humanimperceptible, we usually restrict the perturbation range S to be small (Szegedy et al., 2014; Goodfellow et al., 2015; Lou et al., 2015).\n\nAs shown in equation 1, adversarial training consists of two computation steps: an inner maximization step, which computes adversarial examples, and an outer minimization step, which computes parameter updates.\n\nAdversarial training setup. We choose ResNet-50 (He et al., 2016) as the backbone network, where ReLU is used by default. We apply PGD attacker (Madry et al., 2018) to generate adversarial perturbations ε. Specifically, we select the cheapest version of PGD attacker, single-step PGD (PGD-1), to lower the training cost. Following (Shafahi et al., 2019; Wong et al., 2020), we set the maximum per-pixel change ε = 4 and the attack step size β = 4 in PGD-1. We follow the standard ResNet training recipes to train models on ImageNet: models are trained for a total of 100 epochs using momentum SGD optimizer, with the learning rate decreased by 10× at the 30-th, 60-th and 90-th epoch; no regularization except a weight decay of 1e-4 is applied.\n\nWhen evaluating adversarial robustness, we measure the model’s top-1 accuracy against the 200-step PGD attacker (PGD-200) on the ImageNet validation set, with the maximum perturbation size ε = 4 and the step size β = 1. We note 200 attack iterations are enough to let PGD attacker converge. Meanwhile, we report the model’s top-1 accuracy on the original ImageNet validation set.\n\n3.2 HOW DOES GRADIENT QUALITY AFFECT ADVERSARIAL TRAINING?\n\nFigure 1 shows ReLU is non-smooth. ReLU’s gradient takes an abrupt change, when its input is close 0, therefore significantly degrades the gradient quality. We conjecture this non-smooth nature hurts the training process, especially when we train models adversarially. This is because, compared to standard training which only computes gradients for updating network parameter θ, adversarial training requires additional computations for the inner maximization step to craft the perturbation ε.\n\nTo fix this problem, we first introduce a smooth approximation of ReLU, named Parametric Softplus (Nair & Hinton, 2010), as f (α, x) = 1 α log(1 + exp(αx)), where the hyperparameter α is used to control the curve shape. Its derivative w.r.t. the input x is:\n\nd dx\n\nf (α, x) =\n\n1 1 + exp(−αx)\n\n(2)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nTo better approximate the curve of ReLU, we empirically set α = 10. As shown in Figure 1, compared to ReLU, Parametric Softplus (α=10) is smooth because it has a continuous derivative.\n\nWith Parametric Softplus, we next diagnose how gradient quality in the inner maximization step and the outer minimization step affects accuracy and robustness of ResNet-50 in adversarial training. To clearly benchmark the effects, we only substitute ReLU with equation 2 in the backward pass, while leaving the forward pass unchanged, i.e., ReLU is always used for model inference.\n\nImproving gradient quality for the adversarial attacker. We first take a look at the effects of gradient quality on computing adversarial examples (i.e., the inner maximization step) during training. More precisely, in the inner step of adversarial training, we use ReLU in the forward pass, but Parametric Softplus in the backward pass; and in the outer step of adversarial training, we use ReLU in both the forward and the backward pass.\n\nAs shown in the second row of Table 1, when the attacker uses Parametric Softplus’s gradient (equation 2) to craft adversrial training samples, the resulted model exhibits a performance trade-off. Compared to the ReLU baseline, it improves adversarial robustness by 1.5% but degrades standard accuracy by 0.5%.\n\nInterestingly, we note such performance trade-off can also be observed if harder adversarial examples are used in adversarial training (Wang et al., 2019b). This observation motivates us to hypothesize that better gradients for the inner maximization step may boost the attacker’s strength during training. To verify this hypothesis, we evaluate the robustness of two ResNet-50 models via PGD-1 (vs. PGD-200 in Table 1), one with standard training and one with adversarial training. Specifically, during the evaluation, the PGD-1 attacker uses ReLU in the forward pass, but Parametric Softplus in the backward pass. With better gradients, PGD-1 attacker is strengthened and hurts models more: it can further decrease the top-1 accuracy by 4.0% (from 16.9% to 12.9%) on the ResNet-50 with standard training, and by 0.7% (from 48.7% to 48.0%) on the ResNet-50 with adversarial training (both results are not shown in Table 1).\n\nImproving gradient quality for network parameter updates. We then study the role of gradient quality on updating network parameters (i.e., the outer minimization step) during training. More precisely, in the inner step of adversarial training, we use ReLU; but in the outer step of adversarial training, we use ReLU in the forward pass, and Parametric Softplus in the backward pass.\n\nSurprisingly, by setting the network optimizer to use Parametric Softplus’s gradient (i.e., equation 2) to update network parameters, this strategy improves adversarial robustness for “free”. As shown in the third row of Table 1, without incurring additional computations, adversarial robustness is boosted by 2.8%, and meanwhile accuracy is improved by 0.6%, compared to the ReLU baseline. We note the corresponding training loss also gets lower: the cross-entropy loss on the training set is reduced from 2.71 to 2.59. These results of better robustness and accuracy, and lower training loss together suggest that, by using ReLU’s smooth approximation in the backward pass of the outer minimization step, networks are able to compute better gradient updates in adversarial training.\n\nInterestingly, we observe that better gradient updates can also improve standard training. For example, with ResNet-50, training with better gradients can improve accuracy from 76.8% to 77.0%, and reduces the corresponding training loss from 1.22 to 1.18. These results on both adversarial training and standard training suggest updating network parameters using better gradients could serve as a principle for improving performance in general, while keeping the inference process of the model unchanged (i.e., ReLU is always used for inference).\n\nImproving gradient quality for both the adversarial attacker and network parameter updates. Given the observation that improving ReLU’s gradient for either the adversarial attacker or the network optimizer benefits robustness, we further enhance adversarial training by replacing ReLU with Parametric Softplus in all backward passes, but keeping ReLU in all forward passes.\n\nAs expected, such a trained model reports the best robustness so far. As shown in the last row of Table 1, it substantially outperforms the ReLU baseline by 3.9% for robustness. Interestingly, this improvement still comes for “free”—it reports 0.1% higher accuracy than the ReLU baseline. We conjecture this is due to the positive effect on accuracy brought by computing better gradient updates (increase accuracy) slightly overrides the negative effects on accuracy brought by creating harder training samples (hurt accuracy) in this experiment.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Visualizations of 5 different smooth activation functions and their derivatives.\n\n3.3 CAN RELU’S GRADIENT ISSUE BE REMEDIED?\n\nMore attack iterations. It is known that increasing the number of attack iterations can create harder adversarial examples (Madry et al., 2018; Dong et al., 2018). We confirm in our own experiments that by training with PGD attacker with more iterations, the resulted model exhibits a similar behavior to the case where we apply better gradients for the attacker. For example, by increasing the attacker’s cost by 2×, PGD-2 improves the ReLU baseline by 0.6% for robustness while losing 0.1% for accuracy. This result suggests we can remedy ReLU’s gradient issue in the inner step of adversarial training if more computations are given.\n\nTraining longer. It is also known that longer training can lower the training loss (Hoffer et al., 2017), which we explore next. Interestingly, by extending the default setup to a 2× training schedule (i.e., 200 training epochs), though the final model indeed achieves a lower training loss (from 2.71 to 2.62), there still exhibits a performance trade-off between accuracy and robustness. Longer training gains 2.6% for accuracy but loses 1.8% for robustness. On the contrary, our previous experiment shows that applying better gradients to optimize networks improves both robustness and accuracy. This discouraging result suggests that training longer cannot fix the issues in the outer step of adversarial training caused by ReLU’s poor gradient.\n\nConclusion. Given these results, we conclude that ReLU significantly weakens adversarial training. Moreover, it seems that the degenerated performance cannot be simply remedied even with training enhancements (i.e., increasing the number of attack iterations & training longer). We identify that the key is ReLU’s poor gradient—by replacing ReLU with its smooth approximation only in the backward pass substantially improves robustness, even without sacrificing accuracy and incurring additional computational cost. In the next section, we show that making activation functions smooth is a good design principle for enhancing adversarial training in general.\n\n4 ADVERSARIAL TRAINING WITH SMOOTH ACTIVATION FUNCTIONS\n\nAs demonstrated in Section 3, improving ReLU’s gradient can both strengthen the attacker and provide better gradient updates in adversarial training. Nonetheless, this strategy may be suboptimal as there still exists a discrepancy between the forward pass (for which we use ReLU) and the backward pass (for which we use Parametric Softplus) when training the networks.\n\nTo fully exploit the potential of training with better gradients, we hereby propose to exclusively apply smooth actvaition functions (in both the forward pass and the backward pass) in adversarial training. Noe that we keep all other network components exactly the same, as most of them are smooth and will not result in the issue of poor gradient.3\n\n4.1 SMOOTH ACTIVATION FUNCTIONS\n\nWe consider the following activation functions as ReLU’s smooth approximations in adversarial training (Figure 2 plots these functions and their derivatives):\n\n• Softplus (Nair & Hinton, 2010): Softplus(x) = log(1 + exp(x)). We also consider its\n\nparametric version where α is set to 10 as in Section 3.\n\n3We hereby ignore the gradient issue caused by max pooling, which is also non-smooth. This is because modern architectures rarely adopt it, e.g. only one max pooling layer is adopted in ResNet (He et al., 2016), and none is adopted in EfficientNet (Tan & Le, 2019).\n\n5\n\n-0.20.00.20.40.60.81.01.2-4.0-3.0-2.0-1.00.01.02.03.04.0Backward Parametric SoftPlus(cid:3) (cid:3)(cid:3)S(cid:44)(cid:47)(cid:56) GELU ELU SmoothReLU-1.0-0.30.41.11.82.53.23.9-4.0-3.0-2.0-1.00.01.02.03.04.0Forward Parametric SoftPlus(cid:3) (cid:3)(cid:3)S(cid:44)(cid:47)(cid:56) GELU ELU SmoothReLU-0.20.00.20.40.60.81.01.2-4.0-3.0-2.0-1.00.01.02.03.04.0Backward Parametric SoftPlus(cid:3) (cid:3)(cid:3)S(cid:44)(cid:47)(cid:56) GELU ELU SmoothReLU-1.0-0.30.41.11.82.53.23.9-4.0-3.0-2.0-1.00.01.02.03.04.0Forward Parametric SoftPlus(cid:3) (cid:3)(cid:3)S(cid:44)(cid:47)(cid:56) GELU ELU SmoothReLUUnder review as a conference paper at ICLR 2023\n\n• SILU (Elfwing et al., 2018; Hendrycks & Gimpel, 2016; Ramachandran et al., 2017): SILU(x) = x · sigmoid(x). Compared to other activation functions, SILU has a nonmonotonic “bump” when x < 0.\n\n• Exponential Linear Unit (ELU) (Clevert et al., 2016): if x ≥ 0, ELU(x, α) = x; otherwise ELU(x, α) = α(exp(x) − 1), ELU(x, α) = x if x ≥ 0, α(exp(x) − 1)otherwise, where we set α = 1 as default. Note that when α ̸= 1, the gradient of ELU is not continuously differentiable anymore. We will discuss the effects of these non-smooth variants of ELU (α ̸= 1) on adversarial training in Section 4.3.\n\n• Gaussian Error Linear Unit (GELU) (Hendrycks & Gimpel, 2016): GELU(x) = x · Φ(x), where Φ(x) is cumulative distribution function of the standard normal distribution.\n\nMain results. We follow the settings in Section 3 to adversarially train ResNet-50 equipped with smooth activation functions. The results are shown in Figure 3. Compared to the ReLU baseline, all smooth activation functions substantially boost robustness, while keeping standard accuracy almost the same. For example, smooth activation functions at least boost robustness by 5.7% (using Parametric Softplus, from 33% to 38.7%). Our strongest robustness is achieved by SILU, which enables ResNet-50 to achieve 42.3% robustness and 69.7% standard accuracy. We believe these results can be furthered if more advanced smooth alternatives (e.g., (Misra, 2019; Lokhande et al., 2020; Biswas et al., 2020)) are used.\n\nWe then compare to the setting in Section 3 where Parametric Softplus is only applied at the backward pass during training. Interestingly, by additionally replacing ReLU with Parametric Softplus at the forward pass, the resulted model further improves robustness by 1.8% (from 36.9% to 38.7%) while without hurting accuracy. This result demonstrates the importance of applying smooth activation functions in both forward and backward passes in adversarial training.\n\n4.2 RULING OUT THE EFFECT FROM x < 0\n\nCompared to ReLU, in addition to being smooth, the functions above have non-zero responses to negative inputs (x < 0) which may also affect adversarial training. To rule out this factor, inspired by the design of Rectified Smooth Continuous Unit in (Shamir et al., 2020), we hereby propose SmoothReLU, which flattens the activation function by only modifying ReLU after x ≥ 0,\n\nSmoothReLU(x, α) =\n\n(cid:26)x − 1\n\nα log(αx + 1)\n\n0\n\nif x ≥ 0, otherwise,\n\n(3)\n\nwhere α is a learnable variable shared by all channels of one layer, and is constrained to be positive. We note SmoothReLU is always continuously differentiable regardless the value of α,\n\nd dx\n\nSmoothReLU(x, α) =\n\n(cid:40) αx\n\n1+αx 0\n\nif x ≥ 0, otherwise.\n\n(4)\n\nNote SmoothReLU converges to ReLU when α → ∞. Additionally, in practice, the learnable parameter α needs to be initialized at a large enough value (e.g., 400 in our experiments) to avoid the gradient vanishing problem at the beginning of training. We plot SmoothReLU and its first derivative in Figure 2.\n\nWe observe SmoothReLU substantially outperforms ReLU by 7.3% for robustness (from 33.0% to 40.3%), and by 0.6% for accuracy (from 68.8% to 69.4%), therefore clearly demonstrates the importance of a function to be smooth, and rules out the effect from having responses when x < 0.\n\n4.3 STABILIZING ADVERSARIAL TRAINING WITH ELU USING CELU\n\nIn the analysis above, we show that adversarial training can be greatly improved by replacing ReLU with its smooth approximations. To further demonstrate the importance of being smooth, we provide another case study by using ELU (Clevert et al., 2016). The first derivative of ELU is shown below:\n\nd dx\n\nELU(x, α) =\n\n(cid:26)1\n\nif x ≥ 0,\n\nα exp(x) otherwise.\n\n(5)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Smooth activation functions improve adversarial training. Compared to ReLU, all smooth activation functions significantly boost robustness, while keeping accuracy almost the same.\n\nα\n\n1 1.2 1.4 1.6 1.8 2.0\n\nRobustness (%) CELU ELU\n\n41.1\n\n-0.3 -2.0 -3.7 -6.2 -7.9\n\n+0.1 -0.3 -0.3 -0.2 -0.5\n\nTable 2: Robustness comparison between ELU (non-smooth when α ̸= 1) and CELU (always smooth ∀α).\n\nHere we mainly discuss the scenario when ELU is non-smooth, i.e., α ̸= 1. As can be seen from equation 5, ELU’s gradient is not continuously differentiable anymore, i.e., α exp(x) ̸= 1 when x = 0, therefore resulting in an abrupt gradient change like ReLU. Specifically, we consider the range 1.0 < α ≤ 2.0, where the gradient abruption becomes more drastic with a larger value of α.\n\nWe show adversarial training results in Table 2. Interestingly, we observe that adversarial robustness is highly dependent on the value of α—the strongest robustness is achieved when the function is smooth (i.e., α = 1.0, 41.4% robustness), and all other choices of α monotonically decrease the robustness when α gradually approaches 2.0. For instance, with α = 2.0, the robustness drops to only 33.2%, which is 7.9% lower than that of using α = 1.0. The observed phenomenon here is consistent with our previous conclusion on ReLU—non-smooth activation functions significantly weaken adversarial training.\n\nTo stabilize adversarial training with ELU, we apply its smooth version, CELU (Barron, 2017), which re-parametrize ELU to the following format:\n\nCELU(x, α) =\n\nα (cid:0)exp (cid:0) x Note that CELU is equivalent to ELU when α = 1.0. The first derivatives of CELU can be written as follows:\n\n(cid:1) − 1(cid:1)\n\nα\n\n(6)\n\nif x ≥ 0, otherwise.\n\n(cid:26)x\n\nd dx\n\nCELU(x, α) =\n\n(cid:26)1\n\nexp x α\n\nif x ≥ 0, otherwise.\n\n(7)\n\nWith this parameterization, CELU is now continuously differentiable regardless of the choice of α. As shown in Table 2, we can observe that CELU greatly stabilizes adversarial training. Compared to the best result reported by ELU/CELU (α = 1.0), the worst case in CELU (α = 2.0) is merely 0.5% lower. Recall that this gap for ELU is 7.9%. This case study further support that it is important to apply smooth activation functions in adversarial training.\n\n5 EXPLORING THE LIMITS OF ADVERSARIAL TRAINING WITH SMOOTH\n\nACTIVATION FUNCTIONS\n\nRecent works (Xie & Yuille, 2020; Gao et al., 2019) show that, compared to standard training, adversarial training exhibits a much stronger requirement for larger networks to obtain better performance. Nonetheless, previous explorations in this direction only consider either deeper networks (Xie & Yuille, 2020) or wider networks (Madry et al., 2018), which might be insufficient. To this end, we hereby present a systematic and comprehensive study on showing how network scaling up behaves in adversarial training, but using smooth activation functions. We set SILU as the default activation function, as it achieves the best robustness among all other candidates (see Figure 3).\n\n5.1 SCALING-UP RESNET\n\nWe first perform the network scaling-up experiments with the ResNet family. In standard training, Tan et al. (Tan & Le, 2019) already show that, all three scaling-up factors, i.e., depth, width and image resolutions, are important to further improve ResNet performance. We hereby examine the effects of these scaling-up factors in adversarial training with smooth activation functions. We choose ResNet-50 (with the default image resolution at 224) as the baseline network.\n\n7\n\n3334353637383940414243Robustness (%)686970Accuracy (%)GELUSoftplusELUSILUParametric SoftplusReLUSmoothReLUUnder review as a conference paper at ICLR 2023\n\nTable 3: Scaling-up ResNet with the smooth activation function SILU in adversarial training. We observe larger networks consistently get better performance.\n\nAccuracy (%) Robustness (%)\n\nResNet-50 + 2x deeper (ResNet-101) + 3x deeper (ResNet-152) + 2x wider (ResNeXt-50-32x4d) + 4x wider (ResNeXt-50-32x8d) + image size 299 + image size 380 + 3x deeper & 4x wider (ResNeXt-152-32x8d) & image size 380\n\n69.7 72.9 (+3.2) 73.9 (+4.2) 71.2 (+1.5) 73.6 (+3.9) 70.9 (+1.2) 71.6 (+1.9) 78.2 (+8.5)\n\n42.3 45.5 (+3.2) 46.0 (+3.7) 42.5 (+0.2) 45.1 (+2.8) 43.8 (+1.5) 44.1 (+1.8) 51.2 (+8.9)\n\nDepth & width. Previous works have shown that making networks deeper or wider can yield better model performance in standard adversarial training. We re-verify this conclusion using ResNet with the smooth activation function SILU. As shown in the second to fifth rows of Table 3, we confirm that both deeper or wider networks consistently outperform the baseline network. For instance, by training a 3× deeper ResNet-152, it improves ResNet-50’s performance by 4.2% for accuracy and 3.7% for robustness. Similarly, by training a 4× wider ResNeXt-50-32x8d (Xie et al., 2017), it improves accuracy by 3.9% and robustness by 2.8%.\n\nImage resolution. Though larger image resolution benefits standard training, it is generally believed that scaling up this factor will induce weaker adversarial robustness, as the attacker will have a larger room for crafting adversarial perturbations (Galloway et al., 2019). However, surprisingly, this belief could be invalid when taking adversarial training into consideration. As shown in the sixth and seventh rows of Table 3, ResNet-50 with the smooth activation function SILU consistently achieves better performance when training with larger image resolutions.\n\nWe conjecture this improvement is possibly due to larger image resolution (1) enables attackers to create stronger adversarial examples (Galloway et al., 2019) (which will hurt accuracy but improve robustness); and (2) increases network capacity for better representation learning (Tan & Le, 2019) (which will improve both accuracy and robustness); and the mixture of these two effects empirically yields a positive signal here.\n\nCompound scaling. So far, we have confirmed that the basic scaling of depth, width and image resolution are all important scaling-up factors. As argued in (Tan & Le, 2019) for standard training, scaling up all these factors simultaneously will produce a much stronger model than just focusing on scaling up a single dimension. Hence we make an attempt to create a simple compound scaling for ResNet. As shown in the last row of Table 3, we can observe that the resulted model, ResNeXt-15232x8d with input resolution at 380, achieves significantly better performance than the ResNet-50 baseline, i.e., +8.5% for accuracy and +8.9% for robustness.\n\nAdversarial training with ReLU. We first verify that the basic scaling of depth, width and image resolution also matter when ReLU is used in adversarial training e.g., by scaling up ResNet-50 (33.0% robustness), the deeper ResNet-152 achieves 39.4% robustness (+6.4%), the wider ResNeXt50-32x8d achieves 36.7% robustness (+3.7%), and the ResNet-50 with larger image resolution at 380 achieves 36.9% robustness (+3.9%). Nonetheless, all these robustness performances are still lower than the robustness achieved by the basic ResNet-50 with the smooth activation function SILU (42.3% robustness, first row of Table 3).\n\nWe also find compound scaling is more effective than basic scaling for adversarial training with ReLU, e.g., ResNeXt-152-32x8d with input resolution at 380 reports 46.3% robustness. Although this result is much better than the basic scaling above, it is still ∼5% lower than ResNet + SILU with compound scaling, i.e., 46.3% v.s. 51.2%. In other words, even with larger networks, applying smooth activation functions remains essential for improving performance.\n\n5.2 EFFICIENTNET RESULTS\n\nThe results on ResNet show scaling up networks with smooth activation functions in adversarial training effectively improves performance. Nonetheless, the applied scaling policies could be suboptimal, as they are hand-designed without any optimizations. EfficientNet (Tan & Le, 2019), which uses neural architecture search (Zoph & Le, 2016) to automatically discover the optimal factors for network compound scaling, provides a strong family of models for image recognition. Therefore we next use EfficientNet to replace ResNet. Note that all other training setups (e.g., using SILU) are the same as described in our ResNet experiments.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nSimilar to ResNet, Figure 4 shows stronger EfficientNet consistently achieves better performance. For instance, by scaling the network from EfficientNet-B0 to EfficientNet-B7, the robustness is improved from 37.6% to 57.0%, and the accuracy is improved from 65.1% to 79.8%. Surprisingly, the improvement is still observable for larger networks: EfficientNet-L1 (Xie et al., 2020) further improves robustness by 1.0% and accuracy by 0.7% over EfficientNet-B7. All these results corroborate the finding in (Cubuk et al., 2018) that stronger backbone tend to yield higher robustness.\n\nTraining enhancements. So far all of our experiments follow the training recipes from ResNet, which may not be optimal for EfficientNet training. Therefore, as suggested in the original EfficientNet paper (Tan & Le, 2019), we adopt the following training setups in our experiments: we change weight decay from 1e-4 to 1e-5, and add Dropout (Srivastava et al., 2014), Stochastic Depth (Huang et al., 2016) and AutoAugment (Cubuk et al., 2019) to regularize the training process. Besides, we train models with longer schedule (i.e., 200 training epochs) to better cope with these training enhancements, adopt the early stopping strategy to prevent the catastrophic overfitting issue in robustness (Wong et al., 2020), and save checkpoints using model weight averaging (Izmailov et al., 2018) to approximate the model ensembling for stronger robustness (Pang et al., 2019; Strauss et al., 2017). Additionally, as shown in the concurrent work (Rebuffi et al., 2020), applying weight averaging is the key for enabling data augmentation to improve robustness. With these training enhancements, our EfficientNet-L1 gets further improved, i.e., +1.7% for accuracy (from 80.5% to 82.2%) and +0.6% for robustness (from 58.0% to 58.6%).\n\nFigure 4: Scaling-up EfficientNet with the smooth activation function SILU in adversarial training. Note EfficientNet-L1 is not connected to the rest of the graph because it was not part of the compound scaling suggested by Tan et al. (Tan & Le, 2019).\n\nComparing to the prior art (Qin et al., 2019). Table 4 compares our best results with the prior art. By applying the smooth activation function in adversarial training, we are able to train a model with strong performance on both adversarial robustness and standard accuracy—our best model (i.e., EfficientNet-L1) achieves 82.2% standard accuracy and 58.6% robustness, which largely outperforms the prior art (Qin et al., 2019) by 9.5% for standard accuracy and 11.6% for adversarial robustness. Note this improvement mainly stems from the facts that we hereby exploit a better activation function (SILU vs. Softplus) and a stronger architecture (EfficientNet-L1 vs. ResNet-152).\n\nAccuracy drop vs. model scale. Finally, we emphasize a large reduction in the accuracy gap between adversarially trained models and standard trained models for large networks. With the training setup above, EfficientNet-L1 is able to attain 84.1% accuracy in standard training, while this accuracy only slightly decreases to 82.2% (-1.9%) in adversarial training. Note this gap is substantially smaller than the gap in ResNet-50 of 7.1% (76.8% in standard training v.s. 69.7% in adversarial training with the smooth activation function SILU). Moreover, it is also worth mentioning the high accuracy of 82.2% provides strong support to (Ilyas et al., 2019) on arguing robust features indeed can generalize well to clean inputs.\n\nTable 4: Comparison to the previous state-of-the-art. Robustness (%) 47.0 58.6 (+11.6)\n\nPrior art (Qin et al., 2019) EfficientNet (ours)\n\nAccuracy (%) 72.7 82.2 (+9.5)\n\n6 CONCLUSION\n\nIn this paper, we revisit the activation function design for improving adversarial robustness at scale. Specifically, we propose to replace the non-smooth activation function ReLU with its smooth approximations (like Softplus or SILU) for ensuring architectural smoothness in adversarial training. Applying smooth activation functions in adversarial training improves adversarial robustness without sacrificing accuracy or incurring additional computation cost. Extensive experiments on ImageNet demonstrate the general effectiveness of adversarial training with smooth activation functions. By pushing the network scale to the very large EfficientNet-L1, adversarial training with smooth activation functions reports the state-of-the-art adversarial robustness on ImageNet, which substantially outperforms the prior art (Qin et al., 2019) by 9.5% for accuracy and 11.6% for robustness. Our results also corroborate the recent findings that there exist certain network architectures which have better adversarial robustness (Cubuk et al., 2018; Chen et al., 2020; Guo et al., 2020; Xie & et al., 2019; Huang et al., 2021). We hope these works together can encourage more researchers to investigate this direction.\n\n9\n\n101102103Number of Parameters (Millions, log-scale)354045505560Robustness (%)556065707580Accuracy (%)B7L180.558.0Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMaungMaung AprilPyone and Hitoshi Kiya. Block-wise image transformation with secret key for\n\nadversarially robust defense. arXiv preprint arXiv:2010.00801, 2020.\n\nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of\n\nsecurity: Circumventing defenses to adversarial examples. In ICML, 2018.\n\nJonathan T Barron.\n\nContinuously differentiable exponential\n\nlinear units.\n\narXiv preprint\n\narXiv:1704.07483, 2017.\n\nArjun Nitin Bhagoji, Daniel Cullina, Chawin Sitawarin, and Prateek Mittal. Enhancing robustness\n\nof machine learning systems via data transformations. In CISS, 2018.\n\nKoushik Biswas, Sandeep Kumar, Shilpak Banerjee, and Ashish Kumar Pandey. Tanhsoft–a family of activation functions combining tanh and softplus. arXiv preprint arXiv:2009.03863, 2020.\n\nJacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot\n\nway to resist adversarial examples. In ICLR, 2018.\n\nNicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705, 2019.\n\nHanlin Chen, Baochang Zhang, Song Xue, Xuan Gong, Hong Liu, Rongrong Ji, and David arXiv preprint Anti-bandit neural architecture search for model defense.\n\nDoermann. arXiv:2008.00698, 2020.\n\nDjork-Arn ́e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network\n\nlearning by exponential linear units (elus). ICLR, 2016.\n\nFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble\n\nof diverse parameter-free attacks. In ICML, 2020.\n\nEkin D Cubuk, Barret Zoph, Samuel S Schoenholz, and Quoc V Le. Intriguing properties of adver-\n\nsarial examples. In ICLR Workshop, 2018.\n\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\n\nLearning augmentation policies from data. In CVPR, 2019.\n\nGuneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense. In ICLR, 2018.\n\nGavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Max-margin adversarial (mma) training: Direct input space margin maximization through adversarial training. In ICLR, 2020.\n\nYinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Xiaolin Hu, Jianguo Li, and Jun Zhu. Boost-\n\ning adversarial attacks with momentum. In CVPR, 2018.\n\nGintare Karolina Dziugaite, Zoubin Ghahramani, and Daniel M Roy. A study of the effect of jpg\n\ncompression on adversarial images. arXiv preprint arXiv:1608.00853, 2016.\n\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network\n\nfunction approximation in reinforcement learning. Neural Networks, 2018.\n\nAngus Galloway, Anna Golubeva, Thomas Tanay, Medhat Moussa, and Graham W Taylor. Batch normalization is a cause of adversarial vulnerability. arXiv preprint arXiv:1905.02161, 2019.\n\nRuiqi Gao, Tianle Cai, Haochuan Li, Liwei Wang, Cho-Jui Hsieh, and Jason D Lee. Convergence\n\nof adversarial training in overparametrized networks. In NeurIPS, 2019.\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. In ICLR, 2015.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nChuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial\n\nimages using input transformations. In ICLR, 2018.\n\nMinghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, and Dahua Lin. When nas meets robustness: In\n\nsearch of robust architectures against adversarial attacks. In CVPR, 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In CVPR, 2016.\n\nDan Hendrycks and Kevin Gimpel.\n\nGaussian error linear units (gelus).\n\narXiv preprint\n\narXiv:1606.08415, 2016.\n\nElad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems, 2017.\n\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\n\nstochastic depth. In ECCV, 2016.\n\nHanxun Huang, Yisen Wang, Sarah Erfani, Quanquan Gu, James Bailey, and Xingjun Ma. Exploring\n\narchitectural ingredients of adversarially robust deep neural networks. 2021.\n\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander\n\nMadry. Adversarial examples are not bugs, they are features. In NeurIPS, 2019.\n\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon WilarXiv preprint\n\nson. Averaging weights leads to wider optima and better generalization. arXiv:1803.05407, 2018.\n\nMarkus Kettunen, Erik H ̈ark ̈onen, and Jaakko Lehtinen. E-lpips: Robust perceptual image similarity\n\nvia random transformation ensembles. arXiv preprint arXiv:1906.03973, 2019.\n\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In ICLR,\n\n2017.\n\nHakmin Lee, Hong Joo Lee, Seong Tae Kim, and Yong Man Ro. Robust ensemble model training via random layer sampling against adversarial attack. arXiv preprint arXiv:2005.10757, 2020.\n\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-\n\nscape of neural nets. In NeurIPS, 2018.\n\nFangzhou Liao, Ming Liang, Yinpeng Dong, and Tianyu Pang. Defense against adversarial attacks\n\nusing high-level representation guided denoiser. In CVPR, 2018.\n\nXuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks\n\nvia random self-ensemble. In ECCV, 2018.\n\nVishnu Suresh Lokhande, Songwong Tasneeyapant, Abhay Venkatesh, Sathya N. Ravi, and Vikas Singh. Generating accurate pseudo-labels in semi-supervised learning and avoiding overconfident predictions via hermite polynomial activations. In CVPR, 2020.\n\nYan Lou, Xavier Boix, Gemma Roig, Tomaso Poggio, and Qi Zhao. Foveation-based mechanisms\n\nalleviate adversarial examples. arXiv preprint arXiv:1511.06292, 2015.\n\nTiange Luo, Tianle Cai, Mengxiao Zhang, Siyu Chen, and Liwei Wang. Random mask: Towards\n\nrobust convolutional neural networks. arXiv preprint arXiv:2007.14249, 2020.\n\nXingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using local intrinsic dimensionality. 2018.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\n\nTowards deep learning models resistant to adversarial attacks. In ICLR, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In\n\nCCS, 2017.\n\nDiganta Misra. Mish: A self regularized non-monotonic neural activation function. arXiv preprint\n\narXiv:1908.08681, 2019.\n\nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.\n\nIn ICML, 2010.\n\nPreetum Nakkiran. Adversarial robustness may be at odds with simplicity.\n\narXiv preprint\n\narXiv:1901.00532, 2019.\n\nTianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via\n\npromoting ensemble diversity. In ICML, 2019.\n\nTianyu Pang, Kun Xu, and Jun Zhu. Mixup inference: Better exploiting mixup to defend adversarial\n\nattacks. In ICLR, 2020.\n\nTianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. Bag of tricks for adversarial\n\ntraining. In ICLR, 2021.\n\nNicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a\n\ndefense to adversarial perturbations against deep neural networks. In SP, 2016.\n\nNicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram\n\nSwami. Practical black-box attacks against machine learning. In AsiaCCS, 2017.\n\nAaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, and James Storer. Deflecting\n\nadversarial attacks with pixel deflection. In CVPR, 2018.\n\nChongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvijotham, Alhussein Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli. Adversarial robustness through local linearization. In NeurIPS, 2019.\n\nEdward Raff, Jared Sylvester, Steven Forsyth, and Mark McLean. Barrage of random transforms\n\nfor adversarially robust defense. In CVPR, 2019.\n\nPrajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv\n\npreprint arXiv:1710.05941, 2017.\n\nSylvestre-Alvise Rebuffi, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Timothy\n\nMann. Data augmentation can improve robustness. In NeurIPS, 2020.\n\nAndras Rozsa and Terrance E Boult. Improved adversarial robustness by reducing open space risk\n\nvia tent activations. arXiv preprint arXiv:1908.02435, 2019.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.\n\nPouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against\n\nadversarial attacks using generative models. In ICLR, 2018.\n\nLukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially\n\nrobust neural network model on mnist. In ICLR, 2019.\n\nAli Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S\n\nDavis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In NeurIPS, 2019.\n\nGil Shamir, Dong Lin, and Lorenzo Coviello. Smooth activations and reproducibility in deep net-\n\nworks. arXiv preprint arXiv:2010.09931, 2020.\n\nAman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with\n\nprincipled adversarial training. In ICLR, 2018.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nYang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. In ICLR, 2018.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n\nDropout: A simple way to prevent neural networks from overfitting. JMLR, 2014.\n\nThilo Strauss, Markus Hanselmann, Andrej Junginger, and Holger Ulmer. Ensemble metharXiv preprint\n\nods as a defense to adversarial perturbations against deep neural networks. arXiv:1709.03423, 2017.\n\nDong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness the cost of accuracy?–a comprehensive study on the robustness of 18 deep image classification models. In ECCV, 2018.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,\n\nand Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.\n\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural net-\n\nworks. In ICML, 2019.\n\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. In ICLR,\n\nThere is no free lunch in adversarial robustness (but there are unexpected benefits). 2019.\n\nSiyue Wang, Xiao Wang, Pu Zhao, Wujie Wen, David Kaeli, Peter Chin, and Xue Lin. Defensive\n\ndropout for hardening deep neural networks under adversarial attacks. In ICCAD, 2018.\n\nXiao Wang, Siyue Wang, Pin-Yu Chen, Yanzhi Wang, Brian Kulis, Xue Lin, and Peter Chin. Protecting neural networks with hierarchical random switching: Towards better robustness-accuracy trade-off for stochastic defenses. In IJCAI, 2019a.\n\nYisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the\n\nconvergence and robustness of adversarial training. In ICML, 2019b.\n\nYisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. adversarial robustness requires revisiting misclassified examples. In ICLR, 2020.\n\nImproving\n\nEric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training.\n\nIn ICLR, 2020.\n\nChang Xiao and Changxi Zheng. One man’s trash is another man’s treasure: Resisting adversarial\n\nexamples by adversarial examples. In CVPR, 2020.\n\nChang Xiao, Peilin Zhong, and Changxi Zheng. Enhancing adversarial defense by k-winners-take-\n\nall. In ICLR, 2019.\n\nCihang Xie and et al. Feature denoising for improving adversarial robustness. In CVPR, 2019.\n\nCihang Xie and Alan Yuille. Intriguing properties of adversarial training at scale. In ICLR, 2020.\n\nCihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial\n\neffects through randomization. In ICLR, 2018.\n\nQizhe Xie, Eduard Hovy, Minh-Thang Luong, and Quoc Le. Self-training with noisy student im-\n\nproves imagenet classification. In CVPR, 2020.\n\nSaining Xie, Ross Girshick, Piotr Doll ́ar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-\n\nformations for deep neural networks. In CVPR, 2017.\n\nWeilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep\n\nneural networks. In NDSS, 2017.\n\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.\n\nTheoretically principled trade-off between robustness and accuracy. In ICML, 2019.\n\nBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In ICLR, 2016.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Comparison of loss landscapes between the network using ReLU and the network using SILU, on a randomly selected ImageNet sample.\n\nA SANITY TESTS FOR ROBUSTNESS EVALUATION\n\nCorrectly performing robustness evaluation is non-trivial, as there exist many factors (e.g., gradient mask) which may derail the adversarial attacker from accurately accessing the model performance. To avoid these evaluation pitfalls, we run a set of sanity tests, following the recommendations in (Carlini et al., 2019). Specifically, we take the ResNet-50 with SILU as the evaluation target.\n\nRobustness vs. attack iterations. By increasing the attack iterations from 5 to 200, the resulted PGD attacker consistently hurts the model more. For example, by evaluating against PGD-{5, 10, 50}, the model reports the robustness of 48.7%, 43.7% and 42.7%, respectively. This evaluation finally gets converged at 42.3% when using PGD-200.\n\nRobustness vs. perturbation sizes ε. We also confirm a larger perturbation budget strengthens the attacker. By increasing ε from 4 to 8, the robustness drops more than 25%; the model will be completely circumvented if we set ε = 16.\n\nLandscape visualization. We compare the loss landscapes between ours and the ReLU baseline, on a randomly selected samples from ImageNet val set. Specifically, following (Li et al., 2018), the x/yaxis refer to the directions of adversarial perturbation, and z-axis refers to the corresponding crossentropy loss. As shown in Figure 5, compared to the ReLU baseline, we observe that adversarial training with smooth activation functions produces a much smoother loss landscape. Note this smooth loss landscape can also be observed when using other smooth activation functions (besides SILU) and on other randomly selected images.\n\nIn summary, these observations confirm our robustness evaluation with PGD attacker is properly done in this paper.\n\nB CIFAR-10 RESULTS\n\nThough the main focus of this paper is to study adversarial robustness on the large-scale ImageNet dataset, in this section, we briefly check how adversarial training with smooth activation functions performs on CIFAR-10. We adversarially train ResNet-18 with the following settings: during training, the attacker is PGD-1 with maximum perturbation ε = 8 and step size β = 8; we use AutoAttack (Croce & Hein, 2020) to holistically evaluate these models.\n\nFirstly, as expected, we note Softplus, GELU and SmoothReLU all lead to better results than the ReLU baseline—compared to ReLU, all these three activation functions maintain a similar accuracy, but a much stronger robustness. For examples, the improvements on robustness are ranging from 0.7% (by GELU) to 2.3% (by Softplus).\n\nHowever, unlike our ImageNet experiments where smooth activation functions demonstrate consistent improvements over ReLU, we note there are two exceptions on CIFAR-10—compared to ReLU, ELU and SILU even significantly hurt adversarial training. More interestingly, this inferior\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nperformance can also be observed on the training set. For example, compared to ReLU, SILU yields 4.1% higher training error and ELU yields 10.6% higher training error. As suggested in (Pang et al., 2021; Wong et al., 2020), adversarial training on CIFAR-10 is highly sensitive to different parameter settings, therefore we leave the exploring of the “optimal” adversarial training setting with ELU and SILU on CIFAR-10 as a future work.\n\n15",
    "reference": "# Summary Of The Paper\n\nThis paper finds that non-smooth ReLU activation function weaken the adversarial robustness and smooth activation functions such as SiLU can improve the adversarial robustness. The experiments are based on large-scale ImageNet dataset and show the state-of-the-art performance.\n\n# Strength And Weaknesses\n\nStrength\n- This paper focuses on ImageNet dataset. To my best knoweldge, it is the first time to do extensive experiments for the large scale ImageNet dataset.\n- This paper proposes a new algorithm for adversarial training with ReLU in forward pass and with smooth activation in backward pass without additional computational cost. It improves the adversarial robustness without hampering standard accuracy.\n- This paper empirically shows SiLU is the most effective in adversarial robustness among various smooth activation functions on ImageNet.\n\nWeakness\n- This paper lacks intuitive explanation (and theoretical studies) about why smooth activation functions improve the adversarial robustness.\n- The experiments are conducted on only ImageNet. It is necessary to conduct experiments with other benchmark datasets to support the main claim. If the results of other benchmark datasets are not consistent, the novelty would be small.\n- The evaluation is not properly executed. Checking for gradient masking is very crucial. I admit that autoattack is difficult to use on ImageNet dataset. At least, one black-box attack such as Square Attack [1] is necessary for checking the gradient masking.\n\n[1] Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas and Hein, Matthias, Square Attack: a query-efficient black-box adversarial attack via random search, In ECCV, 2020.\n\nQuestion :\n- Authors insist that the gradient from a smooth activation function is better than ReLU. In what sense is it a better gradient?\n\nMinor\n[At second paragraph in 4th section] Noe that we keep => Note that we keep\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity, Quality, Novelty And Reproducibility are provided in Strength and Weakness Section.\n\n# Summary Of The Review\n\nThe authors empirically show that the smooth activation function improves the adversarial robustness on ImageNet dataset. Extensive experiments on ImageNet dataset are interesting. But, I have concerns about the checking for gradient masking and consistent results for other benchmark dataset and theoretical results ( or explanation ) of why smooth activation function improves adversarial robustness.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMAXIMAL CORRELATION-BASED POST-NONLINEAR LEARNING FOR BIVARIATE CAUSAL DISCOVERY\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nBivariate causal discovery aims to determine the causal relationship between two random variables from passive observational data (as intervention is not affordable in many scientific fields), which is considered fundamental and challenging. Designing algorithms based on the post-nonlinear (PNL) model has aroused much attention for its generality. However, the state-of-the-art (SOTA) PNL-based algorithms involve highly non-convex objectives due to the use of neural networks and non-convex losses, thus optimizing such objectives is often time-consuming and unable to produce meaningful solutions with finite samples. In this paper, we propose a novel method that incorporates maximal correlation into the PNL model learning (short as MC-PNL) such that the underlying nonlinearities can be accurately recovered. Owing to the benign structure of our objective function, when modeling the nonlinearities with linear combinations of random Fourier features, the target optimization problem can be solved rather efficiently and rapidly via the block coordinate descent. We also compare the MC-PNL with SOTA methods on the downstream synthetic and real causal discovery tasks to show its superiority in time and accuracy. Our code is available at https://anonymous.4open.science/r/MCPNL-E446/ .\n\n1\n\nINTRODUCTION AND RELATED WORKS\n\nCausal discovery is an old and new topic to the machine learning community, which aims to find causal relationships among variables. Many recent attempts at application have emerged in various scientific domains, such as climate science (Ebert-Uphoff & Deng, 2012; Runge et al., 2019), bioinformatics (Choi et al., 2020; Foraita et al., 2020; Shen et al., 2020), etc. The gold standard for causal discovery is to conduct randomized experiments (via interventions), however, interventions are often expensive, unethical, and impractical. It is highly demanded to discover causal relationships purely from passive observational data. In the past three decades, many pioneer algorithms for directed acyclic graph (DAG) searching have been developed for multi-variate causal discovery to reduce the computational complexity and improve the accuracy. For example, there are constraint/independencebased algorithms such as IC, PC, FCI (Pearl, 2009; Spirtes et al., 2000), RFCI (Colombo et al., 2012) (too many to be listed), as well as score-based methods such as GES (Chickering, 2002), NOTEARS (Zheng et al., 2018), etc. However, the algorithms mentioned above can merely return a Markov equivalence class (MEC) that encodes the same set of conditional independencies, with many undetermined edge directions; moreover, the discovered DAG may not necessarily be causal. In this paper, we will focus on a fundamental problem, namely bivariate causal discovery, which aims to determine the causal direction between two random variables X and Y . Bivariate causal discovery is one promising routine for further identification of the underlying causal DAG (Peters et al., 2017).\n\nBivariate causal discovery is a challenging task, which cannot be directly solved using the existing methodologies for the multivariate case, as the two candidate DAGs, X → Y and X ← Y , are in the same MEC. More assumptions should be imposed to make bivariate causal discovery feasible, as summarized by Peters et al. (2017). One assumption is on the a priori model class restriction, e.g., linear non-Gaussian acyclic model (LiNGAM) (Shimizu et al., 2006), nonlinear additive noise model (ANM) (Mooij et al., 2016), post-nonlinear (PNL) model (Zhang & Hyvärinen, 2009), etc. The other assumption is on the \"independence of cause and mechanism\" leading to the algorithms of trace condition (Janzing et al., 2010), IGCI (Janzing et al., 2012), distance correlations (Liu & Chan, 2016), meta-transfer (Bengio et al., 2020), CDCI (Duong & Nguyen, 2022), etc. There are\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nalso seminal works focusing on causal discovery in linear/nonlinear dynamic systems, which are out of the scope of this paper, and the corresponding representatives are Granger causality test (Granger, 1969) and convergent cross mapping (Sugihara et al., 2012; Ye et al., 2015).\n\nIn this work, we focus on the PNL model, which is more general than LiNGAM and ANM. The existing works merely show the identifiability results with infinite data samples (i.e. known joint distribution), while practical issues with finite sample size are seldom discussed. We reveal the difficulties with the current PNL-based algorithms in the finite sample regime, such as insufficient model fitting, slow training progress, and unsatisfactory independent test performance, and correspondingly propose novel and practical solutions.\n\nThe main contributions of this work are as follows.\n\n1. We point out various practical training issues with the existing PNL model learning algorithms, in particular PNL-MLP and AbPNL, and propose a new algorithm called MC-PNL (specifically the maximal correlation-based algorithm with independence regularization), which can achieve a better recovery of the underlying nonlinear transformations.\n\n2. We suggest using the randomized dependence coefficient (RDC) instead of the HilbertSchmidt independence criterion (HSIC) for the independent test and give a universal view of some widely used dependence measures.\n\n3. We use MC-PNL for model learning in bivariate causal discovery and show that our method outperforms other SOTA independence test-based methods on various benchmark datasets.\n\n2 PRELIMINARIES\n\nIn this section, we will introduce the HSIC as a dependence measure, the current HSIC-based causal discovery methods for PNL model, and other relevant learning methods based on the HirschfeldGebelein-Rényi (HGR) correlation. Our proposed MC-PNL method exploits all these ingredients.\n\n2.1 HSIC SCORE AND HSIC-BASED REGRESSION\n\nRegression by dependence minimization (Mooij et al., 2009) has attracted lots of attention recently. Greenfeld & Shalit (2020) has shown its power for robust learning, in particular the unsupervised covariate shift task. Let us consider the following regression model,\n\nY = f (X) + (cid:15),\n\n(cid:15) ⊥⊥ X,\n\n(1)\n\nwhere the additive noise (cid:15) is independent (symbolized by ⊥⊥) with the input variable X, and the selected regression model fθ is to be learned via minimizing the dependency between the input variable X and the residual Y − fθ(X). A widely used dependence measure is the Hilbert-Schmidt independence criterion (HSIC) (Gretton et al., 2005; 2007).\n\nDefinition 1 (HSIC). Let X, Z ∼ PXZ be jointly distributed random variables, and F, G be the reproduced kernel Hilbert spaces with kernel functions k and l, the HSIC can be expressed as,\n\nHSIC(X, Z; F, G) =EXZEX (cid:48)Z(cid:48)k (x, x(cid:48)) l (z, z(cid:48)) + EX EX (cid:48)k (x, x(cid:48)) EZEZ(cid:48)l (z, z(cid:48))\n\n− 2EX (cid:48)Z(cid:48) [EX k (x, x(cid:48)) EZl (z, z(cid:48))] ,\n\n(2)\n\nwhere x(cid:48) and z(cid:48) denote independent copies of x and z, respectively.\n\nRemark 2.1. We can conclude that: (a) X ⊥⊥ Z ⇒ HSIC(X, Z) = 0; (b) with a proper universal kernel (e.g., Gaussian kernel), X ⊥⊥ Z ⇐ HSIC(X, Z) = 0 (Gretton et al., 2005).\n\nWhen the joint distribution PXZ is unknown, given a dataset with n samples (x = [x1, x2, . . . , xn]T ∈ Rn, z = [z1, z2, . . . , zn]T ∈ Rn), a biased HSIC estimate can be constructed as,\n\n(cid:92)HSIC (x, z; F, G) =\n\n1\n\nn2 tr(KHLH),\n\n(3)\n\nn 11T ∈ Rn×n is a centering matrix. The where Ki,j = k (xi, xj), Li,j = l (zi, zj), and H = I − 1 Gaussian kernel k (xi, xj) = exp (cid:0)−(xi − xj)2σ−2(cid:1) is commonly used, and the same for l. One\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ncan intuitively interpret this empirical HSIC as the inner-product of two centralized kernel matrices 1\nn2 (cid:104)HKH, HLH(cid:105), where the kernel matrices summarize the sample similarities. Mooij et al. (2009) first proposed to use the above defined empirical HSIC for model learning. Concretely, the regression model is a linear combination of the basis functions, fθ(x) = (cid:80)k i=1 θiφi(x), and the parameters are learned from:\n\nˆθ ∈ arg min\n\nθ∈Rp\n\n(cid:18)\n\n(cid:91)HSIC(x, y − fθ(x)) +\n\n(cid:19)\n\n,\n\n(cid:107)θ(cid:107)2\n\n2\n\nλ 2\n\n(4)\n\nwhere fθ is applied elementwisely to the data points, and λ > 0 is a penalty parameter (we will keep using λ as a penalty parameter under different contexts). One key advantage of this formulation is that it requires no assumption on the noise distribution. Greenfeld & Shalit (2020) implemented fθ using neural networks, and showed the learnability of the HSIC loss theoretically.\n\n2.2 CAUSAL DISCOVERY WITH POST-NONLINEAR MODEL\n\nThe bi-variate post-nonlinear model is expressed as, Y = f2(f1(X) + (cid:15)), where f1 denotes the nonlinear effect of the cause, (cid:15) is the independent noise, and f2 denotes the invertible post-nonlinear distortion from the sensor or measurement side. The goal is to find the causal direction X → Y from a set of passive observations on X and Y . Note that from the data generating process, (cid:15) is independent with X but not Y . Taking this asymmetry as a prior, one can test the causal direction by first learning the underlying transformations, f −1 and f1, and then checking the independence between the residual r(→) = f −1\n\n2 (Y ) − f1(X) and the input X.\n\n2\n\nThe PNL-MLP algorithm proposed by Zhang & Hyvärinen (2009) tests between two hypotheses (X → Y and X ← Y ) as follows. Under the hypothesis X → Y , one can parameterize f1 and f −1 by two multi-layer perceptrons (MLPs) f(→) and g(→), and learn them via minimizing the mutual information (MI):\n\n2\n\nˆf(→), ˆg(→) ∈ arg min\n\nf(→),g(→)\n\nMI (cid:0)r(→) := g(→)(y) − f(→)(x); x(cid:1) ,\n\n(5)\n\nwhere g(→), f(→) are applied elementwisely. The estimated residual is ˆr(→) = ˆg(→)(y) − ˆf(→)(x). Similarly, under the hypothesis X ← Y , one can obtain an estimate of ˆr(←) = ˆg(←)(x) − ˆf(←)(y) via minimizing MI(r(←); y). The causal direction is determined by comparing (cid:92)HSIC (cid:0)ˆr(→), x(cid:1) and (cid:92)HSIC (cid:0)ˆr(←), y(cid:1). If (cid:92)HSIC (cid:0)ˆr(→), x(cid:1) < (cid:92)HSIC (cid:0)ˆr(←), y(cid:1), the hypothesis X → Y is endorsed; otherwise, the hypothesis X ← Y is endorsed.\n\nHowever, the MI between random variables is often difficult to calculate (see supplement A), and tuning the MLPs requires many tricks as mentioned in Zhang & Hyvärinen (2009), altogether bringing huge difficulties to handle large-scale datasets with many variable pairs. Uemura & Shimizu (2020) proposed AbPNL method that uses HSIC instead of MI, and imposes the invertibility restriction of f2 via an auto-encoder to eliminate nonsense solutions,\n\n(cid:92)HSIC (x, r := g(y) − f (x)) + λ(cid:107)y − g(cid:48)(g(y))(cid:107)2 2,\n\nmin f,g,g(cid:48)\n\n(6)\n\nwhere g, g(cid:48) are encoder and decoder MLPs. The subscript (→) is omitted for conciseness here.\n\nWe summarize the architectures of the above-mentioned two methods in Figure 1. Nevertheless, inherent issues exist concerning the cost function and the neural network training procedure when dealing with finite sample datasets, see in Section 3.1.\n\n2.3 PNL LEARNING THROUGH MAXIMAL CORRELATION\n\nAnother routine to learn the nonlinear transformations f and g is through the HGR maximal correlation (Hirschfeld, 1935; Gebelein, 1941; Rényi, 1959).\n\nDefinition 2 (HGR maximal correlation). Let X, Y be jointly distributed random variables. Then,\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nρ∗ = HGR(X; Y ) :=\n\nsup f :X →R,g:Y→R E[f (X)]=E[g(Y )]=0 E[f 2(X)]=E[g2(Y )]=1\n\nE[f (X)g(Y )],\n\n(7)\n\nis the HGR maximal correlation between X and Y , and f, g are the associated maximal correlation functions. Remark 2.2. The HGR maximal correlation ρ∗ is attractive as a measure of dependency due to some useful properties: (1) Bounded ρ∗ : 0 ≤ ρ∗ ≤ 1; (2) X and Y are independent if and only if ρ∗ = 0; (3) there exists f and g such that f (X) = g(Y ) with probability 1 if and only if ρ∗ = 1.\n\nThe optimal unit-variance feature transformations, f ∗ and g∗, can be found by iteratively updating f and g in (7). However, for causal discovery applications, one fatal issue is that the learned f ∗ and g∗ are constrained to have unit-variance, thus being unable to reflect the true magnitudes of the underlying functions f and g. As a consequence, the resulting residual can be incorrect for the independence tests in the next stage. We found two possible remedies in the literature, namely the alternating conditional expectation (ACE) algorithm (Breiman & Friedman, 1985) and a soft version of (7) (Soft-HGR)(Wang et al., 2019).\n\nThe ACE algorithm solves the regression problem (8) by computing the conditional mean alternatively,\n\nE(f (X) − g(Y ))2,\n\ns.t. E[f (X)] = E[g(Y )] = 0, E[g2(Y )] = 1,\n\n(8)\n\nmin f,g\n\nwhich only retains the unit-variance constraint on g. The equivalence to (7) was established, and the regression optimal transformation (f ∗∗, g∗∗) equals (ρ∗f ∗, g∗), see Theorem 5.1 in Breiman & Friedman (1985). The other formulation, Soft-HGR, relaxes the unit-variance constraints as follows,\n\nE [f (X)g(Y )] −\n\nmax f,g\n\n1 2\n\nvar(f (X)) var(g(Y )),\n\ns.t. E[f (X)] = E[g(Y )] = 0.\n\n(9)\n\nIt allows certain linear transformations (af ∗, a−1g∗), where a ∈ R\\{0} can produce infinitely many equivalent local minima. This scale ambiguity results in enormous useless solutions for causal discovery, and the desired one should make the estimated residual independent with the input. We will show how our proposed method is able to eliminate those undesired solutions in Section 4.\n\nConnections to VICReg. We notice that the recent proposed Variance-Invariance-Covariance Regularization (VICReg) (Bardes et al., 2022) shares similar intuitions with the HGR maximal correlation. When the dimension of representation vectors (i.e., f and g) reduces to one, the covariance term disappears, and the VICReg objective becomes,\n\nmin f,g\n\nE(f (X) − g(Y ))2 (cid:125) (cid:123)(cid:122) (cid:124) invariance term\n\n+λ [max(0, γ − var(f (X))) + max(0, γ − var(g(Y )))] (cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) variance term\n\n,\n\n(10)\n\nwhere λ, γ > 0 are the hyper-parameters that need to be tuned. The invariance term encourages the alignment of the learned features; and the variance term encourages a γ-bounded variation to avoid trivial solutions like f (X) = g(Y ) = constant. To see the connections, we rewrite Soft-HGR (9) as,\n\nmin f,g\n\nE [f (X) − g(Y )]2 (cid:125) (cid:123)(cid:122) (cid:124) invariance term s.t. E[f (X)] = E[g(Y )] = 0,\n\n+ var(f (X)) var(g(Y )) − var(f (X)) − var(g(Y )) (cid:125)\n\n(cid:124)\n\n,\n\n(11)\n\n(cid:123)(cid:122) variance term\n\nin which the variance of f and g are also encouraged but not allowed to grow simultaneously.\n\n3 PRACTICAL ISSUES WITH EXISTING ALGORITHMS\n\nIn this section, we summarize several practical issues of the existing algorithms for PNL learning, including among others PNL-MLP (Zhang & Hyvärinen, 2010) and AbPNL (Uemura & Shimizu, 2020). These issues motivate our novel MC-PNL method to be introduced in Section 4, see the comparisons in terms of their architectures in Figure 1.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.1\n\nISSUES ON MODEL LEARNING\n\nOver-fitting issue. The general idea of the PNL model learning, according to Section 2.2, is to encourage the independence between the input and the estimated residual. Both PNL-MLP and AbPNL use neural networks to parameterize f and g. But it is skeptical that meaningful representations can really be learned with finite samples. Let us review the dependence minimization problem below,\n\n(cid:92)HSIC(x, r) =\n\nmin f,g\n\n1\n\nn2 tr(KxxHLrrH), where r = f (x) − g(y).\n\n(12)\n\nWe argue that it is utmost difficult to learn meaningful representations of f and g via minimizing solely the HSIC score, due to the enormous degrees of freedom for f and g to learn arbitrary random noise. We conducted experiments using both wide over-parameterized and narrow deep neural networks with sufficient representation power. In our simulation results (see supplement B), for both network architectures, the objective values can reach zero but unfortunately produce meaningless estimates. This is unsurprising though, as one can force r to be samples from arbitrary independent random noise (Yun et al., 2019; Zhang et al., 2021). To aid with that, we propose to cooperate dependence minimization with maximal correlation, which helps to obtain desired solutions, see Figure 1(c) for illustration and Section 4 for details.\n\nOptimization issue. The optimization of neural networks is a long-standing problem, and yet there is not any study on the optimization landscape of the HSIC loss with neural networks. Typically, first-order methods such as stochastic gradient descent are used in the existing causal discovery methods, and initialization is crucial to the causal discovery accuracy, see in supplement B. In this paper, we suggest parameterizing both f and g as a linear combination of random Fourier features and using a linear kernel for HSIC, which admit a benign landscape with symmetry (see Chapter 7 in Wright & Ma (2022)) for the non-convex optimization.\n\n(a) PNL-MLP\n\n(b) AbPNL\n\n(c) MC-PNL (Ours)\n\nFigure 1: Architectures of PNL learning frameworks.\n\n3.2\n\nISSUES ON INDEPENDENCE TEST\n\nAs the independence test is critical to the accuracy of causal discovery, we have to cautiously choose the dependence measure. Although HSIC is widely used, there are several drawbacks of HSIC (e.g., the choice of kernel and corresponding hyper-parameters are user-defined, the values of HSIC depends on the scale of the random variables). In this section, we show experimentally that the HSIC score is not the best choice, and we favor randomized dependence coefficient (RDC) (Lopez-Paz et al., 2013) particularly for finite samples.\n\nWe generated various synthetic datasets following the PNL models, see supplement C, in which we know in advance that the injected noise (cid:15) ⊥⊥ X and (cid:15) (cid:54)⊥⊥ Y . Thus, we are able to compare various dependence measures, by checking whether Dep(x, (cid:15)) < Dep(y, (cid:15)) on various datasets. In this section, the compared dependence measures are HSIC (Gretton et al., 2005), its normalized variant (NOCCO) (Fukumizu et al., 2007), and RDC (Lopez-Paz et al., 2013). Besides, we also study the impact of different choices of linear, Gaussian radial basis function (RBF), and rational quadratic (RQ) kernels. We note here that RDC is a computational tractable estimator inspired by the HGR maximal correlation. It shows that RDC outperforms other dependence measures especially when the sample size is small, see Table 1. Thus, we advocate to use RDC to measure dependency with finite samples. Finally, we give a universal view of the aforementioned dependence measures in supplement D.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: The independence test accuracy (%) with known injected noise\n\n# samples noise level σ(cid:15)\n\nHSIC-linear HSIC-RBF HSIC-RQ NOCCO-RBF NOCCO-RQ RDC\n\nn = 1000\n\nn = 2000\n\nn = 5000\n\n0.1\n\n1\n\n79 78 79 87 87 94\n\n95 97 98 83 82 100\n\n10\n\n100 100 100 75 73 100\n\n0.1\n\n1\n\n88 85 86 83 81 98\n\n96 99 99 85 86 100\n\n10\n\n100 100 100 72 65 100\n\n0.1\n\n85 86 91 90 85 100\n\n1\n\n97 97 96 92 94 100\n\n10\n\n100 100 100 90 87 100\n\n4 PROPOSED METHOD\n\nIn this section, we propose a maximal correlation-based post-nonlinear model learning framework, called MC-PNL, to accurately estimate the nonlinear functions and compute the corresponding residuals. After then, independence tests will be conducted to determine the causal direction.\n\n4.1 MAXIMAL CORRELATION-BASED PNL MODEL LEARNING\n\nAs we can see in the previous sections, minimizing HSIC (12) requires no assumption on the noise distribution and encourages the independence of the residual, but it can easily get stuck at meaningless local minima. Maximal HGR correlation based methods can learn meaningful transformations as its name suggested, but not necessarily produce independent residual.\n\nTo combine their strengths, we propose the following MC-PNL objective,\n\n−E [f (X)g(Y )] + 1 min f,g s.t. E[f (X)] = E[g(Y )] = 0,\n\n2 var(f (X)) var(g(Y )) + λDep(X, f (X) − g(Y )),\n\n(13)\n\nwhere Dep(·, ·) ≥ 0 is a dependence measure (e.g., HSIC with different kernel functions), and λ > 0 is a hyper-parameter that penalizes the dependence between the input variable X and the estimated residuals f (X) − g(Y ). This novel objective can learn meaningful feature transformations with the Soft-HGR term, and resolve the scale ambiguity via the dependence minimization term. The objective (13) is consistent with minimizing MI principle, under the assumptions of invertible PNL generating functions and Gaussian noise, see details in supplement E.\n\nParameterization with Random Features\n\nFor ease of optimization, we parameterize the transformation functions as the linear combination of the random features, namely f (x; α) := αT φ(x) and g(y; β) := βT ψ(y) , where the random features φ(x) ∈ Rk1, ψ(y) ∈ Rk2 are nonlinear projections as described in López-Paz et al. (2013), see supplement F. For a given dataset {(xi, yi)}n i=1, the corresponding feature matrices are denoted as Φ := [φ(x1), φ(x2), . . . , φ(xn)] ∈ Rk1×n and Ψ := [ψ(y1), ψ(y2), . . . , ψ(yn)] ∈ Rk2×n. We further denote the residual vector as r := ΦT α − ΨT β.\n\nConsequently, (13) can be written as the following non-convex programming problem, 2n2 αT ΦΦT αβT ΨΨT β + λDep(x, r) n αT ΦΨT β + 1\n\nJ(α, β) := − 1\n\nmin α,β\n\ns.t. αT Φ1 = βT Ψ1 = 0,\n\n(14)\n\nwhere 1 is an all-ones vector, and the dependence measure Dep(x, r) can be specially set to the HSIC score with linear kernel, namely,\n\nlin\n\n(cid:92)HSIC\n\n(x, r) =\n\n=\n\n=\n\n1\n\n1\n\n1\n\nrr H) =\n\nn2 tr(KxxHrrT H)\n\nn2 tr(KxxHLlin n2 tr(KxxH(ΦT α − ΨT β)(ΦT α − ΨT β)T H) (αT ΦHKxxHΦT α + βT ΨHKxxHΨT β − 2αT ΦHKxxHΨT β) n2\n\n.(15)\n\nRemark: We adopt the HSIC with linear kernel Llin rr mainly for a favorable optimization structure, as the resulting HSIC score admits a quadratic form w.r.t. both α and β. Note that the penalty HSIC term is always non-negative, but the Soft-HGR objective can be negative.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nThe above problem can be solved via a simple block coordinate descent (BCD) algorithm that updates α and β iteratively, see Algorithm 1. Essentially, (14) is multi-convex (Xu & Yin, 2013), and in each update (line 3 or 4 in Algorithm 1), the sub-problem is a linearly constrained quadratic programming. When the sub-problem is strictly convex, one can obtain the unique minimum in closed-form in each update, which admits convergence guarantee to a critical point (Grippo & Sciandrone, 2000). More details on the subproblem optimization and the landscape study can be found in supplement G.\n\nAlgorithm 1 BCD for problem 14\n\n1: Initialize α(0) and β(0) // Use random initialization 2: for t=1:T do 3:\n\nUpdate α(t) ← arg minα J(α, β(t−1)), Update β(t) ← arg minβ J(α(t), β), if stopping creteria is met then\n\n4: 5:\n\ns.t. αT Φ1 = 0.\n\ns.t. βT Ψ1 = 0.\n\nreturn α(t), β(t)\n\n6: end if 7: 8: end for\n\nRemark: We can also impose the invertability of g by limiting the derivative d (or negative) in line 4, i.e., ̃ΨT β > 0, where ̃Ψ = [ d\n\ndy ψ(y1), d\n\ndy ψ(y2), . . . , d\n\ndy g(y) to be positive\n\ndy ψ(yn)] ∈ Rk2×n.\n\nFine-tune: Algorithm 1 may produce solutions with distortions, see Figure 2, probably due to the use of the linear kernel. To cope with that, one can enlarge the penalty of dependence λ, and use HSIC with universal kernels or other dependence measures. Besides, we propose a banded loss to reinforce a banded residual plot, see in supplement H.\n\n4.2 DISTINGUISH CAUSE FROM EFFECT VIA INDEPENDENCE TEST\n\nFollowing the framework proposed by Zhang & Hyvärinen (2009), we distinguish cause from effect according to Algorithm 2. We first fit nonlinear models f(→), g(→) under hypothesis X → Y , and f(←), g(←) under hypothesis X ← Y . After the learning iterations, we conduct independence tests. If (cid:100)Dep (cid:0)ˆr(→), x(cid:1) < (cid:100)Dep (cid:0)ˆr(←), y(cid:1), the hypothesis X → Y is supported; otherwise, the hypothesis X ← Y is supported. We use the RDC for the independent test, as introduced in Section 3.2.\n\nAlgorithm 2 The MC-PNL method for causal direction prediction. Input: The standardized data x, y ∈ Rn. Output: The causal score CX→Y and direction.\n\n1. Fit PNL models via Algorithm 1 and estimate residuals under hypotheses, X → Y and X ← Y.\n\n• Under hypothesis X → Y : ˆr(→) = ˆg(→)(y) − ˆf(→)(x). • Under hypothesis X ← Y : ˆr(←) = ˆg(←)(x) − ˆf(←)(y). 2. Calculate the causal score CX→Y := (cid:100)Dep (cid:0)ˆr(←), y(cid:1) − (cid:100)Dep (cid:0)ˆr(→), x(cid:1) . 3. Output the causal score CX→Y and\n\ndirection :=\n\n(cid:26)X → Y, X ← Y,\n\nif CX→Y > 0, if CX→Y < 0,\n\nTowards trustworthy decisions, Liu & Chan (2016) proposed to make no decision when |CX→Y | is less than a threshold δ > 0. Besides, bootstrap (Efron, 1992; Zoubir & Boashash, 1998) can also be used for uncertainty quantification, see supplement I.\n\n5 EXPERIMENTS\n\nIn the following, we show the performance of MC-PNL in model learning and its application to bivariate causal discovery.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n5.1 NONLINEAR FUNCTION FITTING\n\nFor better demonstration, we generated two synthetic datasets from the PNL model, Y = f2 (f1(X) + (cid:15)), and each contains 1000 samples. The data generation mechanisms are as follows,\n\n• Syn-1: f1(X) = X −1 + 10X, f2(Z) = Z 3, X ∼ U (0.1, 1.1), (cid:15) ∼ U (0, 5), • Syn-2: f1(X) = sin(7X), f2(Z) = exp(Z), X ∼ U (0, 1), (cid:15) ∼ N (0, 0.32).\n\nWe apply Algorithm 1 to both datasets and show the learned nonlinear transformations as well as the corresponding residual plots in Figure 2. The underlying nonlinear functions are correctly learned under the true hypothesis but with certain distortions. We also show that, after fine-tuning with our proposed banded loss or HSIC-RBF loss, such distortion can be fixed up, see supplement H.\n\n(a) Syn-1\n\n(b) Syn-2\n\nFigure 2: The sub-figures (a) and (b) show the nonlinear function fitting of the two datasets. In each sub-figure, the top row shows the learned f(→)(x) (red line) and the residual plot under the correct hypothesis X → Y , which has lower RDC value; the bottom row is under the opposite X ← Y .\n\nConvergence Results. We demonstrate the convergence profile of our algorithm with Syn-2, see Figure 3. Results for Syn-1 can be found in the supplement J. The top row shows the snapshots of the learned representations, where we do not impose independence regularization (λ = 0). The algorithm, starting from different random initializations, convergences quickly to the local minimizers sharing the same objective value. The bottom row is with independence regularization λ = 5, where the solutions have a sign symmetry.\n\n5.2 BIVARIATE CAUSAL DISCOVERY\n\nWe evaluated the causal discovery accuracy on both synthetic and real datasets.\n\nSynthetic Datasets: The generated synthetic datasets all follow the PNL model. And we considered the following two settings: 1) PNL-A: f1 are general nonlinear functions generated by polynomials with random coefficients; and f2 are monotonic nonlinear functions generated by unconstrained monotonic neural networks (UMNN) (Wehenkel & Louppe, 2019); 2) PNL-B: Both f1 and f2 are monotonic generated by UMNN. The variances of f1, f2 are rescaled to 1. The input variable X is sampled either from Gaussian mixture (mixG) or uniform (unif) distribution, and the injected noise (cid:15) is generated from normal distributions N (0, ns2), where ns ∈ {0.2, 0.4, 0.6, 0.8, 1}. Each configuration contains 100 data pairs, and each data pair has 1000 samples.\n\nGene Data: Discovering gene-gene causal relationships is one important application. We used the data in DREAM4 competition (D4-S1,D4-S2A,D4-S2B,D4-S2C) (Marbach et al., 2009; 2010) and the scRNA-seq data (GSE57872) (Han et al., 2017), see supplement K.\n\nBaselines & Evaluation: Thanks to the implementation by Kalainathan et al. (2020), we can easily compare our proposed method with various existing algorithms. In this paper, we compared our\n\n8\n\n101Normalized X10123Normalized Y101Normalized X1.51.00.50.00.51.01.5Estimated ResidualRDC:0.40101Normalized X101234Normalized Y101Normalized X2101Estimated ResidualRDC:0.3902Normalized Y1.51.00.50.00.51.01.5Normalized X02Normalized Y2.01.51.00.50.00.51.01.5Estimated ResidualRDC:0.62024Normalized Y1.51.00.50.00.51.01.5Normalized X024Normalized Y21012Estimated ResidualRDC:0.49Under review as a conference paper at ICLR 2023\n\nFigure 3: The Algorithm 1 converges on Syn-2. We plot the snapshots of the feature transformations f at training epochs [0, 5, 10, 20], under 15 random initializations (indicated by colors). Upper: λ = 0, most initializations converge to local minimizers (symmetry: (α, β) (cid:55)→ (aα, a−1β)). Lower: λ = 5, most initializations converge to two local minimizers (symmetry: (α, β) (cid:55)→ −(α, β)).\n\nproposed algorithm on both synthetic datasets and real datasets with several baseline algorithms, including ANM (Hoyer et al., 2008), CDS (Fonollosa, 2019), IGCI (Janzing et al., 2012), RECI (Blöbaum et al., 2018), CDCI (Duong & Nguyen, 2022), OT-PNL (Tu et al., 2022), AbPNL (Uemura & Shimizu, 2020). Our implementation of MC-PNL follows Algorithm 1 (without fine-tuning), and we empirically set λ = 5 (the choice of λ is briefly discussed in supplement L). We also conducted causal discovery on the PNL learned by the ACE algorithm. The ROC-AUC score is used for the evaluation.\n\nTable 2: Comparison of bivaraite causal discovery ROC-AUC on synthetic and real datasets\n\nIGCI RECI CDCI OT-PNL AbPNL1 ACE1 MC-PNL1\n\nDataset\n\nPNL-A-mixG PNL-B-mixG PNL-A-unif PNL-B-unif\n\nD4-S1 D4-S2A D4-S2B D4-S2C GSE57872\n\nAverage time2 (s)\n\nANM1\n\n0.256 0.150 0.203 0.094\n\n0.604 0.616 0.521 0.556 0.493\n\n20.11\n\nCDS\n\n0.207 0.160 0.390 0.311\n\n0.582 0.580 0.529 0.564 0.457\n\n0.932 0.908 0.681 0.866\n\n0.380 0.447 0.450 0.441 0.599\n\n0.537 0.462 0.879 0.929\n\n0.550 0.592 0.491 0.521 0.474\n\n0.410 0.304 0.544 0.535\n\n0.651 0.673 0.614 0.590 0.478\n\n0.431 0.309 0.711 0.536\n\n0.474 0.472 0.517 0.490 -\n\n0.645 0.672 0.517 0.599\n\n0.408 0.519 0.501 0.445 -\n\n0.580 0.536 0.514 0.418\n\n0.592 0.558 0.495 0.538 0.538\n\n0.708 0.771 0.617 0.608\n\n0.646 0.626 0.519 0.576 0.499\n\n30.31\n\n7.67\n\n0.50\n\n0.27\n\n0.26\n\n∼ 7220\n\n∼ 9300\n\n21.68\n\n1 Independence test-based methods. 2 Average running time evaluated on synthetic data containing 100 pairs, and each pair has 1000 samples.\n\nWe report the comparison of ROC-AUCs in Table 2. The results are averaged over five different noise scales for the synthetic datasets. Our proposed MC-PNL consistently outperforms other independence test-based methods on the synthetic PNL data. Especially compared with AbPNL, our MC-PNL is not sensitive to the initializations and is much more efficient (w.r.t. training time); compared with ACE (without independence regularizer), MC-PNL has better causal discovery accuracy. And for real datasets, our methods is quite competitive.\n\n6 CONCLUSIONS\n\nIn this paper, we focus on the PNL model learning and propose a maximal correlation-based method, which can recover the nonlinear transformations accurately and swiftly in an iterative manner. The key is to incorporate with maximal correlation to avoid learning arbitrary independent noise, and the proposed MC-PNL is more reliable than previous methods that are solely based on the independence loss. Besides the PNL model learning, we conduct experiments on the downstream causal discovery task where MC-PNL is superior to the SOTA independence test-based methods.\n\n9\n\n1.51.00.50.00.51.01.5X200002000400060008000f(X)epoch=01.51.00.50.00.51.01.5X0.20.00.20.40.6f(X)epoch=51.51.00.50.00.51.01.5X0.40.20.00.20.4f(X)epoch=101.51.00.50.00.51.01.5X0.40.20.00.20.4f(X)epoch=201.51.00.50.00.51.01.5X200002000400060008000f(X)epoch=01.51.00.50.00.51.01.5X2.01.51.00.50.00.51.01.52.0f(X)epoch=51.51.00.50.00.51.01.5X2.01.51.00.50.00.51.01.52.0f(X)epoch=101.51.00.50.00.51.01.5X2.01.51.00.50.00.51.01.52.0f(X)epoch=20Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAdrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for self-supervised learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=xm6YD62D1Ub.\n\nYoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan Rosemary Ke, Sebastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle In International Conference on Learning Representations, 2020. URL causal mechanisms. https://openreview.net/forum?id=ryxWIgBFPS.\n\nPatrick Blöbaum, Dominik Janzing, Takashi Washio, Shohei Shimizu, and Bernhard Schölkopf. Cause-effect inference by comparing regression errors. In International Conference on Artificial Intelligence and Statistics, pp. 900–909. PMLR, 2018.\n\nLeo Breiman and Jerome H Friedman. Estimating optimal transformations for multiple regression\n\nand correlation. Journal of the American statistical Association, 80(391):580–598, 1985.\n\nDavid Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine\n\nlearning research, 3(Nov):507–554, 2002.\n\nJunsouk Choi, Robert Chapkin, and Yang Ni. Bayesian causal structural learning with zero-inflated poisson bayesian networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 5887–5897. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 4175a4b46a45813fccf4bd34c779d817-Paper.pdf.\n\nDiego Colombo, Marloes H Maathuis, Markus Kalisch, and Thomas S Richardson. Learning highdimensional directed acyclic graphs with latent and selection variables. The Annals of Statistics, pp. 294–321, 2012.\n\nPovilas Daniušis, Dominik Janzing, Joris Mooij, Jakob Zscheischler, Bastian Steudel, Kun Zhang, and Bernhard Schölkopf. Inferring deterministic causal relations. In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, pp. 143–150, 2010.\n\nBao Duong and Thin Nguyen. Bivariate causal discovery via conditional divergence.\n\nIn First Conference on Causal Learning and Reasoning, 2022. URL https://openreview.net/ forum?id=8X6cWIvY_2v.\n\nImme Ebert-Uphoff and Yi Deng. Causal discovery for climate research using graphical models.\n\nJournal of Climate, 25(17):5648–5665, 2012.\n\nBradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics, pp.\n\n569–593. Springer, 1992.\n\nJosé AR Fonollosa. Conditional distribution variability measures for causality detection. In Cause\n\nEffect Pairs in Machine Learning, pp. 339–347. Springer, 2019.\n\nRonja Foraita, Juliane Friemel, Kathrin Günther, Thomas Behrens, Jörn Bullerdiek, Rolf Nimzyk, Wolfgang Ahrens, and Vanessa Didelez. Causal discovery of gene regulation with incomplete data. Journal of the Royal Statistical Society: Series A (Statistics in Society), 183(4):1747–1775, 2020.\n\nKenji Fukumizu, Arthur Gretton, Xiaohai Sun, and Bernhard Schölkopf.\n\nKernel measures of conditional dependence. In J. Platt, D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper/2007/file/ 3a0772443a0739141292a5429b952fe6-Paper.pdf.\n\nHans Gebelein. Das statistische problem der korrelation als variations-und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung. ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik, 21(6):364–379, 1941.\n\nClive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.\n\nEconometrica: journal of the Econometric Society, pp. 424–438, 1969.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDaniel Greenfeld and Uri Shalit. Robust learning with the Hilbert-Schmidt independence criterion.\n\nIn International Conference on Machine Learning, pp. 3759–3768. PMLR, 2020.\n\nArthur Gretton, Ralf Herbrich, Alexander Smola, Olivier Bousquet, Bernhard Schölkopf, et al. Kernel\n\nmethods for measuring independence. 2005.\n\nArthur Gretton, Kenji Fukumizu, Choon Teo, Le Song, Bernhard Schölkopf, and Alex Smola. A kernel statistical test of independence. In J. Platt, D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper/2007/file/ d5cfead94f5350c12c322b5b664544c1-Paper.pdf.\n\nArthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A\n\nkernel two-sample test. The Journal of Machine Learning Research, 13(1):723–773, 2012.\n\nL. Grippo and Marco Sciandrone. On the convergence of the block nonlinear Gauss-Seidel method under convex constraints. Oper. Res. Lett., 26(3):127–136, 2000. URL http: //dblp.uni-trier.de/db/journals/orl/orl26.html#GrippoS00.\n\nHeonjong Han, Jae-Won Cho, Sangyoung Lee, Ayoung Yun, Hyojin Kim, Dasom Bae, Sunmo Yang, Chan Yeong Kim, Muyoung Lee, Eunbeen Kim, Sungho Lee, Byunghee Kang, Dabin Jeong, Yaeji Kim, Hyeon-Nae Jeon, Haein Jung, Sunhwee Nam, Michael Chung, Jong-Hoon Kim, and Insuk Lee. TRRUST v2: an expanded reference database of human and mouse transcriptional regulatory interactions. Nucleic Acids Research, 46(D1):D380–D386, October 2017. doi: 10.1093/ nar/gkx1013. URL https://doi.org/10.1093/nar/gkx1013.\n\nHermann O Hirschfeld. A connection between correlation and contingency. In Mathematical Proceedings of the Cambridge Philosophical Society, volume 31, pp. 520–524. Cambridge University Press, 1935.\n\nPatrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear causal discovery with additive noise models. Advances in neural information processing systems, 21, 2008.\n\nDominik Janzing, Patrik O. Hoyer, and Bernhard Schölkopf. Telling cause from effect based on high-dimensional observations. In Johannes Fürnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 479–486, Haifa, Israel, June 2010. Omnipress. URL http://www.icml2010.org/papers/576.pdf.\n\nDominik Janzing, Joris Mooij, Kun Zhang, Jan Lemeire, Jakob Zscheischler, Povilas Daniušis, Bastian Steudel, and Bernhard Schölkopf. Information-geometric approach to inferring causal directions. Artificial Intelligence, 182:1–31, 2012.\n\nDiviyan Kalainathan, Olivier Goudet, and Ritik Dutta. Causal discovery toolbox: Uncovering causal relationships in python. Journal of Machine Learning Research, 21(37):1–5, 2020. URL http://jmlr.org/papers/v21/19-187.html.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),\n\n2015. URL http://arxiv.org/abs/1412.6980.\n\nJoshua Ka-Wing Lee. Maximal Correlation Feature Selection and Suppression With Applications.\n\nPhD thesis, Massachusetts Institute of Technology, 2021.\n\nFurui Liu and Laiwan Chan. Causal inference on discrete data via estimating distance correlations.\n\nNeural computation, 28(5):801–814, 2016.\n\nDavid Lopez-Paz, Philipp Hennig, and Bernhard Schölkopf. The randomized dependence coefficient.\n\nAdvances in neural information processing systems, 26, 2013.\n\nDavid Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Scholkopf, and Léon Bottou. Discovering causal signals in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6979–6987, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDavid López-Paz, Philipp Hennig, and Bernhard Schölkopf. The randomized dependence coefficient. In Christopher J. C. Burges, Léon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger (eds.), NIPS, pp. 1–9, 2013. URL http://dblp.uni-trier.de/db/conf/nips/nips2013. html#Lopez-PazHS13.\n\nWan-Duo Kurt Ma, JP Lewis, and W Bastiaan Kleijn. The HSIC bottleneck: Deep learning without back-propagation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 5085–5092, 2020.\n\nDaniel Marbach, Thomas Schaffter, Claudio Mattiussi, and Dario Floreano. Generating realistic in silico gene networks for performance assessment of reverse engineering methods. J. Comput. Biol., 16(2):229–239, February 2009.\n\nDaniel Marbach, Robert J Prill, Thomas Schaffter, Claudio Mattiussi, Dario Floreano, and Gustavo Stolovitzky. Revealing strengths and weaknesses of methods for gene network inference. Proc. Natl. Acad. Sci. U. S. A., 107(14):6286–6291, April 2010.\n\nJoris Mooij, Dominik Janzing, Jonas Peters, and Bernhard Schölkopf. Regression by dependence minimization and its application to causal inference in additive noise models. In Proceedings of the 26th annual international conference on machine learning, pp. 745–752, 2009.\n\nJoris M Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, and Bernhard Schölkopf. Distinguishing cause from effect using observational data: methods and benchmarks. The Journal of Machine Learning Research, 17(1):1103–1204, 2016.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nAnoop P. Patel, Itay Tirosh, John J. Trombetta, Alex K. Shalek, Shawn M. Gillespie, Hiroaki Wakimoto, Daniel P. Cahill, Brian V. Nahed, William T. Curry, Robert L. Martuza, David N. Louis, Orit Rozenblatt-Rosen, Mario L. Suvà, Aviv Regev, and Bradley E. Bernstein. Single-cell RNA-seq highlights intratumoral heterogeneity in primary glioblastoma. Science, 344(6190):1396–1401, 2014. doi: 10.1126/science.1254257. URL https://www.science.org/doi/abs/10. 1126/science.1254257.\n\nJudea Pearl.\n\nCausality.\n\nCambridge University Press, September 2009.\n\ndoi: 10.1017/\n\ncbo9780511803161. URL https://doi.org/10.1017/cbo9780511803161.\n\nJonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations\n\nand learning algorithms. The MIT Press, 2017.\n\nAli Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimizaIn D. Koller, D. Schuurmans, Y. Bengio, and L. Bottion with randomization in learning. tou (eds.), Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/paper/2008/file/ 0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf.\n\nA. Rényi. On measures of dependence. Acta Mathematica Academiae Scientiarum Hungaricae, 10(3-4):441–451, September 1959. doi: 10.1007/bf02024507. URL https://doi.org/10. 1007/bf02024507.\n\nJakob Runge, Sebastian Bathiany, Erik Bollt, Gustau Camps-Valls, Dim Coumou, Ethan Deyle, Clark Glymour, Marlene Kretschmer, Miguel D. Mahecha, Jordi Muñoz-Marí, Egbert H. van Nes, Jonas Peters, Rick Quax, Markus Reichstein, Marten Scheffer, Bernhard Schölkopf, Peter Spirtes, George Sugihara, Jie Sun, Kun Zhang, and Jakob Zscheischler. Inferring causation from time series in earth system sciences. Nature Communications, 10(1), June 2019. doi: 10.1038/s41467-019-10105-3. URL https://doi.org/10.1038/s41467-019-10105-3.\n\nXinpeng Shen, , Sisi Ma, Prashanthi Vemuri, and Gyorgy Simon. Challenges and opportunities with causal discovery algorithms: Application to Alzheimer’s pathophysiology. Scientific Reports, 10 (1), February 2020. doi: 10.1038/s41598-020-59669-x. URL https://doi.org/10.1038/ s41598-020-59669-x.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nShohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, Antti Kerminen, and Michael Jordan. A linear non-Gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006.\n\nPeter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction,\n\nand search. MIT press, 2000.\n\nGeorge Sugihara, Robert May, Hao Ye, Chih-hao Hsieh, Ethan Deyle, Michael Fogarty, and Stephan\n\nMunch. Detecting causality in complex ecosystems. science, 338(6106):496–500, 2012.\n\nMasashi Sugiyama and Makoto Yamada. On kernel parameter selection in Hilbert-Schmidt independence criterion. IEICE TRANSACTIONS on Information and Systems, 95(10):2564–2567, 2012.\n\nTaiji Suzuki, Masashi Sugiyama, Takafumi Kanamori, and Jun Sese. Mutual information estimation reveals global associations between stimuli and biological processes. BMC Bioinform., 10 (S-1), 2009. URL http://dblp.uni-trier.de/db/journals/bmcbi/bmcbi10S. html#SuzukiSKS09.\n\nSergios Theodoridis. Machine learning: a Bayesian and optimization perspective. Academic press,\n\n2015.\n\nRuibo Tu, Kun Zhang, Hedvig Kjellstrom, and Cheng Zhang. Optimal transport for causal discovery. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=qwBK94cP1y.\n\nKento Uemura and Shohei Shimizu. Estimation of post-nonlinear causal models using autoencoding structure. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3312–3316, 2020. doi: 10.1109/ICASSP40776.2020.9053468.\n\nLichen Wang, Jiaxiang Wu, Shao-Lun Huang, Lizhong Zheng, Xiangxiang Xu, Lin Zhang, and Junzhou Huang. An efficient approach to informative feature extraction from multimodal data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 5281–5288, 2019.\n\nAntoine Wehenkel and Gilles Louppe. Unconstrained monotonic neural networks. In Advances in\n\nNeural Information Processing Systems, pp. 1543–1553, 2019.\n\nJohn Wright and Yi Ma. High-Dimensional Data Analysis with Low-Dimensional Models: Principles,\n\nComputation, and Applications. Cambridge University Press, 2022.\n\nYangyang Xu and Wotao Yin. A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion. SIAM Journal on imaging sciences, 6(3):1758–1789, 2013.\n\nHao Ye, Ethan R Deyle, Luis J Gilarranz, and George Sugihara. Distinguishing time-delayed causal\n\ninteractions using convergent cross mapping. Scientific reports, 5(1):1–9, 2015.\n\nChulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/ file/dbea3d0e2a17c170c412c74273778159-Paper.pdf.\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.\n\nK Zhang and A Hyvärinen. On the identifiability of the post-nonlinear causal model.\n\nIn 25th Conference on Uncertainty in Artificial Intelligence (UAI 2009), pp. 647–655. AUAI Press, 2009.\n\nKun Zhang and Aapo Hyvärinen. Distinguishing causes from effects using nonlinear acyclic causal models. In Isabelle Guyon, Dominik Janzing, and Bernhard Schölkopf (eds.), Proceedings of Workshop on Causality: Objectives and Assessment at NIPS 2008, volume 6 of Proceedings of Machine Learning Research, pp. 157–164, Whistler, Canada, 12 Dec 2010. PMLR. URL https://proceedings.mlr.press/v6/zhang10a.html.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nKun Zhang, Zhikun Wang, Jiji Zhang, and Bernhard Schölkopf. On estimation of functional causal models: general results and application to the post-nonlinear causal model. ACM Transactions on Intelligent Systems and Technology (TIST), 7(2):1–22, 2015.\n\nXun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. DAGs with NO TEARS: Continuous Optimization for Structure Learning. In Advances in Neural Information Processing Systems, 2018.\n\nAbdelhak M Zoubir and Boualem Boashash. The bootstrap and its application in signal processing.\n\nIEEE signal processing magazine, 15(1):56–76, 1998.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nSupplementary Document Maximal Correlation-Based Post-Nonlinear Learning for Bivariate Causal Discovery\n\nA DISCUSSION ON MI MINIMIZATION\n\nd\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (Zhang & Hyvärinen, 2009), where p is the assumed noise density. We find dy g(→) (y) (cid:1), can be understood as the data fitting\n\nIt was shown that minimizing the MI in (5) is equivalent to maximizing E log p (cid:0)r(→) E log this objective interpretable, since the first term, E log p (cid:0)r(→) (cid:12) term, and the second term, E log (cid:12) dy g(→) (y) (cid:12), can be understood from an information-geometric perspective (Daniušis et al., 2010). However, such equivalent form requires a known noise distribution to calculate the log-likelihood. Some works (Ma et al., 2020; Uemura & Shimizu, 2020) have been proposed to avoid this difficulty by using HSIC instead of MI.\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:1) +\n\nd\n\nB EXPERIMENTS ON MINIMIZING HSIC\n\nIn this section, we show the PNL model learning result by minimizing (12). We generated two synthetic datasets from PNL model, Y = f2 (f1(X) + (cid:15)), and each contains 1000 data samples. The data generation mechanisms are as follows (see Figure 4),\n\n• Syn-1: f1(X) = X −1 + 10X, f2(Z) = Z 3, X ∼ U (0.1, 1.1), (cid:15) ∼ U (0, 5), • Syn-2: f1(X) = sin(7X), f2(Z) = exp(Z), X ∼ U (0, 1), (cid:15) ∼ N (0, 0.32).\n\nf ∗\n\ng∗\n\nscatter plot\n\nFigure 4: The ground truth transformations of f ∗ and g∗ of Syn-1 (top) and Syn-2 (bottom).\n\nWe build different MLPs with the following configurations.\n\n• Narrow deep MLP: the input and output are both one-dimensional; there are 9 hidden layers,\n\neach with 5 neurons. The activation function is Leaky-ReLU.\n\n• Wide over-parameterized MLP: the input and output are both one-dimensional; there is only\n\none single hidden layer with 9000 neurons. The activation function is Leaky-ReLU.\n\nWe use the default initialization method in PyTorch (Paszke et al., 2019), and make sure the exact same initial weights for narrow/wide MLPs are used (i.e., the initializations for different datasets are the same).\n\nOptimization Setup: We set the batch size to be 32. We use Adam (Kingma & Ba, 2015) for the optimization (the learning rates are 10−3 and 10−6 for narrow deep and wide over-parameterized MLPs, respectively, while all other parameters are set by default).\n\nWe report the learning results in Figure 5. The learned transformations (see row 3 and row 4 in Figure 5) deviates far away from the underlying functions, and are quite similar across datasets. The\n\n15\n\n0.20.40.60.81.0X789101112f*(X)1000200030004000Y6810121416g*(Y)1.51.00.50.00.51.01.5X10123Y0.00.20.40.60.81.0X1.000.750.500.250.000.250.500.751.00f*(X)012345Y1.51.00.50.00.51.01.5g*(Y)1.51.00.50.00.51.01.5X101234YUnder review as a conference paper at ICLR 2023\n\nSyn-1, narrow MLPs\n\nSyn-1, wide MLPs\n\nSyn-2, narrow MLPs\n\nSyn-2, wide MLPs\n\nFigure 5: Visualization of the learned nonlinearities (trained solely with HSIC, under different datasets and MLP configurations). From top to bottom, the convergence results, residual plot, learned f , learned g, are plotted. Each column shows one specific configuration. None of them learns meaningful nonlinearities, and the learned transformations are quite similar across datasets.\n\npossible reason is that, the solutions were started from the same initialization and trapped at the local minima near the initializations.\n\nTo verify whether such HSIC-based PNL learning algorithm is stable for causal discovery, we further evaluate the AbPNL on the following dataset. We build 100 data pairs with different random seeds, following the same mechanism, Syn-1, and each contains 1000 data samples. And we applied the AbPNL (Uemura & Shimizu, 2020) with different initializations on each of those data pairs. The results in Table 3 show that the causal discovery stableness for AbPNL is not satisfactory.\n\nTable 3: Comparison of bivaraite causal discovery AUC on 100 realizations of Syn-1\n\nDataset ANM CDS\n\nIGCI RECI CDCI AbPNL ACE MC-PNL\n\nSyn-1\n\n0.495\n\n1\n\n0.528\n\n1\n\n1\n\n0.281\n\n1\n\n1\n\nC SYNTHETIC DATASETS FOR INDEPENDENCE TEST\n\nIn this section, we describe the synthetic data generation from PNL model for the independent test. The data were generated from the following model, Y = f2 (f1(X) + (cid:15)) , X ∼ GMM, (cid:15) ∼ N (0, σ2 (cid:15) ), where f1, f2 are randomly initialized monotonic neural networks (Wehenkel & Louppe, 2019) with 3 layers and 100 integration steps, and each layer contains 100 units. The cause term X is sampled from a Gaussian mixture model as described in Lopez-Paz et al. (2017). The datasets were configured with various noise levels and sample sizes. There are three different injected noise levels, σ(cid:15) ∈ {0.1, 1, 10}, and three different sample sizes, N ∈ {1000, 2000, 5000}. And under each configuration, we generated 100 data pairs for evaluating the independence test accuracy.\n\n16\n\n051015202530epoch0.00.20.40.60.81.01.21.41.6HSIC score1e9Narrow Deep MLPs051015202530epoch012345678HSIC score1e4Wide Over-Parameterized MLPs051015202530epoch0.00.20.40.60.81.01.2HSIC score1e9Narrow Deep MLPs051015202530epoch012345HSIC score1e4Wide Over-Parameterized MLPs1.51.00.50.00.51.01.5X1.82.02.22.42.62.83.03.23.4Res1e5+3.814e11.51.00.50.00.51.01.5X1.351.401.451.501.551.601.65Res1e11.51.00.50.00.51.01.5X4.504.755.005.255.505.756.00Res1e5+3.806e11.51.00.50.00.51.01.5X1.31.41.51.61.7Res1e11.51.00.50.00.51.01.5X8.88.68.48.28.07.87.6f(X)1e52.061e11.51.00.50.00.51.01.5X8.07.57.06.56.05.5f(X)1e21.51.00.50.00.51.01.5X5.04.84.64.44.24.0f(X)1e52.06e11.51.00.50.00.51.01.5X9.59.08.58.07.5f(X)1e210123Y2345678g(Y)1e6+1.7524e110123Y7.507.758.008.258.508.759.00g(Y)1e2101234Y5678910g(Y)1e6+1.746e1101234Y5.05.56.06.57.07.58.0g(Y)1e2Under review as a conference paper at ICLR 2023\n\nD A UNIVERSAL VIEW OF DEPENDENCE MEASURES\n\nActually the discussed dependence measures in Section 3.2 are all closely related to the mean squared contingency introduced by (Rényi, 1959) and rediscovered due to its squared version called squared-loss mutual information (SMI) (Suzuki et al., 2009),\n\n(cid:90) (cid:90)\n\nSMI :=\n\np(x)p(y)\n\n(cid:18) p(x, y) p(x)p(y)\n\n(cid:19)2\n\n− 1\n\ndxdy =\n\n(cid:90) (cid:90)\n\np(x, y) p(x)p(y)\n\np(x, y)dxdy − 1.\n\n(16)\n\nWhen the density ratio DR(x, y) := p(x,y) p(x)p(y) is constant 1 (namely X and Y are independent), the SMI should be zero. To estimate the SMI, one can first approximate DR(x, y) by a surrogate function DRθ(x, y) parameterized by θ. The optimal parameter ˆθ can be obtained via minimizing the following squared-error loss J DR,\n\n(cid:90) (cid:90)\n\nJ DR(θ) :=\n\n(DRθ(x, y) − DR(x, y))2 p(x)p(y)dxdy\n\n(cid:90) (cid:90)\n\n=\n\nDRθ(x, y)2p(x)p(y)dxdy − 2\n\n(cid:90) (cid:90)\n\nDRθ(x, y)p(x, y)dxdy + Const.\n\n(17)\n\nThen the empirical SMI can be calculated as, (cid:100)SMI = 1\n\nn\n\n(cid:80)n\n\nj=1 DRˆθ(xj, yj) − 1.\n\nWe show that, with different parameterizations of the density ratio, the resulting SMI will be equivalent to different dependence measures, see Table 4.\n\nTable 4: Connections between DR parameterization and dependence measure\n\nDensity ratio surrogate function DRθ(x, y) DRθ(x, y) = 1 + (cid:80)n DRθ(x, y) = 1 + (cid:80)n\n\ni=1 θiK (x, xi) L (y, yi) n K (x, xi) L (y, yi) i=1 fi(x)gi(y)\n\nDRθ(x, y) = 1 + (cid:80)m\n\ni=1\n\n1\n\nDRθ(x, y) = 1 + f (x)g(y) 1\n\nCorresponding dependence measure\n\nvariant of LSMI (Sugiyama & Yamada, 2012) HSIC (Gretton et al., 2005)\n\nm-mode HGR correlation (Wang et al., 2019) HGR correlation (Rényi, 1959)\n\n1 When f, g are the linear combinations of random features, f (x) = αT φ(x), g(y) = βT ψ(y), the corresponding dependence measure will be RDC (López-Paz et al., 2013),\n\n1 n\n\ni=1\n\ni=1\n\nj=1\n\n(cid:80)n\n\n(cid:80)n\n\nSugiyama & Yamada (2012) proposed to approximate the density ratio by DRˆθ(x, y) = (cid:80)n ˆθiK (x, xi) L (y, yi), where ˆθ has a closed-form solution via minimizing (17). After then, they approximated the SMI using the empirical average of Equation (16), 1 j=1 DRˆθ(xj, yj) − 1 = ˆθiK (x, xi) L (y, yi) − 1. It is shown that, the first term is actually the empirical HSIC, when {ˆθi}n n . We argue that there is a flaw above, as when X and Y are independent, both the SMI and HSIC score should be zero. A simple modification is to model the density ratio by DRθ(x, y) = 1 + (cid:80)n i=1 θiK (x, xi) L (y, yi). The constant 1 here is to exclude all the independence terms, and the rest ones should model the dependency only. This modification will not hurt the quadratic form of J DR(θ), and maintains good interpretation. The SMI reduced to HSIC score, when {θi}n\n\ni=1 = 1\n\n(cid:80)n\n\nn\n\ni=1 = 1 n ,\n\nWe extend this idea to approximate the density ratio by DRθ(x, y) = 1 + f (x)g(y), where f, g are zero mean and unit variance functions parameterized by θ, the resulting SMI will be equal to the HGR maximal correlation. Similarly, the constant 1 will capture the independence part, and f (x)g(y) will capture the dependencies.\n\nProposition 1. The density ratio estimation problem (17) is equivalent to the maximal HGR correlation problem (7), when the density ratio is modeled in the form of DRθ(x, y) = 1 + f (x)g(y), and f, g are restricted to zero mean and unit variance functions.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nProof. We substitute DRˆθ(x, y) into Equation (17),\n\nJ DR(f, g) =\n\n(cid:90) (cid:90)\n\n(1 + f (x)g(y))2p(x)p(y)dxdy − 2\n\n(cid:90) (cid:90)\n\n(1 + f (x)g(y))p(x, y)dxdy + Const.\n\n= 1 + 2E(f (X))E(g(Y )) + var(f (X))var(g(Y )) − 2 − 2E(f (X)g(Y )) + Const.\n\nThen it is not hard to see, minf,g J DR(f, g), subject to E(f ) = E(g) = 0, var(f ) = var(g) = 1, is equivalent to the maximal HGR correlation problem (7) .\n\nProposition 2. The density ratio estimation problem (17) is equivalent to the Soft-HGR problem (9), when the density ratio is modeled in the form of DRθ(x, y) = 1 + f (x)g(y), and f, g are restricted to zero mean functions.\n\nWe further note that the above density ratio estimation can be regard as a truncated singular value decomposition DRˆθ(x, y) = 1+(cid:80)m i=1 fi(x)gi(y), where m = 1. When letting m > 1 and imposing zero mean and unit variance constraints on all fi and gi, the corresponding J DR minimization problem is equivalent to solving the m-mode HGR maximal correlation (Wang et al., 2019; Lee, 2021).\n\nDefinition 3 (m-mode HGR maximal correlation). Given 1 ≤ m ≤ min{|X |, |Y|}, the m-mode maximal correlation problem for random variables X ∈ X , Y ∈ Y is,\n\n(f ∗, g∗) (cid:44)\n\narg max f :X →Rm,g:Y→Rm E[f (X)]=E[g(Y )]=0, E[f (X)f T(X)]=E[g(Y )gT(Y )]=I\n\nE (cid:2)f T(X)g(Y )(cid:3) ,\n\n(18)\n\nwhere f = [f1, f2, . . . , fm]T , g = [g1, g2, . . . , gm]T are referred as the maximal correlation functions.\n\nE CONNECTIONS AMONG MI, ML, AND MC\n\nIn this section, we build connections among minimizing MI, maximum likelihood, and maximal correlation. The equivalence between minimizing MI and maximizing likelihood was built in Zhang & Hyvärinen (2009). The following proposition shows the connection to maximal correlation. Proposition 3. Suppose the dataset {(xi, yi)}n i=1 is generated from a PNL model Y = g−1(f (X) + (cid:15)), where f, g are both invertible functions, and the noise (cid:15) follows a Gaussian density p((cid:15); θ) with zero mean and variance θ, then maximizing the log-likelihood log p({(xi, yi)}n i=1) is equivalent to solving the regression problem (8).\n\nProof. Under proper assumptions in proposition 3, the log-likelihood can be written as follows,\n\nLn(f, g) =\n\n=\n\n=\n\n=\n\n=\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\ni=1 n\n(cid:88)\n\ni=1 n\n(cid:88)\n\ni=1 n\n(cid:88)\n\ni=1\n\nlog p(xi, yi; f, g, θ)\n\nlog p(xi) +\n\nlog p(xi) +\n\nlog p(xi) +\n\nlog p(xi) +\n\nn (cid:88)\n\ni=1 n\n(cid:88)\n\ni=1 n\n(cid:88)\n\ni=1 n\n(cid:88)\n\ni=1\n\nlog p(yi|xi; f, g, θ)\n\nlog p(g(yi)|f (xi); θ)\n\n(f, g are invertible)\n\nlog p(g(yi) − f (xi); θ)\n\n(from PNL model)\n\n−(g(yi) − f (xi))2 2θ\n\n+ n log\n\n√\n\n1\n\n2πθ\n\n(Gaussianity)\n\n(19)\n\nIt is not hard to see, with fixed θ, maximizing the log-likelihood Ln(f, g) is equivalent to minimizing (cid:107)f (x) − g(y)(cid:107)2 with invertible f and g. Without loss of generality, one can make f, g zero mean. To avoid trivial solutions, one can further restrict g to have unit-variance. Then the equivalence to the regression problem (8) is build.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nCorollary 1. When n → ∞, the ground truth transformations f ∗, g∗, minimize the MI(x, ˆr) to zero, achieve optimum of (8), and maximize the log-likelihood Ln(f ∗, g∗).\n\nProof. The proof is directly follows Theorem 3 in (Zhang et al., 2015).\n\nThe reformulating to (8) or (9)1 allows efficient BCD-like optimization algorithms to be exploited.\n\nF RANDOM FEATURE GENERATION\n\nWe generate the random features as described in López-Paz et al. (2013). The generation process has the following two steps: copula transformation (optional) and random nonlinear projection.\n\nStep 1. Copula transformation. We first estimate the empirical cumulative distribution of both X and Y by,\n\nP X\n\nn (x) :=\n\n1 n\n\nn (cid:88)\n\ni=1\n\nI (xi ≤ x) , P Y\n\nn (y) :=\n\n1 n\n\nn (cid:88)\n\ni=1\n\nI (yi ≤ y) .\n\ni = P Y\n\nThen we can apply the empirical copula transformation to data samples {(xi, yi)}n and uY\n\nn (yi), where the marginals U X and U Y follow uniform distribution U (0, 1). Step 2. Random nonlinear projection. We design a k-dimensional random feature vector φ(x) = [sin(w1x + b1), · · · , sin(wkx + bk)]T , where wi, bi ∼ N (0, s2). The random feature matrix Φ ∈ Rk×n is stacked as,\n\ni = P X\n\ni=1, uX\n\nn (xi)\n\nΦ(x; k, s) :=\n\n\n\n \n\nsin (w1x1 + b1) ... sin (wkx1 + bk)\n\n· · ·\n\n...\n\n· · ·\n\nsin (w1xn + b1) ... sin (wkxn + bk)\n\n\n\n  .\n\nOne can replace the xi here by uX from the first step to form the random feature matrix. Similar procedures can be applied to y as well to generate Ψ. The number of random Fourier features k is user-defined, which is typically chosen from a few tens to a few thousands (Rahimi & Recht, 2008; Theodoridis, 2015). In our experiments, we set k = 30 and s = 2.\n\ni\n\nG ON THE OPTIMIZATION OF PROBLEM (14)\n\nG.1 SUBPROBLEM: EQUALITY CONSTRAINED QUADRATIC PROGRAMMING\n\nTo simplify the notation, we rewrite the sub-problem into the following form,\n\nmin x∈Rn s.t.\n\nf (x) := 1\n\n2 xT Ax − bT x,\n\nvT x = c.\n\n(20)\n\nWith the KKT conditions, one can find the unique optimal solution x∗ by solving the following linear system,\n\n(cid:18) A v vT 0\n\n(cid:19)\n\n(cid:18) x∗ λ∗\n\n(cid:19)\n\n(cid:18) b c\n\n=\n\n(cid:19)\n\n,\n\n(21)\n\n(cid:124)\n\n(cid:123)(cid:122) =:KKT\n\n(cid:125)\n\nwhen the KKT matrix is non-singular. In our setting, we can choose Φ and Ψ properly to make ΦΦT and ΨΨT positive definite, or add a small positive definite perturbation matrix (cid:15)I, such that the unique optimum would be obtained. Besides, the sub-problem is of smaller size and easy to solve.\n\n1We note that the optimal solution of (8) is also one solution of (9).\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nG.2 LANDSCAPE STUDY WITH HESSIAN\n\nTo simplify the notation, we rewrite\n\nJ(α, β; A, B, C, D, E) = αT AαβT Bβ − αT Cβ + αT Dα + βT Eβ,\n\n(22)\n\nwhere,\n\n2n2 ΦΦT ,\n\nA = 1 B = ΨΨT , C = 1 D = λ E = λ\n\nn ΦΨT + λ (n−1)2 ΦHKxxHΦT , (n−1)2 ΨHKxxHΨT .\n\n(n−1)2 ΦHKxxHΨT ,\n\nAnd the corresponding Hessian is\n\n∇2J(α, β) =\n\n(cid:18) 2AβT Bβ + 2D BT βαT A − C T\n\nAαβT B − C 2BαT Aα + 2E\n\n(cid:19)\n\n.\n\n(23)\n\n(24)\n\nNow we are able to verify the property of the critical points via checking their Hessians numerically.\n\nOne obvious critical point is the all zero vector 0. From our experiments, the Hessian at 0 is mostly indefinite, as long as the convex regularization term λ is not too huge, which means 0 is a saddle point. In practice, the algorithm rarely converges to 0.\n\nH FINE-TUNE WITH BANDED LOSS / UNIVERSAL HSIC\n\nIn the PNL model, the injected noise are assumed to be independently and identically distributed. Thus, the residual plot should forms a \"horizontal band\". We design a banded residual loss to fine-tune the models as follows. The data samples are separated into b bins {x(i), y(i)}b i=1 according to the ordering of X, and we expect the residuals in those bins Resi = f (x(i)) − g(y(i)) to have the same distribution, see Figure 6. To this end, we adopt the empirical maximum mean discrepancy (MMD) (Gretton et al., 2012) as a measure of distribution discrepancy. The banded residual loss (cid:92)MMD(Resi, Resall), where Resall = f (x) − g(y). Then we is defined as band(MMD) := (cid:80)b append this μ-penalized banded loss to Problem (14) as,\n\ni=1\n\nmin α,β\n\nJ(α, β) + μ · band(MMD),\n\ns.t. αT Φ1 = βT Ψ1 = 0.\n\n(25)\n\nFigure 6: The construction of banded residual loss.\n\nThe above banded residual loss involves MMD, which is highly non-convex and brings difficulties to the optimization. We used the projected gradient descent with momentum to optimize the loss function. The residual plot shows a band shape, see top row in Figure 7.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Syn-1\n\n(b) Syn-2\n\nFigure 7: Fine-tuning with the banded residual loss.\n\nWe also show the results of fine-tuning by enlarging the penalty (to λ = 10000) HSIC term with universal Gaussian RBF kernel in Figure 8.\n\nDefinition 4 (Universal Kernel (Gretton et al., 2005)). A continuous kernel k(·, ·) on a compact metric space (X , d) is called universal if and only if the RKHS F induced by the kernel is dense in C(X ), the space of continuous functions on X , with respect to the infinity norm (cid:107)f − g(cid:107)∞.\n\n(a) Syn-1\n\n(b) Syn-2\n\nFigure 8: Fine-tuning with the HSIC-RBF loss.\n\nI BOOTSTRAP FOR TRUSTWORTHY CAUSAL DISCOVERY\n\nBootstrap is a commonly used technique to estimate the confidence interval. In this section, we show a few examples of bootstrap with Tuebingen data (Mooij et al., 2016). We obtained 30 estimates of RDC from the data re-sampled with replacement, see Figure 9. The blue bars indicate the RDC distribution under the true causal direction; The orange bars indicate the RDC distribution under the false causal direction.\n\n21\n\n101Normalized X10123Normalized Y101Normalized X1.00.50.00.51.0Estimated ResidualRDC:0.11101Normalized X101234Normalized Y101Normalized X1.00.50.00.51.01.5Estimated ResidualRDC:0.1502Normalized Y1.51.00.50.00.51.01.5Normalized X02Normalized Y2.01.51.00.50.00.5Estimated ResidualRDC:0.29024Normalized Y1.51.00.50.00.51.01.5Normalized X024Normalized Y1.51.00.50.00.51.01.52.0Estimated ResidualRDC:0.40101Normalized X10123Normalized Y101Normalized X1.51.00.50.00.51.01.5Estimated ResidualRDC:0.11101Normalized X101234Normalized Y101Normalized X1.51.00.50.00.51.01.5Estimated ResidualRDC:0.1102Normalized Y1.51.00.50.00.51.01.5Normalized X02Normalized Y21012Estimated ResidualRDC:0.59024Normalized Y2.01.51.00.50.00.51.01.5Normalized X024Normalized Y2101234Estimated ResidualRDC:0.40Under review as a conference paper at ICLR 2023\n\nFigure 9: Bootstrap results of MC-PNL on eight Tuebingen datasets. We plot the histogram of the RDC estimates and the estimated causal scores of 30 replications.\n\nJ ADDITIONAL CONVERGENCE RESULTS\n\nIn this section, we show the convergence results on Syn-1 as well.\n\nFigure 10: The Algorithm 1 converges on Syn-1. We plot the snapshots of the feature transformations f at training epochs [0, 5, 10, 20], under 15 random initializations (indicated by colors). Upper: λ = 0, most initializations converge to local minimizers (symmetry: (α, β) (cid:55)→ (aα, a−1β)). Lower: λ = 5, most initializations converge to two local minimizers (symmetry: (α, β) (cid:55)→ −(α, β)).\n\nK DETAILED DATA DESCRIPTIONS\n\nIn this section, we describe the datasets in detail.\n\nGene Datasets:\n\n22\n\n0.5250.5500.5750.6000.6250.6500.675RDC01234567Pair 1True directionFalse direction0.350.400.450.500.550.600.650.70RDC024681012Pair 2True directionFalse direction0.300.350.400.450.500.550.60RDC012345678Pair 3True directionFalse direction0.10.20.30.40.50.60.7RDC0246810Pair 4True directionFalse direction0.020.000.020.040.060.080.100.12C0246810Pair 10.150.100.050.000.050.100.150.20C0246810Pair 20.200.150.100.050.000.050.10C0246810Pair 30.30.20.10.00.10.2C0246810Pair 40.30.40.50.60.7RDC0123456Pair 5True directionFalse direction0.300.350.400.450.500.550.600.65RDC0123456Pair 6True directionFalse direction0.30.40.50.60.7RDC012345678Pair 7True directionFalse direction0.300.350.400.450.500.550.600.65RDC01234567Pair 8True directionFalse direction0.000.050.100.150.200.250.300.350.40C0246810Pair 50.000.050.100.150.200.250.30C0246810Pair 60.00.10.20.30.4C0246810Pair 70.000.050.100.150.200.250.30C0246810Pair 81.51.00.50.00.51.01.5X200002000400060008000f(X)epoch=01.51.00.50.00.51.01.5X0.20.10.00.10.20.30.40.5f(X)epoch=51.51.00.50.00.51.01.5X0.20.10.00.10.20.30.40.5f(X)epoch=101.51.00.50.00.51.01.5X0.20.10.00.10.20.30.40.5f(X)epoch=201.51.00.50.00.51.01.5X200002000400060008000f(X)epoch=01.51.00.50.00.51.01.5X432101234f(X)epoch=51.51.00.50.00.51.01.5X2.01.51.00.50.00.51.01.52.0f(X)epoch=101.51.00.50.00.51.01.5X2.01.51.00.50.00.51.01.52.0f(X)epoch=20Under review as a conference paper at ICLR 2023\n\nFor D4-S1, D4-S2A, D4-S2B, D4-S2C, we used the preprocessed data in Duong & Nguyen (2022) 2. D4-S1 contains 36 variable pairs with 105 samples in each pair; D4-S2A, D4-S2B, D4-S2C contains 528, 747, and 579 variable pairs respectively, and each pair contains 210 samples.\n\nThe GSE57872 dataset is built on Patel et al. (2014), in which the data has continuous values. Following Choi et al. (2020), we first screen out 657 gene pairs that have corresponding labels in the TRRUST database (Han et al., 2017). The gene contains many repeated values. we examined each gene pair and deleted those repeated expression values.\n\nL ON THE CHOICE OF λ\n\nWe tried seven different values for λ, and report the AUC scores on the PNL-A-unif dataset with different noise levels. We found that the MC-PNL is suitable to use in the small noise regime. We also found that for the data with small noise, smaller λ is preferred; and for the data with large injected noise, larger λ is preferred.\n\nFigure 11: The detailed AUC scores vs. λ under five noise levels on PNL-A-unif data.\n\n2https://github.com/baosws/CDCI\n\n23\n\n0.30.40.50.60.70.80.51251050100AUCλ0.20.40.60.81",
    "reference": "# Summary Of The Paper\n\nThe paper focuses on the challenges of estimating post-nonlinear models (PNLs) for causal discovery in the bivariate case. Indeed, solving the practical issues of estimating PNLs is an important topic.\n\nThe problems with existing methods are that they can produce local minima which are trivial and meaningless solutions and that the misspecification of noise distributions can lead to wrong causal discovery results.\nThe paper proposes an objective that avoids relying on noise distributions (similar property as HSIC minimization methods) and enforces the independence of residual and potential direction cause. \n\nIt combines the objective function in the alternating conditional expectation (ACE) algorithm (Breiman & Friedman, 1985) with a dependency measure as the penalty term for enforcing the residual to be independent of the potential direct cause. And it determines the causal direction by using the independent tests of the residual and potential direct cause. \n\nThe main experiments in Table 2 cannot show that the proposed method is significantly better than the other methods empirically.\n\n# Strength And Weaknesses\n\n## Writing \n(+) The paper is well-written in general and makes the problem that it is solving and the contributions clear. \n\n(-) However, I would also suggest spending more effort on the proposed method (Page 6 - Page 7) than illustrating the problem (Page 2 - Page 5). Because I would expect the readers can know some about the problems but not really about the proposed method. It can be not the case but then perhaps readers care more about the concerns of the proposed method.\n\n## Experiments\n(+) The work provides transparent and nice-illustrated figures. \n\n(-) But in Table 2, it cannot show the proposed method stands out compared with the others empirically. A fairer claim is that it is better than the related works based similar framework. As for real-world data, the ANM-based method performs surprisingly well and is comparable with the proposed method. \n\n(-) Moreover, in this case, it would be good to show experimental results on the typical benchmark dataset, the 100 (or more) Tuebingen cause-effect pairs. Not necessary to show superiority, but to provide a comparison with the others.\n\n## Concerns about Algorithm 2\n(-) The causal score is a single number of the difference between the dependency measures in different directions. It would be more convincing to report the numbers for the experiments, especially to which degree (or within which threshold), we can believe the number is larger than zero, smaller than zero, or equal. Moreover, I am worried that in real-world data or finite samples, how much we can trust a single number that gives us the causal relationship. And it doesn't seem to show the experiments about the independent case.\n\n## Proposed method\n(+) The work clearly formulates the proposed method with a discussion of optimization process. \n\n(-) Nevertheless, __my main concern__ is that the proposed objective (13) lacks a justification and theoretical guarantee from the identifiability concern. ( Note that I am not asking for an identifiability proof, but the justification and theoretical guarantee for that given the PNLs under identifiability assumptions, can the method be used for determining the causal direction?)\n\nAs actionable feedback, can the authors show that  \n(i) for objective (8), is it in any way related to maximum likelihood or minimizing mutual information?\n\n(ii) as for the model in the causal direction and the one in the reverse causal direction, can objective (8) imply that the one in the causal direction has a smaller value of the optimal objective function value than the other one? \n\nFurthermore,\n\n(iii) as for objective (13), how can the property maintain by adding the penalty term? A concern is that the penalty term is added as a \"soft\" constraint which is not necessary to be exactly the case. Then, is it possible that the optimal solution by solving (13) can be the local minima which are taken as a trade-off between the objective (8) and the penalty term? Then, will this lead to a misspecified model ? will this lead to a problem for causal discovery?\n\nTo further elaborate on my point:\nThe authors introduced the PNL-MLP of Zhang & Hyvärinen (2009), which uses mutual information for estimating the model and later uses independent tests for causal discovery. And the authors point out the problem of using mutual information as the objective, which can be hard with large-scale datasets. But an important fact of using the maximum likelihood or minimizing mutual information is that they are well justified by the identifiability of PNLs as illustrated in [1], especially, the independent noise assumption. \nSimilarly, as for the regression by dependence minimization (Mooij et al., 2009), it directly minimizes the HSIC score to enforce the independent noise assumption and pick up the model with a smaller score as the causal direction. This is fine because it directly uses the assumption as objective from the perspective. But for the objective in (Uemura & Shimizu 2020) and this paper, they neither directly use the independent noise assumption nor have a theoretical guarantee of the identifiability as Thm.2 and Thm. 3 in [1]. Therefore, to fix my concerns, maybe the authors could consider my actionable feedback.\n\n[1] Zhang, K., Wang, Z., Zhang, J., & Schölkopf, B. (2015). On estimation of functional causal models: general results and application to the post-nonlinear causal model. ACM Transactions on Intelligent Systems and Technology (TIST), 7(2), 1-22.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work focuses on an important problem for causal discovery. It is in general well-written. The related points can be found in the previous comments.\n\n# Summary Of The Review\n\nThe paper works on the practical issues of estimating PNLs and shows the problems of existing methods. It proposes to use the objective,  maximum correlation with a dependency measure as the penalty term. My main concern is about the theoretical guarantee and the identifiability of the results given by the proposed objective. My minor concern is about the experiments and the causal score used in Algorithm 2.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nDENSE RGB SLAM WITH NEURAL IMPLICIT MAPS\n\nHeng Li123∗, Xiaodong Gu2∗, Weihao Yuan2, Luwei Yang3, Zilong Dong2, Ping Tan123 1Hong Kong University of Science and Technology, 2Alibaba Group, 3Simon Fraser University lh.heng.li@connect.ust.hk, luweiy@sfu.ca, pingtan@ust.hk {qianmu.ywh, dadong.gxd, list.dzl}@alibaba-inc.com\n\nABSTRACT\n\nThere is an emerging trend of using neural implicit functions for map representation in Simultaneous Localization and Mapping (SLAM). Some pioneer works have achieved encouraging results on RGB-D SLAM. In this paper, we present a dense RGB SLAM method with neural implicit map representation. To reach this challenging goal without depth input, we introduce a hierarchical feature volume to facilitate the implicit map decoder. This design effectively fuses shape cues across different scales to facilitate map reconstruction. Our method simultaneously solves the camera motion and the neural implicit map by matching the rendered and input video frames. To facilitate optimization, we further propose a photometric warping loss in the spirit of multi-view stereo to better constrain the camera pose and scene geometry. We evaluate our method on commonly used benchmarks and compare it with modern RGB and RGB-D SLAM systems. Our method achieves favorable results than previous methods and even surpasses some recent RGB-D SLAM methods. The code is at poptree.github.io/DIM-SLAM/.\n\n1\n\nINTRODUCTION\n\nVisual SLAM is a fundamental task in 3D computer vision with many applications in AR/VR and robotics. The goal of visual SLAM is to estimate the camera poses and build a 3D map of the environment simultaneously from visual inputs. Visual SLAM methods can be primarily divided into sparse or dense according to their reconstructed 3D maps. Sparse methods (Mur-Artal & Tard ́os, 2017; Engel et al., 2017) focus on recovering camera motion with a set of sparse or semi-dense 3D points. Dense works (Newcombe et al., 2011b) seek to recover the depth of every observed pixel and are often more desirable for many downstream applications such as occlusion in AR/VR or obstacle detection in robotics. Earlier methods (Newcombe et al., 2011a; Whelan et al., 2012) often resort to RGB-D cameras for dense map reconstruction. However, RGB-D cameras are more suitable to indoor scenes and more expensive because of the specialized sensors.\n\nAnother important problem in visual SLAM is map representation. Sparse SLAM methods (MurArtal & Tard ́os, 2017; Engel et al., 2017) typically use point clouds for map representation, while dense methods (Newcombe et al., 2011b;a) usually adopt triangle meshes. As observed in many recent geometry processing works (Mescheder et al., 2019; Park et al., 2019; Chen & Zhang, 2019), neural implicit function offers a promising presentation for 3D data processing. The pioneer work, iMAP (Sucar et al., 2021), introduces an implicit map representation for dense visual SLAM. This map representation is more compact, continuous, and allowing for prediction of unobserved areas, which could potentially benefit applications like path planning (Shrestha et al., 2019) and object manipulation (Sucar et al., 2020). However, as observed in NICE-SLAM (Zhu et al., 2022), iMAP (Sucar et al., 2021) is limited to room-scale scenes due to the restricted representation power of MLPs. NICE-SLAM (Zhu et al., 2022) introduces a hierarchical feature volume to facilitate the map reconstruction and generalize the implicit map to larger scenes. However, both iMAP (Sucar et al., 2021) and NICE-SLAM (Zhu et al., 2022) are limited to RGB-D cameras.\n\nThis paper presents a novel dense visual SLAM method with regular RGB cameras based on the implicit map representation. We also adopt a hierarchical feature volume like NICE-SLAM to deal\n\n∗Equal contribution\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nwith larger scenes. But our formulation is more suitable for visual SLAM. Firstly, the decoders in NICE-SLAM (Zhu et al., 2022) are pretrained, which might cause problems when generalizing to different scenes (Yang et al., 2021), while our method learns the scene features and decoders together on the fly to avoid generalization problem Secondly, NICE-SLAM (Zhu et al., 2022) computes the occupancy at each point from features at different scales respectively and then sums these occupancies together, while we fuse features from all scales to compute the occupancy at once. In this way, our optimization becomes much faster and thus can afford to use more pixels and iterations, enabling our framework to work on the RGB setting. In experiments, we find the number of feature hierarchy is important to enhance the system accuracy and robustness. Intuitively, features from fine volumes capture geometry details, while features from coarse volumes enforce geometry regularity like smoothness or planarity. While NICE-SLAM (Zhu et al., 2022) only optimizes two feature volumes with voxel sizes of 32cm and 16cm, our method solves six feature volumes from 8cm to 64cm. Our fusion of features across many different scales leads to more robust and accurate tracking and mapping as demonstrated in experiments.\n\nAnother challenge in our setting is that there are no input depth observations. Therefore, we design a sophisticated warping loss to further constrain the camera motion and scene map in the same spirit of multi-view stereo (Zheng et al., 2014; Newcombe et al., 2011b; Wang et al., 2021b; Yu et al., 2021). Specifically, we warp one frame to other nearby frames according to the estimated scene map and camera poses and optimize the solution to minimize the warping loss. However, this warping loss is subject to view-dependent intensity changes such as specular reflections. To address this problem, we carefully sample image pixels visible in multiple video frames and evaluate the structural similarity of their surrounding patches to build a robust system.\n\nWe perform extensive evaluations on three different datasets and achieve state-of-the-art performance on both mapping and camera tracking. Our method even surpasses recent RGB-D based methods like iMAP (Sucar et al., 2021) and NICE-SLAM (Zhu et al., 2022) on camera tracking.\n\nOur contributions can be summarized as the following:\n\n• We design the first dense RGB SLAM with neural implicit map representation,\n\n• We introduce a hierarchical feature volume for better occupancy evaluation and a multiscale patchbased warping loss to boost system performance with only RGB inputs,\n\n• We achieve strong results on benchmark datasets and even surpass some recent RGB-D methods.\n\n2 RELATED WORK\n\nVisual SLAM Many visual SLAM algorithms and systems have been developed the last two decades. We only quickly review some of the most relevant works, and more comprehensive surveys can be found at (Cadena et al., 2016; Macario Barros et al., 2022). Sparse visual SLAM algorithms (Klein & Murray, 2007; Mur-Artal & Tard ́os, 2017) focus on solving accurate camera poses and only recover a sparse set of 3D landmarks serving for camera tracking. Semi-dense methods like LSD-SLAM (Engel et al., 2014) and DSO (Engel et al., 2017) achieve more robust tracking in textureless scenes by reconstructing the semi-dense pixels with strong image gradients. In comparison, dense visual SLAM algorithms (Newcombe et al., 2011b) aim to solve the depth of every observed pixel, which is very challenging, especially in featureless regions.\n\nIn the past, dense visual SLAM is often solved with RGB-D sensors. KinectFusion (Newcombe et al., 2011a) and the follow-up works (Whelan et al., 2012; 2021; Dai et al., 2017) register the input sequence of depth images through Truncated Signed Distance Functions (TSDFs) to track camera motion and recover a clean scene model at the same time. Most recently, iMAP and NICE-SLAM, introduce neural implicit functions as the map presentation and achieve better scene completeness, especially for unobserved regions. All these methods are limited to RGB-D cameras.\n\nMore recently, deep neural networks are employed to solve dense depth from regular RGB cameras. Earlier methods (Zhou et al., 2017; Ummenhofer et al., 2017; Zhou et al., 2018) directly regress camera ego-motion and scene depth from input images. Later, multi-view geometry constraints are enforced in (Bloesch et al., 2018; Tang & Tan, 2019; Teed & Deng, 2020; Wei et al., 2020; Teed & Deng, 2021) for better generalization and accuracy. But they all only recover depth maps instead of complete scene models. Our method also employs deep neural networks to solve the challenging\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Pipeline of our framework. Given a camera pose, our method sample the multi-scale features along view rays and pass the concatenated features through an MLP decoder to compute the depth and color at each pixel. In this way, we can solve the camera motion and a 3D scene map (captured by the feature volume and MLP decoder) simultaneously by matching the rendered images with observed ones. We further include warping loss across different views to enhance the system performance.\n\ndense visual SLAM problem. Unlike these previous methods that only recover a depth map per frame, we follow iMAP (Sucar et al., 2021) and NICE-SLAM (Zhu et al., 2022) to compute a neural implicit function for map representation.\n\nNeural Implicit Representations Neural implicit functions have been widely adopted for many different tasks in recent years. It has been introduced for 3D shape modeling (Mescheder et al., 2019; Park et al., 2019; Chen & Zhang, 2019), novel view synthesis (Mildenhall et al., 2020; Zhang et al., 2020; Martin-Brualla et al., 2021; Barron et al., 2021; Sara Fridovich-Keil and Alex Yu et al., 2022; M ̈uller et al., 2022), clothed human reconstruction (Saito et al., 2019; 2020), scene reconstruction (Murez et al., 2020; Sun et al., 2021), and object reconstruction (Yariv et al., 2020; Niemeyer et al., 2020; Wang et al., 2021a), etc. But all these works typically require precisely known camera poses. There are only a few works (Jeong et al., 2021; Zhang et al., 2022; Lin et al., 2021) trying to deal with uncalibrated cameras. Furthermore, all these methods require a long optimization process, and hence, are unsuitable for real-time applications like visual SLAM.\n\nRecently, the two pioneer works, iMAP (Sucar et al., 2021) and NICE-SLAM (Zhu et al., 2022), both adopt neural implicit functions to represent the scene map in visual SLAM. These works can solve camera poses and 3D scene maps in real-time by enforcing the rendered depth and color images to match with the input ones. However, they both require RGB-D sequences as input, which requires specialized sensors. In comparison, our visual SLAM method works with regular RGB cameras and solves camera poses and the 3D scene simultaneously in real-time.\n\n3 APPROACH\n\nThe overall pipeline of our framework is shown in Figure 1. Given an RGB video as input, our method aims to recover a 3D scene map and the camera motions simultaneously. We represent the scene map by a neural implicit function with a learnable multi-resolution feature volume. By sampling features from the volume grid along view rays and querying the sampled feature with the MLP decoder, we can render the depth and color of each pixel given the estimated camera parameters. Since this rendering process is differentiable, we can simultaneously optimize the neural implicit map as well as the camera poses by minimizing an objective function defined on photometric rendering loss and warping loss.\n\n3.1\n\nIMPLICIT MAP REPRESENTATION\n\nThis section introduces the formulation of our implicit map representation that combines a learnable multi-resolution volume encoding {Vl} and an MLP decoder Φ for depth and color prediction.\n\nMulti-resolution Volume Encoding Directly representing the scene map with MLPs, which maps a 3D point to its occupancy and color, confronts a forgetting problem because the MLP is globally updated for any frame (Sucar et al., 2021). To address this, we equip the MLP with multi-resolution volumes {Vl}L l=1, which are updated locally on seen regions of each frame(Sara Fridovich-Keil and Alex Yu et al., 2022; M ̈uller et al., 2022). The input point is encoded by the feature F sampled from the volumes {Vl}, which could also explicitly store the geometric information. We adopt a combination of L volumes, whose resolution arrange from the voxel size of vmax to the voxel size of\n\n3\n\nActive Frames Volume Variable ...Level 4 ~ 6Camera PoseVariableRay SamplerRay Sampler...Sampled 3D Feats.RenderMLPsConcate. SelectAdd after optimizedL1L2L3RenderedDepthWarpingLossPhotometric LossRenderedImageLocal Windowinput streamGlobal KeyFramesframe i-1frame iframe i+1KeyframesSet+keyframe Akeyframe BrayPublished as a conference paper at ICLR 2023\n\nvmin. This hierarchical structure works better than a single-scale volume because the gradient-based camera optimization on high-resolution volume is susceptible to a suboptimal solution if without a good initialization. In contrast, in a coarse-to-fine architecture, the low-resolution volume could enforce the smoothness in 3D space in early registration, while the high-resolution volume could encode the shape details. To compute this encoding of a given 3D point p = (x, y, z), we first interpolate feature Vl(p) at the point p by trilinear sampling. Then the 3D feature encoding F(p) is obtained by concatenating the features from all levels. Notably, the feature channel of each volume is set to 1. The small feature channel reduces memory consumption without losing performance, which is demonstrated in the ablation study.\n\nColor and Depth Prediction The MLP decoder Φ consists of three hidden layers with a feature channel of 32 and two heads that output the color cp and density op as,\n\n(op, cp) = Φ(F(p)). However, extracting scene geometry from the computed density requires careful tuning of the density threshold and leads to artifacts due to the ambiguity present in the density field. To address this problem, following (Oechsle et al., 2021), a sigmoid function is added on the output of op to regularize it to [0, 1], in which case a fixed level set of 0.5 could be used to extract the mesh. Also, the ray termination probability of point pi is now computed as,\n\n(1)\n\nwi = opi\n\ni−1 (cid:89)\n\n(1 − opj ).\n\nj=0\n\n(2)\n\nDuring volume rendering, a hierarchical sampling strategy similar to NeuS (Wang et al., 2021a) is adopted. We first uniformly sample 32 points on a view ray and then subsequently conducts importance sampling of additional 16 points for 4 times based on the previously estimated weights. N = 96 points are sampled in total. For each point, we compute its depth ̃D and color ̃I as,\n\n ̃D =\n\nN (cid:88)\n\ni=0\n\nwizi, ̃I =\n\nN (cid:88)\n\ni=0\n\nwici,\n\n(3)\n\nwhere zi and ci are the depth and color of the point pi along the viewing ray. Notice that since the rendering equation is differentiable, the camera pose, multi-resolution volume encoding, and the parameters of the MLP decoder could be optimized together from the gradient back-propagation.\n\n3.2\n\nJOINT OPTIMIZATION\n\nIn this section, we present the objective function for optimization. The implicit scene representation and the camera poses are jointly optimized in a set of frames W with corresponding poses ̃P : { ̃Pk = [ ̃Rk, ̃tk], k ∈ W}, where R and t are the rotation and translation camera parameters respectively. The frame selection strategy is explained later in section 3.3.\n\nPhotometric Rendering Loss To optimize the scene map and the camera poses, we compute the L1 loss between the rendered and observed color for a set of randomly sampled pixels M:\n\nLrender =\n\n1 |M|\n\n(cid:88)\n\nq∈M\n\n||Iq − ̃Iq||1.\n\n(4)\n\nHere, |M| denotes the number of sampled pixels. This rendering loss alone is insufficient to determine the scene geometry and could lead to ambiguous solutions, where the rendered image matches the observation well but the depth estimation is completely wrong. To overcome this problem, we introduce a photometric warping loss to further constrain the optimization problem.\n\nPhotometric Warping Loss We define the photometric warping loss in a similar spirit as multi-view stereo (Zheng et al., 2014) to enforce the geometry consistency. Let qk denotes a 2D pixel in frame k, we first lift it into 3D space and then project it to another frame l as:\n\nk qhomo (5) k = (u, v, 1) is the homogeneous coordinate of qk, and ̃Dqk denotes its estimated depth.\n\n ̃Dqk + ̃tk − ̃tl),\n\nqk→l = Kl ̃R⊤\n\nl ( ̃RkK−1\n\nwhere qhomo We minimize the warping loss defined as the following,\n\nk\n\nLwarping =\n\n1 |M|\n\n(cid:88)\n\n(cid:88)\n\nqk∈M\n\nl∈W,k̸=l\n\n||Iqk − Iqk→l ||1.\n\n(6)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nHowever, minimizing the warping loss on a single pixel does not model the view-dependent intensity changes, e.g., specularities, and could create artifacts in the scene map. We thus define the warping loss over image patches instead of individual pixels to address this issue. For an image patch N s qk centered on pixel qk and with the size of s × s, we assume all pixels on the patch have the same depth value as qk. Similar to the case of a single pixel, we project all pixels in this patch to another frame l and obtain N s\n\nqk→l . Thus, we extend the pixel-wise warping loss to patch-wise as,\n\nLwarping =\n\n1 |M|\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nqk∈M\n\nl∈W,k̸=l\n\ns∈S\n\nBqk SSIM(N s\n\nqk\n\n, N s\n\nqk→l\n\n),\n\n(7)\n\nwhere Bqk denotes a visibility mask which will be explained later, and S = {1, 5, 11} denotes the patch size. In practice, we select the structure similarity loss (SSIM) (Wang et al., 2004) as it achieves better results than other robust functions, e.g. (Park et al., 2017). Note that this formulation degenerates pixel-wise when a patch of size 1 is used. Further, this warping assumes front-parallel image patches, which might be improved with a perspective warping based on surface normals extracted from the scene map. Here, we choose the front-parallel warping for its better computation efficiency.\n\nVisibility Mask We define a binary visibility mask Bqk to filter pixels with low visibility. For any pixel qk, we count the number of its visible frames by projecting it to all other frames in the set W. The projection is counted as valid if the projected pixel is within the frame boundary. Only pixels with more than 5 valid projections is regarded as visible, whose visibility value is set to 1. Note that we do not model occlusion here for better efficiency.\n\nRegularization Loss To deal with textureless regions, we enforce an edge-preserving regularization term to smooth the rendered depth ̃D as,\n\nLsmooth =\n\n(cid:88)\n\nk∈W\n\ne−||∇ ̃I||2 ||∇ ̃D||1,\n\n(8)\n\nwhere ∇ ̃I, ∇ ̃D are the first-order gradients of the computed color and depth images, respectively. Thus, the final objective for mapping and tracking in a single thread is obtained as,\n\narg min Θ,Pk,k∈W\n\nαwarpingLwarping + αrenderLrender + αsmoothLsmooth,\n\n(9)\n\nwhere α denotes the weighting factors, and Θ denotes the parameters of the implicit scene representation, including the multi-resolution volumes and the MLP decoder.\n\n3.3 SYSTEM\n\nIn this section, we introduce several crucial components to build our complete visual SLAM system.\n\nInitialization The initialization is performed when a small set of frames (13 frames in all our experiments) are collected. The pose of the first frame is set to the identity matrix. Then the poses of the remaining frames are optimized from the objective function for Ni iterations. After the initialization, the first frame is added to the global keyframe set and fixed. The parameters of MLP decoders are also fixed after initialization.\n\nWindow Optimization During camera tracking, our method always maintains a window of active frames (21 frames in all our experiments). The active frames include two different types: local window frames and global keyframes. For a frame k, we regard the frames with index from k − 5 to k + 5 as the local window frames. We further sample 10 keyframes from the global keyframe set. Specifically, we randomly draw 100 pixels on frame k and project them to all keyframes using estimated camera poses ̃P and depths ̃D. We then evaluate the view overlap ratio by counting the percentage of the pixels projected inside the boundary of those keyframes. Afterward, we randomly select 10 keyframes from those with an overlap ratio over 70% to form the global keyframe set. Then, all these active frames are used to optimize the implicit scene representation and the poses for Nw iterations. After the window optimization, the oldest local frame k − 5 is removed, and a new incoming frame of k + 6 with a constant velocity motion model is added to the set of local window frames. The camera poses of all global keyframes are fixed during tracking. Please refer to the appendix about windows optimization for mapping in two threads cases.\n\nKeyframe Selection We follow the simple keyframe selection mechanism similar to iMAP and NICE-SLAM, while a more sophisticated method might be designed based on image retrieval and\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nNICE-SLAM ORB-SLAM2(RGB) DROID-SLAM Ours(two threads) Ours(one thread)\n\nATE RMSE[cm]↓\n\no-0 o-1 o-2 o-3 o-4 r-0 r-1 r-2 Avg.\n\n1.02 0.43 0.25 0.89 0.67\n\n3.91 0.30 0.45 0.72 0.37\n\n1.76 12.2 0.32 1.11 0.36\n\n6.10 0.39 0.44 0.75 0.33\n\n11.9 11.4 0.37 0.92 0.36\n\n4.37 0.30 0.35 0.84 0.48\n\n2.76 0.42 0.33 1.22 0.78\n\n3.54 0.25 0.28 0.66 0.35\n\n4.4 3.21 0.35 0.89 0.46\n\niMAP\n\nNICE-SLAM\n\nDI-Fuison\n\nDROID-SLAM\n\nOurs(two threads)\n\nAcc.[cm]↓ 4.43 4.81 Comp.[cm]↓ 5.56 5.65 Comp. Ratio[≤ 5cm%]↑ 77.71 79.64 77.22 77.34 77.63 83.91 83.45 75.53 79.06\n\n4.68 5.51\n\n4.27 5.45\n\n5.87 6.11\n\n3.69 4.87\n\n4.83 6.59\n\n3.58 5.06\n\n3.71 5.26\n\nAcc.[cm]↓ 2.85 3.82 Comp.[cm]↓ 3.00 3.57 Comp. Ratio[≤ 5cm%]↑ 94.93 92.61 85.20 82.98 86.14 90.93 92.80 89.07 89.33\n\n2.65 3.00\n\n2.58 2.47\n\n2.26 2.02\n\n2.50 2.36\n\n3.50 3.83\n\n2.73 2.87\n\n2.77 3.84\n\nAcc.[cm]↓ 1.79 49.00 26.17 19.40 2.11 Comp.[cm]↓ 3.57 39.40 17.35 10.19 4.83 Comp. Ratio[≤ 5cm%]↑ 87.17 91.85 80.13 79.94 80.21 87.77 32.01 45.61 72.96\n\n70.56 1.42 2.20 3.58\n\n2.11 4.71\n\n2.02 5.84\n\nAcc.[cm]↓ 4.23 4.90 Comp.[cm]↓ 6.82 7.97 Comp. Ratio[≤ 5cm%]↑ 77.57 72.95 59.85 62.30 66.50 68.35 71.68 68.76 68.50\n\n4.40 9.43\n\n4.47 7.77\n\n4.01 7.10\n\n4.53 5.72\n\n5.35 6.80\n\n3.26 4.22\n\n2.92 5.54\n\nAcc.[cm]↓ 4.29 5.02 Comp.[cm]↓ 5.46 6.63 Comp. Ratio[≤ 5cm%]↑ 88.56 84.7 71.40 72.54 73.23 81.12 76.94 70.85 77.41\n\n3.61 5.97\n\n4.12 5.31\n\n6.06 7.43\n\n2.81 2.71\n\n4.83 6.11\n\n5.61 6.33\n\n2.33 3.25\n\nOurs(one thread)\n\nAcc.[cm]↓ 2.02 Comp.[cm]↓ 3.31 Comp. Ratio[≤ 5cm%]↑ 89.57 84.9\n\n2.60 2.65\n\n5.43 5.98\n\n4.50 3.68 6.09 5.32 75.3 73.48 76.62 82.2\n\n4.57 5.81\n\n4.03 3.64 5.84 4.72 4.20 5.70 80.4 74.44 79.6\n\nTable 1: Quantitative results on the Replica dataset. Here, ‘o-x’ and ‘r-x’ denote office-x and room-x sequence respectively. The top part shows the results of tracking, and the lower part shows the results of mapping. DROID-SLAM and our method work with the RGB inputs, while the others are all based on RGB-D data. The best results in RGB and RGB-D SLAMs are bolded respectively.\n\nvisual localization methods like (Arandjelovic et al., 2016; Brachmann et al., 2017). Specifically, a new keyframe is added if the field of view changes substantially. This view change is measured by the mean square f of the optical flow between the last keyframe and the current frame. A new keyframe is added if f > Tkf , where Tkf = 10 pixels in all our experiments.\n\n4 EXPERIMENTS\n\nDatasets We evaluate our method on three widely used datasets: TUM RGB-D (Sturm et al., 2012a), EuRoC (Burri et al., 2016), and Replica (Straub et al., 2019) dataset. The TMU RGB-D and EuRoC dataset are captured in small to medium-scale indoor environments. Each scene in the TUM dataset contains a video stream from a handheld RGB-D camera associated with the ground-truth trajectory. The EuRoC dataset captures stereo images using a drone, and their ground-truth 6D poses are provided with a Vicon System. We use only the monocular setting in the EuRoC dataset. The Replica dataset contains high-quality 3D scene models. We use the camera trajectories provided by iMAP (Sucar et al., 2021) to render the input RGB sequences.\n\nImplementation Details Our method is implemented with PyTorch (Paszke et al., 2017) and runs on a server with two NVIDIA 2080Ti GPUs. Our single-thread implementation, i.e. ours (one thread), and two-thread implementation, i.e. ours (two thread), need one and two GPUs respectively. The voxel sizes of the multi-resolution volumes for ours (one thread) are set to {64, 48, 32, 24, 16, 8}cm, where vmax = 64cm and vmin = 8cm. For ours (two threads), we replace the last 8cm volume with 16cm volume. In all our experiments, we fix the iteration number of initialization Ni to 1500, the number of sampling pixels |M| to 3000, the size of the window |W| to 21. The iteration number of window optimization for ours(one thread) Nw is 100. In our two-thread version, we change Nw to 20 for tracking. We follow NICE-SLAM and perform mapping once every 5 frames. Please refer to the Appendix A.5 for more details. We use the Adam (Kingma & Ba, 2014) optimizer to optimize both the implicit map representation and the camera poses, with learning rates of 0.001 and 0.01, respectively. The αwarping, αrender, αsmooth is 0.5, 0.1, 0.01, respectively. We apply a post-optimization at the end of the sequence to improve the quality of the mesh. Note a similar optimization is included\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nM A\nL S\n\n-\n\nD\n\nI\n\nO R\nD\n\nP A\nM\n\ni\n\ns r\nu O\n\nh\n\nt\n\nu r\n\nT\n\n- d\nn u\no r\n\nG\n\noffice0\n\noffice2\n\nFigure 2: Visual comparison of the reconstructed meshes on the Replica datasets. Our results are more complete and have sharper textures, which indicate more precise surface shapes. More examples are in the appendix.\n\nMethod\n\niMAP (Sucar et al., 2021) iMAP*(Sucar et al., 2021) DI-Fusion(Huang et al., 2021) NICE-SLAM(Zhu et al., 2022) BAD-SLAM(Sch ̈ops et al., 2019) Kintinuous(Whelan et al., 2012) ORB-SLAM2(RGBD)\n\nORB-SLAM2(RGB)(Mur-Artal & Tard ́os, 2017) DROID-SLAM(Teed & Deng, 2021) Ours (one thread)\n\nfr1/desk fr2/xyz fr3/office Mean\n\n4.9 7.2 4.4 2.7 1.7 3.7 1.6\n\n1.9 1.8 2.0\n\n2.0 2.1 2.3 1.8 1.1 2.9 0.4\n\n0.6 0.5 0.6\n\n5.8 9.0 15.6 3.0 1.7 3.0 1.0\n\n2.4 2.8 2.3\n\n4.2 6.1 7.4 2.5 1.5 3.2 1.0\n\n1.6 1.7 1.6\n\nTable 2: Camera tracking results on the TUM-RGBD dataset. The upper part shows RGB-D SLAM methods, while the lower part shows RGB SLAM methods. “∗” denotes the version reproduced by NICE-SLAM. The best results are bolded. The metric unit is [cm].\n\nin iMAP (Sucar et al., 2021) and NICE-SLAM (Zhu et al., 2022). All metrics reported in this paper are an average of the results for 5 runs.\n\n4.1 EVALUATION OF MAPPING AND TRACKING Mapping To compare the quality of the reconstructed scene maps, we first evaluate the RGB-D methods on the Replica dataset, including NICE-SLAM (Zhu et al., 2022), iMAP (Sucar et al., 2021), and DI-Fusion (Huang et al., 2021). Note all these methods use additional depth image inputs. We further compare with the RGB-only method, DROID-SLAM (Teed & Deng, 2021), where the TSDF-Fusion (Curless & Levoy, 1996) is employed to compute a mesh model of the scene map with a voxel grid resolution of 2563 using the estimated depth maps.\n\nWe use three metrics to evaluate the recovered scene map on Replica with 200, 000 sampled points from both the ground-truth and the reconstructed mesh. (i) Accuracy (cm), the mean Euclidean\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nV101 V102 V103 V201 V202 V203\n\n0.389 0.622 0.433 0.749 1.152 0.680 TartanVO(Wang et al., 2020) 0.070 0.210 0.110 0.110 1.080 SVO(Forster et al., 2014) 0.089 0.107 0.903 0.044 0.132 1.152 DSO(Engel et al., 2017) DROID-VO(Teed & Deng, 2021) 0.103 0.165 0.158 0.102 0.115 0.204 0.053 0.158 0.095 0.095 0.063 0.310 DPVO(Teed et al., 2022) 0.068 0.079 Ours (one thread)\n\n0.053 0.178\n\nX\n\nX\n\nX\n\nTable 3: Camera Tracking Results on EuRoC. The best results are bolded. The failure cases are marked as X. The metric unit is [m].\n\ndistance between sampled points from the reconstructed mesh to their nearest points on the GT mesh. (ii) Completion (cm), the mean Euclidean distance between sampled points from the GT mesh to their nearest points on the reconstructed mesh. (iii) Completion Ratio (%), the percentage of points in the reconstructed mesh whose nearest points on the GT mesh is within 5 cm. The unseen region is removed if it is not inside any camera viewing frustum.\n\nAs shown in the lower part of Table 1, our method outperforms iMAP and DROID-SLAM on mapping. Although our results are slightly inferior to those of NICE-SLAM and DI-Fusion, this is reasonable since they have additional depth images as input. A visual comparison with iMAP and DROID-SLAM is provided in Figure 2. From the zoomed regions, we can tell that our method generates more complete and accurate models than iMAP and DROID-SLAM. Additional visual comparisons are provided in the appendix.\n\nTracking To evaluate the quality of camera tracking, we compare our method with many recent visual SLAM methods with RGB or RGB-D inputs. For a fair comparison, we use the official implementation of DROID-SLAM to produce its result and cite other results from NICE-SLAM and DPVO (Teed et al., 2022). As for the evaluation metric, we adopt the RMSE of Absolute Trajectory Error (ATE RMSE) (Sturm et al., 2012b). The estimated trajectory is aligned with the ground truth with scale correction before evaluation.\n\nThe upper part of Table 1 shows the tracking results on the Replica dataset. Our method clearly outperforms NICE-SLAM, though it has additional depth input. Our tracking performance is similar to DROID-SLAM on this dataset.\n\nTable 2 shows our results on the TMU RGB-D dataset. We report results on the same three examples as iMAP. As shown in the table, our method outperforms all previous learning-based methods, including iMAP (Sucar et al., 2021), NICE-SLAM (Zhu et al., 2022), DI-Fusion (Huang et al., 2021), and has comparable performance with robust conventional methods like BAD-SLAM (Sch ̈ops et al., 2019), Kintinuous (Whelan et al., 2012), and ORB-SLAM2 (Mur-Artal & Tard ́os, 2017). Furthermore, our method does not include global bundle adjustment or loop closure, which is common in the conventional SLAM system. Our method achieves reasonable tracking performance while enjoying the benefits of neural implicit map representation, which can produce a watertight scene model or predict unobserved regions.\n\nTable 3 shows experiment results on the EuRoC dataset. We compare the tracking results with several strong baselines including TartanVO (Wang et al., 2020), SVO (Forster et al., 2014), DSO (Engel et al., 2017), DROID-VO (Teed & Deng, 2021), and DPVO (Teed et al., 2022). Our method generates comparable results in successful cases. Note there are two failure cases, where our method loses camera tracking due to the fast camera motion which leads to small view overlap between neighboring frames.\n\n4.2 ABLATION STUDY\n\nTo inspect the effectiveness of the components in our framework, we perform the ablation studies on the office-0 sequence of the Replica dataset.\n\nHierarchical Feature Volume To evaluate the effectiveness of our hierarchical feature volume, we experiment with different levels of hierarchies. We vary the number of hierarchies L and the number of feature channels C to keep the total number of learnable parameters similar. Table 4 shows the results of different settings. Clearly, the system performs worse when there are less hierarchies of feature volumes, even though a longer feature is learned to compromise it. Note the system fails\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nConfigrations\n\n(6,1)\n\n(4,4)\n\n(2,8)\n\n(1,16)\n\n(6,2)\n\n(6,4)\n\n(6,8)\n\n(6,16)\n\n(L, C)\n\nw/o MLP\n\nATE RMSE(cm)↓\n\nAccuacry(cm)↓\n\nMEAN STD\n\nMEAN STD\n\n0.67 0.76 2.27 0.14 0.18 0.25\n\n2.60 3.39 6.78 0.56 0.91 0.95\n\nX X\n\nX X\n\n0.63 0.69 0.61 0.12 0.08 0.11\n\n2.86 2.35 2.96 0.49 0.33 0.40\n\n0.66 0.09\n\n2.48 0.12\n\n8.92 1.49\n\n11.43 3.17\n\nTable 4: Ablation study of different configurations of the feature volume. The parameters L and C are the numbers of hierarchies and the number of the feature channel, respectively. We test on office-0 for 5 times and calculate the mean and standard deviation of the camera tracking error and map accuracy.\n\nMethod\n\nMemory(MB)↓ FLOPs(×103)↓ Mapping(ms)↓ Tracking(ms)↓\n\niMAP NICE-SLAM Ours (two threads) Ours (one thread)\n\n1.02 12.0(16cm) 9.1(16cm) 29.1(8cm)\n\n443.91 104.16 18.76 18.76\n\n448(1000, 10) 130(1000, 10) 330(3000, 100)\n\n101(200, 6) 47(200, 6) 72(3000, 20)\n\n335(3000, 100)\n\nTable 5: Memory, computation, and running time. We report the runtime in time(pixel, iter) format for a fair comparison. Please refer to the appendix for more information about our two-threads version.\n\nto track camera motion at textureless regions when L is set to 1. This experiment demonstrates the effectiveness of fusing features from different scales to better capture scene geometry.\n\nNumber of Feature Channel We further vary the number of feature channel C with L fixed at 6. Table 4 shows similar system performance is achieved for C = 2, 4, 8, 16. Therefore, to keep our method simple, we select C = 1 in experiments to reduce computation cost.\n\nMLP Decoder To study the impact of the MLP decoder, we also experiment without the MLP decoder, where the 1-channel feature can be regarded as the occupancy directly. At a 3D point, we sum the occupancies from all hierarchies for the final result. The color is computed similarly with another 6-level volume of 3 channels. As shown in the right of Table 4, the system performance drops significantly without the MLP decoder.\n\n4.3 MEMORY AND COMPUTATION ANALYSIS\n\nTable 5 analyzes the memory and computation expenses. The memory cost is evaluated on the office-0 sequence of the Replica dataset. We use the same bounding box size of the feature volume in both NICE-SLAM and our method. iMAP is the most memory efficient with only 1.02MB map size. However, as discussed in NICE-SLAM, it cannot deal with larger scenes. Our one thread version, i.e. ours (one thread), recovers the scene map at higher resolution of 8cm and requires a larger map of 29.1 MB, while NICE-SLAM works at 16cm map resolution and takes only a 12 MB map. Our two thread version, i.e. ours (two threads), uses the same scene map resolution of 16cm, and is more memory efficient with map size of 9.1 MB.\n\nTable 5 also reports the number of FLOPs for computing the color and occupancy at a single 3D point. Our method requires much less FLOPS, benefiting from our short features and single shallow MLP decoder. Table 5 further reports the computation time of processing one input frame. We report the results in time(pixel, iter) format for comparison. Without depth input, our method samples more pixels and takes more optimization iterations, while our method is still faster than iMAP. Our running time is reported with one or two RTX 2080TI GPUs, while the running time of NICE-SLAM and iMAP are cited from their papers with an RTX 3090 GPU.\n\n5 CONCLUSION\n\nThis paper introduces a dense visual SLAM method working with regular RGB cameras with neural implicit map representation. The scene map is encoded with a hierarchical feature volume together with an MLP decoder, which are both optimized with the camera poses simultaneously through back-propagation. To deal with regular RGB cameras, a sophisticated warping loss is designed to enforce multi-view geometry consistency during the optimization. The proposed method compares favorably with the widely used benchmark datasets, and even surpasses some recent methods with RGB-D inputs.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nRelja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn architecture for weakly supervised place recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5297–5307, 2016.\n\nJonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the International Conference on Computer Vision (ICCV), 2021.\n\nMichael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew J. Davison. Codeslam – learning a compact, optimisable representation for dense visual slam. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nEric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. Dsac-differentiable ransac for camera localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6684–6692, 2017.\n\nMichael Burri, Janosch Nikolic, Pascal Gohl, Thomas Schneider, Joern Rehder, Sammy Omari, Markus W Achtelik, and Roland Siegwart. The euroc micro aerial vehicle datasets. The International Journal of Robotics Research, 2016. URL http://ijr.sagepub.com/content/ early/2016/01/21/0278364915620033.abstract.\n\nCesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, Jose Neira, Ian Reid, and John J. Leonard. Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. IEEE Transactions on Robotics (TRO), 32:1309—-1332, 2016.\n\nZhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings\n\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nBrian Curless and Marc Levoy. A volumetric method for building complex models from range images. In Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’96, pp. 303–312, 1996.\n\nAngela Dai, Matthias Nießner, Michael Zollhofer, Shahram Izadi, and Christian Theobalt. Bundlefusion: Real-time globally consistent 3d reconstruction using online surface re-integration. ACM Transactions on Graphics (TOG), 2017.\n\nJakob Engel, Thomas Schops, and Daniel Cremers. Lsd-slam: Large-scale direct monocular slam.\n\nIn Proceedings of the European Conference on Computer Vision (ECCV), 2014.\n\nJakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE Transactions on\n\nPattern Analysis and Machine Intelligence (TPAMI), 40(3):611–625, 2017.\n\nChristian Forster, Matia Pizzoli, and Davide Scaramuzza. Svo: Fast semi-direct monocular visual In Proceedings of the IEEE International Conference on Robotics and Automation\n\nodometry. (ICRA), pp. 15–22. IEEE, 2014.\n\nJiahui Huang, Shi-Sheng Huang, Haoxuan Song, and Shi-Min Hu. Di-fusion: Online implicit 3d reconstruction with deep priors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nYoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima Anandkumar, Minsu Cho, and Jaesik Park. Self-calibrating neural radiance fields. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5846–5854, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nGeorg Klein and David Murray. Parallel tracking and mapping for small ar workspaces. In Proceed-\n\nings of the International Symposium on Mixed and Augmented Reality (ISMAR), 2007.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nChen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5741–5751, 2021.\n\nWilliam E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction\n\nalgorithm. ACM siggraph computer graphics, 21(4):163–169, 1987.\n\nAndr ́ea Macario Barros, Maugan Michel, Yoann Moline, Gwenol ́e Corre, and Fr ́ed ́erick Carrel. A comprehensive survey of visual slam algorithms. Robotics, 11(1), 2022. ISSN 2218-6581. URL https://www.mdpi.com/2218-6581/11/1/24.\n\nRicardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\n\nThomas M ̈uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1–102:15, July 2022. doi: 10.1145/3528223.3530127. URL https://doi.org/10.1145/3528223. 3530127.\n\nRa ́ul Mur-Artal and Juan D. Tard ́os. Orb-slam2: An open-source slam system for monocular, stereo,\n\nand rgb-d cameras. IEEE Transactions on Robotics (TRO), 33(5):1255–1262, 2017.\n\nZak Murez, Tarrence van As, James Bartolozzi, Ayan Sinha, Vijay Badrinarayanan, and Andrew Rabinovich. Atlas: End-to-end 3d scene reconstruction from posed images. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\n\nRichard A. Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, and Andrew J. Davison. Kinectfusion: Real-time dense surface mapping and tracking. In Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR), 2011a.\n\nRichard A. Newcombe, Steven J. Lovegrove, and Andrew J. Davison. Dtam: Dense tracking and mapping in real-time. In Proceedings of the International Conference on Computer Vision (ICCV), 2011b.\n\nMichael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n\nMichael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In Proceedings of the International Conference on Computer Vision (ICCV), 2021.\n\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nSeonwook Park, Thomas Sch ̈ops, and Marc Pollefeys.\n\nIllumination change robustness in direct visual slam. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pp. 4523–4530. IEEE, 2017.\n\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nShunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceedings of the International Conference on Computer Vision (ICCV), 2019.\n\nShunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n\nSara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo In Proceedings of the IEEE\n\nKanazawa. Plenoxels: Radiance fields without neural networks. Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n\nThomas Sch ̈ops, Torsten Sattler, and Marc Pollefeys. Bad slam: Bundle adjusted direct rgb-d slam. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 134–144, 2019.\n\nRakesh Shrestha, Fei-Peng Tian, Wei Feng, Ping Tan, and Richard Vaughan. Learned map prediction for enhanced mobile robot exploration. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2019.\n\nRakesh Shrestha, Siqi Hu, Minghao Gou, Ziyuan Liu, and Ping Tan. A real world dataset for multiview 3d reconstruction. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.\n\nJulian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe. The Replica dataset: A digital replica of indoor spaces. 2019.\n\nJ. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. A benchmark for the evaluation of rgb-d slam systems. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Oct. 2012a.\n\nJ ̈urgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. A benchmark for the evaluation of rgb-d slam systems. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 573–580, 2012b.\n\nEdgar Sucar, Kentaro Wada, and Andrew Davison. Nodeslam: Neural object descriptors for multiview shape reconstruction. In Proceedings of the International Conference on 3D Vision (3DV), 2020.\n\nEdgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew Davison.\n\niMAP: Implicit mapping and positioning in real-time. In Proceedings of the International Conference on Computer Vision (ICCV), 2021.\n\nJiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nChengzhou Tang and Ping Tan. Ba-net: Dense bundle adjustment networks. In Proceedings of the\n\nInternational Conference on Learning Representations (ICLR), 2019.\n\nZachary Teed and Jia Deng. Deepv2d: Video to depth with differentiable structure from motion. In\n\nProceedings of the International Conference on Learning Representations (ICLR), 2020.\n\nZachary Teed and Jia Deng. DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-\n\nD Cameras. 2021.\n\nZachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry.\n\narXiv preprint\n\narXiv:2208.04726, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nBenjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. Advances in Neural Information Processing Systems (NeurIPS), 34, 2021a.\n\nQianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multiview image-based rendering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021b.\n\nWenshan Wang, Yaoyu Hu, and Sebastian Scherer. Tartanvo: A generalizable learning-based vo. In\n\nProceedings of the Conference on Robot Learning (CORL), 2020.\n\nZhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, 2004. doi: 10.1109/TIP.2003.819861.\n\nXingkui Wei, Yinda Zhang, Zhuwen Li, Yanwei Fu, and Xiangyang Xue. Deepsfm: Structure from motion via deep bundle adjustment. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\n\nT. Whelan, J. McDonald, Michael Kaess, Maurice Fallon, Hordur Johannsson, and John J. Leonard. Kintinuous: Spatially extended kinectfusion. In RSS Workshop on RGB-D: Advanced Reasoning with Depth Cameras, July 2012.\n\nThomas Whelan, Stefan Leutenegger, Renato F. Salas-Moreno, Ben Glocker, and Andrew J. Davison. Elasticfusion: Dense slam without a pose graph. In Proceedings of Robotics: Science and Systems (RSS), 2021.\n\nXingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. Vox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation. In Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR), pp. 80–89, 2021.\n\nLior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020.\n\nLin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. iNeRF: Inverting neural radiance fields for pose estimation. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021.\n\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nJiahui Zhang, Fangneng Zhan, Rongliang Wu, Yingchen Yu, Wenqing Zhang, Bai Song, XiaoarXiv preprint\n\nqin Zhang, and Shijian Lu. Vmrf: View matching neural radiance fields. arXiv:2207.02621, 2022.\n\nKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving\n\nneural radiance fields. arXiv:2010.07492, 2020.\n\nEnliang Zheng, Enrique Dunn, Vladimir Jojic, and Jan-Michael Frahm. Patch match based joint view selection and depth map estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1510–1517, 2014.\n\nHuizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. Deeptam: Deep tracking and mapping.\n\nIn Proceedings of the European Conference on Computer Vision (ECCV), 2018.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nTinghui Zhou, Matthew Brown, Noah Snavely, and David Lowe. Unsupervised learning of depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nZihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R. Oswald, and Marc Pollefeys. Nice-slam: Neural implicit scalable encoding for slam. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nIn the appendix, we present the following:\n\nA.1 FEATURE VOLUME INITIALIZATION\n\nWe initialize the parameters of all feature volumes under a normal distribution, while the mean and the standard deviation of the distribution are 0 and 0.001, respectively.\n\nA.2 POST OPTIMIZATION IN OURS(ONE THREAD)\n\nTo enhance the geometry and color consistency throughout the entire scene, we also introduce a post-optimization module similar to the one in NICE-SLAM (Zhu et al., 2022) and iMAP (Sucar et al., 2021). This module is optional at the end of processing an input video clip to improve the final mesh quality without heavy computation. This post-optimization module optimizes the implicit scene representation with the camera poses fixed using all keyframes in an alternative way. In each iteration, we sample a random keyframe k and select 20 keyframes from all other keyframes with at least 70% overlap with frame k. We optimize the implicit function using all keyframes 1, 000 iterations.\n\nA.3 ABLATION ON WARPING LOSS\n\nTo see the impact of utilizing multi-scale patch warping photometric loss, we run an ablation study on different settings of the warping loss. We evaluate different configurations during the initialization stage. We mark our default configuration as the baseline: using a multi-scale patch warping loss defined as Equation 7 where S = {1, 5, 11}. We then compare it with two different patch size configurations, where S = {1} and S = {1, 11}, respectively. Note that the setting of S = {1} degrades to the pixel-wise warping loss as Equation 5. As shown in Table 6, our default configuration achieves 1.85cm in terms of the average depth error. The setting of S = {1, 11} is slightly inferior to the baseline, while the setting of pixel-wise warping fails to recover an accurate depth. This is also illustrated in the qualitative comparison in Figure 3.\n\nA.4 MORE ABLATION ON FEATURE CHANNEL\n\nWe further verify the number of feature channels with the initialization stage. As shown in the right of Table 6, increasing the number of features C does not improve the quality of the initialized depth. Note that while the setting (L = 1, c = 16) can still initialize the reconstruction, the camera tracking soon fails in textureless regions. This ablation further justifies our choice of C = 1.\n\nFigure 3: The visualization of the initialized depth map during optimization iterations. (a) The default configuration with multiple patch-wise warping loss, S = {1, 5, 11}. (b) The pixel-wise warping loss, S = {1}.\n\n15\n\n(a)(b)Iteration 0Iteration 160Iteration 1400Iteration 0Iteration 160Iteration 1400RGB imagesGT depthPublished as a conference paper at ICLR 2023\n\nDepth Error(cm)↓\n\nS\n\n(L, C)\n\n{1, 5, 11}∗ {1, 11} {1} (6, 1)∗ (6, 4) (6, 16) (1, 16)\n\nMEAN STD\n\n1.85 0.15\n\n3.73 0.85\n\n35 3.65\n\n1.85 0.15\n\n1.72 0.09\n\n1.93 0.16\n\n1.83 0.19\n\nTable 6: Mean depth errors of initialized map for different settings in the ablation study. The configuration of our method is marked with ∗. We report the mean and std of depth error on 5 runs of office-0 on Replica.\n\nMethod\n\nReplica\n\nTUM-RGBD\n\nMapping Tracking Mapping Tracking\n\nNICE-SLAM (200,10) Ours(two threads) (3000,100) (3000,20) Ours(one thread)\n\n(1000,60)\n\n(3000,100)\n\n(5000,60) (5000,200) -\n\nTable 7: (Pixel, Iteration) used in different datasets. NICE-SLAM needs more pixels and iterations on TUMRGBD, while ours(one thread) keeps the same configuration on all datasets.\n\nA.5 TWO THREADS FOR TRACKING AND MAPPING, OURS(TWO THREADS)\n\nWe provide more details about our configuration which runs tracking and mapping on two threads. The thread of tracking is similar to ours(one thread), while we only optimize the camera poses:\n\narg min Pk,k∈W\n\nαwarpingLwarping + αrenderLrender,\n\n(10)\n\nwhere W is the window of the frames defined in Section 3.3, and we sample high gradient pixels following the strategy in iNeRF (Yen-Chen et al., 2021). We sample 3, 000 pixels and optimize 20 iterations for each frame.\n\nIn the mapping thread, we optimize both cameras poses and the parameters of the scene simultaneously:\n\narg min Θ,Pk,k∈Wmapping\n\nαwarpingLwarping + αrenderLrender + αsmoothLsmooth,\n\n(11)\n\nwhere Wmapping is the keyframe window mentioned in A.2, and we random sample 3, 000 pixels for mapping.\n\nThe major difference between the post-optimization in A.2 and the mapping thread is that we only run pots-optimization at the end of the sequence. Furthermore, we do not optimize the camera pose during post-optimization in ours(one thread).\n\nThe memory and time cost of both ours(one thread) and ours(two threads) are listed in Table 5. To fairly compare the memory consumption between our method and NICE-SLAM (Zhu et al., 2022), we replace the volume with size 8cm by 16cm. NICE-SLAM (Zhu et al., 2022) and ours have the same resolution in this configuration.\n\nWe report our performance of ours(two threads) and ours(one thread) on the Replica Dataset in Table 1. The result of ours(two threads) is slightly inferior compared with ours(one thread), while it still outperforms another RGB method.\n\nIn Table 7, we report the number of pixels and iterations used in different datasets. Our method keeps the same setting for all datasets, while NICE-SLAM requires more pixels and iterations on the real-world dataset.\n\nA.6 MESH EXTRACTION\n\nOur implicit map representation contains a hierarchical feature volume and an MLP decoder. We can compute occupancy for each point in the 3D space from our implicit map representation. Our method can easily predict the occupancy for unseen points. We use the marching cubes algorithm (Lorensen & Cline, 1987) to create a mesh for visualization and evaluation.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nA.7 MORE VISUALIZATION RESULT\n\nTo see more shape details on Replica (Straub et al., 2019), we also render these extracted meshes at a close viewpoint and compare our results with those from iMAP (Sucar et al., 2021) and DROIDSLAM (Teed & Deng, 2021) in Figure 4. It is clear that our method recovers more shape details and can fill in unobserved regions with the neural implicit map.\n\nWe provides visualization of all reconstructed scenes in Replica (Straub et al., 2019) in Figure 5 and 6.\n\nIn Figure 7, we show two mesh reconstructions on the TUM RGB-D dataset. Our method could recover high-quality mesh due to accurate camera poses and the smaller voxel size.\n\nIn Figure 8, we run our method on pikachu blue dress1 camera3 in (Shrestha et al., 2022) with different voxel size. We show the results when the smallest volume size is 8, 2, 0.5cm, respectively. Our method could recover fine detail of the small object if we increase the resolution of volume.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nM A\nL S\n\n-\n\nD\n\nI\n\nO R\nD\n\nP A\nM\n\ni\n\ns r\nu O\n\nh\n\nt\n\nu r\n\nT\n\n- d\nn u\no r\n\nG\n\n(a) Hole-Filling\n\n(b) Fine-Geometry\n\nFigure 4: Rendering of the reconstructed mesh from the close viewpoint. Our results are more complete and accurate than those from iMAP and DROID-SLAM.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\n0 -\ne c\ni f\nf o\n\n1 -\ne c\ni f\nf o\n\n3 -\ne c\ni f\nf o\n\n4 -\ne c\ni f\nf o\n\nOurs\n\nGround Truth\n\nFigure 5: Visual comparison of the reconstructed meshes on the Replica datasets.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\n4 -\ne c\ni f\nf o\n\n0 -\nm o\no r\n\n1 -\nm o\no r\n\n2 -\nm o\no r\n\nOurs\n\nGround Truth\n\nFigure 6: Visual comparison of the reconstructed meshes on the Replica datasets.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nk s\ne d\n\n1 r\nf\n\nz y\nx\n\n2 r\nf\n\nOurs\n\nNICE-SLAM\n\nFigure 7: The reconstructed mesh on TUM-RGBD dataset.\n\n8cm\n\n2cm\n\n0.5cm\n\nReconstruction Mesh\n\nInput Image\n\nFigure 8: The reconstructed mesh of pikachu blue dress1 camera3 on MeshMVS (Shrestha et al., 2022) dataset using different size of voxel.\n\n21",
    "reference": "# Summary Of The Paper\n\nThis paper presents a dense SLAM method using neural implicit model as map representation. Compared to previous methods, the proposed method does not need depth maps as input. A hierarchical feature volume is used to facilitate map decoding. A photometric warping loss is proposed to be combined with the photometric rendering loss for optimizing the scene geoemtry and camera poses. Evaluation on both synthetic and real datasets show comparable or better results compared to previous methods.\n\n# Strength And Weaknesses\n\nStreangth\n- The photometric warping loss is novel and seems to be the main source of the boosted performance compared to NICE-SLAM.\n- The evaluations are thorough and consider both map and camera pose accuracies and cover both synthetic and real datasets.\n\nWeakness\n- Compared to NICE-SLAM, the main differences are (1) using different number of levels for hierarchical feature volume (2) adding the photometric warping loss. However, the camera pose accuracies in Table 1 are ~10 times better than NICE-SLAM. This is quite surprising, especially considering that the proposed method does not consume depth maps. It might be helpful to add more discussions accordingly.\n- In Eq (6), why are the warped images comapred to the rendered images instead of to the real images? This looks a bit strange and more explaination should be added.\n- The way of getting the visibility mask on page 5 is a bit strange. The used method does not handle occlusion at all.\n- In 3.3, the naive way of finding the global keyframes are not scalable as it needs to traverse all the previous keyframes for each new frame. This will never work in a real SLAM system.\n- Please add units to Table 2 and Table 3.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well presented and is easy to follow.\n\n# Summary Of The Review\n\nDespite only moderate modifications and enhancement to NICE-SLAM, the presented work delivers surprisingly good results in some of the evaluations. I feel those parts need to be better supported by further experiments such as more dedicated ablation studies. Compared to the existing works, the overall novelty of this work is marginal. I therefore tend to reject the paper.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nLOSSY COMPRESSION WITH GAUSSIAN DIFFUSION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe consider a novel lossy compression approach based on unconditional diffusion generative models, which we call DiffC. Unlike modern compression schemes which rely on transform coding and quantization to restrict the transmitted information, DiffC relies on the efficient communication of pixels corrupted by Gaussian noise. We implement a proof of concept and find that it works surprisingly well despite the lack of an encoder transform, outperforming the state-of-the-art generative compression method HiFiC on ImageNet 64x64. DiffC only uses a single model to encode and denoise corrupted pixels at arbitrary bitrates. The approach further provides support for progressive coding, that is, decoding from partial bit streams. We perform a rate-distortion analysis to gain a deeper understanding of its performance, providing analytical results for multivariate Gaussian data as well as theoretic bounds for general distributions. Furthermore, we prove that a flow-based reconstruction achieves a 3 dB gain over ancestral sampling at high bitrates.\n\n1\n\nINTRODUCTION\n\nWe are interested in the problem of lossy compression with perfect realism. As in typical lossy compression applications, our goal is to communicate data using as few bits as possible while simultaneously introducing as little distortion as possible. However, we additionally require that reconstructions ˆX have (approximately) the same marginal distribution as the data, ˆX ∼ X. When this constraint is met, reconstructions are indistinguishable from real data or, in other words, appear perfectly realistic. Lossy compression with realism constraints is receiving increasing attention as more powerful generative models bring a solution ever closer within reach. Theoretical arguments (Blau and Michaeli, 2018; 2019; Theis and Agustsson, 2021; Theis and Wagner, 2021) and empirical results (Tschannen et al., 2018; Agustsson et al., 2019; Mentzer et al., 2020) suggest that generative compression approaches have the potential to achieve significantly lower bitrates at similar perceived quality than approaches targeting distortions alone.\n\nThe basic idea behind existing generative compression approaches is to replace the decoder with a conditional generative model and to sample reconstructions. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020)—also known as score-based generative models (Song et al., 2021; Dockhorn et al., 2022)—are a class of generative models which have recently received a lot of attention for their ability to generate realistic images (e.g., Dhariwal and Nichol, 2021; Nichol et al., 2021; Ho et al., 2022; Kim and Ye, 2022; Ramesh et al., 2022). While generative compression work has mostly relied on generative adversarial networks (Goodfellow et al., 2014; Tschannen et al., 2018; Agustsson et al., 2019; Mentzer et al., 2020; Gao et al., 2021), Saharia et al. (2021) provided evidence that this approach may also work well with diffusion models by using conditional diffusion models for JPEG artefact removal.\n\nIn Section 3, we describe a novel lossy compression approach based on diffusion models. Unlike typical generative compression approaches, our approach relies on an unconditionally trained generative model. Modern lossy compression schemes comprise at least an encoder transform, a decoder transform, and an entropy model (Ball ́e et al., 2021; Yang et al., 2022) where our approach only uses a single model. Surprisingly, we find that this simple approach can work well despite lacking an encoder transform—instead, we add isotropic Gaussian noise directly to the pixels (Section 5). By using varying degrees of Gaussian noise, the same model can further be used to communicate data at arbitrary bitrates. The approach is naturally progressive, that is, reconstructions can be generated from an incomplete bitstream.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTo better understand why the approach works well, we perform a rate-distortion analysis in Section 4. We find that isotropic Gaussian noise is generally not optimal even for the case of Gaussian distributed data and mean-squared error (MSE) distortion. However, we also observe that isotropic noise is close to optimal. We further prove that a reconstruction based on the probability flow ODE (Song et al., 2021) cuts the distortion in half at high bit-rates when compared to ancestral sampling from the diffusion model.\n\nWe will use capital letters such as X to denote random variables, lower-case letters such as x to denote corresponding instances and non-bold letters such as xi for scalars. We reserve log for the logarithm to base 2 and will use ln for the natural logarithm.\n\n2 RELATED WORK\n\nMany previous papers observed connections between variational autoencoders (VAEs; Kingma and Welling, 2014; Rezende et al., 2014) and rate-distortion optimization (e.g., Theis et al., 2017; Ball ́e et al., 2017; Alemi et al., 2018; Brekelmans et al., 2019; Agustsson and Theis, 2020). More closely related to our approach, Agustsson and Theis (2020) turned a VAE into a practical lossy compression scheme by using dithered quantization to communicate uniform samples. Similarly, our scheme relies on random coding to communicate Gaussian samples and uses diffusion models, which can be viewed as hierarchical VAEs with a fixed encoder.\n\nHo et al. (2020) considered the rate-distortion performance of an idealized but closely related compression scheme based on diffusion models. In contrast to Ho et al. (2020), we are considering distortion under a perfect realism constraint and provide the first theoretical and empirical results demonstrating that the approach works well. Importantly, random coding is known to provide little benefit and can even hurt performance when only targeting a rate-distortion trade-off (Agustsson and Theis, 2020; Theis and Agustsson, 2021). On the other hand, random codes can perform significantly better than deterministic codes when realism constraints are considered (Theis and Agustsson, 2021). Ho et al. (2020) contemplated the use of minimal random coding (MRC; Havasi et al., 2019) to encode Gaussian samples. However, MRC only communicates an approximate sample. In contrast, we consider schemes which communicate an exact sample, allowing us to avoid issues such as error propagation. Finally, we use an upper bound instead of a lower bound as a proxy for the coding cost, which guarantees that our estimated rates are achievable.\n\nWhile modern lossy compression schemes rely on transform coding, very early work by Roberts (1962) experimented with dithered quantization applied directly to grayscale pixels. Roberts (1962) found that dither was perceptually more pleasing than the banding artefacts caused by quantization. Similarly, we apply Gaussian noise directly to pixels but additionally use a powerful generative model for entropy coding and denoising.\n\nAnother line of work in compression explored anisotropic diffusion to denoise and inpaint missing pixels (Gali ́c et al., 2008). This use of diffusion is fundamentally different from ours. Anisotropic diffusion has the effect of smoothing an individual image whereas the diffusion processes considered in this paper are increasing high spatial frequency content of individual images but have a smoothing effect on the distribution over images.\n\nYan et al. (2021) claimed that under a perfect realism constraint, the best achievable rate is R(D/2), where R is the rate-distortion function (Eq. 7). It was further claimed that optimal performance can be achieved by optimizing an encoder for distortion alone while ignoring the realism constraint and using ancestral sampling at the decoder. Contrary to these claims, we show that our approach can exceed this performance and achieve up to 3 dB better signal-to-noise ratio at the same rate (Figure 2). The discrepancy can be explained by Yan et al. (2021) only considering deterministic codes whereas we allow random codes with access to shared randomness. In random codes, the communicated bits not only depend on the data but are a function of the data and an additional source of randomness shared between the encoder and the decoder (typically implemented by a pseudo-random number generator). Our results are in line with the findings of Theis and Agustsson (2021) who showed on a toy example that shared randomness can lead to significantly better performance in the one-shot setting, and those of Zhang et al. (2021) and Wagner (2022) who studied the rate-distortion-perception function (Blau and Michaeli, 2019) of normal distributions. In this paper, we provide additional results for the multivariate Gaussian case (Section 4.2).\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nA\n\nX\n\nDiffC-F/A HiFiC BPG JPEG\n\nB\n\nq(zs | zs+1, x)\n\np(zs | zs+1)\n\nZT ZT −1\n\nZt+1 Zt Zt−1\n\nZ1\n\nˆX\n\nFigure 1: A: A visualization of lossy compression with unconditional diffusion models. B: Bitrates (bits per pixel; black) and PSNR scores (red) of various approaches including JPEG (4:2:0, headerless) applied to images from the validation set of ImageNet 64x64. For more examples see Appendix I.\n\nAn increasing number of neural compression approaches is targeting realism (e.g., Tschannen et al., 2018; Agustsson et al., 2019; Mentzer et al., 2020; Gao et al., 2021; Lytchier et al., 2021; Zeghidour et al., 2022). However, virtually all of these approaches rely on transform coding combined with distortions based on VGG (Simonyan and Zisserman, 2015) and adversarial losses (Goodfellow et al., 2014). In contrast, we use a single unconditionally trained diffusion model (Sohl-Dickstein et al., 2015). Unconditional diffusion models have been used for lossless compression with the help of bitsback coding (Kingma et al., 2021) but bits-back coding by itself is unsuitable for lossy compression. We show that significant bitrate savings can be achieved compared to lossless compression even by allowing imperceptible distortions (Fig. 3).\n\n3 LOSSY COMPRESSION WITH DIFFUSION\n\nThe basic idea behind our compression approach is to efficiently communicate a corrupted version of the data,\n\n(cid:113)\n\nZt =\n\n1 − σ2\n\nt X + σtU where U ∼ N (0, I),\n\n(1)\n\nfrom the sender to the receiver, and then to use a diffusion generative model to generate a reconstruction. Zt can be viewed as the solution to a Gaussian diffusion process given by the stochastic differential equation (SDE)\n\ndZt = −\n\n1 2\n\nβtZt dt + (cid:112)βtdWt, Z0 = X, where σ2\n\nt = 1 − e− (cid:82) t\n\n0 βτ dτ\n\n(2)\n\nand Wt is Brownian motion. Diffusion generative models try to invert this process by learning the conditional distributions p(zs | zt) for s < t (Song et al., 2021). If s and t are sufficiently close, then this conditional distribution is approximately Gaussian. We refer to Sohl-Dickstein et al. (2015), Ho et al. (2020), and Song et al. (2021) for further background on diffusion models.\n\nNoise has a negative effect on the performance of typical compression schemes (Al-Shaykh and Mersereau, 1998). However, Bennett and Shor (2002) proved that it is possible to communicate an instance of Zt using not much more than I[X, Zt] bits. Note that this mutual information decreases as the level of noise increases. Li and El Gamal (2018) described a more concrete random coding approach for communicating an exact sample of Zt (Appendix A). An upper bound was provided for its coding cost, namely\n\nI[X, Zt] + log(I[X, Zt] + 1) + 5\n\n(3)\n\nbits. Notice that the second and third term become negligible when the mutual information is sufficiently large. If the sender and receiver do not have access to the true marginal of Zt but instead\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nassume the marginal distribution to be pt, the upper bound on the coding cost becomes (Theis and Yosri, 2022)\n\nCt + log(Ct + 1) + 5 where Ct = EX[DKL[q(zt | X) (cid:107) pt(zt)]]\n\n(4)\n\nand q is the distribution of Zt given X, which in our case is Gaussian. In practice, the coding cost can be significantly closer to Ct than the upper bound (Theis and Yosri, 2022; Flamich et al., 2022).\n\nWe refer to Theis and Yosri (2022) for an introduction to the problem of efficient sample communication—also known as reverse channel coding—as well as a discussion of practical implementations of the approach of Li and El Gamal (2018). To follow the results of this paper, the reader only needs to know that an exact sample of a distribution q can be communicated with a number of bits which is at most the bound given in Eq. 4, and that this is possible even when q is continuous. The bound above is analogous to the well-known result that the cost of entropy coding can be bounded in terms of H + 1, where H is a cross-entropy (e.g., Cover and Thomas, 2006). However, to provide some intuition for reverse channel coding, we briefly describe the high-level idea. Candidates Z1 t , . . . are generated by drawing samples from pt. The encoder then selects one of the candidates with index N ∗ in a manner similar to rejection sampling such that ZN ∗ t ∼ q. Since the candidates are independent of the data, they can be generated by both the sender and receiver (for example, using a pseudo-random number generator with the same random seed) and only the selected candidate’s index N ∗ needs to be communicated. The entropy of N ∗ is bounded by Eq. 4. Further details and pseudocode are provided in Appendix A\n\nt , Z2\n\nt , Z3\n\nUnfortunately, Gaussian diffusion models do not provide us with tractable marginal distributions pt. Instead, they give us access to conditional distributions p(zs | zs+1) and assume pT is isotropic Gaussian. This suggests a scheme where we first transmit an instance of ZT and then successively refine the information received by the sender by transmitting an instance of Zs given Zs+1 until Zt is reached. This approach incurs an overhead for the coding cost of each conditional sample (which we consider in Fig. 10 of Appendix I). Alternatively, we can communicate a Gaussian sample from the joint distribution q(zT :t | X) directly while assuming a marginal distribution p(zT :t). This achieves a coding cost upper bounded by Eq. 4 where\n\nCt = E [DKL[q(zT | X) (cid:107) pT (zT )]] + (cid:80)T −1\n\ns=1\n\nE [DKL[q(zs | Zs+1, X) (cid:107) p(zs | Zs+1)]] .\n\n(5)\n\nReverse channel coding still poses several unsolved challenges in practice. In particular, the scheme proposed by Li and El Gamal (2018) is computationally expensive though progress on more efficient schemes is being made (Agustsson and Theis, 2020; Theis and Yosri, 2022; Flamich et al., 2022). In the following we will mostly ignore issues of computational complexity and instead focus on the question of whether the approach described above is worth considering at all. After all, it is not immediately clear that adding isotropic Gaussian noise directly to the data would limit information in a useful way.\n\nWe will consider two alternatives for reconstructing data given Zt. First, we will consider ancestral sampling, ˆX ∼ p(x | Zt), which corresponds to simulating the SDE in Eq. 2 in reverse (Song et al., 2021). Second, we will consider a deterministic reconstruction which instead tries to reverse the ODE\n\n(cid:18)\n\ndzt =\n\n−\n\n1 2\n\nβtzt −\n\n1 2\n\n(cid:19)\n\nβt∇ ln pt(zt)\n\ndt.\n\n(6)\n\nMaoutsa et al. (2020) and Song et al. (2021) showed that this “probability flow” ODE produces the same trajectory of marginal distributions pt as the Gaussian diffusion process in Eq. 2 and that it can be simulated using the same model of ∇ ln pt(zt). We will refer to these alternatives as DiffC-A when ancestral sampling is used and DiffC-F when the flow-based reconstruction is used.\n\n4 A RATE-DISTORTION ANALYSIS\n\nIn this section we try to understand the performance of DiffC from a rate-distortion perspective. This will be achieved by considering the Gaussian case where optimal rate-distortion trade-offs can be computed analytically and by providing bounds on the performance in the general case. Throughout this paper, we measure distortion in terms of squared error. For our theoretical analysis we will further assume that the diffusion model has learned the data distribution perfectly.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nThe (information) rate-distortion function is given by\n\nR(D) = inf ˆX I[X, ˆX]\n\nsubject to E[(cid:107)X − ˆX(cid:107)2] ≤ D.\n\n(7)\n\nIt measures the smallest achievable bitrate for a given level of distortion and decreases as D increases1.\n\nThe rate as defined above does not make any assumptions on the marginal distribution of the reconstructions. However, here we demand perfect realism, that is, ˆX ∼ X. To achieve this constraint, a deterministic encoder requires a higher bitrate of R(D/2) (Blau and Michaeli, 2019; Theis and Agustsson, 2021). As we will see below, lower bitrates can be achieved using random codes as in our diffusion approach. Nevertheless, R(D/2) serves as an interesting benchmark as most existing codecs use deterministic codes, that is, the bits received by the decoder are solely determined by the data.\n\nFor an M -dimensional Gaussian data source whose covariance has eigenvalues λi, the rate-distortion function is known to be (Cover and Thomas, 2006)\n\nR∗(D) = 1 2\n\n(cid:80)\n\ni log(λi/Di) where Di = min(λi, θ)\n\n(8)\n\nfor some threshold θ chosen such that D = (cid:80) positive eigenvalues, we have constant Di = θ = D/M .\n\ni Di. For sufficiently small distortion D and assuming\n\n4.1 STANDARD NORMAL DISTRIBUTION\n\nAs a simple first example, consider a standard normal distribution X ∼ N (0, 1). Using ancestral sampling, the reconstruction becomes\n\n(cid:112)\n\nˆX =\n\n1 − σ2Z + σV where Z =\n\n(cid:112)\n\n1 − σ2X + σU ,\n\n(9)\n\nU , V ∼ N (0, 1) and we have dropped the dependence on t to reduce clutter. The distortion and rate in this case are easily calculated to be\n\nD = E[(X − ˆX)2] = 2σ2,\n\nI[X, Z] = − log σ =\n\n1 2\n\nlog\n\n2 D\n\n= R∗(D/2).\n\n(10)\n\nThis matches the performance of an optimal deterministic code. However, Z already has the desired standard normal distribution and adding further noise to it did nothing to increase the realism or reduce the distortion of the reconstruction. The flow-based reconstruction instead yields dZt = 0 and ˆX = Z (by inserting the standard normal for pt in Eq. 6), resulting in the smaller distortion\n\nD = E[(X − ˆX)2] = E[(X − Z)2] = 2 − 2\n\n(cid:112)\n\n1 − σ2.\n\n(11)\n\n4.2 MULTIVARIATE GAUSSIAN\n\n1 − σ2X + σU where U ∼ N (0, I). Assume λi Next, let us consider X ∼ N (0, Σ) and Z = are the eigenvalues of Σ. Since both the squared reconstruction error and the mutual information between X and Z are invariant under rotations of X, we can assume the covariance to be diagonal. Otherwise we just rotate X to diagonalize the covariance matrix without affecting the results of our analysis. If ˆX ∼ P (X | Z), we get the distortion and rate (Appendix C)\n\n√\n\nD = E[(cid:107)X − ˆX(cid:107)2] = 2 (cid:80)\n\n(12) where ̃Di = λiσ2/(σ2 + λi − λiσ2). That is, the performance is generally worse than the performance achieved by the best deterministic encoder. We can modify the diffusion process to improve the rate-distortion performance of ancestral sampling. Namely, let Vi ∼ N (0, 1),\n\ni log(λi/ ̃Di) ≥ R∗(D/2).\n\nI[X, Z] = 1 2\n\n ̃Di,\n\n(cid:80)\n\ni\n\n(cid:113)\n\nZi =\n\n1 − γ2\n\ni Xi + γi\n\n(cid:112)\n\nλiUi,\n\nˆXi =\n\n(cid:113)\n\n1 − γ2\n\ni Zi + γi\n\n(cid:112)\n\nλiVi,\n\n(13)\n\nwhere γ2 i = min(1, θ/λi) for some θ. This amounts to using a different noise schedule along different principal directions instead of adding the same amount of noise in all directions. For natural images,\n\n1The bitrate given by an information rate-distortion may only be achievable asymptotically by encoding many data points jointly. To keep our discussion focused, we ignore any potential overhead incurred by one-shot coding and use mutual information as a proxy for the rate achieved in practice.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nA\n\n]\n\nB d\n\n[\n\nR N\nS\n\n25\n\n20\n\n15\n\n10\n\n5\n\n0\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\nRate, I[X, Z] [bpd]\n\nB\n\n]\n\nB d\n\n[\n\nR N\nS\n\n25\n\n20\n\n15\n\n10\n\n5\n\n0\n\n0\n\n64\n\n128 192 256\n\nComponent\n\nDiffC-A DiffC-F DiffC-A* DiffC-F* P-A P-F\n\nFigure 2: A: Rate-distortion curves for a Gaussian source fitted to 16x16 image patches extracted from ImageNet 64x64. Isotropic noise performs nearly as well as the optimal noise (dashed). As an additional point of comparison, we include pink noise (P) matching the covariance of the data distribution. The curve of DiffC-A* corresponds to R∗(D/2). A flow-based reconstruction yields up to 3 dB better signal-to-noise ratio (SNR). B: SNR broken down by principal component. The level of noise here is fixed to yield a rate of approximately 0.391 bits per dimension for each type of noise. Note that the SNR of DiffC-A* is zero for over half of the components.\n\nthe modified schedule destroys information in high-frequency components more quickly (Fig. 2B) and for Gaussian data sources again matches the performance of the best deterministic code,\n\nD = 2 (cid:80)\n\ni λiγ2\n\ni = 2 (cid:80)\n\ni Di,\n\nI[X, Z] = − (cid:80)\n\ni log γi = 1\n\n2\n\n(cid:80)\n\ni log(λi/Di) = R∗(D/2)\n\n(14)\n\nwhere Di = λiγ2 i = min(λi, θ). Still better performance can be achieved via flow-based reconstruction. Here, isotropic noise is again suboptimal and the optimal noise for a flow-based reconstruction is given by (Appendix D)\n\nZi = αiXi +\n\n(cid:113)\n\n(cid:112)\n\n1 − α2 i\n\nλiUi, where αi =\n\n(cid:18)(cid:113)\n\nλ2\n\ni + θ2 − θ\n\n(cid:19)\n\n/λi\n\n(15)\n\nfor some θ ≥ 0. Z already has the desired distribution and we can set ˆX = Z.\n\nWe will refer to the two approaches using optimized noise as DiffC-A* and DiffC-F*, respectively, though strictly speaking these types of noise may no longer correspond to diffusion processes. Figure 2A shows the rate-distortion performance of the various noise schedules and reconstructions on the example of a 256-dimensional Gaussian fitted to 16x16 grayscale image patches extracted from 64x64 downsampled ImageNet images (van den Oord et al., 2016). Here, SNR = 10 log10(2 · E[(cid:107)X(cid:107)2]) − 10 log10(E[(cid:107)X − ˆX(cid:107)2]).\n\n4.3 GENERAL DATA DISTRIBUTIONS\n\nConsidering more general source distributions, our first result bounds the rate of DiffC-A*.\n\nTheorem 1. Let X : Ω → RM be a random variable with finite differential entropy, zero mean and covariance diag(λ1, . . . , λM ). Let U ∼ N (0, I) and define\n\nZi =\n\n(cid:113)\n\n1 − γ2\n\ni Xi + γi\n\n(cid:112)\n\nλiUi,\n\nˆX ∼ P (X | Z).\n\n(16)\n\nwhere γ2 i = min(1, θ/λi) for some θ. Further, let X∗ be a Gaussian random variable with the same first and second-order moments as X and let Z∗ be defined analogously to Z but in terms of X∗. Then if R is the rate-distortion function of X and R∗ is the rate-distortion function of X∗,\n\nI[X, Z] ≤ R∗(D/2) − DKL[PZ (cid:107) PZ∗ ] ≤ R(D/2) + DKL[PX (cid:107) PX∗ ] − DKL[PZ (cid:107) PZ∗ ]\n\n(17)\n\nwhere D = E[(cid:107)X − ˆX(cid:107)2].\n\nProof. See Appendix E.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Top images visualize messages communicated at the estimated bitrate (bits per pixel) shown in black. The bottom row shows reconstructions produced by DiffC-F and corresponding PSNR values are shown in red.\n\nIn line with expectations, this result implies that when X is approximately Gaussian, the rate of DiffC-A* is not far from the rate of the best deterministic encoder, R(D/2). It further implies that the rate is close to R(D/2) in the high bitrate regime if the differential entropy of X is finite. This can be seen by noting that the second KL divergence will approach the first KL divergence as the rate increases, since PZ∗ = PX∗ and the distribution of Z will be increasingly similar to X.\n\nOur next result compares the error of DiffC-F with DiffC-A’s at the same bitrate. For simplicity, we assume that X has a smooth density and further consider the following measure of smoothness,\n\nG = E\n\n(cid:104)\n\n(cid:107)∇ ln p(X)(cid:107)2(cid:105)\n\n.\n\n(18)\n\nAmong distributions with a continuously differentiable density and unit variance, the standard normal distribution minimizes G and achieves G = 1. For comparison, the Laplace distribution has G = 2. (Alternatively, imagine a sequence of smooth approximations converging to the Laplace density.) For discrete data such as RGB images, we may instead consider the distribution of pixels with an imperceptible amount of Gaussian noise added to it (see also Fig. 5 in Appendix F).\n\nTheorem 2. Let X : Ω → RM have a smooth density p with finite G (Eq. 18). Let Zt be defined as in Eq. 1, ˆXA ∼ P (X | Zt) and let ˆXF = ˆZ0 be the solution to Eq. 6 with Zt as initial condition. Then\n\nE[(cid:107) ˆXF − X(cid:107)2] E[(cid:107) ˆXA − X(cid:107)2]\n\n=\n\n1 2\n\nlim σt→0\n\n(19)\n\nProof. See Appendix F.\n\nThis result implies that in the limit of high bitrates, the error of a flow-based reconstruction is only half that of the the reconstruction obtained with ancestral sampling from a perfect model. This is consistent with Fig. 2, where we can observe an advantage of roughly 3 dB of DiffC-F over DiffC-A. Finally, we provide conditions under which a flow-based reconstruction is provably the best reconstruction from input corrupted by Gaussian noise.\n\nTheorem 3. Let X = QS where Q is an orthogonal matrix and S : Ω → RM is a random vector with smooth density and Si ⊥⊥ Sj for all i (cid:54)= j. Define Zt as in Eq. 1. If ˆXF = ˆZ0 is the solution to the ODE in Eq. 6 given Zt as initial condition, then\n\nE[(cid:107) ˆXF − X(cid:107)2] ≤ E[(cid:107) ˆX(cid:48) − X(cid:107)2]\n\n(20)\n\nfor any ˆX(cid:48) with ˆX(cid:48) ⊥⊥ X | Zt which achieves perfect realism, ˆX(cid:48) ∼ X.\n\nProof. See Appendix G.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nD F\n\nI\n\n60\n\n50\n\n40\n\n30\n\n20\n\n10\n\n0\n\n0\n\n]\n\nB d\n\n[\n\nR N\nS P\n\n40\n\n35\n\n30\n\n25\n\n20\n\n15\n\n10\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\nBits per pixel\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\nBits per pixel\n\nBPG HiFiC HiFiC (pretrained) DiffC-F DiffC-A\n\nFigure 4: A comparison of DiffC with BPG and the GAN-based neural compression method HiFiC in terms of FID and PSNR on ImageNet 64x64.\n\n5 EXPERIMENTS\n\nAs a proof of concept, we implemented DiffC based on VDM2 (Kingma et al., 2021). VDM is a diffusion model which was optimized for log-likelihood (i.e., lossless compression) but not for perceptual quality. This suggests VDM should work well in the high bitrate regime but not necessarily at lower bitrates. Nevertheless, we find that we achieve surprisingly good performance across a wide range of bitrates. We used exactly the same network architecture and training setup as Kingma et al. (2021) except with a smaller batch size of 64 images and training our model for only 1.34M updates (instead of 2M updates with a batch size of 512) due to resource considerations. We used 1000 diffusion steps.\n\n5.1 DATASET, METRICS, AND BASELINES\n\nWe used the downsampled version of the ImageNet dataset (Deng et al., 2009) (64x64 pixels) first used by van den Oord et al. (2016). The test set of ImageNet is known to contain many duplicates and to overlap with the training set (Kolesnikov et al., 2019). For a more meaningful evaluation (especially when comparing to non-neural baselines), we removed 4952 duplicates from the validation set as well as 744 images also occuring in the training set (based on SHA-256 hashes of the images). On this subset, we measured a negative ELBO of 3.48 bits per dimension for our model.\n\nWe report FID (Heusel et al., 2017) and PSNR scores to quantify the performance of the different approaches. As is common in the compression literature, in this section we calculate a PSNR score for each image before averaging. For easier comparison with our theoretical results, we also offer PSNR scores calculated from the average MSE (Appendix I) although the numbers do not change markedly. When comparing bitrates between models, we used estimates of the upper bound given by Eq. 4 for DiffC.\n\nWe compare against BPG (Bellard, 2018), a strong non-neural image codec based on the HEVC video codec which is known for achieving good rate-distortion results. We also compare against HiFiC (Mentzer et al., 2020), which is the state-of-the-art generative image compression model in terms of visual quality on high-resolution images. The approach is optimized for a combination of LPIPS (Zhang et al., 2018), MSE, and an adversarial loss (Goodfellow et al., 2014). The architecture of HiFiC is optimized for larger images and uses significant downscaling. We found that adapting the architecture of HiFiC slightly by making the last/first layer of the encoder/decoder have stride 1 instead of stride 2 improves FID on ImageNet 64x64 compared to the publicly available model. In addition to training the model from scratch, we also tried initializing the non-adapted filters from the public model and found that this improved results slightly. We trained 5 HiFiC models targeting 5 different bitrates.\n\n2https://github.com/google-research/vdm\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n5.2 RESULTS\n\nWe find that DiffC-F gives perceptually pleasing results even at extremely low bitrates of around 0.2 bits per pixel (Fig. 3). Reconstructions are also still perceptually pleasing when the PSNR is relatively low at around 22 dB (e.g., compare to BPG in Fig. 1B). We further find that at very low bitrates, HiFiC produces artefacts typical for GANs while we did not observe similar artefacts with DiffC. Similar conclusions can be drawn from our quantitative comparison, with DiffC-F significantly outperforming HiFiC in terms of FID. FID scores of DiffC-A were only slightly worse (Fig. 4A).\n\nAt high bitrates, DiffC-F achieves a PSNR roughly 2.4 dB higher than DiffC-A. This is line with our theoretical predictions (3 dB) considering that the diffusion model only approximates the true distribution. PSNR values of DiffC-F and DiffC-A both exceed those of HiFiC and BPG, suggesting that Gaussian diffusion works well in a rate-distortion sense even for highly non-Gaussian distributions (Fig. 4B). Additional results are provided in Appendix I, including results for progressive coding and HiFiC trained for MSE only.\n\n6 DISCUSSION\n\nWe presented and analyzed a new lossy compression approach based on diffusion models. This approach has the potential to greatly simplify lossy compression with realism constraints. Where typical generative approaches use an encoder, a decoder, an entropy model, an adversarial model and another model as part of a perceptual distortion loss, and train multiple sets of models targeting different bitrates, DiffC only uses a single unconditionally trained diffusion model. The fact that adding Gaussian noise to pixels achieves great rate-distortion performance raises interesting questions about the role of the encoder transform in lossy compression. Nevertheless, we expect further improvements are possible in terms of perceptual quality by applying DiffC in a latent space.\n\nApplying DiffC in a lower-dimensional transform space would also help to reduce its computational cost (Vahdat et al., 2021; Rombach et al., 2021; Gu et al., 2021; Pandey et al., 2022). The high computational cost of DiffC makes it impractical in its current form. Generating a single image with VDM requires many diffusion steps, each involving the application of a deep neural network. However, speeding up diffusion models is a highly active area of research (e.g., Watson et al., 2021; Vahdat et al., 2021; Jolicoeur-Martineau et al., 2021; Kong and Ping, 2021; Salimans and Ho, 2022; Zhang and Chen, 2022). For example, Salimans and Ho (2022) were able to reduce the number of diffusion steps from 1000 to around 4 at comparable sample quality. The computational cost of communicating a sample using the approach of Li and El Gamal (2018) grows exponentially with the coding cost. However, reverse channel coding is another active area of research (e.g., Havasi et al., 2019; Agustsson and Theis, 2020; Flamich et al., 2020) and much faster methods already exist for low-dimensional Gaussian distributions (Theis and Yosri, 2022; Flamich et al., 2022). Our work offers strong motivation for further research into more efficient reverse channel coding schemes.\n\nAs mentioned in Section 3, reverse channel coding may be applied after each diffusion step to send a sample of q(zt | Zt+1, X), or alternatively to the joint distribution q(zT :t | X). The former approach has the advantage of lower computational cost due to the exponential growth with the coding cost. Furthermore, the model’s score function only needs to be evaluated once per diffusion step to compute a conditional mean while the latter approach requires many more evaluations (one for each candidate considered by the reverse channel coding scheme). Fig. 10 shows that this approach—which is already much more practical—still significantly outperforms HiFiC. Another interesting avenue to consider is replacing Gaussian q(zt | Zt+1, X) with a uniform distribution, which can be simulated very efficiently (e.g., Zamir and Feder, 1996; Agustsson and Theis, 2020).\n\nWe provided an initial theoretical analysis of DiffC. In particular, we analyzed the Gaussian case and proved that DiffC-A* performs well when either the data distribution is close to Gaussian or when the bitrate is high. In particular, the rate of DiffC-A* approaches R(D/2) at high birates. We further proved that DiffC-F can achieve 3 dB better SNR at high bitrates compared to DiffC-A. Taken together, these results suggest that R(D) may be achievable at high bitrates where current approaches based on nonlinear transform coding can only achieve R(D/2). However, many theoretical questions have been left for future research. For instance, how does the performance of DiffC-A differ from DiffC-A*? And can we extend Theorem 3 to prove optimality of a flow-based reconstruction from noisy data for a broader class of distributions?\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nOur appendix provides extensive proofs of the claims made in our paper. Every effort has been made to make explicit the assumptions in our claims. The models used in our empirical results (VDM, HiFiC) are based on open source code. We further provide code to reproduce the results of Fig. 2.\n\nETHICS STATEMENT\n\nLossy compression achieving perfect realism produces reconstructions which are indistinguishable from real data and which hide a loss of information. This property is desirable in applications such as video streaming for entertainment purposes but can be problematic in applications such as surveillance, medical imaging, or document compression where the image content influences critical decisions. The availability of better generative compression methods increases the risk of misuse in these areas.\n\nREFERENCES\n\nE. Agustsson and L. Theis. Universally Quantized Neural Compression. In Advances in Neural\n\nInformation Processing Systems 33, 2020.\n\nE. Agustsson, M. Tschannen, F. Mentzer, R. Timofte, and L. V. Gool. Generative adversarial networks for extreme learned image compression. In Proceedings of the IEEE International Conference on Computer Vision, pages 221–231, 2019.\n\nO. Al-Shaykh and R. Mersereau. Lossy compression of noisy images. IEEE Transactions on Image\n\nProcessing, 7(12):1641–1652, 1998. doi: 10.1109/83.730376.\n\nA. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken elbo. In\n\nInternational Conference on Machine Learning, pages 159–168. PMLR, 2018.\n\nJ. Ball ́e, V. Laparra, and E. P. Simoncelli. End-to-end Optimized Image Compression. In International\n\nConference on Learning Representations, 2017.\n\nJ. Ball ́e, P. A. Chou, D. Minnen, S. Singh, N. Johnston, E. Agustsson, S. J. Hwang, and G. Toderici. Nonlinear transform coding. IEEE Journal of Selected Topics in Signal Processing, 15(2):339–353, 2021. doi: 10.1109/JSTSP.2020.3034501.\n\nF. Bellard. BPG Image format. https://bellard.org/bpg/, 2018.\n\nC. H. Bennett and P. W. Shor. Entanglement-Assisted Capacity of a Quantum Channel and the\n\nReverse Shannon Theorem. IEEE Trans. Info. Theory, 48(10), 2002.\n\nY. Blau and T. Michaeli. The perception-distortion tradeoff. In IEEE/CVF Conference on Computer\n\nVision and Pattern Recognition, pages 6228–6237, 2018.\n\nY. Blau and T. Michaeli. Rethinking lossy compression: The rate-distortion-perception tradeoff. In\n\nInternational Conference on Machine Learning, 2019.\n\nR. Brekelmans, D. Moyer, A. Galstyan, and G. Ver Steeg. Exact rate-distortion in autoencoders via\n\necho noise. In Advances in Neural Information Processing Systems, volume 32, 2019.\n\nT. M. Cover and J. A. Thomas. Elements of Information Theory 2nd Edition (Wiley Series in\n\nTelecommunications and Signal Processing). Wiley, 2006.\n\nJ. Deng, K. Li, M. Do, H. Su, and L. Fei-Fei. Construction and Analysis of a Large Scale Image\n\nOntology. Vision Sciences Society, 2009.\n\nP. Dhariwal and A. Nichol. Diffusion models beat GANs on image synthesis. Advances in Neural\n\nInformation Processing Systems, 34, 2021.\n\nT. Dockhorn, A. Vahdat, and K. Kreis. Score-based generative modeling with critically-damped\n\nlangevin diffusion. In International Conference on Learning Representations (ICLR), 2022.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nG. Flamich, M. Havasi, and J. M. Hern ́andez-Lobato. Compressing Images by Encoding Their Latent Representations with Relative Entropy Coding, 2020. Advances in Neural Information Processing Systems 34.\n\nG. Flamich, S. Markou, and J. M. Hern ́andez-Lobato. Fast relative entropy coding with a* coding,\n\n2022.\n\nI. Gali ́c, J. Weickert, M. Welk, A. Bruhn, A. Belyaev, and H.-P. Seidel. Image compression with anisotropic diffusion. Journal of Mathematical Imaging and Vision, 31(2):255–269, 2008. doi: 10. 1007/s10851-008-0087-0. URL https://doi.org/10.1007/s10851-008-0087-0.\n\nS. Gao, Y. Shi, T. Guo, Z. Qiu, Y. Ge, Z. Cui, Y. Feng, J. Wang, and B. Bai. Perceptual learned image compression with continuous rate adaptation. In 4th Challenge on Learned Image Compression, Jun 2021.\n\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680, 2014.\n\nS. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and B. Guo. Vector quantized diffusion\n\nmodel for text-to-image synthesis. arXiv preprint arXiv:2111.14822, 2021.\n\nM. Havasi, R. Peharz, and J. M. Hern ́andez-Lobato. Minimal Random Code Learning: Getting Bits Back from Compressed Model Parameters. In International Conference on Learning Representations, 2019.\n\nM. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637, 2017.\n\nJ. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840–6851. Curran Associates, Inc., 2020.\n\nJ. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans. Cascaded diffusion models for\n\nhigh fidelity image generation. Journal of Machine Learning Research, 23(47):1–33, 2022.\n\nA. Jolicoeur-Martineau, K. Li, R. Pich ́e-Taillefer, T. Kachman, and I. Mitliagkas. Gotta Go Fast\n\nWhen Generating Data with Score-Based Models. arXiv preprint arXiv:2105.14080, 2021.\n\nG. Kim and J. C. Ye. DiffusionCLIP: Text-guided image manipulation using diffusion models. In\n\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n\nD. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on\n\nLearning Representations, 2014.\n\nD. P. Kingma, T. Salimans, B. Poole, and J. Ho. On density estimation with diffusion models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\n\nA. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big Transfer\n\n(BiT): General Visual Representation Learning, 2019.\n\nS. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. Rohde. Generalized sliced wasserstein distances. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nZ. Kong and W. Ping. On fast sampling of diffusion probabilistic models.\n\narXiv preprint\n\narXiv:2106.00132, 2021.\n\nC. T. Li and A. El Gamal. Strong Functional Representation Lemma and Applications to Coding Theorems. IEEE Transactions on Information Theory, 64(11):6967–6978, 2018. doi: 10.1109/TIT. 2018.2865570.\n\nA. Lytchier, J. Xu, C. Finlay, C. Cursio, V. Koshkina, C. Besenbruch, and A. Zafar. Perceptuallyguided lossy image compression. In 4th Challenge on Learned Image Compression, Jun 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nC. J. Maddison. A Poisson process model for Monte Carlo. In Perturbation, Optimization, and\n\nStatistics. MIT Press, 2016.\n\nD. Maoutsa, S. Reich, and M. Opper. Interacting particle solutions of fokker–planck equations through gradient–log–density estimation. Entropy, 22(8), 2020. ISSN 1099-4300. doi: 10.3390/e22080802. URL https://www.mdpi.com/1099-4300/22/8/802.\n\nF. Mentzer, G. D. Toderici, M. Tschannen, and E. Agustsson. High-fidelity generative image\n\ncompression. Advances in Neural Information Processing Systems, 33, 2020.\n\nA. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n\nK. Pandey, A. Mukherjee, P. Rai, and A. Kumar. Diffusevae: Efficient, controllable and high-fidelity\n\ngeneration from low-dimensional latents. arXiv preprint arXiv:2201.00308, 2022.\n\nA. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image\n\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nD. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference\n\nin deep generative models. In International Conference on Machine Learning, 2014.\n\nH. Robbins. An empirical Bayes approach to statistics.\n\nIn Proceedings of the Third Berkeley\n\nSymposium on Mathematical Statistics and Probability, volume 1, pages 157–163, 1956.\n\nL. G. Roberts. Picture Coding Using Pseudo-Random Noise. IRE Transactions on Information\n\nTheory, 1962.\n\nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis\n\nwith latent diffusion models. arXiv preprint arXiv:2112.10752, 2021.\n\nC. Saharia, W. Chan, H. Chang, C. A. Lee, J. Ho, T. Salimans, D. J. Fleet, and M. Norouzi. Palette:\n\nImage-to-Image Diffusion Models. CoRR, abs/2111.05826, 2021.\n\nT. Salimans and J. Ho. Progressive distillation for fast sampling of diffusion models. In International\n\nConference on Learning Representations, 2022.\n\nC. Shannon. Communication in the presence of noise. Proceedings of the IRE, 37(1):10–21, 1949.\n\ndoi: 10.1109/JRPROC.1949.232969.\n\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.\n\nIn International Conference on Learning Representations, 2015.\n\nJ. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265. PMLR, 2015.\n\nY. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.\n\nL. Theis and E. Agustsson. On the advantages of stochastic encoders. In Neural Compression\n\nWorkshop at ICLR, 2021.\n\nL. Theis and A. B. Wagner. A coding theorem for the rate-distortion-perception function. In Neural\n\nCompression Workshop at ICLR, 2021.\n\nL. Theis and N. Yosri. Algorithms for the Communication of Samples. In Proceedings of the 39th\n\nInternational Conference on Machine Learning, 2022.\n\nL. Theis, W. Shi, A. Cunningham, and F. Husz ́ar. Lossy image compression with compressive\n\nautoencoders. In International Conference on Learning Representations, 2017.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nM. Tschannen, E. Agustsson, and M. Lucic. Deep generative models for distribution-preserving lossy compression. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\nA. Vahdat, K. Kreis, and J. Kautz. Score-based generative modeling in latent space, 2021.\n\nA. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel Recurrent Neural Networks. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1747–1756. PMLR, 2016.\n\nA. B. Wagner. The Rate-Distortion-Perception Tradeoff: The Role of Common Randomness, 2022.\n\narXiv:2202.04147.\n\nD. Watson, J. Ho, M. Norouzi, and W. Chan. Learning to efficiently sample from diffusion proba-\n\nbilistic models. CoRR, abs/2106.03802, 2021.\n\nY. Wu. ECE598: Information-theoretic methods in high-dimensional statistics, 2016.\n\nZ. Yan, F. Wen, R. Ying, C. Ma, and P. Liu. On perceptual lossy compression: The cost of perceptual reconstruction and an optimal training framework. In Proceedings of the International Conference on Machine Learning (ICML), 2021.\n\nY. Yang, S. Mandt, and L. Theis. An introduction to neural data compression, 2022.\n\nR. Zamir and M. Feder. Information rates of pre/post-filtered dithered quantizers. IEEE Transactions\n\non Information Theory, 42(5):1340–1353, 1996. doi: 10.1109/18.532876.\n\nN. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30: 495–507, 2022. doi: 10.1109/TASLP.2021.3129994.\n\nG. Zhang, J. Qian, J. Chen, and A. J. Khisti. Universal rate-distortion-perception representations for lossy compression. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\n\nQ. Zhang and Y. Chen. Fast sampling of diffusion models with exponential integrator, 2022.\n\nR. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018.\n\nA REVERSE CHANNEL CODING\n\nAlgorithm 1 Encoding (Li and El Gamal, 2018; Theis and Yosri, 2022) Require: p, q, wmin\n\n1: t, n, s∗ ← 0, 1, ∞\n\n2: repeat 3: 4: 5:\n\nz ← simulate(n, p) t ← t + exponential(1) s ← t · p(z)/q(z)\n\n6: 7: 8:\n\nif s < s∗ then\n\ns∗, n∗ ← s, n\n\nend if\n\nn ← n + 1\n\n9: 10: until s∗ ≤ t · wmin\n\n11: return n∗\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Decoding Require: n∗, p\n\n1: return simulate(n∗, p)\n\nFor completeness, we here reproduce pseudocode by Theis and Yosri (2022) of the sampling scheme first considered by Maddison (2016) and later by Li and El Gamal (2018) for the purpose of reverse channel coding. Similar to rejection sampling, the encoding process accepts a candidate generating distribution p, a target distribution q, and a bound on the density ratio\n\nwmin ≤ inf z\n\np(z) q(z)\n\n.\n\n(21)\n\nThe encoding process returns a index N ∗ (random due to the exponential noise) such that ZN ∗ follows the distribution q (Algorithm 1). Importantly, the algorithm produces an exact sample in a finite number of steps (Maddison, 2016). Furthermore, the coding cost of N ∗ is bounded by (Li and El Gamal, 2018)\n\nH[N ∗] + 1 < I[X, Z] + log(I[X, Z] + 1) + 5\n\n(22)\n\nand this bound can be achieved by entropy encoding N ∗ with a Zipf distribution pλ(n) ∝ n−λ which has a single parameter\n\nλ = 1 +\n\n1 I[X, Z] + e−1 log e + 1\n\n.\n\n(23)\n\nIn practice, the coding cost for Gaussians may be significantly lower than this bound.\n\nIn the encoding process, the function simulate(n, p) returns the nth candidate Zn ∼ p (in practice, this would be achieved with a pseudo-random number generator though we could also imagine a large list of previously generated and shared samples). The function exponential(1) produces a random sample from an exponential distribution with rate 1.\n\nUnlike encoding, decoding is fast as it only amounts to selecting the right candidate once N ∗ has been received (Algorithm 2).\n\nB NORMAL DISTRIBUTION WITH NON-UNIT VARIANCE\n\nThe case of a 1-dimensional Gaussian with varying variance is essentially the same as for a standard normal. In both cases, a Gaussian source is communicated through a Gaussian channel. Let X ∼ N (0, λ) and\n\nwhere as before U ∼ N (0, 1). Let ˆX ∼ P (X | Z). We define\n\n(cid:112)\n\nZ =\n\n1 − σ2X + σU\n\n ̃σ2 =\n\nσ2 σ2 + λ − λσ2 .\n\n(24)\n\n(25)\n\nThen\n\nand\n\nI[X, Z] = h[Z] − h[Z | X] =\n\nlog (cid:0)λ − σ2λ + σ2(cid:1) −\n\n1 2\n\n1 2\n\nlog σ2 = − log ̃σ\n\n(26)\n\nD = E[(X − ˆX)2] = λE[(λ− 1\n\n2 X − λ− 1\n\n2 ˆX)2] = 2λ ̃σ2\n\n(27)\n\ndue to Eq. 10 which tells us the squared error of the standard normal λ− 1 information contained in Z. Taken together, we again have\n\n2 X as a function of the\n\nI[X, Z] =\n\n1 2\n\nlog\n\nλ D/2\n\n= R∗(D/2).\n\n(28)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nC MULTIVARIATE GAUSSIAN\n\nLet X ∼ N (0, Σ) and let\n\n(cid:112)\n\nZ =\n\n1 − σ2X + σU\n\n(29)\n\nwhere U ∼ N (0, I). Note that both the mutual information and the squared error are invariant under rotations of X. We can therefore assume that the covariance is diagonal,\n\nDefining\n\nas in Appendix B, we have\n\nΣ = diag(λ1, . . . , λM ).\n\n ̃σ2\n\ni =\n\nσ2 σ2 + λi − λiσ2 .\n\nand\n\nD = E[(cid:107)X − ˆX(cid:107)2] =\n\nE[(Xi − ˆXi)2] = 2\n\n(cid:88)\n\ni\n\n(cid:88)\n\nλi ̃σ2\n\ni\n\ni\n\nI[X, Z] =\n\n(cid:88)\n\ni\n\nI[Xi, Zi] = −\n\n1 2\n\n(cid:88)\n\ni\n\nlog ̃σ2\n\ni\n\nLet ̃Di = λi ̃σ2\n\ni , then we can write\n\nD = 2\n\n(cid:88)\n\n ̃Di,\n\ni\n\nI[X, Z] =\n\n1 2\n\n(cid:88)\n\nlog\n\ni\n\nλi ̃Di\n\n.\n\n(30)\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\nFor fixed distortion D, the rate in Eq. 34 as a function of ̃Di is known to be minimized by the so-called reverse water-filling solution given in Eq. 8 (Shannon, 1949; Cover and Thomas, 2006), that is, Di = min(λi, θ) where θ must be chosen so that D = 2 (cid:80) i Di. Hence, ̃Di as defined above is generally suboptimal and we must have\n\nI[X, Z] ≥\n\n1 2\n\n(cid:88)\n\nlog\n\ni\n\nλi Di\n\n= R∗(D/2).\n\nD OPTIMAL NOISE SCHEDULE FOR FLOW-BASED RECONSTRUCTION\n\nLemma 1. Let X ∼ N (0, Σ) with diagonal covariance matrix\n\nΣ = diag(λ1, . . . , λM )\n\nand λi > 0. Further let\n\nZi = αiXi +\n\n(cid:113)\n\n1 − α2 i\n\n(cid:112)\n\nλiUi,\n\nˆX = Z,\n\nwhere U ∼ N (0, I) and\n\n(cid:18)(cid:113)\n\nαi =\n\nλ2\n\ni + θ2 − θ\n\n(cid:19)\n\n/λi.\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\nThen ˆX achieves the minimal rate at distortion level D = E[(cid:107)X − ˆX(cid:107)2] among all reconstructions satisfying the realism constraint ˆX ∼ X.\n\nProof. The lowest rate achievable by a code (with access to a source of shared randomness) is (Theis and Wagner, 2021)\n\ninf ̃X I[X, ̃X]\n\nsubject to E[(cid:107)X − ̃X(cid:107)] ≤ D and X ∼ ̃X.\n\nWe can rewrite the rate as\n\ninf ̃X I[X, ̃X] = inf ̃X h[X] + h[ ̃X] − h[X, ̃X] = 2h[X] − sup ̃X h[X, ̃X].\n\n(39)\n\n(40)\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nThat is, we need to maximize the differential entropy of (X, ̃X) subject to constraints. For the distortion constraint, we have\n\nE[(cid:107)X − ̃X(cid:107)2] = E[X(cid:62)X] + E[ ̃X(cid:62) ̃X] − 2E[X(cid:62) ̃X] = 2\n\n(cid:88)\n\ni\n\nλi − 2\n\n(cid:88)\n\ni\n\nE[Xi ̃Xi] ≤ D.\n\n(41)\n\nWe will first relax the realism constraint to the weaker constraints below and then show that the solution also satisfies the stronger realism constraint:\n\n(42) Consider relaxing the problem even further and jointly optimize over both X and ̃X with constraints on the first and second moments of both random variables. The joint maximum entropy distribution then takes the form\n\ni ] = λi.\n\nE[ ̃X] = 0, E[ ̃X 2\n\np(x, ̃x) ∝ exp\n\n(cid:32)\n\n(cid:88)\n\ni\n\nβixi +\n\n(cid:88)\n\ni\n\nγi ̃xi +\n\nμix2\n\ni +\n\n(cid:88)\n\ni\n\nνi ̃x2\n\ni +\n\n(cid:88)\n\ni\n\n(cid:88)\n\ni\n\n(cid:33)\n\nζixi ̃xi\n\n,\n\n(43)\n\nthat is, it is Gaussian with a precision matrix which has zeros everywhere except the diagonal and the off-diagonals corresponding to the interactions between Xi and ̃Xi. In other words, the joint precision matrix S consists of four blocks where each block is a diagonal matrix. It is not difficult to see by blockwise inversion that then the covariance matrix C = S−1 must have the same structure. Let CX ̃X be the diagonal matrix corresponding to the covariance between X and ̃X. We need to maximize\n\nh[X, ̃X] + const ∝ ln |C|\n\n= ln |CXXC ̃X ̃X − C ̃XXCX ̃X|\n\n(cid:88)\n\n=\n\ni\n\nln(λ2\n\ni − E[Xi ̃Xi]2)\n\nsubject to\n\nE[Xi ̃Xi] ≥\n\n(cid:88)\n\ni\n\nλi −\n\nD 2\n\n.\n\n(cid:88)\n\ni\n\nLet ci = E[Xi ̃Xi] and form the Lagrangian\n\nL(c, η, μ) =\n\n1 2\n\n(cid:88)\n\ni\n\nln(λ2\n\ni − c2\n\ni ) + η\n\n(cid:32)\n\n(cid:88)\n\ni\n\nci −\n\n(cid:88)\n\ni\n\nλi +\n\n(cid:33)\n\nD 2\n\n(cid:88)\n\n+\n\nμici,\n\ni\n\nwhere the last term is due to the constraint ci ≥ 0. The KKT conditions are\n\n∂L ∂ci (cid:32)\n\nη\n\n= −\n\nci i − c2\n\ni\n\nλ2\n\n(cid:88)\n\nci −\n\n(cid:88)\n\nλi +\n\ni (cid:88)\n\ni (cid:88)\n\nci −\n\ni\n\ni\n\n+ η + μi = 0,\n\n(cid:33)\n\nD 2\n\n= 0,\n\nλi +\n\nD 2\n\n≥ 0,\n\nμi ≥ 0,\n\nμici = 0,\n\nci ≥ 0,\n\nη ≥ 0,\n\nyielding ci = 0 or\n\nci =\n\n(cid:114)\n\nλ2\n\ni +\n\n1\n\n4η2 −\n\n1 2η\n\n.\n\n(44)\n\n(45)\n\n(46)\n\n(47)\n\n(48)\n\n(49)\n\n(50)\n\n(51)\n\n(52)\n\n(53)\n\nIf ci = 0 for some i, then μi = −η (Eq. 49) and therefore μi = η = 0 by Eq. 52. But then ci = 0 by Eq. 49 for all i. By Eq. 51, we must then have\n\nD ≥ 2\n\n(cid:88)\n\nλi.\n\ni\n\n16\n\n(54)\n\nUnder review as a conference paper at ICLR 2023\n\nThis implies that for sufficiently large distortion, we must have ci > 0 for all i. Defining θ = (2η)−1 gives that for ̃X to be optimal, we must have\n\nE[Xi ̃Xi] = ci =\n\n(cid:113)\n\nλ2\n\ni + θ2 − θ\n\n(55)\n\nfor some θ determined by D. Summarizing what we have so far, we have shown that (under relaxed realism constraints) the rate is minimized by a random variable ̃X jointly Gaussian with X and a covariance matrix whose entries are zero except those specified by Eqs. 42 and 55. Since the marginal distribution of ̃X is Gaussian with the desired mean and covariance, it also satisfies the stronger realism constraint ̃X ∼ X. On the other hand, ˆX ∼ ̃X | X. In particular,\n\nE[Xi ˆXi] = E[XiZi]\n\n= E[αiXiXi +\n\n(cid:113)\n\n(cid:112)\n\n1 − α2 i\n\nλiXiUi]\n\n= αiE[X 2 i ] = αiλi (cid:113)\n\n=\n\nλ2\n\ni + θ2 − θ.\n\n(56)\n\n(57)\n\n(58)\n\n(59)\n\n(60)\n\nhas the desired property. Thus, ˆX minimizes the rate at any given level of distortion.\n\nE PROOF OF THEOREM 1\n\nLemma 2. Let X : Ω → RM be a random variable with finite differential entropy and let X∗ be a Gaussian random variable with matching first and second-order moments. Let R(D) be the rate-distortion function of X and R∗(D) be the rate-distortion function of X∗. Then\n\nR∗(D) ≤ R(D) + DKL[PX (cid:107) PX∗ ].\n\n(61)\n\nProof. Zamir and Feder (1996) proved the result for M = 1. We here extend the proof to M > 1. First, observe that\n\nDKL[PX (cid:107) PX∗ ] = E[− log pX∗ (X)] − h[X] = E[− log pX∗ (X∗)] − h[X] = h[X∗] − h[X]\n\n(62) (63)\n\n(64)\n\nsince log pX∗ is a quadratic form and X and X∗ have matching moments. By the Shannon lower bound (Wu, 2016),\n\nR(D) ≥ h[X] −\n\nM 2\n\n(cid:18)\n\nlog\n\n2πe\n\n(cid:19)\n\n.\n\nD M\n\nLet λ1, . . . , λM be the eigenvalues of the covariance of X. Then by Eqs. 64 and 65 we have\n\nR(D) ≥ h[X∗] − D[PX, PX∗ ] −\n\nM 2\n\n(cid:18)\n\nlog\n\n2πe\n\n(cid:19)\n\nD M\n\n=\n\n≥\n\n(cid:88)\n\ni\n\n(cid:88)\n\ni\n\n1 2\n\n1 2\n\nlog\n\nlog\n\n(cid:18) λi\n\n(cid:19)\n\nD/M\n\n− D[PX, PX∗ ]\n\n(cid:19)\n\n(cid:18) λi Di\n\n− D[PX, PX∗ ]\n\n= R∗(D) − D[PX, PX∗ ].\n\n(65)\n\n(66)\n\n(67)\n\n(68)\n\n(69)\n\nwhere Di = min(θ, λi) and θ is such that D = (cid:80) i Di. The inequality follows from the optimality of the water-filling solution given by the Di (Shannon, 1949; Cover and Thomas, 2006). Bringing the KL divergence to the other side of the equation gives the desired result.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem 1. Let X : Ω → RM be a random variable with finite differential entropy, zero mean and covariance diag(λ1, . . . , λM ). Let U ∼ N (0, I) and define\n\n(cid:113)\n\nZi =\n\n1 − γ2\n\ni Xi + γi\n\n(cid:112)\n\nλiUi,\n\nˆX ∼ P (X | Z).\n\n(70)\n\nwhere γ2 i = min(1, θ/λi) for some θ. Further, let X∗ be a Gaussian random variable with the same first and second-order moments as X and let Z∗ be defined analogously to Z but in terms of X∗. Then if R is the rate-distortion function of X and R∗ is the rate-distortion function of X∗,\n\n(71)\n\n(72)\n\n(73)\n\n(74)\n\n(75)\n\n(76)\n\n(77)\n\n(78)\n\n(79)\n\n(80)\n\n(81)\n\n(82)\n\nI[X, Z] ≤ R∗(D/2) − DKL[PZ (cid:107) PZ∗ ]\n\n≤ R(D/2) + DKL[PX (cid:107) PX∗ ] − DKL[PZ (cid:107) PZ∗ ]\n\nwhere D = E[(cid:107)X − ˆX(cid:107)2].\n\nProof. We have\n\nDi = E[(Xi − ˆXi)2]\n\n= E[(Xi − E[Xi | Z] + E[Xi | Z] − ˆXi)2] = E[(Xi − E[Xi | Z])2 + (E[Xi | Z] − ˆXi)2 − 2(Xi − E[Xi | Z])(E[Xi | Z] − ˆXi)] = E[(Xi − E[Xi | Z])2] + E[(E[ ˆXi | Z] − ˆXi)2] EXi[Xi − E[Xi | Z] | Z]E ˆXi\n\n− 2EZ[(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)\n\n[E[Xi | Z] − ˆXi | Z]]\n\n= 2E[(Xi − E[Xi | Z])2]\n\n≤ 2E[(Xi −\n\n(cid:113)\n\n1 − γ2\n\ni Zi)2]\n\n= 2E[(1 − (1 − γ2\n\ni ))Xi −\n\n(cid:113)\n\n1 − γ2\n\ni γi\n\n(cid:112)\n\nλiUi)2]\n\n= 2(Var[γ2\n\ni Xi] + Var[\n\n(cid:113)\n\n1 − γ2\n\ni γi\n\n(cid:112)\n\nλiUi])\n\n= 2(γ4 = 2γ2\n\ni λi + (1 − γ2 i λi,\n\ni )γ2\n\ni λi)\n\n(83) where in Eq. 77 we used that X ⊥⊥ ˆX | Z and in Eq. 78 we used that (X, Z) ∼ ( ˆX, Z). Eq. 79 follows because the conditional expectation minimizes the squared error among all estimators of Xi. For the overall distortion, we therefore have\n\nD = E[(cid:107)X − ˆX(cid:107)2] =\n\n(cid:88)\n\ni\n\nDi ≤ 2\n\n(cid:88)\n\ni\n\ni λi = D∗. γ2\n\nNote that D∗ is the distortion we would have gotten if X were Gaussian (Eq. 14). Define\n\nVi = (1 − γ2\n\ni )− 1\n\n2 γi\n\n(cid:112)\n\nλiUi\n\nand Yi = (1 − γ2\n\ni )− 1\n\n2 Zi = Xi + Vi\n\n(84)\n\n(85)\n\nand let Y∗ be the Gaussian random variable defined analogously to Y except in terms of Z∗ instead of Z. To obtain the rate, first observe that\n\nI[X∗, Z∗] = I[X∗, Y∗]\n\n= I[X∗, X∗ + V] = h[X∗ + V] − h[X∗ + V | X∗] = h[X∗ + V] − h[V] = h[X + V] − h[V] + h[X∗ + V] − h[X + V] = h[X + V] − h[X + V | X] + h[X∗ + V] − h[X + V] = I[X, X + V] − E[log pY∗ (Y∗)] + E[log pY(Y)] = I[X, Y] − E[log pY∗ (Y∗)] + E[log pY(Y)] = I[X, Y] − E[log pY∗ (Y)] + E[log pY(Y)] = I[X, Y] + DKL[PY || PY∗ ] = I[X, Z] + DKL[PZ || PZ∗ ].\n\n18\n\n(86)\n\n(87) (88)\n\n(89)\n\n(90)\n\n(91) (92)\n\n(93)\n\n(94) (95)\n\n(96)\n\nUnder review as a conference paper at ICLR 2023\n\nM /\nG\n\nt\n\n400\n\n350\n\n300\n\n250\n\n200\n\n150\n\n100\n\n50\n\n0\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nσt\n\nFigure 5: While G0 is undefined for images with discretized pixels, we may instead consider the distribution of pixels with imperceptible Gaussian noise added to it. We can estimate the corresponding Gt using the diffusion model, the results of which are shown in this plot. Gt converges to M as σt approaches 1.\n\nwhere the first step and last step follow from the invariance of mutual information and KL divergence under invertible transformations. Eq. 94 follows because log pY∗ is a quadratic form and Y and Y∗ have matching moments. We therefore have\n\nI[X, Z] = I[X∗, Z∗] − DKL[PZ || PZ∗ ] = R∗(D∗/2) − DKL[PZ || PZ∗ ] ≤ R∗(D/2) − DKL[PZ || PZ∗ ] ≤ R(D/2) + DKL[PX || PX∗ ] − DKL[PZ || PZ∗ ],\n\n(97) (98)\n\n(99)\n\n(100)\n\nwhere the second equality is due to Eq. 14, the first inequality is due to D ≤ D∗, and the second inequality follows from the Shannon lower bound and Lemma 2.\n\nF PROOF OF THEOREM 2\n\nTheorem 2 compares the reconstruction error of ˆXA ∼ P (X | Zt) with ˆXF = ˆZ0 where ˆZ0 is the solution of\n\n(cid:18)\n\ndzt =\n\n−\n\n1 2\n\nβtzt −\n\n1 2\n\n(cid:19)\n\nβt∇ ln pt(zt)\n\ndt.\n\n(101)\n\ngiven Zt = (cid:112)1 + σ2 convenient to work with the “variance exploding” diffusion process\n\nt X + σtU as initial condition. To derive the following results it is often more\n\nYt = (1 − σ2\n\nt )− 1\n\n2 Zt ∼ X + (1 − σ2\n\nt )− 1\n\n2 σtU = X + ηtU\n\n(102)\n\ninstead of the “variance preserving” process Zt (Song et al., 2021). We use ̃pt for the marginal density of Yt and reserve pt for the density of Zt.\n\nWe further define the following quantity for continuously differentiable densities, which can be viewed as a measure of smoothness of a density,\n\nG = E[(cid:107)∇ ln p(X)(cid:107)2].\n\n(103)\n\nIt can be shown that when E[(cid:107)X(cid:107)2] = 1, we have G ≥ M with equality when X is isotropic Gaussian. That is, the isotropic Gaussian is the smoothest distribution (with a continuously differentiable density) as measured by G. We further define\n\nGt = E[(cid:107)∇z log pt(Zt)(cid:107)2]\n\nand\n\n ̃Gt = E[(cid:107)∇y log ̃pt(Yt)(cid:107)2]\n\n(104)\n\nwhich are linked by the chain rule, ̃Gt = (1 − σ2 estimates of Gt for ImageNet 64x64, which are shown in Fig. 5.\n\nt )Gt. Using a trained diffusion model, we can obtain\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 3. Diffusion increases the smoothness of a distribution, ̃Gt ≤ G0.\n\nProof. We have\n\n∇y ln ̃pt(yt) =\n\n=\n\n=\n\n=\n\n=\n\nand therefore\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\np(u | yt)∇y ln ̃pt(y) du\n\np(u | yt)∇y ln\n\n ̃pt(y)p(u | yt) p(u | yt)\n\ndu\n\np(u | yt)∇y ln p(u)p(yt | u) du −\n\n(cid:90)\n\np(u | yt)∇y ln p(yt | u) dx − 0\n\np(u | yt)∇y ln p0(yt − ηtu) dx\n\n ̃Gt = E[(cid:107)∇y ln ̃pt(Yt)(cid:107)2]\n\n= E[(cid:107)E[∇y ln p0(Yt − ηtU) | Yt](cid:107)2] ≤ E[E[(cid:107)∇y ln p0(Yt − ηtU)(cid:107)2 | Yt]] = E[(cid:107)∇x ln p0(X)(cid:107)2] = G0\n\n(105)\n\n(106)\n\np(u | yt)∇y ln p(u | yt) du\n\n(107)\n\ndue to Jensen’s inequality.\n\nWe will also need the following known useful identity which is a special case of Tweedie’s formula (Robbins, 1956). We include a derivation for completeness.\n\nLemma 4. Let X have a density and let Yt, ̃pt, and ηt be defined as above. Then\n\nE[X | yt] = yt + η2\n\nt ∇ ln ̃pt(yt).\n\nProof.\n\n∇y log ̃pt(yt) =\n\n=\n\n=\n\n=\n\n=\n\n=\n\n=\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\n−\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\np(x | yt)∇y log ̃pt(yt) dx\n\np(x | yt)∇y log\n\n ̃pt(yt)p(x | yt) p(x | yt)\n\ndx\n\np(x | yt)∇y log (p(yt | x)p(x)) dx\n\n(cid:90)\n\np(x | yt)∇y log p(x | yt) dx\n\np(x | yt)∇y log p(yt | x) dx\n\np(x | yt)∇y log N (cid:0)yt; x, η2\n\nt I(cid:1) dx\n\np(x | yt)\n\n1 η2 t\n\n(x − yt) dx\n\n(E[X | yt] − yt)\n\n1 η2 t\n\n(108)\n\n(109)\n\n(110)\n\n(111)\n\n(112)\n\n(113)\n\n(114)\n\n(115)\n\n(116)\n\n(117)\n\n(118)\n\n(119)\n\n(120)\n\n(121)\n\n(122)\n\n(123)\n\nThe following three lemmas relate the reconstruction errors of ˆXA and ˆXF to the smoothness of the source distribution as measured by G0 and Gt.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nLemma 5. Let X have a density and let ˆXA, ηt, and Gt be defined as above. Then\n\nE[(cid:107) ˆXA − X(cid:107)2] = 2η2 = 2η2\n\nt M − 2η4 t M − 2η4\n\n ̃Gt t (1 − σ2\n\nt\n\nt )Gt\n\nProof.\n\nE[(cid:107) ˆXA − X(cid:107)2] = E[(cid:107) ˆXA − Yt + Yt − X(cid:107)2]\n\n= E[(cid:107) ˆXA − Yt(cid:107)2 + (cid:107)Yt − X(cid:107)2 + 2( ˆXA − Yt)(cid:62)(Yt − X)] = E[(cid:107) ˆXA − Yt(cid:107)2] + E[(cid:107)Yt − X(cid:107)2]\n\n+ 2E[E[ ˆXA − Yt | Yt](cid:62)E[Yt − X | Yt]]\n\n= E[(cid:107)X − Yt(cid:107)2] + E[(cid:107)Yt − X(cid:107)2]\n\n+ 2E[E[X − Yt | Yt](cid:62)E[Yt − X | Yt]] = 2E[(cid:107)Yt − X(cid:107)2] − 2E[||E[Yt − X | Yt]||2] = 2E[(cid:107)ηtUt(cid:107)2] − 2E[||Yt − E[X | Yt]||2] = 2η2\n\nt M − 2E[||η2 t M − 2E[||η2 t M − 2η4\n\nt ∇y ln ̃pt(Yt)||2] t (1 − σ2 t ) t )Gt\n\nt (1 − σ2\n\n1\n\n= 2η2 = 2η2\n\n2 ∇z ln pt(Zt)||2]\n\nLemma 6. Let X have a density and let ˆXF , ηt, and G0 be defined as above. Then\n\nE[(cid:107) ˆXF − Yt(cid:107)2] ≤\n\n1 4\n\nη4\n\nt G0.\n\nProof. We have\n\nE[(cid:107) ˆXF − Yt(cid:107)2] = E[(cid:107)F −1\n\nt\n\n(Yt) − Yt(cid:107)2] = E[(cid:107)X − Ft(X)(cid:107)2]\n\nwhere Ft is the invertible function which maps x to yt according to the ODE\n\ndyt = −αt∇ ln pt(yt) dt, y0 = x,\n\nwhere αt relates to ηt as follows,\n\n(cid:115)\n\nηt =\n\n2\n\n(cid:90) t\n\n0\n\nατ dτ .\n\n(124)\n\n(125)\n\n(126)\n\n(127)\n\n(128)\n\n(129)\n\n(130)\n\n(131)\n\n(132)\n\n(133)\n\n(134)\n\n(135)\n\n(136)\n\n(137)\n\n(138)\n\n(139)\n\n(140)\n\nDifferent schedules are equivalent up to reparametrization of the time parameter (Kingma et al., 2021). For now, assume the parametrization αt = 1 (or ηt = 2t). Integrating the above ODE then yields\n\n√\n\nyt = Ft(x) = x −\n\n(cid:90) t\n\n0\n\n∇ ln pτ (yτ ) dτ .\n\nConsider the following Riemann sum approximation of Ft,\n\nFt,N (x) = x −\n\nN −1 (cid:88)\n\nn=0\n\nt N\n\n∇ ln ptn(ytn )\n\n(141)\n\n(142)\n\nwhere tn = nt/N and ytn = Ftn (x). Since the gradient of the log-density is continuous and the integral is over a compact interval, the partial derivatives are bounded inside the interval and the Riemann sum converges to\n\nFt(x) = lim\n\nN→∞\n\nFt,N (x).\n\n(143)\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nThus,\n\nE[(cid:107)X − Ft(X)(cid:107)2] = E[(cid:107)X − lim\n\nN→∞\n\nFt,N (X)(cid:107)2]\n\n= E\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nlim N→∞\n\nN −1 (cid:88)\n\nn=0\n\n\n\n= E\n\n lim\n\nN→∞\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN −1 (cid:88)\n\nn=0\n\nt N\n\nt N\n\n∇ ln ̃ptn (Ytn )\n\n∇ ln ̃ptn (Ytn )\n\n2\n\n\n\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n\n\n(cid:35)\n\n1 N\n\n(cid:107)t∇ ln ̃ptn (Ytn )(cid:107)2\n\nE\n\n(cid:104)\n\n(cid:107)∇ ln ̃ptn (Ytn )(cid:107)2(cid:105)\n\n ̃Gtn\n\nG0\n\n(cid:34)\n\n≤ E\n\nlim N→∞\n\n= lim\n\nN→∞\n\n= lim\n\nN→∞\n\n≤ lim\n\nN→∞\n\nt2 N\n\nt2 N\n\nt2 N\n\nN −1 (cid:88)\n\nn=0\n\nN −1 (cid:88)\n\nn=0\n\nN −1 (cid:88)\n\nn=0\n\nN −1 (cid:88)\n\nn=0\n\n= t2G0 1\n4\n\nη4\n\n=\n\nt G0,\n\n(144)\n\n(145)\n\n(146)\n\n(147)\n\n(148)\n\n(149)\n\n(150)\n\n(151)\n\n(152)\n\nEq. 147 again uses Jensen’s inequality. Eq. 148 (swapping limit and expectation) follows from the dominated convergence theorem since each element of the sequence is bounded by t2G0 (Lemma 3).\n\nLemma 7. Let X have a smooth density and let ˆXF , ηt, and G0 be defined as above. Then\n\nE[(cid:107) ˆXF − X(cid:107)2] ≤ η2\n\nt M +\n\n1 2\n\nη4 t G0 + 2η4\n\nt (1 − σ2\n\nt )Gt.\n\nProof.\n\nE[(cid:107) ˆXF − X(cid:107)2] = E[(cid:107) ˆXF − E[X | Yt] + E[X | Yt] − X(cid:107)2]\n\n= E[(cid:107) ˆXF − E[X | Yt](cid:107)2] + E[(cid:107)E[X | Yt] − X(cid:107)2] + 0 ≤ E[(cid:107) ˆXF − E[X | Yt](cid:107)2] + E[(cid:107)Yt − X(cid:107)2] = E (cid:2)(cid:107) ˆXF − Yt + η2\n\nt ∇ ln ̃pt(Yt)(cid:107)2(cid:3) + E[(cid:107)ηtU(cid:107)2]\n\n(cid:34)\n\n= E\n\n4\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n1 2\n\n( ˆXF − Yt) +\n\n1 2\n\n(cid:0)η2\n\nt ∇ ln ̃pt(Yt)(cid:1)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+ η2\n\nt M\n\n≤ 2E[(cid:107) ˆXF − Yt(cid:107)2] + 2E[(cid:107)η2 = 2E[(cid:107) ˆXF − Yt(cid:107)2] + 2η4\n\nt ∇ ln ̃pt(Yt)(cid:107)2] + η2\n\nt M\n\nt (1 − σ2\n\nt )Gt + η2\n\nt M\n\n≤\n\n1 2\n\nt G0 + 2η4 η4\n\nt (1 − σ2\n\nt )Gt + η2\n\nt M\n\n(153)\n\n(154)\n\n(155)\n\n(156)\n\n(157)\n\n(158)\n\n(159)\n\n(160)\n\n(161)\n\nwhere the first inequality is due to the conditional expectation minimizing squared error, the second inequality is due to Jensen’s inequality and the last inequality is due to Lemma 6.\n\nWe are finally in a position to prove Theorem 2.\n\nTheorem 2. Let X : Ω → RM have a smooth density p with finite\n\nG = E[(cid:107)∇ ln p(X)(cid:107)2].\n\n(162)\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nLet Zt = (cid:112)1 − σ2 solution to Eq. 6 with Zt as initial condition. Then\n\nt X + σtU with U ∼ N (0, I). Let ˆXA ∼ P (X | Zt) and let ˆXF = ˆZ0 be the\n\nE[(cid:107) ˆXF − X(cid:107)2] E[(cid:107) ˆXA − X(cid:107)2]\n\n=\n\n1 2\n\nlim σt→0\n\nProof. The limit is to be understood as the one-sided limit from above. We have\n\nt )Gt\n\nt (1 − σ2 t )Gt\n\nt G0 + 2η4\n\n2 η4\n\nt M + 1 η2 2η2 t M + 1 η2 2η2 2ηtM + 2η3\n\n2 η4\n\nt M − 2η4\n\nt (1 − σ2 t G0\n\nt G0 + 2η4\n\nt M − 2η4\n\nt G0\n\nt G0 + 8η3\n\nt G0\n\n4ηtM − 8η3\n\nt G0 t G0 + 24η2\n\n2M + 6η2\n\nt G0\n\n4M − 24η2\n\nt G0\n\nE[(cid:107) ˆXF − X(cid:107)2] E[(cid:107) ˆXA − X(cid:107)2]\n\nlim σt→0\n\n≤ lim σt→0\n\n≤ lim σt→0\n\n= lim ηt→0\n\n=\n\n= lim ηt→0 2M 4M 1\n2\n\n=\n\n(163)\n\n(164)\n\n(165)\n\n(166)\n\n(167)\n\n(168)\n\n(169)\n\nwhere the first inequality follows from Lemmas 5 and 7, the second inequality is due to Lemma 3, and we applied L’Hˆopital’s rule twice.\n\nG PROOF OF THEOREM 3\n\nTheorem 3. Let X = QS where Q is an orthogonal matrix and S : Ω → RM is a random vector with smooth density and Si ⊥⊥ Sj for all i (cid:54)= j. Define\n\n(cid:113)\n\nZt =\n\n1 − σ2\n\nt X + σtU where U ∼ N (0, I).\n\nIf ˆXF = ˆZ0 is the solution to the ODE in Eq. 6 given Zt as initial condition, then\n\nfor any ˆX(cid:48) with ˆX(cid:48) ⊥⊥ X | Zt which achieves perfect realism, ˆX(cid:48) ∼ X.\n\nE[(cid:107) ˆXF − X(cid:107)2] ≤ E[(cid:107) ˆX(cid:48) − X(cid:107)2]\n\nProof. Define the variance exploding diffusion process as\n\ndYt = (cid:112)ζtdWt with (1 − σ2\n\nt )− 1\n\n2 σ2\n\nt =\n\n(cid:90) t\n\n0\n\nζτ dτ\n\nso that\n\n(170)\n\n(171)\n\n(172)\n\nYt = (1 − σ2\n\n(173) Further define Ft as the function which maps x to the solution of the ODE in Eq. 6 with starting condition z0 = x. Then Ft is invertible (Song et al., 2021) and we can write ˆXF = F −1 (Zt). Further, let ̃Ft be the corresponding function for the variance exploding process such that\n\nt U = X + ηtU.\n\n2 Zt ∼ X + (1 − σ2\n\n2 σ2\n\nt\n\nt )− 1\n\nt )− 1\n\n ̃F −1\n\nt\n\n(y) = F −1\n\nt\n\n(cid:18)(cid:113)\n\n1 − σ2\n\nt y\n\n(cid:19)\n\n,\n\nˆXF = ̃F −1\n\nt\n\n(Yt).\n\nFor arbitrary ˆX with ˆX ⊥⊥ X | Yt, we have\n\nE[(cid:107) ˆX − X(cid:107)2] = E[(cid:107) ˆX − E[X | Yt] + E[X | Yt] − X(cid:107)2]\n\n= E[(cid:107) ˆX − E[X | Yt](cid:107)2] + E[(cid:107)E[X | Yt] − X(cid:107)2]\n\n+ E[( ˆX − E[X | Yt])(cid:62)(E[X | Yt] − X)]\n\n= E[(cid:107) ˆX − E[X | Yt](cid:107)2] + E[(cid:107)E[X | Yt] − X(cid:107)2]\n\n+ EYt[E ˆX[ ˆX − E[X | Yt] | Yt](cid:62)\n\n= E[(cid:107) ˆX − E[X | Yt](cid:107)2] + E[(cid:107)E[X | Yt] − X(cid:107)2]\n\n(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)\n\nEX[E[X | Yt] − X | Yt]]\n\n23\n\n(174)\n\n(175)\n\n(176)\n\n(177)\n\n(178)\n\n(179)\n\n(180)\n\nUnder review as a conference paper at ICLR 2023\n\nDefine ˆXMSE = ψt(Yt) = E[X | Yt].\n\nAssume M = 1 so that X = S. We first show that then ψt is a monotone function of yt:\n\nψ(cid:48)(yt) =\n\n=\n\n∂ ∂y ∂\n∂y\n\nE[X | yt]\n\n(cid:18)\n\nyt + η2\n\nt\n\n∂ ∂y\n\n(cid:19)\n\nln ̃pt(yt)\n\n= 1 + η2\n\nt\n\n∂2 ∂y2 ln ̃pt(yt)\n\n(cid:90)\n\n(cid:90)\n\n= 1 + η2\n\nt\n\n= 1 + η2\n\nt\n\np(x | yt)\n\np(x | yt)\n\n ̃pt(yt)p(x | yt) p(x | yt)\n\n∂2 ∂y2 ln ∂2 ∂y2 ln ̃pt(yt | x) dx − η2\n\nt\n\ndx\n\n(cid:90)\n\np(x | yt)\n\n∂2 ∂y2 ln p(x | yt) dx\n\n1 2η2 t\n(cid:90)\n\n= 1 −\n\n= 1 −\n\n(cid:90)\n\nη2\n\nt\n\np(x | yt)\n\n∂2 ∂y2 (yt − x)2 dx + η2\n\nt J(yt)\n\np(x | yt)\n\n∂ ∂y\n\n(yt − x) dx + η2\n\nt J(yt)\n\n(cid:90)\n\n= 1 −\n\np(x | yt) dx + η2\n\nt J(yt)\n\n= η2\n\nt J(yt) (cid:90)\n\n= η2\n\nt\n\np(x | yt)\n\n(cid:18) ∂ ∂y\n\n(cid:19)2\n\nln p(x | yt)\n\ndx\n\n≥ 0\n\nwhere J(yt) is the Fisher information of yt. Assume ψ(cid:48)(yt) = 0 for some yt. Then\n\n∂ ∂y\n\nln p(X | yt) = 0\n\nalmost surely (Eq. 190). Then also\n\n(181)\n\n(182)\n\n(183)\n\n(184)\n\n(185)\n\n(186)\n\n(187)\n\n(188)\n\n(189)\n\n(190)\n\n(191)\n\n(192)\n\n∂ ∂y\n\nln p(X | yt) =\n\n∂ ∂y\n\nln\n\np(yt | x)p(X) ̃pt(yt)\n\n=\n\n∂ ∂y\n\nln p(yt | X) +\n\n∂ ∂y\n\nln ̃pt(yt) = 0\n\n(193)\n\nor\n\n1 η2 t\n\n(yt − X) = −\n\n∂ ∂y\n\nln p(yt | X) =\n\n∂ ∂y\n\nln ̃pt(yt)\n\n(194)\n\nalmost surely. This implies X is almost surely constant, that is, p(x | yt) is a degenerate distribution. But this contradicts our assumption that p(x) is smooth. Since p(yt | x) is Gaussian with mean x and therefore smooth as a function of x, p(x | yt) ∝ p(x)p(yt | x) must also be smooth. Hence, we must have ψ(cid:48)(yt) > 0 everywhere. Since ψ(yt) is strictly monotone it is also invertible.\n\nConsider the squared Wasserstein metric,\n\nW 2\n\n2 [PX , P ˆXMSE\n\n] = inf\n\nˆX: ˆX∼X\n\nE[( ˆX − ˆXMSE)2]\n\n(195)\n\nwhere the infimum is over all random variables with the same marginal distribution as X and which may depend on ˆXMSE (or equivalently may depend on Yt). The solution to this problem is known from transportation theory to be ˆX ∗ = Φ−1 0 (Φt( ˆXMSE)) (e.g., Kolouri et al., 2019), where Φ0 is the CDF of X, Φt is the CDF of XMSE, and it is assumed that the measure of X is absolutely continuous\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nwith respect to the Lebesgue measure. We have\n\nΦ0(x) = P (X ≤ x)\n\nt\n\n(Yt) ≤ x)\n\n= P ( ̃F −1 = P (Yt ≤ Ft(x)) = P (ψ−1 = P ( ˆXMSE ≤ ˆψt( ̃Ft(x))) = Φt(ψt( ̃Ft(x))),\n\n( ˆXMSE) ≤ ̃Ft(x))\n\nt\n\nimplying\n\n0 (Φt( ˆXMSE))\n\n(ψ−1\n\nt\n\n( ˆXMSE))\n\n(Yt)\n\nˆX ∗ = Φ−1 = ̃F −1 = ̃F −1 = ˆXF\n\nt\n\nt\n\n(196)\n\n(197)\n\n(198)\n\n(199)\n\n(200)\n\n(201)\n\n(202)\n\n(203)\n\n(204)\n\n(205)\n\nand therefore that ˆXF is optimal. Now let M > 1. Since E[(cid:107) ˆXF − X(cid:107)2] is invariant under the choice of Q, we can assume Q = I without changing the results of our analysis so that X = S and (Xi, Yti) ⊥⊥ (Xj, Ytj) for i (cid:54)= j. Since then\n\nln pt(zt) =\n\n(cid:88)\n\ni\n\nln pti(zti),\n\nthe ODE (Eq. 6) can be decomposed into M separate problems\n\n(cid:18)\n\ndzti =\n\n−\n\n1 2\n\nβtzti −\n\n1 2\n\nβt\n\n∂ ∂zti\n\n(cid:19)\n\nln pti(zti)\n\ndt\n\nfor which we already know the solution is of the form zti = (1 − σ2\n\nt )yti with\n\nyti = ̃Fti(xi) = ˆψ−1\n\nti (Φ−1\n\nti (Φ0i(xi))),\n\nwhere Φti is the CDF of\n\nOn the other hand,\n\nˆXi,MSE = E[Xi | Yti] = E[Xi | Yt] = ˆXMSE,i.\n\ninf ˆX: ˆX∼X\n\nE[(cid:107) ˆX − ˆXMSE(cid:107)2] = inf\n\nˆX: ˆX∼X\n\n(cid:88)\n\ni\n\nE[( ˆXi − ˆXMSE,i)2]\n\n≥\n\n≥\n\n=\n\n=\n\n=\n\n(cid:88)\n\ni (cid:88)\n\ni (cid:88)\n\ni (cid:88)\n\ni (cid:88)\n\ni\n\ninf ˆX: ˆX∼X\n\nE[( ˆXi − ˆXMSE,i)2]\n\ninf ˆXi: ˆXi∼Xi\n\ninf ˆXi: ˆXi∼Xi\n\nE[( ˆXi − ˆXMSE,i)2]\n\nE[( ˆXi − ˆXi,MSE)2]\n\nE[(Φ−1\n\n0i (Φti( ˆXi,MSE)) − ˆXi,MSE)2]\n\nE[( ˆXF ,i − ˆXMSE,i)2]\n\n(206)\n\n(207)\n\n(208)\n\n(209)\n\n(210)\n\n(211)\n\n(212)\n\n(213)\n\n(214)\n\n(215)\n\n(216) That is, ˆXF minimizes the squared error among all reconstructions achieving perfect realism. The second inequality follows due to the weaker constraint on the right-hand side; ˆX ∼ X implies ˆXi ∼ Xi but not vice versa. Eqs. 214 and 215 follow from our proof of the case M = 1.\n\n= E[(cid:107) ˆXF − ˆXMSE(cid:107)2].\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nH COMPUTE RESOURCES\n\nTraining VDM took about 13 days using 32 TPUv3 cores (https://cloud.google.com/tpu). No hyperparameter searches were performed to tune VDM for this paper. Training one HiFiC model took about 4 days using 2 V100 GPUs and we trained 10 models targeting 5 different bitrates (with and without pretrained weights). A few additional training runs were performed for HiFiC to tune the architecture (reducing the stride) while targeting a single bitrate.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nI ADDITIONAL FIGURES\n\nFigure 6: Top images visualize messages communicated at the estimated bitrate shown in black. The left-most bitrate corresponds to lossless compression with our VDM model. The bottom row shows reconstructions produced by DiffC-F and corresponding PSNR values in red.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nDiffC-F/A\n\nHiFiC BPG JPEG\n\nDiffC-F/A\n\nHiFiC BPG JPEG\n\nFigure 7: Additional reconstructions generated by different compression methods. The left-most column shows the uncompressed image. Bitrates are shown in black and PSNR values in red.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nBPG HiFiC HiFiC (pretrained) DiffC-F DiffC-A\n\nR N\nS P\n\n40\n\n35\n\n30\n\n25\n\n20\n\n15\n\n10\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\nBits per pixel\n\nFigure 8: PSNR values in Section 5 were computed by calculating a PSNR score for each image and averaging. In contrast, this plot shows PSNR values corresponding to the average MSE.\n\nD F\n\nI\n\n60\n\n50\n\n40\n\n30\n\n20\n\n10\n\n0\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\nBits per pixel\n\nDiffC-F DiffC-F (100) DiffC-F (40) DiffC-F (20) DiffC-F (10) HiFiC (pretrained)\n\nR N\nS P\n\n40\n\n35\n\n30\n\n25\n\n20\n\n15\n\n10\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\nBits per pixel\n\nFigure 9: Performance relative to an upper bound on the coding cost when progressively communicating information in chunks of B bits using the approach of Li and El Gamal (2018). The coding cost is estimated using Ct B (B + log(B + 1) + 5), where Ct is the total amount of information sent (Eq. 5). At 10 bits the PSNR is comparable to our strongest baseline but the FID remains significantly lower.\n\nD F\n\nI\n\n90 80 70 60 50 40 30 20 10 0\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n]\n\nB d\n\n[\n\nR N\nS P\n\n40\n\n35\n\n30\n\n25\n\n20\n\n15\n\n10\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\nBPG HiFiC HiFiC (pretrained) HiFiC (MSE) DiffC-F DiffC-A\n\nBits per pixel\n\nBits per pixel\n\nFigure 10: This figure contains additional results for HiFiC trained from scratch for MSE only. We only targeted a single bit-rate. The PSNR improves slightly while the FID score gets significantly worse.\n\n29",
    "reference": "# Summary Of The Paper\n\nThis paper proposed a diffusion based lossy image compression approach. In the evaluation, the proposed method outperforms a baseline.\n\n# Strength And Weaknesses\n\nStrength:\n1. The topic of using diffusion for lossy compression is interesting. \n\nWeaknesses:\n1. It’s hard to justify the originality of the proposed main methodology. The contribution is marginal. As far as I understand, all technologies of this paper is introduced from other paper, like diffusion and reserve channel coding.\n2. Although the author claim they have some improvement, but they only compared with HIFIC.  As far as I know, there are plenty of lossy compression works every year [1, 2, 3]. The author should compare with these works, too.\n3. The author claim in abstract, that this method is efficient. However, there’s no following experiment or number that can prove this claim.\n4. The lossy compression techniques are also widely applied on video coding area, too. The author should add experiment on video compression.\n\n[1] Cui Z, Wang J, Gao S, et al. Asymmetric gained deep image compression with continuous rate adaptation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 10532-10541.\n[2] Xie Y, Cheng K L, Chen Q. Enhanced invertible encoding for learned image compression[C]//Proceedings of the 29th ACM International Conference on Multimedia. 2021: 162-170.\n[3] Deng X, Yang W, Yang R, et al. Deep homography for efficient stereo image compression[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 1492-1501.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is written in a needless complex way. Unless further clarified, it is hard to justify the originality of this paper.\n\n# Summary Of The Review\n\nAs the major technical contribution of this paper is not clear, I recommend a reject. I would raise the score if the authors provide a good clarification.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nREVISITING EMBEDDINGS FOR GRAPH NEURAL NETWORKS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nCurrent graph representation learning techniques use Graph Neural Networks (GNNs) to extract features from dataset embeddings. In this work, we examine the quality of these embeddings and assess how changing them can affect the accuracy of GNNs. We explore different embedding extraction techniques for both images and texts; and find that the choice of embedding biases the performance of different GNN architectures and thus the choice of embedding influences the selection of GNNs regardless of the underlying dataset. In addition, we only see an improvement in accuracy from some GNN models compared to the accuracy of models trained from scratch or fine-tuned on the underlying data without utilising the graph connections. As an alternative, we propose Graph-connected Network (GraNet) layers to better leverage existing unconnected models within a GNN. Existing language and vision models are thus improved by allowing neighbourhood aggregation. This gives a chance for the model to use pre-trained weights, if possible, and we demonstrate that this approach improves the accuracy compared to traditional GNNs: on Flickr v2, GraNet beats GAT2 and GraphSAGE by 7.7% and 1.7% respectively.\n\n1\n\nINTRODUCTION\n\nGraph Neural Networks (GNNs) have been successful on a wide array of applications ranging from computational biology (Zitnik & Leskovec, 2017) to social networks (Hamilton et al., 2017). The input for GNNs, although sourced from many different domains, is often data that has been preprocessed to a computationally digestible format. These digestible formats are commonly known as embeddings.\n\nCurrently, improvements made to GNN architecture are tested against these embeddings and the state of the art is determined based on those results. However, this does not necessarily correlate with the GNNs accuracy on the underlying dataset and ignores the influence that the source and style of these embeddings have on the performance of particular GNN architectures. To test existing GNN architectures, and demonstrate the importance of the embeddings used in training them, we provide three new datasets each with a set of embeddings generated using different methods.\n\nWe further analyse the benefit of using GNNs on fixed embeddings. We compare GNNs to standard models that have been trained or fine-tuned on the target raw data; these models treat each data point as unconnected, ignoring the underlying graph information in data. This simple unconnected baseline surprisingly outperforms some strong GNN models. This then prompts the question: Will mixing the two approaches unlock the classification power of existing unconnected models by allowing them to utilize the graph structure in our data?\n\nBased on the question above, we propose a new method of mixing GNNs with unconnected models, allowing them to train simultaneously. To achieve this we introduce a variation of the standard message passing framework. With this new framework a subset of the unconnected model’s layers can each be graph-connected – exploiting useful graph structure information during the forward pass. We demonstrate that this new approach improves the accuracy of using only a pre-trained or fine-tuned model and outperforms a stand-alone GNN on a fixed embedding.\n\nWe call this new approach GraNet (Graph-connected Network), and in summary, this paper has the following contributions:\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n• We provide new datasets and a rich set of accompanying embeddings to better test the\n\nperformance of GNNs.\n\n• We empirically demonstrate that only some existing GNNs improve on unconnected model accuracy and those that do vary depending on the embeddings used. We urge that unconnected models be used as a baseline for assessing GNN performance.\n\n• We provide a new method, named GraNet, that combines GNNs and models (fine-tuned or\n\ntrained from scratch) to efficiently exploit the graph structure in raw data.\n\n• We empirically show that GraNet outperforms both unconnected models (the strong base-\n\nline) and GNNs on a range of datasets and accompanying embeddings.\n\n2 RELATED WORK\n\nGraph Augmented Networks Chen et al. (2021) introduce Graph-augmented MultiLayer Perceptrons (GA-MLPs) as a simplified alternative to Graph Neural Netwroks (GNNs). These models involve a two step process - augmenting the node features of the graph based on the topology and then using these node features applying a learnable function at the node level. This allows a fixed graph operator and two sets of MultiLayer Perceptrons (MLPs) be used to extract features from the graph. This approach is related to similar simplified GNN techniques (Wu et al., 2019; Nt & Maehara, 2019). The paper proves that this simplified approach is not as expressive as standard GNNs when looking at the Weisfeiler-Lehman test for distinguishing non-isomorphic graphs. This suggests that GNNs are well suited for inferring information based on graph structure but the paper does not comment on which approach is best in practice. We differ in our approach to augmenting networks with graph structure by using existing GNNs and do not attempt to simplify the network. We do provide a graph-connected MLP but this looks at adding message passing to MLPs rather than separte funcitons on the graph data.\n\nEffect of training on GNN performance Shchur et al. (2018) look at the effect of hyperparameters and training in GNNs to show that these have dramatic effect on model ordering. Simply changing the split on a dataset caused large changes in accuracy and which GNN performed best, even though the hyperparameters of the GNNs remained constant. We show similar large difference when considering different embeddings with the same splits across embeddings.\n\nAblation studies on GNNs Further to these discoveries Nt & Maehara (2019) demonstrate that GNNs only utilise the graph structure to de-noise already highly informative features. They go as far as to demonstrate in certain conditions GNNs and MLPs perform the same. Chen et al. (2019) demonstrate that linearising the graph filter stage of GNNs does not hinder but actually increases the performance. Similarly Wu et al. (2019) simplify GNNs by removing non-linearity between layers allow for pre-computing the k message passes. This reduces graph representation learning to a simple linear regression. In all of these cases they demonstrate that the major contribution of GNNs is in their graph structure capabilites. We do not analyse these aspects but look at how this capability can be used in existing unconnected networks.\n\n3 BACKGROUND\n\nTable 1: An overview of popular datasets\n\nName\n\nInfo Source Classes\n\nFeature Length\n\nEmbedding\n\nAmazon (Shchur et al., 2018) AmazonProducts (Zeng et al., 2019) Flickr (Zeng et al., 2019) Reddit (Zeng et al., 2019; Hamilton et al., 2017) Cora (Kipf & Welling, 2017) CiteSeer (Kipf & Welling, 2017) PubMed (Kipf & Welling, 2017)\n\nText Text Image Text Text Text Text\n\n10, 8 107 7\n41 7\n6 3\n\n767, 745 200 500 602 1,433 3,703 500\n\nBag of Words 4-gram with SVD Bag of Words Avg. GloVe vectors Bag of Words Bag of Words Bag of Words\n\nWe compare our new method (GraNet) against some standard Graph Neural Networks to demonstrate the improvements that GraNet makes in classifying datasets.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nKipf & Welling (2017) introduce GCN (Graph Convolutional Networks) – a method of applying convolutional layers from CNNs to graph neural networks. It focuses on spectral filters applied to the whole graph structure rather than at the node level.\n\nHamilton et al. (2017) introduce the GraphSAGE model which builds on prior work from GCN focusing on individual node representations. This gives rise to the iterative message passing process on the node level. Though simpler than newer models we find that this approach, when given the right embedding style, can outperform some recently published GNNs.\n\nVeliˇckovi ́c et al. (2018) introduce the idea of graph attention which alters how a node aggregates its neighbours representation. This adds an additional attention mechanism to discern which aspects of the node representations in a nodes neighbourhood are important at a given layer.\n\nBrody et al. (2021) provide a more attentive version based on the graph attention system introduced in Veliˇckovi ́c et al. (2018). We base the graph attention mechanism used in our GraNet models on this improved version of graph attention. We provide both versions of graph attention in our results to compare to our new approach.\n\n3.1 NOTATIONS\n\nGraph Data Let G(V, E, X) denote a graph where V = {v1, v2, ..., vn} is the set of nodes and N = |V| is the number of nodes in the graph, E is the set of edges between nodes in V such that ei,j ∈ E denotes a directed connection from node vi to node vj, ei,j may itself be a feature vector. We say each node vi has a neighbourhood Ni such that vj ∈ Ni ⇐⇒ ej,i ∈ E and we say that vj is a neighbour node to vi. Where X is the raw data matrix where X:,i = xi where xi is the feature vector for node vi.\n\nEmbeddings There normally exists a transformation function, fe, to project the raw data to a more compact feature Xe space such that Xe = fe(X) For instance, we can transform a set of images (X ∈ RN ×C×H×W , where C, H and W are the number of channels, width and height of an image) to 1D features (Xe ∈ RN ×F , where F denotes the feature dimension). In this case, we have an embedding function fe : RN ×C×H×W → RN ×F for the dimensional reduction.\n\nThis paper puts a special focus on the design of fe, and reveals later how the design choice of fe can influence the performance of GNN models without making any changes to the underlying data G(V, E, X). An overview of popular datasets, and the embeddings that they use, is presented in Table 1. We see that the popular graph datasets (Zeng et al., 2019; Kipf & Welling, 2017; Shchur et al., 2018; Hamilton et al., 2017) focus heavily on Bag of Words (BoW) and word vectors. This implies that current GNNs are being tested on and designed for a very narrow class of embedding styles. A more detailed discussion is available in Appendix A.\n\nGraph Neural Networks Current GNNs can be thought of as message passing layers, the l-th layer can be represented as\n\nhl\n\ni = γθγ\n\n(cid:0)hl−1\n\ni\n\n, ψj∈N(i)\n\n(cid:0)φθφ\n\n(cid:0)hl−1\n\ni\n\n, hl−1\n\nj\n\n, ej,i\n\n(cid:1)(cid:1)(cid:1)\n\n(1)\n\nwhere ψ is a differentiable aggregation function and γθγ and φθφ represent differentiable functions with trainable parameters θγ and θφ respectively. hl i is the node representation of vi at layer l, with h0\n\ni = fe(xi).\n\nWe focus on the improved graph attention mechanism (Brody et al., 2021) when looking at potential GNNs for GraNet. Using this new notation we can formulate it as such\n\nαij =\n\n(cid:80)\n\nexp(aT LeakyReLU(θ[hi||hj]))\n\nk∈Ni\n\nexp(aT LeakyReLU(θ[hi||hj]))\n\n(2)\n\nwhere a is a learnable parameter representing the attention of the network. Further discussion of graph attention and why this is useful in GraNet is available in Appendix C.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nPre-trained models are specific neural network architecturwa that have been trained on a dataset D for a specific task, this could be ImageNet classification (Deng et al., 2009) for vision networks (He et al., 2016) or the entire English Wikipedia for language models (Liu et al., 2019). These networks therefore have pre-trained weights θ that can be loaded into the model for further training or evaluation.\n\nWe denote these pre-trained models as fθ that is parameterised by weights θ. We say that a pre- : RF → trained model has a set of functions {f 1 , ..., f M θM θ1 RF ′ → RF ′′ and F is the feature dimension. A pass through a single layer, l, of a (x), shorthand for f l(x; θl). If we concatenate these layers to form a full pass\n\n: RF ′ and f i+1 θi+1 network would be f l θl through the network, we obviously have fθ(x) = f M θM\n\n} for an M -layer model, where f i θi\n\n(...f 2 θ2\n\n(f 1 θ1\n\n(x)))\n\n, f 2 θ2\n\nFine-tuning is therefore adapting θ to a new dataset D′ which is related to G(V, E, X) as illustrated in Section 3.1. In a more standard setup, the target dataset we fine-tune to has the same underlying data-structure as the pre-training dataset, for instance, they might both be images, but the target dataset is a different type of classification. This may involve adding, removing or altering specific layers within the model or simply retraining the model with different labels on D.\n\nIf the architecture of a pre-trained model is altered then a new weight matrix θ′ must be created from θ by adding, removing or reshaping weights.\n\nFreezing layers is the process whereby a selection of weights θf ⊂ θ do not have gradients and thus do not change during back-propagation.\n\nThese ideas allow us to alter these pre-trained models to use information about the graph connections whilst utilising their pre-trained weight θ.\n\nBlending and Fine-tuning models is therefore the process of using existing models, fθf and gθg , with defined set of layers, {f 1 }, and creating a new model, hθf,g , such that hi . We can use pre-trained weights θf or θg and/or freeze either model, and where one of these models is a GNN we say this is fine-tuning on a graph dataset.\n\nθf 1 = f j θf\n\nθf 2 ◦ gk θg\n\n} and {g1\n\n, ..., gM\n\n, ..., f N\n\n, f 2\n\n, g2\n\nθgM\n\nθf,g\n\nθf N\n\nθg1\n\nθg2\n\nUnconnected models are models that, unlike GNNs, do not use any information about graph connections within a dataset. These are trained on datasets that are not graph-connected where each datapoint is consider isolated. We focus on complex unconnected models with multiple layers, such as vision networks, which we call large models. Due to training cost we use pre-trained large models.\n\n4 METHOD\n\nOur proposed method of converting standard neural network models into graph-connected models blends the two networks. This approach can easily be broken down into individual layers. Taking f l as the l-th layer in a standard model, fθ, where we may be given pre-trained weights θ we can θl describe this new layer by reformulating Equation (1) as such\n\n(cid:16)\n\nhl\n\ni = γθγ\n\nhl−1\n\ni\n\n, ψj∈N(i)\n\n(cid:16)\n\n(cid:16)\n\nφθφ\n\nf l−1 θl−1\n\n(cid:0)hl−1\n\ni\n\n(cid:1) , f l−1\n\nθl−1\n\n(cid:0)hl−1\n\nj\n\n(cid:1) , ej,i\n\n(cid:17)(cid:17)(cid:17)\n\n(3)\n\nwhere h0\n\ni = xi rather than applying an embedding function.\n\nFigure 1 is a representation of Equation (3) specifically in the case where the pre-trained model is a CNN and (f l−1 ) is a convolutional layer. The light blue regions perform the standard convolution θl−1 feature extraction, these extracted feature maps are then adjusted, in the light orange region, by a Message Passing layer with graph attentions. These two representations are then combined. The light blue regions and resulting channel stacks, Ai ∈ R1×Cl×Hl×Wl through An ∈ R1×Cl×Hl×Wl , represent the forward pass through a single CNN layer f l−1 . Ai represents the θl−1 forward pass of the current node hi and A{j,...,n} represent the forward pass of the neighbours of hi. The light orange region and resulting channel stacks, B, represent the graph-based Message\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Graphical representation of how our proposed GraNet layer operates for image networks\n\nPassing stage where the new representations are altered (φθφ), aggregated (ψ) and finally combined with the current node representation (γθγ ), following the description in Equation (3).\n\n4.1 GRANET FOR FLICKR V2\n\nInterconnecting every layer in a large pre-trained model is computationally intensive. Therefore, rather than interconnect every single layer in a pre-trained network fθ we can graph-connect only the final layer. We thus split the model into two portions: the first set of unconnected layers and the final GraNet layer.\n\nWe can therefore look at equation Equation (3) and see in this case that we partially carry out the forward pass of fθ which we will denote as fe and then carry out a forward pass through a GraNet layer. This allows us to ignore all the steps involved in applying fe and focus on the final GraNet layer. Denoting this final GraNet as g, and output classification y, we achieve the following equation\n\ny = g(fe(X))\n\n(4)\n\nfe(X) is the same as described in Section 2 and indeed if we were to freeze fθ this would be equivalent to training GraNet with a single layer on the embeddings created by embedding function fe. So instead we also allow fe to train thus fine-tuning the weights θ of fθ. This indirectly allows fθ to learn the graph structure by providing g with better embeddings.\n\nAs GraphSAGE performed the best on ResNet embeddings we use this as our GNN, g. As fθf is already pre-trained but gθg is not fine-tuning the model produces poor results. We therefore initially train gθg on a frozen feθf , after this short training period we unfreeze feθf allowing both to train fine-tuning the weights θg and θf .\n\n4.2 GRANET FOR AMAZON\n\nIn the case of the Amazon dataset we found that Bag of Words embedding performed the best. As this does not have an associated pre-trained model we design a multi-layer perceptron (MLP) to compare against. We then convert all the layers within the MLP to GraNet layers. This method is therefore not fine-tuned but trained from initialised weights.\n\nWe also find that GAT (Brody et al., 2021; Veliˇckovi ́c et al., 2018) models perform the best on this task. We therefore use graph attention message passing as shown in Equation (2) for our GraNet model. Keeping in line with Equation (3) the new graph attention mechanism becomes\n\nαij =\n\n(cid:80)\n\nexp(aT LeakyReLU([f l θl\n\nk∈Ni\n\nexp(aT LeakyReLU([f l θl\n\n(hj)]))\n\n(hi)||f l θl (hi)||f l θl\n\n(hj)]))\n\n(5)\n\nHowever, a single layer of an MLP is f l θl becomes\n\n(x) = θlx. This would therefore mean that Equation (6)\n\n5\n\n{}, ... ,Under review as a conference paper at ICLR 2023\n\nαij =\n\n(cid:80)\n\nexp(aT LeakyReLU([θlhi||θlhj]))\n\nk∈Ni\n\nexp(aT LeakyReLU([θlhi||θlhj]))\n\n(6)\n\nThis is very similar to Equation (2) with the only difference being when we apply the weight matrix, concatenation and attention mechanism. We find this model behaves the same as GAT2 and therefore, given the small size tried a different approach as a comparison.\n\nRather than use the vector parameter a we introduce a linear function aθa : R2F → RF ′ that takes the LeakyReLU of the concatenation of a node, fθl (hi), and its neighbour, fθl (hi), as input. The result is a feature vector in a new feature space, though in our case we have F ′ = F . This allows more complex interactions between the node representations to be exploited by our attention. This approach is too costly to apply to a pre-trained CNN as the size of θa is far too large.\n\n5 EXPERIMENTAL SETUP\n\nFor all test results we run 3 train-test runs each with a different random seed and take the arithmetic mean and include the standard deviation. The three random seeds are the same for every entry in the table for fair comparison. The architecture used for each benchmark GNN is identical across all datasets and embeddings. Training takes 300 epochs of training unless otherwise stated.\n\nWhere a Graph Neural Network (GNN) is used as part of a GraNet model (as described in Section 4) the same sampler is used for consistency. The specific architectures used are designed using the hyperparameters in Zeng et al. (2019). In the case of GraphSAGE the learning rate was decreased to improve convergence. For specific details of each architecture and learning rate see Appendix E.\n\n5.1 DATASETS\n\nAn overview of the datasets is provided below with the specific metrics for each dataset shown in Table 6. A more detailed discussion is available in Appendix B. It is important to note that though these datasets mirror prior datasets due to the need for raw data we diverge from these datasets. Therefore we do not make any direct comparisons to previous datasets though the results we achieve on our new datasets are on par with results seen in prior papers.\n\n5.1.1 FLICKR V2\n\nThe underlying data X is raw images and so Convolutional Neural Networks (CNNs) are used as embedding functions. As a sample of existing pre-trained CNNs we use ResNet18, ResNet50 (He et al., 2016) and VGG16 (Simonyan & Zisserman, 2015). In all three cases we use the pre-trained models provided by TorchVision, using the feature vectors after the final pooling stage before the classification stage.\n\nIt is important to note that there is no Bag of Words (BoW) embedding for Flickr v2 because there is no sensible object that can be considered a “word” for raw image data. The available Flickr (Zeng et al., 2019) uses BoW because the underlying data is image text descriptions not raw images. This limits how a GNN, or general neural network, can classify images as the images must first be processed to provide text descriptions.\n\n5.1.2 AMAZONELECTRONICS AND AMAZONINSTRUMENTS\n\nThe underlying data X is text so text classification transformers such as RoBERTa (Liu et al., 2019) are ideal, specifically we use pre-trained RoBERTa. We extract three different embeddings from RoBERTa using the pre-trained model provided by fairseq toolkit (Ott et al., 2019). The first is the byte pair tokenisation used by RoBERTa, the second is the feature extraction provided by fairseq which occurs after RoBERTa’s transformer heads and before classification, and the final is the feature vector present before the last fully connected layer. Due to restrictions in the token size for RoBERTa we remove all nodes that have reviews with greater than 512 tokens.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nWe also provide the standard Bag of Words embedding as in the case of text classification this is a common embedding practice, keeping in line with prior datasets (Kipf & Welling, 2017; Zeng et al., 2019) we use the top 500 words to create our Bag of Word embeddings.\n\n6 EVALUATION\n\n6.1 RE-EVALUATING EMBEDDINGS\n\nLanguage Tasks Table 2 demonstrates that the particular embedding function used determines which GNN model performs best on the dataset. For instance, GAT2 has the best performance if the data is embedded as BoW (Bag of Words), but performs poorly on other embeddings generated from RoBERTa. GraphSAGE, in contrast, performs poorly on BoW but shows a good performance otherwise. Thus which embeddings are used when comparing models has a large effect on which model appears to be the better model. We see the same effect in Table 3. Comparing the two tables we see that in the case of the RoBERTa Encoded and RoBERTa embeddings the order of the models changes, this likely comes from the training effects described in Shchur et al. (2018).\n\nTable 2 also signifies the importance of good embeddings as in this case BoW is better than RoBERTa. The complexity of the embeddings does not necessarily improve the efficiency of the classification. This ties into the results of Nt & Maehara (2019) as the BoW is more informative of the classification containing the label words in the BoW vector. What is also important to note is that when looking at the performance of the GNN against the unconnected model (∆ ↑) we see consistency in the difference across the RoBERTa encodings. This suggests that the “quality”, denoted by how well the simple MLP performs, is a strong indicator of how well a model will perform, rather than the model architecture itself.\n\nWe see an overall decrease in accuracy in Table 3 across the models but this attributed to the fact that there are more classes for the dataset. We also see that for RoBERTa embeddings the MLP performs poorly though it does occasionally improve on the simpler models, primarily GCN. With more resources it would be better suited to fine-tune RoBERTa (or some other language transformer) to our dataset and use this model as our unconnected model.\n\nTable 2: Test accuracy on AmazonElectronics with different embeddings compared against a standard unconnected MLP model. The embedding styles are explained in Appendix B, Table 6.\n\nModel\n\nBag of Words\n\nByte Pair\n\nRoBERTa Encoded\n\nRoBERTa\n\nEmbedding styles\n\nUnconnected MLP\n\n71.6% (+0.0)\n\n21.6% (+0.0)\n\n55.8% (+0.0)\n\n51.9% (+0.0)\n\nGCN GAT GAT2 GraphSAGE (Random) GraphSAGE (Neighbour)\n\n69.1% (-2.5) 81.1% (+10.5) 81.8% (+10.2) 71.3% (-0.3) 76.4% (+4.8)\n\n21.7% (+0.1) 22.2% (+0.6) 22.2% (+0.6) 26.8% (+5.2) 40.4% (+20.8)\n\n22.7% (-33.1) 46.1% (-9.7) 41.8% (-14.0) 57.0% (+1.2) 67.8% (+12.0)\n\n22.3% (-29.6) 40.3% (-11.6) 35.7% (-16.2) 53.7% (+1.8) 66.4% (+12.5)\n\nTable 3: Test accuracy on AmazonInstruments with different embeddings compared against a standard unconnected MLP model. Included is the difference ∆ of each model to the unconnected MLP and the standard deviation of each result. The embedding styles are explained in Appendix B.\n\nModel\n\nBag of Words\n\nByte Pair\n\nRoBERTa Encoded\n\nRoBERTa\n\nEmbedding Styles\n\nUnconnected MLP\n\n66.1% (+0.0)\n\n21.0% (+0.0)\n\n43.9% (+0.0)\n\n39.8% (+0.0)\n\nGCN GAT GAT2 GraphSAGE (Random) GraphSAGE (Neighbour)\n\n64.0% (-2.1) 79.3% (+13.2) 79.4% (+13.3) 67.5 (+1.4)% 72.6% (+6.5)\n\n20.8% (-0.2) 21.6% (+0.6) 21.2% (+0.2) 23.9% (+2.9) 43.4% (+22.8)\n\n20.4% (-23.5) 47.5% (+3.6) 49.8% (+5.9) 45.1% (+1.2) 62.4% (+18.5)\n\n20.4% (-19.4) 46.1% (+6.3) 47.8% (+8.0) 41.9% (+2.1) 59.9% (+20.1)\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nVision Tasks Table 4 demonstrates the same pattern, that the embedding function (in this case a pre-trained vision model) influences which GNN performs the best. It is interesting to note that we see that none of the GAT models achieve the highest accuracy on any of the Flickr v2 embeddings. Instead, similar to the RoBERTa embeddings, we see that GraphSAGE performs the best.\n\nIt is important to note for VGG16 that the surprisingly poor performance of GNNs is more likely due to the large vector size of more than 25K. With a smaller embedding space better results on par with ResNet may be achieved. Of course, there is also the possibility that the embeddings provided by VGG16 are inferior to ResNet.\n\nTable 4: Test accuracy on Flickr v2 with different embeddings, compared against the corresponding unconnected vision model. Included is the difference ∆ of each model to the unconnected model and the standard deviation of each result. The details of the embedding styles are available in Appendix B.\n\nModel\n\nEmbedding Styles\n\nResNet18\n\nResNet50\n\nVGG16\n\nUnconnected Model\n\n45.2% (+0.0)\n\n46.9% (+0.0)\n\n47.0 (+0.0)\n\nGCN GAT GAT2 GraphSAGE (Random) GraphSAGE (Neighbour)\n\n41.8% (-3.4) 38.1% (-7.1) 42.1% (-3.1) 45.4% (+0.2) 45.8% (0.6)\n\n38.3% (-8.6) 37.1% (-9.8) 41.0% (-5.9) 47.0% (+0.1) 44.5% (-2.4)\n\n35.5% (-11.5) 27.3% (-19.7) 34.2% (-12.8) 35.2% (-11.8) 34.5% (-12.5)\n\nTables 2 and 3, compared to Table 4, have far larger increases in accuracy from the best performing GNN compared to the unconnected models. This is mainly due to the fact that we were unable to fully fine-tune RoBERTa to our datasets given limited hardware and time. We hypothesise that the improvements seen in Table 2 would be smaller when compared to a fine-tuned RoBERTa. Similarly, we did not attempt to create a GraNet model using VGG16 as the results on Flickr v2 are worse than the ResNet models and therefore for Flickr v2 a GraNet ResNet is ideal.\n\nTables 2 and 3, specifically with BoW, are the only instances where GAT and GAT2 are the best models. Furthermore, the entries for BoW follow results from past papers, in contrast to the results shown on all other embeddings. With the prevalence of BoW as shown in Table 1 it begs the question as to whether we are optimising for BoW extraction rather than graph information extraction.\n\nFrom these results in both language and vision tasks, we make the following key observations:\n\n• The function fe used to extract embeddings influences the performance of different GNNs, so the embeddings should influence the choice of GNNs regardless of the underlying data. • GNN models do not always outperform simple unconnected models, graph structure is not\n\nenough to compete against good classifiers.\n\n• The choice of an embedding function contributes more to the final performance compared\n\nto the choice of a GNN model.\n\n6.2 GRANET\n\nTable 5 demonstrates that extending our standard models with graph connections provides a significant improvement. We see that these GraNet models beat the best performing GNN. The table includes two forms of GraNet models, the fine-tuned models and the trained models.\n\nFine-tuned In Flickr v2, both GraNet and Unconnected Model use either ResNet18 or ResNet50 weights pre-trained on ImageNet (Deng et al., 2009) and then fine-tuned on the target dataset. We observe a significant increase (+1.5%, +1.8%) from the GraNet style of training compared to both unconnected models, and improvement (1.1%, 1.7%) on the best performing GNN model.\n\nIntuitively, GraNet layers reframe graph representation learning from training a GNN on a fixed pre-extracted embedding to training both the GNN and the embedding function (fe) together on the underlying data. There are obviously trade-offs between time and versatility as rather than just training a GNN the embedding function must be trained as well. But the current approach of GNN\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\ntraining on images requires human, or other annotators, which is a hidden time cost. By combining the embedding function and GNN we provide a more general model that does not need external annotators and thus can work on any raw images.\n\nTrained from Scratch In the Amazon datasets we do not have any pre-trained and train the GraNet model from initialised weights rather than fine-tuning. However, we still observe an improvement on both the unconnected model and best performing GNN. The improvement is smaller than in Flickr v2 (+0.1%, +0.3% compared to 1.1%, 1.7%). But the training time is quicker (∼ 1hr compared to ∼ 10hrs for 300 epochs).\n\nIn this case we do not have a general model as we rely on BoW embeddings for the GraNet model as well. But we can see that improvements are possible on existing GNN techniques on the embeddings themselves, without having to use large pre-trained models.\n\nTable 5: Comparison of GraNet models against the best performing GNNs for a specific embedding. Unconnected Model refers to ResNet18 or ResNet50, in the case of Flickr v2, and an MLP otherwise. The setup of each GraNet model is detailed in Sections 4.1 and 4.2 and the specifics of the 3 datasets can be found in Appendix B.\n\nModel\n\nFlickr v2\n\nElectronics\n\nInstruments\n\nResNet18\n\nResNet50\n\nBag of Words\n\nBag of Words\n\nUnconnected Model\n\n45.2% (+0.0)\n\n46.9% (+0.0)\n\n71.6% (+0.0)\n\n66.4% (+0.0)\n\nGAT2 GraphSAGE (Random) GraphSAGE (Neighbour)\n\n42.1% (-3.1) 45.4% (+0.2) 45.8% (+0.4)\n\n41.0% (-5.9) 47.0% (+0.1) 44.5% (-2.4)\n\n81.8% (+10.2) 71.3% (-0.3) 76.4% (+4.8)\n\n79.4% (+13.0) 67.5% (+1.1) 72.6% (+6.2)\n\nGraNet\n\n46.7% (+1.5)\n\n48.7% (+1.8)\n\n81.9% (+10.3)\n\n79.7% (+13.3)\n\nWe also make the following key observations:\n\n• Graph-connecting a pre-trained network improves performance by fine-tuning feature ex-\n\ntraction based on the graph structure.\n\n• GraNet models outperform their counterparts by facilitating fine-tuning on the graph\n\ndataset.\n\nThe architecture for each GraNet is a mixture of the best performing GNN for that embedding and the embedding extraction model (fe) (a Multi-Layer Perceptron in the case of Bag of Words).\n\n7 CONCLUSION\n\nIn this paper, we reveal that GNN model designs are overfitting to certain embeddings styles (e.g. BoW and word vectors). To demonstrate this we introduced three new datasets each with a range of embedding styles to be used as a more comprehensive benchmark of GNN performance.\n\nWe demonstrated that embedding style influences the performance of GNNs regardless of the underlying dataset. Equally, the quality of the embedding, measured by how well an unconnected baseline model performs, is a greater indicator of GNN accuracy than the GNN architecture chosen. We therefore stress the importance of creating high quality embeddings and choosing the best GNN architecture based on the style of embedding created rather than using (or trying to improve) the same GNN model for every task.\n\nWe then introduced a new approach named GraNet. This approach allows for any large pre-trained model to be fine-tuned to a graph-connected dataset by altering the standard message passing function. In this way we exploit graph structure information to enhance the pre-trained model performance on graph-connected datasets. We have demonstrated that GraNet outperforms both unconnected pre-trained models and GNNs on a range of datasets.\n\nThere is an increasing trend towards large pre-trained models and graph-connected datasets. Our work demonstrates potential pitfalls in the way GNN architectures are currently evaluated and proposes a new technique to fully exploit the benefits of pre-trained models within a GNN.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n8 ETHICS STATEMENT\n\nWith all computer based research there is the issue of carbon footprint. Each train, validation and test run of a model requires electricity which is currently largely sourced from fossil fuels. This training cost carbon footprint is present in this picture, and attempts to keep this low by running short test on new iterations rather than long runs which could waste GPU time and energy.\n\n9 REPRODUCIBILITY\n\nWe discuss how we setup all of our experiments in Section 5 including how many runs we completed for each data-point. the specific random seeds that we used for the three runs were 42, 9001 and 27032002. The number of runs for each run is also listed here and for the models that have a different number of epochs the specifics are mentioned in Appendix E.\n\nAll test runs were carried out on a single Nvidia A100 80GB GPU, we used pytorch 1.12 with cuda 11.3. All other packages where installed based on these requirements.\n\nAppendix E also contains the specific architectures, samplers, learning rates and learning rate schedulers used in each of the model test runs. These are also linked back to Zeng et al. (2019) to provide comparable results to the ones achieved in that paper for similar datasets Flickr and Amazon.\n\nAppendix B details the specifics of how we formed our datasets including how we downloaded the raw data, steps we took to wash the data, how we formed the graph and how we created our embeddings. The specifics of how we labelled the data and the labels that we chose are also available.\n\nWe have anonymised the dataset including the specific code used to create GraNet models, the config files used and the code to download, wash and build the datasets available here.\n\nREFERENCES\n\nShaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? CoRR,\n\nabs/2105.14491, 2021. URL https://arxiv.org/abs/2105.14491. 3, 5, 15\n\nLei Chen, Zhengdao Chen, and Joan Bruna. On graph neural networks versus graph-augmented In International Conference on Learning Representations, 2021. URL https://\n\n{mlp}s. openreview.net/forum?id=tiqI7w64JG2. 2\n\nTing Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on\n\ngraph classification. 2019. 2\n\nTat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yan-Tao Zheng. NUSWIDE: A real-world web image database from national university of singapore. In Proc. of ACM Conf. on Image and Video Retrieval (CIVR’09), Santorini, Greece., July 8-10, 2009. 14\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. 4, 8\n\nM. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The International Journal of Computer\n\nPASCAL visual object classes challenge: A retrospective. Vision, 111(1):98–136, January 2015. 14\n\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive Representation Learning on Large\n\nGraphs. In NIPS, pp. 1024–1034, 2017. 1, 2, 3, 13, 18\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. 4, 6\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020. 12\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMark J. Huiskes and Michael S. Lew. The MIR Flickr retrieval evaluation. In MIR ’08: Proceedings of the 2008 ACM International Conference on Multimedia Information Retrieval, New York, NY, USA, 2008. ACM. 14\n\nThomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Net-\n\nworks. In ICLR, 2017. 2, 3, 7\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. 2019. 4, 6\n\nJulian McAuley and Jure Leskovec. Image labeling on a network: using social-network metadata for image classification. In European conference on computer vision, pp. 828–841. Springer, 2012. 12, 13\n\nJulian McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’15, pp. 43–52, New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450336215. doi: 10.1145/2766462.2767755. URL https://doi.org/10.1145/2766462.2767755. 13\n\nHoang Nt and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.\n\narXiv preprint arXiv:1905.09550, 2019. 2, 7\n\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48–53, 2019. 6\n\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G ̈unnemann. Pitfalls\n\nof graph neural network evaluation. 2018. 2, 3, 7, 13\n\nK Simonyan and A Zisserman. Very deep convolutional networks for large-scale image recognition.\n\n2015. 6\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua\n\nBengio. Graph Attention Networks. In ICLR, 2018. 3, 5, 14\n\nFelix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. SimIn International conference on machine learning, pp.\n\nplifying graph convolutional networks. 6861–6871. PMLR, 2019. 2\n\nHanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. In International Conference on Learning Representations, 2019. 2, 3, 6, 7, 10, 12, 13, 18\n\nMarinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue\n\nnetworks. Bioinformatics, 33(14):i190–i198, 2017. 1\n\n11",
    "reference": "# Summary Of The Paper\n\nThe paper considers the impact of (pre-trained) embeddings on the performance of different GNN architectures. The datasets under considered are either images and texts with some form of \"graph structures\", e.g., based on “Co-viewed”, “Co-bought” and “Similar Items” (for the Amazon datasets). The authors fund that only some GNN models yield  an improvement in accuracy compared to the accuracy of\nmodels trained from scratch or fine-tuned on the underlying data without utilizing the graph connections. As an alternative,the authors propose Graph-connected Network (GraNet) layers to better leverage existing unconnected models within a GNN, and show 7.7% and  1.7% improvements on Flickr v2, GraNet over  GAT2 and GraphSAGE.\n\n# Strength And Weaknesses\n\nStrength: \n\n + Exploring  the impact of (pre-trained) embeddings on the performance of different GNN architectures. \n\n Weaknesses: \n\n  - The paper is rather poorly written and claims more than what is actually accomplished. \n\n  - The proposed GraNet framework is difficult to understand and needs better explanation and  justification.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nReading the abstract and introduction, it appears that the paper addresses the general issue of graph embeddings and their impacts on the performance of GNN architectures. However, the paper is really about a very specific setting: namely,  it assumes that the \"nodes\" in the so-called \"graph\"  datasets under considered are either images and texts; and they are connected in  some form of \"graph structures\", e.g., based on “Co-viewed”, “Co-bought” and “Similar Items” (for the Amazon datasets).  The \"(node) embeddings\" the paper talks about are essentially some CNN networks or MLP networks that are \"pre-trained\" on the images or texts associated with the nodes, and which are then used as \"node features\".    The \"graph structures\" on the nodes considered (based on the descriptions of the datasets) are rather \"secondary\" in that they are comments, labels provided by humans or activities associated with humans, thus rather \"weak\" or \"ad hoc\" in a sense. It is no wonder that the authors find most GNNs do not necessarily provide additional improvements over the \"fine-tuned\" models without utilizing the graph structures.\n\nI find the description of the  proposed GraNet framework is rather confusing and needs better explanation and justification. For example,  I don't understand how the light blue regions work, in particular, with respect to how the forward pass of the current $h_i$  and the forward pass of its neighbors mean. You also mentioned about \"The light orange region and resulting channel stacks, B, represent the graph-based Message Passing stage where the new representations are altered, aggregated and finally combined\nwith the current node representation, following the description in Equation (3).  How are they altered, aggregated and combined? Or are you simply stacking a GNN on the \"embeddings\"? Are these done in a layer by layer fashion, or only at the last stage (i.e., taking the outputs of a CNN applied to the images associated with the nodes)?\n\nWhy do you think your findings can be generalized to other datasets (where nodes are not images or texts)?\n\n# Summary Of The Review\n\nThe paper aims to investigate the impact of (pre-trained) embeddings on the performance of different GNN architectures. However, both the problem considered and the method proposed are really specific to datasets where nodes are images or texts that are \"connected\" via some (weak) forms of connections. The findings are hard to interpret, and they do not shed light on the performance of general GNNs.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nNo ethical issues."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nATTRIBUTES RECONSTRUCTION IN HETEROGENEOUS\n\nNETWORKS VIA GRAPH AUGMENTATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nHeterogeneous Graph Neural Networks(HGNNs), as an effective tool for mining heterogeneous graphs, have achieved remarkable performance on node classification tasks. Yet, HGNNs are limited in their mining power as they require all nodes to have complete and reliable attributes. It is usually unrealistic since the attributes of many nodes in reality are inevitably missing or defective. Existing methods usually take imputation schemes to complete missing attributes, in which topology information is ignored, leading to suboptimal performance. And some graph augmentation techniques have improved the quality of attributes, while few of them are designed for heterogeneous graphs. In this work, we study the data augmentation on heterogeneous graphs, tackling the missing and defective attributes simultaneously, and propose a novel generic architecture—Attributes Reconstruction in Heterogeneous networks via Graph Augmentation(ARHGA), including random sampling, attribute augmentation and consistency training. In graph augmentation, to ensure attributes plausible and accurate, the attention mechanism is adopted to reconstruct attributes under the guidance of the topological relationship between nodes. Our proposed architecture can be easily combined with any GNN-based heterogeneous model, and improves the performance. Extensive experiments on three benchmark datasets demonstrate the superior performance of ARHGA over strate-of-the-art baselines on semi-supervised node classification.\n\n1\n\nINTRODUCTION\n\nHeterogeneous information networks(HINs)(Yang et al. (2020); Shi et al. (2016); Shen et al. (2017)), which contain multiple types of nodes and edges, have been widely used to model complex systems and solve practical problems. Recently, heterogeneous graph neural networks have emerged as prevalent deep learning architectures to analyze HINs and shown superior performance in various graph analytical tasks, such as node classification(Wang et al. (2019a); Yun et al. (2019)) and link prediction(Fu et al. (2020); Zhang et al. (2019)). Most HGNNs follow a message-passing scheme in which each node updates its embedding by aggregating information of its neighbors’attributes. Such message-passing scheme usually requires that all nodes have complete and reliable attributes, which is not always satisfied in practice due to resource limitation and personal privacy, resulting in missing and defective attributes.\n\nIn general, the attribute missing in heterogeneous graphs means that attributes of partial nodes are entirely missing, compared to that in homogeneous graphs, is more frequent and complex. Take DBLP(Sun & Han (2013)) as an example, the network has four types of nodes(author, paper, term and venue) and three types of links. Only paper nodes have attributes which are extracted from the keywords in their titles, while other types of nodes have no attributes. It impairs the effectiveness of the corresponding graph mining model to certain extents. In another fold, the original attributes of nodes are sometimes not ideal since heterogeneous graphs are extracted from complex systems which inevitably are subject to various forms of contamination, such as mistakes and adversarial attacks, making error propagation and greatly affecting the process of message-passing. This suggests the need for effective approaches able to complete missing attributes and calibrate defective attributes in heterogeneous graphs simultaneously.\n\nTo alleviate the effect incurred from missing attributes, the existing models usually adopt imputation strategy, such as neighbor’s average or one-hot vector as done in MAGNN(Fu et al. (2020)). These\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nimputation methods are nonoptimal because graph structure information is ignored and only rare useful information is provided, hampering subsequent analysis. An alternative technique to tackle this issue is to consider graph topology information and inject it into the completion models. The work of Jin et al. (2021) and He et al. (2022) has shown a significant boost on node classification tasks. But both methods naturally assume that the original attributes are reliable, which is not easy to satisfy in real-world applications. In another concern of research, some graph augmentation techniques are adopted to calibrate original attributes to improve the quality and have shown a promising performance(Xu et al. (2022); Zhu et al. (2021)). However, these methods are deficient for heterogeneous graphs as they are not capable of encoding complex interaction. Further, existing methods either only complete missing attributes or only improve the quality of attributes, while it is worthy making efforts to solving both problems at the same time.\n\nIn this paper, we attempt to deal with the missing and defective attributes simultaneously in heterogeneous graphs, and propose a novel framework for Attributes Reconstruction in Heterogeneous networks via Graph Augmentation(ARHGA). ARHGA repeatedly sample nodes to perform attribute augmentation to obtain multiple augmented attributes for each node, and then utilize consistency training(Xie et al. (2020)) to make the outputs of different augmentations as similar as possible. Moreover, to ensure the augmented attributes more accurate, node topological embeddings are learned through HIN-embedding methods(Dong et al. (2017); Fu et al. (2017); Shang et al. (2016); Wang et al. (2019b)) to capture graph structure information as guidance. In this way, ARHGA effectively enhances the performance of existing GNN-based heterogeneous models in aid of the reconstructed attributes.\n\nContributions. In summary, the main contributions of this paper are as follows:\n\n• We propose a generic architecture of graph augmentation on heterogeneous networks for\n\nattributes reconstruction, focusing both on the missing and defective attributes.\n\n• We design an effective attribute-wise augmentation strategy implemented by attention mechanism, which integrates topology information to increase the reliability of the reconstructed attributes.\n\n• Extensive experimental results on three node classification benchmark datasets demonstrate\n\nthe effectiveness of our proposed model.\n\n2 RELATED WORK\n\nHeterogeneous graph neural networks. Heterogeneous graphs have been widely used to solve real-world problems due to a diversity of node types and relationships between nodes. Recently, many HGNNs have been proposed to analyze HINs. HAN(Wang et al. (2019a)) learns node representations using the node-level attention and the meta-path-level attention. Fu et al. (2020) further consider the intermediate nodes and propose the node-type specific transformation, then use a hierarchical attention structure similar to HAN. Graph Transformer Networks(Yun et al. (2019)) generates a new graph by identifying useful connections between unconnected nodes on the original graph, then performs graph convolution on the new graph. Zhang et al. (2019) sample heterogeneous neighbors for each node by random walk and aggregate node and type level information. Hu et al. (2020) design meta-relation-based mutual attention to handle graph heterogeneity and implicitly learn meta paths. Nevertheless, these methods require that all nodes have complete and reliable attributes and encounter difficulties in dealing with heterogeneous graphs with the missing and defective attributes.\n\nLearning with missing attributes. Previous handcrafted approaches to fill in missing attributes rarely consider graph topology information, making inefficient attributes and compromising model performance. Several deep learning models have been explored. You et al. (2020) design a unified framework in which attribute completion is modeled as an edge-level prediction task and the label prediction as a node-level prediction task. Chen et al. (2020) develop a novel GNN framework to perform the link prediction task and the attribute completion task based on distribution matching. Despite tremendous success, they are not suitable for HINs with missing attributes because the complex interaction has not been addressed. Recent advance in HGNNs has provided new directions to solve the problem. Jin et al. (2021) attempt to complete missing attributes in a learnable manner and propose the framework HGNN-AC, which contains pre-training of topology embedding and attribute completion with attention mechanism. He et al. (2022) design an unsupervised heterogeneous graph contrastive learning approach for analyzing HINs with missing attributes. Both\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The overview of ARHGA framework. ARHGA performs random sampling (a) to produce the AN S. The attributes of nodes in the AN S are completed or calibrated through attribute augmentation (b) to generate multiple graph augmentations (c), which are fed into to shared HGNNs to construct the consistency loss (d). The upper box shows an example of missing attributes in a sampling and the lower represents an example of defective attributes in another sampling.\n\nmethods only complete the missing attributes, while our proposed method not only completes the missing attributes but also calibrates defective attributes to enhance the quality of attributes.\n\nGraph data augmentation. Recently, variants of data augmentation techniques have been explored in deep graph learning and demonstrated remarkable results. Graph data augmentation aims to generate the augmented graph(s) through enriching or changing the information from the original graph. It can be categorized into three classes: attribute-wise augmentation, structure-wise augmentation and label-wise augmentation(Ding et al. (2022)). Among the attribute-wise augmentation strategies, there are a few lines of existing works for attribute calibration/denoising. A straightforward method(Xu et al. (2022)) is to compute the gradient of specific objective functions w.r.t. the node attribute matrix and calibrate the node attributes matrix based on the computed gradient. In addition, as a special case of noisy setting on graph data, the problem of missing attributes also has been studied. Representative works include GCNMF and Feature Propagation. GCNMF(Taguchi et al. (2021)) completes the missing data by Gaussian Mixture Model(GMM). It integrates graph representation learning with attribute completion and can be trained in end-to-end manner. Feature Propagation(Rossi et al. (2021)) reconstructs the missing node attributes based on a diffusion-type differential equation on the graph. It is important to note that the above-mentioned graph augmentation methods only work on homogeneous graphs, heterogeneous attribute-wise augmentation is still an under-explored remains problem.\n\n3 METHODOLOGY\n\nIn this section, we introduce the ARHGA framework, of which the general idea is to tackle the missing and defective attributes simultaneously through an effective graph augmentation strategy, and integrate the augmentation process and HGNN module into a unified framework to benefit the performance on node classification tasks. Figure 1 illustrates the proposed framework.\n\n3.1 RANDOM SAMPLING\n\nRandom sampling randomly selects a part of nodes to form the augmentation node set(AN S) in which nodes may have no attributes or have original attributes. Specifically, given a heterogeneous graph G with node set V of n nodes, edge set E and attribute matrix X, we randomly sample a binary mask εi ∼ Bernoulli(1 − δ) for each node vi in V to determine whether vi belongs to the AN S, where δ is the probability when εi takes 0. We obtain AN S = {vi|εi = 0}, i.e., if εi = 0, then vi ∈ AN S. The attributes of nodes in the AN S are dropped in this step and reconstructed in the phase of attribute augmentation.\n\n3\n\n214537624161376(b) Attribute augmentation(a) Random sampling275...(1)Y...()MYY3An example of completing missing attributesAn example of calibrating defective attributesThe HIN with missing and defective attributes(c) Augmented HINs 21453762145376HGNN(,,)AXHGNN(,,)AXsupconShare params456(d) Consistency training Under review as a conference paper at ICLR 2023\n\nTo reconstruct attributes for all nodes, random sampling is performed repeatedly to guarantee that all nodes are selected. If the number of samplings is set as M , we give the constraint under which M should satisfy from a probabilistic view. Let P denote the probability of all nodes being selected at least once, since each node is sampled independently, we have:\n\nP =\n\n(cid:89)n\n\ni=1\n\nPi = (Pi)n,\n\nPi = 1 − P C\n\ni = 1 − P (k = 0) = 1 − δM ,\n\n(1)\n\n(2)\n\nwhere Pi and P C i are the probability that node vi is selected at least once and that is not selected respectively, k is the number of times being selected. Given a threshold τ close enough to 1, when P is greater than τ , it is considered having the full coverage of all nodes after M random samplings. Substituting Eq.(2) into Eq.(1), it can be deduced that M should satisfy (1 − δM )n > τ , i.e., M > √\nlogδ(1 − n\n\nτ ).\n\nNote that the sampling procedure is only performed during training. During inference, we directly consider the node set V as the AN S.\n\n3.2 ATRRIBUTE AUGMENTATION\n\nIn heterogeneous graphs, attribute information and structure information are two crucial characteristics, and are semantically similar to each other due to the homophily of network(McPherson et al. (2001); Pei et al. (2020); Schlichtkrull et al. (2018)). With this principle, there is a reasonable hypothesis that the topological relationship between nodes can well reflect the relationship of nodes’attributes. Or, briefly, adjacent nodes tend to have similar attributes. So attributes of nodes in the AN S can be reconstructed based on attribute information of their neighbors. Considering the complex interactions in heterogeneous networks, the node topological embeddings H is learned to capture underlying structure information, and used as guidance to reconstruct attributes.\n\nTopological embedding. In graph G, the node set V is associated with corresponding node-type set F and the edge set E is associated with corresponding edge-type set R. To capture the structure information in G, random walk(Huang et al. (2019)) is adopted based on multiple common metapaths, to generate more comprehensive node sequences which are fed into a heterogeneous skipR1−→ Rl−1−→ Fl, for node vi with type Fi, the transition probability at step i can be formulated\n\ngram model to learn node topological embeddings. Given a pre-defined meta-path P : F1\n\nR2−→ · · ·\n\nF2 as:\n\np(vi+1|vi, P) =\n\n(cid:26)\n\n1\n\n|NFi+1 (vi)| , 0,\n\n(vi+1, vi) ∈ E, φ(vi+1) = Fi+1 otherwise\n\n,\n\n(3)\n\nwhere NFi+1(vi) stands for the neighbors with type Fi+1 of node vi. The random walk makes that underlying semantics information in graph G is preserved properly. Then skip-gram(Mikolov et al. (2013)) with one-hot vector as input is adopted to learn topological embeddings by maximizing the probability of the local neighbor structures captured by the random walk, that is:\n\nmax θ\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nv∈V\n\nF ∈F\n\nu∈NF (v)\n\nlog p(u|v; θ),\n\n(4)\n\nwhere NF (v) the set of neighbors with type F of node vi, which is sampled by random walk based on the given metapath. The topological embedding of nodes can be learned and denoted as H.\n\nAttribute-wise augmentation. From the perspective of network science, the information on direct links is more essential, so one-hop neighbors have more contributions to attribute augmentation. And we notice that directly connected neighbors have different importance because the neighbors of a node may be of different types and different local structure in heterogeneous graphs. ARHGA adopts the attention mechanism to learn the importance of different one-hop neighbors based on topological embedding H. Note that ARHGA only computes attention coefficients of nodes’ direct neighbors by performing masked attention, which reduces unnecessary computation and is thus more efficient.\n\nSpecifically, given a node pair (v, u) which are directly connected, the attention layer learns the importance evu which indicates the contribution of node u to node v, u ∈ N + v denotes\n\nv , where N +\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nthe one-hop attributed neighbors of node v, the importance can be computed as:\n\nevu = σ(hT\n\nv W hu),\n\n(5)\n\nwhere hu and hv are the topological embeddings of node u and node v. Here the attention layer, parametrized by a weight matrix W , is shared for all node pairs, and σ is an activation function. To better compare the importance across different nodes, the softmax function is used to normalize them to get weighted coefficient αuv:\n\nαvu = sof tmax(evu) =\n\nexp(evu)\n\n(cid:80)\n\ns∈N +\n\nv\n\nexp(evs)\n\n.\n\n(6)\n\nThen, ARHGA obtains attributes of node v by aggregating attributes of it’s one-hop neighbors according to weighted coefficients αvu:\n\n ̃xv =\n\n(cid:88)\n\nu∈N +\n\nv\n\nαvuxu.\n\n(7)\n\nIn this way, the missing attributes are completed and original defective attributes are calibrated. As shown in Figure 2, we give a brief explanation on the author node(A1) in DBLP and the paper node(P1) in ACM as an example. The node A1 has no attributes and the node P1 has original defective attributes, their attributes are reconstructed effectively using our proposed framework. Importantly, the completed attributes are confirmed and the calibrated attributes are more meaningful than original attributes owing to integration with underlying semantic information in heterogeneous graphs after conducting weighted aggregation.\n\n(a)\n\n(b)\n\nFigure 2: (a) An illustration of completing the attributes of node A1 in DBLP dataset. illustration of calibrating the attributes of node P1 in ACM dataset.\n\n(b) An\n\nFinally, to stabilize the learning process and reduce the high variance (brought by the heterogeneity of graphs), we extend the attention mechanism to multi-head attention, as done in many existing methods(Veliˇckovi ́c et al. (2017), Wang et al. (2019a)). Specifically, K independent attention mechanisms are performed to serve as the final attributes of node v, and then their outputs are averaged to generate the following attributes of node v:\n\n ̃xv = mean(\n\nK (cid:88)\n\n(cid:88)\n\nαvuxu),\n\nk\n\nu∈N +\n\nv\n\n(8)\n\nwhere mean(·) represents an average function and K is time of performing attention mechanism. After attribute-wise augmentation, attributes of all nodes are updated as: ̃X = { ̃Xi, Xj|vi ∈ AN S, vj ∈ V − AN S}.\n\n(9)\n\n3.3 CONSISTENCY TRAINING\n\nAfter performing random sampling and attribute augmentation for M times, we generate M augmented attribute matrices { ̃X (m)|1 ≤ m ≤ M }, each of which together with the original graph structure A is sent into the HINs model to get the corresponding output:\n\n ̃Y (m) = Φ(A, ̃X (m)),\n\n(10)\n\n5\n\nP1A1P3P4P2PaperAuthor11312141AggregationP3P4P2Paper312141AggregationP1Under review as a conference paper at ICLR 2023\n\nwhere ̃Y (m) are the prediction probabilities of ̃X (m), Φ denotes a HINs model. With the augmented attributes, ARHGA can be applied to any other heterogeneous graph models and successfully enhance the performance. To be specific, MAGNN(Fu et al. (2020)) is combined when implementing ARHGA.\n\nSupervised Loss. Under a semi-supervised setting, with s labeled nodes among n nodes, the supervised loss on the node classification task is defined as the average cross-entropy loss over M augmentations:\n\nLsup = −\n\n1 M\n\nM (cid:88)\n\ns−1 (cid:88)\n\nm=1\n\ni=0\n\nY T\n\ni\n\nlog ̃Y (m)\n\ni\n\n,\n\n(11)\n\nwhere Y ∈ {0, 1}n×C are ground-truth labels with C representing the number of classes.\n\nConsistency Loss. Valid graph data augmentation changes the input in a way that has relatively trivial impact on the final node classification. We naturally embed this knowledge into our model by designing a consistency loss over M augmentations.\n\nFor each node vi, we generate a ”guessed label” no matter it originally has a label or not. Specifically, find the average of the model’s prediction distributions across all the M augmentations of vi:\n\ni0, ..., ̄Y (cid:48) then the ”guessed label” ̄Y (cid:48) trick(Berthelot et al. (2019)), in which ̄Y (cid:48)\n\ni = ( ̄Y (cid:48)\n\n ̄Yi =\n\ni\n\n,\n\nm=1\n\n(cid:88)M\n\n ̃Y (m)\n\n1 M\nij, ̄Y (cid:48) iC−1)T for node vi is computed through the sharpening ij is the guessed probability of vi belonging to class j:\n\n(12)\n\n ̄Y (cid:48)\n\nij = ̄Y\n\n1 T\n\nij /\n\nC−1 (cid:88)\n\nc=0\n\n1 T\n\n ̄Y\n\nic , 0 ≤ j ≤ C − 1,\n\n(13)\n\nwhere 0 ≤ T ≤ 1 acts as the “temperature” that controls the sharpness of the distribution. In ARHGA, T is set as a small value to enforce the ”guessed label” to approach a one-hot distribution. Then we minimize the distance between ̃Yi and ̄Y (cid:48)\n\ni to design the consistency loss:\n\nLcon =\n\n1 M\n\nM (cid:88)\n\nn−1 (cid:88)\n\nm=1\n\ni=0\n\n|| ̄Y (cid:48)\n\ni − ̃Y (m)\n\ni\n\n||2 2.\n\n(14)\n\nOptimization Objective. Both the supervised loss and the consistency loss are combined as the final loss of ARHGA:\n\nL = Lsup + λLcon, where the hyper-parameter λ is introduced to control the balance between the two losses. By minimizing the final loss, our model can be optimized via back propagation in an end-to-end manner. Algorithm 1(present in Appendix A.1) outlines ARHGA’s training process. During inference, we directly take V as the AN S, that is, attributes of all nodes are reconstructed after performing attribute augmentation one time. Hence, the inference formula is ̃Y = Φ(A, ̃X, Θ), where ̃X = { ̃Xi|vi ∈ V} and ̃Y is the corresponding prediction probabilities.\n\n(15)\n\n4 EXPERIMENTS\n\nIn this section, we first give the experimental setup, and then evaluate the performance of ARHGA on the node classification and report the visualization results. We also conduct a deep analysis on the effectiveness of ARHGA. Finally, we investigate the sensitivity with respect to hyper-parameters of ARHGA.\n\nDatasets. We conduct experiments on three widely-used HINs datasets, i.e., DBLP1, ACM2, IMDB3, to analyze the effectiveness of ARHGA. Note that, only paper nodes in DBLP and ACM,\n\n1https://dblp.uni-trier.de/ 2https://dl.acm.org/ 3https://www.imdb.com/\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Results(%) of node classification on three datasets\n\nDatasets\n\nMetrics\n\nTraining metapath2vec\n\nDBLP\n\nACM\n\nIMDB\n\nMacro-F1\n\nMicro-F1\n\nMacro-F1\n\nMicro-F1\n\nMacro-F1\n\nMicro-F1\n\n10% 20% 40% 60% 80% 10% 20% 40% 60% 80% 10% 20% 40% 60% 80% 10% 20% 40% 60% 80% 10% 20% 40% 60% 80% 10% 20% 40% 60% 80%\n\n91.09 91.50 92.55 93.25 93.48 91.74 92.14 93.09 93.76 93.94 67.81 69.95 71.15 71.74 72.18 70.29 72.12 73.17 73.65 74.14 44.29 46.42 47.70 48.25 48.73 46.15 48.08 49.55 50.06 50.68\n\nGCN 89.53 90.06 90.37 90.57 90.74 90.02 90.53 90.83 91.01 91.15 70.79 70.41 70.82 69.67 67.23 74.10 74.02 74.57 74.10 72.69 43.70 44.75 45.26 47.70 48.25 47.02 47.44 47.62 48.49 48.73\n\nGAT 64.57 66.92 73.23 77.17 78.20 75.90 76.98 79.61 81.62 82.22 89.29 89.59 89.77 89.72 89.42 89.19 89.47 89.65 89.60 89.29 53.61 54.81 55.09 55.71 55.40 54.14 55.02 55.29 55.91 55.67\n\nHetGNN 91.09 91.72 92.03 92.26 92.39 91.64 92.23 92.55 92.79 92.92 88.20 89.16 90.14 90.71 91.01 88.18 89.12 90.11 90.64 90.93 45.68 48.92 51.61 53.00 53.24 46.56 49.70 52.47 53.91 54.25\n\nHAN MAGNN HGNN-AC ARHGA 91.24 91.69 91.84 92.01 92.15 91.88 92.30 92.46 92.65 92.78 89.39 90.01 90.82 91.51 91.71 89.32 89.89 90.73 91.37 91.56 57.02 57.61 57.75 57.66 57.23 57.35 57.82 57.98 57.87 57.46\n\n93.86 94.12 94.44 94.52 94.81 94.31 94.55 94.84 94.91 95.17 92.07 92.94 94.02 94.36 94.57 91.95 92.82 93.94 94.29 94.52 59.39 60.45 60.98 61.39 61.71 59.60 60.53 61.09 61.52 61.88\n\n92.44 92.53 92.97 93.30 93.77 93.02 93.08 93.50 93.83 94.27 86.60 88.01 89.42 90.39 90.79 86.67 88.08 89.48 90.42 90.80 56.39 58.11 59.39 59.97 60.02 56.53 58.16 59.46 60.05 60.15\n\n93.80 93.92 94.06 94.04 94.22 94.22 94.34 94.46 94.46 94.61 90.29 91.51 92.75 93.46 93.79 90.43 91.64 92.9 93.57 93.87 58.69 59.67 60.18 60.60 60.75 58.97 59.84 60.38 60.79 60.98\n\nand movie nodes in IMDB have original attributes while other types of nodes have on attributes. More details are present in Appendix A.2.1.\n\nBaselines. We compare ARHGA with seven state-of-the-art methods representative of two two traditional homogeneous GNNs, i.e., GCN(Kipf & Welling (2016)), different categories: GAT(Veliˇckovi ́c et al. (2017)), and five heterogeneous graph models, i.e., metapath2vec(Dong et al. (2017)), HetGNN(Zhang et al. (2019)), HAN(Wang et al. (2019a)), MAGNN(Fu et al. (2020)), MAGNN-AC(Jin et al. (2021)). Among heterogeneous graph models, metapath2vec is a network embedding methods and other models are GNN-based methods. MAGNN-AC is an approach to complete missing attributes of nodes having no attributes in heterogenerous graphs.\n\nParameter Settings. For the settings/parameters of different baselines, we use the default hyperparameters suggested in MAGNN-AC. The embedding dimensions of all methods is set to 64 for a fair comparison. In ARHGA, random sampling probability δ is set to 0.5 and the number of augmentations M to 6. The number of attention heads K is set to 8 and λ is 0.5 for the weight of the consistency loss. In addition, ARHGA is trained by adopting Adam optimizer(Kingma & Ba (2014)) with the learning rate 0.005. The early stopping strategy with a patience of 5 epoches is adopted on validation set in our experiments.\n\n4.1 NODE CLASSIFICATION\n\nIn this section, the performance on node classification of ARHGA is compared with that of different models. First, we generate embeddings of labeled nodes(i.e., authors in DBLP, authors in ACM and movies in IMDB), and then feed them into a linear support vector machine(SVM)(Suykens (2001)) classifier with different training ratios from 10% to 80%. Table 1 reports the results of averaged Macro-F1 and Micro-F1 over 5 times, where the best results are in bold fonts.\n\nAs shown, ARHGA consistently achieves a significant margin over baselines and datasets. After conducting attribute augmentation, ARHGA has 0.9%-5.47% higher accuracy than MAGNN and 0.06%-1.78% higher than MAGNN-AC, another MAGNN-based method, which performs the best results from the baseline methods. Specially, although some methods have already achieved high accuracy on DBLP dataset, ARHGA still has nontrivial improvement. The improvement is primarily due to the fact ARHGA provides a better representation of node attributes. In addition, GNN-based\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nheterogeneous methods perform better than metapath2vec since attributes information is integrated, demonstrating the importance of node attributes. The poor performance of GCN and GAT reveals the importance of encoding heterogeneous semantic information when analyzing HINs. Our proposed method not only captures and preserves underlying semantic information in heterogeneous graphs but also effectively reconstructs node attributes to augment graph, which contribute to the great superiority of model.\n\n4.2 VISUALIZATION\n\nFor a more intuitive comparison, we conduct the task of visualization by learning embeddings of paper nodes of MAGNN, MAGNN-AC and ARHGA on the ACM dataset. Then the well-known t-SNE(Van der Maaten & Hinton (2008)) is utilized to project the embeddings into a 2-dimensional space, where nodes with different colors belong to different class.\n\n(a) MAGNN\n\n(b) MAGNN-AC\n\n(c) ARHGA\n\nFigure 3: Visualization of embeddings of paper nodes in ACM. Different colors correspond to different research in ground truth.\n\nAs shown in Figure 3, with the reconstructed attributes, ARHGA has the clearest boundary and densest cluster structure to classify nodes among the three methods. In contrast, MAGNN and MAGNN-AC perform poorly, some paper nodes of different classes are mixed and overlapped obviously. MAGNN adopts imputation methods to fill in the missing attributes, which provides little useful information for node classification, resulting in poor performance. MAGNN-AC only completes the missing attributes and the defective attributes are not addressed properly. ARHGA not only completes missing attributes but also enhances the quality of attributes to make nodes more distinguishable.\n\n4.3 A DEEP ANALYSIS OF ARHGA\n\n(a) DBLP\n\n(b) ACM\n\n(c) IMDB\n\nFigure 4: Comparisons of ARHGA with two variants(ARHGA-1, ARHGA-2) and MAGNN on node classification.\n\nTo verify the effectiveness of attribute completion and attribute calibration in our method, we conduct experiments on comparing ARHGA with two variants. The variants are given as follows: 1) ARHGA of removing attributes calibration, only completing the missing attributes, named ARHGA1; 2) ARHGA of removing attributes completion, only enhancing the quantity of the original defec-\n\n8\n\n92.092.593.093.594.094.595.010%20%40%60%80%Macro-F1MAGNNARHGA-1ARHGA-2ARHGA86.087.088.089.090.091.092.093.094.095.010%20%40%60%80%Macro-F1MAGNNARHGA-1ARHGA-2ARHGA56.057.058.059.060.061.062.010%20%40%60%80%Macro-F1MAGNNARHGA-1ARHGA-2ARHGAUnder review as a conference paper at ICLR 2023\n\ntive attributes, named ARHGA-2, in which, the missing attributes are obtained by imputation strategies. In addition, we also compare with MAGNN to better analyze the benefit of ARHGA. Figure 4 presents the results of the deep analysis. More results see Appendix A.2.2.\n\nAs shown, ARHGA has the best results on all datasets at all label rates. Though ARHGA-1 and ARHGA-2 are lower than ARHGA, still outperform MAGNN. This is mainly because ARHGA-1 completes missing attributes accurately and ARHGA-2 calibrates the defective attributes effectively, while there are no corresponding strategies to tackle missing and defective attributes in MAGNN. Furthermore, all the results of ARHGA-1 are better than those of ARHGA-2, so it is believed that attribute completion contributes more than attribute calibration to the effectiveness of ARHGA. The overall results show that accurate attribute information palys a significant role in learning node representations. And ARHGA is an effective exploration of attribute reconstruction through graph data augmentation.\n\n4.4 PARAMETER ANALYSIS\n\nIn this section, we investigate the sensitivity of ARHGA to the critical hyper-parameters: random sampling probability δ, number of augmentations M and weight of consistency loss λ. We report the results of node classification by varying these parameters on ACM dataset.\n\n(a) The parameters M and δ\n\n(b) weighted coefficient λ\n\nFigure 5: Sensitivity analysis on ACM dataset. We report the average result of the node classification across different training ratios.\n\nFrom Figure 5(a), the performance of ARHGA fluctuates slightly when δ and M are in certain ranges, and drops obviously when δ and M are set too small. This is because small δ and M may result in attributes of some nodes not being reconstructed, impairing the effectiveness of graph augmentation. From Figure 5(b), we observe that the performance of ARHGA shows a trend of first rising and then slowly decreasing when λ takes different values from 0.2 to 0.8, and achieves the best when λ = 0.5. This observation indicates that the consistency loss and supervised loss are both important, each of them contributes to the training of ARHGA. The overall results suggest that ARHGA is relatively stable for hyperparameter perturbation, demonstrating the insensitivity of ARHGA. The results on DBLP and IMDB dataset(provided in Appendix A.2.3) also indicate the similar sensitivity.\n\n5 CONCLUSIONS\n\nIn this paper, we present a novel generic architecture(ARHGA) to reconstruct attributes in heteroIn ARHGA, we design an effective attribute augmentation strategy, which not geneous graphs. only solves the problem of missing attributes, but also enhances the quality of original defective attributes. The augmentation strategy is essentially to conduct weighted aggregation through the attention mechanism guided by the topological relationship between nodes. The results of node classification show its consistent performance superiority over seven state-of-the-art baselines on benchmark datasets. The deep analysis of ARHGA demonstrates the effectiveness of completion of missing attributes and calibration of defective attributes. We conclude that ARHGA provides a better attribute representation helpful for improving semi-supervised classification on heterogeneous networks. In future work, we aim to design a structure-wise augmentation for a better graph structure representation in heterogeneous graphs.\n\n9\n\n1080.9216M0.840.60.420.2000.93Macro-F10.940.9280.9290.930.9310.9320.9330.9340.9350.9360.9370.9200.9250.9300.9350.9400.9450.20.30.40.50.60.70.8Macro-F1Micro-F1Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural information processing systems, 32, 2019.\n\nXu Chen, Siheng Chen, Jiangchao Yao, Huangjie Zheng, Ya Zhang, and Ivor W Tsang. Learning on attribute-missing graphs. IEEE transactions on pattern analysis and machine intelligence, 2020.\n\nKaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph learning:\n\nA survey. arXiv preprint arXiv:2202.08235, 2022.\n\nYuxiao Dong, Nitesh V Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 135–144, 2017.\n\nTao-yang Fu, Wang-Chien Lee, and Zhen Lei. Hin2vec: Explore meta-paths in heterogeneous information networks for representation learning. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pp. 1797–1806, 2017.\n\nXinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding. In Proceedings of The Web Conference 2020, pp. 2331–2341, 2020.\n\nDongxiao He, Chundong Liang, Cuiying Huo, Zhiyong Feng, Di Jin, Liang Yang, and Weixiong Zhang. Analyzing heterogeneous networks with missing attributes by unsupervised contrastive learning. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous graph transformer. In\n\nProceedings of The Web Conference 2020, pp. 2704–2710, 2020.\n\nXiao Huang, Qingquan Song, Yuening Li, and Xia Hu. Graph recurrent networks with attributed random walks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 732–740, 2019.\n\nDi Jin, Cuiying Huo, Chundong Liang, and Liang Yang. Heterogeneous graph neural network via\n\nattribute completion. In Proceedings of the Web Conference 2021, pp. 391–400, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\n\nworks. arXiv preprint arXiv:1609.02907, 2016.\n\nJunhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In International confer-\n\nence on machine learning, pp. 3734–3743. PMLR, 2019.\n\nMiller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social\n\nnetworks. Annual review of sociology, pp. 415–444, 2001.\n\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-\n\ntations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\nHongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric\n\ngraph convolutional networks. arXiv preprint arXiv:2002.05287, 2020.\n\nEmanuele Rossi, Henry Kenlay, Maria I Gorinova, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael Bronstein. On the unreasonable effectiveness of feature propagation in learning on graphs with missing node features. arXiv preprint arXiv:2111.12128, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European semantic web conference, pp. 593–607. Springer, 2018.\n\nJingbo Shang, Meng Qu, Jialu Liu, Lance M Kaplan, Jiawei Han, and Jian Peng. Meta-path guided embedding for similarity search in large-scale heterogeneous information networks. arXiv preprint arXiv:1610.09769, 2016.\n\nWei Shen, Jiawei Han, Jianyong Wang, Xiaojie Yuan, and Zhenglu Yang. Shine+: A general framework for domain-specific entity linking with heterogeneous information networks. IEEE Transactions on Knowledge and Data Engineering, 30(2):353–366, 2017.\n\nChuan Shi, Yitong Li, Jiawei Zhang, Yizhou Sun, and S Yu Philip. A survey of heterogeneous information network analysis. IEEE Transactions on Knowledge and Data Engineering, 29(1): 17–37, 2016.\n\nYizhou Sun and Jiawei Han. Mining heterogeneous information networks: a structural analysis\n\napproach. Acm Sigkdd Explorations Newsletter, 14(2):20–28, 2013.\n\nJohan AK Suykens. Support vector machines: a nonlinear modelling and control perspective. Eu-\n\nropean journal of control, 7(2-3):311–327, 2001.\n\nHibiki Taguchi, Xin Liu, and Tsuyoshi Murata. Graph convolutional networks for graphs containing\n\nmissing features. Future Generation Computer Systems, 117:155–168, 2021.\n\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\n\nlearning research, 9(11), 2008.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\n\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\nXiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous\n\ngraph attention network. In The world wide web conference, pp. 2022–2032, 2019a.\n\nXiao Wang, Yiding Zhang, and Chuan Shi. Hyperbolic heterogeneous information network embedding. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 5337–5344, 2019b.\n\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems, 33:6256–6268, 2020.\n\nZhe Xu, Boxin Du, and Hanghang Tong. Graph sanitation with application to node classification.\n\nIn Proceedings of the ACM Web Conference 2022, pp. 1136–1147, 2022.\n\nCarl Yang, Yuxin Xiao, Yu Zhang, Yizhou Sun, and Jiawei Han. Heterogeneous network represen-\n\ntation learning: Survey, benchmark, evaluation, and beyond. 2020.\n\nJiaxuan You, Xiaobai Ma, Yi Ding, Mykel J Kochenderfer, and Jure Leskovec. Handling missing data with graph representation learning. Advances in Neural Information Processing Systems, 33: 19075–19087, 2020.\n\nSeongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph trans-\n\nformer networks. Advances in neural information processing systems, 32, 2019.\n\nChuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V Chawla. Heterogeneous graph neural network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 793–803, 2019.\n\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, pp. 2069–2080, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 PSEUDOCODE OF ARHGA\n\nAlgorithm 1 ARHGA\n\nInput: The node set V, adjacency matrixA, attribute matrix X ∈ Rn×d, time of augmentations M in each epoch, the number of attention head K, learning rate η, a HIN model:Φ(A, X, Θ). Output: Prediction ̃Y . 1: while not convergence do for m = 1 : M do 2: 3:\n\nRandomly select nodes from V to compose the AN S(m)\n\nPerform attribute augmentation on the AN S(m): ̃xm\n\nv = mean(\n\nK (cid:80)\n\nk\n\n(cid:80)\n\nαvuxu)\n\nu∈N +\n\nv\n\nPredict probability distribution using the HINs model: ̃Y (m) = Φ(A, ̃X (m), Θ)\n\nend for\n\n4:\n\n5: 6:\n\n7: Compute the supervised loss Lsup = − 1\n\nM\n\nM (cid:80)\n\ns−1 (cid:80)\n\nm=1\n\ni=0\n\nY T i\n\nlog ̃Y (m)\n\ni\n\nM (cid:80)\n\nn−1 (cid:80)\n\n|| ̄Y (cid:48)\n\ni − ̃Y (m)\n\ni\n\n||2 2\n\nand the consistency loss Lcon = 1\n\nM\n\nm=1 8: Update the parameters by gradients descending: Θ = Θ − η∇Θ(Lsup + λLcon) 9: end while 10: Output prediction ̃Y via: ̃Y = Φ(A, ̃X, Θ)\n\ni=0\n\nA.2 MORE EXPERIMENTAL RESULTS\n\nIn this section, we provide dataset details and more experimental results besides the results in the main paper.\n\nA.2.1 ADDITIONAL DATASET DETAILS\n\nIn this section, we provide some additional, relevant dataset details, the statistics of these datasets are shown in Table A1.\n\nTable A1: Statistics of the datasets\n\nDatasets\n\nDBLP\n\nACM\n\nIMDB\n\nNodes Author(A):4057 Paper(P):14328 Term(T):7728 Venue(V):20 Paper(P):4019 Author(A):7167 Subject(S):60 Movie(M):4278 Director(D):2081 Actor(A):5257\n\nEdges\n\nHasAttributes\n\nA-P:19645 P-T:85810 P-V:14328\n\nP-P:9615 P-A:13407 P-S:4019\n\nM-D:4278 M-A:12828\n\nPaper\n\nPaper\n\nMovie\n\n• DBLP: This is a subset of DBLP with 4057 authors(A), 14328 papers(P), 20 venue(V), and 8789 terms(t). The authors are divided into four research areas: database, data mining, machinesearching, and information retrieval. Only paper nodes have attributes derived from their keywords, and other nodes have no raw attributes.\n\n• ACM: This is a subset of DBLP with 4057 authors(A), 14328 papers(P), 20 venue(V), and 8789 terms(t). The authors are divided into four research areas: database, data mining, machinesearch-\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\ning, and information retrieval. Only paper nodes have attributes derived from their keywords, and other nodes have no original attributes.\n\n• IMDB: We extract a subset of IMDB with 4278 movies(M), 2081 directors(D), and 5257 actors(A). The movie nodes are labeled by their genres: action, comedy and drama. In this dataset, movies are described by bag-of-words representation of their plot keywords, and other nodes have no original attributes.\n\nA.2.2 A DEEP ANALYSIS OF ARHGA\n\nWe also report the Micro-F1 values on three benchmark datasets to evaluate the effectiveness of attribute completion and attribute calibration in ARHGA. Figure 6 presents the results, which is similar to that of ACM dataset.\n\n(a) DBLP\n\n(b) ACM\n\n(c) IMDB\n\nFigure 6: Comparisons of ARHGA with two variants(ARHGA-1, ARHGA-2) and MAGNN on node classification.\n\nA.2.3 PARAMETER ANALYSIS\n\nWe investigate the sensitivity of critical hyper-parameters on DBLP and IMDB and report MacroF1 values in Figure 7 and Figure 8. The results also demonstrate the insensitivity of ARHGA to hyperparameter perturbation.\n\n(a) DBLP\n\n(b) IMDB\n\nFigure 7: Analysis of the parameter sensitivity of λ. We report the average result of the node classification across different training ratios.\n\n(a) DBLP\n\n(b) IMDB\n\nFigure 8: Analysis of the parameter sensitivity of M and δ. We report the average result of the node classification across different training ratios.\n\n13\n\n92.593.093.594.094.595.095.510%20%40%60%80%Micio-F1MAGNNARHGA-1ARHGA-2ARHGA86.087.088.089.090.091.092.093.094.095.010%20%40%60%80%Micro-F1MAGNNARHGA-1ARHGA-2ARHGA56.057.058.059.060.061.062.063.010%20%40%60%80%Micro-F1MAGNNARHGA-1ARHGA-2ARHGA0.9370.9390.9410.9430.9450.9470.9490.9510.20.30.40.50.60.70.8Macro-F1Micro-F10.6010.6030.6050.6070.6090.6110.6130.20.30.40.50.60.70.8Macro-F1Micro-F1024M0.85600.280.40.60.81010.9Macro-F10.950.880.890.90.910.920.930.94020.404M0.260.40.680.50.8101Macro-F10.60.70.460.480.50.520.540.560.580.6",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a joint learning on node classification and attribute augmentation on heterogeneous graphs. The attribute augmentation is based on the aggregations among one-hop neighbors weighted by attentions. Experiments show the proposed approach results in marginal improvements over the baseline methods.\n\n# Strength And Weaknesses\n\nStrengths:\n- The problem of heterogeneous graph augmentation is interesting and important.\n- Experimental results can show some improvements.\n\nWeaknesses:\n- For Eq. (2), the authors claim that delta should be close enough to 1, but in the experiments, it's set to be 0.5 only\n- The attention mechanism used in the paper is problematic. Attention-based aggregation is designed and applied to be among all types of nodes for attribute augmentation.\n- The experimental results in Table 1 only shows very limited improvements over the baseline methods. \n- Results are not reported by the mean and standard deviations.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe novelty of the paper is quite limited. The proposed method is a combination of (1) metapath2vec, (2) GAT, and (3) consistency loss.\nThe writings of the paper need to be polished and proofread. There exist lots of typos and grammar mistakes.\n\n# Summary Of The Review\n\nOverall, the paper has limited novelty and contributions to be accepted:\n- It's not clear how to set M based on delta. When delta is close to 1, M can be very large, which will be inefficient.\n- The key idea of attention-based aggregation among one-hop neighbors (e.g., Eq. 7) is not meaningful. Different types of nodes have different input node attributes, thus directly aggregating x_u where u can be of any types does not make any sense. E.g., aggregating x_u where u is a paper for a venue node.\n- The experimental results in Table 1 only shows very limited improvements over the baseline methods. \n- Results are not reported by the mean and standard deviations.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  }
]