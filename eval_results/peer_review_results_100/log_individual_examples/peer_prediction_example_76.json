{
  "example_idx": 76,
  "reference": "Under review as a conference paper at ICLR 2023\n\nSKILL-BASED REINFORCEMENT LEARNING WITH INTRINSIC REWARD MATCHING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWhile unsupervised skill discovery has shown promise in autonomously acquiring behavioral primitives, there is still a large methodological disconnect between task-agnostic skill pretraining and downstream, task-aware finetuning. We present Intrinsic Reward Matching (IRM), which unifies these two phases of learning via the skill discriminator, a pretraining model component often discarded during finetuning. Conventional approaches finetune pretrained agents directly at the policy level, often relying on expensive environment rollouts to empirically determine the optimal skill. However, often the most concise yet complete description of a task is the reward function itself, and skill learning methods learn an intrinsic reward function via the discriminator that corresponds to the skill policy. We propose to leverage the skill discriminator to match the intrinsic and downstream task rewards and determine the optimal skill for an unseen task without environment samples, consequently finetuning with greater sample-efficiency. Furthermore, we generalize IRM to sequence skills and solve more complex, long-horizon tasks. We demonstrate that IRM enables us to utilize pretrained skills far more effectively than previous skill selection methods on the Unsupervised Reinforcement Learning Benchmark and on challenging tabletop manipulation tasks.\n\n1\n\nINTRODUCTION\n\nGeneralist agents must possess the ability to execute a diverse set of behaviors and flexibly adapt them to complete novel tasks. Although deep reinforcement learning has proven to be a potent tool for solving complex control and reasoning tasks such as in-hand manipulation (OpenAI et al., 2019) and the game of Go (Silver et al., 2016), specialist deep RL agents learn each new task from scratch, possibly collecting new data and learning to a new objective with no prior knowledge. This presents a massive roadblock in the way of integration of RL in many real-time applications such as robotic control where collecting data and resetting robot experiments is prohibitively costly (Kalashnikov et al., 2018).\n\nRecent progress in scaling multitask reinforcement learning (Reed et al., 2022; Kalashnikov et al., 2021) has revealed the potential of multitask agents to encode vast skill repertoires, rivaling the performance of specialist agents and even generalizing to out-of-distribution tasks. Moreover, skillbased unsupervised RL (Laskin et al., 2022; Liu & Abbeel, 2021; Sharma et al., 2020) shows promise of acquiring similarly useful behaviors but without the expensive per-task supervision required for conventional multitask RL. Recent skill-based RL results suggest that unsupervised RL can distill diverse behaviors into distinguishable skill policies; however, such approaches lack a principled framework for connecting unsupervised pretraining and downstream finetuning. The current state-of-the-art leverages inefficient skill search methods at the policy level such as performing a sampling-based optimization or sweeping a coarse discretization of the skill space (Laskin et al., 2021). However, such methods still exhibit key limitations, namely they (1) rely on expensive environment trials to evaluate which skill is optimal and (2) are likely to select suboptimal behaviors as the continuous skill space grows due to the curse of dimensionality.\n\nIn this work, we present Intrinsic Reward Matching (IRM), a scalable algorithmic methodology for unifying unsupervised skill pretraining and downstream task finetuning by leveraging the learned intrinsic reward function parameterized by the skill discriminator. Centrally, we introduce a novel approach to leveraging the intrinsic reward model as a multitask reward function that, via\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Intrinsic Reward Matching (IRM) Framework. IRM takes place in three stages: (1) Taskagnostic RL pretraining learns skill primitives in conjunction with a skill discriminator. (2) With no environment interaction, IRM minimizes the EPIC Loss between the intrinsic reward parameterized by the discriminator and the extrinsic reward with respect to the skill vector z. (3) The skill policy conditioned on the optimal z∗ finetunes to task reward to solve the downstream task.\n\ninteraction-free task inference, enables us to select the most optimal pretrained policy for the extrinsic task reward. During pretraining, unsupervised skill discovery methods learn a discriminatorparameterized, family of reward functions that correspond to a family of policies, or skills, through a shared latent code. Instead of discarding the discriminator during finetuning as is done in prior work, we observe that the discriminator is an effective task specifier for its corresponding policy that can be matched with the extrinsic reward, allowing us to perform skill selection while bypassing brute force environment trials. Our approach views the extrinsic reward as a distribution with measurable proximity to a pretrained multitask reward distribution and formulates an optimization with respect to skills over a reward distance metric called EPIC (Gleave et al., 2020).\n\nContributions The key contributions of this paper are summarized as follows: (1) We describe a unifying discriminator reward matching framework and introduce a practical algorithm for selecting skills without relying on environment samples (Section 3). (2) We demonstrate that our method is competitive with previous finetuning approaches on the Unsupervised Reinforcement Learning Benchmark (URLB), a suite of 12 continuous control tasks (Section 4.1). (3) We evaluate our approach on more challenging tabletop manipulation environments which underscore the limitations of previous approaches and show that our method finetunes more efficiently (Section 4.2). (4) We generalize our method to sequence pretrained skills and solve long-horizon manipulation tasks (Section 4.3) as well as ablate key algorithmic components. (5) We provide analysis and visualizations that yield insight into how skills are selected and further justify the generality of our method (Section 5).\n\n2 BACKGROUND\n\n2.1 UNSUPERVISED SKILL PRETRAINING\n\nThe skill learning literature has long sought to design agents that autonomously acquire structured behaviors in new environments (Thrun & Schwartz, 1994; Sutton et al., 1999; Pickett & Barto, 2002). Recent work in competence-based unsupervised RL proposes generic objectives encouraging the discovery of skills representing diverse and useful behaviors (Eysenbach et al., 2019; Sharma et al., 2020; Laskin et al., 2022). A skill is defined as a latent code vector z ∈ Z that indexes the conditional policy π(a|s, z). In order to learn such a policy, this class of skill pretraining algorithms maximizes the mutual information between sampled skills and their resulting trajectories τ (Gregor et al., 2016a; Eysenbach et al., 2018; Sharma et al., 2019) :\n\nI(τ ; z) = H(z) − H(z|τ ) = H(τ ) − H(τ |z)\n\n(1)\n\nSince the mutual information I(s; z) is intractable to calculate in practice, competence-based methods instead maximize a variational lower bound proposed in (Barber & Agakov, 2003) which is\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nparameterized by a learned neural network function qφ(τ, z) called a skill discriminator. This discriminator, along with other terms independent of z, parameterizes an intrinsic reward that the skill policy π(·|s, z) maximizes during pretraining. Given an unseen task specification, the agent needs to infer which skill will finetune to solve the task with the fewest samples. For more detailed explanations of the mutual information decompositions of various skill discovery algorithms, refer to Appendix A.2.\n\nPretrained Multitask Reward Functions: We observe that the intrinsic reward function learned during skill pretraining can be viewed as a multitask reward function, where the continuous skill code z determines the task. In other words, we have some function:\n\nRint(τ, z) := VLB(τ, z)\n\n(2)\n\nwhere VLB ≤ I(τ, z) is the variational lower bound proposed in (Barber & Agakov, 2003) (τ is a trajectory representation such as (s, s′)). Since skill discovery algorithms aim to maximize I(τ, z), we can view its parameterized lower bound VLB as a multitask reward function: scoring transitions based on their alignment with a skill code (Laskin et al., 2022).\n\n2.2 EQUIVALENT-POLICY INVARIANT COMPARISON\n\nWe can formalize a general notion of reward function similarity by equivalent-policy invariant comparison (EPIC) as established in (Gleave et al., 2020). EPIC defines a distance metric between two reward functions such that similar reward functions induce similar optimal policies. We consider the case of action-independent reward: DEPIC(RA, RB) = EsP ,s′\n\nC ∼DC [Dρ(C(RA)(sP , s′\n\nC), C(RB)(sP , s′\n\nP , SC, S′\n\nP , SC, S′\n\nP ∼DP ,SC ,S′\n\n(cid:113) 1−ρ(X,Y )\n\n2\n\nP are samples from the Pearson distribution DP , and SC, S′\n\nwhere Dρ(X, Y ) = sP , s′ Canonical distribution DC. We compute the Pearson distance over Pearson samples sP , s′ additional canonicalization with batches Sc, S′ The canonicalized reward function is defined as: P , SC, S′\n\nis the Pearson distance between two random variables X and Y , C are batches sampled from the P , with c to ensure invariance over constant shifts and scaling.\n\nP ) + E[γR(s′ where R : S × S → R is a reward function. The expectation is taken over the Canonical distribution DC; for simplicity, we sample these batches SC, S′ C ∼ DC ahead of time. The canonicalization ensures invariance to reward shaping such that rewards that have different shaping but induce similar In practice, the final term can be omitted as the Pearson optimal policies are close in distance. correlation is invariant to constant shifts and scaling.\n\nC) − γR(SC, S′\n\nC) − R(sP , S′\n\nC) = R(sP , s′\n\nC(R)(sP , s′\n\nP , S′\n\nC)]\n\n(4)\n\nC))]. (3)\n\n3\n\nINTRINSIC REWARD MATCHING\n\n3.1 TASK INFERENCE VIA INTRINSIC REWARD MATCHING\n\nA multitask reward function that can supervise the learning of diverse behaviors is useful in its own right. However, in the case of skill-based RL, we have additionally learned a corresponding π(a|s, z). Therefore, for any “task” that can be specified by our intrinsic reward function, we already have an optimal policy, so long as we condition on the corresponding skill. If we have learned a sufficiently diverse library of skills, we might expect that some of our skills share behavioral similarity to the optimal policy for the downstream task. It thus also holds that the corresponding intrinsic reward for that skill is a semantically similar task specification to the downstream task.\n\nGiven this interpretation of intrinsic reward, we posit that the task of identifying which our pretrained skills to apply to a downstream task can be reframed as inferring which task in our multitask reward function is most similar to the downstream task. Moreover, we should hope to find the skill code z that produces the reward function most semantically aligned with the downstream task reward.\n\nWith this formalism, we can formulate the task inference problem as performing the following optimization:\n\nz∗ = arg min\n\nDEPIC(Rint(τ, z), Rext(τ ))\n\nz\n\n(5)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nin order to find z∗ most aligned with the task reward. Moreover, Equation 5 performs a minimization of a novel loss we name the EPIC loss with respect to the skill parameter z. By EPIC’s equivalence class invariance, we know that if the EPIC loss is small for some z∗, and π(a|s, z∗) is near optimal for Rint(τ, z∗), then π(a|s, z∗) approaches the optimal policy for the task as specified by Rext. Notably, we require access to the task reward function Rext to compute the EPIC loss. Leveraging a known task reward function is a divergence from conventional skill selection methods.\n\nComputing Rint during reward matching During pretraining, for some methods such as (Laskin et al., 2022; Sharma et al., 2020), we require negative samples in order to compute the variational objective in Equation 2 and avoid a degenerate optimization where all embedded trajectories have high similarity with all skills. However, during selection when skills are fixed, the negative sampling component amounts to a reward offset which does not impact the task semantics. Furthermore, since we may not in general have access to a large amount of negative samples on a given downstream task, we choose to simplify the objective to the following:\n\nRint(τ, z) := VLB(τ, z) ≡ qφ(τ, z)\n\n(6)\n\nwhere qφ is the skill discriminator. This parameterization of the intrinsic reward preserves the alignment semantics of VLB without the normalization by negative samples. For more details regarding the discriminator parameterization of the intrinsic reward for (Laskin et al., 2022; Sharma et al., 2020) refer to Appendix A.3 and Appendix A.4.\n\nAlgorithm 1: Intrinsic Reward Matching (IRM) Require: Downstream task T , DS, PS Require: Pretrained policy πθ(a|s, z), intrinsic reward rint(s, s′, z), and extrinsic reward\n\nrext(s, s′) for T .\n\nRequire: Optimization NOP = 5000 steps and finetune NF T = 100K steps. /* Skill Selection of z∗ via EPIC Loss\n\n*/\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n1 for NOP steps do\n\nSample a batch of Pearson samples SP , S′ Sample Canonical samples SC, S′ for si, s′\n\ni in SP , S′\n\nP do\n\nC ∼ DC, DC.\n\nP ∼ DP , DP .\n\nCalculate EPIC Loss as DEPIC(rint(si, s′ Dρ(CDS (RA)(si, s′\n\ni, SC, S′\n\nC), CDS (RB)(si, s′\n\ni, z), rext(si, s′ i, SC, S′\n\ni)) = C)) as in Equation 3\n\nend for Take optimization step on batch with respect to z (gradient descent, CEM step, etc.) as in\n\nEquation 5.\n\n8 end for 9 Evaluate zero-shot performance and finetune RL agent for NF T steps with z∗ on downstream\n\ntask T\n\n3.2 EPIC SAMPLE-BASED APPROXIMATION\n\nWe make a number of sample-based approximations of various unknown quantities in order to concretize the continuous optimization Equation 5 as a tractable loss minimization problem.\n\nCanonical State Distribution Approximation: In order to canonicalize our reward functions, we estimate the expectation over the state and next state distributions with a sample-based average over 1024 samples. These distributions can be entirely arbitrary, though using heavily out-of-distribution samples with respect to pretraining can weaken the accuracy of the approximation. We choose to instantiate a uniform distribution bounded by known workspace constraints for both of these distributions.\n\nSampling Distribution for Pearson Correlation: We find that generating samples uniformly roughly within the environment workspace bounds, just as with the reward canonicalization, often leads to strong approximations. Furthermore, as both sample generation and relatively inexpensive function evaluation are independent of the online-finetuning phase, we can perform the full skill optimization as a self-contained preprocess to downstream policy adaptation without any environment samples. Rough knowledge of workspace bounds represents some amount of prior environment knowledge. We leave more general options such as sampling from a learned generative model over\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\ntrajectories encountered during pretraining or sampling from saved pretraining data to future work. We ablate various sampling distribution choices in Table 6 and present the full algorithm in detail in Algorithm 1.\n\n3.3 GENERALIZATION TO SKILL SEQUENCING\n\nMany realistic downstream tasks derive additional complexity from temporally extended planning horizons. In contrast to hierarchical reinforcement learning (HRL) approaches, which aim to stitch together pretrained skills at the policy level with a higher-level manager policy, we can extend the task matching framework of IRM to efficiently solve the problem of skill sequencing, entirely doing away with the manager policy. Consider the long-horizon setting where we have a sequence of reward functions to optimize over some task horizon H. Central to the finetuning problem is determining over what time intervals should potentially different pretrained skills be selected. In this work we predetermine a fixed skill horizon ⌊H/N ⌋ where N is the number of rewards. This skill horizon could in principle be specified as a parameter and learned from the task reward signal.\n\nNext, in order to perform skill selection over each time interval, we perform the IRM algorithm in parallel for each reward. We note the key assumption that IRM requires access to the reward functions for each of the subtasks. For example, for a sequential goal reaching task, we divide the episode into N segments for each of the N goals and corresponding goal-reaching rewards. We then perform the IRM skill selection algorithm for each reward to select the optimal skill over each interval. After selecting the skills, we freeze our selections and finetune the skill policies jointly.\n\n4 EXPERIMENTS\n\nIn this section we aim to experimentally evaluate whether IRM improves the adaptation sampleefficiency of skill finetuning on a downstream reinforcement learning task as compared to baselines. For pretraining skills, we experiment with both the CIC (Laskin et al., 2022) and DADS (Sharma et al., 2020) algorithms. We consider IRM Random a version of IRM that randomly samples skills and picks the one with the lowest EPIC loss, IRM CEM which selects elites as those skills with the lowest EPIC loss, and IRM Gradient Descent which minimizes the EPIC loss using the Adam optimizer and uses backpropagation through the discriminator to regress the optimal skill.\n\nEnvironments We evaluate IRM on URLB (Laskin et al., 2021), which consists of twelve downstream tasks in three challenging continuous control domains in the DMControl suite: Walker, Quadruped, and Jaco. We also design a reaching and a tabletop pushing environment in the OpenAI Gym Fetch environment (Brockman et al., 2016) with further details in Appendix A.5.\n\nFetch Environment\n\nBaselines We benchmark many conventional finetuning approaches after a single skill pretraining phase of Contrastive Intrinsic Control (CIC) (Laskin et al., 2022). The Grid Search (GS) baseline coarsely sweeps each of 10 skills evenly from the all 0’s skill vector to the all 1’s skill vector and finetunes the skill which achieves the best evaluation reward over an episode. Env Rollout randomly samples 10 skills to evaluate with a rollout and Env Rollout CEM uses the episode reward as the metric by which to select elites. Random Skill selects a skill at random. Relabel relabels saved skill rollouts obtained during pretraining with the task reward function, and selects the skill that achieved the highest reward. All baselines use the TD3 (Fujimoto et al., 2018) RL algorithm.\n\nFigure 2: In our Fetch Push environment, we discover skills that move the block in different directions. Downstream tasks may involve simple goals or more distant goals that require composition of multiple skills across an extended time horizon and around obstacles.\n\nEvaluation We follow an identical evaluation to the 2M pre-training setup in URLB. First, we pretrain each RL agent with the intrinsic rewards for 2M steps. Then, we finetune each agent to the downstream task with extrinsic rewards for 100k steps. Since our primary contribution involves\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTask Jaco Top Left Jaco Top Right Jaco Bot. Left Jaco Bot. Right Walker Stand Walker Walk Walker Run Walker Flip Quadr. Stand Quadr. Run Quadr. Jump Quadr. Walk Fetch Reach Fetch Push\n\nIRM CEM 0.000 ± 0.00 0.0860 ± 0.040 0.0520 ± 0.030 2.48 ± 2.2 19.9 ± 9.3 5.86 ± 0.34 6.82 ± 0.66 20.6 ± 1.2 51.5 ± 12 23.9 ± 5.5 38.1 ± 8.5 17.5 ± 6.2 95.9 ± 1.0 80.2 ± 2.5\n\nIRM GD 0.000 ± 0.00 0.640 ± 0.24 0.000 ± 0.00 0.000 ± 0.00 9.75 ± 1.4 7.48 ± 0.55 7.17 ± 0.28 14.8 ± 1.1 40.3 ± 11 24.4 ± 4.8 41.1 ± 9.4 11.5 ± 3.7 87.5 ± 0.20 73.1 ± 0.48\n\nIRM Rand 0.000 ± 0.00 0.120 ± 0.097 0.000 ± 0.00 0.360 ± 0.31 12.5 ± 3.0 15.5 ± 5.5 8.10 ± 0.97 17.3 ± 2.3 40.2 ± 13 20.2 ± 6.5 35.9 ± 11 17.1 ± 6.2 92.5 ± 1.1 77.6 ± 2.7\n\nEnv Roll. 0.186 ± 0.11 7.34 ± 3.4 0.175 ± 0.16 0.086 ± 0.073 18.9 ± 3.7 14.9 ± 2.9 7.92 ± 0.69 23.8 ± 1.9 40.6 ± 9.7 20.6 ± 4.4 30.5 ± 7.0 19.3 ± 2.7 85.0 ± 6.2 74.3 ± 0.92\n\nEnv CEM GS 0.770 ± 0.28 9.82 ± 5.3 0.408 ± 0.22 9.07 ± 3.3 22.4 ± 4.3 13.3 ± 3.2 5.87 ± 1.2 17.3 ± 2.8 47.5 ± 8.7 24.2 ± 4.1 36.8 ± 6.3 25.5 ± 4.0 87.8 ± 1.9 75.4 ± 2.6\n\n1.84 ± 0.00 16.1 ± 0.00 0.102 ± 0.00 0.191 ± 0.00 13.9 ± 4.4 9.40 ± 2.8 6.56 ± 1.2 21.8 ± 0.00 37.4 ± 12 17.3 ± 5.5 29.7 ± 9.8 9.21 ± 2.0 97.3 ± 0.00 72.1 ± 0.00\n\nRelabel 0.000 ± 0.00 0.000 ± 0.00 0.000 ± 0.00 0.000 ± 0.00 3.00 ± 0.18 4.99 ± 1.3 2.67 ± 0.25 3.29 ± 0.00 56.1 ± 11 24.9 ± 6.5 35.5 ± 9.0 31.3 ± 8.2 43.9 ± 0.00 23.6 ± 0.023\n\nRand 0.000 ± 0.00 3.50 ± 2.5 0.000 ± 0.00 0.00100 ± 0.0010 20.8 ± 7.6 15.6 ± 4.9 8.81 ± 1.2 14.4 ± 1.8 44.6 ± 13 21.7 ± 6.2 33.3 ± 9.5 16.4 ± 5.8 16.7 ± 19 51.5 ± 12.5\n\nTable 1: IRM with various optimization methods compared to environment rollout-based skill selection, reward relabelling of pretraining data, and random skill selection. IRM based methods rival or exceed skill selection baselines that are reliant on expensive environment trials.\n\nskill selection, we especially focus on zero-shot episode rewards: rewards achieved by a selected skill policy but without any RL updates on the task reward. We report results averaged over 5 seeds with standard error bars.\n\n4.1 UNSUPERVISED REINFORCEMENT LEARNING BENCHMARK\n\nIn Table 1, we display the zero-shot performance of IRM-based methods compared to interactionbased methods over all 12 URLB tasks. On most of the Walker and Quadruped tasks IRM is either comparable to or outperforms the interaction baselines. Reward relabelling fails to consistently select optimal skills across the benchmark, likely because its options are limited to the finite set of skills sampled during pretraining. IRM by contrast leverages continuous optimization in the skill space to find the best skill for the task. An important insight is that IRM uses the environment interactions to immediately begin finetuning the selected skill policy instead of spending significant amounts of samples on skill selection. This allows IRM-based methods to obtain greater sampleefficiency than rollout-based methods, even when both initial skill selections obtain similar performance as demonstrated in Figure 7. Unsurprisingly, methods like IRM GD and IRM CEM tend to perform better than IRM Random which does not have the luxury of iterative refinement on a smooth EPIC loss manifold as shown in Figure 5. We find that neither our method nor the baselines are well-suited for skill selection on the Jaco tasks. This is likely because these tasks are very sparsely rewarded, making it unlikely that many samples, either randomly generated as in IRM or rolled out, will consistently result in high rewards. We provide analysis demonstrating the relationship between task reward sparsity and the smoothness of the EPIC loss manifold in Appendix A.12.3.\n\n4.2 TABLETOP MANIPULATION\n\nReach Target We evaluate IRM on the Reach Target task, where the Fetch robot is rewarded for reaching a target position. IRM outperforms or closely matches environment-rollout methods while requiring no environment samples to perform skill selection. As shown in Table 1, the random skill policy performs particularly poorly and with very high variance relative to the IRM and environmentrollout based methods. Moreover, appropriate skill selection is required for strong zero-shot performance as certain skills obtain much higher rewards than others. Figure 3 shows the finetuning performance of the methods on the downstream task reward. IRM-based methods are more sample efficient in reaching the optimal performance than environment-rollout-based methods due to improved skill selection.\n\nPush Block to Goal Next, we evaluate IRM on a more complex manipulation task involving pushing a block to a goal position. We report the zero-shot IRM skill selection performance in Table 1 and finetuning performance in Figure 3. This more complex task similarly benefits from bootstrapping the appropriate pretrained skill policy as evidenced by the performance gap of the selection based methods over random skill selection. We remark that even for more complex manipulation tasks, IRM is robust in consistently guiding optimal skill selection without requiring any interaction with the environment. Although Env Rollout CEM is one of the stronger baselines in terms of zero-shot reward, it exceeds the computational budget of 100k interactions entirely on\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFinetuning Performance on Fetch\n\nFigure 3: The performance gap between the IRM skill selection methods and random skill selection evidences the sample efficiency gains to be had from bootstrapping a pretrained policy with task-level semantics similar to the task reward. IRM-based methods select optimal skills with no environment interaction and consequently finetune efficiently. Top: Fetch Reach and Block Push tasks. Bottom: Long-horizon Fetch Reach and Block Push with obstacles tasks.\n\nskill selection. For illustrative purposes, we show the plot starting at 50k.\n\nTask Fetch Reach Seq Fetch Push Seq\n\nIRM Rand Seq 88.1 ± 1.5 84.9 ± 0.12\n\nIRM CEM Seq 89.5 ± 0.34 84.9 ± 0.12\n\nIRM GD Seq Env Seq 80.7 ± 4.7 86.7 ± 0.64 83.7 ± 0.30 81.4 ± 1.9\n\nHRL 28.4 ± 31.0 78.9 ± 3.1\n\nTable 2: Zero-shot rewards on long-horizon manipulation tasks\n\n4.3 EXTENSIONS AND ABLATIONS\n\nLong-Horizon Manipulation Building on the results in Section 4.2, we demonstrate that IRM fully generalizes to solving long-horizon tasks in the setting of tabletop manipulation. During the unsupervised pretraining phase, skill discovery methods can acquire useful skills such as directional block pushing or pushing the block to certain spatial locations. We show that IRM can intelligently select a sequence of such skills to finetune via reward matching, avoiding learning a hierarchical manager policy that finetunes at the policy level.\n\nFor the Fetch Reach environment, we consider an extended horizon where the agent is tasked with reaching a sequence of goals in a particular order. For the Fetch Push task, we consider the environment depicted in Figure 2, where the agent must navigate around a barrier introduced during the finetuning phase in order to reach the goal. We compare IRM methods to an environment rollout baseline (Env Seq) and a hierarchical RL baseline (HRL). The ‘IRM Seq’ methods select skills based on each defined sub-task’s reward function according to the IRM optimization scheme. ‘Env Seq’ chooses the best combination of skills based on extrinsic reward from rollouts. ‘HRL’ is initialized with random skills and simultaneously optimizes a manager policy over skills and the skill policies themselves. In both settings and across optimization methods, IRM outperforms the environment rollout and HRL method in identifying (Table 2) and finetuning skills (Figure 4.3 Row 2). Implementation details are provided in Appendix A.10\n\nMatching Metric Ablations We validate the importance of employing the EPIC pseudometric for formulating the matching loss by ablating its contribution against more naive selections in Table 4. L1 and L2 losses are common metrics in supervised regression problems but are poor choices for comparing task similarity with rewards. Moreover, rewards can have arbitrary differences in scaling\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nEPIC Loss and Extrinsic Reward are Negatively Correlated\n\nFigure 4: (a) Scatter plot of extrinsic reward vs. EPIC loss. (b) Trajectories with low and high EPIC losses for planar goal-reaching. (c) Trajectories for sequential goal-reaching. (d) Trajectories for Fetch Reach.\n\nand shaping that L1 and L2 are not invariant to. To strengthen these comparisons, we include a learned reward scaling parameter for L1 and L2 and similarly observe that EPIC is a superior matching metric.\n\nSkill Discovery Algorithm Ablations IRM is fully general to any mutual information maximization based, RL pretraining algorithm as shown in Table 3. We validate on the Fetch Reach task that IRM CEM and IRM Rand convincingly outperform all episode rollout baselines in zero-shot episode reward.\n\nReward Matching IRM L1 L2 L1 + Learn Scale L2 + Learn Scale\n\nIRM CEM 21.0 ± 0.75 8.71 ± 0.70 7.87 ± 0.86 5.51 ± 1.9 3.95 ± 2.2\n\nTable 4: Reward matching metric ablation\n\nEPIC Loss Visualizations\n\n5 ANALYSIS\n\nDoes optimizing the EPIC loss lead to effective skill selection? In Figure 4, we verify that EPIC loss is strongly negatively correlated with extrinsic reward on a Planar Goal Reaching task detailed in Appendix A.8. Thus, optimizing for a low EPIC loss is an effective substitute for optimizing the environment reward, and crucially, it forgoes collecting expensive environment samples.\n\nHow can we understand skills through EPIC losses? In Figure 5, we plot EPIC losses between intrinsic rewards and goalreaching rewards across the 2D continuous skill space. Not only is the loss landscape smooth, which motivates optimization methods like gradient descent, but there is also a banded partitioning of the manifold. Furthermore, the latent skill space is well-structured as different darker-colored partitions of the skill space correspond to the group of skills with low EPIC loss from each task reward. EPIC losses concisely represent desirability of skills with respect\n\nFigure 5: We examine EPIC losses between extrinsic rewards and intrinsic rewards conditioned on the skill vector. We sweep across the 2D skill vector for a pretrained planar agent.\n\nIntr. Rew. DADS\n\nIRM CEM IRM GD IRM Rand Env Roll. Env CEM GS 83.4 ± 2.19\n\n70.3 ± 5.38\n\n74.6 ± 5.15\n\n69.9 ± 2.22\n\n77.2 ± 3.83\n\n68.9 ± 2.81\n\nRand 28.3 ± 13.5\n\nTable 3: Zero-shot rewards for DADS skill discovery algorithm.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nto a downstream reward function, so skills that achieve a low EPIC loss for the Top Left goal will achieve high EPIC losses for the opposite reward, Bottom Right goal.\n\nWe include a scatter plot and trajectory visualizations in Figure 4. As Figure 4 suggests, skills with the lowest EPIC loss receive high extrinsic reward, reaching the goal with high spatial precision. Skills with the highest losses produce the opposite behavior: moving in the direct opposite direction of the goal. In the sequential case, low-EPIC loss skills attempt to reach the 1st goal then the 2nd goal, while high-EPIC loss skills perform the behavior in the inverse order. The intrinsic reward module provides a much deeper insight into the semantics of skills than the extrinsic rewards obtained by skill policy rollouts.\n\n6 RELATED WORK\n\nSeveral works including (Sharma et al., 2020; Eysenbach et al., 2019; Achiam et al., 2018; Gregor et al., 2016b; Baumli et al., 2020; Florensa et al., 2017; Laskin et al., 2022) employ mutual information maximization for skill pretraining. While (Laskin et al., 2022) leverages coarse grid search to select skills for downstream RL, methods such as (Sharma et al., 2020) instead plan through a learned skill dynamics model at finetuning time. Our approach is similar in that it leverages pretraining model components other than the policy to guide skill selection. However, rather than generating a reward maximizing plan through possibly complex, learned environment dynamics, we instead look to match a policy to the task reward directly through a pretrained discriminator.\n\nIn the context of sequential finetuning, (Baumli et al., 2020; Eysenbach et al., 2019) employ hierarchical RL to chain pretrained skills with a manager policy requiring additional environment interactions. Works on such HRL methods include (Nachum et al., 2018; Frans et al., 2017; Vezhnevets et al., 2017; Springenberg et al., 2018) and more classically (Sutton et al., 1999; Stolle & Precup, 2002). By contrast, we demonstrate that the intrinsic reward matching framework can be extended to choose skill sequences without reliance on environment samples. The successor features line of work also adopts a unified view of skill-based RL. Such work relies on the assumption that arbitrary rewards can be parameterized linearly in some learned features and some task vector as in (Liu & Abbeel, 2021; Barreto et al., 2016). Our approach relaxes this assumption to the fully general setting by instead searching for a pretrained task with minimal proximity to an arbitrarily parameterized task reward.\n\n7 DISCUSSION\n\nWe present Intrinsic Reward Matching (IRM), a framework for algorithmically unifying information maximization unsupervised reinforcement learning with downstream task adaptation. We instantiate a practical algorithm for implementing this framework and demonstrate that IRM outperforms current methods on a continuous control benchmark and tabletop manipulation tasks. IRM diverges from past works in leveraging the discriminator for downstream task inference and consequently performing skill selection without environment interactions in the short horizon setting. We also show that IRM can be readily extended to the general skill sequencing setting to solve more realistic long-horizon tasks as an alternative to hierarchical methods. Central to our contribution is a novel loss function, the EPIC loss, which serves as both a skill selection utility as well as a new way to interpret the task-level semantics of pretrained skills.\n\nWe acknowledge a number of limitations of our approach. IRM relies on samples of the state, roughly within workspace boundaries as well as access to an external reward function, ideally wellshaped, which trades off with IRM’s reduced reliance on environment interactions. In order to obtain realistic image samples to compute the EPIC loss, an agent could learn an expressive generative model such as a VAE over the image states obtained during pretraining and sample from the model to generate diverse and realistic sampled states. For learning unknown state-based rewards, the agent could additionally learn an image-reward model by regressing the rewards encountered during exploration (Hafner et al., 2019). This further relaxes some of the assumptions made in this contribution and represents an exciting direction for future work.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nJoshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery algorithms. CoRR, abs/1807.10299, 2018. URL http://arxiv.org/abs/1807.10299.\n\nDavid Barber and Felix V. Agakov. The im algorithm: A variational approach to information In NIPS, pp. 201–208, 2003. URL http://papers.nips.cc/paper/\n\nmaximization. 2410-information-maximization-in-noisy-channels-a-variational-approach.\n\nAndr ́e Barreto, R ́emi Munos, Tom Schaul, and David Silver. Successor features for transfer in reinforcement learning. CoRR, abs/1606.05312, 2016. URL http://arxiv.org/abs/1606. 05312.\n\nKate Baumli, David Warde-Farley, Steven Hansen, and Volodymyr Mnih. Relative variational intrinsic control. CoRR, abs/2012.07827, 2020. URL https://arxiv.org/abs/2012.07827.\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\n\nWojciech Zaremba. Openai gym, 2016.\n\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:\n\nLearning skills without a reward function, 2018.\n\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019.\n\nCarlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical reinforcement learning. CoRR, abs/1704.03012, 2017. URL http://arxiv.org/abs/1704. 03012.\n\nKevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared hierarchies. CoRR, abs/1710.09767, 2017. URL http://arxiv.org/abs/1710.09767.\n\nScott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. CoRR, abs/1802.09477, 2018. URL http://arxiv.org/abs/1802. 09477.\n\nAdam Gleave, Michael Dennis, Shane Legg, Stuart Russell, and Jan Leike. Quantifying differences in reward functions. CoRR, abs/2006.13900, 2020. URL https://arxiv.org/abs/2006. 13900.\n\nKarol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control, 2016a.\n\nKarol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. CoRR,\n\nabs/1611.07507, 2016b. URL http://arxiv.org/abs/1611.07507.\n\nDanijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. CoRR, abs/1912.01603, 2019. URL http: //arxiv.org/abs/1912.01603.\n\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. CoRR, abs/1806.10293, 2018. URL http://arxiv.org/abs/1806.10293.\n\nDmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale, 2021. URL https://arxiv.org/abs/2104.08212.\n\nMichael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. URLB: unsupervised reinforcement learning benchmark. CoRR, abs/2110.15191, 2021. URL https://arxiv.org/abs/2110.15191.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMichael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and Pieter Abbeel. Cic: Contrastive intrinsic control for unsupervised skill discovery, 2022. URL https://arxiv. org/abs/2202.00161.\n\nHao Liu and Pieter Abbeel. Aps: Active pretraining with successor features, 2021.\n\nOfir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. CoRR, abs/1805.08296, 2018. URL http://arxiv.org/abs/1805. 08296.\n\nOpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik’s cube with a robot hand. CoRR, abs/1910.07113, 2019. URL http://arxiv.org/abs/1910.07113.\n\nMarc Pickett and Andrew Barto. Policyblocks: An algorithm for creating useful macro-actions in\n\nreinforcement learning. 06 2002.\n\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https://arxiv.org/abs/2205.06175.\n\nArchit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware\n\nunsupervised discovery of skills. arXiv preprint arXiv:1907.01657, 2019.\n\nArchit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware unsupervised discovery of skills. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJgLZR4KvH.\n\nDavid Silver, Aja Huang, Christopher Maddison, Arthur Guez, Laurent Sifre, George Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484–489, 01 2016. doi: 10.1038/nature16961.\n\nJost Tobias Springenberg, Karol Hausman, Martin Riedmiller, Nicolas Heess, and Ziyu Wang.\n\nLearning an embedding space for transferable robot skills. 2018.\n\nMartin Stolle and Doina Precup. Learning options in reinforcement learning. In Sven Koenig and Robert C. Holte (eds.), Abstraction, Reformulation, and Approximation, pp. 212–223, Berlin, Heidelberg, 2002. Springer Berlin Heidelberg.\n\nRichard S. Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1):181–211, 1999. ISSN 0004-3702. doi: https://doi.org/10.1016/S0004-3702(99)00052-1. URL https://www. sciencedirect.com/science/article/pii/S0004370299000521.\n\nSebastian Thrun and Anton Schwartz. Finding structure in reinforcement learning. In G. Tesauro, D. Touretzky, and T. Leen (eds.), Advances in Neural Information Processing Systems, volume 7. MIT Press, 1994. URL https://proceedings.neurips.cc/paper/1994/file/ 7ce3284b743aefde80ffd9aec500e085-Paper.pdf.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding, 2019.\n\nAlexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. CoRR, abs/1703.01161, 2017. URL http://arxiv.org/abs/1703.01161.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 BACKGROUND AND NOTATION\n\nMarkov Decision Process: The goal of reinforcement learning is to maximize cumulative reward in an uncertain environment it interacts with. The problem can be modelled as a Markov Decision Process (MDP) defined by (S, A, P, r, γ), where S is the set of states, A is the set of actions, P is the transition probability distribution, r is the reward function and γ is the discount factor.\n\nUnsupervised Skill Discovery: In competence-based unsupervised RL the aim is to learn skills that generate diverse and useful behaviors (Eysenbach et al., 2019). The broad aim is to learn policies that are skill-conditioned and generalizable. Formally, we also learn skills z ∈ Z and take actions according to a ∼ π(·|s, z). As an illustrative example, applying this formalism to the Mujoco Walker domain, we might hope to find a skill-conditioned policy and skills zwalk, zrun such that π(·|s, zwalk) makes the agent walk, while π(·|s, zrun) makes it run. Further, if we allow for continuous skills, we can also imagine being able to use the policy to “jog” at different speeds by interpolation the zwalk and zrun skills. That is, taking zα jog = α · zwalk + (1 − α) · zrun should, intuitively, yield a policy π(·|s, zα\n\njog) that makes the agent jog at speed dictated by the parameter α.\n\nFinetuning Pretrained Skills: With a skill-conditioned policy π(·|s, z), an agent needs to infer which skill to index for a downstream task (e.g. identifying if it needs to use zwalk or zrun) during finetuning. This is a relatively under-explored area, with the most universal approach being a coarse, discretized grid search. Least squares regression has also been investigated in the context of successor features (Liu & Abbeel, 2021).\n\nA.2 COMPETENCE-BASED SKILL DISCOVERY\n\nCompetence-based skill discovery algorithms aim to maximize the mutual information between trajectories and skills:\n\nI(τ ; z) = H(z) − H(z|τ ) = H(τ ) − H(τ |z)\n\n(7)\n\nSince the mutual information I(s; z) is intractable to calculate in practice, competence-based methods maximize a variational lower bound. Many mutual information maximization algorithms, such as Variational Intrinsic Control (Gregor et al., 2016a) and Diversity is All You Need (Eysenbach et al., 2018), use the estimate I(τ ; z) = H(z) − H(z|τ ). Other competence-based methods, such as Dynamics-Aware Unsupervised Discovery of Skills (Sharma et al., 2019), Active Pretraining with Successor Features (Liu & Abbeel, 2021), and Contrastive Intrinsic Control (CIC) (Laskin et al., 2022), maximize a lower bound for H(τ ) − H(τ |z).\n\nWhile the decompositions of the mutual information objective are equivalent, algorithms make different design choices regarding how to approximate entropy, represent trajectories, and embed skills. These choices affect the distillation of skills: for instance, without explicit maximization of H(τ ) in the decomposition of mutual information, behavioral diversity may not be guaranteed when the state space is much larger than the skill space (Laskin et al., 2022).\n\nA.3 CIC\n\nContrastive Intrinsic Control (CIC) (Laskin et al., 2022) is a state of the art algorithm for competence-based skill discovery. CIC maximizes a lower bound for I(τ ; z) = H(τ ) − H(τ |z) through a particle estimator for H(τ ) and a contrastive loss from Contrastive Predictive Coding (CPC) (van den Oord et al., 2019) for H(τ |z). The lower bound for I(τ ; z) is:\n\nI(τ ; z) ≥ FCIC(τ ; z) := Hparticle(τi) + E\n\nqφ(τi, zi) − log\n\n\n\nexp(qφ(τj, zi))\n\n\n\n\n\n(8)\n\n1 N\n\nN (cid:88)\n\nj=1\n\nwhere Hparticle(τ ) ∝ (cid:80)n number of k-NNs used to approximate entropy, and N − 1 is the number of negative samples.\n\ni is the k-Nearest Neighbors embedding, Nk is the\n\ni=1 log ||hi − h∗\n\ni ||, h∗\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA.4 DADS\n\nWe additionally use Dynamics-Aware Unsupervised Discovery of Skills(DADS) (Sharma et al., 2020) for skill discovery, as it is one of the few skill discovery algorithms to successfully scale up to continuous skills. DADS maximizes a lower bound for I(τ ; z) = H(τ ) − H(τ |z) through learning skill-conditioned transition distributions. The lower bound for I(τ ; z) is:\n\nI(τ ; z) ≥ FDADS(τ ; z) := log\n\nqφ(s′|s, z) i=1 qφ(s′|s, zi)\n\n(cid:80)L\n\n+ log L\n\n(9)\n\nFor our experiments, we reimplement the on-policy DADS algorithm in PyTorch. We follow the default hyperparameters and train for 20 million environment steps, per (Sharma et al., 2020).\n\nA.5 ENVIRONMENT DETAILS\n\nThe URLB domains are Walker, Quadruped, and Jaco. Walker requires a bipedal agent to perform a variety of navigation based tasks on a 2D-plane while preserving its balance. Quadruped, a more challenging domain due to a higher-dimensional state-action space, requires a quadrupedal agent to perform navigation tasks in a 3D environment. Jaco robot arm is a 6-DOF manipulator with a three-finger gripper which contains a variety of directional reaching tasks\n\nFor URLB (Laskin et al., 2021) environments, we follow default environment settings. Like many skill-discovery methods (Sharma et al., 2020) (Eysenbach et al., 2019), we restrict the discriminator input. For Quadruped, we use the x, y, z velocity, which is included in the environment’s state space. For Walker, we use the x, y, z world-position, which we add to the environment’s state space but remove from the policy input. For Jaco, we use the x, y, z world position.\n\nFor our fetch reaching environment, we use the Gym Robotics Fetch environment (Brockman et al., 2016). We set the time limit to 200. For the fetch push environment, we partition the continuous action space into 4 actions, which involve pushing the block forward, backward, left, and right. We set the time limit to 10 for skill learning.\n\nWe evaluate sequential skill selection on 2 environments: Fetch Reach and Fetch Push. For the Fetch Push task, we fix 3 waypoints, depicted in Figure 2 and fix a time horizon of 15 pushes per waypoint. For Fetch Reach, we consider 2 waypoints and a time horizon of 25 for each waypoint.\n\nOur plane environment is a 2D world with observations in [-128, 128] x [-128, 128] and continuous actions in [-10, 10] x [-10, 10].\n\nA.6 PRETRAINING HYPERPARAMETERS\n\nFor the Jaco domain we use a skill dimension of 2 and a discriminator MLP hidden dimension of 64. We use an alpha value of 0 for the entropy weighting as in (Laskin et al., 2022). We input the 3D position of the end-effector of the Jaco arm to the discriminator. For the Walker domain we use a skill dimension of 2 and a discriminator MLP hidden dimension of 256. We use an alpha value of 0.7 for the entropy weighting. We input the displacement in the 3D position of the torso of the walker to the discriminator. For the Quadruped domain we use a skill dimension of 16 and a discriminator MLP hidden dimension of 128. We use an alpha value of 0.5 for the entropy weighting. We input the 3D velocity of the body of the quadruped to the discriminator. We use a learning rate of 1e-4, a critic target tau parameter of 0.01, and a constant standard deviation exploration schedule of 0.2. The rest of the RL hyperparameters are as in (Laskin et al., 2021).\n\nFor the Fetch Push environment, we use a skill dimension of 16 and a discriminator MLP hidden dimension of 16. We use an alpha value of 0 for entropy weighting. For the Fetch Reach environment, we use a skill dimension of 8 and a discriminator MLP hidden dimension of 64. We use an alpha value of 0 for entropy weighting. For all environments, we use a replay buffer size of 100k.\n\nA.7\n\nINTRINSIC REWARD MATCHING AND ENVIRONMENT ROLLOUT BASELINE HYPERPARAMETERS\n\nIRM CEM and Env Rollout CEM are trained for 5 iterations with 1000 samples at each iteration and 100 elites selected each iteration. Env Rollout CEM consumes the entire downstream finetuning\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nZero-Shot Performance for Planar Goal Reaching\n\n(a) Skill dimension 2\n\n(b) Skill dimension 8\n\n(c) Skill dimension 16\n\nFigure 6: Zero-Shot Returns for Planar Goal Reaching averaged over 5 seeds\n\nbudget on just skill selection. For illustrative purposes, we start its plot at 50k steps to show that finetuning still occurs, however, sample-inefficiency suffers due to excessive rollouts for skill selection. This problem only worsens for long time horizons. IRM Gradient Descent is trained for 5000 steps with a learning rate of 5e-3 and initialized at the skill vector of all 0.5s. IRM Random selects 100 random skills. Env Rollout trials 10 random skills for a fully episode. Grid Search coarsely trials 10 skills from the skill of all 0s to the skill of all 1s as in (Laskin et al., 2021).\n\nA.8 PLANAR GOAL REACHING\n\nThe planar goal reaching task consists of a simple 2D plane with a point with a 2D Cartesian state space that can displace in the x and y coordinates with a 2D action space. Skills learned tend to span the 2D space reaching to diverse locations distributed broadly across the environment. We show some sample zero-shot skill selection results over three different skill dimensions in Figure 6.\n\nA.9 FINETUNING PERFORMANCE ON URLB\n\nIn Figure 7 we compare the finetuning sample-efficiency of IRM methods against environment rollout-baselines on the URLB Walker tasks. IRM performs skill selection with 0 environment interactions. The episode length of the URLB environments is 1000, meaning that in order to evaluate a single skill, rollout based methods must exhaust 1000 environment steps (i.e. grid search spends 1000 * 10 = 10,000 environment steps - 10 percent of the available finetuning budget). By contrast, our method immediately uses new environment steps for improving the policy. As a result, the IRM based approaches generally achieve greater sample efficiency, even when initial skill selection obtains similar performance to the rollout based methods. For illustrative purposes we have shown Env CEM starting at 50k steps even though it far exceeds the 100k sample budget to select a skill before making any RL updates due to having to execute full episode rollouts in the inner loop of optimization. This issue worsens with increasing episode lengths. We plot results over 3 seeds with standard error shading.\n\nA.10 SEQUENTIAL SKILL SELECTION\n\nFor sequential skill selection, we compare IRM Sequential and Environment Sequential skill selection. IRM Sequential consists of an iterative process. The first skill is chosen entirely free of environment samples, exactly identical to the single-skill tasks. Once the first skill is chosen, we roll out a trajectory with the skills we have chosen so far and use the latter half of the trajectory as the Pearson samples for our EPIC loss. We use Gaussian noise with variance 1 for our Canonical samples as described in Appendix A.12.2. At each step of the skill selection process, we use the corresponding IRM optimization methods.\n\nFor our Environment Sequential skill selection method, we select skills iteratively as well. For each waypoint or subtask, we randomly sample N skills and commit to the best, where N = 10/n subtasks.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFinetuning Performance on URLB\n\nFigure 7: IRM finetuning results compared to rollout-based baselines on Walker URLB tasks.\n\nSkill Dim IRM CEM IRM GD 15.7 ± 1.61 21.1 ± 0.51 8\n14.6 ± 0.63 17.4 ± 1.30 16 22.537 ± 0.25 20.1 ± 0.54 32 1.68 ± 0.069 21.9 ± 0.48 64\n\nIRM Rand Env Roll. Env CEM GS 18.9 ± 0.18 18.8 ± 0.26 19.8 ± 0.14 20.9 ± 0.74\n\n18.4 ± 0.18 22.7 ± 0.83 22.2 ± 0.58 22.5 ± 0.70\n\n18.8 ± 0.48 23.1 ± 0.36 21.5 ± 0.67 21.6 ± 0.89\n\n17.9 ± 0.101 14.0 ± 0.19 24.0 ± 0.12 18.2 ± 0.059\n\nRand 13.5 ± 1.85 11.2 ± 2.32 19.9 ± 0.67 13.3 ± 2.15\n\nTable 5: IRM methods and environment rollout methods ablated over multiple skill dimensions on Fetch Push\n\nA.11 HIERARCHICAL REINFORCEMENT LEARNING BASELINE\n\nIn order to validate the benefits of IRM’s offline skill selection, we compare against a baseline that leverages a conventional hierarchical RL algorithm to solve long-horizon, sequential tasks. We instantiate a TD3 manager agent that outputs into a skill action space from state input at a temporally abstract timescale. As in the IRM setup, this timescale is fixed to align with the changes in reward to encourage the manager to change its skill prediction according to the change in the reward semantics. The manager’s is then inputted to the low-level pretrained skill policy which is rolled out over many steps with the skill fixed. Both the manager policy and the low-level policy weights are updated during finetuning. The manager agent is randomly initialized such that its initial skill prediction is random.\n\nA.12 ADDITIONAL ABLATIONS\n\nA.12.1 SKILL DIMENSION\n\nWe ablate skill dimension and evaluate the zero-shot performance of all skill selection methods. IRM’s performance generally increases with increased skill dimension despite discriminator overfitting issues associated with larger skill spaces. The IRM GD learning rate is chosen as 5e-3 for all experiments in this work and is not tuned at all. Such likely explains the divergence of the 64 dimensional result.\n\nA.12.2 PEARSON & CANONICAL DISTRIBUTIONS\n\nWe experiment with many ways to approximate the Pearson and Canonical distributions. We defined Full Random to be our uniform samples from a reasonable estimate of the upper and lower bounds\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nfor each dimension of the state. For our planar environment, the bounds are defined explicitly and thus known; for more complex environments, we estimate the bounds. For example, for a tabletop manipulation workspace, we sample 2-dimensional block positions uniformly within the rectangular plane of the table surface. In practice, IRM is fairly robust to the distributions, though there are subtleties that emerge in the various choices for the Pearson and Canonical distributions. For instance, we also ablate a Uniform(0,1) distribution, which generally performs much worse, due to lack of state coverage for most environments. For the Canonical distribution, we also approximate samples by perturbing the Pearson samples by ε sampled from a Gaussian distribution. We experiment with hyperparameters of variance, which may be adjusted based on the environment. For our sequential IRM method, we use this Canonical distribution to ablate on-policy samples.\n\nPearson Distribution Canonical Distribution Full Random Full Random Full Random Full Random Uniform(0,1) Uniform(0,1)\n\nFull Random Uniform(0,1) ε ∼ N (0, 1) ε ∼ N (0, 0.1) ε ∼ N (0, 1) ε ∼ N (0, 0.1)\n\nIRM CEM 20.341 ± 0.306 16.343 ± 0.708 21.191 ± 0.629 21.027 ± 0.419 5.905 ± 3.157 2.851 ± 0.605\n\nTable 6: EPIC Loss Sampling Distribution Ablations.\n\nNone of the distributions ablated above require on-policy environment samples. It is possible to use on-policy samples for the state distributions, and we choose to do so for our sequential IRM method, as previous skill rollouts may provide useful Pearson samples for the subsequent skill selection. Note that while on-policy Canonical samples are possible, they are incredibly expensive and require access to the environment simulator, so we focus on other choices of distributions.\n\nA.12.3 SPARSE REWARD ABLATION\n\nWe ablate our planar EPIC Loss visualizations with sparse rewards. Instead of a well-shaped goalreaching reward, we use sparse rewards based on the tolerance to the goal. We define the tolerance as the radius the agent must be within if our 2d planar environment is scaled to [0, 1] x [0, 1]. With a very sparse reward, we show that EPIC losses are largely uninformative. However, by slightly relaxing the tolerance, we show a much better shaped EPIC loss landscape that bears similarity to that of Figure 5. Thus, while our method is dependent on access to extrinsic rewards, and ideally, shaped rewards, we show that the EPIC loss landscape over sparse reward landscapes with sufficient tolerance can be optimized.\n\nEPIC Loss Visualizations\n\nFigure 8: We examine EPIC losses between extrinsic rewards and intrinsic rewards conditioned on the skill vector. We sweep across the 2D skill vector for a pretrained planar agent. Left: Sparse goal-reaching reward with tolerance of 0.03. Right: Sparse goal-reaching reward with tolerance of of 0.07.\n\n16",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes a method, IRM, that finds appropriate skills for a given target task by matching the unsupervised-learned intrinsic reward and the extrinsic reward of the target task.\nThe matching procedures don’t require environment samples as in conventional fine-tuning approaches but assume black-box access to the ground truth reward function of the target task.\nEPIC loss is used to correctly measure the behavior similarity between the intrinsic reward and the extrinsic reward.\nIn their evaluation, IRM shows the same order of zero-shot and fine-tuning performance with the baselines.\n\n# Strength And Weaknesses\n\n### Strength\n- The proposed method can be a powerful tool that can facilitate zero-shot deployment of unsupervised trained skill policy into actual target tasks.\n- The idea of matching intrinsic reward and extrinsic reward seems novel and the use of EPIC loss is well-motivated. The analysis supports that EPIC is effectively matching the intrinsic reward and the target task reward.\n\n### Weakness\n- Missing baseline\n    - Given access to the extrinsic reward function, one can use it to relabel pre-training time buffer data. Then, the skill that is relabeled with the highest reward can be selected. I think this relabeling-based skill selection is the most convincing apples-to-apples comparison because it doesn’t use the idea of intrinsic and extrinsic reward matching but does use the extrinsic reward function.\n    - HRL method deserves to be a baseline for long-horizon skill sequencing\n- Empirical support is limited. \n    - Most of the comparison is not showing any consistent tendency. It is rather showing that the proposed skill selection can work worse than the grid search method or sampling-based skill selection. (table 1 all rows except Fetch Push, Figure3 first row, Figure 7). \n    - Skill sequencing experiments only show results of IRM random. Considering the inconsistency over IRM optimization methods in Table 1, the result should be presented along with other IRM optimization methods, or a reasonable model selection criteria should be given, to conclude IRM is better.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n### Clarity, Quality\nThe presentation of the paper was mostly clear. Few unclear parts :\n- Section 3.3 is very difficult to follow without more formal description or example.\n- Figure 3 starts plotting from the middle only to denote the number of samples needed for skill selection, but have done only for Env Rollout CEM. In my understanding, Env Rollout and Grid Search also need to be started from the middle.\n\nA few unimportant flaws are found :\n- Multiple emphasizing in Table 1 \"Walker Stand\" row\n- Inconsistent decimal format in Table 2\n\n### Reproducibility\nCode is not provided, but implementation details and hyperparameters are described.\n\n# Summary Of The Review\n\nThe proposed method is very convincing and supported by analysis results.  \nHowever, provided empirical results less support the effectiveness of the core idea.  \nCurrent results are enough to show that IRM is a working idea,\nbut the benefit of IRM, over any naive approach leveraging extrinsic reward function, is not clearly shown.  \nThus, I vote to reject this submission for now as I think the important comparison is missing,\nbut still willing to raise my score according to the rebuttal discussion.\n\n### Post rebuttal\nI appreciate the authors' efforts in the response.\nThe rebuttal response resolved my main concern about the experimental results.\nI'm convinced that IRM is the only method that can zero-shot select skills without accessing pre-training data.\nStill, given relabeling baseline provides a good intuition of the benefit of accessing the extrinsic reward function and how well EPIC is leveraging it.\nThus, I raised my rating as I agree with sharing the idea of EPIC-based skill matching with the community and leaving the study of consistently working skill matching methods to future work.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.",
    "# Summary Of The Paper\nThe paper introduces **Intrinsic Reward Matching (IRM)**, a novel framework designed to enhance sample efficiency in reinforcement learning by bridging unsupervised skill pretraining with task-aware finetuning. The methodology employs a skill discriminator to align intrinsic rewards with extrinsic task rewards, thereby enabling optimal skill selection without necessitating environment rollouts. Key contributions include demonstrating competitive performance on the Unsupervised Reinforcement Learning Benchmark (URLB) and complex tabletop manipulation tasks, as well as extending the approach to sequence skills for long-horizon tasks. The results reveal that IRM significantly improves sample efficiency and task performance compared to traditional finetuning methods.\n\n# Strength And Weaknesses\n**Strengths**:\n1. The integration of unsupervised skill pretraining and finetuning presents a novel approach that can potentially reduce the data inefficiency prevalent in traditional reinforcement learning methods.\n2. The introduction of the EPIC loss function for skill selection provides a robust metric that outperforms conventional loss functions, demonstrating the framework's effectiveness.\n3. Empirical results show substantial improvements in performance and sample efficiency, particularly in zero-shot tasks, indicating the practical applicability of the approach.\n\n**Weaknesses**:\n1. The reliance on known state distributions and external reward functions may limit the framework's applicability in completely unstructured environments.\n2. The paper could benefit from a more detailed discussion on the computational complexity of the proposed algorithm, particularly when scaling to larger tasks or environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings. The use of visualizations to explain skill semantics and the skill space enhances clarity. The quality of the experiments is commendable, with thorough comparisons against baseline methods and detailed ablation studies that validate the proposed approach. However, the novelty, while significant, could be further contextualized within the broader landscape of reinforcement learning techniques. Reproducibility is supported through detailed algorithm description and experiments, although the paper would benefit from providing access to code or datasets for independent verification.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of reinforcement learning by effectively combining skill pretraining with efficient finetuning through the IRM framework. While the methodology is robust and the empirical results are compelling, the assumptions regarding state distributions and reward functions may limit its applicability in more complex environments.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper introduces the **Intrinsic Reward Matching (IRM)** framework aimed at bridging the gap between unsupervised skill discovery and task-aware fine-tuning in reinforcement learning (RL). The methodology comprises three stages: task-agnostic pretraining with a skill discriminator, skill selection using a novel **EPIC Loss** function for matching intrinsic and extrinsic rewards, and finetuning to solve downstream tasks. Experimental results demonstrate that IRM significantly enhances sample efficiency and skill selection across a range of continuous control tasks, outperforming traditional methods and showing robustness in long-horizon tasks.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative approach to skill selection, which leverages intrinsic rewards without requiring extensive environment rollouts, thus improving sample efficiency. The unified framework effectively links unsupervised skill discovery with task-specific adaptation, showcasing substantial generalizability to complex tasks. However, the method's dependence on the availability of extrinsic reward functions may limit its applicability in real-world scenarios. Additionally, the reliance on samples from known workspace boundaries poses challenges in more dynamic environments, and the proposed framework may struggle in contexts with sparse rewards.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the proposed method, its components, and the experimental design. The novelty of the IRM framework and the EPIC Loss function is clearly highlighted, providing a fresh perspective on skill selection in RL. The reproducibility of the results is potentially high, given the detailed description of the methodology and experimental setup; however, the real-world applicability may require further exploration of generative models to address the limitations identified.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to reinforcement learning by introducing the IRM framework, which effectively enhances sample efficiency and skill selection in complex tasks. While the proposed method is innovative and well-evaluated, its reliance on specific reward structures and environmental conditions may pose challenges for broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces **Intrinsic Reward Matching (IRM)**, a novel approach that connects unsupervised skill pretraining with task-aware finetuning, utilizing a skill discriminator to facilitate intrinsic reward matching instead of relying solely on direct finetuning. The methodology emphasizes sample-efficient optimization of skills for unseen tasks by leveraging learned intrinsic rewards to improve skill selection, thereby reducing the dependence on costly environment interactions. Empirical results demonstrate the effectiveness of IRM on the **Unsupervised Reinforcement Learning Benchmark (URLB)** and in tabletop manipulation tasks, showcasing significant improvements in sample efficiency and zero-shot performance compared to baseline methods.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative integration of intrinsic rewards into the skill selection process, which addresses a notable gap in current reinforcement learning methodologies focused on efficiency. The empirical results support the claims made, with IRM methods outperforming traditional approaches in various tasks. However, a notable weakness is the reliance on the availability of task reward functions, which may limit the applicability of the approach in environments where such information is not readily available. Additionally, the paper could benefit from clearer explanations of some complex concepts, particularly for readers who may not be familiar with the intricacies of mutual information maximization and the EPIC framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its contributions clearly, although some sections could be more accessible to a broader audience. The quality of the writing is high, with appropriate mathematical formulations supporting the methodology. The novelty of the approach is significant, as it combines elements from unsupervised skill discovery and task adaptation in a way that has not been extensively explored in prior literature. The reproducibility of the experiments is supported by detailed descriptions of the methodologies and hyperparameters, which is a positive aspect of the paper.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and novel framework for skill-based reinforcement learning that effectively addresses limitations in existing methods. While the empirical results are promising, the reliance on task reward functions could limit practical applications. Enhancing clarity in the presentation of complex concepts would further improve the paper's accessibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework called Intrinsic Reward Matching (IRM) that effectively integrates unsupervised skill pretraining with task-aware finetuning in the context of reinforcement learning (RL). The authors propose a unique loss function, EPIC, to facilitate skill selection and task inference, thereby improving sample efficiency and performance. Experimental results on benchmark tasks indicate that IRM outperforms existing methods, particularly in complex tasks, while demonstrating robustness across various environments.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its unified framework, which addresses a methodological gap in RL by combining skill pretraining with finetuning. The improved sample efficiency is another notable contribution, as it allows for skill selection with minimal environment rollouts. However, the reliance on well-defined task reward functions during finetuning poses limitations, especially in environments with sparse rewards. While the framework shows promise in generalizing to long-horizon tasks, the fixed skill horizon assumption may not be optimal in all scenarios. Furthermore, the robustness of the methods is contingent upon the environment dynamics, which may lead to variable performance. Lastly, while the visualizations provided are insightful, they may not suffice for thorough understanding without accompanying quantitative metrics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its ideas clearly, making it accessible to the reader. The quality of the methodology is high, with a solid theoretical foundation supporting the proposed framework. The novelty of the approach is significant, particularly with the introduction of the EPIC loss function. However, reproducibility may be impacted by the specific tuning required for the loss function and the dependence on task reward structures.\n\n# Summary Of The Review\nIn summary, the paper makes a substantial contribution to the field of reinforcement learning by introducing the IRM framework, which enhances sample efficiency and skill selection. While the approach shows notable strengths, it is also accompanied by limitations that warrant further exploration and refinement in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Skill-Centric Reinforcement Learning through Contextual Reward Alignment\" introduces a novel approach called Contextual Reward Alignment (CRA), which aims to integrate unsupervised skill acquisition with task-specific reinforcement learning. The CRA framework employs a skill representative model and a contextual discriminator to enhance skill selection efficiency and improve downstream task performance. Key contributions include a unified learning framework that leverages a contextual discriminator actively during finetuning, the introduction of the Contextual Alignment Loss (CAL) for optimal skill determination, and empirical validation across multiple challenging benchmarks. The results indicate significant improvements in sample efficiency and task performance compared to traditional methods.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative integration of unsupervised skill discovery with reinforcement learning, which is a significant advancement in the field. The use of the contextual discriminator as an active component during finetuning is a novel perspective that enhances learning efficiency. Additionally, the empirical results demonstrate strong performance across various tasks, underscoring the practical applicability of the proposed method. However, a notable weakness is the reliance on an initial set of contextual rewards, which may not always be available or well-defined in real-world scenarios. Furthermore, the performance of the CRA framework may be sensitive to the quality of the contextual discriminator and the skills learned during the unsupervised phase.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, contributions, and findings. The writing quality is high, with a coherent flow of ideas and logical progression through the proposed framework. The novel aspects of the CRA approach, particularly the introduction of CAL and the active role of the skill discriminator, contribute to the originality of the work. However, reproducibility could be a concern due to the dependence on the contextual discriminator's quality and the need for well-defined contextual rewards, which may not be straightforward in practice.\n\n# Summary Of The Review\nOverall, the paper presents a significant and innovative advancement in reinforcement learning by effectively combining unsupervised skill discovery with task-specific learning. The empirical results validate the proposed methodology, showcasing its potential for practical applications, although some limitations regarding the requirement for contextual rewards and sensitivity to discriminator quality should be addressed.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel adversarial training framework known as Intrinsic Reward Matching (IRM). The authors address inefficiencies in existing adversarial training methods that typically depend on extensive environment rollouts and a lack of systematic adversarial example selection. The IRM framework integrates adversarial example generation with a discriminator model, effectively bridging the gap between task-agnostic adversarial generation and task-specific fine-tuning. Key contributions include a unified training framework, improved sample efficiency, optimization without environment interactions, and the capability to generalize to long-horizon tasks.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to adversarial training, particularly the use of a discriminator to select adversarial examples, which can significantly improve training efficiency. The experimental results are robust, demonstrating the efficacy of IRM against traditional methods. The theoretical foundations, including an analysis of the EPIC loss function, add depth to the proposed methodology. However, the reliance on the quality of the discriminator poses a risk for optimal performance. Additionally, the evaluation scope is somewhat limited to specific tasks, and the complexity of implementation may deter practical adoption.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, and the methodology is articulated clearly, making it accessible to readers. The quality of the experimental setup is high, with thorough evaluations presented. The novelty of the proposed approach is significant, as it provides a new perspective on adversarial training. However, reproducibility could be a concern due to the reliance on the discriminator's performance and the added complexity of the training process.\n\n# Summary Of The Review\nOverall, this paper offers a promising new approach to adversarial training that effectively addresses existing challenges related to sample efficiency and adaptability. The incorporation of a discriminator for adversarial example selection presents a noteworthy advancement that could influence future research in adversarial machine learning. With minor adjustments in evaluation scope and practical implementation discussion, the work could have a substantial impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework called Intrinsic Reward Matching (IRM) aimed at unifying skill pretraining and downstream finetuning in reinforcement learning (RL). The authors claim that IRM resolves long-standing challenges by utilizing a skill discriminator for task inference and skill selection without requiring environment samples. They demonstrate competitive performance on various benchmarks, including the Universal Reinforcement Learning Benchmark (URLB), and show improved finetuning on complex tabletop tasks while claiming generalization to long-horizon tasks. The paper concludes by asserting the transformative potential of IRM for future research in RL.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its proposed framework, IRM, which attempts to address the limitations of current RL methods by emphasizing skill selection and efficiency. However, the paper suffers from significant weaknesses, including exaggerated claims regarding the framework's capabilities and effectiveness. The authors misrepresent the novelty and applicability of IRM, suggesting that it is a universally applicable solution and downplaying the limitations and challenges that remain in the field. Additionally, the experimental results, while competitive, are framed as definitive proof of superiority without adequately addressing potential shortcomings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by its tendency to overstate the significance of the contributions, which may lead to misconceptions about the actual impact of IRM. While the methodology is described in a coherent manner, the exaggeration of its novelty detracts from the overall quality. Reproducibility may be a concern, as the paper does not provide sufficient detail on the experimental setups or the specific conditions under which the results were obtained, making it difficult for others to validate the findings.\n\n# Summary Of The Review\nOverall, the paper presents an ambitious framework aimed at advancing skill-based reinforcement learning. However, the claims regarding the contributions and effectiveness of the proposed methodology are overstated, leading to potential misunderstandings of its true impact and applicability in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the Intrinsic Reward Matching (IRM) framework, which aims to unify unsupervised skill pretraining and downstream task finetuning in reinforcement learning (RL). The authors claim that IRM enhances sample efficiency and skill selection without necessitating additional environment samples. The methodology involves evaluating IRM across 12 continuous control tasks using the Unsupervised Reinforcement Learning Benchmark (URLB) and demonstrates superior performance in both general task execution and specific tabletop manipulation tasks. The findings indicate that IRM not only outperforms traditional methods but also provides insights into effective skill selection through empirical analysis.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to skill selection, effectively addressing significant challenges in RL related to data collection costs. The experimental results show that IRM achieves competitive performance, particularly in complex tasks where sample efficiency is crucial. However, the reliance on an external reward function and assumptions about workspace boundaries are notable weaknesses, as they may limit the applicability of the proposed framework. Additionally, while the paper provides strong empirical results, the discussion on limitations could be expanded to address potential scenarios where IRM may underperform.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The clarity of presentation is supported by visualizations that enhance understanding of the optimization landscape and skill selection process. The quality of the experiments is commendable, although the reproducibility may be hindered by the reliance on specific external reward functions and the potential need for extensive parameter tuning. Overall, the novelty of the IRM framework is evident, making significant strides in addressing common challenges in RL.\n\n# Summary Of The Review\nThe paper presents a compelling framework for skill-based reinforcement learning that shows promise in improving sample efficiency and performance across various tasks. While the contributions are significant and the methodology is sound, certain limitations regarding external dependencies warrant further exploration. Overall, IRM represents a noteworthy advancement in the field of reinforcement learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel method called Intrinsic Reward Matching (IRM) aimed at improving sample efficiency in reinforcement learning by leveraging a skill discriminator to align intrinsic rewards from pretraining tasks with extrinsic rewards in downstream tasks. The authors claim that their approach circumvents the need for environment rollouts during skill selection, thereby enhancing efficiency. However, the methodology is predicated on several assumptions regarding reward function accessibility and task similarity, which may limit its applicability in real-world scenarios. The findings suggest that while IRM has the potential to improve skill selection and performance, its effectiveness is contingent upon the validity of these assumptions.\n\n# Strength And Weaknesses\nThe strengths of this paper include the introduction of the IRM framework, which proposes an innovative approach to skill selection without requiring extensive environment interactions. The potential for improved sample efficiency is also a noteworthy contribution. However, there are significant weaknesses. The reliance on the reward function being accessible (R_ext) raises questions about real-world applicability, as many environments do not provide explicit reward signals. Additionally, the method's dependence on a skill discriminator introduces risks related to generalization across tasks. The implicit assumption of task similarity and the neglect of exploration mechanisms further limit the approach's robustness and adaptability. Overall, while the theoretical basis is interesting, practical challenges remain unaddressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of its methodology and findings. However, clarity is somewhat diminished by the heavy reliance on assumptions that are not sufficiently justified or explored. The novelty of the proposed method is notable, but the paper does not adequately address the implications of its assumptions on reproducibility and generalizability across diverse tasks. The experimental evaluation is limited to specific environments, raising concerns about the broader applicability of the results.\n\n# Summary Of The Review\nThe paper introduces a promising framework for intrinsic reward matching, but it is hindered by several critical assumptions that may undermine its practical applicability. While the methodology is innovative, the concerns regarding generalization, robustness, and exploration mechanisms warrant further investigation. Overall, the contributions are significant but require more empirical validation across a wider range of tasks.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents Intrinsic Reward Matching (IRM), a novel framework that bridges unsupervised skill discovery and task-aware fine-tuning in reinforcement learning (RL). IRM employs a skill discriminator to align intrinsic rewards with downstream task rewards, significantly enhancing sample efficiency during fine-tuning. The authors demonstrate that IRM generalizes effectively to complex long-horizon tasks, outperforming existing methods in zero-shot performance and sample efficiency, particularly in intricate manipulation scenarios.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to integrating skill discovery and task adaptation, addressing a critical gap in the literature on deep RL. The introduction of the EPIC loss as an optimization target for skill selection is a significant contribution that enhances the connection between pretraining and fine-tuning. However, the paper's reliance on well-defined task rewards and state distributions may limit its applicability in environments where such specifications are not readily available. Additionally, while the empirical evaluations show promising results, further exploration of the method's robustness across diverse environments could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and experimental results. The discussions around the EPIC loss and its implications for skill selection are particularly well-presented, aiding in comprehension. The quality of the writing is high, with appropriate use of technical language. The novelty of the proposed method is evident, and the experiments are designed to showcase its effectiveness. However, the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setups and hyperparameter choices.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling framework that significantly advances the field of unsupervised skill discovery and task adaptation in RL. While the methodology is novel and the empirical results are promising, the reliance on specific task definitions and the need for clearer reproducibility guidelines are notable areas for improvement.\n\n# Correctness\nRating: 4/5\nThe methodology and results presented in the paper appear to be sound, with experiments validating the proposed approach.\n\n# Technical Novelty And Significance\nRating: 5/5\nThe introduction of the EPIC loss and the IRM framework represents a substantial advancement in the integration of skill discovery and task adaptation in RL, marking a significant contribution to the field.\n\n# Empirical Novelty And Significance\nRating: 4/5\nThe empirical results demonstrate the effectiveness of IRM against various baselines, although further exploration in diverse environments would strengthen the significance of these findings.",
    "# Summary Of The Paper\nThe paper presents a novel framework for improving the performance of deep learning models through the integration of adaptive learning rate techniques and regularization strategies. The authors propose a hybrid methodology that combines these elements to enhance model robustness and generalization on various benchmark datasets. Experimental results demonstrate that their approach outperforms existing state-of-the-art methods in terms of accuracy and computational efficiency, particularly in scenarios with limited labeled data.\n\n# Strength And Weaknesses\n**Strengths**:\n1. **Innovative Integration**: The combination of adaptive learning rates with regularization techniques is a promising approach that addresses multiple challenges in model training.\n2. **Strong Theoretical Backing**: The paper is grounded in solid theoretical principles, providing a sound rationale for the proposed framework.\n3. **Clear Evaluation**: The authors conduct comprehensive experiments across multiple datasets, showcasing the versatility and effectiveness of their method.\n4. **Relevance**: The addressed problem is pertinent to current challenges in the field, making the research timely and impactful.\n\n**Weaknesses**:\n1. **Limited Comparison with Baselines**: While the results are impressive, there is a lack of rigorous comparison with a broader range of existing methods that would contextualize the improvements more effectively.\n2. **Complexity of Implementation**: The proposed methodology is relatively complex, which may hinder practical adoption among practitioners who could benefit from simpler alternatives.\n3. **Narrow Focus on Datasets**: The experiments are primarily conducted on a limited set of benchmark datasets, raising questions about the generalizability of the results.\n4. **Evaluation Metrics**: The choice of metrics could be expanded to include other performance aspects such as robustness to noisy data or scalability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and generally clear, making it accessible to a wide audience. Key concepts are adequately explained, although some areas could benefit from further elaboration, especially regarding the practical implications of the methodology. The novelty of the approach is significant, particularly in its integration of adaptive techniques. However, the complexity of the framework raises concerns about reproducibility, as detailed implementation guidelines may be lacking.\n\n# Summary Of The Review\nOverall, this paper offers an innovative contribution to the field of deep learning by merging adaptive learning rates with regularization strategies. While the theoretical foundation and experimental results are compelling, enhancements in baseline comparisons and simplification of the methodology would bolster its impact and applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces Intrinsic Reward Matching (IRM), a novel methodology aimed at bridging the gap between unsupervised skill pretraining and downstream task finetuning in reinforcement learning (RL). By utilizing a skill discriminator, IRM aligns intrinsic rewards with task rewards, enabling optimal skill selection without the need for extensive environment interactions, thereby enhancing sample efficiency. The authors extend their framework to handle complex, long-horizon tasks through skill sequencing, demonstrating its effectiveness on various benchmarks and challenging tabletop tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to skill selection, which alleviates the inefficiencies associated with traditional RL methods that depend on environment trials. The introduction of IRM facilitates a more systematic integration of pretraining and finetuning, potentially leading to improved performance in real-time applications. However, a notable weakness is the reliance on external reward functions and the challenge posed by the need for bounded state samples, which may limit the method's applicability in real-world scenarios. Additionally, the paper could benefit from a more detailed discussion on the limitations and potential pitfalls of implementing IRM.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations and contributions of the proposed methodology. The explanations of the methodology are generally straightforward, but some technical details may require further elaboration for readers not deeply familiar with the field. The novelty of the approach is significant, as it addresses a pressing issue in RL by effectively linking skill acquisition and task performance. The reproducibility of the experiments appears to be feasible, contingent upon the provision of relevant implementation details and datasets, which are not fully discussed in the current work.\n\n# Summary Of The Review\nOverall, the paper presents a compelling advancement in the field of reinforcement learning by introducing the IRM framework, which efficiently integrates skill-based learning with task adaptation. While the contributions are noteworthy and the methodology demonstrates promise, some limitations regarding real-world applicability and clarity in implementation could be addressed in future iterations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach called Intrinsic Reward Matching (IRM) aimed at bridging the gap between task-agnostic skill pretraining and task-aware finetuning in reinforcement learning. The methodology introduces a unique loss function termed EPIC loss, which facilitates the alignment of intrinsic and extrinsic rewards, optimizing skill selection without requiring environment rollouts. The authors demonstrate the effectiveness of their approach through experiments on the Unsupervised Reinforcement Learning Benchmark (URLB) and tabletop manipulation tasks, showcasing improvements in sample efficiency and performance on both simple and complex tasks.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative framework for skill selection that enhances sample efficiency and its competitive performance against existing methods, which is validated through rigorous experiments. The proposed EPIC loss is a notable contribution, as it effectively guides the skill selection process. However, a weakness lies in the reliance on external reward functions, which may limit the applicability of the method in environments where such rewards are not easily defined. Additionally, while the results are compelling, further exploration into the scalability and robustness of the approach in diverse settings would strengthen the paper's claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly outlines its contributions and methodologies. The clarity of the presentation, particularly in the definition of IRM and the explanation of the EPIC loss, allows for a good understanding of the proposed approach. The quality of the experiments is high, with comparisons to baseline methods and detailed reporting of results. However, reproducibility could be enhanced by providing more extensive details about the experimental setups, hyperparameters, and environment specifications in the main text, rather than relegating them to the appendix.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of reinforcement learning by introducing Intrinsic Reward Matching, which effectively integrates skill pretraining and finetuning processes. While the proposed method shows promising results in terms of efficiency and performance, further exploration of its limitations and scalability is warranted.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper, titled \"SKILL-BASED REINFORCEMENT LEARNING WITH INTRINSIC REWARD MATCHING,\" addresses the methodological disconnect in skill-based learning within reinforcement learning (RL) frameworks, particularly in robotic control applications. The authors introduce the Intrinsic Reward Matching (IRM) framework, which allows for skill selection without requiring environment samples. The paper details the algorithmic approach, including the EPIC loss for skill evaluation, and presents experimental results that demonstrate the effectiveness of IRM compared to various baselines, indicating significant performance improvements in skill selection processes.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its clear identification of a relevant problem in skill-based RL and the introduction of a novel framework that effectively addresses this issue. The methodology is sound, and the empirical results provide strong evidence for the proposed approach's efficacy. However, a notable weakness is the reliance on external reward functions, which limits the generalizability of the findings. Additionally, while the experiments are well-structured, further exploration of broader applications could strengthen the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and generally clear, with sections logically ordered to facilitate understanding. The methodology is described in sufficient detail to allow for reproducibility, although some technical terms could benefit from further clarification. The novelty of the proposed framework is significant, as it integrates existing methodologies in a unique manner that advances the field of skill-based RL. The quality of the figures and tables is high, effectively complementing the textual data.\n\n# Summary Of The Review\nOverall, this paper presents a valuable contribution to the field of skill-based reinforcement learning, with a well-defined methodology and compelling experimental results. While minor revisions for clarity and additional experimental validations could enhance the work, it is suitable for presentation at the conference.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents \"Intrinsic Reward Matching\" (IRM), a novel framework aimed at bridging the gap between task-agnostic skill pretraining and task-aware fine-tuning in reinforcement learning (RL). It introduces a skill discriminator to optimize intrinsic rewards, allowing for effective skill selection without extensive environment rollouts. The authors demonstrate the efficacy of IRM through empirical validation on the Unsupervised Reinforcement Learning Benchmark (URLB) and complex manipulation tasks, showcasing improved performance in skill selection and long-horizon task execution compared to conventional methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the inefficiencies of existing RL paradigms by utilizing intrinsic rewards for skill selection. The formulation of the IRM framework allows for sample-efficient learning, which is particularly advantageous in real-world applications where data collection is costly. However, a notable weakness is the reliance on external reward functions, which may limit the applicability of the approach in scenarios where such functions are difficult to define. Additionally, the paper could benefit from a more extensive exploration of the limitations and potential biases in the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, with adequate background information provided for context. The methodology is articulated coherently, allowing readers to follow the progression from problem statement to proposed solution. The novelty of the IRM framework is significant, as it represents a meaningful advance in skill-based reinforcement learning. However, the reproducibility of the results could be enhanced with more detailed descriptions of the experimental setup, hyperparameter tuning, and additional ablation studies.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of reinforcement learning by introducing a novel framework for intrinsic reward matching that enhances skill selection and fine-tuning efficiency. While the methodology is innovative and well-presented, some areas could benefit from further clarification and exploration of limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Intrinsic Reward Matching (IRM), a novel approach aimed at unifying skill pretraining and downstream task fine-tuning in reinforcement learning. The methodology incorporates a skill discriminator during the finetuning process and introduces the EPIC loss function. The authors claim that IRM enhances sample efficiency compared to conventional methods, especially in tasks with sparse rewards. However, the findings indicate mixed performance across different environments, raising questions about the method's robustness and generalizability.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to integrate skill pretraining with downstream task finetuning, which is a relevant and important area of research in reinforcement learning. However, several weaknesses undermine the paper's contributions. The methodology lacks clarity, particularly regarding the practical implications of the skill discriminator during finetuning. The claim of improved sample efficiency is not convincingly supported by the results, as metrics do not show significant advantages. Additionally, the performance of IRM is inconsistent across tasks, particularly in the Jaco environment, which raises concerns about its generalizability. The reliance on a well-defined reward function limits the approach's applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is a significant concern; the proposed methodology is not well articulated, making it challenging for readers to understand its practical applications. The quality of the empirical results is questionable due to inconsistent performance across various tasks and the limited scope of benchmarks used for evaluation. While the introduction of the EPIC loss is presented as a novel contribution, it lacks substantial differentiation from existing loss functions in reinforcement learning. Furthermore, the paper does not adequately address reproducibility, particularly regarding the computational complexity associated with the proposed methods.\n\n# Summary Of The Review\nIn summary, the paper presents an interesting concept in the form of Intrinsic Reward Matching, but it suffers from significant issues related to clarity, methodological robustness, and empirical validation. The numerous weaknesses and unclear contributions considerably diminish the paper's overall impact.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents **Intrinsic Reward Matching (IRM)**, a novel framework that integrates unsupervised skill pretraining with downstream finetuning, significantly enhancing reinforcement learning methodologies. The key contributions include the innovative use of a skill discriminator during finetuning, the introduction of the EPIC loss for skill selection, and the capability to sequence skills effectively for complex tasks. The findings demonstrate that IRM achieves competitive performance on the Unsupervised Reinforcement Learning Benchmark and excels in challenging tabletop manipulation tasks, showcasing improved sample efficiency and adaptability.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to skill-based learning, particularly the use of a skill discriminator that boosts sample efficiency and the EPIC loss which refines task alignment with intrinsic rewards. These contributions are significant in advancing both theoretical and practical aspects of reinforcement learning. However, potential weaknesses may arise from the complexity of the methodology, which could pose challenges in reproducibility or understanding for practitioners less familiar with the underlying concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and results. The quality of the writing supports comprehension of the complex technical details involved in IRM. The novelty is evident in the unique integration of skill discriminator usage and the EPIC loss, marking a substantial advancement in the field. Nevertheless, the paper could benefit from more detailed explanations of the mechanisms behind these innovations to enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in reinforcement learning through the introduction of Intrinsic Reward Matching. Its innovative methodologies and competitive results highlight its potential for real-world applications. Despite some complexity in the proposed framework, the contributions of this research are impactful and pave the way for future explorations in skill-based reinforcement learning.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents the Intrinsic Reward Matching (IRM) framework, which is grounded in the theoretical principles of competence-based unsupervised reinforcement learning. The main contributions include the innovative representation of skills as latent code vectors, the development of an intrinsic reward function that aligns with extrinsic task rewards, and the introduction of the EPIC loss framework for measuring reward function similarity. The methodology involves skill discovery without explicit supervision and a theoretical reframing of task inference as an optimization problem. The findings suggest that IRM can effectively enable agents to autonomously select skills for complex, long-horizon tasks, offering a comprehensive approach to skill-based reinforcement learning.\n\n# Strength And Weaknesses\nStrengths of the paper include its strong theoretical foundation and the introduction of novel concepts such as the EPIC loss framework, which provides valuable insights into the relationships between tasks and skills. The approach of using a skill discriminator as a task specifier challenges traditional views and opens up new avenues for utilizing pretrained models. However, a notable weakness is the acknowledgment of limitations regarding the dependency on well-shaped reward functions and the need for external rewards, which may restrict the framework's applicability in certain contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical contributions, making complex concepts accessible to the reader. The quality of the writing is high, and the theoretical insights are presented in a coherent manner. However, the reproducibility of the findings may be hindered by the abstract nature of the theoretical constructs, which may require further empirical validation to fully assess their practical implications.\n\n# Summary Of The Review\nOverall, the paper provides a significant theoretical advancement in the field of skill-based reinforcement learning through the IRM framework. While the theoretical contributions are robust, the practical applicability of the framework may be limited due to certain assumptions regarding reward functions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Skill-Based Reinforcement Learning with Intrinsic Reward Matching\" presents a novel framework for skill selection and fine-tuning in reinforcement learning (RL) through a method called Intrinsic Reward Matching (IRM). The methodology consists of three primary stages: task-agnostic pretraining of skill primitives, minimization of EPIC loss to align intrinsic and extrinsic rewards, and finetuning of policies using optimal skill vectors. The findings demonstrate that IRM effectively enhances performance across various environments, outperforming traditional methods while requiring fewer interactions with the environment.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its structured approach to skill discovery and the comprehensive implementation details provided, which facilitate reproducibility. The extensive ablation studies and comparisons with existing methods showcase the robustness of the IRM framework. However, the paper primarily focuses on technical details, which may obscure the broader implications of the findings. Additionally, while the methodology is well-articulated, a more integrated discussion of the theoretical contributions and their relevance to the field could enhance the overall impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with detailed descriptions of the algorithm, implementation specifics, and hyperparameter configurations that contribute to reproducibility. The quality of the research is high, given the rigorous experimental validation and thorough ablation studies. In terms of novelty, the approach of intrinsic reward matching represents a significant advancement in RL methodologies, although it builds on existing frameworks for skill discovery. The reproducibility aspect is well-addressed through the availability of implementation details and code.\n\n# Summary Of The Review\nOverall, this paper presents a significant contribution to the field of reinforcement learning through the introduction of the Intrinsic Reward Matching framework. While the technical quality and clarity are strong, the paper could benefit from a deeper exploration of the broader implications and contributions of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Intrinsic Reward Matching (IRM), a novel framework that integrates unsupervised skill discovery with downstream task adaptation. The authors present IRM as a method that claims to enhance sample efficiency and facilitate skill selection without requiring extensive interactions with the environment. Experimental results demonstrate that IRM outperforms several baselines in the Unsupervised Reinforcement Learning Benchmark (URLB) tasks, and the authors propose a new metric, the EPIC loss, for evaluating skill effectiveness.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to skill discovery and its potential for sample efficiency. However, the paper is weakened by insufficient comparisons to established baselines, such as the coarse grid search method, which raises questions about the validity of its claims. Additionally, the lack of rigorous ablation studies and the omission of extensive environment interactions undermine the robustness of the findings. The authors' assertions regarding generalization to long-horizon tasks and zero-shot performance evaluations also appear overly optimistic and lack necessary contextual grounding in the existing literature.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate; while the main ideas are presented, the depth of analysis is lacking, particularly when compared to prior works that have explored similar topics. The novelty of the IRM approach is commendable, but it does not sufficiently challenge existing methodologies, which diminishes its impact. The reproducibility of the results is questionable due to the absence of comprehensive validation and rigorous comparisons with established methods.\n\n# Summary Of The Review\nOverall, while the paper introduces some interesting concepts such as IRM and EPIC loss, it fails to provide the depth of analysis and comparative framework necessary to validate its contributions fully. The novelty and significance of the work are evident but are overshadowed by a lack of rigorous evaluation against existing methodologies.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Skill-Based Reinforcement Learning with Intrinsic Reward Matching\" proposes a novel framework that integrates intrinsic rewards for skill-based reinforcement learning (RL). The main contributions include the introduction of the Intrinsic Reward Matching (IRM) algorithm, which facilitates the identification and application of pretrained skills in RL tasks. The methodology involves unsupervised skill pretraining and a comparison of policies using an equivalent-policy invariant comparison (EPIC). Empirical results demonstrate that the proposed approach enhances performance on various RL benchmarks, particularly in complex control tasks.\n\n# Strength And Weaknesses\nStrengths of the paper lie in its innovative approach to bridging the gap between skill learning and RL, which is a relevant area in the field. The empirical results suggest that IRM can significantly improve the efficiency of skill application in challenging tasks, showcasing its practical utility. However, the paper has several weaknesses including unclear definitions of key terms and inconsistent formatting throughout the text, which hinder readability. Additionally, the results could benefit from a more comprehensive discussion on the limitations and potential applications of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper suffers due to several grammatical issues and formatting inconsistencies, including missing definitions for key notations and non-uniform citation styles. The quality of the empirical evaluation is reasonable, but the lack of detailed explanations in some sections may hinder reproducibility. The novelty of the proposed IRM approach is significant, offering a fresh perspective on skill-based RL, though the paper could strengthen its contributions by addressing the aforementioned clarity issues.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to skill-based reinforcement learning through intrinsic reward matching. While the contributions are noteworthy, the clarity and consistency of the writing and formatting need substantial improvement to enhance its readability and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel approach to skill-based reinforcement learning through Intrinsic Reward Matching (IRM). It proposes the EPIC loss as an optimization metric to enhance sample efficiency and improve performance in continuous control tasks. The authors present empirical results demonstrating that IRM outperforms existing methods in specific environments, while also discussing the implications of the skill dimension on performance. However, the paper identifies limitations regarding the reliance on external reward functions and the need for exploration into alternative reward shaping techniques.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to intrinsic rewards and the introduction of the EPIC loss, which shows promise in improving sample efficiency. However, the paper lacks a thorough exploration of potential scalability issues in larger or more complex environments. The evaluation is limited to specific continuous control tasks, and expanding it to include a broader range of environments would strengthen the findings. Additionally, while the discussion of skill selection is valuable, it lacks comparative analysis with other contemporary skill discovery approaches.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and generally clear in its presentation of the methodology and findings. However, certain aspects, such as the limitations of the EPIC loss and the role of the skill discriminator, would benefit from deeper theoretical justification. The novelty of the proposed method is evident, but the paper could improve on reproducibility by providing more detailed experimental setups and results across a wider variety of tasks.\n\n# Summary Of The Review\nThe paper presents a promising approach to skill-based reinforcement learning through Intrinsic Reward Matching, demonstrating improvements in sample efficiency. However, it requires further exploration of its limitations and broader applicability to enhance the overall significance of its contributions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach called Intrinsic Reward Matching (IRM) to improve sample efficiency in reinforcement learning (RL). The proposed framework utilizes a skill discriminator to facilitate task inference without requiring environmental samples, thereby enabling better skill selection. The authors demonstrate that IRM performs competitively on the Unsupervised Reinforcement Learning Benchmark (URLB) across 12 continuous control tasks, showcasing improved sample efficiency on tabletop manipulation tasks and generalizing to long-horizon tasks through skill sequencing. Additionally, the paper includes visualizations and analytical insights that justify the effectiveness of the skill selection process.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative framework that consolidates skill selection and task inference without environmental samples, which is a significant advancement in the field of RL. The empirical results demonstrate that IRM outperforms baseline methods in terms of sample efficiency and generalization, particularly in complex tasks. However, a notable weakness is the lack of explicit statistical hypothesis testing to validate performance differences, which could undermine confidence in the reported results. Additionally, while the paper discusses generalization to long-horizon tasks, it does not fully explore the trade-off between performance improvement and overfitting risks associated with increasing skill dimensions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers. The methodology is presented with sufficient detail, enabling reproducibility, particularly through the inclusion of performance metrics and statistical evaluations. However, the absence of explicit statistical significance tests limits the robustness of the findings, which could be addressed in future work. The novelty of the approach is significant, as it presents a fresh perspective on sample efficiency in RL, yet it would benefit from a deeper exploration of the implications of its findings.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of reinforcement learning through the introduction of the Intrinsic Reward Matching framework, which enhances sample efficiency and skill selection without relying on environmental samples. While the findings are promising, the lack of rigorous statistical testing and the discussion of potential overfitting risks warrant further attention.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a framework for skill selection in reinforcement learning environments, leveraging an EPIC loss function to optimize the learning of skills based on task rewards. The methodology emphasizes the use of well-defined external reward functions to guide skill selection, but it is limited by its reliance on specific spatial constraints and the availability of accurate reward signals. The findings indicate that while the proposed method shows promise, it struggles with tasks characterized by sparse rewards and high-dimensional skill spaces.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its structured approach to skill selection, which is well-articulated and theoretically grounded. However, the methodology exhibits significant weaknesses, particularly in its reliance on clearly defined external reward functions, which may not be available in many real-world scenarios. The approach requires samples within specific workspace boundaries, limiting its generalizability. Additionally, the paper does not adequately address the challenges of scaling to high-dimensional skill spaces or the optimization complexities associated with the EPIC loss function.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, although certain aspects, such as the computational complexity of the optimization process and the exploration of intrinsic reward functions, are under-explained. The novelty of the proposed framework is moderate; while it introduces an interesting approach to skill selection, it does not sufficiently leverage existing methods or explore alternative models that could enhance its effectiveness. Reproducibility is hindered by the lack of detailed experimental setups and the omission of variations in sampling distributions, leaving important questions unanswered.\n\n# Summary Of The Review\nOverall, the paper contributes a structured method for skill selection in reinforcement learning, though it is hampered by significant limitations in generalizability and reliance on well-defined reward functions. The findings suggest potential applications, but the paper could benefit from addressing the challenges of sparse rewards and high-dimensional spaces more robustly.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Skill-Based Reinforcement Learning with Intrinsic Reward Matching\" proposes a unifying framework that combines skill pretraining with task-specific reinforcement learning using a method termed Intrinsic Reward Matching (IRM). The authors introduce an \"EPIC Loss\" function to facilitate this process, aiming to improve sample efficiency and performance on long-horizon tasks. The experiments conducted on benchmark URLB tasks yield marginal improvements in performance, particularly in zero-shot settings, but the overall contributions appear to be incremental rather than groundbreaking.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to unify various methods in reinforcement learning, addressing the challenge of skill discovery and task adaptation. However, the weaknesses are pronounced; the contributions are not particularly novel, as many ideas presented have been explored previously in the literature. The paper's reliance on existing benchmarks, combined with its lack of substantial empirical improvements, raises questions about the significance of its findings. Additionally, the overly complex language and presentation detract from the accessibility and clarity of the work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper suffers from convoluted language and jargon that could be simplified for better comprehension. The quality of the methodology is sound, but the novelty is questionable, as many concepts appear to be repackaged rather than genuinely innovative. Reproducibility may be a concern, especially since the findings hinge on marginal improvements without robust validation across diverse tasks.\n\n# Summary Of The Review\nOverall, the paper presents an attempt to synthesize existing ideas in reinforcement learning, but it does so with limited novelty and empirical significance. While the framework is structured and methodologically sound, the contributions are overshadowed by a lack of originality and clarity.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces a novel framework called Intrinsic Reward Mechanism (IRM) that integrates unsupervised skill discovery with downstream task adaptation in reinforcement learning. The authors propose a methodology that allows agents to autonomously discover and sequence skills without relying on a manager policy, thereby enhancing their adaptability to various tasks. The findings demonstrate that this integration improves performance in environments with complex reward structures, suggesting a promising avenue for improving reinforcement learning efficiency and effectiveness.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to skill discovery and the successful integration of intrinsic rewards, which addresses significant challenges in reinforcement learning. The empirical results indicate that the IRM framework outperforms several baseline methods in specific environments. However, the paper could benefit from a more extensive comparison with recent state-of-the-art techniques, particularly those leveraging self-supervised learning. Additionally, while the lack of a manager policy is an interesting angle, exploring a hierarchical approach could provide further insights into skill management under varying conditions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections and clear explanations of the methodology and findings. The quality of the experiments is robust, showcasing the performance of the proposed framework. However, the reproducibility could be improved by providing more detailed descriptions of the experimental setups and hyperparameter choices. The novelty of the approach is evident, particularly in the integration of unsupervised skill discovery and intrinsic rewards, although further exploration of generative models and reward shaping techniques could enhance its significance.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of reinforcement learning by introducing the IRM framework, which effectively integrates skill discovery with task adaptation. While the findings are promising, further comparisons with recent methodologies and a deeper exploration of aspects such as reward shaping and generative models would strengthen the contributions and practical applicability of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel approach called Intrinsic Reward Matching (IRM) aimed at enhancing sample efficiency in reinforcement learning (RL) by optimizing skill selection without extensive environment interactions. The authors benchmark IRM performance on the Unsupervised Reinforcement Learning Benchmark (URLB) across twelve continuous control tasks, demonstrating that IRM methods, specifically IRM CEM and IRM GD, achieve strong zero-shot performance, often outperforming or matching traditional baseline methods reliant on environment rollouts. Notably, the findings highlight significant improvements in sample efficiency and effectiveness in skill selection, particularly in tasks such as Fetch Reach and Fetch Push, while also showcasing IRM's robustness in complex manipulation and long-horizon tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to skill selection, which circumvents the need for extensive environment interactions, thus increasing sample efficiency. The empirical results substantiate the claims, indicating that IRM methods significantly outperform traditional methods across various tasks, including long-horizon challenges. However, a potential weakness is the limited exploration of the theoretical underpinnings of IRM, which could provide deeper insights into its effectiveness. Additionally, while the reported results include statistical measures, further exploration of the conditions under which IRM excels or may falter would strengthen the paper’s contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its findings clearly, with a logical flow from methodology to results. The quality of the experiments is high, and the use of the URLB for benchmarking provides a comprehensive evaluation of IRM's performance. In terms of novelty, IRM presents a significant advancement over existing methods in skill selection for RL, particularly due to its focus on intrinsic rewards. Reproducibility is supported by the inclusion of performance metrics with standard error bars, although further details on implementation specifics would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of reinforcement learning with its novel IRM approach, demonstrating significant improvements in sample efficiency and skill selection across various tasks. While the empirical findings are robust, a deeper theoretical analysis and exploration of limitations would further enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to [specific problem or topic], introducing key contributions such as [list main contributions]. The methodology involves [briefly describe methodology], and the findings indicate that [summarize key findings]. The authors demonstrate that their approach significantly outperforms existing methods in terms of [mention metrics or outcomes], providing new insights into [domain or application].\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative methodology and the comprehensive evaluation against state-of-the-art benchmarks, which highlights its effectiveness. However, weaknesses include the dense abstract that may overwhelm readers and the use of technical jargon that could alienate a broader audience. Additionally, while the contributions are significant, the paper could benefit from clearer organization and structure to enhance readability and engagement.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by long and complex sentence structures and inconsistent terminology, which may confuse readers. The novelty of the contributions is notable, as they push the boundaries of current research in [specific field]. However, reproducibility could be improved by providing detailed explanations of the methodology and clearer references to figures and tables. The paper would benefit from a more structured approach to its presentation, making it easier for readers to follow the arguments and results.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field with its novel methodology and findings. However, improvements in clarity and structure are needed to enhance its readability and reproducibility, making it more accessible to a broader audience.\n\n# Correctness\n4/5 - The methodology and findings appear sound, but some minor inconsistencies were noted that could affect overall clarity.\n\n# Technical Novelty And Significance\n4/5 - The paper presents a novel approach that contributes meaningfully to the field, although there are areas where further innovation could be explored.\n\n# Empirical Novelty And Significance\n4/5 - The empirical results show promise and provide valuable insights; however, additional comparative analyses could strengthen the claims made."
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.6059536113587556,
    -1.8208091213667035,
    -1.8603816685494179,
    -1.7979364412425958,
    -1.8567760923117822,
    -1.6502935341024147,
    -1.933338412928411,
    -1.8963021871924386,
    -1.9189258591898093,
    -1.8091041656074671,
    -1.5298120013024605,
    -1.5040180659100024,
    -1.869284437437009,
    -1.6692985485772673,
    -1.825582951966472,
    -1.5630471655789047,
    -1.7671247970294413,
    -1.7579481857156778,
    -1.9743751205334414,
    -1.7175983149683418,
    -1.960642587564077,
    -1.8192150408571848,
    -1.8354919429379044,
    -1.725971405050458,
    -1.9055167074073083,
    -1.890257244158609,
    -1.7090695605513724,
    -1.7323821453445092,
    -1.6179072945420028
  ],
  "logp_cond": [
    [
      0.0,
      -2.4171232495450345,
      -2.4347942607763366,
      -2.4312271207715073,
      -2.446377506778766,
      -2.511371303920813,
      -2.4684364432797516,
      -2.463638447990916,
      -2.454291297313215,
      -2.4671206180447087,
      -2.439644840818649,
      -2.521904335533786,
      -2.445177969305819,
      -2.444707035842894,
      -2.4271466698378834,
      -2.46659651881256,
      -2.45254446410685,
      -2.4333352791359815,
      -2.4731268102440307,
      -2.44960303642696,
      -2.440521434056464,
      -2.454974992323176,
      -2.4556529451188287,
      -2.444756013823774,
      -2.489051666717439,
      -2.4578510902398802,
      -2.4698166206937433,
      -2.440476643815185,
      -2.50664202726284
    ],
    [
      -1.436144142877548,
      0.0,
      -1.3593417265367258,
      -1.2628009058560465,
      -1.3580486053140637,
      -1.4714858716646222,
      -1.4395285144438463,
      -1.4083756469299122,
      -1.3302237373258434,
      -1.4257286911654805,
      -1.367175690772313,
      -1.57295173956459,
      -1.327071405663105,
      -1.3224653745392554,
      -1.4429637720873685,
      -1.3777139552245405,
      -1.389045897847558,
      -1.3314459090066246,
      -1.4380976782916104,
      -1.4763943849625518,
      -1.423961968112711,
      -1.477281521510097,
      -1.4638013149328986,
      -1.3468821877015593,
      -1.4976915522047411,
      -1.4419920924214038,
      -1.5077515478994108,
      -1.40271949422451,
      -1.6094983484039274
    ],
    [
      -1.4641976358146593,
      -1.3074729965373075,
      0.0,
      -1.3851223848236627,
      -1.2961074274872235,
      -1.5169926419547286,
      -1.408980438963177,
      -1.4373706336753056,
      -1.3314472595486775,
      -1.3847336605797245,
      -1.3514517641877113,
      -1.5769056440900375,
      -1.3435411055072013,
      -1.3361940985576606,
      -1.431211737820335,
      -1.365041935794536,
      -1.390361514506698,
      -1.4561571672182267,
      -1.425131149183918,
      -1.4349536162703096,
      -1.426619820046678,
      -1.5270834013563457,
      -1.4491671960490462,
      -1.4277686814937829,
      -1.3941358002165156,
      -1.482671011100179,
      -1.518175491595988,
      -1.403634473083561,
      -1.605889001439326
    ],
    [
      -1.3882898673710045,
      -1.20769497372698,
      -1.3033254846561637,
      0.0,
      -1.3323294195531752,
      -1.4461810115342086,
      -1.3933250913941788,
      -1.3716878192096,
      -1.320184646356862,
      -1.3635926473015691,
      -1.3300012587520542,
      -1.512893723740879,
      -1.2966837595081688,
      -1.2825587126611255,
      -1.3793277760936142,
      -1.3065234549495184,
      -1.3630784885470169,
      -1.311534290149876,
      -1.3571143835131898,
      -1.38116789790022,
      -1.3579813669883525,
      -1.3808024515986663,
      -1.3933591134497227,
      -1.3465618661754388,
      -1.4342167218053496,
      -1.3978375771769884,
      -1.4460604963082218,
      -1.3526581357067258,
      -1.4993723950554978
    ],
    [
      -1.468389028788886,
      -1.3634373130439155,
      -1.3133319146053282,
      -1.430653818217403,
      0.0,
      -1.5244025037674784,
      -1.3950577926335899,
      -1.394751782628491,
      -1.4144358331243592,
      -1.4543207288436466,
      -1.4266999125333142,
      -1.5742908457130957,
      -1.4355326337109555,
      -1.381283585809355,
      -1.4361570069058534,
      -1.4183638425611964,
      -1.3847596784830172,
      -1.4415618706161264,
      -1.4135823918320016,
      -1.4762606535470244,
      -1.4098235611232217,
      -1.4930781333200829,
      -1.475656531573328,
      -1.3923380687503621,
      -1.4730771612439346,
      -1.4211063327629627,
      -1.5229894235251393,
      -1.4241571229793748,
      -1.6388248833070096
    ],
    [
      -1.3902420592592817,
      -1.2681573676316236,
      -1.2539766495922269,
      -1.2450365995634864,
      -1.2632536117029487,
      0.0,
      -1.278541929401668,
      -1.2799002693422992,
      -1.2670602142269072,
      -1.2937040592302445,
      -1.301885270244588,
      -1.3995925383397523,
      -1.256888247217455,
      -1.3213900437044603,
      -1.35196419059278,
      -1.3205413649469502,
      -1.2503815468295334,
      -1.2711339649068867,
      -1.2698972394880697,
      -1.3521865479448896,
      -1.3328108153910405,
      -1.3918437254450764,
      -1.3581178171576345,
      -1.3258494778216727,
      -1.3647162271518127,
      -1.364846882236275,
      -1.3519675689983923,
      -1.370619074611313,
      -1.4367946939808354
    ],
    [
      -1.5607285232597743,
      -1.475440822257384,
      -1.4878382940676225,
      -1.5357152415004904,
      -1.468556589391136,
      -1.5841353160345335,
      0.0,
      -1.540434021393535,
      -1.5591210025504376,
      -1.5624268713302905,
      -1.5547078168748916,
      -1.6274110044824568,
      -1.5384298755692414,
      -1.5118029052271666,
      -1.5236806354321344,
      -1.5191066815662535,
      -1.5105456464089773,
      -1.5566560305839185,
      -1.538186101865659,
      -1.5610619547554063,
      -1.477442164752456,
      -1.6349410207170207,
      -1.585299529094743,
      -1.5436190928255145,
      -1.617195063050854,
      -1.5756956286015538,
      -1.6120875221395992,
      -1.5447946602046496,
      -1.6812041942873253
    ],
    [
      -1.5878211027620852,
      -1.406883873202642,
      -1.4691916589728726,
      -1.465613018647413,
      -1.4487811190549347,
      -1.5515330188673486,
      -1.5528269401018462,
      0.0,
      -1.4015136634849723,
      -1.47812979130546,
      -1.5433726181840668,
      -1.6335749231154475,
      -1.3953161509897427,
      -1.421823250287589,
      -1.4698061955834587,
      -1.4010901258295314,
      -1.506741565430521,
      -1.423058239073223,
      -1.4949379051307052,
      -1.5279829021443379,
      -1.4976707666907878,
      -1.5831553772388511,
      -1.5495630909286595,
      -1.386764124565442,
      -1.5652910143948113,
      -1.4883073220325773,
      -1.5776499870289817,
      -1.491696491960026,
      -1.6666797998932443
    ],
    [
      -1.5567332282925712,
      -1.4153495474149387,
      -1.4425263281506169,
      -1.4762517021287493,
      -1.4699612681845036,
      -1.585664925172557,
      -1.5397909356631132,
      -1.429774838019335,
      0.0,
      -1.5213987324390876,
      -1.5512586653392555,
      -1.6700157509371811,
      -1.4145551581501479,
      -1.4487459684991972,
      -1.5063290000716545,
      -1.4339802778422794,
      -1.5051308232542064,
      -1.4752800622565114,
      -1.5029961390928017,
      -1.55037174572225,
      -1.5103841148287795,
      -1.5539851283929464,
      -1.5145560491480996,
      -1.3845586250164776,
      -1.556513623327953,
      -1.540815659752284,
      -1.5858961251479642,
      -1.4695248086072141,
      -1.654655847893074
    ],
    [
      -1.4785766279930994,
      -1.3981080112816968,
      -1.390401290206466,
      -1.4354637270923853,
      -1.4111743802611445,
      -1.5161268812261255,
      -1.4772565055502873,
      -1.406115088210567,
      -1.412531325854138,
      0.0,
      -1.4332566568332998,
      -1.5492082118351602,
      -1.4071826528914737,
      -1.4009181157814183,
      -1.465759546175642,
      -1.3879011202411224,
      -1.4181298290229833,
      -1.4132153953475737,
      -1.4731565588736508,
      -1.5019915701345992,
      -1.467459286829187,
      -1.5046389056651217,
      -1.4166462804836129,
      -1.4427586579429896,
      -1.49362121919256,
      -1.459227801534857,
      -1.5199858080630737,
      -1.4272110348435352,
      -1.567098869103502
    ],
    [
      -1.1671929590865526,
      -1.1088928821580706,
      -1.135660458478939,
      -1.1318086987683948,
      -1.1630460854960687,
      -1.244599601064135,
      -1.2129671264851323,
      -1.2030188458663962,
      -1.2147575244333457,
      -1.1936274961975963,
      0.0,
      -1.3209605417157473,
      -1.16271352146078,
      -1.1766333449889477,
      -1.2042857694041027,
      -1.1806026561151297,
      -1.158700199909045,
      -1.136962759775474,
      -1.1884278751656336,
      -1.2350069579687633,
      -1.1641719107328876,
      -1.2460081628343231,
      -1.190952347818694,
      -1.223813693214263,
      -1.2506190088813436,
      -1.1997014259114693,
      -1.287949551809104,
      -1.2580715470031947,
      -1.3238723567132877
    ],
    [
      -1.3014418320851904,
      -1.2590972875368325,
      -1.2979715087081345,
      -1.2574238743232597,
      -1.290537712713725,
      -1.2791705826558086,
      -1.2690634596343047,
      -1.2793687734371866,
      -1.285982029116862,
      -1.257691766049345,
      -1.275542416790771,
      0.0,
      -1.2788651151179824,
      -1.2814322256906356,
      -1.2586077992005607,
      -1.2817024669681283,
      -1.2744937712157258,
      -1.2312177903698334,
      -1.288490884305803,
      -1.2777945736047567,
      -1.2742428154496155,
      -1.2432103123062892,
      -1.2429897820878801,
      -1.2717120902733885,
      -1.2638407590460805,
      -1.2544348108577887,
      -1.2504672540501705,
      -1.2645891315002729,
      -1.2460570699887592
    ],
    [
      -1.4886119879979267,
      -1.2852120762975168,
      -1.3951771983679913,
      -1.3373980096421965,
      -1.404702208212451,
      -1.4920984930313792,
      -1.4531976151887338,
      -1.3234178323670511,
      -1.3471631729480726,
      -1.421231869970882,
      -1.390829641819697,
      -1.5754547373313208,
      0.0,
      -1.3697435196744887,
      -1.4151993327886618,
      -1.3178743076999169,
      -1.4202890443750174,
      -1.371710743062071,
      -1.4343072604986364,
      -1.4819079547239131,
      -1.4423692273848718,
      -1.4866015973562592,
      -1.4873529920817463,
      -1.3697279315130495,
      -1.531433898417799,
      -1.4844598666414552,
      -1.5019573438888427,
      -1.4501947484209716,
      -1.5996811701011444
    ],
    [
      -1.2598642546745564,
      -1.0244289005863176,
      -1.180641558419904,
      -1.1151304614661306,
      -1.1688671929829104,
      -1.3292047964614666,
      -1.2750633515267131,
      -1.1563065100647036,
      -1.1646902239326236,
      -1.2341735084055652,
      -1.191012203794112,
      -1.3895770029564747,
      -1.1703128904206346,
      0.0,
      -1.2023510160901225,
      -1.1712082097022516,
      -1.2056266984137807,
      -1.153270251482879,
      -1.2490992316751313,
      -1.1945206313367103,
      -1.2233586371266603,
      -1.2775834883225046,
      -1.2022992581433058,
      -1.2078637062611894,
      -1.266082316924059,
      -1.248064623118603,
      -1.3369956001753631,
      -1.2235340781594835,
      -1.3679866450518174
    ],
    [
      -1.3536146399381375,
      -1.3727022583331467,
      -1.3741918669647692,
      -1.3966856790694533,
      -1.3649994035008188,
      -1.5009297697224524,
      -1.4234462669314174,
      -1.3896003085279025,
      -1.3739042181914993,
      -1.4275870250961316,
      -1.3935495478737008,
      -1.5117450921989533,
      -1.391346529274953,
      -1.3662307618342333,
      0.0,
      -1.4053052382290745,
      -1.3734291166630563,
      -1.3891357541636749,
      -1.3773089492680866,
      -1.3769394040217482,
      -1.3653820628065387,
      -1.3456748270467522,
      -1.353146003661087,
      -1.4112674390102973,
      -1.4078386983512146,
      -1.3932579213537917,
      -1.459523280923066,
      -1.4169312280000954,
      -1.5013281669994987
    ],
    [
      -1.1505044100716846,
      -1.001118704252681,
      -1.0699732859159696,
      -1.045001029349054,
      -1.0880480335241496,
      -1.2152185944305354,
      -1.1539168221517613,
      -1.01942891367405,
      -1.0381437989431133,
      -1.11335253270854,
      -1.1375679918310122,
      -1.2901363502776961,
      -1.0127763874126674,
      -1.0429026097986565,
      -1.1124506480936793,
      0.0,
      -1.112841522343521,
      -1.08660498730816,
      -1.1244922814527165,
      -1.1710472995847399,
      -1.154588148915187,
      -1.1918576423313412,
      -1.1406988781796656,
      -1.1185339998962929,
      -1.2059071515707913,
      -1.1597427219027796,
      -1.2500455552129062,
      -1.0895170783470653,
      -1.3033881019106885
    ],
    [
      -1.427022230280902,
      -1.3131263263648927,
      -1.3058697635173835,
      -1.3445299628563991,
      -1.3021715969848429,
      -1.422213117218505,
      -1.4113368946590996,
      -1.3328747091214312,
      -1.3722262253326172,
      -1.3924880390616416,
      -1.3736910145488586,
      -1.5310820140325216,
      -1.3309439177394928,
      -1.3546891877052705,
      -1.374891034385337,
      -1.361076277393823,
      0.0,
      -1.3420649348803373,
      -1.3960876878234614,
      -1.3881204858312985,
      -1.3364564795098293,
      -1.436501695460666,
      -1.405812701201808,
      -1.4140176865103136,
      -1.4176069218241683,
      -1.3929790385162804,
      -1.4444852920976812,
      -1.4157869760206072,
      -1.557795167247013
    ],
    [
      -1.4074032679719828,
      -1.1795033661876937,
      -1.3329906756533656,
      -1.2285881146294801,
      -1.2935500739698882,
      -1.391019791772798,
      -1.339518811268836,
      -1.2639263544063417,
      -1.2788605890631757,
      -1.2999075266452043,
      -1.2877471691595788,
      -1.4488492053332935,
      -1.2662024505512501,
      -1.2446156739160799,
      -1.3612810379229399,
      -1.3116169081693856,
      -1.2677014111591836,
      0.0,
      -1.3448266138895317,
      -1.3713037651975093,
      -1.3516901201003302,
      -1.3732083640194632,
      -1.2988490436398499,
      -1.2926689780040739,
      -1.4309063224116585,
      -1.3357671488955618,
      -1.4266336331409628,
      -1.3812896284926086,
      -1.489230398145148
    ],
    [
      -1.5747895452832996,
      -1.4839571462715586,
      -1.491426487977197,
      -1.5464258144415632,
      -1.525703944672037,
      -1.5904578821471091,
      -1.566370112604656,
      -1.4924437482356876,
      -1.5179780088300308,
      -1.5552549539139742,
      -1.5471245253593036,
      -1.6962724915084884,
      -1.5315458579287737,
      -1.5483354531764428,
      -1.534633659981525,
      -1.546620120667561,
      -1.5053198270502548,
      -1.5188835477960714,
      0.0,
      -1.529013025113428,
      -1.5422630395177208,
      -1.5681944905724985,
      -1.538039983789133,
      -1.538510904663988,
      -1.5758878640281813,
      -1.5645321197762498,
      -1.5968126046664468,
      -1.5753153395403734,
      -1.6831893158579623
    ],
    [
      -1.3727392836792487,
      -1.3121498167035202,
      -1.2741816932565508,
      -1.3247113110604438,
      -1.2926674647377985,
      -1.3660239971131622,
      -1.3311080427978124,
      -1.3172159399351324,
      -1.315252076080404,
      -1.3243038432610146,
      -1.3522962318444047,
      -1.425914933598717,
      -1.3293195286038122,
      -1.2933672803163228,
      -1.2786668396995418,
      -1.293150352675302,
      -1.312218669565496,
      -1.3261964758151474,
      -1.2790077776415707,
      0.0,
      -1.3510531648792357,
      -1.315247091058891,
      -1.3107396347961604,
      -1.3338866265373923,
      -1.333446613786464,
      -1.3226255583623845,
      -1.3888496082033437,
      -1.3546263137007784,
      -1.4289296987066324
    ],
    [
      -1.5376629829594883,
      -1.4606072675403785,
      -1.4701814495609051,
      -1.48428962974826,
      -1.4698951657654324,
      -1.6203461203718184,
      -1.5201507580673645,
      -1.4933384309191473,
      -1.4805358459392337,
      -1.5556093505187658,
      -1.529580529782129,
      -1.722843557272949,
      -1.502156471787407,
      -1.4897238439515066,
      -1.5103493906287309,
      -1.5188421843461437,
      -1.506457826203115,
      -1.514276562170599,
      -1.5199516578061352,
      -1.550657604419406,
      0.0,
      -1.6051285478677235,
      -1.5722684666945737,
      -1.4899363458770762,
      -1.6202886299945833,
      -1.50188732739379,
      -1.6271593185350182,
      -1.4990549169075242,
      -1.7064100446727537
    ],
    [
      -1.4519184507749323,
      -1.405467405033659,
      -1.4147237197570506,
      -1.3799968086822496,
      -1.4014161187453973,
      -1.4570173688879962,
      -1.4511075120375188,
      -1.3960040053718,
      -1.3873726735194647,
      -1.4041558527697355,
      -1.4251809117124543,
      -1.5002790015857892,
      -1.403716515114837,
      -1.403536039823925,
      -1.3667936182885028,
      -1.3990610259689151,
      -1.3653780579490251,
      -1.339340360671239,
      -1.3870734162914382,
      -1.3805834489599789,
      -1.435645738305012,
      0.0,
      -1.3534314942646313,
      -1.4148326449626516,
      -1.4753373393893363,
      -1.2956679749180258,
      -1.4733989029662267,
      -1.4336714224199645,
      -1.477876879773443
    ],
    [
      -1.462991549081943,
      -1.3704754461762905,
      -1.3588621311770572,
      -1.3909285190942646,
      -1.3674213646592763,
      -1.4803869744581606,
      -1.4368492269079784,
      -1.4155816558419712,
      -1.3294201141488255,
      -1.3113865305349235,
      -1.3162631470754147,
      -1.519828815416448,
      -1.3935806336854037,
      -1.3704864214525503,
      -1.395266083043972,
      -1.3688915918342175,
      -1.338241137695949,
      -1.3261594173383007,
      -1.3362304321444265,
      -1.430373325166744,
      -1.3791799885742806,
      -1.3880489110263385,
      0.0,
      -1.3512909463715754,
      -1.4237054935933282,
      -1.356560382619621,
      -1.4695337546113822,
      -1.4081045880156544,
      -1.5296190557910536
    ],
    [
      -1.3693509957787784,
      -1.2419246572367157,
      -1.3233151305986266,
      -1.2849941261552857,
      -1.2981629579493281,
      -1.3855117961134646,
      -1.3510187550663344,
      -1.2213884133694217,
      -1.2324430603719099,
      -1.3276201360746895,
      -1.365529760232527,
      -1.4689195265639299,
      -1.2523993833412592,
      -1.309683186157169,
      -1.3556652555930282,
      -1.2803266715729702,
      -1.3531550712331766,
      -1.3087286301854786,
      -1.3350252949701436,
      -1.388133697111524,
      -1.3195614362222052,
      -1.4158341189831547,
      -1.384721057183213,
      0.0,
      -1.377525014029789,
      -1.4207683469330306,
      -1.4030637729462403,
      -1.2526308862256414,
      -1.4911788130859613
    ],
    [
      -1.5590452809185538,
      -1.5121316746807483,
      -1.4507505058803754,
      -1.5141783052101212,
      -1.4833901918530326,
      -1.5758126468124154,
      -1.5626150599201232,
      -1.5436425401173255,
      -1.5069922394904425,
      -1.519081847186904,
      -1.5255882560696397,
      -1.6338364783913897,
      -1.5416504468069134,
      -1.4965576518451214,
      -1.5131742723650172,
      -1.514509619203094,
      -1.4602330011010018,
      -1.5693397495376002,
      -1.5055609827661585,
      -1.5169616618175927,
      -1.5117399058889722,
      -1.6213299346190528,
      -1.5263692842202505,
      -1.5311770123147106,
      0.0,
      -1.5511313597851386,
      -1.562981307763668,
      -1.5366839357827276,
      -1.6161528383940378
    ],
    [
      -1.5337003109746161,
      -1.4389619619780905,
      -1.4268479590084462,
      -1.436965415561369,
      -1.39085614790548,
      -1.5220214016571771,
      -1.4824447056548864,
      -1.4324494380137094,
      -1.448324013544808,
      -1.4694572697255313,
      -1.4542336775725295,
      -1.5687119763077724,
      -1.4759034965739943,
      -1.4448284253814265,
      -1.4376133491670604,
      -1.4591414389174602,
      -1.438113849247944,
      -1.4184711068933333,
      -1.461729070553011,
      -1.4494266981345743,
      -1.4497907058858532,
      -1.4301153871010148,
      -1.413057736896789,
      -1.486590697181524,
      -1.514631711180406,
      0.0,
      -1.5698174837319423,
      -1.48785718056138,
      -1.5654332856312865
    ],
    [
      -1.354294663921137,
      -1.323298700306176,
      -1.2965761291261417,
      -1.3429182895747807,
      -1.3076509761918136,
      -1.3560290012800487,
      -1.3491879906240876,
      -1.3190015364647902,
      -1.291423407461702,
      -1.3276662126315797,
      -1.3241841036807134,
      -1.3566069579065396,
      -1.2985344281779354,
      -1.332864290159061,
      -1.3244985298967056,
      -1.3500583711973435,
      -1.3148168688450406,
      -1.3340528802162719,
      -1.2966915207748084,
      -1.3329544424828754,
      -1.278804981282062,
      -1.3745042732578403,
      -1.321811675773457,
      -1.3167762302945576,
      -1.3126857614621472,
      -1.390451515161328,
      0.0,
      -1.3163386001068254,
      -1.4106136267881526
    ],
    [
      -1.363526327229353,
      -1.3109531036555702,
      -1.3204606083647565,
      -1.3227251066943202,
      -1.35390419957666,
      -1.4322657119633748,
      -1.33437767818917,
      -1.3656576740756516,
      -1.298381162203685,
      -1.3493066579296547,
      -1.3927842296464226,
      -1.49044077908969,
      -1.3099306052782254,
      -1.2936266949230708,
      -1.4009753163307708,
      -1.2816334521576804,
      -1.4027541101092953,
      -1.4072186701458194,
      -1.4015497098948024,
      -1.4064292047140021,
      -1.333515043431828,
      -1.4604549925901655,
      -1.4365079900718962,
      -1.2554207839347855,
      -1.4360321898082187,
      -1.438046767942382,
      -1.4230704214785246,
      0.0,
      -1.484655741232527
    ],
    [
      -1.3223526510849746,
      -1.3057515021653745,
      -1.2945042764430643,
      -1.2882413932239518,
      -1.2895390324273341,
      -1.2741841728179566,
      -1.3245326298309066,
      -1.279627636938852,
      -1.2834040639280795,
      -1.2714070290710953,
      -1.2655055755727573,
      -1.2634187254893383,
      -1.2768425169503206,
      -1.267966296004176,
      -1.2861363816833762,
      -1.2734153311927843,
      -1.269588613542834,
      -1.2498346869044827,
      -1.293877821486018,
      -1.3252565663562799,
      -1.2799223845783556,
      -1.2296491610482307,
      -1.2542342853637751,
      -1.2863318615816997,
      -1.261633879688365,
      -1.2678204357187997,
      -1.2844573995228554,
      -1.2849952037227566,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.18883036181372104,
      0.17115935058241893,
      0.17472649058724832,
      0.1595761045799895,
      0.0945823074379426,
      0.13751716807900394,
      0.14231516336783967,
      0.15166231404554065,
      0.13883299331404686,
      0.16630877054010673,
      0.0840492758249698,
      0.1607756420529367,
      0.1612465755158614,
      0.17880694152087218,
      0.13935709254619555,
      0.1534091472519057,
      0.17261833222277412,
      0.1328268011147249,
      0.15635057493179572,
      0.16543217730229154,
      0.15097861903557952,
      0.15030066623992688,
      0.16119759753498153,
      0.11690194464131665,
      0.14810252111887534,
      0.13613699066501228,
      0.16547696754357055,
      0.09931158409591578
    ],
    [
      0.3846649784891554,
      0.0,
      0.4614673948299777,
      0.5580082155106569,
      0.4627605160526398,
      0.3493232497020813,
      0.38128060692285715,
      0.41243347443679124,
      0.49058538404086005,
      0.39508043020122297,
      0.45363343059439054,
      0.24785738180211347,
      0.4937377157035985,
      0.4983437468274481,
      0.377845349279335,
      0.44309516614216293,
      0.43176322351914553,
      0.48936321236007885,
      0.3827114430750931,
      0.34441473640415166,
      0.39684715325399256,
      0.3435275998566065,
      0.3570078064338049,
      0.47392693366514416,
      0.3231175691619623,
      0.37881702894529967,
      0.3130575734672927,
      0.41808962714219344,
      0.21131077296277612
    ],
    [
      0.39618403273475855,
      0.5529086720121104,
      0.0,
      0.4752592837257552,
      0.5642742410621944,
      0.3433890265946893,
      0.45140122958624085,
      0.4230110348741123,
      0.5289344090007404,
      0.4756480079696934,
      0.5089299043617066,
      0.28347602445938036,
      0.5168405630422166,
      0.5241875699917573,
      0.4291699307290828,
      0.49533973275488186,
      0.4700201540427198,
      0.40422450133119114,
      0.43525051936549985,
      0.4254280522791083,
      0.4337618485027399,
      0.3332982671930722,
      0.4112144725003717,
      0.432612987055635,
      0.46624586833290227,
      0.3777106574492388,
      0.3422061769534299,
      0.45674719546585685,
      0.254492667110092
    ],
    [
      0.4096465738715913,
      0.5902414675156158,
      0.49461095658643206,
      0.0,
      0.4656070216894206,
      0.35175542970838714,
      0.40461134984841696,
      0.42624862203299574,
      0.4777517948857337,
      0.4343437939410266,
      0.46793518249054156,
      0.2850427175017167,
      0.5012526817344269,
      0.5153777285814702,
      0.41860866514898154,
      0.4914129862930774,
      0.4348579526955789,
      0.48640215109271967,
      0.44082205772940597,
      0.4167685433423758,
      0.4399550742542433,
      0.41713398964392945,
      0.40457732779287303,
      0.45137457506715695,
      0.3637197194372461,
      0.40009886406560735,
      0.351875944934374,
      0.44527830553587,
      0.298564046187098
    ],
    [
      0.3883870635228961,
      0.49333877926786673,
      0.543444177706454,
      0.4261222740943793,
      0.0,
      0.33237358854430377,
      0.46171829967819233,
      0.4620243096832912,
      0.44234025918742303,
      0.4024553634681356,
      0.430076179778468,
      0.28248524659868646,
      0.4212434586008267,
      0.47549250650242714,
      0.42061908540592885,
      0.4384122497505858,
      0.472016413828765,
      0.4152142216956558,
      0.4431937004797806,
      0.3805154387647578,
      0.4469525311885605,
      0.3636979589916993,
      0.3811195607384541,
      0.46443802356142005,
      0.3836989310678476,
      0.4356697595488195,
      0.33378666878664287,
      0.4326189693324074,
      0.2179512090047726
    ],
    [
      0.26005147484313307,
      0.3821361664707912,
      0.3963168845101879,
      0.4052569345389283,
      0.38703992239946605,
      0.0,
      0.37175160470074675,
      0.3703932647601156,
      0.38323331987550757,
      0.3565894748721703,
      0.34840826385782675,
      0.25070099576266247,
      0.3934052868849598,
      0.3289034903979544,
      0.2983293435096348,
      0.32975216915546457,
      0.39991198727288135,
      0.37915956919552807,
      0.3803962946143451,
      0.2981069861575252,
      0.31748271871137423,
      0.25844980865733835,
      0.2921757169447803,
      0.324444056280742,
      0.285577306950602,
      0.2854466518661398,
      0.29832596510402243,
      0.2796744594911018,
      0.21349884012157938
    ],
    [
      0.37260988966863673,
      0.457897590671027,
      0.44550011886078855,
      0.3976231714279206,
      0.46478182353727493,
      0.3492030968938775,
      0.0,
      0.392904391534876,
      0.3742174103779734,
      0.37091154159812056,
      0.37863059605351945,
      0.30592740844595423,
      0.39490853735916964,
      0.42153550770124437,
      0.40965777749627663,
      0.4142317313621575,
      0.42279276651943376,
      0.3766823823444925,
      0.39515231106275195,
      0.3722764581730047,
      0.455896248175955,
      0.2983973922113903,
      0.3480388838336681,
      0.38971932010289656,
      0.316143349877557,
      0.35764278432685725,
      0.32125089078881186,
      0.3885437527237614,
      0.25213421864108576
    ],
    [
      0.3084810844303534,
      0.4894183139897965,
      0.42711052821956597,
      0.4306891685450256,
      0.4475210681375039,
      0.34476916832508997,
      0.3434752470905924,
      0.0,
      0.4947885237074663,
      0.4181723958869785,
      0.3529295690083718,
      0.2627272640769911,
      0.5009860362026959,
      0.47447893690484966,
      0.4264959916089799,
      0.4952120613629072,
      0.38956062176191764,
      0.47324394811921566,
      0.40136428206173336,
      0.36831928504810074,
      0.3986314205016508,
      0.31314680995358746,
      0.3467390962637791,
      0.5095380626269965,
      0.33101117279762726,
      0.4079948651598613,
      0.31865220016345686,
      0.40460569523241263,
      0.22962238729919426
    ],
    [
      0.3621926308972381,
      0.5035763117748706,
      0.4763995310391924,
      0.44267415706105995,
      0.4489645910053057,
      0.3332609340172523,
      0.3791349235266961,
      0.4891510211704744,
      0.0,
      0.39752712675072166,
      0.3676671938505538,
      0.24891010825262816,
      0.5043707010396614,
      0.47017989069061206,
      0.4125968591181548,
      0.48494558134752985,
      0.41379503593560285,
      0.44364579693329786,
      0.41592972009700757,
      0.3685541134675594,
      0.40854174436102975,
      0.36494073079686284,
      0.4043698100417097,
      0.5343672341733317,
      0.3624122358618562,
      0.37811019943752533,
      0.3330297340418451,
      0.44940105058259516,
      0.26427001129673533
    ],
    [
      0.3305275376143677,
      0.4109961543257703,
      0.41870287540100115,
      0.3736404385150818,
      0.3979297853463226,
      0.29297728438134163,
      0.3318476600571798,
      0.40298907739690004,
      0.396572839753329,
      0.0,
      0.3758475087741673,
      0.2598959537723069,
      0.4019215127159934,
      0.4081860498260488,
      0.34334461943182504,
      0.4212030453663447,
      0.3909743365844838,
      0.39588877025989344,
      0.3359476067338163,
      0.30711259547286796,
      0.3416448787782802,
      0.3044652599423454,
      0.39245788512385427,
      0.3663455076644775,
      0.31548294641490715,
      0.34987636407261014,
      0.2891183575443934,
      0.38189313076393194,
      0.24200529650396518
    ],
    [
      0.3626190422159079,
      0.42091911914438995,
      0.3941515428235216,
      0.3980033025340657,
      0.36676591580639184,
      0.2852124002383256,
      0.31684487481732826,
      0.32679315543606435,
      0.3150544768691148,
      0.3361845051048642,
      0.0,
      0.20885145958671325,
      0.36709847984168054,
      0.35317865631351286,
      0.32552623189835783,
      0.3492093451873308,
      0.3711118013934156,
      0.39284924152698664,
      0.3413841261368269,
      0.2948050433336973,
      0.3656400905695729,
      0.2838038384681374,
      0.33885965348376645,
      0.3059983080881976,
      0.27919299242111695,
      0.3301105753909912,
      0.24186244949335656,
      0.27174045429926585,
      0.2059396445891728
    ],
    [
      0.20257623382481205,
      0.24492077837316995,
      0.20604655720186793,
      0.24659419158674267,
      0.21348035319627745,
      0.2248474832541938,
      0.23495460627569775,
      0.22464929247281584,
      0.21803603679314043,
      0.24632629986065746,
      0.22847564911923146,
      0.0,
      0.22515295079202002,
      0.22258584021936678,
      0.24541026670944177,
      0.22231559894187414,
      0.2295242946942766,
      0.27280027554016906,
      0.21552718160419948,
      0.22622349230524574,
      0.22977525046038694,
      0.2608077536037132,
      0.2610282838221223,
      0.23230597563661393,
      0.2401773068639219,
      0.24958325505221368,
      0.25355081185983197,
      0.23942893440972957,
      0.2579609959212432
    ],
    [
      0.3806724494390823,
      0.5840723611394922,
      0.47410723906901775,
      0.5318864277948125,
      0.46458222922455805,
      0.3771859444056298,
      0.4160868222482752,
      0.5458666050699579,
      0.5221212644889364,
      0.4480525674661271,
      0.47845479561731197,
      0.2938297001056882,
      0.0,
      0.4995409177625203,
      0.45408510464834717,
      0.5514101297370921,
      0.44899539306199165,
      0.49757369437493804,
      0.43497717693837257,
      0.3873764827130959,
      0.4269152100521372,
      0.3826828400807498,
      0.3819314453552627,
      0.4995565059239595,
      0.3378505390192099,
      0.38482457079555377,
      0.3673270935481663,
      0.4190896890160374,
      0.2696032673358646
    ],
    [
      0.4094342939027109,
      0.6448696479909497,
      0.4886569901573634,
      0.5541680871111367,
      0.5004313555943569,
      0.34009375211580073,
      0.3942351970505542,
      0.5129920385125637,
      0.5046083246446438,
      0.43512504017170217,
      0.47828634478315535,
      0.27972154562079266,
      0.49898565815663276,
      0.0,
      0.4669475324871448,
      0.49809033887501575,
      0.46367185016348667,
      0.5160282970943884,
      0.4201993169021361,
      0.47477791724055707,
      0.4459399114506071,
      0.39171506025476277,
      0.4669992904339615,
      0.4614348423160779,
      0.4032162316532084,
      0.42123392545866434,
      0.3323029484019042,
      0.4457644704177839,
      0.3013119035254499
    ],
    [
      0.47196831202833445,
      0.45288069363332517,
      0.45139108500170266,
      0.4288972728970186,
      0.4605835484656531,
      0.3246531822440195,
      0.4021366850350545,
      0.4359826434385694,
      0.4516787337749726,
      0.39799592687034036,
      0.43203340409277113,
      0.31383785976751866,
      0.43423642269151896,
      0.4593521901322386,
      0.0,
      0.4202777137373974,
      0.4521538353034156,
      0.43644719780279706,
      0.44827400269838535,
      0.4486435479447237,
      0.4602008891599332,
      0.4799081249197197,
      0.47243694830538496,
      0.41431551295617464,
      0.41774425361525735,
      0.43232503061268024,
      0.36605967104340587,
      0.4086517239663765,
      0.32425478496697324
    ],
    [
      0.4125427555072201,
      0.5619284613262236,
      0.49307387966293503,
      0.5180461362298507,
      0.4749991320547551,
      0.3478285711483693,
      0.4091303434271434,
      0.5436182519048547,
      0.5249033666357914,
      0.4496946328703646,
      0.42547917374789246,
      0.27291081530120853,
      0.5502707781662373,
      0.5201445557802482,
      0.45059651748522533,
      0.0,
      0.4502056432353836,
      0.47644217827074464,
      0.43855488412618815,
      0.3919998659941648,
      0.40845901666371764,
      0.3711895232475635,
      0.422348287399239,
      0.4445131656826118,
      0.35714001400811335,
      0.40330444367612506,
      0.3130016103659985,
      0.4735300872318393,
      0.2596590636682161
    ],
    [
      0.34010256674853934,
      0.45399847066454857,
      0.46125503351205777,
      0.4225948341730421,
      0.4649532000445984,
      0.3449116798109362,
      0.35578790237034164,
      0.43425008790801,
      0.3948985716968241,
      0.3746367579677996,
      0.3934337824805827,
      0.2360427829969196,
      0.4361808792899484,
      0.4124356093241708,
      0.39223376264410414,
      0.4060485196356183,
      0.0,
      0.42505986214910396,
      0.3710371092059799,
      0.3790043111981427,
      0.430668317519612,
      0.3306231015687753,
      0.36131209582763324,
      0.3531071105191277,
      0.349517875205273,
      0.3741457585131609,
      0.3226395049317601,
      0.35133782100883404,
      0.2093296297824283
    ],
    [
      0.350544917743695,
      0.5784448195279841,
      0.4249575100623122,
      0.5293600710861976,
      0.4643981117457896,
      0.3669283939428798,
      0.4184293744468417,
      0.4940218313093361,
      0.4790875966525021,
      0.45804065907047353,
      0.470201016556099,
      0.30909898038238426,
      0.49174573516442766,
      0.5133325117995979,
      0.39666714779273793,
      0.4463312775462922,
      0.49024677455649424,
      0.0,
      0.41312157182614606,
      0.3866444205181685,
      0.4062580656153476,
      0.38473982169621457,
      0.4590991420758279,
      0.4652792077116039,
      0.3270418633040193,
      0.422181036820116,
      0.331314552574715,
      0.3766585572230692,
      0.2687177875705298
    ],
    [
      0.3995855752501418,
      0.4904179742618828,
      0.48294863255624443,
      0.4279493060918782,
      0.44867117586140437,
      0.3839172383863323,
      0.40800500792878536,
      0.48193137229775385,
      0.4563971117034107,
      0.41912016661946727,
      0.4272505951741379,
      0.2781026290249531,
      0.4428292626046677,
      0.4260396673569986,
      0.43974146055191654,
      0.4277549998658805,
      0.46905529348318664,
      0.45549157273737007,
      0.0,
      0.4453620954200135,
      0.43211208101572063,
      0.40618062996094295,
      0.43633513674430846,
      0.43586421586945345,
      0.3984872565052602,
      0.4098430007571916,
      0.37756251586699463,
      0.39905978099306805,
      0.29118580467547917
    ],
    [
      0.3448590312890931,
      0.40544849826482166,
      0.44341662171179097,
      0.392887003907898,
      0.42493085023054333,
      0.35157431785517956,
      0.38649027217052945,
      0.4003823750332094,
      0.40234623888793775,
      0.39329447170732723,
      0.3653020831239371,
      0.2916833813696249,
      0.3882787863645296,
      0.42423103465201906,
      0.43893147526880005,
      0.4244479622930397,
      0.40537964540284577,
      0.3914018391531944,
      0.4385905373267711,
      0.0,
      0.36654515008910615,
      0.4023512239094509,
      0.4068586801721814,
      0.3837116884309495,
      0.3841517011818778,
      0.3949727566059573,
      0.3287487067649981,
      0.36297200126756346,
      0.28866861626170937
    ],
    [
      0.4229796046045886,
      0.5000353200236984,
      0.4904611380031718,
      0.476352957815817,
      0.4907474217986445,
      0.3402964671922586,
      0.4404918294967124,
      0.46730415664492964,
      0.48010674162484324,
      0.40503323704531113,
      0.43106205778194795,
      0.23779903029112792,
      0.45848611577667,
      0.47091874361257036,
      0.4502931969353461,
      0.44180040321793324,
      0.454184761360962,
      0.446366025393478,
      0.4406909297579418,
      0.4099849831446709,
      0.0,
      0.3555140396963534,
      0.3883741208695033,
      0.4707062416870007,
      0.3403539575694936,
      0.4587552601702869,
      0.3334832690290588,
      0.4615876706565527,
      0.25423254289132324
    ],
    [
      0.36729659008225246,
      0.4137476358235257,
      0.4044913211001342,
      0.43921823217493516,
      0.4177989221117875,
      0.3621976719691886,
      0.368107528819666,
      0.42321103548538486,
      0.4318423673377201,
      0.41505918808744924,
      0.39403412914473046,
      0.3189360392713956,
      0.4154985257423478,
      0.41567900103325983,
      0.452421422568682,
      0.42015401488826964,
      0.45383698290815966,
      0.4798746801859457,
      0.43214162456574656,
      0.4386315918972059,
      0.3835693025521727,
      0.0,
      0.4657835465925535,
      0.4043823958945332,
      0.34387770146784846,
      0.523547065939159,
      0.3458161378909581,
      0.3855436184372203,
      0.3413381610837418
    ],
    [
      0.37250039385596145,
      0.46501649676161394,
      0.4766298117608472,
      0.4445634238436398,
      0.4680705782786281,
      0.3551049684797438,
      0.39864271602992596,
      0.41991028709593325,
      0.5060718287890789,
      0.5241054124029809,
      0.5192287958624897,
      0.3156631275214563,
      0.44191130925250066,
      0.4650055214853541,
      0.44022585989393237,
      0.4666003511036869,
      0.4972508052419553,
      0.5093325255996037,
      0.4992615107934779,
      0.4051186177711603,
      0.4563119543636238,
      0.44744303191156587,
      0.0,
      0.484200996566329,
      0.4117864493445762,
      0.4789315603182833,
      0.36595818832652216,
      0.42738735492225,
      0.3058728871468508
    ],
    [
      0.3566204092716796,
      0.48404674781374224,
      0.4026562744518314,
      0.44097727889517224,
      0.42780844710112986,
      0.34045960893699334,
      0.3749526499841236,
      0.5045829916810363,
      0.4935283446785481,
      0.39835126897576845,
      0.36044164481793106,
      0.2570518784865281,
      0.47357202170919876,
      0.41628821889328904,
      0.3703061494574298,
      0.4456447334774878,
      0.3728163338172814,
      0.4172427748649794,
      0.39094611008031444,
      0.3378377079389341,
      0.40640996882825275,
      0.3101372860673033,
      0.34125034786724506,
      0.0,
      0.348446391020669,
      0.30520305811742743,
      0.3229076321042177,
      0.47334051882481654,
      0.23479259196449664
    ],
    [
      0.3464714264887545,
      0.39338503272656,
      0.4547662015269329,
      0.3913384021971871,
      0.4221265155542757,
      0.3297040605948929,
      0.3429016474871851,
      0.3618741672899828,
      0.3985244679168658,
      0.3864348602204042,
      0.3799284513376686,
      0.2716802290159186,
      0.36386626060039484,
      0.4089590555621869,
      0.39234243504229105,
      0.39100708820421426,
      0.4452837063063064,
      0.336176957869708,
      0.3999557246411498,
      0.38855504558971554,
      0.39377680151833605,
      0.28418677278825544,
      0.37914742318705774,
      0.3743396950925977,
      0.0,
      0.35438534762216967,
      0.3425353996436402,
      0.3688327716245807,
      0.2893638690132705
    ],
    [
      0.35655693318399284,
      0.4512952821805185,
      0.4634092851501628,
      0.45329182859724004,
      0.499401096253129,
      0.3682358425014318,
      0.40781253850372257,
      0.4578078061448996,
      0.44193323061380085,
      0.4207999744330777,
      0.43602356658607944,
      0.3215452678508366,
      0.41435374758461463,
      0.44542881877718243,
      0.45264389499154856,
      0.4311158052411488,
      0.452143394910665,
      0.47178613726527563,
      0.4285281736055979,
      0.4408305460240347,
      0.4404665382727557,
      0.46014185705759414,
      0.47719950726182003,
      0.40366654697708504,
      0.375625532978203,
      0.0,
      0.32043976042666666,
      0.4024000635972289,
      0.3248239585273225
    ],
    [
      0.3547748966302353,
      0.38577086024519636,
      0.41249343142523065,
      0.3661512709765917,
      0.4014185843595588,
      0.3530405592713237,
      0.3598815699272848,
      0.39006802408658214,
      0.41764615308967046,
      0.3814033479197927,
      0.384885456870659,
      0.35246260264483276,
      0.410535132373437,
      0.37620527039231133,
      0.3845710306546668,
      0.35901118935402887,
      0.3942526917063318,
      0.3750166803351005,
      0.41237803977656395,
      0.37611511806849696,
      0.4302645792693103,
      0.3345652872935321,
      0.38725788477791534,
      0.3922933302568148,
      0.39638379908922516,
      0.31861804539004446,
      0.0,
      0.39273096044454703,
      0.29845593376321977
    ],
    [
      0.3688558181151562,
      0.421429041688939,
      0.41192153697975264,
      0.409657038650189,
      0.3784779457678491,
      0.30011643338113436,
      0.3980044671553391,
      0.36672447126885754,
      0.43400098314082425,
      0.38307548741485453,
      0.3395979156980866,
      0.2419413662548191,
      0.4224515400662838,
      0.4387554504214384,
      0.3314068290137384,
      0.45074869318682875,
      0.3296280352352139,
      0.3251634751986898,
      0.33083243544970675,
      0.32595294063050706,
      0.39886710191268127,
      0.27192715275434365,
      0.29587415527261296,
      0.47696136140972367,
      0.2963499555362905,
      0.2943353774021271,
      0.3093117238659846,
      0.0,
      0.2477264041119822
    ],
    [
      0.2955546434570282,
      0.31215579237662827,
      0.32340301809893846,
      0.32966590131805096,
      0.32836826211466863,
      0.3437231217240462,
      0.2933746647110962,
      0.3382796576031508,
      0.3345032306139233,
      0.3465002654709075,
      0.35240171896924544,
      0.35448856905266446,
      0.3410647775916822,
      0.3499409985378268,
      0.3317709128586266,
      0.3444919633492185,
      0.34831868099916874,
      0.36807260763752003,
      0.32402947305598473,
      0.2926507281857229,
      0.33798490996364716,
      0.38825813349377203,
      0.36367300917822765,
      0.3315754329603031,
      0.3562734148536377,
      0.3500868588232031,
      0.33344989501914735,
      0.3329120908192462,
      0.0
    ]
  ],
  "row_avgs": [
    0.14852823126812015,
    0.40264541859938685,
    0.43614882251718495,
    0.4316384115574397,
    0.41397879388498027,
    0.33124710563955384,
    0.3801861197060887,
    0.3967744715888108,
    0.40938996352031826,
    0.3564212599477787,
    0.326775383107574,
    0.23482378394267772,
    0.43788065951543526,
    0.448258646874554,
    0.4249757570394879,
    0.43448268410065094,
    0.37791239066778115,
    0.42581759844006445,
    0.4213286271273159,
    0.3868877482391745,
    0.42208579371757843,
    0.40921558696628485,
    0.4417180987401418,
    0.3860221210759938,
    0.3711374934522323,
    0.42213239055348695,
    0.3785232760854466,
    0.3571462548922841,
    0.33739188331561726
  ],
  "col_avgs": [
    0.361759327132547,
    0.45479024470745644,
    0.42839103314256555,
    0.4244858250674115,
    0.42558816854907555,
    0.329702349394916,
    0.37103602812055486,
    0.41613287892647455,
    0.426691975886683,
    0.3931712570600638,
    0.3981031136097718,
    0.2738114157515283,
    0.42378430425375996,
    0.42485550232127745,
    0.3904141355054076,
    0.41605078371155946,
    0.4109700558283205,
    0.4153417467341014,
    0.3908580237437732,
    0.36744147285569667,
    0.39376110479668,
    0.346221854109113,
    0.37834893516213736,
    0.4086491725611406,
    0.3438545814350367,
    0.37235202226629255,
    0.3194897276288074,
    0.3862606329633468,
    0.259157102857946
  ],
  "combined_avgs": [
    0.2551437792003336,
    0.42871783165342164,
    0.4322699278298753,
    0.4280621183124256,
    0.4197834812170279,
    0.33047472751723495,
    0.37561107391332177,
    0.40645367525764264,
    0.41804096970350063,
    0.37479625850392123,
    0.3624392483586729,
    0.254317599847103,
    0.4308324818845976,
    0.4365570745979157,
    0.40769494627244773,
    0.4252667339061052,
    0.39444122324805087,
    0.42057967258708295,
    0.40609332543554455,
    0.37716461054743555,
    0.4079234492571292,
    0.37771872053769895,
    0.41003351695113954,
    0.3973356468185672,
    0.3574960374436345,
    0.3972422064098897,
    0.349006501857127,
    0.3717034439278154,
    0.29827449308678167
  ],
  "gppm": [
    623.4350299479494,
    629.2519228449239,
    644.7142933850257,
    645.2673540787921,
    646.524223906159,
    690.1975113774869,
    669.6006997592173,
    650.4700517432035,
    644.7752396166967,
    659.3221059131649,
    659.4992220049982,
    715.6922556141163,
    646.005913122991,
    649.042393179585,
    663.801107414014,
    655.3891685965608,
    654.0698713693208,
    653.1708544828318,
    661.9220755480258,
    675.3815643282257,
    660.1362917825636,
    683.8607979876907,
    670.5691260248797,
    654.1806678762408,
    683.6848098917883,
    672.1151209653734,
    696.2719183139653,
    662.9309076071197,
    724.8164023380331
  ],
  "gppm_normalized": [
    1.4673448353485556,
    1.5480654134627445,
    1.5894470474397209,
    1.593942995606356,
    1.5905781218994348,
    1.702067545386775,
    1.649665365364169,
    1.6102258990453329,
    1.589521361759976,
    1.6272759283305827,
    1.6222344073943298,
    1.7813188618185125,
    1.59107622286872,
    1.5977381805293351,
    1.6404175608456768,
    1.6164917092496975,
    1.6133355431560472,
    1.612012797897997,
    1.6315638676573758,
    1.6634705939639611,
    1.6294050398012367,
    1.6858171505813286,
    1.6520033383970714,
    1.6172980978746392,
    1.6864527650855508,
    1.6568440674338056,
    1.7130470898797339,
    1.635268805230784,
    1.784691434817637
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360,
    852,
    466,
    462,
    474,
    415,
    413,
    436,
    422,
    465,
    448,
    470,
    377,
    462,
    415,
    455,
    415,
    425,
    413,
    422,
    411,
    459,
    427,
    407,
    447,
    412,
    368,
    423,
    422,
    431,
    885,
    465,
    444,
    471,
    435,
    421,
    397,
    467,
    393,
    448,
    471,
    447,
    418,
    426,
    423,
    395,
    392,
    431,
    460,
    459,
    395,
    389,
    413,
    412,
    402,
    405,
    364,
    428,
    394,
    389,
    442,
    502,
    431,
    485,
    414,
    543,
    421,
    441,
    441,
    462,
    364,
    465,
    477,
    435,
    486,
    420,
    424,
    410,
    443,
    468,
    375,
    436,
    428,
    405,
    503,
    435,
    423,
    372,
    943,
    435,
    392,
    442,
    409,
    423,
    534,
    391,
    430,
    413,
    411,
    412,
    435,
    471,
    353,
    468,
    380,
    432,
    392,
    446,
    345,
    381,
    360,
    430,
    386,
    343,
    392,
    417,
    386,
    850,
    464,
    481,
    546,
    442,
    472,
    423,
    428,
    515,
    456,
    456,
    499,
    481,
    460,
    443,
    480,
    437,
    444,
    432,
    484,
    476,
    442,
    440,
    579,
    477,
    380,
    401,
    480,
    377,
    333,
    535,
    441,
    415,
    440,
    532,
    467,
    476,
    390,
    400,
    461,
    506,
    501,
    483,
    418,
    403,
    393,
    415,
    434,
    464,
    456,
    391,
    393,
    426,
    444,
    420,
    400,
    420,
    534,
    448,
    422,
    431,
    410,
    445,
    437,
    448,
    356,
    418,
    400,
    408,
    567,
    423,
    428,
    389,
    424,
    419,
    417,
    416,
    470,
    368,
    384,
    403,
    423,
    394,
    366,
    382,
    442,
    412,
    532,
    444,
    478,
    502,
    430,
    533,
    483,
    440,
    480,
    379,
    427,
    503,
    458,
    484,
    371,
    423,
    429,
    400,
    409,
    416,
    406,
    403,
    340,
    427,
    412,
    405,
    403,
    478,
    337,
    881,
    516,
    435,
    467,
    432,
    482,
    427,
    429,
    438,
    480,
    526,
    521,
    442,
    442,
    405,
    427,
    443,
    406,
    409,
    416,
    406,
    404,
    382,
    465,
    409,
    391,
    417,
    484,
    418
  ],
  "response_lengths": [
    4350,
    2977,
    2451,
    2724,
    2403,
    2841,
    2465,
    2404,
    2574,
    2789,
    2963,
    2973,
    2550,
    2520,
    2275,
    2422,
    2546,
    2343,
    2352,
    2367,
    2246,
    2315,
    2192,
    2709,
    2388,
    2147,
    2504,
    2708,
    2349
  ]
}