{
  "example_idx": 94,
  "reference": "Under review as a conference paper at ICLR 2023\n\nROBUST TRANSFER LEARNING BASED ON MINIMAX PRINCIPLE\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe similarity between target and source tasks is a crucial quantity for theoretical analyses and algorithm designs in transfer learning studies. However, this quantity is often difficult to be precisely captured. To address this issue, we make a boundedness assumption on the task similarity and then propose a mathematical framework based on the minimax principle, which minimizes the worst-case expected population risk under this assumption. Furthermore, our proposed minimax problem can be solved analytically, which provides a guideline for designing robust transfer learning models. According to the analytical expression, we interpret the influences of sample sizes, task distances, and the model dimensionality in knowledge transferring. Then, practical algorithms are developed based on the theoretical results. Finally, experiments conducted on image classification tasks show that our approaches can achieve robust and competitive accuracies under random selections of training sets.\n\n1\n\nINTRODUCTION\n\nThe goal of the transfer learning is to solve target tasks by the learning results from some source tasks. In order to study the fundamental aspects of the transfer learning problems, it is important to define and quantify the similarity between source and target tasks (Pan & Yang, 2009). While it is assumed that the source and target tasks are kind of similar in transfer learning problems (Weiss et al., 2016), the joint structures and similarity between the tasks can only be learned from the training data, which is challenging to be practically computed due to the limited availability of the labeled target samples. Therefore, in order to conduct meaningful theoretical analyses, it is often necessary to make extra assumptions, such as the linear combination of learning results (Ben-David et al., 2010) and linear regression transferring (Kuzborskij & Orabona, 2013), which could be limited in many applications.\n\nAs such, in this paper, we attempt to theoretically study the transfer learning by only assuming that the similarity between the source and target tasks is bounded, which is a weaker assumption, and is often valid in transfer learning problems. Under such an assumption, the minimax principle can be applied (Verdu & Poor, 1984) for estimating the target distribution. Based on this principle, the estimator minimizes the worst-case expected population risk (EPR) (Jin et al., 2018) under the bounded task distance constraint, which maintains robustness against the weak assumption. Practically, many empirical works have also followed the minimax setting and verify its validness (Zhang et al., 2019), while the theoretical analyses appear to be rather behind. The main challenge of analyzing general minimax problems in transfer learning is due to the difficulty of computing the expectations of the population risk under popular distance measures, such as the Kullback–Leibler (K-L) divergence (Thomas & Joy, 2006).\n\nTo deal with this difficulty, we adopt the widely used χ2-distance and Hellinger distance (Csisz ́ar & Shields, 2004) as the distance measure between data distributions of the tasks, and present a minimax formulation of transfer learning. By adopting such measures, the proposed minimax problems can be analytically solved. In particular, we show that the optimal estimation is to linearly combine the learning results of two tasks, where the combining coefficient can be computed from the training data. This provides a theoretical justification for many existing analyzing framework and algorithms (BenDavid et al., 2010; Garcke & Vanck, 2014). Note that the recent work (Tong et al., 2021) also analytically evaluates the combining coefficients, which rely on the underlying task distributions that are not available for real applications. Our work essentially provides the combining coefficient that\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nare both theoretical optimal and computable from data, which can be more appealing in practical applications.\n\nMoreover, the analyses of the minimax transfer learning problem on discrete data can be extended to the continuous data for real applications. In the continuous case, we consider similar transfer learning scheme as in (Nguyen et al., 2020), which transfers the topmost layer of the neural networks between source and target tasks. In particular, we show the analytical solution of optimal weights in the topmost layer, which is again a linear combination of the weights of the source and target problem. Furthermore, we propose the transfer learning algorithm guided by the theoretical results, where the robustness and performances of the algorithms are validated by several experiments on real datasets.\n\nThe contribution of this paper can be summarized as follows:\n\n• We make mild assumptions of the task distance and propose a minimax framework for analyzing transfer learning. Additionally, we establish the analytical solutions of the minimax problems in the discrete data space.\n\n• We extend the analyses to continuous data and establish similar results for the learning models with neural networks. Furthermore, we apply our theoretical results to develop robust transfer learning algorithms.\n\n• The experiments in real datasets validate our proposed algorithms, where our approaches\n\ncan have higher robustness and competitive accuracy.\n\nDue to the space limitations, the proofs of theorems are presented in the supplemental materials.\n\n2 PROBLEM FORMULATION\n\n2.1 NOTATIONS AND DEFINITIONS\n\nWe denote X and Y as the random variables of data and label with domains X and Y, respectively. For ease of illustration, the data X is set as a discrete random variable in section 2 and section 3.\n\nWe consider the transfer learning problem that has a target task and a source task, denoted as task T and S, respectively. For each task i = T, S, there are ni training samples {(x(i) l=1 i.i.d. generated from the underlying joint distributions P (i) XY (x, y) > 0, for all (x, y) ∈ X × Y. The empirical distributions ˆP (i)\n\nXY (i = T, S) of the samples are defined as\n\nXY with1 P (i)\n\nl , y(i)\n\nl )}ni\n\nXY (x, y) ≜ 1 ˆP (i)\n\nni\n\nni(cid:88)\n\nl=1\n\n1{x(i)\n\nl = x, y(i)\n\nl = y},\n\nwhere 1{·} denotes the indicator function (Feller, 2008) and let Pn be the set of all the possible empirical distributions supported by X × Y with n samples.\n\nIn this paper, we employ the following two distance measures for probability distributions, which are also widely used in statistics (Csisz ́ar & Shields, 2004), and more convenient in our analyses. Definition 2.1 (Referenced χ2-distance). Let R(z), P (z), and Q(z) be the distributions supported by Z. The χ2-distance between P (z) and Q(z) referenced by R(z) is defined as follows,\n\nR(P, Q) ≜ (cid:88) χ2\n\nz∈Z\n\n(P (z) − Q(z))2 R(z)\n\n.\n\n(1)\n\nDefinition 2.2 (Hellinger Distance). Let P (z) and Q(z) be the distributions supported by Z, The Hellinger distance between P (z) and Q(z) is defined as follows.\n\nH 2(P, Q) ≜ 1\n\n2\n\n(cid:88)\n\n(cid:16)(cid:112)P (z) − (cid:112)Q(z)\n\n(cid:17)2\n\n, and H(P, Q) ≜ (cid:112)H 2(P, Q).\n\n(2)\n\nz∈Z\n\n1This assumption comes from the fact that in practice such joint distributions are typically modeled by some\n\npositive parameterized families, e.g., the softmax function.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2.2 MINIMAX FORMULATION\n\nSince estimating the similarity between target and source tasks from data is challenging, we attempt to only make the assumption that the distance between two tasks is bounded by some constant D under a distance measure d(·, ·), i.e.,\n\n(cid:16)\n\nd\n\nP (T )\n\nXY , P (S)\n\nXY\n\n(cid:17)\n\n≤ D.\n\n(3)\n\nBased on this assumption, we consider a minimax formulation for estimating the target distribution with estimator QXY ( ˆP (T ) XY ), where the notation represents that the estimator QXY is a function of the empirical distributions ˆP (T )\n\nXY , ˆP (S)\n\nXY and ˆP (S) XY .\n\nMinimax Formulation:\n\nmin XY , ˆP (S) QXY ( ˆP (T ) XY )\n\nP (S)\n\nXY :d(P (T )\n\nmax XY ,P (S)\n\nXY )≤D\n\n(cid:16)\n\n(cid:104)\n\nd\n\nE\n\nP (T )\n\nXY , QXY\n\n(cid:17)(cid:105)\n\n,\n\n(4)\n\nwhere the expectation is taken over all possible ˆP (T )\n\nXY and ˆP (S)\n\nXY in PnT and PnS .\n\nThis formulation can be divided into two parts: (1) for given estimator QXY , we consider the largest expected population risk (EPR) of the distance between the underlying target distribution and the estimator; (2) we find the best estimator QXY as the function of training data that could minimize the worst risk. Note that the empirical distributions are sufficient statistics for the underlying distributions (Van der Vaart, 2000). We therefore consider QXY as the function of both empirical distributions. Accordingly, the EPR of the derived estimator under the true similarity is always smaller than the result of formulation (4). In other word, we design an estimator that has an upper-bounded EPR and it thus leads to robustness.\n\nNotice that the formulation (4) is generally difficult to be solved analytically due to: (i) the distance measure d(·, ·) can cause difficulty in computation, e.g., the logarithm function in the K-L divergence; (ii) the expectations over ˆP (T ) XY follow the multinomial distribution (Csisz ́ar, 1998), i.e., the probability of the empirical distribution P( ˆP (i) XY ) ∝ exp(−niD( ˆP (i) XY )), which is complicated to analyze. To address the issue (i), we choose the χ2-distance and Hellinger distance, which are more convenient to be analyzed in minimax problems. Moreover, for the issue (ii), we propose to study the surrogate problem which replaces the expectation computation in (4) by the integral\n\nXY and ˆP (S)\n\nXY ∥P (i)\n\nXY ; P (i)\n\n(cid:90) (cid:90)\n\n(cid:16)\n\nd\n\nP (T )\n\nXY , QXY\n\n(cid:17) (cid:89)\n\n(cid:16)\n\nexp\n\n−nid\n\ni=T,S\n\n(cid:16) ˆP (i)\n\nXY , P (i)\n\nXY\n\n(cid:17)(cid:17)\n\nd ˆP (i)\n\nXY .\n\n(5)\n\nNote that (5) is the asymptotic approximation of the expectation over multinomial distributions with the additional surrogation that the exponent in (5) can be chosen different from the K-L divergence. Such asymptotic approximation is also applied for theoretical analyses in high dimensional statistics (Morris, 1975). Then, the goal of this paper is to study the following minimax problems for transfer learning.\n\nFormulation 1 (referenced χ2-distance):\n\nmin XY , ˆP (S) QXY ( ˆP (T ) XY )\n\nmax XY ,P (S)\n\nR(P (T )\n\nXY )≤D2\n\nP (S)\n\nXY :χ2\n\n(cid:104)\n\nE\n\nχ2\n\nR(P (T )\n\nXY , QXY )\n\n(cid:105)\n\n,\n\nwhere the expectation is the integral over\n\nP\n\n(cid:16) ˆP (i)\n\nXY ; P (i)\n\nXY\n\n(cid:17)\n\n∝ exp\n\n(cid:16)\n\n−\n\nni 2\n\nχ2\n\nR\n\n(cid:16) ˆP (i)\n\nXY , P (i)\n\nXY\n\n(cid:17)(cid:17)\n\n, i = T, S.\n\n(6)\n\n(7)\n\nNote that the referenced-χ2 distance can be recognized as an asymptotic approximation of K-L divergence by Lemma A.1 in Appendix A. Here the reference distribution R is selected as ˆP (S)\n\nXY .2\n\nFormulation 2 (Hellinger distance):\n\nmin XY , ˆP (S) QXY ( ˆP (T ) XY )\n\nmax XY ,P (S) XY :H(P (T )\n\nP (S)\n\nXY )≤D\n\n(cid:104)\n\nH 2 (cid:16)\n\nE\n\nP (T )\n\nXY , QXY\n\n(cid:17)(cid:105)\n\n,\n\n(8)\n\n2Since the source samples are sufficient, with high probability, all the entries of ˆP (S)\n\nXY are positive.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nwhere the expectation is the integral over\n\nP\n\n(cid:16) ˆP (i)\n\nXY ; P (i)\n\nXY\n\n(cid:17)\n\n(cid:16)\n\n∝ exp\n\n−2niH 2 (cid:16) ˆP (i)\n\nXY , P (i)\n\nXY\n\n(cid:17)(cid:17)\n\n, i = T, S.\n\n(9)\n\nNote that Hellinger distance provides a lower bound of the K-L divergence with Lemma A.2 in Appendix A, and thus Formulation 2 computes a lower bound of the population risk in (4).\n\n3 ANALYSES FOR DISCRETE DATA\n\nIn this section, we provide the analytical solutions of the formulations (6) and (8). Similar minimax estimation problems have been studied in early works (Trybula, 1958; Berry, 1990). We now directly give the detailed expressions and the proof is provided in the supplementary material.\n\n3.1 ANALYTICAL SOLUTION OF FORMULATION 1\n\nTheorem 3.1. Let Q(1)\n\nXY be the estimator that achieves the minimax solution of problem (6) and then3 XY (x, y) + α1 ˆP (S) (10)\n\nXY (x, y) = (1 − α1) ˆP (T )\n\nXY (x, y),\n\nQ(1)\n\nfor all (x, y) ∈ X × Y, where\n\nα1 ≜\n\nnS nT + nS\n\n(cid:32)\n\n1 −\n\nI |X ||Y|\n\n2\n\n)\n\n( nT nS DD1 nT +nS 2 −1( nT nS DD1\n\nnT +nS\n\nI |X ||Y|\n\n(cid:33)\n\n,\n\nD D1\n\n)\n\n(11)\n\n(cid:113)\n\nand D1 ≜ kind with order ν (Abramowitz et al., 1988), whose definition is in Appendix B.\n\nXY ). Specifically, Iν(·) denotes the modified Bessel function of the first\n\nXY , ˆP (S)\n\nR( ˆP (T )\n\nχ2\n\nIn the remaining parts of this paper, we denote for ease of presentation Jν(x) ≜ I ν\n\n2\n\n(x)/I ν\n\n2 −1(x).\n\nTheorem 3.1 implies that linearly combining the learning results of different tasks is a preferable method for robust transfer learning, which is also widely used in existing algorithms and theoretical frameworks (Ben-David et al., 2010). Moreover, these works intuitively assume it, whereas we provide a theoretic support that could help explain the rationality. Remark 3.2. To help understand the related factors contained in (11), we consider a special regime that ν ≫ x, where Jν(x) ∼ x/ν. Then the expression of (11) can be approximated by\n\nα1 ∼\n\nnS nT + nS\n\n(cid:18)\n\n1 −\n\nnT nSD2 (nS + nT )|X ||Y|\n\n(cid:19)\n\n.\n\n(12)\n\nThis result is consistent with Eq.(6) in (Tong et al., 2021) under this special regime as explained in Appendix C. The coefficient α1, which represents the requirement of source samples, is positively associated with the model dimensionality |X ||Y|, which comes from that we learn all the |X ||Y| entries of the target distribution, and negatively associated with the target sample size nT and task distance D. These relationships are examined in the experimental part. We also provide an interesting geometric explanation of this pattern, which is shown in Appendix C.\n\n3.2 ANALYTICAL SOLUTION OF FORMULATION 2\n\nIn the following, we provide the solution of problem (8) based on Hellinger distance. Theorem 3.3. Let Q(2)\n\nXY be the estimator that achieves the minimax solution of problem (8) and then4\n\nQ(2)\n\nXY (x, y) =\n\n(1 − α2)\n\nˆP (T )\n\nXY (x, y) + α2\n\nˆP (S)\n\nXY (x, y)\n\n(cid:20)\n\n(cid:113)\n\n(cid:113)\n\n(cid:21)2\n\n,\n\nfor all (x, y) ∈ X × Y, where\n\nα2 =\n\nand D2 ≜ H( ˆP (T )\n\nXY , ˆP (S)\n\nXY ).\n\nnS nT + nS\n\n(cid:18)\n\n1 −\n\nD D2\n\nJ|X ||Y|\n\n(cid:18) 4nSnT DD2 nT + nS\n\n(cid:19)(cid:19)\n\n,\n\nAccordingly, we can also achieve a similar interpretation of the affecting factors as in section 3.1. 3This solution actually requests D/(cid:112)1/nT + 1/nS ≤ (cid:112)|X ||Y|, which can be easily guaranteed. 4It requests D/(cid:112)1/4nT + 1/4nS ≤ (cid:112)|X ||Y|.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n4 CONTINUOUS CASE AND ALGORITHM\n\nIn this section, we extend the previous analyses of discrete data to continuous data, which conform to the practical setting. In such cases, the previously adopted empirical distributions can not be seen as valid observations due to the infinite cardinality |X |, where most of the possible data are not sampled.\n\nIn order to apply the previous analyzing framework, we consider the retrain-head method (Nguyen et al., 2020), which is a commonly used transfer learning technique. With this method, a pre-trained network is prepared for extracting the feature of the data and then the topmost layer (known as “head”) can be retrained with observed samples. Under such a setting, we can recognize the weights in retrained topmost layers as the observations from the corresponding tasks. Note that compared with the cardinality of the large data space, the topmost layer has much fewer parameters.\n\nIn detail, a pre-trained network is composed of two parts of networks: (a) the previous layers whose input is data x and output is the d-dimensional features f (x) ∈ Rd, and (b) the topmost layer for linear classification, with weights g(y) ∈ Rd of each label y. Each task provides the learned weights to estimate the optimal weights for the target task. Moreover, the learned weights are decided by the model proposed for describing the data. For theoretical analyses, we provide the discriminative models for χ2-distance and Hellinger distance, where thereupon we can design practical algorithms. For convenience, we use notation h to represent the topmost layer in section 4.2.\n\n4.1 REVISED FORMULATION 1 AND MM-χ2 ALGORITHM\n\nWhen χ2-distance is chosen as the distance measure, we consider the discriminative model for the target distribution in the factorization form\n\nQ(f ,g)\n\nY |X (y|x) ≜ P (T )\n\nY (y) (cid:0)1 + f T(x)g(y)(cid:1) ,\n\n(13)\n\nwhich provides the probability of each label y ∈ Y for any data x. Such a model has been introduced in factorization machines (Rendle, 2010) and is commonly used in natural language processing problems (Levy & Goldberg, 2014).\n\nUnder the pre-trained feature extractor f ∗(·), the learned weights of topmost layers can be derived by minimizing the distance between the empirical distribution and the model. For computation, we avoid using the joint distribution as the reference and define the χ2-distance measure referenced by the product marginal distribution, i.e., χ2 (·, ·). Then, the learned weights ˆgi of\n\nM (·, ·) ≜ χ2\n\nP (T )\n\nX P (T )\n\nY\n\neach task i = T, S can be defined as\n\nˆgi ≜ arg ming χ2\n\nM\n\n(cid:16) ˆP (i)\n\nXY , P (T )\n\nX Q(f ∗,g)\n\nY |X\n\n(cid:17)\n\n.\n\n(14)\n\nNow ˆgT and ˆgS are the observations to generate the minimax solution, where the expectations can be defined as gi(y) = E[ˆgi(y)]. Note that the parameters of gT are just the topmost weights we hope to achieve. Then, the minimax problem can be defined as follows [cf. (6)]:\n\ng∗ = arg min g(ˆgT ,ˆgS )\n\nmax gS ∈G\n\n(cid:104)\n\nE\n\nχ2\n\nM (P (T )\n\nX Q(f ∗,gT )\n\nY |X , P (T )\n\nX Q(f ∗,g) Y |X )\n\n(cid:105)\n\n,\n\n(15)\n\nwhere G ≜\n\n(cid:110)\n\ng : χ2\n\nM (P (T )\n\nX Q(f ∗,gT )\n\nY |X , P (T )\n\nX Q(f ∗,g)\n\nY |X ) ≤ D2(cid:111)\n\n.\n\nWe can directly apply Theorem 3.1 and obtain the following theorem. Theorem 4.1. When the empirical distributions ˆP (T ) minimax solution as defined in (15) is 5\n\nXY and ˆP (S)\n\nXY follow the density function (7), the\n\ng∗ = (1 − ̃α1)ˆgT + ̃α1 ˆgS,\n\n(16)\n\n ̃α1 =\n\nnS nT + nS\n\n(cid:32)\n\n1 −\n\nD ̃D1\n\nJd|Y|\n\n(cid:32)\n\nnT nSD ̃D1 nT + nS\n\n(cid:33)(cid:33)\n\n.\n\n≜ χ2\n\nM (P (T )\n\nX Q(f ∗,ˆgT )\n\nY |X , P (T )\n\nX Q(f ∗,ˆgS )\n\nY |X ).\n\nwhere\n\nand ̃D2\n\n1\n\n5In practice, when the assumption D/(cid:112)1/nT + 1/nS < (cid:112)d|Y| does not hold, the estimator can still\n\nprovide a sub-optimal solution.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Minimax χ2-Algorithm (MM-χ2)\n\n, y(i)\n\nl )}ni\n\nl=1 (i = T, S), learning rate η\n\n1: Input: target and source data samples {(x(i) 2: Randomly initialize α, f ∗, g∗ 3: repeat 4:\n\n(f ∗, g∗) ← (f ∗, g∗) − η∇(f ,g)L1(α, f ∗, g∗) α ← nS\n\n(cid:16) nT nS D ̃D1\n\n(cid:17)(cid:17)\n\n(cid:16)\n\nl\n\nJd|Y|\n\nnT +nS\n\nnT +nS\n\n5: 6: until f ∗, g∗ converge 7: return f ∗, g∗\n\n1 − D ̃D1\n\nAccordingly, we can design an algorithm based on Theorem 4.1. Despite the theoretical analyses where the feature extractor is fixed, our algorithm jointly optimizes the feature extraction f and the topmost layers g, which is a typical retraining procedure. It is proved that the linearly combined weights (16) can be achieved by minimizing the linearly combined training loss with the same coefficient. We therefore define\n\nL1(α, f , g) ≜ (1 − α)χ2\n\nM\n\n(cid:16) ˆP (S)\n\nXY , P (T )\n\nX Q(f ,g)\n\nY |X\n\n(cid:17)\n\n+ αχ2\n\nM\n\n(cid:16) ˆP (T )\n\nXY , P (T )\n\nX Q(f ,g)\n\nY |X\n\n(cid:17)\n\n.\n\n(17)\n\nThen, the MM-χ2 algorithm is given in Algorithm 1. In practice, the loss L1(α, f , g), the related quantities D, and ̃D1 in Theorem 4.1 can be estimated by the empirical means of the features of samples. Detailed implementations are provided in the supplementary material. With the f ∗ and g∗ computed by Algorithm 1, the predicted label ˆy(x) for sample x is given by the maximum a posterior (MAP) decision rule with ˆy(x) = arg maxy∈Y Q(f ∗, g∗)\n\nY |X (y|x).\n\n4.2 REVISED FORMULATION 2 AND MM-HEL ALGORITHM\n\nWhen Hellinger distance is chosen as the distance measure, we consider the discriminative model for the target distribution in the following form. For each i = T, S,\n\n(cid:113)\n\n ̃Q(i,f ,h)\n\nY |X (y|x) ≜\n\n(cid:113)\n\nY (y) (cid:0)1 + f T(x)h(y)(cid:1) . P (i)\n\n(18)\n\nThis model is a deformation of (13), which makes the model trainable under Hellinger distance. Similarly, under the pre-trained feature extractor f ∗, the learned weights ˆhi of each task i = T, S can be defined as\n\nˆhi ≜ arg min\n\nh\n\nH 2 (cid:16) ˆP (i)\n\nXY , P (i)\n\nX\n\n ̃Q(i,f ∗,h)\n\nY |X\n\n(cid:17)\n\n.\n\n(19)\n\nNow ˆhT and ˆhS are the observations to generate the minimax solution, where the expectations can be defined as hi(y) = E[ˆhi(y)], i = T, S. The minimax problem can be defined as follows [cf. (8)]: P (T )\n\n ̃Q(T,f ∗,hT )\n\n ̃Q(T,f ∗,h)\n\nH 2 (cid:16)\n\n, P (T )\n\n(cid:17)(cid:105)\n\n(20)\n\nE\n\n(cid:104)\n\n,\n\nX\n\nY |X\n\nX\n\nY |X\n\nP (T )\n\nY (y)Λ\n\n1 2\n\nT hT (y) −\n\n(cid:113)\n\nP (S)\n\nY (y)Λ\n\nS h(y)\n\n1 2\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≤ D2(cid:111)\n\n, and Λi ≜\n\nmax hS ∈H\n\nh∗ ≜ arg min h(ˆhT ,ˆhS ) (cid:13) (cid:13) (cid:13)\n\n(cid:80)\n\n(cid:113)\n\n(cid:110)\n\nwhere H ≜ E\n\nh : 1 2\n[f ∗(X)f ∗T(X)], for i = T, S.\n\ny∈Y\n\nP (i)\n\nX\n\nWe can directly apply Theorem 3.3 and obtain the following theorem. Theorem 4.2. When the empirical distrbutions ˆP (T ) minimax solution as defined in (20) is\n\nXY and ˆP (S)\n\nXY follow the density function (9), the\n\nh∗(y) = (1 − ̃α2)ˆhT (y) + ̃α2\n\n(cid:113)\n\nfor all y ∈ Y , where\n\nP (S)\n\nY (y)/P (T )\n\nY (y)Λ− 1\n\nT Λ\n\n2\n\n1 2\nS\n\nˆhS(y),\n\n ̃α2 =\n\nnS nT + nS\n\n(cid:32)\n\n1 −\n\nD ̃D2 (cid:113)\n\nJd|Y|\n\n(cid:32)\n\n4nT nSD ̃D2 nT + nS\n\n(cid:33)(cid:33)\n\n,\n\n(21)\n\nand ̃D2\n\n2\n\n≜ 1 2\n\n(cid:80)\n\ny∈Y\n\n(cid:113)\n\n(cid:13) (cid:13) (cid:13)\n\nP (T )\n\nY (y)Λ\n\n1 2\nT\n\nˆhT (y) −\n\nP (S)\n\nY (y)Λ\n\n1 2\nS\n\nˆhS(y)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Minimax Hellinger-Algorithm (MM-Hel)\n\n, y(i)\n\nl )}ni\n\nl=1 (i = T, S)\n\n1: Input: target and source data samples {(x(i) 2: (f ∗, h∗ 3: α ← nS\n\n2) ← arg minf ,h1,h2 L2(f , h1, h2)\n\n(cid:16) 4nT nS D ̃D2\n\n1, h∗\n\n(cid:17)(cid:17)\n\n(cid:16)\n\nl\n\nJd|Y|\n\n1 − D ̃D2\n\nnT +nS\n\nnT +nS\n\n4: h∗(y) ← (1 − α)h∗\n\n1(y) + α\n\n5: return f ∗, h∗\n\n(cid:114)\n\nP (S) P (T )\n\nY (y) Y (y)\n\nΛ− 1\n\nT Λ\n\n2\n\n1 2\n\nS h∗\n\n2(y)\n\nSimilarly, we can design an algorithm based on Theorem 4.2. Note that we cannot apply the linearly combined training loss in the Hellinger distance setting due to the design of two different distribution models ̃Q(T,f ,h) Y |X . We choose jointly training the shared feature extractor f and individual topmost layers of the target and source task. The training loss is defined as\n\nand ̃Q(S,f ,h)\n\nY |X\n\nL2(f , h1, h2) ≜ H 2 (cid:16) ˆP (T )\n\nXY , P (T )\n\nX\n\n(cid:17)\n\n ̃Q(T,f ,h1)\n\nY |X\n\n+ H 2 (cid:16) ˆP (S)\n\nXY , P (S)\n\nX\n\n ̃Q(S,f ,h2)\n\nY |X\n\n(cid:17)\n\n.\n\n(22)\n\nThen, the MM-Hel algorithm is given in Algorithm 2. Similarly, we also provide the estimation of the related quantities in the supplementary material. With the computed f ∗ and h∗, the predicted label\n\nˆy(x) for sample x is given by the MAP decision rule with ˆy(x) = arg maxy∈Y\n\n5 EXPERIMENTS\n\n(cid:113)\n\n ̃Q(T,f ∗, g∗)\n\nY |X\n\n(y|x).\n\nTo validate the theoretical analyses in Theorem 3.1 and Theorem 3.3, and the robustness of our algorithms, we conduct a series of experiments on common datasets for image recognition, including CIFAR-10 (Krizhevsky et al., 2009), Office-31 and Office-Caltech (Gong et al., 2012b) datasets. For convenience, different transfer settings are denoted by “source→target”.\n\n5.1 CIFAR-10\n\nWe conduct transfer learning experiments on CIFAR-10 dataset in order to verify the theoretical interpretations of the related factors in Remark 3.2, which mainly cover the sample size and task distance. Specifically, CIFAR-10 dataset contains 50 000 training images and 10 000 testing images in 10 classes. We first construct the source tasks and target task by dividing the original CIFAR10 dataset into five disjoint sub-datasets, each containing two classes of the original data, which corresponds to a binary classification task. Then, we choose one as our target task (task 1), and use the other four as source tasks referred to as task 2, 3, 4, 5, where four corresponding transfer learning tasks are established.\n\nIn each transfer learning task, we use 2000 source images, with 1000 images per binary class. Target sample size n is set as n = 12, 20, 60, 200 for four sub-tasks. Throughout this experiment, the feature f is of dimensionality d = 10, generated by GoogLeNet (Szegedy et al., 2015) pre-trained by ImageNet (Russakovsky et al., 2015), and followed by a fully connected layer.\n\nThe accuracies on the target testing images of MM-χ2 and MM-Hel algorithms are summarized in Table 1 and Table 2. In each task, target samples are randomly picked from the target training set. All the accuracies and standard deviations are reported over five random selections of target samples. In detail, we analyze the effect of target sample sizes as in figure 1. figure 1 shows the changes of the accuracies and coefficient ̃α1 as defined in (16) (averaged over 5 tests) of different target sample sizes. The coefficient ̃α1 represents how much the final model relies on the source task. It corresponds to our interpretation in Remark 3.2 that a larger target sample size could lead to less dependency on the source task, and meanwhile the accuracies become higher and more stable. We also analyze the effect of task distances as in figure 2. figure 2 shows the changes of the accuracies and task distance ̃D2 as defined in (21) (averaged over 5 tests) of different source tasks, where a larger task distance can lead to a worse accuracy and stability.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Accuracies (%) of CIFAR-10 transfer learning tasks based on MM-χ2 algorithm, where nT represents the target sample size. The baseline is trained with merely target samples.\n\nTable 2: Accuracies (%) of CIFAR-10 transfer learning tasks based on MM-Hel algorithm, where nT represents the target sample size and it shares the same baseline with Table 1.\n\nTasks\n\nnT = 12\n\nnT = 20\n\nnT = 60\n\nnT = 200\n\nBaseline 2 → 1 3 → 1 4 → 1 5 → 1\n\n77.8±2.7 82.6±1.4 80.2±1.7 79.1±2.2 85.6±1.5\n\n83.4±1.70 87.9±1.2 85.9±1.4 85.1±1.7 89.7±1.3\n\n90.0±1.0 90.9±0.7 90.6±1.2 90.0±1.3 91.1±0.7\n\n93.4±0.7 94.2±0.6 93.9±0.7 93.4±0.8 94.3±0.5\n\nTasks\n\nnT = 12\n\nnT = 20\n\nnT = 60\n\nnT = 200\n\n2 → 1 3 → 1 4 → 1 5 → 1\n\n81.5±1.2 81.3±1.7 77.5±1.8 84.2±1.2\n\n88.5±1.1 87.5±1.2 83.9±1.7 90.2±1.1\n\n91.2±0.8 91.2±0.8 90.2±1.0 92.2±0.7\n\n94.9±0.5 94.2±0.6 93.5±0.6 94.7±0.5\n\nFigure 1: The accuracies and coefficient ̃α1 in transferring task 5 → 1 based on MM-χ2 algorithm under different target sample sizes.\n\nFigure 2: The accuracies and distance measure ̃D2 of target sample size nT = 12 based on MM-Hel algorithm under different source tasks.\n\n5.2 OFFICE-31\n\nOffice-31 dataset contains images of 31 categories with 3 sub-datasets, including Amazon (A, 2817 images), Dslr (D, 498 images), and Webcam (W, 795 images). Six transferring tasks can be established as A→D, A→W, D→W, D→A, W→A, and W→D. We adopt the transfer learning setting in (Tzeng et al., 2015), illustrated as follows. Specifically, 3 target samples per category are used for training, and the training sample size (per category) for source task is set to 20 or 8, depending on whether the source task is Amazon or not.\n\nIn this experiment, the feature f is extracted by the VGG-16 (Simonyan & Zisserman, 2014) network pre-trained on the ImageNet, succeeded by fully connected layers, and the output is 64-dimensional. We introduce the UDDA (Motiian et al., 2017) algorithm as the typical baseline and the iterative linear combination method (Tong et al., 2021) (ILCM) for comparison, which employs a similar linear combination method.\n\nTable 3 summarizes test accuracies under different transfer settings, where all reported accuracies and standard deviations are averaged over five train-test splits. The results indicate that our algorithms generally have higher robustness and competitive accuracies.\n\n5.3 OFFICE-CALTECH\n\nOffice-Caltech dataset is composed of 10 categories, divided as four sub-datasets: Amazon (A, 958 images), Caltech (C, 1123 images), Webcam (W, 295 images), and Dslr (D, 157 images). We focus on the six transfer settings depending on C, i.e., A→C, W→C, D→C, C→A, C→W, and C→D. The train-test split is as introduced in (Gong et al., 2012a). The feature f is based on the pre-trained DeCAF network (Donahue et al., 2014), succeeded by fully connected layers, and the output dimension is d = 10.\n\nTable 4 shows the performances in comparison with CPNN (Ding et al., 2018) and ILCM (Tong et al., 2021) algorithms, where CPNN is chosen as the baseline to be consistent with ILCM.\n\n8\n\n61030100Target sample size848688909294Accuracies on target task/%Accuracies0.080.100.120.140.16Coefficient ̃α1Coefficient ̃α12345Source Task65.067.570.072.575.077.580.082.585.0Accuracies on target task/%Accuracies1.01.11.21.31.41.51.61.71.8Task distance ̃D2Task distance ̃D2Under review as a conference paper at ICLR 2023\n\nTable 3: Test accuracies for target tasks under different transfer settings on Office-31.\n\nMethod\n\nA→D\n\nA→W\n\nD→W\n\nD→A\n\nW→A\n\nW→D\n\nUDDA ILCM MM-χ2 (Ours) MM-Hel (Ours)\n\n89.0±1.2 90.0±1.4 90.2±0.6 89.8±0.5\n\n88.2±1.0 87.3±1.1 87.5±0.5 88.1±0.4\n\n96.4±0.8 96.5±1.0 96.7±0.5 96.0±0.5\n\n71.8±0.5 72.4±1.0 72.9±0.8 72.2±0.5\n\n72.1±1.0 72.1±0.9 72.2±0.6 72.0±0.4\n\n97.6±0.4 97.2±0.4 97.6±0.3 97.5±0.4\n\nTable 4: Test accuracies for target tasks under different transfer settings on Office-Caltech.\n\nMethod\n\nA→C\n\nW→C\n\nD→C\n\nC→A\n\nC→W\n\nC→D\n\nCPNN ILCM MM-χ2 (Ours) MM-Hel (Ours)\n\n74.3±0.6 80.3±0.7 79.9±0.7 79.2±0.3\n\n72.1±0.8 72.9±0.7 73.5±0.5 72.7±0.5\n\n66.6±0.8 72.2±0.9 71.7±0.7 72.1±0.6\n\n86.2±0.5 88.4±0.7 90.1±0.5 88.5±0.4\n\n86.0±0.5 85.9±0.5 86.8±0.5 85.5±0.4\n\n79.9±0.7 83.5±0.9 84.0±0.7 83.5±0.5\n\n6 RELATED WORKS\n\n6.1 MINIMAX ESTIMATOR OF BOUNDED NORMAL MEAN\n\nMinimax estimator is a significant theme in statistical decision theory, which deals with the problem of estimating a deterministic parameter in a certain family (Hodges & Lehmann, 2012). Under the special setting of bounded normal mean, many works study the analytical solution when the centers of Gaussian observations are restricted, including analytical solution for 1-dimensional observations (Casella & Strawderman, 1981), high-dimensional observations (Berry, 1990; Marchand & Perron, 2002). Moreover, the objective function can also be measured by norms other than mean square error (Bischoff et al., 1995), which allows the applications in machine learning scenarios and help derive the solutions of this paper’s formulations.\n\n6.2 SELECTION OF DISTANCE MEASURE\n\nIn this paper, we select χ2-distance and Hellinger distance as the distance measure in (3), which help analytically solve the minimax problem. These two measurements are both from the family of f -divergence (Csisz ́ar & Shields, 2004) and are widely-used in machine learning. Specifically, χ2-distance can lead to the typical alternating conditional expectation algorithm (Xu & Huang, 2020). Hellinger distance is also used to evaluate the domain adaptation in transfer learning (Baktashmotlagh et al., 2014; 2016). Moreover, most existing measurements with non-linear functions, e.g, K-L divergence containing the logarithm function, could be ill-defined, as explained in section 2.2.\n\n6.3 MINIMAX TRANSFER LEARNING AND ROBUSTNESS\n\nMinimax principle has been widely-used in transfer learning to promote the robustness of algorithms (Verdu & Poor, 1984). The most common empirical method is connected to the adversarial learning methods (Shafahi et al., 2019), including maximizing the training loss of adversarial classifier (Tzeng et al., 2017) and maximizing the discrepancy between classifiers’ outputs (Saito et al., 2018). Meanwhile, transfer learning settings can naturally imply minimax optimization problems in view of the relationship between target and source tasks (Zhang et al., 2019). Recent researches reveal that the maximization can succeed over the constraints on the distribution shift (Lei et al., 2021), the similarity between neaural network parameters (Kalan et al., 2020), and optional source tasks (Cai & Wei, 2021).\n\n7 CONCLUSION\n\nThis paper introduces a minimax framework for transfer learning based on the assumption of task distance. We provide the analytical solution of the minimax problem and characterize the roles of sample sizes, task distance, and model dimensionality in knowledge transferring. In addition, we develop robust transfer learning algorithms based on theoretical analyses. Experiments on practical tasks show the robustness and effectiveness of our proposed algorithms.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMilton Abramowitz, Irene A Stegun, and Robert H Romer. Handbook of mathematical functions\n\nwith formulas, graphs, and mathematical tables, 1988.\n\nMahsa Baktashmotlagh, Mehrtash T Harandi, Brian C Lovell, and Mathieu Salzmann. Domain adaptation on the statistical manifold. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2481–2488, 2014.\n\nMahsa Baktashmotlagh, Mehrtash Harandi, and Mathieu Salzmann. Distribution-matching embedding for visual domain adaptation. Journal of Machine Learning Research, 17:Article–number, 2016.\n\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1):151–175, 2010.\n\nJ Calvin Berry. Minimax estimation of a bounded normal mean vector. Journal of Multivariate\n\nAnalysis, 35(1):130–139, 1990.\n\nWolfgang Bischoff, Werner Fieger, and Stefanie Wulfert. Minimax-and γ-minimax estimation of a\n\nbounded normal mean under linex loss. Statistics & Risk Modeling, 13(3):287–298, 1995.\n\nT Tony Cai and Hongji Wei. Transfer learning for nonparametric classification: Minimax rate and\n\nadaptive classifier. The Annals of Statistics, 49(1):100–128, 2021.\n\nGeorge Casella and William E Strawderman. Estimating a bounded normal mean. The Annals of\n\nStatistics, 9(4):870–878, 1981.\n\nImre Csisz ́ar. The method of types [information theory]. IEEE Transactions on Information Theory,\n\n44(6):2505–2523, 1998.\n\nImre Csisz ́ar and Paul C Shields. Information theory and statistics: A tutorial. 2004.\n\nZhengming Ding, Nasser M Nasrabadi, and Yun Fu. Semi-supervised deep domain adaptation via coupled neural networks. IEEE Transactions on Image Processing, 27(11):5214–5224, 2018.\n\nJeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pp. 647–655. PMLR, 2014.\n\nWillliam Feller. An introduction to probability theory and its applications, vol 2. John Wiley & Sons,\n\n2008.\n\nJochen Garcke and Thomas Vanck. Importance weighted inductive transfer learning for regression. In Joint European conference on machine learning and knowledge discovery in databases, pp. 466–481. Springer, 2014.\n\nBoqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In 2012 IEEE conference on computer vision and pattern recognition, pp. 2066–2073. IEEE, 2012a.\n\nBoqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In 2012 IEEE conference on computer vision and pattern recognition, pp. 2066–2073. IEEE, 2012b.\n\nJL Hodges and Eric L Lehmann. Some problems in minimax point estimation. In Selected Works of\n\nEL Lehmann, pp. 15–30. Springer, 2012.\n\nChi Jin, Lydia T. Liu, Rong Ge, and Michael I Jordan. On the local minima of the empirical risk. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\nSeyed Mohammadreza Mousavi Kalan, Zalan Fabian, A Salman Avestimehr, and Mahdi Soltanolkotabi. Minimax lower bounds for transfer learning with linear and one-hidden layer neural networks. arXiv preprint arXiv:2006.10581, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nIlja Kuzborskij and Francesco Orabona. Stability and hypothesis transfer learning. In International\n\nConference on Machine Learning, pp. 942–950. PMLR, 2013.\n\nQi Lei, Wei Hu, and Jason Lee. Near-optimal linear regression under distribution shift. In Interna-\n\ntional Conference on Machine Learning, pp. 6164–6174. PMLR, 2021.\n\nOmer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. Advances\n\nin neural information processing systems, 27:2177–2185, 2014.\n\n ́Eric Marchand and Franc ̧ois Perron. On the minimax estimator of a bounded normal mean. Statistics\n\n& probability letters, 58(4):327–333, 2002.\n\nCarl Morris. Central limit theorems for multinomial sums. The Annals of Statistics, pp. 165–188,\n\n1975.\n\nSaeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified deep supervised domain adaptation and generalization. In Proceedings of the IEEE international conference on computer vision, pp. 5715–5725, 2017.\n\nCuong Nguyen, Tal Hassner, Matthias Seeger, and Cedric Archambeau. Leep: A new measure to evaluate transferability of learned representations. In International Conference on Machine Learning, pp. 7294–7305. PMLR, 2020.\n\nSinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge\n\nand data engineering, 22(10):1345–1359, 2009.\n\nSteffen Rendle. Factorization machines. In 2010 IEEE International Conference on Data Mining, pp.\n\n995–1000. IEEE, 2010.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115 (3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.\n\nKuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3723–3732, 2018.\n\nAli Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David Jacobs, and Tom\n\nGoldstein. Adversarially robust transfer learning. arXiv preprint arXiv:1905.08232, 2019.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015.\n\nMTCAJ Thomas and A Thomas Joy. Elements of information theory, 2006.\n\nXinyi Tong, Xiangxiang Xu, Shao-Lun Huang, and Lizhong Zheng. A mathematical framework for quantifying transferability in multi-source transfer learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nStanislaw Trybula. Some problems of simultaneous minimax estimation. The Annals of Mathematical\n\nStatistics, 29(1):245–253, 1958.\n\nEric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pp. 4068–4076, 2015.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nEric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7167–7176, 2017.\n\nAad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.\n\nSergio Verdu and H Poor. On minimax robustness: A general approach and applications. IEEE\n\ntransactions on Information Theory, 30(2):328–340, 1984.\n\nKarl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of\n\nBig data, 3(1):1–40, 2016.\n\nXiangxiang Xu and Shao-Lun Huang. Maximal correlation regression. IEEE Access, 8:26591–26601,\n\n2020. doi: 10.1109/ACCESS.2020.2971386.\n\nYuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In International Conference on Machine Learning, pp. 7404–7413. PMLR, 2019.\n\nA APPROXIMATION OF K-L DIVERGENCE\n\nFirstly, we provide the following approximation of K-L divergence, under the assumption that R(z), P (z), and Q(z) are close to each other.\n\nLemma A.1. Suppose that |P (z) − R(z)| < ε and |Q(z) − R(z)| < ε for each z ∈ Z, where ε/|Z| ≪ 1, K-L divergence between P (z) and Q(z) can have the following approximation\n\nD (P ∥Q) =\n\n1 2\n\nχ2\n\nR(P, Q) + O(ε3).\n\n(23)\n\nProof.\n\nD (P ∥Q) =\n\n=\n\n=\n\n=\n\n=\n\n=\n\n−P (z) log\n\nQ(z) P (z)\n\n(cid:18)\n\n−P (z) log\n\n1 +\n\n(cid:19)\n\nQ(z) − P (z) P (z)\n\n−P (z)\n\n(cid:18) Q(z) − P (z) P (z)\n\n−\n\n(Q(z) − P (z))2 2P 2(z)\n\n(cid:19)\n\n+ O(ε3)\n\n(Q(z) − P (z))2 2P (z)\n\n+ O(ε3)\n\n(Q(z) − P (z))2 2(R(z) + P (z) − R(z))\n\n+ O(ε3)\n\n(cid:88)\n\nz∈Z\n\n(cid:88)\n\nz∈Z\n\n(cid:88)\n\nz∈Z\n\n(cid:88)\n\nz∈Z\n\n(cid:88)\n\nz∈Z\n\n1 2\n\n(cid:88)\n\nz∈Z\n\n(Q(z) − P (z))2 R(z)\n\n+ O(ε3).\n\nSecondly, we consider Hellinger distance is closely connected with K-L divergence as the lower bound, which is explained in the following lemma.\n\nLemma A.2. Let P (z), and Q(z) be the distribution supported by Z. We have\n\nD (P ∥Q) ≥ 2H 2(P, Q).\n\n(24)\n\nThe proof of this lemma is easy to find and omitted here.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nB PROOF OF THEOREM 3.1 AND THEOREM 3.3\n\nFirst, the regular definition of the modified Bessel functions of the first kind is\n\nIν(x) =\n\n∞ (cid:88)\n\nm=0\n\n1 m!Γ(m + ν + 1)\n\n(cid:16) x 2\n\n(cid:17)(2m+ν)\n\n,\n\nwhere Γ(·) denotes the gamma function.\n\nTo solve the minimax problem (6) and (8), we apply the minimax estimation of a bounded normal mean vector Berry (1990). The following lemma is a direct extension of the bounded normal mean results. Lemma B.1. Given two observations y ∼ N (x, σ2 k × k identity matrix, their centers satisfy ∥x − z∥ ≤ D, and D/(cid:112)σ2 minimax estimator for the following minimax problem, e.g.,\n\n2Ik), where Ik denotes the k. Let ˆx∗ be the\n\n1Ik) and w ∼ N (z, σ2\n\n1 + σ2\n\n2 ≤\n\n√\n\nˆx∗ = arg min ˆx(y,w)\n\nmax z:∥x−z∥2≤D2\n\nE[∥ ˆx(y, w) − x∥2].\n\n(25)\n\nThen, the expression of ˆx∗ is\n\nˆx∗ =\n\nσ2 1\n1 + σ2\n\n2\n\nσ2\n\nw +\n\nσ2 2\n1 + σ2\n\n2\n\nσ2\n\ny +\n\nσ2 1\n1 + σ2\n\n2\n\nσ2\n\n2\n\n( D I k σ2 1 +σ2 2 −1( D I k\n\nσ2\n\n2\n\n1 +σ2\n\n2\n\n∥y − w∥)\n\n∥y − w∥)\n\nD ∥y − w∥\n\n(y − w).\n\n(26)\n\nProof. First, we derive the posterior MMSE estimator for x under the uniform prior distribution on the surface of the sphere.\n\nLemma B.2 (MMSE estimator). When the means and variances are finite, the MMSE estimator for parameter x of observation y is uniquely defined and is given by\n\nˆx(y) = E[x|y].\n\nThe likelihood of the observation y, w is\n\nP (y, w|x, z) = P (y|x) P (w|z) ∝ exp\n\n(cid:18)\n\n−\n\n(y − x)2 2σ2 1\n\n(cid:19)\n\n(cid:18)\n\nexp\n\n−\n\n(z − w)2 2σ2 2\n\n(cid:19)\n\n.\n\n(27)\n\n(28)\n\nLet π(x|z) be the uniform prior distribution on the surface of the sphere in k dimensions with center at z and radius D. Let t ≜ x − z, and we denote the prior as π(t) = 1{∥t∥=D}(t).\n\nUnder such a prior distribution,\n\nP(y, w|x) ∝\n\n(cid:18)\n\nexp\n\n−\n\n(cid:90)\n\nRk\n\n(y − x)2 2σ2 1\n\n(cid:19)\n\n(cid:18)\n\nexp\n\n−\n\n(cid:19)\n\n(z − w)2 2σ2 2\n\n1{∥x−z∥=D}(x − z)dz.\n\n(29)\n\nThen, the posterior distribution is\n\nP (x|y, w) ∝\n\n∝\n\n∝\n\n(cid:18)\n\nexp\n\n−\n\n(cid:18)\n\nexp\n\n−\n\n(cid:32)\n\nexp\n\n−\n\n(cid:90)\n\nRk\n\n(cid:90)\n\nRk\n\n(cid:90)\n\nRk\n\n(y − x)2 2σ2 1\n(x − y)2 2σ2 1\n(cid:18) 1 σ2 1\n\n1 2\n\n+\n\n(cid:19)\n\n(cid:19)\n\n(cid:18)\n\nexp\n\n−\n\n(cid:18)\n\nexp\n\n−\n\n(cid:19)\n\n(z − w)2 2σ2 2\n\n(x − (w + t))2 2σ2 2\n\n1{∥x−z∥=D}(z)dz\n\n(cid:19)\n\n1{∥t∥=D}(t)dt\n\n(cid:19) (cid:18)\n\nx −\n\n1 σ2 2\n\n1(w + t)\n\nσ2\n\n2y + σ2 σ2\n\n1 + σ2\n\n2\n\n(cid:19)2(cid:33)\n\n(cid:18)\n\n· exp\n\n−\n\n1 2\n\n(y − (w + t))2 1 + σ2\n\nσ2\n\n2\n\n(cid:19)\n\n1{∥t∥=D}(t)dt.\n\n(30)\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTherefore, the Bayes estimator of the posterior is\n\nˆx∗ = E [x|y, w]\n\n(cid:90)\n\n= A\n\n(cid:90)\n\nx\n\n(cid:32)\n\nexp\n\n−\n\nRk\n\nRk\n\n(cid:18)\n\n· exp\n\n−\n\n1 2\n\n(y − (w + t))2 1 + σ2\n\n2\n\nσ2 (cid:32)\n\n1 2\n\n(cid:18) 1 σ2 1\n\n+\n\n1 σ2 2\n\n(cid:19) (cid:18)\n\nx −\n\n1(w + t)\n\nσ2\n\n2y + σ2 σ2\n\n1 + σ2\n\n2\n\n(cid:19)\n\n1{∥t∥=D}(t)dtdx\n\n(cid:90)\n\n(cid:90)\n\n= A\n\nRk\n\nRk\n\nx exp\n\n−\n\n1 2\n\n(cid:18) 1 σ2 1\n\n+\n\n1 σ2 2\n\n(cid:19) (cid:18)\n\nx −\n\n1(w + t)\n\nσ2\n\n2y + σ2 σ2\n\n1 + σ2\n\n2\n\n(cid:19)2(cid:33)\n\n(cid:19)2(cid:33)\n\n(cid:19)\n\n1{∥t∥=D}(t)dxdt\n\n· exp\n\n(cid:90)\n\n= A′\n\nRk\n\n(cid:18)\n\n(y − (w + t))2 1 + σ2 1(w + t)\n\nσ2\n\n2\n\n−\n\n1 2\n2y + σ2 σ2\n\nexp\n\nσ2\n\n(cid:18)\n\n−\n\n1 2\n\n=\n\n=\n\n1w\n\n1w\n\nσ2\n\nσ2\n\n2\n\n2y + σ2 1 + σ2 σ2 2y + σ2 σ2 1 + σ2\n\n2\n\n+\n\nσ2\n\n1 + σ2 2\nσ2 1\n1 + σ2 σ2 1\n1 + σ2\n\nσ2\n\n2\n\n2\n\n+\n\n(cid:90)\n\nA′\n\nt exp\n\nRk ( D I k σ2 1 +σ2 2 −1( D I k\n\nσ2\n\n2\n\n2\n\n1 +σ2\n\n2\n\n(cid:19)\n\n1{∥t∥=D}(t)dt\n\nσ2\n\n(y − (w + t))2 1 + σ2 1\n2\n\n−\n\n(cid:18)\n\n2\n\n(y − (w + t))2 1 + σ2\n\nσ2\n\n2\n\n(cid:19)\n\n1{∥t∥=D}(t)dt\n\n∥y − w∥)\n\n∥y − w∥)\n\nD ∥y − w∥\n\n(y − w),\n\n(31)\n\nwhere A and A′ are the normalization constants.\n\nThen, we will prove that ˆx∗ is the minimax estimator. Specifically , we prove that σ2 independent of y − w. Since both two r.v.s are normal, we only need to prove Cov(σ2 w) = 0, i.e.,\n\n2y + σ2\n\n1w is 1w, y −\n\n2y + σ2\n\nCov(σ2\n\n2y + σ2\n\n1w, y − w) = σ2\n\n2Var(y) + σ2\n\n1Var(w) = σ2\n\n1σ2\n\n2 − σ2\n\n1σ2\n\n2 = 0.\n\n(32)\n\nWe then define the risk function\n\n\n\n(cid:32)\n\nR ˆx∗ (t) ≜ E\n\n\n\n1w\n\nσ2\n\n2y + σ2 σ2 1 + σ2\n\n2\n\n+\n\nσ2 1\n1 + σ2\n\n2\n\nσ2\n\n2\n\n( D I k σ2 1 +σ2 2 −1( D I k\n\nσ2\n\n2\n\n1 +σ2\n\n2\n\n∥y − w∥)\n\n∥y − w∥)\n\nD ∥y − w∥\n\n(y − w) − x\n\n(cid:33)2 \n\n=\n\n2σ2 1σ2 σ2 1 + σ2\n\n2\n\n2\n\n+\n\n(cid:19)2\n\n(cid:18) σ2 σ2\n\n1\n\n1 + σ2 2\n\n\n· E\n\ny−w∼N (t,σ2\n\n1 +σ2 2 )\n\n\n\n(cid:32) I k\n\n2\n\n( D σ2 1 +σ2 2 −1( D I k\n\nσ2\n\n2\n\n1 +σ2\n\n2\n\n∥y − w∥)\n\n∥y − w∥)\n\nD ∥y − w∥\n\n(y − w) − t\n\n(cid:33)2  .\n\n(33)\n\nLemma B.3 (Minimax Theorem Marchand & Perron (2002)). The unique Bayes estimator ˆx∗ is also the unique minimax estimator when\n\nmax t\n\nR ˆx∗ (t) =\n\n(cid:90)\n\nR ˆx∗ (t)dπ(t).\n\nLet R′(t) ≜ E\n\n(y−w)∼N (t,σ2\n\n1 +σ2 2 )\n\n(cid:34)(cid:18) I k\n\n2\n\n( D 1 +σ2 σ2 2\n( D σ2\n\n1 +σ2\n\n2\n\nI k 2\n\n−1\n\n∥y−w∥)\n\n∥y−w∥)\n\nD\n\n∥y−w∥ (y − w) − t\n\n(cid:19)2(cid:35)\n\nLemma B.4 (Berry (1990)). When D/(cid:112)σ2\n\n√\n\nk,\n\n1 + σ2 2 ≤ (cid:90)\n\nR′(t) =\n\nR′(t)dπ(t)\n\nmax t\n\nBased on Lemma B.3 and Lemma B.4, ˆx∗ is the minimax estimator.\n\n14\n\n.\n\n(34)\n\nUnder review as a conference paper at ICLR 2023\n\nLemma B.1 reveals that when two Gaussian observations contain a prior knowledge that their centers have a maximum distance, the optimal estimator is a linear combination of the observations, where the combining coefficient is related to the variances of the two observations, the maximum center distance, and the dimension of observations. For Formulation 1 (6), we can define a random vector u, v ∈ R|X ||Y|, where for all (x, y) ∈ X × Y,\n\nand\n\nu(x, y) ≜\n\nˆP (T ) XY (x, y) (cid:112)R(x, y)\n\n,\n\nv(x, y) ≜\n\nˆP (S) XY (x, y) (cid:112)R(x, y)\n\n.\n\n(35)\n\n(36)\n\nTheir centers are u0(x, y) ≜ P (T ) u ∼ N (u0, 1 nT\n\nI|X ||Y|) and v ∼ N (v0, 1\n\nXY (x,y) √\nR(x,y)\n\nnS\n\nand v0(x, y) ≜ P (S)\n\nXY (x,y) √\nR(x,y)\n\n. According to (7), we have\n\nI|X ||Y|) , and problem (6) can be re-defined as\n\nmin w(u,v)\n\nmax v0:∥v0−u0∥2≤D2\n\nE (cid:2)∥u0 − w∥2(cid:3) .\n\n(37)\n\nWith Lemma B.1, we derive Theorem 3.1. For Formulation 2 (8), we can define a random vector u, v ∈ R|X ||Y|, where for all (x, y) ∈ X × Y, (cid:113)\n\nu(x, y) ≜\n\nˆP (T )\n\nXY (x, y),\n\nand\n\n(cid:113)\n\nv(x, y) ≜\n\nˆP (S)\n\nXY (x, y). (cid:113)\n\nTheir centers are u0(x, y) ≜ u ∼ N (u0,\n\n1 2nT\n\nI|X ||Y|) and v ∼ N (v0, 1\n\n2nS\n\n(cid:113)\n\nP (T )\n\nXY (x, y) and v0(x, y) ≜\n\nP (S)\n\nXY (x, y). According to (9), we have\n\nI|X ||Y|) , and problem (8) can be re-defined as\n\nmin w(u,v)\n\nmax\n\nv0: 1\n\n2 ∥v0−u0∥2≤D2\n\nE\n\n(cid:20) 1 2\n\n∥u0 − w∥2\n\n(cid:21)\n\n.\n\n(40)\n\nWith Lemma B.1, we derive Theorem 3.3.\n\nC INTERPRETATION OF THEOREM 3.1\n\nFirstly, Remark 3.2 leads to that α1 ≃ nS we have\n\nnT +nS\n\n(cid:16)\n\n1 − nT nS D2\n\n(nS +nT )|X ||Y|\n\n(cid:17)\n\n. Considering that |X ||Y| ≪ D2,\n\nα1 ≃\n\nnS nT + nS\n\n1\n\n1 + nT nS D2\n\n(nS +nT )|X ||Y|\n\n=\n\n|X ||Y| nT\n\n|X ||Y| nT + |X ||Y|\n\nnS\n\n,\n\n+ D2\n\n(41)\n\nwhich is close to Eq.(6) in Tong et al. (2021).\n\nA geometric explanation for Theorem 3.1 can be depicted in figure 3, where the entire space represents all the distributions supported by X × Y. Two balls centered at the target and source distributions represent the empirical distribution sets PnT and PnS . In particular, the radii of the balls show the variances of the empirical distributions, which are inversely proportional to the sample sizes. The area with a deeper color contains those paracentral empirical distributions, which have higher probability according to (7). In addition, the distance between the centers is determined by the task distance assumption. The minimax problem (6) is meant to find estimator Q(1) XY , which is closest to the target distribution in average, where the dash line implies the linear family of ˆP (T ) XY . When the target sample size increases, the blue ball will shrink. When distance D increases, two balls will be farther away. Then, ˆP (T ) XY would be closer to the target distribution, which implies a smaller coefficient α1.\n\nXY and ˆP (S)\n\n15\n\n(38)\n\n(39)\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: A geometrical explanation of the minimax setting (6). Two balls centered at the underlying distributions represent all possible empirical distributions.\n\nD PROOF OF THEOREM 4.1 AND THEOREM 4.2\n\nWithout loss of generality, we assume E the average feature when computing the feature of each x.\n\nP (T )\n\nX\n\n[f (X)] = 0. It can be easily guaranteed by deducting\n\nWe denote ΛT ≜ E\n\nP (T )\n\nX\n\n[f ∗(X)f ∗T(X)] and ΛS ≜ E\n\nP (S)\n\nX\n\n[f ∗(X)f ∗T(X)].\n\nD.1 REVISED FORMULATION 1\n\nFor revised Formulation 1 (15), first, we give the expression of ˆgi as defined in (14) Tong et al. (2021)\n\nˆgi(y) =\n\n1 P (T )\n\nY (y)\n\nΛ−1\n\nT\n\n(cid:32)\n\n(cid:88)\n\nx∈X\n\n(cid:33)\n\nˆP (i)\n\nXY (x, y)f ∗(x)\n\n.\n\nThen, we can define a random vector u, v ∈ Rd|Y|, where\n\nand\n\nu ≜\n\nv ≜\n\n(cid:20)(cid:113)\n\nP (T )\n\nY (1)Λ\n\n1 2\n\nT ˆgT\n\nT (1), · · · ,\n\n(cid:113)\n\nP (T )\n\nY (|Y|)Λ\n\nT ˆgT\n\nT (|Y|)\n\n1 2\n\n(cid:21)T\n\n(cid:20)(cid:113)\n\nP (T )\n\nY (1)Λ\n\nT ˆgT\n\nS (1), · · · ,\n\n1 2\n\n(cid:113)\n\nP (T )\n\nY (|Y|)Λ\n\nT ˆgT\n\nS (|Y|)\n\n1 2\n\n(cid:21)T\n\n,\n\n.\n\nTheir centers are\n\n(cid:20)(cid:113)\n\nu0 ≜\n\nand\n\nP (T )\n\nY (1)Λ\n\nT gT\n\nT (1), · · · ,\n\n1 2\n\n(cid:113)\n\nP (T )\n\nY (|Y|)Λ\n\n(cid:21)T\n\n1 2\n\nT gT\n\nT (|Y|)\n\n(cid:20)(cid:113)\n\nv0 ≜\n\nP (T )\n\nY (1)Λ\n\nWith (7), we have\n\n1 2\n\nT gT\n\nS (1), · · · ,\n\n(cid:113)\n\nP (T )\n\nY (|Y|)Λ\n\nT gT\n\nS (|Y|)\n\n1 2\n\n(cid:21)T\n\n,\n\n.\n\nu ∼ N (u0,\n\n1 nT\n\nId|Y|), v ∼ N (v0,\n\n1 nS\n\nId|Y|).\n\nProblem (15) can be re-defined as\n\nmin w(u,v)\n\nmax v0:∥v0−u0∥2≤D2\n\nE (cid:2)∥u0 − w∥2(cid:3) ,\n\nand thus Theorem 4.1 is proved.\n\n16\n\n(42)\n\n(43)\n\n(44)\n\n(45)\n\n(46)\n\n(47)\n\n(48)\n\nUnder review as a conference paper at ICLR 2023\n\nD.2 REVISED FORMULATION 2\n\nFor revised Formulation 2 (15), we give the expression of ˆhi as defined in (19), (cid:33)\n\n(cid:32)\n\nˆhi(y) =\n\n(cid:113)\n\n1\n\nΛ−1\n\ni\n\nP (i)\n\nY (y)\n\n(cid:113)\n\n(cid:88)\n\nx∈X\n\nˆP (i)\n\nXY (x, y)P (i)\n\nX (x)f ∗(x)\n\nThen, we can define a random vector u, v ∈ Rd|Y|, where\n\nu ≜\n\n(cid:20)(cid:113)\n\nP (T )\n\nY (1)Λ\n\n(cid:113)\n\nˆhT\n\nT (1), · · · ,\n\nP (T )\n\nY (|Y|)Λ\n\n1 2\nT\n\n(cid:21)T\n\nˆhT\n\nT (|Y|)\n\n1 2\nT\n\n.\n\n,\n\nand\n\n(cid:20)(cid:113)\n\nv ≜\n\nP (S)\n\nY (1)Λ\n\n1 2\nS\n\n(cid:113)\n\nˆhT\n\nS (1), · · · ,\n\nP (S)\n\nY (|Y|)Λ\n\n1 2\nS\n\nˆhT\n\nS (|Y|)\n\n(cid:21)T\n\n.\n\nTheir centers are\n\n(cid:20)(cid:113)\n\nu0 ≜\n\nand\n\nP (T )\n\nY (1)Λ\n\nT hT\n\nT (1), · · · ,\n\n1 2\n\n(cid:113)\n\nP (T )\n\nY (|Y|)Λ\n\n(cid:21)T\n\n1 2\n\nT hT\n\nT (|Y|)\n\n(cid:20)(cid:113)\n\nv0 ≜\n\nWith (7), we have\n\nP (S)\n\nY (1)Λ\n\nS hT\n\nS (1), · · · ,\n\n1 2\n\n(cid:113)\n\nP (S)\n\nY (|Y|)Λ\n\nS hT\n\nS (|Y|)\n\n1 2\n\n(cid:21)T\n\n,\n\n.\n\nu ∼ N (u0,\n\n1 2nT\n\nId|Y|), v ∼ N (v0,\n\n1 2nS\n\nId|Y|).\n\nProblem (15) can be re-defined as\n\nmin w(u,v)\n\nand the optimal estimator is\n\nmax\n\nv0: 1\n\n2 ∥v0−u0∥2≤D2\n\nE\n\n(cid:20) 1 2\n\n∥u0 − w∥2\n\n(cid:21)\n\n,\n\nNote that w here refers to the vector\n\n(1 − ̃α2)u + ̃α2.\n\n(cid:20)(cid:113)\n\nP (T )\n\nY (1)Λ\n\nT hT(1), · · · ,\n\n1 2\n\n(cid:113)\n\nP (T )\n\nY (|Y|)Λ\n\nT hT(|Y|)\n\n1 2\n\n(cid:21)T\n\n.\n\nThus, Theorem 4.1 is proved.\n\nE DETAILS OF ALGORITHM IMPLEMENTATIONS\n\nE.1 ALGORITHM 1\n\n(49)\n\n(50)\n\n(51)\n\n(52)\n\n(53)\n\n(54)\n\n(55)\n\n(56)\n\nHere we provide the details of the loss function L1(α, f , g) in line 4 of Algorithm 1, the quantity D and the quantity ̃D1 in line 5. Our main procedures follow the results in Xu & Huang (2020) As for the zero-mean assumption of f ∗, which results in zero-mean weights g, we first define ̃f (X) ≜ f (X) − E\n\n[f (X)] and ̃g(Y ) ≜ g(Y ) − E\n\n[g(Y )].\n\nˆP (T )\n\nX\n\nˆP (T )\n\nY\n\nThen, we define ˆΛf and ˆΛg as the covariance matrices of features on target samples:\n\nˆΛf ≜ E\n\nˆP (T )\n\nX\n\n[ ̃f (X) ̃f T(X)],\n\nˆΛg ≜ E\n\nˆP (T )\n\nY\n\n[ ̃g(Y ) ̃gT(Y )].\n\n17\n\n(57)\n\n(58)\n\nUnder review as a conference paper at ICLR 2023\n\nIn our implementations, all the computations in terms of the underlying distribution are replaced by the corresponding empirical distributions. As proved in Xu & Huang (2020), minimizing χ2\n\nY (y) (cid:0)1 + f T(x)g(y)(cid:1)) is equivalent to maximizing\n\nY |X (y|x) ≜ ˆP (T )\n\nY |X ) ( ˆQ(f ,g)\n\nXY , ˆP (T )\n\nM ( ˆP (i)\n\nˆQ(f ,g)\n\nX\n\nH (i)(f , g) ≜ E\n\nˆP (i)\n\nXY\n\n[ ̃f T(X) ̃g(Y )] −\n\n1 2\n\ntr( ˆΛf ˆΛg),\n\n(59)\n\nwhere i = T, S. Then, line 4 in Algorithm 1 can be implemented by\n\nL1(α, f , g) ← (1 − α)H (T )(f , g) + αH (S)(f , g). Meanwhile, the distance bound D is also estimated from samples. As the simplest way, we let D = ̃D1 (D can actually be adjusted), where ̃D2\n\n(60)\n\n1 = χ2\n\nM (P (T )\n\nX Q(f ∗,ˆgS ) Y |X )\n\nX Q(f ∗,ˆgT ) Y |X , P (T ) (cid:18)\n\n←\n\n(cid:88)\n\ny∈Y\n\nˆP (T )\n\nY (y)\n\nE\n\nˆP (T )\n\nX|Y =y\n\n[ ̃f (X)] − E\n\nˆP (S)\n\nX|Y =y\n\n(cid:19)T\n\n[ ̃f (X)]\n\nˆΛ−1\n\nf\n\n(cid:18)\n\nE\n\nˆP (T )\n\nX|Y =y\n\n[ ̃f (X)] − E\n\nˆP (S)\n\nX|Y =y\n\n(cid:19)\n\n[ ̃f (X)]\n\n.\n\n(61)\n\nE.2 ALGORITHM 2\n\nHere we provide the details of the loss function L2(f , h1, h2) in line 2 of Algorithm 2, the quantity D and the quantity ̃D2 in line 3. Similarly, we define ̃f1(X) ≜ f (X) − E h1(Y ) − E\n\n[g1(Y )], and ̃h2(Y ) ≜ h2(Y ) − E\n\n[f (X)], ̃f2(X) ≜ f (X) − E\n\n[f (X)], ̃h1(Y ) ≜\n\n[h2(Y )].\n\nˆP (T )\n\nˆP (S)\n\nX\n\nX\n\nˆP (T )\n\nY\n\nˆP (S)\n\nY\n\nThen the covariance matrices are\n\nˆΛf1\n\nˆΛf2\n\nˆΛg1\n\nˆΛg2\n\n≜ E\n\nˆP (T )\n\nX\n\n≜ E\n\nˆP (S)\n\nX\n\n[ ̃f1(X) ̃f T\n\n1 (X)],\n\n[ ̃f2(X) ̃f T\n\n2 (X)],\n\n≜ E\n\nˆP (T )\n\nY\n\n≜ E\n\nˆP (S)\n\nY\n\n[ ̃g1(Y ) ̃gT\n\n1 (Y ),\n\n[ ̃g2(Y ) ̃gT\n\n2 (Y ).\n\n(62)\n\n(63)\n\n(64)\n\n(65)\n\nStill, all the computations in terms of the underlying distribution are replaced by the corresponding empirical distributions. Considering the local approximation that P (i) Y (y) , the Hellinger distance loss function H 2 (cid:16) ˆP (T )\n\ncan be implemented by maximizing\n\nXY (x, y) ∼ P (i)\n\nX (x)P (i)\n\n ̃Q(T,f ,h1)\n\n(cid:17)\n\nXY , P (T )\n\nX\n\nY |X\n\nSimilarly,\n\n ̃H (T )(f , h1) =\n\n(cid:88)\n\nx∈X ,y∈Y\n\n ̃H (S)(f , h2) =\n\n(cid:88)\n\nx∈X ,y∈Y\n\nˆP (T )\n\nXY (x, y) ̃f T\n\n1 (x) ̃g1(y) −\n\nˆP (S)\n\nXY (x, y) ̃f T\n\n2 (x) ̃g2(y) −\n\n1 2\n\n1 2\n\ntr( ˆΛf1\n\nˆΛg1 ).\n\ntr( ˆΛf2\n\nˆΛg2).\n\nLine 2 in Algorithm 2 can be implemented by\n\nL2(f , h1, h2) ← ̃H (T )(f , h1) + ̃H (S)(f , h2).\n\nWe let D = ̃D2 (D can actually be adjusted), where\n\n ̃D2\n\n2 =\n\n1 2\n\n←\n\n1 2\n\n(cid:88)\n\ny∈Y\n\n(cid:88)\n\ny∈Y (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:113)\n\n(cid:113)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nP (T )\n\nY (y)Λ\n\n1 2\nT\n\nˆhT (y) −\n\n(cid:113)\n\nP (S)\n\nY (y)Λ\n\n1 2\nS\n\nˆhS(y)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\nˆP (T )\n\nY (y) ˆΛ\n\n1 2\nf1\n\n ̃h1(y) −\n\n(cid:113)\n\nˆP (S)\n\nY (y) ˆΛ\n\n1 2\nf2\n\n ̃h2(y)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n,\n\n18\n\n(66)\n\n(67)\n\n(68)\n\n(69)\n\nUnder review as a conference paper at ICLR 2023\n\nand\n\nΛT ← ˆΛf1 ,\n\nΛS ← ˆΛf2.\n\n(70)\n\n(71)\n\nSpecifically, to compute the value of Bessel functions, we make the following approximations when needed. When x ≪ ν, Jν(x) ∼ x/v. When x ≫ ν, Jν(x) ∼ (2x − ν)/(2x).\n\nF INSTRUCTION FOR CODES\n\nWe provide code examples in “supplementary material.zip”. In the folder “./cifar10”, we provide the code examples on CIFAR-10 dataset for feature extraction (construct feature vectors.py), MM-χ2 algorithm (mmchi2.py), and MM-Hel algorithm (mmhel.py). Folder “./office-31/feature” contains the features of Office-31 dataset. Folder “./office-31/minimax” contains the code examples for Office-31 dataset, including the feature extraction code (feature extract 4096.py), MM-χ2 algorithm (oc atod vgg16 4096 chi2.py), and MM-Hel algorithm (oc atod vgg16 4096 hel.py).\n\n19",
  "translations": [
    "# Summary Of The Paper\n\nThis paper studies transfer learning from the aspect of the minimax principle. Accordingly, the strategies of minimizing the worst-case EPR based on $\\chi^2$-distance and Hellinger distance are proposed for transfer learning. The effectiveness of the proposed algorithms is supported by empirical results on several benchmark data sets.\n\n# Strength And Weaknesses\n\nStrengths:\n\n1.  This paper investigates transfer learning as a minimax problem, which is interesting and novel. \n\n2. The proposed robust transfer learning algorithms are theoretically justified.\n\n3. The paper is well-organized\n\nWeaknesses:\n\n1. The claims made in the paper are not clearly verified. For example, in the abstract, it is said the similarity between the source and target domains is “difficult to be precisely captured”. However, after reading the paper, it is still not clear to me compared to existing quantities, how and why the $\\chi^2$-distance and Hellinger distance can “precisely” capture the similarity. Moreover, it is unclear to me why the boundedness assumption is weaker than existing assumptions and notions of similarity (e.g., $\\lambda$ and $H$-divergence in [1]). More specifically, I didn't any analysis of the generalization bound of the proposed method. Given that, how can we conclude whether the assumption is \\emph{milder } or the proposed estimator can \\emph{precisely} capture the similarity?\n\n2. The problem setting studied in this paper is supervised transfer learning, where the label information is available in the target domain. However, it seems that the related work in this field is not reviewed (i.e., theoretical analysis of supervised transfer learning).\n\n3. Another major concern comes from the empirical results, which are quite weak from my aspect. In particular, Office-Caltech and Office-31 are relatively easy tasks compared with more realistic datasets such as Office-Home, VisDA, and mention DomainNet. I believe these (at least the first two) are standard benchmarks for transfer learning and domain adaptation in this field. In addition, the baselines adopted in this paper are not strong enough. For example, in [2], the average accuracies on Office-Caltech and Office-31 are 93% and 89.6%, much higher than the accuracies reported in this paper. Lastly, it is not clear to me how the robustness is empirically verified in the paper\n\n[1] Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., & Vaughan, J. W. (2010). A theory of learning from different domains. Machine learning, 79(1), 151-175.\n\n[2] Wang, Q., & Breckon, T. (2020). Unsupervised domain adaptation via structured prediction based selective pseudo-labeling. In Proceedings of the AAAI conference on artificial intelligence.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well organized, but the main contributions seem overstated.\n\n# Summary Of The Review\n\nWhile applying the minimax principle to transfer learning is an interesting idea, I encourage the authors to address my concerns and comments before it is ready to publish.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents a novel approach to transfer learning by proposing a boundedness assumption on task similarity and a mathematical framework based on the minimax principle. The authors formulate a minimax problem that aims to minimize the worst-case expected population risk (EPR) while considering the bounded task distance. The framework leads to analytical solutions that provide insights into the effects of sample sizes, task distances, and model dimensionality on knowledge transfer. The proposed algorithms, MM-χ² and MM-Hel, were empirically validated through experiments on CIFAR-10, Office-31, and Office-Caltech datasets, demonstrating competitive performance in image classification tasks.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its theoretical contributions, which provide a solid foundation for understanding task similarity in transfer learning through a minimax framework. The analytical solutions for different distance measures, χ²-distance and Hellinger distance, are well-articulated and offer valuable insights into combining source and target task distributions. However, a potential weakness is the reliance on specific distance measures, which may limit the applicability of the proposed methods to other scenarios or types of data distributions. Additionally, while the empirical results are promising, further exploration of the algorithms' performance across various domains would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the theoretical foundations and empirical results. The methodology is described in detail, allowing for reproducibility of the proposed algorithms. The novelty of applying the minimax principle to quantify task similarity in transfer learning is significant and addresses a critical gap in existing literature. However, the clarity could be improved in some sections, particularly in explaining the implications of the theoretical findings for practical applications.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of transfer learning by introducing a robust theoretical framework based on the minimax principle. The analytical solutions provided enhance understanding of task similarity, and the empirical results demonstrate the effectiveness of the proposed algorithms. However, further validation across diverse datasets and conditions would enhance the robustness of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Robust Transfer Learning Based on Minimax Principle\" proposes a novel framework for transfer learning that leverages the minimax principle to effectively estimate the similarity between target and source tasks. The main contributions include the introduction of a boundedness assumption for task similarity, the development of a minimax framework utilizing distance measures such as χ²-distance and Hellinger distance, and the derivation of analytical solutions for minimax problems in both discrete and continuous settings. Empirical results on standard datasets (CIFAR-10, Office-31, and Office-Caltech) demonstrate that the proposed algorithms, MM-χ2 and MM-Hel, achieve competitive accuracy and robustness, particularly as the target sample size increases.\n\n# Strength And Weaknesses\nStrengths of the paper include its solid theoretical foundation, which enhances the understanding of task similarity in transfer learning, and the derivation of analytical solutions that provide practical insights. The robustness of the proposed algorithms is well-validated through empirical evaluations across diverse datasets, showcasing their effectiveness in real-world applications. However, the key limitation lies in the boundedness assumption, which may not be applicable in all scenarios, potentially restricting the generalizability of the findings. Additionally, the focus on specific distance measures may limit the applicability of the methods to other contexts that could benefit from different metrics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly. The theoretical formulations and algorithmic developments are articulated with sufficient detail, allowing for reproducibility. The novelty of the approach is significant, as it addresses a critical gap in transfer learning by providing a fresh perspective on task similarity estimation. However, the reliance on specific distance measures may affect the broader applicability of the findings, which could be a concern for future researchers.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of transfer learning by introducing a minimax-based framework that is both theoretically sound and empirically validated. While the assumptions and specific distance measures may limit the broader applicability of the findings, the robustness of the proposed algorithms presents a noteworthy advancement in the area.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel mathematical framework for transfer learning based on the minimax principle, which focuses on minimizing the worst-case expected population risk under a bounded similarity assumption between source and target tasks. The authors formulate a robust transfer learning problem using distance measures (χ²-distance and Hellinger distance) that allow for analytical solutions. They demonstrate the effectiveness of their approach through extensive experiments on popular datasets such as CIFAR-10 and Office-31, showing that their MM-χ² and MM-Hel algorithms achieve competitive performance depending on the sample size and task distance.\n\n# Strength And Weaknesses\nThe paper introduces a significant advancement in transfer learning by leveraging a minimax framework that addresses the limitations of previous methodologies that relied on overly strong assumptions. The analytical solutions provided for both discrete and continuous data cases contribute to the theoretical foundation of the proposed approach. However, the paper could benefit from a more comprehensive exploration of the practical implications and limitations of the proposed algorithms, particularly in real-world scenarios where task similarity may not be easily quantifiable.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear definitions and logical progression through the problem formulation, theoretical analyses, and experimental validation. The quality of the writing is high, making complex concepts accessible. The novelty lies in the application of the minimax principle to transfer learning, which is a relatively unexplored area. The reproducibility of the results is supported by the detailed description of the experimental setup and the datasets used, although supplementary material or code availability would enhance this aspect.\n\n# Summary Of The Review\nThis paper provides a robust and innovative framework for transfer learning that incorporates the minimax principle, yielding analytical solutions that are substantiated by experimental results. While the theoretical contributions are strong, further exploration of practical implications could enhance the overall impact of the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel minimax framework for transfer learning that introduces a boundedness assumption on task similarity, allowing for greater flexibility compared to traditional approaches. The authors develop an analytical solution using χ²-distance and Hellinger distance to minimize worst-case expected population risk, thereby establishing a robust theoretical foundation. Empirical validation on image classification tasks indicates that the proposed algorithms yield competitive performance, although the experiments are limited to specific datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of weak assumptions on task similarity, which enhances the applicability of the proposed framework. The minimax framework itself is a significant contribution, offering a clear theoretical basis for transfer learning. However, the complexity of the minimax problem may hinder practical applications, particularly in high-dimensional settings. While the analytical solution aids in understanding, its reliance on specific distance measures could restrict general applicability. The extension to continuous data is valuable, yet it raises concerns about additional unexamined assumptions. Moreover, the empirical validation, although promising, is confined to a limited range of datasets, which may affect the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, with a logical flow of concepts. The theoretical contributions are novel and significant, particularly in the context of transfer learning. However, the reproducibility of results could be improved by providing more details on the experimental setup and broader empirical validation across diverse datasets and tasks.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of transfer learning by introducing a flexible minimax framework and providing robust theoretical insights. However, the limitations in empirical validation and the reliance on specific assumptions suggest that further research is needed to enhance the framework's applicability and generalizability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents a novel approach to transfer learning by employing a minimax framework aimed at enhancing robustness in learning from source to target tasks. The authors establish a boundedness assumption regarding task similarity and derive an analytical solution that minimizes the worst-case expected population risk (EPR). Key contributions include the development of a minimax methodology for transfer learning, the provision of analytical solutions that simplify model design, and a robust experimental validation across popular image classification tasks, demonstrating competitive performance against established methods.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative minimax framework, which addresses critical challenges in transfer learning, such as limited labeled data and task similarity. The analytical solutions provided are a significant advancement, offering clarity and ease of implementation for practitioners. However, a notable weakness is the lack of a thorough discussion on the limitations of the minimax approach, particularly in cases with heterogeneous source-target task distributions. Additionally, the comparison to a broader range of existing methodologies could further substantiate the claims of superiority.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its ideas clearly, making the complex relationships between variables more accessible. The novelty of the minimax approach in transfer learning is evident, and the analytical nature of the solution contributes to reproducibility, as the methodology is clearly defined and derivable. Nonetheless, further elaboration on practical limitations and comparisons could enhance the clarity and impact of the findings.\n\n# Summary Of The Review\nThis paper introduces a compelling and novel approach to transfer learning through a minimax framework, providing both theoretical and empirical contributions. While the methodology is sound and well-validated, additional discussions on limitations and broader comparisons could strengthen the overall impact of the work. The paper is recommended for acceptance with minor revisions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Robust Transfer Learning Based on Minimax Principle\" introduces a novel approach to adversarial training that employs a minimax optimization framework to enhance model robustness against adversarial attacks. The authors formulate a minimax problem that minimizes the worst-case expected loss under adversarial perturbations, leading to more reliable model performance. Key contributions include a theoretical foundation for minimax adversarial training, analytical solutions for adversarial perturbation bounds, insights into the impact of adversarial distances, practical algorithms for implementation, and extensive empirical validation across various datasets demonstrating improved adversarial accuracy.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its theoretical rigor and practical relevance, providing a solid mathematical foundation for adversarial robustness and algorithms that can be directly applied in real-world scenarios. The comprehensive evaluation across multiple datasets further strengthens the authors' claims regarding the effectiveness of their approach. However, a notable weakness is the complexity of implementing the proposed algorithms, which may pose challenges for practitioners lacking a strong mathematical background. Additionally, the focus on image classification tasks limits the scope of evaluation, suggesting a need for further exploration in diverse domains such as natural language processing or reinforcement learning to enhance generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and presents its theoretical contributions clearly, with a strong emphasis on mathematical rigor. The quality of the writing is high, making complex concepts accessible, though some sections could benefit from more detailed implementation guidelines. The novelty of applying a minimax framework to adversarial training is significant, offering a fresh perspective in the field. Reproducibility is supported by extensive experimental results, although additional details on practical implementation would enhance confidence in replicating the findings.\n\n# Summary Of The Review\nOverall, this paper presents a substantial advancement in adversarial training by framing it within a minimax context. The theoretical contributions are robust and relevant, while the practical implications suggest promising avenues for real-world applications. Minor revisions are needed to clarify implementation details and to broaden the scope of experimental validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Robust Transfer Learning Based on Minimax Principle\" proposes a novel mathematical framework aimed at transforming transfer learning methodologies. The authors introduce a minimax principle designed to minimize worst-case population risk, claiming that their analytical solutions provide a universal guideline for future models in this domain. However, the methodology relies on what are described as mild assumptions about task distance, and while the authors present their findings as significant advancements, many aspects of their approach appear to be incremental rather than groundbreaking.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to formalize transfer learning through the minimax principle, which could provide useful insights if properly contextualized. However, the weaknesses are pronounced: the authors overstate the significance of their contributions, relying on trivial assumptions and providing complex formulations that do not add substantial value. The experimental results are underwhelming, with claims of robust performance not sufficiently supported by data, indicating a selective reporting bias. Overall, the paper lacks the depth and novelty expected in a major publication.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by overly complex notation and definitions, which obscure the simplicity of the findings. The quality of the contributions is questionable, as many claims are not robustly supported by empirical evidence. In terms of novelty, while the minimax principle is a well-known concept, the authors fail to provide a truly innovative application to transfer learning. Reproducibility appears feasible given the methodologies described; however, the lack of comprehensive experimental detail may limit practical implementation.\n\n# Summary Of The Review\nOverall, the paper presents a potentially interesting framework for transfer learning but suffers from significant issues related to clarity, overstatements of novelty, and insufficient empirical support. The contributions, while framed as transformative, appear largely derivative and incremental.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to transfer learning based on a minimax framework that incorporates a boundedness assumption regarding task similarity. By utilizing this principle, the authors aim to minimize the worst-case expected population risk when transitioning knowledge from source tasks to target tasks. The methodology includes defining a minimax formulation to estimate target distributions and deriving analytical solutions for different distance measures (χ2-distance and Hellinger distance). Empirical results on image classification datasets, such as CIFAR-10 and Office-31, demonstrate that the proposed methods yield robust performance, although subsequent analysis reveals more conservative estimates of their efficacy.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of a clear mathematical framework that allows for the robust design of transfer learning models and the derivation of analytical solutions, which can be beneficial for practitioners. The exploration of the influences of sample sizes, model dimensionality, and task distances adds depth to the analysis. However, weaknesses are apparent in the experimental results, which, upon scrutiny, show a less competitive performance than initially reported, indicating potential overstatements in the robustness of the proposed algorithms. The altered findings suggest that the algorithms may not generalize well across all transfer settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, presenting a clear progression from theory to application. The clarity of the mathematical formulations is commendable, making the theoretical contributions accessible. However, the reproducibility of the experiments may be compromised due to the adjustments in reported results, which raise concerns about the reliability of the empirical findings. While the core idea is novel, the empirical evidence supporting its significance is less robust than initially claimed.\n\n# Summary Of The Review\nOverall, the paper introduces a promising theoretical framework for transfer learning based on the minimax principle, with well-defined mathematical formulations. However, the empirical results suggest a more cautious interpretation of the proposed algorithms' effectiveness, indicating that their robustness may be overstated. Further validation of the experimental findings is needed to support the claims made.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper investigates knowledge transfer in machine learning through a formalized framework grounded in the assumption of bounded task similarity. The authors apply the minimax principle to develop algorithms that leverage distance metrics (specifically χ2-distance and Hellinger distance) for effective transfer learning. The findings suggest that linear combinations of learning results can enhance transfer effectiveness, although the authors acknowledge limitations related to sample size and model dimensionality.\n\n# Strength And Weaknesses\nThe paper contributes to the understanding of transfer learning by articulating a structured approach that relies on bounded task similarity, which is frequently applicable in practice. However, several weaknesses are evident: the oversimplification of task similarity, reliance on specific distance metrics, and the robustness claims lack empirical validation across diverse scenarios. Furthermore, the assumption of linear relationships in knowledge transfer and the limited empirical validation on a narrow set of datasets constrain the paper's broader applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly, though some concepts require more detailed explanations, particularly regarding robustness and generalizability. While the novelty lies in the application of the minimax principle to transfer learning, the assumptions made about task similarity and distance metrics could benefit from further empirical exploration. Reproducibility is hindered by the limited datasets used in experiments, which may not sufficiently represent the diversity of real-world applications.\n\n# Summary Of The Review\nOverall, the paper presents a relevant approach to knowledge transfer in machine learning but is weakened by its reliance on oversimplified assumptions and limited empirical validation. The discussion around robustness and generalizability requires strengthening to enhance the paper's impact and applicability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a minimax framework for transfer learning that operates under the assumption of bounded task similarity. This theoretical foundation allows for analytical solutions aimed at minimizing the worst-case expected population risk. The methodology includes formulations that utilize χ²-distance and Hellinger distance, leading to the derivation of two practical algorithms, MM-χ² and MM-Hel, for robust transfer learning. The authors validate their framework through experiments on several datasets, including CIFAR-10, Office-31, and Office-Caltech, demonstrating competitive performance and robustness compared to existing methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel approach to defining task similarity, which addresses a significant limitation in existing transfer learning methodologies. By employing a minimax principle, the authors provide a robust theoretical framework that is both insightful and practically applicable. However, the paper could benefit from a more comprehensive discussion of the limitations associated with the bounded similarity assumption, as well as a deeper exploration of the implications of model dimensionality on the proposed algorithms.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the theoretical concepts accessible to readers. The quality of the writing is high, and the formulations are presented with sufficient detail for reproducibility. However, while the methods are theoretically sound, more extensive empirical comparisons with a broader range of existing methods would enhance the assessment of novelty and significance.\n\n# Summary Of The Review\nIn summary, this paper provides a significant contribution to the field of transfer learning by introducing a minimax framework that is both theoretically robust and empirically validated. While the approach is innovative and well-articulated, further exploration of its limitations could strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper addresses the challenge of data scarcity in supervised learning by proposing a novel semi-supervised learning framework that leverages generative adversarial networks (GANs) for data augmentation. The methodology involves a two-step process where the first step generates synthetic data through a pretrained GAN, and the second step fine-tunes a classifier using both the real and augmented datasets. The findings demonstrate that this approach significantly improves classification performance compared to traditional methods, particularly in scenarios with limited labeled data.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Approach:** The integration of GANs for data augmentation in semi-supervised learning is a fresh perspective that could advance the understanding of data scarcity issues in machine learning.\n2. **Robust Empirical Results:** The experiments conducted across multiple datasets show consistent improvements in classification accuracy, underscoring the proposed method's effectiveness.\n3. **Clear Methodology:** The two-step process is well-defined, allowing for easy understanding and potential replication by other researchers.\n\n**Weaknesses:**\n1. **Limited Benchmarking:** While the paper includes some comparisons to existing semi-supervised methods, it lacks a comprehensive evaluation against leading state-of-the-art techniques, which would provide a clearer context for its contributions.\n2. **Assumption on Data Distribution:** The reliance on the assumption that the synthetic data distribution is similar to the real data distribution may not hold in all cases, which could limit the generalizability of the findings.\n3. **Insufficient Discussion on Limitations:** The paper does not adequately address potential limitations related to the choice of GAN architecture and the impact of hyperparameter tuning on performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making the complex ideas accessible to a broad audience. The figures and tables effectively illustrate the results, although additional details regarding the experimental setup would enhance reproducibility. The novelty of applying GANs in this context is noteworthy, although the lack of extensive benchmarking slightly hampers the overall impact.\n\n# Summary Of The Review\nOverall, the paper presents a promising contribution to the field of semi-supervised learning by leveraging GANs for data augmentation. While the results are compelling and the methodology is clear, the work would benefit from deeper comparative analysis and a more thorough discussion of assumptions and limitations.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel mathematical framework for transfer learning, leveraging the minimax principle to address the challenge of capturing task similarity between source and target domains. By introducing a bounded similarity assumption, the authors provide a more realistic foundation for transfer learning compared to traditional, overly simplistic assumptions. The framework allows for analytical solutions to minimax problems in both discrete and continuous data settings, revealing how factors such as sample sizes, task distances, and model dimensionality impact knowledge transfer. The empirical results demonstrate that the proposed algorithms exhibit robustness and competitive accuracy in image classification tasks.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its theoretical contributions, particularly the introduction of a bounded similarity assumption, which enhances the understanding of task relationships in transfer learning. The use of the minimax principle to derive robust estimators is innovative and adds depth to the field. Furthermore, the analytical solvability of the minimax problems is a notable achievement that could facilitate further research. However, a potential weakness is the limited scope of empirical validation, as the experiments primarily focus on image classification tasks. It would be beneficial to see applications in a broader range of domains or tasks to fully assess the framework's generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The theoretical aspects are presented with sufficient rigor, making them accessible to the audience. The novelty of the bounded similarity assumption and its implications for transfer learning are clearly outlined. In terms of reproducibility, the paper provides practical algorithms based on the theoretical insights, yet more detailed descriptions of experimental setups and hyperparameter tuning would enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper provides significant theoretical advancements in transfer learning through the introduction of a bounded similarity assumption and the application of the minimax principle. While the empirical results are promising, the scope of applications could be expanded to better validate the proposed framework.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to transfer learning by introducing a minimax framework that addresses the challenge of quantifying task similarity between source and target tasks. The authors propose a boundedness assumption on task similarity and derive analytical solutions to minimize worst-case expected population risk. The experimental results on datasets such as CIFAR-10, Office-31, and Office-Caltech demonstrate that the proposed algorithms yield competitive performance and robustness, particularly in scenarios with varying sample sizes and task distances.\n\n# Strength And Weaknesses\nStrengths of the paper include its strong theoretical foundation, particularly the introduction of a minimax framework that is well-suited to tackle the complexities of transfer learning. The analytical solutions provided for both discrete and continuous data are valuable for guiding practical implementations. Additionally, the comprehensive empirical evaluation across multiple datasets illustrates the effectiveness of the proposed methods. However, the paper may benefit from a more detailed discussion on the limitations of the boundedness assumption and its implications on the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The use of precise mathematical notations and definitions aids in understanding the proposed framework. The novelty of applying the minimax principle in transfer learning is significant, and the analytical solutions enhance the paper's quality. While the experiments are thorough, the reproducibility could be improved by providing more details about the implementation and hyperparameter choices used in the algorithms.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of transfer learning by introducing a robust minimax framework that effectively addresses the challenge of task similarity. The theoretical and empirical results are promising, although there are minor areas for improvement in terms of clarity regarding assumptions and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a robust approach to transfer learning grounded in a minimax principle while considering bounded task similarity assumptions. It introduces a theoretical framework that utilizes χ2-distance and Hellinger distance to quantify the relationship between tasks, leading to the development of practical algorithms. Experimental results on standard datasets such as CIFAR-10 and Office-31 demonstrate the proposed method's robustness and competitive accuracy, validating the theoretical contributions through empirical evidence.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its novel application of the minimax principle to transfer learning, which addresses a significant gap in the existing literature concerning task similarity quantification. The methodology is rigorous, featuring analytical solutions for both discrete and continuous scenarios, which showcases the authors' deep understanding of the subject. However, a potential weakness is the lack of exploration into the broader implications of the bounded task similarity assumption beyond the specific experiments conducted. Additionally, while the empirical validation is robust, further real-world application examples could strengthen the argument for practical implications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulated, with logical flow and effective presentation of mathematical formulations. The clarity of figures and tables enhances the overall understanding of the results. The originality of the approach is commendable, and the supplemental materials provided, including proofs and algorithmic details, significantly contribute to reproducibility. Minor edits for clarity and conciseness in certain sections could improve the overall quality.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of transfer learning by introducing a novel minimax-based approach that effectively addresses bounded task similarity. The rigorous methodology and thorough experimental validation indicate strong potential for practical applications in real-world scenarios, although some areas could benefit from further exploration.\n\n# Correctness\n5/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Robust Transfer Learning Based on Minimax Principle\" introduces a novel framework for quantifying task similarity in transfer learning, addressing a key challenge in the field. The authors propose a boundedness hypothesis regarding task similarity, which enables the formulation of a minimax problem aimed at minimizing the worst-case expected population risk under this assumption. The paper presents analytical solutions to the proposed problem and derives empirical algorithms (MM-χ2 and MM-Hel) based on two distance measures: the referenced χ²-distance and Hellinger distance. Extensive experiments on datasets such as CIFAR-10 and Office-31 demonstrate the effectiveness and robustness of the proposed algorithms, confirming the theoretical insights regarding task distances, sample sizes, and their impact on knowledge transfer.\n\n# Strength And Weaknesses\nStrengths of the paper include its theoretical rigor in addressing the quantification of task similarity, which has been a longstanding challenge in transfer learning. The use of the minimax principle provides a solid mathematical foundation for the proposed algorithms, and the analytical solutions enhance the understanding of the relationships between critical variables in transfer learning. However, the paper could benefit from a more thorough exploration of the assumptions behind the boundedness hypothesis, as well as a discussion of its limitations. Additionally, while the empirical results are compelling, a comparison with existing state-of-the-art methods could further contextualize the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents complex mathematical concepts with clarity. The authors effectively communicate the theoretical underpinnings and algorithmic implementations, making the content accessible to readers familiar with transfer learning and optimization. The novelty of the approach lies in the formalization of the bounded task similarity and its implications for transfer learning algorithms. The reproducibility of the results is supported by detailed descriptions of the algorithms and experimental setups, although providing code or additional resources would enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper makes significant contributions to the field of transfer learning by providing a robust theoretical framework and practical algorithms based on the minimax principle. Its findings are well-supported by empirical evidence, although further discussions on the assumptions and comparisons with existing methodologies would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to transfer learning based on a minimax framework, positing that task similarity can be leveraged to enhance learning efficiency across related tasks. The authors claim to derive analytical solutions using bounded assumptions about task similarity and utilize distance measures such as χ2-distance and Hellinger distance to evaluate performance. Experimental results are provided to demonstrate the effectiveness of their proposed methodology; however, the scope of these experiments appears limited.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to formalize the relationship between task similarity and transfer learning through a minimax framework. However, the paper suffers from several weaknesses. The foundational assumptions regarding task similarity are overly simplistic and may not apply in complex real-world scenarios, raising concerns about the robustness of the proposed solutions. Additionally, the reliance on specific distance measures lacks creativity and does not convincingly establish their effectiveness in a variety of transfer learning contexts. The experiments are insufficiently comprehensive, lacking diversity in tasks and data distributions, which undermines the validity of the findings. Furthermore, the absence of critical self-analysis regarding potential limitations and the superficial discussion of theoretical implications detracts from the paper's overall contribution to the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, though the depth of discussion regarding theoretical insights and empirical results could be improved. Quality is compromised by the limited experimental scope and lack of comparisons with state-of-the-art methods, making it challenging to assess the reproducibility of the findings. The novelty of the approach is hindered by the reliance on established methodologies without significant advancements or innovations that contribute new insights to transfer learning.\n\n# Summary Of The Review\nWhile the paper introduces a structured approach to transfer learning using a minimax framework, its reliance on simplistic assumptions and limited empirical validation diminishes its contributions to the field. The lack of critical self-analysis and depth in theoretical discussion further limits its impact on advancing transfer learning methodologies.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel framework for transfer learning based on the minimax principle, which enhances the robustness of transfer learning models through a boundedness assumption on task similarity. The authors present analytical solutions to the minimax problems, enabling clearer model design guidelines, and develop practical algorithms demonstrating competitive accuracy and robustness across various tasks. Empirical validation on image classification tasks, particularly on datasets like CIFAR-10 and Office-31, indicates that the proposed methods achieve impressive performance even with random training set selections, showcasing their versatility in real-world applications.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach and the theoretical rigor it brings to transfer learning, coupled with practical algorithms that yield notable empirical results. The analytical nature of the proposed solutions provides a solid foundation for future research. However, weaknesses include the boundedness assumption, which may limit the applicability of the framework in scenarios where task similarity is not well-defined, potentially constraining its effectiveness in more diverse transfer learning settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the main contributions, with a logical progression from theoretical foundations to empirical validation. The novel approach based on the minimax principle is an exciting addition to the literature, although the reliance on boundedness assumptions may raise questions about generalizability. The reproducibility of results is supported by thorough experimentation on widely used datasets, making it easier for other researchers to validate and build upon this work.\n\n# Summary Of The Review\nThis paper makes a significant contribution to the field of transfer learning by introducing a robust framework based on the minimax principle, supported by analytical solutions and practical algorithms. While the boundedness assumption may limit its generalizability, the empirical results suggest strong potential for future applications. Overall, this research positions itself as a valuable advancement in both theoretical and practical aspects of transfer learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Robust Transfer Learning Based on Minimax Principle\" presents a theoretical framework for transfer learning that emphasizes the importance of task similarity between source and target tasks. The authors introduce a boundedness assumption to facilitate theoretical exploration while maintaining generality. They propose a minimax principle framework designed to minimize the worst-case expected population risk (EPR) and provide analytical solutions for the proposed minimax problems. Additionally, the paper extends theoretical analyses to continuous data, elucidates the influences of various factors such as sample sizes and task distances, and presents algorithms (MM-χ2 and MM-Hel) as practical applications of their theoretical insights.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its rigorous theoretical contributions, which enhance the understanding of task similarity and its implications for transfer learning. The introduction of the boundedness assumption and the minimax framework are significant advancements that provide a solid foundation for future research. However, the paper's focus on theoretical aspects may detract from its practical applicability, as the empirical results serve mainly as supporting evidence rather than a comprehensive evaluation of the algorithms' effectiveness in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its theoretical constructs clearly, with adequate justification for the choices made in terms of distance measures and algorithmic design. The novelty is significant, as the minimax approach introduces new avenues for understanding transfer learning dynamics. However, reproducibility may be limited due to the lack of detailed empirical evaluations or extensive experiments to validate the theoretical claims, which could raise concerns about the practical implementation of the proposed methods.\n\n# Summary Of The Review\nOverall, the paper offers a substantial theoretical contribution to the field of transfer learning through its minimax framework and boundedness assumption. While the theoretical insights are robust and well-articulated, the limited empirical validation may hinder its practical applicability. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel minimax framework for transfer learning aimed at improving the performance and adaptability of models across different tasks. The authors develop two algorithms—MM-χ2 and MM-Hel—utilizing χ2-distance and Hellinger distance, respectively, as computationally convenient measures for optimizing the transfer learning process. The methodology involves a detailed implementation of these algorithms, focusing on the optimization of parameters based on empirical distributions derived from target and source data samples. The findings suggest that the proposed methods are effective in enhancing transfer learning capabilities, with empirical results provided in the supplementary materials.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical contributions to the minimax problem in transfer learning and the practical algorithms developed, which are backed by detailed implementation steps. The use of two distance measures provides flexibility and robustness in the learning process. However, the paper could benefit from a more extensive empirical evaluation across various datasets to fully demonstrate the algorithms' effectiveness. Additionally, while the implementation details are thorough, the reliance on specific distance metrics may limit the generalizability of the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, making it accessible to readers familiar with transfer learning and minimax optimization. The quality of the writing is high, with a logical flow from theoretical foundations to practical applications. The novelty of the minimax approach in the context of transfer learning is noteworthy, although the reliance on specific distance measures may reduce its broader impact. The reproducibility of the results is supported by the provision of code examples and detailed implementation steps, facilitating practical application by other researchers.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of transfer learning through its innovative minimax framework and practical algorithms. While the theoretical and implementation aspects are well executed, additional empirical validation across diverse datasets would enhance the findings’ robustness and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a minimax framework for transfer learning, claiming to enhance robustness through a boundedness assumption on task similarity. The authors propose the use of χ2-distance and Hellinger distance for measuring task similarity and provide analytical solutions for their minimax problem. Experimental results on CIFAR-10 and other datasets are included to demonstrate the efficacy of their approach, although comparisons with state-of-the-art methods are limited.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to introduce a minimax approach to transfer learning, which may provide a theoretical foundation for future research. However, the paper has notable weaknesses, including a lack of novelty in its assumptions and methodologies, which have been addressed in existing literature. The experimental validation, while promising, lacks comprehensive benchmarking against state-of-the-art techniques, which raises questions about the robustness and practical applicability of the proposed algorithms.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is reasonable, but it could be improved by providing a more thorough review of related work and contrasting its contributions more explicitly. The quality of the methodology is sound but does not present significant advancements over prior research, diminishing its novelty. Reproducibility appears feasible based on the information provided, though additional details on experimental setups would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper seeks to contribute to the field of transfer learning through a minimax framework, but it fails to adequately distinguish itself from prior works. The contributions are incremental rather than groundbreaking, and the lack of thorough comparisons with existing methods weakens the overall impact of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach grounded in the Minimax Principle to improve the performance of transfer learning tasks. The authors propose a framework that utilizes a boundedness assumption to derive estimators that minimize expected population risk (EPR) under a distance constraint. Through extensive empirical evaluation, the findings demonstrate that the proposed method achieves robust and competitive accuracy across various datasets, outperforming existing baselines.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its rigorous theoretical foundation, which clearly delineates how the Minimax Principle can be applied to transfer learning. The methodology is well-structured, and the empirical results provide compelling evidence of the approach's effectiveness. However, the paper suffers from several clarity issues, particularly regarding terminology and notation consistency. Some definitions are not sufficiently clear, which may hinder reproducibility. Additionally, a more detailed comparison with state-of-the-art methods would strengthen the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the theoretical contributions are novel, the paper's clarity could be improved. The inconsistent notation and terminology, such as the definition of boundedness assumption and the notation for empirical distributions, detract from the overall readability. The methodology is reproducible, but the lack of clarity in the definitions and notation may pose challenges for readers attempting to implement the proposed approach. Furthermore, the empirical results are well-presented, but the discussion lacks depth regarding the implications of the findings.\n\n# Summary Of The Review\nOverall, the paper introduces a promising approach to transfer learning based on the Minimax Principle, supported by theoretical insights and empirical validation. However, clarity issues in notation and terminology may hamper the reader's understanding and the paper's broader impact. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework based on the minimax principle to address the boundedness assumption of task similarity in transfer learning. The authors propose algorithms that demonstrate competitive performance in image classification tasks, aiming to enhance transfer learning effectiveness. However, the study primarily focuses on discrete and continuous data types, neglecting mixed data scenarios, and does not explore the implications of varying task similarities or the algorithms' performance across different domains.\n\n# Strength And Weaknesses\nThe theoretical contributions of the paper provide valuable insights into the minimax formulation in transfer learning. However, the analysis is limited in scope, failing to address heterogeneous task similarities and the computational complexity of the proposed algorithms. Additionally, the experimental validation is constrained to specific datasets, primarily in image classification, without considering other domains such as natural language processing or time-series analysis. The lack of a comprehensive literature review and a discussion on ethical implications further diminish the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally satisfactory, though the discussions on robustness, interpretability, and empirical findings are not sufficiently developed. The novelty of the proposed minimax framework is notable; however, it lacks thorough empirical validation across diverse tasks and datasets, which raises concerns about reproducibility. The authors could enhance the quality of their findings by incorporating additional case studies, exploring the implications of biases in the training data, and addressing potential overfitting issues.\n\n# Summary Of The Review\nOverall, while the paper introduces an interesting framework for transfer learning based on the minimax principle, it falls short in empirical validation and discussion of practical implications. A more comprehensive exploration of task similarity, data types, and the broader landscape of transfer learning techniques would strengthen the paper's contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a robust transfer learning framework based on the minimax principle, aiming to minimize the worst-case expected population risk (EPR) under a boundedness assumption on task similarity. The authors derive analytical solutions for both discrete and continuous data cases, utilizing distance measures such as χ2-distance and Hellinger distance. Empirical validation is provided through experiments on datasets like CIFAR-10 and Office-31, showing that their proposed algorithms, MM-χ2 and MM-Hel, effectively optimize performance concerning sample sizes and task distances.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its introduction of a more generalized boundedness assumption for task similarity, which potentially expands the applicability of transfer learning methods. The analytical solutions derived for both discrete and continuous cases offer significant contributions to the theoretical understanding of minimax estimation in this context. However, a potential weakness is that the reliance on specific distance measures may limit the generality of the results. Additionally, while the experiments are thorough, a broader range of datasets could further validate the robustness of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings. The use of formal definitions and theorems enhances the clarity of the proposed minimax framework. The empirical results are presented with appropriate statistical rigor, including averages and standard deviations, which support reproducibility. However, details regarding the implementation of the algorithms could be elaborated to enhance reproducibility further.\n\n# Summary Of The Review\nThis paper presents a valuable contribution to the field of transfer learning by introducing a robust minimax framework that considers task similarity under bounded assumptions. While the theoretical insights and empirical validations are compelling, the reliance on specific distance measures may limit broader applicability. Overall, the work is a significant step forward in understanding robustness in transfer learning.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a new framework for transfer learning that relies on specific boundedness assumptions regarding task similarity. It provides analytical solutions for minimax formulations using distance measures such as χ2-distance and Hellinger distance. The findings suggest that the proposed algorithms can effectively address certain transfer learning tasks, particularly in image classification, though the applicability of the framework to a wider range of tasks remains uncertain.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical contributions, particularly the provision of analytical solutions for specific distance measures, which facilitates a deeper understanding of the underlying mechanics of the proposed framework. However, the reliance on boundedness assumptions for task similarity could limit its applicability in diverse real-world scenarios. Additionally, the lack of exploration of alternative distance measures and the limited scope of empirical validation primarily focused on image classification raises concerns about the generalizability and robustness of the proposed methods. The paper also inadequately addresses the computational challenges in high-dimensional spaces, which could hinder practical implementations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat hindered by the relegation of proofs and detailed derivations to supplementary materials, making it less accessible to readers unfamiliar with the mathematical concepts. While the theoretical contributions are novel, the overall quality is diminished by the shortcomings in empirical validation and a lack of comprehensive discussion regarding practical implications. Reproducibility may be affected due to the limited guidance provided for optimizing and tuning the developed algorithms in different settings.\n\n# Summary Of The Review\nOverall, the paper presents an interesting theoretical framework for transfer learning; however, its practical applicability is limited due to narrow assumptions and insufficient empirical validation. Future work is necessary to explore the framework's robustness across various domains and data types.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a framework aimed at quantifying task similarity in the context of transfer learning, employing a minimax principle for analytical problem-solving. The authors propose a \"boundedness assumption\" and utilize standard metrics such as χ² and Hellinger distances to formulate their approach. They claim to derive analytical solutions to the minimax problem and introduce practical algorithms, labeled \"MM-χ2\" and \"MM-Hel\", while evaluating their effectiveness on well-known datasets like CIFAR-10 and Office.\n\n# Strength And Weaknesses\nThe paper’s main contribution appears to be its attempt to formalize task similarity using established principles in transfer learning, but this effort suffers from a lack of originality. The theoretical underpinnings, including the minimax formulation, do not provide new insights, as they largely reiterate concepts already documented in the field. While the empirical results show acceptable performance on standard datasets, they fail to demonstrate significant innovation or advancement over existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written in a clear and structured manner, making it accessible for readers familiar with the domain. However, the novelty of the work is questionable, as it builds on well-trodden ideas without introducing substantive new concepts. Reproducibility is likely achievable since the paper references standard methodologies and datasets, yet the lack of innovative algorithms diminishes the significance of the claimed contributions.\n\n# Summary Of The Review\nOverall, the paper lacks true innovation, rehashing established concepts in transfer learning while offering minimal new insights. While the clarity of the writing is commendable, the contributions do not significantly advance the field. The authors would benefit from focusing on more original ideas rather than reiterating common knowledge.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to transfer learning grounded in the minimax principle, leveraging bounded assumptions on task similarity to enhance robustness in model performance across diverse tasks. The authors propose two algorithms, MM-χ2 and MM-Hel, which utilize χ2-distance and Hellinger distance measures to evaluate the similarity between source and target distributions. Experimental results on image classification tasks demonstrate the efficacy of the proposed methods, though the authors acknowledge the need for further validation across other domains.\n\n# Strength And Weaknesses\nThe paper offers significant contributions through its analytical solutions for minimax problems and the introduction of robust algorithms tailored for transfer learning. However, while the bounded assumptions on task similarity are practical, they may limit the applicability of the methods in more complex scenarios. Additionally, the focus on specific distance measures could be expanded to include a broader range of divergence metrics to improve robustness further. The experimental validation is somewhat limited, and the inclusion of a wider variety of tasks would strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, making it accessible to readers with a foundational understanding of transfer learning. The methodology is sound, although the novelty rests primarily on the minimax framework applied to transfer learning rather than introducing fundamentally new concepts. Reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and datasets used, as well as sharing code or supplementary materials for other researchers to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of transfer learning with its novel algorithms and analytical framework. However, it would benefit from a broader exploration of task similarities and distance measures, as well as more extensive empirical validation across diverse applications to solidify its claims.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents two novel minimax-based transfer learning algorithms, MM-χ2 and MM-Hel, and evaluates their performance on three benchmark datasets: CIFAR-10, Office-31, and Office-Caltech. The authors report substantial improvements in accuracy over baseline methods across various sample sizes and transfer tasks. Specifically, MM-χ2 achieves a maximum accuracy of 94.3% on CIFAR-10 and 96.7% on Office-31, while MM-Hel shows competitive results with a maximum accuracy of 94.9% and 96.5% for the same datasets, respectively. The findings suggest that both algorithms are effective for domain adaptation scenarios, demonstrating robustness and reliability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its empirical results, which consistently show that MM-χ2 and MM-Hel outperform existing baseline methods across all tested datasets. This indicates a significant contribution to the field of transfer learning, particularly in terms of practical performance. However, a notable weakness is the lack of theoretical insights or discussions surrounding the algorithms, as the focus predominantly remains on numerical results. This could limit the understanding of the underlying mechanisms driving the observed improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodologies and results. However, while the experimental findings are significant, the novelty of the approaches could be better contextualized within the existing literature. The reproducibility aspect is somewhat addressed through the detailed reporting of experimental setups and results, but a more comprehensive description of the algorithms and their implementations would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the transfer learning domain through its empirical results, showcasing the effectiveness of the proposed algorithms on multiple datasets. However, the lack of theoretical exploration limits a deeper understanding of the methods' innovations and implications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to improving task similarity in machine learning through the application of a minimax principle under boundedness assumptions. The authors propose a new algorithm that effectively learns joint structures from training data, addressing the challenges of task similarity computation. The findings indicate that the proposed algorithm outperforms existing methods in terms of accuracy and efficiency, particularly in complex environments with multiple tasks.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative use of the minimax principle, which offers a fresh perspective on task similarity. The proposed contributions are clearly outlined in bullet point format, allowing for easy reference. However, the paper suffers from several weaknesses, such as a lack of clear definitions for key terminology, which may hinder reader comprehension. Additionally, the writing is often dense and complex, which could alienate readers unfamiliar with the subject matter. The redundancy of certain points throughout the paper also detracts from its overall conciseness and impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by excessive jargon and complex sentence structures, making it difficult for a broader audience to grasp the main ideas effectively. The quality is impacted by grammatical issues and awkward phrasing in certain sections, which may require thorough proofreading. In terms of novelty, the paper presents a significant advancement in the understanding of task similarity, but the need for clearer explanations and visual aids limits its accessibility. Reproducibility could be improved by integrating references to supplemental materials more effectively within the main text.\n\n# Summary Of The Review\nOverall, the paper introduces a valuable contribution to the field of task similarity in machine learning through a novel algorithmic approach. However, issues related to clarity, consistency, and redundancy hinder its overall effectiveness. A thorough revision focusing on simplifying language and enhancing the presentation of ideas is recommended.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -1.898177372217468,
    -1.5153430627257538,
    -1.5726543170445573,
    -1.6823200253094632,
    -1.7155345727198654,
    -1.7980678533774608,
    -1.6913800307949112,
    -2.0690994168401473,
    -1.94206055749537,
    -2.027276254200856,
    -1.7275023126191968,
    -1.2776213113754045,
    -1.58420734834362,
    -1.580928496180184,
    -1.8353843278866342,
    -1.6744832121846138,
    -1.8043232318707476,
    -1.8012039477250277,
    -1.9220920257086114,
    -1.7126574054215296,
    -1.7098038400721287,
    -1.7339740629729623,
    -1.9155523152703409,
    -1.7435813900194457,
    -1.7980555915527507,
    -1.9901847986713246,
    -1.6862425809408534,
    -1.6003750705984083,
    -1.8684918159906048
  ],
  "logp_cond": [
    [
      0.0,
      -1.682134105239645,
      -1.6815034203142183,
      -1.6920483863235425,
      -1.7046580767968316,
      -1.726369591573035,
      -1.7313904852307709,
      -1.722653121134355,
      -1.6989627526484357,
      -1.7147538986924014,
      -1.6713193447189831,
      -1.7745638641360992,
      -1.7124954754463857,
      -1.7055938872556966,
      -1.7175679896875653,
      -1.6900500798089504,
      -1.7233222654569404,
      -1.722361022668495,
      -1.6919498692712907,
      -1.7299945301457815,
      -1.7274235718545192,
      -1.7121672115758062,
      -1.7359052803392132,
      -1.6870217911740348,
      -1.7189198579814768,
      -1.7261438132216183,
      -1.715277326466181,
      -1.7149292835101018,
      -1.7606627423423182
    ],
    [
      -1.1817661602286555,
      0.0,
      -1.0314388424563956,
      -1.0111472418245604,
      -1.0726698662937173,
      -1.1123623859514489,
      -1.2068313016058132,
      -1.1888913551388056,
      -0.9568362940327347,
      -1.1734477101099854,
      -1.0212467164580952,
      -1.264534750051301,
      -1.0927289134719087,
      -1.0139802558979576,
      -1.1146518935903802,
      -0.991152748866375,
      -1.171214755212026,
      -1.1337452086873063,
      -1.0167253694857585,
      -1.1482964074352695,
      -1.1309383215490172,
      -1.1584294035795801,
      -1.1761089476850175,
      -0.9963396367217678,
      -1.1081100779532471,
      -1.060817877434248,
      -1.0749480795056168,
      -1.1379164196058038,
      -1.2638490588335853
    ],
    [
      -1.2376617623572286,
      -1.1042924199503867,
      0.0,
      -1.1461512321888547,
      -1.154669921202738,
      -1.2906303800860948,
      -1.2560294682324447,
      -1.2359914685791606,
      -1.1279321575455357,
      -1.2152947947300121,
      -1.124754100310764,
      -1.3501904971231613,
      -1.1820875163744935,
      -1.166824229112041,
      -1.223368650784504,
      -1.1054837268553144,
      -1.1940017848569124,
      -1.181308285783674,
      -1.0458580506611341,
      -1.1809363721751271,
      -1.1998544441606749,
      -1.270164272366565,
      -1.2365427306077734,
      -1.0778562644451761,
      -1.1897189515823827,
      -1.1538199314880238,
      -1.1494025172024664,
      -1.1730839072350048,
      -1.2708383403002463
    ],
    [
      -1.3105398186809725,
      -1.1614380312036643,
      -1.1906304488348882,
      0.0,
      -1.1614158895131053,
      -1.2689338399439307,
      -1.3639485682831065,
      -1.2768972157192613,
      -1.1641393742418322,
      -1.3438702129559694,
      -1.1451347157673994,
      -1.42191863001222,
      -1.231888247366153,
      -1.1905135481567521,
      -1.2729288296027181,
      -1.1424102774868319,
      -1.2807442816556023,
      -1.2827290095565331,
      -1.1084603869371268,
      -1.3034178944993768,
      -1.272391835828283,
      -1.3159213047698177,
      -1.313168250482244,
      -1.0190359281811032,
      -1.31761913042511,
      -1.243525129051497,
      -1.2480745815089467,
      -1.2639696304136743,
      -1.3713445215180555
    ],
    [
      -1.352476253606038,
      -1.2461654342250832,
      -1.2476985380060246,
      -1.295227987769503,
      0.0,
      -1.3773946094088723,
      -1.426928349239896,
      -1.4017933566135374,
      -1.2377522603143896,
      -1.302222946568481,
      -1.2372053386435204,
      -1.4667401079773406,
      -1.3116129144835047,
      -1.2605468156500603,
      -1.4107317909366983,
      -1.2935305406038329,
      -1.286204706218876,
      -1.3093431893956977,
      -1.2387788127826194,
      -1.3210319782299527,
      -1.3136098165231067,
      -1.3627449324942718,
      -1.2960358264302654,
      -1.2228838297679276,
      -1.261621014946678,
      -1.3726270893452894,
      -1.3244804359164546,
      -1.3757396798749546,
      -1.4166892497294619
    ],
    [
      -1.4543226050586913,
      -1.3031397316906053,
      -1.3963061538262773,
      -1.3872236395917112,
      -1.3548635653071692,
      0.0,
      -1.4232055036610771,
      -1.4062035780406645,
      -1.3107871427753497,
      -1.467511396391183,
      -1.3659233421428967,
      -1.49478661143108,
      -1.4006817343694677,
      -1.3057442375155055,
      -1.4218703631631688,
      -1.3733774617506833,
      -1.4471962137219712,
      -1.3495346328770053,
      -1.3558456766362645,
      -1.4594936609495304,
      -1.3686054957562757,
      -1.4342674305473841,
      -1.3947062574993547,
      -1.3285842217460846,
      -1.4038531161933798,
      -1.3865228622450392,
      -1.3991202662946787,
      -1.4766532973296052,
      -1.494648582273053
    ],
    [
      -1.3586210416457853,
      -1.3355268597108774,
      -1.3063692241017162,
      -1.324336260447905,
      -1.3237277119937398,
      -1.2881602289972605,
      0.0,
      -1.320586403471821,
      -1.2986332014058097,
      -1.315081669145314,
      -1.3238458254348677,
      -1.375758443575727,
      -1.2997756520566666,
      -1.306363826809192,
      -1.3536841496989103,
      -1.311615912436275,
      -1.3354031219673979,
      -1.2923556057125334,
      -1.290118914377309,
      -1.3446267112643788,
      -1.3907442210961165,
      -1.3598365052734516,
      -1.3411892835400068,
      -1.286425437836902,
      -1.362901860575031,
      -1.4004979204670378,
      -1.3503699260588666,
      -1.343659386780924,
      -1.3478869316733242
    ],
    [
      -1.7368627822300697,
      -1.6887128804630327,
      -1.649409935336143,
      -1.6373797483375818,
      -1.6644517260258684,
      -1.689086212464533,
      -1.6791610623226123,
      0.0,
      -1.644573659874892,
      -1.6940972342817284,
      -1.6394464080249969,
      -1.7903542812290734,
      -1.640126187537562,
      -1.6882710020355371,
      -1.7348410569814012,
      -1.6017959415821625,
      -1.6560696730447624,
      -1.656582097738338,
      -1.5750740865419695,
      -1.7194355556372398,
      -1.704423706952304,
      -1.659204224528649,
      -1.6944737549962627,
      -1.622765099933969,
      -1.7307961693482856,
      -1.6971914740223522,
      -1.7257956250835667,
      -1.7092745719302744,
      -1.6779748780014596
    ],
    [
      -1.559198876569715,
      -1.402173578194698,
      -1.4514080167635957,
      -1.4459423053754281,
      -1.4973271027889212,
      -1.5258537771903906,
      -1.599914738182289,
      -1.541743449114338,
      0.0,
      -1.5475907724771196,
      -1.4676350126285373,
      -1.7152679557506239,
      -1.5228508689511315,
      -1.4697823307293807,
      -1.5731701457688132,
      -1.4269986163885628,
      -1.5407778156820024,
      -1.498484553100378,
      -1.4252899511057595,
      -1.5486758919187797,
      -1.5339657507302127,
      -1.5423464113864715,
      -1.5689477209396943,
      -1.4204221110149613,
      -1.547006744400247,
      -1.577912317417198,
      -1.5295585255904616,
      -1.5636009760761187,
      -1.6459800244520604
    ],
    [
      -1.5701435343010295,
      -1.5950763970764474,
      -1.5302104361333118,
      -1.6085200726351236,
      -1.5246335599643364,
      -1.6993143496078362,
      -1.662228541916215,
      -1.6168962121738744,
      -1.524899121084042,
      0.0,
      -1.519227614736089,
      -1.7600132714322438,
      -1.6105594832442764,
      -1.674374356306361,
      -1.6253350030879636,
      -1.5507084998662672,
      -1.5006274918891223,
      -1.601921482353049,
      -1.556170940973838,
      -1.6220930084742067,
      -1.5801435730580145,
      -1.6166719236643239,
      -1.5751376678413556,
      -1.5373356032378618,
      -1.6016800690353703,
      -1.6362809833673748,
      -1.5948178139217672,
      -1.645414295591511,
      -1.6556085109328633
    ],
    [
      -1.301517663434794,
      -1.1095386065608384,
      -1.1502238589779956,
      -1.1425649283764063,
      -1.156108032861675,
      -1.2234794733889551,
      -1.3330549228452153,
      -1.2779128654351923,
      -1.1326544251014259,
      -1.2560490169818115,
      0.0,
      -1.4023908781131165,
      -1.1989828277260839,
      -1.207625291357433,
      -1.246562722727456,
      -1.1278416871777786,
      -1.2753548016573724,
      -1.2385437164346655,
      -1.16402351150746,
      -1.2796094399408986,
      -1.2645222465266595,
      -1.3065258465628666,
      -1.31501448501152,
      -1.141971651042317,
      -1.317716577500932,
      -1.208175330812231,
      -1.2229169042915904,
      -1.2590995338586775,
      -1.3738786659586761
    ],
    [
      -1.0851020888076077,
      -1.0350872317110893,
      -1.0438295076874433,
      -1.0664036021210401,
      -1.044757316089659,
      -1.009141048533944,
      -1.0356031274373976,
      -1.0327587849493889,
      -1.0529979978779216,
      -1.014891144115266,
      -1.0310263928930083,
      0.0,
      -1.0447727907395499,
      -1.0110260307085535,
      -1.005793915928525,
      -1.0415838101682753,
      -1.0368514651103684,
      -1.039329318083418,
      -1.0678900252920605,
      -1.0401190993787424,
      -1.0438035076789298,
      -1.0247856966042022,
      -1.0336334547745842,
      -1.0511023784544589,
      -1.0240833603083122,
      -1.0124722344531187,
      -1.0087058515346992,
      -1.0544082863021382,
      -1.0341052510854183
    ],
    [
      -1.2859978815156479,
      -1.1530207625173714,
      -1.2291218796271353,
      -1.2473900491079635,
      -1.1718535165379427,
      -1.2617976112714773,
      -1.282172904203573,
      -1.2651138910301267,
      -1.1841841522839862,
      -1.2442097720240166,
      -1.2251002900011867,
      -1.3505552602922684,
      0.0,
      -1.175215904868288,
      -1.2601331507520115,
      -1.2168041980172666,
      -1.2662889307060814,
      -1.2040360895833122,
      -1.145247200545325,
      -1.308083403448875,
      -1.253030110065866,
      -1.254823424636522,
      -1.226556183881149,
      -1.1910097673835056,
      -1.2730774720624831,
      -1.285202616899024,
      -1.2493972841190149,
      -1.267436832750218,
      -1.2764354286083361
    ],
    [
      -1.2007901784840345,
      -0.9614252241384694,
      -1.0730726202607195,
      -1.0487521041718084,
      -1.0679280129597075,
      -1.0949889732255103,
      -1.202548832733437,
      -1.1962809210642738,
      -1.018300662527065,
      -1.157028368284208,
      -1.1267933880522143,
      -1.2556314471168148,
      -1.0364434230514945,
      0.0,
      -1.1597170266048746,
      -1.0366627760629823,
      -1.1958583234304236,
      -1.0849813229719958,
      -1.0646411811478875,
      -1.2247651585100507,
      -1.1590320582453182,
      -1.1819938537648345,
      -1.135312566334858,
      -1.0113175307423559,
      -1.148955976132506,
      -1.109161013807781,
      -1.1241697068238108,
      -1.2062209427286388,
      -1.2025176762168375
    ],
    [
      -1.4451339349483363,
      -1.3681863975770634,
      -1.3616828804296512,
      -1.3617078677906065,
      -1.3853624572912597,
      -1.4218832774353973,
      -1.4924740565814987,
      -1.4990577105279526,
      -1.3820346079061971,
      -1.3637650489845947,
      -1.3586528895684964,
      -1.504229838831966,
      -1.3996515057897931,
      -1.3864784128967136,
      0.0,
      -1.3843461679897353,
      -1.423076402350224,
      -1.3834264362311859,
      -1.3901675666134274,
      -1.472001961462495,
      -1.401206328629164,
      -1.4559875454601956,
      -1.4192564469674227,
      -1.317530849924524,
      -1.3705959967944708,
      -1.3857507647732037,
      -1.3108799848706558,
      -1.4696548982121023,
      -1.4876568889851847
    ],
    [
      -1.3577131973598722,
      -1.0920615493279868,
      -1.1777870798773646,
      -1.164512896938332,
      -1.2586295147594433,
      -1.3005895847153026,
      -1.3106426626496153,
      -1.2799255402596803,
      -1.1622651716420618,
      -1.281303011682909,
      -1.1883479290797367,
      -1.4540271973310581,
      -1.2221437717567658,
      -1.2091405546165637,
      -1.2659152087745196,
      0.0,
      -1.3002416978941147,
      -1.2680523702791349,
      -1.1527990602710065,
      -1.2822856572859382,
      -1.3208807225969659,
      -1.3289915748982948,
      -1.3514177034516113,
      -1.126459398371682,
      -1.2951804717826563,
      -1.2553907978436987,
      -1.2521633323270152,
      -1.2849644627274015,
      -1.360244135038896
    ],
    [
      -1.4596210918433028,
      -1.4607927787209398,
      -1.3813848747032758,
      -1.404557647685814,
      -1.3872504934305125,
      -1.4975528037238677,
      -1.5057513418564945,
      -1.4442239732197968,
      -1.3410692798130146,
      -1.417292757373954,
      -1.370131604138859,
      -1.5813922678013337,
      -1.453757703848431,
      -1.484772827214453,
      -1.4849277396042235,
      -1.3943879085141606,
      0.0,
      -1.4335992592287579,
      -1.378056599441249,
      -1.428838876627348,
      -1.3630363871143771,
      -1.4601471393271164,
      -1.4617247961597755,
      -1.3436687429820078,
      -1.4031532636338446,
      -1.4636926358685984,
      -1.4186767181416047,
      -1.4430204367391288,
      -1.5331633076035398
    ],
    [
      -1.440010550660077,
      -1.3869865500301268,
      -1.3313948898026078,
      -1.3765951284718414,
      -1.3445154305601865,
      -1.372249282384415,
      -1.3940754159613287,
      -1.3759590211922283,
      -1.271912866397451,
      -1.4254134977309565,
      -1.386183191158829,
      -1.5245385231095625,
      -1.359823379402289,
      -1.338253697313948,
      -1.4253436509707402,
      -1.3479135536602214,
      -1.3927912971690513,
      0.0,
      -1.3250765324137588,
      -1.4312259677122283,
      -1.3607978190986176,
      -1.410470633303387,
      -1.3509237819786428,
      -1.3332541797549482,
      -1.3448732754686934,
      -1.3766808211101413,
      -1.3734046189376274,
      -1.415297737500661,
      -1.4293337327030664
    ],
    [
      -1.512185705822422,
      -1.337770319617723,
      -1.3616121984959766,
      -1.4104793035925094,
      -1.3838809196884634,
      -1.4944909197330132,
      -1.5102660130830723,
      -1.5080875600747494,
      -1.3648625976864692,
      -1.4879460467875596,
      -1.406114142748589,
      -1.6332872860322651,
      -1.4430326494386794,
      -1.443973856336615,
      -1.5316008343647256,
      -1.3776786197773623,
      -1.4836531858247117,
      -1.4889061774174412,
      0.0,
      -1.4435457399759848,
      -1.4552930997728397,
      -1.5172912378823697,
      -1.5112975827671584,
      -1.3290152546145704,
      -1.4804810982802827,
      -1.442829078717695,
      -1.4212675231803036,
      -1.460070494176082,
      -1.5437298880734185
    ],
    [
      -1.362860041318669,
      -1.2686986906576698,
      -1.2766818687888344,
      -1.3171144410599036,
      -1.279294393887722,
      -1.3840125314568068,
      -1.3342484511907173,
      -1.3542552854511452,
      -1.2323500251687591,
      -1.2817148948250021,
      -1.2666640710537589,
      -1.4167346596062695,
      -1.3399710974362014,
      -1.3627395142587446,
      -1.3620071330745809,
      -1.2835063287789725,
      -1.2987570561692086,
      -1.352564590941908,
      -1.2383472226321712,
      0.0,
      -1.2929658469082026,
      -1.3512940402575682,
      -1.3522982501706142,
      -1.249723661752212,
      -1.3189995570307596,
      -1.3161183682913495,
      -1.253370855971731,
      -1.2947788658550117,
      -1.3655526944386511
    ],
    [
      -1.2883339124235393,
      -1.1490625660872973,
      -1.1474581201826393,
      -1.2274818307775643,
      -1.1530810602047603,
      -1.2425041178020981,
      -1.3195781952997907,
      -1.2220323738090473,
      -1.166005675975874,
      -1.2341160706769256,
      -1.146310634143188,
      -1.3911803617551977,
      -1.2479677178656996,
      -1.2374367012092238,
      -1.255656203248739,
      -1.163266838539652,
      -1.1745927523875277,
      -1.2039228940835733,
      -1.1701679668453242,
      -1.2398653273546785,
      0.0,
      -1.309738098133791,
      -1.2441010585641101,
      -1.164930452747174,
      -1.1968929750220634,
      -1.1845426401105645,
      -1.1744504889956444,
      -1.2635441954545703,
      -1.3295862569327253
    ],
    [
      -1.3923691395787885,
      -1.3467190108250302,
      -1.3447884828158347,
      -1.4084583449781753,
      -1.3687848708684098,
      -1.4007214830353165,
      -1.4515893189890245,
      -1.4216448655469176,
      -1.3300626326208669,
      -1.3867968629225464,
      -1.3946145229367424,
      -1.4830264415437728,
      -1.377771404815243,
      -1.3804822676899757,
      -1.3916497163083015,
      -1.3822124313715956,
      -1.4338023600572178,
      -1.3693063368149205,
      -1.3417501390197488,
      -1.3792852752688254,
      -1.4096337132871593,
      0.0,
      -1.3918669416125253,
      -1.341181895659018,
      -1.4114440674551991,
      -1.4084527029900795,
      -1.4091124222725036,
      -1.3881158096375956,
      -1.4287745327254198
    ],
    [
      -1.5547158559660677,
      -1.4118225003591183,
      -1.476757123363036,
      -1.4442182125897112,
      -1.3784679605110295,
      -1.497180252964213,
      -1.546424268775488,
      -1.510563307403222,
      -1.4386138430652349,
      -1.4201037675154713,
      -1.438935140178826,
      -1.5779343577661202,
      -1.441894260864953,
      -1.4661333133505028,
      -1.5364531449759364,
      -1.4783535292083931,
      -1.4400772604787768,
      -1.460137270801121,
      -1.4566113575240338,
      -1.5227031002674065,
      -1.4741453604912675,
      -1.4945657854071273,
      0.0,
      -1.4411382150762257,
      -1.4242804272047476,
      -1.507666065445555,
      -1.4444700817179106,
      -1.5282054892008972,
      -1.515907744881952
    ],
    [
      -1.357008946993268,
      -1.1576979518043353,
      -1.1640691912001635,
      -1.1787889815386727,
      -1.199943526920392,
      -1.326444199444531,
      -1.39074304531201,
      -1.3441548964641088,
      -1.1630094107673798,
      -1.320116463404527,
      -1.2274070095067333,
      -1.491541466731723,
      -1.2840019213570857,
      -1.2431719019363265,
      -1.3281608804857146,
      -1.1568623939765867,
      -1.3012411802846946,
      -1.2845570673681193,
      -1.1098312795153702,
      -1.3132541876115518,
      -1.2991272994097245,
      -1.334198593824296,
      -1.3694405408697186,
      0.0,
      -1.325668684300206,
      -1.3149541415660222,
      -1.2549091942287116,
      -1.3104683926953662,
      -1.4033730673329625
    ],
    [
      -1.4738411115703804,
      -1.363802941020772,
      -1.420566066821638,
      -1.4773916404993899,
      -1.3738584066456612,
      -1.4577844979301542,
      -1.5247284123181073,
      -1.4958359057811133,
      -1.3646310256793897,
      -1.4476712479694174,
      -1.4183231444188573,
      -1.5296683503486705,
      -1.4692727616557264,
      -1.4545055752587517,
      -1.4494148937632574,
      -1.4395002547988858,
      -1.3686456860562561,
      -1.4514739220332393,
      -1.4341929208881208,
      -1.472929680176821,
      -1.44064338460456,
      -1.5159303014960424,
      -1.4239629565740426,
      -1.3962509691218827,
      0.0,
      -1.3923041533094187,
      -1.4059395343046879,
      -1.491994030451139,
      -1.5429570789649107
    ],
    [
      -1.6391962740353672,
      -1.4240609177764225,
      -1.4812184823401278,
      -1.545277222135098,
      -1.56362138388734,
      -1.5772982212673259,
      -1.6713401550467086,
      -1.587046347823689,
      -1.5604586472067996,
      -1.6408555597032428,
      -1.4887670724587867,
      -1.7091991048660395,
      -1.5749847072005922,
      -1.5218901918519117,
      -1.5343541858359575,
      -1.4458850689048188,
      -1.583603731896353,
      -1.5648084483506075,
      -1.4582261506424736,
      -1.5889377884445195,
      -1.5725534232877283,
      -1.6443132167191528,
      -1.605488674301571,
      -1.4991908218323726,
      -1.566714307168506,
      0.0,
      -1.5227178469887523,
      -1.5670809399762164,
      -1.635002514528163
    ],
    [
      -1.3390550144645426,
      -1.2444386778544034,
      -1.2531399555369638,
      -1.2703209486290081,
      -1.2506609160930329,
      -1.2984645513650723,
      -1.3803921087197104,
      -1.3509217052130509,
      -1.2529587513390212,
      -1.2403418313967434,
      -1.250525268576852,
      -1.3886960340697163,
      -1.3282699252832497,
      -1.3166865349290453,
      -1.2487423543014935,
      -1.2547870991350534,
      -1.2305765023233937,
      -1.298503742959667,
      -1.2216781921131479,
      -1.2434653523476817,
      -1.2513836999551133,
      -1.3748920594840792,
      -1.2975312018287057,
      -1.1855737062479275,
      -1.1773539705774556,
      -1.222782720417348,
      0.0,
      -1.3230957020184462,
      -1.4031669677895844
    ],
    [
      -1.2454961809121126,
      -1.1525307974537011,
      -1.1188698861024278,
      -1.1615536412076142,
      -1.1919974187061442,
      -1.284035342509552,
      -1.249343142086508,
      -1.2309542098525794,
      -1.1519143071620441,
      -1.198851730567071,
      -1.1213496512728207,
      -1.325191579211193,
      -1.2227817019562315,
      -1.228275979884216,
      -1.2413307412689734,
      -1.1412437529100055,
      -1.2165812445265398,
      -1.2300391580740127,
      -1.134803911626348,
      -1.1390164911834,
      -1.18744323158549,
      -1.223706943013074,
      -1.2525772928763628,
      -1.111437039749511,
      -1.2187759168520709,
      -1.150037253384411,
      -1.1792265524942662,
      0.0,
      -1.2750485404279994
    ],
    [
      -1.5329794294912387,
      -1.4928647279421423,
      -1.5032082970691725,
      -1.5192855433624561,
      -1.5081730820208874,
      -1.5349051911982645,
      -1.5575980708860315,
      -1.544247343966106,
      -1.5003250882697035,
      -1.502597577688962,
      -1.5156354834188845,
      -1.528063701094142,
      -1.4939172770447469,
      -1.4988408923724572,
      -1.5293225533434782,
      -1.4907923496215156,
      -1.52212568770759,
      -1.485167051400336,
      -1.4975506411271364,
      -1.5425859410013545,
      -1.519996126957084,
      -1.4768166614356886,
      -1.5106584650003263,
      -1.4864293420597252,
      -1.4876089303601543,
      -1.512248221304566,
      -1.5212331347611052,
      -1.5215522098245646,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.21604326697782295,
      0.21667395190324967,
      0.20612898589392548,
      0.1935192954206364,
      0.17180778064443292,
      0.16678688698669708,
      0.17552425108311298,
      0.1992146195690323,
      0.1834234735250666,
      0.22685802749848483,
      0.12361350808136873,
      0.18568189677108227,
      0.19258348496177136,
      0.18060938252990266,
      0.20812729240851757,
      0.1748551067605275,
      0.17581634954897307,
      0.2062275029461773,
      0.16818284207168643,
      0.17075380036294874,
      0.18601016064166176,
      0.16227209187825475,
      0.21115558104343313,
      0.17925751423599112,
      0.1720335589958497,
      0.18290004575128704,
      0.1832480887073662,
      0.13751462987514973
    ],
    [
      0.33357690249709826,
      0.0,
      0.4839042202693582,
      0.5041958209011934,
      0.4426731964320365,
      0.40298067677430494,
      0.3085117611199406,
      0.3264517075869482,
      0.5585067686930191,
      0.3418953526157684,
      0.49409634626765864,
      0.2508083126744529,
      0.4226141492538451,
      0.5013628068277962,
      0.4006911691353736,
      0.5241903138593788,
      0.3441283075137278,
      0.3815978540384475,
      0.4986176932399953,
      0.3670466552904843,
      0.3844047411767366,
      0.3569136591461737,
      0.3392341150407363,
      0.519003426003986,
      0.4072329847725067,
      0.45452518529150576,
      0.44039498322013704,
      0.37742664311995,
      0.25149400389216847
    ],
    [
      0.3349925546873287,
      0.46836189709417053,
      0.0,
      0.42650308485570254,
      0.41798439584181923,
      0.2820239369584625,
      0.3166248488121126,
      0.3366628484653966,
      0.4447221594990216,
      0.35735952231454515,
      0.4479002167337933,
      0.22246381992139597,
      0.3905668006700638,
      0.4058300879325163,
      0.3492856662600532,
      0.4671705901892429,
      0.37865253218764483,
      0.3913460312608832,
      0.5267962663834231,
      0.3917179448694301,
      0.3727998728838824,
      0.30249004467799234,
      0.33611158643678385,
      0.49479805259938114,
      0.38293536546217455,
      0.4188343855565335,
      0.4232517998420908,
      0.3995704098095525,
      0.30181597674431093
    ],
    [
      0.37178020662849076,
      0.5208819941057989,
      0.491689576474575,
      0.0,
      0.520904135796358,
      0.4133861853655325,
      0.3183714570263567,
      0.40542280959020194,
      0.518180651067631,
      0.3384498123534938,
      0.5371853095420638,
      0.26040139529724327,
      0.45043177794331024,
      0.4918064771527111,
      0.4093911957067451,
      0.5399097478226313,
      0.40157574365386095,
      0.3995910157529301,
      0.5738596383723364,
      0.3789021308100864,
      0.40992818948118015,
      0.3663987205396455,
      0.3691517748272193,
      0.66328409712836,
      0.3647008948843533,
      0.4387948962579662,
      0.43424544380051655,
      0.41835039489578896,
      0.3109755037914077
    ],
    [
      0.36305831911382747,
      0.46936913849478223,
      0.46783603471384083,
      0.42030658495036244,
      0.0,
      0.33813996331099316,
      0.28860622347996934,
      0.313741216106328,
      0.47778231240547586,
      0.4133116261513845,
      0.478329234076345,
      0.24879446474252487,
      0.4039216582363607,
      0.4549877570698051,
      0.3048027817831671,
      0.42200403211603255,
      0.4293298665009895,
      0.4061913833241677,
      0.47675575993724606,
      0.39450259448991276,
      0.4019247561967587,
      0.35278964022559367,
      0.41949874628960004,
      0.4926507429519378,
      0.4539135577731874,
      0.342907483374576,
      0.39105413680341083,
      0.3397948928449108,
      0.29884532299040356
    ],
    [
      0.3437452483187695,
      0.4949281216868555,
      0.40176169955118346,
      0.41084421378574953,
      0.44320428807029155,
      0.0,
      0.37486234971638366,
      0.39186427533679624,
      0.4872807106021111,
      0.3305564569862778,
      0.4321445112345641,
      0.30328124194638084,
      0.39738611900799303,
      0.4923236158619553,
      0.37619749021429194,
      0.42469039162677746,
      0.3508716396554896,
      0.4485332205004555,
      0.44222217674119624,
      0.3385741924279304,
      0.4294623576211851,
      0.36380042283007663,
      0.40336159587810605,
      0.4694836316313762,
      0.39421473718408095,
      0.4115449911324216,
      0.3989475870827821,
      0.3214145560478556,
      0.3034192711044077
    ],
    [
      0.33275898914912583,
      0.3558531710840338,
      0.385010806693195,
      0.3670437703470062,
      0.36765231880117133,
      0.4032198017976507,
      0.0,
      0.37079362732309007,
      0.39274682938910144,
      0.3762983616495972,
      0.36753420536004344,
      0.3156215872191841,
      0.3916043787382446,
      0.38501620398571923,
      0.3376958810960009,
      0.3797641183586362,
      0.3559769088275133,
      0.3990244250823778,
      0.4012611164176021,
      0.34675331953053234,
      0.30063580969879466,
      0.3315435255214596,
      0.3501907472549044,
      0.4049545929580092,
      0.32847817021988024,
      0.29088211032787337,
      0.34101010473604454,
      0.3477206440139873,
      0.343493099121587
    ],
    [
      0.3322366346100776,
      0.3803865363771146,
      0.4196894815040042,
      0.4317196685025655,
      0.40464769081427887,
      0.38001320437561437,
      0.389938354517535,
      0.0,
      0.4245257569652554,
      0.3750021825584189,
      0.42965300881515045,
      0.27874513561107395,
      0.4289732293025854,
      0.38082841480461016,
      0.3342583598587461,
      0.46730347525798477,
      0.41302974379538493,
      0.4125173191018092,
      0.49402533029817786,
      0.3496638612029075,
      0.36467570988784326,
      0.4098951923114984,
      0.37462566184388457,
      0.4463343169061784,
      0.3383032474918617,
      0.37190794281779516,
      0.34330379175658066,
      0.3598248449098729,
      0.3911245388386877
    ],
    [
      0.3828616809256551,
      0.539886979300672,
      0.49065254073177433,
      0.49611825211994187,
      0.44473345470644876,
      0.4162067803049794,
      0.3421458193130811,
      0.40031710838103196,
      0.0,
      0.39446978501825036,
      0.47442554486683264,
      0.22679260174474614,
      0.41920968854423846,
      0.4722782267659893,
      0.3688904117265568,
      0.5150619411068071,
      0.40128274181336754,
      0.4435760043949919,
      0.5167706063896105,
      0.39338466557659024,
      0.4080948067651573,
      0.39971414610889844,
      0.37311283655567573,
      0.5216384464804087,
      0.3950538130951229,
      0.3641482400781719,
      0.4125020319049084,
      0.3784595814192513,
      0.2960805330433096
    ],
    [
      0.4571327198998265,
      0.43219985712440856,
      0.49706581806754424,
      0.41875618156573235,
      0.5026426942365196,
      0.32796190459301977,
      0.36504771228464095,
      0.41038004202698164,
      0.5023771331168141,
      0.0,
      0.508048639464767,
      0.26726298276861216,
      0.41671677095657955,
      0.3529018978944951,
      0.40194125111289236,
      0.47656775433458876,
      0.5266487623117337,
      0.425354771847807,
      0.471105313227018,
      0.4051832457266493,
      0.44713268114284155,
      0.4106043305365321,
      0.45213858635950044,
      0.4899406509629942,
      0.4255961851654857,
      0.3909952708334812,
      0.43245844027908875,
      0.381861958609345,
      0.3716677432679927
    ],
    [
      0.4259846491844028,
      0.6179637060583585,
      0.5772784536412012,
      0.5849373842427905,
      0.5713942797575218,
      0.5040228392302417,
      0.3944473897739815,
      0.4495894471840045,
      0.594847887517771,
      0.47145329563738536,
      0.0,
      0.3251114345060804,
      0.528519484893113,
      0.5198770212617638,
      0.4809395898917408,
      0.5996606254414183,
      0.4521475109618245,
      0.4889585961845313,
      0.5634788011117369,
      0.44789287267829825,
      0.4629800660925374,
      0.4209764660563302,
      0.4124878276076769,
      0.5855306615768798,
      0.40978573511826477,
      0.5193269818069659,
      0.5045854083276065,
      0.4684027787605194,
      0.3536236466605207
    ],
    [
      0.19251922256779674,
      0.24253407966431517,
      0.23379180368796115,
      0.21121770925436434,
      0.23286399528574542,
      0.2684802628414604,
      0.2420181839380069,
      0.24486252642601558,
      0.22462331349748288,
      0.26273016726013854,
      0.24659491848239612,
      0.0,
      0.2328485206358546,
      0.266595280666851,
      0.2718273954468795,
      0.23603750120712919,
      0.24076984626503606,
      0.23829199329198647,
      0.20973128608334402,
      0.2375022119966621,
      0.23381780369647465,
      0.2528356147712023,
      0.24398785660082023,
      0.2265189329209456,
      0.2535379510670923,
      0.26514907692228573,
      0.2689154598407053,
      0.22321302507326624,
      0.24351606028998618
    ],
    [
      0.2982094668279722,
      0.4311865858262487,
      0.35508546871648483,
      0.3368172992356566,
      0.4123538318056774,
      0.3224097370721428,
      0.3020344441400471,
      0.31909345731349337,
      0.4000231960596339,
      0.3399975763196035,
      0.3591070583424334,
      0.23365208805135174,
      0.0,
      0.4089914434753321,
      0.3240741975916086,
      0.3674031503263535,
      0.3179184176375387,
      0.3801712587603079,
      0.4389601477982952,
      0.276123944894745,
      0.331177238277754,
      0.3293839237070981,
      0.35765116446247114,
      0.3931975809601145,
      0.311129876281137,
      0.29900473144459605,
      0.33481006422460524,
      0.3167705155934022,
      0.30777191973528395
    ],
    [
      0.3801383176961495,
      0.6195032720417145,
      0.5078558759194645,
      0.5321763920083755,
      0.5130004832204764,
      0.4859395229546737,
      0.3783796634467469,
      0.3846475751159102,
      0.562627833653119,
      0.423900127895976,
      0.45413510812796964,
      0.3252970490633691,
      0.5444850731286894,
      0.0,
      0.4212114695753093,
      0.5442657201172016,
      0.3850701727497603,
      0.49594717320818815,
      0.5162873150322964,
      0.3561633376701332,
      0.42189643793486575,
      0.3989346424153495,
      0.44561592984532594,
      0.5696109654378281,
      0.43197252004767783,
      0.47176748237240296,
      0.45675878935637315,
      0.37470755345154516,
      0.37841081996334647
    ],
    [
      0.3902503929382979,
      0.4671979303095708,
      0.47370144745698295,
      0.4736764600960277,
      0.45002187059537446,
      0.41350105045123686,
      0.3429102713051355,
      0.33632661735868163,
      0.45334971998043705,
      0.4716192789020395,
      0.4767314383181378,
      0.33115448905466827,
      0.43573282209684105,
      0.4489059149899206,
      0.0,
      0.4510381598968989,
      0.41230792553641016,
      0.4519578916554483,
      0.44521676127320675,
      0.3633823664241391,
      0.43417799925747014,
      0.3793967824264386,
      0.4161278809192115,
      0.5178534779621102,
      0.46478833109216344,
      0.4496335631134305,
      0.5245043430159784,
      0.3657294296745319,
      0.3477274389014495
    ],
    [
      0.31677001482474165,
      0.582421662856627,
      0.4966961323072492,
      0.5099703152462818,
      0.41585369742517053,
      0.3738936274693112,
      0.36384054953499856,
      0.3945576719249335,
      0.512218040542552,
      0.3931802005017049,
      0.4861352831048771,
      0.2204560148535557,
      0.452339440427848,
      0.46534265756805016,
      0.40856800341009425,
      0.0,
      0.3742415142904991,
      0.40643084190547896,
      0.5216841519136073,
      0.3921975548986756,
      0.35360248958764795,
      0.34549163728631904,
      0.32306550873300255,
      0.5480238138129319,
      0.37930274040195755,
      0.41909241434091515,
      0.42231987985759867,
      0.3895187494572123,
      0.31423907714571775
    ],
    [
      0.3447021400274448,
      0.3435304531498078,
      0.42293835716747186,
      0.3997655841849337,
      0.41707273844023507,
      0.30677042814687994,
      0.2985718900142531,
      0.3600992586509508,
      0.463253952057733,
      0.3870304744967936,
      0.4341916277318887,
      0.22293096406941393,
      0.35056552802231655,
      0.3195504046562947,
      0.3193954922665241,
      0.409935323356587,
      0.0,
      0.37072397264198975,
      0.4262666324294986,
      0.3754843552433995,
      0.4412868447563705,
      0.34417609254363124,
      0.34259843571097215,
      0.46065448888873983,
      0.401169968236903,
      0.3406305960021492,
      0.38564651372914294,
      0.3613027951316188,
      0.2711599242672078
    ],
    [
      0.36119339706495057,
      0.41421739769490085,
      0.4698090579224199,
      0.4246088192531863,
      0.45668851716484116,
      0.42895466534061266,
      0.407128531763699,
      0.4252449265327993,
      0.5292910813275766,
      0.3757904499940712,
      0.41502075656619875,
      0.27666542461546517,
      0.44138056832273875,
      0.4629502504110796,
      0.37586029675428745,
      0.4532903940648063,
      0.4084126505559764,
      0.0,
      0.4761274153112689,
      0.36997798001279936,
      0.44040612862641004,
      0.3907333144216407,
      0.45028016574638485,
      0.4679497679700795,
      0.4563306722563343,
      0.42452312661488634,
      0.42779932878740023,
      0.38590621022436666,
      0.37187021502196127
    ],
    [
      0.4099063198861894,
      0.5843217060908883,
      0.5604798272126348,
      0.511612722116102,
      0.538211106020148,
      0.4276011059755982,
      0.4118260126255391,
      0.41400446563386195,
      0.5572294280221421,
      0.4341459789210518,
      0.5159778829600223,
      0.28880473967634623,
      0.479059376269932,
      0.47811816937199647,
      0.39049119134388577,
      0.5444134059312491,
      0.43843883988389964,
      0.43318584829117013,
      0.0,
      0.4785462857326266,
      0.46679892593577166,
      0.4048007878262416,
      0.41079444294145295,
      0.593076771094041,
      0.44161092742832864,
      0.4792629469909164,
      0.5008245025283078,
      0.46202153153252934,
      0.3783621376351929
    ],
    [
      0.34979736410286066,
      0.44395871476385973,
      0.43597553663269517,
      0.39554296436162595,
      0.43336301153380763,
      0.32864487396472275,
      0.3784089542308122,
      0.3584021199703844,
      0.4803073802527704,
      0.43094251059652744,
      0.44599333436777067,
      0.29592274581526,
      0.3726863079853282,
      0.34991789116278493,
      0.3506502723469487,
      0.42915107664255703,
      0.413900349252321,
      0.3600928144796216,
      0.4743101827893583,
      0.0,
      0.41969155851332696,
      0.3613633651639614,
      0.36035915525091533,
      0.4629337436693175,
      0.39365784839077,
      0.39653903713018,
      0.45928654944979863,
      0.4178785395665179,
      0.34710471098287843
    ],
    [
      0.42146992764858937,
      0.5607412739848314,
      0.5623457198894894,
      0.48232200929456437,
      0.5567227798673684,
      0.4672997222700306,
      0.390225644772338,
      0.48777146626308143,
      0.5437981640962548,
      0.4756877693952031,
      0.5634932059289408,
      0.31862347831693105,
      0.4618361222064291,
      0.47236713886290493,
      0.45414763682338966,
      0.5465370015324766,
      0.535211087684601,
      0.5058809459885554,
      0.5396358732268045,
      0.46993851271745024,
      0.0,
      0.40006574193833777,
      0.4657027815080186,
      0.5448733873249547,
      0.5129108650500653,
      0.5252611999615642,
      0.5353533510764843,
      0.4462596446175584,
      0.38021758313940346
    ],
    [
      0.3416049233941738,
      0.3872550521479321,
      0.38918558015712756,
      0.32551571799478696,
      0.3651891921045525,
      0.3332525799376458,
      0.28238474398393776,
      0.3123291974260447,
      0.4039114303520954,
      0.34717720005041586,
      0.33935954003621993,
      0.25094762142918947,
      0.35620265815771934,
      0.3534917952829866,
      0.34232434666466083,
      0.35176163160136675,
      0.30017170291574446,
      0.36466772615804177,
      0.3922239239532135,
      0.3546887877041369,
      0.32434034968580305,
      0.0,
      0.34210712136043697,
      0.3927921673139443,
      0.3225299955177632,
      0.3255213599828828,
      0.32486164070045875,
      0.3458582533353667,
      0.30519953024754254
    ],
    [
      0.3608364593042732,
      0.5037298149112226,
      0.43879519190730476,
      0.4713341026806297,
      0.5370843547593114,
      0.4183720623061278,
      0.36912804649485276,
      0.4049890078671188,
      0.476938472205106,
      0.4954485477548696,
      0.4766171750915149,
      0.33761795750422063,
      0.4736580544053879,
      0.4494190019198381,
      0.37909917029440443,
      0.43719878606194773,
      0.47547505479156404,
      0.45541504446921977,
      0.45894095774630705,
      0.39284921500293435,
      0.4414069547790733,
      0.4209865298632136,
      0.0,
      0.4744141001941151,
      0.4912718880655933,
      0.40788624982478594,
      0.4710822335524303,
      0.3873468260694437,
      0.39964457038838885
    ],
    [
      0.38657244302617766,
      0.5858834382151104,
      0.5795121988192822,
      0.564792408480773,
      0.5436378630990537,
      0.4171371905749146,
      0.3528383447074357,
      0.3994264935553369,
      0.5805719792520658,
      0.42346492661491864,
      0.5161743805127124,
      0.2520399232877226,
      0.45957946866236,
      0.5004094880831191,
      0.4154205095337311,
      0.5867189960428589,
      0.4423402097347511,
      0.4590243226513264,
      0.6337501105040755,
      0.4303272024078939,
      0.4444540906097212,
      0.4093827961951497,
      0.3741408491497271,
      0.0,
      0.4179127057192398,
      0.4286272484534235,
      0.48867219579073407,
      0.4331129973240795,
      0.34020832268648316
    ],
    [
      0.3242144799823703,
      0.4342526505319788,
      0.37748952473111275,
      0.3206639510533609,
      0.4241971849070896,
      0.34027109362259655,
      0.27332717923464345,
      0.3022196857716375,
      0.4334245658733611,
      0.35038434358333337,
      0.3797324471338934,
      0.2683872412040802,
      0.32878282989702434,
      0.34355001629399906,
      0.34864069778949336,
      0.358555336753865,
      0.4294099054964946,
      0.3465816695195114,
      0.36386267066462996,
      0.3251259113759297,
      0.35741220694819065,
      0.2821252900567084,
      0.37409263497870815,
      0.401804622430868,
      0.0,
      0.40575143824333204,
      0.39211605724806287,
      0.30606156110161176,
      0.25509851258784
    ],
    [
      0.3509885246359574,
      0.5661238808949021,
      0.5089663163311968,
      0.4449075765362265,
      0.4265634147839845,
      0.4128865774039987,
      0.31884464362461595,
      0.40313845084763567,
      0.429726151464525,
      0.34932923896808177,
      0.5014177262125379,
      0.28098569380528504,
      0.41520009147073234,
      0.46829460681941293,
      0.4558306128353671,
      0.5442997297665058,
      0.40658106677497163,
      0.4253763503207171,
      0.531958648028851,
      0.4012470102268051,
      0.4176313753835963,
      0.3458715819521718,
      0.38469612436975353,
      0.49099397683895196,
      0.4234704915028187,
      0.0,
      0.46746695168257224,
      0.42310385869510814,
      0.35518228414316155
    ],
    [
      0.3471875664763109,
      0.44180390308645,
      0.43310262540388966,
      0.4159216323118453,
      0.43558166484782057,
      0.3877780295757811,
      0.30585047222114303,
      0.33532087572780256,
      0.4332838296018322,
      0.4459007495441101,
      0.43571731236400146,
      0.29754654687113713,
      0.3579726556576037,
      0.36955604601180814,
      0.43750022663935995,
      0.43145548180580007,
      0.4556660786174598,
      0.38773883798118636,
      0.46456438882770557,
      0.4427772285931717,
      0.4348588809857401,
      0.3113505214567742,
      0.3887113791121477,
      0.500668874692926,
      0.5088886103633978,
      0.4634598605235054,
      0.0,
      0.36314687892240727,
      0.28307561315126906
    ],
    [
      0.3548788896862958,
      0.4478442731447072,
      0.48150518449598056,
      0.43882142939079416,
      0.40837765189226416,
      0.31633972808885624,
      0.35103192851190035,
      0.3694208607458289,
      0.4484607634363642,
      0.4015233400313374,
      0.4790254193255876,
      0.2751834913872153,
      0.37759336864217685,
      0.37209909071419234,
      0.359044329329435,
      0.4591313176884029,
      0.3837938260718685,
      0.3703359125243957,
      0.46557115897206036,
      0.46135857941500835,
      0.4129318390129184,
      0.3766681275853343,
      0.34779777772204556,
      0.4889380308488973,
      0.38159915374633746,
      0.45033781721399735,
      0.4211485181041421,
      0.0,
      0.3253265301704089
    ],
    [
      0.3355123864993661,
      0.37562708804846245,
      0.3652835189214323,
      0.34920627262814863,
      0.36031873396971736,
      0.33358662479234025,
      0.31089374510457324,
      0.3242444720244988,
      0.3681667277209013,
      0.3658942383016428,
      0.3528563325717202,
      0.34042811489646274,
      0.3745745389458579,
      0.36965092361814755,
      0.33916926264712655,
      0.37769946636908913,
      0.34636612828301483,
      0.3833247645902689,
      0.37094117486346834,
      0.3259058749892503,
      0.3484956890335207,
      0.3916751545549162,
      0.35783335099027846,
      0.38206247393087955,
      0.3808828856304505,
      0.3562435946860387,
      0.34725868122949954,
      0.34693960616604014,
      0.0
    ]
  ],
  "row_avgs": [
    0.18417226346694318,
    0.40780284845195464,
    0.38534188210534676,
    0.43278397057388557,
    0.39518393680192476,
    0.3993186112065623,
    0.3596620937393702,
    0.3902554512513392,
    0.4174239024708025,
    0.4274175464184962,
    0.49057874432733817,
    0.240976142845793,
    0.34301820660290677,
    0.45645380798036556,
    0.42817578767865144,
    0.4136233459153413,
    0.36862161557218404,
    0.4210146967979694,
    0.4654974066385024,
    0.3980993897632029,
    0.48666785840666504,
    0.3420305631998639,
    0.4395352082219714,
    0.45950475370336347,
    0.3516977038934188,
    0.4268243912971587,
    0.4041566704062281,
    0.4009317263535269,
    0.35646577950025404
  ],
  "col_avgs": [
    0.35517429077158996,
    0.4617145659166981,
    0.447288641115361,
    0.4241224040461661,
    0.4370163514857043,
    0.37224578414800585,
    0.33374950188162067,
    0.36274451650856815,
    0.46076395922218516,
    0.3841559638550716,
    0.43837342825137593,
    0.2728407167290978,
    0.40536154925900914,
    0.4124645040867089,
    0.36921279609314206,
    0.44833367006025393,
    0.39266334430442773,
    0.4002733692669567,
    0.46075546444580395,
    0.37269288157072394,
    0.38847070015480434,
    0.35879922188429825,
    0.3706338617633577,
    0.4719693348762353,
    0.39472998700717643,
    0.3958783139390869,
    0.41190995834566957,
    0.36874831318125023,
    0.32014891142098095
  ],
  "combined_avgs": [
    0.2696732771192666,
    0.4347587071843264,
    0.41631526161035387,
    0.4284531873100258,
    0.4161001441438145,
    0.38578219767728406,
    0.34670579781049543,
    0.37649998387995365,
    0.43909393084649384,
    0.4057867551367839,
    0.46447608628935705,
    0.25690842978744544,
    0.3741898779309579,
    0.43445915603353724,
    0.3986942918858968,
    0.4309785079877976,
    0.3806424799383059,
    0.41064403303246305,
    0.46312643554215316,
    0.3853961356669634,
    0.4375692792807347,
    0.3504148925420811,
    0.40508453499266456,
    0.4657370442897994,
    0.3732138454502976,
    0.41135135261812283,
    0.4080333143759488,
    0.3848400197673886,
    0.33830734546061747
  ],
  "gppm": [
    574.4351828751268,
    554.0461826414711,
    558.8700286533647,
    569.1935306011206,
    564.4625335606071,
    589.6927013123604,
    604.7060267651902,
    589.7312932230692,
    548.7456708902722,
    583.2235889701188,
    563.5777822487236,
    634.4560184758118,
    577.7596078364012,
    575.5456289791193,
    590.418121619282,
    556.4048199586085,
    580.2657218525015,
    577.5622364242279,
    550.1552400936727,
    589.5529867872021,
    586.1812659307758,
    597.0625252505232,
    590.0632123457826,
    548.3210759862783,
    580.8456425653068,
    577.928346685264,
    574.9007011719666,
    592.9627607849404,
    610.4427037415579
  ],
  "gppm_normalized": [
    1.3431197523241571,
    1.2299940895502093,
    1.2386765765179462,
    1.2689693335246444,
    1.2451378053553939,
    1.3081432212331188,
    1.354001083178425,
    1.3072844653772557,
    1.2110741102413278,
    1.2867165554674258,
    1.2419574744196533,
    1.4188891943145916,
    1.2784511348096286,
    1.2762601586790763,
    1.3103335851863562,
    1.238295238793445,
    1.2787447328914523,
    1.2834943310670843,
    1.2134468346157241,
    1.3027042417013557,
    1.2954815446713492,
    1.3250371764551123,
    1.3044323724088194,
    1.2089812815012653,
    1.2791568328561798,
    1.2857876249058489,
    1.2669391663299518,
    1.314525332686826,
    1.345515364828238
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401,
    389,
    491,
    424,
    450,
    396,
    441,
    392,
    376,
    422,
    476,
    440,
    374,
    446,
    482,
    442,
    441,
    362,
    424,
    410,
    414,
    410,
    353,
    400,
    382,
    444,
    360,
    404,
    406,
    395,
    339,
    446,
    477,
    497,
    454,
    431,
    442,
    455,
    560,
    421,
    424,
    522,
    488,
    411,
    400,
    410,
    451,
    469,
    411,
    433,
    451,
    378,
    376,
    454,
    411,
    399,
    394,
    441,
    372,
    1166,
    412,
    405,
    456,
    421,
    410,
    395,
    446,
    422,
    371,
    413,
    468,
    458,
    414,
    419,
    422,
    375,
    410,
    404,
    390,
    423,
    455,
    433,
    401,
    393,
    441,
    387,
    438,
    457,
    475,
    438,
    496,
    454,
    447,
    436,
    483,
    459,
    425,
    390,
    418,
    596,
    408,
    457,
    477,
    438,
    347,
    421,
    393,
    423,
    422,
    403,
    347,
    442,
    425,
    397,
    418,
    466,
    401,
    668,
    451,
    488,
    442,
    477,
    399,
    415,
    391,
    477,
    467,
    410,
    552,
    421,
    402,
    423,
    411,
    434,
    392,
    381,
    432,
    411,
    388,
    434,
    406,
    404,
    403,
    408,
    482,
    364,
    515,
    394,
    445,
    456,
    590,
    399,
    441,
    475,
    430,
    427,
    416,
    502,
    397,
    443,
    381,
    435,
    391,
    431,
    382,
    457,
    431,
    379,
    342,
    407,
    378,
    371,
    372,
    478,
    412,
    676,
    441,
    444,
    451,
    462,
    493,
    380,
    407,
    445,
    468,
    464,
    604,
    461,
    488,
    449,
    498,
    445,
    422,
    409,
    421,
    392,
    428,
    402,
    455,
    441,
    429,
    408,
    494,
    372,
    307,
    405,
    426,
    444,
    444,
    447,
    430,
    442,
    415,
    412,
    410,
    547,
    406,
    431,
    431,
    401,
    422,
    414,
    409,
    427,
    431,
    407,
    386,
    438,
    364,
    436,
    366,
    452,
    371,
    499,
    431,
    463,
    475,
    444,
    451,
    385,
    460,
    462,
    388,
    408,
    370,
    480,
    462,
    364,
    435,
    420,
    473,
    381,
    480,
    406,
    350,
    421,
    441,
    416,
    367,
    438,
    468,
    386,
    745,
    461,
    466,
    403,
    398,
    400,
    445,
    399,
    439,
    369,
    381,
    515,
    427,
    400,
    396,
    475,
    427,
    413,
    414,
    424,
    362,
    378,
    383,
    411,
    394,
    387,
    398,
    416,
    403
  ],
  "response_lengths": [
    3428,
    2612,
    2609,
    2357,
    2324,
    2326,
    2668,
    2307,
    2557,
    2169,
    2165,
    2795,
    2521,
    2288,
    2343,
    2760,
    2559,
    2428,
    2388,
    2484,
    2017,
    2146,
    2276,
    2321,
    2333,
    2050,
    2274,
    2217,
    2283
  ]
}